,query,data
0,debugging-a-mixed-python-and-c-language-stack,"Originally published at:			https://developer.nvidia.com/blog/debugging-mixed-python-and-c-language-stack/
Debugging is difficult. Debugging across multiple languages is especially challenging, and debugging across devices often requires a team with varying skill sets and expertise to reveal the underlying problem.  Yet projects often require using multiple languages, to ensure high performance where necessary, a user-friendly experience, and compatibility where possible. Unfortunately, there is no single programming…Powered by Discourse, best viewed with JavaScript enabled"
1,new-deep-learning-based-denoising-model-improves-microscopy-images-by-16x,"Originally published at:			New Deep Learning-Based Denoising Model Improves Microscopy Images by 16x | NVIDIA Technical Blog
To help accelerate microscopic systems, researchers from the Salk Institute, the University of Texas at Austin, fast.AI, and others, developed a new AI-based microscopy approach that has the potential to make microscopic techniques used for brain imaging 16 times faster.  “Point scanning imaging systems are perhaps the most widely used tools for high-resolution cellular and…Powered by Discourse, best viewed with JavaScript enabled"
2,guide-to-computer-vision-why-it-matters-and-how-it-helps-solve-problems,"Originally published at:			https://developer.nvidia.com/blog/guide-to-computer-vision-why-it-matters-and-how-it-helps-solve-problems/
This post unpacks the term computer vision. It answers how it works, explores common tasks and use cases, and invites you to get started.The human vision system is a wonder!  By design, computer vision is a marvel, a technology software developers and data scientists are using to enable and enhance existing and emerging industry applications.  What types of real-world problems and solutions are you developing with computer vision?  Have an interesting project?  Share here or write us at cv-dev@nvidia.com.Powered by Discourse, best viewed with JavaScript enabled"
3,winning-team-sets-record-at-sc16-student-cluster-competition,"Originally published at:			Winning Team Sets Record at SC16 Student Cluster Competition | NVIDIA Technical Blog
A team from the University of Science and Technology of China (USTC) used 16 NVIDIA GPUs to pull off an SC first, winning both the first place prize for highest overall score and the highest LINPACK run at the SC16 supercomputing show in Salt Lake City, Utah. Held every at the annual SC conference, teams…Powered by Discourse, best viewed with JavaScript enabled"
4,detecting-obstacles-and-drivable-free-space-with-radarnet,"Originally published at:			https://developer.nvidia.com/blog/detecting-obstacles-and-drivable-free-space-with-radarnet/
Detecting drivable free space is key for robust AV perception. RadarNet DNN is an AI-based free space detection system using only radar detections as an input.Powered by Discourse, best viewed with JavaScript enabled"
5,accelerating-digital-pathology-workflows-using-cucim-and-nvidia-gpudirect-storage,"Originally published at:			https://developer.nvidia.com/blog/accelerating-digital-pathology-workflows-using-cucim-and-nvidia-gpudirect-storage/
Learn how GPU-accelerated toolkits improve the input/output performance and image processing tasks for digital pathology workflows.Use of AI in Whole Slide Imaging (WSI) applications is growing. Please share what I/O or computational bottlenecks that you encounter in your use cases for digital pathology.Powered by Discourse, best viewed with JavaScript enabled"
6,cybersecurity-workshops-at-gtc-2022,"Originally published at:			Conference Session Catalog | GTC 2022 | NVIDIA
Discover the latest cybersecurity tools and trends with NVIDIA Deep Learning Institute workshops at GTC 2022.Powered by Discourse, best viewed with JavaScript enabled"
7,creating-3d-visualizations-from-x-ray-data-with-deep-learning,"Originally published at:			https://developer.nvidia.com/blog/creating-3d-visualizations-from-x-ray-data-with-deep-learning/
Using AI researchers have developed a new method for turning X-ray data into 3D visualizations, hundreds of times faster than traditional methods.In case this forum is still open, is this technology applicable to creating 3D visualizations from medical x-rays that visualize the structure of the body from  anterior-posterior and lateral views?@momentumhealth  – That would be a great question for the original researcher team!Powered by Discourse, best viewed with JavaScript enabled"
8,new-on-ngc-simplify-and-unify-biomedical-analytics-with-vyasa,"Originally published at:			https://developer.nvidia.com/blog/new-on-ngc-simplify-and-unify-biomedical-analytics-with-vyasa/
Vyasa, a leading provider of tools in the field of biomedical analytics, developed a suite of products that efficiently integrate with existing compute infrastructure via an extensive RESTful API architecture.Powered by Discourse, best viewed with JavaScript enabled"
9,streamlining-kubernetes-networking-in-scale-out-gpu-clusters-with-the-new-nvidia-network-operator-1-0,"Originally published at:			https://developer.nvidia.com/blog/streamlining-kubernetes-networking-in-scale-out-gpu-clusters-with-the-new-nvidia-network-operator-1-0/
NVIDIA EGX contains the NVIDIA GPU Operator and the new NVIDIA Network Operator 1.0 to standardize and automate the deployment of all the necessary components for provisioning Kubernetes clusters. Now released for use in production environments, the NVIDIA Network Operator makes Kubernetes networking simple and effortless for bringing AI to the enterprise.Powered by Discourse, best viewed with JavaScript enabled"
10,zebra-medical-vision-wins-nvidia-s-inception-champion-award,"Originally published at:			Zebra Medical Vision Wins NVIDIA’s “Inception Champion” Award | NVIDIA Technical Blog
Big data visualization startup Zebra Medical Vision took home the inaugural award  for their significant advancements of Artificial Intelligence and deep learning using NVIDIA GPUs. The world has an aging population and the demand for medical imaging services is quickly outpacing the supply of radiologists. Israel-based Zebra Medical Vision uses big data to deliver large-scale…Powered by Discourse, best viewed with JavaScript enabled"
11,webinar-how-telcos-transform-customer-experiences-with-conversational-ai,"Originally published at:			How Telcos Transform Customer Experiences with Conversational AI
Join Infosys, Quantiphi, Talkmap, and NVIDIA on May 31 for a live webinar to learn how telecommunications companies are using AI to improve operational efficiency and enhance customer engagement.Powered by Discourse, best viewed with JavaScript enabled"
12,building-an-edge-strategy-cost-factors,"Originally published at:			https://developer.nvidia.com/blog/building-an-edge-strategy-cost-factors/
Edge computing is a new paradigm shift for organizations, this article looks at the cost factors for rolling out these solutions.Powered by Discourse, best viewed with JavaScript enabled"
13,turn-2d-images-into-immersive-3d-scenes-with-nvidia-instant-nerf-in-vr,"Originally published at:			Turn 2D Images into Immersive 3D Scenes with NVIDIA Instant NeRF in VR | NVIDIA Technical Blog
Thousands of developers and content creators have built stunning 3D visuals with NVIDIA Instant NeRF, a rendering tool that turns a set of static images into a realistic 3D scene. Now, it is also possible to navigate Instant NeRF in VR and step into 3D creations with the latest Instant NeRF software update. Named by…Powered by Discourse, best viewed with JavaScript enabled"
14,from-munch-to-hunch-ai-classifies-your-waste-at-lunch,"Originally published at:			From Munch to Hunch: AI Classifies Your Waste at Lunch | NVIDIA Technical Blog
By Karly Hou, Shruthi Jaganathan, Isaac Wilcove Compost, recycle, or non-compost? The debate rages in your mind as you pace back and forth with your coffee cup in hand. With the help of the Green Machine, developed by three high school interns at NVIDIA this summer, classifying your waste has never been easier. The three…Powered by Discourse, best viewed with JavaScript enabled"
15,explainer-what-is-an-autonomous-truck,"Originally published at:			What Is an Autonomous Truck? | NVIDIA Blog
Autonomous trucks are commercial vehicles that use AI to automate everything from shipping yard operations to long-haul deliveries.Powered by Discourse, best viewed with JavaScript enabled"
16,machine-learning-frameworks-interoperability-part-3-zero-copy-in-action-using-an-e2e-pipeline,"Originally published at:			https://developer.nvidia.com/blog/machine-learning-frameworks-interoperability-part-3-zero-copy-in-action-using-an-e2e-pipeline/
Machine Learning Frameworks Interoperability part 3 covers on the implementation of an end-to-end pipeline demonstrating the discussed techniques for optimal data transfer across data science frameworks.Powered by Discourse, best viewed with JavaScript enabled"
17,microsoft-releases-new-version-of-high-performance-open-source-deep-learning-toolkit,"Originally published at:			Microsoft Releases New Version of High-Performance, Open-Source, Deep Learning Toolkit | NVIDIA Technical Blog
Previously known as CNTK, the Microsoft Cognitive Toolkit version 2.0 allows developers to create, train, and evaluate their own neural networks that can scale across multiple GPUs and multiple machines on massive data sets. The open-source toolkit, available on GitHub, offers hundreds of new features, performance improvements and fixes that have been added since the…Powered by Discourse, best viewed with JavaScript enabled"
18,explain-your-machine-learning-model-predictions-with-gpu-accelerated-shap,"Originally published at:			https://developer.nvidia.com/blog/explain-your-machine-learning-model-predictions-with-gpu-accelerated-shap/
Learn how to train an XGBoost model, implement the SHAP technique in Python using a CPU and GPU, and compare results between the two.Thanks for checking out my blog on GPU accelerated SHAP values. Let me know if you have any questions about the blog or the code implementation.First off, thanks a lot for such a good blog!
I think I should check something what I missed.At Summary_plot,
it raiseand the progress never stop. what should I do?Hi @94cogus ,Firstly, apologies for replying late. I inadvertently missed the notification to your query. As for your issue, I tried it on my end but I encountered no error. Here is the link to the code that I tried let me know if you are still having issues reproducing the results : Google ColabHi! Can I use a summary plot with the implementation of SHAP on RAPIDS? I already tried, but it doesn’t results. Are there any way? Thank you!Powered by Discourse, best viewed with JavaScript enabled"
19,just-released-cutlass-v2-9,"Originally published at:			https://developer.nvidia.com/blog/just-released-cutlass-v2-9/
Powered by Discourse, best viewed with JavaScript enabled"
20,researchers-develop-an-ai-system-that-can-help-locate-missing-vehicles,"Originally published at:			Researchers Develop an AI System that Can Help Locate Missing Vehicles | NVIDIA Technical Blog
To help monitor traffic conditions, locate missing vehicles, and possibly help find lost children, University of Toronto and Northeastern University researchers developed a deep learning-based vehicle image search engine that can be used to narrow down the location of a vehicle. “We developed the Vehicle Image Search Engine (VISE) to support the fast search of…Powered by Discourse, best viewed with JavaScript enabled"
21,share-your-science-unraveling-membrane-proteins-with-gpus,"Originally published at:			Share Your Science: Unraveling Membrane Proteins with GPUs | NVIDIA Technical Blog
Erik Lindahl, Professor of Biophysics at Stockholm University, talks about using the GPU-accelerated GROMACS application to simulate protein dynamics. This approach helps researchers learn how to design better drugs, combat alcoholism and understand how certain diseases occur. Lindahl mentions they started using CUDA in their work nearly five years ago and now 90% of their…Powered by Discourse, best viewed with JavaScript enabled"
22,training-with-custom-pretrained-models-using-the-nvidia-transfer-learning-toolkit,"Originally published at:			https://developer.nvidia.com/blog/training-custom-pretrained-models-using-tlt/
Get notified about production releases: DeepStream SDK 5.0 Transfer Learning Toolkit 2.0 Supervised training of deep neural networks is now a common method of creating AI applications. To achieve accurate AI for your application, you generally need a very large dataset especially if you create from scratch. Gathering and preparing a large dataset and labeling…Powered by Discourse, best viewed with JavaScript enabled"
23,if-the-virtual-zapato-fits-wear-it-gpu-accelerated-augmented-reality,"Originally published at:			https://developer.nvidia.com/blog/cuda-spotlight-gpu-accelerated-augmented-reality/
This week’s Spotlight is on Néstor Gómez, CEO of Artefacto Estudio in Mexico City. Artefacto Estudio is a developer of interactive applications and games. The company’s projects include a real-time virtual shoe fitting kiosk that allows people to “try on” shoes using augmented reality powered by Microsoft Kinect and GPU computing (see the video). NVIDIA: Néstor,…Powered by Discourse, best viewed with JavaScript enabled"
24,deep-learning-helps-transfer-famous-artistic-styles-to-videos,"Originally published at:			Deep Learning Helps Transfer Famous Artistic Styles to Videos | NVIDIA Technical Blog
A new style transfer algorithm makes clips from Ice Age appear as paintings in the style of van Gogh’s “Starry Night”. Researchers from University of Freiburg in Germany are using cuDNN deep learning software and NVIDIA TITAN X GPUs to extract a specific artistic style from a source painting, and then synthesizes this information with…Powered by Discourse, best viewed with JavaScript enabled"
25,maximizing-network-performance-for-storage-with-nvidia-spectrum-ethernet,"Originally published at:			https://developer.nvidia.com/blog/maximizing-network-performance-for-storage-with-nvidia-spectrum-ethernet/
NVIDIA accelerated Ethernet can remove performance bottlenecks, enabling maximum storage performance for applications in general, and AI/ML in particular.Powered by Discourse, best viewed with JavaScript enabled"
26,speech-ai-spotlight-how-pendulum-nabs-harmful-narratives-online,"Originally published at:			https://developer.nvidia.com/blog/speech-ai-spotlight-how-pendulum-nabs-harmful-narratives-online/
Over 55% of the global population uses social media, easily sharing online content with just one click. While connecting with others and consuming entertaining content, you can also spot harmful narratives posing real-life threats. That’s why VP of Engineering at Pendulum, Ammar Haris, wants his company’s AI to help clients to gain deeper insight into…Enjoyed writing this spotlight piece. I appreciate the work that Pendulum is doing to discover and call out harmful narratives, which will become even more important with the advent of text-based Generative AI.What are your thoughts on applying AI for good in other ways?Powered by Discourse, best viewed with JavaScript enabled"
27,gtc-2020-whats-new-with-nvidia-virtual-gpu-technology-march-2020,"GTC 2020 S21843
Presenters: Anne Hecht,NVIDIA; John Fanelli, NVIDIA
Abstract
NVIDIA virtual GPU (vGPU) technology transformed virtual desktop infrastructure by delivering a GPU-accelerated user experience. Innovations in NVIDIA vGPU technology enable IT to virtualize a GPU to be shared among multiple users, or enable a single user to harness the power of multiple GPUs in a virtual machine. We introduced a new way of GPU computing to virtualization and enabled increased data center efficiency when running multiple workloads. We’ll talk about the latest evolution in NVIDIA vGPU and the latest enhancements to NVIDIA vGPU software. Now you can access the power of an NVIDIA GPU from anyplace — from the data center, in the cloud, to any endpoint you choose. Learn how NVIDIA vGPU technology is accelerating data center workloads with reduced complexity and improved utilization, and find out how to select the right server and GPU configuration to suit your specific virtualization use case.Watch this session
Join in the conversation below.Powered by Discourse, best viewed with JavaScript enabled"
28,accelerating-machine-learning-on-a-linux-laptop-with-an-external-gpu,"Hi Dhruv:Our company (https://kfocus.org) is working with organizations like JPL and other big data users that have interest in eGPU.  We really appreciate your post and am very interested in pursuing providing solutions for them. However, it is not clear how to use the iGPU, dGPU, and eGPU concurrently - for example, use the dGPU for display and then the eGPU for blender rendering.  Might you have recommendations on resources for that?Any help would be greatly appreciated!Sincerely, MikeHi Druv: Is this something you can help with? Cheers, MikeHi @deppman
While there isn’t a turnkey solution to the iGPU+dGPU+eGPU problem, there are a couple of ways of going about creaing a solution.
You could set the iGPU to be the X screen renderer on battery and the dGPU to be the X screen renderer on AC. There are some tools within Ubuntu for this like gpu-manager/prime-select or you can use Offloading Graphics Display with RandR 1.4.When it comes to using the eGPU, you can use it either as a PRIME Render Offload device or a Compute device, depending on the task. PRIME Render Offload is meant for applications that require the X screen to be “rendered” on a different GPU for example Blender(an older alternative was Bumblebee). Compute is meant for CUDA/Accelerated Data Science tasks.For my machine, I’m using the eGPU as my primary X renderer as I don’t have a dGPU. For a laptop with an iGPU + dGPU + eGPU, I’d imagine that wouldn’t be the case, and if it is, then you can use “AllowExternalGpus” to use the eGPU as the primary X renderer. Otherwise, you could “Prime Render Offload” Blender to the eGPU/dGPU (depending on what is connected and what is the power source) and use the eGPU as a compute device otherwise if it is connected.In case you have both the dGPU and eGPU connected and can’t use Prime Render Offload, you’d have to rely on some other way of hiding the other GPU. A way that I’ve been using has been docker with the --gpus or NVIDIA_VISIBLE_DEVICES flag/envvar. Some other applications like OBS allow you select the GPU if you happen to have multiple GPUs in you system.Here are some links to relevant information:
http://us.download.nvidia.com/XFree86/Linux-x86_64/455.28/README/optimus.html
http://us.download.nvidia.com/XFree86/Linux-x86_64/455.28/README/randr14.html
http://us.download.nvidia.com/XFree86/Linux-x86_64/455.28/README/primerenderoffload.html
https://wiki.archlinux.org/index.php/bumblebeeThanks for the great tutorial. Is it possible to use two eGPUs as well?Hi Dsinga:Sorry for the delay, but I must have missed the notification. Thank you for all your help. We will follow up on your articles. For our customers’ purposes, the use of the eGPU as an add-on compute unit is the most common use and here the CoreX is working great, and we have been able to run dGPU + eGPU to run two separate GPGPU workloads concurrently. I will report back when we resume testing.  Thanks again!Sincerely, Mikegreat info here. I have a Razer laptop with Quadro5000RTX. I’m wondering if I get a CoreX with a Desktop Quadro 5000RTX, will I be able to use both at the same time (mobile Quadro5000+ eGPUDesktopQuadro5000) for my DL training in principle?Hi Dhruv,Really appreciate the detail your post goes into. I am however unclear about the following:Make sure that the NVIDIA GPU is detected by the system and a suitable driver is loaded:$ lspci | grep -i “nvidia”$ lsmod | grep -i “nvidia”The existing driver is most likely Nouveau, an open-source driver for NVIDIA GPUs. Because Nouveau doesn’t support eGPU setups, install the NVIDIA CUDA and NVIDIA drivers instead. You must also stop the kernel from loading Nouveau.
Get the latest version of the NVIDIA CUDA Toolkit for your distribution. For Ubuntu 20.04, this toolkit is available from the standard repository:$ sudo apt-get install nvidia-cuda-toolkitDoes this mean if lsmod | grep -i ""nvidia"" returns nothing that I need to install the NVIDIA drivers using sudo apt-get install nvidia-cuda-toolkit? It seems to me from your post that the drivers and cuda toolkit should all be installed using that command (unless installing the drivers is outside the scope of your post).However, when I check the dependencies of nvidia-cuda-tookit on Ubuntu 20.04 using apt, there are no nvidia-driver dependencies. Are the drivers contained in some of the libnvidia or nvidia-cuda packages?Thanks for your time.EDIT: installed nvidia-cuda-toolkit and confirmed that the drivers are not installed. Thus the installation of CUDA and drivers is likely outside of the scope of this post.Sure! As long as you have enough PCIe lanes provided by your CPU+motherboard to your Thunderbolt ports(check with your laptop/NUC manufacturer or see if they have a engineering diagram for it) you can run 2 eGPUs. Use nvidia-settings to configure your displays and NVIDIA GPUsYes, if you’re trying to use your laptop dGPU or iGP to drive the display don’t add the “AllowExternalGpus” “True” to the 10-nvidia.conf xorg config file since that makes it so that the eGPU drives Xorg and thus the displays. Most machines also support connecting an eGPU after boot(although disconnecting while booted can cause Xorg to crash or a kernel panic).
The Desktop Quadro 5000 would show up as another GPU in your system along with your mobile Quadro 5000. Then you’d have to structure your code to leverage multi-GPU through layer or model parallelism. That said, you might find a difference in the bandwidth between your eGPU and dGPU, so in order to get the best training performance you should profile to see if you’re bandwidth bound for the eGPU and if you are, have different batch sizes based on the ability of the GPU to iterate through it.lsmod returns a list of modules loaded by the Linux kernel. If the $lspci | grep -i “nvidia” command shows that there is a NVIDIA GPU connected to the PCI bus, and $lsmod | grep -i “nvidia” returns nothing, you either don’t have the NVIDIA driver installed, or there’s something wrong with the driver installation that doesn’t allow the kernel to load it which is very unlikely. Regarding nvidia-cuda-toolkit providing drivers, it should provide you with NVIDIA driver. Once you install nvidia-cuda-toolkit, what’s the output of $nvidia-smi and $lsmod | gpre -i “nvidia”. It might be that nvidia-cuda-toolkit installed the driver but didn’t add the location of the nvidia-smi binary to PATH.So I have to be honest here and say that this isnt the right method for my system. I found as long as I enabled thunderbolt I could see both the GPU in my laptop and I could also use the eGPU be recognized for CUDA. The minute I added in the comment on “AllowExternalGpus” “True” in Grub, I lost the ability to use the HDMI input form my eGPU. This command would show up in my display setting as a second display but my mouse wouldnt move anywhere. In the end I went back to level 3 and my external monitor coming from the eGPU showed up without this command. the nvidia-smi command was showing it before hand. So I am not sure this should be kept around as an instruction…What does the command actually do? It doesnt really make sense to me. If you have a thunderbolt and you check the nvidia-smi showing both are connected. I think the blog should give a pre warning at the point of the section saying try. When I did this I know cant get my second monitor to get picked up even though it worked just before these changes. So a little annoyed… to say the least…Probably best to stick in a bit clearer that in your case (which you mention but should be clearer) that your set up is with a laptop without a gpu?edit:
As a warning,  this appears to basically wipe all the setting that nvidia run automatically such as turning off nouveau. Now having turned it off, the grub loads on the egpu hdmi but is shut down as soon as it boots to a gui.2nd time… after 30 mins of trying i just swapped the hdmi to my native laptop gpu and it seems to be working fine. At leats the egpu is free for other stuff and cuda 11.0 can see it with nvcc…Spent hours trying to get an eGPU to work with my laptop.  No dice.  Just a complete and total nightmare and frankly not worth spending days-to-weeks battling with poorly written software and random chipsets that may or may not work. LINUX has a long way to go in the eGPU area. Until someone makes a completely straight-forward, “drivers included with enclosure” system (like winblows) for LINUX (Ubuntu), an eGPU is a really mixed bag.  Distro, enclosure, drivers, chipsets, cards, laptops models => all can have a major effect on the ability to make this work.Not sure if this is a mistake or just something that varies between systems, but this didn’t work for me until I changed “Option “AllowExternalGpus” “True”” to “Option “AllowExternalGpus” “true””. Took me a while to locate the issue so I thought I’d try and save someone else the hassle!Thanks! Looking at xorg.conf, it looks like any of 1 , on , true , yes would be accepted as the boolean True when encased in quotation marks.Fun fact, the massively oversized PCI BAR on the Tesla K80 doesn’t play well with Thunderbolt 3 eGPU solutions.  I haven’t found a documented limitation in Thunderbolt 3, but it “doesn’t work anywhere I try it” lol.I fixed this; turns out it wasn’t me, but rather the PCI board that SONNET Breakaway Box 750 now uses is not LINUX compliant.  This was disappointing as SONNET was one of the eGPU enclosure manufacturers that were producing good quality compatible hardware.  Now they just produce Windows only (read you need a hacky driver) hardware.  Tried again with a Razer Core X eGPU enclosure and a MSI RTX 3060Ti 3x OC Ventus card.  Works like a charm!  The Razer Core series is fully compliant. I would really, really like to see hot-plug work at some time, but I gather the window manager Wayland is almost there (on Ubuntu).Hi @dsingalNV,Thanks for the informative post.! I have some difficulty with the setup though.I use a Sonic breakaway box 550 with a desktop 3070 RTX card attached to it. The laptop that I use has a 3050 Ti internal card. My application requires computing to be done on the egpu, while the xserver and all renders need to run on the internal 3050 Ti card.I use ubuntu 20.04 and installed the correct nvidia drivers. nvidia-smi only shows the internal graphics card and not the graphics card connected through thunderbolt. But lspci shows both cards and boltctl shows the correct thunder bolt device connected.  As I understand it, since I don’t want X to run on the eGPU, I don’t really have to change any xconfig files. I’m not sure what else to do.I don’t know why nvidia-smi doesn’t identify the eGPU. Can you help me out?Cheers,
VarunHi, Dhruv.I recently tried a configuration of a host with Thunderbolt 4 connected to a Thunderbolt 4 hub that is connected to three Thunderbolt-to-PCIe enclosures which each contain
a GTX 1060.Each GTX 1060 enumerated in Windows 11 22H2 and Ubuntu 22.04 (5.15-60) with current Nvidia drivers ( as of 2023-04-06).What a pleasant surprise!I used a parallel test case in MATLAB to confirm all GPUs were utilized.I do not know if this configuration works in general with other computer models, Thunderbolt devices and Nvidia GPU models.I work at a company that makes Thunderbolt devices.Could we get in touch to share notes?   Or could you introduce me to a colleague involved with product compatibility testing?You said,The existing driver is most likely Nouveau, an open-source driver for NVIDIA GPUs. Because Nouveau doesn’t support eGPU setups, install the NVIDIA CUDA and NVIDIA drivers instead. You must also stop the kernel from loading Nouveau.but no commands were provided to install the required driver for egpu and disable Nouveau@ramkumarkoppu you do not need to disable nouveau manually when installing the NVIDIA driver through apt-get. If you use the runfile method you will have to blacklist the nouveau kernel module through:
$ cat /etc/modprobe.d/blacklist-nvidia-nouveau.conf
blacklist nouveau
options nouveau modeset=0
That said, Ubuntu 22.04 and later ship with the NVIDIA driver which will be loaded by default instead of Nouveau.
And I would recommend GitHub - hertg/egpu-switcher: 🖥🐧 Setup script for eGPUs in Linux (X.Org) for switching between GPUs on a hybrid device going forward.Powered by Discourse, best viewed with JavaScript enabled"
29,leverage-deep-learning-in-scala-with-gpus-on-amazon-emr,"Originally published at:			https://developer.nvidia.com/blog/leverage-deep-learning-in-scala-with-gpus-on-amazon-emr/
With the growing interest in deep learning (DL), more and more users are using DL in production environments. Because DL requires intensive computational power, developers are leveraging GPUs to do their training and inference jobs. Recently, as part of a major Apache Spark initiative to better unify DL and data processing on Spark, GPUs became…Powered by Discourse, best viewed with JavaScript enabled"
30,running-an-ultrasound-multi-ai-pipeline-from-icardio-ai-in-real-time-with-nvidia-holoscan,"Originally published at:			https://developer.nvidia.com/blog/running-an-ultrasound-multi-ai-pipeline-from-icardio-ai-in-real-time-with-nvidia-clara-holoscan/
Discover how NVIDIA Inception member iCardio.ai leverages NVIDIA Clara Holoscan to run their multi-AI pipelines in real time, increasing the accuracy of cardiovascular diagnoses.Powered by Discourse, best viewed with JavaScript enabled"
31,learning-to-rank-with-xgboost-and-gpu,"Originally published at:			Learning to Rank with XGBoost and GPU | NVIDIA Technical Blog
XGBoost is a widely used machine learning library, which uses gradient boosting techniques to incrementally build a better model during the training phase by combining multiple weak models. Weak models are generated by computing the gradient descent using an objective function. The model thus built is then used for prediction in a future inference phase.…Powered by Discourse, best viewed with JavaScript enabled"
32,high-school-student-uses-ai-to-detect-gravitational-waves,"Originally published at:			High School Student Uses AI to Detect Gravitational Waves | NVIDIA Technical Blog
Before he could legally drive, high school student Adam Rebei was already submitting jobs on the Blue Waters supercomputer at the National Center for Supercomputing Applications at the University of Illinois at Urbana-Champaign (NCSA) to run complex simulations of black holes. “My first time using Blue Waters, we did a tour first and got to see the computer, which…Powered by Discourse, best viewed with JavaScript enabled"
33,inception-spotlight-mapillary-introduces-street-level-dataset-for-lifelong-place-recognition,"Originally published at:			Inception Spotlight: Mapillary Introduces Street-Level Dataset for Lifelong Place Recognition | NVIDIA Technical Blog
Sweden-based Mapillary, a premier member of NVIDIA Inception, uses deep learning to automate mapping. Their platform provides street-level mapping by stitching together images sourced from its community of individual contributors, companies, and governments. NVIDIA Inception is a startup accelerator. Yesterday, the company announced the release of a new product, the Mapillary Street-Level Sequences dataset, a…Powered by Discourse, best viewed with JavaScript enabled"
34,improving-biomolecular-design-with-gpu-accelerated-supercomputers,"Originally published at:			https://developer.nvidia.com/blog/improving-biomolecular-design-with-gpu-accelerated-supercomputers/
For the first time ever, chemists from the University of California, San Diego designed a two-dimensional protein crystal simulation that toggles between states of varying porosity and density. The research could help scientists create new materials for water purification, renewable energy, breakthroughs in medicine, drug development, and many more possible applications. The work of combining…Powered by Discourse, best viewed with JavaScript enabled"
35,advancing-ai-sports-analytics-through-the-data-driven-sky-engine-ai-platform-and-nvidia-rtx,"Originally published at:			https://developer.nvidia.com/blog/advancing-ai-sports-analytics-through-the-data-driven-sky-engine-ai-platform-and-rtx/
Learn how SKY ENGINE AI simulates rugby games to generate synthetic data to train sports analytics AI models.Powered by Discourse, best viewed with JavaScript enabled"
36,openseq2seq-new-toolkit-for-distributed-and-mixed-precision-training-of-sequence-to-sequence-models,"Originally published at:			OpenSeq2Seq: New Toolkit for Distributed and Mixed-Precision Training of Sequence-to-Sequence Models | NVIDIA Technical Blog
Researchers at NVIDIA open-sourced v0.2 of OpenSeq2Seq – a new toolkit built on top of TensorFlow for training sequence-to-sequence models. OpenSeq2Seq provides researchers with optimized implementation of various sequence-to-sequence models commonly used for applications such as machine translation and speech recognition. OpenSeq2Seq is performance optimized for mixed-precision training using Tensor Cores on NVIDIA Volta GPUs. With…Powered by Discourse, best viewed with JavaScript enabled"
37,new-pgi-community-edition-now-available,"Originally published at:			https://developer.nvidia.com/blog/new-pgi-community-edition-now-available/
PGI Compilers & Tools are used by scientists and engineers developing applications for high-performance computing (HPC). PGI products deliver world-class multicore CPU performance, an easy on-ramp to GPU computing with OpenACC directives, and performance portability across all major HPC platforms. Version 17.10 is available now for users with current PGI Professional support. The free PGI…Powered by Discourse, best viewed with JavaScript enabled"
38,accelerating-ai-inference-workloads-with-nvidia-a30-gpu,"Originally published at:			https://developer.nvidia.com/blog/accelerating-ai-inference-workloads-with-nvidia-a30-gpu/
A30 enables researchers, engineers, and data scientists to deliver real-world results and deploy solutions into production at scale.This is the first blog of the A30 blog series, the next blog will show you how to use MIG on A30!Powered by Discourse, best viewed with JavaScript enabled"
39,understanding-and-measuring-pc-latency,"Originally published at:			https://developer.nvidia.com/blog/understanding-and-measuring-pc-latency/
Learn about PC Latency and how to leverage PCL Stats to accurately track, measure, and improve the latency within your rendering pipeline.Powered by Discourse, best viewed with JavaScript enabled"
40,stop-modern-security-attacks-in-real-time-with-aria-cybersecurity-and-nvidia,"Originally published at:			https://developer.nvidia.com/blog/stop-modern-security-attacks-in-real-time-with-aria-cybersecurity-and-nvidia/
The newly announced ARIA Zero Trust Gateway uses NVIDIA BlueField DPUs to provide a sophisticated cybersecurity solution.Powered by Discourse, best viewed with JavaScript enabled"
41,explainer-what-is-a-smartnic,"Originally published at:			What Is a SmartNIC? | NVIDIA Blog
A SmartNIC is a programmable accelerator that makes data center networking, security and storage efficient and flexible.Powered by Discourse, best viewed with JavaScript enabled"
42,flavorgraph-serves-up-food-pairings-with-ai-molecular-science,"Originally published at:			FlavorGraph Serves Up Food Pairings with AI, Molecular Science | NVIDIA Technical Blog
A new ingredient mapping tool by Sony AI and Korea University uses molecular science and recipe data to predict how two ingredients will pair together.Powered by Discourse, best viewed with JavaScript enabled"
43,upcoming-webinar-edge-ai-and-robotics-solutions-with-nvidia,"Originally published at:			https://developer.nvidia.com/blog/upcoming-webinar-edge-ai-and-robotics-solutions-with-nvidia/
Powered by Discourse, best viewed with JavaScript enabled"
44,gtc-2020-nvidia-rapids-on-summit-supercomputer-early-experiences,"GTC 2020 S21517
Presenters: Hao Lu,Oak Ridge National Laboratory
Abstract
Learn about the ongoing efforts at the Oak Ridge Leadership Computing Facility to deploy and support data analytics workloads on the Summit supercomputer using DASK and NVIDIA Rapids. First, we’ll cover deployment and execution details of these frameworks for the Summit supercomputer. Then we’ll introduce cuDF and cuML performance results of scientific dataset manipulation and CORAL-2 benchmarks.Watch this session
Join in the conversation below.This video will NOT play.  I was able to download the slide deck PDF, but is it possible to still get access to the video?Thanks!Hi Shannon,
I confirm this is a bug, and I am checking if a video recordings is available.
I will respond back on this thread once I know.
Thanks,Powered by Discourse, best viewed with JavaScript enabled"
45,share-your-science-visualizing-200m-cybersecurity-alerts-daily-with-gpus,"Originally published at:			https://developer.nvidia.com/blog/share-your-science-visualizing-200m-cybersecurity-alerts-daily-with-gpus/
Joshua Patterson, principal data scientist of Accenture Labs shares how his team is using NVIDIA GPUs and GPU-accelerated libraries to quickly detect security threats by analyzing anomalies in large-scale network graphs. “When we can move 4 billion node graphs onto a GPU and have the shared memory of all the other GPUs and have that…Powered by Discourse, best viewed with JavaScript enabled"
46,run-jupyter-notebooks-on-google-cloud-with-new-one-click-deploy-feature-in-the-ngc-catalog,"Originally published at:			https://developer.nvidia.com/blog/run-jupyter-notebooks-on-google-cloud-with-new-one-click-deploy-feature-in-the-ngc-catalog/
Today NVIDIA announced the NGC catalog now provides a one-click deploy feature to run Jupyter notebooks on Google Cloud’s Vertex AI Workbench.Powered by Discourse, best viewed with JavaScript enabled"
47,gpu-operator-1-9-adds-support-for-dgx-a100-with-dgx-os,"Originally published at:			https://developer.nvidia.com/blog/gpu-operator-1-9-adds-support-for-dgx-a100-with-dgx-os/
GPU Operator 1.9 includes support NVIDIA DGX A100 systems with DGX OS and streamlined installation processes.Powered by Discourse, best viewed with JavaScript enabled"
48,writing-ray-tracing-applications-in-python-using-the-numba-extension-for-pyoptix,"Originally published at:			https://developer.nvidia.com/blog/writing-ray-tracing-apps-in-python-using-numba-for-pyoptix/
Using Numba and PyOptiX, NVIIDA enables you to configure ray tracing pipeline and write kernels in Python compatible with the OptiX pipeline.Thank you for reading the post! The example outlined in this blog post was adapted from the triangle example in the OptiX package. Several modifications was made to the kernel code to address the differences in python and CUDA C++. The extension currently have several hard coded constraints on some of the API calls, such as the number of payload registers for optix.Trace. While it’s trivial to expand the lowering of these methods to support more overloads, I’m exploring a more programmatic way to provide a full support to all APIs available in optix kernel. In general, this work is in active development and I’d love to hear from you while you are playing with the demo repository.I have a few higher level questions, my background is previously using CUDA for doing ray tracing based simulations for medical imaging and computed tomography projections.Does OptiX lend itself to these applications? Typically summing a path integral as a trilinear interpolation through a voxellized space.Any idea what kind of performance difference with this and a CUDA implementation?To what numerical precision does OptiX support (float or double)?Hi Michael!If you are able to say, is this work still in active development?  Using Numba to access the OptiX API would be a perfect workflow for a new project that I am starting.  I know it is not ready for prime time yet, but it would be useful to know whether there is value in planning my work to slot in to OptiX using Numba in the future, or whether I should assume that if I want to use OptiX’s features then I should assume I’ll only ever be able to use the c++ API directly.I ask because I noticed that there’s been no development in the PyOptiX repository since before this post was made.Powered by Discourse, best viewed with JavaScript enabled"
49,cuda-12-0-compiler-support-for-runtime-lto-using-nvjitlink-library,"Originally published at:			https://developer.nvidia.com/blog/cuda-12-0-compiler-support-for-runtime-lto-using-nvjitlink-library/
CUDA Toolkit 12.0 introduces a new nvJitLink library for Just-in-Time Link Time Optimization (JIT LTO) support.Hi,
some problems have annoyed me,like following statement:
""JIT LTO minimizes the impact on binary size by enabling the cuFFT library to build LTO optimized speed-of-light (SOL) kernels for any parameter combination, at runtime. This is achieved by shipping the building blocks of FFT kernels instead of specialized FFT kernels. ""
can you explain what ”the building blocks of FFT kernels“ means？ThanksThanks for the question.  I am not the FFT developer, but in general what they have done is decompose their algorithm into individual pieces.  Previously the library was very large because they provided all permutations of an algorithm.  Now they just have a handful of building blocks which they can combine into a specific permutation at runtime.  They gave a GTC talk about the work they have done which has some more details.Looking at cuFFTDx library (C++ header only) can give good insight on what can be considered FFT building blocks. Bit more summarized view from another point of view would be SIAM PP22 presentation (slide 10).Links:Powered by Discourse, best viewed with JavaScript enabled"
50,nvidia-announces-generative-ai-services-for-language-visual-content-and-biology-applications,"Originally published at:			https://developer.nvidia.com/blog/nvidia-announces-generative-ai-services-for-language-visual-content-and-biology-applications/
To enable enterprises to take advantage of the possibilities with generative AI, NVIDIA has launched NVIDIA AI Foundations and the NVIDIA NeMo framework, powered by NVIDIA DGX Cloud.Powered by Discourse, best viewed with JavaScript enabled"
51,applications-open-compute-the-cure-research-grant,"Originally published at:			Applications Open: Compute the Cure Research Grant | NVIDIA Technical Blog
The NVIDIA Foundation announced today the availability of up to two research grants worth $200,000 each as part of its Compute the Cure initiative to advance the fight against cancer. The Foundation is seeking projects that use computational omics to dramatically impact the battle against cancer and reduce the time it takes for research outcomes…Powered by Discourse, best viewed with JavaScript enabled"
52,gtc-2020-the-speechbrain-project,"GTC 2020 S21648
Presenters: Mirco Ravanelli,Mila - Université de Motréal
Abstract
SpeechBrain is an open-source project that aims to develop an all-in-one speech toolkit based on PyTorch. Our goal is to create a single, flexible, user-friendly toolkit that can be used to easily develop state-of-the-art speech technologies. SpeechBrain will be a standalone framework that can significantly speed up research and development of speech and audio processing techniques. Indeed, it’s a lot easier to familiarize oneself with a single toolkit than to learn several different frameworks, as one must do today. Moreover, using a single platform makes it easier to build a strong and fruitful community where members can share models, codes, baselines, and suggestions with a possible positive impact in the field of speech technologies. SpeechBrain is currently under development, and a first alpha version will be available in the next months. We’ll describe the motivations, goals, and current status of the project.Watch this session
Join in the conversation below.Powered by Discourse, best viewed with JavaScript enabled"
53,nvidia-announces-new-software-and-updates-to-cuda-deep-learning-sdk-and-more,"Originally published at:			https://developer.nvidia.com/blog/nvidia-announces-new-software-and-updates-to-cuda-deep-learning-sdk-and-more/
At the GPU Technology Conference, NVIDIA announced new updates and software available to download for members of the NVIDIA Developer Program. CUDA Toolkit CUDA 9.2 includes updates to libraries, a new library for accelerating custom linear-algebra algorithms, and lower kernel launch latency. With CUDA 9.2, you can: Speed up RNNs and CNNs through cuBLAS optimizations…Powered by Discourse, best viewed with JavaScript enabled"
54,jetson-project-of-the-month-qrio-an-interactive-ai-bot,"Originally published at:			https://developer.nvidia.com/blog/jetson-project-of-the-month-qrio-an-interactive-ai-bot/
Agustinus (Gus) Nalwan was awarded the Jetson Project of the Month for his interactive AI bot, Qrio. This bot, running on the NVIDIA Jetson Nano, can ask for a toy, identify and state its name and play videos related to it. Gus was inspired by the curiosity of his toddler and fondly named the bot,…Powered by Discourse, best viewed with JavaScript enabled"
55,rapids-accelerates-data-science-end-to-end,"Originally published at:			RAPIDS Accelerates Data Science End-to-End | NVIDIA Technical Blog
At GTC Europe in Munich Germany, NVIDIA announced RAPIDS, a suite of open-source software libraries for executing end-to-end data science and analytics pipelines entirely on GPUs.  RAPIDS aims to accelerate the entire data science pipeline including data loading, ETL, model training, and inference. This will enable more productive, interactive, and exploratory workflows. The RAPIDS libraries…Powered by Discourse, best viewed with JavaScript enabled"
56,nvidia-announces-arm-hpc-developer-kit-for-hpc-ai-and-scientific-computing-applications,"Originally published at:			NVIDIA Announces Arm HPC Developer Kit for HPC, AI, and Scientific Computing Applications | NVIDIA Technical Blog
This DevKit targets heterogeneous GPU/CPU system development, and includes an Arm CPU, an NVIDIA A100 Tensor Core GPU, and the NVIDIA HPC SDK suite of tools.Powered by Discourse, best viewed with JavaScript enabled"
57,gtc-2020-understanding-the-interconnected-ai-data-centers-of-the-future,"GTC 2020 S21992
Presenters: Chris Kawalek,NVIDIA; William Vick, NVIDIA
Abstract
Today’s computing challenges are outpacing the capabilities of traditional data center design. AI requires tremendous processing power that GPUs can easily provide. However, GPU-accelerated systems have different power, cooling, and connectivity needs than traditional IT infrastructure. This creates a growing need to update data center planning principles to keep pace. We’ll present strategies for AI data centers of the future. We’ll discuss design requirements for space, power, cooling, and networking, we’ll talk about edge-to-core data centers using a micro/meso/macro approach, and we’ll discuss considerations for when to leverage on-premises, colocation, cloud, hybrid cloud, and AI-as-a-service. Learn how to create an AI “center of excellence” that democratizes the use of AI across your organization and avoids the pitfalls of siloed, one-off implementations that fail to maximize ROI.Watch this session
Join in the conversation below.Powered by Discourse, best viewed with JavaScript enabled"
58,top-financial-services-sessions-at-nvidia-gtc-2022,"Originally published at:			AI Centers of Excellence | GTC 2022 | NVIDIA
Discover how Deutsche Bank, U.S. Bank, Capital One, and other firms are using AI technologies to optimize customer experience in financial services through recommender systems, NLP, and more.Powered by Discourse, best viewed with JavaScript enabled"
59,nvidia-brings-low-code-development-to-vision-ai-with-deepstream-6-0,"Originally published at:			https://developer.nvidia.com/blog/nvidia-brings-low-code-development-to-vision-ai-with-deepstream-6-0/
AI streaming analytics toolkit, DeepStream helps build high-performance, low-latency, complex, video analytics applications and services.Powered by Discourse, best viewed with JavaScript enabled"
60,develop-ai-powered-robots-smart-vision-systems-and-more-with-nvidia-jetson-orin-nano-developer-kit,"Originally published at:			https://developer.nvidia.com/blog/develop-ai-powered-robots-smart-vision-systems-and-more-with-nvidia-jetson-orin-nano-developer-kit/
Discover how easy it can be to develop robotics and edge AI applications with the new NVIDIA Jetson Orin Nano Developer Kit.Jetson Orin Nano devkit does not have a hardware encoder and I will need use cpu to encode CSI camera inputs? This seems like a step back from Jetson Nano to me. I would expect hardware encode for anything related to computer vision and robot vision these days.Where to buy? :)   I kept looking for an Amazon link or even the gear store but still seeing the previous nano there.  It would be great if the article could link to a few of the resellers.All I see is coming soon. Nothing in stock. So… Where is that Orin Nano? You get an email that says Now Available except its notThe Jetson Orin Nano Developer Kit is now available to buy from some distributors and will be available from others soon. Please see the Buy Link that is on Jetson Orin Page: NVIDIA Jetson OrinPowered by Discourse, best viewed with JavaScript enabled"
61,evaluating-hidden-costs-when-building-or-buying-an-edge-management-platform,"Originally published at:			https://developer.nvidia.com/blog/evaluating-hidden-costs-when-building-or-buying-an-edge-management-platform/
Edge computing and edge AI are powering the digital transformation of business processes. But, as a growing field, there are still many questions about what exactly needs to be in an edge management platform. The benefits of edge computing include low latency for real-time responses, using local area networks for higher bandwidth, and storage at…Powered by Discourse, best viewed with JavaScript enabled"
62,evaluating-the-performance-of-openacc-in-gcc,"Originally published at:			Evaluating the Performance of OpenACC in GCC | NVIDIA Technical Blog
A new blog details the history of the OpenACC GCC implementation, its availability, and enhancements to OpenACC support in GCC. You will also learn about a recent project to assess and improve the performance of codes compiled with GCC’s OpenACC support. The Role of OpenACC A scalar optimizing compiler has a really good day when…Powered by Discourse, best viewed with JavaScript enabled"
63,ray-tracing-in-unreal-engine-4-top-10-developer-questions,"Originally published at:			Ray Tracing in Unreal Engine 4: Top 10 Developer Questions | NVIDIA Technical Blog
We recently hosted Ray Tracing in Unreal Engine 4, a webinar now available on-demand that guides developers on best practices for producing real-time ray-traced reflections, global illumination, shadows, and more. We hosted a Q&A session, and received an overflow of excellent questions from the audience. This posts includes the top 10.Powered by Discourse, best viewed with JavaScript enabled"
64,optimizing-light-source-perception-with-software-defined-ai,"Originally published at:			https://developer.nvidia.com/blog/optimizing-light-source-perception-with-software-defined-ai/
By Igor Tryndin, Xiaolin Lin, Eric Yuan, HaeJong Seo As autonomous driving technology continues to advance, performance and functionality enhancements via software-defined AI are key to implementing the latest capabilities for safer, more efficient operation. In the latest DRIVE Labs episode, we show how software-defined AI techniques can be used to significantly improve performance and…Powered by Discourse, best viewed with JavaScript enabled"
65,monai-expands-its-horizons-with-healthcare-imaging-annotation,"Originally published at:			https://developer.nvidia.com/blog/monai-expands-its-horizons-with-healthcare-imaging-annotation/
Deep learning models have been successfully used in medical image analysis problems but they require a large, curated amount of labeled images to obtain good performance. Creating such annotations are tedious, time-consuming and typically require clinical expertise. To address this gap, Project MONAI has released MONAI Label v0.1 – an intelligent open source image labeling…Powered by Discourse, best viewed with JavaScript enabled"
66,advanced-api-performance-pipeline-state-objects,"Originally published at:			Advanced API Performance: Pipeline State Objects | NVIDIA Technical Blog
Pipeline state objects (PSOs) define how input data is interpreted and rendered by the hardware when submitting work to the GPUs. Proper management of PSOs is essential for optimal usage of system resources and smooth gameplay. Recommended: Create PSOs on worker threads asynchronously. PSO creation is where shaders compilation and related stalls happen. Start with…Powered by Discourse, best viewed with JavaScript enabled"
67,the-new-york-times-and-wrnch-developed-an-ai-model-to-improve-sports-storytelling,"Originally published at:			The New York Times and wrnch Developed an AI Model to Improve Sports Storytelling | NVIDIA Technical Blog
To improve how sporting events are covered in the news, a new AI 3D pose estimation model was recently developed by a group of researchers from The New York Times R&D group, and wrnch, an AI-computer vision company and member of the NVIDIA Inception program. The 3D pose estimation model can help extract data the human eye can easily miss and help journalists…Powered by Discourse, best viewed with JavaScript enabled"
68,ai-can-generate-synthetic-mris-to-advance-medical-research,"Originally published at:			AI Can Generate Synthetic MRIs to Advance Medical Research | NVIDIA Technical Blog
Artificial intelligence is revolutionizing how medical images are interpreted, helping medical professionals save time analyzing magnetic resonance imaging, CT scans, and X-rays. However, one of the significant challenges deep learning scientists working in the medical community face is the lack of accurate and reliable data to train their neural networks. For the first time, researchers…Powered by Discourse, best viewed with JavaScript enabled"
69,nvidia-experts-explore-robotics-gnns-and-nlp-advancements-at-the-wearedevelopers-world-congress,"Originally published at:			https://developer.nvidia.com/blog/nvidia-experts-explore-robotics-gnns-and-nlp-advancements-at-the-wearedevelopers-world-congress/
Join expert speakers and the developer community at WeAreDevelopers World Congress June 14-15 to exchange ideas, share knowledge, and facilitate networking.Powered by Discourse, best viewed with JavaScript enabled"
70,introducing-nvidia-hgx-h100-an-accelerated-server-platform-for-ai-and-high-performance-computing,"Originally published at:			https://developer.nvidia.com/blog/introducing-nvidia-hgx-h100-an-accelerated-server-platform-for-ai-and-high-performance-computing/
Introducing the NVIDIA HGX H100, a key GPU server building block powered by the Hopper architecture.Why does not put NVSwitches into the 4H100 Server in order to increase the P2P bandwidth from 300GB/s to 900GB/s?Powered by Discourse, best viewed with JavaScript enabled"
71,implementing-nvidia-highlights-plugin-for-unreal-engine-4,"Originally published at:			Implementing NVIDIA Highlights Plugin for Unreal Engine 4 | NVIDIA Technical Blog
NVIDIA Highlights (or just Highlights) is a feature of NVIDIA Shadowplay that allows players to capture in-game moments automatically based on in-game events. (More information about Shadowplay can be found here). This tutorial focuses on the Unreal Engine plugin which adds support for NVIDIA Highlights to Blueprints and C++, allowing developers to integrate the SDK into…Thank you for the great work, I've seen this feature is massively used by game casters.Hi, thanks for this, i have successfully implemented this today.Some work is needed on the documentation though, things to note:Lack of c++ documentation (would be great to have a guide on this, perhaps just a small example project would do the trick)If no default locale is set ""Highlights Configure"" fails with the error ""error"", it needs to be mentioned in the docs that this is *required*.Examples of ""Group description translation"" fields, and ""Name Translation"" fields (takes a bit to figure out with no instructions :)Thanks for all the work on this, its such a superb feature.:-)Powered by Discourse, best viewed with JavaScript enabled"
72,microsoft-announces-new-breakthroughs-in-ai-speech-tasks,"Originally published at:			Microsoft Announces New Breakthroughs in AI Speech Tasks | NVIDIA Technical Blog
Microsoft AI Research just announced a new breakthrough in the field of conversational AI that achieves new records in seven of nine natural language processing tasks from the General Language Understanding Evaluation (GLUE) benchmark. Microsoft’s natural language processing algorithm called Multi-Task DNN, first released in January and updated this month, incorporates Google’s BERT NLP model to achieve groundbreaking…Powered by Discourse, best viewed with JavaScript enabled"
73,microsoft-research-unveils-new-bert-based-biomedical-nlp-ai-model,"Originally published at:			Microsoft Research Unveils New BERT-Based Biomedical NLP AI Model | NVIDIA Technical Blog
To help accelerate natural language processing in biomedicine, Microsoft Research developed a BERT-based AI model that outperforms previous biomedicine natural languag processing (NLP) methods. The work promises to help researchers rapidly advance research in this field.  The model, built on top of Google’s BERT, can classify documents, extract medical information, detect specific name entities, and…Powered by Discourse, best viewed with JavaScript enabled"
74,share-your-science-improving-the-geolocation-accuracy-of-aerial-and-orbital-imagery-with-gpus,"Originally published at:			Share Your Science: Improving the Geolocation Accuracy of Aerial and Orbital Imagery with GPUs | NVIDIA Technical Blog
Devin White, Senior Researcher at Oak Ridge National Laboratory shares how they are using GPUs to improve the geolocation accuracy of imagery collected by a satellite, manned aircraft, or an unmanned aerial system. Using Tesla K80 GPUs and CUDA, the researchers in the Geographic Information Science and Technology Group at ORNL developed a sensor-agnostic, plugin-based…Powered by Discourse, best viewed with JavaScript enabled"
75,cvpr-2020-protect-healthcare-data-with-federated-learning,"CVPR 2020 dcv09
Presenters: Tech Demo Team, NVIDIA
Abstract
NVIDIA Clara Federated Learning is a reference application for distributed, collaborative AI model training that preserves patient privacy.Watch this session
Join in the conversation below.Powered by Discourse, best viewed with JavaScript enabled"
76,profiling-and-optimizing-deep-neural-networks-with-dlprof-and-pyprof,"Originally published at:			https://developer.nvidia.com/blog/profiling-and-optimizing-deep-neural-networks-with-dlprof-and-pyprof/
Software profiling is key for achieving the best performance on a system and that’s true for the data science and machine learning applications as well. In the era of GPU-accelerated deep learning, when profiling deep neural networks, it is important to understand CPU, GPU, and even memory bottlenecks, which could cause slowdowns in training or…We have been using these profiling tools for my deep learning models frequently and were taking notes about my experiences here and there. This blog is a good summary of those experiences. Please feel free ask any questions you might have and/or share your feedback/comments with us.Dear @ecan & @jwitsoe, thank you for your amazing post! I have just started into DL profiling, and had the following doubt.
I am trying nvidia-smi to observe distributed model training performance, and I seem to have some conflicting results. As shown in the picture below, both the gpus have full memory occupied, but if you look at GPU-Util, GPU[0] has very less utilization ( ~ 0% & fluctuating with wide range) when compared to GPU[1] (consistent around 100%). Now when I look at Pwr:Usage/Cap, one can observe the opposite trend, GPU[0] has high power usage (range 120-240W) as compared to GPU[1] (consistent ~110-120W).
nvidia-smi777×327 36.6 KB
Now as mentioned in the blog post, Pwr:Usage and Volatile GPU Util should be correlated, but as in this case one can see opposite trends. So my query is what should I consider while monitoring my model performance, and is there any way I could improve upon those numbers so that both my GPUs are used in its full capacity while distributed training.System/Program info:
Dell Precision Tower 7920 (Single Machine)
Multiple GPUs: 2x Nvidia RTX 2080 Ti
Python 3.8
Pytorch: 1.7.1
Model: Self-Attention based visual model
Using mixed precision distributed data training along with PyTorch AutocastKindly let me know if any other information is needed. Thanks in advance for your help.Hi @yashasvi1997,
Thanks for the question and reading the post :).I would recommend upgrading the CUDA version as well as the Driver version to make sure that we are getting these results even with the upgraded drivers.Second thing to try is to use the PyTorch container from the NGC. NGC containers has the best settings for the NVIDIA hardware. If you are getting the similarIf above does not help, I have another thought:
I believe the first GPU does the reduce operation. Possibly GPU1 does its processing (so is GPU0) and then GPU0 waits some numbers from GPU1 (e.g., gradients).Given that now you have a little bit more complicated scenario and our naive initial weapon didn’t solve it. I would recommend you to start experimenting DLProf. I would even go a little bit further and try NSight to have a deeper analysis.
Even though nvidia-smi is a great tool, we might need a bit more analysis with other NVIDIA tools.Please let me know if any of these help.ThanksThank you @ecan for your quick response!
Sure I’ll look into these options. My CUDA version is 10.2 and the Nvidia-Driver version is 440.33.01I think using Nvidia NGC container seems to be the most optimized option. Just a small question though, does GeForce RTX 2080 Ti support running NGC containers? Because a quick search shows that support’s available for DGX/Titan/Quadro Pcs only. (I’ll definitely search more on this and revert back if the problem still prevails).Thanks again for your support.@yashasvi1997 I still recommend updating your driver and CUDA. NGC should work with your GPU - I haven’t tested myself though.Hi,
Is there a possibility to profile the NVLink traffic?Hi @mohsin.shaikh, you can monitor NVLink traffic using nvidia-smi command line tool in terminal.
e.g. while you workload/model is running
user@dgx:~$ nvidia-smi nvlink -h
nvlink – Display NvLink information.
Usage: nvidia-smi nvlink [options]
Options include:
[-h | --help]: Display help information
[-i | --id]: Enumeration index, PCI bus ID or UUID.
[-l | --link]: Limit a command to a specific link.  Without this flag, all link information is displayed.
[-s | --status]: Display link state (active/inactive).
[-c | --capabilities]: Display link capabilities.
[-p | --pcibusid]: Display remote node PCI bus ID for a link.
[-R | --remotelinkinfo]: Display remote device PCI bus ID and NvLink ID for a link.
[-sc | --setcontrol]: Setting counter control is deprecated!
[-gc | --getcontrol]: Getting counter control is deprecated!
[-g | --getcounters]: Getting counters using option -g is deprecated.
Please use option -gt/–getthroughput instead.
[-r | --resetcounters]: Resetting counters is deprecated!
[-e | --errorcounters]: Display error counters for a link.
[-ec | --crcerrorcounters]: Display per-lane CRC error counters for a link.
[-re | --reseterrorcounters]: Reset all error counters to zero.
[-gt | --getthroughput]: Display link throughput counters for specified counter type
The arguments consist of character string representing the type of traffic counted:
d: Display tx and rx data payload in KiB
r: Display tx and rx data payload and protocol overhead in KiB if supporteduser@dgx:~$ nvidia-smi nvlink -gt d
GPU 0: Tesla V100-SXM2-16GB (UUID: GPU-a01f73b9-e8c9-89e5-6c19-1beaa6d64907)
Link 0: Data Tx: 0 KiB
Link 0: Data Rx: 0 KiB
Link 1: Data Tx: 0 KiB
Link 1: Data Rx: 0 KiB
Link 2: Data Tx: 0 KiB
Link 2: Data Rx: 0 KiB
Link 3: Data Tx: 0 KiB
Link 3: Data Rx: 0 KiB
Link 4: Data Tx: 0 KiB
Link 4: Data Rx: 0 KiB
Link 5: Data Tx: 0 KiB
Link 5: Data Rx: 0 KiB
GPU 1: Tesla V100-SXM2-16GB (UUID: GPU-bb6446d0-c867-43e5-1eae-7ced263f2372)
Link 0: Data Tx: 0 KiB
Link 0: Data Rx: 0 KiBHi  @ecan
I‘m trying to use Dlprof to profile my  tensorflow  script  ,  and want to  evaluate   training & eval  step  separately  .
Found <nsys_profile_range > parameter  in  section 4.6 of dlprof  user guide，But how could I set the start or stop point just for traing section ？
ngc : nvcr.io/nvidia/tensorflow:21.06-tf1-py3Thanks  for your helpHi,
How about disabling eval and doing training first and profile it? Then you can profile the eval with no training. Would that work?
ThanksThanks  for your help  @ecan !
Yes,  it can be work if disabled  training step .
By the way ,  cloud  I set  profile.start()  &  profile.end() around sess.run to profile separately, or that’s  only useful for pytorch dlprof？
ThanksThat is in pytorch. I believe tf has something similar but I couldn’t remember right now. I will update here if I find it. ThanksHi ecan ,   @ecan
Now I try to batch profile of different networks inference  by trtexec in one shell script，such as：xx.trt is  build from onnx  file by tensorRT.However, an error will be reported that the database is locked on the network at randomIf it’s possible, could you tell me what’s the reason and how to avoid it .Can you please check how many files / folders 1 dlprof call creates? dlprof uses default names for a range out output files / folders and this error seemed to me like a file from the previous call is still locked while another one is trying to write to that file. Thanks.Powered by Discourse, best viewed with JavaScript enabled"
77,nvidia-wins-major-competitions-and-awards-at-cvpr-2018,"Originally published at:			NVIDIA Wins Major Competitions and Awards at CVPR 2018 | NVIDIA Technical Blog
This week at the annual Computer Vision and Pattern Recognition Conference in Salt Lake City, Utah, NVIDIA researchers won several awards and competitions for their work in furthering AI technologies. The research teams, led by Jan Kautz, Bryan Catanzaro, and John Zedlewski, were awarded for the following competitions: Robust Vision Challenge – Optical Flow Category:…Powered by Discourse, best viewed with JavaScript enabled"
78,designing-a-new-net-for-phishing-detection-with-nvidia-morpheus,"Originally published at:			https://developer.nvidia.com/blog/designing-a-new-net-for-phishing-detection-with-nvidia-morpheus/
NVIDIA Morpheus, now available for download, enables you to achieve up to 99% accuracy for phishing detection, using AI.Powered by Discourse, best viewed with JavaScript enabled"
79,nvidia-first-to-offer-pure-sonic,"Originally published at:			https://developer.nvidia.com/blog/nvidia-first-to-offer-pure-sonic/
SONiC (Software for Open Networking in the Cloud) is a Linux-based, open network operating system that has been hardened in the data centers of some of the largest cloud-service providers. SONiC was initiated by Microsoft in 2016 and contributed to the Open Compute Project (OCP) a year later and continues to gain strong momentum with…Powered by Discourse, best viewed with JavaScript enabled"
80,gtc-2020-new-generation-speech-technology-revolutionizing-virtual-environments,"GTC 2020 S21963
Presenters: Miikka Rosendahl,ZOAN
Abstract
Learn how speech control can make VR environments more natural and intuitive. Tech startups from Finland (Speechly and ZOAN) are creating a tool for virtual environments using speech control to make the experience more human-centric. We’ll explain the prototype tool for voice control in VR and its huge potential. In one demo, the user can move around a virtual apartment and change the decor easily, just by speaking. The goal is to find out how virtual experiences can become more immersive by using natural communication and eliminating obstacles to interaction. This is a step toward smoothing the interaction between humans and data.Watch this session
Join in the conversation below.Powered by Discourse, best viewed with JavaScript enabled"
81,choosing-the-right-storage-for-enterprise-ai-workloads,"Originally published at:			http://127.0.0.1:8089/choosing-the-right-storage-for-enterprise-ai-workloads/
Effective enterprise AI requires the right storage for specific workloads. Storage decisions based on performance and affordability are key.Powered by Discourse, best viewed with JavaScript enabled"
82,nvidia-releases-updates-to-cuda-x-ai-at-cvpr,"Originally published at:			https://developer.nvidia.com/blog/nvidia-releases-updates-to-cuda-x-ai-at-cvpr/
NVIDIA released Feature Map Explorer for visualizing 4D image-based feature maps, new computer vision models on NGC, and CUDA-X libraries such as cuDNN, NCCL and DALI to support the new NVIDIA Ampere architecture. In this post, you can also watch a demo of NVIDIA Riva.Powered by Discourse, best viewed with JavaScript enabled"
83,video-introduction-to-recurrent-neural-networks-in-tensorrt,"Originally published at:			Video: Introduction to Recurrent Neural Networks in TensorRT | NVIDIA Technical Blog
NVIDIA TensorRT™ is a high-performance deep learning inference optimizer and runtime that delivers low latency and high-throughput. TensorRT can import trained models from every deep learning framework to easily create highly efficient inference engines that can be incorporated into larger applications and services. This video demonstrates how to configure a simple Recurrent Neural Network (RNN)…is there any pricing for NVIDIA TensorRT Inference ServerPowered by Discourse, best viewed with JavaScript enabled"
84,dlss-2-0-a-big-leap-in-ai-rendering,"Originally published at:			https://developer.nvidia.com/blog/dlss-2-0-ai-rendering/
Artificial Intelligence creates a breakthrough Deep Learning Super Sampling (DLSS) is an NVIDIA RTX technology that uses the power of deep learning and AI to boost frame rates while generating beautiful, sharp images for your games. DLSS gives you the performance headroom to maximize your graphics settings or increase output resolution.  What is it? A…Powered by Discourse, best viewed with JavaScript enabled"
85,accelerating-analytics-and-ai-with-alluxio-and-nvidia-gpus,"Originally published at:			https://developer.nvidia.com/blog/accelerating-analytics-and-ai-with-alluxio-and-nvidia-gpus/
Data processing is increasingly making use of NVIDIA computing for massive parallelism. Advancements in accelerated compute mean that access to storage must also be quicker, whether in analytics, artificial intelligence (AI), or machine learning (ML) pipelines. The benefits from GPU acceleration are limited if data access dominates the execution time. GPU-based processing drives a higher…Powered by Discourse, best viewed with JavaScript enabled"
86,nvidias-top-5-ai-stories-of-the-week-4-15,"Originally published at:			https://developer.nvidia.com/blog/nvidias-top-5-ai-stories-of-the-week-4-15/
Every week we highlight NVIDIA’s Top 5 AI stories of the week. In this week’s edition we highlight a new deep learning-based algorithm that is working towards replicating the energy of the sun on earth. Plus, a robot that can efficiently interact with liquid and moldable items. Watch below: 5 – Isaac SDK Now Available…Powered by Discourse, best viewed with JavaScript enabled"
87,deploying-healthcare-ai-workflows-with-the-nvidia-clara-deploy-application-framework,"Originally published at:			Deploying Healthcare AI Workflows with the NVIDIA Clara Deploy Application Framework | NVIDIA Technical Blog
This post has been updated at Deploying Healthcare AI Workflows with the NVIDIA Clara Deploy Application Framework (updated). The new version adds information about configuring the DICOM adapter and three new reference pipelines. Figure 1. NVIDIA Clara Deploy SDK in the healthcare ecosystem. The adoption of AI in hospitals is accelerating rapidly. There are many…Powered by Discourse, best viewed with JavaScript enabled"
88,efficient-bert-finding-your-optimal-model-with-multimetric-bayesian-optimization-part-2,"Originally published at:			https://developer.nvidia.com/blog/efficient-bert-finding-your-optimal-model-with-multimetric-bayesian-optimization-part-2/
This is the second post in this series about distilling BERT with multimetric Bayesian optimization. Part 1 discusses the background for the experiment and Part 3 discusses the results. In my previous post, I discussed the importance of the BERT architecture in making transfer learning accessible in NLP. BERT allows a variety of problems to…Powered by Discourse, best viewed with JavaScript enabled"
89,meet-the-researcher-marco-aldinucci-convergence-of-hpc-and-ai-to-fight-against-covid,"Originally published at:			https://developer.nvidia.com/blog/meet-the-researcher-marco-aldinucci-convergence-of-hpc-and-ai-to-fight-against-covid/
‘Meet the Researcher’ is a series in which we spotlight different researchers in academia who use NVIDIA technologies to accelerate their work.  This month we spotlight Marco Aldinucci, Full Professor at the University of Torino, Italy, whose research focuses on parallel programming models, language, and tools.  Since March 2021, Marco Aldinucci has been the Director…Powered by Discourse, best viewed with JavaScript enabled"
90,nvidia-tensorrt-inference-server-and-kubeflow-make-deploying-data-center-inference-simple,"Originally published at:			NVIDIA TensorRT Inference Server and Kubeflow Make Deploying Data Center Inference Simple | NVIDIA Technical Blog
AI has become a crucial technology for end user applications and services. The daily interactions we have with search engines, voice assistants, recommender applications, and more, all use AI models to derive their particular form of insight. When using AI in an application, it is necessary to perform ‘inference’ on trained AI models — in…Powered by Discourse, best viewed with JavaScript enabled"
91,share-your-science-quantum-transport-simulations-on-hybrid-supercomputers,"Originally published at:			https://developer.nvidia.com/blog/share-your-science-quantum-transport-simulations-on-hybrid-supercomputers/
Mauro Calderara, PhD student at ETH Zurich and 2015 Gordon Bell Finalist talks about his GPU-accelerated work on “Pushing Back the Limit of Ab-initio Quantum Transport Simulations on Hybrid Supercomputers.”   Share your GPU-accelerated science with us: http://nvda.ly/Vpjxr Watch more scientists and researchers share how accelerated computing is #thepathforward at http://nvda.ly/UOan1Powered by Discourse, best viewed with JavaScript enabled"
92,update-nvidia-driver-on-multiple-windows-10-machines-via-power-shell,"Hi,I want to know if you can help with Update Nvidia driver on multiple Windows 10 machines via Power Shell.Kindly asking for your help.Powered by Discourse, best viewed with JavaScript enabled"
93,gpt-agent-in-omniverse,"Hi!Two questions!Would it be possible to create entire environments based on GPT prompts within Replicator? The idea is to fine-tune computer vision models on synthetic datasets (models + environment)I would like to create an AI agent operating from visual feedback.
Can we use other python libraries within Omniverse (such as YOLO or SegmentAnything) and is it possible to feed camera coordinates to a GPT prompt and then update the position based on the reply from GPT, similar to the 3D object placing pipeline?Thanks!You could certainly do that, you can even have GPT create USD directly rather than json files, the problem I started to run into were with token limits. Your GPT response can only be so long and if you’re creating meshes and textures from scratch, that takes up a lot of tokens very quickly.A different approach I might take would be to compose your scene, keeping in mind those things you would like to randomize, and then ask GPT to give you random values for only those key items. That way you would be able to work with a much larger scene. That said, this is what replicator already does and I might use replicator for this specific task over GPT.That sounds like such a cool project! Can’t wait to see what you put together! You can incorporate pretty much any python package you want into your extension, the Omniverse platform is extremely flexible. You could definitely tap into one of the scene’s update callabacks, get camera coordinates and feed those into GPT, just be ready for a really, really slow frame rate because it takes a few seconds to get that reply back from GPT :DThis topic was automatically closed 14 days after the last reply. New replies are no longer allowed.Powered by Discourse, best viewed with JavaScript enabled"
94,increase-performance-with-gpu-boost-and-k80-autoboost,"Originally published at:			https://developer.nvidia.com/blog/increase-performance-gpu-boost-k80-autoboost/
NVIDIA® GPU Boost™ is a feature available on NVIDIA® GeForce® and Tesla® GPUs that boosts application performance by increasing GPU core and memory clock rates when sufficient power and thermal headroom are available (See the earlier Parallel Forall post about GPU Boost by Mark Harris).  In the case of Tesla GPUs, GPU Boost is customized for compute-intensive workloads running on…Jiri, another great post. How does this work with bandwidth bound applications? On K40 you had to use the 875MHz to get full bandwidth. Given that bandwidth bound applications usually don't come close to TDP they can probably run with the highest clock, but if on of the 24 lower clocks already maxes out the achievable bandwidth one could get away with a lower clock. What about a plot similar to Fig.2 for the stream triad memory bandwidth?Hi Mathias, thanks for the feedback and the goodquestion. The Tesla K80 doesn’t need to run at the max application clocks toachieve its full memory bandwidth. In my experiments, a GPU clock of 705 issufficient for memory bound applications.Two more questions out of curiosity:1. Is AutoBoost smart enough to only increase the clock to 705 (or whatever is sufficient to get the best performance) ? If performance does not profit from higher frequencies anymore this would be nice in terms of energy savings, stability, durability. But there is probably no way the CUDA driver can detect this. But maybe the programmer could specify a maximum boost frequency?2. When I run a K40 with 875 MHz is will throttle if power or temperature exceed the limits. How is this different (apart from the finer levels) then Autoboost? I am sure it is  - I am just curious.1.      As you say autoboost can't know that a kernel is memory bound and thus will increase the clock beyond 705 Mhz if the power and thermal conditions allow it. To avoid this you need to disable autoboost and set application clocks to the right value. 2.      The difference between autoboost and setting application clocks to the highest possible clocks is that autoboost starts at the lowest clock and increase it while with application clocks the clocks are reduced from the specified value until no clock throttle reasons occur. In some cases application clocks might be faster than autoboost, e.g. for very short kernels it might take too long to spin up the clocks, while in other cases autoboost is faster, e.g. if the application clock setting is too aggressive. Energy wise autoboost is always better as it reduces the clocks when the GPU is less loaded.I just bought a 980Ti and both the driver and nvidia-smi are showing the ""Max clocks"" as 1595mhz. The ""graphics clock"" gets up to 1430 (probably because boost is turned off, luckily!). Is this a display issue or is my card clocked wrongly? Something isn't right!Hmm, is this a second-hand board? Clocks seem high. Saw your post on devtalk.nvidia.com (which is a more appropriate forum for this question). As to why you aren't hitting the max, a full nvidia-smi -q output posted to the forums might help to understand why you can't reach them.Cheers, I found this site before I found the correct forum, was goggling linux nvidia boost and this came up. Happy to discuss in the forum if it's more appropriate. It's a brand new card (Inno3d Hybrid Black), in Windows it shows the correct clocks. I'll post the full output in the forum.The auto-boost option can not be enabled on the GeForce GTX TITAN Black. I see ""Changing auto boosted clocks permissions is not supported for GPU"" message. Maybe changing this option only allowed on the Nvidia Tesla cards?Your GeForce GTX Titan Black will automatically increase its clocks if power and thermal budges allow. You do not need to explicitly enable that. However auto boost as it is described in this blog post is different from the GPU Boost 2.0 features of your GeForce GTX Titan Black.Dear Jiri,I see my card stays at P0 mode (max.performance) when it is idle after we forced it with gromacs jobs then it switches to P2 performance level. Do you know the reason?I use the latest verion driver, 352.41.Best,DoganHi Dogan,How long does your card stay at P0 mode when it is idle before switching to P2?JiriP.S. As Mark says below https://devtalk.nvidia.com/ is the more appropriate forum for this kind of question.Immediately. Within one second after the process starts, even if there is still 0% GPU-Utilization.Sorry I am not sure if I fully understand what you are seeing. Are you saying that the GPU goes into P0 mode when GROMACS is running but does not execute a GPU kernel? That would be expected behavior because GROMACS creates a compute context. Would you mind creating a topic at devtalk.nvidia.com to continue this discussion?Dear Jiri, Thank you, I posted my question here:https://devtalk.nvidia.com/...Is this card good with string handling and natural language processing as well?Figure 3  implies that Boost clocks can EXCEED the maximum flat-rate application clocks. Is this true? Ie, if my Tesla is well cooled and is still under the wattage limit, the GPU might have an autoboost  clock rate FASTER than the maximum  choosable rate using  nvidia-smi -ac ? Or is that application clock setting ALSO still applicable and is setting the maximum clocks regardless of power and temperature? If so, Figure 3 is very misleading.Hi Gerry, no boost clocks cannot exceed the maximum configurable clocks. Figure 3 visualizes how GPU clocks of a hypothetical applications could behave on a Tesla K80 with maximum clocks of 875 Mhz and configured application clocks set to ~750 Mhz. When Auto Boost is disabled (left part) the GPU runs constantly at 750 Mhz not hitting power or thermal limits. With enabled Auto Boost the Tesla K80 can raise it clocks up to the maximum of 875 Mhz during the phases of the application where the thermal and power budget allows it to raise above 750 Mhz. However clocks cannot exceed the maximum of 875 Mhz. Hope this clarifies the situation for you. Thanks JiriAh, I see. So setting maximum application clocks will give performance limited by maximum temperature and maximum wattage. Using boost clocks will give performance limited by some default clock limit, by maximum temperature, and by a (lower) boost wattage setting.But then it seems that there are NO situations where dynamic boost will ever outperform simply setting maximum clocks. Is that correct?  If so, it seems like boost clocks are a tool useful when deliberately running at reduced wattage, but not as a performace optimization.When setting application clocks to the maximum the achievable clocks are still limited by the power and thermal budget, i.e. all application clock settings are safe to use. It is still not recommended to set application clocks too aggressively because thermal or power violations can cause significant GPU clock drops. Autoboost raises the GPU clocks incrementally avoiding these significant drops. There is also a change how application clocks and Autoboost work together with Pascal. On Pre-Pascal, setting application clocks ensures GPU clocks between the application clock value and the max clock value unless throttled for power or thermal reasons. On Pascal and later, setting application clocks disables Autoboost and locks the GPU to the application clock setting unless throttled for power or thermal reasons.Powered by Discourse, best viewed with JavaScript enabled"
95,deep-learning-to-unlock-mysteries-of-parkinsons-disease,"Originally published at:			https://developer.nvidia.com/blog/deep-learning-to-unlock-mysteries-of-parkinsons-disease/
Researchers at The Australian National University are using deep learning and NVIDIA technologies to better understand the progression of Parkinson’s disease. Currently it is difficult to determine what type of Parkinson’s someone has or how quickly the condition will progress. The study will be conducted over the next five years at the Canberra Hospital in…Powered by Discourse, best viewed with JavaScript enabled"
96,speeding-up-semantic-segmentation-using-matlab-container-from-nvidia-ngc,"Originally published at:			Speeding Up Semantic Segmentation Using MATLAB Container from NVIDIA NGC | NVIDIA Technical Blog
Gone are the days of using a single GPU to train a deep learning model.  With computationally intensive algorithms such as semantic segmentation, a single GPU can take days to optimize a model. But multi-GPU hardware is expensive, you say. Not any longer;  NVIDIA multi-GPU hardware on cloud instances like the AWS P3 allow you to pay…Powered by Discourse, best viewed with JavaScript enabled"
97,multi-camera-large-scale-intelligent-video-analytics-with-deepstream-sdk,"Originally published at:			Multi-Camera Large-Scale Intelligent Video Analytics with DeepStream SDK | NVIDIA Technical Blog
The advent of the Internet of things (IoT) and smart cities has seen billions of video sensors deployed worldwide, generating massive amounts of data. Through the use of AI, that data can be turned into rich insights in areas such as traffic optimization for roadways, retail analytics, and smart parking. DeepStream enables developers to design…removedHow does kafka call geogle map key api?thaxor how to use kafka to message distribution?Powered by Discourse, best viewed with JavaScript enabled"
98,leveraging-machine-learning-to-detect-fraud-tips-to-developing-a-winning-kaggle-solution,"Originally published at:			https://developer.nvidia.com/blog/leveraging-machine-learning-to-detect-fraud-tips-to-developing-a-winning-kaggle-solution/
Kaggle is an online community that allows data scientists and machine learning engineers to find and publish data sets, learn, explore, build models, and collaborate with their peers. Members also enter competitions to solve data science challenges. Kaggle members earn the following medals for their achievements: Novice, Contributor, Expert, Master, and Grandmaster. The quality and…The secret to creating a high-scoring model in this competition was feature engineering. The features that made the difference for the winning team were new columns created from group aggregations of other columns. Computing group aggregations can naturally be done in parallel and they benefit from using GPU instead of CPU. Chris created a notebook containing the XGBoost model of the 1st place solution converted to use RAPIDS cuDF. To read one million rows and create 262 features on the CPU using pandas took 5 minutes. To read and develop those features on GPU with RAPIDS cuDF took 20 seconds, 15x faster!Powered by Discourse, best viewed with JavaScript enabled"
99,nvidia-releases-updates-to-cuda-x-ai-libraries,"Originally published at:			https://developer.nvidia.com/blog/nvidia-releases-updates-to-cuda-x-ai-libraries/
NVIDIA CUDA-X AI are deep learning libraries for researchers and software developers to build high performance GPU-accelerated applications for conversational AI, recommendation systems and computer vision. CUDA-X AI libraries deliver world leading performance for both training and inference across industry benchmarks such as MLPerf. Learn what’s new in the latest releases of CUDA-X AI libraries…Powered by Discourse, best viewed with JavaScript enabled"
100,game-stack-live-to-feature-rtxdi-and-minecraft-graphics-sessions-april-20-21,"Originally published at:			https://developer.nvidia.com/blog/game-stack-live-to-feature-rtxdi-and-minecraft-graphics-sessions-april-20-21/
This year, Microsoft’s free Game Stack Live event (April 20-21), starting at 8am PDT, will offer a wide range of can’t-miss sessions for game developers, in categories that include Graphics, System & Tools, Production & Publishing, Accessibility & Inclusion, Audio, Multiplayer, and Community Connections.Powered by Discourse, best viewed with JavaScript enabled"
101,zero-to-data-science-making-data-science-teams-productive-with-kubernetes-and-rapids,"Originally published at:			Zero to Data Science: Making Data Science Teams Productive with Kubernetes and RAPIDS | NVIDIA Technical Blog
  Data collected on a vast scale has fundamentally changed the way organizations do business, driving demand for teams to provide meaningful data science, machine learning, and deep learning-based business insights quickly. Data science leaders, plus the Dev Ops and IT teams supporting them, constantly look for ways to make their teams productive while optimizing their costs…Powered by Discourse, best viewed with JavaScript enabled"
102,dx12-ray-tracing-tutorials,"Originally published at:			https://developer.nvidia.com/blog/dx12-raytracing-tutorials/
Real-time ray tracing represents a new milestone in gaming graphics. Game developers have relied on rasterization techniques which looked very, very good. However, rasterization good enough to achieve near-realism requires a substantial time investment on the part of developers and artists. NVIDIA RTX technology combined with Microsoft’s DXR ray tracing extension for DirectX 12 will…Powered by Discourse, best viewed with JavaScript enabled"
103,top-mlops-sessions-at-nvidia-gtc-2023,"Originally published at:			https://developer.nvidia.com/blog/top-mlops-sessions-at-nvidia-gtc-2023/
Discover how to build a robust MLOps practice for continuous delivery and automated deployment of AI workloads at scale.Powered by Discourse, best viewed with JavaScript enabled"
104,ai-can-now-fix-your-grainy-photos-by-only-looking-at-grainy-photos,"Originally published at:			https://developer.nvidia.com/blog/ai-can-now-fix-your-grainy-photos-by-only-looking-at-grainy-photos/
What if you could take your photos that were originally taken in low light and automatically remove the noise and artifacts? Have grainy or pixelated images in your photo library and want to fix them? This deep learning-based approach has learned to fix photos by simply looking at examples of corrupted photos only. The work…Powered by Discourse, best viewed with JavaScript enabled"
105,bringing-networking-into-view-with-the-nvidia-air-marketplace,"Originally published at:			https://developer.nvidia.com/blog/bringing-networking-into-view-with-the-air-marketplace/
NVIDIA Air now includes the NVIDIA Air Marketplace; A collection of demos to help you get started building your network digital twin.Powered by Discourse, best viewed with JavaScript enabled"
106,get-ready-for-the-low-power-image-recognition-challenge-with-jetson-tk1,"Originally published at:			https://developer.nvidia.com/blog/low-power-image-recognition-challenge-jets/
Image recognition and GPUs go hand-in-hand, particularly when using deep neural networks (DNNs). The strength of GPU-based DNNs for image recognition has been unequivocally demonstrated by their success over the past few years in the ImageNet Large Scale Visual Recognition Challenge (ILSVRC), and DNNs have recently achieved classification accuracy on par with trained humans, as…Powered by Discourse, best viewed with JavaScript enabled"
107,improving-aiming-time-on-small-fps-targets-with-higher-resolutions-and-larger-screen-sizes,"Originally published at:			https://developer.nvidia.com/blog/improving-aiming-time-on-small-fps-targets-with-higher-resolutions-and-larger-screen-sizes/
Competitive gamers prefer to play at the highest refresh rate possible, but new higher resolution monitors increase aiming performance for small targets.It was fun to put together this experiment and somewhat surprising to me that we actually found a difference for such a small jump in in screen size.If you have any questions or comments, feel free to let us know. In particular, I’m happy to provide support for anyone who downloads the experiment and wants to try it themselves!Powered by Discourse, best viewed with JavaScript enabled"
108,a-comprehensive-guide-to-interaction-terms-in-linear-regression,"Originally published at:			https://developer.nvidia.com/blog/a-comprehensive-guide-to-interaction-terms-in-linear-regression/
Linear regression is a powerful statistical tool used to model the relationship between a dependent variable and one or more independent variables (features). An important, and often forgotten, concept in regression analysis is that of interaction terms. In short, interaction terms enable you to examine whether the relationship between the target and the independent variable…Have you ever utilized interaction terms in linear models? If you have, was it mostly for econometric models or have you also applied them in business use cases?Powered by Discourse, best viewed with JavaScript enabled"
109,nvidia-announces-nsight-systems-2021-1,"Originally published at:			https://developer.nvidia.com/blog/nvidia-announces-nsight-systems-2021-1/
This release includes support and performance improvements for the latest top ray tracing game tiles on DirectX and Vulkan. Also included is support for the stats command on Windows CLI.Powered by Discourse, best viewed with JavaScript enabled"
110,develop-physics-informed-machine-learning-models-with-graph-neural-networks,"Originally published at:			https://developer.nvidia.com/blog/develop-physics-informed-machine-learning-models-with-graph-neural-networks/
Modulus 23.05 brings together new capabilities, empowering the research community and industries to develop research into enterprise-grade solutions through open-source collaboration.I would love to see if the PyTorch Modulus implementation of Meshgraphnets achieves the same accuracy as the original tensorflow implementation? Are there any public experiment records on how Modulus performed, for e.g. the cylinderflow example?Hello, and thanks for your comment! Unfortunately, we do not have any quantitative comparison between the Modulus implementation and the original TF implementation of MeshGraphNet yet.Powered by Discourse, best viewed with JavaScript enabled"
111,effectively-integrating-rtx-ray-tracing-into-a-real-time-rendering-engine,"Originally published at:			Effectively Integrating RTX Ray Tracing into a Real-Time Rendering Engine | NVIDIA Technical Blog
RTX is NVIDIA’s new platform for hybrid rendering, allowing the combination of rasterization and compute-based techniques with hardware-accelerated ray tracing and deep learning. It has already been adopted in a number of games and engines. Based on those experiences, this blog aims to give the reader an insight into how RTX ray tracing is best…Powered by Discourse, best viewed with JavaScript enabled"
112,gpu-accelerated-supercomputers-create-largest-simulation-of-the-universe,"Originally published at:			GPU-Accelerated Supercomputers Create Largest Simulation of the Universe | NVIDIA Technical Blog
To investigate the nature of dark matter and dark energy, researchers from University of Zurich simulated the formation of our entire universe with the help of two GPU-accelerated supercomputers. With their revolutionary code called PKDGRAV3, the group of astrophysicists developed a catalog of nearly 25 billion virtual galaxies generated from a two-trillion particle cosmological simulation…Powered by Discourse, best viewed with JavaScript enabled"
113,ai-helps-automate-the-drug-discovery-process,"Originally published at:			AI Helps Automate the Drug Discovery Process | NVIDIA Technical Blog
Researchers from Google, along with collaborators from academia, announced today they developed a deep learning-based system for identifying protein crystallization, achieving a 94 percent accuracy rate. Protein crystallization plays a vital role in the drug discovery process, helping determine the shape of cells. The work has the potential to further the drug discovery process by…Powered by Discourse, best viewed with JavaScript enabled"
114,gtc-2020-creating-performant-dl-models-for-use-in-client-applications,"GTC 2020 S21317
Presenters: Don Brittain,NVIDIA
Abstract
Find out what it takes to bring the benefits of deep learning to applications that run at interactive speeds. We’ll explain how to think about inference performance throughout the DL development pipeline, covering topics like network design, training considerations, debugging, profiling performance, and optimizing GPU-based code for fastest throughput using tensor core acceleration. Our talk is aimed at application programmers and other AI practitioners, but it should also be accessible to program designers and project managers interested in incorporating AI features into their products.Watch this session
Join in the conversation below.Powered by Discourse, best viewed with JavaScript enabled"
115,explainer-what-is-mlops,"Originally published at:			What is MLOps? | NVIDIA Blog
Machine learning operations, MLOps, are best practices for businesses to run AI successfully with help from an expanding smorgasbord of software products and cloud services.Powered by Discourse, best viewed with JavaScript enabled"
116,new-languages-enhanced-cybersecurity-and-medical-ai-frameworks-unveiled-at-gtc,"Originally published at:			https://developer.nvidia.com/blog/new-languages-enhanced-cybersecurity-and-medical-ai-frameworks-unveiled-at-gtc/
Major updates to AI frameworks for building real-time speech AI apps, designing high-performing recommenders at scale, applying AI to cybersecurity challenges, and creating AI-powered medical devices.Powered by Discourse, best viewed with JavaScript enabled"
117,recreate-any-voice-using-one-minute-of-sample-audio,"Originally published at:			Recreate Any Voice Using One Minute of Sample Audio | NVIDIA Technical Blog
A Montreal-based startup developed a set of deep learning algorithms that can copy anyone’s voice with only 60 seconds of sample audio. Lyrebird, a startup spin-off from the MILA lab at University of Montréal and advised by Aaron Courville and Yoshua Bengio claims to be the first of its kind to allow copying voices in…Powered by Discourse, best viewed with JavaScript enabled"
118,upcoming-event-how-to-build-an-edge-solution,"Originally published at:			https://nvda.ws/3MRma3l
​Join us on June 16 to take a deep dive into AI at the edge and learn how you can build an edge computing solution that delivers real-time results.Powered by Discourse, best viewed with JavaScript enabled"
119,improving-int8-accuracy-using-quantization-aware-training-and-the-nvidia-transfer-learning-toolkit,"Originally published at:			https://developer.nvidia.com/blog/improving-int8-accuracy-using-quantization-aware-training-and-the-transfer-learning-toolkit/
Deep neural network (DNN) models are routinely used in applications requiring analysis of video stream content. These may include object detection, classification, and segmentation. Typically, these models are trained on servers with high-end GPUs, either in stand-alone servers, such as NVIDIA DGX1, or on servers available in data centers or private or public clouds. Such…Powered by Discourse, best viewed with JavaScript enabled"
120,can-i-use-other-tools-than-chatgpt,"This is a bit of a follow-up to my earlier question.If I don’t want to use ChatGPT, do I have any other options?Omniverse is a platform and open to integrating other LLMs. You can use alternatives like NVIDIA NeMo, which allows you to train on your own data and get responses that speak in your brand’s language and reflect your specific domain.This topic was automatically closed 14 days after the last reply. New replies are no longer allowed.Powered by Discourse, best viewed with JavaScript enabled"
121,cuda-10-features-revealed-turing-cuda-graphs-and-more,"Originally published at:			CUDA 10 Features Revealed: Turing, CUDA Graphs, and More | NVIDIA Technical Blog
For the last eleven years, NVIDIA’s CUDA development platform has unleashed the power of GPUs for general purpose processing in a wide variety of applications. These include: high performance computing (HPC), data center applications, and content creation workflows. Most recently, artificial intelligence systems and applications ranging from embedded systems to the cloud have benefited from…The RT Core is the most exciting part of the Turing architecture to me, since BVH traversal is a big performance bottleneck for my applications. Is OptiX the only way to access this new hardware, or will there be an API for accessing the RT core directly from CUDA in the future?Unfortunately yes, according to the presentation at GTC 2018 in Munich, OptiX (or Vulkan/DX12) is currently the only way to make use of the RT cores.MPS(Multi-Process Service) has a few restrictions. One of the most mysterious one is unsupport of dynamic parallelism. Is it still prohibited on the Turing generation?What is L1/shared bandwidth per smx in Turing? Around 1 TB/s? I can get mostly 110 MB/s per sm from a low end Kepler gpu.Powered by Discourse, best viewed with JavaScript enabled"
122,latest-discoveries-at-the-healthcare-life-sciences-developer-summit,"Originally published at:			https://developer.nvidia.com/blog/latest-discoveries-at-the-healthcare-life-sciences-developer-summit/
Humanity has seen major scientific breakthroughs directly related to discoveries that do not share the glamor of the breakthrough they enabled. Sir Alexander Fleming’s penicillin gave rise to effective treatments for infections like pneumonia, but penicillin’s importance outshines a technology known as the Petri dish, invented by a German physician. It was in a Petri…Powered by Discourse, best viewed with JavaScript enabled"
123,share-your-science-artificial-intelligent-robot-for-children,"Originally published at:			Share Your Science: Artificial Intelligent Robot for Children | NVIDIA Technical Blog
Yi-Jian Wu, Founder & CEO of Yuanqu Tech in China, talks about how NVIDIA Tesla GPUs are being used to train their interactive educational robot for children. Call the robot’s name and the speech-controlled robot is able to tell jokes, answer educational questions, teach English and act as a patient tutor for a child.  For…Powered by Discourse, best viewed with JavaScript enabled"
124,deep-learning-in-aerial-systems-using-jetson,"Originally published at:			https://developer.nvidia.com/blog/deep-learning-in-aerial-systems-jetson/
The adoption of unmanned aerial systems (UAS) has been steadily growing over the last decade. While UAS originated with military applications, they have proven to be beneficial in a variety of other fields including agriculture, geographical mapping, aerial photography, and search and rescue. These systems, however, require a person in the loop for remote control,…What is the type of deep learning model: Supervised or Unsupervised? SupervisedHow can I increase the number of letter samples created by the target synthesis code?While training my shape detector, the accuracy seems to stagnate. Is it because of the rotated shapes which are provided in the dataset, maybe the CNN is getting confused and is not able to generalise in a powerful enough manner?Powered by Discourse, best viewed with JavaScript enabled"
125,open-access-visual-search-tool-for-satellite-imagery,"Originally published at:			https://developer.nvidia.com/blog/open-access-visual-search-tool-for-satellite-imagery/
A new project by Carnegie Mellon University researchers provides journalists, citizen scientists, and other researchers with the ability to quickly scan large geographical regions for specific visual features. Simply click on a feature in the satellite imagery – a baseball diamond, cul-de-sac, tennis court – and Terrapattern will find other things that look similar in the…Powered by Discourse, best viewed with JavaScript enabled"
126,12-startups-vying-for-100-000-at-gpu-technology-conference,"Originally published at:			12 Startups Vying for $100,000 at GPU Technology Conference | NVIDIA Technical Blog
Each startup will be given four minutes to present their GPU-accelerated tech and business plan live on stage to an audience of technology executives. The challenge is designed for startups in the GPU ecosystem that have raised less than $1 million in seed funding and are ready to expand their visibility and demonstrate their potential…Powered by Discourse, best viewed with JavaScript enabled"
127,how-to-optimize-data-transfers-in-cuda-c-c,"Originally published at:			https://developer.nvidia.com/blog/how-optimize-data-transfers-cuda-cc/
In the previous three posts of this CUDA C & C++ series we laid the groundwork for the major thrust of the series: how to optimize CUDA C/C++ code. In this and the following post we begin our discussion of code optimization with how to efficiently transfer data between the host and device. The peak…Nice article. Do I need to be worried about kernel calls, which operate on a mix of host and device variables ? Would you recommend to use all device variables within a kernel.Kernels are by definition device code.  Therefore they must operate only on device memory, or on __managed__ memory (see my post on Unified Memory for more on this: http://devblogs.nvidia.com/...Hi, MarkUsing Pinned memory I achieved 11GB/s H2D transfer on Titan X (PCI-E 3.0 16x).Do you think this could be improved?Hi Mark,Thank you for a very interesting and helpful article. I have a question: I transfer big data to the GPU once, and than create a cut of it using the GPU, transfer the result back to cpu, and than again, new cut, new transfer. Like scrolling of a plane-cut on 3d data. The most time consuming operation is the transfer back of the result (about 5 Mb) which is about 300 msec. Is it reasonable? is there a way to improve it or this not a task that is suitable for GPU?ThanksHadarwonderful post! Very helpful.for future visitors, I found this link helpful https://www.cs.virginia.edu...Hi,Why not just use Zero copy, instead of pinned memory and explicit data transfer?Zero-copy with cudaHostAlloc() allocates Pinned memory and eliminates the need to explicit data transfer with cudaMemcpy().Also in some cases it allows the kernel to execute while data is being transferred (preventing the need to use multiple streams for the same effect).Chapter 9.1.3 Zero copyhttps://docs.nvidia.com/cud...Zero copy is a useful tool for your toolbelt. But so are pinned memory transfers. If you will access the data from the device multiple times, it usually makes sense to copy it to device memory and access it there. But if you only need to access it once, from a kernel, then zero copy makes total sense.ok,thanksHi All!
I’ve read the article and tried the code provided.
However, the results I’ve got are not as expected. I can’t see any significant bandwidth increase for the “pinned memory” case. In some test runs it’s even worse then for “usual” memory. Any idea, why, please?Device: NVIDIA GeForce GTX 1060 6GB
Transfer size (MB): 16Pageable transfers
Host to Device bandwidth (GB/s): 0.388620
Device to Host bandwidth (GB/s): 0.417222
Pinned transfers
Host to Device bandwidth (GB/s): 0.359622
Device to Host bandwidth (GB/s): 0.419293Pageable transfers
Host to Device bandwidth (GB/s): 0.387992
Device to Host bandwidth (GB/s): 0.417588
Pinned transfers
Host to Device bandwidth (GB/s): 0.390906
Device to Host bandwidth (GB/s): 0.418569Pageable transfers
Host to Device bandwidth (GB/s): 0.387717
Device to Host bandwidth (GB/s): 0.416807
Pinned transfers
Host to Device bandwidth (GB/s): 0.390276
Device to Host bandwidth (GB/s): 0.419327Pageable transfers
Host to Device bandwidth (GB/s): 0.387582
Device to Host bandwidth (GB/s): 0.417603
Pinned transfers
Host to Device bandwidth (GB/s): 0.389954
Device to Host bandwidth (GB/s): 0.419298I can’t be sure from the information provided. Perhaps there is a problem with PCI-e on your system. What OS are you on? CUDA Version? Can you include the output of running nvidia-smi?LinuxPowered by Discourse, best viewed with JavaScript enabled"
128,breakthrough-ai-technique-could-revolutionize-the-animation-industry,"Originally published at:			Breakthrough AI Technique Could Revolutionize the Animation Industry | NVIDIA Technical Blog
Researchers recently developed the first deep learning based system that can transfer the full 3D head position, facial expression and eye gaze from a source actor to a target actor. “Synthesizing and editing video portraits, i.e., videos framed to show a person’s head and upper body, is an important problem in computer graphics, with applications…Powered by Discourse, best viewed with JavaScript enabled"
129,latest-nsight-developer-tools-releases-nsight-systems-2021-1-nsight-compute-2021-2-nsight-visual-studio-code-edition,"Originally published at:			Latest Nsight Developer Tools Releases: Nsight Systems 2021.2, Nsight Compute 2021.1, Nsight Visual Studio Code Edition | NVIDIA Technical Blog
 The latest versions of Nsight Systems 2021.2 and, Nsight Compute 2021.1 are now available with new features for GPU profiling and performance optimization.Powered by Discourse, best viewed with JavaScript enabled"
130,gtc-2020-a-data-driven-dnn-based-fast-chip-thermal-solver,"GTC 2020 S21631
Presenters: Jimin Wen,ANSYS INC; Akhilesh Kumar,Ansys
Abstract
Get to know ANSYS’ strategy for the AI-based thermal solution of the modern chip-package-system design, as well as the data-driven DNN-based fast chip thermal solver we’re going to announce. IC/package/system designers in the traditional semiconductor industry or 5G, automotive, and AI application will benefit the most from our session.Watch this session
Join in the conversation below.Powered by Discourse, best viewed with JavaScript enabled"
131,dividing-nvidia-a30-gpus-and-conquering-multiple-workloads,"Originally published at:			Dividing NVIDIA A30 GPUs and Conquering Multiple Workloads | NVIDIA Technical Blog
Here’s how to use MIG on A30, from partitioning MIG instances to running deep learning inference on MIG instances simultaneously.Powered by Discourse, best viewed with JavaScript enabled"
132,university-of-bristol-s-new-tesla-p100-accelerated-supercomputer,"Originally published at:			University of Bristol’s New Tesla P100-Accelerated Supercomputer | NVIDIA Technical Blog
Three times faster than its predecessor, Blue Crystal 4 (BC4) at University of Bristol can perform 600 trillion calculations per second and will accelerate the work of more than 1,000 researchers and engineers. According to Dr. Christopher Woods, EPSRC Research Software Engineer Fellow at the University of Bristol, “research that used to take a month,…Powered by Discourse, best viewed with JavaScript enabled"
133,nvidia-and-oracle-team-up-to-support-ai-startups,"Originally published at:			NVIDIA and Oracle Team up to Support AI Startups | NVIDIA Technical Blog
NVIDIA Inception and Oracle for Startups announced a collaboration to Offer AI Startups Global Business Resources As part of GTC Digital, NVIDIA Inception has formalized its partnership with Oracle for Startups. The goal of this collaboration is to help AI startups achieve scale and success. Among these benefits for startups is the option to use…Powered by Discourse, best viewed with JavaScript enabled"
134,expanding-hybrid-cloud-support-in-virtualized-data-centers-with-new-nvidia-ai-enterprise-integrations,"Originally published at:			https://developer.nvidia.com/blog/expanding-hybrid-cloud-support-in-virtualized-data-centers-with-new-nvidia-ai-enterprise-integrations/
Get the latest on NVIDIA AI Enterprise on VMware Cloud Foundation, VMware Cloud Director, and new curated labs with VMware Tanzu and Domino Data.Powered by Discourse, best viewed with JavaScript enabled"
135,postmates-presents-new-nvidia-jetson-agx-xavier-equipped-delivery-robot-at-gtc,"Originally published at:			https://developer.nvidia.com/blog/postmates-unveils-new-nvidia-jetson-agx-xavier-equipped-delivery-robot/
Postmates, the on-demand delivery company that operates across the United States presented a new all-electric autonomous delivery robot equipped with the latest NVIDIA Jetson AGX Xavier module for autonomous machines at GTC Silicon Valley in San Jose.  The robot called Serve can carry 50 pounds of goods with a range of 30 miles. The system…Powered by Discourse, best viewed with JavaScript enabled"
136,ai-helps-generate-interactive-demo-of-salvador-dali,"Originally published at:			AI Helps Generate Interactive Demo of Salvador Dali | NVIDIA Technical Blog
To help create a unique experience for museum attendees, the Salvador Dali Museum in St Petersburg, Florida has just developed a deep learning-based version of Dali himself. Using thousands of hours of archival footage from interviews, developers from Goodby, Silverstein & Partners trained a convolutional neural network using NVIDIA GeForce GTX 1080TI GPUs, with the cuDNN-accelerated PyTorch deep…Powered by Discourse, best viewed with JavaScript enabled"
137,new-ai-technique-helps-robots-work-alongside-humans,"Originally published at:			New AI Technique Helps Robots Work Alongside Humans | NVIDIA Technical Blog
Researchers from NVIDIA, led by Stan Birchfield and Jonathan Tremblay, developed a first of its kind deep learning-based system that can teach a robot to complete a task by just observing the actions of a human. The method is designed to enhance communication between humans and robots and at the same time further research that…Powered by Discourse, best viewed with JavaScript enabled"
138,idea-to-make-gpus-that-can-be-put-into-the-odd-for-laptops,"So the idea is to make a gpu that can be put into the ODD slot and somehow connect it to the motherboard.
This would help upgrade older and newer laptops.
As this would allow the laptop to be as powerful as it would have with a external and be more mobile and possibly cheaper. Disk drives are becoming less useful and why not use the extra space for better gpu’s make them more powerful. All we need to do is find a way for the cable to get through the drive compartment and into the motherboard compartment.Powered by Discourse, best viewed with JavaScript enabled"
139,icymi-exploring-challenges-posed-by-biased-datasets-using-rapids-cudf,"Originally published at:			There and Back Again… a RAPIDS Tale - KDnuggets
Read about an innovative GPU solution that solves limitations posed by small biased datasets using RAPIDS cuDF.Powered by Discourse, best viewed with JavaScript enabled"
140,ray-tracing-in-4a-s-metro-exodus-and-remedy-s-control,"Originally published at:			Ray Tracing in 4A’s Metro Exodus and Remedy’s Control | NVIDIA Technical Blog
At GDC 2019, NVIDIA’s Martin Stich walked attendees through ray tracing in 4A’s Metro: Exodus (available now) and Remedy’s Control (coming later in 2019) on PC. You can watch that excerpt of his talk here. Prior Metro games took place in claustrophobic railway systems. Metro Exodus, by contrast, lets players step outside to explore an…Powered by Discourse, best viewed with JavaScript enabled"
141,reimagining-drug-discovery-with-computational-biology-at-gtc-2022,"Originally published at:			Data Science Conference Sessions | GTC 2022 | NVIDIA
Take a deep dive into the latest advances in drug research with AI and accelerated computing at these GTC 2022 featured sessions.Powered by Discourse, best viewed with JavaScript enabled"
142,unleashing-the-power-of-nvidia-ampere-architecture-with-nvidia-nsight-developer-tools,"Originally published at:			Unleashing the Power of NVIDIA Ampere Architecture with NVIDIA Nsight Developer Tools | NVIDIA Technical Blog
The NVIDIA Ampere GPU architecture has arrived! It’s time to make sure that your applications are getting the most out of the powerful compute resources in this new architecture. With the release of CUDA 11, we are adding several features to the Nsight family of Developer Tools to help you do just that. These additions…Powered by Discourse, best viewed with JavaScript enabled"
143,developer-spotlight-earth-science-monitoring-with-satellite-imagery,"Originally published at:			https://developer.nvidia.com/blog/developer-spotlight-earth-science-monitoring-with-satellite-imagery/
Sangram Ganguly, a senior research scientist at the NASA Ames Research Center shares how they are analyzing satellite imagery with deep learning to gain a better understanding of our planet. As a founding member of NASA Earth Exchange (NEX), which utilizes NASA’s GPU-accelerated Pleiades supercomputer, Ganguly helped develop the collaboration platform that combines state-of-the-art supercomputing,…Powered by Discourse, best viewed with JavaScript enabled"
144,nvidia-researchers-release-trailblazing-deep-learning-based-framework-for-autonomous-drone-navigation,"Originally published at:			https://developer.nvidia.com/blog/nvidia-researchers-release-trailblazing-deep-learning-based-framework-for-autonomous-drone-navigation/
NVIDIA’s autonomous mobile robotics team today released a framework to enable developers to create autonomous drones that can navigate complex, unmapped places without GPS. All of this is done through deep learning and computer vision powered by NVIDIA Jetson TX1/TX2 embedded AI supercomputers. The drone, nicknamed Redtail, can fly along forest trails autonomously, achieving record-breaking…Powered by Discourse, best viewed with JavaScript enabled"
145,turn-your-selfies-into-chat-stickers,"Originally published at:			Turn Your Selfies Into Chat Stickers | NVIDIA Technical Blog
The developers of Prisma, Apple’s 2016 iPhone App of the Year, launched their second AI-based app called Sticky AI that turns your selfies into stickers to use in messages and on social networks. Using GTX 1080 GPUs and the cuDNN-accelerated TensorFlow deep learning framework, the app automatically extracts your selfie from its background to create…Powered by Discourse, best viewed with JavaScript enabled"
146,optimizing-and-improving-spark-3-0-performance-with-gpus,"Originally published at:			https://developer.nvidia.com/blog/optimizing-and-improving-spark-3-0-performance-with-gpus/
Apache Spark continued the effort to analyze big data that Apache Hadoop started over 15 years ago and has become the leading framework for large-scale distributed data processing. Today, hundreds of thousands of data engineers and scientists are working with Spark across 16,000+ enterprises and organizations. One reason why Spark has taken the torch from…If you have any questions or comments about Spark 3.0 performance with GPUs, let us know.Hi everyone, @carolm I need support for implement new functionality in rapids udfs with scala, I want to those implementation which are not available in the core implementation. Can you guide?Hi, Can you please file a feature request for the UDF functionality here: Issues · NVIDIA/spark-rapids · GitHubPlease add the data types and format used in the UDF.thanks
KarthikPowered by Discourse, best viewed with JavaScript enabled"
147,3d-volumetric-visualization-with-nvidia-index-now-available-on-amazon-web-services,"Originally published at:			3D Volumetric Visualization with NVIDIA IndeX Now Available on Amazon Web Services | NVIDIA Technical Blog
NVIDIA IndeX, a 3D volumetric interactive visualization framework that allows scientists and researchers to visualize and interact with massive high performance computing (HPC) datasets, is now on the AWS cloud.Powered by Discourse, best viewed with JavaScript enabled"
148,webinar-empower-your-industrial-edge-ai-applications-with-nvidia-jetson,"Originally published at:			https://nvda.ws/3pGXqEY#new_tab
Gain insights from advanced AI use cases powered by the NVIDIA Jetson Orin in ruggedized environments.Powered by Discourse, best viewed with JavaScript enabled"
149,minerva-cq-deploys-nvidia-riva-enterprise-in-the-energy-sector,"Originally published at:			https://www.minervacq.com/minervacq-first-to-deploy-nvidia-riva-in-energy#new_tab
Learn how NVIDIA Inception member Minerva CQ is using NVIDIA Riva to deliver faster, personalized experiences within a global EV charging and electric mobility company.Powered by Discourse, best viewed with JavaScript enabled"
150,using-nsight-compute-to-inspect-your-kernels,"Originally published at:			https://developer.nvidia.com/blog/using-nsight-compute-to-inspect-your-kernels/
By now, hopefully you read the first two blogs in this series “Migrating to NVIDIA Nsight Tools from NVVP and Nvprof” and “Transitioning to Nsight Systems from NVIDIA Visual Profiler / nvprof,” and you’ve discovered NVIDIA added a few new tools, both Nsight Compute and Nsight Systems, to the repertoire of CUDA tools available for…How am I able to use Nsight Compute with python scripts?For general questions about using the tools, as indicated in the blog article, the suggestion is to ask those questions on the tools forum. For nsight compute, that is here: Nsight Compute - NVIDIA Developer ForumsGenerally speaking, there shouldn’t be anything special required to use nsight compute with python scripts.  Using the sample python/cupy code here: python 3.x - cupy indexing is slow - Stack Overflow  If I do:ncu python  t1.pyI get output like this:I probably won’t be able to respond to further/detailed questions here. Please ask nsight compute usage questions on the forum I already linked.Powered by Discourse, best viewed with JavaScript enabled"
151,lose-it-s-new-food-recognition-app-counts-calories,"Originally published at:			Lose It!’s New Food Recognition App Counts Calories | NVIDIA Technical Blog
The diet app LoseIt! released a new deep learning feature called Snap It that lets users take photos of their food and then it automatically logs the calorie count and nutritional information. Using the NVIDIA DIGITS deep learning training system on four TITAN X GPUs, the company trained their network on a vast database of…Powered by Discourse, best viewed with JavaScript enabled"
152,gpu-dashboards-in-jupyter-lab,"Originally published at:			https://developer.nvidia.com/blog/gpu-dashboards-in-jupyter-lab/
Learn how NVDashboard in Jupyter Lab is a great open-source package to monitor system resources for all GPU and RAPIDS users to achieve optimal performance and day to day model and workflow development.The blog article was posted 3 days ago and contains long detailed “documentation” of how to install this extension using various wayBut this article is already wrong (probably outdated) and misleading for potential users.The article instructions to install:Gives:According to the official docs (GitHub - rapidsai/jupyterlab-nvdashboard: A JupyterLab extension for displaying dashboards of GPU usage.) we only need to do the pip install (like for most extension on JupyterLab 3). And the extension is only supported on JupyterLab > 3, so the whole installation process documented in this 3 days-old article already does not exist anymore@vincent.emonet Thank you for catching that. We have updated it.It seems like it is still broken. In my case installing this package automatically updated jupyterlab to v3.. and broke the Web UI. Uninstalling jupyterlab-nvdashboard and downgrading jupyterlab back to v2 solved the broken UI at least.Are there any plans to update the dashboards to work with the current version of JupyterLab?Could I also use this on VSCODE? Thanks!I’m afraid not. Currently, only Jupyter Lab and the standalone dashboard is supported.Powered by Discourse, best viewed with JavaScript enabled"
153,tsinghua-university-wins-eighth-student-cluster-championship,"Originally published at:			Tsinghua University Wins Eighth Student Cluster Championship | NVIDIA Technical Blog
Tsinghua University became the only team to achieve a Triple Crown victory by winning supercomputing competitions at ISC, SC (Supercomputing Conference) and ASC (Asia Student Supercomputer Challenge), and have won eight championships in total — this is their third win at the ISC High Performance Conference. Using Tesla P100 GPU accelerators, the Tsinghua team was…Powered by Discourse, best viewed with JavaScript enabled"
154,gtc-2020-building-a-medical-imaging-ai-ecosystem-using-clara-sdks,"GTC 2020 S22295
Presenters: Prerna Dogra,NVIDIA; Daniel Marcus, Washington University School of Medicine
Abstract
We’ll give an overview of Clara SDKs and focus on the latest feature updates for accelerating data annotation, domain-optimized training, and AI inference workflow deployment. We’ll also cover the integration of Clara with XNAT—a medical-imaging research platform—and showcase the power of combining Clara’s accelerated SDKs with cohort management and web GUI that gives data scientists and researchers the ability to go from data to detection and final deployment in a seamless, integrated manner. Using standardized tools for data management and annotation, model creation and validation, and deployment and workflow streamlining, XNAT and Clara lowers the barrier to adoption of AI in the medical imaging ecosystem.Watch this session
Join in the conversation below.Powered by Discourse, best viewed with JavaScript enabled"
155,ai-predicts-when-heart-will-fail,"Originally published at:			https://developer.nvidia.com/blog/ai-creates-virtual-hearts-to-predict-patient-survival/
Scientists from Imperial College London developed 3D virtual hearts from MRI scans to predict heart disease outcomes. “This is the first time computers have interpreted heart scans to accurately predict how long patients will live. It could transform the way doctors treat heart patients,” said Dr Declan O’Regan, lead author from the Imperial College London…Powered by Discourse, best viewed with JavaScript enabled"
156,nvidia-discounts-deep-learning-institute-training-at-gtc-digital,"Originally published at:			NVIDIA Discounts Deep Learning Institute Training at GTC Digital | NVIDIA Technical Blog
As part of GTC Digital, the NVIDIA Deep Learning Institute (DLI) is ramping up the ability for developers, researchers, data scientists, and students to receive hands-on, instructor-led, online training at home. This includes full-day courses, where developers can earn a certificate in AI or accelerated computing, or two-hour instructor-led courses, offered during the next three weeks. The courses are available for…Powered by Discourse, best viewed with JavaScript enabled"
157,destination-earth-supercomputer-simulation-to-support-europe-s-climate-neutral-goals,"Originally published at:			https://developer.nvidia.com/blog/destination-earth-supercomputer-simulation-to-support-europes-climate-neutral-goals/
To support its efforts to become climate neutral by 2050, the European Union has launched a Destination Earth initiative to build a detailed digital simulation of the planet that will help scientists map climate development and extreme weather events with high accuracy.Powered by Discourse, best viewed with JavaScript enabled"
158,gpu-accelerated-supercomputers-aid-researchers-working-to-stop-the-dengue-virus,"Originally published at:			GPU-Accelerated Supercomputers Aid Researchers Working to Stop the Dengue Virus | NVIDIA Technical Blog
Using supercomputers equipped with NVIDIA Tesla GPUs, a team of researchers at Colorado State University have identified a critical protein structure in the dengue virus that could potentially prevent the replication of the disease. The study, recently published in the PLOS Computational Biology Journal, focused on a small segment of a viral enzyme called nonstructural…Powered by Discourse, best viewed with JavaScript enabled"
159,nvidia-offers-covid-19-researchers-free-access-to-parabricks,"Originally published at:			https://developer.nvidia.com/blog/nvidia-offers-covid-19-researchers-free-access-to-parabricks/
Starting now, NVIDIA will provide a free 90-day license to Parabricks to any researcher in the worldwide effort to fight the novel coronavirus. Based on the well-known Genome Analysis Toolkit, Parabricks uses GPUs to accelerate by as much as 50x the analysis of sequence data. If you have access to NVIDIA GPUs, fill out this form to request a Parabricks…Powered by Discourse, best viewed with JavaScript enabled"
160,nvidia-rtx-top-3-week-of-november-9-2018,"Originally published at:			NVIDIA RTX Top 3: Week of November 9, 2018 | NVIDIA Technical Blog
Every week, we’ll be delivering 3 interesting stories coming from the world of RTX Game Development. 1 – “Final Fantasy XV” Director Takashi Aramaki shares his thoughts on DLSS Square Enix has strived to keep Final Fantasy XV on the bleeding edge of the technological curve, evolving the product over time with new game development tools.…Powered by Discourse, best viewed with JavaScript enabled"
161,nvidia-research-learning-and-rendering-dynamic-global-illumination-with-one-tiny-neural-network-in-real-time,"Originally published at:			https://developer.nvidia.com/blog/nvidia-research-learning-and-rendering-dynamic-global-illumination-with-one-tiny-neural-network-in-real-time/
Today, NVIDIA is releasing a SIGGRAPH 2021 technical paper, “Real-time Neural Radiance Caching for Path Tracing” that introduces another leap forward in real-time global illumination: Neural Radiance Caching. Global illumination, that is, illumination due to light bouncing around in a scene, is essential for rich, realistic visuals. This is a challenging task, even in cinematic…Powered by Discourse, best viewed with JavaScript enabled"
162,nvidia-launches-teaching-kit-for-edge-ai-and-robotics-educators,"Originally published at:			NVIDIA Launches Teaching Kit for Edge AI and Robotics Educators | NVIDIA Technical Blog
NVIDIA DLI, with two leading universities, released a new multimodule teaching kit for educators to teach the latest material on edge AI and robotics.Yes, it was very helpful.  Wonderful Progress.  I would like to receive access to modules 5 and 6 if possible.  Thanks in Advance.  You all did a wonderful job - just what the students needed.  I would prefer they use the Nvidia capabilities vs anything else.  Beautiful.Hi @g.brown1, thanks for your interest - modules 5 and 6 are still in development.  We’ll release an update to the Teaching Kit when these modules are complete.Dusty, I have followed everything Jetson for a couple of years beginning with the TK1 and TX2 through Nano models. I have also followed the sessions you have done, and I am ecstatic about ROS2 inclusion as well as how nicely the courses are laid out. We work with a great developer network here in Houston that has climbed all over Jetson. Needless to say, I am more than just a fan. We are building a big program in A.I. here at Houston Community College because Houston and Texas are booming. So, we are trying to get students trained with the best of the best. I have been through the different iterations of Jetbot and what you presented in the recent session makes it easier for our faculty to dig into. I don’t care what the state of 5 and 6 are, in fact we would love to contribute if you find that helpful. But I took your highlights and words seriously that you might make early versions available. We have been working with Louis Stewart and Joe Bungo on finding ways to collaborate.I think what you have done with the Jetson Course is just right for our students. One of the things that distinguishes our program in A.I. from any other is the emphasis we place on the High-Quality Hand’s-on nature of our Laboratories. This makes all the difference in how well students will acquire the necessary skills to become A.I. Support Specialists. Our job at the Community College Level is to train as many people as fast as possible with the best foundations possible. I firmly believe Nvidia is key to how we attain “Highest Quality in our laboratories.” So, my appeal is please let us help and participate. We can be users, contributors, developers, and assessment feedback providers. So, thank you in advance for all the great work. You have made our jobs infinitely easier in training a very diverse workforce.As a side note, prior to Nano I taught Raspberry Pi laboratories (up to RPI4 8Gig) to well over 1,000 HCC students to help them get into programming and understanding robotics because no formal courses were offered to “No Prerequisite” students. This was a testbed to understand what was really needed. We began to look at TK1 but affordability and capability were a little daunting. When Nano came out - we burned through quite a few trying to set up more official programs. Then Covid hit. All we could do was experiment with the different models and prepare lab setups. We had postponed the laboratory developments until Spring of 2022. Given that background we are more than prepared to implement your programs. HCC has more than 100,000 students across 21 campuses, so we are a ready and willing partner.Kindest Regards, RaymondHi Raymond - thanks for the kind words!   It sounds like a great program that you are building down there.Many of the labs from the course use the various GitHub projects/tutorials that we have.The GitHub for Module 6 (conversational AI) is already functional - https://github.com/dusty-nv/jetson-voice
So you can take that for a spin now if you want.The code for Module 5 (reinforcement learning) I still need to update, but will let you know when it’s ready to try.  It will be in this repo, but it needs updated in order to be functional again.  https://github.com/dusty-nv/jetson-reinforcementSincerest thanks. You are amazing!Best regards, Raymond.Powered by Discourse, best viewed with JavaScript enabled"
163,upcoming-event-generating-synthetic-data-using-isaac-replicator-composer,"Originally published at:			https://developer.nvidia.com/blog/upcoming-event-generating-synthetic-data-using-isaac-replicator-composer/
Powered by Discourse, best viewed with JavaScript enabled"
164,explainer-what-is-confidential-computing,"Originally published at:			What Is Confidential Computing? | NVIDIA Blogs
Confidential computing is a way of processing data in a protected zone of a computer’s processor, often inside a remote edge or public cloud server, and proving that no one viewed or altered the work.Powered by Discourse, best viewed with JavaScript enabled"
165,inception-spotlight-watch-deepgram-transcribe-10-hours-of-audio-in-just-40-seconds-using-gpus,"Originally published at:			https://developer.nvidia.com/blog/inception-spotlight-deepgram/
Deepgram, an NVIDIA Inception startup developing automatic speech recognition (ASR) deep learning models, recently published a new demo that highlights the speed and scalability of its platform on NVIDIA GPUs.   “We’ve reinvented Automatic Speech Recognition (ASR) with a complete, deep learning model that allows companies to get faster, more accurate transcription, resulting in more reliable…Powered by Discourse, best viewed with JavaScript enabled"
166,drive-px-application-development-using-nsight-eclipse-edition,"Originally published at:			https://developer.nvidia.com/blog/drivepx-application-development-using-nsight-eclipse-edition/
The NVIDIA DRIVE PX AI car computer enables OEMs, tier 1 suppliers, startups and research institutions to accelerate the self-driving car systems development. The NVIDIA DriveWorks companion Software Development Kit (SDK) for DRIVE PX includes a number of open-source reference samples, development tools and library modules targeting autonomous driving applications. After properly configuring your development environment, you can customize these…Thanks Davide and will keep cooperation..This is the most comprehensive walkthrough for DriveWorks PX2 for Nsight Eclipse that I have found while getting started on the platform. We have printed it out and will be reading it over lunch...lol....line by line. We greatly appreciate the time and effort that David and Nvidia took to create this document. Your hard work will not go unnoticed and we truly, truly look forward to more amazing documentation like this from you and your team. Wishing you all the best! The Deep Learning Teamcool!!Hello how to apply Driveworks 1.2?Driveworks 0.6 worked, but 1.2 not.hi, thank u  Davide, it's really help.I can compile success for host, but not for px2. It shows error  ""fatal error: dw/core/EGL.h: not found"" , i have checked that it exists.By the way, i can compile for px2 manually not by  NVIDIA Nsight Eclipse Edition.Thank u ~~~Powered by Discourse, best viewed with JavaScript enabled"
167,watch-the-gtc-2020-keynote,"Originally published at:			Watch the GTC 2020 Keynote | NVIDIA Technical Blog
NVIDIA CEO Jensen Huang Monday kicked off this week’s GPU Technology Conference. Huang made major announcements in data centers, edge AI, collaboration tools and healthcare in a talk simultaneously released in nine episodes, each under 10 minutes. The announcements touch on everything from healthcare to robotics to videoconferencing, Huang’s underlying story was simple: AI is changing…Powered by Discourse, best viewed with JavaScript enabled"
168,interpreting-privacy-policies-with-artificial-intelligence,"Originally published at:			Interpreting Privacy Policies with Artificial Intelligence | NVIDIA Technical Blog
A team of researchers from EPFL in Switzerland, University of Wisconsin and University of Michigan developed a deep learning-based program that can automatically read and make sense of any online service’s privacy policy. “Our program uses simple graphs and color codes to show users exactly how their data could be used. For instance, some websites…Powered by Discourse, best viewed with JavaScript enabled"
169,dli-course-optimizing-cuda-machine-learning-codes-with-nsight-profiling-tools,"Originally published at:			Courses – NVIDIA
Learn how to use NVIDIA developer tools, Nsight Systems, and Nsight Compute to optimize CUDA applications in this new course from DLI.Powered by Discourse, best viewed with JavaScript enabled"
170,a-toolkit-to-fine-tune-deep-neural-networks-and-simplify-training-tasks-for-intelligent-video-analytics,"Originally published at:			A Toolkit to fine-tune deep neural networks and simplify training tasks for Intelligent Video Analytics | NVIDIA Technical Blog
By Amulya Vishwanath Announcing Transfer Learning Toolkit General Availability  Warehouses, cities, retail and factory floors are going through massive transformations to support the need for an intelligent and sustainable environment powered by artificial intelligence.  Building an Intelligent Video Analytics (IVA) pipeline by training deep neural networks from scratch is a time consuming process and requires…Powered by Discourse, best viewed with JavaScript enabled"
171,nvidia-to-benefit-from-shift-to-gpu-powered-deep-learning,"Originally published at:			https://developer.nvidia.com/blog/nvidia-to-benefit-from-shift-to-gpu-powered-deep-learning/
Wired discusses Google’s announcement that it is open sourcing its TensorFlow machine learning system – noting the system uses GPUs to both train and run artificial intelligence services at the company. Inside Google, when tackling tasks like image recognition and speech recognition and language translation, TensorFlow depends on machines equipped with GPUs that were originally designed…Powered by Discourse, best viewed with JavaScript enabled"
172,solving-spacenet-road-detection-challenge-with-deep-learning,"Originally published at:			Solving SpaceNet Road Detection Challenge With Deep Learning | NVIDIA Technical Blog
It’s that time again — SpaceNet raised the bar in their third challenge to detect road-networks in overhead imagery around the world.  Today, map features such as roads, building footprints, and points of interest are primarily created through manual techniques. In the third SpaceNet challenge, competitors were tasked with finding automated methods for extracting map-ready…Hi guys, thank you for a nice article but I would like to point out one thing regarding the floodfill.Actually you do not have to do this. The hosts of the competitions provided road width in their geojson files.I tried it creating masks from this - and succeeded - loss was lower by ~25%.But graph creation algorithm worked worse.See my post for some details - https://spark-in.me/post/sp... - just search for the word ""wide""Finally, we take creative liberties to think about how we might apply these types of deep learning solutions in a broader operational sense using conditional random fields, percolation theory, and reinforcement learning.If you use the same tiramisu architecture for two experiments, one using an 8-channel input, the other using a 3-channel input, will your GPU footprint be larger on the 8-channel experiment? Is there a downside in training performance by adding many more channels? I have tested 1-channel and 3-channel tiramisu and have found the GPU footprint and training performance to be very similar.https://hardwarereview79405...https://www.blogger.com/u/1...https://wordpress.com/block...https://www.blogger.com/u/3...https://greenwheelbarrow.we...https://videoshopgalley.wee...https://wheelbarrow4201.wix...https://sitebuilder.yola.co...https://sites.google.com/s/...https://sites.google.com/s/...Powered by Discourse, best viewed with JavaScript enabled"
173,introducing-the-nvidia-jetson-nano-2gb-developer-kit,"Originally published at:			https://developer.nvidia.com/blog/developer-blog-introducing-the-nvidia-jetson-nano-2gb-developer-kit/
NVIDIA announced the Jetson Nano 2GB Developer Kit, the ideal hands-on platform for teaching, learning, and developing AI and robotics applications. The NVIDIA Jetson platform introduced six years ago revolutionized embedded computing by delivering the power of artificial intelligence to edge computing devices. NVIDIA Jetson today is widely used in diverse fields such as robotics, retail,…Powered by Discourse, best viewed with JavaScript enabled"
174,nvidia-gives-away-more-v100s-to-ai-researchers,"Originally published at:			NVIDIA Gives Away More V100s to AI Researchers | NVIDIA Technical Blog
After surprising some of the top AI researchers with the first Tesla V100 GPU accelerators last month at CVPR, NVIDIA struck again by handing out 15 more to researchers at the International Conference on Machine Learning (ICML) in Sydney. Given out at a meetup for participants in our NVIDIA AI Labs program at ICML, and signed by…Powered by Discourse, best viewed with JavaScript enabled"
175,multinode-multi-gpu-using-nvidia-cufftmp-ffts-at-scale,"Originally published at:			https://developer.nvidia.com/blog/multinode-multi-gpu-using-nvidia-cufftmp-ffts-at-scale/
cuFFTMp is a multi-node, multi-process extension to cuFFT that enables scientists and engineers to solve challenging problems on exascale platforms.I would like to thank the cuFFT team for all their hard work and congratulate them on this amazing product! We are looking forward to customer feedback :)Excited to see this released! Will be running this on our systems asapPowered by Discourse, best viewed with JavaScript enabled"
176,scaling-zero-touch-roce-technology-with-round-trip-time-congestion-control,"Originally published at:			https://developer.nvidia.com/blog/scaling-zero-touch-roce-technology-with-round-trip-time-congestion-control/
The new NVIDIA RTTCC congestion control algorithm for ZTR delivers RoCE performance at scale, without special switch infrastructure configuration.Thanks for this write-up. I tried ZTR-RTTCC today between a Windows machine and Linux, with the Linux machine running a ConnectX-6 Dx card. I enabled ZTR-RTTCC with the mlxconfig -d /dev/mst/mt4125_pciconf0 -y s ROCE_CC_LEGACY_DCQCN=0 command, and the rping + nd_rping works with my Windows 10 machine (which is using a ConnectX-4 NIC). While continously running the rping + nd_rping commands, I did not see any traffic over my NIC on the Windows Task Manager Performance tab (the graph showing traffic over the NIC did not move whatsoever).But then, when I put in the command that was used on the blog: mlxreg -d /dev/mst/mt4125_pciconf0 --reg_id 0x506e --reg_len 0x40 --set ""0x0.0:8=2,0x4.0:4=15"" -y
I noticed that when trying the ping again, I saw the traffic on the Task Manager Performance tab of my Windows machine. About 5MB/s for the continuous nd_rping. This used to be not visible at all, as RDMA bypasses the host.Hi @user135095Regards,
AvivHi @AvivBThank you for your reply, much appreciated. I did get it working on another system between a Linux server (ConnectX-6 DX card, 100G) and a Windows PC (ConnectX-4 EN card, specifically MCX416A-BCAT).
A few more questions that aren’t clear yet:Thank you for your help and explanation.Hi again,
ZTR contains several sub-features, ConnectX4-EN indeed supports adaptive retransmission.
Newer devices support more (for example ConnectX5+ devices support tx-window).
ZTR-CC is the new congestion control algorithm and is only supported in ConnectX6-DX+ devices. Part of ZTR-CC is RTT measurements, and only these devices support handling the RTT packets. Other devices will fwd these packets to SW where it will be ignored.The counters for drops/OOS are exposed in windows perfmon and in linux in /sys/class/infinibandHi Aviv, thanks for the clarification.Just to confirm - ConnectX-4 does support ZTR, but not ZTR-CC. I have enabled ZTR on my ConnectX-4 NIC, and have enabled ZTR-CC on my ConnectX-6 NIC. Currently, RDMA works just fine between those two cards. This should mean the regular ZTR is working, but not ZTR-CC correct? I followed the WinOF-2 documentation but there isn’t a way to verify that this is ZTR doing it work - although I do believe ZTR must be functioning correctly.What would be really helpful is if the WinOF-2 and MLNX EN / OFED docs were more clear and went more in-depth about ZTR and ZTR-CC - it’s really exciting technology that I’m glad NVidia is working on.EDIT: I’m also getting this error when trying to put in this command :Hi @AvivBThank you for sharing the wonderful experience.
I am a student in NXC lab at Seoul National University, Korea and my research interest is RDMA-based congestion control.
We are considering purchasing ConnectX-6 DX for test bed configuration. Before that, I’d like to ask you a few questions based on this post. It would be very helpful if you could answer.Can you explain about the congestion control programability of the ConnectX-6 DX? It was difficult to find this information in documents other than firmware updates. Can you slightly explain about which route and programming language can change and update the congestion control algorithm in ConnectX-6 DX.To the best of my knowledge, there are a variety of RoCE-based congestion controls(e.g., TIMELY, HPCC, Swift) in addition to  ZTR-RTTCC and DCQCN. Some of those algorithms were implemented in forms of NIC+FPGA due to unsufficient NIC programmability. Do you think can these algorithms be implemented on top of ConnectX-6 DX?I have a question about RTT measurement. As far as I understand, in  ZTR-RTTCC,  RTT measurement was not done for all data packets, but only for RTT packets. Is it possible to program in ConnectX-6 DX that measures RTT for each data packet on the sender? Also, if you have any interesting experience(e.g., noise) about the precision or accuracy of RTT measurement, I would really appreciate it if you could share it with me.Best regards,
TaekyoungPowered by Discourse, best viewed with JavaScript enabled"
177,swinburnes-new-gpu-accelerated-supercomputer-to-study-gravity,"Originally published at:			Swinburne’s New GPU-Accelerated Supercomputer to Study Gravity | NVIDIA Technical Blog
Melbourne’s Swinburne University of Technology announced plans of a new supercomputer that will be used to power research into astrophysics and gravitational waves, with the university seeking to further prove the science behind Einstein’s theory of general relativity. “While Einstein predicted the existence of gravitational waves, it took one hundred years for technology to advance…Powered by Discourse, best viewed with JavaScript enabled"
178,reducing-temporal-noise-on-images-with-nvidia-vpi-on-nvidia-jetson-embedded-computers,"Originally published at:			https://developer.nvidia.com/blog/reducing-temporal-noise-on-images-with-vpi-on-jetson-embedded-computers/
The NVIDIA Vision Programming Interface (VPI) is a software library that provides a set of computer-vision and image-processing algorithms. The implementations of these algorithms are accelerated on different hardware engines available on NVIDIA Jetson embedded computers or discrete GPUs. In this post, we show you how to run the Temporal Noise Reduction (TNR) sample application…I hope you had a great start with VPI and is already perceiving the advantages the API is able to deliver. Feel free to bring up points you’d like to discuss and we will gladly support you in your journey towards an optimal and optimized implementation of your application. For more dedicated discussions, start a thread under the section for your chosen Jetson platform in our forum, as in the link below:Discussions relating to the Jetson DevKits and other Embedded computing devicesHi! This reply might seem abrupt, but it is so difficult for us to contact anyone who is developing VPI. So firstly, I apologize for any inconvenience that may occur.My team in CMU and I are currently trying to incorporate VPI into our project (see our work using VPI on Toward Efficient and Robust Multiple Camera Visual-inertial Odometry - YouTube for ICRA 2022). We use  Harris Corner Detector and Pyramidal LK Optical Flow in our work.  During our work we notice some issues with these two modules:The distribution of feature points detected by VPI is not as uniform as that of goodfeaturetotrack in OpenCV, even though we apply a quad-tree algorithm to filter the feature points. Such nonuniformity makes the performance of our VPI based system worse than the OpenCV based system. Can you try to improve the Harris Corner Detector, making the distribution more uniform? Or can you implement a new module using Shi-tomasi
features like goodfeaturetotrack in OpenCV? We will consider using this new module as our feature detector if it yields a more uniform distribution.The feature tracker (Pyramidal LK Optical Flow) is not stable. More points are lost during tracking compared to the calcOpticalFlowPyrLK in OpenCV. We prefer a more stable tracking performance. Therefore, can you improve the tracking stability?Besides, there is a tiny bug related to the LK tracker. On the VPI official website, it says that the key point is not being tracked if the tracking status is zero. However, during our tests, we found that the status is actually one if the key point is not tracked.I really appreciate it if someone can spot this message and help us fix the problems mentioned above. It would be better if someone can contact us for further development on VPI. We are also looking forward to further cooperation with VPI developers. Here is my email: 118010095@link.cuhk.edu.cn.Looking forward to your reply!Thank you for your interest in VPI and your report. NVIDIA developers forum is the right place to contact VPI developers and ask questions like this.Which backend are you using for Harris and feature tracker?Harris Corner Detector can’t guarantee uniformity in general, as it returns keypoints whose score is at least the strengthThresh (default == 20). These come from salient regions in the image, and its distribution basically depends on the image contents. I’d suggest you to play with values in VPIHarrisCornerDetectorParams and see if you can get better results. We’ll evaluate your suggestion of using Shi-Tomasi criteria for scoring function, thereby improving results.Regarding the feature tracker, it is a known issue that PVA backend loses track rather quickly. We didn’t see anything abnormal with the CPU and CUDA implementations, though. We’ve noticed, however, that results are sensitive to the values given to VPIKLTFeatureTrackerParams. I’d suggest to play with these parameters too and see if you get the tracking quality you’re after.Excited to see your response! We are using CUDA/GPU as backend.We have been playing with the parameters for quite a long time, and the best results we can get are still below our expectation.Besides, Is there any possibility for our team to cooperate with the VPI developer teams and make contributions to VPI? Or is there any possibility for you to make VPI open-source so that we can make our own modification on the modules? If your team has interests, you could  send the contact information through the email I provided in the previous message.Powered by Discourse, best viewed with JavaScript enabled"
179,attend-expert-led-developer-sessions-at-gtc-2022,"Originally published at:			Top Conference Sessions for Developers | GTC 2022 | NVIDIA
Register now and get ready to explore cutting-edge technology and the latest developer tools at GTC.Powered by Discourse, best viewed with JavaScript enabled"
180,predicting-aggressive-prostate-cancer-with-ai,"Originally published at:			Predicting Aggressive Prostate Cancer with AI | NVIDIA Technical Blog
University of Alberta scientists developed a deep learning-based prostate cancer diagnostic platform that only uses a single drop of blood which will allow men to bypass the current painful biopsy methods. Using a GTX 1060 GPU, CUDA and the MathWorks Neural Network Toolbox, the scientists’ trained their model on information from millions of cancer cell…Powered by Discourse, best viewed with JavaScript enabled"
181,immortal-content-by-the-help-of-optimization,"It is a known fact that optimization helps in keeping the content keeping relevant in all times, whether the year is 2020 or 2030; the content is going to maintain its value. Most of the wiki page creation services help people understand that how to make a wiki page and help them understand its value. They tend to optimize their content for the same reason, ensuring that the topics they use for their content assist viewers accordingly no matter what era it is. It re purposes the content in a manner that it stays ahead ofits time, standing out from the competition in the industry. Can you suggest ways to create an evergreen content?Powered by Discourse, best viewed with JavaScript enabled"
182,nvidia-invents-ai-interactive-graphics,"Originally published at:			NVIDIA Invents AI Interactive Graphics | NVIDIA Technical Blog
Twenty-five years ago NVIDIA transformed the computer graphics industry by building the first GPU, the modern day tool of the da Vincis and Einsteins of our time. Now, a new deep learning-based model developed by NVIDIA researchers aims to catapult the graphics industry into the AI chapter. Using a conditional generative neural network as a…Powered by Discourse, best viewed with JavaScript enabled"
183,atomwise-raises-123-million-to-accelerate-ai-drug-discovery,"Originally published at:			https://developer.nvidia.com/blog/atomwise-raises-123-million-to-speed-up-ai-drug-discovery/
Drug discovery startup Atomwise recently raised $123 million bringing the total amount of capital the company has raised to date to almost $175 million.Powered by Discourse, best viewed with JavaScript enabled"
184,just-released-nvidia-hpc-sdk-v23-3,"Originally published at:			Release Notes Version 23.3
Version 23.3 expands platform support and provides minor updates to the NVIDIA HPC SDK.Powered by Discourse, best viewed with JavaScript enabled"
185,gtc-2020-nvidia-quadro-rtx-for-healthcare-improving-patient-outcomes,"GTC 2020 S22422
Presenters: CARL FLYGARE,PNY Technologies
Abstract
This talk covers how Quadro RTX features like real-time ray tracing and AI can reshape healthcare and the life sciences disciplines that support basic research leading to new treatment options and better patient outcomesWatch this session
Join in the conversation below.Powered by Discourse, best viewed with JavaScript enabled"
186,deep-learning-accurately-forecasts-extreme-weather-events,"Originally published at:			https://developer.nvidia.com/blog/deep-learning-accurately-forecasts-extreme-weather-events/
To accurately forecast heat waves and cold spells, Rice University engineers developed a deep learning-based system that can accurately predict extreme weather events up to five days in advance with 85% accuracy.  “When you get these heat waves or cold spells, if you look at the weather map, you are often going to see some…Powered by Discourse, best viewed with JavaScript enabled"
187,accelerating-ultra-realistic-game-development-with-nvidia-dlss-3-and-nvidia-rtx-path-tracing,"Originally published at:			https://developer.nvidia.com/blog/accelerating-ultrarealistic-game-development-with-nvidia-dlss-3-and-rtx-path-tracing/
Discover how NVIDIA DLSS 3 and RTX path tracing are accelerating ultra realistic game development.Powered by Discourse, best viewed with JavaScript enabled"
188,design-your-robot-on-hardware-in-the-loop-with-nvidia-jetson,"Originally published at:			https://developer.nvidia.com/blog/design-your-robot-on-hardware-in-the-loop-with-nvidia-jetson/
Hardware-in-the-loop (HIL) testing is a powerful tool used to validate and verify the performance of complex systems, including robotics and computer vision. This post explores how HIL testing is being used in these fields with the NVIDIA Isaac platform. The NVIDIA Isaac platform consists of NVIDIA Isaac Sim, a simulator that provides a simulated environment…is it possible to use an ethernet cable w/o a router to connect the workstation and the Jetson? I tried to follow the instructions below:Set of demo to try Isaac ROS with Isaac SIM. Contribute to NVIDIA-AI-IOT/isaac_demo development by creating an account on GitHub.Run the script at Jetson:
$ bash src/isaac_demo/scripts/run_in_docker.shAnd launch foxglove studio at workstation and try to connect jetson, it always show connection failed.If run a command on Jetson (w/o container) like:
$ python3 -m foxglove_websocket.examples.json_serverFoxglove can be connected, only issue is that messages cannot be seen on Foxglove.My questions are:Powered by Discourse, best viewed with JavaScript enabled"
189,turing-texture-space-shading,"Originally published at:			Turing Texture Space Shading | NVIDIA Technical Blog
Turing GPUs introduce a new shading capability called Texture Space Shading (TSS), where shading values are dynamically computed and stored in a texture as texels in a texture space. Later, pixels are texture mapped, where pixels in screen-space are mapped into texture space, and the corresponding texels are sampled and filtered using a standard texture…I love these blog posts because they explain me how all these new tools work. Keep on writing them!Thanks for this post, very interesting.Would it be possible to have some sample code on how to implement that in dx12? A github project with different use cases would be awesome!I couldn't find any sample for turing extensions except for raytracing and variable rate shading (in vrworks 3), I'm really looking forward to texture space shading and mesh shading samples.Powered by Discourse, best viewed with JavaScript enabled"
190,diy-cat-stopper-sprinkler-system,"Originally published at:			https://developer.nvidia.com/blog/diy-cat-stopper-sprinkler-system/
Motivated by neighbor’s cat that poops on his lawn, an NVIDIA engineer built a smart system in less than fifteen hours that automatically turns on the sprinklers when a cat is on his lawn. Robert Bond, a system software engineer, first trained a neural network to recognize cat images using a TITAN GPU and the…Powered by Discourse, best viewed with JavaScript enabled"
191,training-ai-for-self-driving-vehicles-the-challenge-of-scale,"Originally published at:			https://developer.nvidia.com/blog/training-ai-for-self-driving-vehicles-the-challenge-of-scale/
Modern deep neural networks, such as those used in self-driving vehicles, require a mind boggling amount of computational power. Today a single computer, like NVIDIA DGX-1, can achieve computational performance on par with the world’s biggest supercomputers in the year 2010 (“Top 500”, 2010). Even though this technological advance is unprecedented, it is being dwarfed…Powered by Discourse, best viewed with JavaScript enabled"
192,maximizing-nvidia-dgx-with-kubernetes,"Originally published at:			Maximizing NVIDIA DGX with Kubernetes | NVIDIA Technical Blog
For the latest information about how to deploy Kubernetes on NVIDIA GPUs, see the Kubernetes section of the NVIDIA Data Center documentation. NVIDIA GPU Cloud (NGC) provides access to a number of containers for deep learning, HPC, and HPC visualization, as well as containers with applications from our NVIDIA partners – all optimized for NVIDIA GPUs…Powered by Discourse, best viewed with JavaScript enabled"
193,explainer-what-is-green-computing,"Originally published at:			What Is Green Computing? | NVIDIA Blog
reen computing, also called sustainable computing, aims to maximize energy efficiency and minimize environmental impact in the ways computer chips, systems, and software are designed and used.Powered by Discourse, best viewed with JavaScript enabled"
194,nvidia-researchers-to-present-groundbreaking-ai-projects-at-cvpr-2018,"Originally published at:			https://developer.nvidia.com/blog/nvidia-researchers-to-present-groundbreaking-ai-projects-at-cvpr-2018/
NVIDIA Researchers will present 19 accepted papers and posters, seven of them speaking sessions, at the annual Computer Vision and Pattern Recognition (CVPR) conference June 18 – 22 in Salt Lake City, Utah. Speaking Sessions SPLATNet: Sparse Lattice Networks for Point Cloud Processing Hang Su, Varun Jampani, Deqing Sun, Subhransu Maji, Evangelos Kalogerakis, Ming-Hsuan Yang,…Powered by Discourse, best viewed with JavaScript enabled"
195,this-reinforcement-learning-algorithm-can-capture-motion-and-recreate-it,"Originally published at:			This Reinforcement Learning Algorithm Can Capture Motion and Recreate It | NVIDIA Technical Blog
Researchers from the University of California, Berkeley developed a reinforcement learning-based system that can automatically capture and mimic the motions it sees in YouTube videos.  “Data-driven methods have been a cornerstone of character animation for decades, with motion-capture being one of the most popular sources of motion data. Mocap data is a staple for kinematic…Powered by Discourse, best viewed with JavaScript enabled"
196,apple-watch-detects-signs-of-diabetes,"Originally published at:			Apple Watch Detects Signs of Diabetes | NVIDIA Technical Blog
Researchers at digital health startup Cardiogram and UCSF (University of California San Francisco) developed a deep learning algorithm that can diagnose diabetes with an 85% accuracy by using the smart watch’s built-in heart rate sensor. With the help of Tesla P100 GPUs and the cuDNN-accelerated TensorFlow deep learning framework, the team trained their DeepHeart neural…Powered by Discourse, best viewed with JavaScript enabled"
197,gtc-2020-an-overview-of-gpu-recurrent-neural-network-performance,"GTC 2020 S21301
Presenters: Jeremy Appleyard,NVIDIA
Abstract
We’ll focus on performance of recurrent neural networks (RNNs) at the sub-framework level. There are several distinct methods one can use to implement an RNN, each with advantages and disadvantages, and the right choice will depend on both the target GPU and the network hyperparameters. We’ll cover these methods in detail and give approximate hyper-parameter ranges for each method.Watch this session
Join in the conversation below.Powered by Discourse, best viewed with JavaScript enabled"
198,cuda-pro-tip-view-assembly-code-correlation-in-nsight-visual-studio-edition,"Originally published at:			https://developer.nvidia.com/blog/cuda-pro-tip-view-assembly-code-correlation-nsight-visual-studio-edition/
While high-level languages for GPU programming like CUDA C offer a useful level of abstraction, convenience, and maintainability, they inherently hide some of the details of the execution on the hardware. It is sometimes helpful to dig into the underlying assembly code that the hardware is executing to explore performance problems, or to make sure…Powered by Discourse, best viewed with JavaScript enabled"
199,8i-raises-13-5m-to-chase-the-human-side-of-virtual-reality,"Originally published at:			https://developer.nvidia.com/blog/8i-raises-13-5m-to-chase-the-human-side-of-virtual-reality/
Making virtual reality more human by creating the most immersive 3D video of real people. Running CUDA optimized algorithms on a cluster of NVIDIA GPUs, 8i is building a platform for content creators to deliver their own 3D renderings and virtual reality experiences in everything from dating apps to feature films. 8i, co-founded by a…Powered by Discourse, best viewed with JavaScript enabled"
200,nvidia-compatible-warping-and-edge-blending-applications,"Hello,
Does anyone have a list of software that does not only edge blending but twist and warp as well? We have 3 projectors pointing at a curved Screen Innovations screen. We’re unable to get the corners of the image to sit nice.Powered by Discourse, best viewed with JavaScript enabled"
201,jumpstarting-link-level-simulations-with-nvidia-sionna,"Originally published at:			https://developer.nvidia.com/blog/jumpstarting-link-level-simulations-with-sionna/
Sionna is a GPU-accelerated open-source library for link-level simulations.Is there any instruction for setting NVIDIA sionna in google colab?I solved by myself sorry for your asking.
I recognized pip package already supplied.
And following command (currently 0.8.0) works fine .pip install sionnaHi,
how “soon” will SionnaRTX become availible? Any idea yet?Powered by Discourse, best viewed with JavaScript enabled"
202,visual-foundation-models-for-medical-image-analysis,"Originally published at:			https://developer.nvidia.com/blog/visual-foundation-models-for-medical-image-analysis/
The analysis of 3D medical images is crucial for advancing clinical responses, disease tracking, and overall patient survival. Deep learning models form the backbone of modern 3D medical representation learning, enabling precise spatial context measurements that are essential for clinical decision-making. These 3D representations are highly sensitive to the physiological properties of medical imaging data,…Powered by Discourse, best viewed with JavaScript enabled"
203,dli-course-getting-started-with-doca-flow,"Originally published at:			Courses – NVIDIA
In this new course learn about creating software-defined, cloud-native, DPU-accelerated services with zero-trust protection for increasing the performance and security demands of modern data centers.Powered by Discourse, best viewed with JavaScript enabled"
204,deep-learning-training-at-gdc-2017,"Originally published at:			Deep Learning Training at GDC 2017 | NVIDIA Technical Blog
For the first time at GDC, the NVIDIA Deep Learning Institute will be hosting two half-day workshops consisting of a seminar, hands-on lab, and Q&A session on Tuesday, February 28th targeted at game developers who wish to gain insight into the principles of deep learning and its applications. NVIDIA instructors will cover fundamental concepts of…Powered by Discourse, best viewed with JavaScript enabled"
205,facebook-donating-200-gpus-to-european-researchers,"Originally published at:			Facebook Donating 200 GPUs to European Researchers | NVIDIA Technical Blog
The Facebook Artificial Intelligence Research (FAIR) lab announced a new Research Partnership Program to spur advances in Artificial Intelligence and machine learning — Facebook will be giving out 25 servers powered with GPUs, free of charge. The first recipient to receive 32 GPUs in four GPU servers is Klaus-Robert Müller of TU Berlin. “Dr. Müller…Powered by Discourse, best viewed with JavaScript enabled"
206,top-nine-developer-questions-about-the-ray-tracing-essentials-series,"Originally published at:			Top Nine Developer Questions About the Ray Tracing Essentials Series | NVIDIA Technical Blog
During our recent webinar on ray tracing, available on-demand now, we received over 800 questions from developers who attended the session. Thanks to everyone who participated! After combing through the stack, NVIDIA researchers Eric Haines and Adam Marrs have selected the nine most compelling questions, and provided in-depth answers.Powered by Discourse, best viewed with JavaScript enabled"
207,gtc-2020-animation-segmentation-and-statistical-modeling-of-biological-cells-using-microscopy-imaging-and-gpu-compute,"GTC 2020 S21944
Presenters: Theo Knijnenburg,Allen Institute for Cell Science ; Jianxu Chen,Allen Institute for Cell Science
Abstract
This presentation describes the use of GPU compute in the Allen Institute for Cell Science. The mission of the Allen Institute for Cell Science is to create dynamic and multi-scale visual models of cell organization, dynamics, and activities that capture experimental observation, theory, and prediction to understand and predict cellular behavior in its normal, regenerative, and pathological contexts. Our first project is to understand how the parts of the cells integrate to determine diverse cellular behaviors as revealed through 3D live-cell imaging, creating a dynamic and animated virtual model of the cell. We will describe three applications where we use GPU-compute solutions for our research; these include (1) animation, (2) segmentation, and (3) statistical modeling of biological cell images.Watch this session
Join in the conversation below.More info about our work at Allen Institute for Cell Science, feel free to check out https://www.allencell.org/   and  https://forum.allencell.org/Powered by Discourse, best viewed with JavaScript enabled"
208,train-and-deploy-deep-learning-applications-with-nvidia-digits-5-and-new-tensorrt,"Originally published at:			https://developer.nvidia.com/blog/train-and-deploy-deep-learning-applications-with-nvidia-digits-5-and-new-tensorrt/
Today we are announcing the production release of NVIDIA DIGITS 5 and  NVIDIA TensorRT. DIGITS is an interactive deep neural network training application for developers to rapidly train highly accurate neural networks for image classification, segmentation and object detection. Trained models can be deployed to the cloud, PC, embedded or automotive GPU platforms, with TensorRT…Powered by Discourse, best viewed with JavaScript enabled"
209,rethinking-remote-console-for-edge-computing,"Originally published at:			https://developer.nvidia.com/blog/rethinking-remote-console-for-edge-computing/
For IT administrators, remote access to all of their machines reduces the time and effort spent on urgent fixes and provides peace-of-mind in the case of emergencies or major issues.Powered by Discourse, best viewed with JavaScript enabled"
210,this-is-an-example-question-post-for-the-ama-with-the-get3d-team,"This would be an example of a question for the GET3D Team.
The Team will post a reply to these questions.
See you right here on May 4th.Powered by Discourse, best viewed with JavaScript enabled"
211,benchmarking-gpudirect-rdma-on-modern-server-platforms,"Originally published at:			https://developer.nvidia.com/blog/benchmarking-gpudirect-rdma-on-modern-server-platforms/
NVIDIA GPUDirect RDMA is a technology which enables a direct path for data exchange between the GPU and third-party peer devices using standard features of PCI Express. Examples of third-party devices include network interfaces, video acquisition devices, storage adapters, and medical equipment. Enabled on Tesla and Quadro-class GPUs, GPUDirect RDMA relies on the ability of NVIDIA…It's very interesting how poor the bandwidth is across the QPI.  Do you see similar poor performance with a CPU-CPU transfer which goes across the QPI and then goes to a remote CPU?That is a well tested data path, reading from a peripheral through the QPI host memory of another socket. I would be surprised if there had been a severe problem.Anyway there is a visible NUMA effect on the bandwidth below 4KB, irrespective of whether the remote node destination is GPU or Host memory. On the PLX architecture (N.1), at 1KB bandwidth drops from 8.9GB/s (numactl --cpunodebind=0 --localalloc) to 5.6GB/s (numactl --cpunodebind=1 --localalloc). On architecture N.2, from 8.2GB/s to 5.9GB/s.Beyond 4KB there are no visible effects on bandwidth.Does this mean that if we want to do GPU-GPU transfers, and we can't guarantee that it won't go across the QPI, then we are better off staging the transfer via the CPUs on either end?I'm thinking of big MPI applications running on typical cluster compute nodes which are dual socket with 1 or 2 GPUs attached to each socket, and a single IB NIC attached to one of them.Do you have any idea why the PLX switch perform much better than the ivy bridge's built-in pci-e switch?Did you mean: --cpunodebind=1 ?Correct! I edited my comment above. thank you.We briefly discussed this topic off-line. For the records, it is MPI responsibility to use the most performing data path, possibly taking into account all architectural constraints.Sorry if its stupid questions: If I use RDMA and CUDA then do I seen remote GPUs locally in my code?I am trying to duplicate your test but I can't setup well. Can you post your test code or can you explain how to setup your test environment in detail?really helpful benchmark!a quick question, I have two servers with RDMA networking, but server1 has no GPU installed, can I enable GPUDirect from server1 host mem to server.GPU? if yes, what kinds of drivers shall be installed in server1? could u share your modified ibv_ud_pingpong and ib_write_bw to ensure it's GPU memory rather than host mem. thanks a lot.Hi, can you share the source code of the modified test programs? I want to benchmark our clusters.Siyuan,in the mean time, CUDA support has been added to OpenFabrics perftest.You can try the following sequence:  git clone git://git.openfabrics.org/~... cd perftest ./autogen.sh export CUDA_H_PATH=/usr/local/cuda-8.0/include/cuda.h ./configure makeIf you experience problems with the upstream version, you might also try https://github.com/drossett....With a CUDA enabled build, you can use --use_cuda option to ib_rdma_wr to pick the GPU with device id 0. You can also use the CUDA_VISIBLE_DEVICES environment variable to pick a different GPU.For example:# on host_a  ib_write_bw -n 1000 -a --use_cuda# on host_b  ib_write_bw -n 1000 -a --use_cuda host_aPlease note that the perftest latency test does not work with GPU memory. In my case, I modified the gpu_ud_pingpong test which comes with libibverbs. I apologize but I do not have an easy way to share that code.Zhao,Please see my reply to Siouan.Once compiled with CUDA support, ib_write_bw requires libcuda.so at run time. If server1 has no GPU, you might not be able to install the NVIDIA driver package at all. That package contains libcuda.so, so on server1 you might need to run a non-CUDA enabled version of ib_write_bw.Charlie,Please see my reply to Siouan.Hi Mrez,Unfortunately, it is not that simple. You might want to have a look at the rCUDA project (http://www.rcuda.net/index.....Thanks for your quickly reply, I will try the perftest. If there are problems I cannot fix I will come back to you. :)Hi,I tired the proposed sequence (git clone git://git.openfabrics.org/~..., cd perftest, ./autogen.sh ...) but at the end I got the following error:./ib_read_bw -s 1024 --use_cuda:initializing CUDAThere are 3 devices supporting CUDA, picking first...[pid = 52821, dev = 0] device name = [Tesla K20Xm]creating CUDA Ctxmaking it the current CUDA CtxcuMemAlloc() of a 65536 bytes GPU bufferallocated GPU buffer address at 0000002305440000 pointer=0x2305440000Couldn't allocate MRfailed to create mrFailed to create MRWithout using cuda the test runs correctly.Do You have an idea what could be wrong?(Cuda 8.0.61, nvidia 375.66, kernel 2.6.32-696.3.2.el6.x86_64)Hi balazs, have you solved this problem? I also come up against the same problems, and have no idea.I have a 'solution' for this issue. I got false for this query: ompi_info --all | grep btl_openib_have_driver_gdrI asked for a 'gpu rdma driver' update, then I was able to use the test (query result was true). But the result was unexpected, I got very poor send performance 0.8 GB/s (6 GB/s was the result for Host to Host)Powered by Discourse, best viewed with JavaScript enabled"
212,faster-hdbscan-soft-clustering-with-rapids-cuml,"Originally published at:			https://developer.nvidia.com/blog/faster-hdbscan-soft-clustering-with-rapids-cuml/
Discover the importance of using soft clustering to better capture nuance in downstream analysis and the performance gains possible with RAPIDS.Powered by Discourse, best viewed with JavaScript enabled"
213,transformers4rec-building-session-based-recommendations-with-an-nvidia-merlin-library,"Originally published at:			https://developer.nvidia.com/blog/transformers4rec-building-session-based-recommendations-with-an-nvidia-merlin-library/
Transformers4Rec makes it easy to use SOTA NLP architectures for sequential and session-based recommendation by leveraging HuggingFace Transformers.We have been excited on the effectiveness of Transformers even for short sessions compared to other state-of-the-art session-based recommendation models, as we observed in our research experiments and also for the competitions we won using the library.
Looking forward to hear from you if you have already tried or plan to try Transformers4rec to your own use case.Powered by Discourse, best viewed with JavaScript enabled"
214,healthcare-new-resources-from-gtc-21,"Originally published at:			https://developer.nvidia.com/blog/healthcare-new-resources-from-gtc-21/
Here are the latest resources and news for healthcare developers from GTC 21, including demos and specialized sessions for building AI in drug discovery, medical imaging, genomics, and smart hospitals.Powered by Discourse, best viewed with JavaScript enabled"
215,softbank-solves-key-mobile-edge-computing-challenges-using-nvidia-maxine,"Originally published at:			https://developer.nvidia.com/blog/softbank-solves-key-mobile-edge-computing-challenges-using-nvidia-maxine/
SoftBank is a global technology player that aspires to drive the Information Revolution.  The company operates in broadband, fixed-line telecommunications, ecommerce, information technology, finance, media, and marketing. To improve their users’ communication experience, and overcome the 5G capacity and coverage issues, SoftBank has used NVIDIA Maxine GPU-accelerated SDKs with state-of-the-art AI features to build virtual…Powered by Discourse, best viewed with JavaScript enabled"
216,advanced-api-performance-sampler-feedback,"Originally published at:			https://developer.nvidia.com/blog/advanced-api-performance-sampler-feedback/
This post covers best practices for using sampler feedback on NVIDIA GPUs. To get a high and consistent frame rate in your applications, see all Advanced API Performance tips. Sampler feedback is a DirectX 12 Ultimate feature for capturing and recording texture sampling information and locations. Sampler feedback was designed to provide better support for…Powered by Discourse, best viewed with JavaScript enabled"
217,new-nvidia-updates-for-unreal-engine-developers,"Originally published at:			https://developer.nvidia.com/blog/new-nvidia-updates-for-unreal-engine-developers/
Unreal Engine developers receive access to several NVIDIA updates. Our custom branch of Unreal Engine 4.27 (NvRTX) improves Deep Learning Super Sampling (DLSS), RTX Global Illumination (RTXGI), RTX Direct Illumination (RTXDI), and NVIDIA Real-Time Denoisers (NRD).Powered by Discourse, best viewed with JavaScript enabled"
218,bringing-hlsl-ray-tracing-to-vulkan,"Originally published at:			Bringing HLSL Ray Tracing to Vulkan | NVIDIA Technical Blog
This post was revised March 2020 to reflect newly added support in DXC for targeting the SPV_KHR_ray_tracing multi-vendor extension. Vulkan logo DirectX Ray Tracing (DXR) allows you to render graphics using ray tracing instead of the traditional method of rasterization. This API was created by NVIDIA and Microsoft back in 2018. A few months later,…Powered by Discourse, best viewed with JavaScript enabled"
219,ai-helps-programmers-write-code-more-efficiently,"Originally published at:			https://developer.nvidia.com/blog/ai-helps-programmers-write-code-more-efficiently/
Today at ICLR in Vancouver, researchers from Rice University presented a deep learning-based application that helps programmers write code in the growing multitude of application programming interfaces or APIs. The application, called Bayou, was developed by a team of computer scientists from the Intelligent Software Systems Laboratory at Rice University and funded by DARPA, the…Powered by Discourse, best viewed with JavaScript enabled"
220,developer-meetup-learn-how-metropolis-boosts-go-to-market-efforts,"Originally published at:			https://developer.nvidia.com/blog/developer-meetup-learn-how-metropolis-boosts-go-to-market-efforts​/
The new expanded NVIDIA Metropolis program offers access to the world’s best development tools and services to reduce time and cost of managing your vision AI deployments.Powered by Discourse, best viewed with JavaScript enabled"
221,nvidia-clara-guardian-enables-smart-hospitals-through-ai-at-the-edge,"Originally published at:			https://developer.nvidia.com/blog/clara-guardian-ai-edge/
NVIDIA announced Clara Guardian, an application framework and partner ecosystem that accelerates the development and deployment of smart sensors with multimodal AI anywhere in the hospital.Powered by Discourse, best viewed with JavaScript enabled"
222,inception-spotlight-startup-uses-ai-to-identify-products-in-videos,"Originally published at:			https://developer.nvidia.com/blog/inception-spotlight-startup-uses-ai-to-identify-products-in-videos/
New York City-based startup TheTake, a member of the NVIDIA Inception program, recently unveiled a new deep learning-based algorithm that can automatically decode what a celebrity, athlete, or other public figure is wearing in a video in near real time. “TheTake’s mission is simple: making media content shoppable,” said Jared Browarnik, the company’s Co-founder and…Powered by Discourse, best viewed with JavaScript enabled"
223,aws-brings-nvidia-a10g-tensor-core-gpus-to-the-cloud-with-new-ec2-g5-instances,"Originally published at:			https://developer.nvidia.com/blog/aws-brings-nvidia-a10g-tensor-core-gpus-to-the-cloud-with-new-ec2-g5-instances/
Read about the new EC2 G5 instance that powers remote graphics, visual computing, AI/ML training, and inference workloads on AWS cloud.Powered by Discourse, best viewed with JavaScript enabled"
224,generative-ai-research-empowers-creators-with-guided-image-structure-control,"Originally published at:			https://developer.nvidia.com/blog/generative-ai-research-empowers-creators-with-guided-image-structure-control/
New research is boosting the creative potential of generative AI with a text-guided image-editing tool. The innovative study presents a framework using plug-and-play diffusion features (PnP DFs) that guides realistic and precise image generation. With this work, visual content creators can transform images into visuals with just a single prompt image and a few descriptive…Powered by Discourse, best viewed with JavaScript enabled"
225,video-series-shiny-pixels-and-beyond-real-time-ray-tracing-at-seed,"Originally published at:			Video Series: Shiny Pixels and Beyond: Real-Time Ray Tracing at SEED | NVIDIA Technical Blog
Video Series: Shiny Pixels and Beyond –  Real-Time Raytracing at SEED SEED, Electronic Art’s “Search for Extraordinary Experiences Divison”, walks through what they’ve learned about real-time ray tracing when they built the impressive “PICA PICA” demo over the next four videos… Exploring Real-Time Ray Tracing and Self-Learning AI with the “PICA PICA” Demo (8:23 min)…Powered by Discourse, best viewed with JavaScript enabled"
226,ai-robot-learns-how-to-help-people-get-dressed,"Originally published at:			https://developer.nvidia.com/blog/ai-robot-learns-how-to-help-people-get-dressed/
Every day, more than 1 million people in the United States require physical assistance to get dressed, whether because of injury, permanent disability, age, or other debilitating factors. To alleviate the problem, researchers from Georgia Tech built a deep learning-equipped robot that can help people get dressed. “What the robot is trying to do is…Powered by Discourse, best viewed with JavaScript enabled"
227,gtc-2020-lucid-high-resolution-ground-based-observations-of-leo-satellites-with-multi-frame-blind-deconvolution,"GTC 2020 S21270
Presenters: Michael Werth,The Boeing Company; Kevin Roe,Maui High Performance Computing Center
Abstract
High-resolution imaging of objects in space from a ground-based observatory is achievable with a sufficiently large aperture, but atmospheric turbulence causes significant degradation. Computationally expensive algorithms can mitigate the blurring effects of turbulence, and these algorithms have only recently begun to leave the domain of CPU-bound computation. We’ll describe space domain awareness, the imaging-through-turbulence problem, and algorithms that attempt to solve it. We’ll also describe Likelihood-based Uncertainty Constrained Iterative Deconvolution (LUCID), a new multi-frame blind deconvolution implementation that uses CUDA to extract high-resolution images of low-Earth-orbit (LEO) satellites from a series of short-exposure observations.Watch this session
Join in the conversation below.Powered by Discourse, best viewed with JavaScript enabled"
228,nvidia-ai-enterprise-optimized-certified-and-supported-on-vmware-vsphere,"Originally published at:			https://developer.nvidia.com/blog/nvidia-ai-enterprise-optimized-certified-and-supported-on-vmware-vsphere/
NVIDIA AI Enterprise is a suite of AI software, certified to run on VMware vSphere 7 Update 2 with NVIDIA-Certified volume servers.  It includes key enabling technologies and software from NVIDIA for rapid deployment, management and scaling of AI workloads in the virtualized data center running on VMware vSphere.Powered by Discourse, best viewed with JavaScript enabled"
229,building-simulation-ready-usd-3d-assets-in-nvidia-omniverse,"Originally published at:			https://developer.nvidia.com/blog/building-simulation-ready-usd-3d-assets-in-nvidia-omniverse/
Many companies are embracing digital twins to improve their products and services. Digital twins can be used for complex simulations of factories and warehouses or to understand how products will look and behave in the real world. However, many businesses don’t know how to begin making their existing 3D art assets valuable within a simulation…Hey Everyone,I’ve been driving the development of our SimReady specifications internally to help users begin to understand how NVIDIA sees 3D content evolving to provide extensive value within Omniverse and the simulation experiences and tools you’re creating on top of the platform.Having been involved in the 3D industry for a long time now, I want to start a discussion to help enable talented 3D designers so you can provide content that is immediately valuable for simulation, synthetic data generation (SDG) and more. Whether you’re an individual artist, or work for a company producing goods for manufacture, we would love to engage.And as I learn more about various customer needs, I’m finding SimReady is an endless well of opportunity for creators, and this blog talks about the high-level ways we’re starting to consider the standardization of 3D asset creation (whether it comes from CAD/CAM tools or a DCC package like Blender, 3ds Max or Houdini) and how these 3D assets can be effectively leveraged within the Omniverse ecosystem.I’ll be hosting a session and Q&A at GTC in March around practical SimReady asset creation and use within our simulation tools like IsaacSim, but would love to hear from you in the interim.Let me know what you’re up to and what questions you already have around SimReady. We’re just getting started, so there is lots to explore and understand as everyone’s workflows and simulation needs are different.-=BeauI think you are exactly right. A SIMREADY 3D Object WebSite would do very well andwill be the new population objects of the Metaverse.
3D objects with PHYS added and ready to be added to a world that will behave as it would in a real world. You are perfectly placed for this type of venture. Its like yoiu unconsciously choose all the right steps to be here at this time.
Get at it.
Sim Ready 3d Objects with varing levels of complexity.Hi,I am involved in the 3D industry and simulation twins for many years in industrial machinery equipments.One of most challenging task is to convert 3d assets coming from CAD/CAM (pro-e /creo/ solidworks …) to 3D meshes , conserving frame orientations between the conversions
How do you achieve this task ?Defining Collide shapes is a challenging process too.Then, for some reasons, the final product must target a web application , from what I ve seen USD format is not web ready at the moment.  How can it compete with gltf solutions?Regards,
StéphPowered by Discourse, best viewed with JavaScript enabled"
230,new-on-ngc-pytorch-lightning-container-speeds-up-deep-learning-research,"Originally published at:			https://developer.nvidia.com/blog/new-on-ngc-pytorch-lightning-container-speeds-up-deep-learning-research/
With PyTorch Lightning, you can scale your models to multiple GPUs and leverage state-of-the-art training features such as 16-bit precision, early stopping, logging, pruning and quantization, while enabling faster iteration and reproducibility.Powered by Discourse, best viewed with JavaScript enabled"
231,identifying-shader-limiters-with-the-shader-profiler-in-nvidia-nsight-graphics,"Originally published at:			https://developer.nvidia.com/blog/identifying-shader-limiters-with-the-shader-profiler-in-nvidia-nsight-graphics/
This is a deep dive into the Shader Profiler feature of NVIDIA Nsight Graphics. The Shader Profiler allows you to find hotspots in your shaders and why they’re hot.The Shader Profiler is a fantastic addition to NVIDIA Nsight Graphics.  It really is worth spending some time getting to know it.If you have any questions or comments, let us know.Powered by Discourse, best viewed with JavaScript enabled"
232,upcoming-webinar-transforming-transportation-with-the-metaverse-and-ai,"Originally published at:			https://info.nvidia.com/transforming-transportation-with-the-metaverse-and-ai.html
Learn how NVIDIA Omniverse and NVIDIA DRIVE Sim are used to create digital twin environments to train, test, and validate autonomous driving systems.Powered by Discourse, best viewed with JavaScript enabled"
233,cuda-pro-tip-control-gpu-visibility-with-cuda-visible-devices,"Originally published at:			https://developer.nvidia.com/blog/cuda-pro-tip-control-gpu-visibility-cuda_visible_devices/
As a CUDA developer, you will often need to control which devices your application uses. In a short-but-sweet post on the Acceleware blog, Chris Mason writes: Does your CUDA application need to target a specific GPU? If you are writing GPU enabled code, you would typically use a device query to select the desired GPUs.…Hi, does the CUDA_VISIBLE_DEVICE permits to be sure to activate the desired GPU for a specific calculation if OPTIMUS technology is present?Hi, I am wondering what’s the difference between CUDA_VISIBLE_DEVICES and NVIDIA_VISIBLE_DEVICESI’m not 100% sure, but I believe NVIDIA_VISIBLE_DEVICES is used by the NVIDIA Docker Runtime to select GPUs visible inside a container. CUDA_VISIBLE_DEVICES is used by the CUDA driver to decide what devices should be visible to CUDA.Powered by Discourse, best viewed with JavaScript enabled"
234,top-5-deep-learning-sessions-at-gtc,"Originally published at:			Top 5 Deep Learning Sessions at GTC | NVIDIA Technical Blog
NVIDIA’s GPU Technology Conference (GTC) is the premier AI conference, offering hundreds of workshops, sessions, and keynotes hosted by organizations like Google, Amazon, Facebook as well as rising startups. GTC showcases the latest breakthroughs in AI training and inference, industry-changing technologies, and successful implementations from research to production. In the video below, see the the…Powered by Discourse, best viewed with JavaScript enabled"
235,do-ray-tracing-and-deep-learning-have-any-overlap,"Hi everyone,Do ray tracing and deep learning have any overlap?Thanks!Many ways. Deep learning can be used to complement ray tracing, “filling in” missing information with plausible interpolated data, such as with NVIDIA’s DLSS (Deep Learning Super Sampling). This works today.Neural rendering and neural graphics primitives are hot areas of research currently. Last year’s SIGGRAPH course is one place to start. Another good resource is this recent overview of NeRF techniques at CVPR 2022, where ray tracing is used to render radiance fields. NVIDIA has some published work in these areas, see this search.Is there any potential in using ML to adjust screen-space or GI sampling strategies aside from the volumetric NeRF stuff? Perhaps hooking up lower sampling strategies with the DLSS upscaling? IOW, can we “train” a ML hemispherical sampler that somehow uses geometric and shading data (curvature, reflectivity) as inputs and then generates a set of sampling points on the hemisphere that is significantly better than pseudo-random stratified sampling?Probably? More seriously, this whole area of research is hot and there are lots of people working, so this sounds promising.The links I gave didn’t transfer, so here you go:SIGGRAPH 2021 CourseThere are more than 50 papers related to Neural Radiance Fields (NeRFs) at the CVPR 2022 conference. With my former student and now colleague at Google Research, Andrew Marmon, we rounded up all papers we could find and organized them here for our...https://research.nvidia.com/search/node?keys=neural%20rendering
Happy hunting!Powered by Discourse, best viewed with JavaScript enabled"
236,upcoming-event-recommender-systems-sessions-at-gtc-2022,"Originally published at:			AI Developer Conference Session Catalog | GTC 2022 | NVIDIA
Learn about transformer-powered personalized online advertising, cross-framework model evaluation, the NVIDIA Merlin ecosystem, and more with these featured GTC 2022 sessions.Powered by Discourse, best viewed with JavaScript enabled"
237,deep-learning-helps-demystify-authorship-of-a-dead-sea-scroll,"Originally published at:			https://developer.nvidia.com/blog/deep-learning-helps-demystify-authorship-of-a-dead-sea-scroll/
Researchers from the Netherlands’ University of Groningen have used AI to reveal that the Great Isaiah Scroll — the only entirely preserved volume from the original Dead Sea Scrolls — was likely copied by two scribes who wrote in a similar style.Powered by Discourse, best viewed with JavaScript enabled"
238,detecting-objects-from-space-with-artificial-intelligence,"Originally published at:			https://developer.nvidia.com/blog/detecting-objects-from-space-with-artificial-intelligence/
To reveal deeper insights into important activities taking place around the world, DigitalGlobe’s advanced satellite constellation collects nearly 4 million km2 of high-resolution earth imagery each day. The company announced they will now rely on NVIDIA GPUs and deep learning to automatically identify objects such as airplanes, vehicles, and gray elephants, as well as to…Powered by Discourse, best viewed with JavaScript enabled"
239,cudacasts-episode-9-explore-gpu-device-memory-with-nsight-eclipse-edition,"Originally published at:			https://developer.nvidia.com/blog/cudacasts-episode-9-explore-gpu-device-memory-with-nsight-eclipse-edition/
Visual tools offer a very efficient method for developing and debugging applications. When working on massively parallel codes built on the CUDA Platform, this visual approach is even more important because you could be dealing with tens of thousands of parallel threads. With the free NVIDIA Nsight Eclipse Edition IDE, you can quickly and easily examine the…Powered by Discourse, best viewed with JavaScript enabled"
240,rainbow-six-siege-gold,"I was wondering if I could still get the r6 gold edition, I have a 2070 super, but it’s from gigabyte. To get r6 gold would I had to have bought it from you guys?ThanksPowered by Discourse, best viewed with JavaScript enabled"
241,simulating-real-world-floods-on-gpus,"Originally published at:			https://developer.nvidia.com/blog/simulating-real-world-floods-on-gpus/
Flood risk assessment is important in minimizing damages and economic losses caused by flood events. A team of researchers from Vienna University of Technology and visual computing firm VRVis, are using GPUs to run fast simulations of large-scale scenarios, including river flooding, storm-water events and underground flows. The researcher’s primary interest is in decision-making systems, where they evaluate…Powered by Discourse, best viewed with JavaScript enabled"
242,nvidia-research-at-cvpr-2021,"Originally published at:			https://developer.nvidia.com/blog/nvidia-research-at-cvpr-2021/
The 15 accepted papers and posters from NVIDIA range from simulating dynamic driving environments, to powering neural architecture search for medical imaging.Powered by Discourse, best viewed with JavaScript enabled"
243,inception-spotlight-supercharging-synthetic-speech-with-resemble-ai,"Originally published at:			https://developer.nvidia.com/blog/inception-spotlight-supercharging-synthetic-speech-with-resemble-ai/
This NVIDIA Inception Spotlight features Resemble AI, a new generative voice technology startup able to create high-quality synthetic AI voices.Powered by Discourse, best viewed with JavaScript enabled"
244,an-easy-introduction-to-speech-ai,"Originally published at:			https://developer.nvidia.com/blog/an-easy-introduction-to-speech-ai/
A simple introduction to speech AI technology, use cases and benefits for practitioners.Has Speech AI suddenly popped on your radar but you have no idea what it is or how Speech Ai can be used in your projects or company? This is the article for you.If you have any questions or comments, let us know!Powered by Discourse, best viewed with JavaScript enabled"
245,deep-learning-helps-iheartradio-personalize-music-recommendations,"Originally published at:			Deep Learning Helps iHeartRadio Personalize Music Recommendations | NVIDIA Technical Blog
The popular digital radio platform iHeartRadio is using deep learning to help uncover what listeners find important when enjoying music. “Humans perceive music through a wide variety of factors, including rhythm, instrumentation, tempo, and vocals,” wrote Tim Schmeier, a data scientist at iHeartRadio in a recent blog. “Some artists have a distinct “sound” while others…Powered by Discourse, best viewed with JavaScript enabled"
246,cudacasts-episode-2-your-first-cuda-c-program,"Originally published at:			https://developer.nvidia.com/blog/cudacasts-episode-2-your-first-cuda-c-program/
In the last episode of CUDACasts, we learned how to install the CUDA Toolkit on Windows. Now we’re going to quickly move on and accelerate code on the GPU. For this episode, we’ll use the CUDA C programming language.  However, as I will show in future CUDACasts, there are other CUDA enabled languages, including C++, Fortran,…Hi !I tried this example and all the values are zeros!I have a GeForce 9700M GTS with 8.17.12.8026 driver, when i update with the NVIDIA experience it's breaks down or i get blue screens... do you know any driver that i could use?""Cannot find or open the PDB file ""Powered by Discourse, best viewed with JavaScript enabled"
247,nvidia-and-king-s-college-london-debut-first-privacy-preserving-federated-learning-system-for-medical-imaging,"Originally published at:			https://developer.nvidia.com/blog/first-privacy-preserving-federated-learning-system/
NVIDIA researchers in collaboration with King’s College London today announced a breakthrough in healthcare AI, with the introduction of the first privacy-preserving federated learning system for medical image analysis.Powered by Discourse, best viewed with JavaScript enabled"
248,indiana-university-unveils-new-gpu-accelerated-supercomputer-big-red-200,"Originally published at:			https://developer.nvidia.com/blog/indiana-university-unveils-new-gpu-accelerated-supercomputer-big-red-200/
To help accelerate research in medicine, climate modeling, physics, and other academic research, Indiana University has just unveiled a new 6 petaFLOPS capable GPU-accelerated supercomputer named Big Red 200. The new supercomputer is specifically designed for AI applications and will include 256 NVIDIA Tensor Core GPUs.  Big Red 200 is based on HPE’s Cray Shasta…Powered by Discourse, best viewed with JavaScript enabled"
249,learn-more-about-real-time-ray-traced-caustics-in-free-ray-tracing-gems-ii-chapter,"Originally published at:			https://developer.nvidia.com/blog/learn-more-about-real-time-ray-traced-caustics-in-free-ray-tracing-gems-ii-chapter/
We’ve been counting down to the release of Ray Tracing Gems II by providing early releases of select chapters once every week in July. This week’s chapter presents two real-time techniques for rendering caustics effects with ray tracing.Powered by Discourse, best viewed with JavaScript enabled"
250,improving-diffusion-models-as-an-alternative-to-gans-part-2,"Originally published at:			https://developer.nvidia.com/blog/improving-diffusion-models-as-an-alternative-to-gans-part-2/
Researchers at NVIDIA have developed methods to improve and accelerate sampling from diffusion models, a novel and powerful class of generative models.Powered by Discourse, best viewed with JavaScript enabled"
251,accelerating-graph-betweenness-centrality-with-cuda,"Originally published at:			https://developer.nvidia.com/blog/accelerating-graph-betweenness-centrality-cuda/
Graph analysis is a fundamental tool for domains as diverse as social networks, computational biology, and machine learning. Real-world applications of graph algorithms involve tremendously large networks that cannot be inspected manually. Betweenness Centrality (BC) is a popular analytic that determines vertex influence in a graph. It has many practical use cases, including finding the…I left this tab open overnight and my chrome memory usage went up to 1.5GB.  None of my other tabs do this so I think that you have something allocating memory in the background?This would probably be a better conversation to start via our contact form, but...  We do nothing special (we're not knowingly allocating memory), but we do use Wordpress plugins including this Disqus plugin and the twitter feed plugin, which might be different from some sites. I will look into it. However in my experience Chrome can be a memory hog (still my preferred browser).Hello, I have a question regarding paralleling graph operations. Can we find a simple path weight between one node to all other nodes of different type in a heterogeneous graph using similar strategy? Of course I intend to do that using the GPU. Thanks..Powered by Discourse, best viewed with JavaScript enabled"
252,enter-the-nips-deepart-poster-contest-to-win-an-nvidia-dgx-station,"Originally published at:			https://developer.nvidia.com/blog/enter-the-nips-deepart-poster-contest-to-win-an-nvidia-dgx-station/
Based on the popular AI “style transfer” algorithm, DeepArt has created an online competition for people to create neural art to help beautify the walls at the NIPS 2017 conference. Entrants can submit their artwork at DeepArt.io/NIPS by Friday, November 17th and the 50 art pieces with the most votes will be printed and hung…Powered by Discourse, best viewed with JavaScript enabled"
253,cuda-spotlight-gpu-accelerated-nanotechnology,"Originally published at:			CUDA Spotlight: GPU-Accelerated Nanotechnology | Parallel Forall | NVIDIA Technical Blog
Our Spotlight is on Dr. Mark Bathe, Associate Professor of Biological Engineering at the Massachusetts Institute of Technology. Mark’s lab focuses on in silico design and programming of synthetic nucleic acid scaffolds for engineering light-harvesting antennas, multi-enzyme cascades, cellular delivery vehicles, and fluorescent biomolecular probes, which he assays using innovative quantitative imaging techniques. NVIDIA: Mark, tell us about…Powered by Discourse, best viewed with JavaScript enabled"
254,accelerating-a-c-cfd-code-with-openacc,"Originally published at:			https://developer.nvidia.com/blog/accelerating-cfd-code-openacc/
Computational Fluid Dynamics (CFD) is a valuable tool to study the behavior of fluids. Today, many areas of engineering use CFD. For example, the automotive industry uses CFD to study airflow around cars, and to optimize the car body shapes to reduce drag and improve fuel efficiency. To get accurate results in fluid simulation it is…Powered by Discourse, best viewed with JavaScript enabled"
255,top-5-ai-developer-stories-of-the-week,"Originally published at:			Top 5 AI Developer Stories of the Week | NVIDIA Technical Blog
Every week we bring you the top NVIDIA updates and stories for developers. In this week’s edition of our top 5 videos, we highlight a new GPU-accelerated supercomputer at MIT, a new Jetson-based drone, a GAN for fashion, and the brand new RTX Broadcast Engines SDK. Watch below: 5 – New Skydio 2 Drone Powered…Powered by Discourse, best viewed with JavaScript enabled"
256,accelerating-python-applications-with-cunumeric-and-legate,"Originally published at:			https://developer.nvidia.com/blog/accelerating-python-applications-with-cunumeric-and-legate/
cuNumeric is an endeavoring drop-in replacement for NumPy based on the Legion programming system.Whether cumumeric exports native BLAS API calls to be picked up in cython with cimport, the same way as numpy or scipy do (e.g. cython_blas.pxd) ?There are no plans to provide an interface at the level of the BLAS API. The BLAS API operates at the level of raw pointers and strides, and this is too low-level for cuNumeric to target effectively. cuNumeric aims to provide an implementation that can scale to multiple nodes, and thus may need to split the data across multiple address spaces, meaning that a single pointer would not be a good representation for an array. The NumPy level, which uses abstract array objects, is easier to transparently re-implement as a distributed library.We do provide an “entry point” into cuNumeric, where a cuNumeric array can be initialized from an existing memory buffer. Then you can use the NumPy API to operate on that array, and cuNumeric will take care of sharding the data, parallelizing the work etc. The final result at the end of this process can be “inline mapped”, which pulls all data to one place and provides it as a local buffer. This happens automatically if you try to do anything with a cuNumeric array that requires all the data to be in one place, e.g. printing it out.Note that it’s likely not optimal to be switching in and out of cuNumeric repeatedly, as every switch causes blocking and other overheads.Powered by Discourse, best viewed with JavaScript enabled"
257,interactively-visualizing-a-drivetime-radius-from-any-point-in-the-us,"Originally published at:			https://developer.nvidia.com/blog/interactively-visualizing-a-drivetime-radius-from-any-point-in-the-us/
What is geospatial drive time? Geospatial analytics is an important part of real estate decisions for businesses, especially for retailers. There are many factors that go into deciding where to place a new store (demographics, competitors, traffic) and such a decision is often a significant investment. Retailers who understand these factors have an advantage over…Powered by Discourse, best viewed with JavaScript enabled"
258,accelerating-recommendation-system-inference-performance-with-tensorrt,"Originally published at:			Accelerating Recommendation System Inference Performance with TensorRT | NVIDIA Technical Blog
NVIDIA TensorRT is a high-performance deep learning inference optimizer and runtime that delivers low latency and high-throughput for deep learning inference applications. You can import trained models from every deep learning framework into TensorRT, easily create highly efficient inference engines that can be incorporated into larger applications and services. This video demonstrates the steps for…Powered by Discourse, best viewed with JavaScript enabled"
259,grandmaster-series-how-to-perform-large-scale-image-classification,"Originally published at:			https://developer.nvidia.com/blog/grandmaster-series-how-to-perform-large-scale-image-classification/
In episode two of the Grandmaster Series, learn how participating members of the Kaggle Grandmasters of NVIDIA (KGMON) built large-scale image classification models to win the Google Landmark Recognition 2020 Kaggle competition.  In this landmark recognition challenge, the team had to build models that recognize the correct landmark (if any) in a dataset of complicated…Powered by Discourse, best viewed with JavaScript enabled"
260,training-a-state-of-the-art-imagenet-1k-visual-transformer-model-using-nvidia-dgx-superpod,"Originally published at:			https://developer.nvidia.com/blog/training-a-state-of-the-art-imagenet-1k-visual-transformer-model-using-nvidia-dgx-superpod/
This post shows how the SOTA  Visual Transformer model, VOLO, is trained on the NVIDIA DGX SuperPOD. VOLO_D5 model.Because of the dockerhub per image pull limit, we’ve re-uploaded our MNMG training image with a new ID:
terryjx/volo:nvdocker_cuda11.1_devel_cudnn8_ubuntu20.04  on dockerhub.Powered by Discourse, best viewed with JavaScript enabled"
261,production-deep-learning-with-nvidia-gpu-inference-engine,"Originally published at:			https://developer.nvidia.com/blog/production-deep-learning-nvidia-gpu-inference-engine/
Figure 1. NVIDIA GPU Inference Engine (GIE) provides even higher efficiency and performance for neural network inference. Tests performed using GoogLenet. CPU-only: Single-socket Intel Xeon (Haswell) E5-2698 v3@2.3GHz with HT.GPU: NVIDIA Tesla M4 + cuDNN 5 RC.GPU + GIE: NVIDIA Tesla M4 + GIE. [Update September 13, 2016: GPU Inference Engine is now TensorRT] Today…Will the new fused kernels be available in cuDNN?The fusion of layers is dynamic, so the fused kernels are generated by TensorRT at optimization time.Thanks for your rapid reply! nvprof on GIE shows that there are kernels like `maxwell_scudnn_winograd_128x128_mobile_relu_tile148t_nt`; and for normal developers without access to cudnn source code, it would be difficult to implement such kernel fusing. The problem is that we do have our own inference framework that needs to be deployed on all platforms, not only GPU, and completely migrating to TensorRT is not really feasible. So would it be possible that TensorRT provides a lower-level API, like cuDNN, to enable users directly calling the fused kernels? Thanks a lot!Thank your for the great article.I have just tested out the listing 1 code by specifying the path to the caffe model and the path to prototxt file. Unfortunately I got a seg fault in the cuda engine build (last line). How can I debug this ?can someone tell me how to find the document or the use code (example) of TensorRT? it's so wired.Hi Kai. We are working on including the fused kernels in a future release of cuDNN. Thanks!Sign up for the release candidate testing at developer.nvidia.com/tensorrt and when you get the download bundle it will contain the header files, a full set of documentation, and three example programs that you can examine, build and run.We will be releasing fused kernels in a future release of cuDNN.Really glad to hear that! Thanks very much for your effort :)This article mentions that GIE supports networks trained in tensorflow and other frameworks.  However, the release candidate examples focus specifically on Caffe and it only has a parser for Caffe network definition files.  Will there be a parser or some examples for tensorflow in the first release?What operating systems are supported for deployment? Does this only work on Linux? Is Windows 7 or 10 supported as a deployment OS?Linux right now. We are looking at Windows support in a future release.GIE (TensorRT) has a documented API that you can use to describe a network that you trained using any framework. Right now it has a parser which makes it especially easy to import a model from Caffe. We will have an example code as part of the bundle that will show using the API to express a network.Djeb, sorry for the delay in responding. Is this still an issue or have you resolved it?Hi Chris, Yeah, still having this issue.  Ant thoughts ? I have cudnn 5.1 with a GTX 1070.How to benchmark GoogleNet performance for TensorRT? I have Jetpack 2.3 installed on TX1, I can run ImageNet classification but I cannot find out the images/sec performance.Hello! I tried to test yours TensorRT samples with my caffe nets. And I recieved the following messages.1) If my net contains Eltwise-Max layer then error:  ""cudnnElementWiseLayer.cpp:51: virtual void nvinfer1::cudnn::ElementWiseLayer::execute(const nvinfer1::cudnn::CommonContext&): Assertion `mParams.operation == ElementWiseOperation::kSUM' failed.""2) If my net contains TanH layer then error:  ""could not parse layer type TanH  Engine could not be created"".Here (https://devblogs.nvidia.com... is written that ""GIE supports the following layer types.  - Activation: ReLU, tanh and sigmoid  - ElementWise: sum, product or max of two tensors""This is exactly my cases.Are these two layers not supported yet in first release indeed? Is it my bugs?Thank you.Thanks very much for reporting this! There is a bug with eltwise and a gap in the parser for tanh. We have bugs filed for each of these in our tracking system.Powered by Discourse, best viewed with JavaScript enabled"
262,nvidia-smp-assist-api-for-vr-programming,"Originally published at:			NVIDIA SMP Assist API for VR Programming | NVIDIA Technical Blog
The NVIDIA SMP (simultaneous multi-projection) Assist NVAPI driver extension is a simple method for integrating Multi-Res Shading and Lens-Matched Shading into a VR application. It encapsulates a notable amount of state setup and API calls into a simplified API, thereby substantially reducing the complexity of integrating NVIDIA VRWorks into an application. Specifically, the SMP Assist driver handles…I checked the sample in SDK 2.6, but the combination of IS + Lens Matched Shading, IS + multi-res, IS + SMP Assist + multi-res is incompatible.Why the sample assist code does not work with IS mode?Expecting reply as soon as possible (urgent)Hi, Sunil:The writers of the post are looking into this and will get back to you fairly soon.Thanks,Loyd Case, Editor, Developer Tech BlogIs the SM Assist API available for Turing architecture?  It sounds like there is an enhanced version of SMP for RTX. Is that a separate branch of VR Works or something that will be available on both Pascal and Turing???And The SMP Assist API, currently available for only DirectX11 applications? not 12?Yes. Currently available for DX11 applications only.SMP Assist API supports MRS as well as LMS with Instanced Stereo mode. The SDK sample code does not implement all of the combinations.Powered by Discourse, best viewed with JavaScript enabled"
263,achieve-innovative-hyperconverged-networking-with-nvidia-spectrum-ethernet-and-microsoft-azure-stack-hci,"Originally published at:			https://developer.nvidia.com/blog/achieve-innovative-hyperconverged-networking-with-nvidia-spectrum-ethernet-and-microsoft-azure-stack-hci/
NVIDIA Spectrum Ethernet switches have been added to the Microsoft Azure Stack HCI supported device list, enabling innovative hyperconverged networking.Powered by Discourse, best viewed with JavaScript enabled"
264,doubling-network-file-system-performance-with-rdma-enabled-networking,"Originally published at:			https://developer.nvidia.com/blog/doubling-network-file-system-performance-with-rdma-enabled-networking/
This post was originally published on the Mellanox blog. Network File System (NFS) is a ubiquitous component of most modern clusters. It was initially designed as a work-group filesystem, making a central file store available to and shared among several client servers. As NFS became more popular, it was used for mission-critical applications, which required access…Powered by Discourse, best viewed with JavaScript enabled"
265,nvidia-aerial-developer-kit-jumpstart-5g-vran-development,"Originally published at:			https://developer.nvidia.com/blog/aerial-dev-kit-5g-vran/
The nexus of 5G, IoT, and edge computing is turbocharging network performance. NVIDIA is working with the world’s leading telecommunications companies to build software-defined infrastructure that can meet the demand for real-time data processing at the edge for a variety of smart services. Today we announced the first NVIDIA Aerial Developer Kit. Designed to jumpstart…Powered by Discourse, best viewed with JavaScript enabled"
266,king-s-college-london-accelerates-synthetic-brain-3d-image-creation-using-ai-models-powered-by-cambridge-1-supercomputer,"Originally published at:			https://developer.nvidia.com/blog/kings-college-london-accelerates-synthetic-brain-3d-image-creation-using-ai-models-powered-by-cambridge-1-supercomputer/
King’s College London, along with partner hospitals and university collaborators, unveiled new details today about one of the first projects on Cambridge-1, the United Kingdom’s most powerful supercomputer. The Synthetic Brain Project is focused on building deep learning models that can synthesize artificial 3D MRI images of human brains. These models can help scientists understand…Powered by Discourse, best viewed with JavaScript enabled"
267,nvidia-dgx-2-ai-supercomputer-and-new-tesla-t4-set-image-recognition-records-for-training-and-inference,"Originally published at:			NVIDIA DGX-2 AI Supercomputer and New Tesla T4 Set Image Recognition Records for Training and Inference | NVIDIA Technical Blog
Earlier this year, we talked about performance breakthroughs enabled by two new technologies we developed. Related to data augmentation and tensor parameter interleaving, these technologies enabled a single Tesla V100 GPU to do a ResNet-50 training run in just under 24 hours (1,350 images/second). We also discussed how these same technologies enabled a single DGX-1 server…Powered by Discourse, best viewed with JavaScript enabled"
268,microsofts-voice-recognition-technology-almost-as-accurate-as-humans,"Originally published at:			Microsoft’s Voice Recognition Technology Almost as Accurate as Humans | NVIDIA Technical Blog
Microsoft reached a new milestone in the development of more accurate speech recognition. Using a cluster of Tesla M40 GPUs and the cuDNN version of Computational Network Toolkit (CNTK), their latest version of the technology achieved the lowest word error rate (WER) in the industry. “Our best single system achieves an error rate of 6.9%…Powered by Discourse, best viewed with JavaScript enabled"
269,transitioning-to-nsight-systems-from-nvidia-visual-profiler-nvprof,"Originally published at:			Transitioning to Nsight Systems from NVIDIA Visual Profiler / nvprof | NVIDIA Technical Blog
The Nsight suite of profiling tools now supersedes the NVIDIA Visual Profiler (NVVP) and nvprof. Let’s look at what this means for NVIDIA Visual Profiler or nvprof users. Before diving in, let’s first review what is not changing. The Assess, Parallelize, Optimize, Deploy (“APOD”) methodology is the same. When profiling a workload you will continue to…I got the following error message,# __PREFETCH=off nsys profile -o noprefetch --stats=true ./add_cudaunrecognised option '--stats=true'Is the above '--stats=true' option right?What version of Nsight Systems are you using?  The examples in this article require version 2019.3.6 or later.when I try to run nsys, it throws up an error:Error: Nsight Systems 2019.3.7 hasn't been installed with CUDA Toolkit 10.1What can I doPowered by Discourse, best viewed with JavaScript enabled"
270,just-released-nvidia-hpc-sdk-v23-5,"Originally published at:			Release Notes Version 23.5
This update expands platform support and provides minor updates.Powered by Discourse, best viewed with JavaScript enabled"
271,do-more-code-less-with-arrayfire-gpu-matrix-library,"Originally published at:			https://developer.nvidia.com/blog/do-more-code-less-arrayfire-gpu-matrix-library/
This is a guest post by Chris McClanahan from ArrayFire (formerly AccelerEyes). ArrayFire is a fast and easy-to-use GPU matrix library developed by ArrayFire. ArrayFire wraps GPU memory into a simple “array” object, enabling developers to process vectors, matrices, and volumes on the GPU using high-level routines, without having to get involved with device kernel…Powered by Discourse, best viewed with JavaScript enabled"
272,cudacasts-episode-3-your-first-openacc-program,"Originally published at:			https://developer.nvidia.com/blog/cudacasts-episode-3-your-first-openacc-program/
In the last episode of CUDACasts, we wrote our first accelerated program using CUDA C. In this episode, we will explore an alternate method of accelerating code by using OpenACC directives. These directives give hints to the compiler on how to accelerate sections of code, without having to write CUDA code or change the underlying…Powered by Discourse, best viewed with JavaScript enabled"
273,integrating-the-nav2-stack-with-nvidia-isaac-ros-gems,"Originally published at:			https://developer.nvidia.com/blog/integrating-the-nav2-stack-with-nvidia-isaac-ros-gems/
I show how NVIDIA Isaac Sim and NVIDIA Isaac ROS GEMs can be used with the Nav2 navigation stack for robotic navigation.In this work, I reproduce a real-world problem in Isaac Sim and show how to tackle it using Isaac ROS Stereo and Segmentation GEMs. If you have any questions or comments, please let us know!Powered by Discourse, best viewed with JavaScript enabled"
274,just-released-cutensor-v1-5,"Originally published at:			https://developer.nvidia.com/blog/just-released-cutensor-v1-5/
Powered by Discourse, best viewed with JavaScript enabled"
275,top-5-ai-sessions-for-graphics-developers-from-gtc-2021,"Originally published at:			https://developer.nvidia.com/blog/top-5-ai-sessions-for-graphics-developers-from-gtc-2021/
We showcased the NVIDIA-powered AI technologies and features that have made creativity faster and easier for artists and designers worldwide.Powered by Discourse, best viewed with JavaScript enabled"
276,building-image-segmentation-faster-using-jupyter-notebooks-from-ngc,"Originally published at:			https://developer.nvidia.com/blog/building-image-segmentation-faster-using-jupyter-notebooks-from-ngc/
The NVIDIA NGC team is hosting a webinar with live Q&A to dive into this Jupyter notebook available from the NGC catalog. Learn how to use these resources to kickstart your AI journey. Register now: NVIDIA NGC Jupyter Notebook Day: Image Segmentation. Image segmentation is the process of partitioning a digital image into multiple segments…Thank you for providing today’s webinar !When attempting to train with the suggested command./UNet_1GPU.sh /results /data 1the script crashes due to the following error:I downloaded the dataset from two different sources (*) and none includes the CSV file.
Could you elaborate on how to generate the CSV file from the provided dataset ?Thanks !(*) data sourcesJust found that the order of some commands should be different than suggested. Rather than executingone should first download the dataset from Weakly Supervised Learning for Industrial Optical Inspection | Heidelberg Collaboratory for Image Processing (HCI) and then execute:Thank you for your comment. these two lines download the public part of data:
chmod + x ./download_and_preprocess_dagm2007.sh./download_and_preprocess_dagm2007.sh /dataafter that we can download the private part of the dataset manually from provided link by the script and put it inside the container using these two commands:docker cp /home/ :/data/raw_images/ privateunzip /folder/path/.zipthe order is correct.For the demo, I used the link that script provided to make the account. When you have the account you can send a request for private data. they will send you a link to the correct dataset. you will have access to the dataset for limited time to download the correct dataset. I didn’t generate csv file, the link that I received after sending request for private data has everything.2 questions:does it run the training for just one class? How should I understand the guide?In addition, the script fails becuase it cannot find train_list.csv@joepareti54 each script train using one class dataset for training the class id at the end of command shows the class that the script use for training. run the command like this: * ./UNet_1GPU.sh  /results  /data 1  . You don’t need to make any change except the class id , if you want to train the model using another class.yes, but what is the meaning of training a neural network on just one class? What is the best performance option? I suppose you may want tot use the entire training set? and why is the train_list.csv file not there?@joepareti54 the dataset that we used includes 10 different smaller datasets and the images in each smaller dataset have similar background and different defect parts. these are 10 independent datasets, it is not one datasets that divided to 10 parts.
for training you need to download the private part of dataset that you want to use for training.  for example in the demo and here we downloaded private part of dataset class1 and put it in the private folder inside container  in this path: data/raw_images/private .  if you download private part of dataset and put it in the correct folder it has this csv file in it.is this syntax correct:docker cp /home/skouchak/Class1.zip 1877b7cc7625:/data/raw_images/privatemeaning that one must create the directories data raw_images private inside the container?no way to find train_list.csv, where is it supposed to come fromPowered by Discourse, best viewed with JavaScript enabled"
277,just-released-hpc-sdk-v22-7-with-aws-graviton3-c7g-support,"Originally published at:			Release Notes Version 22.7
Enhancements, fixes, and new support for AWS Graviton3 C7g instances, Arm SVE, Rocky Linux OS, OpenMP Tools visibility in Nsight Developer Tools, and more.Powered by Discourse, best viewed with JavaScript enabled"
278,share-your-science-the-making-of-a-gpu-accelerated-weather-forecasting-system,"Originally published at:			https://developer.nvidia.com/blog/share-your-science-the-making-of-a-gpu-accelerated-weather-forecasting-system/
Thomas Schulthess, of ETH Zurich and Director of the Swiss National Supercomputing Centre (CSCS) discusses how they are using GPU-accelerated supercomputers for more detailed weather forecasts.   Watch Thomas’ talk “From “Piz Daint” to “Piz Kesch”: The Making of a GPU-based weather forecasting system” in the NVIDIA GPU Technology Theater at SC15: Watch Now Share…Powered by Discourse, best viewed with JavaScript enabled"
279,nvidia-researchers-present-pixel-adaptive-convolutional-neural-networks-at-cvpr-2019,"Originally published at:			NVIDIA Researchers Present Pixel Adaptive Convolutional Neural Networks at CVPR 2019 | NVIDIA Technical Blog
Despite the widespread use of convolutional neural networks (CNN), the convolution operations used in standard CNNs have some limitations. To overcome these limitations, Researchers from NVIDIA and University of Massachusetts Amherst, developed a new type of convolutional operations that can dynamically adapt to input images to generate filters specific to the content. The researchers will…Powered by Discourse, best viewed with JavaScript enabled"
280,identifying-endangered-whales-with-deep-neural-networks,"Originally published at:			https://developer.nvidia.com/blog/identifying-endangered-whales-with-deep-neural-networks/
With less than 500 North Atlantic right whales left in the world’s oceans, knowing the health and status of each whale is integral to the efforts of researchers working to protect the species from extinction. The current process is quite time-consuming and laborious. It starts with photographing right whales during aerial surveys, selecting and importing the photos into a…Powered by Discourse, best viewed with JavaScript enabled"
281,how-gpus-are-helping-disrupt-the-virus-life-cycle,"Originally published at:			How GPUs Are Helping Disrupt the Virus Life Cycle | NVIDIA Technical Blog
University of Illinois researchers at the Beckman Institute for Advanced Science and Technology’s theoretical and computational biophysics group are accelerating simulations of immature retroviruses using GPU technology on some of the world’s most powerful computers. The researchers are examining ways to prevent their spread by locking the viral particles in this non-infectious stage before they…Powered by Discourse, best viewed with JavaScript enabled"
282,just-released-modulus-v22-07,"Originally published at:			Modulus | NVIDIA NGC
Accelerate your AI-based simulations using NVIDIA Modulus. The 22.07 release brings advancements with weather modeling, novel network architectures, geometry modeling, performance, and more.Powered by Discourse, best viewed with JavaScript enabled"
283,exploring-nvidia-tensorrt-engines-with-trex,"Originally published at:			https://developer.nvidia.com/blog/exploring-tensorrt-engines-with-trex/
This walkthrough summarizes the TREx workflow and highlight API features for examining data and TensorRT engines.Powered by Discourse, best viewed with JavaScript enabled"
284,scientists-gather-at-university-of-delaware-for-openacc-hackathon,"Originally published at:			Scientists Gather at University of Delaware for OpenACC Hackathon | NVIDIA Technical Blog
Oak Ridge National Lab, NVIDIA and PGI launched the OpenACC Hackathon initiative last year to help scientists accelerate applications on GPUs. OpenACC was selected as a primary tool since it offers acceleration without significant programming effort and works great with existing application codes. University of Delaware (UDEL) hosted a five-day Hackathon last week. Selected teams…Powered by Discourse, best viewed with JavaScript enabled"
285,step-by-step-guide-to-building-a-machine-learning-application-with-rapids,"Originally published at:			https://developer.nvidia.com/blog/step-by-step-guide-to-building-a-machine-learning-application-with-rapids/
Here’s how you can quickly get up-and-running using RAPIDS machine learning pipeline with the NVIDIA NGC catalog and Google Vertex AI.Powered by Discourse, best viewed with JavaScript enabled"
286,gtc-2020-cloudxr-streaming-ar-and-vr,"GTC 2020 S22178
Presenters: Greg Jones,NVIDIA
Abstract
Learn how the NVIDIA CloudXR SDK can help you drive immersive extended reality (XR) experiences from anywhere using NVIDIA Quadro GPUs. NVIDIA CloudXR is the groundbreaking technology that delivers wireless virtual and augmented reality from NVIDIA RTX GPUs across performant networks. By dynamically adjusting to network conditions, CloudXR maximizes image quality and frame rates. Scale XR capabilities throughout your enterprise by combining CloudXR with NVIDIA GPU virtualization software to provide seamless experiences comparable to the most robust tethered configurations. Included in this presentation will be examples of partner ISVs extending the OpenVR applications into XR streaming applications.Watch this session
Join in the conversation below.Powered by Discourse, best viewed with JavaScript enabled"
287,mit-develops-an-ai-model-that-can-enrich-digital-maps,"Originally published at:			MIT Develops an AI Model That Can Enrich Digital Maps | NVIDIA Technical Blog
Creating detailed maps is an expensive, time-consuming process.  To help improve GPS navigation in places with limited map data, MIT researchers in collaboration with the Qatar Computing Research Institute developed a deep learning model that can automatically enhance existing maps.  RoadTagger, as the model is called, can use satellite imagery to predict the number of…Powered by Discourse, best viewed with JavaScript enabled"
288,artificial-intelligence-helps-identify-plant-species-for-science,"Originally published at:			Artificial Intelligence Helps Identify Plant Species for Science | NVIDIA Technical Blog
Researchers from the Costa Rica Institute of Technology and French Agricultural Research Centre for International Development developed a deep learning algorithm to automatically identify plant specimens that have been pressed, dried and mounted on herbarium sheets. According to the researchers, this is the first attempt to use deep learning to tackle the difficult taxonomic task…Powered by Discourse, best viewed with JavaScript enabled"
289,using-calibration-to-translate-video-data-to-the-real-world,"Originally published at:			https://developer.nvidia.com/blog/calibration-translate-video-data/
DeepStream SDK 3.0 is about seeing beyond pixels. DeepStream exists to make it easier for you to go from raw video data to metadata that can be analyzed for actionable insights. Calibration is a key step in this process, in which the location of objects present in a video stream is translated into real-world geo-coordinates.…Excellent and in-detail article. Many thanks.Do you happen to have an example of a final application/configuration for this? I want to try it out but I'm unclear what an example of an app with proper configuration. Thanks for the article.Do you know how to use this method now？I also want to know how to use it.Hi,
Is any approach available for moving objects in case of calibration to translate video data to the real world using DeepStream?RegradsHi paravarz,Please help to open a new topic at DeepStream SDK forum, be sure to describe the use case further, we will support you via there.ThanksHi, I did make a new topic, Thank you so muchPowered by Discourse, best viewed with JavaScript enabled"
290,shopping-for-clothes-is-easier-with-new-ai-powered-mobile-app,"Originally published at:			Shopping for Clothes is Easier with New AI-Powered Mobile App | NVIDIA Technical Blog
California-based startup GoFindAI launched their new fashion app that makes shopping for clothes a breeze. Simply snap a photo of something you like, and the app’s AI-powered search engine scours over one million products from 1,000 online retailers looking for the same or similar items. Using CUDA and Tesla K80 GPUs on the Amazon and…Powered by Discourse, best viewed with JavaScript enabled"
291,whole-slide-image-analysis-in-real-time-with-monai-and-rapids,"Originally published at:			https://developer.nvidia.com/blog/whole-slide-image-analysis-in-real-time-with-monai-and-rapids/
Learn how tools such as MONAI and RAPIDS can unlock meaningful insights in the analysis of whole slide image data.Powered by Discourse, best viewed with JavaScript enabled"
292,advancing-the-state-of-the-art-in-automl-now-10x-faster-with-nvidia-gpus-and-rapids,"Originally published at:			https://developer.nvidia.com/blog/advancing-the-state-of-the-art-in-automl-now-10x-faster-with-nvidia-gpus-and-rapids/
To achieve state-of-the-art machine learning (ML) solutions, data scientists often build complex ML models. However,  these techniques are computationally expensive, and until recently required extensive background knowledge, experience, and human effort. Recently, at GTC21, AWS Senior Data Scientist  Nick Erickson gave a session sharing how the combination of AutoGluon, RAPIDS, and NVIDIA GPU computing simplifies…Dear Team i have various questions regarding the AutogluonPowered by Discourse, best viewed with JavaScript enabled"
293,building-an-end-to-end-retail-analytics-application-with-nvidia-deepstream-and-nvidia-tao-toolkit,"Originally published at:			https://developer.nvidia.com/blog/building-an-end-to-end-retail-analytics-application-with-nvidia-deepstream-and-nvidia-tao-toolkit/
Learn how to build a sample application that can perform real-time intelligent video analytics (IVA) in the retail domain using NVIDIA DeepStream SDK and NVIDIA TAO Toolkit.Powered by Discourse, best viewed with JavaScript enabled"
294,how-to-build-a-winning-deep-learning-powered-recommender-system-part-3,"Originally published at:			https://developer.nvidia.com/blog/how-to-build-a-winning-deep-learning-powered-recommender-system-part-3/
Recommender systems (RecSys) have become a key component in many online services, such as e-commerce, social media, news service, or online video streaming. However with the growth in importance, the growth in scale of industry datasets, and more sophisticated models, the bar has been raised for computational resources required for recommendation systems.  To meet the…Powered by Discourse, best viewed with JavaScript enabled"
295,university-material-for-building-an-academic-course-on-accelerated-computing-and-cuda,"Hi
I teach at a university and I would like to create an academic course on Accelerated computing and CUDA.Is there any material, such as, syllabus, power presentations, e-books, etc. that I can use for this?Many thanksPowered by Discourse, best viewed with JavaScript enabled"
296,ai-solves-the-rubik-s-cube-in-a-fraction-of-a-second,"Originally published at:			AI Solves the Rubik’s Cube in a Fraction of a Second | NVIDIA Technical Blog
University of California Irvine researchers recently developed a deep neural network that can solve a Rubik’s cube in a fraction of a second with 100 percent accuracy in all testing configurations.  “Artificial intelligence can defeat the world’s best human chess and Go players, but some of the more difficult puzzles, such as the Rubik’s Cube,…Powered by Discourse, best viewed with JavaScript enabled"
297,ai-helps-save-bees,"Originally published at:			AI Helps Save Bees | NVIDIA Technical Blog
In 2016 through 2017 U.S. beekeepers lost 33 percent of bees. Many factors contribute to the death of bees, but the number one culprit is the Varroa mite. Manually detecting the mites is tedious and prone to human error. Researchers from EPFL, a research institution and university in Lausanne, Switzerland, developed a deep learning-based app that…Powered by Discourse, best viewed with JavaScript enabled"
298,new-video-rendering-games-with-millions-of-ray-traced-lights,"Originally published at:			https://developer.nvidia.com/blog/new-video-rendering-games-with-millions-of-ray-traced-lights/
In this video, NVIDIA’s Chris Wyman provides an overview on reservoir spatiotemporal importance resampling (ReSTIR). This algorithm makes it possible to effectively sample from millions of area lights with just a few rays per pixel and produce results that can be easily denoised. He also explains why now is the right time for game developers…Powered by Discourse, best viewed with JavaScript enabled"
299,gtc-2020-megatron-lm-training-multi-billion-parameter-language-models-using-model-parallelism,"GTC 2020 S21496
Presenters: Mostofa Patwary,NVIDIA; Raul Puri,NVIDIA
Abstract
We’ll cover an efficient model parallel approach by making only a few targeted modifications to existing PyTorch transformer implementations. Training the largest neural language model has recently been the best way to advance the state of the art in NLP applications. However, for models beyond a billion parameters, a single GPU doesn’t have enough memory to fit the model along with the training parameters, requiring model parallelism to split the parameters across multiple GPUs. We’ll showcase our approach by training an 8.3 billion parameter transformer language model with 8-way model parallelism and 64-way data parallelism on 512 GPUs, making it the largest transformer-based language model ever trained. This model establishes new state-of-the-art results in downstream tasks.Watch this session
Join in the conversation below.Powered by Discourse, best viewed with JavaScript enabled"
300,making-containers-easier-with-hpc-container-maker,"Originally published at:			Making Containers Easier with HPC Container Maker | NVIDIA Technical Blog
Today’s groundbreaking scientific discoveries are taking place in high performance computing (HPC) data centers. However, installing and upgrading HPC applications on those shared systems come with a set of unique challenges that decrease accessibility, limit users to old features, and ultimately lower productivity. Containers simplify application deployments in the data centers by wrapping applications into…Powered by Discourse, best viewed with JavaScript enabled"
301,using-federated-learning-to-bridge-data-silos-in-financial-services,"Originally published at:			https://developer.nvidia.com/blog/using-federated-learning-to-bridge-data-silos-in-financial-services/
Discover three ways federated learning can be used in financial services for managing sensitive datasets.Powered by Discourse, best viewed with JavaScript enabled"
302,isc-2020-running-multiple-workloads-on-a-single-a100-gpu,"ISC 2020 disc03
Presenters: DemoTeam, NVIDIA
Abstract
Today, researchers and developers get a dedicated GPU to run their workload, even if the workload only uses a fraction of the GPU’s compute power. The NVIDIA A100 Tensor Core GPU includes a groundbreaking feature called Multi-Instance GPU (MIG), which partitions the GPU into as many as seven instances, each with dedicated compute, memory, and bandwidth. This allows multiple users to run their workloads on the same GPU, maximizing per-GPU utilization and user productivity. This demo runs AI and high-performance computing (HPC) workloads simultaneously on the same A100 GPU.Watch this session
Join in the conversation below.Powered by Discourse, best viewed with JavaScript enabled"
303,oxford-nanopore-selects-nvidia-agx-for-personal-dna-rna-sequencer,"Originally published at:			Oxford Nanopore Selects NVIDIA AGX for Personal DNA/RNA Sequencer | NVIDIA Technical Blog
At GTC Europe in Munich, Germany, U.K. startup Oxford Nanopore Technologies just introduced the MiniIT hand-held AI supercomputer, powered by NVIDIA AGX, which delivers a low-cost, real-time DNA and RNA sequencer. “MinIT is the perfect, powerful companion to MinION, the only portable real-time DNA sequencer,” said Gordon Sanghera, CEO of Oxford Nanopore. “As data streams…Powered by Discourse, best viewed with JavaScript enabled"
304,gtc-2020-unlocking-business-transformation-with-an-ai-center-of-excellence,"GTC 2020 S22548
Presenters: Tony Paikeday,NVIDIA ; Santosh Rao,NetApp
Abstract
Everyone knows the journey to AI is a progression of steps. Whether your organization is just getting started with AI or you’ve been experimenting for a while and are ready to move from pilot to production to prove the value, there comes a time when the business realizes that in order to unlock the full potential of AI, IT will need to own and operate AI infrastructure at scale. This session will uncover the secrets to operationalizing AI across the enterprise, by building a mature service that supports data science teams in the business units to build impactful AI applications faster.Watch this session
Join in the conversation below.Powered by Discourse, best viewed with JavaScript enabled"
305,three-approaches-to-encoding-time-information-as-features-for-ml-models,"Originally published at:			https://developer.nvidia.com/blog/three-approaches-to-encoding-time-information-as-features-for-ml-models/
Learn the easier way to encode time-related Information by using dummy variables, cyclical coding with sine/cosine information, and radial basis functions.Powered by Discourse, best viewed with JavaScript enabled"
306,maximize-performance-of-hpc-apps-with-hpc-sdk-21-11-available-now,"Originally published at:			https://developer.nvidia.com/blog/maximize-performance-of-hpc-apps-with-hpc-sdk-21-11-available-now/
The latest NVIDIA HPC SDK includes a variety of tools to maximize developer productivity, as well as the performance, and portability of HPC applications.Powered by Discourse, best viewed with JavaScript enabled"
307,share-your-science-accelerating-microsoft-cortana-and-skype-translator,"Originally published at:			Share Your Science: Accelerating Microsoft Cortana and Skype Translator | NVIDIA Technical Blog
Alexey Kamenev, Software Engineer at Microsoft Research talks about their open-source Computational Network Toolkit (CNTK) for deep learning, which describes neural networks as a series of computational steps via a directed graph. Kamenev also shares a bit about how they’re using GPUs, the CUDA Toolkit and GPU-accelerated libraries for the variety of Microsoft products that…Powered by Discourse, best viewed with JavaScript enabled"
308,cuda-development-for-jetson-with-nvidia-nsight-eclipse-edition,"Originally published at:			https://developer.nvidia.com/blog/cuda-jetson-nvidia-nsight-eclipse-edition/
Figure 1: NVIDIA Jetson TX2 Developer Kit. NVIDIA Nsight Eclipse Edition is a full-featured, integrated development environment that lets you easily develop CUDA applications for either your local (x86) system or a remote (x86 or ARM) target. In this post, I will walk you through the process of remote-developing CUDA applications for the NVIDIA Jetson…Is it true that if one has fully installed JetPack 3.0 on the host machine connected to the Jetson TX2, then the section ""CUDA Cross-Platform Environment Setup"" can be skipped (i.e., its content took place during the JetPack 3.0 installation)?Hi, Run and Debug setup information in your blog is not enough to setup my TX2 dev env with my TX2 dev kit. Can you provide these steps in details with screen shots pl ease?Yes, cross compile libs and compiler will be installed before JetPack installing ‘Compile CUDA Samples’.Can you provide more details on which part you are having issues? The remote connection dialog should look the same as shown in the screenshot.I'm having an issue to run the example on TX1. I get the following error: ""CUDA driver version is insufficient for CUDA runtime version"".My dev machine has Cuda 8.0.61 while the TX1 has 8.0.33. I checked my GPU and PTX for 5.3 generation and still the same problem. I can compile and the samples normally directly on TX1, but the cross compile isnt working.Can you help me?Edit1: My dev machine has a GTX 1080, which is only supported at 367 driver with cuda 8.0.44. Is there a way to use cuda 8.0.33 with GTX 1080 so I can have the same version of TX1?https://uploads.disquscdn.c...You can have multiple versions of CUDA installed on your host machine. You don't have to install the driver on the host machine when installing CUDA toolkit on host for Cross compiling. You can download Jetpack to install compatible host and target CUDA toolkits for TX1.Hi I am having difficulty getting the boxFilter sample to display on the host machine. I have Cuda Toolkit 8.0 install and went through the entire walk through and everything works with compiling and running the example yet it does not display. I checked to make sure the line setenv(“DISPLAY”, “:0”, 0); is present in the main method in boxFilter.cpp and it is. If you could please help me I would greatly appreciate it.Hi, The boxFilter sample will run and display on the target system(Jetson) if you have followed the instructions to setup the remote connection. You need to check it in the monitor attached to Jetson and not on the host machine. If you are already checking on the target system then please post the output from Nsight EE console view.Hi, I follow your steps and install cuda-8.0.I got an error : 'cudaErrorNvlinkUncorrectable' was not declared in this scope.May I have your advise ?ThanksI fixed it by copy helper_cuda.h to 'include' folder.No idea why it doesn't exist.""Enabled the foreign architecture"" step seems to assume I'm using a linux / Ubuntu dev machine. What's the equivalent of ""sudo dpkg --add-architecture arm64"" for mac? NSight eclipse is, after all, available on Mac. I just can't seem to get it set up for TX2/CUDA development.EDIT: So I went back to the drawing board and started over in an Ubuntu 16.04 Parallels VM. Naturally, after installing the DEB package and rebooting, the entire desktop is now black (can only see the launcher), and I can't go any further. So it doesn't work on Mac, and it bricks Ubuntu 16.04 (VM). Seems like options are pretty limited. Does it work in Windows?I had to change to:setenv (""DISPLAY"", "":1"", 0);Otherwise the following error:/nsight-debug$ ./boxfilter-arm ./boxfilter-arm Starting...Loaded './data/lenaRGB.ppm', 1024 x 1024 pixelsGPU Device 0: ""NVIDIA Tegra X2"" with compute capability 6.2No protocol specifiedfreeglut (./boxfilter-arm): failed to open display ':0'Anybody have an updated version of this? Following the tutorial for CUDA 10.2 on a Jetston Nano + Ubuntu 16 gets me stuck on finding the libgl libraries for aarch64. cuda-cross-aarch64 does not install them and I can’t make Eclipse compile the program.Is it possible to deploy the [NVIDIA Nsight Eclipse Edition] on Jetson Nano?Does the host Ubuntu need to have Cuda GPU?Update: I have verified that if remote target is to be used, host Ubuntu does not need to have Cuda driver.Powered by Discourse, best viewed with JavaScript enabled"
309,behind-the-scenes-of-opus-visual-s-lp-trailer-how-a-small-team-delivered-life-like-visuals-with-ray-tracing,"Originally published at:			Behind the scenes of Opus Visual’s LP Trailer: How a Small Team Delivered Life-like Visuals With Ray Tracing | NVIDIA Technical Blog
Opus Visual, based in Houston, is a team of artists and engineers serving the architectural visualization market. Realistic rendering is critically important in this space; photorealistic simulations are quickly becoming the industry’s new standard. Real-time ray tracing allows Opus Visual to capture the look and feel of a real space without sacrificing interaction.  Rendering, not…Powered by Discourse, best viewed with JavaScript enabled"
310,how-to-watch,"Yes, how to watch?Post your question here and the team will respond real time. There is no video to watch.We will have other events which have a live stream aspect but they will be hosted on our nvidia developer discord.
Sorry for any confusionNo problem, chat can be pretty easy. How does Chat GPT4 understand 3d layouts?No live, just chat. I’m out, can’t type fast enough maybe next time.No worries thanks for your interest.In this paper, Large Language Models as Tool Makers, we see GPT4 being used to generate tools in Python, and GPT3.5 being used as the consumer of those tools along with validation. Do we anticipate a time when GPT4 could make tools inside of Omniverse?Recent research shows the potential of enhancing the problem-solving ability
of large language models (LLMs) through the use of external tools. However,
prior work along this line depends on the availability of existing tools. In
this work, we take...Don’t worry, take your time with any question, we will also try to answer after the event is over.Thanks Mark, I have something I want to ask as part of a bigger question but looks like I’ll need bit more time on  my end.still trying to attend.
went on discord.
went on omniverse
went to livestream.
nothing?
what’s wrong?It is just this chat, here in the forum. Different format so far.ok. which chat?
livestream?
always returns to…
Build Custom AI Tools With ChatGPT and NVIDIA Omniverse : AMA June 28, 9am PDT - Connect With Experts - AMA / ChatGPT and Omniverse: AMA June 28th, 2023 - NVIDIA Developer ForumsLook at the directory in which this post is - and you will see other folks posting questions and getting answers. Sorry for any confusionUse this link to get to the directory if you like : ChatGPT and Omniverse: AMA June 28th, 2023 - NVIDIA Developer ForumsHey @zia_s_ideas, did you want to post this as a question?How does Chat GPT4 understand 3d layouts?We might have missed it in this thread, so please go ahead and re-post it in the main category.The LLM related one below is already “in the works”.It’s an interesting paper. If you’re a tool developer and want to incorporate AI, it’s a good one to look at. I think there will be many LLMs and LLM centric tools that can make tools inside of Omniverse. Already, ChatGPT is pretty good with Python and USD - both foundations of Omniverse extension and scene building. Because of this, I am able to use ChatGPT in my tool building workflow now. That said, it’s going to get much better. It’s easy to imagine LLMs helping with more of my existing work, and I’ve also started thinking that LLMs will become useful for temporary tools that I only need while working on a specific task. For example, maybe I want to make a temporary tool to create a UI that lists all of my lights and a brightness slider for each. I also think that tools that mix procedural algorythms + validation + AI are interesting for tool builders to explore.is it 28th or 29th? When will the event go live??Its live right now - folks are posting  questions  and getting answers . Look at the parent directory of this post.Hi @prateekha3 and welcome to the NVIDIA
developer forums.The event is live right now, here in this forum. Check the main category ChatGPT and Omniverse: AMA June 28th, 2023 - NVIDIA Developer Forums and you will see live questions and answers.Hey Paul! Thanks for the reply.
Yeah, that would be very cool. Using prompts to generate UI’s, or in my case, I’d rather skip UI’s all together and just have prompts drive the input. I think GUI’s have been necessary so average people don’t have to assiduously type in commands but now I think with the constraints of exacting syntax being relaxed, artists could start to bypass the need to learn sequences of GUI steps.
This could be especially useful for things like rendering which can get pretty involved.
Prompt driven rendering could be something like:“Render out this sequence from selected camera at 30 frames per second, 2k resolution, add the bloom filter, set to 6 percent in post and add an edge vignette with 6% inset. Also set the motion blur samples to low. Send a notification when the render is half done and totally done.”This way, we know what we want, but we don’t have to know how to get it.
Initially, we don’t have to dig through settings experimentally, nor dig through documentation or someone’s youtube tutorial. As an idea.could you post this on the main area - so that we can comment and others can join in too.Powered by Discourse, best viewed with JavaScript enabled"
311,running-python-udfs-in-native-nvidia-cuda-kernels-with-the-rapids-cudf,"Originally published at:			Running Python UDFs in Native NVIDIA CUDA Kernels with the RAPIDS cuDF | NVIDIA Technical Blog
In this post, I introduce a design and implementation of a framework within RAPIDS cuDF that enables compiling Python user-defined functions (UDF) and inlining them into native CUDA kernels. This framework uses the Numba Python compiler and Jitify CUDA just-in-time (JIT) compilation library to provide cuDF users the flexibility of Python with the performance of…Powered by Discourse, best viewed with JavaScript enabled"
312,estimating-6d-pose-from-regular-2d-images-with-ai,"Originally published at:			Estimating 6D Pose from Regular 2D Images with AI | NVIDIA Technical Blog
Researchers from NVIDIA, along with collaborators from academia, developed a deep learning-based system that performs 6D object pose estimation from a standard 2D color image with superb accuracy.    In robotics, a robotic arm needs to know the location and orientation to detect and move objects in its vicinity successfully.  This allows the robot to…Powered by Discourse, best viewed with JavaScript enabled"
313,nvidia-and-palo-alto-networks-deliver-unprecedented-firewall-performance-for-5g-cloud-native-security-with-dpu-acceleration,"Originally published at:			https://developer.nvidia.com/blog/nvidia-and-palo-alto-networks-deliver-unprecedented-firewall-performance-for-5g-cloud-native-security-with-dpu-acceleration/
Watch the replay of this joint session at GTC 21 to learn about achieving near-line rate speed of a next-generation firewall through the use of DPUs for a highly efficient 5G native security solution.Powered by Discourse, best viewed with JavaScript enabled"
314,nvidia-announces-new-deep-learning-software-for-developers,"Originally published at:			https://developer.nvidia.com/blog/nvidia-announces-new-deep-learning-software-for-developers/
Just in time for the International Supercomputing show (ISC 2016) and International Conference on Machine Learning (ICML 2016), NVIDIA announced three new deep learning software tools for data scientists and developers to make the most of the vast opportunities in deep learning. NVIDIA DIGITS 4   A new workflow for training object detection neural networks…Powered by Discourse, best viewed with JavaScript enabled"
315,isc-2020-maximizing-performance-for-distributed-machine-and-deep-learning-with-sharp,"ISC 2020 disc06
Presenters: DemoTeam, NVIDIA
Abstract
Today’s modern-day machine learning data centers require complex computations and fast, efficient data delivery. The NVIDIA Mellanox Scalable Hierarchical Aggregation and Reduction Protocol, or SHARP, takes advantage of the in-network computing capabilities in the NVIDIA Mellanox Quantum switch, dramatically improving the performance of distributed machine learning workloads.Watch this session
Join in the conversation below.Powered by Discourse, best viewed with JavaScript enabled"
316,transforming-next-generation-wireless-with-5t-for-5g-and-the-nvidia-aerial-sdk,"Originally published at:			Transforming Next-Generation Wireless with 5T for 5G and the NVIDIA Aerial SDK | NVIDIA Technical Blog
Figure 1. 5G wireless uses an open and cloud-native radio area network (CloudRAN). NVIDIA Mellanox 5T for 5G technology provides a real-time and high-performance solution for building an efficient, time-synchronized CloudRAN infrastructure. Time synchronization and achieving high time accuracy for network traffic between O-RAN 7.2x compliant front-haul, mid-haul, and back-haul components in a cloud-native RAN…Dear Jen,Aerial SDK seems very promising. My research group at ELTE University works on ngRAN and UPF use cases intensively using DPDK and programmable switches (Tofino). For me, one of the most exciting parts of Aerial SDK is the GPU-accelerated DPDK which would be intresting to involve in our research. What is the official way for a university research group to get access to the early access program of Aerial SDK?Many thanks in advance,
Sandor LakiDear Sandor,Thanks for your interest. We will be glad to discuss further your research goals and how we can support you.To kickstart the process, sign up for NVIDIA’s Aerial developer program here: https://developer.nvidia.com/aerial-sdk
We are accepting all research applications at this time.If you are interested in early access to the Aerial developer kit, you can request it here: https://developer.nvidia.com/nvidia-aerial-devkitAll the best and we look forward to working with your team.Dear Ash,I applied for the early access program about 3 weeks ago. How long does it take to get the approval? We see several opportunities for exploiting the benefits of Aerial: 1) We work on an open source P4 compiler for DPDK target. It is called T4P4S and available on GitHub: GitHub - P4ELTE/t4p4s: Retargetable compiler for the P4 language 2) We also work on a hybrid RAN solution where Intel/Barefoot Tofino switches are extended with external computational units to implement the required features that are not supported by the Tofnio ASIC (a short paper will soon be available in the proc. of EUROP4 WS). In our lab, we have Mellanox ConnectX-5 cards and Quadro cards supporting GPUDirect RDMA, so I hope they are enough for carrying out the first tests with GPU-DPDK.Thank you in advance,
SandorHi Sandor,You are approved to get access to Aerial SDK. You should have received an email about Approval. Please follow the instructions on https://developer.nvidia.com/nvidia-aerial-get-started to download the files. Let us know if you have more queries.Thank you,
ChaitraliDear Chaitrali,In the FAQ of the Aerial developer kit link, an answer says that NVIDIA Quadro GV100 and NVIDIA T4 Tensor Core GPUs are supported to run the Aerial SDK. Does Tesla V100 GPU support to run the Aerial SDK?Powered by Discourse, best viewed with JavaScript enabled"
317,gtc-2020-nvidia-video-technologies-video-codec-and-optical-flow-sdk,"GTC 2020 S21337
Presenters: Abhijit Patait,NVIDIA
Abstract
We’ll present details of the recent updates to NVIDIA Video Codec SDK and NVIDIA Optical Flow SDK. Turing and later GPUs bring significant enhancements to video codec and optical flow hardware functionality. We’ll discuss these enhancements, including how to use the features effectively for the best trade-off between performance and quality, as well as important use cases and their results.Watch this session
Join in the conversation below.Powered by Discourse, best viewed with JavaScript enabled"
318,int4-precision-for-ai-inference,"Originally published at:			Int4 Precision for AI Inference | NVIDIA Technical Blog
INT4 Precision Can Bring an Additional 59% Speedup Compared to INT8 If there’s one constant in AI and deep learning, it’s never-ending optimization to wring every possible bit of performance out of a given platform. Many inference applications benefit from reduced precision, whether it’s mixed precision for recurrent neural networks (RNNs) or INT8 for convolutional…Has this been made available in production in some way? It would be great if I could use INT4 with TensorRT.Also doubted that Tesla T4 announced to supporting int4 inference, but I didn’t find any usages or document about int4 in TensorRT doc or anywhere else… Could somebody help?Powered by Discourse, best viewed with JavaScript enabled"
319,share-your-science-scientists-and-researchers-in-their-own-words,"Originally published at:			Share Your Science: Scientists and Researchers in Their Own Words | NVIDIA Technical Blog
We asked scientists and researchers to share how they are advancing their field using accelerated computing and here’s what they told us.   Share your GPU-accelerated science with us: http://nvda.ly/Vpjxr Watch more scientists and researchers share how accelerated computing is #thepathforward: Watch NowPowered by Discourse, best viewed with JavaScript enabled"
320,supercharging-ai-accelerated-cybersecurity-threat-detection,"Originally published at:			https://developer.nvidia.com/blog/supercharging-ai-accelerated-cybersecurity-threat-detection/
NVIDIA Morpheus, now available for download, enables you to use AI to achieve up to 1000x improved performance.We are excited to make Morpheus more broadly available for cybersecurity developers and ISVs next month. If you have any questions or comments, let us know!Powered by Discourse, best viewed with JavaScript enabled"
321,aws-optimizes-tensorflow-on-nvidia-tensor-core-gpus,"Originally published at:			AWS Optimizes TensorFlow on NVIDIA Tensor Core GPUs | NVIDIA Technical Blog
Earlier this week, Amazon announced new AWS Deep Learning AMIs tuned for high-performance training on Amazon EC2 instances powered by NVIDIA Tensor Core GPUs. The Deep Learning AMI on Ubuntu, Amazon Linux, and Amazon Linux 2 now come with an optimized build of TensorFlow 1.13.1 and CUDA 10. GPU instances come with an optimized build…Powered by Discourse, best viewed with JavaScript enabled"
322,monai-v0-2-brings-domain-specialized-best-practices-to-medical-imaging-ai-researchers,"Originally published at:			MONAI v0.2 Brings Domain Specialized Best Practices to Medical Imaging AI Researchers | NVIDIA Technical Blog
Building on the public alpha release announced at GTC 2020 in April, MONAI (Medical Open Network for AI) is an open source and community-supported PyTorch-based framework for healthcare imaging — providing domain-optimized foundational capabilities for developing deep learning training workflows. MONAI v0.2 brings new capabilities, examples, and research implementations for medical imaging researchers to accelerate…Powered by Discourse, best viewed with JavaScript enabled"
323,building-and-deploying-hpc-applications-using-nvidia-hpc-sdk-from-the-nvidia-ngc-catalog,"Originally published at:			https://developer.nvidia.com/blog/building-and-deploying-hpc-applications-using-hpc-sdk-from-ngc-catalog/
HPC development environments are typically complex configurations composed of multiple software packages, each providing unique capabilities. In addition to the core set of compilers used for building software from source code, they often include a number of specialty packages covering a broad range of operations such as communications, data structures, mathematics, I/O control, and analysis…Powered by Discourse, best viewed with JavaScript enabled"
324,accelerating-geoscience-workflows-with-high-performance-virtual-workstations,"Originally published at:			https://developer.nvidia.com/blog/accelerating-geoscience-workflows-with-high-performance-virtual-workstations/
NVIDIA partners with GeoComputing Group and Lenovo on a high-performance, secure, hybrid platform that enhances productivity for geoscientists.Powered by Discourse, best viewed with JavaScript enabled"
325,integrating-with-telephone-networks-to-enable-real-time-ai-services,"Originally published at:			https://developer.nvidia.com/blog/integrating-with-telephone-networks-to-enable-real-time-ai-services/
Many of you may not recognize my company, Ribbon Communications. We are best known for building and securing large telecom networks for communication service providers (also known as phone companies). However, there’s a good chance that in the next day or two, you’ll place a call that traverses a piece of our gear somewhere in…Powered by Discourse, best viewed with JavaScript enabled"
326,inside-nvidia-grace-cpu-nvidia-amps-up-superchip-engineering-for-hpc-and-ai,"Originally published at:			Inside NVIDIA Grace CPU: NVIDIA Amps Up Superchip Engineering for HPC and AI | NVIDIA Technical Blog
Discover the key features and benefits of NVIDIA Grace CPU, the first data center CPU developed by NVIDIA. It has been built from the ground up to create the world’s first superchips.Powered by Discourse, best viewed with JavaScript enabled"
327,nvidia-research-warp-drive-gaming-eliminate-more-than-80-of-the-latency-performance-penalty,"Originally published at:			https://developer.nvidia.com/blog/nvidia-research-warp-drive-gaming-eliminate-more-than-80-of-the-latency-performance-penalty/
Virtual reality (VR) is very sensitive to latency. For an immersive experience, new images must be rendered quickly to reflect the slightest head movements – in a matter of milliseconds. Here, late-warp is a necessary trick, which “re-projects” a rendered image for a more recent view just before displaying it. Can this be useful for…Powered by Discourse, best viewed with JavaScript enabled"
328,artificial-intelligence-system-comprehends-text-like-humans-do,"Originally published at:			Artificial Intelligence System Comprehends Text Like Humans Do | NVIDIA Technical Blog
Maluuba, a Canadian artificial intelligence startup, created a system that can read, comprehend and reason, almost as well as humans. Their deep learning-based program called EpiReader is designed to solve a specific kind of comprehension task: a word is removed from a block of text and then the system is able to determine the missing…Powered by Discourse, best viewed with JavaScript enabled"
329,explainer-what-s-a-cloud-native-supercomputer,"Originally published at:			What's a Cloud-Native Supercomputer? | NVIDIA Blogs
The University of Cambridge, in the UK, and an NVIDIA DGX SuperPOD point way to the next generation of secure, efficient HPC clouds.Powered by Discourse, best viewed with JavaScript enabled"
330,build-and-deploy-powerful-robots-with-the-new-isaac-sdk-version-2019-2,"Originally published at:			Build and Deploy Powerful Robots with the New Isaac SDK Version 2019.2 | NVIDIA Technical Blog
Today, NVIDIA announced the release of ISAAC SDK 2019.2, bringing numerous features and enhancements to the robotics community.  The NVIDIA® Isaac Software Development Kit (SDK) is a comprehensive toolbox for accelerating the development and deployment of AI-powered robots.   With the latest release, we are enabling full capabilities for AI perception and navigation making it easier…Powered by Discourse, best viewed with JavaScript enabled"
331,ai-displays-your-mood-in-emojis,"Originally published at:			https://developer.nvidia.com/blog/ai-displays-your-mood-in-emojis/
What does your emoji say about you? One of the demos at the GPU Technology Conference in San Jose, California this week is a deep learning-based real-time face detection and emotion recognition system. Using an off-the-shelf camera, and dual NVIDIA TITAN V GPUs, the deep learning system developed by NVIDIA engineers captures and analyzes dozens…Powered by Discourse, best viewed with JavaScript enabled"
332,nvidia-nsight-compute-2019-1-is-now-available,"Originally published at:			NVIDIA Nsight Compute 2019.1 is now available | NVIDIA Technical Blog
NVIDIA Nsight Compute 2019.1 is now available for download in the NVIDIA Registered Developer Program.   Version 2019.1 supports the CUDA Toolkit 10.1 and introduces support for child processes, Compute + D3D11/D3D12 interoperability, NVTX, and the latest Turing GPUs. Nsight Compute also has updated CUDA 10.1 task graph support and has many other usability and…Powered by Discourse, best viewed with JavaScript enabled"
333,zero-to-rapids-in-minutes-with-nvidia-gpus-saturn-cloud,"Originally published at:			https://developer.nvidia.com/blog/zero-to-rapids-in-minutes-with-nvidia-gpus-saturn-cloud/
When using RAPIDS, practitioners can quickly accelerate data science workloads on NVIDIA GPUs, and with Saturn Cloud allows practitioners and enterprises to focus on solving their business challenges.Powered by Discourse, best viewed with JavaScript enabled"
334,gtc-2020-streamlining-signal-processing-and-deep-learning-for-radio-and-5g-networks,"GTC 2020 S22519
Presenters: Adam Thompson,NVIDIA; John Ferguson, Deepwave Digital
Abstract
With emerging bandwidth and data-rate hungry technologies like 5G, there’s an essential need to make better use of the wireless spectrum—allowing signal transmission in bands designated for other purposes when available. In this joint talk between NVIDIA and Deepwave Digital, NVIDIA will provide an overview of the RAPIDS cuSignal library (GPU-accelerated SciPy Signal), focused on building high performance signal processing workflows from Python. Deepwave will expand on this infrastructure and highlight their work creating the first deep learning radio frequency sensor for the Citizens Broadband Radio Service (CBRS) 5G network. By leveraging the Artificial Intelligence Radio Transceiver (AIR-T) and its NVIDIA Jetson TX2i, the Deepwave team implemented a deep neural network on the AIR-T that is capable of channelization, detection, classification, and reporting the presence of signal emissions coming from non-cooperative priority users with extreme accuracy.Watch this session
Join in the conversation below.Powered by Discourse, best viewed with JavaScript enabled"
335,cuda-spotlight-cuda-accelerated-adventures,"Originally published at:			CUDA Spotlight: CUDA-Accelerated Adventures | NVIDIA Technical Blog
This week’s Spotlight is on Cyrille Favreau and Christophe Favreau, brothers who leverage GPU computing in different ways, with equally compelling results. Cyrille, a technical architect by day, uses CUDA in his free time to pursue his interest in visualization technologies. His projects include building a real-time ray-tracing engine and molecule visualizer, and exploring fractal theory.…Powered by Discourse, best viewed with JavaScript enabled"
336,scraping-real-estate-sites-for-data-acquisition-with-scrapy,"Originally published at:			https://developer.nvidia.com/blog/scraping-real-estate-sites-for-data-acquisition-with-scrapy/
Data is one of the most valuable assets that a business can possess. It sits at the core of data science and data analysis: without data, they’re both obsolete. Businesses that actively collect data may have a competitive advantage over those that do not. With sufficient data, organizations can better determine the cause of problems…Powered by Discourse, best viewed with JavaScript enabled"
337,graph-characteristics-taxonomy,"Your customers surely ask you for ALL KINDS of input graphs. Can you give a rough taxonomy of the kind of graphs you actually see in practice? For instance, I would say that one category is social-network graphs that are scale-free-ish, where there’s a wide range of input degrees and a wide variance, and then another category might be road-network graphs that are planar, have a small degree and very little variance in degree. How do you characterize the kind of graphs you see?We work with a wide range of customer, which means that we see a wide range of graph types. The vast majority could be classified as power-law graph, both with and without
properties. But we also see a lot of bipartite and N-partite graphs. Graphs that look like road networks, high diameter with low average degree. We are also seeing an increase in the number
of multi-graphs and have had to augment our data structure to support edge ids.I have not taken the time to count the various graph types, but that is a great
thing to do.Oh, cool, that’s very helpful. At a high level can you talk about the use cases you see for bi/N-partite graphs and multi-graphs? Like, how are people looking at them?Do you specialize the data structure for partite graphs?Can you expand on “augment our data structure to support edge ids” for multi-graphs? Are you extending COO/CSR and/or is this in your blocked data structure?Bipartite graphs come up a lot in retail and cyber.  Within retail it is a mapping of customer to products (really n-partite of customers to stores to product types to product
items, etc…).   In cyber it typically is computers outside the firewall and computers inside the firewall.    For Multigraphs, this comes up in retail and finance
where there are lot of transactions (edges) happening every minute/hour/day.
At the C/CUDA layer, we only have a single structure.  AT the Python layer we try and add additional
functions to help users interact with the various types of graphs.Related to augmenting the CSR.  We ran into the problem that if we  ran some
type of sampling or pathfinding that returned an edge, we were unable to determine
which edge was selected.  We expanded from a simple weight value in the CSR to supporting a tuple so that an edge id
could be passed through (also support passing through the edge types)This topic was automatically closed 14 days after the last reply. New replies are no longer allowed.Powered by Discourse, best viewed with JavaScript enabled"
338,new-open-source-gpu-accelerated-atari-emulator-for-reinforcement-learning-now-available,"Originally published at:			New Open Source GPU-Accelerated Atari Emulator for Reinforcement Learning Now Available | NVIDIA Technical Blog
To help accelerate the development and testing of new deep reinforcement learning algorithms, NVIDIA researchers have just published a new research paper and corresponding code that introduces an open source CUDA-based Learning Environment (CuLE) for Atari 2600 games. In the newly published paper, NVIDIA researchers Steven Dalton, Iuri Frosio, and Michael Garland identify computational bottlenecks…Powered by Discourse, best viewed with JavaScript enabled"
339,using-ai-to-monitor-horse-safety,"Originally published at:			Using AI to Monitor Horse Safety | NVIDIA Technical Blog
Magic AI, a Seattle, Washington based startup, developed the world’s first deep learning system that monitors horses and alerts owners of distress, security issues, and horse-specific wellness trends. The startup recently received $1.2 million in funding to expand StableGuard, the company’s 24-hour video monitoring and alert system for horses. Alexa Anthony, CEO of Magic AI,…Powered by Discourse, best viewed with JavaScript enabled"
340,cybersecurity-framework-an-introduction-to-nvidia-morpheus,"Originally published at:			https://developer.nvidia.com/blog/cybersecurity-framework-an-introduction-to-nvidia-morpheus/
NVIDIA recently announced Morpheus, an AI application framework that provides cybersecurity developers with a highly optimized AI pipeline and pre-trained AI capabilities. Morpheus allows developers for the first time to instantaneously inspect all IP network communications through their data center fabric. Attacks are becoming more and more frequent and dangerous despite the advancements in cybersecurity,…How to deploy Morpheus in customer’s data center?Powered by Discourse, best viewed with JavaScript enabled"
341,advanced-api-performance-cpus,"Originally published at:			https://developer.nvidia.com/blog/advanced-api-performance-cpus/
To get the best performance from your NVIDIA GPU, pair it with efficient work delegation on the CPU.Powered by Discourse, best viewed with JavaScript enabled"
342,gtc-2020-gpu-accelerated-scientific-visualization,"GTC 2020 CWE21942
Presenters: ,
Abstract
Visualization is a powerful method to understand and communicate complex data. However, the wealth of tools and technologies in the scientific visualization space can be overwhelming. A lot of NVIDIA visualization technologies are going to be introduced in depth at GTC in various specialized sessions. The goal of this Connect with the Experts session is to help you find the right presentation and discuss with experts in the various visualization technologies how to apply the tools most efficiently.Watch this session
Join in the conversation below.Powered by Discourse, best viewed with JavaScript enabled"
343,webinar-top-11-questions-from-ray-tracing-with-unity-s-high-definition-render-pipeline,"Originally published at:			Webinar: Top 11 Questions from “Ray Tracing with Unity’s High Definition Render Pipeline” | NVIDIA Technical Blog
The recent webinar shares how to use Unity’s High Definition Render Pipeline (HDRP) wizard to enable ray-tracing in your Unity project with just a few clicks. At the end of the webinar, we hosted a Q&A session with our guest speakers from Unity Pierre Yves Donzallaz and Anis Benyoub and below are the top 11…Powered by Discourse, best viewed with JavaScript enabled"
344,gtc-2020-from-cloud-to-edge-building-secure-and-efficient-data-centers-with-nvidia-mellanox-connectx-smartnics,"GTC 2020 S22718
Presenters: Yoni Luzon,NVIDIA; Ariel Kit, NVIDIA; Rony Efraim, NVIDIA
Abstract
The crucial need for organizations to deliver applications securely and efficiently across cloud, on-premises, and edge domains has become an immense challenge in view of the increasing demand for information, and breadth of cyber attacks. This session will cover some of the key challenges in protecting modern data centers, and how NVIDIA Mellanox SmartNICs can enable security everywhere within the data center. We’ll explain how you can take advantage of the software-defined hardware accelerators built into these SmartNICs to connect the cloud and edge with the highest levels of data privacy, integrity and reliability.Watch this session
Join in the conversation below.Powered by Discourse, best viewed with JavaScript enabled"
345,upcoming-workshop-training-amp-tuning-text-to-speech-with-nvidia-nemo-and-w-amp-b,"Originally published at:			Speech AI workshop - NVIDIA x W&B
Learn to train an end-to-end TTS system and track experiments in this live workshop on December 8. Set up the environment, review code blocks, test the model, and more.Powered by Discourse, best viewed with JavaScript enabled"
346,saving-apache-spark-big-data-processing-costs-on-google-cloud-dataproc,"Originally published at:			https://developer.nvidia.com/blog/saving-apache-spark-big-data-processing-costs-on-google-cloud-dataproc/
According to IDC, the volume of data generated each year is growing exponentially.  IDC’s Global DataSphere projects that the world will generate 221 ZB of data by 2026. This data holds fantastic information. But as the volume of data grows, so does the processing cost. As a data scientist or engineer, you’ve certainly felt the…Powered by Discourse, best viewed with JavaScript enabled"
347,cuda-11-1-post-installation-actions,"Well, the instructions for post-installation:export PATH=/usr/local/cuda-11.1/bin${PATH:+:${PATH}}export LD_LIBRARY_PATH=/usr/local/cuda-11.1/lib64${LD_LIBRARY_PATH:+:${LD_LIBRARY_PATH}}After that, Ubuntu 20.04 path not working property and Cuda 11.1 path, not a valid identifier!My question: Why? Why are the instructions not working? Aren’t these instructions wrong?Those are the correct instructions. I have those running in my .bashrc.Hi mnicely,1 - Yes, I wrote in .bashrc!
I copy and paste from the post-installation instructions. But seems to be a problem with the indentation…
bashrc916×131 13.5 KB
2 - Yes, the installation seems to be correct!Could you tell me what’s wrong? Thank you very much for your help!!!Powered by Discourse, best viewed with JavaScript enabled"
348,open-source-time-synchronization-services-for-data-center-operators,"Originally published at:			https://developer.nvidia.com/blog/open-source-time-synchronization-services-for-data-center-operators/
Learn how NVIDIA, Meta, and others collaborated to establish blueprints for a modern time synchronization solution that are open, reliable, and scalable.Powered by Discourse, best viewed with JavaScript enabled"
349,nvidia-kicks-off-siggraph-with-talk-series-on-deep-learning,"Originally published at:			NVIDIA Kicks Off SIGGRAPH with Talk Series on Deep Learning | NVIDIA Technical Blog
SIGGRAPH 2019 gets off to a great start next Sunday (July 28th), as NVIDIA hosts a series of talks about deep learning for content creation and real-time rendering. The three hour series will be packed with all-new insights and information. It’s a great opportunity to connect with and learn from leading engineers in the deep…Powered by Discourse, best viewed with JavaScript enabled"
350,gtc-2020-immersive-reality-improving-the-construction-industry,"GTC 2020 S22168
Presenters: Tom Bossow,Mortenson Construction; Steve Baret, Mortenson
Abstract
The health care industry is continually looking to improve how doctors, nurses, and patients view hospitals and care centers throughout the country. With a primary focus on making hospitals and immediate care centers more welcoming, building teams are embracing new technology and using virtual mock-ups to revolutionize how patients and customers see and interact with spaces. Use of virtual reality mock-ups will bring notable changes in planning, acquiring new equipment, combining old and new standards, and incorporating feedback from doctors into ROI results.Watch this session
Join in the conversation below.Powered by Discourse, best viewed with JavaScript enabled"
351,dive-into-the-future-of-graphics-with-nvidia-omniverse-on-demand-sessions,"Originally published at:			Dive into the Future of Graphics with NVIDIA Omniverse On-Demand Sessions | NVIDIA Technical Blog
NVIDIA Omniverse is bringing the new standard in real-time graphics for developers. Teams across industries are now using the open, cloud-native platform to deliver new levels of virtual collaboration and photorealistic simulation to their projects. And with open beta availability recently announced, more developers around the world can experience Omniverse and explore ways to integrate…Powered by Discourse, best viewed with JavaScript enabled"
352,gtc-2019-named-a-top-conference-for-women-in-ai,"Originally published at:			GTC 2019: Named a Top Conference for Women in AI | NVIDIA Technical Blog
NVIDIA’s GPU Technology Conference (GTC) taking place March 17-21 in San Jose, Calif. is among America’s top conferences for women to attend to further their careers in AI, according to Forbes. The article, which ranks GTC first on the list, mentions technology conferences are a bright spot in boosting diversity and inclusion for women in…Powered by Discourse, best viewed with JavaScript enabled"
353,accelerated-portfolio-construction-with-numba-and-dask-in-python,"Originally published at:			https://developer.nvidia.com/blog/accelerated-portfolio-construction-with-numba-and-dask-in-python/
Learn the power of Numba and Dask in Python for high performance portfolio construction.Powered by Discourse, best viewed with JavaScript enabled"
354,head-injury-study-validates-nasa-safety-testing,"Originally published at:			Head Injury Study Validates NASA Safety Testing | NVIDIA Technical Blog
Designing safety restraints for automobiles is a challenging endeavor, now imagine doing the same for a NASA spacecraft. Unlike a vehicle, impacts can come from any direction. Injuries that are minor after a car crash could prevent an astronaut from exiting a capsule that has just landed on water.  Now a team of Wake Forest…Powered by Discourse, best viewed with JavaScript enabled"
355,new-course-scaling-gpu-accelerated-applications-with-the-c-standard-library,"Originally published at:			Courses – NVIDIA
Learn how to write scalable GPU-accelerated hybrid applications using C++ standard language features alongside MPI in this interactive hands-on self-paced course.Powered by Discourse, best viewed with JavaScript enabled"
356,new-game-title-with-ray-tracing-support,"Is there any upcoming game title with ray tracing support that you’re looking forward to?Warhammer 40K: Darktide. Check out the Summer Game Fest gameplay trailer here: Warhammer 40,000: Darktide - Summer Game Fest Gameplay Trailer - YouTubePowered by Discourse, best viewed with JavaScript enabled"
357,jump-start-training-for-speech-recognition-models-in-different-languages-with-nvidia-nemo,"Originally published at:			Jump-start Training for Speech Recognition Models in Different Languages with NVIDIA NeMo | NVIDIA Technical Blog
Transfer learning is an important machine learning technique that uses a model’s knowledge of one task to make it perform better on another. Fine-tuning is one of the techniques to perform transfer learning. It is an essential part of the recipe for many state-of-the-art results where a base model is first pretrained on a task…Powered by Discourse, best viewed with JavaScript enabled"
358,experience-the-ease-of-ai-model-creation-with-the-tao-toolkit-on-launchpad,"Originally published at:			Experience the Ease of AI Model Creation with the TAO Toolkit on LaunchPad | NVIDIA Technical Blog
The TAO Toolkit lab on LaunchPad has everything you need to experience the end-to-end process of fine-tuning and deploying an object detection application.Powered by Discourse, best viewed with JavaScript enabled"
359,make-a-digital-twin-of-your-data-center-with-sonic-running-on-nvidia-air,"Originally published at:			https://developer.nvidia.com/blog/make-a-digital-twin-of-your-data-center-with-sonic-running-on-nvidia-air/
We have made it incredibly easy to try-out a full multi-switch network fabric using the Microsoft SONiC operating system - in a virtual data center that is available to anyone free of charge.Powered by Discourse, best viewed with JavaScript enabled"
360,ohio-supercomputer-center-installs-pitzer-a-new-gpu-accelerated-cluster,"Originally published at:			Ohio Supercomputer Center Installs Pitzer, a New GPU-Accelerated Cluster | NVIDIA Technical Blog
The Ohio Supercomputer Center has just announced Pitzer, a brand new GPU-accelerated cluster powered by NVIDIA Tesla V100 GPUs. “Ohio continues to make significant investments in the Ohio Supercomputer Center to benefit higher education institutions and industry throughout the state by making additional high-performance computing (HPC) services available,” said John Carey, chancellor of the Ohio Department…Powered by Discourse, best viewed with JavaScript enabled"
361,gtc-digital-graphics-simulation-presentations-demos-and-posters,"Originally published at:			https://developer.nvidia.com/blog/gtc-digital-graphics-simulation/
GTC Digital is all the great training, research, insights, and direct access to the brilliant minds of NVIDIA’s GPU Technology Conference, now online. Join live webinars, training, and Connect with the Experts sessions, or choose from a library of talks, panels, research posters, and demos that you can view on your own schedule, at your own…Powered by Discourse, best viewed with JavaScript enabled"
362,inferencing-images-100x-faster-with-gpus-and-tensorrt,"Originally published at:			Inferencing Images 100x Faster with GPUs and TensorRT | NVIDIA Technical Blog
At this week’s Computer Vision and Pattern Recognition conference, NVIDIA demonstrated how one Tesla V100 running NVIDIA TensorRT can perform a common inferencing task 100X faster than a system without GPUs. In the video below, the CPU-only Intel Skylake-based system (on the left) can classify five flower images per second with a Resnet-152 trained classification…Powered by Discourse, best viewed with JavaScript enabled"
363,can-we-mix-llms-with-other-models-like-stable-diffusion-for-gen-ai,"Can we mix LLM’s with other models like Stable Diffusion for Gen AI?Yes - LLM can be used in various ways as a preprocess pipeline to connect to SD. For example, you might be able to ask LLM to to stylize a prompt based on plain English, then it can be passed to SD. Technically, it’s all about connecting APIs togetherThis topic was automatically closed 14 days after the last reply. New replies are no longer allowed.Powered by Discourse, best viewed with JavaScript enabled"
364,top-ai-for-creative-applications-sessions-at-nvidia-gtc-2023,"Originally published at:			Developer Conference Session Catalog | NVIDIA GTC
Learn how AI is boosting creative applications for creators during NVIDIA GTC 2023, March 20-23.Powered by Discourse, best viewed with JavaScript enabled"
365,goai-gpu-open-analytics-initiative,"Originally published at:			https://developer.nvidia.com/blog/goai-gpu-open-analytics-initiative/
Continuum Analytics, H2O.ai, and MapD recently announced the formation of the GPU Open Analytics Initiative (GOAI) to create common data frameworks enabling developers and statistical researchers to accelerate data science on GPUs. GOAI, also joined by BlazingDB, Graphistry and Gunrock from UC Davis, will foster the development of a data science ecosystem on GPUs by allowing…Powered by Discourse, best viewed with JavaScript enabled"
366,improving-the-image-quality-of-ray-tracing-with-240hz-displays,"Originally published at:			Improving the Image Quality of Ray Tracing with 240Hz Displays | NVIDIA Technical Blog
At SIGGRAPH 2019, NVIDIA’s Josef Spjut discussed best practices for real-time ray tracing when the display target is 240 Hz refresh rate monitors. The full talk, entitled “Ray Tracing at 240Hz”, can be viewed here. “The computer can collaborate with the human visual system to integrate high quality images and video in the brain,” explained…Powered by Discourse, best viewed with JavaScript enabled"
367,accelerating-machine-learning-model-inference-on-google-cloud-dataflow-with-nvidia-gpus,"Originally published at:			https://developer.nvidia.com/blog/accelerating-machine-learning-model-inference-on-google-cloud-dataflow-with-nvidia-gpus/
Today, in partnership with NVIDIA, Google Cloud announced Dataflow is bringing GPUs to the world of big data processing to unlock new possibilities. With Dataflow GPU, users can now leverage the power of NVIDIA GPUs in their machine learning inference workflows. Here we show you how to access these performance benefits with BERT.  Google Cloud’s Dataflow…Powered by Discourse, best viewed with JavaScript enabled"
368,designing-deep-learning-applications-with-nvidia-nsight-deep-learning-designer,"Originally published at:			https://developer.nvidia.com/blog/designing-deep-learning-applications-with-nsight-dl-designer/
Technical overview of the Nsight DL Designer tool to help ease the process of performant model design.Powered by Discourse, best viewed with JavaScript enabled"
369,interns-top-competition-with-jetson-nano-at-booz-allen-summer-games-challenge,"Originally published at:			https://developer.nvidia.com/blog/interns-top-competition-with-jetson-nano-at-booz-allen-summer-games-challenge/
This summer, student interns at Booz Allen Hamilton bested the competition on edge computing with the help of NVIDIA Jetson Nano. The Booz Allen Summer Games Challenge (SGC) calls on student interns across the U.S. to develop breakthrough solutions for its clients’ most pressing problems. This summer, Project RAZOR placed top 10 among artificial intelligence…Powered by Discourse, best viewed with JavaScript enabled"
370,using-hashicorp-nomad-to-schedule-gpu-workloads,"Originally published at:			Using HashiCorp Nomad to Schedule GPU Workloads | NVIDIA Technical Blog
HashiCorp Nomad 0.9 introduces device plugins which support an extensible set of devices for scheduling and deploying workloads. A device plugin allows physical hardware devices to be detected, fingerprinted, and made available to the Nomad job scheduler. The 0.9 release includes a device plugin for NVIDIA GPUs. Some example use cases are: Compute-intensive workloads employing accelerators like…Powered by Discourse, best viewed with JavaScript enabled"
371,try-nvidia-game-development-sdks-in-the-interactive-rtx-technology-showcase,"Originally published at:			Try NVIDIA Game Development SDKs in the Interactive RTX Technology Showcase | NVIDIA Technical Blog
Finding ways to improve performance and visual fidelity in your games and applications is challenging. To help during the game development process, NVIDIA has packaged and released a suite of SDKs through our branch of Unreal Engine for all developers, from independent to AAA, to harness the power of RTX.In the youtube video, I can see fully dynamic shadows casted by light ball(timestamp 1:10) but in the executable demo, light ball do not cast shadows. I’m using RTX 3080 with v466.11 driver. Am I missed something?
Untitled1922×1112 738 KB
@topspoiler – The project files are a slightly different version of the .exe (ahead). This is a WIP detail change. You can enable shadows on the light if you want. Hope this helps…Powered by Discourse, best viewed with JavaScript enabled"
372,research-neural-fields-your-way-with-nvidia-kaolin-wisp,"Originally published at:			https://developer.nvidia.com/blog/research-neural-fields-your-way-with-nvidia-kaolin-wisp/
NVIDIA Kaolin Wisp is a library for researching neural fields within a framework that allows you to adjust and interact with the neural field at every step.Will there be a public video intro to Wisp like the presentation you are giving at Siggraph?Hi @billkatz , a recording of the SIGGRAPH presentation is publicly available:Neural field research is one of the hottest topics in neural graphics today - with the number of papers on fields such as neural radiance fields (NeRF) reaPowered by Discourse, best viewed with JavaScript enabled"
373,optimizing-your-data-center-network,"Originally published at:			https://developer.nvidia.com/blog/optimizing-your-data-center-network/
This post covers how network professionals can update their data center network infrastructure and protocol stack.Powered by Discourse, best viewed with JavaScript enabled"
374,creating-robust-neural-speech-synthesis-with-forwardtacotron,"Originally published at:			Creating Robust Neural Speech Synthesis with ForwardTacotron | NVIDIA Technical Blog
Photo by Thomas Le: Thomas Le (@thomasble) | Unsplash Photo Community The artificial production of human speech, also known as speech synthesis, has always been a fascinating field for researchers, including our AI team at Axel Springer SE. For a long time, people have worked on creating text-to-speech (TTS) systems that reach human level. Following the field’s transition to deep learning…Powered by Discourse, best viewed with JavaScript enabled"
375,how-to-build-an-instant-machine-learning-web-application-with-streamlit-and-fastapi,"Originally published at:			https://developer.nvidia.com/blog/how-to-build-an-instant-machine-learning-web-application-with-streamlit-and-fastapi/
Learn how to rapidly build your own machine learning web application using Streamlit for your frontend and FastAPI for your microservice.Powered by Discourse, best viewed with JavaScript enabled"
376,dli-course-disaster-risk-monitoring-using-satellite-imagery,"Originally published at:			https://developer.nvidia.com/blog/dli-course-disaster-risk-monitoring-using-satellite-imagery/
Powered by Discourse, best viewed with JavaScript enabled"
377,see-how-nvidia-is-helping-transform-healthcare-at-rsna-2017,"Originally published at:			https://developer.nvidia.com/blog/see-how-nvidia-is-helping-transform-healthcare-at-rsna-2017/
By partnering with leading healthcare institutions and organizations like the American College of Radiology, GE Healthcare and Nuance, NVIDIA will help bring to market new solutions that transform healthcare. At the annual meeting of the Radiological Society of North America (RSNA) in Chicago, attended by more than 50,000 professionals, NVIDIA announced partnerships with two leading…Powered by Discourse, best viewed with JavaScript enabled"
378,just-released-nvidia-nsight-compute-2023-1,"Originally published at:			https://nvda.ws/3Zu7U76#new_tab
NVIDIA Nsight Compute 2023.1 adds more metrics and usability to the source view, a sample for shared memory banks, and improves the application replay collection mode.Powered by Discourse, best viewed with JavaScript enabled"
379,building-an-intelligent-robot-dog-with-the-nvidia-isaac-sdk,"Originally published at:			Building an Intelligent Robot Dog with the NVIDIA Isaac SDK | NVIDIA Technical Blog
Figure 1. Some of the mobile robots running Isaac SDK for autonomous navigation. From left to right, NVIDIA Carter, BMW STR, NVIDIA Kaya, and Laikago. The navigation stack supports different bases, brains, and sensors. The modular and easy-to-use navigation stack of the NVIDIA Isaac SDK continues to accelerate the development of various mobile robots. Isaac…Is the TX1 powerful enough to run the software? I have one from a previous project.what chances do we have to control Go Pro 1 [ non EDU] ?Powered by Discourse, best viewed with JavaScript enabled"
380,gtc-2020-scaling-data-by-109x-and-compute-for-deep-learning-applications,"GTC 2020 S21390
Presenters: John Taylor,DST/CSIRO ; Pablo Rozas Larraondo,Australian National University
Abstract
We’ll explore the scalable applications of artificial intelligence on massive data sets. First, we’ll cover how we optimized and developed highly parallelized implementations of DL algorithms and tested them on HPC GPU clusters. Then we’ll demonstrate how to develop models that can run over large high-resolution datasets, identifying the spatial and temporal relationships between physical parameters in global-scale high-resolution numerical weather prediction models.Watch this session
Join in the conversation below.Video not workingPowered by Discourse, best viewed with JavaScript enabled"
381,top-path-tracing-sessions-at-nvidia-gtc-2023,"Originally published at:			Game Development Session Catalog | NVIDIA GTC
Learn about the latest path tracing technologies and how they’re accelerating game development.Powered by Discourse, best viewed with JavaScript enabled"
382,university-of-reims-champagne-ardenne-gets-a-new-gpu-accelerated-supercomputer,"Originally published at:			University of Reims Champagne-Ardenne Gets a New GPU-Accelerated Supercomputer | NVIDIA Technical Blog
Earlier this month the University of Reims Champagne-Ardenne in France announced the availability of a new GPU-accelerated supercomputer ROMEO. The supercomputer is ranked amongst the most-powerful supercomputers worldwide, with a  peak performance of 1 petaflop. “Equipped with some of the most powerful processors on the market and the latest NVIDIA Tesla GPUs, this supercomputer provides…Powered by Discourse, best viewed with JavaScript enabled"
383,baidu-collaborates-with-peel-for-voice-controlled-smart-home-products,"Originally published at:			https://developer.nvidia.com/blog/baidu-collaborates-with-peel-for-voice-controlled-smart-home-products/
With over 150 million users and 10 billion monthly remote commands, Peel will begin using Baidu’s Deep Speech technology in their next-generation voice-enabled products. Deep Speech is a state-of-the-art GPU-trained speech recognition system developed using “end-to-end deep learning” by Baidu Research’s Silicon Valley AI Lab (SVAIL). Baidu’s Adam Coates, who leads the SVAIL team, said:…Powered by Discourse, best viewed with JavaScript enabled"
384,accelerating-intelligent-video-analytics-using-ultra-efficient-5g-core-with-mavenir-and-nvidia-edge-ai,"Originally published at:			https://developer.nvidia.com/blog/accelerating-iva-using-ultra-efficient-5g-core-with-mavenir-and-nvidia-edge-ai/
For the past decade and a half, an increasing number of businesses have moved their traditional IT applications from on-premises to public clouds. This first phase of the revolution, which you could call “enterprise cloud transformation,” has allowed enterprises to reap the benefits of the scale, expertise, and flexibility of the cloud. The second phase…Powered by Discourse, best viewed with JavaScript enabled"
385,nvidia-tools-extension-api-nvtx-annotation-tool-for-profiling-code-in-python-and-c-c,"Originally published at:			https://developer.nvidia.com/blog/nvidia-tools-extension-api-nvtx-annotation-tool-for-profiling-code-in-python-and-c-c/
As PyData leverages much of the static language world for speed including CUDA, we need tools which not only profile and measure across languages but also devices, CPU, and GPU.  While there are many great profiling tools within the Python ecosystem: line-profilers like cProfile and profilers which can observe code execution in C-extensions like PySpy/Viztracer. …Thanks Ben Zaitlen - this is an excellent tutorial and resource!Powered by Discourse, best viewed with JavaScript enabled"
386,top-manufacturing-sessions-at-gtc-2022,"Originally published at:			AI Developer Conference Session Catalog | GTC 2022 | NVIDIA
Discover the latest innovations in manufacturing and aerospace with GTC sessions from leaders at Siemens, Boeing, BMW, and more.Powered by Discourse, best viewed with JavaScript enabled"
387,new-nvidia-deepstream-sdk-3-0-removes-boundaries-of-video-analytics,"Originally published at:			New NVIDIA DeepStream SDK 3.0 Removes Boundaries of Video Analytics | NVIDIA Technical Blog
NVIDIA today announced the software release of DeepStream SDK 3.0 for Tesla GPUs. Developers can now focus on building core deep learning networks rather than designing end-to-end applications from scratch given its modular framework and hardware-accelerated building blocks. The SDK’s latest features make it easy for you to create scalable solutions for the most complex…Powered by Discourse, best viewed with JavaScript enabled"
388,nvidia-research-featured-at-european-conference-on-computer-vision-eccv-2020,"Originally published at:			NVIDIA Research Featured at European Conference on Computer Vision (ECCV) 2020 | NVIDIA Technical Blog
Researchers, developers, and engineers from all over the world are gathering virtually this year for the European Conference on Computer Vision (ECCV) 2020.  Among the papers being presented by NVIDIA researchers at ECCV this year, COCO-FUNIT: Few-Shot Unsupervised Image Translation with a Content Conditioned Style Encoder offers significant visual improvements to the popular GANimal demo…Powered by Discourse, best viewed with JavaScript enabled"
389,webinar-machine-learning-and-computer-vision-are-redefining-visual-search,"Originally published at:			https://developer.nvidia.com/blog/upcoming-webinar-machine-learning-and-computer-vision-are-redefining-visual-search/
Every second, millions of videos are generated and consumed every second. From the projected 859 petabytes of footage from surveillance cameras to the over two billion images and videos uploaded daily to social platforms, visual content is exploding. However, huge gaps exist between simply storing lots of data and the intelligent, insightful, and actionable understanding…Powered by Discourse, best viewed with JavaScript enabled"
390,nvidia-grace-hopper-superchip-architecture-in-depth,"Originally published at:			https://developer.nvidia.com/blog/nvidia-grace-hopper-superchip-architecture-in-depth/
The NVIDIA Grace Hopper Superchip Architecture is the first true heterogeneous accelerated platform for high-performance computing (HPC) and AI workloads. It accelerates applications with the strengths of both GPUs and CPUs while providing the simplest and most productive distributed heterogeneous programming model to date. Scientists and engineers can focus on solving the world’s most important…Hello! I would be very interested in trying out the chip for AI inference over encrypted data (see Sentiment Analysis on Encrypted Data with FHE - a Hugging Face Space by zama-fhe for example). Is the chip available via a cloud solution already?Powered by Discourse, best viewed with JavaScript enabled"
391,three-things-you-need-to-know-about-waveworks-2-0,"Originally published at:			Three Things You Need to Know About WaveWorks 2.0 | NVIDIA Technical Blog
In this video, Timothy Cheblokov, Senior Software Engineer at NVIDIA, details the three most important things developers need to know about WaveWorks 2.0. Watch below: 3: WaveWorks 2.0 Supports all Major Graphics APIs WaveWorks supports all the major graphics APIs now, and uses DirectX or Vulkan Compute shaders to run simulations and postprocess the results…Powered by Discourse, best viewed with JavaScript enabled"
392,scaling-scientific-computing-with-nvshmem,"Originally published at:			https://developer.nvidia.com/blog/scaling-scientific-computing-with-nvshmem/
Figure 1. In the NVSHMEM memory model, each process (PE) has private memory, as well as symmetric memory that forms a partition of the partitioned global address space. When you double the number of processors used to solve a given problem, you expect the solution time to be cut in half. However, most programmers know…This was a great, collaborative (and fun!) effort that has helped to drive the NVSHMEM programming model. Please let us know if you have any questions.Powered by Discourse, best viewed with JavaScript enabled"
393,just-released-nvidia-modulus-v23-05,"Originally published at:			https://developer.nvidia.com/blog/just-released-nvidia-modulus-v23-04/
This version 23.05 update to the NVIDIA Modulus platform expands support for physics-ML and provides minor updates.Powered by Discourse, best viewed with JavaScript enabled"
394,rapids-cugraph-multi-gpu-pagerank,"Originally published at:			RAPIDS cuGraph: multi-GPU PageRank | NVIDIA Technical Blog
RAPIDS cuGraph is on a mission to provide multi-GPU graph analytics to allow users to scale to billion and even trillion scale graphs. The first step along that path is the new release of a single-node multi-GPU version of PageRank. Experimental results show that an end-to-end pipeline involving the new multi-GPU PageRank is on average 80x faster…Powered by Discourse, best viewed with JavaScript enabled"
395,uc-berkeley-s-badgr-robot-learns-to-navigate-on-its-own,"Originally published at:			UC Berkeley’s BADGR Robot Learns to Navigate on Its Own | NVIDIA Technical Blog
Many robots rely entirely on geometry to plan a collision-free path. However, geometric reasoning can cause autonomous machines to think that they can’t traverse through tall grass or bumpy terrain to reach their goal.  To help solve this problem, UC Berkeley researchers Greg Kahn, Pieter Abbeel, and Sergey Levine developed the Berkeley Autonomous Driving Ground…Powered by Discourse, best viewed with JavaScript enabled"
396,the-intersection-of-large-scale-graph-analytics-and-deep-learning,"Originally published at:			https://developer.nvidia.com/blog/intersection-large-scale-graph-analytics-deep-learning/
Figure 1: An example graph in which entities are represented by nodes and relationships are represented by edges. Suppose you want to find the most influential user of Twitter. You would need to know not only how many followers everyone has, but also who those followers are, who the followers of those followers are, and…Great article. Very good explanation of the partitioning of graphs. FUNL sounds very interesting and will help to implement many graph algorithms on the GPU much faster. Will there be an open source license of FUNL for academia or non-commercial use?We are actively looking at releasing a license for academia and want to do it in partnership with the academic organization that might find it useful.  Contact us if you would like to discuss your project.Thank you for very interested topic. Did you release the open source for FUNL and DeepInsight..؟im interested to get a license for an academia like me. I want to study more about graph and supply chain networks.Powered by Discourse, best viewed with JavaScript enabled"
397,offloading-and-isolating-data-center-workloads-with-nvidia-bluefield-dpu,"Originally published at:			https://developer.nvidia.com/blog/offloading-and-isolating-data-center-workloads-with-bluefield-dpu/
The Data Processing Unit, or DPU, has recently become popular in data center circles. But not everyone agrees on what tasks a DPU should perform or how it should do them. Idan Burstein, DPU Architect at NVIDIA, presents the applications and use cases that drive the architecture of the NVIDIA BlueField DPU.NVIDIA’s architects and engineers have put a lot of thought into the BlueField DPU architecture, including what to accelerate, how to accelerate it, and which applications the DPU should offload.  Idan’s talk at Hot Chips 33 will cover the design philosophy behind BlueField and the architecture of NVIDIA’s BlueField-2 and BlueField-3.  If you have any questions or comments about this talk or about the NVIDIA DPU architecture, please post them here.Powered by Discourse, best viewed with JavaScript enabled"
398,gpt-4-and-omniverse,"It was not completely clear on this, can I actually use GPT-4 in Omniverse?Yes, you can connect the OpenAI API and use ChatGPT and GPT-4 APIs in Omniverse. You can use both the Python API or write a HTTPS call to their web API. There is a project that you can check out on GitHub that demonstrates using GPT-4 and Omniverse, you can find it here: GitHub - NVIDIA-Omniverse/kit-extension-sample-airoomgenerator: A tool used to create 3D content for rooms by calling OpenAI's APIThis topic was automatically closed 14 days after the last reply. New replies are no longer allowed.Powered by Discourse, best viewed with JavaScript enabled"
399,high-performance-storage-on-nvidia-dgx-cloud-with-oracle-cloud-infrastructure,"Originally published at:			https://developer.nvidia.com/blog/high-performance-storage-on-nvidia-dgx-cloud-with-oracle-cloud-infrastructure/
Learn how NVIDIA partnered with Oracle Compute Infrastructure to build high-performance storage for NVIDIA DGX Cloud with NVMesh software.Powered by Discourse, best viewed with JavaScript enabled"
400,implementing-real-time-multi-camera-pipelines-with-nvidia-jetson,"Originally published at:			https://developer.nvidia.com/blog/implementing-real-time-multi-camera-pipelines-with-nvidia-jetson/
Here’s how to build efficient multi-camera pipelines taking advantage of the hardware accelerators on NVIDIA Jetson platform in just a few lines of Python.I am trying to get this example to work on the Jetson AGX Orin Dev kit running jetpack 5.0 EA. Now I know its not really made to run with this setup but if its not to far of a reach to get it to work it would be nice. I already modified “setup.py” to not load deepstream 5.1. I already have a version running that works on the jetpack 5.0 EAThat being said here is where it hangs up.oev@ubuntu:~/jetson-multicamera-pipelines/examples$ python3 example-no-ai.py
Traceback (most recent call last):
File “example-no-ai.py”, line 7, in 
p = CameraPipeline([0, 2, 4])
File “/home/joev/.local/lib/python3.8/site-packages/jetmulticam/pipelines/multicam.py”, line 63, in init
super().init(**kwargs)
File “/home/joev/.local/lib/python3.8/site-packages/jetmulticam/pipelines/basepipeline.py”, line 25, in init
self._p = self._create_pipeline(**kwargs)
File “/home/joev/.local/lib/python3.8/site-packages/jetmulticam/pipelines/multicam.py”, line 68, in _create_pipeline
cameras = [
File “/home/joev/.local/lib/python3.8/site-packages/jetmulticam/pipelines/multicam.py”, line 69, in 
make_argus_camera_configured(c, bufapi_version=0) for c in self._cams
File “/home/joev/.local/lib/python3.8/site-packages/jetmulticam/bins/cameras.py”, line 18, in make_argus_camera_configured
cam.set_property(“bufapi-version”, bufapi_version)
TypeError: object of type GstNvArgusCameraSrc' does not have property bufapi-version’
Exception ignored in: <function BasePipeline.del at 0xffffb721c160>
Traceback (most recent call last):
File “/home/joev/.local/lib/python3.8/site-packages/jetmulticam/pipelines/basepipeline.py”, line 38, in del
self.stop()
File “/home/joev/.local/lib/python3.8/site-packages/jetmulticam/pipelines/basepipeline.py”, line 41, in stop
self._p.send_event(Gst.Event.new_eos())
AttributeError: ‘CameraPipeline’ object has no attribute ‘_p’object has no attribute ‘_p’Can you run your *.py file with the -tt option (ex. python3 -tt test.py)?My guess is there is a mix of spaces/tabs in the file when you modified it that is throwing off the interpreter.HI i am also getting the same error. Can anyone guide please @adventuredaisy did you solve the issue.here is the error that i get:Any help please ?@sharoseali23 @jwitsoe I am facing the same issue when trying on all example elements. Anyone found a solution?@philippe-mo9 Unfortunately No, I again tried yesterday with deepstream 6.1.0 but this error is still there. I think the authors @jwitsoe  should reply and guide us to proceed.Powered by Discourse, best viewed with JavaScript enabled"
401,real-time-image-moderation-api,"Originally published at:			https://developer.nvidia.com/blog/real-time-image-moderation-api/
Artificial intelligence startup Sightengine developed image analysis technology to detect if images and videos contain offensive material, such as nudity, adult content or suggestive scenes. Using CUDA, Tesla K80s and cuDNN with the TensorFlow deep learning framework, the startup trained their deep learning models to determine whether or not an image is safe content and…Powered by Discourse, best viewed with JavaScript enabled"
402,artificial-intelligence-helping-to-ensure-humanity-s-future-food-supply,"Originally published at:			https://developer.nvidia.com/blog/artificial-intelligence-helping-to-ensure-humanitys-future-food-supply/
The Earth isn’t getting any bigger, so we need to start finding more efficient ways to feed the projected 10 billion people by 2050 using the same amount of land. WIRED recently published an article highlighting several examples of how artificial intelligence technology can be used to tackle this challenge: Crop Disease Diagnosis Researchers from…Powered by Discourse, best viewed with JavaScript enabled"
403,improve-perception-performance-for-ros-2-applications-with-nvidia-isaac-transport-for-ros,"Originally published at:			https://developer.nvidia.com/blog/improve-perception-performance-for-ros-2-applications-with-nvidia-isaac-transport-for-ros/
Announcing NVIDIA Isaac Transport for ROS (NITROS) pipelines that use new ROS Humble features developed jointly with Open Robotics.These sound great. I just have a question, how can us developers take advantage of these taking into account that jetpack currently supports only Ubuntu 18.04 → ROS melodic, or is there an official way to install ROS 2 humble (Ubuntu 22.04) on any Jetson hardware?
ThanksGood question! You’re right that Foxy is only supported on Ubuntu 20.04 Focal and Humble in Ubuntu 22.04 Jammy, but Jetpack 4.6.1 is Ubuntu 18.04 Bionic and Jetpack 5 is 20.04 Focal. In order to help with this, we build ROS2 distributions from source into Docker images and provide utilities to use them for development in Isaac ROS Common](GitHub - NVIDIA-ISAAC-ROS/isaac_ros_common: Common utilities, packages, scripts, Dockerfiles, and testing infrastructure for Isaac ROS packages.). Currently, this supports Jetpack 4.6.1 with Foxy and will soon support Jetpack 5 with Humble in our upcoming release in June 2022.Powered by Discourse, best viewed with JavaScript enabled"
404,implementing-industrial-inference-pipelines-for-smart-manufacturing,"Originally published at:			https://developer.nvidia.com/blog/implementing-industrial-inference-pipelines-for-smart-manufacturing/
Using an NVIDIA Triton Inference Server, industrial manufacturer Sansera improved quality control and documentation through a custom AI pipeline.Powered by Discourse, best viewed with JavaScript enabled"
405,sharpen-your-edge-ai-and-robotics-skills-with-the-nvidia-jetson-nano-developer-kit,"Originally published at:			https://developer.nvidia.com/blog/sharpen-your-edge-ai-and-robotics-skills-with-the-nvidia-jetson-nano-developer-kit/
Are you interested in getting started with edge AI and robotics but not sure where to begin?  Look at the relaunched NVIDIA Jetson Nano Developer Kit available for purchase from partners starting November 25, 2022 in the US and worldwide in December. Introduced 3 years ago, the NVIDIA Jetson Nano is a low-cost, entry-level AI…is it the same as the jetson nano released a few years ago on the hardware? I thought it’s EOL.it’s unclear about the software support though, are we stuck with ubuntu 18.04 for good with jetson nano?Does this post mean that we can expect JetPack updates to continue to support the Jetson Nano?  You really should do that, it is an excellent platform for makers and not doing so will make obsolete the equipment we purchased.To partially answer my own question:
Jetson Nano 2GB is EOL.
Jetson Nano 4GB is revived and we can continue to use.
But the software will be stuck with Jetpack4 and ubuntu 18.04(which will be EOL in 4 months).Jetson Nano Developer Kit V3 has two 15-pin 2-lane MIPI CSI-2 camera connectors vs. one on old Jetson Nano DK.Wutt??I thought this was EOL??  I have a few of these and was dissapointed you EOL them after only a year after I bought it…Does this mean latest JetPack will be available for Nano 4GB??I probably spent too much money on a Jetson Xavier due to the EOL on the Nano.  If you’re now going to keep this great little module alive, then please get Jetpack up to 5.0.Please, please, please support the Nano! Nano & its bus(es) are for AI what RPi is for general computing. Thank you for leading the way, but it is very DEC-ish (Digital Equipment Company) of you to redefine the Nano bus(es), Nano eXtension bus(es). Now Orin Nano (thank you) & what should be called Orin eXtension (OX)? All of these buses make my hardware obsolete!?! The Orin NX is a physical standard and not a bus standard? Really? You are telling me that my credit card is NX compatible!! It is almost as NX compatible after all. I must acknowledge bus fluttering, though it is exclusionary and appears reactionary, has specific purpose. Please stabilize eXtensions for Entry Levels and us Life Long Learners, including buses and software updates (Cuda, JetPack, Ubuntu). SoMs have no value without a carrier board, a big effort for a one off. My frustration stems from planning to populate my Turing Pi 2s, only to find more power planned away from me and years away at that (TPi3 . . ?).We do owe thanks to DEC for motivating Thomson, Ritchie & Kernighan. In turn Linus was motivated. We do not know where the next Linus, Jenson . . . will come from. Notably, NVIDIA is not on Luxonis RAE, special sarcasm in their video as RAE runs circles around a CORAL box.0:48:12 to 1:01:40?@jwitsoe - any comment on your Jetson Nano “Re-Launch” announcement and questions on Nano support for latest Jetpack?Or was NVidia just looking to offload old stock and does not intend to support it?Hi everyone, Nano will continue to be supported with updates to JetPack 4.6 - please refer to Jetson Software Roadmap for 2H-2021 and 2022 for more info.   The latest update was released earlier this month (JetPack 4.6.3/L4T 32.7.3 released).  JetPack 5 is for Xavier and Orin only and there aren’t plans to backport it to Nano/TX1/TX2.Nano 4GB wasn’t EOL and thanks for your patience while it was out of stock due to supply-chain issues (for EOL announcements see the Jetson Product Lifecycle page).  Please direct further questions to the Nano forum where they will be more visible to the Jetson team so that we can reply in a more timely manner.Powered by Discourse, best viewed with JavaScript enabled"
406,cuda-toolkit-12-0-released-for-general-availability,"Originally published at:			https://developer.nvidia.com/blog/cuda-toolkit-12-0-released-for-general-availability/
NVIDIA announces the newest CUDA Toolkit software release, 12.0. This release is the first major release in many years and it focuses on new programming models and CUDA application acceleration through new hardware capabilities. You can now target architecture-specific features and instructions in the NVIDIA Hopper and NVIDIA Ada Lovelace architectures with CUDA custom code,…Powered by Discourse, best viewed with JavaScript enabled"
407,nvidia-gameworks-wins-grand-prize-at-cedec-awards-2016,"Originally published at:			NVIDIA GameWorks Wins Grand Prize at CEDEC AWARDS 2016 | NVIDIA Technical Blog
NVIDIA GameWorks was awarded the CEDEC AWARDS 2016 Grand Prize, hosted this year at the Computer Entertainment Developers Conference in Yokohama, Japan. “We’re working hard every day to make interactive simulations that are used for drawings beautiful during game development, and also to improve their quality,” said NVIDIA’s Senior Manager of Asia-Pacific Developer Technology Bryan…Powered by Discourse, best viewed with JavaScript enabled"
408,faster-parallel-reductions-on-kepler,"Nope.  It is undefined.  Undefined means that we reserve the right to change the behavior at any time (for example, new driver, new hardware, new toolkit, etc).It would be amazing if the whole code was global, I mean that can work in Fermi and Kepler architecture.Hi, this is a great post! Thanks for sharing it.Is it possible to have a kernel, where I fill the vector and after that  I can callyour reduce function, instead of copying the vector to Host and Copy it back to the reduce function? It would save time by doing that.Thanks.sorry but i can't understand this: in BLOCK REDUCEi use a block with 1024 threads; each warp calculates its own reduction within 32 threads; 1°question: what value has val? 2°question: the meaning of val = (threadIdx.x < blockDim.x / warpSize) ? shared[lane] : 0;Justin, Very interesting. In the case using atomics you have to reset ""out"" on before every reduction; what is the recommended way to do this since I understand that out must live in device memory?excellent article! took me a little to digest it but it is super smart. thank you for taking time to write this, the initial loop grid-wide is especially smart! also because it naturally handles boundary conditions.One question, I tried the atomic optimization for reducing the the different reduction per block, where basically the 1024 values (one per block) get written to global memory. An atomic add is not enough to ensure correctness, due to the fact we don't know what value is in  out[0]. I tried to a cudaMemSet but was painfully slow, so I ended up doing  something like this:if ( threadIdx.x == 0 && blockIdx.x ==0 )     out[0] = 0;this happens as first thing in the kernel, before we call block_reduce().When I did that, my test suite started to pass again,it is not pretty, but is the fastest thing i found, only one warp will be affected since only the first thread of the first block will take that branch. Is there a better way to achieve this ? I was thinking to try a ptx instruction using a predicate but i don't think might actually yield better result.Best regardsM.Either do a cudaMemset prior to calling the kernel or write a different kernel which sets this value to 0.  Then launch that kernel prior to this kernel.  I would not use the first thread of that kernel to set it as there is no guarantee in CUDA for the order that blocks get scheduled.absolutely true! I will move that to a separated kernel call and see if is still faster, the only speed up I was getting from the atomic operation was due to the fact I don't need to call the second kernel reduction, hopefully running the kernel to zero out that value won't be as slow as the reduction kernel. Thank you for the heads up.Great as always!I just want to know that ,Is it possible to use this function for multiplication by substitution the (*) mark instead of (+) ?Technically you can use any associative operator (including *) in a parallel reduction. Just keep in mind the properties of floating point arithmetic: e.g. you can quickly overflow if *-reducing a lot of large enough numbers (which don't have to be that large depending on how many there are!).Hello,It is mentioned that CUB library selects Algorithm based on GPU architecture.For Kepler Architecture, does CUB also make use of shuffle instructions or atomic operations to obtain the result ?Thank you for the great article.-Ameya.Hello,Following is a thrust code-h_in_value[7] = thrust::reduce(thrust::device, d_in1 + a - b, d_ori_rho_L1 + a);Here, the thrust reduce takes first and last input iterator, and thrust returns the value back to the CPU(copied to h_in_value)Can this functionality be obtained using CUB? -1. First and Last iterators as inputs2. Returning the result back to hostThank you,Waiting for your valuable reply,-Ameya.Hello @justinluitjens:disqus , I just wanted to let you know of a small issue with the code in the post. For kernel deviceReduceWarpAtomicKernel, the line:if (threadIdx.x & (warpSize - 1) == 0)has wrong parenthesization, as == has higher precedence than bitwise &. Took me a while to find out why my kernel wasn't working, but it turned out to be this. I believe and earlier version of this blogpost used % warpSize, which didn't have this issue.Anyhow, thanks for writing this!Thanks for helpfully pointing this out, Felipe. I've corrected the error.Hi Mark, while a friend in QLD, programmed a H2 fusion reactor, like cloud sims, entropy stems from randomness ...yet real world works from base statistics (much as I denied this for years due to selection by convenience) I set out to build an in-mem TByte DB, thinking we jus use Redis or Lustre then package dependency took away real-time.. & virtualisation removed paged tables.. shattered dream.. as the operational structure needs tables in precise boundary alignment..  Then CUDA pops.. it shares that concept   .. and I haven't brough IB  hardware to the wrong place.. have u time to discern if I can make do with sandy bridge PCIe on the Xeons, with 2 GPUs per node, each on separate bus? We Mod the driver to write host and GPU simultaneously, only that the old way is too slow. Now UM is a virtual mem manager, with conditional use of pointers to the PHY mem ...no?Nice article!Comparing deviceReduceWarpAtomicKernel and deviceReduceBlockAtomicKernel, one checks threadIdx.x for 0,but the other does a convoluted “if ((threadIdx.x & (warpSize - 1)) == 0)”. Why is that?Isn't that the same? What is the exact type of warpSize? I cannot find a define.ThanksIn the block case threadIdx.x==0 does the atomicAdd.  In the warp case threadIdx.x==thread_zero_of_warp does the update.  The code in the if is just a quick way to check if we are the first thread in the warp.  It is functionally equivalent to if ( (threadIdx.x%warpSize) == 0 ) but avoids the expensive modulo operation.   On all our hardware warpSize = 32 but that in theory could change in the future and is a runtime constant.I spent some effort getting the parallel_reduction_with_shfl code to work with int, float and double types using templates.  The results from using double shows some interesting behavior on a V100 under cuda-11.2.  I put my fork at GitHub - dkokron/code-samples at dkokron/templateIt is pretty helpful for my implementation. There is one spot which might give wrong answer for general case.
For block reduction://read from shared memory only if that warp existed
val = (threadIdx.x < blockDim.x / warpSize) ? shared[lane] : 0;If the blocksize (blockDim.x) is not a multiple of warpSize, the data from the last warp will be missed.
It is probably better withval = (threadIdx.x < (blockDim.x+warpSize-1) / warpSize) ? shared[lane] : 0;Please correct me if I am wrong. Thanks!Powered by Discourse, best viewed with JavaScript enabled"
409,top-hpc-sessions-at-nvidia-gtc-2023,"Originally published at:			HPC Conference Session Catalog | NVIDIA GTC
From climate modeling to quantum computing, large language models to molecular dynamics; see how HPC is transforming the world.Powered by Discourse, best viewed with JavaScript enabled"
410,copperhead-data-parallel-python,"Originally published at:			https://developer.nvidia.com/blog/copperhead-data-parallel-python/
Programming environments like C and Fortran allow complete and unrestricted access to computing hardware, but often require programmers to understand the low-level details of the hardware they target. Although these efficiency-oriented systems are essential to every computing platform, many programmers prefer to use higher level programming environments like Python or Ruby, focused on productivity rather…Powered by Discourse, best viewed with JavaScript enabled"
411,nvidia-tesla-p4-gpus-available-now-on-the-google-cloud-platform,"Originally published at:			NVIDIA Tesla P4 GPUs Available Now on the Google Cloud Platform | NVIDIA Technical Blog
For highly demanding interactive and immersive graphics applications, developers need the best GPU accelerators at their disposal. To help artists, architects, and engineers create stunning scenes, Google just announced the availability of the NVIDIA Tesla P4 GPUs on the Google Cloud Platform. The GPUs are optimized for graphics-intensive applications and machine learning. “The new P4…Powered by Discourse, best viewed with JavaScript enabled"
412,vrworks-2-5-sdk-release,"Originally published at:			https://developer.nvidia.com/blog/vrworks-2-5-sdk-release/
NVIDIA released VRWorks SDK V2.5 for application and headset developers along with the NVIDIA display drivers 387.92 (Windows) and 387.12 (Linux/Beta). The drivers are available for download at: Official Drivers | NVIDIA and the SDK has been posted at developer.nvidia.com/vrworks.   RELEASE HIGHLIGHTS Updated Vulkan Interop (interop_vk) sample to use the ratified VK_KHR_external_memory* and VK_KHR_external_semaphore* extensions replacing the…Powered by Discourse, best viewed with JavaScript enabled"
413,dxr-training-a-conversation-with-adam-marrs-sr-graphics-engineer-at-nvidia,"Originally published at:			DXR Training: A Conversation with Adam Marrs, Sr. Graphics Engineer at NVIDIA | NVIDIA Technical Blog
On Tuesday, March 21st at 2:40pm, NVIDIA will host “Introduction to DirectX Raytracing,” a three hour tutorial session. Chris Wyman, Adam Marrs, and Juha Sjöholm will provide GDC attendees with an overview on how to integrate ray tracing into existing raster applications. Adam Marrs, Sr. Graphics Engineer at NVIDIA We asked Adam about adding ray…Powered by Discourse, best viewed with JavaScript enabled"
414,gtc-wrapup-nvidia-ceo-outlines-vision-for-accelerated-computing-data-center-architecture-ai-robotics-omniverse-avatars-and-digital-twins-in-keynote,"Originally published at:			GTC Wrap-Up: NVIDIA CEO Outlines Vision for Accelerated Computing, Data Center Architecture, AI, Robotics, Omniverse Avatars and Digital Twins in Keynote | NVIDIA Blog
Powered by Discourse, best viewed with JavaScript enabled"
415,using-ai-to-stop-child-trafficking,"Originally published at:			Using AI To Stop Child Trafficking | NVIDIA Technical Blog
According to UNICEF, 1.2 million children are trafficked every year. To help identify and rescue victims of trafficking, researchers from George Washington University, Adobe, and Temple University released a new dataset called Hotels-50K and developed an AI-based algorithm that can be used to identify possible locations of where children are being held. “Recognizing a hotel…Powered by Discourse, best viewed with JavaScript enabled"
416,gtc-digital-demo-rapids-gpu-accelerated-data-analytics-machine-learning,"Originally published at:			GTC Digital Demo: RAPIDS: GPU-Accelerated Data Analytics & Machine Learning | NVIDIA Technical Blog
This demonstration released at GTC Digital 2020 uses RAPIDS, and OmniSci’s GPU-accelerated analytics platform to quickly visualize and run queries on the 1.1 billion New York City taxi ride dataset. The RAPIDS suite of software libraries gives you the freedom to execute end-to-end data science and analytics pipelines entirely on GPUs. RAPIDS relies on NVIDIA…Powered by Discourse, best viewed with JavaScript enabled"
417,lawrence-livermore-unveils-sierra-worlds-third-fastest-supercomputer,"Originally published at:			Lawrence Livermore Unveils Sierra, World’s Third Fastest Supercomputer | NVIDIA Technical Blog
The U.S. Department of Energy and the Lawrence Livermore National Laboratory (LLNL) last week announced the unveiling of Sierra, one of the world’s fastest supercomputers. Sierra is ranked as the world’s third-fastest supercomputer in the world with a peak performance of 125 petaFLOPS -125 quadrillion floating-point operations per second. “Sierra can perform most required calculations…Powered by Discourse, best viewed with JavaScript enabled"
418,gtc-2020-anomaly-detection-on-aircraft-sensor-data-using-deep-learning,"GTC 2020 S21454
Presenters: Stephane Rion,Teradata; Amgad Muhammad,Teradata
Abstract
A vast amount of aircraft sensor data still remains unexploited due to its volume and shear complexity. In 2019, Airbus released, as part of a four-month long challenge, a 65+ gigabyte dataset recorded from real aircraft systems. The goal was to detect a set of anomalies from an unlabeled multidimension time series dataset. We’ll describe our solution, which is based on a two-stage approach using auto encoders and long short-term memory neural networks, and how we reached third place out of 160 teams competing. You’ll learn the benefits of using autoencoder to achieve dimension reduction in a clustering problem and how LSTM-based neural networks can be applied to detect anomalies in an unsupervised way. We’ll dive into the technical details of the solution and discuss the results obtained, as well as potential next steps. Basic knowledge of deep learning-techniques such as Autoencoder or LSTM will help, but isn’t required.Watch this session
Join in the conversation below.Is the dataset used in this presentation publicly available for download?Powered by Discourse, best viewed with JavaScript enabled"
419,pro-tip-cublas-strided-batched-matrix-multiply,"Originally published at:			https://developer.nvidia.com/blog/cublas-strided-batched-matrix-multiply/
There’s a new computational workhorse in town. For decades, general matrix-matrix multiply—known as GEMM in Basic Linear Algebra Subroutines (BLAS) libraries—has been a standard benchmark for computational performance. GEMM is possibly the most optimized and widely used routine in scientific computing. Expert implementations are available for every architecture and quickly achieve the peak performance of…Very good post Cris. I have been dealing with batch problems since the beginning of CUDA and that's a good thing to see works ongoing on this topic.Are there any people working on cuSOLVER batch functions ?Oh yes, there is lots of interest in batched LU (getrf/getrs), QR (geqrf), Cholesky (potrf/potrs), and SVD (gesvd).MAGMA already has implementations of many of these batched algorithms on GPUs:http://icl.cs.utk.edu/magma...Thanks for the info, it's a while since I didn't check magma release notes and at that time there were no batched functions. That's good news !Hi @disqus_dqT1zVDaih:disqus, what cuSOLVER batch functions are you specifically interested in?Hi @Mark_Harris:disqus , sorry for the late response. I'm using Cholesky (potrf) factorization and system solver (potrs) to solve w^H = s^H R^(-1) systems where R is a Hermitian positive definie matrix and s a known vector.This typically done when looking to find the filter vector w in adpative signal processing.Unluckily for me, matrix size is generally between 64x64 and 256x256, which is not enough to run efficiently on GPU in sequential mode. Using streams parallelism, I'have managed to make the GPU competitive against a multicore CPU. I will check out Magma's functions to see if there are doing better than CUDA streamed functions.Jeff@Mark_Harris:disqus   : Is there a road map for supporting different batch linear algebra, especially with different layouts. The strided layout here is useful, but other layouts could help us more. We are interested in GETRF, TRSM, QR etc.Hi Siva, cuBLAS has batched getrf, getrs and geqrf; are these not sufficient for your needs? If you'd like, you can contact me via email directly and I will share your specific needs with our libraries team and product managers.Hi Cris,In my application I need to multiply each slice of a 3D matrix (i.e A[NXNXP]) with a 2D matrix(B[NXN]) and sum up the 2D matrices together to get another matrix(C[NXN]). I can do this by using Dgemm and launch P kernels each calculating the multiplication of a slice of A with B and then add the result to an existing C matrix (it seems that cublasDgemm has atomicAdd inside that lets me add different slices).Now, instead of launching P kernels, using DgemmStridedBatched if I feed(see below) it with A as a 3D and B, C as 2D matrices and then put strideB=0, strideC =0 and beta = 1, I get an error of ""on entry to DEMM paramter number 15 had an illegal value"".cublasDgemmStridedBatched(cnpHandle, CUBLAS_OP_N, CUBLAS_OP_N, WIDTH, WIDTH, WIDTH, d_alpha, d_A, WIDTH, WIDTH*WIDTH,d_B, WIDTH, 0, d_beta, d_C, WIDTH, 0, P); Does C have to be a 3D matrix and after filling it I should call another kernel and sum the slices of C?Thanks.Hi Soroush,It turns out CUDA 8.0 disallowed certain strideC values, but this is eliminated in CUDA 9.0 (RC).That said, what you are proposing won't work -- there's no reduction between output matrices that is implied. Setting strideC = 0 means that all output matrices will just be overwriting each other. Your method of using gemms works not because there are atomicAdds, but because the sequence of gemms is serialized (red line in Figure 1).To accomplish what you describe, I would call a gemv(A, vector of 1s, A'), then a gemm(A', B, C) with the appropriate strides and flattenings.Thanks Cris!I have a quick question. If I use cublassDgemmStridedBatched in a kernel and want to launch many kernels (i.e. 100000 times), then does each kernel call create a grid when reaches to cublassDgemmStridedBatched?  Is it even possible to launch that many kernels assuming that there is no limitation on memory. Or it is better to use cudaStream.Thanks in advance!The ""zero stride"" optimization mentioned inside the article (many small matrices are multiplied by a single matrix) is this already implemented in CUDA 9.0 ? This optimization would be really useful in both interfaces (pointer to pointer and strided batch).Powered by Discourse, best viewed with JavaScript enabled"
420,upcoming-webinar-how-to-maintain-and-optimize-edge-computing-deployments,"Originally published at:			https://info.nvidia.com/edge-computing-301.html?nvid=nv-int-txtad-547198#cid=dl23_nv-int-txtad_en-us
Join us on August 11, 2022 to learn how to design edge deployments for future-proof scale, best practices for optimizing multiple deployments on edge systems, and tips for remotely repairing systems and applications.Powered by Discourse, best viewed with JavaScript enabled"
421,aaa-games-using-path-tracing,"Can I check out some AAA games using Path Tracing today outside of Cyberpunk?Other path traced games available include Minecraft RTX and Quake II RTX. Portal RTX is coming in the future.Powered by Discourse, best viewed with JavaScript enabled"
422,supercomputer-helps-understand-how-jupiter-evolved,"Originally published at:			Supercomputer Helps Understand How Jupiter Evolved | NVIDIA Technical Blog
Researchers from ETH Zürich and the Universities of Zürich and Bern simulated different scenarios on the computing power of the GPU-accelerated Swiss National Supercomputing Centre (CSCS) to find out how young giant planets exactly form and evolve. “We pushed our simulations to the limits in terms of the complexity of the physics added to the…Powered by Discourse, best viewed with JavaScript enabled"
423,upcoming-webinar-be-an-nvidia-isaac-ros-devops-hero-with-containerized-development,"Originally published at:			Isaac ROS Webinar Series
On February 14, get introduced to Docker and Continuous Integration, and deep dive into NVIDIA Isaac ROS, setting up a basic docker, and building Isaac ROS packages for distribution.Powered by Discourse, best viewed with JavaScript enabled"
424,nvidias-2017-open-source-deep-learning-frameworks-contributions,"Originally published at:			NVIDIA’s 2017 Open-Source Deep Learning Frameworks Contributions | NVIDIA Technical Blog
Many may not know, NVIDIA is a significant contributor to the open-source deep learning community. How significant? Let’s reflect and explore the highlights and volume of activity from last year. NVIDIA and Deep Learning Community The deep learning frameworks team at NVIDIA is focused on directly improving and accelerating the deep learning communities’ frameworks. We…Powered by Discourse, best viewed with JavaScript enabled"
425,breaking-the-boundaries-of-intelligent-video-analytics-with-deepstream-sdk-3-0,"Originally published at:			Breaking the Boundaries of Intelligent Video Analytics with DeepStream SDK 3.0 | NVIDIA Technical Blog
A picture is worth a thousand words and videos have thousands of pictures. Both contain incredible amounts of insights only revealed through the power of intelligent video analytics (IVA). The NVIDIA DeepStream SDK accelerates development of scalable IVA applications, making it easier for developers to build core deep learning networks instead of designing end-to-end applications…I am running sample (deepstream-app -c configs/deepstream-app/source30_720p_dec_infer-resnet_tiled_display_int8.txt) DeepStream 3.0 with Tesla P4. Initially it process videos faster, but with the passage of time its performance is going down. Using Nvidia-smi I can see increase in Volatile GPU-Util that reaches to 100%. At [sink0] with type=2(EglSink) and sync=1 I see following message in console log        There may be a timestamping problem, or this computer is too slow.        WARNING from sink_sub_bin_sink1: A lot of buffers are being dropped.        Debug info: gstbasesink.c(2854): gst_base_sink_is_too_late ():         /GstPipeline:pipeline/GstBin:processing_bin_0/GstBin:sink_bin/GstBin:sink_sub_bin1/GstEglGlesSink:sink_sub_bin_sink1:Tahir,Can you please report this issue in the DeepStream forum on devtalk.nvidia.com (https://devtalk.nvidia.com/....Our support team will be able to work with you on your specific use case and offer help.Hi all, does TensorRT Inference Server (TRTIS) support running video analytics inference with DeepStream?. I read that DeepStream works with TensorRT optimized inference engines as input to run the inference with DeepStream, and I need to know if DeepStream works with TRTIS also.Powered by Discourse, best viewed with JavaScript enabled"
426,remote-application-development-using-nvidia-nsight-x2122-eclipse-edition,"Originally published at:			https://developer.nvidia.com/blog/remote-application-development-nvidia-nsight-eclipse-edition/
NVIDIA® Nsight™ Eclipse Edition (NSEE) is a full-featured unified CPU+GPU integrated development environment(IDE) that lets you easily develop CUDA applications for either your local (x86_64) system or a remote (x86_64 or ARM) target system. In my last post on remote development of CUDA applications, I covered NSEE’s cross compilation mode. In this post I will focus…I followed the above instructions up to trying to run the particles example.Here is what I get:Last login: Sun Aug 31 21:22:47 2014 from 192.168.0.2echo $PWD'>'/bin/sh -c ""cd \""/home/ubuntu/cuda-wrokspace\"";export LD_LIBRARY_PATH=\""/usr/local/cuda-6.0/lib\"":\${LD_LIBRARY_PATH};\""/home/ubuntu/cuda-wrokspace/particles\"""";exitubuntu@tegra-ubuntu:~$ echo $PWD'>'/home/ubuntu>ubuntu@tegra-ubuntu:~$ /bin/sh -c ""cd \""/home/ubuntu/cuda-wrokspace\"";export LD_ LIBRARY_PATH=\""/usr/local/cuda-6.0/lib\"":\${LD_LIBRARY_PATH};\""/home/ubuntu/cuda -wrokspace/particles\"""";exit/bin/sh: 1: /home/ubuntu/cuda-wrokspace/particles: Permission deniedlogoutDo you have any idea what is the source of this problem?Sorry about the late response. Nsight 6.0 had some flacky connection bug that may cause such file permission update issues on the JetsonTK1. Nsight 6.5 has this bug fixed. Since your target is JetsonTK1, you are doing the right thing by continuing to use 6.0 toolkit. To work around this issue you may want to try updating the file permission manually on the target by enabling execute and write permissions using ""chmod 777 particles"".Thanks a lot for the excellent tutorial. In my case the host is a MacOSX and the remote target is a Jetson TK1 and I made it to successfully compile and run some CUDA projects. I would just like to add that it was necessary to change the (remote) compiler's path to /usr/bin/arm-linux-gnueabihf-g++-4.8 (the default was g++ version 4.6 for the ARM architecture, while my target had version 4.8 already installed). To do so, go to ""Project name>Properties>Build>Settings>Build stages>Compiler path"" and do the same at ""Project name>Properties>Build>Settings>NVCC Linker>Miscelaneous>Compiler path"" (see the attached figures).Excellent great to know that you are successful in creating CUDA applications for JetsonTK1 using your MacOSX host system. For Jetson TK1 targets with default g++-4.8, yes your proposed change in NsightEclipse is necessary. One could also create a g++-4.6 symlink on the target as follows: sudo ln -sf `which arm-linux-gnueabihf-g++` /usr/bin/arm-linux-gnueabihf-g++-4.6bug #1566745MacOSX host Jetson target builds and runs particles fine, however:Debugging Your Remote Applicationset break point then“Debug As->Remote C/C++ Application”error message:Error in final launch sequenceFailed to execute MI command:-target-select remote 192.168.1.91:2345Error message from debugger back end:cuda-gdb version (6.5.121) is not compatible with cuda-gdbserver version (6.0.116).\nPlease use the same version of cuda-gdb and cuda-gdbserver. NB “Debug Perspective” dialogue not raisedplease note on Jetson target:/usr/local/cuda-6.0/bin$ ./cuda-gdbserver --versionNVIDIA (R) CUDA gdbserver6.5 release.../usr/local/cuda-6.0/bin$ ./cuda-gdb -vNVIDIA (R) CUDA Debugger6.0 release...seems for Jetson one must install CUDA 6.0 toolkit on MacOSXadd ""/usr/local/cuda-6.0/samples/common/inc""to Properties Settings NVCC compiler Includeswfm  cuda-6.0 on OSX with same on Jetsonnot needed for some reason on cuda 6.5 but see my note belowThat's right if you are using Jetson TK1 as a target device please continue to use 6.0 toolkit as mentioned in the CUDA TK setup section. CUDA6.5TK will be available in a future JetsonTK1 OS image version Rel21.2. You can check your JetsonTK1 release as follows:> head -1 /etc/nv_tegra_releasecan step through CPU code, but having set breakpoint and clicked resume, Registers Value are all Error: Target not available and Disassembly No debug contextplease advise furtherJonathanOSX host Jetson targetI followed this sample using a 64-bit linux host and the Jetson TK1 remote. It worked just fine, but the remote application only runs at 6-7 frames per second. The one made on the Jetson from the the cuda samples directory runs at 60 fps. I was wonder what caused the slow down and how to fix it?Great good to know you are running code on your JetsonTK1. On the perf issue note that Jetson TK1 has a Kepler class GPU so make sure you check SM32(3.2) in the ""Generate GPU code"" option under Project>Build>Settings>CUDA.I set the GPU code to 3.2 and the PTX code to 3.0 and still didn't see a performance increase.Make sure you are not running in the debugger and that the ""Enable CUDA memcheck"" box is unchecked under debug configurations->Debugger tab.Ok it worked. I just needed to run it from the release build.Is it possible to have Nsight index also header files on the remote machine? For example, I have project which uses the OpenCV library, which is installed only on the Jetson TK1 system. Locally I don't have an OpenCV installation...Nope you can index only project files on the host system. In sync project mode if you maintain files on the host then those will get sync'd with the target.I've read that newer Eclipse versions provide this functionality. AFAIK, Nsight is currently based on Eclipse Juno.Are there any plans to upgrade to a newer version of Eclipse?Yeah sometime later 2015 to 4.4 Luna.Hi,I'm using freshly installed JetPack (CUDA 6.5) and trying to remotly debug particles example (Jetson target). I'm able to build and run example, but when in the debug, kernel breakpoint are not hit, it stops once in the main and after I click resume I can see that application starts on target (application window opens) but never stops in the kernel and nothing going on in the application window.Any suggestions?Thanks.Powered by Discourse, best viewed with JavaScript enabled"
427,university-of-sydney-unveils-a-new-gpu-accelerated-supercomputer-to-fuel-ai-research,"Originally published at:			University of Sydney Unveils a New GPU-accelerated Supercomputer to Fuel AI Research | NVIDIA Technical Blog
The University of Sydney recently installed a new GPU-accelerated supercomputer called Artemis 3 to further the university’s artificial intelligence research in the areas of geophysics, cosmology, genomics, proteomics, economics, transport logistics, and medical imaging. “The University’s research continues to grow in computational intensity,” said Dr. Jeremy Hammond, director of Strategic Ventures at The University of…Powered by Discourse, best viewed with JavaScript enabled"
428,nvidia-nsight-systems-2020-4-is-now-available-for-download,"Originally published at:			NVIDIA Nsight Systems 2020.4 is Now Available for Download | NVIDIA Technical Blog
This release adds support for NVIDIA GeForce RTX 30 Series and Quadro RTX Series GPU platforms. A rich set of CUDA features and improvements have been added including: CUDA 11.1, CUDA memory allocation trace, CLI support on Windows, and the ability to launch Nsight Compute directly to inspect a selected CUDA kernel.  Nsight Systems can…Powered by Discourse, best viewed with JavaScript enabled"
429,kubernetes-for-ai-hyperparameter-search-experiments,"Originally published at:			Kubernetes For AI Hyperparameter Search Experiments | NVIDIA Technical Blog
The software industry has recently seen a huge shift in how software deployments are done thanks to technologies such as containers and orchestrators. While container technologies have been around, credit goes to Docker for making containers mainstream, by greatly simplifying the process of creating, managing and deploying containerized applications. We’re now seeing a similar paradigm…Powered by Discourse, best viewed with JavaScript enabled"
430,gtc-2020-provr-vulkan-and-opengl-using-quadro-gpus,"GTC 2020 S21197
Presenters: Ingo Esser,NVIDIA; Robert Menzel,NVIDIA
Abstract
We’ll show recent developments in Vulkan and OpenGL for virtual reality, as part of our yearly talks about professional VR. Learn how new extensions in OpenGL and Vulkan improve the use of recent hardware features like variable rate shading and multiview rendering, as well as explicit multi-GPU rendering for VR head-mounted displays, multimonitor, and CAVE configurations.Watch this session
Join in the conversation below.Powered by Discourse, best viewed with JavaScript enabled"
431,gtc-2020-jumpstart-av-development-with-drive-os-and-driveworks,"GTC 2020 CWE21184
Presenters: Aaraadhya Narra,NVIDIA; Kamal Balagopalan, ; Wei Chen, ; Tomasz Borusiak, ; Dennis Lui ,
Abstract
Learn from the NVIDIA Developer Zone Forum experts how to leverage the DRIVE OS system software along with the DriveWorks middleware layer for efficient autonomous vehicle development. DRIVE OS provides a flexible end-to-end software and hardware development platform. Together with DriveWorks, it delivers plug-n-play components as well as the ability to customize applications for specialized use cases.Watch this session
Join in the conversation below.Powered by Discourse, best viewed with JavaScript enabled"
432,share-your-science-understanding-cancer-biology-with-gpus,"Originally published at:			https://developer.nvidia.com/blog/share-your-science-understanding-cancer-biology-with-gpus/
Gil Speyer, Senior Postdoctoral Fellow at the Translational Genomics Research Institute (TGen) shares how NVIDIA technology is accelerating the computer processing of transcriptomes from thousands of cells gleaned from patient tumor samples. Using NVIDIA Tesla K40 GPUs and CUDA, the scientists developed a statistical analysis tool called EDDY (evaluation of differential dependency) that examines in…Powered by Discourse, best viewed with JavaScript enabled"
433,developer-voices,"Originally published at:			Developer Voices | NVIDIA Technical Blog
We love seeing all of the NVIDIA GPU-related tweets – here’s some that we came across this week: Putting the finishing touches on a #KNIME workflow for #deeplearning and #digitalpathology using #Azure and #spark and #nvidia #gpus — Jon Fuller (@JonathanCFuller) April 3, 2017 @HULL_HPC_VIPER new @nvidia P100 accelerators will now be used intensely for…Powered by Discourse, best viewed with JavaScript enabled"
434,create-xr-experiences-using-natural-language-voice-commands-test-project-mellon,"Originally published at:			https://developer.nvidia.com/blog/create-xr-experiences-using-natural-language-voice-commands-test-project-mellon/
At GTC 2023, NVIDIA announced that developers can start testing Project Mellon to explore creating hands-free extended reality (XR) experiences controlled by natural-language voice commands.Hi,
I’m trying to install Project Mellon but it seems the documentation points out to an unexisting repository:https://gitlab-master.nvidia.com/dialogue-research/mellontoolkit.gitCan you help me to find it?I can’t move forward without this component.Thanks,Powered by Discourse, best viewed with JavaScript enabled"
435,new-nvidia-research-helps-robots-improve-their-grasp,"Originally published at:			https://developer.nvidia.com/blog/new-nvidia-research-helps-robots-improve-their-grasp/
Researchers at NVIDIA’s Seattle Robotics Lab developed a new algorithm, called 6-DoF GraspNet, that allows robots to grasp arbitrary objects. Grasping is an essential building block in current and future robotic systems, ranging from warehouse logistics to service robotics. Imagine a robot is asked to pick up an object and put it in a container.…Powered by Discourse, best viewed with JavaScript enabled"
436,accelerated-computing-workshops-at-gtc-2022,"Originally published at:			Conference Session Catalog | GTC 2022 | NVIDIA
Expand your accelerated computing skills at GTC 2022 with hands-on NVIDIA Deep Learning Institute workshops.Powered by Discourse, best viewed with JavaScript enabled"
437,amgx-multi-grid-accelerated-linear-solvers-for-industrial-applications,"Originally published at:			https://developer.nvidia.com/blog/amgx-multi-grid-accelerated-linear-solvers-industrial-applications/
Many industries use Computational Fluid Dynamics (CFD) to predict fluid flow forces on products during the design phase, using only numerical methods. A famous example is Boeing’s  777 airliner, which was designed and built without the construction (or destruction) of a single model in a wind tunnel, an industry first. This approach dramatically reduces the cost…Hi dear dear dear dear Dr.Eaton:Is there any people for which I could ask questions? Is there a contact email for maintatin or general questions? I think this lib is awsome and really like to get more information.Thanks very.Hello, Sir i am working on CUDA and i want to use AmgX with OpenFOAM but due to lack of resources i am not able to identify feasibility with OpenFOAM. can u suggest me some guidelinesPowered by Discourse, best viewed with JavaScript enabled"
438,dynamic-scale-weighting-through-multiscale-speaker-diarization,"Originally published at:			https://developer.nvidia.com/blog/dynamic-scale-weighting-through-multiscale-speaker-diarization/
MSDD is a neural model that can be trained on 2-speaker dataset and the proposed model enables overlap-aware speaker diarization on flexible number of speakers.Powered by Discourse, best viewed with JavaScript enabled"
439,eni-deploys-echelon-an-advanced-dynamic-reservoir-simulator-powered-by-nvidia-gpus,"Originally published at:			Eni Deploys ECHELON, an Advanced Dynamic Reservoir Simulator, Powered by NVIDIA GPUs | NVIDIA Technical Blog
Eni, the Italian energy company headquartered in Milan, today announced the deployment of the GPU-based reservoir simulator, ECHELON, an advanced dynamic reservoir simulator, for optimization of field monitoring, development, and production.  ECHELON, developed jointly with Balitmore-based Stone Ridge Technology, achieves a 5-fold reduction in processing times compared to other commercial reservoir simulators, the company said. …Powered by Discourse, best viewed with JavaScript enabled"
440,deep-learning-hyperparameter-optimization-with-competing-objectives,"Originally published at:			Deep Learning Hyperparameter Optimization with Competing Objectives | NVIDIA Technical Blog
Learn how to use SigOpt’s Bayesian optimization platform to jointly optimize competing objectives in deep learning pipelines on NVIDIA GPUs more than ten times faster than traditional approaches like random search. Measuring how well a model performs may not come down to a single factor. Often there are multiple, sometimes competing, ways to measure model…Powered by Discourse, best viewed with JavaScript enabled"
441,researchers-develop-ai-system-for-license-plate-recognition,"Originally published at:			https://developer.nvidia.com/blog/researchers-develop-ai-system-for-license-plate-recognition/
Researchers in Brazil developed a deep learning system that captures license plate data in real-time, resulting in better performance than most commercially available products in Brazil. License plate readers have the potential to eliminate toll booths, help monitor traffic conditions, track carpool lane usage, and even help find lost children or missing vehicles. Because of…Powered by Discourse, best viewed with JavaScript enabled"
442,how-drive-agx-cuda-and-tensorrt-achieve-fast-accurate-autonomous-vehicle-perception,"Originally published at:			How DRIVE AGX, CUDA and TensorRT Achieve Fast, Accurate Autonomous Vehicle Perception | NVIDIA Technical Blog
Autonomous vehicles are complex, requiring fast and accurate perception of their surroundings to make decisions in real-time. This capability calls for high-performance AI compute to enable the wide range of tasks required for autonomous driving, including multi-stage sensor data processing, and obstacle detection as well as traffic sign and lane recognition. With the CUDA toolkit…Powered by Discourse, best viewed with JavaScript enabled"
443,upcoming-webinar-transform-your-vision-ai-business-with-nvidia-jetson-orin-and-nvidia-launchpad,"Originally published at:			https://info.nvidia.com/metropolis-meetup-july2022.html
Join this webinar and Metropolis meetup on July 20 and 21, to learn how NVIDIA Jetson Orin and NVIDIA Launchpad boost your go-to-market efforts for vision AI applications.Powered by Discourse, best viewed with JavaScript enabled"
444,voting-and-shuffling-to-optimize-atomic-operations,"Originally published at:			https://developer.nvidia.com/blog/voting-and-shuffling-optimize-atomic-operations/
2iSome years ago I started work on my first CUDA implementation of the Multiparticle Collision Dynamics (MPC) algorithm, a particle-in-cell code used to simulate hydrodynamic interactions between solvents and solutes. As part of this algorithm, a number of particle parameters are summed to calculate certain cell parameters. This was in the days of the Tesla…Powered by Discourse, best viewed with JavaScript enabled"
445,nvidia-simnet-v21-06-released-for-general-availability,"Originally published at:			https://developer.nvidia.com/blog/nvidia-simnet-v21-06-released-for-general-availability/
NVIDIA SimNet is a physics-informed neural network (PINNs) toolkit, which addresses these challenges using AI and physics.Powered by Discourse, best viewed with JavaScript enabled"
446,ai-spots-mysterious-signals-coming-from-deep-in-space,"Originally published at:			AI Spots Mysterious Signals Coming from Deep in Space | NVIDIA Technical Blog
Fast radio bursts are some of the most mysterious high-energy astrophysical phenomena in the entire universe. They are intense blasts of radio emissions that last just milliseconds in duration and are thought to originate from distant galaxies. The exact nature of the objects is uncertain, but they could point to extraterrestrial intelligence. “Previous studies have…Powered by Discourse, best viewed with JavaScript enabled"
447,gtc-digital-demo-accelerating-scientific-engineering-simulation-workflows-with-ai,"Originally published at:			https://developer.nvidia.com/blog/gtc-digital-demo-accelerating-scientific-engineering-simulation-workflows-with-ai/
A new demo introduces the recently announced NVIDIA SimNet Toolkit, the first multi-physics (CFD and Heat Transfer) analysis using physics-informed neural networks. Simulations form an integral part of product design to reduce significant iterations in physical prototyping and testing to improve quality, cost and time-to-market. However, this process is very time consuming and can take…Powered by Discourse, best viewed with JavaScript enabled"
448,gtc-2020-improving-cnn-performance-with-spatial-context,"GTC 2020 S21388
Presenters: Daniel Russakoff,Voxeleron
Abstract
Deep learning with convolutional neural networks (CNN) is a powerful technique with wide-ranging applications. It has largely replaced traditional computer vision as the go-to method for solving image-analysis and classification problems. At its essence, however, training a CNN is an enormous global optimization problem which, like all optimizations, can fall victim to local extrema. We’ll discuss ways of mitigating this issue using computer vision to add spatial context information to restrict the domain of optimization. These techniques not only speed up the training, but also improve the overall performance of the networks. We’ll demonstrate results on real-world classification and segmentation problems.Watch this session
Join in the conversation below.I did not expect this content when reading the abstract, but I am happy to have attended this lecture. Thank you for sharing these interesting techniques.Powered by Discourse, best viewed with JavaScript enabled"
449,researchers-use-gpus-to-detect-earthquake-hazards-coming-our-way,"Originally published at:			https://developer.nvidia.com/blog/researchers-use-gpus-to-detect-earthquake-hazards-coming-our-way/
Southern California Earthquake Center researchers are using GPUs to develop a complex model that calculates how earthquake waves move through a 3D model of the Earth. This helps develop earthquake forecasts and more accurate hazard assessments. SCEC’s initial target is the Los Angeles region, where the Pacific and American tectonic plates run into each other to…Powered by Discourse, best viewed with JavaScript enabled"
450,app-uses-deep-learning-to-automatically-retouch-photos-before-you-take-them,"Originally published at:			App Uses Deep Learning to Automatically Retouch Photos Before You Take Them | NVIDIA Technical Blog
At this week’s SIGGRAPH conference, researchers from MIT and Google presented a new deep learning system that automatically retouches your photos like a professional photographer in real-time, before you even take them. Using a TITAN X GPU and the cuDNN-accelerated TensorFlow deep learning framework, the researchers trained their neural network on a dataset of 5,000…Powered by Discourse, best viewed with JavaScript enabled"
451,fueling-high-performance-computing-with-full-stack-innovation,"Originally published at:			https://developer.nvidia.com/blog/fueling-high-performance-computing-with-full-stack-innovation/
The NVIDIA platform, powered by the A100 Tensor Core GPU, delivers leading performance and versatility for accelerated HPC.The GBT machine name is incorrect, and 500w is not the production product.Powered by Discourse, best viewed with JavaScript enabled"
452,python-pandas-tutorial-beginners-guide-to-gpu-accelerated-dataframes-for-pandas-users,"Originally published at:			https://developer.nvidia.com/blog/python-pandas-tutorial-beginners-guide-to-gpu-accelerated-dataframes-for-pandas-users/
This tutorial is the second part of a series of introductions to the RAPIDS ecosystem. The series explores and discusses various aspects of RAPIDS that allow its users solve ETL (Extract, Transform, Load) problems, build ML (Machine Learning) and DL (Deep Learning) models, explore expansive graphs, process signal and system log, or use SQL language…Powered by Discourse, best viewed with JavaScript enabled"
453,gtc-2020-accelerate-your-online-speech-recognition-pipeline-with-gpus,"GTC 2020 S21832
Presenters: Hugo Braun,NVIDIA
Abstract
Automatic speech recognition (ASR) algorithms allow us to interact with devices, appliances, and services using spoken language. Used in cloud services like Siri, Google Voice, and Amazon Echo, speech recognition is growing in popularity, which substantially increases the computational demand for ASR inference. We are now bringing low-latency online ASR support to our GPU-accelerated pipeline, bringing order-of-magnitude speedups to your existing Kaldi models. That technology is available both in the data center for a high-throughput ASR cloud service, and at the edge on our low-power embedded devices from the Jetson family.Watch this session
Join in the conversation below.Powered by Discourse, best viewed with JavaScript enabled"
454,is-there-any-upcoming-game-title-with-ray-tracing-support-that-you-re-looking-forward-to,"Is there any upcoming game title with ray tracing support that you’re looking forward to?Powered by Discourse, best viewed with JavaScript enabled"
455,nvidia-at-interspeech-2021,"Originally published at:			https://developer.nvidia.com/blog/nvidia-at-interspeech-2021/
NVIDIA researchers are presenting five papers on our groundbreaking research in speech recognition and synthesis at INTERSPEECH 2021.Powered by Discourse, best viewed with JavaScript enabled"
456,generative-ai-sparks-life-into-virtual-characters-with-nvidia-ace-for-games,"Originally published at:			https://developer.nvidia.com/blog/generative-ai-sparks-life-into-virtual-characters-with-ace-for-games/
Use NVIDIA ACE for Games to build and deploy customized speech, conversation, and animation AI models in software and games.Powered by Discourse, best viewed with JavaScript enabled"
457,nvidia-breaks-ai-performance-records-in-latest-mlperf-benchmarks,"Originally published at:			NVIDIA Breaks AI Performance Records in Latest MLPerf Benchmarks | NVIDIA Technical Blog
NVIDIA today announced the fastest AI training performance among commercially available products, according to MLPerf benchmarks.  The A100 Tensor Core GPU demonstrated the fastest performance per accelerator on all eight MLPerf benchmarks. The DGX SuperPOD system, a massive cluster of DGX A100 systems connected with HDR InfiniBand, also set eight new performance milestones.  Today’s announcement…Powered by Discourse, best viewed with JavaScript enabled"
458,defining-ai-innovation-with-nvidia-dgx-a100,"Originally published at:			Defining AI Innovation with NVIDIA DGX A100 | NVIDIA Technical Blog
Organizations of all kinds are incorporating AI into their research, development, product, and business processes. This helps them meet and exceed their particular goals, and also helps them gain experience and knowledge to take on even bigger challenges. However, traditional compute infrastructures aren’t suitable for AI due to slow CPU architectures and varying system requirements…Powered by Discourse, best viewed with JavaScript enabled"
459,gtc-2020-clara-developer-day-federated-learning-using-clara-train-sdk,"GTC 2020 S22564
Presenters: Nicola Rieke,NVIDIA
Abstract
Federated Learning techniques enable training robust AI models in a de-centralized manner – meaning that the models can learn from diverse data but that data doesn’t leave the local site and always stays secure. This is achieved by sharing model-weights or partial model weights from each local client and aggregating these on a server that never accesses the source data. In this session we will deep dive into the federated learning architecture of latest Clara Train SDK. We will cover the core concepts of Federated Learning and the different collaborative learning techniques. Afterward, we will dive deeper into how using the Clara Train SDK enables privacy-preserving Federated Learning. This session will also cover the ease of bringing up Federated Learning clients and establishing communication between various clients and a server for model aggregation.Watch this session
Join in the conversation below.Powered by Discourse, best viewed with JavaScript enabled"
460,new-gpu-library-lowers-compute-costs-for-apache-spark-ml,"Originally published at:			https://developer.nvidia.com/blog/new-gpu-library-lowers-compute-costs-for-apache-spark-ml/
Spark MLlib is a key component of Apache Spark for large-scale machine learning and provides built-in implementations of many popular machine learning algorithms. These implementations were created a decade ago, but do not leverage modern computing accelerators, such as NVIDIA GPUs. To address this gap, we have recently open-sourced Spark RAPIDS ML (NVIDIA/spark-rapids-ml), a Python…Powered by Discourse, best viewed with JavaScript enabled"
461,programming-the-quantum-classical-supercomputer,"Originally published at:			https://developer.nvidia.com/blog/programming-the-quantum-classical-supercomputer/
This post introduces CUDA Quantum, highlights its unique features, and demonstrates how researchers can leverage it to gather momentum in day-to-day quantum algorithmic research and development.Powered by Discourse, best viewed with JavaScript enabled"
462,winners-announced-from-nvidia-inception-program-contest,"Originally published at:			https://developer.nvidia.com/blog/winners-announced-for-nvidia-inception-program-contest/
Late last year, the NVIDIA Inception Program hosted a “Cool Demo Contest” for GPU-accelerated startups that are applying deep learning to their innovations. A variety of companies from around the world submitted their demos, ranging from defense to healthcare applications. Below are highlights from three of the 14 winners who each won a Pascal TITAN…Powered by Discourse, best viewed with JavaScript enabled"
463,ai-identifies-changes-in-microcirculation-that-could-help-detect-sepsis,"Originally published at:			AI Identifies Changes In Microcirculation that Could Help Detect Sepsis | NVIDIA Technical Blog
One in three patients who die in a U.S. hospital has sepsis, a life-threatening complication of an infection. To help detect the disease as quickly as possible, researchers from MIT are exploring a deep learning-based approach that could one day automatically detect the condition in human patients in near real time. Using NVIDIA TITAN X…Powered by Discourse, best viewed with JavaScript enabled"
464,nvidia-developer-blog-2018-highlights,"Originally published at:			NVIDIA Developer Blog 2018 Highlights | NVIDIA Technical Blog
This year proved to be a banner year for the NVIDIA Developer Blog, publishing over 80 new technical articles. Given this year’s pace, it’s easy to miss out on cool new insights and information, so let’s take a little time to review the year. First, let’s look at the most popular new articles. The Five…Powered by Discourse, best viewed with JavaScript enabled"
465,saving-time-and-money-in-the-cloud-with-the-latest-nvidia-powered-instances,"Originally published at:			https://developer.nvidia.com/blog/saving-time-and-money-in-the-cloud-with-the-latest-nvidia-powered-instances/
The greater performance delivered by current-generation NVIDIA GPU-accelerated instances more than outweighs the per-hour pricing differences of prior-generation GPUs.Hi! I hope you enjoyed this blog post. If you have any questions or comments, please let us know.Thank you!
–AshrafPowered by Discourse, best viewed with JavaScript enabled"
466,ai-automatically-detects-lung-abnormalities,"Originally published at:			AI Automatically Detects Lung Abnormalities | NVIDIA Technical Blog
Researchers at the National Institutes of Health (NIH), led by visting fellow Dakai Jin, developed a deep learning-based system to automatically inpaint and detect pulmonary nodules, which are round or oval-shaped growths in the lung. “Deep learning has achieved significant recent successes. However, large amounts of training samples, which sufficiently cover the population diversity, are often…Powered by Discourse, best viewed with JavaScript enabled"
467,rapids-blog-a-compilation-pipeline-taking-user-defined-functions-in-python-to-cuda-kernels,"Originally published at:			RAPIDS Blog: A Compilation Pipeline Taking User Defined Functions in Python to CUDA Kernels | NVIDIA Technical Blog
Numba is the Just-in-time compiler used in RAPIDS cuDF to implement high-performance User-Defined Functions (UDFs) by turning user-supplied Python functions into CUDA kernels – but how does it go from Python code to CUDA kernel? In this post we’ll take a look at Numba’s compilation pipeline. If you enjoy diving into Numba’s internals, check out…Powered by Discourse, best viewed with JavaScript enabled"
468,google-develops-asr-system-to-help-people-with-speech-impairments,"Originally published at:			Google Develops ASR System To Help People with Speech Impairments | NVIDIA Technical Blog
To help people with speech impairments better interact with every-day smart devices, Google researchers have developed a deep learning-based automatic speech recognition (ASR) system that aims to improve communications for people with amyotrophic lateral sclerosis (ALS), a disease that can affect a person’s speech. The research, part of Project Euphoria, is an ASR platform that performs speech-to-text…Powered by Discourse, best viewed with JavaScript enabled"
469,nvswitch-accelerates-nvidia-dgx-2,"Originally published at:			NVSwitch Accelerates NVIDIA DGX-2 | NVIDIA Technical Blog
NVIDIA CEO Jensen Huang described the NVIDIA® DGX-2™ server as “the world’s largest GPU” at its launch during GPU Technology Conference earlier this year. DGX-2 comprises 16 NVIDIA Tesla™ V100 32 GB GPUs and other top-drawer components (two 24 core Xeon CPUs, 1.5 TB of DDR4 DRAM  memory, and 30 TB of NVMe storage) in a…Also see https://www.nextplatform.co...Can NVLInk be leveraged by games in SLI as on the RTX cards?Please see the ""NVLINK"" tab at:https://www.nvidia.com/en-u...for information on the:GeForce RTX NVLINK™ BridgePowered by Discourse, best viewed with JavaScript enabled"
470,gtc-21-top-xr-sessions-you-might-have-missed,"Originally published at:			https://developer.nvidia.com/blog/gtc-21-top-xr-sessions-you-might-have-missed/
From real-time ray tracing, to streaming from the cloud, find out more about the breakthroughs that are helping organizations across industries enhance their XR workflows.Powered by Discourse, best viewed with JavaScript enabled"
471,citi-ventures-invests-in-real-time-fraud-prevention-startup,"Originally published at:			https://developer.nvidia.com/blog/citi-ventures-invests-in-real-time-fraud-prevention-startup/
Citi Ventures made a strategic investment in Feedzai which uses deep learning to provide real-time fraud prevention in ecommerce and banking. Using CUDA, GTX 1080 GPUs and cuDNN with TensorFlow to train their deep learning models, Feedzai’s platform is able to scan large amounts of data to recognize evolving threats and then alerts customers in…Powered by Discourse, best viewed with JavaScript enabled"
472,navigating-generative-ai-for-network-admins,"Originally published at:			https://developer.nvidia.com/blog/navigating-generative-ai-for-network-admins/
We all know that AI is changing the world. For network admins, AI can improve day-to-day operations in some amazing ways: Automation of repetitive tasks: This includes monitoring, troubleshooting, and upgrades, saving time while lowering the risk of human errors. Network security: AI can help detect and respond to security threats in real time. For…Powered by Discourse, best viewed with JavaScript enabled"
473,jetson-project-of-the-month-livechess2fen-provides-real-time-game-analysis,"Originally published at:			https://developer.nvidia.com/blog/jetson-project-of-the-month-livechess2fen-provides-real-time-game-analysis/
Jetson Project of the Month “LiveChess2FEN” is a fully-functional framework that automatically digitizes a chessboard in real time using NVIDIA Jetson Nano.It would be great to have a docker image available with all dependencies installed for Jetson Nano because I am facing difficulties while installing them.Please feel free to open any issue you may find in GitHub: https://github.com/davidmallasen/LiveChess2FEN/issuesPowered by Discourse, best viewed with JavaScript enabled"
474,gtc-2020-validating-dgx-superpod-to-solve-the-worlds-most-challenging-ai-workloads,"GTC 2020 S21738
Presenters: Jacci Cenci-McGrody,NVIDIA; Darrin Johnson,NVIDIA
Abstract
The NVIDIA DGX SuperPOD is a first-of-its-kind artificial intelligence supercomputing infrastructure that delivers groundbreaking performance and deploys in two weeks as a fully integrated system. Validating reference architecture for a flexible and scalable AI infrastructure on DGX SuperPOD requires customers to follow a prescribed computer networking methodology and optimized storage infrastructure for data access, model training, and inferencing. If you want to install an NVIDIA SuperPOD reference architecture for a specific AI use-case, such as natural language processing at massive scale, we’ll provide the insights that you need for planning, design considerations, and best practices.Watch this session
Join in the conversation below.Powered by Discourse, best viewed with JavaScript enabled"
475,mixed-precision-training-for-nlp-and-speech-recognition-with-openseq2seq,"Originally published at:			Mixed Precision Training for NLP and Speech Recognition with OpenSeq2Seq | NVIDIA Technical Blog
The success of neural networks thus far has been built on bigger datasets, better theoretical models, and reduced training time. Sequential models, in particular, could stand to benefit from even more from these. To this end, we created OpenSeq2Seq – an open-source, TensorFlow-based toolkit. OpenSeq2Seq supports a wide range of off-the-shelf models, featuring multi-GPU and…Did you get NaN gradient problem when training model?I mean `Vanishing Gradient Problem`Powered by Discourse, best viewed with JavaScript enabled"
476,gtc-2020-virtual-gpu-computing-for-hpc-improving-system-utilization-through-gpu-virtualization,"GTC 2020 S21827
Presenters: Konstantin Cvetanov,NVIDIA; Xizhou Feng,Clemson University
Abstract
We’ll discuss Clemson University’s early experience of applying the NVIDIA vComputeServer in the high performance computing infrastructure to maximize their investment in GPU resources. We’ll present two use cases: one for running the Metamoto simulation workload for the Open Connected Autonomous Vehicle project, and the other for general GPU-accelerated applications in a production HPC cluster.Watch this session
Join in the conversation below.Powered by Discourse, best viewed with JavaScript enabled"
477,reshaping-the-light-on-extreme-diameter-telescopes-with-gpus,"Originally published at:			Reshaping the Light on Extreme Diameter Telescopes with GPUs | NVIDIA Technical Blog
The atmospheric turbulence that distorts the trajectory of lights rays have been a source of frustration for astronomers using huge telescopes. Damien Gratadour, an associate professor at Université Paris Diderot, was at the GPU Technology Conference last month to talk about how his team is using NVIDIA GPUs to reshape light beams so astronomers can…Powered by Discourse, best viewed with JavaScript enabled"
478,cuda-pro-tip-flush-denormals-with-confidence,"Originally published at:			https://developer.nvidia.com/blog/cuda-pro-tip-flush-denormals-confidence/
I want to keep this post fairly brief, so I will only give minimal background on floating point numbers. If you need a refresher on floating point representation, I recommend starting with the Wikipedia entry on floating point, and for more detail about NVIDIA GPU floating point, check out this excellent white paper. The Wikipedia entry…Powered by Discourse, best viewed with JavaScript enabled"
479,scaling-deep-learning-training-with-nccl,"Originally published at:			Scaling Deep Learning Training with NCCL | NVIDIA Technical Blog
NVIDIA Collective Communications Library (NCCL) provides optimized implementation of inter-GPU communication operations, such as allreduce and variants. Developers using deep learning frameworks can rely on NCCL’s highly optimized, MPI compatible and topology aware routines, to take full advantage of all available GPUs within and across multiple nodes. NCCL is optimized for high bandwidth and low latency…Hi，My cluster is equipped with both PCIe switch and NvLink. As known, NCCL will automatically choose reduction method regarding to topology. How can I subjectively choose the interconnect, PCIe or NvLink, by using knob?Thanks,J suPowered by Discourse, best viewed with JavaScript enabled"
480,an-easy-introduction-to-cuda-c-and-c,"Originally published at:			https://developer.nvidia.com/blog/easy-introduction-cuda-c-and-c/
Learn more with these hands-on DLI courses: Fundamentals of Accelerated Computing with CUDA C/C++ Fundamentals of Accelerated Computing with CUDA Python Update (January 2017): Check out a new, even easier introduction to CUDA! This post is the first in a series on CUDA C and C++, which is the C/C++ interface to the CUDA parallel…I think there's an error in the line ""In this case we use cudaMemcpyHostToDevice to specify that the first argument is a host pointer and the second argument is a device pointer."". Shouldn't it be ""In this case we use cudaMemcpyHostToDevice to specify that the first argument is a device pointer and the second argument is a host pointer.""? I think you exchanged host pointer with device pointer.Thanks Nisarg, good catch.  I've fixed this error.Hi, thanks for this tutorial!I have a GeForce 210 card, and when I run this program, I getMax error: 2.000000Whereas you see a max error of zero. It seems like the y[i] array is not getting operated on with my computer setup, but I get no compiler errors with nvcc. When I print out the result of max(maxError, abs(y[i]-4.0f)), I see the value 2.000000 every time, indicating that nothing happened to the y array on the device.Do you have any advice on what might be going wrong here?Thanks,Charles.I suspect a CUDA error.  Unfortunately the code example doesn't check for errors, because that is the lesson of the follow up post.  Can you add error checking as shown in the post http://devblogs.nvidia.com/... and see what errors you see?Ah, I did have some errors. I was using Ubuntu 13.10 and tried some ""workarounds"" involving third-party PPAs since this isn't an officially supported platform. I couldn't get the workarounds to work it seems, but instead of tracking down the problem, I installed 12.04 LTS and followed the official instructions, now everything seems to be working and I get the ""Max error: 0.00000"" output now. Yay!Thank you for taking the time to help me, Mark.My pleasure, glad you got it working.There are typos in the first paragraphs of both the Device Code and Host Code sections, x_d and y_d should be d_x and d_y.I've fixed these.  Thank you!I got the error: #include expects filename, what is the filename?Sorry about that, the code got mangled by the site.  I've fixed it. (The filename is <stdio.h>).There are missing '&'s in front of 'd_x' and 'd_y' when calling cudaMalloc in the first code snippets of the Host Code section.Fixed. Thanks!Thanks for your posting. I'm newbie for CUDA. When I compile your code, i got the same result ""Max error: 0.00..0n"". However, when I debug it, it doesn't get in __global__ void saxpy function. Is it normal?I mean it gets in saxpy but it doesn't readint i = blockIdx.x*blockDim.x + threadIdx.x;if (i < n) y[i] = a*x[i] + y[i];sorry, one more!How can ""int N = 1<<20"" work?i have no idea.1<<20 shifts the value 1 left 20 bits. This is equivalent to 2 raised to the power of 20 == 1048576.What debugger are you using? You need to use a gpu-aware debugger such as cuda-gdb or NSight (Visual Studio Edition or Eclipse Edition).Thanks Mark!!, then N should be 1048576/32 = 32768 right?And i'm using Nsight but__global__void saxpy(int n, float a, float *x, float *y){  <---- here    int i = blockIdx.x*blockDim.x + threadIdx.x;    if (i < n) y[i] = a*x[i] + y[i];}in this function, i cannot get under the first bracket, which i pointed. just empty red circleis there anything more header file than stdio.h?Powered by Discourse, best viewed with JavaScript enabled"
481,tensorflow-performance-logging-plugin-nvtx-plugins-tf-goes-public,"Originally published at:			TensorFlow Performance Logging Plugin nvtx-plugins-tf Goes Public | NVIDIA Technical Blog
The new nvtx-plugins-tf library enables users to add performance logging nodes to TensorFlow graphs. (TensorFlow is an open source library widely used for training DNN—deep neural network—models). These nodes log performance data using the NVTX (NVIDIA’s Tools Extension) library. The logged performance data can then be viewed in tools such as NVIDIA Nsight Systems and NVIDIA Nsight…Is this still operational for Keras and TF2?
pip install nvtx-plugins  failed for meINFO: Unable to build TensorFlow plugin, will skip it.Traceback (most recent call last):
File “/usr/lib/python3.8/distutils/unixccompiler.py”, line 117, in _compile
self.spawn(compiler_so + cc_args + [src, ‘-o’, obj] +
File “/usr/lib/python3.8/distutils/ccompiler.py”, line 910, in spawn
spawn(cmd, dry_run=self.dry_run)
File “/usr/lib/python3.8/distutils/spawn.py”, line 36, in spawn
_spawn_posix(cmd, search_path, dry_run=dry_run)
File “/usr/lib/python3.8/distutils/spawn.py”, line 157, in _spawn_posix
raise DistutilsExecError(
distutils.errors.DistutilsExecError: command ‘/usr/bin/gcc’ failed with exit status 1Powered by Discourse, best viewed with JavaScript enabled"
482,top-omniverse-sessions-for-developers-at-gtc-2022,"Originally published at:			3D Creator & Developer Conference Sessions |GTC 2022 | NVIDIA
Learn how to develop and distribute custom applications for the metaverse with the NVIDIA Omniverse platform at GTC.Powered by Discourse, best viewed with JavaScript enabled"
483,gtc-2020-gpu-accelerated-genome-assembly-a-deep-dive-into-clara-genomics-analysis-sdk,"GTC 2020 S21968
Presenters: Andreas Hehn,NVIDIA; Milos Maric, NVIDIA
Abstract
We’ll present de novo genome assembly for long DNA reads using the Clara Genomics Analysis SDK and dig deep into the SDK implementation. We’ll discuss the suitability of GPUs for genomics workloads and present algorithms suitable for massively parallel systems. We’ll also review the challenges we faced while implementing the algorithms in CUDA, and present the solutions used in cudaAligner, cudaPoa, and cudamapper.Watch this session
Join in the conversation below.Powered by Discourse, best viewed with JavaScript enabled"
484,nvidia-dli-releases-new-accelerated-computing-teaching-kit,"Originally published at:			NVIDIA DLI Releases New Accelerated Computing Teaching Kit | NVIDIA Technical Blog
The NVIDIA Deep Learning Institute (DLI) recently released the latest version of the Accelerated Computing Teaching Kit. NVIDIA Teaching Kits are complete course solutions for use by educators in a variety of academic disciplines that benefit from GPU-accelerated computing.  The new modules were developed in collaboration with University of Delaware Professor Sunita Chandrasekaran and University…Powered by Discourse, best viewed with JavaScript enabled"
485,gtc-21-top-5-game-development-technical-sessions,"Originally published at:			https://developer.nvidia.com/blog/gtc-21-top-5-game-development-technical-sessions/
This year at GTC we have a new track for Game Developers where you can attend sessions for covering the latest in ray tracing, optimizing game performance, and content creation in NVIDIA Omniverse.Powered by Discourse, best viewed with JavaScript enabled"
486,nvidia-announces-cuda-toolkit-11,"Originally published at:			NVIDIA Announces CUDA Toolkit 11 | NVIDIA Technical Blog
CUDA is the most powerful software development platform for building GPU-accelerated applications, providing all the components needed to develop applications targeting every GPU platform. CUDA 11 introduces support for the new NVIDIA A100 based on the NVIDIA Ampere architecture, Arm server processors, performance-optimized libraries, and new developer tools and improvements for A100.  Read our new…Powered by Discourse, best viewed with JavaScript enabled"
487,top-5-ai-stories-of-the-week-3-1,"Originally published at:			Top 5 AI Stories of the Week: 3/1 | NVIDIA Technical Blog
From an AI algorithm that can predict earthquakes to a system that can decode rodent chatter – here are the top 5 AI stories of the week. 5 – Deep Learning Shakes Up Seismology with Quake Early Warning System Most people can’t detect an earthquake until the ground under their feet is already shaking or…Powered by Discourse, best viewed with JavaScript enabled"
488,gpu-data-speed,"Hello,Actually, I need to sum matrix, and calculate the average.
The data are from a camera (6004*7920), in 8 bits. Because of the amount of data, the GPU is the most efficient way to obtain a sum of 5 of those images.For that, I use the example vectorAdd. I’m very very happy about the calculation time, it’s about 0.08ms against 700ms using the CPU (in sequential).My problem is about the time to transfert data.
I copy 5 vectors from the CPU to the GPU, of 45 Mo, it’s about 225Mo, and I calculate a data speed transfert of 2 Go/s.
Then, I copy 1 vector of 90 Mo (unsigned short), and I calulate a data speed transfert of 1 Go/s.
Can you confirm ?Moreother, is there a way to increase the data speed transfert, using an other memory ?Thank you for your help !Powered by Discourse, best viewed with JavaScript enabled"
489,nvidia-dlss-plugin-and-reflex-now-available-for-unreal-engine,"Originally published at:			https://developer.nvidia.com/blog/nvidia-dlss-and-reflex-now-available-for-unreal-engine-4-26/
Unreal Engine 4 (UE4) developers can now access DLSS as a plugin for Unreal Engine 4.26. Additionally, NVIDIA Reflex is now available as a feature in Unreal Engine 4. The NVIDIA RTX UE4 4.25 and 4.26 branches have also received updates.Powered by Discourse, best viewed with JavaScript enabled"
490,artificial-intelligence-helps-identify-molecular-structures,"Originally published at:			Artificial Intelligence Helps Identify Molecular Structures | NVIDIA Technical Blog
University of San Diego researchers developed a deep learning-based method to identify the molecular structures of natural products such as soil microorganisms, terrestrial plants and, marine life forms. According to the researchers, SMART (Small Molecule Accurate Recognition) “has the potential to accelerate the molecular structure identification process ten-fold. This development could represent a paradigm shift…Powered by Discourse, best viewed with JavaScript enabled"
491,major-updates-to-nvidia-ai-software-advancing-speech-recommenders-inference-and-more-announced-at-nvidia-gtc-2022,"Originally published at:			https://developer.nvidia.com/blog/major-updates-to-nvidia-ai-software-advancing-speech-recommenders-inference-and-more-announced-at-nvidia-gtc-2022/
At GTC 2022, NVIDIA announced Riva 2.0, Merlin 1.0, new features to NVIDIA Triton, and more.Powered by Discourse, best viewed with JavaScript enabled"
492,identifying-network-and-storage-issues-with-nvidia-advanced-streaming-telemetry,"Originally published at:			https://developer.nvidia.com/blog/identifying-network-and-storage-issues-with-advanced-streaming-telemetry/
NVIDIA What Just Happened (WJH) is a hardware-accelerated telemetry technology where the switch ASIC holds onto important parts of dropped packets. WJH helps you quickly find the root causes of network performance problems.Powered by Discourse, best viewed with JavaScript enabled"
493,nvidia-dlss-sdk-now-available-for-all-developers-with-linux-support-unreal-engine-5-plugin-and-new-customizable-options,"Originally published at:			https://developer.nvidia.com/blog/nvidia-dlss-sdk-now-available-for-all-developers-with-linux-support-unreal-engine-5-plugin-and-new-customizable-options/
NVIDIA has made Deep Learning Super Sampling (DLSS) easier and more flexible than ever for developers to access and integrate in their games. The latest DLSS SDK update (version 2.2.1) enables new user customizable options, delivers Linux support and streamlines access.The file dlss-2.2.1_updated.zip is broken. Tried to download it twice with the same result. infozip can’t open it, 7z can but it fails partway through:Testing archive: dlss-2.2.1_updated.zipERRORS:
Unexpected end of archive–
Path = dlss-2.2.1_updated.zip
Type = zip
ERRORS:
Unexpected end of archive
Physical Size = 371002040ERROR: CRC Failed : ngx_dlss_demo_windows.zipFile  size 359071744 bytes, MD5 checksum 4d51f61824eba9887e50e681b1353456The file has been updated. Please re-download.@turol Good catch!Works now.Are there plans to add DLAA to the SDK and/or the UE5 plugin?We are currently evaluating DLAA and collecting feedback on developer interest. Your feedback is welcome!Powered by Discourse, best viewed with JavaScript enabled"
494,pruning-models-with-nvidia-transfer-learning-toolkit,"Originally published at:			Pruning Models with NVIDIA Transfer Learning Toolkit | NVIDIA Technical Blog
It’s important for the model to make accurate predictions when using a deep learning model for production. How efficiently these predictions happen also matters. Examples of efficiency measurements include electrical engineers measuring energy consumption to pick the best voltage regulator, mechanical engineers timing inference latency to determine motor control loop maximum speed, embedded software engineers needing…Powered by Discourse, best viewed with JavaScript enabled"
495,upcoming-event-session-on-vision-ai-at-the-edge-from-zero-to-deployment-using-low-code-development-at-embedded-vision-summit,"Originally published at:			Vision AI At the Edge: From Zero to Deployment Using Low-Code Development (NVIDIA Tools, Part 2) - 2022 Summit
In this Embedded Vision Summit session where we’ll showcase a low-code approach to develop fully optimized and accelerated vision AI applications using DeepStream SDK and graph composer.Powered by Discourse, best viewed with JavaScript enabled"
496,linking-opengl-for-server-side-rendering,"Originally published at:			Linking OpenGL for Server-Side Rendering | NVIDIA Technical Blog
Visualization is a great tool for understanding large amounts of data, but transferring the data from an HPC system or from the cloud to a local workstation for analysis can be a painful experience. It’s increasingly popular to avoid the transfer by analyzing and visualizing data in situ: right where it is generated. Moreover, using…Powered by Discourse, best viewed with JavaScript enabled"
497,announcing-the-nvidia-nvtabular-open-beta-with-multi-gpu-support-and-new-data-loaders,"Originally published at:			https://developer.nvidia.com/blog/announcing-the-nvtabular-open-beta-with-multi-gpu-support-and-new-data-loaders/
Recently, NVIDIA CEO Jensen Huang announced updates to the open beta of NVIDIA Merlin, an end-to-end framework that democratizes the development of large-scale deep learning recommenders. With NVIDIA Merlin, data scientists, machine learning engineers, and researchers can accelerate their entire workflow pipeline from ingesting and training to deploying GPU-accelerated recommenders (Figure 1). While NVIDIA Merlin…Powered by Discourse, best viewed with JavaScript enabled"
498,nsight-aftermath-sdk-2019-1-launching-today,"Originally published at:			Nsight Aftermath SDK 2019.1 Launching Today | NVIDIA Technical Blog
A consistent challenge when working with D3D12/DXR or Vulkan is having to manage GPU crashes that are difficult to track down and resolve.  Consider this scenario: A programmer inadvertently induces a TDR (GPU timeout) by causing a shader to execute an infinite loop or by attempting to access a deleted resource. This TDR forces a…Powered by Discourse, best viewed with JavaScript enabled"
499,how-to-optimize-self-driving-dnns-with-tensorrt,"Originally published at:			https://developer.nvidia.com/blog/optimize-self-driving-dnns-tensorrt/
Register for our upcoming webinar to learn how to use TensorRT to optimize autonomous driving DNNs for robust AV development.Powered by Discourse, best viewed with JavaScript enabled"
500,pytorch-releases-pytorch-hub-includes-nvidia-tacotron2-and-waveglow-models,"Originally published at:			PyTorch Releases PyTorch Hub, Includes NVIDIA Tacotron2 and WaveGlow Models | NVIDIA Technical Blog
At ICML 2019, PyTorch released PyTorch Hub, a repository of pre-trained models designed specifically for reproducible research. PyTorch is one of the most widely used deep learning frameworks by researchers and developers. PyTorch 1.0, announced by Facebook in 2018, is a deep learning framework that powers numerous products and services at scale by merging the…Powered by Discourse, best viewed with JavaScript enabled"
501,fitbit-for-cows-uses-deep-learning-to-provide-insights-for-dairy-farmers,"Originally published at:			https://developer.nvidia.com/blog/fitbit-for-cows-uses-deep-learning-to-provide-insights-for-dairy-farmers/
To meet the demand of the world’s growing population, farmers need to improve the productivity of their herds. Amsterdam-based Connecterra recently raised nearly $2 million to further develop their GPU-accelerated deep learning solution that consists of a wearable device that monitors each animal in the herd and transmits the data to a cloud platform for…Powered by Discourse, best viewed with JavaScript enabled"
502,ai-can-predict-the-future-location-of-vehicles,"Originally published at:			AI Can Predict the Future Location of Vehicles | NVIDIA Technical Blog
Researchers from Honda, the University of Michigan, and Indiana University developed a deep learning system that can predict the trajectories of vehicles at road intersections. “Safe driving requires not just accurately identifying and locating nearby objects, but also predicting their future locations and actions so that there is enough time to avoid collisions,” the researchers…Powered by Discourse, best viewed with JavaScript enabled"
503,upcoming-webinar-kick-start-deep-learning-on-the-cloud,"Originally published at:			Event Registration
Join experts from NVIDIA and Microsoft on November 30 for the latest developments in deep learning through hands-on demos and practical guidance for getting started on the cloud.Powered by Discourse, best viewed with JavaScript enabled"
504,ai-helps-verify-id-documents,"Originally published at:			AI Helps Verify ID Documents | NVIDIA Technical Blog
Researchers at the University of Michigan developed a deep learning based-system that performs real-time facial recognition and verifies the photo against the corresponding passport and government-issued IDs. The method has the potential to help law enforcement prevent fraud, could serve as an alternative payment method, and could also keep known criminals from entering a sensitive…Powered by Discourse, best viewed with JavaScript enabled"
505,deepsig-deep-learning-for-wireless-communications,"Originally published at:			DeepSig: Deep Learning for Wireless Communications | NVIDIA Technical Blog
Figure 1. The channel autoencoder matches the optimal known solution for BPSK in an AWGN channel The complexity of wireless system design is continually growing. Communications engineering strives to further improve metrics like throughput and interference robustness while simultaneously scaling to support the explosion of low-cost wireless devices. These often-competing needs makes system complexity intractable.…Powered by Discourse, best viewed with JavaScript enabled"
506,enabling-gpu-acceleration-in-near-realtime-ran-intelligent-controllers,"Originally published at:			https://developer.nvidia.com/blog/enabling-gpu-acceleration-in-near-realtime-ran-intelligent-controllers/
The time that it took to discover the COVID-19 vaccine is a testament to the pace of innovation in the healthcare industry. Pace of innovation can be directly linked to the thriving innovator ecosystem and the large number of AI-based healthcare startups. In comparison, the 5G wireless industry takes approximately a decade to introduce next…Powered by Discourse, best viewed with JavaScript enabled"
507,gtc-2020-tensorrt-inference-with-tensorflow-2-0,"GTC 2020 S22408
Presenters: Jonathan Dekhtiar,NVIDIA; Tamas Bela Feher,NVIDIA; Xuan Vinh Nguyen,NVIDIA
Abstract
NVIDIA TensorRT is a platform for high-performance deep learning inference. We’ll describe how TensorRT is integrated with TensorFlow and show how combining the two improves the efficiency of machine-learning models while retaining the convenience and ease-of-use of a TF Python development environment. We’ll provide updates for the TF 2.0 TRT interface, C++ API, dynamic shape support, and latest performance benchmarking.Watch this session
Join in the conversation below.Hi and thank you for the presentation.At 17:00, the slide mentions TF-TRT inference in C++ where you can port a TF model to a C++ environment. However, the github page contains an example with a TF1.14 docker container. This is supposed to be a TRT for TF2.+ talk…
Is there any progress on doing TF2.0 → standalone C++ engine conversion?link: https://github.com/tensorflow/tensorrt/tree/r1.14%2B/tftrt/examples/cpp/image-classificationThanksThanks Raphael, that’s correct. The current C++ example is for 1.14. We will work on updating it to 2.x and update you soon. Stay tuned!Great, thanks! Looking forward to it.I would lie to know if TF native (AMP + XLA) works for any type of Deep learning architecture. In the presentation, Natural language processing has been mentionned. But what about Computer Vision?
Also, do AMP + XLA related to the TRAX project?Thank youPowered by Discourse, best viewed with JavaScript enabled"
508,a-comprehensive-guide-on-interaction-terms-in-time-series-forecasting,"Originally published at:			https://developer.nvidia.com/blog/a-comprehensive-guide-on-interaction-terms-in-time-series-forecasting/
Learn about interaction terms in the context of time series forecasting and how to effectively implement interaction terms in your models.Powered by Discourse, best viewed with JavaScript enabled"
509,explainer-what-are-graph-neural-networks,"Originally published at:			What Are Graph Neural Networks? | NVIDIA Blogs
GNNs apply the predictive power of deep learning to rich data structures that depict objects and their relationships as points connected by lines in a graph.Powered by Discourse, best viewed with JavaScript enabled"
510,cuda-spotlight-women-and-cuda,"Originally published at:			https://developer.nvidia.com/blog/cuda-spotlight-women-cuda/
Today we are pleased to launch the new Women and CUDA website. We received a wide variety of entries from around the world, representing professors, students, researchers, scientists and domain experts. We had recognized several participants earlier as CUDA Spotlights, including Valerie Halyo of Princeton and Monica Syal of Advanced Rotorcraft Technology. CUDA Fellow Lorena Barba…Powered by Discourse, best viewed with JavaScript enabled"
511,training-a-text2sparql-model-with-mk-squit-and-nemo,"Originally published at:			https://developer.nvidia.com/blog/training-a-text2sparql-model-with-mk-squit-and-nemo/
Across several verticals, question answering (QA) is one of the fastest ways to deliver business value using conversational AI. Informally, QA is the task of mapping a user query to an answer from a given context. This open-ended definition is best understood through an example: Question: Where are the headquarters of NVIDIA?Context: “NVIDIA Corporation (en-VID-ee-\u0259)…Powered by Discourse, best viewed with JavaScript enabled"
512,gtc-2020-distributed-training-and-fast-inter-gpu-communication-with-nccl,"GTC 2020 S21107
Presenters: Sylvain Jeaugey,NVIDIA
Abstract
NCCL, NVIDIA Collective Communication Library, is used by all Deep Learning frameworks to distribute computing on multiple GPUs, allowing users to train very large networks in minutes instead of weeks. In this session, we will present how NCCL combines hardware technologies such as NVLink, PCI, Ethernet and Infiniband to achieve maximum speed for inter-GPU communication. We will detail how those technologies compare, and how much of a difference they make for users. We will also detail how we continue to innovate to accelerate distributed GPU computing and support new models.Watch this session
Join in the conversation below.Powered by Discourse, best viewed with JavaScript enabled"
513,top-5-ai-speech-applications-using-nvidia-s-gpus-for-inference,"Originally published at:			Top 5 AI Speech Applications Using NVIDIA’s GPUs for Inference | NVIDIA Technical Blog
AI-enabled services such as speech recognition and natural language processing are increasing in demand. To help developers manage growing datasets, latency requirements, customer requirements, and more complex neural networks, we are highlighting a few AI speech applications that rely on NVIDIA’s inference platform to solve common AI speech challenges.  From Amazon’s Alexa Research group enhancing…Powered by Discourse, best viewed with JavaScript enabled"
514,from-earth-sciences-to-factory-production-gpu-hackathon-optimizes-modeling-results,"Originally published at:			https://developer.nvidia.com/blog/from-earth-sciences-to-factory-production-gpu-hackathon-optimizes-modeling-results/
A recent GPU Hackathon in Taiwan helped 12 teams advance their HPC and AI projects, using innovative technologies to address pressing global challenges.Powered by Discourse, best viewed with JavaScript enabled"
515,nvidia-research-presenting-20-papers-at-neurips-2021,"Originally published at:			NVIDIA Research Presents 20 Papers at NeurIPS 2021 | NVIDIA Technical Blog
At the forefront of AI innovation, NVIDIA continues to push the boundaries of technology in machine learning, self-driving cars, robotics, graphics, and more.Powered by Discourse, best viewed with JavaScript enabled"
516,cuda-pro-tip-nvprof-is-your-handy-universal-gpu-profiler,"Originally published at:			https://developer.nvidia.com/blog/cuda-pro-tip-nvprof-your-handy-universal-gpu-profiler/
CUDA 5 added a powerful new tool to the CUDA Toolkit: nvprof. nvprof is a command-line profiler available for Linux, Windows, and OS X. At first glance, nvprof seems to be just a GUI-less version of the graphical profiling features available in the NVIDIA Visual Profiler and NSight Eclipse edition. But nvprof is much more than…Hello and thanks for the CUDA posts.I wanted to ask you.I used the nvprof in the example (saxpy http://devblogs.nvidia.com/... and it gives me 7 registers.When I use --ptxas it gives me 3 registers.Can you explain me that?Thank you!Hi George, thanks for your comment. I think I need more details to help.  What exact command line are you using to compile?  And what GPU / compute capability are you running on?Hello,I am running on 2.1 compute capability.I used the commandsnvprof --print-gpu-trace ./run --benchmark -i=1andnvcc -o run test.cu --ptxas-options=-vYou are using the default architecture, which is sm_10.  On sm_10, the code uses 3 registers. But your binary also includes PTX, which is JITed at load time to sm_21 when you run on your CC 2.1 GPU. See this pro tip: http://devblogs.nvidia.com/...sm_21 requires more registers for the same code (but also has a larger register file).When I run this:nvcc -arch=sm_21 -o run saxpy.cu --ptxas-options=-vI see this output:c:\src\test>nvcc -arch=sm_21 -o run saxpy.cu --ptxas-options=-vptxas : info : 0 bytes gmemptxas : info : Compiling entry function '_Z5saxpyifPfS_' for 'sm_21'ptxas : info : Function properties for _Z5saxpyifPfS_0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loadsptxas : info : Used 6 registers, 56 bytes cmem[0]   Creating library run.lib and object run.expSo 6 registers. However, running in nvprof still shows 7 registers.  I'm not sure about the cause of this discrepancy but I will file a bug!  Thanks!Ok!Same output here.So, I must always use the sm_21 (for 2.1capability).So, --ptxas and nvprof must give the same results always?Thank you!You don't have to explicitly specify the arch version (sm_21), but if you want full control over what code is generated you might want to. I recommend you read my post linked above about fat binaries and JIT linking.As I wrote I think the profiler *should* match the ptxas output, so I have filed an issue internally to figure that out.Ok,thank you!I got the answer.  To support profiling (for example of concurrent kernels), the profiler has to patch kernel code with some additional instructions, sometimes consuming extra registers. So in this case it uses an extra register.  You can verify this by runningnvprof --print-gpu-trace --concurrent-kernels-off ./runThis disables profiling of concurrent kernels (not needed for this app), and you will see the register count drop to 6.Ok!Thanks for the tip!Hello, I have one question about CSV file.When I run nvpro --csv my.exe in the windows system.I can't find my csv file in tmp floder.Can I set the path for my csv file?how can I do it ?HelloI see that nvprof can even profile the number of flop in the kernel (using the parameters as below). Also when I browse through the documentation (here  http://docs.nvidia.com/cuda... it says flop_count_sp is ""Number of single-precision floating-point operations executed by non-predicated threads (add, multiply, multiply-accumulate and special). Each multiply-accumulate operation contributes 2 to the count."" However when I run, the result of flop_count_sp (which is supposed to be  flop_count_sp_add + flop_count_sp_mul + flop_count_sp_special + 2 * flop_count_sp_fma) but in my case I find that it does not include in the summation the value of ""flop_count_sp_special"".Could you suggest me what I am supposed to use? Should I add this value to the sum of flop_count_sp or I should consider the formula does not include the value of ""flop_count_sp_special""?Also could you please tell me what are these special operations?nvprof --metrics flops_sp --metrics flops_sp_add --metrics flops_sp_mul --metrics flops_sp_fma --metrics flops_sp_special myKernel argsWhere myKernel is the name of my CUDA kernel which has some input arguments given by args.Many Thanks in advanceSpecial functions are things like sin(), cos(), exp(), etc. If you are not using these in your code, then that explains why they would not be included. If you do use them, then I'm not sure what is wrong, but would need to see an example code that demonstrates the problem (you can share via GitHub Gist if you like).Thanks for clearing my ignorance. Of course I do not have these special functions in my kernel.Could you please make me aware  what other functions apart from functions like sin(), cos(), etc could contribute the values for ""flop_count_sp_special""?As per your suggestion I have shared my code via Mercurial Hub in BitBucket.org ( https://bitbucket.org/rajgu... ).Thanks in Advance.Basically these: http://docs.nvidia.com/cuda...Thank you, that was eye opening.Can we output register value on the console in PTX assembly? Furthermore is there any way to get register addresses in PTX assembly?Any help will be appreciated.ThanksI don't follow your questions. Can you clarify?Thanks for your reply mark. Basically i am currently working on a research project in which i need to access the registers value and their addresses in order to recognize the pattern of their access in CUDA. That's why i need to print these values on the console or write them in a file. In short is there any way to use print or write command in PTX assembly. Thank YouThis isn't something the profiler supports.  You can use inline PTX in the kernel code to write the values of specific registers to memory. Registers don't have ""addresses"", but you could write the register names with their values, since the names are explicit in your inline PTX. http://docs.nvidia.com/cuda...Powered by Discourse, best viewed with JavaScript enabled"
517,gtc-2020-learning-to-estimate-object-viewpoint-without-labels,"GTC 2020 S22193
Presenters: Shalini De Mello,NVIDIA; Siva Mustikovela,University of Heidelberg
Abstract
Learning-based methods for viewpoint estimation of object categories (for example, faces or cars) require many images with labeled viewpoints. Viewpoint annotations are cumbersome to acquire and often contain errors. On the other hand, it is relatively easy to mine large collections of unlabelled images of a category from the internet. We investigate whether such image collections can be used to successfully train viewpoint-estimation networks purely via self-supervision, where the only ground-truth label available is the image itself. We design a framework that leverages the analysis-by-synthesis paradigm and couples the viewpoint network with a viewpoint-aware synthesis network to supervise it. We additionally propose various losses that enforce symmetry, realism, and better disengagement of the latent space of the image synthesizer to further supervise the viewpoint network. For faces, cars, buses, and trains, our technique performs competitively to the existing fully-supervised approaches.Watch this session
Join in the conversation below.Powered by Discourse, best viewed with JavaScript enabled"
518,identifying-and-tracking-individual-coconut-crabs-with-ai,"Originally published at:			https://developer.nvidia.com/blog/identifying-and-tracking-individual-coconut-crabs-with-ai/
The worldwide coconut crab population is in decline, and to get an accurate count of how many remain, researchers in Japan developed a deep learning system to identify and track specific coconut crabs individually. Coconut crabs are among the biggest crabs in the world weighing up to four pounds and measuring nearly three feet long. …Powered by Discourse, best viewed with JavaScript enabled"
519,speedy-model-training-with-rapids-determined-ai,"Originally published at:			https://developer.nvidia.com/blog/speedy-model-training-with-rapids-determined-ai/
Model developers no longer face a steep learning curve to accelerate model training. By utilizing two open-source software projects, Determined AI’s Deep Learning Training Platform and the RAPIDS accelerated data science toolkit, they can easily achieve up to 10x speedups in data preprocessing and train models at scale.  Making GPUs accessible As the field of…Powered by Discourse, best viewed with JavaScript enabled"
520,calling-cuda-accelerated-libraries-from-matlab-a-computer-vision-example,"Originally published at:			https://developer.nvidia.com/blog/calling-cuda-accelerated-libraries-matlab-computer-vision-example/
In an earlier post we showed how MATLAB® can support CUDA kernel prototyping and development by providing an environment for quick evaluation and visualization using the CUDAKernel object. In this post I will show you how to integrate an existing library of both host and device code implemented in C++ or another CUDA-accelerated language using MEX.…Note that with the recent release of MATLAB R2015b comes a new tool called MEXCUDA. This function displaces the use of MEX for CUDA code and eliminates the need to copy an options file or specify 'largeArrayDims'. See the documentation: http://www.mathworks.com/he....Powered by Discourse, best viewed with JavaScript enabled"
521,gtc-2020-supercharging-adobe-dimension-with-rtx-enabled-gpu-raytracing,"GTC 2020 S22485
Presenters: Marcos Slomp ,Adobe ; Krishna Mullia,Adobe
Abstract
Adobe Dimension enables users to bring their designs to life in 3D. The application recently gained significant user experience improvements with the release of a new RTX-enabled GPU renderer. This new renderer was developed to match the feature set and quality of our reference CPU renderer, with support for high-quality models, materials, and lighting. We’ll go through some of the goals, design choices, and challenges that we encountered while building the renderer with OptiX.Watch this session
Join in the conversation below.Powered by Discourse, best viewed with JavaScript enabled"
522,data-science-top-new-resources-from-gtc-21,"Originally published at:			https://developer.nvidia.com/blog/data-science-top-new-resources-from-gtc-21/
Accelerated data science can dramatically boost the performance of end-to-end analytics workflows, speeding up value generation while reducing cost. Learn how companies like Spotify and Walmart use NVIDIA-accelerated data science.Powered by Discourse, best viewed with JavaScript enabled"
523,gtc-2020-object-recognition-and-tracking-utilizing-millimeter-wave-radar-by-deep-neural-networks,"GTC 2020 S21188
Presenters: Tokihiko Akita,Toyota Technological Institute
Abstract
All-weather sensors are necessary for autonomous-driving Level 3 and higher. Millimeter-wave radar is the most robust sensor for adverse weather. However, the signal is noisy and fluctuated, and the resolution is low. Thus, recognition using the radar is difficult. Deep-learning algorithms are an effective solution. We’ll show a method to classify and track objects in driving scenes with a high-resolution millimeter-wave radar applying long short-term memory. We designed and compared various types of input features and LSTM for our measured dataset and achieved high accuracy through cross validation. We’ll also show a method to reconstruct shapes of parking spaces and cars with convolutional neural networks. Parking cars were scanned with side radar. The reflection signals were accumulated, and the shape was estimated by semantic segmentation framework, applying CNN for the ground-truth shape, annotated by a lidar.Watch this session
Join in the conversation below.Powered by Discourse, best viewed with JavaScript enabled"
524,enhancing-architectural-3d-modeling-collaboration-with-universal-scene-description,"Originally published at:			https://developer.nvidia.com/blog/enhancing-architectural-3d-modeling-collaboration-with-universal-scene-description/
Learn about the possibilities and benefits of using USD with NVIDIA Omniverse for architecture, design, engineering, and construction projects.Powered by Discourse, best viewed with JavaScript enabled"
525,the-authoritative-book-on-real-time-ray-tracing-has-arrived,"Originally published at:			The Authoritative Book on Real-Time Ray Tracing Has Arrived | NVIDIA Technical Blog
In 2018, with the release of NVIDIA’s RTX series GPUs, real-time ray tracing finally became accessible to game developers, content creators, and consumers. It is a technology that will forever change graphics processing. To help developers navigate this new technology, a wide-ranging book on the topic is being published early this year: Ray Tracing Gems.…Powered by Discourse, best viewed with JavaScript enabled"
526,exploiting-nvidia-ampere-structured-sparsity-with-cusparselt,"Originally published at:			https://developer.nvidia.com/blog/exploiting-ampere-structured-sparsity-with-cusparselt/
Deep neural networks achieve outstanding performance in a variety of fields, such as computer vision, speech recognition, and natural language processing. The computational power needed to process these neural networks is rapidly increasing, so efficient models and computation are crucial. Neural network pruning, removing unnecessary model parameters to yield a sparse network, is a useful…Hi,Does the cuSPARSELt also support the SM86 for RTX3090 GPU?
Currently, I only see it limits its support for SM80.
How could I leverage CuSPARSELt on SM86?Hi Daniel,Yes, cuSPARSELt is currently limited to SM80. The support for SM86 will be added soon.Ok, Thanks.I recently found the example of the sparse Tensorcore GEMM example (15_ampere_sparse_tensorop_gemm) on CUTLASS.However,  it seems that it only supports INT4 input and int32 output on SM86, when I change the data type to float or half or int8 as the input, it can successfully compile but always fail to launch during the initialization of GEMM object.Is there any reason or solution for this?  Thanks!sorry, I’m not involved in the CUTLASS project. My suggestion is to use the official Github issue panel for your question.Similar question has been asked before.  Please check the bottom half of Issue 103 in cutlass github website (I don’t know why I am not allowed to post a link here)If you still have question, feel free to ask in cutlass issues.  We try to answer every questions there in time.Hi guys. I am currently exploring the new sparse tensor core feature on Ampere GPU (RTX 3090). I use cudaEvent related APIs to measure the execution time of cusparseLtMatmul and cublasLtMatmul, but the result of dense and sparse GEMM is very similar. The detail of experiment configuration and result are displayed bellow. Could anyone give me some advice?Execution time of dense and sparse kernel (time unit: ms):Code:dense GEMM implemented by cuBLASLt
sparse GEMM implemented by cuSPARSELtHow I record execution time:My Env:
GPU Type : RTX 3090
Nvidia Driver Version : 460
CUDA Version : 11.3
CUDNN Version : 8.0.4
Operating System + Version : Ubuntu 18.04
Python Version (if applicable) : 3.6Thanks very much!Sorry for the delay. We are investigating the issue. Can you please provide the output of nvidia-smi -a? Also, did you change some parameters outside matrix sizes in the examples? e.g. precisions, layouts, etc.The figures of this blog no longer load.
Is it just for me, or anyone else?@vinhn  – Thanks for the heads-up! I fixed the broken images. Let me know if you see any other problems with our posts.Hi guys, I am currently working on sparse matrix-vector multiplication (SPMV). Due to Tensor cores promise high performance in GEMM operations, I am curious if there is a way to use Tensor cores in SPMV operation. I know that currently cuSPARSELt is only working for Sparse GEMM operation. Is there a way to use tensor cores for SPMV, if so do the dimesions have to be multiples of 4?Powered by Discourse, best viewed with JavaScript enabled"
527,gtc-2020-under-the-hood-of-the-new-dgx-a100-system-architecture,"GTC 2020 S21884
Presenters: Alex Ishii,NVIDIA; Nick Whidden, NVIDIA
Abstract
Deep dive into the new DGX A100, NVIDIA’s new Deep Learning Supercomputer with 8x A100 GPUs (Based on NVIDIA Ampere microarchitecture). Presenting system hardware architecture, feeds and speeds, new features and capabilities. Key performance metrics, and a summary of observed performance on DL/mL/HPC benchmark applications. Basic familiarity with server architectures, server components, and application-performance metrics assumed, and some familiarity with previous NVIDIA DGX server products a plus.Watch this session
Join in the conversation below.Powered by Discourse, best viewed with JavaScript enabled"
528,what-to-do-with-all-that-bandwidth-gpus-for-graph-and-predictive-analytics,"Originally published at:			https://developer.nvidia.com/blog/gpus-graph-predictive-analytics/
Figure 1: Graph algorithms exhibit non-locality and data-dependent parallelism. Large graphs, such as this map of the internet, represent billion-edge challenges to existing hardware architectures. Did you see the White House’s recent initiative on Precision Medicine and how it is transforming the ways we can treat cancer? Have you avoided clicking on a malicious website based…Increasing memory bandwidth with GPU technology -- just the ticket for all the intensive analytics coming down the pike.Thanks for a great overview of the graph analytics using GPUs. The article is quite informative and summarizes quite in detail the present state of affairs in GPU-based graph analytics.I personally am trying to explore a research topic in the domain of graph-based predictive algorithms especially in the domain of predicting the spread of epidemic in a region. I intend to use CUDA to simulate the results. Can anyone provide an insight into the suitable areas in which one can research in this regard.Powered by Discourse, best viewed with JavaScript enabled"
529,scaling-keras-model-training-to-multiple-gpus,"Originally published at:			https://developer.nvidia.com/blog/scaling-keras-training-multiple-gpus/
Keras is a powerful deep learning meta-framework which sits on top of existing frameworks such as TensorFlow and Theano. Keras is highly productive for developers; it often requires 50% less code to define a model than native APIs of deep learning frameworks require (here’s an example of LeNet-5 trained on MNIST data in Keras (71 lines) and TensorFlow…The following code works for me. Simply drop the code in and it's good to go. As far as I can tell it only scales on a single box, not a distributed cluster.https://github.com/kuza55/k...https://github.com/fchollet...https://medium.com/@kuza55/...@nor_he:disqus Yes, these approaches do work. In fact, I did mention this in the blog post. Recall the following sentence:""Keras’s official blog also demonstrates that by breaking the backend-independent abstraction and exposing TensorFlow’s multi-GPU primitives, it’s possible to get Keras to scale. ""There was also a hyperlink associated with the above: https://blog.keras.io/keras...The main problem I see here is that this requires the user to mix the interface (Keras) with the implementation (""with tf.device()"" wrappers), which effectively makes the code non-portable to other backends.The simplest analogy that I can see here is the SLF4J logging wrapper (Simple Logging Facade for Java). The user only need to concern themselves with writing API calls using the facade, and can keep swapping backends (log4j, java.util.logging, logback) at will. Also thinking in terms of object-oriented programming, having an interface shouldn't require the user to look into the implementation, which ""with tf.device()"" requries us to do.Granted, the GPU list provided to  Keras's model.compile() only works in the MxNet Keras fork now, but I would argue that this is the right approach. The  change has been made at the interface level, which will hopefully soon become absorbed into mainstream Keras, and it's the Keras backends' job to detemine how to  make multi-GPU data parallelism happen. That way one can have one abstraction that's stable, and can swap out the backends while maintaining a portable multi-GPU solution.The second issue I mentioned in my post is that TensorFlow performance varies widely, e.g. the difference between this ResNet-50 tutorial implementation (https://github.com/tensorfl... and this performance-optimized implementation (https://github.com/tensorfl... is about 2x on a DGX-1. Since TensorFlow has many ways of doing the same thing, it can't always be expected that a highly abstract implementation will be fast, even if the backend can support that. MxNet's approach is usually to have one ""mainstream"" way of doing a particular thing, so it is easier to map the Keras frontend to the MxNet backend while keeping performance reasonably close to that of the native API.I encourage you to try both backends and see what performance differences you get.What I didn't really get:Is the performance issue of Keras for single GPUs mostly based on the need for efficient data pipelines (could also just use Dataflow from Tensorpack then) or is it primarily based on how Keras builds a network internally?Framework-native Keras input is currently also in the works:https://github.com/fchollet...It's actually both a pipelines and a backend issue.The pipeline is certainly quite important, but it probably matters more for multiple GPUs. The single-GPU performance may still be affected by the pipeline, particularly in case of large GPUs (e.g. Tesla GP100, as opposed to say a GeForce GTX 1070) - one needs to provide the data quickly for the GPU not to stall waiting on data.Regarding the backend, Keras constructs the TF graph very differently than a highly optimized TF graph. It seems to me that the Keras backend for TF uses high-level ops that are not particularly efficient, and to get full perf in TF, one has to use low-level ops such as the ""staging area"" node, which eessentially does explicit data prefetch (which is implicit in other frameworks, rather than part of the graph definition). See here, for example (https://github.com/tensorfl.... I provided links to the ""unoptimized"" and ""optimized"" TF ResNet-50 implementations above, that should be helpful. The interested reader can run these examples and note the significant perf difference, even using TF's native API. Since Keras is not tuned to any particular network but is rather a general abstraction, perhaps using all of the optimized ops is non-trivial (unless the graph is optimized by TF after construction, during optimization passes). That said, some aspects such as handling data prefetch are probably common to all models.The backend issue involves many details, such as the fact that the original Keras was based on Theano, which had the NCHW image layout, while TF's native layout is NHWC. cuDNN convolution kernels are NCHW (up to Volta), and if the layout conversion requires many transposes, that  will come at a significant cost (transposes are very quick, they are bandwidth-bound, but there is a kernel launch overhead to perform them, unless the transpose is fused with some other kernel that will use the transposed data).IMHO the TF backend for Keras can definitely be improved to be competitive. I heard that this is already in the works, beyond the framework-native input pipeline component.Unfortunately, Keras is quite slow in terms of single-GPU training and inference time (regardless of the backend). It is also hard to get it to work on multiple GPUs without breaking its framework-independent abstraction.Powered by Discourse, best viewed with JavaScript enabled"
530,just-released-nvcomp-v2-3,"Originally published at:			nvcomp/CHANGELOG.md at main · NVIDIA/nvcomp · GitHub
The CUDA library, nvCOMP, now offers support for zStandard and Deflate standards as well as modified-CRC32 checksum support, and improved ANS performance.Powered by Discourse, best viewed with JavaScript enabled"
531,watch-the-nvidia-gtc-2020-keynote,"Originally published at:			https://developer.nvidia.com/blog/watch-the-nvidia-gtc-2020-keynote/
NVIDIA founder and CEO Jensen Huang kicks off the company’s first “kitchen keynote” to announce the latest innovations in AI, HPC, robotics, graphics, healthcare, data science, and more.Powered by Discourse, best viewed with JavaScript enabled"
532,validating-active-sensors-in-nvidia-drive-sim,"Originally published at:			https://developer.nvidia.com/blog/validating-active-sensors-in-nvidia-drive-sim/
Autonomous vehicle development is all about scale. Engineers must collect and label massive amounts of data to train self-driving neural networks.  This data is then used to test and validate the AV system, which is also an immense undertaking to ensure robustness.  Simulation is an important tool to reach this level of scale, but accuracy…Powered by Discourse, best viewed with JavaScript enabled"
533,the-force-is-strong-with-nvidia-jetson,"Originally published at:			The Force is Strong with NVIDIA Jetson | NVIDIA Technical Blog
This post features winners from the NVIDIA sponsored contest with Make: where makers submit their best robotics projects with a galactic theme.Great to know ! Some of paper is state of the art tech. Thanks for sharing.
Cheers!Powered by Discourse, best viewed with JavaScript enabled"
534,gtc-2020-minecraft-with-rtx-crafting-a-real-time-path-tracer-for-gaming,"GTC 2020 S22678
Presenters: Jakub Boksansky,NVIDIA; Oli Wright, NVIDIA
Abstract
Engineers from NVIDIA and Microsoft cover the technical aspects of adding path tracing to the hugely popular Minecraft.  We’ll walk through the technology and engineering that went into creating the RTX version of Minecraft; a glimpse at the technical decisions and process involved in the NVIDIA/Microsoft collaboration, and a deep dive into the details of the ray tracing and denoising.Watch this session
Join in the conversation below.Powered by Discourse, best viewed with JavaScript enabled"
535,ai-startup-aims-to-redefine-how-people-interact-with-technology,"Originally published at:			AI Startup Aims To Redefine How People Interact with Technology | NVIDIA Technical Blog
TwentyBN is a German/Canadian based AI startup that is using computer vision to redefine how people interact with technology.  At NeurIPS in Montreal, Canada this week the company demonstrated a brand new computer vision based-avatar named Millie that is designed for retail and entertainment applications. “Millie is a context-aware avatar that we’re showing for the…Powered by Discourse, best viewed with JavaScript enabled"
536,scientists-develop-3d-simulation-of-a-living-cell,"Originally published at:			https://developer.nvidia.com/blog/scientists-develop-3d-simulation-of-a-living-cell/
Powered by Discourse, best viewed with JavaScript enabled"
537,accelerating-tensorflow-on-nvidia-a100-gpus,"Originally published at:			Accelerating TensorFlow on NVIDIA A100 GPUs | NVIDIA Technical Blog
The NVIDIA A100, based on the NVIDIA Ampere GPU architecture, offers a suite of exciting new features: third-generation Tensor Cores, Multi-Instance GPU (MIG) and third-generation NVLink. Ampere Tensor Cores introduce a novel math mode dedicated for AI training: the TensorFloat-32 (TF32). TF32 is designed to accelerate the processing of FP32 data types, commonly used in…Powered by Discourse, best viewed with JavaScript enabled"
538,fast-ai-data-preprocessing-with-nvidia-dali,"Originally published at:			Fast AI Data Preprocessing with NVIDIA DALI | NVIDIA Technical Blog
Training deep learning models with vast amounts of data is necessary to achieve accurate results. Data in the wild, or even prepared data sets, is usually not in the form that can be directly fed into neural network. This is where NVIDIA DALI data preprocessing comes into play. There are various reasons for that: Different…Powered by Discourse, best viewed with JavaScript enabled"
539,explainer-what-is-quantum-computing,"Originally published at:			What Is Quantum Computing? | NVIDIA Blog
Quantum computers, still in their infancy, are influencing a new generation of simulations already running on classical computers, and now accelerated with the NVIDIA cuQuantum SDK.Powered by Discourse, best viewed with JavaScript enabled"
540,isc-2020-vendor-showdown,"ISC 2020 disc07
Presenters: Marc Hamilton, NVIDIA; Gilad Shainer, NVIDIA
AbstractWatch this session
Join in the conversation below.Powered by Discourse, best viewed with JavaScript enabled"
541,cuda-pro-tip-how-to-call-batched-cublas-routines-from-cuda-fortran,"Originally published at:			https://developer.nvidia.com/blog/cuda-pro-tip-how-call-batched-cublas-routines-cuda-fortran/
CUDA Fortran for Scientists and Engineers shows how high-performance application developers can leverage the power of GPUs using Fortran. When dealing with small arrays and matrices, one method of exposing parallelism on the GPU is to execute the same cuBLAS call on multiple independent systems simultaneously. While you can do this manually by calling multiple cuBLAS…Great article! Just one question; is this possible without PGI's CUDA Fortran? I've been trying to implement these batched routines from Fortran, but I don't have access to the PGI compiler. I've been able to make some progress with Fortran interfaces to CUDA/cuBLAS library routines, but I can't get the batched routines to work.Thanks for the feedback and your question, Austin.To use batched CUBLAS routines from regular Fortran, you'd need to write CUDA C to code to manage the array of device pointers and then write some C stubs callable from Fortran.  Hello Greg. I am trying to compile a code using cublas from Fortran VS 2013 adding the linker -lcublas but I haven't been able to do it. Could you please give any recommedation on this? Thank you very much.Hi Ivonne:Rather than using ""-lcublas"" try adding ""cublas.lib"" to the link.  ""-lcublas"" looks for ""libcublas.lib"" which doesn't exist.Another option is to use ""-defaultlib:cublas""I hope one of these resolves your issue.- GregPowered by Discourse, best viewed with JavaScript enabled"
542,maximizing-unified-memory-performance-in-cuda,"Originally published at:			Maximizing Unified Memory Performance in CUDA | NVIDIA Technical Blog
Many of today’s applications process large volumes of data. While GPU architectures have very fast HBM or GDDR memory, they have limited capacity. Making the most of GPU performance requires the data to be as close to the GPU as possible. This is especially important for applications that iterate over the same data multiple times…Nice!  I have been waiting many years for this.  Do you also intend to make this hardware feature available via OpenCL?Thanks Nikolay! This article is very useful. I had a question. To clarify pushing device-host prefetches in a busy stream helps since it allows for prioritizing the host-device prefetches to use the CPU - since the CPU is only required for unmapping page table entries from the CPU. Is that understanding correct? Secondly, you mentioned that the Linux UVM driver is opensource - could you please provide a pointer to that?You said ""the input data (ptr) is allocated with cudaMallocManaged or cudaMallocHost and initially populated on the CPU."" But the CUDA programming guide said the data allocated via cudaMallocManaged is hosted in physical GPU storage. Any conflicts?Currently we do not have any plans of adding support for the feature for OpenCL.Data allocated via cudaMallocManaged is hosted initially in physical GPU memory only for pre-Pascal devices. On Pascal and beyond when cudaMallocManaged is called the data is not populated until first touch, so it could be on the CPU or the GPU. In my setup I write the data on the CPU to make sure it's resident in system memory before running any experiments. Thanks for reporting the issue with the programming guide, we'll fix the documentation.Yes, your understanding is correct. When pushing the device-to-host prefetches in a busy stream they will be deferred which allows you to submit host-to-device prefetches right away that can release the CPU sooner. In this case the two prefetches can be overlapped. If submitted to an idle stream, the device-to-host prefetches will use the CPU during the whole duration of the prefetch call and it won't be possible to launch prefetches in other direction.Regarding Unified Memory sources - please go here to download a public driver package: http://www.nvidia.com/objec.... You can then extract it via ""sh NV*.run -x"". Go to the kernel subdirectory within the extracted files directory, and you'll find all of the Unified Memory source code.This article is very helpful : ) Are we able to do this with regular malloc calls (heterogeneous memory management)? Is there a performance difference?You said that in this blog ""Note that the Linux Unified Memory driver is open source, so keen developers can review what happens under the hood"". Could you give me a link to find the source? Many thanks!When trying to overlap one-way host-to-device prefetch, you recommended we launch kernels first, then call the prefetch. This becuase host-to-device prefetch is not completely asynchronous ( the part of changing TLB within CPU is synchronous ). But isn't this under the assumption that the memory fetch will complete before the kernel accesses the unified memory region? How can we assume this? If we don't assume this, then there would be no point in prefetching because if kernel accesses the unified memory region before prefetch is complete, it will raise a page fault. Please share with me what you think. Thanks!Glad to hear that you found this useful. HMM driver is not production ready currently, so I don't have any performance numbers to report yet, but stay tuned for updates!Please see the first comment http://disq.us/p/1p7hvwb for instructions on how to obtain the sources.Hi Joong, your understanding is correct - if the kernel is trying to access memory that has not been prefetched, then it will generate page faults. The idea is to set up a pipeline that will chunk the dataset into A1, A2, ... parts, so in the first step 1) we prefetch A1, wait until it's done, then 2) submit the kernel working on A1, and right after that issue prefetching of A2; and continue for all the remaining chunks. Note that with cudaMemcpy you can submit it before or after the kernel, since it's completely asynchronous, but with the prefetches you need to create your pipeline in such a way that you're not blocked by the CPU work in the prefetching calls. Hope it helps!Hello Nikolay, thanks for your reply! Your explanations, especially how you detail about the return behavior and stream behavior of different prefetches, was REALLY helpful! Do you think you can explain such details regarding cudaMemcpyAsync ( host-to-device, device-to-host, device-to-device ) and how many memory transactions can occur in parallel ( in respect of the memory engines )? I have read all the posts from Nvidia regarding memcpy, but they don't go into as much detail as you do with prefetch.Thanks again!Hi Joong, sorry for delayed response. cudaMemcpyAsync works differently than cudaMemPrefetchAsync, since the former does not require any CPU work if the source and the destination are pinned/mapped buffers (in system memory or GPU memory). In that case, the CPU just submits a task for the copy engine to execute the copy. Depending on how many copy engines are available on the corresponding GPU, you could run multiple copies in parallel. Hopefully, it clarifies your question!Hello Nikolay, thanks a lot! yes, it really helped.Hi Nikolay, I have a concern about Warp-Per-Page Approach: it seems that this reduces the number of active warps, and thus reduce the degree of parallelism.Actually in my experience, I observed that the gld_throughput of Warp-Per-Page Approach is lower than the normal kernel (without prefetching), though the execution time of the normal kernel is still longer due to more page faults. But I'm still concerned that Warp-Per-Page Approach may not be well generalizable to other applications.Is my concern resonable?I'm new to CUDA, so my thought may not make sense.Each copy engine on the GPU can execute a separate memory transfer (host registered-to-device, device-to-host registered). Note, that you need to register (page lock and set mappings) the host buffer to use the copy engines for cudaMemcpy (see cudaHostRegister), otherwise the CUDA driver will create a pipeline and stage the memory transfers from cudaMemcpy through a temporary small pinned buffer. The latter may have an impact on performance since the CPU will be involved to copy the memory, and for small sizes there could be overhead from setting up the pipeline. If you're copying from/to registered host buffers then it should be fairly easy to achieve good overlap by using separate CUDA streams for cudaMemcpyAsync. Sorry for delayed reply, hope this helps!Hi Harry, I agree that this is a valid concern and the warp-per-page approach may not be generally applicable to many applications. The point is to reduce the number of page faults, if it's impossible to eliminate them completely. We're constantly working on improving the internal prefetching mechanism in the driver to support most common application patterns, so hopefully that would alleviate the need to change the parallelization strategy. The warp-per-page approach I described is just an interim solution that also serves educational purpose to showcase profiling stats.Powered by Discourse, best viewed with JavaScript enabled"
543,administrator-account-on-welcome-scrren,"Can anyone help me in enabling Administrator account on Welcome Screen? I tried my best but could not.Which main issue are you facing?I can’t see administrators account on welcome windowHave you tried CMD?Yes, I tried that too. But it is not working also.Powered by Discourse, best viewed with JavaScript enabled"
544,gtc-2020-deploying-fda-cleared-trt-powered-ai-radiology-products-to-improve-quality-and-productivity-in-clinical-radiology,"GTC 2020 S22400
Presenters: Enhao Gong,Subtle Medical; Tao Zhang, Subtle Medical Inc
Abstract
SubtlePET and SubtleMR are FDA-cleared AI software products developed by Subtle Medical that use deep learning to significantly improve the image quality and efficiency for PET-CT and MRI exams. We’ll introduce how Subtle develops AI that is generalizable and seamlessly deployed in clinical settings. Join our session to: Learn how to clinically evaluate and deploy AI software solutions at multiple hospitals and imaging centers, through Subtle’s experience of working with Stanford University, the University of California at San Francisco, Hoag hospital, U-C San Diego, Middlesex, RadNet, and others; Understand how attention models and improved deep-learning architectures improve model performance; Learn benefits of training algorithms on NVIDIA DGX Station and DGX-1 systems, and how it provides flexibility and efficiency from prototyping to product; Learn how integrating with NVIDIA TensorRT can accelerate inference; SubtleMR will share there experience where turnaround time is critical, TRT provided additional speed of up to about 8.8x; and Learn techniques to show how to show evidence of significant clinical values and financial ROI to customer hospitals and imaging centers.Watch this session
Join in the conversation below.Powered by Discourse, best viewed with JavaScript enabled"
545,explainer-what-is-active-learning,"Originally published at:			How Can Active Learning Help Train Autonomous Vehicles? | NVIDIA Blog
Finding the right self-driving training data doesn’t have to take a swarm of human labelers.Powered by Discourse, best viewed with JavaScript enabled"
546,nvidia-gtc-top-xr-sessions,"Originally published at:			https://developer.nvidia.com/blog/nvidia-gtc-top-xr-sessions/
This year, GTC features several sessions covering topics across the future of immersive environments, XR streaming, interactive trainings and simulations, and more.Powered by Discourse, best viewed with JavaScript enabled"
547,setting-new-records-in-mlperf-inference-v3-0-with-full-stack-optimizations-for-ai,"Originally published at:			https://developer.nvidia.com/blog/setting-new-records-in-mlperf-inference-v3-0-with-full-stack-optimizations-for-ai/
Learn about the innovations behind the record-setting NVIDIA performance in MLPerf Inference v3.0.Powered by Discourse, best viewed with JavaScript enabled"
548,cuda-pro-tip-clean-up-after-yourself-to-ensure-correct-profiling,"Originally published at:			https://developer.nvidia.com/blog/pro-tip-clean-up-after-yourself-ensure-correct-profiling/
NVIDIA’s profiling and tracing tools, including the NVIDIA Visual Profiler, NSight Eclipse and Visual Studio editions, cuda-memcheck, and the nvprof command line profiler are powerful tools that can give you deep insight into the performance and correctness of your GPU-accelerated applications. These tools gather data while your application is running, and use it to create…Powered by Discourse, best viewed with JavaScript enabled"
549,closing-the-sim2real-gap-with-nvidia-isaac-sim-and-nvidia-isaac-replicator,"Originally published at:			Closing the Sim2Real Gap with NVIDIA Isaac Sim and NVIDIA Isaac Replicator | NVIDIA Technical Blog
NVIDIA Isaac Replicator, built on the Omniverse Replicator SDK, can help you develop a cost-effective and reliable workflow to train computer vision models using synthetic data.Thank you for the very interesting article. I am amazed what can be done with Omniverse and Isaac Sim.Being new to the two as well as USD, I wondered how these things can be accomplished. Would be happy if you can guide me in the right direction.My thoughts are concerning the following:Hope you can answer those questions and guide me towards more ressources. Looking forward what can already done with omniverse and Isaac SimPowered by Discourse, best viewed with JavaScript enabled"
550,boosting-nvidia-mlperf-training-v1-1-performance-with-full-stack-optimization,"Originally published at:			https://developer.nvidia.com/blog/boosting-mlperf-training-v1-1-performance-with-full-stack-optimization/
In this round of MLPerf training v1.1, optimization across the entire stack including hardware, system software, libraries, and algorithms continue to boost NVIDIA MLPerf training performance.Hi, I recently came across this work and think that it is very cool! One thing that I was wondering, is there anywhere that profile traces for the different training pipelines are available, using either something like nvprof or tensorboard? I’m interested in seeing how much time is spent passing data from node to node when training with large number of nodes (like the 540 node case for ResNet50).Hi,
I am trying to run Minigo on a single 3080 device with the docker and readme files. However, I see some errors that can not fix them with your help.I have followed the steps in [1] and all was good for setting up the docker image… As I run the run command likeI get the following error:Looking at the code of train_loop.py and printing variablesShows thatI also have set the following parametersAny idea about that?  Any comment is appreciated.[1] training_results_v1.1/NVIDIA/benchmarks/minigo/implementations/tensorflow at main · mlcommons/training_results_v1.1 · GitHubPowered by Discourse, best viewed with JavaScript enabled"
551,gpu-accelerated-video-processing-with-nvidia-in-depth-support-for-vulkan-video,"Originally published at:			https://developer.nvidia.com/blog/gpu-accelerated-video-processing-with-nvidia-in-depth-support-for-vulkan-video/
Vulkan Video extensions for video decoding get a finalized release and support from Vulkan SDK, bringing highly tunable video processing to cross-platform applications.Discord link is useless. You have to give an invite to a discord server first, then give a link for the specific channel.This link you gave won’t work for anyone that is not part of the discord server.DiscordPlease consider this, thanks.Please use this invite link to the NVIDIA Developer discord server: https://discord.gg/nvidiadeveloperThanks! Appreciate it.
I think it should be fixed in this post tooPowered by Discourse, best viewed with JavaScript enabled"
552,end-to-end-ai-for-nvidia-based-pcs-onnx-and-directml,"Originally published at:			https://developer.nvidia.com/blog/end-to-end-ai-for-nvidia-based-pcs-onnx-and-directml/
This post is part of a series about optimizing end-to-end AI. While NVIDIA hardware can process the individual operations that constitute a neural network incredibly fast, it is important to ensure that you are using the tools correctly. Using the respective tools such as ONNX Runtime or TensorRT out of the box with ONNX usually…Powered by Discourse, best viewed with JavaScript enabled"
553,jetson-nano-brings-ai-computing-to-everyone,"For robotics, I imagine one Nano correlating 360 lidar with 360 panoramic camera for obstacle detection and path planning.  A second Nano with cameras in the cardinal directions captures interesting things coming and going for ID and possible inclusion into future training sets.  Third Nano handles proprioception and human interface.  4th Nano is the executive, looks at processed output from the other 3, identifies needs/wants, opportunities, generates and refines behaviors.Is it going to be available in Amazon? I've seen only from resellers and they cost more than 99$@DustinFranklin do you have any more info on this benchmark? The highest we were able to achieve at 256x256 resolution openpose was 7fps.Hi @jostrinidadsoto:disqus, see the available distributors here:   https://devtalk.nvidia.com/...Hi @alextyshka:disqus , please see here for the instructions to run this:   https://devtalk.nvidia.com/...My sincere thanks Dustin. Not only the pose detection but all of those models will be invaluable. Hopefully we can reproduce these results and use the Jetson nano for our product!Hi Dustin. I'm confused about ""DeepStream on Jetson Nano"" youtube video. Can you explain this setup, all 8 channels inference running on a Jetson Nano?https://www.youtube.com/wat...Hi Dustin,we have purchased one piece of the board,and we are gonna connect camera with the board,so we wants to know the definition of each pin of J13.Can you help to advise? https://uploads.disquscdn.c...Hi @disqus_vRGQGjkPZm:disqus , this is an application build with NVIDIA's DeepStream SDK that is processing 8 streams simultaneously using TensorRT for inferencing on a ResNet-based object detection model for tracking vehicles and pedestrians.  Recommend that you join our webinar next week if you are interested in seeing more:  https://info.nvidia.com/hel...Hi @disqus_6LLhqeKm1u:disqus , this connector is the same as on the Raspberry Pi, here is a pin diagram for it: https://uploads.disquscdn.c...Thanks for reply @disqus_Qgrz6nmZ6N:disqus  Registered for your webinar :)In your benchmark result, Jetson Nano capable of 18fps with SSD-Resnet model. I think DeepStream can succeed this scenario (object detection from 8 streams) with help of the object tracking information, not just detection with deep learning inference.Hi Dustin,thanks for your kindly advice,well received.Our engineer guy is trying to figure it out,the information you provide is really helpful.Appreciate!By the way,this board is amazing!I'm so hyped about this board and learning about ML & DL... Received my Jetson Nano Devkit (from SeeedStudio, shipped to Spain) last friday. Haven't had time to test it yet. Can't wait!Hi Dustin, i'm looking for jetson nano file to run it in a circuit simulator for my project .....can you help..what about running text to speech on the edge? mozilla tts still perform badly...we reached 25 seconds to synthetize simple phrasesHi am trying to replicate this using DEEPSTREAM SDK, but i cant seem to find a hardware ""PCIE to Gbe x8 switch"" thats compatible over a PCIE slot of the NANOJust got my Nano and I am very disappointed. Glad I got it from Amazon so I can return it. Won't boot right out of the box. May be a hardware problem but with the crappy documentation, who knows. Sounds like Rasberry Pi is a better alternative.Hi,What is ""PCIE to Gbe x8 switch and POE injector"" shown on the system architecture figure?  Does it allow to connect 8 HD cameras to jetson? Currenlty if try to connect more than 2 usb cameras and extract frames (with python & open cv) I get an ""VIDIOC_STREAMON: No space left on device"" error (and I  have plenty of disk space and using only 2 usb cameras everything works ok so i suspect that's not the problem)Hi @boaz_petersil:disqus , please follow-up in this forum topic:  https://devtalk.nvidia.com/...The V4L2 error is not about disk space, rather it is about USB bandwidth.  You can try reducing the resolution or framerate of the cameras, or as suggested in that topic make sure they are being enumerated with USB 3.0 SuperSpeed.  Alternatively, using IP camera you could connect an external GbE to connect multiple IP cameras, or try an M.2 Key-E GbE adapter for more IP bandwidth from the Nano devkit.Alternatively, see this Nano enclosure that has 5x integrated GbE ports for a similar implementation to the NVR concept:  https://www.aaeon.com/en/p/...did any of you perhaps know about the topic implementation of a video store and forward systems what was done with nano jetson ? thanksPowered by Discourse, best viewed with JavaScript enabled"
554,new-asynchronous-programming-model-library-now-available-with-nvidia-hpc-sdk-v22-11,"Originally published at:			https://developer.nvidia.com/blog/new-asynchronous-programming-model-library-now-available-with-nvidia-hpc-sdk-v22-11/
Find out what’s in the NVIDIA HPC SDK v22.11 release, including a preview of an innovative library for standardizing and asynchronously scheduling C++ work.Powered by Discourse, best viewed with JavaScript enabled"
555,tensor-core-performance-on-nvidia-gpus-the-ultimate-guide,"GTC 2020 S21929
Presenters: Valerie Sarge, NVIDIA
Abstract
Learn what’s needed to achieve optimal performance on NVIDIA Tensor Core GPUs, including the brand-new A100 GPU based on the NVIDIA Ampere architecture. We’ll review the fundamentals of GPU performance, explain how Tensor Core-accelerated operations work, and use this knowledge to infer how to structure and size neural network operations (layers) to achieve ideal performance. We’ll also provide a cheat sheet of Tensor Core performance guidelines. The talk aims to provide tools to understand why neural networks perform a certain way on Tensor Core GPUs and to enable changes to network architecture to further improve performance.Watch this session
Join in the conversation below.Powered by Discourse, best viewed with JavaScript enabled"
556,ignacio-llamas-interview-unearthing-ray-tracing,"Originally published at:			Ignacio Llamas Interview: Unearthing Ray Tracing | NVIDIA Technical Blog
We spoke with Ignacio Llamas, Director of Real Time Ray Tracing Software at NVIDIA about the introduction of real-time ray tracing in consumer GPUs. How did you get started on ray tracing at NVIDIA? I joined NVIDIA about four years ago, on the GPU architecture team. If somebody told you that you’d be shipping consumer…Powered by Discourse, best viewed with JavaScript enabled"
557,nvidia-turing-optimized-developer-sdks,"Originally published at:			https://developer.nvidia.com/blog/nvidia-turing-optimized-sdks/
The NVIDIA Turing architecture is one of the biggest leaps in computer graphics in 20 years. Here’s a look at all of the developer software announced to take advantage of this cutting-edge GPU. NVIDIA RTX Platform With NVIDIA Turing GPU architecture at its foundation, the NVIDIA RTX development platform fuses ray tracing, deep learning, simulation,…Powered by Discourse, best viewed with JavaScript enabled"
558,seamlessly-develop-vision-ai-applications-with-nvidia-deepstream-sdk-6-2,"Originally published at:			https://developer.nvidia.com/blog/seamlessly-develop-vision-ai-applications-with-nvidia-deepstream-sdk-6-2/
NVIDIA announced the general availability of the NVIDIA DeepStream SDK 6.2, an AI analytics toolkit for building high-performance video analytics and streaming applications. The update adds new features including enhanced multi-object trackers, support for new sensors, integration with REST APIs, updated Graph Composer, and enterprise-grade support through NVIDIA AI Enterprise.  Download the DeepStream SDK 6.2…Powered by Discourse, best viewed with JavaScript enabled"
559,nvidia-gpu-operator-simplifying-gpu-management-in-kubernetes,"Originally published at:			NVIDIA GPU Operator: Simplifying GPU Management in Kubernetes | NVIDIA Technical Blog
Over the last few years, NVIDIA has leveraged GPU containers in a variety of ways for testing, development and running AI workloads in production at scale. Containers optimized for NVIDIA GPUs and systems such as the DGX and OEM NGC-Ready servers  are available as part of NGC. But provisioning servers with GPUs reliably and scaling…Powered by Discourse, best viewed with JavaScript enabled"
560,logitech-s-new-webcam-includes-dynamic-background-removal-feature,"Originally published at:			https://developer.nvidia.com/blog/logitechs-new-webcam-includes-dynamic-background-removal-feature/
Announced at TwitchCon in San Diego, Logitech introduced their newest webcam designed to meet the needs for serious streamers. “With the C922 Pro Stream Webcam, we are taking game casting to the next level so you can easily broadcast your gameplay like a pro, better engage with your community, and stand out from the crowd,”…Powered by Discourse, best viewed with JavaScript enabled"
561,improve-guidance-and-performance-visualization-with-the-new-nsight-compute,"Originally published at:			https://developer.nvidia.com/blog/improve-guidance-and-performance-visualization-with-the-new-nsight-compute/
Learn more about new features and ways to improve system performance using Nsight Compute 2022.2Powered by Discourse, best viewed with JavaScript enabled"
562,gtc-2020-inside-the-nvidia-ampere-architecture,"GTC 2020 S21730
Presenters: Olivier Giroux,NVIDIA; Ronny Krashinsky, NVIDIA
Abstract
This talk will cover what’s new with the NVIDIA Ampere Architecture, and its implementation in A100.Watch this session
Join in the conversation below.Powered by Discourse, best viewed with JavaScript enabled"
563,gtc-2020-a-deep-learning-approach-to-improve-the-reliability-of-oil-and-gas-systems,"GTC 2020 S21624
Presenters: Henrik Ohlsson ,C3.ai ; Nikhil Krishnan,c3.ai
Abstract
The session is focused on applying deep learning to improve the reliability of large scale oil and gas processes by proactively predicting system failures. In particular, we will discuss BHC3 Reliability™, a productized, deep learning-based anomaly detection framework that uses system-of-system models for large-scale oil and gas processes.Watch this session
Join in the conversation below.Powered by Discourse, best viewed with JavaScript enabled"
564,ai-animation-support-sam-walks-up-to-hank-and-slaps-him-on-the-back,"NVIDIA has some features like path animations (get a character to walk along a path). Any plans to have navigation support (walk around objects in room like tables) so you can use ChatGPT or similar to generate an animation clip from text such as “Sam walks up to Hank and slaps him on the back”? Obviously you have to tell it the starting point, but there isE.g. with “digital humans” effort, how far is that likely to go? There were some early videos doing text to animation - is that work continuing?What a great scenario. That sounds like a great extension that someone should make. If you’d like to make it, I think you’d find enthusiastic support in the Omniverse discord channel.There was an old NVIDIA video about generating a single animation clip from multiple movements. Sounds like that is not an active area of development then?Powered by Discourse, best viewed with JavaScript enabled"
565,deploying-and-accelerating-ai-at-the-edge-with-the-nvidia-egx-platform,"Originally published at:			https://developer.nvidia.com/blog/deploying-and-accelerating-ai-at-the-edge-with-the-nvidia-egx-platform/
Edge computing has been around for a long time, but has recently become a hot topic because of the convergence of three major trends - IoT, 5G, and AI.Powered by Discourse, best viewed with JavaScript enabled"
566,anyone-can-build-metaverse-applications-with-new-beta-release-of-omniverse,"Originally published at:			Anyone Can Build Metaverse Applications With New Beta Release of Omniverse | NVIDIA Technical Blog
Learn about the new beta release of NVIDIA Omniverse, which includes major updates to core reference applications and tools for developers looking to build metaverse applications.@jwitsoe Sounds interesting, Is there a workflow or some kind of plugin/application with or for Blender and macOs ?Powered by Discourse, best viewed with JavaScript enabled"
567,inception-spotlight-ai-app-can-create-a-cartoon-emoji-of-yourself,"Originally published at:			https://developer.nvidia.com/blog/inception-spotlight-ai-app-chudo-can-create-a-cartoon-emoji-of-yourself/
Looking to create a cartoon emoji version of yourself? This new AI app can help.  A new deep learning-based messenger app can turn users into personalized cartoon characters.  “Each avatar is unique and personalized to its user,” the company explained. “In this app you can be anyone you want: a human digital copy of yourself, a…Powered by Discourse, best viewed with JavaScript enabled"
568,ai-helps-you-paint-like-van-gogh,"Originally published at:			AI Helps You Paint Like Van Gogh | NVIDIA Technical Blog
Product design and development firm Cambridge Consultants developed a deep learning-based system that turns human sketches into paintings that resemble Van Gogh, Cézanne and Picasso. “What we’ve built would have been unthinkable to the original deep learning pioneers,” said Monty Barlow, director machine learning at Cambridge Consultants in reference to their interactive system that call…Powered by Discourse, best viewed with JavaScript enabled"
569,accelerating-io-in-the-modern-data-center-magnum-io-storage-partnerships,"Originally published at:			https://developer.nvidia.com/blog/accelerating-io-in-the-modern-data-center-magnum-io-storage-partnerships/
With computation shifting from the CPU to faster GPUs for AI, ML and HPC applications, IO into and out of the GPU can become the primary bottleneck to the overall application performance. NVIDIA created Magnum IO GPUDirect Storage (GDS) to streamline data movement between storage and GPU memory and remove performance bottlenecks in the platform,…We’re delighted to have so many effective partners to work with who are working to broaden the adoption of Accelerated Storage IO to GPUs in modern data centers.  Come check out a Birds of a Feature session at SC’21 on Tue Nov 16, 12:15-1:15 CST - see Presentation • SC21 and Accelerating Storage IO to GPUs - Google Docs.Powered by Discourse, best viewed with JavaScript enabled"
570,gtc-2020-directive-based-gpu-programming-with-openacc,"GTC 2020 CWE21815
Presenters: ,
Abstract
OpenACC is a programming model designed to help scientists and developers to start with GPUs faster and be more efficient by maintaining a single code source for multiple platforms. Come join OpenACC experts to ask about how to start accelerating your code on GPUs, continue optimizing your GPU code, start teaching OpenACC, host or participate in a hackathon, and more!Watch this session
Join in the conversation below.Powered by Discourse, best viewed with JavaScript enabled"
571,cuda-refresher-reviewing-the-origins-of-gpu-computing,"Originally published at:			CUDA Refresher: Reviewing the Origins of GPU Computing | NVIDIA Technical Blog
This is the first post in the CUDA Refresher series, which has the goal of refreshing key concepts in CUDA, tools, and optimization for beginning or intermediate developers. Scientific discovery and business analytics drive an insatiable demand for more computing resources. Many applications—weather forecasting, computational fluid dynamics simulations, and more recently machine learning and deep…Powered by Discourse, best viewed with JavaScript enabled"
572,developers-drive-dpu-evolution-in-the-nvidia-doca-hackathon,"Originally published at:			https://developer.nvidia.com/blog/developers-drive-dpu-evolution-in-the-nvidia-doca-hackathon/
The recent DOCA Hackathon in Europe revealed streamlined innovation in video processing, storage solutions, and switching protocols using  the BlueField DPU and DOCA SDK.We had a great turnout for the recent DOCA Hackathon in Europe and were super impressed with all the teams that participated.  Let me know if you have any questions or if you’d like to join a future BlueField DPU/DOCA Hackathon!Hi Scott @sciccone, We are from NetLOX, a startup based in South Korea. We are interested in participating in the next DOCA Hackathon. We would like to know about the registration date and process for the event. Would you care to share the details?Hi @nikhil.malik , Thank you for your note and for your interest in participating in the next DOCA Hackathon.  I’ll forward your note to our Marketing team to provide you with the details of our upcoming Hackathons.  Registrations usually open about 6 weeks prior to the specific Hackathon, but I’m happy to hear from you!Powered by Discourse, best viewed with JavaScript enabled"
573,upcoming-event-the-next-frontier-of-computer-vision-simulation-and-synthetic-data,"Originally published at:			https://nvda.ws/NextFrontierofComputerVision#new_tab
Learn how simulation and synthetic data are transforming vision AI applications at the NVIDIA Metropolis meetup on February 22 and 23.Powered by Discourse, best viewed with JavaScript enabled"
574,interactive-supercomputing-with-in-situ-visualization-on-tesla-gpus,"Originally published at:			https://developer.nvidia.com/blog/interactive-supercomputing-in-situ-visualization-tesla-gpus/
So, you just got access to the latest supercomputer with thousands of GPUs. Obviously this is going to help you a lot with accelerating your scientific calculations, but how are you going to analyze, reduce and visualize this data?  Historically, you would be forced to write everything out to disk, just to later read it…What are the drawbacks of this approach?The simple answer is: there are no drawbacks.The details depend a bit on what you mean by ""this approach"":Enabling rendering on Tesla GPUs doesn't have any noticeable implications in terms of e.g. power consumption, GPU performance etc. And actually, on post-K20 GPUs (K40, K80, .. ) rendering is enabled by default. So no drawback there.Context management via X server does require an extra process on your system. In most cases, this is a non-issue, but some HPC centers are hesitant to enable this for various reasons. If that's the case, we'd like to hear about this and help address it. Also, with our latest drivers we support OpenGL context management via EGL, which makes the X server largely unnecessary (see https://devblogs.nvidia.com...And as pointed out in the article, you will need a remoting solution, to get the rendered frames off the HPC system. Again, this shouldn't be a real issue, as most HPC centers have remote visualization software setup anyway for their users. All that's needed is to enable this solution on the actual HPC system.Finally, in-situ visualization means that some cycles are spent on visualization rather than on your simulation. The exact cost obviously depends on your use case but it is often offset by the time saved avoiding lots of disk IO or a dedicated post-processing runs.Powered by Discourse, best viewed with JavaScript enabled"
575,new-mit-video-recognition-model-dramatically-improves-latency-on-edge-devices,"Originally published at:			New MIT Video Recognition Model Dramatically Improves Latency on Edge Devices | NVIDIA Technical Blog
To improve the speed of video recognition applications on edge devices such as NVIDIA’s Jetson Nano and Jetson TX2, MIT researchers developed a new deep learning model that outperforms previous state-of-the-art models in video recognition tasks. Trained using 1,536 NVIDIA V100 GPUs at Oak Ridge National Laboratory’s Summit supercomputer, the model earned the top spot in the Something-Something video dataset public challenge,…Powered by Discourse, best viewed with JavaScript enabled"
576,ai-co-pilot-rnns-for-dynamic-facial-analysis,"Originally published at:			AI Co-Pilot: RNNs for Dynamic Facial Analysis | NVIDIA Technical Blog
Figure 1. Head tracking and dynamic facial analysis in NVIDIA AI Co-Pilot. Deep learning is enabling a revolution in how we interact with technology in our daily lives, from personalized healthcare to how we drive our cars. NVIDIA AI Co-Pilot combines deep learning and visual computing to enable augmented driving. Co-Pilot uses sensor data from…Powered by Discourse, best viewed with JavaScript enabled"
577,gtc-2020-how-nvidia-quadro-virtual-workstations-virtual-pcs-and-virtual-apps-are-transforming-industries,"GTC 2020 S21670
Presenters: Jits Langedijk,NVIDIA; Tony Foster,Dell EMC; Sean Massey,VMware; Thomas Poppelgaard,Poppelgaard.com; Dane Young,YOUNG TECHNOLOGIES, LLC (“YOUNGTECH”)
Abstract
Graphics virtualization thought leaders and experts from across the globe who have deep knowledge of NVIDIA virtual GPU architecture and years of experience implementing virtual desktop infrastructure across multiple hypervisors will discuss how they transformed organizations, including how they leveraged multi-GPU support to boost GPU horsepower for photorealistic rendering and data-intensive simulation, and how they used GPU-accelerated deep learning or HPC virtual desktop infrastructure environments with ease using NVIDIA GPU CLOUD containers. VDI users across multiple industries can now harness the power of the world’s most advanced virtual workstation to enable increasingly demanding workflows.Watch this session
Join in the conversation below.Powered by Discourse, best viewed with JavaScript enabled"
578,creating-physically-based-materials-for-minecraft-with-nvidia-rtx,"Originally published at:			Creating Physically Based Materials for Minecraft with NVIDIA RTX | NVIDIA Technical Blog
Are you an experienced Minecraft content creator, but new to physically based materials? Or someone who just wants to learn the basics behind physically based rendering to create your own PBR resource packs? Great! This talk is for you. In “Creating Physically Based Materials for Minecraft with RTX,” we introduce you to the new look…Powered by Discourse, best viewed with JavaScript enabled"
579,upcoming-webinar-migrating-ros-based-robot-simulations-from-ignition-gazebo-to-nvidia-isaac-sim,"Originally published at:			NVIDIA: World Leader in Artificial Intelligence Computing
Join this webinar on August 4, 2022 to learn about moving from an Ignition Gazebo simulation to Isaac Sim using the Ignition-Omniverse experimental converter.Powered by Discourse, best viewed with JavaScript enabled"
580,nvidia-vrss-a-zero-effort-way-to-improve-your-vr-image-quality,"Originally published at:			NVIDIA VRSS, a Zero-Effort Way to Improve Your VR Image Quality | NVIDIA Technical Blog
The Virtual Reality (VR) industry is in the midst of a new hardware cycle – higher resolution headsets and better optics being the key focus points for the device manufacturers. Similarly on the software front, there has been a wave of content-rich applications and an emphasis on flawless VR experiences for the end user.  With…What VRSS titles were added with 442.19 ?Hey Nvidia Devs,VRSS is interesting technology, but the latest driver releases seems to have forgot this feature. No new games were listed, neither new blog articles coming out about this topic. I'm show some love for VRSS: please keep up with it and try to improve and made it widely adopted! I can't wait for it!Hi Tommaso,Thank you for your enthusiastic interest in VRSS! We are currently evaluating different games and applications for VRSS based on image quality improvements. Stay tuned…Could a developer explain this one to me, this feature makes no sense at all.  In steamVR the entire game is already supersampled based on the users settings, usually 150% on native resolution and then the image is distorted to fit the lens profile and rendered onto the headsets native resolution. Turning on VRSS would improve the quality of the centre of the image a bit however that still needs to be distorted and mapped to the lens profile which would cause image degradation.I would much prefer a setting that would allow under sampling of the image parts that are outside of the centre of view. Then the entire game could be super sampled at 200% and reduce the quality loss when the image is distorted to fit the lens native resolution.zero effort, zero result
I am working in Unity and when I try to set the Nvidia 3D settings panel to use VRSS, it says that it is not possible for this application…
MSAA is set in my application.Powered by Discourse, best viewed with JavaScript enabled"
581,ran-in-the-cloud-delivering-cloud-economics-to-5g-ran,"Originally published at:			https://developer.nvidia.com/blog/ran-in-the-cloud-delivering-cloud-economics-to-5g-ran/
Realizing the cloud economics for 5G RAN and driving the co-innovation of 5G with edge AI applications requires embracing RAN-in-the-Cloud.Powered by Discourse, best viewed with JavaScript enabled"
582,bias-variance-decompositions-using-xgboost,"Originally published at:			Bias Variance Decompositions using XGBoost | NVIDIA Technical Blog
This blog dives into a theoretical machine learning concept called the bias variance decomposition. This decomposition is a method which examines the expected generalization error for a given learning algorithm and a given data source. This helps us understand questions like: How can I achieve higher accuracy with my model without overfitting? Why are my…Powered by Discourse, best viewed with JavaScript enabled"
583,gtc-2020-wide-and-deep-recommender-inference-on-gpu,"GTC 2020 S21559
Presenters: Alec Gunny ,NVIDIA; Chirayu Garg,NVIDIA
Abstract
We’ll discuss using GPUs to accelerate so-called “wide and deep” models in the recommendation inference setting. Machine learning-powered recommender systems permeate modern online platforms. Wide and deep models have become a popular choice for recommendation problems due to their high expressiveness compared to more traditional machine learning models, and the ease with which they can be trained and deployed using Tensorflow. We’ll demonstrate simple APIs to convert trained canned Tensorflow estimators to TensorRT executable engines and deploy them for inference using NVIDIA’s TensorRT Inference Server. The generated TensorRT engines can also be configured to enable reduced-precision computation that leverages tensor cores in NVIDIA GPUs. Finally, we’ll show how to integrate these served models into an optimized inference pipeline, exploiting shared request-level features across batches of items to minimize network traffic and fully leverage GPU acceleration.Watch this session
Join in the conversation below.Powered by Discourse, best viewed with JavaScript enabled"
584,nvidia-dgx-2-accelerates-weather-forecasting,"Originally published at:			NVIDIA DGX-2 Accelerates Weather Forecasting | NVIDIA Technical Blog
Today, weather agencies and other organizations such as wind farms, airports, logistic centers, marine operations and many others run customized weather forecasts on complex computing clusters across multiple nodes. The problem with this approach is that clusters pose a set of challenges including multiple operating systems, shared file systems, and complicated management operations. To help…Powered by Discourse, best viewed with JavaScript enabled"
585,nvidia-research-disect-a-differentiable-simulation-engine-for-autonomous-robotic-cutting,"Originally published at:			https://developer.nvidia.com/blog/nvidia-research-disect-a-differentiable-simulation-engine-for-autonomous-robotic-cutting/
Robotics researchers from NVIDIA and University of Southern California recently presented their work at the 2021 RSS conference called DiSECt, the first differentiable simulator for robotic cutting.Powered by Discourse, best viewed with JavaScript enabled"
586,introduction-to-ray-tracing-in-unreal-engine-4-22,"Originally published at:			Introduction to Ray Tracing in Unreal Engine 4.22 | NVIDIA Technical Blog
Epic Games is adding ‘Early Access’ support for ray tracing through the DirectX Raytracing API (DXR) to Unreal Engine with the pending release of Unreal Engine 4.22. Demos dating back to GDC 2018 show impressive ray tracing results using DXR. However, UE 4.22 integrates ray tracing support into the mainline branch, making ray tracing available to…Powered by Discourse, best viewed with JavaScript enabled"
587,meet-the-new-tesla-accelerated-supercomputer-at-michigan-state-university,"Originally published at:			Meet the New Tesla-Accelerated Supercomputer at Michigan State University | NVIDIA Technical Blog
Accelerated by 200 Tesla K80 GPUs, the Laconia Supercomputer was recently unveiled at the Institute for Cyber-Enabled Research at Michigan State University. Named after a region in Greece that was home to the original Spartans, the mascot of Michigan State University, the supecomputer ranks in the TOP500 fastest computers in the world and is projected…Powered by Discourse, best viewed with JavaScript enabled"
588,monai-drives-medical-ai-on-google-cloud-with-medical-imaging-suite,"Originally published at:			MONAI Drives Medical AI on Google Cloud with Medical Imaging Suite | NVIDIA Technical Blog
Google Cloud’s Medical Imaging Suite adopts MONAI to deliver AI-assisted annotation workflows at scale.Powered by Discourse, best viewed with JavaScript enabled"
589,nvidia-omniverse-machinima-releasing-in-open-beta,"Originally published at:			NVIDIA Omniverse Machinima Now Available | NVIDIA Technical Blog
Technical artists, developers and content creators can now take 3D storytelling to the next level: NVIDIA Omniverse Machinima is available in an open beta.Powered by Discourse, best viewed with JavaScript enabled"
590,new-ai-app-provides-personalized-skin-care-evaluation,"Originally published at:			New AI App Provides Personalized Skin Care Evaluation | NVIDIA Technical Blog
With 65 years of skincare experience, Olay developed a deep learning-based tool that can act as your personalized beauty skin care advisor. “It’s become a real challenge to shop for skin care,” said Frauke Neuser, principal scientist at Olay. “You go into a store, and there are 50 yards of facial care products.” Olay, the…A very good app for those who want to maintain the basic beauty.Powered by Discourse, best viewed with JavaScript enabled"
591,delivering-dynamic-foveated-rendering-with-nvidia-vrss-2,"Originally published at:			Delivering Dynamic Foveated Rendering with NVIDIA VRSS 2 | NVIDIA Technical Blog
Designing rich content and graphics for VR experiences means creating complex materials and high-resolution textures. But rendering all that content at VR resolutions and frame rates can be challenging, especially when rendering at the highest quality. You can address this challenge by using variable rate shading (VRS) to focus shader resources on certain parts of…Powered by Discourse, best viewed with JavaScript enabled"
592,ai-pioneers-write-so-should-data-scientists,"Originally published at:			AI Pioneers Write So Should Data Scientists | NVIDIA Technical Blog
Data Scientists role in producing AI related written content to be consumed by the publicPowered by Discourse, best viewed with JavaScript enabled"
593,gtc-digital-demo-nvidia-tool-to-visualize-and-interact-with-feature-maps,"Originally published at:			GTC Digital Demo: NVIDIA Tool to Visualize and Interact with Feature Maps | NVIDIA Technical Blog
NVIDIA Feature Map Explorer is a new powerful tool that visualizes 4-dimensional image-based feature map data in a fluid and interactive fashion. It provides users with a rich set of views into feature map data that range from high-level summary to low-level channel slices, as well as detailed statistics information.   The visualization and statistics enable…Powered by Discourse, best viewed with JavaScript enabled"
594,simplifying-and-accelerating-machine-learning-predictions-in-apache-beam-with-nvidia-tensorrt,"Originally published at:			https://developer.nvidia.com/blog/simplifying-and-accelerating-machine-learning-predictions-in-apache-beam-with-nvidia-tensorrt/
Loading and preprocessing data for running machine learning models at scale often requires seamlessly stitching the data processing framework and inference engine together. In this post, we walk through the integration of NVIDIA TensorRT with Apache Beam SDK and show how complex inference scenarios can be fully encapsulated within a data processing pipeline. We also…Powered by Discourse, best viewed with JavaScript enabled"
595,tools-and-best-practices-for-remote-work,"Originally published at:			https://developer.nvidia.com/blog/tools-and-best-practices-for-remote-work/
As more developers and scientists across industries work from home – a great user experience is critical to maintaining productivity. To help fellow researchers and developers effectively work from home, here are some best practices from NVIDIA researchers and scientists. Remotely Connect to Your Office Workstation Having access to a secondary machine is useful. It…Hi,
where can I get the nvidiaopenglrdp.exe file?thx for your Help.best regards,
AlfredAlfred, try this page: https://developer.nvidia.com/nvidia-opengl-rdp. You’ll need to be a member of our Developer Zone, but it’s free.Powered by Discourse, best viewed with JavaScript enabled"
596,customize-your-own-carrier-board-with-nvidia-sdk-manager,"Originally published at:			https://developer.nvidia.com/blog/customize-your-own-carrier-board-with-nvidia-sdk-manager/
NVIDIA SDK Manager is the go-to tool for installing the NVIDIA JetPack SDK on NVIDIA Jetson Developer Kits. It provides a guided and simple way to install the development environment and get started with the developer kits in a matter of minutes. SDK Manager handles the dependencies between the components and brings the latest software…Powered by Discourse, best viewed with JavaScript enabled"
597,what-short-cuts-are-needed-for-real-time-path-tracing,"The movie industry uses render farms for this level of realism, so you must cut corners to make real-time path tracing a thing, right?Well you might consider denoising to be a short-cut - but it’s not really. It allows you to extract a much better result from a poor signal. Users of offline path tracing, like the film industry, will still use denoising, but typically to a lesser degree.To extract the most performance, real time path tracing will typically use fewer bounces than offline path tracing too. As with denoising, other parts of the pipeline (like ReSTIR DI and ReSTIR GI from RTXDI) can also rely on temporal reuse of data generated in previous frames.
Likewise, we can use lower precision math, simplified materials and skip a lot of computation necessary for subtle detail that is less noticeable in real-time.Powered by Discourse, best viewed with JavaScript enabled"
598,latest-releases-and-resources-nvidia-gtc-2022,"Originally published at:			https://developer.nvidia.com/blog/latest-releases-and-resources-nvidia-gtc-2022/
This GTC focused roundup features updates to   the HPC SDK, cuQuantum SDK, Nsight Graphics and Systems 2022.2, CUDA 11.6, Update 1, cuNumeric, and Warp.Powered by Discourse, best viewed with JavaScript enabled"
599,low-code-building-blocks-for-speech-ai-robotics,"Originally published at:			Low-Code Building Blocks for Speech AI Robotics | NVIDIA Technical Blog
Learn how to add speech AI skills, like speech-to-text (STT) and text-to-speech (TTS), to your intelligent robotics application using low-code building blocks.Powered by Discourse, best viewed with JavaScript enabled"
600,microsoft-and-nvidia-announce-june-preview-for-gpu-acceleration-support-for-wsl,"Originally published at:			Microsoft and NVIDIA Announce June Preview for GPU-Acceleration Support for WSL | NVIDIA Technical Blog
At their Build digital developers conference on May 19, Microsoft announced a Public Preview for GPU in Windows Subsystem for Linux (WSL). WSL is a layer that enables executing Linux binaries on Microsoft Windows computing systems.  The announcement disclosed the collaboration with NVIDIA to deliver CUDA GPU-acceleration support to masses of Windows users. A live…Powered by Discourse, best viewed with JavaScript enabled"
601,identifying-the-best-ai-model-serving-configurations-at-scale-with-nvidia-triton-model-analyzer,"Originally published at:			https://developer.nvidia.com/blog/identifying-the-best-ai-model-serving-configurations-at-scale-with-triton-model-analyzer/
This post presents an overview of NVIDIA Triton Model Analyzer and how it can be used to find the optimal AI model-serving configuration to satisfy application requirements.Powered by Discourse, best viewed with JavaScript enabled"
602,real-time-serving-for-xgboost-scikit-learn-randomforest-lightgbm-and-more,"Originally published at:			https://developer.nvidia.com/blog/real-time-serving-for-xgboost-scikit-learn-randomforest-lightgbm-and-more/
This post-dive into how the NVIDIA
Triton Inference Server offers highly optimized real-time serving forest models by using the Forest Inference Library backend.Powered by Discourse, best viewed with JavaScript enabled"
603,top-robotics-sessions-at-nvidia-gtc-2023,"Originally published at:			https://nvda.ws/3IISW7q
Get to know the NVIDIA technologies and software development tools powering the latest in robotics and edge AI.Powered by Discourse, best viewed with JavaScript enabled"
604,introducing-nvidia-merlin-hugectr-a-training-framework-dedicated-to-recommender-systems,"Originally published at:			Introducing NVIDIA Merlin HugeCTR: A Training Framework Dedicated to Recommender Systems | NVIDIA Technical Blog
Click-through rate (CTR) estimation is one of the most critical components of modern recommender systems. As the volume of data and its complexity grow rapidly, the use of deep learning (DL) models to improve the quality of estimations has become widespread. They generally have greater expressive power than traditional machine learning (ML) approaches. Frequently evolving…Powered by Discourse, best viewed with JavaScript enabled"
605,how-alden-filion-developed-an-award-winning-ray-traced-game-demo-all-on-his-own,"Originally published at:			How Alden Filion Developed an Award-Winning Ray Traced Game Demo All on His Own | NVIDIA Technical Blog
NVIDIA RTX GPUs arrived at the end of 2018, supported by AAA releases like Battlefield V,  Metro: Exodus, and Control. Real-time ray tracing is a game changer; it’s the defining feature of the new-generation of video games. But what does the technology mean for indie developers? Diode, a DXR Spotlight Contest Winner, shows off what…Powered by Discourse, best viewed with JavaScript enabled"
606,self-taught-ai-bot-beat-professional-players-at-super-smash-bros,"Originally published at:			Self-Taught AI Bot Beat Professional Players at Super Smash Bros | NVIDIA Technical Blog
Students from MIT and New York University developed an AI bot that ended up teaching itself in two weeks to beat professional gamers during the Genesis 4 Super Smash Bros tournament last month. The AI, nicknamed Phillip, was originally trained with CUDA, Tesla K20/TITAN X GPUs and the TensorFlow deep learning framework – but the…Powered by Discourse, best viewed with JavaScript enabled"
607,rtx-coffee-break-ray-tracing-and-denoising-9-52-minutes,"Originally published at:			RTX Coffee Break: Ray Tracing and Denoising (9:52 minutes) | NVIDIA Technical Blog
Ray tracing can lead to very noisy images without the right tools. In this video, we explain the challenges of developing a real-time denoising solution, and describe the results NVIDIA has achieved using RTX.   Five Things to Remember: The realistic budget is 1-2 samples per pixel (which is insufficient to get anything reliable) with…Powered by Discourse, best viewed with JavaScript enabled"
608,hgx-2-fuses-hpc-and-ai-computing-architectures,"Originally published at:			HGX-2 Fuses HPC and AI Computing Architectures | NVIDIA Technical Blog
The continuing explosive growth of AI model size and complexity means the appetite for more powerful compute solutions continues to accelerate rapidly. NVIDIA is introducing HGX-2, our latest, most powerful cloud server platform for AI and High-Performance Computing (HPC) workloads, to meet these new requirements. HGX-2 comes with multi-precision computing capabilities, allowing high-precision calculations using…Powered by Discourse, best viewed with JavaScript enabled"
609,boosting-dynamic-programming-performance-using-nvidia-hopper-gpu-dpx-instructions,"Originally published at:			https://developer.nvidia.com/blog/boosting-dynamic-programming-performance-using-nvidia-hopper-gpu-dpx-instructions/
Dynamic programming (DP) is a well-known algorithmic technique and a mathematical optimization that has been used for several decades to solve groundbreaking problems in computer science. An example DP use case is route optimization with hundreds or thousands of constraints or weights using the Floyd-Warshall all-pair shortest paths algorithm. Another use case is the alignment…Powered by Discourse, best viewed with JavaScript enabled"
610,share-your-science-accelerating-oil-discovery-with-gpus,"Originally published at:			https://developer.nvidia.com/blog/share-your-science-accelerating-oil-discovery-with-gpus/
Thor Johnsen, an HPC Expert at Chevron, talks about how they are using NVIDIA GPUs for seismic imaging and modeling of the subsurface of the earth to create detailed 3D maps used to identify locations for oil drilling.   Share your GPU-accelerated science with us: http://nvda.ly/Vpjxr Watch more scientists and researchers share how accelerated computing…Powered by Discourse, best viewed with JavaScript enabled"
611,new-video-series-what-developers-need-to-know-about-universal-scene-description,"Originally published at:			https://developer.nvidia.com/blog/new-video-series-what-developers-need-to-know-about-universal-scene-description/
Universal Scene Description (OpenUSD) is an open and extensible framework for creating, editing, querying, rendering, collaborating, and simulating within 3D worlds. Invented by Pixar Animation Studios, USD is much more than a file format. It is an ecosystem and interchange paradigm that models, labels, classifies, and combines a wide range of data sources into a…Powered by Discourse, best viewed with JavaScript enabled"
612,time-magazine-names-nvidia-instant-nerf-a-best-invention-of-2022,"Originally published at:			TIME Magazine Names NVIDIA Instant NeRF a Best Invention of 2022 | NVIDIA Technical Blog
TIME Magazine named NVIDIA Instant NeRF, a technology capable of transforming 2D images into 3D scenes, one of the Best Inventions of 2022.  “Before NVIDIA Instant NeRF, creating 3D scenes required specialized equipment, expertise, and lots of time and money. Now it just takes a few photos and a few minutes,” TIME writes in their…Powered by Discourse, best viewed with JavaScript enabled"
613,rt-applications,"If you look at the typical RT applications like translucency, reflections, shadows, GI or diffuse illumination, are there some rules of thumb one should follow when starting off adding these to games? Specifically wrt performance, known pitfalls or typical errors one can do etc. And ideally some guidance on how to balance visual effects and RT versus Performance?Thanks for joining the AMA - and great question - its been queued up for the experts to look at - thanks !Excellent question - there are many things to take into consideration when adding ray traced effects to a game’s renderer. The main consideration to keep top of mind is for the ray traced effects to work hand-in-hand with the goals of your game’s art direction. This will change what performance costs are reasonable for any given effect. For example, if shadows are an important game mechanic (think of Splinter Cell) then a higher cost for extra nice ray traced shadows makes sense, but spending extra performance on RT translucency probably doesn’t make as much sense. For guidance on how to balance ray tracing and performance, we have a variety of webinars and other content that you can learn from. In fact, there’s a event coming up in August about RTX in Unreal Engine 5.Powered by Discourse, best viewed with JavaScript enabled"
614,diy-autonomous-car-racing-with-nvidia-jetson,"Originally published at:			DIY Autonomous Car Racing with NVIDIA Jetson | NVIDIA Technical Blog
Interested in seeing AI races in person? Visit our amAIzing race track to watch or compete as DIY autonomous cars battle it out to the finals. The DIY Robocars community will be revving up their RC-sized cars at NVIDIA’s GTC Silicon Valley 2020. Attendees are invited to root for their favorite team and learn about…Powered by Discourse, best viewed with JavaScript enabled"
615,unreal-engine-5-early-access-available-now-with-directx-raytracing-nvidia-dlss-and-nvidia-reflex-support,"Originally published at:			https://developer.nvidia.com/blog/unreal-engine-5-early-access-available-now-with-directx-raytracing-nvidia-dlss-and-nvidia-reflex-support/
Unreal Engine 5 (UE5) is available in Early Access, delivering the next-generation engine from Epic Games that will further propel the industry forward.Hi, is there any news on when RTXGI (DDGI) will be available for Unreal Engine 5?We are looking into it. Keep a look out for updates from Nvidia in the future.Powered by Discourse, best viewed with JavaScript enabled"
616,gtc-2020-deploying-a-scalable-gpu-as-a-service-platform-and-building-a-deep-learning-project-in-under-80-minutes,"GTC 2020 S22086
Presenters: Andrew Bull,NVIDIA; Adam Tetelman, NVIDIA; Mark Skinner, NVIDIA; Sumit Kumar, NVIDIA
Abstract
In less than 80 minutes, we’ll demonstrate how you can deploy and manage a scalable GPU cluster through DeepOps, an NVIDIA open-source project. We’ll start with deploying several cluster services, including Kubeflow — a project that makes it easy to build and deploy AI workflows on Kubernetes. Then we’ll dive into how you can use these cluster services with the NVIDIA Transfer Learning Toolkit from the NVIDIA GPU Cloud (NGC) to train a deep neural network model, and go through the process of deploying the trained model on the cluster with TensorRT Inference Server for a high-throughput, low-latency inferencing service.Watch this session
Join in the conversation below.Powered by Discourse, best viewed with JavaScript enabled"
617,building-world-class-ai-models-with-nvidia-nemo-and-definedcrowd,"Originally published at:			Building World-Class AI Models with NVIDIA NeMo and DefinedCrowd | NVIDIA Technical Blog
Speech is the most natural form of human communication. So, it’s not surprising that we’ve always wanted to interact with and command machines by voice. However, for conversational AI to provide a seamless, natural, and human-like experience, it needs to be trained on large amounts of data representative of the problem the model is trying…Powered by Discourse, best viewed with JavaScript enabled"
618,gtc-2020-3d-deep-learning-in-function-space,"GTC 2020 S21764
Presenters: Michael Niemeyer,MPI-IS and University of Tübingen
Abstract
Recent advances in GPU technology and scalable algorithms have led to breakthroughs in deep learning. In particular, convolutional neural networks (CNNs) achieve state-of-the-art results in longstanding vision problems, such as image classification or object detection. However, autonomous agents that navigate and interact in our world need to reason in 3D. Unlike images in the 2D case, it is not clear how to represent 3D geometry and how to make it amenable for deep-learning techniques. We’ll introduce our approach to learning 3D representations in function space. First, we’ll show how this approach can represent arbitrary topologies without discretization at fixed memory cost. Then we’ll extend this framework to learning to predict not only the shape of an object, but also its texture and motion. Finally, we’ll show how we can scale our method to real-world scenarios using state-of-the art NVIDIA GPU technology.Watch this session
Join in the conversation below.Really interesting and  well presented work Micheal.
You actually want to try this when you see your presentation.What are the limitations right know and what are your future work on this?
Ia this something for NVIDIA Isaac like system and autonomous systems right know?Hi! Amazing approach to 3D characterization. The 3D representations in function space is a really clever idea! But above all, very well explained: going into technical details, such the analytic gradient function, without losing the focus on the main idea… and only in 25 min. Bravo!I could see this being used in game development for making models. I could start off with just a picture and then refine the GPU generated model.Good idea using a continuous function to create a smooth object, where as the discrete methods generate ruff objects.Is your code on github? I would love to try it on my GTX 970. Thank you!It was actually 13 minutes. I like how it started off very simple and evolved into a very technical video.This repository contains the code for the CVPR 2020 paper ""Differentiable Volumetric Rendering: Learning Implicit 3D Representations without 3D Supervision"" - GitHub - autonomousvision/d...Nice presentation and interesting concept.Powered by Discourse, best viewed with JavaScript enabled"
619,sorting-shopping-lists-with-artificial-intelligence,"Originally published at:			https://developer.nvidia.com/blog/sorting-shopping-lists-with-deep-learning/
Instacart, an Internet-based grocery delivery service, shares how they are using deep learning to help their tens of thousands personal shoppers be more efficient. “By observing how our shoppers have picked millions of customer orders through our app, we have built models that predict the sequences our fastest shoppers will follow,” mentions Instacart VP of…Powered by Discourse, best viewed with JavaScript enabled"
620,does-the-training-dataset-contain-3d-models-for-the-simpsons-character,"GET3D seems to be able to generate the Simpson styled objects. The Simpsons as idea or concept is copyrighted as far as I know. Does the training dataset contain 3D models for the Simpsons character? If so, how do you avoid the copyright claim from its owners?Thank you for the question. The dataset used to train Get3D does not contain Simpsons characters. The examples you are probably referring to are from text-guided shape generation where we use pre-trained weights of StyleGan-Nada to guide the generation process to a specific style.Thank you. Best wishes.This topic was automatically closed 14 days after the last reply. New replies are no longer allowed.Powered by Discourse, best viewed with JavaScript enabled"
621,pgi-community-edition-17-4-now-available,"Originally published at:			PGI Community Edition 17.4 Now Available | NVIDIA Technical Blog
PGI compilers and tools are used by scientists and engineers who develop applications for high-performance computing (HPC) systems. They deliver world-class multicore CPU performance, an easy on-ramp to GPU computing with OpenACC directives, and performance portability across all major HPC platforms. New update now available at no cost. At least 52 applications in active development…Powered by Discourse, best viewed with JavaScript enabled"
622,tensorrt-radically-improves-real-time-object-detection-by-6x,"Originally published at:			TensorRT Radically Improves Real-Time Object Detection by 6x | NVIDIA Technical Blog
Researchers at SK Telecom developed a new method that uses the NVIDIA TensorRT high-performance deep learning inference engine to accelerate deep learning-based object detection. The method can be used on a variety of projects including monitoring patients in hospitals or nursing homes, performing in-depth player analysis in sports, to helping law enforcement find lost or…Powered by Discourse, best viewed with JavaScript enabled"
623,partitioning-offline-online-techniques,"How do you do partitioning of a graph? Do you have offline (you have infinite time) techniques as well as online (you’re seeing the graph for the first time and have to do it quickly)?We use vertex randomization (with a hash function) based 2D partitioning. This is quick and robust to graph updates. We currently do not use a more sophisticated partitioning scheme, but we can replace the default partitioning with a more complex strategy if necessary (we don’t see the need yet). Or if you need graph partitioning as a feature, please let us know.This topic was automatically closed 14 days after the last reply. New replies are no longer allowed.Powered by Discourse, best viewed with JavaScript enabled"
624,nvidia-deep-learning-institute-announces-public-workshop-summer-schedule,"Originally published at:			https://developer.nvidia.com/blog/nvidia-deep-learning-institute-announces-public-workshop-summer-schedule/
Workshops are conducted live in a virtual classroom environment with expert guidance from NVIDIA-certified instructors.Powered by Discourse, best viewed with JavaScript enabled"
625,goai-open-gpu-accelerated-data-analytics,"Originally published at:			GOAI: Open GPU-Accelerated Data Analytics | NVIDIA Technical Blog
Recently, Continuum Analytics, H2O.ai, and MapD announced the formation of the GPU Open Analytics Initiative (GOAI). GOAI—also joined by BlazingDB, Graphistry and the Gunrock project from the University of California, Davis—aims to create open frameworks that allow developers and data scientists to build applications using standard data formats and APIs on GPUs. Bringing standard analytics data…Can you share the code to generate the results in the video?Publishing a docker for this stack is perfect. Unfortunately, for some reason, I'm not able to overcome ""Temporary failure resolving 'archive.ubuntu.com'"" when executing ""docker build -t conda_cuda_base:latest ./base"".  The nvidia-docker2 installation appears to be working ok, but I'm not able to overcome the dependency on 'root' ...Any ideas?EddiePowered by Discourse, best viewed with JavaScript enabled"
626,nvidia-research-released-at-cvpr-helps-developers-create-better-visual-datasets,"Originally published at:			NVIDIA Research Released at CVPR Helps Developers Create Better Visual Datasets | NVIDIA Technical Blog
AI models that interpret and interact with the physical world rely on large image datasets, annotated so the neural network can learn exactly where one object ends and another begins. But most models rely on crowdsourced datasets that are imperfect for the job — because an AI learning from a messy dataset doesn’t know that…Powered by Discourse, best viewed with JavaScript enabled"
627,nvidia-gpus-help-developers-score-1-million-prize-for-improving-zillow-s-zestimate,"Originally published at:			NVIDIA GPUs Help Developers’ Score $1 Million Prize For Improving Zillow’s Zestimate | NVIDIA Technical Blog
To help prospective buyers and sellers know exactly how much a home is worth, Zillow, the online real estate company awarded a team of three, a million-dollar prize for developing an algorithm that can better predict the value of a home. After a two-year open challenge, the company awarded the coveted prize to a team…Powered by Discourse, best viewed with JavaScript enabled"
628,unreal-engine-4-25-released-ray-tracing-features-now-ready-for-production,"Originally published at:			Unreal Engine 4.25 Released, Ray Tracing Features Now Ready for Production | NVIDIA Technical Blog
All Unreal Engine developers can now produce sophisticated lighting simulations in their applications. With the release of update 4.25, the engine’s robust ray-tracing features have come out of beta, and are ready for production. It’s easier than ever to add global illumination, realistic shadows and reflections, and ambient occlusion to your art pipeline. This release…Powered by Discourse, best viewed with JavaScript enabled"
629,google-colabs-pay-as-you-go-offers-more-access-to-powerful-nvidia-compute-for-machine-learning,"Originally published at:			Colab's ‘Pay As You Go’ Offers More Access to Powerful NVIDIA Compute for Machine Learning — The TensorFlow Blog
Colabs’s new Pay as You Go option helps you accomplish more with machine learning. Access more time on NVIDIA GPUs and upgrade to NVIDIA A100 Tensor Core GPUs when you need more power.Powered by Discourse, best viewed with JavaScript enabled"
630,deploying-a-1-3b-gpt-3-model-with-nvidia-nemo-megatron,"Originally published at:			https://developer.nvidia.com/blog/deploying-a-1-3b-gpt-3-model-with-nvidia-nemo-megatron/
Large language models (LLMs) are some of the most advanced deep learning algorithms that are capable of understanding written language. Many modern LLMs are built using the transformer network introduced by Google in 2017 in the Attention Is All You Need research paper. NVIDIA NeMo Megatron is an end-to-end GPU-accelerated framework for training and deploying…I loved deploying NeMo Megatron locally to power language-based applications and look forward to seeing exciting new ways to use LLMs. Let me know if there are any questions and I will be happy to help!Hey that was a great article! It worked well (20b param) but I’m having no luck in changing the temperature. Tried changing it where it was defined and no luck, tried adding it to the argparser, and still no luck… what am I missing?!? and still no luck… what am I missing?!? and still no luck… what am I missing?!? and still nojkjk thank you for your patience and maybe your guidance :DThanks for the excellent tutorial!  Everything worked well.    I did have one question about the tokenizer.   Why is the tokenizer GPT2 even though the model is GPT3 ?Powered by Discourse, best viewed with JavaScript enabled"
631,cuda-8-features-revealed,"Linux. Yes, you need to use cudaMallocManaged to allocate Unified Memory for now, but you should get the page faulting behavior and you will have access to cudaMemAdvise and cudaMemPrefetchAsync, etc. We are working with the Linux community to allow Pascal to page fault on regularly allocated system memory, but that is not part of the CUDA Toolkit (read about HMM, e.g. https://www.phoronix.com/sc...GCC 6 support will be included in the next release of the CUDA Toolkit.Does the sentence ""PASCAL UNIFIED MEMORY"" means this feature not suit for previous GPU architecture, even with CUDA8?Pascal is the first GPU architecture with the page faulting capability. Unified Memory works on Kepler and Maxwell GPUs but it has limitations because the GPU is not able to page fault.Thanks for the explanation. Could you look into http://stackoverflow.com/qu..., which describes the problem of Pascal Titan X running slower than Maxwell Titan X. Is the behavior expected because of the new unified memory framework?Currently NVIDIA supports OpenCL 1.2.Will games be using unified memory soon?Is CUDA 8 going to support Pascal-based cards inside a Mac Pro tower running Mac OS X 10.11?Hey does cuda 8.0's prefectching, direct mapping or read duplication features work on any maxwell or kepler based cards? If not, what new features that cuda 8.0 introduces are supported on those chip types?While CUDA 8 includes performance improvements for multi-GPU Unified Memory on Maxwell and Kepler GPUs, the new API features (cudaMemPrefetchAsync() and cudaMemAdvise()) currently require and rely on Pascal hardware capabilities.Thanks mark!In the CUDA 8 download area there is only SLES 12. Does CUDA 8 also work on SLES 12 SP1? Thx, PeterCUDA 8 will probably work with SLES 12 SP1 but is not *officially* supported as it has not undergone thorough testing (like SLES 12 has). The next release of CUDA will add support for SLES SP2 (which should be out before the next CUDA release).Hey Mark, I'm totally new to CUDA. I've downloaded the run file of cuda 8.0 for ubuntu 16.04 LTS. while checking for the version with nvcc -V an error is coming stating that cannot executebinary file: Exec format error. I've tried setting up  PATH and LD_LIBRARY_PATH but for no use. could you please help me in thisSounds like you have the wrong binaries installed. Are you on an x86_64 machine? Did you install the x86_64 version of the toolkit?Thanks for the response Mark. I've understood the problem and now I'm running Cuda on windows. It's working fine now. Hi are there any plans to incorporate any aspects of the cuDNN library into the CUDA Toolkit?Are the following features for just the GP100 or are they features available for all Pascal GPUs?1) GP100 extends GPU addressing capabilities to enable 49-bit (512 TB) virtual memory addressing (note that GP100 also supports 47-bit (128 TB) physical memory addressing).2) Memory page faulting support in GP100 is a crucial new feature that provides more seamless Unified Memory functionality.Both are available for all Pascal GPUs. However GPU page faults, oversubscription, system-wide atomics, etc. are supported only on Linux today.Can gpu poll on an host address atomically until cpu signals there atomically ? Can this be quicker than simply synchronizing? I mean the uva and system level atomics and ubuntu and gt1030. For offloading to gpu as less latency as possible.Powered by Discourse, best viewed with JavaScript enabled"
632,nvidia-research-appearance-driven-automatic-3d-model-simplification,"Originally published at:			https://developer.nvidia.com/blog/nvidia-research-appearance-driven-automatic-3d-model-simplification/
NVIDIA will be presenting a new paper introducing a new method for generating level-of-detail of complex models, taking both geometry and surface appearance into account.Powered by Discourse, best viewed with JavaScript enabled"
633,thanks-for-joining-the-cuda-12-ama-is-now-closed,"If you do have questions please do post on our general Accelerated Computing Forums where the community and NVIDIA technical staff do engage and answer questions.Thanks again for participating in this AMA.Powered by Discourse, best viewed with JavaScript enabled"
634,scaling-tensorflow-and-caffe-to-256-gpus,"Originally published at:			Scaling TensorFlow and Caffe to 256 GPUs | NVIDIA Technical Blog
IBM Research unveiled a “Distributed Deep Learning” (DDL) library that enables cuDNN-accelerated deep learning frameworks like TensorFlow, Caffe, Torch and Chainer to scale to tens of IBM servers leveraging hundreds of GPUs. “With the DDL library, it took us just 7 hours to train ImageNet-22K using ResNet-101 on 64 IBM Power Systems servers that have…Powered by Discourse, best viewed with JavaScript enabled"
635,achieving-noise-free-audio-for-virtual-collaboration-and-content-creation-applications,"Originally published at:			Achieving Noise-Free Audio for Virtual Collaboration and Content Creation Applications | NVIDIA Technical Blog
Maxine’s Audio Effects SDK enables you to build applications that integrate features such as noise removal and room echo removal into your applications to improve audio quality. This post showcases these effects and how to build applications that provide high audio quality.This was a fun blog to write! Please feel free to ask any questions that you might have!Powered by Discourse, best viewed with JavaScript enabled"
636,novel-transformer-model-achieves-state-of-the-art-benchmarks-in-3d-medical-image-analysis,"Originally published at:			https://developer.nvidia.com/blog/novel-transformer-model-achieves-state-of-the-art-benchmarks-in-3d-medical-image-analysis/
The NVIDIA Swin UNETR model is the first attempt for large-scale transformer-based self-supervised learning in 3D medical imaging.I am trying to train on custom data using the colab file provided in the github i.e. "" swin_unetr_btcv_segmentation_3d.ipynb"". I am getting cuda out of memory error. How much cuda memory is required for the SWIN UNETR model? my patch size is (96,96,32). number of features=48. input channel = 1, output channel = 2transformers throwing VNet&UNet under the bus?Thanks for your interest in this work. We tested the memory usage locally with 3D volume of (96, 96, 32), batch size = 1, sample_num = , it takes around ~9G memory.
However, the memory usage also depends on how large the validation or test data are, a basic training/validation consumption is to use a 11G GPU with batch size=1, sample_num=1.Powered by Discourse, best viewed with JavaScript enabled"
637,profiling-the-ai-performance-boost-in-optix-5,"Originally published at:			https://developer.nvidia.com/blog/profiling-the-ai-performance-boost-in-optix-5/
By Vincent Brisebois and Ankit Patel, NVIDIA OptiX 5.0 introduces a new post-processing feature to denoise images. This denoiser is based on a paper published by NVIDIA research “Interactive Reconstruction of Monte Carlo Image Sequences using a Recurrent Denoising Autoencoder”. It uses GPU-accelerated artificial intelligence to dramatically reduce the time to render a high fidelity…Powered by Discourse, best viewed with JavaScript enabled"
638,retracing-ai-s-steps-go-explore-algorithms-solve-trickiest-atari-games,"Originally published at:			https://developer.nvidia.com/blog/retracing-ais-steps-go-explore-algorithms-solve-trickiest-atari-games/
A team of Uber AI researchers has achieved record high scores and beaten previously unsolved Atari games with algorithms that remember and build off their past successes.  Highlighted this week in Nature, the Go-Explore family of algorithms to address limitations of traditional reinforcement learning algorithms, which struggle with complex games that provide sparse or deceptive…Powered by Discourse, best viewed with JavaScript enabled"
639,automating-downloads-with-ngc-container-replicator-ready-to-run-on-singularity,"Originally published at:			Automating Downloads with NGC Container Replicator, Ready-to-Run on Singularity | NVIDIA Technical Blog
AI and HPC software environments present complex and time consuming challenges to build, test, and maintain. The pace of innovation continues to accelerate, making it even more difficult to provide an up-to-date software environment for your user community, especially for Deep Learning. With NGC, system admins can provide faster application access to users so that…Powered by Discourse, best viewed with JavaScript enabled"
640,is-cuda-12toolkit-with-tensorflow-and-nvidia-driver-535-compatible-on-nvidia-a100-i-seem-to-run-into-issues,"tf.config.list_physical_devices(‘GPU’)the above command returns and empty list instead of showing devicesmy os is ubuntu 20.4also can you help me know what tensorflow version are supported by cuda 12 and nvidia driver 535 .Yes, CUDA 12.x and R535 are compatible with the A100 GPUs on Ubuntu 20.04. You may want to double check that the GPU is visible through nvidia-smi. If it’s not, you may have a driver installation issue, or if it is you may have installed TensorFlow without GPU support or without the required dependencies.If you want further help debugging this it might be good to post on the general forum, we’re not able to get into specific issue debug here in this AMA.Thank you lastly one qestion like does running the code inside a  conda enviroment affect the command output for detecting tensorflow  also can you refer me  a guide which i could follow for proper driver and installation with cuda 12 it will  be great help for me.One great option for you might be to use one of the developer containers from NGC that comes with TensorFlow and other machine learning frameworks already set up in a Docker container ready-to-use. You can sidestep a lot of the environment setup and get straight to the good stuff: TensorFlow | NVIDIA NGCThese containers are usable on both Linux and Windows. But, for a specific TensorFlow setup guide using Anaconda I am unfortunately not the right person to ask, you might check on our ML forums for more information or refer to the TensorFlow documentation.unfournatley i cant use the container as we run our codes on virtual server and there are conda enviroment set up i seem to list the gou using driver 470 and cuda 11.8 also i can try it without the conda enviroment can you provide me some documents to follow to properly install latest nvidia driver and cuda 12 properly without the conda enviroment on nvidia a100Jump on over to this download page:Get the latest feature updates to NVIDIA's proprietary compute stack.Here you’ll find the Ubuntu 20.04 deb package, as well as instructions to get it installed. If you follow those, you wll end up with your system configured for 12.2 CUDA (paired with r535 driver), which is currently the latest versions released.Thank youPowered by Discourse, best viewed with JavaScript enabled"
641,nvidia-announces-availability-of-new-isaac-platform-to-power-next-gen-autonomous-machines,"Originally published at:			NVIDIA Announces Availability of New Isaac Platform to Power Next-Gen Autonomous Machines | NVIDIA Technical Blog
At Computex in Taiwan, NVIDIA CEO Jensen Huang announced the availability of the NVIDIA Isaac platform that includes new hardware, software and a virtual-world robot simulator. “AI is the most powerful technology force of our time. Its first phase will enable new levels of software automation that boost productivity in many industries,” said Huang. “Someday,…Powered by Discourse, best viewed with JavaScript enabled"
642,meet-the-researcher-anna-choromanska-optimizing-deep-learning-models-for-autonomous-vehicles-and-robotics,"Originally published at:			Meet the Researcher: Anna Choromanska, Optimizing Deep Learning Models for Autonomous Vehicles and Robotics | NVIDIA Technical Blog
‘Meet the Researcher’ is a new series in which we spotlight different researchers in academia who are using GPUs to accelerate their work. In this first edition, we spotlight Anna Choromanska, an assistant professor at the New York University (NYU) Tandon School of Engineering and a recipient of the Alfred. P. Sloan Fellowship. Choromanska’s NYU…Powered by Discourse, best viewed with JavaScript enabled"
643,ask-me-anything-unleashing-the-power-of-nvidia-rtx-path-tracing,"Originally published at:			Unleashing the Power of RTX Path Tracing : AMA June 6, 2023 - Path Tracing : AMA June 6, 2023 - NVIDIA Developer Forums
Join us on June 6 and learn how to mimic real-world lighting for lifelike 3D graphics with the NVIDIA RTX Path Tracing SDK.Powered by Discourse, best viewed with JavaScript enabled"
644,building-nvidia-gpu-accelerated-pipelines-on-azure-synapse-analytics-with-rapids,"Originally published at:			https://developer.nvidia.com/blog/building-nvidia-gpu-accelerated-pipelines-on-azure-synapse-analytics-with-rapids/
Azure recently announced support for NVIDIA’s T4 Tensor Core Graphics Processing Units (GPUs) which are optimized for deploying machine learning inferencing or analytical workloads in a cost-effective manner. With Apache Spark™ deployments tuned for NVIDIA GPUs, plus pre-installed libraries, Azure Synapse Analytics offers a simple way to leverage GPUs to power a variety of data…Powered by Discourse, best viewed with JavaScript enabled"
645,from-the-omniverse-experiment-archives-nvidia-omniverse-rtx-racing-demo-showcases-powerful-rendering-realistic-simulation,"Originally published at:			https://developer.nvidia.com/blog/nvidia-omniverse-rtx-racing-demo-showcases-powerful-rendering-realistic-simulation/
A team of NVIDIA artists released never-before-seen imagery and behind-the-scenes videos from the Omniverse RTX Racer playable sample project. The clips and imagery are the result of 3 weeks of progress, showcasing the Omniverse platform’s power in multi-GPU rendering, dynamic lighting, and real-time rendering.Why not using trees and ocean?  they are ever moving objects, real time rendering can be very interesting,  the environment is alive.Powered by Discourse, best viewed with JavaScript enabled"
646,sdks-accelerating-industry-5-0-data-pipelines-computational-science-and-more-featured-at-gtc-2023,"Originally published at:			https://developer.nvidia.com/blog/sdks-accelerating-industry-5-0-data-pipelines-computational-science-and-more-featured-at-gtc-2023/
At NVIDIA GTC 2023, NVIDIA unveiled notable updates to its suite of NVIDIA AI software for developers to accelerate computing. The updates reduce costs in several areas, such as data science workloads with NVIDIA RAPIDS, model analysis with NVIDIA Triton, AI imaging and computer vision with NVIDIA CV-CUDA, and many more. To keep up with…Powered by Discourse, best viewed with JavaScript enabled"
647,artificial-intelligence-can-beat-humans-at-31-atari-games,"Originally published at:			Artificial Intelligence Can Beat Humans at 31 Atari Games | NVIDIA Technical Blog
Google DeepMind revealed that its deep learning software is now able to outperform humans in 31 different Atari games. The algorithm, which uses reinforcement learning to master the games, has been described as the “first significant rung of the ladder” towards proving such a system can work, and a significant step towards use in real-world…Powered by Discourse, best viewed with JavaScript enabled"
648,updating-the-cuda-linux-gpg-repository-key,"I run sudo apt update && sudo apt full-upgrade -y every day. Today it failed for an CUDA related reason that I think is related to this key rotation. Could anyone guide me on resolving this issue? I can’t update or install new packages now.I followed the steps in the blog:Every time I run sudo apt-key del 7fa2af80 I get this output:I’m concerned that I can’t update due to apt-key deprecation or that I haven’t deleted and updated they older key fast enough. I’m also concerned the problem with the MergeList is due to something at the file /var/lib/apt/lists/developer.download.nvidia.com_compute_cuda_repos_ubuntu2204_x86%5f64_Packages and wonder if there is a way I can reset that file somehow.Hi @MicahParks
Thank you for bring this to my attention. The default .deb compression changed from XZ to Zstandard in Ubuntu 22.04, which is not recognized by the build of the dpkg executable currently in use for updating the repository metadata.We had four postings yesterday, one of which was for NCCL; these .deb packages are compressed with Zstandard, other packages in the repository continue use XZ compression.Working to resolve this issue on our end, though it may also require users to delete /var/lib/apt/lists/*cuda_repos* after the repository metadata has been re-generated.Hi. It would be great if CuDNN could also be added to the 22.04 repos.
None of the libcudnn8 files seem to be available for 22.04 yet on ::
https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64/
Thanks.Would like to see cudnn/trt/nccl for debian 11 as well.@kmittman Thank you for your response. Could you link the other postings so I can follow along for updates? My plan is to wait for a fix since I don’t want to change anything now and require more work when an official fix is available. In the meantime, I can’t update apt or install new packages, which is limiting during development, so I’ll want to be in the loop for any official fixes.Hi @MicahParks
The Ubuntu 22.04 repository metadata issue has been resolved (the two NCCL .deb packages were removed for now).
Added a pinned topic here: Notice: Ubuntu 22.04 repository MergeList corruptionUnfortunately, this requires manually deleting malformed MergeList files on machines that ran apt-get update during the affected period.Sorry for the inconvenience.Hi @steven.ramboer and @xkszltl
I have filed an internal task to notify the cuDNN, NCCL, and TensorRT teams about this request for Ubuntu 22.04 .deb packages to be available (hopefully sooner than later). Also will ask about .deb packages for the Debian 11 repo too.Fantastic. Looks like everything is back to normal. Thank you, @kmittman.Hi all. For anyone annoyed by the apt-key warning :)
Easy fix …Thanks! Had to keep stealing things from 20.04 to 11 and hope that’ll be fixed soon.For whoever had the same challenge, here’s how you can do it properly, with priority to avoid overwriting debian 11’s own deb: Roaster/repo.sh at 460b2e126909448e0c285b3de4eaeb4ce5cdb009 · xkszltl/Roaster · GitHubI’ve discovered this discussion while trying to update some CentOS 7 severs.I have tried to follow the instructions here but they simply do not work. There is some circular logic happening and I have no idea how to break it:I remove the bad key using sudo rpm -e gpg-pubkey-7fa2af80-576db785I verify that it is gone with rpm -q gpg-pubkey --qf ‘%{NAME}-%{VERSION}-%{RELEASE}\t%{SUMMARY}\n’Yet when I try to install the latest nvidia driver, the bad key is RE-DOWNLOADED!How can I get the correct key? RPM is supposed to download it according to the documentation yet it’s getting the old key. Please help!Hi @lh2332
Yes, you will need to update the .repo file for RHEL-based, Fedora, and SUSE distros. The cuda-rhel7.repo file on your system is pointing at the old GPG key location, which is why it is re-downloading it.This is mentioned very briefly in the CUDA Installation GuideFor upgrades, you must also also fetch an updated .repo entry:sudo yum-config-manager --add-repo https://developer.download.nvidia.com/compute/cuda/repos/$distro/$arch/cuda-$distro.repoIt should like look thisHi @kmittmanThanks so much for the prompt reply.I actually did that step, and the file is correct but the error persists.-LokkeHi @kmittmanI got it to work by deleting the key, using wget to download the key, and then using rpmkeys to install the key manually.I just wanted you to know the other method was failing-LokkeI just wanted you to know the other method was failingDo you happen to have other cuda repos listed under /etc/yum.repos.d?
Yours called cuda in log, but these days it’s called cuda-rhel7, so probably you have an old version as well and that’s the one complaining.Ah yes there are other cuda repos. Since my fix worked, I’m not going to mess with that however but will keep it in mind.Thanks for the help!Go into /etc/apt/sources.list.d and delete cuda.list and cuda_learn.list (there should be another file there e.g. cuda-ubuntu2004-x86_64.list)I’ve solved it with “gpgcheck” off :[cuda-rhel7-x86_64]
name=cuda-rhel7-x86_64
baseurl=Index of /compute/cuda/repos/rhel7/x86_64
enabled=1
gpgcheck=0
…#yum updateHi @schwab1976
I do not recommend disabling the GPG signature check. This is an important security feature.Updating the local gpg-pubkey on RHEL7-like distros can be achieved by refreshing the .repo file like so:Below is a demonstration of the migration from the old GPG key to the new GPG key.The deprecated GPG pubkey does not match the RPMs in the repository …Then force refresh the .repo fileNow packages can be installedit seems the keys changed today. it is working!Powered by Discourse, best viewed with JavaScript enabled"
649,nvidia-deep-learning-inference-platform-performance-study,"Originally published at:			NVIDIA Deep Learning Inference Platform Performance Study | NVIDIA Technical Blog
The NVIDIA deep learning platform spans from the data center to the network’s edge. A new paper describes how the platform delivers giant leaps in performance and efficiency, resulting in dramatic cost savings in the data center and power savings at the edge. High-level deep learning workflow showing training, then followed by inference. GPUs have…Powered by Discourse, best viewed with JavaScript enabled"
650,announcing-the-summer-of-jetson-sparkfun-contest,"Originally published at:			Submit Your Jetson Project to the SparkFun and NVIDIA Community Project Contest! - News - SparkFun Electronics
Create a project using the NVIDIA Jetson Nano developer kit and submit it by September 30, 2022 for a chance to win a Machine Learning at Home Kit.Powered by Discourse, best viewed with JavaScript enabled"
651,machine-learning-frameworks-interoperability-part-1-memory-layouts-and-memory-pools,"Originally published at:			https://developer.nvidia.com/blog/machine-learning-frameworks-interoperability-part-1-memory-layouts-and-memory-pools/
This post discussing pros and cons of distinct memory layouts as well as memory pools for asynchronous memory allocation to enable zero-copy functionality.Powered by Discourse, best viewed with JavaScript enabled"
652,nvidia-announces-nsight-graphics-2020-5,"Originally published at:			https://developer.nvidia.com/blog/nvidia-announces-nsight-graphics-2020-5/
Nsight Graphics 2020.5 is now available for download. With the release of the brand new NVIDIA Ampere microarchitecture, we’ve made some significant updates. We also have an exciting announcement for Vulkan developers.Powered by Discourse, best viewed with JavaScript enabled"
653,building-optimal-asn-configurations-in-data-centers-with-auto-bgp,"Originally published at:			https://developer.nvidia.com/blog/building-optimal-asn-configurations-in-data-centers-with-auto-bgp/
The NVIDIA Cumulus Linux 4.2.0 release introduces a new feature called auto BGP, which makes BGP ASN assignment in a two-tier leaf and spine network configuration quick and easy. Auto BGP does the work for you without making changes to standard BGP behavior or configuration so that you don’t have to think about which numbers…Powered by Discourse, best viewed with JavaScript enabled"
654,jetpack-3-1-doubles-jetson-s-low-latency-inference-performance,"Originally published at:			JetPack 3.1 Doubles Jetson’s Low-Latency Inference Performance | NVIDIA Technical Blog
Today, NVIDIA released JetPack 3.1, the production Linux software release for Jetson TX1 and TX2. With upgrades to TensorRT 2.1 and cuDNN 6.0, JetPack 3.1 delivers up to a 2x increase in deep learning inference performance for real-time applications like vision-guided navigation and motion control, which benefit from accelerated batch size 1. The improved features…Powered by Discourse, best viewed with JavaScript enabled"
655,accelerating-with-xdp-over-mellanox-connectx-nics,"Originally published at:			Accelerating with XDP over Mellanox ConnectX NICs | NVIDIA Technical Blog
This post was originally published on the Mellanox blog. XDP (eXpress Data Path) is a programmable data path in the Linux kernel network stack. It provides a framework to BPF and can enable high performance packet processing at runtime. XDP works in concert with the Linux network stack and is not a kernel bypass. Because…Powered by Discourse, best viewed with JavaScript enabled"
656,a-startup-is-automating-home-insurance-quotes-with-ai,"Originally published at:			https://developer.nvidia.com/blog/a-startup-is-automating-home-insurance-quotes-with-ai/
Home insurance quotes could become more accurate as a result of work by Palo Alto-based Cape Analytics, which raised $14 million to use computer vision and deep learning to improve automated property underwriting for insurance companies. The company brings geo-imagery from satellites into its cloud-based platform, and then runs the images through their deep learning…Powered by Discourse, best viewed with JavaScript enabled"
657,using-matlab-and-tensorrt-on-nvidia-gpus,"Originally published at:			https://developer.nvidia.com/blog/using-matlab-and-tensorrt-on-nvidia-gpus/
As we design deep learning networks, how can we quickly prototype the complete algorithm—including pre- and postprocessing logic around deep neural networks (DNNs) —to get a sense of timing and performance on standalone GPUs? This question comes up frequently from the scientists and engineers I work with. Traditionally, they would hand translate the complete algorithm…As far as I know, the inference process using INT8 with TensorRT requires calibration of the network in MATLAB with a dataset. This has not been carried out in the work explained in this article. Could you give an explanation about this, please?Also, I am training a deep CNN on raw time-series data which I have stored on the hard disk as MAT files. I noticed that TensorRT accepts only images for the calibration process. Is there a trick to calibrate my network using the MAT files?Powered by Discourse, best viewed with JavaScript enabled"
658,managing-edge-ai-with-the-nvidia-launchpad-free-trial,"Originally published at:			https://developer.nvidia.com/blog/managing-edge-ai-with-fleet-command-and-launchpad/
Try Fleet Command for free on NVIDIA LaunchPad.Powered by Discourse, best viewed with JavaScript enabled"
659,accelerating-redis-performance-using-vmware-vsphere-8-and-nvidia-bluefield-dpus,"Originally published at:			https://developer.nvidia.com/blog/accelerating-redis-performance-using-vmware-vsphere-8-and-nvidia-bluefield-dpus/
A shift to modern distributed workloads, along with higher networking speeds, has increased the overhead of infrastructure services. There are fewer CPU cycles available for the applications that power businesses. Deploying data processing units (DPUs) to offload and accelerate these infrastructure services delivers faster performance, lower CPU utilization, and better energy efficiency. Many modern workloads…Powered by Discourse, best viewed with JavaScript enabled"
660,how-to-overlap-data-transfers-in-cuda-c-c,"I added mentions of the CUDA 7 behavior along with a link to the post GPU Pro Tip: CUDA 7 Streams Simplify Concurrency.Hi. I want to also overlap/hide the memory copy from host pageable to host pinned (following the model in your last post) but cudaMemcpyAsync with hosttohost does not do. it also destroy the overlapping of memcpyasync from hosttodevice. do you have any idea why?Hi Mark,I have a question, if the time required for the host-to-device transfer, kernel execution, and device-to-host transfer are not the same, as contrary to the above post. Then is there any formula to compute ""the optimal number of streams to be created"", for example on a Tesla K40 GPU?To be more precise, if the time to transfer from HtoD (input data) is much higher than DtoH (result data) and the time for kernel execution is even higher than both the memory transfer, then is there any formula to compute the optimal number of streams to be created to achieve maximum performance.If there is any documentation or papers on this it would be of great help, if you can cite them here.Thanks in advanceHi, thank you for this great article. I have some observation with Quadro K420. When using multiple streams on their own CPU threads and synchronizing(after each copy + kernel + copy) within their own CPU threads, they get serialized in timeline. When I enqueue many copy + kernel + copy per stream and synchronize only once at the end, they all overlap. Why would cuStreamSynchronize(streamHandle) stop other streams overlapping with this? I tried changing synchronization policies such as spin wait, block and yield. They all do same. How can I copy+kernel+copy+synchronize on different threads with their own streams and expect them to overlap in time? It does this on only first sync but can't overlap anything on next syncs.Maybe this is possible with only hyper-q?Note: all CPU threads I mentioned are completely free of each other. They don't wait for any specific order. They just issue commands to their own streams as soon as possible(maybe not a good practice but) then expect drivers to handle the overlapping.- Tested with both WDDM and TCC mode (I have 2 of same card)- Using driver api equivalent commands (with async suffix).- If arrays are not pinned, they do overlap but nearly %30 slower overall- kernel is just vecAdd and data is 1M unsigned char elements per stream (for a,b,c arrays)- arrays are same but regions are 1M leaping per stream- 3 streams- tried with and without #define CUDA_API_PER_THREAD_DEFAULT_STREAMIt's really hard to help without more detailed information, and it's hard to debug programs in the comments of a blog post. May I suggest you post your question, along with a test program, on either the cuda tag of StackOverflow or on devtalk.nvidia.com forums? The experts on those sites are likely to be able to help find the issue. Thanks!Thank you very much. I'll prepare a retriggerable version and post them.Issue was environment variable for cuda max connections. Setting it to 16 and using TCC mode solved the problem.Thanks for sharing your solution!Forgot to say this was windows.In linux, all are ok with or without max connections setting.Maybe windows is not so focuesd on computing.Hi,Nice article. Definitely a good read.I had a clarification which may not have been considered by some. This article and method assumes that all the data would fit on the GPU to be run in a single stream (stream0) right? In other words, this method would not work if I were trying to overlap processing and data transfer for a workload which does not fit in the GPU main memory all at once. Is there a way to signal the next memory preload as soon as the current main memory data is moved to local scratch pad memory? I am imagining this optimization for something like PiRNA which takes an enormous amount of memory to process.ThanksThanks a lot for the article Mark. I notice that the kernel executed in sequential version uses 4x more threads compared to the kernels executed in the asynchronous versions. However, the each of the kernels in the asynchronous version only spent 1/4 time compared to the kernel in sequential version. I was expecting the they should almost be the same. Could you please explain why? Thank you very much.First of all, I would like to thank for coming up with such an elaborative and wonderful article.I just had one query. As per this article, ""When multiple kernels are issued back-to-back in different streams, the scheduler tries to enable concurrent execution of these kernels and as a result delays a signal that normally occurs after each kernel completion"". I would just like to ask whether this property is followed in all the different types of GPU devices or this is a device specific property. If it is a device specific property, do we have any method to find out whether this occurs in any particular GPU device.Hi @user34605 , there is a device property you can query called concurrentKernels.  See the cudaDevAttrConcurrentKernels attribute and the cudaDeviceGetAttribute API.At this time, most modern CUDA GPUs support concurrent kernels. However some Tegra (embedded) GPUs may not (I’m not sure about this), especially small GPUs with only a single multiprocessor.Glad you liked the post, thanks!Currently, I am using a volta V100-32GB GPU. The value of the variable cudaDeviceProp:asyncEngineCount for this GPU is 6.So, can it allow 6 concurrent copying operation ( 3 in D2H direction and other 3 in H2D direction) ?Assume that a GPU device allows 4 concurrent copying operations (2 in D2H direction and 2 in H2D direction).I issue following two copying operations in the same direction and in different streams:cudaMemcpyAsync(A, dA, n, cudaMemcpyDeviceToHost,stream[i])cudaMemcpyAsync(B, dB, n, cudaMemcpyDeviceToHost,stream[i+1])Will these two copy operations happen concurrently?In some of the V100-32GB GPUs, the value of cudaDeviceProp:asyncEngineCount is 5.What is the interpretation of having an odd number of copy engines? Does it mean that 2 copy engines are used for copying in D2H direction, the other 2 engines are used for copying in H2D direction and the remaining one is used for copying in D2D direction?In a system like DGX-1, there are 8 V100 GPUs connected via NVLink. The topology is such that each GPU is connected to 4 others. The topology is shown in Figure 4 of the DGX-1 white paper.5 copy engines enables copying data to or from all 4 connected neighbor GPUs and the CPU simultaneously (while also computing). This enables a system such as DGX-1 to keep all GPUs busy.MarkHello,
The number of copy engines in my device, V100-32 GPU, is 6.The overall execution time of the following two copying operations in the same direction in different streams is 10.1 seconds:cudaMemcpyAsync(A, dA, n, cudaMemcpyDeviceToHost,stream[i])cudaMemcpyAsync(B,dB,n, cudaMemcpyDeviceToHost,stream[i+1])When I issue them in same streams as follows, the combined overall execution time remains same, i.e., 10.1 seconds:cudaMemcpyAsync(A, dA, n, cudaMemcpyDeviceToHost,stream[i])cudaMemcpyAsync(B, dB, n, cudaMemcpyDeviceToHost,stream[i])So, can I conclude from this that multiple copying operations could not be performed in parallel in same direction, despite having multiple copy engines??The corresponding interconnect is NVLink here.Dear @Mark_Harris
Thanks for all the explanations and the article. I have two more small questions:If this is correct, then in a set of A30s (which have 3 engines), one device could exchange data with a second device via PCIe and simultanously exchange data with a third device via NVLink (bidirectional exchange for both PCIe and NVLink). Am I understanding this correctly?Cheers
DaveHi! thanks, nice articlePowered by Discourse, best viewed with JavaScript enabled"
661,explainer-what-is-the-metaverse,"Originally published at:			What Is the Metaverse? | NVIDIA Blog
The metaverse is the “next evolution of the internet, the 3D internet,” according to NVIDIA CEO Jensen Huang.Powered by Discourse, best viewed with JavaScript enabled"
662,explainer-what-is-zero-trust,"Originally published at:			What Is Zero Trust? | NVIDIA Blogs
Zero trust is a cybersecurity strategy for verifying every user, device, application, and transaction in the belief that no user or process should be trusted.Powered by Discourse, best viewed with JavaScript enabled"
663,emory-university-students-win-amazon-s-alexa-prize-for-their-ai-chatbot,"Originally published at:			Emory University Students Win Amazon’s Alexa Prize for their AI Chatbot | NVIDIA Technical Blog
A team of Emory University students won Amazon’s 2020 Alexa Socialbot Grand Challenge, a worldwide competition to create that most engaging AI chatbot. The team earned $500,000 for their chatbot named Emora.  The researchers developed Emora as a social companion that can provide comfort and warmth to people interacting with Alexa-enabled devices. Emora can chat…Powered by Discourse, best viewed with JavaScript enabled"
664,gtc-2020-aiaas-scaling-ai-infrastructure-for-the-enterprise-from-dgx-1-to-superpod,"GTC 2020 S21996
Presenters: Michael Balint,NVIDIA; John Barco, NVIDIA
Abstract
Are you getting the most out of your GPU-accelerated hardware? AIaaS (AI-as-a-Service) deployments provide powerful new ways for enterprises to stand up AI infrastructure on top of GPU-accelerated servers (such as NVIDIA DGX Servers). Data scientist users can interact with the cluster via GUI or a simplified command-line interface, where knowledge of underlying container, orchestration, and scheduling technologies is obfuscated. Such systems allow administrators to maximize use of resources and users to get more work done. We’ll focus on the variety of AIaaS stacks, considerations, and how to deploy them. We’ll demonstrate scaling common AI workloads from a single GPU on an NVIDIA DGX-1 to a SuperPOD cluster (64x DGX-2). We’ll also discuss methods to integrate storage and manage datasets, tie in authentication and authorize users, train on NGC containers, and deploy models to production.Watch this session
Join in the conversation below.Powered by Discourse, best viewed with JavaScript enabled"
665,ai-powered-robotic-cameraman-tracks-moving-objects,"Originally published at:			AI-Powered Robotic Cameraman Tracks Moving Objects | NVIDIA Technical Blog
A team of students and alumni from UC Santa Cruz developed a prototype device called Raven that clips onto a GoPro camera and automatically tracks a selected target with the help of computer vision. Enes Mentese, cofounder of Raven, thought of the idea for the device after he filmed his snowboarding experience and majority of…Powered by Discourse, best viewed with JavaScript enabled"
666,an-introduction-to-the-nvidia-optical-flow-sdk,"Originally published at:			An Introduction to the NVIDIA Optical Flow SDK | NVIDIA Technical Blog
NVIDIA’s Turing GPUs  introduced a new hardware functionality for computing optical flow between images with very high performance. The Optical Flow SDK 1.0 enables developers to tap into the new optical flow functionality. You can download the Optical Flow SDK 1.0 from the NVIDIA developer zone. Until a few years ago, tasks such as recognizing and tracking…Hello,Where can I find full source code of ""Using Optical Flow on Decoded Frames from NVDEC"" ? , I can't find it in SDK samples.I am also having trouble finding the source code for the NVOF API. Does NVIDIA have a link they can share or is this API’s source code proprietary?The article says : “NVIDIA GPUs from Maxwell, Pascal, and Volta generations include one or more video encoder (NVENC) engines which provided a mode called Motion-Estimation-only mode. This mode allowed users to run only motion estimation on NVENC and retrieve the resulting motion vectors (MVs).”We found the cheapest NVIDIA Maxwell-based card is OEM GTX 745. But with latest driver from November 2021 and Windows 10 build 19041 the test software do not found hardware ME functionality. The returned D3D12_VIDEO_SIZE_RANGE SizeRange are all zeroes and ID3D12VideoMotionEstimator COM object can NOT be created from ID3D12VideoDevice1 interface.How to find which cheapest NVIDIA cards can fully support DX12 Hardware Motion Estimation functionality. We tested on GTX 1060 and 1070 cards but they are too expensive now for poor developers.Also do any software emulation possible for developing applications using DX12 Motion Estimation API on the system without hardware chip providing DX12 Hardware Motion Estimation functionality ? I currently in the development attempt to add DX12 ME function to freeware plugin to Avisynth - mvtools2. Using remote debugging to the system with GTX1060 card is rare possible and unability to have software emulation slow down the development process.Powered by Discourse, best viewed with JavaScript enabled"
667,cyber-security-analysis-beginner-s-guide-to-processing-security-logs-in-python,"Originally published at:			https://developer.nvidia.com/blog/cyber-security-analysis-beginners-guide-to-processing-security-logs-in-python/
This is the last installment of the series of articles on the RAPIDS ecosystem with this being the ninth installment. The series explores and discusses various aspects of RAPIDS that allow its users solve ETL (Extract, Transform, Load) problems, build ML (Machine Learning) and DL (Deep Learning) models, explore expansive graphs, process signal and system…Powered by Discourse, best viewed with JavaScript enabled"
668,ai-powered-nightmare-machine-generates-horrifying-images,"Originally published at:			AI-Powered ‘Nightmare Machine’ Generates Horrifying Images | NVIDIA Technical Blog
MIT researchers developed an algorithm trained to generate horrifying images in an attempt to find the scariest faces and locations possible, and then rely on humans to see which approach makes the freakiest images. Using TITAN X GPUs and cuDNN to train their deep learning models, the researchers used the infamous style transfer technique and…Powered by Discourse, best viewed with JavaScript enabled"
669,new-video-top-5-ai-developer-stories-of-the-month,"Originally published at:			New Video: Top 5 AI Developer Stories of the Month | NVIDIA Technical Blog
Every month we bring you the top NVIDIA updates and stories for developers. In this month’s edition of our top 5 videos, we highlight the latest version of TensorRT 6, plus, we highlight an NVIDIA Jetson-based robodog designed to serve as a service dog. Watch below: 5 – TensorRT 6 Now Available; Helps Break BERT-Large…Powered by Discourse, best viewed with JavaScript enabled"
670,university-of-pisa-enables-remote-research-with-virtualized-ai,"Originally published at:			https://developer.nvidia.com/blog/university-of-pisa-enables-remote-research-with-virtualized-ai/
The NVIDIA AI Enterprise software suite includes key enabling technologies and software from NVIDIA for rapid deployment, management, and scaling of AI workloads in the virtualized data center.Powered by Discourse, best viewed with JavaScript enabled"
671,painting-a-clearer-picture-of-the-heart-with-gpu-accelerated-computing,"Originally published at:			Painting a Clearer Picture of the Heart with GPU-accelerated Computing | NVIDIA Technical Blog
Coronary artery disease affects more than two million people annually in the United States, and is the single largest health problem in the world. The condition is normally caused by plaque buildup, which leads the coronary arteries to narrow. To help doctors improve the efficiency of diagnosis, researchers from IBM are using high-performance computing, mathematics,…Powered by Discourse, best viewed with JavaScript enabled"
672,nvidia-gtc-industrial-at-the-edge,"Originally published at:			https://developer.nvidia.com/blog/nvidia-gtc-industrial-at-the-edge/
Learn more about the NVIDIA GTC manufacturing sessions.Powered by Discourse, best viewed with JavaScript enabled"
673,gtc-2020-from-high-to-low-level-a-comparative-study-of-programming-approaches-for-nvidia-gpus,"GTC 2020 S21308
Presenters: Joshua Romero,NVIDIA; Mauro Bisson,NVIDIA
Abstract
Learn about various available methods to program NVIDIA GPUs, from using high-level GPU libraries in Python to optimized CUDA C programming. We’ll discuss the development and performance of several implementations of software to simulate the 2D Ising model for spin systems, comparing the different programming approaches in terms of development effort and simulation performance. We’ll show how Python, in combination with the Numba/CuPy packages, enables users to write programs, with all the productivity benefits of a high-level language, that can still provide competitive performance to lower-level implementations. We’ll also highlight some performance pitfalls encountered with these tools and discuss how we addressed them. Finally, we’ll compare performance against published results on other hardware platforms and show that even simple programming methods on GPUs can provide competitive performance, while our optimized low-level implementation can rapidly simulate lattices sizes outside the scope of comparable field-programmable gate array solutions.Watch this session
Join in the conversation below.Link to pdf file for the session is brokenThanks I’ll have it checked - thanks for reporting.
Update:
Looks like we are still waiting for the pdf, the link was premature.
Please check back next week.Powered by Discourse, best viewed with JavaScript enabled"
674,mit-researchers-use-deep-learning-to-develop-real-time-3d-holograms,"Originally published at:			https://developer.nvidia.com/blog/mit-3d-holograms-deep-learning/
Computer-generated holograms powered by deep learning could make real-time 3D holography feasible on laptops and smartphones, an advancement with potential applications in fields including virtual reality, microscopy, and 3D printing. Published this week in Nature, an MIT study outlines a novel approach called tensor holography, where researchers trained and optimized a convolutional neural network to…Powered by Discourse, best viewed with JavaScript enabled"
675,ai-of-earthshaking-magnitude-deepshake-predicts-quake-intensity,"Originally published at:			https://developer.nvidia.com/blog/deepshake-predicts-quake-intensity/
In a major earthquake, even a few seconds of advance warning can help people prepare — so Stanford University researchers have turned to deep learning to predict strong shaking and issue early alerts.The response time of the system is what allows AI systems to have an advantage over the response time of relief groups, in addition to allowing the prediction of behavior in general, based on the premises that can be identified and processedPowered by Discourse, best viewed with JavaScript enabled"
676,deep-learning-can-effectively-identify-sleep-stages,"Originally published at:			Deep Learning Can Effectively Identify Sleep Stages | NVIDIA Technical Blog
According to a recent study, up to one billion people worldwide suffer from obstructive sleep apnea (OSA), a nocturnal breathing disorder that causes a major impact on healthcare systems and national economies.  Helping physicians more effectively identify sleep stages is essential in the diagnostics of sleep disorders including OSA. A team of researchers in Finland…Powered by Discourse, best viewed with JavaScript enabled"
677,gtc-2020-sensor-processing-with-the-nvidia-driveworks-sdk-abstraction-algorithms-and-acceleration,"GTC 2020 S21714
Presenters: Hope Allen,NVIDIA; Miguel Sainz,NVIDIA
Abstract
Autonomous vehicles (AV) rely on sensors to represent the world around them, so onboard processing must react to rapidly changing environments. The NVIDIA DriveWorks SDK enables developers to implement such AV solutions by providing an exhaustive library of software modules and tools that leverage the computing power of the NVIDIA DRIVE AGX platform. With DriveWorks, developers can focus on their applications instead of spending time on fundamental functionality and infrastructure.
Our session will cover the DriveWorks Sensor Abstraction Layer, a unified interface for sensor life-cycle management, timestamp synchronization, and recording. We’ll then discuss the DriveWorks optimized low-level image and point cloud processing modules for processing incoming sensor data to enable advanced AV algorithms. DriveWorks also supports the DRIVE AGX hardware engines so that these modules can seamlessly run across the Xavier SoC, providing flexibility and performance.Watch this session
Join in the conversation below.Powered by Discourse, best viewed with JavaScript enabled"
678,gtc-2020-machine-learning-on-the-edge-for-5g-signal-processing,"GTC 2020 S22144
Presenters: Alex Keller,NVIDIA; Nikolaus Binder,NVIDIA
Abstract
Edge computing means processing information close to where it’s generated. In modern mobile communication, reasons to do so include latency constraints, bandwidth reduction, and saving energy. We’ll shed light on three machine learning approaches that are especially suitable for edge computing on modern GPUs and show their interrelations. First, we’ll discuss and demonstrate the application of hardware ray tracing in mobile communication. Then, we’ll review efficiency gains from neural networks trained sparse from scratch. Finally, we’ll look at kernel-based learning methods with applications to channel estimation and beam forming. Combined on one platform, kernel-based methods for online learning, artificial neural networks for fast data understanding, reinforcement learning for prediction, and hardware-accelerated ray tracing are the basis of highly efficient and scalable solutions for edge computing on GPUs for 5G.Watch this session
Join in the conversation below.Powered by Discourse, best viewed with JavaScript enabled"
679,nvidia-inception-partners-win-ucsf-health-awards-using-latest-ai-technologies,"Originally published at:			NVIDIA Inception Partners Win UCSF Health Awards Using Latest AI Technologies | NVIDIA Technical Blog
Read about the nine NVIDIA Inception partners named winners or finalists at the UCSF Health Awards.Powered by Discourse, best viewed with JavaScript enabled"
680,ai-system-used-at-gymnastics-world-championships-helps-judges-with-scoring,"Originally published at:			AI System Used at Gymnastics World Championships Helps Judges with Scoring | NVIDIA Technical Blog
To assist gymnastics judges in their decision-making process, Fujitsu developed a GPU-accelerated deep learning application that can sense the movements of athletes and analyze them as scoring data.  “[This is] a big step towards the future,” said, Morinari Watanabe, president of the International Gymnastics Federation “Scoring controversies must become a thing of the past, and…Powered by Discourse, best viewed with JavaScript enabled"
681,designing-digital-twins-with-flexible-workflows-on-nvidia-base-command-platform,"Originally published at:			https://developer.nvidia.com/blog/designing-digital-twins-with-flexible-workflows-on-nvidia-base-command-platform/
Creating high-fidelity digital twins across teams and locations using NVIDIA Modulus with NVIDIA Base Command Platform is the newest tool available for HPC workflows.Hi! I’m Joe, one of the authors of this blog - we’re excited to show off some of the hard work from our Modulus team on Base Command Platform, powering DGX Cloud. I’m happy to answer any questions that come to mind after reading our post!Hello,Hi, Joe, …
I’ll have further questions on the expectable output from DGX Cloud interactions, …I watched videos, read much of the material (glance-wise), and now am hoping simply to “Anchor” said process, with personal interact… (The following, mostly brief, …tweet-length circumstance/definition.)Link, to Document Remainder [Multiple Links]:Access Google Docs with a personal Google account or Google Workspace account (for business use).J.Powered by Discourse, best viewed with JavaScript enabled"
682,a-guide-to-monitoring-machine-learning-models-in-production,"Originally published at:			https://developer.nvidia.com/blog/a-guide-to-monitoring-machine-learning-models-in-production/
How can machine learning models in production be monitored effectively? What specific metrics need to be monitored? What tools are most effective? Get the answers to these questions and more.""""In the context of machine learning, monitoring refers to the process of tracking the behavior of a deployed model to analyze performance. Monitoring a machine learning model after deployment is vital, as models can break and degrade in production. Deployment is not a one-time action that you do and forget about. “”Deployment is not  — I guess it should say, monitoring is not a one time actionPowered by Discourse, best viewed with JavaScript enabled"
683,developers-design-innovative-network-security-solutions-at-the-nvidia-cybersecurity-hackathon,"Originally published at:			https://developer.nvidia.com/blog/developers-design-innovative-network-security-solutions-at-the-nvidia-cybersecurity-hackathon/
The latest NVIDIA Cybersecurity Hackathon brought together 10 teams to create exciting cybersecurity innovations using the NVIDIA Morpheus cybersecurity AI framework, NVIDIA BlueField data processing unit (DPU), and NVIDIA DOCA. The event featured seven onsite Israeli teams and three remote teams from India and the UK. Working around the clock for 24 hours, the teams…In this Hackathon I personally met with diverse teams and learned about the great solutions they invented. It was great seeing how quickly developers adopt DOCA and start developing their own apps. Kudos to all the teams and developers!
If you have any comments or questions, please let us know.Powered by Discourse, best viewed with JavaScript enabled"
684,kiwibot-jetson-powered-robot-arrives-in-san-jose-with-shopify-and-ordermark,"Originally published at:			Kiwibot, Jetson-Powered Robot, Arrives in San Jose with Shopify and Ordermark | NVIDIA Technical Blog
Kiwibot, a Colombia-based robotics startup, in residence at the University of California, Berkeley’s Skydeck accelerator, is expanding to San Jose, California. Starting today the company will begin delivering food and goods to patrons in downtown San Jose in partnership with Shopify and Ordermark.  The expansion is part of the company’s strategic shift to a business-to-business API…Powered by Discourse, best viewed with JavaScript enabled"
685,worlds-first-real-time-3d-oil-painting-simulator,"Originally published at:			World’s First Real-Time 3D Oil Painting Simulator | NVIDIA Technical Blog
The painting and drawing tools most people use are 2D, but now a new project gives artists the ability to choose any brush they like, a limitless array of paint colors, and use the same natural twists and turns of the brush to create the rich textures of oil painting, all on a digital canvas.…Powered by Discourse, best viewed with JavaScript enabled"
686,trimble-explores-acceleration-of-autonomous-robot-training-with-synthetic-data-generation-and-nvidia-isaac-sim,"Originally published at:			https://developer.nvidia.com/blog/trimble-explores-acceleration-of-autonomous-robot-training-with-synthetic-data-generation-and-nvidia-isaac-sim/
Learn how Trimble used NVIDIA Isaac Sim on Omniverse to generate synthetic datasets to train a robot for an indoor operating environment.Excited to share the work that @nyla.worker and team has been doing with Trimble on leveraging synthetic data in Isaac Sim. If there are any questions or comments, let us know.Powered by Discourse, best viewed with JavaScript enabled"
687,get-the-inside-track-on-building-self-driving-cars-at-gtc-drive-developer-day,"Originally published at:			Get the Inside Track on Building Self-Driving Cars at GTC DRIVE Developer Day | NVIDIA Technical Blog
Editors note: Our annual GPU Technology Conference will be virtual. Stay tuned for more details. Autonomous vehicles are a complex AI challenge. Bringing them to market requires sharing knowledge and expertise in solutions from end-to-end. GTC is the place to experience what’s next for AI-powered transportation. And this year, NVIDIA DRIVE customers will get access to…Powered by Discourse, best viewed with JavaScript enabled"
688,nvidia-opens-omniverse-platform-to-developers-worldwide,"Originally published at:			https://developer.nvidia.com/blog/nvidia-opens-omniverse-platform-to-developers-worldwide/
NVIDIA Developer Program provides access to integrated technologies for simulation and real time rendering with Omniverse.Powered by Discourse, best viewed with JavaScript enabled"
689,nvidia-shines-at-2021-sonic-plugfest,"Originally published at:			https://developer.nvidia.com/blog/nvidia-shines-at-2021-sonic-plugfest/
Using the 100% open-source Pure SONiC, NVIDIA excelled in all the tests at the 2021 SONiC PlugFest.Powered by Discourse, best viewed with JavaScript enabled"
690,12-gtc-2015-sessions-not-to-miss,"Originally published at:			https://developer.nvidia.com/blog/12-gtc-2015-sessions-not-to-miss/
With one week to go until we all descend on GTC 2015, I’ve scoured through the list of Accelerated Computing sessions and put together 12 diverse “not to miss” talks you should add to your planner. This year, the conference is highlighting the revolution in Deep Learning that will affect every aspect of computing. GTC…Powered by Discourse, best viewed with JavaScript enabled"
691,how-to-build-a-winning-recommendation-system-part-1,"Originally published at:			How to Build a Winning Recommendation System, Part 1 | NVIDIA Technical Blog
Recommender systems (RecSys) have become a key component in many online services, such as e-commerce, social media, news service, or online video streaming. However with their growth in importance,  the growth in scale of industry datasets, and more sophisticated models, the bar has been raised for computational resources required for recommendation systems.  After NVIDIA introduced Merlin…Powered by Discourse, best viewed with JavaScript enabled"
692,explainer-what-is-a-qpu,"Originally published at:			What Is a QPU? | NVIDIA Blogs
A QPU is the brain of a quantum computer that uses the behavior of particles like electrons or photons to make certain kinds of calculations much faster than processors in today’s computers.Powered by Discourse, best viewed with JavaScript enabled"
693,drive-labs-detecting-road-markings-and-landmarks-with-high-precision,"Originally published at:			https://developer.nvidia.com/blog/drive-labs-detecting-road-markings-and-landmarks-with-high-precision/
In this week’s DRIVE Labs, we present the evolution of LaneNet DNN, which delivers high-precision, stable detection of painted lane lines on the road, into our high-precision MapNet DNN.Powered by Discourse, best viewed with JavaScript enabled"
694,building-scientifically-accurate-digital-twins-using-modulus-with-omniverse-and-ai,"Originally published at:			https://developer.nvidia.com/blog/building-scientifically-accurate-digital-twins-using-modulus-with-omniverse-and-ai/
The latest release features Omniverse integration for interactive visualization, new AI architectures to accelerate the scientific simulations using data, and more.Powered by Discourse, best viewed with JavaScript enabled"
695,omniverse-user-group-spotlights-talented-community-members,"Originally published at:			https://developer.nvidia.com/blog/omniverse-user-group-spotlights-talented-community-members/
At NVIDIA GTC, the Omniverse User Group held its 2nd meeting, focusing on developers and users of the NVIDIA open platform for collaboration and simulation.Powered by Discourse, best viewed with JavaScript enabled"
696,using-deep-learning-to-help-solve-the-spacenet-road-detection-challenge,"Originally published at:			Using Deep Learning to Help Solve the SpaceNet Road Detection Challenge | NVIDIA Technical Blog
In the third SpaceNet challenge, competitors were tasked with finding automated methods for extracting map-ready road networks from high-resolution satellite imagery. This move towards automated extraction of road networks will help bring innovation to computer vision methodologies applied to high-resolution satellite imagery and ultimately help create better maps where they are needed most such as…Powered by Discourse, best viewed with JavaScript enabled"
697,cuda-for-arm-platforms-is-now-available,"Originally published at:			https://developer.nvidia.com/blog/cuda-arm-platforms-now-available/
In 2012 alone, over 8.7 billion ARM-based chips were shipped worldwide. Many developers of GPU-accelerated applications are planning to port their applications to ARM platforms, and some have already started. I recently chatted about this with John Stone, the lead developer of VMD, a high performance (and CUDA-accelerated) molecular visualization tool used by researchers all over the world.…Powered by Discourse, best viewed with JavaScript enabled"
698,new-ray-tracing-sdk-improves-memory-allocation-for-games,"Originally published at:			https://developer.nvidia.com/blog/new-ray-tracing-sdk-improves-memory-allocation-for-games/
Real-time ray tracing has advanced the art of lighting in video games, but it’s a computationally expensive process. Aiming to reduce these costs, NVIDIA has developed a memory utility that combines both compaction and suballocation techniques to optimize and reduce memory consumption of acceleration structures.Powered by Discourse, best viewed with JavaScript enabled"
699,protecting-sensitive-data-and-ai-models-with-confidential-computing,"Originally published at:			https://developer.nvidia.com/blog/protecting-sensitive-data-and-ai-models-with-confidential-computing/
Rapid digital transformation has led to an explosion of sensitive data being generated across the enterprise. That data has to be stored and processed in data centers on-premises, in the cloud, or at the edge. Examples of activities that generate sensitive and personally identifiable information (PII) include credit card transactions, medical imaging or other diagnostic…Powered by Discourse, best viewed with JavaScript enabled"
700,share-your-science-understanding-agricultural-genomics-with-gpus,"Originally published at:			Share Your Science: Understanding Agricultural Genomics with GPUs | NVIDIA Technical Blog
Understanding the critical genetic traits of agricultural crops will help develop crops faster in order to handle the world’s population growth and climate change. In a collaboration between Biochemist Alex Feltus and Computer Engineering PhD student Karan Sapra, the Clemson University researchers have created an interactive visualization tool using NVIDIA GPUs that allows them to…Powered by Discourse, best viewed with JavaScript enabled"
701,edit-photos-with-gans,"Originally published at:			Edit Photos with GANs | NVIDIA Technical Blog
In machine learning, a generative model learns to generate samples that have a high probability of being real samples like the samples from the training dataset. Generative Adversarial Networks (GANs) are a very hot topic in Machine Learning. A typical GAN comprises two agents: a Generator G that produces samples, and a Discriminator D that receives samples from both G and…Powered by Discourse, best viewed with JavaScript enabled"
702,share-your-science-accelerating-cognitive-workloads-with-machine-learning,"Originally published at:			Share Your Science: Accelerating Cognitive Workloads with Machine Learning | NVIDIA Technical Blog
Ruchir Puri, an IBM Fellow at IBM’s Thomas J. Watson Research Center shares how they are building large-scale big data systems and delivering real-time solutions, such as using machine learning and GPUs to predict drug reactions. Learn more about the work IBM Thomas J. Watson Research Center is doing with GPUs at Thomas J. Watson Research Center - Locations Share your…Powered by Discourse, best viewed with JavaScript enabled"
703,upcoming-webinar-deep-learning-demystified,"Originally published at:			NVIDIA Emerging Chapters Education Series
Join NVIDIA on December 1 at 3 pm GMT to learn the fundamentals of accelerated data analytics, high-level use cases, and problem-solving methods.Powered by Discourse, best viewed with JavaScript enabled"
704,introducing-ray-tracing-in-unreal-engine-4,"Originally published at:			Introducing Ray Tracing in Unreal Engine 4 | NVIDIA Technical Blog
Ray tracing in Unreal Engine 4 is a powerful and flexible lighting system. It’s powerful because of its accuracy and quality. Never have you had the ability to do things like shadows and reflections like this in real time. It’s flexible because you can mix raster and ray tracing rendering features together as you see…Powered by Discourse, best viewed with JavaScript enabled"
705,upcoming-event-register-to-watch-deepu-talla-at-the-re-mars-keynote,"Originally published at:			Amazon re:MARS 2022
Sign up to watch Deepu Talla present “Machines That Can See: Deploying Computer Vision into Production” during the re:Mars keynote session June 24, from 9-11 am.Powered by Discourse, best viewed with JavaScript enabled"
706,gtc-2020-creating-physically-based-materials-for-minecraft-with-rtx,"GTC 2020 S22677
Presenters: Kelsey Blanton,NVIDIA; Paula Jukarainen, NVIDIA; Joel Garvin , Microsoft; , ; , ; , ; , ; , ; , ; , ; ,
Abstract
We will describe and demonstrate the tools and pipeline that enable anyone to create physically based materials for ray tracing in Minecraft with RTX. We will create traditional 16x16 physical textures, as well as high definition ones and demonstrate them in Minecraft, with characteristics such as metallicity, roughness, emissivity and transparency. This talk serves as a great visual and practical introduction to ray tracing basics, physically based rendering and materials, and modding Minecraft with RTX.Watch this session
Join in the conversation below.Powered by Discourse, best viewed with JavaScript enabled"
707,aligning-time-series-at-the-speed-of-light,"Originally published at:			https://developer.nvidia.com/blog/aligning-time-series-at-the-speed-of-light/
To say it with the words of Eamonn Keogh: “Time series is a ubiquitous and increasingly prevalent type of data […]”. Virtually any incrementally measured signal, be it along a time axis or a linearly ordered set, can be treated as time series. Examples include electrocardiograms, temperature or voltage measurements, audio, server logs, but also…Hi Christian,My name is Bastien. I work at the GET lab in Toulouse (France). We found a very interesting application for your RapidAligner library to evaluate waveform similarity and improve the detection of small earthquakes. We would like to contact you by mail to give you more details about what we did and eventually further discuss the method. The thing is that we could not find your address on the internet.Le me know the best way to get in touch with you.Best regards,BastienPowered by Discourse, best viewed with JavaScript enabled"
708,icymi-nvidia-jetson-for-robot-operating-system,"Originally published at:			https://developer.nvidia.com/blog/icymi-nvidia-jetson-for-robot-operating-system/
In this month’s technical digest we’re highlighting this powerful capability and offer a collection of resources to help ROS users familiarize with the power of Jetson, ISAAC SDK, ISAAC Sim and a success story from NVIDIA Inception Member, Aerobotics.Powered by Discourse, best viewed with JavaScript enabled"
709,developer-news-weekly-top-5-video-episode-1,"Originally published at:			Developer News Weekly Top 5 Video: Episode 1 | NVIDIA Technical Blog
In this week’s edition of the Developer Top 5 we revisit the top stories of the week. From GPU-accelerated weather forecasts, Stephen Curry and robots, to new GPU availability on Google Cloud. Watch via the link below. 5 – New GPU-accelerated Weather Forecasting System Dramatically Improves Accuracy At CES in Las Vegas, Nevada, The Weather Company,…Powered by Discourse, best viewed with JavaScript enabled"
710,facing-the-edge-data-challenge-with-hpc-ai,"Originally published at:			https://developer.nvidia.com/blog/facing-the-edge-data-challenge-with-hpc-ai/
NVIDIA Holoscan for HPC brings AI to edge computing. Streaming Reactive Framework will be released in June to simplify code changes to stream AI for instrument processing workflows.Powered by Discourse, best viewed with JavaScript enabled"
711,supercharging-the-world-s-fastest-ai-supercomputing-platform-on-nvidia-hgx-a100-80gb-gpus,"Originally published at:			https://developer.nvidia.com/blog/supercharging-worlds-fastest-ai-supercomputing-platform-on-hgx-a100-80gb-gpus/
Exploding model sizes in deep learning and AI, complex simulations in high-performance computing (HPC), and massive datasets in data analytics all continue to demand faster and more advanced GPUs and platforms. At SC20, we announced the NVIDIA A100 80GB GPU, the latest addition to the NVIDIA Ampere family, to help developers, researchers, and scientists tackle…Powered by Discourse, best viewed with JavaScript enabled"
712,ai-powered-crib-cam-monitors-your-baby,"Originally published at:			https://developer.nvidia.com/blog/ai-powered-crib-cam-monitors-your-baby/
BabbyCam is a new deep learning baby monitor that recognizes your baby, monitors their emotions and will alert you if their face is covered. As a new parent himself, the developer of the camera was in search for a solution with the ability to identify if the infant was on its stomach, one of the…Powered by Discourse, best viewed with JavaScript enabled"
713,new-dli-hands-on-course-shows-how-to-optimize-and-deploy-tensorflow-models-with-nvidia-tensorrt,"Originally published at:			New DLI Hands-On Course Shows How to Optimize and Deploy TensorFlow Models with NVIDIA TensorRT | NVIDIA Technical Blog
The NVIDIA Deep Learning Institute (DLI) offers hands-on training in AI, accelerated computing, and accelerated data science. Developers, data scientists, researchers, and students can get practical experience powered by GPUs in the cloud and earn a certificate of competency to support professional growth. Now, as developers continue to work remotely, NVIDIA is offering new online…Powered by Discourse, best viewed with JavaScript enabled"
714,ai-system-understands-music-like-humans-do,"Originally published at:			AI System Understands Music Like Humans Do | NVIDIA Technical Blog
Researchers at MIT and Stanford University developed a deep learning system that can process sounds just like humans. The method, which is the first model of its kind, can replicate listening tasks such as identifying a musical genre or identifying words. The researchers built the model to shed light on how the human brain may…Powered by Discourse, best viewed with JavaScript enabled"
715,merging-telemetry-and-logs-from-microservices-at-scale-with-apache-spark,"Originally published at:			Merging Telemetry and Logs from Microservices at Scale with Apache Spark | NVIDIA Technical Blog
One of the most common challenges with big data is the ability to merge data from several sources with minimal cost and latency. It’s an even bigger challenge merging from various streaming sources in near-real time—along with batch logs data—in a continuous fashion. At NVIDIA, GeForce NOW is a cloud game-streaming service where users can…Powered by Discourse, best viewed with JavaScript enabled"
716,deep-learning-in-a-nutshell-core-concepts,"Originally published at:			https://developer.nvidia.com/blog/deep-learning-nutshell-core-concepts/
This post is the first in a series I’ll be writing for Parallel Forall that aims to provide an intuitive and gentle introduction to deep learning. It covers the most important deep learning concepts and aims to provide an understanding of each concept rather than its mathematical and theoretical details. While the mathematical terminology is…great intro. Keep it up!!Do you have a printable version of this course?Hi there. Could you make a printable PDF version of your posts?Thank you so much and cheers from Brazil.Great Intro! Thanks... I think the most beautiful idea is that why a non-linear transform is needed to make a layer..Nice and Easy to Understand .In Chrome, Ctrl+p and select ""Simplify page"". It looks perfect to me.It seems straight forward to get it in Chrome:Ctrl+p and select ""Simplify page"".Wow! Pretty good summary! I had to go through several sources to find what this articles describes really well.Is there a pdf version of all parts in one file ?Do you have a printable version of this course?I think the most beautiful idea is that why a non-linear transform is needed to make a layer...Thanks for this gentle introduction to deep learning.Thanks Nice introduction to Deep Learning !!Thanks a lot for the intro helped a lot, I happened to come across another very detailed explanation on artificial neural networksPowered by Discourse, best viewed with JavaScript enabled"
717,just-released-cuda-toolkit-12-1,"Originally published at:			https://developer.nvidia.com/blog/just-released-cuda-toolkit-12-1/
Available now for download, the CUDA Toolkit 12.1 release provides support for NVIDIA Hopper and NVIDIA Ada Lovelace architecture.Powered by Discourse, best viewed with JavaScript enabled"
718,using-vrworks-in-the-cloud-with-pixvana-spin-studio,"Originally published at:			Using VRworks in the Cloud with Pixvana SPIN Studio | NVIDIA Technical Blog
Pixvana’s cloud-based VR pipeline now incorporates the NVIDIA VRWorks 360 Video SDK. Pixvana strives to solve a number of challenges facing the VR creator by leveraging the power of cloud computing. Highlights of Pixvana’s VR pipeline include: A video player with spatialized audio for all the major VR headsets (Oculus Go, Oculus Rift, HTC Vive,…Powered by Discourse, best viewed with JavaScript enabled"
719,nvidia-dlss-updates-for-super-resolution-and-unreal-engine,"Originally published at:			https://developer.nvidia.com/blog/nvidia-dlss-updates-for-super-resolution-and-unreal-engine/
Learn about the latest NVIDIA DLSS updates for Super Resolution and Unreal Engine.Powered by Discourse, best viewed with JavaScript enabled"
720,ai-is-changing-how-enterprises-manage-edge-applications,"Originally published at:			https://developer.nvidia.com/blog/ai-is-changing-how-enterprises-manage-edge-applications/
AI-based applications demand entirely new tools and procedures to deploy and manage at the edge. Learn about the distinctive challenges associated with edge AI application deployment.Powered by Discourse, best viewed with JavaScript enabled"
721,ai-art-gallery-ai-in-the-hand-of-the-artist,"Originally published at:			AI Art Gallery: AI in the Hand of the Artist | NVIDIA Technical Blog
NVIDIA recently reached the 2 million registered developers milestone. To help commemorate the milestone, artist Pindar Van Arman this week at GTC painted a unique portrait of the NVIDIA Developer community. Using portraits submitted at the recent GTC AI Art Gallery, Pindar enlisted the help of an AI-painting robot to complete the job.  Pindar uses…Powered by Discourse, best viewed with JavaScript enabled"
722,how-gpu-computing-is-changing-the-shape-of-architecture,"Originally published at:			https://developer.nvidia.com/blog/how-gpu-computing-is-changing-the-shape-of-architecture/
Combine the Concorde airplane’s swooping lines, the Sydney Opera House’s soaring spaces and the intricacies of a bird’s nest, and you’ll begin to describe the transformative work of architect Daghan Cam. The teaching fellow at University College London’s Bartlett School of Architecture is using GPU computing to render stunning, abstract 3D-printed designs and train image-processing…Powered by Discourse, best viewed with JavaScript enabled"
723,ai-helps-predict-soybean-production-in-argentina-and-brazil,"Originally published at:			AI Helps Predict Soybean Production in Argentina and Brazil | NVIDIA Technical Blog
Researchers from Stanford University recently developed a deep learning-based system that can predict soybean production from satellite imagery.   “Accurate prediction of crop yields in developing countries in advance of harvest time is central to preventing famine, improving food security, and sustainable development of agriculture,” the researchers stated in their paper.  “Existing techniques are expensive…Powered by Discourse, best viewed with JavaScript enabled"
724,gtc-2020-few-shot-adaptive-gaze-estimation,"GTC 2020 S22192
Presenters: Shalini De Mello,NVIDIA
Abstract
Deep networks designed to observe human behavior that are trained on one set of subjects often do not perform optimally on others. Gaze-estimation is one such problem, where anatomical variations between subjects limit the performance of cross-person networks. Personalizing neural networks for each person on devices on the edge is the key to obtaining the best performance in such scenarios. However, this entails collecting thousands of training samples per person at the deployment site, which is simply not viable. To solve this, we present a novel and effective algorithm for training gaze networks with very few (less than 10) training examples per subject to create highly accurate, personalized models for them. We leverage two ideas to achieve this challenging goal: a) learning a compact interpretable latent representation for our task, and b) meta-learning the algorithm to effectively train person-specific networks in the few-shot manner without overfitting.Watch this session
Join in the conversation below.Powered by Discourse, best viewed with JavaScript enabled"
725,gtc-2020-data-center-monitoring-and-profiling,"GTC 2020 CWE21705
Presenters: Brent-Stolle,NVIDIA; David-Beer, ; Ahmed-Al-Sudan, ; Nik-Konyuchenko, ; Pramod-Ramarao,
Abstract
Connect with developers from NVIDIA’s Data Center GPU Manager software (https://developer.nvidia.com/dcgm) on how to effectively monitor NVIDIA GPUs in your data center. Ask questions and see demos of new Data Center Profiling (DCP) features that allow you to monitor high-resolution profiling counters across your data center. Additionally, we can help you strategize how to integrate DCGM monitoring into third-party tools like Kubernetes, Prometheus, Collectd, Telegraf, and other data collectors.Watch this session
Join in the conversation below.Powered by Discourse, best viewed with JavaScript enabled"
726,accelerating-deep-learning-research-in-medical-imaging-using-monai,"Originally published at:			Accelerating Deep Learning Research in Medical Imaging Using MONAI | NVIDIA Technical Blog
The Medical Open Network for AI (MONAI), is a freely available, community-supported, PyTorch-based framework for deep learning in healthcare imaging. It provides domain-optimized, foundational capabilities for developing a training workflow. Building upon the GTC 2020 alpha release announcement back in April, MONAI has now released version 0.2 with new capabilities, examples, and research implementations for…Powered by Discourse, best viewed with JavaScript enabled"
727,turing-multi-view-rendering-in-vrworks,"Originally published at:			Turing Multi-View Rendering in VRWorks | NVIDIA Technical Blog
Virtual reality displays continue to evolve and now include advanced configurations such as canted HMDs with non-coplanar displays. Other headsets offer ultra-wide fields-of-view as well as other novel configurations. NVIDIA Turing GPUs incorporate a new feature called Multi-View Rendering (MVR) which expands upon Single Pass Stereo, increasing the number of projection views for a single…This seems very similar to D3D12_VIEW_INSTANCING_TIER_3, is that the native DX12 API for this hardware functionality, or is this exclusively supported by NvAPI for now?Powered by Discourse, best viewed with JavaScript enabled"
728,gtc-digital-capturing-the-reality-of-space-with-rtx-in-deliver-us-the-moon,"Originally published at:			GTC Digital: Capturing the Reality of Space with RTX in ‘Deliver Us the Moon’ | NVIDIA Technical Blog
At GTC Digital KeokeN describes the company’s experience using ray tracing to enable new graphical realism in Deliver Us The Moon, with UE4 and NVIDIA’s RTX technology.  The game is a Sci-Fi thriller set in an apocalyptic near-future where Earth’s natural resources are depleted. In the talk, the team describes what optimizations they did, why…Powered by Discourse, best viewed with JavaScript enabled"
729,state-of-the-art-language-modeling-using-megatron-on-the-nvidia-a100-gpu,"Originally published at:			State-of-the-Art Language Modeling Using Megatron on the NVIDIA A100 GPU | NVIDIA Technical Blog
Recent work has demonstrated that larger language models dramatically advance the state of the art in natural language processing (NLP) applications such as question-answering, dialog systems, summarization, and article completion. However, during training, large models do not fit in the available memory of a single accelerator, requiring model parallelism to split the parameters across multiple…Could I expect to be able to run this Megatron Q&A model on a Jetson Xavier NX device if it was the only model loaded?Powered by Discourse, best viewed with JavaScript enabled"
730,ai-helps-visualize-human-cells-like-we-ve-never-seen-them-before,"Originally published at:			AI Helps Visualize Human Cells Like We’ve Never Seen Them Before | NVIDIA Technical Blog
The method has been described as a “total game changer.” Researchers at the Allen Institute for Cell Science, a Seattle research group founded by Microsoft co-founder Paul Allen, announced this week they developed the first predictive 3D model of a live human cell. The model, called the Allen Integrated Cell, uses the power of deep…Powered by Discourse, best viewed with JavaScript enabled"
731,nvidia-jetpack-3-2-production-release-now-available,"Originally published at:			https://developer.nvidia.com/blog/nvidia-jetpack-3-2-production-release-now-available/
JetPack 3.2 with L4T R28.2 is the latest production software release for NVIDIA Jetson TX2, Jetson TX2i and Jetson TX1. It bundles all the Jetson platform software, including TensorRT, cuDNN, CUDA Toolkit, VisionWorks, GStreamer, and OpenCV, all built on top of L4T with LTS Linux kernel. Jetson TX2 is the fastest, most power-efficient AI computing…Powered by Discourse, best viewed with JavaScript enabled"
732,artificial-intelligence-generates-christmas-song-from-holiday-image,"Originally published at:			https://developer.nvidia.com/blog/artificial-intelligence-generates-christmas-song-from-holiday-image/
Researchers from University of Toronto developed an AI system that creates and then sings a Christmas song based by analyzing the visual components of an uploaded image. With the help of CUDA, Tesla K40 GPUs and cuDNN to train their deep learning models, the researchers trained their neural network on 100 hours of online music.…Powered by Discourse, best viewed with JavaScript enabled"
733,new-compiler-features-in-cuda-8,"Originally published at:			https://developer.nvidia.com/blog/new-compiler-features-cuda-8/
CUDA 8 is one of the most significant updates in the history of the CUDA platform. In addition to Unified Memory and the many new API and library features in CUDA 8, the NVIDIA compiler team has added a heap of improvements to the CUDA compiler toolchain. The latest CUDA compiler incorporates many bug fixes,…I'd love a nice work-stealing task library like Intel's Tthreading Building Blocks on CUDA.Lost registers made working with CUDA 8 package? GPU programming is a future, but unluckily would become popular within dozen years.Better ask Intel for making theirs integrated GPU's working for programming stuff. The same architecture for few dozen years in electronics - no comment left in my opinion.... But Intel's psyhicists are great.Powered by Discourse, best viewed with JavaScript enabled"
734,raising-the-bar-for-path-traced-rendering-in-unreal-engine-5-q-amp-a-with-3d-artist-daniel-martinger,"Originally published at:			https://developer.nvidia.com/blog/raising-the-bar-for-path-traced-rendering-in-unreal-engine-5-qa-with-3d-artist-daniel-martinger/
3D Artist Daniel Martinger discusses how he captured the attention of the computer graphics world with a path-traced rendered scene using an NVIDIA RTX 3090 and Unreal Engine 5.Powered by Discourse, best viewed with JavaScript enabled"
735,gtc-2020-dl-basics,"GTC 2020 CWE22315
Presenters: ,
Abstract
This is an introductory session where we discuss the basics of deep learning. We will answer questions on training, inference, DL models and performance optimization on the GPU.Watch this session
Join in the conversation below.Powered by Discourse, best viewed with JavaScript enabled"
736,five-features-for-enhancing-your-workspace-with-nvidia-rtx-software,"Originally published at:			https://developer.nvidia.com/blog/five-features-for-enhancing-your-workspace-with-nvidia-rtx-software/
Learn how you can make the most out of graphics workflows with NVIDIA RTX Desktop Manager and NVIDIA RTX Experience.Powered by Discourse, best viewed with JavaScript enabled"
737,gtc-2020-scaling-deep-learning-interpretability-by-visualizing-activation-and-attribution-summarizations,"GTC 2020 S21808
Presenters: Polo Chau,The Georgia Institute of Technology; Fred Hohman,Georgia Tech
Abstract
We’ll present Summit, an interactive system that scalably and systematically summarizes and visualizes the features that a deep learning model has learned, and how those features interact to make predictions. Summit introduces two new scalable summarization techniques: activation aggregation discovers important neurons, and neuron-influence aggregation identifies relationships among such neurons. Summit combines these techniques to create the novel attribution graph that reveals and summarizes crucial neuron associations and substructures that contribute to a model’s outcomes. Summit scales to large data, such as the ImageNet dataset with 1.2 million images, and leverages neural network feature visualization and dataset examples to help users distill large, complex neural network models into compact, interactive visualizations. We’ll present neural network exploration scenarios where Summit helps us discover multiple surprising insights into a prevalent, large-scale image classifier’s learned representations and informs future neural network architecture design. The Summit visualization runs in modern web browsers and is open sourced.Watch this session
Join in the conversation below.Powered by Discourse, best viewed with JavaScript enabled"
738,upcoming-event-vpi-and-pytorch-interoperability-demo,"Originally published at:			https://info.nvidia.com/vpi-and-pytorch-interop.html
Join this webinar on June 14 and learn how to program computer vision algorithms using VPI’s Python interface.Powered by Discourse, best viewed with JavaScript enabled"
739,deeppavlov-open-source-framework-for-building-chatbots-available-on-ngc,"Originally published at:			DeepPavlov, Open-Source Framework for Building Chatbots, Available on NGC | NVIDIA Technical Blog
DeepPavlov is an open-source framework for building chatbots and virtual assistants. It comes with a set of predefined components for solving Natural Language Processing (NLP) related problems and a framework for building a modular pipeline. This lets developers and NLP researchers create production-ready conversational skills and complex multi-skill conversational assistants. DeepPavlov is used across many…Powered by Discourse, best viewed with JavaScript enabled"
740,developing-accelerated-code-with-standard-language-parallelism,"Originally published at:			Developing Accelerated Code with Standard Language Parallelism | NVIDIA Technical Blog
Learn how standard language parallelism can be used for programming accelerated computing applications on NVIDIA GPUs with ISO C++, ISO Fortran, or Python.Powered by Discourse, best viewed with JavaScript enabled"
741,accelerating-standard-c-with-gpus-using-stdpar,"Originally published at:			https://developer.nvidia.com/blog/accelerating-standard-c-with-gpus-using-stdpar/
Historically, accelerating your C++ code with GPUs has not been possible in Standard C++ without using language extensions or additional libraries: CUDA C++ requires the use of host and device attributes on functions and the triple-chevron syntax for GPU kernel launches.OpenACC uses #pragmas to control GPU acceleration.Thrust lets you express parallelism portably but uses language…This was a great article! It appears that all the discussions and example are based on accelerating standard C++ (code) without any need for CUDA programming but only on one single GPU.From my work so far on multi-GPU programming, invoking two GPUs and partitioning the data in between always needs some CUDA related code – for instance, binding a MPI rank or a thread to one of the GPUs, or using CUDA Streams for simultaneous use of multiple GPUs and probably other approaches to enable accelerations on multi-GPUs all need selecting the device one way or another which needs CUDA.All of these are in the opposite direction of “Accelerating Standard C++ with a GPU Using stdpar”, where the goal is to not change the CPU-based code (with no CUDA runtime API, etc.) and compile the code simply with NVC++. So I’m very curious if there any way around this currently, and if not is this something to look forward to in the future? I’d appreciate any insights here.Yes, multi-GPU stdpar support is on the roadmap.Great to hear! Here in August 2021 is there any new developments on the NVC++ compiler using multiple GPUs?Not yet I’m afraid, stay tuned!I understand that the containers in use must be using the heap, not the stack, in order for unified memory to have the data visible to both CPU and GPU.  My question is whether or when it will be possible for the containers memory to be a mmap pointer instead of a RAM pointer?Hi, thanks for the question. We are working on enabling more memory types such as stack memory for use with the parallel algorithms. mmap memory is not on our near-term roadmap, but I have forwarded your inquiry on to the team.I’ve created a benchmark for Standard C++ Parallel STL functions. When compiling it with nvc++ with -stdpar these functions run much slower than the serial (single-core CPU) versions and sort() along with stable_sort produce Segmentation Fault (when sorting a vector of 100 Million 32-bit integers). This is running on a Dell Alienware laptop with GeForce RTX 3060 GPU and 12-th Gen Intel 14-core CPU.Are there certain compiler switches that should be used to produce results that accelerate these functions? I use -stdpar and -O3 for the nvc++When compiling (using nvc++) without -stdpar all benchmarks, including sort() and stable_sort(), run to completion without segmentation fault, executing on a single-core of the Intel CPU.Thank you,
-VictorPowered by Discourse, best viewed with JavaScript enabled"
742,visual-smoke-detection-using-low-cost-surveillance-cameras,"Originally published at:			Visual Smoke Detection Using Low-Cost Surveillance Cameras | NVIDIA Technical Blog
Researchers from the University of Ulsan in Korea have demonstrated how to spot fires before they spread by using GPU-accelerated video processing to detect smoke, eliminating the need for expensive infra-red cameras and laser range finders. Smoke detection false positives: (a) Original image without reflection detected; (b) Result image without reflection detected; (c) Original image…Powered by Discourse, best viewed with JavaScript enabled"
743,introducing-nvidia-hgx-a100-the-most-powerful-accelerated-server-platform-for-ai-and-high-performance-computing,"Originally published at:			Introducing NVIDIA HGX A100: The Most Powerful Accelerated Server Platform for AI and High Performance Computing | NVIDIA Technical Blog
The NVIDIA mission is to accelerate the work of the da Vincis and Einsteins of our time. Scientists, researchers, and engineers are focused on solving some of the world’s most important scientific, industrial, and big data challenges using artificial intelligence (AI) and high performance computing (HPC).  The NVIDIA HGX A100 with A100 Tensor Core GPUs…Powered by Discourse, best viewed with JavaScript enabled"
744,scaling-ai-for-financial-services-with-full-stack-solutions-to-implementation-challenges,"Originally published at:			https://developer.nvidia.com/blog/scaling-ai-for-financial-services-with-full-stack-solutions-to-implementation-challenges/
To scale AI for financial services, companies must face several resource hurdles. The full-stack solution from NVIDIA and VMware helps you leverage the competitive advantages of AI.Powered by Discourse, best viewed with JavaScript enabled"
745,tips-getting-the-most-out-of-the-dlss-unreal-engine-4-plugin,"Originally published at:			https://developer.nvidia.com/blog/tips-getting-the-most-out-of-the-dlss-unreal-engine-4-plugin/
DLSS is a deep learning, super-resolution network that boosts frame rates by rendering fewer pixels and then using AI to construct sharp, higher-resolution images. Dedicated computational units on NVIDIA RTX GPUs called Tensor Cores accelerate the AI calculations, allowing the algorithm to run in real time. DLSS pairs perfectly with computationally intensive rendering algorithms such…Is there any way dlss can be used with the movie render queue? Its urgent. Thank you in advanceHey ar.redzwanThere is no official support for MRQ with the DLSS Plugin. The DLSS Plugin interface only enables DLSS for the main views.Powered by Discourse, best viewed with JavaScript enabled"
746,ornl-researchers-use-ai-to-map-arctic-vegetation,"Originally published at:			ORNL Researchers Use AI to Map Arctic Vegetation | NVIDIA Technical Blog
Because of climate change, vegetation zones in Alaska and the pan-arctic are shifting. This shift could affect ecosystem factors such as carbon cycling, permafrost dynamics, and fire regimes. To help generate up-to-date vegetation maps that account for these shits, a team of scientists from Oak Ridge National Laboratory developed a deep learning-based method that can…Powered by Discourse, best viewed with JavaScript enabled"
747,optimizing-gpu-utilization-with-nsight-compute-2021-3,"Originally published at:			https://developer.nvidia.com/blog/optimizing-gpu-utilization-with-nsight-compute-2021-3/
Featured image for Nsight development tools product line.Powered by Discourse, best viewed with JavaScript enabled"
748,getting-started-with-nvidia-omniverse-ace-early-access,"Originally published at:			https://developer.nvidia.com/blog/getting-started-with-nvidia-omniverse-ace-early-access/
NVIDIA just announced at CES 2023 that early access is now available for NVIDIA Omniverse Avatar Cloud Engine (ACE). Developers and teams building avatars and virtual assistants can register to join the program, which includes access to the Omniverse ACE suite of cloud-native AI microservices for faster, easier development of interactive avatars. Early partners include…Powered by Discourse, best viewed with JavaScript enabled"
749,building-a-four-node-cluster-with-nvidia-jetson-xavier-nx,"Originally published at:			Building a Four-Node Cluster with NVIDIA Jetson Xavier NX | NVIDIA Technical Blog
Create a compact desktop cluster with four NVIDIA Jetson Xavier NX modules to accelerate training and inference of AI and deep learning workflows.Powered by Discourse, best viewed with JavaScript enabled"
750,omniverse-open-beta-now-available-for-linux,"Originally published at:			Omniverse Open Beta Now Available for Linux | NVIDIA Technical Blog
The NVIDIA Omniverse open beta expands to linux by releasing a linux-based launcher and applications. The new launcher provides the latest Omniverse news and updates, as well as the exchange where users can install and update applications and components like Omniverse Create, Kit, Cache, Drive and the Autodesk Maya Connector. The launcher also provides a…Powered by Discourse, best viewed with JavaScript enabled"
751,imaging-the-earths-interior-with-the-summit-supercomputer,"Originally published at:			Imaging the Earth’s Interior with the Summit Supercomputer | NVIDIA Technical Blog
In a new video, Jeroen Tromp a professor in the Department of Geosciences at Princeton University describes how he and his team are imaging the earth’s interior with Summit, the world’s fastest supercomputer. “The goal of our research is to image the Earth’s interior on a global scale. We’re basically 3D cartographers of the Earth’s…Powered by Discourse, best viewed with JavaScript enabled"
752,what-is-an-exaflop,"Originally published at:			What Is an Exaflop? | NVIDIA Blogs
An exaflop is a measure of performance for a supercomputer that can calculate at least one quintillion floating point operations per second.Powered by Discourse, best viewed with JavaScript enabled"
753,gtc-2020-clara-developer-day-scalable-and-modular-deployment-powered-by-clara-deploy-sdk,"GTC 2020 S22565
Presenters: Jesse Tetreault,NVIDIA
Abstract
Clara Deploy SDK provides a reference framework for developers, data scientists and engineers to make seamless the process to turn trained AI models into operators. These operators can be stitched together to define an AI deployment pipeline, using reference DICOM adapters and sample pipelines that can interface with a medical imaging environment, like a PACS or VNA. In this session, we will do a walk-through of platform features that enable scalable deployment of multiple AI based pipelines in a hospital IT-like infrastructure. We will also do a hands-on session enabling end-end deployment of a Clara Train model. The users will interact with the SDK to deploy reference pipelines and learn how they can use the modular nature of the overall framework to power deployment of AI models in medical imaging workflows.Watch this session
Join in the conversation below.Powered by Discourse, best viewed with JavaScript enabled"
754,predict-protein-structures-and-properties-with-biomolecular-large-language-models,"Originally published at:			https://developer.nvidia.com/blog/predict-protein-structures-and-properties-with-biomolecular-large-language-models/
The NVIDIA BioNeMo service is now available for early access. With the BioNeMo service, scientists and researchers now have access to pretrained biomolecular LLMs through a cloud API.Powered by Discourse, best viewed with JavaScript enabled"
755,gtc-2020-blender-cycles-rtx-on,"GTC 2020 D2S21
Presenters: Tech Demo Team,NVIDIA
Abstract
Blender is an open-source 3D software package that comes with the Cycles Renderer. Cycles is already a GPU enabled path-tracer, now super-charged with the latest generation of RTX GPUs.Furthering the rendering speed, RTX AI features such as the Optix Denoiser infers rendering results for a truly interactive ray tracing experience.Watch this session
Join in the conversation below.Powered by Discourse, best viewed with JavaScript enabled"
756,pandas-dataframe-tutorial-beginners-guide-to-gpu-accelerated-dataframes-in-python,"Originally published at:			https://developer.nvidia.com/blog/pandas-dataframe-tutorial-beginners-guide-to-gpu-accelerated-dataframes-in-python/
This post is the first installment of the series of introductions to the RAPIDS ecosystem. The series explores and discusses various aspects of RAPIDS that allow its users solve ETL (Extract, Transform, Load) problems, build ML (Machine Learning) and DL (Deep Learning) models, explore expansive graphs, process geospatial, signal, and system log data, or use…Thanks for this post, it’s great.  I don’t understand if Rapids is a library that relies on CUDA, why can’t a computer like AGX Xavier run Rapids by just installing the library?
I’ve tried numerous incarnations of attempts by people trying to do just that. The repository GitHub - rapidsai-community/rapids-l4t: Feedstock-like set of scripts for building RAPIDS components for NVIDIA Jetson tried as well, but running the latest JetPack 4.6.xx has no success.
Couldn’t Nvidia allocate some resources to making an l4t compatible Rapids library to support the Xavier AGX and NX?Powered by Discourse, best viewed with JavaScript enabled"
757,supercomputing-demystifies-how-metals-crystallize-at-atomic-scale,"Originally published at:			https://developer.nvidia.com/blog/lawrence-livermore-simulations-metal-freezing/
Simulations by Lawrence Livermore National Laboratory researchers have uncovered a new mechanism for freezing in metals, advancing scientists’ understanding of nucleation, the process of gases or liquids cooling into crystalline solids.  Run on 256 NVIDIA Tensor Core GPUs on the Lassen supercomputer, the simulations modeled how heated copper solidifies, providing atomic-scale insights into the Ostwald…Powered by Discourse, best viewed with JavaScript enabled"
758,rapid-data-pre-processing-with-nvidia-dali,"Originally published at:			https://developer.nvidia.com/blog/rapid-data-pre-processing-with-nvidia-dali/
NVIDIA Data Loading Library is an open-source project and can help you accelerate data pre-processing for DL application.Powered by Discourse, best viewed with JavaScript enabled"
759,coffee-break-series-nvidia-highlights,"Originally published at:			Coffee Break Series: NVIDIA Highlights | NVIDIA Technical Blog
NVIDIA Highlights enables automatic video capture of key moments, clutch kills, and match-winning plays, ensuring gamers’ best gaming moments are always saved without requiring player intervention. NVIDIA Highlights automatically captures high quality video clips of those dramatic moments. Many of the world’s biggest games – Fortnite, Call of Duty World War II, PLAYERUNKNOWN’S Battlegrounds, and more…Thanks for the post and videos Bryan! Wondering though when you're speaking about registering the game on the developer portal, how are we supposed to let the SDK know about it?Powered by Discourse, best viewed with JavaScript enabled"
760,army-unveils-worlds-19th-most-powerful-supercomputer,"Originally published at:			Army Unveils World’s 19th Most Powerful Supercomputer | NVIDIA Technical Blog
The U.S. Army introduced its newest supercomputer, Excalibur, which will help to ensure Soldiers have the technological advantage on the battlefield. Increased computational capabilities will allow researchers bring improved communications, data and intelligence to Soldiers in the field, said Maj. Gen. John F. Wharton, commanding general of the U.S. Army Research, Development and Engineering Command.…Powered by Discourse, best viewed with JavaScript enabled"
761,new-jetson-nano-2gb-developer-kit-grant-program-launches,"Originally published at:			New Jetson Nano 2GB Developer Kit Grant Program Launches | NVIDIA Technical Blog
NVIDIA recently launched the Jetson Nano 2GB Developer Kit Grant Program which offers limited quantities of Jetson Developer Kits to professors, educators and trainers across the globe. Ideal for hands-on teaching, the Jetson Nano 2GB Developer Kit is the perfect tool for introducing AI and robotics to all kinds of learners, from high school students…Powered by Discourse, best viewed with JavaScript enabled"
762,get-free-training-in-deep-learning-accelerated-computing-and-data-science,"Originally published at:			https://developer.nvidia.com/blog/get-free-training-in-deep-learning-accelerated-computing-and-data-science/
The NVIDIA Deep Learning Institute offers free courses for all experience levels in deep learning, accelerated computing, and accelerated data science.Powered by Discourse, best viewed with JavaScript enabled"
763,ai-tackles-offensive-language-on-social-media,"Originally published at:			AI Tackles Offensive Language on Social Media | NVIDIA Technical Blog
IBM researchers are working to fix one of the internet’s most significant problems — offensive, and abusive language. With the help of deep learning, the team trained their neural network to automatically turn offensive comments into non-offensive ones.   “The use of offensive language is a common problem of abusive behavior on online social media…That’s interesting! Have there been any updates on it? I wish AI could do SM promoting, btw…Wow, it’s such an importany step forward. Dealing with offensive and abusive comments can be a real challenge online, so it’s great to see IBM researchers using deep learning to address this issue. Kudos to them!Powered by Discourse, best viewed with JavaScript enabled"
764,improving-gpu-application-performance-with-nvidia-cuda-11-2-device-link-time-optimization,"Originally published at:			https://developer.nvidia.com/blog/improving-gpu-app-performance-with-cuda-11-2-device-lto/
CUDA 11.2 features the powerful link time optimization (LTO) feature for device code in GPU-accelerated applications. Device LTO brings the performance advantages of device code optimization that were only possible in the nvcc whole program compilation mode to the nvcc separate compilation mode, which was introduced in CUDA 5.0.Separate compilation mode allows CUDA device kernel…Figure 2 seems to be wrong, it’s the same as Figure 1. Also it would be nice to get the figures in a higher resolution.@rkobus – Sorry about that! It’s fixed now. Hope the larger size helps as well. Thanks for the feedback!Is the MonteCarlo benchmark in the CUDA 11.2 sample code?No.  It was used for some internal benchmarking.  Unfortunately most of the sample code does not involve separate compilation so are not good tests for LTO.Good question.  We are working on support for JIT LTO, but in 11.2 it is not supported.  So in the example you give at JIT time it will JIT each individual PTX to cubin and then do a cubin link.  This is the same as we have always done for JIT linking.  But we should have more support for JIT LTO in future releases.@mmurphy1 Thanks for the reply - I look forward to seeing more information in the future :)Would you also be able to shed any light on the following: Using device link-time optimization results in much larger fatbinaries@mmurphy1 Are there any reasons that DLTO cannot achieve the same runtime performance as the whole program compilation? Performing DLTO should be able to inline and optimize all functions thus will generate the same code as the whole program compilation, unless the linker does not always inline and optimize the code (since DLTO doesn’t have enough memory to perform the linking?).DLTO should provide the same runtime performance as whole program.  If doing “partial LTO” where some objects were not compiled with -dlto then the scope of optimization will be smaller.but in 11.2 it is not supported.Ok, is it supported in 11.7?JIT LTO is supported as of 11.4, but only as a preview feature.  There will be a change to the interface in 12.0 to better support our compatibility guarantees.Thanks! However, judging by the release notes for 11.4, it looks to be more for manual JIT (i.e. explicitly invoking nvcc), whereas I was thinking more about the “automatic” JIT that the NVIDIA GPU driver performs if a fatbinary doesn’t include SASS for the target GPU arch. Do you know how/if the driver handles DLTO JIT?That is correct, JIT LTO is only supported manually at this time, not as part of the automatic or implicit runtime. JIT linking at the ELF level is supported in the runtime. By default when you compile with -dlto -dc it stores both LTO-IR and PTX in the fatbinary, so if you update your chip it will then do JIT compile and link of the PTX and it will work functionally, but you won’t get the LTO optimization from that. This is something that we may release later, depending on customer feedback.Thanks for the clarification :) Having the driver automatically perform LTO when JIT compiling/linking from PTX/LTO-IR would be a great feature from our point of view, so fingers crossed!Powered by Discourse, best viewed with JavaScript enabled"
765,gtc-2020-distributed-machine-learning-on-virtualized-servers,"GTC 2020 S21191
Presenters: Luke Wignall,NVIDIA ; Mohan Potheri,VMware; Boris Kovalev,Mellanox
Abstract
Horovod is a distributed machine learning platform that can leverage GPUs for deep learning. We’ll talk about a joint project between NVIDIA, Mellanox, and VMware to create a high-performance platform leveraging NVIDIA vCompute Server, Mellanox-based high speed networking, and vSphere PVRDMA. We’ll compare the results of common benchmarks that ran with and without PVRDMA. We’ll also discuss a reference architecture for leveraging vCompute server for ML.Watch this session
Join in the conversation below.Powered by Discourse, best viewed with JavaScript enabled"
766,gtc-2020-advanced-scientific-visualization-with-nvidia-omniverse,"GTC 2020 S21973
Presenters: Kees van Kooten ,NVIDIA; Mathias Hummel,NVIDIA
Abstract
Historically, scientific visualization has served two main purposes: on one hand, for the scientist to gain insight into the data and the processes under investigation; on the other hand, for outreach and education, to communicate scientific results to the public and to funding bodies. The tools associated with these workflows are hugely diverse, often preventing scientists from getting access to the highest quality visuals for telling their stories. Especially in times where the general public has been spoiled with computer-generated content in movies, the gap in visual quality between scientific content and entertainment content has never been bigger. Tools that seamlessly fit into the scientific workflow while being capable of producing the highest quality visuals are therefore needed. We’ll outline how Omniverse, NVIDIA’s collaboration platform for 3D content creation, can be leveraged to greatly simplify, accelerate, and enhance scientific visualization. We’ll introduce the overall Omniverse architecture and then focus on the scientific workflow. We cover how content can be ingested into Omniverse, as well as how content from different sources can be fused and how to produce rich, high-quality visualizations. We’ll then show how to augment these visualizations with the embedded real-time physics engine PhysX. Further, we’ll demonstrate how, through Omniverse, virtual reality setups that incorporate scientific visualizations can be defined quickly and easily.Watch this session
Join in the conversation below.Powered by Discourse, best viewed with JavaScript enabled"
767,gtc-2020-building-blocks-for-machine-learning-integration-into-clinical-workflow,"GTC 2020 S22189
Presenters: Krishna Juluru,Memorial Sloan Kettering Cancer Center
Abstract
Applications of machine learning in radiology image analysis continue to grow at an increasing pace. For these tools to make an impact in diagnostics, they need to be well integrated into a clinical workflow. We’ll review the radiology diagnostic interpretation process and the role of several machine-learning algorithms that support radiologists in this effort. We’ll then focus on seven generalizable building blocks that are needed to integrate algorithm results into clinical workflow, with roles ranging from quality control and results presentation to error correction and active learning. We’ll discuss current standards and the need for new standards, and highlight our experience in applying these algorithms and building blocks in a large cancer center.Watch this session
Join in the conversation below.Powered by Discourse, best viewed with JavaScript enabled"
768,how-can-i-build-an-ai-powered-extension-for-nvidia-omniverse-using-chatgpt-and-gpt-4,"As the title says - what’s the starting point?You can find the example project AI Room Generator on GitHub: GitHub - NVIDIA-Omniverse/kit-extension-sample-airoomgenerator: A tool used to create 3D content for rooms by calling OpenAI's APIGreat - thanksThis topic was automatically closed 14 days after the last reply. New replies are no longer allowed.Powered by Discourse, best viewed with JavaScript enabled"
769,creating-a-human-pose-estimation-application-with-nvidia-deepstream,"Originally published at:			https://developer.nvidia.com/blog/creating-a-human-pose-estimation-application-with-deepstream-sdk/
Human pose estimation is the computer vision task of estimating the configuration (‘the pose’) of the human body by localizing certain key points on a body within a video or a photo. This localization can be used to predict if the person is standing, sitting, lying down, or doing some activity like dancing or jumping.…Hi awesome blog!
I have been trying to run this pose over multiple sources or video files.
Could you let me know how do I proceed with that? Do I make changes to the cpp file? If so, what?
Also, if I make a source file for making the pipeline and refer the config file, what changes do I need to make to the osd plugin, to display the lines and vectors of the keypoint?does it work taking video input from jetson csi camera?
which changes are required to the config file in order to do so?
Any ideas why execution  never completes?triied with the fileHi,
To change the source, you need to update deepstream_pose_estimation/deepstream_pose_estimation_app.cpp at master · NVIDIA-AI-IOT/deepstream_pose_estimation · GitHub file.Hi,
Does it provide better performance compare to Isaac implementation ?
Other pro or con compare to using it in Isaac ?
ThanksHi ,
The base model is the same as Isaac 2D Skeleton Pose estimation model. In this project, we have integrated the model with Deepstream.Hello, a question if I wanted to add RA, would it be possible to correct the code? ’Hi, I’m trying to run the sample and it looks like it’s stalling at Running… and never ends.
The result file is being created but is empty.
I tried with several different onnx and video files, but get the same result.I’m running this on Ubuntu 20.04Any idea how to proceed with this?Hi Jesperlyng,did you update anything in the source? It seems like you are using .mp4 input source and the pose estimation app supports h264 input streams.Hi, well mp4 is just a container and as the file name suggests it’s h264 encoded so I expect that to be fine?One thing to note here is that since I’m on Ubuntu 20.04 the Gstreamer version is 2.x and that might cause problems according to prior posts.I figured out how to debug gst and got loads of warnings. If this could be caused by the wrong gstreamer version I’ll downgrade to Ubuntu 18 which has gstreamer 1.14 natively.Thanks in advanceHi Jesperlyng,
Thank you for providing the log!Are you able to use this stream with any other deepstream test sample?platform: jetson xavier
I using the follow command:
$ ./deepstream-pose-estimation-app 20210110-02.mp4 /opt/nvidia/deepstream/deepstream-5.0/sources/apps/sample_apps/deepstream-pose-estimation/images/output:
One element could not be created. Exiting.what is operation wrong  to me?output:May you should pass the uri format like $ ./deepstream-pose-estimation-app file://./20210110-02.mp4 /opt/nvidia/deepstream/deepstream-5.0/sources/apps/sample_apps/deepstream-pose-estimation/images/
I tried passing uri but my deepstream pose estimation app is killed when converting onnx model to tensorrt runtime model.Thank you!@mjhuria
I run my app on Jetson Xavier NX and got the same problem, anyway to solve the issue?
thank advance.Hi Samuel,Could you please run export GST_DEBUG=3 and then run the app? So, we can see more info on the error?Thank you!I converted onnx model to tensorrt engine first then passing the tensorrt path to config file and run successfully but I see that when I run on deepstream in my Jetson nano the results are worse than when I run the tensorrt model on my PC not via deepstream. What is wrong? Tks!Hi mjhuria, thanks for replying :-)
I switched to Ubuntu 18.04, Gstreamer-1.14.5, Cuda 11.1 and TensorRT 7.2.1.6-ga. Seems it’s not that trivial to find the right combo of  libs to use…
And I don’t think it made a lot of difference anyway except maybe for the gstreamer.It still never ends unless I use a correct movie file as input, e.g. sample_720.h264.
It seems that the h264parser doesn’t like the mp4 container so even if the sample_1080p_264.mp4 is h264 encoded it fails. Or - actually it just warns about the h264parser just like before even though it should probably fail in this case.When using the .h264 file it fails withERROR                   v4l2 gstv4l2object.c:2074:gst_v4l2_object_get_interlace_mode: Driver bug detected - check driver with v4l2-compliance from v4l-utils.git - media (V4L2, DVB and IR) applications and librariesSo I’m investigating this now.When I run it in the nvidia container it works flawlessly.Even though the sample .h264 file works it would be good to have a sample file specifically for this pose estimation app. Maybe you could share the one you use as example on blog :-) ?Thanks for the work !A minor update: in order to convert any video to the proper format needed by the h264parser, install ffmpeg with your preferred package manager and useffmpeg -i [input file]  out.h264Please, did you test the application for high resolution images (e.g. 1920 x 1080 ) using the provided pre-trained models (224x224 or 256x256). Thanks in advance.Powered by Discourse, best viewed with JavaScript enabled"
770,integrating-nvidia-reflex-q-a-with-pathea-head-of-technology-jingyang-xu,"Originally published at:			Integrating NVIDIA Reflex: Q&A with Pathea Head of Technology Jingyang Xu | NVIDIA Technical Blog
NVIDIA sat down with Pathea’s newly appointed Chief Wizard, Jingyang Xu, to discuss how his company integrated NVIDIA Reflex into its latest project.Powered by Discourse, best viewed with JavaScript enabled"
771,enhancing-ai-transparency-and-ethical-considerations-with-model-card,"Originally published at:			https://developer.nvidia.com/blog/enhancing-ai-transparency-and-ethical-considerations-with-model-card/
Learn about the importance of the artificial intelligence (AI) model card and how NIVIDIA is improving it to enhance transparency.Powered by Discourse, best viewed with JavaScript enabled"
772,ask-me-anything-nvidia-cuda-toolkit-12,"Originally published at:			AddEvent
On July 26, connect with CUDA product team experts on the latest NVIDIA CUDA Toolkit 12.Powered by Discourse, best viewed with JavaScript enabled"
773,explainer-what-is-a-transformer-model,"Originally published at:			What Is a Transformer Model? | NVIDIA Blogs
A transformer model is a neural network that learns context and thus meaning by tracking relationships in sequential data like the words in this sentence.Powered by Discourse, best viewed with JavaScript enabled"
774,the-best-tools-to-improve-gameplay-performance,"Originally published at:			The Best Tools to Improve Gameplay Performance | NVIDIA Technical Blog
Developers are using the latest NVIDIA RTX technology to deliver the best gaming experience to players, complete with high-quality graphics and fast performance. But improving gameplay takes more than GPU power alone.Powered by Discourse, best viewed with JavaScript enabled"
775,ai-helps-farmers-distinguish-crop-data-in-real-time,"Originally published at:			https://developer.nvidia.com/blog/ai-helps-farmers-distinguish-crop-data-in-real-time/
Researchers with the National Center for Supercomputing Applications (NCSA) developed a deep learning-based technique that uses satellite data and supercomputers to distinguish between corn and soybean fields. The research, published in the Remote Sensing Environment journal, is a major breakthrough in the agricultural industry, as it allows a variety of stakeholders to get real-time analytics.…Powered by Discourse, best viewed with JavaScript enabled"
776,advancing-robotic-assembly-with-a-novel-simulation-approach-using-nvidia-isaac,"Originally published at:			https://developer.nvidia.com/blog/advancing-robotic-assembly-with-a-novel-simulation-approach-using-nvidia-isaac/
A breakthrough in the simulation and learning of contact-rich interactions provides tools and methods to accelerate robotic assembly and simulation research.Powered by Discourse, best viewed with JavaScript enabled"
777,top-game-development-sessions-at-nvidia-gtc,"Originally published at:			https://developer.nvidia.com/blog/top-game-development-sessions-at-nvidia-gtc/
Several GTC sessions are available for game developers, content creators, and engineers looking to explore new tools and techniques accelerated by NVIDIA technologies.Powered by Discourse, best viewed with JavaScript enabled"
778,ai-helps-amputee-play-piano-for-first-time-since-2012,"Originally published at:			AI Helps Amputee Play Piano for First Time Since 2012 | NVIDIA Technical Blog
Researchers at the Georgia Institute of Technology developed an ultrasonic sensor using GPUs and deep learning that allows amputees to control individual fingers on a prosthetic hand. “Our prosthetic arm is powered by ultrasound signals,” said Gil Weinberg, the Georgia Tech College of Design professor who leads the project. “By using this new technology, the…Powered by Discourse, best viewed with JavaScript enabled"
779,gpus-dominate-isc15-student-cluster-contest,"Originally published at:			GPUs Dominate ISC’15 Student Cluster Contest | NVIDIA Technical Blog
Using Tesla K80s, China’s Tsinghua University team and JMI University in India both took top honors at the popular student contest. At the International Supercomputing Conference (ISC) in Frankfurt, Germany, China’s Tsinghua University team collected their fifth student challenge gold cup (and second ISC win).   China’s Tsinghua University team The popular student contest brings together…Powered by Discourse, best viewed with JavaScript enabled"
780,an-important-skill-for-data-scientists-and-machine-learning-practitioners,"Originally published at:			An Important Skill for Data Scientists and Machine Learning Practitioners | NVIDIA Technical Blog
The most important soft skill for ML practitioners and Data ScientistsPowered by Discourse, best viewed with JavaScript enabled"
781,gpu-operator-1-8-adds-support-for-dgx-hgx-and-upgrades,"Originally published at:			https://developer.nvidia.com/blog/gpu-operator-1-8-adds-support-for-dgx-hgx-and-upgrades/
In the last post, we looked at how the GPU Operator has evolved, adding a rich feature set to handle GPU discovery, support for the new Multi-Instance GPU (MIG) capability of the NVIDIA Ampere Architecture, vGPU, and certification for use with Red Hat OpenShift.  In this post, we look at the new features added in…The article notes that support for a100 will be available at a future date.  When will this occur?Powered by Discourse, best viewed with JavaScript enabled"
782,how-to-build-domain-specific-automatic-speech-recognition-models-on-gpus,"Originally published at:			How to Build Domain Specific Automatic Speech Recognition Models on GPUs | NVIDIA Technical Blog
In simple terms, conversational AI is the use of natural language to communicate with machines. Deep learning applications in conversational AI are growing every day, from voice assistants and chatbots, to question answering systems that enable customer self-service. The range of industries adapting conversational AI into their solutions are wide, and have diverse domains extending…Powered by Discourse, best viewed with JavaScript enabled"
783,gtc-2020-the-importance-of-gpu-technology-in-3d-photography-for-lightfield-displays,"GTC 2020 S22065
Presenters: Edward Li,Leia, Inc.; Puneet Kohli,Leia, Inc
Abstract
Equipping smartphones with multiple cameras paves the way for 3D photography. Coupled with a modern Lightfield display, images can come to life with a depth and parallax effect. We’ll first highlight the key challenges with 3D photography for Lightfield Displays compared to traditional formats. We’ll then show the necessary computer vision algorithms required to enable Lightfield 3D Photography, and follow up with the work that we’ve done in stylization, relighting, and other depth-based effects. GPU technology is imperative to all of the processes involved, and we’ll highlight this throughout our talk.Watch this session
Join in the conversation below.Powered by Discourse, best viewed with JavaScript enabled"
784,ray-tracing-gems-ii-available-today-in-hardcover,"Originally published at:			https://developer.nvidia.com/blog/ray-tracing-gems-ii-available-today-in-hardcover/
Ray Tracing Games II is now available as a hardcover on Apress and Amazon.Powered by Discourse, best viewed with JavaScript enabled"
785,20th-century-fox-uses-ai-to-predict-who-will-watch-a-movie-from-its-trailer,"Originally published at:			20th Century Fox Uses AI to Predict Who Will Watch a Movie From Its Trailer | NVIDIA Technical Blog
To help determine the best previews to show before a specific movie, researchers from 20th Century Fox film studios developed a deep learning model to predict what audience will most likely see a film, based on the film’s movie trailer. The system, which extracts features such as color, illumination, faces, objects, and landscapes, achieves accurate…Powered by Discourse, best viewed with JavaScript enabled"
786,your-brains-capacity-is-10-times-greater-than-earlier-expected,"Originally published at:			Your Brain’s Capacity is 10 Times Greater Than Earlier Expected | NVIDIA Technical Blog
Researchers at the Salk Institute for Biological Studies and several collaborators used NVIDIA GPUs to create a highly detailed 3D digital reconstruction of tissue from a rat’s hippocampus, the memory center of the brain. The reconstruction, powered by TITAN GPUs, helped the researchers precisely determine the number and  size categories of synapses — the connections…Powered by Discourse, best viewed with JavaScript enabled"
787,nvidia-releases-open-source-gpu-kernel-modules,"I created an account just so I could give praise to this initiative. Finally! Great NVIDIA. both your brand and us end users will benefit greatly from this in the long term.Hello, me the old librist, am very happy: Nvidia does what it says and says what it does, we will not be able to fix this world and make it evolve if open source is not part of it. . On the other hand, why not offer anything to Debian? This is a source distribution, plastic and runs both workstations and servers. Canonical is nice, but so fresh it’s unusableThese are wonderful news! I hope this is a great start for a long open source journey ahead 👍Happy to hear this.
Is there any feature comparison list between the binary and the OSS?Hi sakaia! Please take a look at our supported features section to see if this helps answer your question:http://us.download.nvidia.com/XFree86/Linux-x86_64/515.43.04/README/kernel_open.htmlIt’s a shame that the Pascal GTX10 series won’t be a part of this, would’ve helped in a lot of things :(Thanks Dasein420 for your feedback! Yes, we’re hearing Pascal support a lot following our announcement and are taking it very seriously. As this is our initial release, we’re gathering all feedback to help determine possible future engineering projects. Appreciate it!Thank you for your reply.It is very helpful. The concept of this open source driver is clearified.Sorry ridge, we can’t replace your monitor. But we do appreciate your sentiment, and the visual! ;-)Hi devhci, we understand your request. Take heart in knowing that the driver code does run on Debian. But, you’re probably asking more about the repackaging? [See updated response below.]Hello, thank you for this answer. Debian, I think should not move before, I hope the next minor update, at worst version 12, you make stability, but I prefer this stability a thousand times, knowing that my GPUs are underused, that drivers distributed in a hurry, unstable in Ubuntu fashionHi devhci, thanks to @kmittman, I have more information! We do provide official distro packages for Debian (the distro) and others, please see: Installation Guide Linux :: CUDA Toolkit DocumentationHope this helps! :)Fantastic news, I can’t wait to see this implemented into Arch.Nvidia, you have a plan!
Cool!Hello Dasein420, I checked with engineering for further clarification on your request. Open kernel modules support all Ampere and Turing GPUs because the new modular driver architecture depends on GSP hardware introduced in the Turing family. However, for volta and older GPU architectures, GSP hardware is not available and hence they cannot leverage the open GPU kernel modules. Apologies for giving you any false hopes. :(Will this release make it possible to legally apply the RT patches on Nvidia systems?Thanks, NVIDIA! The Linux community has waited for this for a very long time.
We hope in the future not only kernel drivers will be open-sourced, but this is a huge step forward.I still remember the talk by Linus Torvalds and his frustration with Nvidia but this is finally going to fix it!!!But today no fishes for the dolphins… ?
Is it not compatible to UE 5.0.2  ?!?
Or its the dolphin who not catches the fish right way ?https://forums.unrealengine.com/t/ue5-0-2-crashed-after-install-nvidia-dlss-open-kernel/572577I’m not aware of any fundamental incompatibilities between the open kernel modules and unrealengine.  Maybe the best thing to do is to file an issue on github: Issues · NVIDIA/open-gpu-kernel-modules · GitHub  Filling in the bug template (including information like an nvidia-bug-report.log.gz) will help us diagnose things.Powered by Discourse, best viewed with JavaScript enabled"
788,the-9am-ama-session-is-closed-but-the-team-will-be-back-live-at-6pm-pst-again,"Please feel free to post new questions and topics and the team will answer them when they return at 6pm (PST) later today
Thanks so muchPowered by Discourse, best viewed with JavaScript enabled"
789,top-rendering-sessions-for-developers-at-gtc-2022,"Originally published at:			AI Developer Conference Session Catalog | GTC 2022 | NVIDIA
Learn about the latest traditional and neural rendering technologies and how they are accelerating professional visualization.Powered by Discourse, best viewed with JavaScript enabled"
790,x-ray-research-reveals-hazards-in-airport-luggage-using-crystal-physics,"Originally published at:			https://developer.nvidia.com/blog/x-ray-research-reveals-hazards-in-airport-luggage-using-crystal-physics/
X-ray-powered research is aiming to target sneaky hazardous materials making their way through airport security. The study, recently published in Scientific Reports, proposes a new design for a fast and powerful X-ray diffraction (XRD) technology able to identify potential threats. The work could be a notable step toward more accurate luggage scanning in airports.  “The…Powered by Discourse, best viewed with JavaScript enabled"
791,drive-labs-tracking-objects-with-surround-camera-vision,"Originally published at:			https://developer.nvidia.com/blog/drive-labs-tracking-objects-with-surround-camera-vision/
Many cars on the road today equipped with advanced driver assistance systems rely on front and rear cameras to perform automatic cruise control (ACC) and lane keep assist. However, full autonomous driving requires complete, 360-degree surround camera vision.Powered by Discourse, best viewed with JavaScript enabled"
792,autonomous-ai-outraces-gran-turismo-world-champs,"Originally published at:			https://developer.nvidia.com/blog/autonomous-ai-outraces-gran-turismo-world-champs/
The newly announced AI racer, Gran Turismo Sophy, uses deep reinforcement learning to beat human Gran Turismo Sport drivers in real-time competitions.Powered by Discourse, best viewed with JavaScript enabled"
793,efficient-bert-finding-your-optimal-model-with-multimetric-bayesian-optimization-part-3,"Originally published at:			https://developer.nvidia.com/blog/efficient-bert-finding-your-optimal-model-with-multimetric-bayesian-optimization-part-3/
This is the third post in this series about distilling BERT with multimetric Bayesian optimization. Part 1 discusses the background for the experiment and Part 2 discusses the setup for the Bayesian optimization. In my previous posts, I discussed the importance of BERT for transfer learning in NLP, and established the foundations of this experiment’s…Powered by Discourse, best viewed with JavaScript enabled"
794,accelerating-lidar-for-robotics-with-nvidia-cuda-based-pcl,"Originally published at:			https://developer.nvidia.com/blog/accelerating-lidar-for-robotics-with-cuda-based-pcl/
Many Jetson users choose lidars as their major sensors for localization and perception in autonomous solutions. Lidars describe the spatial environment around the vehicle as a collection of three-dimensional points known as a point cloud. Point clouds sample the surface of the surrounding objects in long range and high precision, which are well-suited for use…Hi, jwitsoe!
I am trying to use CUDA-PCL on Jetson TX2. But I have encountered a CUDA failure problem which seems to be hard for me to deal with. It would be great if you can give some suggestions.Environment: Jetson TX2 with Jetpack 4.5 (Ubuntu 18.04, CUDA-10.2, PCL 1.8.1)
Problem: When I run the built demo in each subfolder, it turn out to be a CUDA failure. One output example is given below as I run the demo in cuda-pcl/cuda-segmentation:nvidia@nvidia-tx2:~/Downloads/cuda-pcl/cuda-segmentation$ ./demo sample.pcdGPU has cuda devices: 1
----device id: 0 info----
GPU : NVIDIA Tegra X2
Capbility: 6.2
Global memory: 7850MB
Const memory: 64KB
SM in a block: 48KB
warp size: 32
threads in a block: 1024
block dim: (1024,1024,64)
grid dim: (2147483647,65535,65535)Cuda failure: no kernel image is available for execution on the device at line 310 in file cudaSegmentation.cpp error status: 209
Aborted (core dumped)I have managed several attempts to solve it.
First, I downgrade to Jetpack 4.4.1 which is the same as the official test environment. But it did not work.
Next, I followed solutions to other similar problem. Specifically, I manually add 62 (which corresponds to the compute capability 6.2 of Jetson TX2) to the SMS variable in makefile. Still, nothing changed.
Since the source code is not there, I can’t do more with it.I don’t know much about CUDA programing, but I guess the .so file is not compiled with sms=62 so it can’t be executed on Jetson TX2. I would be appreciated if you could fix it for us TX2 users.Hi triokun,
You are right that the error below means there is no kernel for CURRENT device.
This is because CUDA-PCL was not compiled for SM62.
Cuda failure: no kernel image is available for execution on the device at line 310 in file cudaSegmentation.cpp error status: 209
Aborted (core dumped)Hi, leif!
Thanks for your answering.
I’m wondering if you can recompile the library for TX2 if you have the source code. It would help me a lot.This is lib for TX2, but it has not been tested because there is no TX2 on local side.Google Drive file.I’m grateful for your help. I have tested it on TX2 and it worked perfectly!
Would you mind recompiling the other two lib (libcudafilter.so and libcudaicp.so) for TX2?
Again, thank you so much!Please check the two libs.Google Drive file.Google Drive file.Hi @leif ,Try building the CUDA-ICP example and got a usr/bin/ld: ./lib/libcudaicp.so: error adding symbols: file in wrong format error.Environment:  GTX 1050 (Ubuntu 20.04, CUDA-10.2, PCL 1.10.1)How do I get passed that?Looks like the libraries are compiled for ARM processors. Can you recompile (or provide the source code) for x86_64 ?Yes, they all work well on TX2 except for CUDA_VoxelGrid. Here is the output；---------------checking CUDA VoxelGrid---------------------
ERROR case
status = 11Jetson has a  GPU with known type but PC not.
It is hard to adjust cuda-pcl for all GPUs.
We may support X86_64 later.The VoxelGrid may be not suitable for TX2.
We will try to check it later.Hi @leif
When i run cuda-icp example and output:$~/cuda-pcl-main/cuda-icp$ ./demo
GPU has cuda devices: 1
----device id: 0 info----
GPU : NVIDIA Tegra X1
Capbility: 5.3
Global memory: 3956MB
Const memory: 64KB
SM in a block: 48KB
warp size: 32
threads in a block: 1024
block dim: (1024,1024,64)
grid dim: (2147483647,65535,65535)Loaded 859059 data points for P with the following fields: x y z rgb
Loaded 784546 data points for Q with the following fields: x y z rgb
iter.Maxiterate 20
iter.threshold 1e-12
iter.acceptrate 1Target rigid transformation : cloud_in → cloud_icp
Rotation matrix :
| 0.923880 -0.382683 0.000000 |
R = | 0.382683 0.923880 0.000000 |
| 0.000000 0.000000 1.000000 |
Translation vector :
t = < 0.000000, 0.000000, 0.200000 >matrix_icp native value
Rotation matrix :
| 1.000000 0.000000 0.000000 |
R = | 0.000000 1.000000 0.000000 |
| 0.000000 0.000000 1.000000 |
Translation vector :
t = < 0.000000, 0.000000, 0.000000 >------------checking CUDA ICP(GPU)----------------
Cuda failure: the launch timed out and was terminated at line 59 in file cudaICP.cpp error status: 702
Aborted (core dumped)Can you help me fix problem ?
Thanks you.Hi @nghiaphamsg
Error status: 702 means that :
Specified whether there is a run time limit on kernels
https://docs.nvidia.com/cuda/cuda-runtime-api/structcudaDeviceProp.html#structcudaDeviceProp_19a63114766c4d2309f00403c1bf056c8
Could you try to boost your device firstly?Hi jwitose,
I am processing 3D points in a custom way to create 2D images. So, I am going through all the points and want to speed up the process using CUDA. The code is part a ROS node. Could you please let me know whether you have any samples or tutorials for ROS and CUDA especially for Point Cloud processing?
Thanks in advance,
AhmetHi asagllam,
Our cuda-pcl provide some libs and head files which can be used directly for any framework include ROS.Hello @leif ,Could you compile the ICP, filter, and segmentation libraries for the TX1 as well?Thanks,
RyanHi @rchen390
Please check the three libs which was compiled for TX1 with Jetpack 4.4.1.Google Drive file.Google Drive file.Google Drive file.Hi @leif,Thank you for compiling the libraries for me last time. If possible, could you also compile the ICP, filter, and segmentation libraries for the NVIDIA GeForce RTX 2080?Happy holidays!Hi @rchen390
Please check these libraries.Google Drive file.Google Drive file.Google Drive file.Google Drive file.Google Drive file.Powered by Discourse, best viewed with JavaScript enabled"
795,detecting-gravitational-waves-in-real-time-with-deep-learning,"Originally published at:			Detecting Gravitational Waves in Real-Time with Deep Learning | NVIDIA Technical Blog
Scientists at the National Center for Supercomputing Applications (NCSA), located at the University of Illinois at Urbana-Champaign used GPUs and deep learning to rapidly detect and characterize gravitational waves. This new approach enables astronomers to study gravitational waves using minimal computational resources, reducing time to discovery and increasing the scientific reach of gravitational wave astrophysics.…Powered by Discourse, best viewed with JavaScript enabled"
796,ai-can-now-create-websites-from-drawings,"Originally published at:			AI Can Now Create Websites From Drawings | NVIDIA Technical Blog
A new deep learning model allows users to create working HTML websites from hand-drawn wireframes, creating a solution for a process that normally takes weeks and involves multiple stakeholders. The developer, Ashwin Kumar, recently participated in Insight’s Data Science Fellowship Program, which aims to bridge the gap between academia and data science. There, he developed…Powered by Discourse, best viewed with JavaScript enabled"
797,is-your-country-free-from-covid19,"Hi Guys.
My country is free now from Covid 19. And all private and government offices have been opened even restaurant dining also opened. Just on 15 September, they are going to open school colleges and universities. They have announced on the news as well. What about your country? Are you safe?Powered by Discourse, best viewed with JavaScript enabled"
798,the-top-5-ai-stories-of-the-week-1-28,"Originally published at:			The Top 5 AI Stories of the Week: 1/28 | NVIDIA Technical Blog
In this week’s edition of the NVIDIA Developer Top 5 video, we revisit the top developer stories of the week. From a new set of DGX-2 systems at ORNL to a million-dollar prize for improving Zillow’s AI algorithm. Plus, learn more about our new how-to series that explain how to use Tensor Cores for deep…Powered by Discourse, best viewed with JavaScript enabled"
799,gtc-2020-leveraging-optix-7-for-high-performance-multi-gpu-ray-tracing-on-head-mounted-displays,"GTC 2020 S21425
Presenters: Andreas Dietrich,ESI Group; Eric Kam, ESI Group
Abstract
NVIDIA recently introduced OptiX 7, which provides low-level access to the RTX technology and raytracing (RT) cores of the Turing architecture. Its CUDA-centric nature enables direct control over GPU resources — particularly in a multi-GPU context — which allows for a much more efficient scalability compared to previous OptiX versions. We’ll show how to utilize OptiX 7 for full-frame raytracing on professional head-mounted displays (HMDs). Specifically, we’ll demonstrate how OptiX is used in a dual-GPU setup to rapidly generate stereoscopic output, tailored to the specific optical characteristics of an HMD. Finally, we’ll provide an overview of how RTX and OptiX 7 are integrated into ESI’s in-house rendering engine Helios, and will demonstrate the potential of real-time raytracing with practical use cases.Watch this session
Join in the conversation below.Powered by Discourse, best viewed with JavaScript enabled"
800,best-avatars-to-use-for-lipsyncing,"What kind of avatars (metahuman etc.) would you recommend to connect to gpt?
Especially when it comes to A2FAs a developer platform, we welcome the use of any ecosystem APIs. Your usecase requirements decide on which which avatar system you’d like to use. For example, if you need real-time performance, then you’d select avatar systems (3D, 2D) accordingly. We have showcased a demo at Computex using LLMs with Avatars by Convai. Computex 2023 NVIDIA Generative AI Sparks Life into Virtual Characters - YouTube. It’s using Unreal engine and Metahuman.This topic was automatically closed 14 days after the last reply. New replies are no longer allowed.Powered by Discourse, best viewed with JavaScript enabled"
801,building-the-future-of-real-time-graphics-with-nvidia-and-unreal-engine-5-1,"Originally published at:			Building the Future of Real-Time Graphics with NVIDIA and Unreal Engine 5.1 | NVIDIA Technical Blog
Learn about the Unreal Engine 5.1 release, including next-generation RTX lighting and speed increases to help you keep pace with rigorous development cycles.Powered by Discourse, best viewed with JavaScript enabled"
802,streaming-simulation-and-training-applications-with-project-anywhere,"Originally published at:			https://developer.nvidia.com/blog/streaming-simulation-and-training-applications-with-project-anywhere/
Imagine a future where ultra-high-fidelity simulation and training applications are deployed over any network topology from a centralized secure cloud or on-premises infrastructure. Imagine that you can stream graphical training content from the datacenter to remote end devices ranging from a single flat-screen or synchronized displays to AR/VR/MR head-mounted displays. This datacenter architecture enables the…Powered by Discourse, best viewed with JavaScript enabled"
803,detecting-divergence-using-pcast-to-compare-gpu-to-cpu-results,"Originally published at:			https://developer.nvidia.com/blog/detecting-divergence-using-pcast-to-compare-gpu-to-cpu-results/
Parallel Compiler Assisted Software Testing (PCAST) is a feature available in the NVIDIA HPC Fortran, C++, and C compilers. PCAST has two use cases. The first is testing changes to parts of a program, new compile-time flags, or a port to a new compiler or to a new processor. You might want to test whether…Powered by Discourse, best viewed with JavaScript enabled"
804,five-tips-for-building-a-cybersecurity-career-in-the-age-of-ai,"Originally published at:			https://developer.nvidia.com/blog/five-tips-for-building-a-cybersecurity-career-in-the-age-of-ai/
Check out these five tips for pursuing a career in cybersecurity, whether you’re just starting out or are looking to make a mid-career change.Powered by Discourse, best viewed with JavaScript enabled"
805,cuda-pro-tip-increase-application-performance-with-nvidia-gpu-boost,"Originally published at:			https://developer.nvidia.com/blog/cuda-pro-tip-increase-application-performance-nvidia-gpu-boost/
NVIDIA GPU Boost™ is a feature available on NVIDIA® GeForce® products and NVIDIA® Tesla® products. It makes use of any power headroom to boost application performance. In the case of Tesla, the NVIDIA GPU Boost feature is customized for compute intensive workloads running on clusters. This application note is useful for anyone who wants to take advantage of the…Powered by Discourse, best viewed with JavaScript enabled"
806,facebook-rolls-out-a-gpu-accelerated-ai-shopping-tool-for-marketplace,"Originally published at:			Facebook Rolls out a GPU-Accelerated AI Shopping Tool for Marketplace | NVIDIA Technical Blog
Facebook this week announced a GPU-accelerated model designed for shopping. The model uses AI to automatically identify consumer goods from images to help make them shoppable. GrokNet, a universal computer vision system, can identify items in categories such as fashion, auto, and home decor.  The model is in production today and is available for buyers…Powered by Discourse, best viewed with JavaScript enabled"
807,nvidia-splatnet-research-paper-wins-a-major-cvpr-2018-award,"Originally published at:			https://developer.nvidia.com/blog/nvidia-splatnet-research-paper-wins-a-major-cvpr-2018-award/
Today at the annual Computer Vision and Pattern Recognition conference in Salt Lake City, Utah, researchers from NVIDIA, the University of Massachusetts Amherst, and the University of California, Merced received the “Best Paper Honorable Mention Award,” for their paper, “SPLATNet: Sparse Lattice Networks for Point Cloud Processing”. “We are truly honored to receive this award.…Powered by Discourse, best viewed with JavaScript enabled"
808,nvidia-releases-tensorrt-4,"Originally published at:			NVIDIA Releases TensorRT 4 | NVIDIA Technical Blog
Today we are releasing TensorRT 4 with capabilities for accelerating popular inference applications such as neural machine translation, recommender systems and speech. You also get an easy way to import models from popular deep learning frameworks such as Caffe 2, Chainer, MxNet, Microsoft Cognitive Toolkit and PyTorch through the ONNX format. TensorRT delivers: Up to…Powered by Discourse, best viewed with JavaScript enabled"
809,deploying-a-natural-language-processing-service-on-a-kubernetes-cluster-with-helm-charts-from-nvidia-ngc,"Originally published at:			https://developer.nvidia.com/blog/deploying-a-natural-language-processing-service-on-a-kubernetes-cluster-with-helm-charts-from-ngc/
Conversational AI solutions such as chatbots are now deployed in the data center, on the cloud, and at the edge to deliver lower latency and high quality of service while meeting an ever-increasing demand. The strategic decision to run AI inference on any or all these compute platforms varies not only by the use case…Not directly relevant but hopefully, we can see an on-prem version of image based inference example, with multinode autoscaling. There are not much how-to data about on-prem k8s + Triton server, with horizontal PA.On-prem scenario is very relevant as well although what we covered in the blog above should be mostly applicable to on-prem deployment as well. Nevertheless, we’ll consider it for future writing.Thanks,Thanks @jamess , I keep investigating possibilities of load balancing and autoscaling (particularly with KFserving/kubernetes), but its seems chances are very little for a no-cloud solution, one is https://metallb.universe.tf/
I really apreciate if you provide some documentation for load balancing/scaling of local&multinode kubernetes installations, I believe this may be a common case for many people out there, who are trying to develop small scale setups, prior to move billing-clouds.Thank you for your suggestions. We will see what we can do!Thanks,Powered by Discourse, best viewed with JavaScript enabled"
810,gtc-2020-gpu-acceleration-of-an-fem-based-commercial-cfd-solver,"GTC 2020 S21690
Presenters: Yi Chen,Altair Engineering
Abstract
We’ll talk about how to design proper algorithms and data structure to migrate parallel, sparse, and iterative linear solvers in a commercial CFD code that was previously optimized for CPUs to NVIDIA GPUs with CUDA libraries. Efficient linear solvers on GPUs allow for tighter tolerance of convergence of the iterative process with considerably less cost than that on CPUs, which leads to faster convergence of the non-linear Newton’s iterations, and thus much less wall time.Watch this session
Join in the conversation below.@nadeemm Hi this is Yi Chen, speaker of this session. I cannot even watch the video, it says membership required. I have registered with GTC of course. Please keep me posted. Thanks!Hi Yi,
to watch the recorded session - login to the GTC system, and then follow the link in the catalog. That one should work
-NadeemPowered by Discourse, best viewed with JavaScript enabled"
811,upcoming-event-how-synthetic-data-is-supercharging-vision-ai-development,"Originally published at:			https://developer.nvidia.com/blog/upcoming-event-how-synthetic-data-is-supercharging-vision-ai-development/
Powered by Discourse, best viewed with JavaScript enabled"
812,interview-with-the-team-behind-metro-exodus,"Originally published at:			Interview with the Team Behind Metro Exodus | NVIDIA Technical Blog
We recently caught up with Oles Shyshkovtsov, Ben Archard, and Sergei Karmalsy from 4A – the team behind Metro Exodus – about the unique features of the PC version of the game. Metro Exodus finds beauty in the darkest possible places. As players, we see fetid sewers, terrifying cannibal camps, and ruined compounds. Yet even…Powered by Discourse, best viewed with JavaScript enabled"
813,drive-labs-visual-feature-tracking-for-autonomous-vehicles,"Originally published at:			https://developer.nvidia.com/blog/drive-labs-visual-feature-tracking-for-autonomous-vehicles/
By: Yue Wu, Cheng-Chieh Yang, Xin Tong Editor’s note: This is the latest post in our NVIDIA DRIVE Labs series, which takes an engineering-focused look at individual autonomous vehicle challenges and how NVIDIA DRIVE addresses them. Catch up on all of our automotive posts, here. Feature tracking, the estimation of pixel-level correspondences and pixel-level changes among adjacent…Powered by Discourse, best viewed with JavaScript enabled"
814,ask-me-anything-build-custom-ai-tools-with-chatgpt-and-nvidia-omniverse,"Originally published at:			Connect with Experts - Ask Me Anything Series | NVIDIA Developer
On June 28, join us to ask our experts how to build an AI-powered extension for NVIDIA Omniverse using ChatGPT.Powered by Discourse, best viewed with JavaScript enabled"
815,real-time-seizure-detection-of-newborn-infants,"Originally published at:			Real-Time Seizure Detection of Newborn Infants | NVIDIA Technical Blog
Researchers from University of College Cork in Ireland developed a deep learning-based monitoring system that automatically detects seizures as they occur which can help improve the outcomes for these babies who have a difficult start in life. Detecting when a newborn is having a seizure is very difficult for healthcare professionals since they often don’t…Powered by Discourse, best viewed with JavaScript enabled"
816,how-to-create-a-custom-language-model,"Originally published at:			https://developer.nvidia.com/blog/how-to-create-a-custom-language-model/
Large language models are powerful and versatile, yet zero-shot and few-shot prompting techniques may not fully leverage their power. Parameter-efficient customization techniques offer a solution.Powered by Discourse, best viewed with JavaScript enabled"
817,training-instance-segmentation-models-using-mask-r-cnn-on-the-nvidia-transfer-learning-toolkit,"Originally published at:			https://developer.nvidia.com/blog/training-instance-segmentation-models-using-maskrcnn-on-the-transfer-learning-toolkit/
To convert pixels to actionable insights, computer vision relies on deep learning to provide an understanding of the environment. Object detection is a commonly used technique to identify individual objects in a frame such as to identify people or cars. While object detection is beneficial for certain applications, it falls short when you want to…Hi,I’ve been experimenting around with the mask rcnn model, but have some questions regarding the achievable performance:In the post it shows it’s possible to achieve 15+ FPS using a Resnet50 backbone.However, I’ve been trying to replicate the results (using the same model, backbone and dataset) and so far have been able to achieve ~7FPS (measured with trtexec). What am I missing?Thanks!Following the guide to do Mask R-CNN training had an error for ModuleNotFound No module named third party with references to user vpraveen. Others are having the same error with details at TLT V2.0 Classification - #9 by MorganhGiven the nature of running this from a provided nvidia docker container not sure what to resolve.Any suggestions?I was unable to replicate the published results on the AGX just like @pbcorrea1 , i was only able to achieve around 7 fps using trtexec and around 6.7 with deepstream.Is there an explanation for this? what am i missing?Powered by Discourse, best viewed with JavaScript enabled"
818,generating-character-animations-from-speech-with-ai,"Originally published at:			Generating Character Animations from Speech with AI | NVIDIA Technical Blog
Researchers from the Max Planck Institute for Intelligent Systems, a member of NVIDIA’s NVAIL program, developed an end-to-end deep learning algorithm that can take any speech signal as input – and realistically animate it in a wide range of adult faces.  “There is an extensive literature on estimating 3D face shape, facial expressions, and facial…Powered by Discourse, best viewed with JavaScript enabled"
819,better-predictions-at-scale-with-merlin,"Originally published at:			Better Predictions at Scale with Merlin | NVIDIA Technical Blog
Data scientists, researchers, and machine learning engineers seek to provide relevant and impactful insights. With the increasing availability of large datasets, industry practitioners building effective recommenders are evaluating deep learning methods versus the traditional content-based, neighborhood, and latent factor methods. This consideration is often driven by the need for better predictions coupled with the desire…Powered by Discourse, best viewed with JavaScript enabled"
820,year-in-review-trending-posts-of-2022,"Originally published at:			https://developer.nvidia.com/blog/year-in-review-trending-posts-of-2022/
Marking a year of new and evolving technologies, 2022 produced wide-ranging advancements and AI-powered solutions across industries. These include boosting HPC and AI workload power, research breakthroughs, and new capabilities in 3D graphics, gaming, simulation, robotics, and more. In a record-breaking year, the NVIDIA Technical Blog published nearly 550 posts and received over 2 million…Powered by Discourse, best viewed with JavaScript enabled"
821,gpus-and-deep-learning-could-help-control-robots-in-space,"Originally published at:			https://developer.nvidia.com/blog/gpus-and-deep-learning-could-help-control-robots-in-space/
A six-person startup from Seattle developed augmented telerobotics software that gives humans better control of remotely operated robots which can be useful for exploring Mars or other planets. BluHaptics specializes in robotic control for underwater environments, but with a recently awarded grant funded by NASA, they are now applying their software to control robotic operations…Powered by Discourse, best viewed with JavaScript enabled"
822,integrating-nvidia-triton-inference-server-with-kaldi-asr,"Originally published at:			Integrating NVIDIA Triton Inference Server with Kaldi ASR | NVIDIA Technical Blog
Speech processing is compute-intensive and requires a powerful and flexible platform to power modern conversational AI applications. It seemed natural to combine the de facto standard platform for automatic speech recognition (ASR), the Kaldi Speech Recognition Toolkit, with the power and flexibility of NVIDIA GPUs. Kaldi adopted GPU acceleration for training workloads early on. NVIDIA…The Jupyter Notebook referred to in this post is not accessible.   Would it be possible to include it with the github repo vs. your internal gitlab?Upon following the instructions in the quick start guide we were able to launch the Triton server successfully, but upon running scripts/docker/launch_client.sh the client hung after outputting “Opening GRPC contextes…” (without the “done”) and before outputting “Streaming utterances…”A quick glance at the client code appears it is likely hanging on line 273 of kaldi-asr-client/kaldi_asr_parallel_client.cc in the TritonASRClient constructor call.Has anyone seen this issue before?This problem appears to be on our end, result of a k8s networking issue in our on-prem k8s cluster. Was eventually able to successfully run the Triton Kaldi ASR client against the server.Powered by Discourse, best viewed with JavaScript enabled"
823,gtc-2020-accelerated-computing-teaching-kit-for-university-educators-introduction-and-use-cases,"GTC 2020 S22414
Presenters: Joseph Bungo,NVIDIA ; Adarsh Krishnamurthy,Iowa State University
Abstract
As performance and functionality requirements for computing applications rise, industry demand for new graduates familiar with accelerated computing with GPUs grows. This session introduces the newest version of the Accelerated Computing Teaching Kit: a comprehensive set of academic labs, university teaching material, and e-book for use in introductory and advanced parallel programming courses. The teaching materials start with the basics and focus on programming GPUs, and include advanced topics such as optimization, advanced architectural enhancements, and integration of a variety of programming languages. We’ll present a successful course-adoption case at Iowa State University — one of many worldwide — along with student feedback. The course at Iowa State covers an introduction to parallel computing using GPUs and its application to solid modeling and CAD. As part of the course outcomes, students were able to demonstrate the use of GPU-accelerated computing in their research by working on an individual course project that uses some of the techniques taught in the class. Finally, we’ll discuss brand-new teaching kit modules covering CUDA 11, multi-GPU, and the latest libraries.Watch this session
Join in the conversation below.Powered by Discourse, best viewed with JavaScript enabled"
824,how-to-optimize-data-transfers-in-cuda-fortran,"Originally published at:			https://developer.nvidia.com/blog/how-optimize-data-transfers-cuda-fortran/
CUDA Fortran for Scientists and Engineers shows how high-performance application developers can leverage the power of GPUs using Fortran. In the previous three posts of this CUDA Fortran series we laid the groundwork for the major thrust of the series: how to optimize CUDA Fortran code. In this and the following post we begin our…Hi, how is the syntax to use cudaMemcpy3dAsync? I can't understand what the documentation says with cudaMemcpy3DParms derived type.Thanks in advance.Seconding the confusion. The documentation has:

image794×824 81.8 KB
which doesn’t specify the form of the derived type p (cudaMemcpy3DParms) at all.Powered by Discourse, best viewed with JavaScript enabled"
825,explainer-what-is-denoising,"Originally published at:			What Is Denoising? | NVIDIA Blog
Denoising is an advanced technique used to decrease grainy spots and discoloration in images while minimizing the loss of quality.Powered by Discourse, best viewed with JavaScript enabled"
826,say-cheese-make-your-day-flamin-hot-by-turning-your-photos-into-cheetos-with-artificial-intelligence,"Originally published at:			Say Cheese! Make Your Day “Flamin’ Hot” By Turning Your Photos into Cheetos With Artificial Intelligence | NVIDIA Technical Blog
Have you ever thought what the world would look like if it was made out of Cheetos? Now you can. The popular snack is inviting their fans, and anyone else with an iPhone, to see the world in “Cheetos Vision.” Based on the popular ‘style transfer’ algorithm that applies a unique style to the photo…Powered by Discourse, best viewed with JavaScript enabled"
827,neuralangelo-by-nvidia-research-reconstructs-3d-scenes-from-2d-videos,"Originally published at:			https://developer.nvidia.com/blog/neuralangelo-by-nvidia-research-reconstructs-3d-scenes-from-2d-videos/
A new model generates 3D reconstructions using neural networks, turns 2D video clips into detailed 3D structures — generating lifelike virtual replicas of buildings, sculptures and other real-world objects.Powered by Discourse, best viewed with JavaScript enabled"
828,stream-from-the-cloud-nvidia-cloudxr-release-2-0-now-available,"Originally published at:			https://developer.nvidia.com/blog/cloudxr-release-2-0-now-available/
NVIDIA CloudXR Release 2.0 is now available. With NVIDIA CloudXR, users don’t need to be physically tethered to a high-performance computer to drive rich, immersive environments. The CloudXR SDK runs on NVIDIA servers located in the cloud, edge or on-premises, delivering the advanced graphics performance needed for wireless virtual, augmented or mixed reality environments —…Powered by Discourse, best viewed with JavaScript enabled"
829,creating-dynamic-lighting-solutions-for-unreal-engine-5,"RTX GI or RTX for Global Illumination allows for innovative new ways of bringing dynamic lighting to your game. It offers indirect lighting, infinite colored bounces, and soft shadows while being fast and scalable. Check out this video to see how to achieve this.
This is part of a series of Unreal 5 related introductory videos first listed in our technical blog on the developer pages. https://developer.nvidia.com/blog/shaping-the-future-of-graphics-with-nvidia-technologies-in-unreal-engine-5/Powered by Discourse, best viewed with JavaScript enabled"
830,deploying-deep-neural-networks-with-nvidia-tensorrt,"Originally published at:			https://developer.nvidia.com/blog/deploying-deep-learning-nvidia-tensorrt/
Figure 1: NVIDIA Tensor RT provides 16x higher energy efficiency for neural network inference with FP16 on Tesla P100. Editor’s Note: An updated version of this, with additional tutorial content, is now available. See “How to Speed Up Deep Learning Using TensorRT“. NVIDIA TensorRT is a high-performance deep learning inference library for production environments. Power…https://www.facebook.com/gr... All things new and interesting on the frontier of A.I. and Deep Learning.Hi all, I would like to enquire more on the methods to use TensorRT on Faster RCNN using a ZF/VGG16 model. I'm trying to carry out a real time object detection using Faster RCNN on a Jetson TX1. I know that for convenience, I should use DetectNet instead, however, I was assigned to use the Faster RCNN framework. With ./jetson_clocks.sh, the fastest detection time took 0.48s for 300 object proposals. As such, it would like to make use of TensorRT to reduce the detection time.I researched and read up a lot of forums, including https://github.com/dusty-nv..., but I'm still confused on the methods to implement TensorRT on a Faster RCNN caffe model. I tried executing ./giexec --model=/usr/src/gie_samples/samples/data/samples/googlenet/googlenet.caffemodel --deploy=/usr/src/gie_samples/samples/data/samples/googlenet/googlenet.prototxt --output=prob --half2=true --batch=12 and I got around 63ms. Thus, I would like to use a small net to run my detection task. (With --batch=2, I get around 14ms)I have trouble understanding and following the steps on this page and the dusty-nv/jetson-inference page as I do not know which file to edit, which part to edit and etcetc. Are there any guides or other websites that are more comprehensive which you can recommend?Alternatively, I tried to run demo.py (I'm using py-faster-rcnn) using the googlenet.caffemodel but I ran into ""Check failed: K_ == new_K (1024 vs. 281600) Input size incompatible with inner product parameters.""Also, how do I enable the use of TensorRT when running the detection with VGG16 or ZF.Thank you and I would really appreciate any help given!Hi ALL, I do really want to know how to use TensorRT to run a detector on Faster-RCNN framework. Do you have any idea?when i use rndint8calibrator to run VGG19 for ILSVRC2015, why i couldn't get the same classification accuracy, there is about 5% decrease in accuracy.try new release version 2.1Hi,can you share more details how to use it?TensorRT 2.1 provides a series of samples. One of those samples is a Faster R-CNN. The ROI pooling and faster R-CNN style reshape layers are provided as IPlugin layers.Charles -- that calibrator is just intended for cases where you want to test the speed of a network in INT8 without regard to accuracy. rnd is for ""random"". You want to use Int8EntropyCalibrator. See the users guide for details.Hi All, Now I can use TensorRT to run any model, but only one model at same time. If I want to run multiple models at the same time. Can TensorRT  be able to achieve this?There are multiple ways you can do this. You can create more than one engine and switch back and forth between them within your code. Additionally TensorRT can make use of CUDA concepts like contexts and streams to allow you to coordinate work related to two or more DL models on the same GPU. This is helpful to arrange for overlapping communication and computation. See the CUDA documentation for details.Hi Leon,I am working in a similar project than yours to detect and to track players in a soccer field, I have used different techniques though. But, I can see you are using Yolo to the same task and it seems quite interesting. I wonder if it would be possible to contact via email so I can discuss with you my steps I have done until now and also yours as I said before it seems quite promising.But, if you prefer it we can open a discuss here in this forum or if you have any other suggestion, please let me know as well.I am looking forward hearing from you.Kind regards,HemechaHi Chris!  I have a similar question.  I can see that TensorRT allows us to make use of CUDA concepts like contexts and stream to run multiple models on the same GPU.I'm curious if this is an advisable approach?  How would you expect it to affect throughput and overall latency?  Does it matter whether the models are run from the same host thread or if they are run from different host threads?I just call the same caffeToGIEModel twice, with different stream out; but it broke with"" segmentation fault"" only the second time ; I locate it on pasering the model ; solved now.if load multi-models, note:  the function (ShutdownProtobufLibrary())in last line of caffeToGIEModel, only shutdown once;Chris, what kinds of DNNs require fp16 for inference vs int8? Do you think apps which require fp16 could be refactored to use the efficiency of int8? Promising improvements that you reported!Hi Chris & Team!My company is trying to deploy a DNN and we want to optimize it with TensorRT, however we need to deploy it within the client application, and this means deploying it in a Windows10 / C++ environment.  On the NVidia site it states that you can use TensorRT to "" deploy fast AI services to platforms running Linux, Microsoft Windows, BlackBerry QNX or Android operating systems"".  I have downloaded TRT 3 RC1 but do not see an infer library for Windows.  Is this supported?  Do I need to install it all on an Ubuntu system first to get access to the Windows library?  And if not supported, any idea where Windows support is on the Roadmap?Thanks!BrianBrian,Thanks for writing. We are working on adding windows support to TensorRT,  but that will roll out to customers in a 3.X or 4.0 release early next year. **: current estimate,  all plans and estimates subject to change at any time without notice.I will work with my colleagues to find the potentially misleading comment about windows and remove it.Thanks,ChrisHi Chris,I am trying to compare googlenet in Tensorrt and Caffe, when I run the googlenet sample in Tensorrt, I found there is no real data input, so I want to know do you have official sample code of googlenet dealing with real data input or what did you use to train your googlenet?Powered by Discourse, best viewed with JavaScript enabled"
831,gtc-2020-gpipe-efficient-training-of-giant-neural-networks-using-pipeline-parallelism,"GTC 2020 S21873
Presenters: Yanping Huang,Google
Abstract
Scaling up deep neural network capacity is an effective way to improve model quality for several different machine learning tasks. In many cases, increasing model capacity beyond the memory limit of a single accelerator has required developing special algorithms or infrastructure. These solutions are often architecture-specific, and do not transfer to other tasks. To address the need for efficient and task-independent model parallelism, we’ll introduce GPipe, a pipeline parallelism library that allows scaling any network that can be expressed as a sequence of layers. By pipelining different sub-sequences of layers on separate accelerators, GPipe provides the flexibility of efficiently scaling a variety of different networks to gigantic sizes. Moreover, GPipe utilizes a novel batch-splitting pipelining algorithm, resulting in almost linear speedup when a model is partitioned across multiple accelerators.Watch this session
Join in the conversation below.Powered by Discourse, best viewed with JavaScript enabled"
832,a-new-frontier-for-5g-network-security,"Originally published at:			https://developer.nvidia.com/blog/a-new-frontier-for-5g-network-security/
Wireless technology has evolved rapidly and the 5G deployments have made good progress around the world. Up until recently, wireless RAN was deployed using closed-box appliance solutions by traditional RAN vendors. This closed-box approach is not scalable, underuses the infrastructure, and does not deliver optimal RAN TCO. It has many shortcomings. We have come to…Powered by Discourse, best viewed with JavaScript enabled"
833,gtc-2020-exploiting-novel-gpu-parallelism-in-the-snap-interatomic-potential,"GTC 2020 S21976
Presenters: Evan Weinberg ,NVIDIA ; Stan Moore, Sandia National Laboratories
Abstract
Cutting-edge investigations of material behavior via classical molecular dynamics simulation methods require application-specific, quantum-accurate interatomic potentials (IAPs). The SNAP machine-learning IAP, formulated in terms of general four-body geometric invariants, is trained against quantum electronic structure calculations. This enables the verifiably high-fidelity investigation of diverse material systems at length- and timescales unattainable by purely quantum calculations. Despite the high arithmetic complexity, achieving good SNAP performance with the increasing parallelism provided by modern GPU architectures is challenging. To address this, we have developed a novel parallelization over the geometric structure of the SNAP IAP, prompting memory layout optimizations which facilitate data reuse and reduce memory bandwidth requirements. The new SNAP algorithm will be deployed in the GPU-optimized LAMMPS implementation using the Kokkos templated C++ library.Watch this session
Join in the conversation below.Powered by Discourse, best viewed with JavaScript enabled"
834,ai4kids-taiwan-inspires-ai-students-by-introducing-nvidia-jetson-nano,"Originally published at:			https://developer.nvidia.com/blog/ai4kids-taiwan-inspires-ai-students-by-introducing-nvidia-jetson-nano/
AI4Kids Taiwan held a 4-day sumer camp centered around NVIDIA JetBot robot design, industry applications, motion control, NLP, and neural networks.Powered by Discourse, best viewed with JavaScript enabled"
835,everything-you-ever-wanted-to-know-about-floating-point-but-were-afraid-to-ask,"Originally published at:			Everything You Ever Wanted to Know About Floating Point but Were Afraid to Ask | NVIDIA Technical Blog
This post was written by Nathan Whitehead A few days ago a friend came to me with a question about floating point.  Let me start by saying that my friend knows his stuff, he doesn’t ask stupid questions.  So he had my attention.  He was working on some biosciences simulation code and was getting answers of…Hello,The white paper is no longer available. Where can I get it?Thank you so much in advance.Hi Sidafa, I have fixed the broken link, so you should be able to access the white paper again.Powered by Discourse, best viewed with JavaScript enabled"
836,share-your-science-real-time-facial-reenactment-of-youtube-videos,"Originally published at:			https://developer.nvidia.com/blog/share-your-science-real-time-facial-reenactment-of-youtube-videos/
Matthias Niessner of Stanford University shares how his team of researchers are using TITAN X GPUs and CUDA to manipulate YouTube videos with real-time facial reenactment that works with any commodity webcam. The project called ‘Face2Face’ captures the facial expressions of both the source and target video using a dense photometric consistency measure. Reenactment is…Powered by Discourse, best viewed with JavaScript enabled"
837,omniverse-assets-available-for-download-on-turbosquid,"Originally published at:			Omniverse Assets Available for Download on TurboSquid | NVIDIA Technical Blog
TurboSquid and NVIDIA are collaborating to curate thousands of USD models that are available today and ready to use with NVIDIA Omniverse. Many developers using Omniverse are experiencing enhanced workflows with virtual collaboration and photorealistic simulation. The open platform, which is available now in open beta, enables teams around the world to simultaneously collaborate in…Powered by Discourse, best viewed with JavaScript enabled"
838,adding-external-knowledge-and-controllability-to-language-models-with-megatron-cntrl,"Originally published at:			Adding External Knowledge and Controllability to Language Models with Megatron-CNTRL | NVIDIA Technical Blog
Large language models such as Megatron and GPT-3 are transforming AI. We are excited about applications that can take advantage of these models to create better conversational AI. One main problem that generative language models have in conversational AI applications is their lack of controllability and consistency with real-world facts. In this work, we try…The next level of coversational AI - story generation with control keywords. What’s your next AI story about? Tell us how this paper will help you in your work.Powered by Discourse, best viewed with JavaScript enabled"
839,understanding-when-to-use-doca-drivers-and-doca-libraries,"Originally published at:			https://developer.nvidia.com/blog/understanding-when-to-use-doca-drivers-and-doca-libraries/
Libraries and drivers are not one and the same. This blog explains which is the best for your need to clear up any confusion.Powered by Discourse, best viewed with JavaScript enabled"
840,cuda-pro-tip-the-fast-way-to-query-device-properties,"Originally published at:			CUDA Pro Tip: The Fast Way to Query Device Properties | NVIDIA Technical Blog
CUDA applications often need to know the maximum available shared memory per block or to query the number of multiprocessors in the active GPU. One way to do this is by calling cudaGetDeviceProperties(). Unfortunately, calling this function inside a performance-critical section of your code lead to huge slowdowns, depending on your code. We found out…CUDA should provide an balance between everything per API (cudaGetDeviceProperties) v/s one query per API (cudaDeviceGetAttribute()).
An API with dynamic number of queries would allow user to choose what it needs without multiple function calls.
for an example: vulkan has VkPhysicalDeviceProperties2(3)Another suggestion, Avoid doing a PCIE read access for many of the properties which are constant and can be cached at cuInit/ cuDevice creation.i regular read your website all the blog of theses websites is  amazingUsing Titan V, cuda V10.1.105, driver version 418.56, I got, for example,cudaGetDeviceProperties -> 194847uscudaDeviceGetAttribute -> 160394usThey are quite similar.  Why?Interesting. Did you run the code in the post as-is, or did you query different attributes using  cudaGetDeviceAttribute ?Yes, I run the code in the post as it is.Where I find the names for each property (e.g. warp size, etc)?Powered by Discourse, best viewed with JavaScript enabled"
841,developing-applications-with-nvidia-bluefield-dpu-and-nvidia-doca-libraries,"Originally published at:			https://developer.nvidia.com/blog/developing-applications-with-bluefield-dpu-and-doca-libraries/
The development process for DPUs can get complex. This is where NVIDIA DOCA comes in. With several built-in libraries that allows for plug-n-play and simple application development.Hi, jwitsoe,I hope you have a great day today.
How to offload data/application/training from host to DPU?
Similar to this work “Optimizing Distributed DNN Training Using
CPUs and BlueField-2 DPUs”
Could you give more guidance on the techniques?Thank you so much in advance.Powered by Discourse, best viewed with JavaScript enabled"
842,disney-ai-system-associates-images-with-sounds,"Originally published at:			Disney AI System Associates Images with Sounds | NVIDIA Technical Blog
Disney Research developed a system that can recognize various objects in videos and automatically add related sound effects, such as a glasses clinking or cars driving down the road. Using a GeForce GTX 980 Ti GPU and the Caffe deep learning framework, the researchers trained their model to recognize the sound of images by feeding…Powered by Discourse, best viewed with JavaScript enabled"
843,gpu-accelerating-node-js-javascript-for-visualization-and-beyond,"Originally published at:			https://developer.nvidia.com/blog/gpu-accelerating-node-js-javascript-for-visualization-and-beyond/
NVIDIA GTC21 had numerous great and engaging contents, especially around RAPIDS, so it would be easy to miss our debut presentation “Using RAPIDS to Accelerate Node.js JavaScript for Visualization and Beyond.” Yep – we are bringing the power of GPU accelerated data science to the JavaScript Node.js community with the Node-RAPIDS project. Node-RAPIDS is an…As we are early in development, we’d welcome requests and collaborations. Feel free to reach out about other parts of the node.js ecosystem you’d like to see GPU accelerated.Powered by Discourse, best viewed with JavaScript enabled"
844,use-automatic-mixed-precision-on-tensor-cores-in-frameworks-today,"Originally published at:			Use Automatic Mixed Precision on Tensor Cores in Frameworks Today | NVIDIA Technical Blog
NVIDIA Tensor Core GPU architecture now automatically and natively supported in TensorFlow, PyTorch and MXNet NVIDIA CUDA X AI enables mixed precision AI training with just two lines of code, delivering up to 3x speedup Mixed precision training utilizes half-precision to speed up training, achieving the same accuracy as single-precision training using the same hyper-parameters.…Powered by Discourse, best viewed with JavaScript enabled"
845,german-researchers-develop-early-warning-ai-for-self-driving-systems,"Originally published at:			German Researchers Develop Early Warning AI for Self-Driving Systems | NVIDIA Technical Blog
Self-driving cars can run into critical situations where a human driver must retake control for safety reasons. Researchers from the Technical University of Munich have developed an AI early warning system that can give human drivers a seven-second heads-up about these critical driving scenarios.Powered by Discourse, best viewed with JavaScript enabled"
846,building-video-game-levels-with-the-help-of-ai,"Originally published at:			Building Video Game Levels with the Help of AI | NVIDIA Technical Blog
Content creation in the gaming industry is one of the most expensive and time-consuming tasks in the development process. That is why a team of researchers in Italy recently developed a deep learning-based method that can help developers build new video game levels more quickly and more efficiently. The method uses generative adversarial networks (GANs)…Powered by Discourse, best viewed with JavaScript enabled"
847,top-5-reasons-to-attend-gtc,"Originally published at:			Top 5 Reasons to Attend GTC | NVIDIA Technical Blog
NVIDIA’s GPU Technology Conference (GTC) is just around the corner. To give you a preview, we’ve put together a list highlighting the top 5 reasons why you should attend. Watch below:Powered by Discourse, best viewed with JavaScript enabled"
848,nvidia-variable-rate-shading-demonstrated-in-autodesk-vred,"Originally published at:			NVIDIA Variable Rate Shading Demonstrated in Autodesk VRED | NVIDIA Technical Blog
We introduced Variable Rate Shading (VRS) last year with the Turing architecture. This new, easy to implement rendering technique allows developers to vary the amount of processing power spent across the image, to spend time and increase quality in the areas where they are most needed. Since virtual reality demands both a high performance and…Powered by Discourse, best viewed with JavaScript enabled"
849,top-5-healthcare-and-medical-research-gtc-sessions,"Originally published at:			Top 5 Healthcare and Medical Research GTC Sessions | NVIDIA Technical Blog
NVIDIA’s GPU Technology Conference (GTC) is the premier AI and deep learning conference, providing training, insights, and direct access to experts on advancing life science, pharmaceutical, and biomedical research.Powered by Discourse, best viewed with JavaScript enabled"
850,cudacasts-episode-19-cuda-6-guided-performance-analysis-with-the-visual-profiler,"Originally published at:			https://developer.nvidia.com/blog/cudacasts-episode-19-cuda-6-guided-performance-analysis-visual-profiler/
One of the main reasons for accelerating code on an NVIDIA GPU is for an increase in application performance. This is why it’s important to use the best tools available to help you get the performance you’re looking for. CUDA 6 includes great improvements to the guided analysis tool in the NVIDIA Visual Profiler. Watch today’s CUDACast…Powered by Discourse, best viewed with JavaScript enabled"
851,csiro-powers-bionic-vision-research-with-new-gpu-accelerated-supercomputer,"Originally published at:			CSIRO Powers Bionic Vision Research with New GPU-Accelerated Supercomputer | NVIDIA Technical Blog
The Commonwealth Scientific and Industrial Research Organisation (CSIRO) in Australia welcomed a new large scale scientific computing system to expand their capability in deep learning, a key approach to furthering progress towards artificial intelligence. “This is a critical enabler for CSIRO science, engineering and innovation. As a leading global research organization, it’s important to sustain…Powered by Discourse, best viewed with JavaScript enabled"
852,this-ai-selfie-transformation-app-can-even-make-mona-lisa-smile,"Originally published at:			This AI Selfie Transformation App Can Even Make Mona Lisa Smile | NVIDIA Technical Blog
A new mobile app called FaceApp uses neural networks to edit your selfie via photo-realistic filters – letting you add a smile, swap genders, and can take years off your age. Developed and self-funded by a small group out of Russia in eight months, the app uses CUDA, TITAN X Pascal GPUs, Tesla K80s and…Powered by Discourse, best viewed with JavaScript enabled"
853,ray-tracing-magic-from-a-one-person-team,"Originally published at:			https://developer.nvidia.com/blog/ray-tracing-magic-from-a-one-person-team/
Abstraction, the winner of the DXR Spotlight Contest 2020, takes its audience on a tour of a world that tricks the eye and startles the senses.  We sat down with the creator of Abstraction, Jonah Walters, to learn how he built it all by himself.Powered by Discourse, best viewed with JavaScript enabled"
854,top-5-ai-stories-of-the-week-2-18,"Originally published at:			Top 5 AI Stories of the Week: 2/18 | NVIDIA Technical Blog
In this week’s edition of the NVIDIA Developer Top 5 video, we revisit the top AI developer stories of the week. From an AI drone challenge to a system that can predict pedestrians next steps. Watch all five stories here and see all the links below: 5 – Deep Learning Identifies Depression in Speech Patterns…Powered by Discourse, best viewed with JavaScript enabled"
855,mit-researchers-created-eye-tracking-solution-for-your-smartphone,"Originally published at:			MIT Researchers Created Eye-Tracking Solution For Your Smartphone | NVIDIA Technical Blog
From scientific research to commercial applications, eye tracking is an important tool across many domains. Researchers from MIT and University of Georgia developed software that can turn any smartphone into an eye-tracking device. Besides making existing applications of eye-tracking technology more accessible, the system could also help detect signs of incipient neurological disease or mental…Powered by Discourse, best viewed with JavaScript enabled"
856,upcoming-workshop-building-conversational-ai-applications,"Originally published at:			​ - Building Conversational AI Applications (EMEA)
On May 23 at 9 am CEST learn to build and deploy production-quality conversational AI applications with real-time transcription and natural language processing capabilities.Powered by Discourse, best viewed with JavaScript enabled"
857,gpus-help-measure-rising-sea-levels-in-real-time,"Originally published at:			GPUs Help Measure Rising Sea Levels in Real-Time | NVIDIA Technical Blog
Sea levels have traditionally been measured by marks on land – but the problem with this approach is that parts of the earth’s crust move too. A group of researchers from Chalmers University of Technology in Sweden are using GPS receivers along the coastline in combination with reflections of GPS signals that bounce off the…Powered by Discourse, best viewed with JavaScript enabled"
858,reducing-development-time-for-intelligent-virtual-assistants-in-contact-centers,"Originally published at:			https://developer.nvidia.com/blog/reducing-development-time-for-intelligent-virtual-assistants-in-contact-centers/
As the global service economy grows, companies rely increasingly on contact centers to drive better customer experiences, increase customer satisfaction, and lower costs with increased efficiencies. Customer demand has increased far more rapidly than contact center employment ever could. Combined with the high agent churn rate, customer demand creates a need for more automated real-time…Powered by Discourse, best viewed with JavaScript enabled"
859,powering-the-new-age-of-scientific-computing-sc20,"Originally published at:			Powering the New Age of Scientific Computing - SC20 | NVIDIA Technical Blog
Join us online at Supercomputing 2020 (SC20) to see how GPU computing and high-performance networking are transforming computational science and AI. We’re featuring our own virtual theater with speakers from various leading industries and domains, along with demos, related content, and more. Virtual Theater Tune in to these short, on-demand sessions to hear from industry…Powered by Discourse, best viewed with JavaScript enabled"
860,benchmarking-cuda-aware-mpi,"Originally published at:			https://developer.nvidia.com/blog/benchmarking-cuda-aware-mpi/
I introduced CUDA-aware MPI in my last post, with an introduction to MPI and a description of the functionality and benefits of CUDA-aware MPI. In this post I will demonstrate the performance of MPI through both synthetic and realistic benchmarks. Synthetic MPI Benchmark results Since you now know why CUDA-aware MPI is more efficient from…Is this code compatible with the graphics cores in an cluster of Jetson TK1s?Hi Spencer, yes the Jacobi solver should run on a Tegra K1 if you can get a working CUDA-aware MPI there. I would not expect issues with compiling OpenMPI or MVAPICH2 on the Tegra K1, but I have not tested this myself. One change you should make is to change USE_FLOAT in Jacobi.h to 1. The code would also work with double precision but slower. Also please be aware that no variant of GPUDirect is available on a cluster of Tegra K1. JiriHi Jiri, Thanks for responding, I was able to get the code to compile.  I have 40 Jetson TK1s, so would it be best to run this code with mpirun -np 40 ./jacobi_cuda_aware_mpi -t 40 1 ? or with the number of cores the jetson has; mpirun -np 160 ./jacobi_cuda_aware_mpi -160 1 ?  Also, would it be best to compile with GENCODE 30, since the jetson supports 3.2?  In addition, running the jacobi_cuda_aware_mpi give me the error ""The call to cuIpcGetMemHandle failed. This means the GPU RDMA protocol cannot be used.""  This is what you were referring to when you said GPUDirect is not available?Thanks!Hi Spencer,good to hear that you could get it to compile. To answer your questions:1. Its most efficient to start one MPI process per GPU, i.e. one process per Jetson TK1, which is 40 in your case. You probably want to try different process layouts. E.g. Try a 8 x 5 layout with mpirun -np 40 ./jacobi_cuda_aware_mpi -t 8 5.2. You should compile for compute capability 3.2 by adding -gencode arch=compute_32,code=sm_32 to the Makefile.3. You are right you get the error because GPUDirect is not available. If you are using OpenMPI you can disable CUDA IPC (GPUDirect P2P between processes) by passing --mca btl_smcuda_use_cuda_ipc 0 as an argument to mpirun (see https://www.open-mpi.org/fa... to avoid that error.JiriThanks again,Yes, I am running OpenMPI, 1.10.2, I was able to run the code.  The cuda_aware version with the CUDA IPC disabled performs about 4 times fewer GFLOPS compared to the cuda_normal code.  I was expecting it to perform better.  I was reading about using ZeroCopy to replace the GPUDirect, because the Jetson TK1 is a shared memory system.  In this case, both codes use CUDA, but CUDA_AWARE uses CUDA with MPI, and CUDA_NORMAL uses CUDA on each host, is this correct?Hi Spencer, it is indeed unexpected that the CUDA-aware version performs so much worse compared to the regular MPI version. However as you point out using zero-copy memory and a regular MPI will be the more efficient thing to do on the Jetson's anyway. Regarding the difference between the two versions of the code: The CUDA-aware version passes device pointers directly into MPI and let the CUDA-aware MPI handle the data transport. The regular MPI version uses cudaMemcpy to stage device memory to host memory before passing it into MPI. When you switch to zero-copy you would use a regular MPI and don't do any staging with cudaMemcpy. JiriHello, I'm running the code on a single tesla k40 card with the default size (4096x4096) and it gives me GLU / s only 2.53 in double precision, if I do it in single presicion I get double, but I can not reproduce the first point of the weak scalability graph, I have to take into account something else? regardsHi Ferche, did you pass in the command line option for fast swap (-fs)? Sorry that this is not explicitly mentioned in the post above.Hi Jiri, thanks for you reply, now with that option I reach on average 7.68 GLU / s that is closer to benchmark, this in single precision. In double precision I get an average of 5.17 GLU / s. The benchmark is in single precision?We are configuring a cluster of GPUs, and to corroborate the scalability and make sure that we have it correctly configured, we are basing ourselves on this benchmark.We are running for 4, 8 and 16 GPUs but we are far from hitting the benchmark, I think we are doing something wrong.If you can help us it would be very valuable, I leave you my e-mail if you wish, ferchelab23@gmail.comRegardsSorry, the e-mail is wrong, the correct is ferchelab23@gmail.comTo validate your cluster setup I would recommend that you start of with some MPI micro benchmarks and check if you are hitting expected inter GPU bandwidths and latencies (both intra node and inter node). E.g. the OSU ones: http://mvapich.cse.ohio-sta... Regarding the Jacobi benchmark described in this blog post one reason why you might not see the expected behavior is that GPU affinity is not handled correctly. Did you make sure that the macro ENV_LOCAL_RANK in src/Jacobi.h matches the environment variable exported by your MPI launcher? It defaults to MV2_COMM_WORLD_LOCAL_RANK and would need to be changed to OMPI_COMM_WORLD_LOCAL_RANK in case of OpenMPI or something different depending on your MPI launcher. Last but not least I would also like to point you to another github repository with some multi GPU example codes: https://github.com/NVIDIA/m... and a recording of a GTC Talk on Multi GPU Programming with MPI: http://on-demand-gtc.gputec...Thanks again Jiri, your help has been very valuable! RegardsGood article!.  I have some questions related to CUDA aware MPI.  Consider this common pattern: when one sends/receives messages, on the sender side, one has to gather data from local data structures and put it in a send buffer (i.e., Pack); on the receiver side, one has to scatter (i.e., unpack) data from recv buffer to local data structures. a) On communication / computation overlapping   On sender side, my practice is: pack data for one neighbor, then MPI_Isend it, then pack for next neighbor. It looks packing and sending are overlapped. If use CUDA aware MPI, the pack routine is going to be a CUDA kernel. I can either  1) Call one kernel to pack data for all neighbors, then call multiple MPI_Isends; OR  2) Call multiple kernels and interleave pack, Isend, pack, Isend.Method 1 uses fewer kernels, but does not do comm / comp overlapping, while method 2 does. So, which one is better? Is comm. / comp. overlapping worthwhile on GPU?b) On atomic operationOn the receiver side, occasionally, the received data is supposed to be reduced to user's data. At runtime, some entries (not all) in the received buffer have the same destination -- they are reduced to the same location in the user's data structure (e.g., ADD, MAX, etc). Are there tips to implement this unpack kernel efficiently?Thanks.Hi Junchao,thanks for your comments. These are good questions. Let me try to answers:a) On communication / computation overlapping: Ultimately what is the best strategy will depend on multi factors. To my experience a generally good strategy is to first asynchronously launch CUDA kernels operating on data not involved in communication and then do the MPI communication synchronously (might be a series of MPI_Isend + MPI_Irecv + MPI_Waitall) while the CUDA kernel runs. In case of non contiguous communication buffers a possible strategy would be to first launch 1 packing kernel in stream A, then launch the kernel processing the interior of the domain in stream B, wait for stream A to finish and do all MPI while the kernel in stream B is still running. That way you do not get overlap between packing an communication, but in many cases you can still hide communication times and avoid launch overheads for many small packing kernels. On this topic you might also want to look at GPU side data type processing as e.g. MVAPICH2-GDR support it: http://mvapich.cse.ohio-sta...b) On atomic operation: I am sorry I don't think I understand you question sufficiently. Is you use case covered by a collective implemented in CUDA-aware MPI or NCCL: https://developer.nvidia.co... ? NCCL is interoperabel with MPI so it might be a good option for you.Hope this helpsJiriJiri, Thanks for the info. By atomic operations, I meant this common code pattern: Once MPI ranks received their data in their receive buffer, they need to ""Reduce"" the data into their local data structure (for example, an array u[]). Let's say we need to do u[idx[i]] += buf[i]. The index array idx[] is given but its content is runtime dependent.  If there are chances that idx[i] = idx[j], to avoid data race, the ADD operation has to be atomic.  I can blindly use atomics even in reality there is no conflicts, or, I can analyze the idx[] array to test if there are conflicts and dispatch the code path to one without conflicts using regular operations. and the other with conflicts using atomicd. Of course, the latter will complicate the code. I was told today that as long as the data is in global memory, atomics are as fast as regular instructions when there are no conflicts. So I prefer to using atomics blindly in this case.Thanks for clarifying. So you are referring to a completely local operation. I was confused as I have read your statement in the context of MPI. In this case I agree with your conclusion. It is very likely that just using global memory atomic operations will give you the best performance (compared with other more complicated approaches like sorting the index array).Powered by Discourse, best viewed with JavaScript enabled"
861,transform-the-future-with-robotics-at-nvidia-gtc,"Originally published at:			https://developer.nvidia.com/blog/transform-the-future-with-robotics-at-nvidia-gtc/
More than 15 sessions will be presented covering autonomous machines and robotics-specific topics, ranging from manufacturing automation, to robotics research and learning.Powered by Discourse, best viewed with JavaScript enabled"
862,mlperf-hpc-v1-0-deep-dive-into-optimizations-leading-to-record-setting-nvidia-performance,"Originally published at:			https://developer.nvidia.com/blog/mlperf-hpc-v1-0-deep-dive-into-optimizations-leading-to-record-setting-nvidia-performance/
Learn about the optimizations and techniques used across the full stack in the NVIDIA AI platform that led to a record-setting performance in MLPerf HPC v1.0.@jwitsoe , can I get any assitance on Nvidia’s MLPerf HPC v1.0 implementation on Cosmoflow , please direct me to correct forum , I rasied issue on github but looks like its a dead horsePowered by Discourse, best viewed with JavaScript enabled"
863,upcoming-workshop-fundamentals-of-accelerated-computing-with-cuda-c-c,"Originally published at:			Personal Information - Fundamentals of Accelerated Computing with CUDA C/C++ (EMEA)
Learn tools and techniques for accelerating C/C++ applications to run on massively parallel GPUs with CUDA.Powered by Discourse, best viewed with JavaScript enabled"
864,explainer-what-is-agent-assist,"Originally published at:			What Is Agent Assist? | NVIDIA Blog
Agent-assist technology uses AI and ML to provide facts and make real-time suggestions that help human agents across retail, telecom, and other industries conduct conversations with customers.Powered by Discourse, best viewed with JavaScript enabled"
865,rtx3060-along-side-quadro-p1000-for-machine-learning-in-ms-windows-11,"I just purchased an Nvidia RTX3060 to use or machine learning and artificial intelligence. I couldn’t get my freebie legacy Tesla K40c to be detected for a MS Windows install with Anaconda and WSL2 confguration for machine learning.My question to the development team is, is my RTX3060 fully compatible with the latest Nvidia GPU driver, CUDA Toolkit, CuDNN, Nvidia Control Panel, and Anaconda for machine learning development with Python and Jupyter Notebooks? Will it be detected by Python?If you have a cookbook to setup and configure my RTX3060 GPU in MS Windows 11 then I’ll be sure to follow it fully because it’s very painful to setup in Windows to get it working with my Tesla K40c headless GPU.Thanks and I really enjoy using Nvidia products like the Jetson embedded solutions and own the stock.Hello! Yes, this combination (RTX 3060) is fully supported in CUDA, WSL2, cuDNN, etc. I use an RTX 3090 on one of my personal systems for similar. However, your other GPU is a Kepler architecture GPU which is deprecated and won’t be recognized by modern drivers, so you wouldn’t be able to use that one with modern CUDA. The last driver release that supported it was R470, from back in 2021.Thank you so very much for your detailed reply and clarification to my question. I’m so glad I participated in today’s AMA and I am more confident now to get everything fully working later this week when I have time long enough to configure it and test it with your answer. :-)Thanks for participating. Feel free to take the opportunity to post new questions.Powered by Discourse, best viewed with JavaScript enabled"
866,jetson-project-of-the-month-creating-intelligent-music-with-the-neurorack-deep-ai-based-synthesizer,"Originally published at:			https://developer.nvidia.com/blog/jetson-neurorack-deep-ai-synthesizer/
This Jetson Project of the Month enhances synthesizer-based music by applying deep generative models to a classic Eurorack machine.Powered by Discourse, best viewed with JavaScript enabled"
867,cuda-pro-tip-generate-custom-application-profile-timelines-with-nvtx,"Originally published at:			https://developer.nvidia.com/blog/cuda-pro-tip-generate-custom-application-profile-timelines-nvtx/
The last time you used the timeline feature in the NVIDIA Visual Profiler, Nsight VSE or the new Nsight Systems to analyze a complex application, you might have wished to see a bit more than just CUDA API calls and GPU kernels. In this post I will show you how you can use the NVIDIA…Is it me or that all the colors in your example are transpararent?Hi Elad, sorry for the long delay with my response. That all the colors are transparent was caused by the alpha channels set to 00 in the initial version of this post. The post and the code examples have been fixed. Thanks JiriHello,Is there a way that I can find the duration of NVTX range? I have function which contains a mix of CPU and GPU activity. Using Nsight Systems would give me the runtime of just the kernels, but I was wondering if there is any functionality in the NVTX API that can let me gather the duration of the NVTX range around this function?Hi, NSight systems displays NVTX ranges. You might need to expand some rows to see them. In addition to that you can get some statistics also for NVTX ranges with --stats (see https://docs.nvidia.com/nsi.... NVTX does not provide an API to query the runtime of an already passed range. Hope this helps JiriHi, I am trying to get the NVTX ranges from my application automatically as you explained in the post. However, the ranges are not created for all the functions. For example, my application has forward and backward FFT functions, however, there are no NVTX ranges for the forward FFTs.
Thanks in advance for your help.Hi Vahdaneh,
interesting that you see some NVTX ranges but not all expected. Given that you see some I think you NSight Systems and NVTX setup is probably correct so I would look at the compiler instrumentation side of things. I would specifically check what happens with regards to function inlining. Some compilers disable function inlining when compiler instrumentation is used and some don’t instrument inlined functions. Often this depends on your compile flags.
Hope this helps
JiriPowered by Discourse, best viewed with JavaScript enabled"
868,google-cloud-announces-a-hybrid-platform-for-nvidia-gpu-workloads-in-the-cloud-on-prem-and-at-the-edge,"Originally published at:			https://developer.nvidia.com/blog/google-cloud-announces-a-hybrid-platform-for-nvidia-gpu-workloads-in-the-cloud-on-prem-and-at-the-edge/
Now available, Google Cloud Anthos is an application modernization platform powered by Kubernetes for customers looking for a hybrid architecture.Powered by Discourse, best viewed with JavaScript enabled"
869,beginners-guide-to-gpu-accelerated-graph-analytics-in-python,"Originally published at:			https://developer.nvidia.com/blog/beginners-guide-to-gpu-accelerated-graph-analytics-in-python/
This tutorial is the sixth installment of introductions to the RAPIDS ecosystem. The series explores and discusses various aspects of RAPIDS that allow its users solve ETL (Extract, Transform, Load) problems, build ML (Machine Learning) and DL (Deep Learning) models, explore expansive graphs, process geospatial, signal, and system log data, or use SQL language via…Hello… I was wondering if cugraph supports ismags algorithm of networkxPowered by Discourse, best viewed with JavaScript enabled"
870,upcoming-webinar-build-a-computer-vision-application-with-nvidia-ai-on-google-cloud-vertex-ai,"Originally published at:			https://info.nvidia.com/build-a-computer-vision-app-with-nvidia-ai-webinar.html
Register for this live webinar to learn how to build an action recognition application with NVIDIA AI on Google Cloud Vertex AI.Powered by Discourse, best viewed with JavaScript enabled"
871,using-physx-for-vehicle-simulations-in-games,"Originally published at:			https://developer.nvidia.com/blog/using-physx-for-vehicle-simulations-in-games/
Learn how NVIDIA PhysX 4.1 is now being used for self-driving car training.Powered by Discourse, best viewed with JavaScript enabled"
872,differences-between-ai-servers-and-ai-workstations,"Originally published at:			Differences between AI Servers and AI Workstations | NVIDIA Technical Blog
AI servers and AI workstations are commonly confused with one another. Learn about key differences between the two and why the distinction is important.Powered by Discourse, best viewed with JavaScript enabled"
873,how-to-query-device-properties-and-handle-errors-in-cuda-fortran,"Originally published at:			https://developer.nvidia.com/blog/how-query-device-properties-and-handle-errors-cuda-fortran/
CUDA Fortran for Scientists and Engineers shows how high-performance application developers can leverage the power of GPUs using Fortran. In this third post of the CUDA Fortran series we discuss various characteristics of the wide range of CUDA-capable GPUs, how to query device properties from within a CUDA Fortran program, and how to handle errors.…Is it possible to get the unique device BUS ID through the ""prop"" derived type? In the native C interface, it is possible to fetch the CHARACTER valued attribute called ""luid"", but that is apparently missing in the Fortran API.I suggest you take this question to the developer forums, devtalk.nvidia.com.Powered by Discourse, best viewed with JavaScript enabled"
874,webinar-empower-telco-contact-center-agents-with-multi-language-speech-ai-customized-agent-assists,"Originally published at:			Empower Telco Contact Center Agents with Multi-Language Speech-AI-Customized Agent Assists
Join Infosys, NVIDIA and Quantiphi on June 7 to learn how to use speech and translation AI to improve agent assist solutions in multiple languages.Powered by Discourse, best viewed with JavaScript enabled"
875,unleash-your-creativity-with-rtx-real-time-ray-tracing,"Originally published at:			Unleash Your Creativity with RTX Real-Time Ray Tracing | NVIDIA Technical Blog
Real-time ray tracing was once considered impossible in graphics, but it’s now become a reality for anyone with an NVIDIA RTX GPU. With real-time ray tracing, artists and designers can create cinematic-quality graphics faster than before. And now you have a chance to show us what you can design with RTX. Enter our contest, Real-Time…Powered by Discourse, best viewed with JavaScript enabled"
876,researchers-use-ai-to-map-how-the-immune-system-fights-cancer,"Originally published at:			Researchers Use AI to Map How the Immune System Fights Cancer | NVIDIA Technical Blog
Researchers from Stony Brook University, the University of Texas, Emory University, and the Institute for Systems Biology, developed a deep learning system that maps how the immune system fights cancer.   The study, published in the journal Cell Reports, has the potential to help pathologists more precisely define cancers and develop better treatment plans for…Powered by Discourse, best viewed with JavaScript enabled"
877,using-gpus-to-analyze-covid-19-short-read-sequencing-data,"Originally published at:			https://developer.nvidia.com/blog/using-gpus-to-analyze-covid-19-short-read-sequencing-data/
Genomics analysis is playing a key role in COVID-19 studies, as the data from sequencing projects is helping researchers better understand and characterize the coronavirus. With NVIDIA Parabricks, researchers can integrate GPU power into existing genomics workflows and rapidly sequence fragments to enhance research. In the COMPUTE4COVID webinar series, we discuss in detail how sequencing…Powered by Discourse, best viewed with JavaScript enabled"
878,improving-breast-cancer-detection-in-ultrasound-imaging-using-ai,"Originally published at:			https://developer.nvidia.com/blog/improving-breast-cancer-detection-in-ultrasound-imaging-using-ai/
Although ultrasound imaging is often used to detect breast cancer, especially mammographically occult cancers, its disadvantage is that it leads to high false-positive rates. We develop an AI system that achieves radiologist-level accuracy in identifying cancer. It is interpretable, achieves high accuracy on an external test set, and is trained in a weakly supervised manner.Powered by Discourse, best viewed with JavaScript enabled"
879,efficient-bert-finding-your-optimal-model-with-multimetric-bayesian-optimization-part-1,"Originally published at:			Efficient BERT: Finding Your Optimal Model with Multimetric Bayesian Optimization, Part 1 | NVIDIA Technical Blog
This is the first post in a series about distilling BERT with multimetric Bayesian optimization. Part 2 discusses the set up for the Bayesian experiment, and Part 3 discusses the results. You’ve all heard of BERT: Ernie’s partner in crime. Just kidding! I mean the natural language processing (NLP) architecture developed by Google in 2018.…Powered by Discourse, best viewed with JavaScript enabled"
880,performance-boosts-and-enhanced-features-in-new-nsight-graphics-nsight-aftermath-releases,"Originally published at:			https://developer.nvidia.com/blog/performance-boosts-and-enhanced-features-in-new-nsight-graphics-nsight-aftermath-releases/
Significant performance gains and Vulkan optimizations in the Nsight Graphics 2022.3 release alongside improvements to GPU crash logs in Nsight Aftermath 2022.2.Powered by Discourse, best viewed with JavaScript enabled"
881,transforming-brain-waves-into-words-with-ai,"Originally published at:			https://developer.nvidia.com/blog/transforming-brain-waves-into-words-with-ai/
New research out of the University of California, San Francisco has given a paralyzed man the ability to communicate by translating his brain signals into computer generated writing. The study, published in The New England Journal of Medicine, marks a significant milestone toward restoring communication for people who have lost the ability to speak.  “To…Powered by Discourse, best viewed with JavaScript enabled"
882,gpu-integration-propels-data-center-efficiency-and-cost-savings-for-taboola,"Originally published at:			https://developer.nvidia.com/blog/gpu-integration-propels-data-center-efficiency-and-cost-savings-for-taboola/
When you see a context-relevant advertisement on a web page, it’s most likely content served by a Taboola data pipeline. As the leading content recommendation company in the world, a big challenge for Taboola was the frequent need to scale Apache Spark CPU cluster capacity to address the constantly growing compute and storage requirements. Data…So much time could have been saved not to mention high potential for better performance results if they would have used our Synopsys Optimizer Studio (former Concertio) tools.Powered by Discourse, best viewed with JavaScript enabled"
883,gpu-accelerated-supercomputer-targets-tumors,"Originally published at:			GPU-Accelerated Supercomputer Targets Tumors | NVIDIA Technical Blog
A team of researchers from the  Helmholtz-Zentrum Dresden-Rossendorf (HZDR) research lab in Germany are using the Titan Supercomputer at Oak Ridge National Laboratory to advance laser-driven radiation treatment of cancerous tumors. Recently, doctors have been using beams of heavy particles, such as protons or ions, to treat cancer tumors. These beams can deposit most of…Powered by Discourse, best viewed with JavaScript enabled"
884,meet-the-gpu-accelerated-latte-making-robot,"Originally published at:			https://developer.nvidia.com/blog/meet-the-gpu-accelerated-latte-making-robot/
Researchers in the Robot Learning Lab at Cornell University developed a robot that can prepare a cup of latte without ever having seen the machine before – the robot does this by visually observing the machine and by reading online instruction manuals, similar to how humans learn. The team used CUDA and TITAN X GPUs…Powered by Discourse, best viewed with JavaScript enabled"
885,nvidia-maxine-elevates-video-conferencing-in-the-cloud,"Originally published at:			https://developer.nvidia.com/blog/nvidia-maxine-elevates-video-conferencing-in-the-cloud/
NVIDIA Maxine has expanded to provide microservices that can be deployed in private or public clouds, enabling developers to leverage GPU power from remote servers.Thank you for sharing this incredible development with us. NVIDIA Maxine is definitely revolutionizing the way we connect and collaborate remotely.Powered by Discourse, best viewed with JavaScript enabled"
886,new-pre-trained-models-and-management-platform-for-smart-hospitals,"Originally published at:			https://developer.nvidia.com/blog/new-pre-trained-models-and-management-clara-guardian/
We previously announced Clara Guardian, an application framework and partner ecosystem that accelerates the development and deployment of smart sensors with multimodal AI anywhere in the hospital. Today four pre-trained models and NVIDIA Fleet Command are officially available for early access.  Clara Guardian’s key components include healthcare pre-trained models for computer vision and speech, training…Powered by Discourse, best viewed with JavaScript enabled"
887,automatic-object-removal-and-realistic-image-completion,"Originally published at:			Automatic Object Removal and Realistic Image Completion | NVIDIA Technical Blog
Researchers from Waseda University in Japan developed a deep learning-based method that removes unwanted objects from images and can complete images by filling-in missing regions. Image completion is a challenging problem because it requires a high-level recognition of scenes. “To train this image completion network to be consistent, we use global and local context discriminators…Powered by Discourse, best viewed with JavaScript enabled"
888,nvidia-gtc-top-data-science-sessions,"Originally published at:			NVIDIA GTC: Top Data Science Sessions | NVIDIA Technical Blog
NVIDIA’s GTC is the # 1 Artificial Intelligence Conference. Attend the top six amazing Data Science Sessions, fee and on-demand.There seems to be an error on the page, the description of PyTorch Ecosystem: The State of the State 2021 [A31212] is the same as for Accelerated Data Science for Molecule Design (A31104)@zerweck Thank you for catching that. The link has been updated.
image758×738 54.2 KB

Still looks the same for meFixed now. Thank you!!Powered by Discourse, best viewed with JavaScript enabled"
889,developing-ai-powered-digital-health-applications-using-nvidia-jetson-edge,"Originally published at:			https://developer.nvidia.com/blog/developing-ai-powered-digital-health-applications-using-jetson-edge/
Traditional healthcare systems have large amounts of patient data in the form of physiological signals, medical records, provider notes, and comments. The biggest challenges involved in developing digital health applications are analyzing the vast amounts of data available, deriving actionable insights, and developing solutions that can run on embedded devices. Engineers and data scientists working…Powered by Discourse, best viewed with JavaScript enabled"
890,gpu-accelerated-molecular-dynamics-applications-help-fight-covid-19,"Originally published at:			GPU-Accelerated Molecular Dynamics Applications Help Fight COVID-19 | NVIDIA Technical Blog
By Geetika Gupta As the world battles to reach a scientific breakthrough in the fight against COVID-19, scientists are turning to computing resources to accelerate their research. To help make the process for scientists more accessible, we’re spotlighting a few of the GPU-accelerated applications that developers can use right now in the fight against this…Powered by Discourse, best viewed with JavaScript enabled"
891,building-a-foundation-for-zero-trust-security-with-nvidia-doca-1-2,"Originally published at:			New features in NVIDIA DOCA 1.2 | NVIDIA Technical Blog
Dive deep into the new features and use cases available for networking, security, storage in the latest release of the DOCA software framework.Powered by Discourse, best viewed with JavaScript enabled"
892,using-real-time-ray-tracing-in-the-production-of-fortnite-game-trailers,"Originally published at:			Using Real-Time Ray Tracing in the Production of Fortnite Game Trailers | NVIDIA Technical Blog
Fans of Fortnite look forward to the release of game trailers, which set the tone for upcoming in-game events. We talked with Andrew Harris (Studio CG Supervisor at Epic Games), Juan Cañada (Lead Programmer, Epic Games), and Juan Collado (FX Artist, Epic Games) to learn about how they are using real-time ray tracing in the…Powered by Discourse, best viewed with JavaScript enabled"
893,an-openacc-example-part-1,"Originally published at:			https://developer.nvidia.com/blog/openacc-example-part-1/
You may want to read the more recent post Getting Started with OpenACC by Jeff Larkin. In this post I’ll continue where I left off in my introductory post about OpenACC and provide a somewhat more realistic example. This simple C/Fortran code example demonstrates a 2x speedup with the addition of just a few lines of OpenACC directives, and in the…Powered by Discourse, best viewed with JavaScript enabled"
894,ai-classifies-galaxies-using-hubble-space-telescope-images,"Originally published at:			https://developer.nvidia.com/blog/ai-classifies-galaxies-using-hubble-space-telescope-images/
A new study published in the Astrophysical Journal this week describes how a team of researchers from all over the globe developed a deep learning system that can classify galaxies with superb accuracy.  Using NVIDIA TITAN Xp GPUs and the cuDNN-accelerated Keras and Theano deep learning frameworks, the team, made up of researchers from institutions…Yes, it is possible to classify galaxies using Hubble Space Telescope images with the help of artificial intelligence (AI). AI algorithms can be trained to recognize and classify different types of galaxies based on their visual characteristics, such as their shape, color, and texture. This can be done using machine learning techniques such as neural networks, which are designed to recognize patterns in large amounts of data.In fact, the Hubble Space Telescope has been used in several studies to classify galaxies using AI. For example, in 2019, a team of astronomers used deep learning techniques to analyze Hubble images of galaxies and classify them into different categories based on their morphologies. The results of this study demonstrated the potential of AI in helping to automate the analysis of large datasets and improve our understanding of the universe.Powered by Discourse, best viewed with JavaScript enabled"
895,achieve-up-to-75-performance-improvement-for-communication-intensive-hpc-applications-with-nvtags,"Originally published at:			https://developer.nvidia.com/blog/achieve-up-to-75-performance-improvement-for-communication-intensive-hpc-applications-with-nvtags/
NVTAGS automates intelligent GPU assignment by profiling HPC applications and launching them with a custom GPU assignment tailored to an application and system to minimize communication costs.Powered by Discourse, best viewed with JavaScript enabled"
896,isc20-featured-demo-boosting-performance-and-utilization-with-multi-instance-gpu,"Originally published at:			ISC20 Featured Demo: Boosting Performance and Utilization with Multi-Instance GPU | NVIDIA Technical Blog
The NVIDIA A100 Tensor Core GPU features a new technology – Multi-Instance GPU (MIG), which can guarantee performance for up to seven jobs running concurrently on the same GPU. This unique capability of the A100 GPU offers the right-sized GPU for every job and maximizes data center utilization.  This demo runs a natural language processing…Powered by Discourse, best viewed with JavaScript enabled"
897,recursive-neural-networks-with-pytorch,"Originally published at:			https://developer.nvidia.com/blog/recursive-neural-networks-pytorch/
From Siri to Google Translate, deep neural networks have enabled breakthroughs in machine understanding of natural language. Most of these models treat language as a flat sequence of words or characters, and use a kind of model called a recurrent neural network (RNN) to process this sequence. But many linguists think that language is best…Python:SnakePyTorch:PyroBack propagated RL of Pyros - Dante's Inferno was a lovefest.How about a Turing test for the NLP age:  Process top 100 classic novels and summarize top 10 common themes.  2001 is not a classic.  Summary of Deep Learning from the 3 godfathers of NNs, prior to recent advances by Socher, Bowman, et. al.https://www.cs.toronto.edu/... Come to the GPU Technology Conference, May 8-11 in San Jose, California, to learn more about deep learning and PyTorch. GTC is the largest and most important event of the year for AI and GPU developers. Use code CMDLIPF to receive 20% off registration!Degree in linguistics - that's  why you can actually write, not just engineer or machine garbled ""natural"" language, eh? Well done we need more native speakers using natural style.Thanks for the post. Which installation of torch are you using? I'm getting the error:TypeError: splits() got an unexpected keyword argument 'wv_type'In the data loading partThanks for your code. I have a question.answers.build_vocab(train) makes answers has ""neutral"", ""contradiction"", ""entailment"", and ""<unk>"". Also config.d_out = len(answers.vocab) makes config.d_out equals to 4.It means you classify data into 4 classes. Is it a mistake or does it still work as you thought?Powered by Discourse, best viewed with JavaScript enabled"
898,mit-lincoln-laboratory-supercomputing-center-installs-world-s-fastest-supercomputer-at-a-university-powered-by-nvidia-v100-gpus,"Originally published at:			MIT Lincoln Laboratory Supercomputing Center Installs World’s Fastest Supercomputer at a University, powered by NVIDIA V100 GPUs | NVIDIA Technical Blog
To power AI applications and research across engineering, science, and medicine, the Massachusetts Institute of Technology (MIT) Lincoln Laboratory Supercomputing Center has just installed a new GPU-accelerated supercomputer, powered by 896 NVIDIA Tensor Core V100 GPUs.  According to MIT, the new system named TX-GAIA for Green AI Accelerator was ranked by TOP500 as the most…Powered by Discourse, best viewed with JavaScript enabled"
899,nvidia-titan-v-transforms-the-pc-into-ai-supercomputer,"Originally published at:			https://developer.nvidia.com/blog/nvidia-titan-v-transforms-the-pc-into-ai-supercomputer/
NVIDIA introduced TITAN V, the world’s most powerful GPU for the PC, driven by the world’s most advanced GPU architecture, NVIDIA Volta. Announced by NVIDIA founder and CEO Jensen Huang at the annual NIPS conference, TITAN V excels at computational processing for scientific simulation. Its 21.1 billion transistors deliver 110 teraflops of raw horsepower, 9x…Powered by Discourse, best viewed with JavaScript enabled"
900,accelerating-gpu-applications-with-nvidia-math-libraries,"Originally published at:			https://developer.nvidia.com/blog/accelerating-gpu-applications-with-nvidia-math-libraries/
NVIDIA Math Libraries are available to boost your application’s performance, from GPU-accelerated implementations of BLAS to random number generation.Powered by Discourse, best viewed with JavaScript enabled"
901,advanced-api-performance-vulkan-clearing-and-presenting,"Originally published at:			https://developer.nvidia.com/blog/advanced-api-performance-vulkan-clearing-and-presenting/
This post covers best practices for Vulkan clearing and presentation on NVIDIA GPUs. To get a high and consistent frame rate in your applications, see all Advanced API Performance tips.Thanks for this article! One question, when following this advice:Should we be using VK_FULL_SCREEN_EXCLUSIVE_APPLICATION_CONTROLLED_EXT, or
VK_FULL_SCREEN_EXCLUSIVE_ALLOWED_EXT?Powered by Discourse, best viewed with JavaScript enabled"
902,data-center-networking-top-resources-from-gtc-21,"Originally published at:			https://developer.nvidia.com/blog/data-center-networking-top-resources-from-gtc-21/
NVIDIA is enabling these organizations to easily develop accelerated applications and implement cybersecurity frameworks in order to deliver breakthrough networking, security, and storage performance with a comprehensive, open development platform.Powered by Discourse, best viewed with JavaScript enabled"
903,expressive-algorithmic-programming-with-thrust,"Originally published at:			https://developer.nvidia.com/blog/expressive-algorithmic-programming-thrust/
Thrust is a parallel algorithms library which resembles the C++ Standard Template Library (STL). Thrust’s High-Level interface greatly enhances programmer Productivity while enabling performance portability between GPUs and multicore CPUs. Interoperability with established technologies (such as CUDA, TBB, and OpenMP) facilitates integration with existing software. Develop High-Performance applications rapidly with Thrust! This excerpt from the Thrust home page perfectly summarizes the benefits of the Thrust…#includes are broken... maybe use &lt; and &gt; instead of < and > ? Otherwise, awesome and informative article, thanks!Fixed, thanks. Wordpress sometimes converts < to < and then eats it.Powered by Discourse, best viewed with JavaScript enabled"
904,upcoming-event-improve-your-cybersecurity-posture-with-ai,"Originally published at:			https://info.nvidia.com/improve-your-cybersecurity-posture-with-ai.html?nvid=nv-int-tblg-746753-vt22#cid=dl24_nv-int-tblg_en-us
Find out how federal agencies are adopting AI to improve cybersecurity in this November 16 webinar featuring Booz Allen Hamilton.Powered by Discourse, best viewed with JavaScript enabled"
905,nvidia-optix-ray-tracing-powered-by-rtx,"Originally published at:			NVIDIA OptiX Ray Tracing Powered by RTX | NVIDIA Technical Blog
Ray Tracing vs Rasterization Conventional 3D rendering has typically used a process called rasterization since the 1990s. Rasterization uses objects created from a mesh of triangles or polygons to represent a 3D model of an object. The rendering pipeline then converts each triangle of the 3D models into pixels on a 2D image plane. These…Does it have hardware acceleration for rays?Performance graph shows Optix 5.1 while latest available is 5.0.1 is that a typographical error or is tested with not public Optix 5.1? Thanks..Of course the version that is not distributed. Nvidia MUST need to test their products.A possible typo in the Ray Generation example.  This line: Prd.result = make_float4(0.0f); Should be: prd.result = make_float4(0.0f);Hi, Jason:Thanks for catching that. We've fixed it in the post.Powered by Discourse, best viewed with JavaScript enabled"
906,find-furniture-you-love-with-houzz-s-new-visual-recognition-tool,"Originally published at:			https://developer.nvidia.com/blog/find-furniture-you-love-with-houzzs-new-visual-recognition-tool/
With 40 million monthly unique users, Houzz is leveraging deep learning technology to make it easier for people to discover and buy products and materials that inspire them. “People come to Houzz because they want to get everything they need to improve their homes in one place, from inspiration to execution,” said Alon Cohen, Houzz…Perhaps something depends on the grade of the aluminum as well. If it’s cheap, it might not last very longPowered by Discourse, best viewed with JavaScript enabled"
907,rapid-registration-of-aerial-and-orbital-imagery,"Originally published at:			https://developer.nvidia.com/blog/rapid-registraion-aerial-orbital-imagery/
Devin White, Senior Research Scientist in the Geographic Information Science and Technology Group at Oak Ridge National Laboratory, shares how his team uses computer vision, photogrammetry and high performance techniques accelerated with GPUs to automatically validate the geopositioning accuracy of satellite imagery. White talked with us about the research at the 2016 GPU Technology Conference. Brad…Powered by Discourse, best viewed with JavaScript enabled"
908,amazon-improves-speech-emotion-detection-with-adversarial-training-using-nvidia-gpus,"Originally published at:			Amazon Improves Speech Emotion Detection with Adversarial Training Using NVIDIA GPUs | NVIDIA Technical Blog
Developers from Amazon’s Alexa Research group have just published a developer blog and published a paper describing how they are using adversarial training to recognize and improve emotion detection. “A person’s tone of voice can tell you a lot about how they’re feeling. Not surprisingly, emotion recognition is an increasingly popular conversational-AI research topic,” said Viktor Rozgic, a senior applied scientist…Powered by Discourse, best viewed with JavaScript enabled"
909,nih-nvidia-use-ai-to-trace-covid-19-disease-progression-in-chest-ct-images,"Originally published at:			https://developer.nvidia.com/blog/nih-nvidia-ai-covid-19-disease-progression-chest-ct/
Researchers from the U.S. National Institutes of Health have collaborated with NVIDIA experts on an AI-accelerated method to monitor COVID-19 disease severity over time from patient chest CT scans.  Published today in Scientific Reports, this work studied the progression of lung opacities in chest CT images of COVID patients, and extracted insights about the temporal…Powered by Discourse, best viewed with JavaScript enabled"
910,nvidia-nsight-systems-2020-5-now-available,"Originally published at:			NVIDIA Nsight Systems 2020.5 Now Available | NVIDIA Technical Blog
Nsight Systems is a system-wide performance analysis tool, designed to help developers tune and scale software across CPUs and GPUs. This release includes support for the Vulkan Ray Tracing Final Specification. A rich set of CUDA features and improvements have also been added including support for NVIDIA Collectives Communication Library (NCCL) tracing and CUDA Memory…Powered by Discourse, best viewed with JavaScript enabled"
911,gtc-2020-accelerating-python-with-cuda,"GTC 2020 CWE21742
Presenters: Keith Kraus,NVIDIA; Dante (ML), ; Siu / Graham (Numba), ; Chris (C/C++, Python), ; Ashwin (Pyton, C/C++), ; Brandon (Higher level distributed systems), ; Peter (lower level distributed systems),
Abstract
The Python ecosystem is composed of a rich set of powerful libraries that work wonderfully well together, providing coherent, beautiful, Pythonic APIs that let developers think less about programming and more about solving problems. On the other hand, Python is known to have performance limitations, and the scale of today’s problems have pushed users to look for more efficient solutions. This session focuses on Python-CUDA capabilities, including libraries available in the ecosystem as options and strategies for building your own Python interface for custom solutions. We have a team of experts who architect, develop, and maintain open-source Python-CUDA libraries who are ready to discuss how to take advantage of CUDA and GPUs from Python.Watch this session
Join in the conversation below.Powered by Discourse, best viewed with JavaScript enabled"
912,updated-course-integrating-sensors-with-nvidia-drive,"Originally published at:			Courses – NVIDIA
Learn how to integrate your sensor of choice for NVIDIA DRIVE. This updated self-paced course now uses DriveWorks 5.8 and includes lidar examples.Powered by Discourse, best viewed with JavaScript enabled"
913,smart-home-hub-brings-artificial-intelligence-into-your-home,"Originally published at:			https://developer.nvidia.com/blog/smart-home-hub-brings-artificial-intelligence-into-your-home/
A new AI-powered device will be able to replace all of your various smart home control apps, as well as being able to recognize specific people and respond to a range of emotions and gestures. AI Build is a London-based startup focused on making your smart home more natural and intuitive. Powered by an NVIDIA…Powered by Discourse, best viewed with JavaScript enabled"
914,neural-machine-translation-now-available-with-tensorrt,"Originally published at:			Neural Machine Translation Now Available with TensorRT | NVIDIA Technical Blog
NVIDIA released TensorRT 4 with new features to accelerate inference of neural machine translation (NMT) applications on GPUs. Neural machine translation offers AI-based text translation for large number of consumer applications, including web sites, road signs, generating subtitles in foreign languages, and more. The new TensorRT 4 release brings support for new RNN layers such…Powered by Discourse, best viewed with JavaScript enabled"
915,cuda-on-turing-opens-new-gpu-compute-possibilities,"Originally published at:			CUDA on Turing Opens New GPU Compute Possibilities | NVIDIA Technical Blog
The Turing architecture introduces so many cool new features that it’s easy to miss the quiet revolution in GPU programming that it also represents: all of the features introduced with Volta now exist in a GeForce product. This means more programmers than ever have access to a GPU with Independent Thread Scheduling, the PTX6 memory consistency model, and improved…The C++14 is a bit painful for those of us stuck on EAL-certified platforms...Powered by Discourse, best viewed with JavaScript enabled"
916,graph-coloring-more-parallelism-for-incomplete-lu-factorization,"Originally published at:			https://developer.nvidia.com/blog/graph-coloring-more-parallelism-for-incomplete-lu-factorization/
In this blog post I will briefly discuss the importance and simplicity of graph coloring and its application to one of the most common problems in sparse linear algebra – the incomplete-LU factorization. My goal is to convince you that graph coloring is a problem that is well-suited for GPUs and that it should be…Powered by Discourse, best viewed with JavaScript enabled"
917,turning-frowns-into-smiles-with-artificial-intelligence,"Originally published at:			Turning Frowns Into Smiles With Artificial Intelligence | NVIDIA Technical Blog
Researchers from Korea University, Clova AI Research (NAVER), The College of New Jersey, and Hong Kong University of Science & Technology developed a Generative Adversarial Networks (GAN)-based approach that transforms the facial expressions of still images. Using an NVIDIA Tesla GPU and the cuDNN-accelerated PyTorch deep learning framework, the team trained their models on the…Powered by Discourse, best viewed with JavaScript enabled"
918,fast-terabyte-scale-recommender-training-made-easy-with-nvidia-merlin-distributed-embeddings,"Originally published at:			Fast, Terabyte-Scale Recommender Training Made Easy with NVIDIA Merlin Distributed-Embeddings | NVIDIA Technical Blog
NVIDIA Merlin Distributed Embeddings enables developers to rapidly train Terabyte-scale embedding based models in TensorFlow 2 with just a few lines of code!Powered by Discourse, best viewed with JavaScript enabled"
919,gpu-based-design-to-achieve-100-s-scheduling-for-5g-nr,"Originally published at:			GPU-Based Design to Achieve 100µs Scheduling for 5G NR | NVIDIA Technical Blog
The next-generation 5G New Radio (NR) cellular technology design supports extremely diverse use cases, such as broadband human-oriented communications and time-sensitive applications with ultra-low latency. 5G NR operates on a very broad frequency spectrum (from sub-GHz to 100 GHz). NR employs multiple different OFDM numerologies in physical air interface to enable such diversity, as shown…Powered by Discourse, best viewed with JavaScript enabled"
920,visualizing-an-entire-brain-at-nanoscale-resolution,"Originally published at:			Visualizing An Entire Brain At Nanoscale Resolution | NVIDIA Technical Blog
At Supercomputing 2019 in Denver, researchers from MIT, Harvard, University of California, Berkeley, and other leading organizations showed the results of their research, published on Science, Cortical column and whole-brain imaging with molecular contrast and nanoscale resolution, which visualizes an entire brain at nanoscale resolution.  At the core of the work is the combination of…Powered by Discourse, best viewed with JavaScript enabled"
921,building-cross-platform-cuda-applications-with-cmake,"Thank you for the link will check it out. BTW I think I narrow down the problem. When setting PROJECT(myProject LANGUAGES CUDA CXX) will everything be compiled with nvcc? What I mean is that my main file is a main.cpp which I don't know if it goes through nvcc or directly through cl, could it be the problem? How to set main.cpp to be treated by nvcc so that device linker resolves whatever it needs to resolve?Only .cu files will be handled by nvcc. Any .cpp/cxx/c++ files will be handled by cl.exe . If you have .cpp files that should be compiled with nvcc you will want to set the LANGUAGE property on that file to CUDA ( https://gitlab.kitware.com/... )And so that implies that if my .exe links to a .lib which has been compiled with separable_compilation=on I have to tell CMake to compile my main.cpp with NVCC and activate also separable_compilation and all should be good to go, right?Yes.In CMake version 3.11.1, CMAKE_CUDA_HOST_IMPLICIT_LINK_LIBRARIES does not use absolute paths (I think).You are correct they are relative paths. I don't know why I said absolute.Hi!Thanks so much for this. I am running into problems pretty early in the process. I upgraded to CUDA 10.0 and downloaded VS2017 with cmake 3.12.2.Trying to run the github code i get different errors when opening via visual studio or through the cmake.when opening the cmake via VS i get:1> Command line: C:\PROGRAM FILES (X86)\MICROSOFT VISUAL STUDIO\2017\COMMUNITY\COMMON7\IDE\COMMONEXTENSIONS\MICROSOFT\CMAKE\CMake\bin\cmake.exe  -G ""Ninja"" -DCMAKE_INSTALL_PREFIX:PATH=""C:\Users\bensr\CMakeBuilds\205fdb7c-dc35-3038-9b74-9f4be7ba3cf4\install\x86-Debug""  -DCMAKE_CXX_COMPILER=""C:/Program Files (x86)/Microsoft Visual Studio/2017/Community/VC/Tools/MSVC/14.15.26726/bin/HostX86/x86/cl.exe""  -DCMAKE_C_COMPILER=""C:/Program Files (x86)/Microsoft Visual Studio/2017/Community/VC/Tools/MSVC/14.15.26726/bin/HostX86/x86/cl.exe""  -DCMAKE_BUILD_TYPE=""Debug"" -DCMAKE_MAKE_PROGRAM=""C:\PROGRAM FILES (X86)\MICROSOFT VISUAL STUDIO\2017\COMMUNITY\COMMON7\IDE\COMMONEXTENSIONS\MICROSOFT\CMAKE\Ninja\ninja.exe"" ""C:\Users\bensr\Documents\GitHub\code-samples\posts\cmake""1> Working directory: C:\Users\bensr\CMakeBuilds\205fdb7c-dc35-3038-9b74-9f4be7ba3cf4\build\x86-Debug1> -- The CUDA compiler identification is unknown1> -- Check for working CUDA compiler: C:/Program Files/NVIDIA GPU Computing Toolkit/CUDA/v10.0/bin/nvcc.exe1> -- Check for working CUDA compiler: C:/Program Files/NVIDIA GPU Computing Toolkit/CUDA/v10.0/bin/nvcc.exe -- broken1> CMake Error at C:/Program Files (x86)/Microsoft Visual Studio/2017/Community/Common7/IDE/CommonExtensions/Microsoft/CMake/CMake/share/cmake-3.11/Modules/CMakeTestCUDACompiler.cmake:46 (message):1>   The CUDA compiler1> 1>     ""C:/Program Files/NVIDIA GPU Computing Toolkit/CUDA/v10.0/bin/nvcc.exe""1> 1>   is not able to compile a simple test program.1> 1>   It fails with the following output:1> 1>     Change Dir: C:/Users/bensr/CMakeBuilds/205fdb7c-dc35-3038-9b74-9f4be7ba3cf4/build/x86-Debug/CMakeFiles/CMakeTmp1>     1>     Run Build Command:""C:/Program Files (x86)/Microsoft Visual Studio/2017/Community/Common7/IDE/CommonExtensions/Microsoft/CMake/Ninja/ninja.exe"" ""cmTC_25c6c""1>     [1/3] Building CUDA object CMakeFiles\cmTC_25c6c.dir\main.cu.obj1>     FAILED: CMakeFiles/cmTC_25c6c.dir/main.cu.obj 1>     cmd.exe /C ""C:\PROGRA~1\NVIDIA~2\CUDA\v10.0\bin\nvcc.exe     -x cu -c main.cu -o CMakeFiles\cmTC_25c6c.dir\main.cu.obj && C:\PROGRA~1\NVIDIA~2\CUDA\v10.0\bin\nvcc.exe     -x cu -M main.cu -MT CMakeFiles\cmTC_25c6c.dir\main.cu.obj -o CMakeFiles\cmTC_25c6c.dir\main.cu.obj.d""1>     C:\Program Files (x86)\Microsoft Visual Studio\2017\Community\VC\Tools\MSVC\14.15.26726\include\vcruntime.h(184): error: invalid redeclaration of type name ""size_t""1>     1>     C:\Program Files (x86)\Microsoft Visual Studio\2017\Community\VC\Tools\MSVC\14.15.26726\include\vcruntime_new.h(66): error: first parameter of allocation function must be of type ""size_t""and many more of this kinde###############################################end of visual studio error##############################################33When running through the cmake application it saysThe CUDA compiler identification is unknownCheck for working CUDA compiler: C:/Program Files/NVIDIA GPU Computing Toolkit/CUDA/v10.0/bin/nvcc.exeCheck for working CUDA compiler: C:/Program Files/NVIDIA GPU Computing Toolkit/CUDA/v10.0/bin/nvcc.exe -- brokenCMake Error at C:/Program Files/CMake/share/cmake-3.12/Modules/CMakeTestCUDACompiler.cmake:46 (message):  The CUDA compiler    ""C:/Program Files/NVIDIA GPU Computing Toolkit/CUDA/v10.0/bin/nvcc.exe""  is not able to compile a simple test program.  It fails with the following output:    Change Dir: C:/Users/bensr/Documents/GitHub/code-samples/posts/cmake_build/CMakeFiles/CMakeTmp    Run Build Command:""C:/Program Files (x86)/Microsoft Visual Studio/2017/Community/MSBuild/15.0/Bin/MSBuild.exe"" ""cmTC_71df4.vcxproj"" ""/p:Configuration=Debug"" ""/p:VisualStudioVersion=15.0""    Microsoft (R) Build Engine version 15.8.169+g1ccb72aefa for .NET Framework    Copyright (C) Microsoft Corporation. All rights reserved.    Build started 10/2/2018 11:01:22 PM.    Project ""C:\Users\bensr\Documents\GitHub\code-samples\posts\cmake_build\CMakeFiles\CMakeTmp\cmTC_71df4.vcxproj"" on node 1 (default targets).    PrepareForBuild:      Creating directory ""cmTC_71df4.dir\Debug\"".      Creating directory ""C:\Users\bensr\Documents\GitHub\code-samples\posts\cmake_build\CMakeFiles\CMakeTmp\Debug\"".      Creating directory ""cmTC_71df4.dir\Debug\cmTC_71df4.tlog\"".    InitializeBuildStatus:      Creating ""cmTC_71df4.dir\Debug\cmTC_71df4.tlog\unsuccessfulbuild"" because ""AlwaysCreate"" was specified.    AddCudaCompileDeps:      C:\Program Files (x86)\Microsoft Visual Studio\2017\Community\VC\Tools\MSVC\14.15.26726\bin\HostX86\x86\cl.exe /E /nologo /showIncludes /TP /D__CUDACC__ /DCMAKE_INTDIR=""Debug"" /DCMAKE_INTDIR=""Debug"" /D_MBCS /I""C:\Program Files\NVIDIA GPU Computing Toolkit\CUDA\v10.0\bin"" /I""C:\Program Files\NVIDIA GPU Computing Toolkit\CUDA\v10.0\include"" /I. /FIcuda_runtime.h /c C:\Users\bensr\Documents\GitHub\code-samples\posts\cmake_build\CMakeFiles\CMakeTmp\main.cu    Project ""C:\Users\bensr\Documents\GitHub\code-samples\posts\cmake_build\CMakeFiles\CMakeTmp\cmTC_71df4.vcxproj"" (1) is building ""C:\Users\bensr\Documents\GitHub\code-samples\posts\cmake_build\CMakeFiles\CMakeTmp\cmTC_71df4.vcxproj"" (1:2) on node 1 (CudaBuildCore target(s)).    CudaBuildCore:      Compiling CUDA source file main.cu...      cmd.exe /C ""C:\Users\bensr\AppData\Local\Temp\tmp6e2fda3fa9c44f3584da74512023f7e9.cmd""      ""C:\Program Files\NVIDIA GPU Computing Toolkit\CUDA\v10.0\bin\nvcc.exe"" -gencode=arch=compute_35,code=\""sm_35,compute_35\"" --use-local-env -ccbin ""C:\Program Files (x86)\Microsoft Visual Studio\2017\Community\VC\Tools\MSVC\14.15.26726\bin\HostX86\x86"" -x cu   -I""C:\Program Files\NVIDIA GPU Computing Toolkit\CUDA\v10.0\include""     --keep-dir Debug -maxrregcount=0  --machine 32 --compile -cudart static  -g   -D""CMAKE_INTDIR=\""Debug\"""" -D""CMAKE_INTDIR=\""Debug\"""" -D_MBCS -Xcompiler ""/EHsc /W1 /nologo /O2 /FdcmTC_71df4.dir\Debug\vc141.pdb /FS /Zi  /MD "" -o cmTC_71df4.dir\Debug\main.obj ""C:\Users\bensr\Documents\GitHub\code-samples\posts\cmake_build\CMakeFiles\CMakeTmp\main.cu""      C:\Users\bensr\Documents\GitHub\code-samples\posts\cmake_build\CMakeFiles\CMakeTmp>""C:\Program Files\NVIDIA GPU Computing Toolkit\CUDA\v10.0\bin\nvcc.exe"" -gencode=arch=compute_35,code=\""sm_35,compute_35\"" --use-local-env -ccbin ""C:\Program Files (x86)\Microsoft Visual Studio\2017\Community\VC\Tools\MSVC\14.15.26726\bin\HostX86\x86"" -x cu   -I""C:\Program Files\NVIDIA GPU Computing Toolkit\CUDA\v10.0\include""     --keep-dir Debug -maxrregcount=0  --machine 32 --compile -cudart static  -g   -D""CMAKE_INTDIR=\""Debug\"""" -D""CMAKE_INTDIR=\""Debug\"""" -D_MBCS -Xcompiler ""/EHsc /W1 /nologo /O2 /FdcmTC_71df4.dir\Debug\vc141.pdb /FS /Zi  /MD "" -o cmTC_71df4.dir\Debug\main.obj ""C:\Users\bensr\Documents\GitHub\code-samples\posts\cmake_build\CMakeFiles\CMakeTmp\main.cu""      nvcc fatal   : 32 bit compilation is only supported for Microsoft Visual Studio 2013 and earlierPlease help!!!The problem is that you are asking CMake to build a 32bit application which isn't supported by CUDA anymore.To fix this issue you should run Ninja from a shell with the 64bit environment setup ( something like ""C:\<path>\vcvarsall.bat"" amd64).I pulled down your github example here and tried it on my Macbook Pro (my trusty old one with nvidia graphics) and it builds just fine, but upon running I get: 0 CUDA driver version is insufficient for CUDA runtime versionI have older CUDA projects (that used the old cuda_add_executable from old CMake) that build and run just fine, so my setup is good (also deviceQuery passes). I'm on CUDA 10.0, OSX 10.13. I've been trying to make a new project with your modern CMake usage, but I'm hitting this same version error. Any idea why?I expect the problem is related to setting up RPATH. This is a fairly common issue on OSX with CUDA.  Add the following logic to the CMakeLists.txt and it should fix the problemif(APPLE)  # Help the static cuda runtime find the driver (libcuda.dyllib) at runtime.  set_property(TARGET <target_name> PROPERTY BUILD_RPATH ${CMAKE_CUDA_IMPLICIT_LINK_DIRECTORIES})endif()The trouble is that I pulled down your example verbatim (which already includes those lines) and I get this result. So I think there must be a little more to it.Okay, after downgrading to CUDA 9.2 and CMake 3.12 I got your sample to build and run. My old code also works (though oddly the Nvidia samples now fail to build saying ld: library not found for -lnvrtc)However, my new cmake project is still failing with the same old error when I try to run any part of it that accesses the GPU. I decided to query the  versions and it output driver version = 0 and runtime version = 203845632, while I put the same query in your sample and got driver version = 9020 and runtime version = 9020. Mine is obviously wrong, but I can't imagine why.The primary difference between my test and yours is that your test file is .cu, while mine is .cpp and I'm using the pimpl idiom to wrap up the cuda code so I can link against it from a .cpp. Is that somehow not allowed?You most likely need to tell CMake that the .cpp file should be compiled with the cuda compiler by using:set_source_files_properties(${sources} PROPERTIES LANGUAGE ""CUDA"")A sample small example project would be help track down the issue.The advice to make a small example to track down the issue was a good one (always is...). Your first thought about BUILD_RPATH was indeed the issue; I had missed a library that needed that specified. Thanks!Hi, I found this new methods of using CUDA in cmake does not handle Mac framework as external library very well. For example if I added OpenGL to the project and trying to wrap all OpenGL dependencies in the particle lib with the target_link_libraries(particles ${OPENGL_LIBRARIES}) command. It will report nvcc fatal   : Don't know what to do with 'OpenGL'The ${OPENGL_LIBRARIES} under Mac would be ""/System/Library/Frameworks/OpenGL.framework"". But somehow while doing the device linking, ""OpenGL"" was passed to nvcc instead of ""-lOpenGL"", hence the unknown option.Here is the minimal patch file applied to the particle example to replicate the problem. diff --git a/posts/cmake/CMakeLists.txt b/posts/cmake/CMakeLists.txtindex 0e39e4c..a485ed4 100644--- a/posts/cmake/CMakeLists.txt+++ b/posts/cmake/CMakeLists.txt@@ -1,7 +1,7 @@cmake_minimum_required(VERSION 3.8 FATAL_ERROR)project(cmake_and_cuda LANGUAGES CXX CUDA)-+find_package(OpenGL REQUIRED)include(CTest)add_library(particles STATIC@@ -24,6 +24,7 @@ target_compile_features(particles PUBLIC cxx_std_11)set_target_properties( particles                        PROPERTIES CUDA_SEPARABLE_COMPILATION ON                       )+target_link_libraries(particles ${OPENGL_LIBRARIES})if(BUILD_TESTING)This looks to be an issue with how CMake constructs the link line when doing device linking.You can track the issue at: https://gitlab.kitware.com/...The quick workaround is to use the `OpenGL::GL` target instead of `${OPENGL_LIBRARIES}`.I knew I should have tried latest CMake first :) . I fixed this issue starting in 3.12.4 with the new rules on what libraries we pass to the device linker.My recommendation is to bump your minimum required to CMake 3.12.4 ( and use 3.13+ ) and this will work with no modifications to your code.I have tried using CMake 3.13.4 with ${OPENGL_LIBRARIES}, but it is still not working. I think the reason is .framework suffix is not properly handled. I have moved the discuss to the issue tracker https://gitlab.kitware.com/cmake/cmake/issues/18911I’m trying to build this with CMake 3.19.8, CUDA 11.4.2, and Visual Studio 16.9.6. CMake generates and builds the project, but Visual Studio fails to build. I’m getting MSB3721 errors for the CUDA 11.4.targets file on lines 785 and 874. It says that nvcc.exe exited with code 1.Any tips?I’m getting MSB3721 errors for the CUDA 11.4.targets file on lines 785 and 874. It says that nvcc.exe exited with code 1 .To understand why nvcc is returning an error code of 1, it’s necessary to increase the verbosity of VS output so that it shows the actual invocation of nvcc and the actual error. If you increase the VS verbosity what is the exact error message?Powered by Discourse, best viewed with JavaScript enabled"
922,quadro-driver-release-440-now-available,"Originally published at:			Quadro Driver Release 440 Now Available | NVIDIA Technical Blog
Learn about the new features in the latest Quadro driver release NVIDIA Quadro Driver Release 440 is the latest driver from NVIDIA Quadro’s Optimal Driver for Enterprise (ODE) branches. These drivers are designed and tested to provide long-term stability and availability for professionals, from enterprise users to content creators. What’s New in Release 440  The…Powered by Discourse, best viewed with JavaScript enabled"
923,ai-achieves-better-results-in-finding-moon-craters-than-humans,"Originally published at:			AI Achieves Better Results In Finding Moon Craters Than Humans | NVIDIA Technical Blog
Researchers in North America recently published a paper outlining how deep learning has helped create a new technique to measure the size and location of crater impacts on the moon. The process has traditionally been done by visual inspection of images, thus limiting the scope. However, when using neural networks, the researchers were able to increase…Powered by Discourse, best viewed with JavaScript enabled"
924,university-of-torontos-gpu-accelerated-cancer-research-wins-nvidia-foundation-award,"Originally published at:			https://developer.nvidia.com/blog/university-of-torontos-gpu-accelerated-cancer-research-wins-nvidia-foundation-award/
Cancer kills almost 600,000 people each year in the U.S. alone. Researchers from the University of Toronto are advancing computational cancer research by developing a “genetic interpretation engine” – a GPU-powered, deep learning method for identifying cancer-causing mutations. Under its Compute the Cure initiative, the NVIDIA Foundation awarded the team a $200,000 research grant to…Powered by Discourse, best viewed with JavaScript enabled"
925,topic-modeling-and-image-classification-with-dataiku-and-nvidia-data-science,"Originally published at:			https://developer.nvidia.com/blog/topic-modeling-and-image-classification-with-dataiku-and-nvidia-data-science/
Learn about Dataiku and NVIDIA integrations for image classification and object detection.Powered by Discourse, best viewed with JavaScript enabled"
926,cuda-pro-tip-occupancy-api-simplifies-launch-configuration,"Originally published at:			https://developer.nvidia.com/blog/cuda-pro-tip-occupancy-api-simplifies-launch-configuration/
CUDA programmers often need to decide on a block size to use for a kernel launch. For key kernels, its important to understand the constraints of the kernel and the GPU it is running on to choose a block size that will result in good performance. One common heuristic used to choose a good block…Nice. That looks quite useful!Cooooooool!How does it look when we try 2d or even 3d block?For now you will need to compute your own 2D/3D block dimensions from the 1D thread counts suggested by the API.Hello Mark,This API looks great.  I compiled the example you provided above using CUDA 6.5 install.  Also wanted to comment that I got a warning concerning the method signature for the kernel parameter.$ nvcc example_occupancy.cu /usr/local/cuda-6.5/bin/../targets/x86_64-linux/include/cuda_runtime.h(1394): warning: argument of type ""void (*)(int *, int)"" is incompatible with parameter of type ""const void *""          detected during:            instantiation of ""cudaError_t <unnamed>::cudaOccupancyMaxPotentialBlockSizeVariableSMem(int *, int *, T, UnaryFunction, int) [with UnaryFunction=<unnamed>::__cudaOccupancyB2DHelper, T=void (*)(int *, int)]"" (1278): here            instantiation of ""cudaError_t <unnamed>::cudaOccupancyMaxPotentialBlockSize(int *, int *, T, size_t, int) [with T=void (*)(int *, int)]"" example_occupancy.cu(19):Nevertheless the code is running fine. I just wanted to tell in case someone else experienced this.  I should also tell my compiler is gcc$ gcc --versiongcc (Ubuntu/Linaro 4.6.3-1ubuntu5) 4.6.3Cheers,Launched blocks of size 768. Theoretical occupancy: 0.000000GPU - Tesla C2075Why I have 0 occupancy when I use cudaSetDevice and GPU provided above ?What are you using to measure Theoretical occupancy?  What are the resources used by your kernel (registers per thread, shared memory per block)?Hi, very helpful, thanks! However I have a kernel where the amount of shared memory depends on the block dimensions, what can I do in this case?There's a C++ version of the API which takes a unary function callback as an argument. You define this function to take a block size and return a dynamic shared memory size in bytes, and the API uses this in its calculations. See http://docs.nvidia.com/cuda...Is it possible for these values to change at runtime?float occupancy = (maxActiveBlocks * blockSize / props.warpSize) /                     (float)(props.maxThreadsPerMultiProcessor /                             props.warpSize);why do we divide twice by props.warpSize ??? it's a redundant operation that can be mathematically simplifiedoccupancy = maxActiveBlocks * blockSize / props.maxThreadsPerMultiProcessor;Your calculation is semantically different because it ignores integer division. Remember that blockSize might not be a multiple of warpSize (although that's generally not a good idea, it's legal).Powered by Discourse, best viewed with JavaScript enabled"
927,towards-environment-specific-base-stations-ai-ml-driven-neural-5g-nr-multi-user-mimo-receiver,"Originally published at:			https://developer.nvidia.com/blog/towards-environment-specific-base-stations-ai-ml-driven-neural-5g-nr-multi-user-mimo-receiver/
At this year’s Mobile World Congress (MWC), NVIDIA showcased a neural receiver​ for a 5G New Radio (NR) uplink multi-user MIMO scenario, which could be seen as​ a blueprint for possible 6G physical-layer architectures. For the first time, NVIDIA demonstrated a research prototype of a trainable neural network-based receiver that learns to replace significant parts…Powered by Discourse, best viewed with JavaScript enabled"
928,gtc-2020-hedgehog-a-performance-oriented-general-purpose-library-that-exploits-multi-gpu-systems,"GTC 2020 S21227
Presenters: Tim Blattner,NIST; Alexandre Bardakoff,NIST
Abstract
We’ll present Hedgehog, a general-purpose library for taking advantage of powerful compute nodes, multicore CPUs, and multiple GPUs. The novel aspects of Hedgehog are: (1) its explicit representation of a program as a dataflow graph, (2) its pure dataflow-driven scheduling, (3) its maintenance of a computation’s localized state via state managers, and (4) its fine control of memory via memory managers. This dataflow approach results in extremely low overhead for task executions (< 1 microsecond) and no-cost profiling at the task level. This allows us to prototype operations that compare favorably with leading libraries such as cuBLAS-XT.Watch this session
Join in the conversation below.Powered by Discourse, best viewed with JavaScript enabled"
929,teaching-cameras-to-read-body-language-with-ai,"Originally published at:			Teaching Cameras to Read Body Language with AI | NVIDIA Technical Blog
At GTC 2018 in San Jose, California, AI developers from all over the world come to demo their work. One of those developers at the conference is a Canadian-based startup that developed a real-time deep learning software platform that can read body language from standard video. Wrnch, a Mark Cuban-backed startup, and a member of…Powered by Discourse, best viewed with JavaScript enabled"
930,generating-expressive-3d-facial-animations-from-audio,"Originally published at:			Generating Expressive 3D Facial Animations From Audio | NVIDIA Technical Blog
Researchers from NVIDIA and the independent game developer Remedy Entertainment developed an automated real-time deep learning technique to create 3D facial animations from audio with low latency. Using a TITAN Xp GPU and the cuDNN-accelerated Theano deep learning framework, the researchers trained their neural network on nearly ten minutes of high-quality audio and expression data…Powered by Discourse, best viewed with JavaScript enabled"
931,send-instant-business-payments-with-photon-commerce-s-financial-ai-platform,"Originally published at:			https://developer.nvidia.com/blog/send-instant-business-payments-with-photon-commerces-financial-ai-platform/
Partnering with NVIDIA and the ICC, Photon Commerce is creating the world’s most intelligent financial AI platform for instant B2B payments, invoices, statements, and receipts.Powered by Discourse, best viewed with JavaScript enabled"
932,fourth-installment-of-ray-tracing-gems-now-available-for-free,"Originally published at:			https://developer.nvidia.com/blog/fourth-installment-of-ray-tracing-gems-now-available-for-free/
We’re humbled and delighted by the excitement and interest that our rollout of Ray Tracing Gems has generated! It was a labor of love to write, and we can’t wait to see how it influences the development community. Part IV of Ray Tracing Gems is now available for FREE on  NVIDIA Developer Zone. In this…Powered by Discourse, best viewed with JavaScript enabled"
933,webinar-cybersecurity-and-ai-in-retail,"Originally published at:			https://nvda.ws/44L3bRZ
​Join NVIDIA on Tuesday, June 6, 2023 at 9AM PT, for a webinar on cybersecurity and AI in retail. We discuss how AI can bring a new level of information security to the data center, cloud, and edge.Powered by Discourse, best viewed with JavaScript enabled"
934,harnessing-the-nvidia-ada-architecture-for-frame-rate-up-conversion-in-the-nvidia-optical-flow-sdk,"Originally published at:			https://developer.nvidia.com/blog/harnessing-the-nvidia-ada-architecture-for-frame-rate-up-conversion-in-the-nvidia-optical-flow-sdk/
The NVIDIA Optical Flow SDK 4.0 is now available, enabling you to fully harness the new NVIDIA Optical Flow Accelerator on the NVIDIA Ada architecture with NvOFFRUC. Optical flow on the NVIDIA Ada Lovelace architecture Starting from the NVIDIA Turing architecture, NVIDIA GPUs have dedicated hardware for optical flow computation between a pair of frames.…Powered by Discourse, best viewed with JavaScript enabled"
935,building-the-smart-cloud-using-the-best-smartnics-and-dpus-part-1,"Originally published at:			https://developer.nvidia.com/blog/building-the-smart-cloud-using-the-best-smartnics-and-dpus-part-1/
This post was originally published on the Mellanox blog. Amazon recently announced that Alexa, the smart personal voice assistant is expanding from a small desktop gadget and getting into the fabric of our life. Anything and everything can get an Alexa boost, including your microwave, home security, car infotainment, and even your wall clock. Soon,…Powered by Discourse, best viewed with JavaScript enabled"
936,thinking-parallel-part-iii-tree-construction-on-the-gpu,"Originally published at:			https://developer.nvidia.com/blog/thinking-parallel-part-iii-tree-construction-gpu/
In part II of this series, we looked at hierarchical tree traversal as a means of quickly identifying pairs of potentially colliding 3D objects and we demonstrated how optimizing for low divergence can result in substantial performance gains on massively parallel processors. Having a fast traversal algorithm is not very useful, though, unless we also have a tree…Its not really clear from the paper - http://devblogs.nvidia.com/...  as to how the Octree nodes are allocated and how parent-child relation between the octree nodes are established ?Hi ertf23, I'm not sure if you have figured it out already... I have thought for a (relatively) long time about it and think that I came to a working solution (I am trying to code it in a few minutes.) I'm not sure but if you or anyone else is interested I am willen to share my code. Greetings, Neko (and sorry for my bad english, I still am learning that language)I did figured out both - how to allocate octree nodes and connecting the octree nodes in parent-child relationship. It also took me a long time to find the solution. I think my solution is sub-optimal especially the algorithm for connecting nodes in parent-child relationship. It will be great if you could share the C or pseudo code on github or ideone. I will share mine too. We can then figure out the fastest algorithmis pastebin okay too? (for now, we can work on a project at github if I get all my code snippets together ^^"" )And do you prefer CPU code with ""virtual"" parallelism?I mean like:#pragma omp parallel for schedule(dynamic, 32)for(int idx = 0; idx < max; ++idx)   dosth(idx);or would it be better if I write the CUDA code already?I have just fixed out a bug where multiple equal morton codes generate a broken tree.On my Intel i-7 4800M the sequentiell algorithm takes 4.07 seconds and the parallel (with 8 threads) takes 1.625 seconds to build the tree, both on 50.000.000 Objects (binary search NOT implemented in the parallel one, still looking index by index there ^^"" )I will clean my code and as soon as I get the binary search in there you can see it (but for today I still have a more important work to do, so it may take 1 or 2 more days but I guess it doesn't add much to 10 months)About the image: I build an example main function that tests the CLZ(0) (because I was unsure if it will give me 32 or 31 ^^"" haven't removed it yet, my fault)it creates a list of 50 Million points from [0,0,0] to [1,1,1] and generates the morton code for each objectthen sort it by the morton codeThen I count through the objects and count morton code that are twice, trice, 4 times or 5 times in the list (for output reason, the old version generated false trees as soon as there were more than one group of 4 equal codes...)generate a tree with the old sequential methode and another tree with the parallel methodethen I walk through every node of both trees, count the times and the objects in the tree (for validation)destruct the trees and tadaah//TODO: Clean the code...Pastebin or any other code sharing website will also be fine. Even if your code is broken, I am alright with it. Just need to know the high level algorithm for connecting octree nodes in parent child relationship. You can share c,c++ or cuda c or even pseudo code. Below is the link to the pseudo code I wrote. If you have any trouble viewing it, please let me know. The first page is terminology, second page is Octree node construction algorithm, third & fourth page present two algorithms for linking the octree nodes in parent-child relationshiphttps://docs.google.com/doc...Okay, I'll read it in a sec (after commiting to github... still kinda figuring out the linux way... new to linux ^^"")https://github.com/Ooken/Ne...Here you can find it in the ""Tree"" folderI always wanted to write a path-tracing engine (already did kinda like that... but it was half a year ago and very slow, without bvh's and other nice stuff... only simple diffuse surface, spheres, endless planes and so on...)I hope you like how I write code and I do invite you to work on this engine together (I aim it to be realtime on one side but nice-looking on the other side. Anyway nice and readable code/easy to use code is the main goal) since I think you are a nice person and know how to code(?).Also sorry for packing it into one file (I tried to hold it simple for this example)*just red your psedocode* I'm a little bit confused...Give me a little more time to understand it...Oh, and before I forget it:I found a solution for ""multiple equal morton codes"", also I already tested with 30mio, 50mio and (ram kill) 100mio objects, also with some equal codes inserted by hand at the front, randomly in the middle and at the end and it generated fully valid trees (was kinda fun to write a self-diagnostic code and let it run over night and while in school, ran 18 hours without a memory leak and without an error, I guess that should do it) ^^Hallo NekoVielen Danke schön!! :))Thanks for sharing the code. It looks good. Actually what I am looking for is not how to make Binary radix tree but how to create Octree using the the binary radix tree. If you go to page 4 of the original paper by Tero Karras, on the right column you can see a section titled ""Octrees"".I am copy pasting the below the steps needed to create octree as per this paper:(1) calculate Morton codes for the points, (2) sort the Morton codes, (3) identify duplicates, i.e. points falling within the same leafnode, by comparing each pair of adjacent Morton codes,(4) remove the duplicates using parallel compaction, (5) construct a binary radix tree(6) perform parallel prefix sum to allocate the octree nodes, and (7) find the parent of each octree node.Your code implements only steps 1-5 but not 6 and 7. I also implemented steps 1-5 similar to the way you implemented but I also implemented steps 6,7 (sub-optimal). The pseudo-code I shared with you performs steps 6 and 7(sub-optimal).The author gives some description about steps 6 and 7 but the algorithm for step 7 is not clear, especially the last cryptic line : ""The parents of the octree nodes can then be found by looking at the immediate ancestors of each radix tree node.""So what I am looking for is optimal solution to step 7.hi!hello Nekohow to deal with the case of duplicate Morton codes?hi!hello Neko!how to deal with the case of duplicate Morton codes?Just scan the whole array of objects and remove objects with duplicate morton codes. Another way is to somehow prevent duplicate morton codes."" remove any objects""  I think this way is not feasible.I read the paper http://devblogs.nvidia.com/...The Chaper 4 give a solution , but I did not understand it. Can you help me.I did it like this:mcodeleft = lst[index-1];mcoderigt = lst[index+1];when (mcodeleft == mcoderight)then set index as start point and go through the elements, till there is a diffrent element, also change the ""find split"" function, so it doesn't return the middle, when the start and end elemet are equal but return the first element, so that when there is a group of equal codes, the subtrees will start at the second element and will go on like ""left node = object, right node = subnode index+1"" and so on, till all objects are includedso for example if you have multiple triangles with the same midpoint, they won't be deletet@nurabha:disqus Could we get in touch in a more ""direct"" chat? I want to help you with the octree but somehow I don't get the ""parallel concept"" (I do get the concept of ocrtees but I don't get the parallel concept...)and I ask again, are you willen to participate in coding a nice to use (I hope realtime) Path/RayTracer API in c++&CUDA?did this code deal with  duplicate Morton codes?https://github.com/Ooken/NekoE...if not could you wtire an short exampleI fixed all bugs and yes, both examples deal with duplicate morton codes ^^I achieved it by testing index-1==index+1 and if both are equal I will do a forward search till a diffrent morton code comes, also I changed the ""find split"" function so that it splits in equal-morton-code-cases always at the beginning so that it will go down like stairs and the nodes A is always the object and B is always the node to the other objects/the last object ^^Do you understand my idea? If not I can try to visualize it a little :3Also the BoundingBox project already deals with triangles, bounding boxes and duplicate morton codes (if there are any by randomness or made ba hand)hi，NekoOk, I will read it after a minutes.Can I get an email address from you?  If I have some questions, I can send you email.when the BVH tree is build,  how to travel  the tree with raytracing?After building the tree, I will add bounding boxes (as described in the BoundingBox project in my Git :3 )When tracing I will start at the root node, check it's bounding boxes and when it hit I will test both children, and whenever I hit a bounding box, I will test it's children and so on.If you got more questions you can ask me at nekoyuke@gmail.com, but if you follow my project along I will try to build a fast, easy to understand and to use ray/path-tracer and a few sideprojects (like the TimeLapseCapture program I just wrote ^^"" OpenCV c++ code will follow)In general I try to update it every day :3Ok I will try and follow you project.And  I guess if there is an efficient way to travel the bvh tree, becase we build the tree using Mordon tree?Hi!I'm also trying to build an octree, but I can't figure it out. How does Karras means, that ""δchild/3┘−└δparent/3┘ are divisible by 3""? How do you calculate those counts for the number of octree nodes? How do you decide, which radix tree node becomes an octree node?Thank you!Powered by Discourse, best viewed with JavaScript enabled"
937,automatic-mixed-precision-now-natively-available-in-mxnet,"Originally published at:			Automatic Mixed Precision Now Natively Available in MXNet | NVIDIA Technical Blog
NVIDIA and Apache MXNet partner to simplify mixed precision training in MXNet. Today, Apache MXNet announced native support for NVIDIA’s Automatic Mixed Precision (AMP) training feature on Volta and Turing GPUs.  Developers can now easily access deep learning training speedups available from NVIDIA Tensor Cores using reduced precision. “Designed specifically for deep learning, the first-generation…Powered by Discourse, best viewed with JavaScript enabled"
938,oak-ridge-national-laboratory-installs-two-nvidia-dgx-2-systems,"Originally published at:			Oak Ridge National Laboratory Installs Two NVIDIA DGX-2 Systems | NVIDIA Technical Blog
The U.S. Department of Energy’s Oak Ridge National Laboratory (ORNL), home to the world’s fastest supercomputer, just installed two NVIDIA DGX-2 systems for use in machine learning tasks. “[The] powerful GPU-accelerated appliances will provide ORNL researchers with enhanced opportunities to conduct science—machine learning and data-intensive workloads,” the ORNL team wrote in a blog post. Last…Powered by Discourse, best viewed with JavaScript enabled"
939,restb-ai-offers-custom-computer-vision-as-a-service,"Originally published at:			Restb.ai Offers Custom Computer Vision as a Service | NVIDIA Technical Blog
The Barcelona-based startup developed a deep learning algorithm to determine in real-time what is in a particular image. RestB is commercializing their high-precision models by charging customers an initial training fee to build a custom model for their needs and charging per API call thereafter. “If we take a picture of a city, (other company’s…Powered by Discourse, best viewed with JavaScript enabled"
940,accelerating-ai-development-with-nvidia-tao-toolkit-and-weights-amp-biases,"Originally published at:			https://developer.nvidia.com/blog/accelerating-ai-development-with-nvidia-tao-toolkit-and-weights-biases/
Learn how to integrate NVIDIA TAO Toolkit and the Weights and Biases MLOps platform to accelerate common AI tasks.Powered by Discourse, best viewed with JavaScript enabled"
941,update-your-email-preferences-to-receive-personalized-news-from-nvidia,"Originally published at:			Update your Email Preferences to Receive Personalized News from NVIDIA | NVIDIA Technical Blog
Delivered every other week to your inbox, “Latest Developer News from NVIDIA” is a curated email that compiles the latest GPU-accelerated news, product announcements, and resources published on the NVIDIA Developer News Center and Developer Blog. The emails cover a variety of development topic areas including AI / Deep Learning, Autonomous Machines, Autonomous Vehicles, Data…Powered by Discourse, best viewed with JavaScript enabled"
942,isc-2020-exhibitor-forum,"ISC 2020 disc08
Presenters: DemoTeam, NVIDIA
AbstractWatch this session
Join in the conversation below.Powered by Discourse, best viewed with JavaScript enabled"
943,nvidia-simnet-v0-2-released,"Originally published at:			https://developer.nvidia.com/blog/nvidia-simnet-v0-2-released/
Today, NVIDIA announced the release of SimNet v0.2 with new features including support for A100 GPUs and multi-GPU/multi-node, as well as adding a larger set of neural network architectures and a greater solution space addressability. These new features allow a user to advance simulations for more advanced physics like turbulence and work with complex geometries. Previously…Powered by Discourse, best viewed with JavaScript enabled"
944,nvidia-rtx-gpus-and-keyshot-accelerate-rendering-for-caustics,"Originally published at:			https://developer.nvidia.com/blog/nvidia-rtx-gpus-and-keyshot-accelerate-rendering-for-caustics/
NVIDIA RTX ray tracing has transformed graphics and rendering. With powerful software applications like Luxion KeyShot, more users can take advantage of RTX technology to speed up graphic workflows — like rendering caustics.Powered by Discourse, best viewed with JavaScript enabled"
945,gtc-2020-working-with-tensor-cores-and-rtcore-for-compute,"GTC 2020 CWE21106
Presenters: Vishal-Mehta,NVIDIA; Jakob-Progsch, NVIDIA; Jiqun-Tu, NVIDIA; Jeff-Larkin, NVIDIA; Tim Biedert, NVIDIA
Abstract
Get your queries regarding Tensor Cores and RTCore answered. Learn how to exploit Tensor Core from CUDA & RTCore from Optix for general-purpose compute and understand algorithmic patterns that can be accelerated using NVIDIA’s unique hardware features. Learn how to profile and optimize HPC / ML codes for Tensor Core and RTCore.Watch this session
Join in the conversation below.Hi! This sounds very interesting but I get a 404 error on the video, hopefully it can be fixed? :-)Powered by Discourse, best viewed with JavaScript enabled"
946,jetson-project-of-the-month-drowsiness-blindspot-amp-emotion-monitor,"Originally published at:			https://developer.nvidia.com/blog/jetson-project-of-the-month-drowsiness-blindspot-emotion-monitor/
The trio of Luis Oliver, Victor Izquierdo and Alejandro Gutiérrez won the Jetson Project of the Month for their Drowsiness, Blindspot and Emotion Monitor (DBSE). This project, powered by NVIDIA Jetson Nano, is an in-car assistance system that alerts the driver if they’re drowsy or distracted and notifies them about objects in their blindspot.  The…Powered by Discourse, best viewed with JavaScript enabled"
947,announcing-nvidia-nsight-systems-2021-5,"Originally published at:			https://developer.nvidia.com/blog/announcing-nsight-systems-2021-5-with-support-for-infiniband-nic-metrics-sampling/
Nsight Systems helps you tune and scale software across CPUs and GPUs.Powered by Discourse, best viewed with JavaScript enabled"
948,explore-resources-and-activities-for-jetson-nano-users-with-the-summer-of-jetson-from-nvidia-and-sparkfun,"Originally published at:			Summer of Jetson Nano! Great Resources & Activities for Jetson Nano Users - News - SparkFun Electronics
Experience the “Summer of Jetson” now through Sept. 30, with quizzes, prizes, and a project showcase to learn about the joys of working with Jetson Nano developer kit.Powered by Discourse, best viewed with JavaScript enabled"
949,nvidia-announces-nsight-graphics-2020-6,"Originally published at:			https://developer.nvidia.com/blog/nvidia-announces-nsight-graphics-2020-6/
Nsight Graphics 2020.6 is now available for download. When NVIDIA RTX-series GPUs was unveiled in 2018, ray tracing became a viable alternative to rasterization. To enable this functionality, we provided the VK_NV_ray_tracing extension in Nsight Graphics. Today, Nsight Graphics ships with support for the final version of the Khronos Vulkan Ray Tracing extensions; VK_KHR_acceleration_structure, VK_KHR_deferred_host_operations,…Powered by Discourse, best viewed with JavaScript enabled"
950,real-time-ai-model-aims-to-help-protect-the-great-barrier-reef,"Originally published at:			https://developer.nvidia.com/blog/real-time-ai-model-aims-to-help-protect-the-great-barrier-reef/
Google worked with Australia’s national science agency to train ML models that monitor and map harmful coral-eating crown-of-thorns starfish outbreaks along the Great Barrier Reef.Powered by Discourse, best viewed with JavaScript enabled"
951,evaluating-data-lakes-and-data-warehouses-as-machine-learning-data-repositories,"Originally published at:			Evaluating Data Lakes and Data Warehouses as Machine Learning Data Repositories | NVIDIA Technical Blog
Data lakes can ingest a wide range of data types for big data and AI repositories. Data warehouses use structured data, mainly from business applications, with a focus on data transformation.Powered by Discourse, best viewed with JavaScript enabled"
952,cudacasts-episode-20-getting-started-with-jetson-tk1-and-opencv,"Originally published at:			https://developer.nvidia.com/blog/cudacasts-episode-20-getting-started-jetson-tk1-opencv/
The Jetson TK1 development kit has fast become a must-have for mobile and embedded parallel computing due the amazing level of performance packed into such a low-power board. In this and the following CUDACast, you’ll learn how to get started building computer vision applications on your Jetson TK1 using CUDA and the OpenCV library. CUDACasts…Awesome, thanks!I just installed JetPack 2.3   Isn't OpenCV included in the Jetpack?  If so, how do I make use of it?Where does the folder ""bgfg"" come from and how did you run that sample?  I don't have that folder.  How would I find and try this same sample?Powered by Discourse, best viewed with JavaScript enabled"
953,accelerating-medical-imaging-with-deep-learning,"Originally published at:			https://developer.nvidia.com/blog/accelerating-medical-imaging-with-deep-learning/
This week at the GPU Technology Conference in San Jose, California, a startup demoed their portable ultrasound machine that will help medical professionals save lives. Ultrasee is a Silicon Valley startup and an imaging development partner for NVIDIA’s Project Clara. Their goal is to enable ultrasound imaging for every doctor, nurse, paramedic, and eventually for patient self-monitoring.  The…Powered by Discourse, best viewed with JavaScript enabled"
954,isc20-featured-demo-accelerating-covid-19-research-with-nvidia-gpus,"Originally published at:			ISC20 Featured Demo: Accelerating COVID-19 Research with NVIDIA GPUs | NVIDIA Technical Blog
Scientists and researchers are racing against the clock to find a cure for COVID-19. The search requires the screening of billions of drug candidates and identifying the right chemical structure that will most favorably bind—and interfere—with the virus. However, calculating the binding potential is computationally intensive, taking years to screen a billion compounds on a…Powered by Discourse, best viewed with JavaScript enabled"
955,new-on-ngc-matlab-r2021a-container-for-ampere-gpus-fast-tracks-deep-learning-and-scientific-computing,"Originally published at:			https://developer.nvidia.com/blog/new-on-ngc-matlab-r2021a-container-for-ampere-gpus-fast-tracks-deep-learning-and-scientific-computing/
The latest version provides full support for running deep learning, automotive and scientific analysis on NVIDIA’s Ampere GPUs.Hello! Recently downloaded MATLAB following the instructions here https://mundomatlab.com/ but I’m not sure if it this version, I think is the newest one, still provides support for running deep learning. Can you confirm?? Thank you.Powered by Discourse, best viewed with JavaScript enabled"
956,rapidsfire-podcast-cybersecurity-data-science-with-rachel-allen-and-bartley-richardson,"Originally published at:			RAPIDSFire Podcast: Cybersecurity Data Science with Rachel Allen and Bartley Richardson | NVIDIA Technical Blog
Kick off 2021 with a listen to the newest episode of RAPIDSFire: the Accelerated Data Science Podcast!  Host Paul Mahler sits down with AI Infrastructure Manager at NVIDIA, Bartley Richardson, and Senior Cybersecurity Data Scientist at NVIDIA, Rachel Allen. In this episode, they discuss the intersection of Data Science and Cybersecurity, specifically, how NVIDIA GPUs…Powered by Discourse, best viewed with JavaScript enabled"
957,gtc-2020-cudnn-v8-new-advances-in-deep-learning-acceleration-apis-optimizations-and-how-to-tackle-the-future-challenges-in-hardware-and-software,"GTC 2020 S21685
Presenters: Mostafa Hagog, Kevin Vincent, and Yang Xu,NVIDIA
Abstract
This talk will include a high level description of the new cuDNN v8 API, its software architecture,  and a walk through multiple user-scenarios and how the API can be used for the users benefits.
Watch this session
Join in the conversation below.Where can the cuDNN v8 API be seen or tried out? I cannot find a download for it.Powered by Discourse, best viewed with JavaScript enabled"
958,share-your-science-leveraging-deep-learning-for-personalized-drug-treatment-recommendations,"Originally published at:			Share Your Science: Leveraging Deep Learning for Personalized Drug Treatment Recommendations | NVIDIA Technical Blog
David Ledbetter, data scientist at the Children’s Hospital Los Angeles, shares how his team is using TITAN X GPUs and deep learning to help provide better recommendations of drug treatments for children in their pediatric intensive care unit. To train their models, 13,000 patient snapshots were created from ten years of electronic health records at…Powered by Discourse, best viewed with JavaScript enabled"
959,watch-an-ai-play-mario-kart,"Originally published at:			Watch an AI Play Mario Kart | NVIDIA Technical Blog
A developer spent a couple of days over his winter break training an artificial neural network to play the classic racing game Mario Kart 64 and documented his results to share what he learned in the process. “It had been a few years since I’d done any serious machine learning, and I wanted to try…Powered by Discourse, best viewed with JavaScript enabled"
960,new-nvidia-neural-graphics-sdks-make-metaverse-content-creation-available-to-all,"Originally published at:			NVIDIA Neural Graphics Makes Content Creation Open to All | NVIDIA Blog
A dozen tools and programs—including new releases NeuralVDB and Kaolin Wisp—offer easy and fast 3D content creation for millions of designers and creators.Powered by Discourse, best viewed with JavaScript enabled"
961,understanding-the-need-for-adaptive-temporal-antialiasing-ataa,"Originally published at:			Understanding the Need for Adaptive Temporal Antialiasing (ATAA) | NVIDIA Technical Blog
The next generation of antialiasing is called ATAA, which stands for “Adaptive Temporal Antialiasing”. This new approach solves for the weakness of TAA (temporal anti-aliasing) – blurring and ghosting artifacts – while remaining light enough to avoid introducing a significant performance hit. ATAA results are remarkably close to what a developer would achieve through 16x…Powered by Discourse, best viewed with JavaScript enabled"
962,gtc-making-waves-for-ai-in-diy-robotics-applications-and-autonomous-delivery,"Originally published at:			GTC: Making Waves for AI in DIY, Robotics Applications, and Autonomous Delivery | NVIDIA Technical Blog
NVIDIA’s GTC is happening on October 5-9, featuring a huge catalog of live and on-demand sessions. Attendees can check out innovative demos, get access to the Inception AI Startup Pavilion, and connect with the developer community and technical experts. Recommended Live Sessions With Q&A  Bringing AI to DIY: NVIDIA JetsonAll Things Jetson: LIVE Q&A with NVIDIA…Powered by Discourse, best viewed with JavaScript enabled"
963,gtc-2020-cuda-and-ray-tracing-developer-tools,"GTC 2020 CWE21165
Presenters: ,
Abstract
With the advances in accelerated GPU computing and rendering come new development challenges. The new NVIDIA Nsight developer tools portfolio enables developers to embrace new CUDA features like CUDA graphs and accelerated ray tracing rendering with NVIDIA OptiX, DX12/DXR, or Vulkan Raytracing. Stop by to talk to the developer tools engineering team to learn more about how Nsight tools can help you. Share your wish list or challenges so we can shape the future of our tools accordingly.Watch this session
Join in the conversation below.Powered by Discourse, best viewed with JavaScript enabled"
964,simplifying-and-scaling-inference-serving-with-nvidia-triton-2-3,"Originally published at:			https://developer.nvidia.com/blog/simplifying-and-scaling-inference-serving-with-triton-2-3/
AI, machine learning (ML), and deep learning (DL) are effective tools for solving diverse computing problems such as product recommendations, customer interactions, financial risk assessment, manufacturing defect detection, and more. Using an AI model in production, called inference serving, is the most complex part of incorporating AI in applications. Triton Inference Server takes care of…Powered by Discourse, best viewed with JavaScript enabled"
965,nvidia-omniverse-audio2face-app-now-available-in-open-beta,"Originally published at:			NVIDIA Omniverse Audio2Face Available Later This Week in Open Beta | NVIDIA Technical Blog
NVIDIA Omniverse Audio2Face is now available in open beta. With the Audio2Face app, Omniverse users can generate AI-driven facial animation from audio sources.Hi,
I don’t see Audio2Face in the exchange app section of the launcher. Where is it available ?
Thanks for your help.Good question! I suggest asking in the Omniverse - NVIDIA Developer Forums. They’ll be able to help you better.Thank you for your interest for Audio2Face.  The release is scheduled for 4/16 and it will be published soon.  Watch out for an announcement soon!Hi,
I don’t see Audio2Face in the exchange app section of the launcher. Where is it available ?
Thanks for your help.We just released a new version for test, 2021.2.3, you should be able to see it in the Exchange now.Hey! Do you plan to make Audio2Face available for Linux?Powered by Discourse, best viewed with JavaScript enabled"
966,upcoming-event-data-science-sessions-at-gtc-2022,"Originally published at:			Data Science Conference Sessions | GTC 2022 | NVIDIA
Learn about the latest AI and data science breakthroughs from the world’s leading data science teams at GTC 2022.Powered by Discourse, best viewed with JavaScript enabled"
967,detecting-rotated-objects-using-the-nvidia-object-detection-toolkit,"Originally published at:			Detecting Rotated Objects Using the NVIDIA Object Detection Toolkit | NVIDIA Technical Blog
Figure 1. A portion of the International Society for Remote Sensing and Photogrammetry (ISPRS) Potsdam dataset. Rotated bounding boxes of the vehicle class, calculated using the segmentation masks labels, are shown in green. Object detection and classification in imagery using deep neural networks (DNNs) and convolutional neural networks (CNNs) is a well-studied area. For some…Interested in using this approach with an over head camera system to track moving objects using jetson nano. The FPS on T4 and V100 are great but wanted to get thoughts on running on a Jetson Nano?Is there somewhere a github repository with the code for the augmentation for rotated bounding boxes? I mean the code in which e.g. _corners2rotatedbbox() is called. I can not find this part on the ODTK github page.Working through a generic example using a small COCO dataset for microcontrollers at GitHub - TannerGilbert/Detectron2-Train-a-Instance-Segmentation-Model: Learn how to train a custom instance segmentation model with Detectron2Using the following command line to process and runs to the 10000 iteration and hangs. Waited for an hour+ and did CTRL-C and included that stack trace.Any suggestions on why it is hanging? Running on ubuntu 18 with two 1080 GPU cards.Also wanted to confirm that if I use the resize and jitter option the mask will need to change as well as the corresponding rotated bounding box. Does the resize and jitter transformation get applied to the mask/rotated bounding box?odtk train model.pth --backbone ResNet18FPN --iters 10000 --val-iters 1000 --lr 0.0001 --images /data/micro/segmentation/train/ --annotations /data/micro/segmentation/train.json --val-images /data/micro/segmentation/test --val-annotations /data/micro/segmentation/test.json --rotated-bboxInitializing model…
model: RetinaNet
backbone: ResNet18FPN
classes: 80, anchors: 27
Selected optimization level O2:  FP16 training with FP32 batchnorm and FP32 master weights.Defaults for this optimization level are:
enabled                : True
opt_level              : O2
cast_model_type        : torch.float16
patch_torch_functions  : False
keep_batchnorm_fp32    : True
master_weights         : True
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)…
After processing overrides, optimization options are:
enabled                : True
opt_level              : O2
cast_model_type        : torch.float16
patch_torch_functions  : False
keep_batchnorm_fp32    : True
master_weights         : True
loss_scale             : 128.0
Preparing dataset…
loader: pytorch
resize: [640, 1024], max: 1333
device: 2 GPUs
batch: 4, precision: mixed
BBOX type: rotated
Training model for 10000 iterations…
[   53/10000] focal loss: 1.608, box loss: 27.825, 1.138s/4-batch (fw: 0.431s, bw: 0.603s), 3.5 im/s, lr: 1.5e-05
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.
warnings.warn(SAVE_STATE_WARNING, UserWarning)
[  109/10000] focal loss: 1.573, box loss: 27.593, 1.081s/4-batch (fw: 0.402s, bw: 0.583s), 3.7 im/s, lr: 2e-05
[  165/10000] focal loss: 1.616, box loss: 27.829, 1.081s/4-batch (fw: 0.402s, bw: 0.628s), 3.7 im/s, lr: 2.5e-05
[  219/10000] focal loss: 1.736, box loss: 27.580, 1.118s/4-batch (fw: 0.408s, bw: 0.607s), 3.6 im/s, lr: 3e-05
[  274/10000] focal loss: 1.630, box loss: 27.808, 1.105s/4-batch (fw: 0.418s, bw: 0.635s), 3.6 im/s, lr: 3.5e-05
[  326/10000] focal loss: 1.788, box loss: 27.528, 1.168s/4-batch (fw: 0.417s, bw: 0.640s), 3.4 im/s, lr: 3.9e-05
[  380/10000] focal loss: 1.582, box loss: 27.621, 1.124s/4-batch (fw: 0.426s, bw: 0.644s), 3.6 im/s, lr: 4.4e-05
[  433/10000] focal loss: 1.838, box loss: 22.619, 1.135s/4-batch (fw: 0.405s, bw: 0.623s), 3.5 im/s, lr: 4.9e-05
[  488/10000] focal loss: 1.572, box loss: 3.746, 1.098s/4-batch (fw: 0.395s, bw: 0.653s), 3.6 im/s, lr: 5.4e-05
[  541/10000] focal loss: 1.879, box loss: 1.380, 1.175s/4-batch (fw: 0.428s, bw: 0.642s), 3.4 im/s, lr: 5.9e-05
[  595/10000] focal loss: 1.593, box loss: 2.943, 1.125s/4-batch (fw: 0.436s, bw: 0.636s), 3.6 im/s, lr: 6.4e-05
[  649/10000] focal loss: 1.712, box loss: 5.588, 1.112s/4-batch (fw: 0.406s, bw: 0.603s), 3.6 im/s, lr: 6.8e-05
[  701/10000] focal loss: 1.625, box loss: 3.093, 1.154s/4-batch (fw: 0.449s, bw: 0.652s), 3.5 im/s, lr: 7.3e-05
[  755/10000] focal loss: 1.594, box loss: 3.657, 1.121s/4-batch (fw: 0.428s, bw: 0.642s), 3.6 im/s, lr: 7.8e-05
[  807/10000] focal loss: 1.633, box loss: 2.056, 1.155s/4-batch (fw: 0.412s, bw: 0.635s), 3.5 im/s, lr: 8.3e-05
[  862/10000] focal loss: 1.599, box loss: 2.000, 1.099s/4-batch (fw: 0.400s, bw: 0.647s), 3.6 im/s, lr: 8.8e-05
[  914/10000] focal loss: 1.583, box loss: 4.031, 1.170s/4-batch (fw: 0.417s, bw: 0.644s), 3.4 im/s, lr: 9.2e-05
[  970/10000] focal loss: 1.570, box loss: 2.684, 1.085s/4-batch (fw: 0.412s, bw: 0.623s), 3.7 im/s, lr: 9.7e-05
No detections!
[ 1019/10000] focal loss: 1.596, box loss: 6.587, 1.229s/4-batch (fw: 0.445s, bw: 0.630s), 3.3 im/s, lr: 0.0001
[ 1073/10000] focal loss: 1.579, box loss: 2.010, 1.117s/4-batch (fw: 0.429s, bw: 0.635s), 3.6 im/s, lr: 0.0001
[ 1124/10000] focal loss: 1.641, box loss: 2.213, 1.188s/4-batch (fw: 0.438s, bw: 0.639s), 3.4 im/s, lr: 0.0001
[ 1178/10000] focal loss: 1.600, box loss: 2.111, 1.130s/4-batch (fw: 0.434s, bw: 0.647s), 3.5 im/s, lr: 0.0001
[ 1230/10000] focal loss: 1.606, box loss: 1.746, 1.159s/4-batch (fw: 0.423s, bw: 0.628s), 3.4 im/s, lr: 0.0001
[ 1285/10000] focal loss: 1.561, box loss: 1.938, 1.092s/4-batch (fw: 0.405s, bw: 0.636s), 3.7 im/s, lr: 0.0001
[ 1336/10000] focal loss: 1.779, box loss: 2.265, 1.192s/4-batch (fw: 0.442s, bw: 0.637s), 3.4 im/s, lr: 0.0001
[ 1392/10000] focal loss: 1.554, box loss: 1.222, 1.076s/4-batch (fw: 0.403s, bw: 0.624s), 3.7 im/s, lr: 0.0001
[ 1446/10000] focal loss: 1.633, box loss: 2.403, 1.124s/4-batch (fw: 0.393s, bw: 0.624s), 3.6 im/s, lr: 0.0001
[ 1500/10000] focal loss: 1.610, box loss: 1.566, 1.124s/4-batch (fw: 0.418s, bw: 0.653s), 3.6 im/s, lr: 0.0001
[ 1552/10000] focal loss: 1.660, box loss: 0.746, 1.156s/4-batch (fw: 0.404s, bw: 0.647s), 3.5 im/s, lr: 0.0001
[ 1607/10000] focal loss: 1.583, box loss: 1.120, 1.091s/4-batch (fw: 0.401s, bw: 0.641s), 3.7 im/s, lr: 0.0001
[ 1660/10000] focal loss: 1.653, box loss: 0.905, 1.152s/4-batch (fw: 0.419s, bw: 0.626s), 3.5 im/s, lr: 0.0001
[ 1715/10000] focal loss: 1.575, box loss: 2.376, 1.094s/4-batch (fw: 0.408s, bw: 0.635s), 3.7 im/s, lr: 0.0001
[ 1767/10000] focal loss: 1.689, box loss: 1.207, 1.171s/4-batch (fw: 0.411s, bw: 0.652s), 3.4 im/s, lr: 0.0001
[ 1819/10000] focal loss: 1.614, box loss: 2.224, 1.157s/4-batch (fw: 0.444s, bw: 0.659s), 3.5 im/s, lr: 0.0001
[ 1873/10000] focal loss: 1.503, box loss: 0.833, 1.125s/4-batch (fw: 0.400s, bw: 0.623s), 3.6 im/s, lr: 0.0001
[ 1930/10000] focal loss: 1.545, box loss: 0.840, 1.058s/4-batch (fw: 0.393s, bw: 0.616s), 3.8 im/s, lr: 0.0001
[ 1983/10000] focal loss: 1.678, box loss: 1.809, 1.145s/4-batch (fw: 0.411s, bw: 0.628s), 3.5 im/s, lr: 0.0001
No detections!
[ 2036/10000] focal loss: 1.584, box loss: 1.383, 1.150s/4-batch (fw: 0.432s, bw: 0.629s), 3.5 im/s, lr: 0.0001
[ 2090/10000] focal loss: 1.679, box loss: 1.153, 1.114s/4-batch (fw: 0.395s, bw: 0.615s), 3.6 im/s, lr: 0.0001
[ 2144/10000] focal loss: 1.568, box loss: 1.296, 1.114s/4-batch (fw: 0.411s, bw: 0.649s), 3.6 im/s, lr: 0.0001
[ 2197/10000] focal loss: 1.732, box loss: 0.429, 1.153s/4-batch (fw: 0.420s, bw: 0.625s), 3.5 im/s, lr: 0.0001
[ 2252/10000] focal loss: 1.495, box loss: 1.195, 1.102s/4-batch (fw: 0.415s, bw: 0.635s), 3.6 im/s, lr: 0.0001
[ 2305/10000] focal loss: 1.740, box loss: 0.764, 1.162s/4-batch (fw: 0.420s, bw: 0.634s), 3.4 im/s, lr: 0.0001
[ 2360/10000] focal loss: 1.341, box loss: 1.305, 1.097s/4-batch (fw: 0.405s, bw: 0.641s), 3.6 im/s, lr: 0.0001
[ 2413/10000] focal loss: 1.620, box loss: 1.702, 1.161s/4-batch (fw: 0.411s, bw: 0.642s), 3.4 im/s, lr: 0.0001
[ 2466/10000] focal loss: 1.300, box loss: 1.425, 1.146s/4-batch (fw: 0.443s, bw: 0.648s), 3.5 im/s, lr: 0.0001
[ 2521/10000] focal loss: 1.109, box loss: 0.828, 1.135s/4-batch (fw: 0.414s, bw: 0.619s), 3.5 im/s, lr: 0.0001
[ 2576/10000] focal loss: 1.254, box loss: 1.163, 1.106s/4-batch (fw: 0.429s, bw: 0.625s), 3.6 im/s, lr: 0.0001
[ 2629/10000] focal loss: 1.376, box loss: 1.663, 1.143s/4-batch (fw: 0.408s, bw: 0.630s), 3.5 im/s, lr: 0.0001
[ 2683/10000] focal loss: 1.228, box loss: 1.062, 1.129s/4-batch (fw: 0.439s, bw: 0.638s), 3.5 im/s, lr: 0.0001
[ 2737/10000] focal loss: 1.470, box loss: 1.112, 1.157s/4-batch (fw: 0.411s, bw: 0.642s), 3.5 im/s, lr: 0.0001
[ 2791/10000] focal loss: 1.174, box loss: 2.000, 1.119s/4-batch (fw: 0.412s, bw: 0.653s), 3.6 im/s, lr: 0.0001
[ 2845/10000] focal loss: 1.341, box loss: 0.803, 1.123s/4-batch (fw: 0.403s, bw: 0.616s), 3.6 im/s, lr: 0.0001
[ 2900/10000] focal loss: 1.133, box loss: 1.144, 1.098s/4-batch (fw: 0.417s, bw: 0.629s), 3.6 im/s, lr: 0.0001
[ 2955/10000] focal loss: 1.146, box loss: 2.585, 1.103s/4-batch (fw: 0.411s, bw: 0.591s), 3.6 im/s, lr: 0.0001
No detections!
[ 3008/10000] focal loss: 1.077, box loss: 1.042, 1.143s/4-batch (fw: 0.422s, bw: 0.633s), 3.5 im/s, lr: 0.0001
[ 3061/10000] focal loss: 1.310, box loss: 1.282, 1.171s/4-batch (fw: 0.427s, bw: 0.634s), 3.4 im/s, lr: 0.0001
[ 3116/10000] focal loss: 1.007, box loss: 0.955, 1.102s/4-batch (fw: 0.428s, bw: 0.624s), 3.6 im/s, lr: 0.0001
[ 3169/10000] focal loss: 1.185, box loss: 1.076, 1.182s/4-batch (fw: 0.435s, bw: 0.640s), 3.4 im/s, lr: 0.0001
[ 3222/10000] focal loss: 0.944, box loss: 0.855, 1.136s/4-batch (fw: 0.407s, bw: 0.676s), 3.5 im/s, lr: 0.0001
[ 3277/10000] focal loss: 0.950, box loss: 0.866, 1.143s/4-batch (fw: 0.399s, bw: 0.641s), 3.5 im/s, lr: 0.0001
[ 3332/10000] focal loss: 0.830, box loss: 1.257, 1.103s/4-batch (fw: 0.404s, bw: 0.648s), 3.6 im/s, lr: 0.0001
[ 3385/10000] focal loss: 0.737, box loss: 0.568, 1.136s/4-batch (fw: 0.412s, bw: 0.620s), 3.5 im/s, lr: 0.0001
[ 3440/10000] focal loss: 0.749, box loss: 0.626, 1.109s/4-batch (fw: 0.422s, bw: 0.637s), 3.6 im/s, lr: 0.0001
[ 3493/10000] focal loss: 0.774, box loss: 0.862, 1.162s/4-batch (fw: 0.418s, bw: 0.639s), 3.4 im/s, lr: 0.0001
[ 3546/10000] focal loss: 0.714, box loss: 1.785, 1.134s/4-batch (fw: 0.411s, bw: 0.668s), 3.5 im/s, lr: 0.0001
[ 3599/10000] focal loss: 0.666, box loss: 1.586, 1.133s/4-batch (fw: 0.422s, bw: 0.657s), 3.5 im/s, lr: 0.0001
[ 3650/10000] focal loss: 0.703, box loss: 2.287, 1.186s/4-batch (fw: 0.418s, bw: 0.656s), 3.4 im/s, lr: 0.0001
[ 3704/10000] focal loss: 0.665, box loss: 1.457, 1.113s/4-batch (fw: 0.426s, bw: 0.636s), 3.6 im/s, lr: 0.0001
[ 3756/10000] focal loss: 0.667, box loss: 1.809, 1.160s/4-batch (fw: 0.410s, bw: 0.639s), 3.4 im/s, lr: 0.0001
[ 3809/10000] focal loss: 0.657, box loss: 0.753, 1.136s/4-batch (fw: 0.428s, bw: 0.655s), 3.5 im/s, lr: 0.0001
[ 3861/10000] focal loss: 0.654, box loss: 0.820, 1.160s/4-batch (fw: 0.393s, bw: 0.657s), 3.4 im/s, lr: 0.0001
[ 3916/10000] focal loss: 0.618, box loss: 0.965, 1.094s/4-batch (fw: 0.414s, bw: 0.627s), 3.7 im/s, lr: 0.0001
[ 3968/10000] focal loss: 0.636, box loss: 1.640, 1.159s/4-batch (fw: 0.398s, bw: 0.654s), 3.5 im/s, lr: 0.0001
No detections!
[ 4020/10000] focal loss: 0.600, box loss: 2.283, 1.157s/4-batch (fw: 0.415s, bw: 0.654s), 3.5 im/s, lr: 0.0001
[ 4071/10000] focal loss: 0.627, box loss: 0.755, 1.190s/4-batch (fw: 0.433s, bw: 0.648s), 3.4 im/s, lr: 0.0001
[ 4125/10000] focal loss: 0.600, box loss: 1.796, 1.112s/4-batch (fw: 0.398s, bw: 0.663s), 3.6 im/s, lr: 0.0001
[ 4177/10000] focal loss: 0.546, box loss: 0.562, 1.172s/4-batch (fw: 0.409s, bw: 0.656s), 3.4 im/s, lr: 0.0001
[ 4230/10000] focal loss: 0.621, box loss: 1.456, 1.132s/4-batch (fw: 0.432s, bw: 0.647s), 3.5 im/s, lr: 0.0001
[ 4285/10000] focal loss: 0.561, box loss: 0.906, 1.129s/4-batch (fw: 0.420s, bw: 0.607s), 3.5 im/s, lr: 0.0001
[ 4341/10000] focal loss: 0.570, box loss: 1.330, 1.089s/4-batch (fw: 0.403s, bw: 0.635s), 3.7 im/s, lr: 0.0001
[ 4394/10000] focal loss: 0.565, box loss: 0.601, 1.146s/4-batch (fw: 0.393s, bw: 0.644s), 3.5 im/s, lr: 0.0001
[ 4448/10000] focal loss: 0.542, box loss: 1.136, 1.120s/4-batch (fw: 0.408s, bw: 0.659s), 3.6 im/s, lr: 0.0001
[ 4501/10000] focal loss: 0.498, box loss: 1.067, 1.148s/4-batch (fw: 0.407s, bw: 0.636s), 3.5 im/s, lr: 0.0001
[ 4555/10000] focal loss: 0.519, box loss: 1.624, 1.117s/4-batch (fw: 0.430s, bw: 0.635s), 3.6 im/s, lr: 0.0001
[ 4609/10000] focal loss: 0.446, box loss: 0.803, 1.165s/4-batch (fw: 0.402s, bw: 0.663s), 3.4 im/s, lr: 0.0001
[ 4662/10000] focal loss: 0.545, box loss: 1.195, 1.144s/4-batch (fw: 0.432s, bw: 0.658s), 3.5 im/s, lr: 0.0001
[ 4717/10000] focal loss: 0.512, box loss: 0.843, 1.143s/4-batch (fw: 0.412s, bw: 0.625s), 3.5 im/s, lr: 0.0001
[ 4771/10000] focal loss: 0.535, box loss: 1.508, 1.128s/4-batch (fw: 0.413s, bw: 0.662s), 3.5 im/s, lr: 0.0001
[ 4825/10000] focal loss: 0.473, box loss: 2.096, 1.166s/4-batch (fw: 0.448s, bw: 0.613s), 3.4 im/s, lr: 0.0001
[ 4879/10000] focal loss: 0.533, box loss: 2.383, 1.129s/4-batch (fw: 0.413s, bw: 0.664s), 3.5 im/s, lr: 0.0001
[ 4933/10000] focal loss: 0.464, box loss: 2.563, 1.166s/4-batch (fw: 0.410s, bw: 0.650s), 3.4 im/s, lr: 0.0001
[ 4988/10000] focal loss: 0.505, box loss: 2.629, 1.097s/4-batch (fw: 0.407s, bw: 0.639s), 3.6 im/s, lr: 0.0001
No detections!
[ 5041/10000] focal loss: 0.474, box loss: 1.094, 1.189s/4-batch (fw: 0.415s, bw: 0.632s), 3.4 im/s, lr: 0.0001
[ 5095/10000] focal loss: 0.501, box loss: 0.931, 1.125s/4-batch (fw: 0.437s, bw: 0.635s), 3.6 im/s, lr: 0.0001
[ 5149/10000] focal loss: 0.438, box loss: 0.670, 1.156s/4-batch (fw: 0.414s, bw: 0.638s), 3.5 im/s, lr: 0.0001
[ 5203/10000] focal loss: 0.450, box loss: 0.881, 1.129s/4-batch (fw: 0.434s, bw: 0.643s), 3.5 im/s, lr: 0.0001
[ 5257/10000] focal loss: 0.420, box loss: 0.725, 1.131s/4-batch (fw: 0.403s, bw: 0.625s), 3.5 im/s, lr: 0.0001
[ 5311/10000] focal loss: 0.474, box loss: 1.346, 1.114s/4-batch (fw: 0.408s, bw: 0.652s), 3.6 im/s, lr: 0.0001
[ 5365/10000] focal loss: 0.410, box loss: 0.584, 1.157s/4-batch (fw: 0.406s, bw: 0.647s), 3.5 im/s, lr: 0.0001
[ 5421/10000] focal loss: 0.437, box loss: 1.293, 1.077s/4-batch (fw: 0.409s, bw: 0.617s), 3.7 im/s, lr: 0.0001
[ 5473/10000] focal loss: 0.402, box loss: 0.727, 1.157s/4-batch (fw: 0.395s, bw: 0.652s), 3.5 im/s, lr: 0.0001
[ 5527/10000] focal loss: 0.422, box loss: 0.977, 1.122s/4-batch (fw: 0.416s, bw: 0.654s), 3.6 im/s, lr: 0.0001
[ 5581/10000] focal loss: 0.449, box loss: 0.997, 1.153s/4-batch (fw: 0.397s, bw: 0.652s), 3.5 im/s, lr: 0.0001
[ 5634/10000] focal loss: 0.433, box loss: 3.886, 1.132s/4-batch (fw: 0.419s, bw: 0.659s), 3.5 im/s, lr: 0.0001
[ 5689/10000] focal loss: 0.388, box loss: 1.617, 1.154s/4-batch (fw: 0.395s, bw: 0.658s), 3.5 im/s, lr: 0.0001
[ 5743/10000] focal loss: 0.478, box loss: 1.465, 1.118s/4-batch (fw: 0.402s, bw: 0.662s), 3.6 im/s, lr: 0.0001
[ 5797/10000] focal loss: 0.464, box loss: 2.164, 1.172s/4-batch (fw: 0.423s, bw: 0.644s), 3.4 im/s, lr: 0.0001
[ 5851/10000] focal loss: 0.429, box loss: 1.199, 1.119s/4-batch (fw: 0.417s, bw: 0.650s), 3.6 im/s, lr: 0.0001
[ 5905/10000] focal loss: 0.378, box loss: 0.750, 1.134s/4-batch (fw: 0.403s, bw: 0.626s), 3.5 im/s, lr: 0.0001
[ 5958/10000] focal loss: 0.406, box loss: 1.123, 1.139s/4-batch (fw: 0.442s, bw: 0.644s), 3.5 im/s, lr: 0.0001
No detections!
[ 6011/10000] focal loss: 0.402, box loss: 1.244, 1.134s/4-batch (fw: 0.420s, bw: 0.627s), 3.5 im/s, lr: 0.0001
[ 6062/10000] focal loss: 0.401, box loss: 1.378, 1.191s/4-batch (fw: 0.434s, bw: 0.647s), 3.4 im/s, lr: 0.0001
[ 6116/10000] focal loss: 0.394, box loss: 0.824, 1.124s/4-batch (fw: 0.429s, bw: 0.643s), 3.6 im/s, lr: 0.0001
[ 6169/10000] focal loss: 0.419, box loss: 1.083, 1.146s/4-batch (fw: 0.417s, bw: 0.622s), 3.5 im/s, lr: 0.0001
[ 6223/10000] focal loss: 0.401, box loss: 0.606, 1.112s/4-batch (fw: 0.413s, bw: 0.645s), 3.6 im/s, lr: 0.0001
[ 6275/10000] focal loss: 0.410, box loss: 1.546, 1.158s/4-batch (fw: 0.404s, bw: 0.645s), 3.5 im/s, lr: 0.0001
[ 6329/10000] focal loss: 0.401, box loss: 0.995, 1.131s/4-batch (fw: 0.438s, bw: 0.639s), 3.5 im/s, lr: 0.0001
[ 6381/10000] focal loss: 0.373, box loss: 1.228, 1.165s/4-batch (fw: 0.396s, bw: 0.663s), 3.4 im/s, lr: 0.0001
[ 6434/10000] focal loss: 0.391, box loss: 1.282, 1.145s/4-batch (fw: 0.416s, bw: 0.673s), 3.5 im/s, lr: 0.0001
[ 6486/10000] focal loss: 0.388, box loss: 1.842, 1.166s/4-batch (fw: 0.400s, bw: 0.655s), 3.4 im/s, lr: 0.0001
[ 6541/10000] focal loss: 0.376, box loss: 0.896, 1.095s/4-batch (fw: 0.407s, bw: 0.637s), 3.7 im/s, lr: 0.0001
[ 6593/10000] focal loss: 0.361, box loss: 0.763, 1.173s/4-batch (fw: 0.422s, bw: 0.642s), 3.4 im/s, lr: 0.0001
[ 6648/10000] focal loss: 0.383, box loss: 0.714, 1.106s/4-batch (fw: 0.424s, bw: 0.631s), 3.6 im/s, lr: 0.0001
[ 6700/10000] focal loss: 0.426, box loss: 0.775, 1.158s/4-batch (fw: 0.414s, bw: 0.633s), 3.5 im/s, lr: 0.0001
[ 6756/10000] focal loss: 0.375, box loss: 1.123, 1.085s/4-batch (fw: 0.407s, bw: 0.627s), 3.7 im/s, lr: 0.0001
[ 6808/10000] focal loss: 0.396, box loss: 1.614, 1.164s/4-batch (fw: 0.420s, bw: 0.632s), 3.4 im/s, lr: 0.0001
[ 6862/10000] focal loss: 0.386, box loss: 0.799, 1.125s/4-batch (fw: 0.428s, bw: 0.645s), 3.6 im/s, lr: 0.0001
[ 6914/10000] focal loss: 0.390, box loss: 0.618, 1.165s/4-batch (fw: 0.414s, bw: 0.645s), 3.4 im/s, lr: 0.0001
[ 6968/10000] focal loss: 0.377, box loss: 1.434, 1.113s/4-batch (fw: 0.407s, bw: 0.655s), 3.6 im/s, lr: 0.0001
No detections!
[ 7021/10000] focal loss: 0.385, box loss: 0.466, 1.188s/4-batch (fw: 0.417s, bw: 0.633s), 3.4 im/s, lr: 0.0001
[ 7075/10000] focal loss: 0.371, box loss: 1.603, 1.125s/4-batch (fw: 0.407s, bw: 0.665s), 3.6 im/s, lr: 0.0001
[ 7129/10000] focal loss: 0.361, box loss: 2.978, 1.159s/4-batch (fw: 0.408s, bw: 0.647s), 3.5 im/s, lr: 0.0001
[ 7184/10000] focal loss: 0.392, box loss: 0.868, 1.109s/4-batch (fw: 0.407s, bw: 0.649s), 3.6 im/s, lr: 0.0001
[ 7237/10000] focal loss: 0.364, box loss: 1.274, 1.176s/4-batch (fw: 0.413s, bw: 0.655s), 3.4 im/s, lr: 0.0001
[ 7290/10000] focal loss: 0.391, box loss: 1.061, 1.145s/4-batch (fw: 0.420s, bw: 0.669s), 3.5 im/s, lr: 0.0001
[ 7345/10000] focal loss: 0.375, box loss: 0.844, 1.130s/4-batch (fw: 0.400s, bw: 0.628s), 3.5 im/s, lr: 0.0001
[ 7399/10000] focal loss: 0.362, box loss: 0.732, 1.111s/4-batch (fw: 0.431s, bw: 0.626s), 3.6 im/s, lr: 0.0001
[ 7453/10000] focal loss: 0.404, box loss: 0.646, 1.139s/4-batch (fw: 0.398s, bw: 0.634s), 3.5 im/s, lr: 0.0001
[ 7508/10000] focal loss: 0.355, box loss: 0.695, 1.101s/4-batch (fw: 0.401s, bw: 0.646s), 3.6 im/s, lr: 0.0001
[ 7561/10000] focal loss: 0.362, box loss: 0.707, 1.153s/4-batch (fw: 0.416s, bw: 0.631s), 3.5 im/s, lr: 0.0001
[ 7615/10000] focal loss: 0.353, box loss: 1.325, 1.112s/4-batch (fw: 0.431s, bw: 0.629s), 3.6 im/s, lr: 0.0001
[ 7669/10000] focal loss: 0.346, box loss: 0.762, 1.158s/4-batch (fw: 0.408s, bw: 0.646s), 3.5 im/s, lr: 0.0001
[ 7722/10000] focal loss: 0.361, box loss: 0.610, 1.139s/4-batch (fw: 0.420s, bw: 0.666s), 3.5 im/s, lr: 0.0001
[ 7776/10000] focal loss: 0.347, box loss: 1.628, 1.119s/4-batch (fw: 0.416s, bw: 0.650s), 3.6 im/s, lr: 0.0001
[ 7830/10000] focal loss: 0.353, box loss: 0.710, 1.132s/4-batch (fw: 0.414s, bw: 0.617s), 3.5 im/s, lr: 0.0001
[ 7884/10000] focal loss: 0.357, box loss: 1.223, 1.115s/4-batch (fw: 0.411s, bw: 0.652s), 3.6 im/s, lr: 0.0001
[ 7935/10000] focal loss: 0.344, box loss: 2.945, 1.197s/4-batch (fw: 0.427s, bw: 0.661s), 3.3 im/s, lr: 0.0001
[ 7989/10000] focal loss: 0.361, box loss: 1.690, 1.124s/4-batch (fw: 0.426s, bw: 0.645s), 3.6 im/s, lr: 0.0001
No detections!
[ 8042/10000] focal loss: 0.369, box loss: 1.466, 1.151s/4-batch (fw: 0.394s, bw: 0.617s), 3.5 im/s, lr: 0.0001
[ 8096/10000] focal loss: 0.362, box loss: 0.753, 1.121s/4-batch (fw: 0.400s, bw: 0.669s), 3.6 im/s, lr: 0.0001
[ 8148/10000] focal loss: 0.354, box loss: 0.929, 1.176s/4-batch (fw: 0.412s, bw: 0.654s), 3.4 im/s, lr: 0.0001
[ 8203/10000] focal loss: 0.354, box loss: 0.899, 1.093s/4-batch (fw: 0.391s, bw: 0.652s), 3.7 im/s, lr: 0.0001
[ 8255/10000] focal loss: 0.434, box loss: 2.358, 1.159s/4-batch (fw: 0.403s, bw: 0.648s), 3.5 im/s, lr: 0.0001
[ 8310/10000] focal loss: 0.359, box loss: 1.237, 1.094s/4-batch (fw: 0.396s, bw: 0.645s), 3.7 im/s, lr: 0.0001
[ 8361/10000] focal loss: 0.357, box loss: 0.735, 1.198s/4-batch (fw: 0.434s, bw: 0.651s), 3.3 im/s, lr: 0.0001
[ 8414/10000] focal loss: 0.354, box loss: 0.787, 1.143s/4-batch (fw: 0.425s, bw: 0.664s), 3.5 im/s, lr: 0.0001
[ 8466/10000] focal loss: 0.357, box loss: 0.731, 1.159s/4-batch (fw: 0.417s, bw: 0.634s), 3.5 im/s, lr: 0.0001
[ 8522/10000] focal loss: 0.377, box loss: 0.574, 1.080s/4-batch (fw: 0.395s, bw: 0.634s), 3.7 im/s, lr: 0.0001
[ 8575/10000] focal loss: 0.378, box loss: 2.198, 1.136s/4-batch (fw: 0.401s, bw: 0.631s), 3.5 im/s, lr: 0.0001
[ 8630/10000] focal loss: 0.395, box loss: 0.847, 1.098s/4-batch (fw: 0.415s, bw: 0.629s), 3.6 im/s, lr: 0.0001
[ 8680/10000] focal loss: 0.423, box loss: 1.142, 1.206s/4-batch (fw: 0.450s, bw: 0.641s), 3.3 im/s, lr: 0.0001
[ 8734/10000] focal loss: 0.341, box loss: 1.229, 1.113s/4-batch (fw: 0.423s, bw: 0.638s), 3.6 im/s, lr: 0.0001
[ 8786/10000] focal loss: 0.350, box loss: 0.521, 1.168s/4-batch (fw: 0.399s, bw: 0.660s), 3.4 im/s, lr: 0.0001
[ 8840/10000] focal loss: 0.349, box loss: 0.756, 1.127s/4-batch (fw: 0.421s, bw: 0.652s), 3.5 im/s, lr: 0.0001
[ 8894/10000] focal loss: 0.395, box loss: 0.733, 1.119s/4-batch (fw: 0.374s, bw: 0.644s), 3.6 im/s, lr: 0.0001
[ 8948/10000] focal loss: 0.376, box loss: 2.057, 1.127s/4-batch (fw: 0.418s, bw: 0.656s), 3.5 im/s, lr: 0.0001
No detections!
[ 9001/10000] focal loss: 0.309, box loss: 1.173, 1.185s/4-batch (fw: 0.411s, bw: 0.630s), 3.4 im/s, lr: 0.0001
[ 9055/10000] focal loss: 0.387, box loss: 1.072, 1.127s/4-batch (fw: 0.439s, bw: 0.635s), 3.6 im/s, lr: 0.0001
[ 9108/10000] focal loss: 0.342, box loss: 1.415, 1.140s/4-batch (fw: 0.438s, bw: 0.647s), 3.5 im/s, lr: 0.0001
[ 9162/10000] focal loss: 0.368, box loss: 0.844, 1.128s/4-batch (fw: 0.377s, bw: 0.648s), 3.5 im/s, lr: 0.0001
[ 9215/10000] focal loss: 0.361, box loss: 0.831, 1.141s/4-batch (fw: 0.435s, bw: 0.651s), 3.5 im/s, lr: 0.0001
[ 9267/10000] focal loss: 0.344, box loss: 1.460, 1.156s/4-batch (fw: 0.397s, bw: 0.650s), 3.5 im/s, lr: 0.0001
[ 9321/10000] focal loss: 0.329, box loss: 1.530, 1.130s/4-batch (fw: 0.422s, bw: 0.655s), 3.5 im/s, lr: 0.0001
[ 9374/10000] focal loss: 0.343, box loss: 1.406, 1.146s/4-batch (fw: 0.396s, bw: 0.643s), 3.5 im/s, lr: 0.0001
[ 9428/10000] focal loss: 0.328, box loss: 0.683, 1.125s/4-batch (fw: 0.421s, bw: 0.653s), 3.6 im/s, lr: 0.0001
[ 9481/10000] focal loss: 0.406, box loss: 1.147, 1.141s/4-batch (fw: 0.413s, bw: 0.623s), 3.5 im/s, lr: 0.0001
[ 9533/10000] focal loss: 0.349, box loss: 0.781, 1.154s/4-batch (fw: 0.435s, bw: 0.665s), 3.5 im/s, lr: 0.0001
[ 9585/10000] focal loss: 0.310, box loss: 1.570, 1.159s/4-batch (fw: 0.405s, bw: 0.644s), 3.5 im/s, lr: 0.0001
[ 9640/10000] focal loss: 0.343, box loss: 1.507, 1.107s/4-batch (fw: 0.423s, bw: 0.630s), 3.6 im/s, lr: 0.0001
[ 9693/10000] focal loss: 0.319, box loss: 2.601, 1.139s/4-batch (fw: 0.414s, bw: 0.618s), 3.5 im/s, lr: 0.0001
[ 9746/10000] focal loss: 0.347, box loss: 1.979, 1.146s/4-batch (fw: 0.427s, bw: 0.665s), 3.5 im/s, lr: 0.0001
[ 9798/10000] focal loss: 0.315, box loss: 1.664, 1.166s/4-batch (fw: 0.416s, bw: 0.641s), 3.4 im/s, lr: 0.0001
[ 9852/10000] focal loss: 0.322, box loss: 0.816, 1.125s/4-batch (fw: 0.422s, bw: 0.649s), 3.6 im/s, lr: 0.0001
[ 9902/10000] focal loss: 0.331, box loss: 0.884, 1.213s/4-batch (fw: 0.426s, bw: 0.672s), 3.3 im/s, lr: 0.0001
[ 9957/10000] focal loss: 0.327, box loss: 2.321, 1.100s/4-batch (fw: 0.431s, bw: 0.614s), 3.6 im/s, lr: 0.0001
[10000/10000] focal loss: 0.327, box loss: 1.680, 1.128s/4-batch (fw: 0.429s, bw: 0.630s), 3.5 im/s, lr: 0.0001^CTraceback (most recent call last):
File “/opt/conda/bin/odtk”, line 11, in 
load_entry_point(‘odtk’, ‘console_scripts’, ‘odtk’)()
File “/workspace/retinanet/retinanet/main.py”, line 245, in main
torch.multiprocessing.spawn(worker, args=(args, world, model, state), nprocs=world)
File “/opt/conda/lib/python3.6/site-packages/torch/multiprocessing/spawn.py”, line 200, in spawn
return start_processes(fn, args, nprocs, join, daemon, start_method=‘spawn’)
File “/opt/conda/lib/python3.6/site-packages/torch/multiprocessing/spawn.py”, line 158, in start_processes
while not context.join():
File “/opt/conda/lib/python3.6/site-packages/torch/multiprocessing/spawn.py”, line 78, in join
timeout=timeout,
File “/opt/conda/lib/python3.6/multiprocessing/connection.py”, line 911, in wait
ready = selector.select(timeout)
File “/opt/conda/lib/python3.6/selectors.py”, line 376, in select
fd_event_list = self._poll.poll(timeout)
KeyboardInterruptHi. im quite new using nvidia dockers and nvidia tools.
i tried this example on my dataset, but its seems something is wrong, i believe its my labels.
so im trying to get usa plate numbers. when the plate are quite rotates and with a perspective.
i am trying first with a sintetic database
Here 2 casescan anyone giveme some hint ? where to look for error, or what to do?I am also having a problem with training against the coco dataset.  I have tried version 20.03 and the latest version of odtk with the same result.  I am training about 10K iterations.I have a visualizer for the rotated bounding boxes using tensor board and they look correct.  However, I am interpreting the rotation being about the center of the box as opposed to the xy min.  Also is the range -pi/4 to pi/4 or -pi/2 to pi/2?Have you made progress since you posted this question?Was trying to use object detection for given video, but its failing, seems like a rotating object in video is hard to grab.Did you find code for this “Many datasets (for example, COCO and ISPRS) come with segmentation masks. These masks can be converted into rotated bounding boxes by using a geometry package.”?Powered by Discourse, best viewed with JavaScript enabled"
968,nvidia-digits-assists-alzheimers-disease-prediction,"Originally published at:			https://developer.nvidia.com/blog/nvidia-digits-alzheimers-disease-prediction/
Pattern recognition and classification in medical image analysis has been of interest to scientists for many years. Machine learning techniques have enabled researchers to develop and utilize complicated models to classify or predict various abnormalities or diseases. Recently, the successful applications of state-of-the-art deep learning architectures have rapidly expanded in medical imaging. Cutting-edge deep learning…Why after showing me a beautiful ROC curve do you in the result section state the accuracy result and not the ROAUC value? The whole reason for ROAUC is to avoid the pitfalls of using accuracy as your evaluation metric, ie high accuracies can be achieved concurrently with very low sensitivities or specificities in rare or common conditions, respectively. I'm left wondering what the actual AUC value is. If you need help with medical/scientific writing from someone who also knows machine learning, feel free to contact me. Chip Reuben, MSThank you for this article! It seems that axial slices at different location/time from the *same* subject were used both in training and testing sets, meaning that these sets are not separated on the subject level. Am I understanding this correctly?Dear Tanya, Thanks for your interest in this work. And I also need to thank you for attention to details . In this work, we performed the classification in the slice level. As you understood, we created samples from all subjects' fMRI time series. Next, we shuffled the data and created training and testing samples. The reported accuracy is for the slice level classification. One more thing I'd like to share with you is slices from a given subject are independent even they are highly correlated. It means our training and testing datasets are completely independent from each other in ""slice-level"". I realize the medical imaging researchers are more interested in ""subject-level"" classification that's why I continued the project.In our more complete paper called DeepAD, we performed ""subject level"" classification which means we divided the subjects into two groups : training and testing and then we did the ""subject level"" classification. Again, we achieved a very high accuracy rate. We also designed a decision making algorithm to stabilize the prediction process in order to make a decision whether a subject is Alzheimer's or not. The beauty of CNN architecture is to generate a well-generalised model once it was successfully trained and validated by high volume of data. Please feel free to have a look at the DeepAD at http://biorxiv.org/content/... where we used a huge dataset to classify slice-level, subject-level structural and functional MRI data.Hope it helps,SamanDear Chip, Thanks for your interest in this paper. There are different ideas regarding using ROC / AUC as the classification metric or not. I personally have more tendency to stick with the accuracy as the performance metric of classification since it makes more sense in our research field. The reason, I generated the ROC curve was to ensure that the classification has been successfully performed. In addition, ROC / AUC is more informative in case of imbalanced data which is not the case in our work. Thanks for your comment and offer. I will keep your contact information.Best,SamanThank you for the elaborate response and the link to the DeepAD paper. After reading it, I'm still not clear how the subject-level classification was done and I would appreciate some clarification since the accuracy you report is pretty remarkable.In section 6 of that paper you mention that ""the adopted LeNet model and GoogleNet were adjusted..."" - by ""adjusted"" do you mean ""fine-tuned""? I.e. have you used the networks you previously trained for the slice-level classification and fine-tuned the last layers on the subject-level task? If so, I'm assuming that none of the slices from the subjects chosen for the subject-level classification were seen by the networks during the training stage of the slice-level classification. This would leave very little data to fine-tune with as you only have 52AD/92NC for rs-fMRI and 211AD/91NC for MRI. Can you please provide more details as to what layers you fine-tuned and how much data was used for this purpose?Dear Tanya, 1 - We trained the networks from “scratch”. “NO fine-tuning” was performed and it has been clearly mentioned in the paper. The initial LeNet and GoogleNet architectures have been designed for different number of classes but I used them for a binary classification so that some adjustment were required.2 - If you take a look at the pipeline and data conversion section in the paper, I explained how I extracted 2D slices from the data to generate a huge dataset for both fMRI and MRI pipeline. As I said in my previous comment, for “subject-level” classification, the “subjects” were divided into the training and testing group. Next, the 2D slice samples were generated. It means there was no slice from the same subject in the training and testing datasets, simultaneously. In another word, the training and testing data had “NO” subjects in common. Regarding the accuracy reported, some research groups using different strategies could achieve a very high accuracy rate and I mentioned them in the literature review. Please look at the comparison table. However, I could improve the accuracy rate for MRI data by much more accurate preprocessing and some tricks in DL. In addition, for the first time, fMRI used for this classification and thanks to a very accurate and massive preprocessing pipeline and certain optimization, I achieved the highest accuracy rate reported so far. Hope it helps. If it is still unclear or you need more clarification, I will be more than happy to help you or anybody else to replicate the DeepAD paper and achieve the same accuracy as long as you use exactly the “identical” methods I used in the paper. You can reach me at samansarraf@ieee.org.Best,SamanThanks for your quick response, Saman! For subject level, I understand that you do the subject level separation prior to generation of the 2D slices for classification, but since your test set include multiple slices from the same subject, how do you calculate subject-level accuracy? Do you average accuracy across all slices of the same test subject? Or is it still slice-level accuracy?That’s my pleasure to help.In the subject-level, the accuracy reported is still based on the slice classification. How can we report an accuracy rate for a subject? It does not make sense at all. What you could do is to measure the probability of the subject whether to be AD or NC. Does it make sense?What I developed was a decision making algorithm that was counting the number of slices classified as AD or NC and then calculate the probability and vote for the majority. Let’s say (the number is just an example) , for a given subject having 1000 slices, 900 slices were recognized as AD which can say the probability is 90% to be AD. In this case, the decision maker votes for AD. In DeepAD, the table 5 and figure 15 summarizes what I explained above. Feel free to post your comments or reach me out if you have more questions. Best,SamanThat is very nice paper and useful report for every user including me as a beginner in deep learning. My question is the accuracy of 97% is the best accuracy you got from your data?Hi there, This is the averaged accuracy after 5 time shuffling the data in this conference paper. As I showed in DeepAD , by updating the preprocessing pipeline and adding more samples for the training, I could achieve up to 99.9% for slice level recognition. Thanks,Hi. I really admire your work and I am trying to replicate it as well. I want to ask regarding table 1. what is actually the volume is? I was taking a volume as one .nii image. but now I am totally confused about getting huge number of total images.Regards.Is there any other good tool to replace fsl-VBM to get GM as I have tried it and it is taking a lot of time.Dear Ammarah, Thanks for your interest in this paper and the expanded version DeepAD. Let me answer your both questions in this reply. I think there is a misleading here, what we tried in this paper was ""functional MRI"" data which are 4D data (3Dxtime) . The volume in table 1 means 3D volumes of a given subject have been collected 300 times (time points).  Regarding your second question, actually FSL VBM is a tool for structural MRI not functional MRI. You can also use SPM8 to process you structural MRI data. Good luck,SamanThank you,Saman. please also make me clear about selecting slices for sMRI. I get about 256 slices for one subject/nifti image. then I discard slices from start and end which are just black and have no information.it gives me about 70 to 100 useful slices but having different brain portions like very small from top as well as good looking axial slices. Am I doing it correctly? Also have you used data augmentation for sMRI.Regardsgood job. thanks for sharing your experineces. do you write Roc curve code in digit? How can I access to Roc curve and confusion matrix in digits ? can I addany code?Hi there, thanks for your interest in this work and paper. Actually, what I did to generate ROC curves was done out of DIGITS . Firstly, you need to use classify many option of DIGITS to get predicted labels and scores for your testing samples. Next, you need to save the results as html files or any format (that you are more comfortable with) and write your in-house codes to draw ROCs. I did it in MATLAB.thanks a millionDear Prof. Sarraf,Thanks for your work. I may ask a very stupid question. In your original paper, you just spit the data into training and testing dataset, and in the original paper, you gave the loss and accuracy figures over the 30 epochs for both training and testing. From what I understand, in the original paper, you 'testing data' is actually used to validate your model, not really a testing dataset. How did you calculate the accuracy of your testing data??In the post here, you split the data into 3 dataset, training, testing and validation dataset. You mentioned 'We repeated the entire dataset generation and classification process five times for 5-fold cross validation, achieving an average accuracy rate of 96.85%.' So this accuracy is based on the real testing dataset, not like in the original paper, no?Look forward to your responseBestHaoDear Hao,The accuracy rates reported in this tutorial have been extracted from the original paper. No matter what if you use testing or testing / validation datasets for evaluating your model against unseen data , the evaluation is completely valid.Hope it helps.Powered by Discourse, best viewed with JavaScript enabled"
969,share-your-science-high-performance-computing-for-network-intelligence,"Originally published at:			https://developer.nvidia.com/blog/share-your-science-high-performance-computing-for-network-intelligence/
Hugo Latapie, principal engineer at Cisco shares how they are using NVIDIA GPUs and deep learning in their products for a variety of applications such as encrypted network traffic classification, video compression and state-of-the-art crowd analytics. Latapie also covers how his team takes advantage of NVIDIA-Docker to share machines with different kinds of GPUs and…Powered by Discourse, best viewed with JavaScript enabled"
970,nyu-using-neural-networks-to-improve-radiologist-performance-in-breast-cancer-screening,"Originally published at:			https://developer.nvidia.com/blog/nyu-using-neural-networks-to-improve-radiologist-performance-in-breast-cancer-screening/
By Jason Phang, PhD student at the NYU Center for Data Science Breast cancer is the second leading cancer-related cause of death among women in the US. However, screening mammograms require radiologists to arduously pore over extremely high-resolution mammography images, looking for features suggestive of cancerous or suspicious lesions.  This appears like an ideal situation…Powered by Discourse, best viewed with JavaScript enabled"
971,google-uses-ai-to-diagnose-breast-cancer,"Originally published at:			https://developer.nvidia.com/blog/google-uses-ai-to-diagnose-breast-cancer/
Google researchers developed a deep learning-based framework that automatically identifies tumors. “What we’ve trained is just a little sliver of software that helps with one part of a very complex series of tasks,” said Lily Peng, the project manager behind Google’s work. “There will hopefully be more and more of these tools that help doctors…Powered by Discourse, best viewed with JavaScript enabled"
972,predicting-photos-memorability-at-near-human-levels-with-deep-learning,"Originally published at:			Predicting Photos’ Memorability at “Near-Human” Levels with Deep Learning | NVIDIA Technical Blog
MIT researchers just released a new GPU-accelerated algorithm that can predict how memorable or forgettable an image is almost as accurately as humans — and they plan to turn it into an app that subtly tweaks photos to make them more memorable. Using a TITAN X GPU and deep learning, the “MemNet” algorithm creates a…Powered by Discourse, best viewed with JavaScript enabled"
973,using-gpus-to-predict-national-weather-forecasts,"Originally published at:			Using GPUs to Predict National Weather Forecasts | NVIDIA Technical Blog
Thanks to GPUs, the Swiss have made significant advancements in their ability to predict storms and other weather hazards with higher levels of accuracy. The Swiss Federal Office of Meteorology and Climatology, MeteoSwiss, is the first major national weather service to deploy a GPU-accelerated supercomputer to improve its daily weather forecasts. The new system, powered…Powered by Discourse, best viewed with JavaScript enabled"
974,ai-tourist-finds-its-way-around-new-york-city-with-the-help-of-another-ai-algorithm,"Originally published at:			AI Tourist Finds Its Way Around New York City with the Help of Another AI Algorithm | NVIDIA Technical Blog
Can you picture what it would be like to navigate the streets of New York City without a smartphone? If you’re a local, it’s easy, but if you’re a tourist, it can be daunting. To help alleviate the problem, researchers from the University of Montreal in Canada, and Facebook developed a deep learning-based system called “Talk…Powered by Discourse, best viewed with JavaScript enabled"
975,point-cloud-processing-with-nvidia-driveworks-sdk,"Originally published at:			Point Cloud Processing with NVIDIA DriveWorks SDK | NVIDIA Technical Blog
The NVIDIA DriveWorks SDK contains a collection of CUDA-based low level point cloud processing modules optimized for NVIDIA DRIVE AGX platforms. The DriveWorks Point Cloud Processing modules include common algorithms that any AV developer working with point cloud representations would need, such as accumulation and registration. Figure 1 shows NVIDIA test vehicles outfitted with lidar. Figure…Powered by Discourse, best viewed with JavaScript enabled"
976,gtc-2020-accelerating-cancer-research-vdi-by-day-compute-by-night,"GTC 2020 S21845
Presenters: Erik Bohnhorst,NVIDIA; Jits Langedijk,NVIDIA; Roel Sijstermans,Antoni van Leeuwenhoek Hospital and Netherlands Cancer Institute
Abstract
The Netherlands Cancer Institute–Antoni van Leeuwenhoek Hospital (NKI-AVL) is one of the top 10 comprehensive cancer centers in the world. By combining cancer care, research, and by exchanging knowledge internationally, they make a significant contribution to solving the cancer problem in the 21st century. To meet that challenge, NKI-AVL built a software-defined infrastructure to accelerate research and enhance efficiency for clinicians. During daytime, the VDI infrastructure will give health care professionals fast, remote, and secure access to patient data. At night, the same VDI platform is utilized by researchers to execute computational GPU workloads. As a result, the high-performance and flexible IT infrastructure enables physicians and nurses to spend more focused time on patient care, and researchers to advance new discoveries in cancer treatment.Watch this session
Join in the conversation below.Powered by Discourse, best viewed with JavaScript enabled"
977,structured-domain-randomization-makes-deep-learning-more-accessible,"Originally published at:			Structured Domain Randomization Makes Deep Learning More Accessible | NVIDIA Technical Blog
Manually annotating training data is laborious, and time-consuming. This means that training deep networks for computer vision tasks typically requires an enormous amount of labeled training data, which can be expensive and difficult to obtain. To help make deep learning more accessible, researchers from NVIDIA have introduced a structured domain randomization system to help developers…Powered by Discourse, best viewed with JavaScript enabled"
978,etsy-buys-artificial-intelligence-startup-to-enhance-search-capabilities,"Originally published at:			Etsy Buys Artificial Intelligence Startup to Enhance Search Capabilities | NVIDIA Technical Blog
The popular handmade-goods site Etsy acquired Blackbird Technologies, a startup using GPUs to develop superior search relevance and recommendation technology. Using CUDA, TITAN X GPUs and the Theano deep learning framework with cuDNN to train their models, Blackbird’s technology combines image and text recognition to power a range of search capabilities, including personalized search, ranking,…Powered by Discourse, best viewed with JavaScript enabled"
979,nvidia-releases-updates-and-new-features-in-cuda-x-ai-software,"Originally published at:			https://developer.nvidia.com/blog/nvidia-releases-updates-and-new-features-in-cuda-x-ai-software/
Learn what’s new CUDA-X AI— a deep learning software stack for researchers and developers to build GPU-accelerated applications.Powered by Discourse, best viewed with JavaScript enabled"
980,gpu-optimized-software-ngc-collections-now-available,"Originally published at:			GPU-Optimized Software: NGC Collections Now Available | NVIDIA Technical Blog
With Collections in the NGC catalog, we have curated all the content you need for your use case into one package.Powered by Discourse, best viewed with JavaScript enabled"
981,upcoming-event-get-to-know-nvidia-jetson-ecosystem-partners-at-embedded-world,"Originally published at:			https://developer.nvidia.com/blog/upcoming-event-get-to-know-nvidia-jetson-ecosystem-partners-at-embedded-world/
Check out object detection solutions, 360° camera views, and more from NVIDIA Jetson ecosystem partners this week at Embedded World in Germany.Powered by Discourse, best viewed with JavaScript enabled"
982,on-demand-session-accelerating-kubernetes-with-nvidia-operators,"Originally published at:			https://developer.nvidia.com/blog/accelerating-kubernetes-with-nvidia-operators/
NVIDIA Operators streamline installing and managing GPUs and NICs on Kubernetes to make the software stack ready to run the most resource-demanding workloads, such as AI, ML, DL, and HPC, in the cloud, data center, and at the edge.Powered by Discourse, best viewed with JavaScript enabled"
983,gtc-2020-faster-transformer,"GTC 2020 S21417
Presenters: Bo Yang Hsueh,NVIDIA
Abstract
Recently, models such as BERT and XLNet, which adopt a stack of transformer layers as key components, show breakthrough performance in various deep learning tasks. Consequently, the inference performance of the transformer layer greatly limits the possibility that such models can be adopted in online services. First, we’ll show how Faster Transformer optimizes the inference computation of both the transformer encoder and decoder layers. In addition to optimizations on the standard transformer, we’ll get into how to customize Faster Transformer to accelerate a pruned transformer encoder layer together with the CUTLASS library.Watch this session
Join in the conversation below.Powered by Discourse, best viewed with JavaScript enabled"
984,easy-vrs-integration-with-eye-tracking,"Originally published at:			Easy VRS Integration with Eye Tracking | NVIDIA Technical Blog
VR head-mounted displays (HMDs) continue to dramatically improve with each generation. Resolution, refresh rate, field of view, and other features bring unique challenges to the table. The NVIDIA VRWorks Graphics SDK has been offering various rendering technologies to tackle the challenges brought forth by the increasing capabilities of these new HMDs. NVIDIA Turing introduced Variable Rate…Great improvement, I will be looking forward to try it!Amazing... foveated rendering is fundamental for the future of Virtual Reality!Here is a detailed guide on ""Getting Started with VRS & Foveated Rendering using HTC Vive Pro Eye & Unreal Engine""  https://uploads.disquscdn.c...VRWorks SDK only limited to rasterization pipeline.Powered by Discourse, best viewed with JavaScript enabled"
985,gtc-2020-enabling-workloads-using-high-end-graphics-through-windows-virtual-desktop,"GTC 2020 S21309
Presenters: Manvender Rawat,NVIDIA; Denis Gundarev, Microsoft
Abstract
Learn how NVIDIA and Microsoft partner to bring Windows Virtual Desktop, using NVIDIA GPU instance, to provide a flexible and affordable cloud option for professional graphics workstations — used for CAD and 3D graphics workloads as well as virtual desktops — for knowledge workers who rely on graphics- and multimedia-intensive applications. We’ll talk about multi-session desktops, other best practices for getting the best out of your deployments, and how to right-size your deployments in the cloud with Azure.Watch this session
Join in the conversation below.Powered by Discourse, best viewed with JavaScript enabled"
986,developers-look-to-openusd-in-era-of-ai-and-industrial-digitalization,"Originally published at:			Developers Look to OpenUSD in Era of AI and Industrial Digitalization | NVIDIA Blog
A new paradigm for data modeling and interchange is unlocking possibilities for 3D workflows and virtual worlds.Powered by Discourse, best viewed with JavaScript enabled"
987,what-are-foundation-models,"Originally published at:			What Are Foundation Models? | NVIDIA Blogs
Foundation models are AI neural networks trained on massive unlabeled datasets to handle a wide variety of jobs from translating text to analyzing medical images.Powered by Discourse, best viewed with JavaScript enabled"
988,getting-the-most-out-of-the-nvidia-a100-gpu-with-multi-instance-gpu,"Originally published at:			https://developer.nvidia.com/blog/getting-the-most-out-of-the-a100-gpu-with-multi-instance-gpu/
With the third-generation Tensor Core technology, NVIDIA recently unveiled A100 Tensor Core GPU that delivers unprecedented acceleration at every scale for AI, data analytics, and high-performance computing. Along with the great performance increase over prior generation GPUs comes another groundbreaking innovation, Multi-Instance GPU (MIG). With MIG, each A100 GPU can be partitioned up to seven…I have followed all the instructions  referred in the MIG Manual , however, when I run “sudo nvidia-smi mig -cgi 9,3g.20gb -C”, it turns out to be
Option “-C” is not recognized.
How should I solve this problem?
And without the “-C” option, though I can find the MIGs by “nvidia-smi mig -lgi”, but neither can I get it through “nvidia-smi” nor “ls -l /proc/driver/nvidia/capabilities/gpu1/mig/gi*”
What should I do with this problem?Hi ryy19Option “-C” is not recognized.As mentioned in the software pre-requisites, are you running at least R450.80.02 as the driver version for A100? The “-C” option is only available starting with this driver version.but neither can I get it through “nvidia-smi” nor “ls -l /proc/driver/nvidia/capabilities/gpu1/mig/gi*”Can you please provide more information on what you’re not able to see? MIG devices once created can be accessed either through “nvidia-smi -L” or “nvidia-smi mig -lgi”hi, is there a good way for users without sudo rights to use the MIG functionality? I think running multiple scripts in parallel on the same A100 sounds very interesting, but it needs to work without admin rights (at least after the admin has enabled MIG on the GPU).is there a way to do that?Hi @mikkelsen.kaare - not today. We expect that clusters with A100 GPUs are configured in desired MIG geometries - the configurations can be static (a priori by the infra team) or dynamic (using a systemd service for example as nodes are brought online when used in an autoscaler environment). We have created tooling that can be used for these purposes.Please check this project for a declarative way to create the desired MIG geometries: https://github.com/nvidia/mig-parted and the associated systemd service that can be used in conjunction with provisioning nodes: https://github.com/NVIDIA/mig-parted/tree/master/deployments/systemd. We expect that these tools be used instead of nvidia-smi commands, which can be error prone when used in a production environment. Hope these are useful.Hi,
whether the A100 GPU with Multi-Instance GPU (MIG) allow users to set the application clock (graphic or memory) for a specific GPU instance? Or when we set the application clock via Nvidia-smi, it applies to all instances within the GPU.Hi @kz181, it should apply to all MIG instances, as all MIG instances share a single clock and power limit.is it possible to enumerate multiple MIG compute instances?For example, can I pass the UUIDs of multiple compute instances of MIG as the CUDA_VISIABLE_DEVICES or --gpus for the docker, such that my program or docker container can find those MIG GPU devices and using the cudaSetDevice to index them by number, such as 0,1,2 for three different compute instances?Thanks!Under “single” strategy,  “num_gpus” doesn’t work. It always uses one MIG device.
python tf_cnn_benchmarks.py --num_gpus=2 --batch_size=64 --model=resnet50 --use_fp16Hi,
When multiple users log into the same A100 (linux ssh), how to allocate the MIG GPUs to each so that one user does not step on to other user’s GPU slices? Lets say we use 3g.20GB i.e. each A100 GPU is split into 2 slices. So totally 16 slices are available. There are device ids now uuids. Is there a way to allocate devices to individual users? @maggiez @chetantekurIs MIG meant for only docker containers? Can multiple users ssh directly to the VM and use ?CUDA_VISIBLE_DEVICES, I am not sure whether this could help you.Powered by Discourse, best viewed with JavaScript enabled"
989,nvidia-and-king-s-college-london-announce-monai-ai-framework-for-healthcare-research,"Originally published at:			https://developer.nvidia.com/blog/nvidia-and-kings-college-london-announce-monai-ai-framework-for-healthcare-research/
To help place AI tools in the hands of the world’s leading medical researchers, NVIDIA, in collaboration with King’s College London, introduced MONAI, an open-source AI framework for healthcare research.Powered by Discourse, best viewed with JavaScript enabled"
990,build-3d-virtual-worlds-at-human-scale-with-the-nvidia-omniverse-xr-app,"Originally published at:			https://developer.nvidia.com/blog/build-3d-virtual-worlds-at-human-scale-with-the-nvidia-omniverse-xr-app/
Users can now produce 3D virtual worlds at human scale with the new Omniverse XR App available in beta from the NVIDIA Omniverse launcher.Hi everyones, when, and where, are we gonna be able to test / subscribe to these VR tools ???@weexe
Omniverse XR Beta 2022.01 ReleasedAny word on how/when this can be integrated into Windows Mixed Reality for HP Reverb G2(VR)/ Hololens 2(AR)?  Is an SDK on the way?Powered by Discourse, best viewed with JavaScript enabled"
991,a-guide-to-understanding-essential-speech-ai-terms,"Originally published at:			Speech AI Concepts You Should Know | NVIDIA Technical Blog
NVIDIA Riva facilitates the process of creating ASR services with the tools and methodologies to help you realize your skills, all the way from raw data to a ready-to-use service.Powered by Discourse, best viewed with JavaScript enabled"
992,scale-cancer-genome-sequencing-analysis-and-variant-annotation-using-nvidia-clara-parabricks-3-8,"Originally published at:			https://developer.nvidia.com/blog/scale-cancer-genome-sequencing-analysis-and-variant-annotation-using-nvidia-clara-parabricks-3-8/
Clara Parabricks now includes rapid variant annotation tools, support for tumor-only variant calling in clinical settings, and additional support on ampere GPUs.Powered by Discourse, best viewed with JavaScript enabled"
993,google-develops-an-ar-microscope-to-detect-cancer-in-real-time,"Originally published at:			Google Develops an AR Microscope To Detect Cancer in Real-Time | NVIDIA Technical Blog
Researchers at Google announced this week they developed a deep learning-based model that uses an augmented reality microscope to help physicians diagnose cancer. The researchers say existing light microscopes found in hospitals and clinics around the world can be easily retrofitted with readily-available components, to mirror this prototype. The AR system consists of a modified…Powered by Discourse, best viewed with JavaScript enabled"
994,unlocking-the-promise-of-5g-with-100gb-ethernet-and-dpdk-drivers,"Originally published at:			https://developer.nvidia.com/blog/unlocking-the-promise-of-5g-with-100gb-ethernet-and-dpdk-drivers/
This post was originally published on the Mellanox blog. Wireless carriers have been hyping the next-generation cellular technology of 5G for years but the reality of it is certain to start rolling out this year. Wireless networks are always evolving. but this is more than a cellular upgrade. 5G not only increases speeds but offers…Powered by Discourse, best viewed with JavaScript enabled"
995,rapid-prototyping-on-nvidia-jetson-platforms-with-matlab,"Originally published at:			Rapid Prototyping on NVIDIA Jetson Platforms with MATLAB | NVIDIA Technical Blog
This blog discusses how an application developer can prototype and deploy deep learning algorithms on hardware like the NVIDIA Jetson Nano Developer Kit with MATLAB. In previous posts, we explored how you can design and train deep learning networks in MATLAB and how you can generate optimized CUDA code from your deep learning algorithms.  In…Thank You for sharing this blog! You shared such a piece of grateful information, this really awesome.Powered by Discourse, best viewed with JavaScript enabled"
996,google-ai-algorithm-masters-ancient-game-of-go,"Originally published at:			https://developer.nvidia.com/blog/google-ai-algorithm-masters-ancient-game-of-go/
For the first time, a computer has beaten a human professional at the game of Go — an ancient board game that has long been viewed as one of the greatest challenges for Artificial Intelligence. Google DeepMind’s GPU-accelerated AlphaGo program beat Fan Hui, the European Go champion, five times out of five in tournament conditions.…Powered by Discourse, best viewed with JavaScript enabled"
997,gtc-2020-combating-problems-like-asteroid-detection-climate-change-security-and-disaster-recovery-with-gpu-accelerated-ai,"GTC 2020 S22282
Presenters: Alison Lowndes,NVIDIA
Abstract
This talk will include collaborative work utilizing the power of GPU-accelerated AI with the United Nations, NASA, the European Space Agency, and multiple players in the Earth observation and space industry.Watch this session
Join in the conversation below.Powered by Discourse, best viewed with JavaScript enabled"
998,ai-and-the-summit-gpu-accelerated-supercomputer-helps-identify-extreme-weather-patterns,"Originally published at:			AI and the Summit GPU-Accelerated Supercomputer Helps Identify Extreme Weather Patterns | NVIDIA Technical Blog
According to the National Centers for Environmental Information (NOAA) In 2018, there have been over seven weather and climate disaster events with losses exceeding $1 billion each in the United States. The most recent, Hurricane Florence, has already claimed over 51 lives.   To help identify and predict weather patterns that have the potential to cause…Powered by Discourse, best viewed with JavaScript enabled"
999,building-robotics-applications-using-ros-and-nvidia-isaac-sdk,"Originally published at:			https://developer.nvidia.com/blog/building-collaborative-robotics-using-ros-and-isaac-sdk/
The Robot Operating System (ROS) offers many software libraries and tools to help build robot applications, including framework, algorithms, sensors, and robot platforms. It is extremely popular with roboticists and researchers everywhere. A continuous influx of packages and libraries from the ecosystem makes it easy to get started on robotics projects. Figure 1. Using the…Hi Guys.
I am having hard times locating the files for execution of the commandat my 2020.1 NX installation there is no file with name ros_to_superpixels.
Maybe there is a newer version of the IsaaC SDK that includes it?
Otherwise how to get it installed? Is there a specific file that needs to be created in order to execute the proposed command above? Which exactly file with which content? could you supply the missed file, please?
ThanksHello Andrey,Sorry we forgot to mention: As you have suspected, ros_to_superpixels application will be part of the upcoming release (soon). In the mean time, ros_to_navigation app should be functional in earlier releases including 2020.1. Also, RosToImage codelet released by source shows how to convert ROS images to the Isaac format. Application using RosToImage will be shared with 2020.2.Best,
Oguz@OguzSaglam
Thank you for following up.
Maybe you could suggest quick patch for manual integration of the ros_to_superpixels application  into 2020 nx release? like to copy code and paste it to files?When is this release expected ?2020.2 goes out in a week or so. stay tuned; you should receive an email soon.how do we deploy the app to the jetson?
how do we get it deployed in the released release?Hi Andrey,Looks like there is a typo in the instructions. Please replace “ros_to_superpixel” with “ros_to_superpixels”. Thanks for pointing this out.Best,
OguzFixed the typo!what we can run on Host PC isit works with realsense d435.
now the concern, however is to find the command to deploy it on jetson
could you extend on this, please?
cause there is no deployment file for robot[jetson] as far as I can seeGlad it worked on your host PC Andrey! For Jetson, please see the section on the blog that starts with the following text.Could we have a helping hand getting zed2 to work with ros bridge, please? In a manner similar to the realsense implementation? Somehow? Thanksros_to_superpixels application shows how to receive images from ROS. It doesn’t assume a realsense camera. Isaac also has camera drivers including v4l2, realsense, and Zed. I believe the current implementation of the superpixels requires depth data, but you could run other perception algorithms similarly if your camera doesn’t give depth.Hi Andrey,
ros_to_superpixels requires depth topic from ROS. ROS Zed/Zed2 publishes depth topic in 32FC1 format, which is not currently supported by Isaac ros bridge depth converter. However you can try to update Isaac ros bridge depth converter for 32FC1 or any other format.
Thanks !!Powered by Discourse, best viewed with JavaScript enabled"
1000,new-research-highlights-speed-and-cost-savings-of-clara-parabricks-for-genomic-analyses,"Originally published at:			https://developer.nvidia.com/blog/new-research-highlights-speed-and-cost-savings-of-clara-parabricks-for-genomic-analyses/
Learn about two recent research papers highlighting the speed, accuracy, and cost savings of Clara Parabricks for genomic analyses.Powered by Discourse, best viewed with JavaScript enabled"
1001,reallusion-is-releasing-character-creator-connector-for-nvidia-omniverse,"Originally published at:			Reallusion is Releasing Character Creator Connector for NVIDIA Omniverse | NVIDIA Technical Blog
USD workflow and integration into NVIDIA’s 3D collaboration and simulation platform simplifies digital human creation and animation.Powered by Discourse, best viewed with JavaScript enabled"
1002,nvidia-jetson-tx2-the-new-gold-standard-for-ai-at-the-edge,"Originally published at:			NVIDIA Jetson TX2: The New Gold Standard for AI at the Edge | NVIDIA Technical Blog
The newest member of the Jetson family — Jetson TX2 — offers a comprehensive solution to challenges faced by developers looking to push the boundaries of AI at the edge. The credit card-sized Jetson TX2 is the world’s leading high-performance, low-power embedded platform. It features a 256-core NVIDIA Pascal GPU, a hex-core ARMv8 64-bit CPU…Powered by Discourse, best viewed with JavaScript enabled"
1003,netflix-builds-proof-of-concept-ai-model-to-simplify-subtitles-for-translation,"Originally published at:			Netflix Builds Proof-of-Concept AI Model to Simplify Subtitles for Translation | NVIDIA Technical Blog
To help localize subtitles from English to other languages, such as Russian, Spanish, or Portuguese, Netflix developed a proof-of-concept AI model that can automatically simplify and translate subtitles to multiple languages.Powered by Discourse, best viewed with JavaScript enabled"
1004,leveling-up-cuda-performance-on-wsl2-with-new-enhancements,"Originally published at:			Leveling up CUDA Performance on WSL2 with New Enhancements | NVIDIA Technical Blog
In June 2020, we released the first NVIDIA Display Driver that enabled GPU acceleration in the Windows Subsystem for Linux (WSL) 2 for Windows Insider Program (WIP) Preview users. At that time, it was still an early preview with a limited set of features. A year later, as we have steadily added new capabilities, we…Hi,First, I want to thank all of you for the work you’re doing. It has made so much possible for so many people. I’ve been using RAPIDS with WSL to develop a GPU based ray tracer for astronomical simulations. I’ve had some trouble with share memory, linked below. Do you have any insight into if this is a limitation of WLS, or perhaps the existing memory space can be resized?WSL2 CUDA/CUDF Unable to establish a shared memory space between system and Vram · Issue #7198 · microsoft/WSL (github.com)Any further enhancements in WSL2 cuda performance done (or planned) since this update a year ago. Very much looking forward to using it when it gets closer to native Linux. Thx!Powered by Discourse, best viewed with JavaScript enabled"
1005,assessing-traumatic-brain-injuries-with-deep-learning,"Originally published at:			Assessing Traumatic Brain Injuries with Deep Learning | NVIDIA Technical Blog
More than one million athletes experience a concussion each year in the United States. Researchers from a California-based startup Neural Analytics have designed a portable headset device that maps blood flow in the brain, which may make it easier to recognize concussions. “There is growing evidence that concussions can change the blood flow in the…Powered by Discourse, best viewed with JavaScript enabled"
1006,no-way-to-watch-this,"no posted url to watch this event.Powered by Discourse, best viewed with JavaScript enabled"
1007,autonomous-robot-will-iron-your-clothes,"Originally published at:			Autonomous Robot Will Iron Your Clothes | NVIDIA Technical Blog
Columbia University researchers have created a robotic system that detects wrinkles and then irons the piece of cloth autonomously. Their paper highlights the ironing process is the final step needed in their “pipeline” of a robot picking up a wrinkled shirt, then laying it on the table and lastly, folding the shirt with robotic arms.…Powered by Discourse, best viewed with JavaScript enabled"
1008,5-powerful-new-features-in-cuda-6,"Originally published at:			https://developer.nvidia.com/blog/powerful-new-features-cuda-6/
Today I’m excited to announce the release of CUDA 6, a new version of the CUDA Toolkit that includes some of the most significant new functionality in the history of CUDA. In this brief post I will share with you the most important new features in CUDA 6 and tell you where to get more…This is a beautiful thing; keep innovating and doing the amazing things you guys do at Nvidia, Mark!KUDOS to CUDA team. Yet to try v6. hoping for a unified memory addressing for multiGPU solutions soon from NVIDIA.What about the V-Nova's Perseus codec with Nvidia GPU support?Powered by Discourse, best viewed with JavaScript enabled"
1009,facebook-and-cuda-accelerate-deep-learning-research,"Originally published at:			Facebook and CUDA Accelerate Deep Learning Research | NVIDIA Technical Blog
Last Thursday at the International Conference on Machine Learning (ICML) in New York, Facebook announced a new piece of open source software aimed at streamlining and accelerating deep learning research. The software, named Torchnet, provides developers with a consistent set of widely used  deep learning functions and utilities. Torchnet allows developers to write code in…Powered by Discourse, best viewed with JavaScript enabled"
1010,digits-deep-learning-gpu-training-system,"Originally published at:			https://developer.nvidia.com/blog/digits-deep-learning-gpu-training-system/
The hottest area in machine learning today is Deep Learning, which uses Deep Neural Networks (DNNs) to teach computers to detect recognizable concepts in data. Researchers and industry practitioners are using DNNs in image and video classification, computer vision, speech recognition, natural language processing, and audio recognition, among other applications. The success of DNNs has…awesome workbrilliant!Good job!Any idea where we can find a recording of today's DIGITS webinar?Does anyone know where the AMI for digits is released?Also, I don't see where in digits the test accuracy for models is reported?Hi Michael, the accuracy of the network is posted in real time while training. It will look like what is shown in Figure 5 above.Hi Daniel, Links to the recording and presentation are below,http://on-demand.gputechcon...http://on-demand.gputechcon...Is it possible to see the training progress graphs of an externally created network? I mean, it looks like DIGITS is only for image data networks, but I want to use Caffe for numerical data and I want to use DIGITS just to monitor the learning rate, etc.Any news on EC2 AMI with DIGITS pre-installed ?Unfortunately, it has not been posted.However, you could download the DIGITS installer (http://developer.nvidia.com... onto a AWS AMI to get started now. You will need a g2.2xlarge instance with Ubuntu 14.04.Once the DIGITS AMI is available I will post information on it.Thanks!Right, DIGITS is only for image data at this time. Currently you cannot use the visualization tools in DIGITS for other types of data.Thank you Allison, I tried it (installed it) and discovered it's not possible. I think it would be great to be able to use DIGITS for that kind of use cases. I was able to start creating a custom network and see the visualization of the graph, but nothing else.I have almost the same question. I am running Ubuntu with a virtual machine (i.e. low memory and no GPU support) and I would like to use DIGITS only for classification of images by loading the network that I trained with Caffe under Windows. Do you know if it's possible? Thanks in advance for your help.DIGITS creates the same network files that caffe does and it is easy to load a pretrained network.The last time I tired to just paste in externally created network files, it didn't work. I was missing some of the configuration files that DIGITS creates and uses to load pretrained networks. I did not look into recreating mock versions of the missing files. Outside of this brief attempt some time ago, I haven't spent much time on this. Things may be different now. Please post this question on the DIGITS Users Google forum and someone will get back to you.   https://groups.google.com/f...Thanks for the the reply! I will go on trying DIGITS and post on the forum. Regards. StephaneSo why isn't there any real deep learning work being done for Windows?  Most all the work is pointing toward Linux.  What is the reasoning behind this trend?I have noticed a similar trend when I peruse deep learning sites as well. But Windows can be used for deep learning work. You can build and use Theano. I have it running on my Windows machine. You can use Caffe on Windows too. Here is a really helpful blog post about building it with Visual Studio 2013, https://initialneil.wordpre...Someone else asked a similar question on github. You can find instructions for loading a pretrained network and performing classification on one or many here-  https://github.com/NVIDIA/D...Powered by Discourse, best viewed with JavaScript enabled"
1011,openacc-directives-for-gpus,"Originally published at:			https://developer.nvidia.com/blog/openacc-directives-gpus/
NVIDIA has made a lot of progress with CUDA over the past five years; we estimate that there are over 150,000 CUDA developers, and important science is being accomplished with the help of CUDA. But we have a long way to go to help everyone benefit from GPU computing. There are many programmers who can’t afford the…Powered by Discourse, best viewed with JavaScript enabled"
1012,real-time-epidemic-forecasting-is-now-a-reality,"Originally published at:			Real-Time Epidemic Forecasting is Now a Reality | NVIDIA Technical Blog
An Epidemiologist is using GPUs for faster and more accurate disease forecasting. Chris Jewell, Senior Lecturer in Epidemiology at Lancaster Medical School in the UK, has been focusing his research on livestock epidemics such as foot and mouth disease, theileriosis, and avian influenza. Figure 1: Predicting the risk of infection by the tick-borne cattle parasite…Powered by Discourse, best viewed with JavaScript enabled"
1013,startup-uses-ai-to-identify-crop-diseases-with-superb-accuracy,"Originally published at:			https://developer.nvidia.com/blog/startup-uses-ai-to-identify-crop-diseases-with-superb-accuracy/
Saillog, an Israeli-based startup, developed a mobile application that leverages deep learning to identify over 500 diseases and pest infestations affecting farmers crops. The app can also notify users about which crop diseases and pests have been detected close to their farms. Using NVIDIA TITAN X GPUs with the cuDNN-accelerated TensorFlow deep learning framework, the…Powered by Discourse, best viewed with JavaScript enabled"
1014,create-realistic-synthetic-faces-that-look-older-with-deep-learning,"Originally published at:			https://developer.nvidia.com/blog/create-realistic-synthetic-faces-that-look-older-with-deep-learning/
Developers from Orange Labs in France developed a deep learning system that can quickly make young faces look older, and older faces look younger. A number of techniques already exist, but they are expensive and time consuming. Using CUDA, Tesla K40 GPUs and cuDNN for the deep learning work, they trained their neural network on…Powered by Discourse, best viewed with JavaScript enabled"
1015,researchers-train-ai-to-think-like-a-dog,"Originally published at:			Researchers Train AI to Think Like a Dog | NVIDIA Technical Blog
AI has been trained to drive cars, recognize faces, operate heavy machinery, and even detect disease, but can it act like a dog? Researchers at the University of Washington, in partnership with the Allen Institute for AI, developed a deep learning system that can act and take actions like a dog. The idea of training…Powered by Discourse, best viewed with JavaScript enabled"
1016,jetson-tk1-mobile-embedded-supercomputer-takes-cuda-everywhere,"Originally published at:			https://developer.nvidia.com/blog/jetson-tk1-mobile-embedded-supercomputer-cuda-everywhere/
Today, cars are learning to see pedestrians and road hazards; robots are becoming higher functioning; complex medical diagnostic devices are becoming more portable; and unmanned aircraft are learning to navigate autonomously. As a result, the computational requirements for these devices are increasing exponentially, while their size, weight, and power limits continue to decrease. Aimed at…Do the drivers for this board provide OpenCL support?Not at this time. Jetson TK1 is targeted at embedded CUDA applications.Getting mine today ;-)Excellent!  Let us know how it goes!how much is the maximum output mipi?so i sent email asking for mechanical drawing (dxf or gerber) and drill data so I can make an add-on board utilizing the highdensity board connector and I never got a reply. Is this normal? Should I be  contacting someone specific about this kinda stuff?I think you're looking for the Jetson TK1 mechanical design link found on the support page: https://developer.nvidia.co...If that is not enough, you can try asking specific questions on the support forum here: https://devtalk.nvidia.com/...ya saw that. I'm not a 3d cad engineer, so I don't have any tools to deal with 3d model. all I need is 2d drawing of the PCB outline, mounting holes, and any connector etc position. the documentation didn't even mention connector part number or manufacturer etc.  schematic just shows pin out and nothing else. it's kinda hard to do add-on board without this info. We're working on making the details you're looking for available on the Jetson TK1 support page.  Will probably be a few days, so please check back later!great. my board hasn't shipped yet ( i pre ordered from this site), so hopefully by the time it does, the info will be available.Hopefully I'll be lucky enough to win one. I'm only a senior in high school and can't quite afford one yet. I'd like to see if i'd be able to make this into a mobile hand held solution of some sort, or at the very least a portable solution.  I'm still new to all this but I feel that when I get my hands on one it will help improve my knowledge greatly as i love to learn about new technologies.OK, so few days have passed, the board was shipped and delivered, and still no trace of mechanical drawing/connector positions/board outline/mounting holes dimensions etc.Is there any ETA to when this info will be on the site?gerbers for production board would be sufficient, however if that's not available/under NDA, then just mechanical outline/drill file and maybe top silk or something is all I really need...Trust me, it's in process. The content needs to go through a number of reviews to make sure we aren't posting information which isn't supposed to be public, and to make sure it's in a usable format.>> In addition the following signals are available through an expansion port: DP/LVDS, Touch SPI, 1×4 + 1×1 CSI-2, GPIOs, UART, HSIC, and i2c.How to get this expansion port?The expansion ports are built into the Jetson TK1 board, so they are already present.No OpenCL?!Hi, I want to buy this board to develop an autonomous robot. 1. Assuming that I will only be using an open source tools like OpenCV, may use this robot commercially afterwards (e.g. sell it)? 2. What is the difference between Linux for Tegra and Debian/Ubuntu/CentOS (assuming that L4T is a Linux distribution, not just a driver package)?3. May I install RTLinux kernel patch on it and use it efficiently?4. What kind of OS is installed on the board initially?Best regardsAs one who purchased a Jetson TK1 so I could develop for Tegra K1... and having seen several documents from Nvidia indicating that Tegra K1 would support OpenCL 1.2... this is very disappointing.Try the support page now and see if everything you need is there: https://developer.nvidia.co...Powered by Discourse, best viewed with JavaScript enabled"
1017,accelerating-computational-drug-discovery-with-clara-discovery-from-nvidia-ngc,"Originally published at:			https://developer.nvidia.com/blog/accelerating-computational-drug-discovery-with-clara-discovery-from-nvidia-ngc/
Clara Discovery is a collection of frameworks, applications, and AI models enabling GPU-accelerated computational drug discovery in proteomics, microscopy, virtual screening, computational chemistry, visualization, genomics, clinical imaging, and natural language processing.Powered by Discourse, best viewed with JavaScript enabled"
1018,what-s-new-in-nvidia-ai-enterprise-2-1,"Originally published at:			http://127.0.0.1:8089/whats-new-in-nvidia-ai-enterprise-2-1/
The latest release of the NVIDIA AI software suite includes updated AI frameworks for data science and AI model development, and extended support for AI in the public cloud.Powered by Discourse, best viewed with JavaScript enabled"
1019,pro-tip-improved-glsl-syntax-for-vulkan-descriptorset-indexing,"Originally published at:			Pro Tip: Improved GLSL Syntax for Vulkan DescriptorSet Indexing | NVIDIA Technical Blog
Sometimes the evolution of programming languages creates situations where “simple” tasks take a bit more complexity to express. Syntax annoyance slows down development or can negatively affect readability of code during maintenance. With this in mind, we recently released an open-source sample of a GLSL header generator for DescriptorSet-indexed types in Vulkan. For example, look at…Can object libraries and textures be held in spectroscopy and bump maps for game development?Powered by Discourse, best viewed with JavaScript enabled"
1020,explaining-and-accelerating-machine-learning-for-loan-delinquencies,"Originally published at:			https://developer.nvidia.com/blog/explaining-and-accelerating-machine-learning-for-loan-delinquencies/
Machine learning (ML) can extract deep, complex insights out of data to help make decisions. In many cases, using more advanced models delivers real business value through significantly improving on traditional regression models. Unfortunately, using traditional infrastructure to explain what drove a particular decision with a more advanced model can be difficult, time-consuming, and expensive. Figure…We at NVIDIA hope this post was helpful for you: If you have any questions or comments, please let us know.excellent blog post chaps. Even I understood it.  Very excited about Nvidia in finance and looking to use julia on cuda. Great to have Dr Bennett in Chicago. We used his book to build out our first fpga laptops.We were disappointed that the Chicago Trade show bundled Dr Bennett into a panel discussion. He did a marvelous job BUT 90% of time was spent on meaningless rubbish, Dr Bennett was the only person who seemed to be addressing the future.
well DONE Nvidia for getting Dr Bennett on board. We are happy bunniesdave in ChicagoPowered by Discourse, best viewed with JavaScript enabled"
1021,drive-labs-classifying-traffic-signs-and-traffic-lights-with-signnet-and-lightnet-dnns,"Originally published at:			DRIVE Labs: Classifying Traffic Signs and Traffic Lights with SignNet and LightNet DNNs | NVIDIA Technical Blog
 NVIDIA DRIVE AV software relies on a combination of DNNs to detect and classify traffic signs and lights.Powered by Discourse, best viewed with JavaScript enabled"
1022,discovering-new-features-in-cuda-11-4,"Originally published at:			https://developer.nvidia.com/blog/discovering-new-features-in-cuda-11-4/
NVIDIA announces the newest release of the CUDA development environment, CUDA 11.4. This release includes GPU-accelerated libraries, debugging and optimization tools, programming language enhancements, and a runtime library to build and deploy your application on GPUs across the major CPU architectures: x86, Arm, and POWER. CUDA 11.4 is focused on enhancing the programming model and…Powered by Discourse, best viewed with JavaScript enabled"
1023,gtc-2020-nvidia-jetson-enabling-ai-powered-autonomous-machines-at-scale,"GTC 2020 S22410
Presenters: Amit Goel,NVIDIA
Abstract
Learn about NVIDIA’s Jetson platform for deploying AI at edge for robotics, video analytics, health care, industrial automation, retail, and more. Learn about the key hardware features of the Jetson family, the unified software stack that enables seamless path from development to deployment, and the huge ecosystem that facilitates fast time-to-market. Finally, we’ll cover the latest product announcements, roadmap, and success stories from our partners.Watch this session
Join in the conversation below.Powered by Discourse, best viewed with JavaScript enabled"
1024,gtc-2020-ai-infrastructure-for-real-world-use-cases-from-netapp-and-nvidia-presented-by-netapp,"GTC 2020 S22550
Presenters: David Arnette,NetApp; Sung-Han Lin, NetApp
Abstract
Artificial intelligence (AI), machine learning (ML), and deep learning (DL) are changing the way we get around, the quality of health care we receive, the security of our finances, and the humanity of our digital assistants. The businesses that are attacking these problems are faced not only with the data science challenges, but also with creating the infrastructure for the development and production deployment of these capabilities. In this session, learn about NetApp solutions for AI/ML/DL software development in autonomous vehicles, health care, financial services, and retail use cases. We’ll discuss the specific models, data, and workflows in each use case and the infrastructure requirements to support them. You’ll learn how NetApp reference architectures streamline deployment, operational management, and software development workflows using native integration with industry-standard IT and data science tools.Watch this session
Join in the conversation below.Powered by Discourse, best viewed with JavaScript enabled"
1025,gtc-2020-deep-learning-training-with-tensor-cores-and-automatic-mixed-precision,"GTC 2020 CWE22672
Presenters: Davide Onofrio,NVIDIA; Kushan Ahmadian, NVIDIA; Michael Carilli, NVIDIA; Carl Case, NVIDIA; Nathan Luehr, NVIDIA; Dusan Stosic, NVIDIA; Przemek Tredak, NVIDIA
Abstract
We will discuss techniques for training deep learning models taking advantage of Tensor Cores and Automatic Mixed PrecisionWatch this session
Join in the conversation below.Powered by Discourse, best viewed with JavaScript enabled"
1026,5-things-you-should-know-about-the-new-maxwell-gpu-architecture,"Originally published at:			https://developer.nvidia.com/blog/5-things-you-should-know-about-new-maxwell-gpu-architecture/
[Be sure to check out Maxwell: The Most Advanced CUDA GPU Ever Made, a newer post about the second-generation Maxwell GPU architecture.] The introduction this week of NVIDIA’s first-generation “Maxwell” GPUs is a very exciting moment for GPU computing. These first Maxwell products, such as the GeForce GTX 750 Ti, are based on the GM107 GPU…Thanks for the details! The improved shared memory atomics are a welcome surprise.The links to the Maxwell tuning and compatibility guides do not work.Sorry about that, Steve.  The web cache for our registered developer site seems a little slow to sync up today.  The files should show up any time now; please check again in a bit.Thanks,Cliff WoolleyNVIDIAFYI, the links are still not working.Apologies again for the delay.  The download links have now been fixed.Excuse me for the question here but Nvidia in the future gpus will be use Hevc hardware acceleration? Thanks.Hevc h.265 ,, nvidia quadro k600 have h.264 encoder there must be aprotoyp  h.265 going on...Hi !Now its time for Nvidia to explain which of the Gpus  will carry hevc/H.265 encoder.All quadro got the h.265 encoder but but that i s yesterdeay.You cannot put all your time and Money on the android platforms... and let uswait.So may I ask for a roadmap including h.265 encoder  gpu in desktop platforms...correction nvidia quadro i ment ""all quadro got the h.264 encoder....I am currently writing a textbook on Parallel Computing to be published soon. Both CPU and GPGPU parallelization are covered. Maxwell the latest GPU architecture will be included.Though I am happy that size of the shared memory is back to how it was originally for easy one-to-one correspondence with threadblocks, I am worried about the L1 cache. Has access become slower like the Tex cache or has the Tex cache become faster like the shared memory? I have searched everywhere to determine the new latency of L1 cache in Maxwell archi, but in vain. This surely will have an impact on current codes relying on L1 cache. Hope this will have answers, thanks.Will it always support PCI 2.0Powered by Discourse, best viewed with JavaScript enabled"
1027,nvidia-announces-nsight-systems-2018-3,"Originally published at:			NVIDIA Announces Nsight Systems 2018.3! | NVIDIA Technical Blog
NVIDIA is proud to announce Nsight Systems 2018.3!  In this release, we introduce support for profiling Windows 10 target machines.   For those unfamiliar with Nsight Systems, it is a system-wide performance analysis tool designed to visualize an application’s algorithms, help you identify the largest opportunities to optimize, and tune, to scale efficiently across your…Powered by Discourse, best viewed with JavaScript enabled"
1028,ai-can-transform-anyone-into-a-professional-dancer,"Originally published at:			AI Can Transform Anyone Into a Professional Dancer | NVIDIA Technical Blog
Think of it as style transfer for dancing, a deep learning based algorithm that can convincingly show a real person mirroring the moves of their favorite dancers. The work, developed by a team of researchers from the University of California Berkeley, allows anyone to portray themselves as a world-class ballerina or a pop superstar like…Powered by Discourse, best viewed with JavaScript enabled"
1029,scale-ai-training-using-uber-s-horovod-framework-in-new-dli-course,"Originally published at:			Scale AI Training Using Uber’s Horovod Framework in New DLI Course | NVIDIA Technical Blog
With the immense amounts of data required to train neural networks effectively, it’s critical to know how to leverage the power of multiple GPUs. The NVIDIA Deep Learning Institute (DLI) launched a new online course today in collaboration with Uber called Deep Learning at Scale with Horovod. The course teaches you how to scale deep…Powered by Discourse, best viewed with JavaScript enabled"
1030,nvidia-announces-nsight-graphics-2020-3,"Originally published at:			https://developer.nvidia.com/blog/nvidia-announces-nsight-graphics-2020-3/
Nsight Graphics 2020.3 is now available for download. We’ve added a number of features that dramatically expands the functionality of our tools. Applications that use the provisional VK_KHR_ray_tracing extension can be captured, profiled and exported to a C++ Capture. While the extension is still evolving, the NVIDIA Vulkan Beta Driver will allow for experimentation before…Powered by Discourse, best viewed with JavaScript enabled"
1031,major-companies-in-japan-select-jetson-agx-xavier,"Originally published at:			Major Companies in Japan Select Jetson AGX Xavier | NVIDIA Technical Blog
At GTC Japan in Tokyo,  NVIDIA Founder and CEO Jensen Huang announced that leading Japanese companies FANUC, Komatsu, Musashi Seimitsu, and Kawada Technologies will adopt Jetson AGX Xavier in their next generation autonomous machines. “Japan is driving the world of robotics in numerous industries — from factory automation to construction to manufacturing,” said Deepu Talla,…Powered by Discourse, best viewed with JavaScript enabled"
1032,new-gpu-accelerated-weather-forecasting-system-dramatically-improves-accuracy,"Originally published at:			New GPU-accelerated Weather Forecasting System Dramatically Improves Accuracy | NVIDIA Technical Blog
At CES in Las Vegas, Nevada, The Weather Company, an IBM subsidiary, announced a new GPU-accelerated global weather forecasting system that uses crowdsourced data to deliver hourly weather updates worldwide. The new system named GRAF, Global High-Resolution Atmospheric Forecasting System, can predict something as small as thunderstorms globally.  “Compared to existing models, GRAF will provide…Powered by Discourse, best viewed with JavaScript enabled"
1033,simplifying-ai-inference-in-production-with-nvidia-triton,"Originally published at:			https://developer.nvidia.com/blog/simplifying-ai-inference-in-production-with-triton/
AI machine learning is unlocking breakthrough applications in fields such as online product recommendations, image classification, chatbots, forecasting, and manufacturing quality inspection. There are two parts to AI: training and inference. Inference is the production phase of AI. The trained model and associated code are deployed in the data center or public cloud, or at…Is it possible to use Triton server for real-time object detection and recognition from video?
Or TensorRT or something else is more suitable?have you looked at NVIDIA DeepStream SDK? Triton & TensorRT  are integrated  in it and DeepStream has a variety  of features for real time  object detection from video streams.Develop and deploy AI-powered intelligent video analytics apps and services faster anywhere.Thank you @shankarc !
I missed a notification in my inbox. Yeah, we are converting everything to deepstream now.Powered by Discourse, best viewed with JavaScript enabled"
1034,upcoming-workshop-model-parallelism-building-and-deploying-large-neural-networks-emea,"Originally published at:			Personal Information - Model Parallelism: Building and Deploying Large Neural Networks (EMEA)
Learn how to train the largest of neural networks and deploy them to production.Powered by Discourse, best viewed with JavaScript enabled"
1035,3d-content-interoperability-with-topology-free-modeling,"Originally published at:			https://developer.nvidia.com/blog/3d-content-interoperability-with-topology-free-modeling/
NVIDIA Inception Program member Shapeyard is solving the metaverse 3D content interoperability challenge by automating topology generation in multiple levels of detail at exporting or streaming.Really interesting work being done here by Shapeyard. If people could use ipad and phones for 3d modelling, that would be a game changer.
Does anyone know if there will be a symmetry feature added soon? That would cut creator development time in 1/2.
Thanks,
-ZiaI had a go at this app , it does have potential however the app has a lot of bugs and issues at the moment to fix before I will give it another go.It crashed a lot throughout testing the app, so if you are going to try out this app , save your progress constantly and I mean constantly.The rotation snapping is not 100% perfect all the time as well so you may not get a perfect angle shape. check it from all axis and you will see what I mean. It sometimes snaps at offset angle so you have to judge by eye to correct it. As I said it does have potential but I will wait for updates to fix these issues before trying the app again.Powered by Discourse, best viewed with JavaScript enabled"
1036,gpu-exhaust-error-in-fundamentals-of-deep-learning-workshop-from-deep-learning-institute,"Greetings of the day,Today i attended a wonderful Deep Learning session. While performing its assessment i am facing an error which says “ResourceExhaustedError”. I am trying to resolve this issue for over an hour now. I am attaching a screenshot of the error as well. Due to this error i am facing difficulty completing the assessment on time. If i am will not be able to complete the assessment on time, today before 7:30pm, i may not be able to get certificate.Kindly look into it as soon as possible, and suggest a solution.
Screenshot (61)1920×1080 294 KB
Regards
Aditi VermaHi @aditi21300,Thanks for reaching out. We have the team looking into this issue now. Thanks for your patience.TomHi @aditi21300 , thanks for flagging. The error statements seem to indicate that the GPU has run out of memory (OOM). This can be resolved by clicking the grey circle with a white square in it under the folder icon. This should list all active kernels of the notebooks, and shutting down a few of them should resolve the memory issue.
image966×747 35.9 KB
Hope that helps resolve the issue, please feel free to reach out if not.
All the best,
DaniellePowered by Discourse, best viewed with JavaScript enabled"
1037,metropolis-spotlight-lumeo-simplifies-vision-ai-development,"Originally published at:			https://developer.nvidia.com/blog/metropolis-spotlight-lumeo-simplifies-vision-ai-development/
Over a billion cameras are deployed in the most important spaces worldwide and these cameras are critical sources of video and data. It is becoming increasingly important to understand how to harness this data to make our spaces and processes more efficient and safer. Lumeo, an NVIDIA Metropolis partner, provides a ‘no-code’ video analytics platform…Powered by Discourse, best viewed with JavaScript enabled"
1038,share-your-science-using-deep-learning-to-automatically-detect-geophysical-features,"Originally published at:			https://developer.nvidia.com/blog/share-your-science-using-deep-learning-to-automatically-detect-geophysical-features/
Chiyuan Zhang, PhD student at MIT talks about his joint project with Shell using GPUs and deep learning to automatically detect subsurface faults from seismic traces for oil and gas exploration. Using a Tesla K80 GPU, CUDA, cuBLAS and the cuDNN-accelerated Mocha.jl deep learning framework, the researchers were able to speed-up up their experiments nearly…Powered by Discourse, best viewed with JavaScript enabled"
1039,the-human-genome-center-at-university-of-tokyo-adopts-nvidia-clara-parabricks-for-rapid-genomic-analysis,"Originally published at:			https://developer.nvidia.com/blog/the-human-genome-center-at-university-of-tokyo-adopts-nvidia-clara-parabricks-for-rapid-genomic-analysis/
The Human Genome Center (HGC) at University of Tokyo announced a new genomics platform to accelerate genomic analysis by 40X compared to a CPU-based environment, utilizing NVIDIA Clara Parabricks Pipelines genomics software. The genomic analysis will run on SHIROKANE, HGC’s fastest supercomputer for life sciences in Japan, and powered by NVIDIA DGX A100. The platform…Powered by Discourse, best viewed with JavaScript enabled"
1040,using-gpus-to-accelerate-digital-rock-physics,"Originally published at:			https://developer.nvidia.com/blog/using-gpus-accelerate-digital-rock-physics/
James McClure, a Computational Scientist with Advanced Research Computing at Virginia Tech shares how his group uses the NVIDIA Tesla GPU-accelerated Titan Supercomputer at Oak Ridge National Laboratory to combine mathematical models with 3D visualization to provide insight on how fluids move below the surface of the earth. McClure spoke with us about his research…Powered by Discourse, best viewed with JavaScript enabled"
1041,accelerating-trustworthy-ai-for-credit-risk-management,"Originally published at:			https://developer.nvidia.com/blog/accelerating-trustworthy-ai-for-credit-risk-management-2/
On 21 April 2021, the EU Commission of the European Union issued a proposal for a regulation to harmonise the rules governing the design and marketing of AI systems called the Artificial Intelligence Act (AIA). AI systems are considered to be risky by regulatory bodies. High-risk AI systems will be subject to specific design and…Powered by Discourse, best viewed with JavaScript enabled"
1042,genomic-llms-show-superior-performance-and-generalization-across-diverse-tasks,"Originally published at:			https://developer.nvidia.com/blog/genomic-llms-show-superior-performance-and-generalization-across-diverse-tasks/
A collaboration between InstaDeep, the Technical University of Munich (TUM), and NVIDIA has led to the development of multiple super-computing scale foundation models for genomics. These models demonstrate state-of-the-art performance across many prediction tasks, such as promoter and enhancer site predictions. The joint team of researchers showed that large language models (LLMs) trained on genomics…Powered by Discourse, best viewed with JavaScript enabled"
1043,hello-world-robot-responds-to-human-gestures,"Originally published at:			Hello World! Robot Responds to Human Gestures | NVIDIA Technical Blog
By: Madeleine Waldie, Abhinav Ayalur, Jackson Moffet, and Nikhil Suresh This summer a team of four high school interns, the Neural Ninjas, developed a gesture recognition neural network using Python and C++ to teach a robot to recognize a human wave. Working with robots is familiar territory for them. They’re all members of the FIRST…I’m wondering could this be adapted to react to an ADS (Aim Down Sight) pose? I have an Airsoft arena and I have an idea for a dummy turret that reacts to people ADSing in the waiting area.Powered by Discourse, best viewed with JavaScript enabled"
1044,5g-cloudran-and-edge-ai-end-to-end-system-featuring-nvidia-aerial-sdk-and-egx-platform,"Originally published at:			https://developer.nvidia.com/blog/5g-cloudran-and-edge-ai-end-to-end-system-featuring-nvidia-aerial-sdk-and-egx-platform/
5G CloudRAN is the cloud-native architecture that supports PHY layer processing for high-speed, low bandwidth, software-defined network applications.  The Aerial SDK provides libraries and functions to implement L1 layer processing with LDPC optimization and other features. With O-RAN fronthaul and Aerial SDK PHY capabilities, seamless packet flow can be achieved from edge application to COTS…Powered by Discourse, best viewed with JavaScript enabled"
1045,the-live-ama-is-now-finished,"Thanks for all the great questions.
We still have some responses we will post.
We welcome more questions but the responses will be be offline.
Thanks again .Powered by Discourse, best viewed with JavaScript enabled"
1046,ai-algorithm-for-autonomous-machines-can-predict-human-movement,"Originally published at:			https://developer.nvidia.com/blog/ai-algorithm-for-autonomous-machines-can-predict-human-movement/
University of Michigan researchers recently published a paper describing a new deep learning based-algorithm that can predict the future location of a pedestrian, along with their pose and gait. “The proposed network is able to predict poses and global locations for multiple pedestrians simultaneously for pedestrians up to 45 meters from the cameras,” the researchers stated in their paper.  The…Powered by Discourse, best viewed with JavaScript enabled"
1047,using-rapids-with-pytorch,"Originally published at:			https://developer.nvidia.com/blog/using-rapids-with-pytorch/
This post was originally published on RAPIDSai. In this post we take a look at how to use cuDF, the RAPIDS dataframe library, to do some of the preprocessing steps required to get the mortgage data in a format that PyTorch can process so that we can explore the performance of deep learning on tabular…Powered by Discourse, best viewed with JavaScript enabled"
1048,2016-global-impact-award-nominations-now-open,"Originally published at:			https://developer.nvidia.com/blog/2016-global-impact-award-nominations-now-open/
The $150,000 award will go to a researcher or institution that has used NVIDIA technology to achieve breakthrough results with positive social and humanitarian impact. This includes – but is not limited to – the areas of disease research, drug design & development, medical imaging, energy & fuel efficiency, weather prediction, natural disaster response and…Powered by Discourse, best viewed with JavaScript enabled"
1049,gauss-rank-transformation-is-100x-faster-with-rapids-and-cupy,"Originally published at:			https://developer.nvidia.com/blog/gauss-rank-transformation-is-100x-faster-with-rapids-and-cupy/
As explained in the Batch Normalization paper, training neural networks becomes way easier if its input is Gaussian. This is clear. And if your model inputs are not Gaussian, RAPIDS will just transform it to Gaussian in the blink of an eye. Gauss rank transformation is a novel standardization technique to transform input data for…Powered by Discourse, best viewed with JavaScript enabled"
1050,hpc-visualization-on-nvidia-tesla-gpus,"Originally published at:			https://developer.nvidia.com/blog/hpc-visualization-nvidia-tesla-gpus/
HPC looks very different today than it did when I was a graduate student in the mid-90s. Today’s supercomputers are many orders of magnitude faster than the machines of the 90s, and GPUs have helped push arithmetic performance on several leading systems to stratospheric levels. Unfortunately, the arithmetic performance wrought by two decades of supercomputer…Powered by Discourse, best viewed with JavaScript enabled"
1051,announcing-nvidia-dgx-gh200-the-first-100-terabyte-gpu-memory-system,"Originally published at:			https://developer.nvidia.com/blog/announcing-nvidia-dgx-gh200-first-100-terabyte-gpu-memory-system/
At COMPUTEX 2023, NVIDIA announced NVIDIA DGX GH200, which marks another breakthrough in GPU-accelerated computing to power the most demanding giant AI workloads.Powered by Discourse, best viewed with JavaScript enabled"
1052,omniverse-kaolin-app-now-available-for-3d-deep-learning-researchers,"Originally published at:			NVIDIA Omniverse Kaolin App Now Available for 3D Deep Learning Researchers | NVIDIA Technical Blog
3D deep learning researchers can enter NVIDIA Omniverse and simplify their workflows with the Omniverse Kaolin app, now available in open beta.Powered by Discourse, best viewed with JavaScript enabled"
1053,clara-genomics-analysis-sdk-goes-open-source,"Originally published at:			Clara Genomics Analysis SDK goes open source | NVIDIA Technical Blog
Today, NVIDIA is releasing the Clara Genomics Analysis SDK – an open source toolkit for biological sequence analysis, that is part of Clara Genomics. The last few years have seen a revolution in genome sequencing. New high-throughput sequencing techniques such as those developed by Oxford Nanopore, PacBio and Illumina have led to increases in throughput and quality as well…Powered by Discourse, best viewed with JavaScript enabled"
1054,an-era-of-digital-humans-pushing-the-envelope-of-photorealistic-digital-character-creation,"Originally published at:			https://developer.nvidia.com/blog/an-era-of-digital-humans-pushing-the-envelope-of-photorealistic-digital-character-creation/
The process to create a digital human is extremely labor-intensive and manual. NVIDIA is researching tools and developing ways to accelerate and simplify digital human creation—and we believe that AI and simulation are the key to doing this.Powered by Discourse, best viewed with JavaScript enabled"
1055,share-your-science-gpu-accelerated-3d-oil-painting-simulation,"Originally published at:			Share Your Science: GPU-Accelerated 3D Oil Painting Simulation | NVIDIA Technical Blog
Zhili Chen, a 3D Graphics Researcher from Adobe Research, shares how they are using TITAN X GPUs and CUDA to create a real-time painting system that simulates the interactions between brush, paint, and canvas at the bristle level. He describes how artists can use the system to draw realistic and vivid digital paintings, by applying…Powered by Discourse, best viewed with JavaScript enabled"
1056,new-course-introduction-to-physics-informed-machine-learning-with-modulus,"Originally published at:			Courses – NVIDIA
Learn the basics of physics-informed deep learning and how to use NVIDIA Modulus, the physics machine learning platform, in this self-paced online course.Powered by Discourse, best viewed with JavaScript enabled"
1057,training-like-an-ai-pro-using-nvidia-tao-automl,"Originally published at:			https://developer.nvidia.com/blog/training-like-an-ai-pro-using-tao-automl/
There has been tremendous growth in AI over the years. With that, comes a larger demand for AI models and applications. Creating production-quality AI requires expertise in AI and data science and can still be intimidating for many developers. To develop accurate AI, you must choose what model architecture to use, what data to collect,…The one click deploy file is not a valid link: “Download the one-click deploy tar file and untar the package:”
Could you provide a valid link for tar file? Thanks!@behna.rahimi Can you try again, we have updated the linksPowered by Discourse, best viewed with JavaScript enabled"
1058,accelerating-data-center-ai-with-the-nvidia-converged-accelerator-developer-kit,"Originally published at:			Accelerating Data Center AI with the NVIDIA Converged Accelerator Developer Kit | NVIDIA Technical Blog
Introducing the first GPU+DPU in a single package.Powered by Discourse, best viewed with JavaScript enabled"
1059,the-team-is-live-now-answering-questions-post-your-questions,"Post your questions NOW - the RayTracing team is giving some great responses - great opportunity - don’t miss it.Powered by Discourse, best viewed with JavaScript enabled"
1060,gpu-accelerated-cluster-helps-researchers-simulate-the-spin-of-a-golf-ball,"Originally published at:			GPU-Accelerated Cluster Helps Researchers Simulate the Spin of a Golf Ball | NVIDIA Technical Blog
The professional golfer can hit a golf ball at a speed of 120 to 134 mph, resulting in a spin rate of 2800 to 3500 revolutions per minute, and a distance over 200 yards. To get a better understanding of how the ball performs under different conditions, and to gather data for designing the next…Powered by Discourse, best viewed with JavaScript enabled"
1061,upcoming-event-siggraph-2022,"Originally published at:			SIGGRAPH 2022 Conference | August 8-11 | NVIDIA
Join us at SIGGRAPH Aug. 8-11 to explore how NVIDIA technology is driving innovations in simulation, collaboration, and design across industries.Powered by Discourse, best viewed with JavaScript enabled"
1062,nvidia-to-acquire-mellanox,"Originally published at:			NVIDIA to Acquire Mellanox | NVIDIA Technical Blog
NVIDIA and Mellanox today announced that the companies have reached a definitive agreement under which NVIDIA will acquire Mellanox. The acquisition will unite two of the world’s leading companies in high performance computing (HPC). Together, NVIDIA’s computing platform and Mellanox’s interconnects power over 250 of the world’s TOP500 supercomputers and have as customers every major…Powered by Discourse, best viewed with JavaScript enabled"
1063,different-types-of-edge-computing,"Originally published at:			https://developer.nvidia.com/blog/different-types-of-edge-computing/
The types of edge computing and examples of use cases for each.Powered by Discourse, best viewed with JavaScript enabled"
1064,ushering-in-a-new-era-of-hpc-and-supercomputing-performance-with-dpus,"Originally published at:			https://developer.nvidia.com/blog/ushering-in-a-new-era-of-hpc-and-supercomputing-performance-with-dpus/
Providing powerful computing, high-speed networking and highly programmable engines, BlueField-3 ignites innovation and performance for scientific applications.Powered by Discourse, best viewed with JavaScript enabled"
1065,share-your-science-pushing-the-limits-of-computational-photography,"Originally published at:			Share Your Science: Pushing the Limits of Computational Photography | NVIDIA Technical Blog
Daniel Ambrosi, Artist and Photographer, is using NVIDIA GPUs in the Amazon cloud and CUDA to create giant 2D-stitched HDR panoramas called “Dreamscapes.” Ambrosi applies a modified version of Google’s DeepDream neural net visualization code to his original panoramic landscape images to create truly one-of-a-kind pieces of art. For more information visit Dreamscapes: A Collaboration of Nature, Man, and Machine - Daniel Ambrosi. Share your…Powered by Discourse, best viewed with JavaScript enabled"
1066,predicting-bond-energies-with-ai,"Originally published at:			https://developer.nvidia.com/blog/predicting-bond-energies-with-ai/
University of Notre Dame researchers developed a deep learning-based system that can accurately determine bond energies. “Neural networks can be used to make quantitative models of chemical concepts that are not possible with just quantum mechanics,” explains John Parkhill, Assistant Professor of Chemistry & Biochemistry at the University of Notre Dame in Indiana and co-author…Powered by Discourse, best viewed with JavaScript enabled"
1067,solving-ai-challenges-by-playing-starcraft,"Originally published at:			https://developer.nvidia.com/blog/using-ai-to-solve-collaborative-challenges-by-playing-starcraft/
By Matthew J.A. Smith, Mikayel Samvelyan, Tabish Rashid, University of Oxford Editors note: The story below is a guest post written by current and former postgraduate students at the University of Oxford, a member of the NVIDIA AI Labs (NVAIL) program. If you follow science news, you’ve probably heard about the latest machine-over-man triumph by…Powered by Discourse, best viewed with JavaScript enabled"
1068,titan-rtx-edu-discount-now-available,"Originally published at:			TITAN RTX EDU Discount Now Available | NVIDIA Technical Blog
Data Science is growing fast – it’s emerging as a critical discipline across many industries and it’s foundational to some of the world’s most important research. Today’s students and researchers are working with larger data sets and more demanding workflows. They need systems that enable them to iterate faster and get results sooner. TITAN RTX…Powered by Discourse, best viewed with JavaScript enabled"
1069,personalized-aesthetics-recording-the-visual-mind,"Originally published at:			https://developer.nvidia.com/blog/personalized-aesthetics-recording-the-visual-mind/
Most people in the world own a camera and take photographs—trillions of photos every year. An imminent problem is how to organize, search and re-use our photographs, and deep learning methods have already been successful in such tasks. But visual aesthetics are very personal, often subconscious, and hard to express. In a world with an…Powered by Discourse, best viewed with JavaScript enabled"
1070,breaking-mlperf-training-records-with-nvidia-h100-gpus,"Originally published at:			https://developer.nvidia.com/blog/breaking-mlperf-training-records-with-nvidia-h100-gpus/
In MLPerf Training v3.0, the NVIDIA AI platform powered by the NVIDIA H100 Tensor Core GPU set new performance records.Powered by Discourse, best viewed with JavaScript enabled"
1071,improving-5g-performance-using-ovs-over-asap-with-amd-epyc-7002-and-nvidia-mellanox-smartnics,"Originally published at:			https://developer.nvidia.com/blog/improving-5g-performance-using-ovs-over-asap-with-amd-epyc-7002-and-nvidia-mellanox-smartnics/
Over the last five years, compute and storage technology have achieved substantial performance increases. At the same time, they’ve been hampered by PCI Express Gen3 (PCIe Gen3) bandwidth limitations. AMD is the first X86 processor company to release support for the PCIe fourth generation bus (PCIe Gen4) with the AMD EPYC 7002 Series processor. This…Powered by Discourse, best viewed with JavaScript enabled"
1072,gtc-2020-toward-an-autonomous-and-safe-deep-learning-framework-for-time-series-data,"GTC 2020 S21553
Presenters: Jian Chang,Alibaba Group ; Sanjian Chen,Alibaba Group
Abstract
Global retailers, such as Alibaba, generate hundreds of petabytes of time-series data every day. Making intelligent decisions based upon such time-series data is essential to many business units in Alibaba. We’ll discuss our approach of combining two major themes — “autonomous” and “safe” — in a unified machine-learning framework for time-series data. To improve the accuracy of time-series forecasting and classification, we leverage graph neural networks (GNN) to model the complexity of multi-variant time-series. To improve the computation efficiency, we borrow ideas from Transformer and WaveNet architecture to enable parallel forecast generation. The vulnerability of deep neural networks to adversarial attacks has also generated a lot of attention and discussion on the topic of “safe AI”. We’ll also discuss the importance and challenges in developing “safe” deep-learning models for time-series, and how we can defend against those attacks.Watch this session
Join in the conversation below.Powered by Discourse, best viewed with JavaScript enabled"
1073,nvidia-jetson-agx-xavier-powers-lockheed-martin-and-drone-racing-league-ai-challenge,"Originally published at:			NVIDIA Jetson AGX Xavier Powers Lockheed Martin and Drone Racing League AI Challenge | NVIDIA Technical Blog
Lockheed Martin and Drone Racing League recently announced a $2 million innovation competition, challenging teams to develop an autonomous drone that can race against a pilot operated one. Competing teams will have access to the latest NVIDIA Jetson AGX Xavier modules – designed to make AI-autonomous machines possible – to build their autonomous drones. The…Powered by Discourse, best viewed with JavaScript enabled"
1074,nvidia-deep-learning-sdk-now-available,"Originally published at:			NVIDIA Deep Learning SDK Now Available | NVIDIA Technical Blog
The NVIDIA Deep Learning SDK brings high-performance GPU acceleration to widely used deep learning frameworks such as Caffe, TensorFlow, Theano, and Torch. The powerful suite of tools and libraries are for data scientists to design and deploy deep learning applications. Following the Beta release a few months ago, the production release is now available with:…Powered by Discourse, best viewed with JavaScript enabled"
1075,edge-computing-in-ethiopia-a-quest-for-an-ai-solution,"Originally published at:			https://developer.nvidia.com/blog/edge-computing-in-ethiopia-a-quest-for-an-ai-solution/
This is a guest submitted post by Natnael Kebede, Co-founder and Chief NERD at New Era Research and Development Center It was one year ago in a random conversation that a friend told me about a piece of hardware in excitement. At that point I never imagined how that conversation would have the potential to…Powered by Discourse, best viewed with JavaScript enabled"
1076,this-ai-app-can-help-you-improve-your-jump-shot,"Originally published at:			https://developer.nvidia.com/blog/this-ai-app-can-help-you-improve-your-jump-shot/
Looking to shoot a jump shot like Stephen Curry or Kevin Durant? This AI can help. HomeCourt is a basketball training app that uses deep learning to record, track, and chart shots for basketball players in real time.   NEX Team Inc., the California-based company behind the app, unveiled the product this week and also…Powered by Discourse, best viewed with JavaScript enabled"
1077,upcoming-event-why-gpus-are-important-to-ai,"Originally published at:			NVIDIA Emerging Chapters Education Series
Join us on October 20 to learn how NVIDIA GPUs can dramatically accelerate your machine learning workloads.Powered by Discourse, best viewed with JavaScript enabled"
1078,specular-flickering-with-rtxdi-in-vr,"We are developing a full ray tracing engine for VR, using SDKs like DLSS, RTXDI and NRD. We’ve achieved a reliable 90 FPS for two eyes at 3k x 3k using two RTX 3090 GPUs (one per eye). But there is a problem with RTXDI in combination with VR. In some light conditions there is some flickering/noise of the specular reflections. This is acceptable as long as this flickering is the same in both eyes. But light selection is based on a random generator that uses a seed based on the pixel index, which naturally results in different 3D positions being lit differently between the two (slightly offset) eyes. As a result of this the flickering of specular light is different in both eyes, which can ruin depth perception in VR.Do you have suggestions how we could solve this problem?
Will there be a RTXDI release in the near future suitable for VR?Sounds like an interesting project!If the random number generator is based on pixel index (not the world-space position of the surface at the pixel) and the texels for the left and right eyes are contained in a single texture, then you should be able to remap the pixel indices of the right eye to the equivalent indices of the left eye (or vice versa) before feeding the pixel index to the random number generator.A bit of modulus math to the rescue! e.g. PixelIndexForRNG.xy = PixelIndex.xy % SingleEyeResolution.xyI don’t know specifics about RTXDI’s feature roadmap, but you can ask the RTXDI team by posting in the forum Raytracing - NVIDIA Developer ForumsThanks for the answer.
This modulus math is to prevent float rounding errors result in different random seed?We want for each pixel on the surface that for left and right eye the same light are selected.
But pixels for left and right eye will be slightly different and the the number of pixels for both eyes may even be not the same. So you can not prevent that for one of the eyes some light is choosen which is not choosen for the other eye, Which can result in a flickering on only one eye.
I do not see how to prevent this.This modulus math is to prevent float rounding errors result in different random seed?The modulus math is to remap pixel IDs from a second eye into the first eye’s space - so you get the same seed values for both eyes.for each pixel on the surface that for left and right eye the same light are selectedRight! To accomplish this, you’ll need the random number generator to produce the same results for pixel locations on both eyes. If the pixel resolution differs for each eye, then you won’t be able to use pixel index as a random seed. An idea you could try is to use world-space positions for seed IDs - but you’ll need to quantize world-space into chunks so that both eyes use similar seed IDs.Hi, thank you for doing this AMA! I’m part of this project as well, I’d like to clarify that the problem doesn’t have to do with a large offset like two eyes on a single texture would have. Both eyes have their own individual texture, so they both use pixel coordinates of e.g. [0…3000], but since VR eyes are slightly offset in 3D space, the world positions that the pixels represent tend to differ slightly per eye.This difference means that different seeds are used for the same world positions, which results in different noise on 3D surfaces on the left and right eye, which can be a bit off-putting.I’ve briefly attempted a world-space seed solution like the one you’re suggesting before, the biggest issue was maintaining stability and handling floating point errors with it, but perhaps it deserves another attempt with a closer look at balancing chunk sizesPowered by Discourse, best viewed with JavaScript enabled"
1079,hybridizer-high-performance-c-on-gpus,"Originally published at:			Hybridizer: High-Performance C# on GPUs | NVIDIA Technical Blog
Hybridizer is a compiler from Altimesh that lets you program GPUs and other accelerators from C# code or .NET Assembly. Using decorated symbols to express parallelism, Hybridizer generates source code or binaries optimized for multicore CPUs and GPUs. In this blog post we illustrate the CUDA target. Using parallelization patterns such as Parallel.For, or by…Powered by Discourse, best viewed with JavaScript enabled"
1080,gtc-2020-nvidia-tools-to-train-build-and-deploy-intelligent-vision-applications-at-the-edge,"GTC 2020 S22100
Presenters: Amulya Vishwanath,NVIDIA; Chintan Shah,NVIDIA
Abstract
Learn how to make sense of data ingested from sensors, cameras, and other internet-of-things devices. See how to train with massive datasets and deploy in real time to create a high-throughput, low-latency, end-to-end video analytics pipeline. We’ll show how to optimize your training workflow, use pre-trained models to build applications such as smart parking, infrastructure monitoring, disaster relief, retail analytics or logistics, and more. Get to know the suite of tools available to create, build, and deploy video apps that will gather insights and deliver business efficacy.Watch this session
Join in the conversation below.Powered by Discourse, best viewed with JavaScript enabled"
1081,analyzing-the-rna-sequence-of-1-3m-mouse-brain-cells-with-rapids-on-an-nvidia-dgx-server,"Originally published at:			https://developer.nvidia.com/blog/analyzing-the-rna-sequence-of-1-3m-mouse-brain-cells-with-rapids-on-an-nvidia-dgx-server/
Learn how the use of RAPIDS to accelerate the analysis of single-cell RNA-sequence on a single NVIDIA V100 GPU shows a massive performance increase.Powered by Discourse, best viewed with JavaScript enabled"
1082,develop-deploy-and-distribute-immersive-experiences-with-nvidia-cloudxr-and-amazon-web-services,"Originally published at:			https://developer.nvidia.com/blog/develop-deploy-and-distribute-immersive-experiences-with-cloudxr-and-aws/
Use NVIDIA CloudXR alongside AWS to build immersive XR experiences from the cloud for key advantages at every stage from development to distribution.Powered by Discourse, best viewed with JavaScript enabled"
1083,how-are-the-objects-aligned-in-your-example,"The objects in your demo look perfectly arranged at right angles, was that already part of the ChatGPT response?
Or is that done by the the placement script? If so, how?GPT-4 has a good spatial awareness and can place objects at right positions and even rotations. A great way to have more control on this is to include the rotation angle on X, Y, Z, axis in your request to the LLM as in, for example “include the X,Y,Z rotation for each object” and store that information in the desired result (for example, have the LLM store the info in a JSON variable).This topic was automatically closed 14 days after the last reply. New replies are no longer allowed.Powered by Discourse, best viewed with JavaScript enabled"
1084,improving-performance-for-nfv-infrastructure-and-agile-cloud-data-centers,"Originally published at:			https://developer.nvidia.com/blog/improving-performance-for-nfv-infrastructure-and-agile-cloud-data-centers/
This post was originally published on the Mellanox blog. At Red Hat Summit 2018, NVIDIA Mellanox announced an open network functions virtualization infrastructure (NFVI) and cloud data center solution. The solution combined Red Hat Enterprise Linux cloud software with in-box support of NVIDIA Mellanox NIC hardware. Our close collaboration and joint validation with Red Hat…Powered by Discourse, best viewed with JavaScript enabled"
1085,advanced-deep-learning-q-a,"GTC 2020 cwe22381
Presenters: Davide Onofrio, NVIDIA;  Cliff Woolley, ; Alex Qi, ; Joey Conway, ; Alvaro Garcia, ; Alvaro Garcia, ; Vinh Nguyen, ; Abhishek Sawarkar, ; Maggie Zhang, ; Kaixi Hou, ; Przemek Tredak,
Abstract
We will talk about the latest Deep Learning developments. We will also discuss how to increase training/inference performance by taking advantage of Tensor Cores.Watch this session
Join in the conversation below.Powered by Discourse, best viewed with JavaScript enabled"
1086,a-trio-of-new-nsight-tools-that-empower-developers-to-fully-optimize-their-cpu-and-gpu-performance,"Originally published at:			A Trio of New Nsight Tools That Empower Developers to Fully Optimize their CPU and GPU Performance | NVIDIA Technical Blog
Three big NVIDIA Nsight releases on the same day! NSight Systems is a brand new optimization tool; Nsight Visual Studio Edition 5.6 extends support to Volta GPUs and Win10 RS4; and NSight GRAPHICS 1.2 replaces the current Linux Graphics Debugger. NVIDIA Nsight Systems is a low overhead performance analysis tool designed to provide insights developers…Powered by Discourse, best viewed with JavaScript enabled"
1087,gtc-inception-fireside-chat,"Originally published at:			GTC: Inception Fireside Chat | NVIDIA Technical Blog
Hear from NVIDIA’s co-founder and CEO Jensen Huang in the NVIDIA Inception Fireside Chat, an unplugged conversation around topics ranging from NVIDIA’s strategies, to industry trends and developments. Plus, how NVIDIA supports the entire startup ecosystem. Jeff Herbst, NVIDIA VP of Business Development and Head of Inception, will moderate the panel, which will include questions from…Powered by Discourse, best viewed with JavaScript enabled"
1088,game-creation-with-ue5-omniverse-and-gpt4,"Where I should look for a Job that is gonna accept above workflow by that I mean adaptation USD in workflow as well.The NVIDIA careers place would be a great place to start! Jobs at NVIDIA | NVIDIA CareersThis topic was automatically closed 14 days after the last reply. New replies are no longer allowed.Powered by Discourse, best viewed with JavaScript enabled"
1089,an-efficient-matrix-transpose-in-cuda-fortran,"Originally published at:			https://developer.nvidia.com/blog/efficient-matrix-transpose-cuda-fortran/
CUDA Fortran for Scientists and Engineers shows how high-performance application developers can leverage the power of GPUs using Fortran. My previous CUDA Fortran post covered the mechanics of using shared memory, including static and dynamic allocation. In this post I will show some of the performance gains achievable using shared memory. Specifically, I will optimize a…Why does padding the 1st dimension of the tile variable result in a less-bad shared memory bank conflict? I understand why having a 32x32 element tile results in a 32-way shared memory bank conflict, but I don't understand how adding an extra row fixes it.Because memory is linear. :)  When a warp accesses column 0 of a 32x32 tile, its 32 threads access memory words 0, 32, 64, 96...  Since bank == word index % 32, that means the threads access banks 0, 0, 0, 0... They all access the SAME bank, meaning 32-way bank conflicts.  But if the array is 33x32 (33 columns by 32 rows), they access words 0, 33, 66, 99 == banks 0, 1, 2, 3, ....  They all access DIFFERENT banks.  So it's an easy fix.Powered by Discourse, best viewed with JavaScript enabled"
1090,nvidia-slashes-bert-training-and-inference-times,"Originally published at:			https://developer.nvidia.com/blog/nvidia-slashes-bert-training-and-inference-times/
NVIDIA announced breakthroughs today in language understanding that give developers the opportunity to more naturally develop conversational AI applications using BERT and real-time inference tools, such as TensorRT to dramatically speed up their AI speech applications.  In today’s announcement, researchers and developers from NVIDIA set records in both training and inference of BERT, one of…Powered by Discourse, best viewed with JavaScript enabled"
1091,gpus-for-etl-run-faster-less-costly-workloads-with-nvidia-rapids-accelerator-for-apache-spark-and-databricks,"Originally published at:			GPUs for ETL? Run Faster, Less Costly Workloads with NVIDIA RAPIDS Accelerator for Apache Spark and Databricks | NVIDIA Technical Blog
We were stuck. Really stuck. With a hard delivery deadline looming, our team needed to figure out how to process a complex extract-transform-load (ETL) job on trillions of point-of-sale transaction records in a few hours. The results of this job would feed a series of downstream machine learning (ML) models that would make critical retail…Powered by Discourse, best viewed with JavaScript enabled"
1092,finding-out-where-your-application-and-network-intersect,"Originally published at:			https://developer.nvidia.com/blog/finding-out-where-your-application-and-network-intersect/
NetQ is a scalable, modern network operations toolset that provides network visibility in real-time.Powered by Discourse, best viewed with JavaScript enabled"
1093,nvidia-announces-nsight-systems-2019-6,"Originally published at:			NVIDIA announces Nsight Systems 2019.6 | NVIDIA Technical Blog
NVIDIA Nsight Systems 2019.6 is now available for download. This release expands graphics trace on Windows by adding support for Direct3D 11, WDDM CPU+GPU queues, and OpenGL. On Linux, new features include support for CUDA 10.2, simultaneous CLI sessions, DWARF unwind and capture by hotkey.   Nsight Systems is a system-wide performance analysis tool, designed to…Powered by Discourse, best viewed with JavaScript enabled"
1094,gtc-digital-demo-nvidia-clara-deploy-application-framework-powers-smart-hospitals,"Originally published at:			GTC Digital Demo: NVIDIA Clara Deploy Application Framework Powers Smart Hospitals | NVIDIA Technical Blog
Hospitals are moving to edge solutions for real-time processing and to maintain patient data privacy. This comes at a time when hundreds of AI algorithms are being deployed in smart hospitals for imaging, genomics, and video batch processing. In this new demo released at GTC Digital 2020, we’ll highlight the latest features of the NVIDIA…Powered by Discourse, best viewed with JavaScript enabled"
1095,bolster-network-storage-and-security-infrastructure-services-with-nvidia-doca-1-3,"Originally published at:			https://developer.nvidia.com/blog/bolster-network-storage-and-security-infrastructure-services-with-nvidia-doca-1-3/
The latest release of the NVIDIA DOCA software framework focus on enhancements to DOCA infrastructure services.Powered by Discourse, best viewed with JavaScript enabled"
1096,nvidia-announces-nsight-systems-2019-5,"Originally published at:			NVIDIA announces Nsight Systems 2019.5 | NVIDIA Technical Blog
NVIDIA Nsight Systems 2019.5 is now available for download. This release aims to refine the user experience with CLI sessions for simultaneous usage of more commands, improved GUI timeline zooming levels of detail, enhanced Vulkan API coloring, and Linux GPU context switch trace.  Nsight Systems is a system-wide performance analysis tool, designed to help developers…Powered by Discourse, best viewed with JavaScript enabled"
1097,universal-scene-description-as-the-language-of-the-metaverse,"Originally published at:			Universal Scene Description as the Language of the Metaverse | NVIDIA Technical Blog
Universal Scene Description (USD) is the standard for the next evolution of the Internet, creating portals between 3D worlds.Powered by Discourse, best viewed with JavaScript enabled"
1098,run-multiple-ai-models-on-the-same-gpu-with-amazon-sagemaker-multi-model-endpoints-powered-by-nvidia-triton-inference-server,"Originally published at:			https://developer.nvidia.com/blog/run-multiple-ai-models-on-same-gpu-with-sagemaker-mme-powered-by-triton/
Last November, AWS integrated open-source inference serving software, NVIDIA Triton Inference Server, in Amazon SageMaker. Machine learning (ML) teams can use Amazon SageMaker as a fully managed service to build and deploy ML models at scale. With this integration, data scientists and ML engineers can easily use the NVIDIA Triton multi-framework, high-performance inference serving with…Powered by Discourse, best viewed with JavaScript enabled"
1099,u-s-library-of-congress-processes-over-16-million-historic-newspaper-pages-using-ai,"Originally published at:			U.S. Library of Congress Processes over 16 Million Historic Newspaper Pages Using AI | NVIDIA Technical Blog
Digitizing millions of historical documents and newspapers is a challenging task.  To help speed up the process, the U.S. Library of Congress developed a GPU-accelerated, deep learning model to automatically extract, categorize, and caption over 16 million pages of historic American newspapers published between 1789 and 1963.  The work, which is being made publicly available…Powered by Discourse, best viewed with JavaScript enabled"
1100,building-ai-bridge-to-expand-vision-ai-adoption-to-every-industry,"Originally published at:			Building AI Bridge to Expand Vision AI Adoption to Every Industry | NVIDIA Technical Blog
Thanks to AI Bridge, ISVs can easily connect IVA applications into video management systems.Powered by Discourse, best viewed with JavaScript enabled"
1101,cuda-pro-tip-always-set-the-current-device-to-avoid-multithreading-bugs,"Originally published at:			https://developer.nvidia.com/blog/cuda-pro-tip-always-set-current-device-avoid-multithreading-bugs/
We often say that to reach high performance on GPUs you should expose as much parallelism in your code as possible, and we don’t mean just parallelism within one GPU, but also across multiple GPUs and CPUs. It’s common for high-performance software to parallelize across multiple GPUs by assigning one or more CPU threads to each GPU. In this post I’ll cover a…I had suffered the bug, which always crashs in MPI while receiving message;The most confuse thing is That, it not crash while the buffer to receive is not very large; This lead me to suspect the stability of MPI;And more foolish, about a week before, I suffered another bug, which is an issue of open-mpi related to this more or less;   https://github.com/open-mpi...This issue was tread as a bug of open-mpi, and fixed in later version.After costing days to find the bug, I really hope to saw this post earlier!Is there any measurable performance impact by calling setCudaDevice unnecessarily?  I would hope that if the current device is already 1 then calling setCudaDevice(1) would just exit right away without doing anything significant like talking to the GPU over the PCI bus.  Is that how it actually works?I’d also know about overhead of unnecessarily calling cudaSetDevice().
My problem is that don’t know actually, where the calls re-directed into device 0 as I only used NPP & nvJPEG -libraries.
Now I set a-lot of calls to set device cause don’t know necessarily pints those needed.
Tried with two Quadro 2x00 -series controllers, where that symptom popped out.I have never seen cudaSetDevice be a major limiter in perf.  As far as I know the update is local to the thread only and requires no synchronization.Powered by Discourse, best viewed with JavaScript enabled"
1102,building-a-machine-learning-microservice-with-fastapi,"Originally published at:			Building a Machine Learning Microservice with FastAPI | NVIDIA Technical Blog
Here’s how to use FastAPI to build a production-grade machine learning microservice.Powered by Discourse, best viewed with JavaScript enabled"
1103,accelerating-billion-vector-similarity-searches-with-gpus,"Originally published at:			https://developer.nvidia.com/blog/accelerating-billion-vector-similarity-searches-with-gpus/
Relying on the capabilities of GPUs, a team from Facebook AI Research has developed a faster, more efficient way for AI to run similarity searches. The study, published in IEEE Transactions on Big Data, creates a deep learning algorithm capable of handling and comparing high-dimensional data from media that is notably faster, while just as…Powered by Discourse, best viewed with JavaScript enabled"
1104,this-drone-uses-ai-to-automatically-create-the-perfect-cinematic-shots,"Originally published at:			This Drone Uses AI to Automatically Create the Perfect Cinematic Shots | NVIDIA Technical Blog
Drones are becoming an artist’s best friend, helping amateurs and experienced filmmakers create smooth and aesthetically pleasing videos.  However, the use of drones for cinematography is extremely challenging as it requires a skilled drone operator and a safe trajectory planned in advance. To solve the problem and allow filmmakers more space for creativity, researchers from…Powered by Discourse, best viewed with JavaScript enabled"
1105,drive-labs-how-ai-helps-autonomous-vehicles-understand-intersections,"Originally published at:			https://developer.nvidia.com/blog/drive-labs-how-ai-helps-autonomous-vehicles-understand-intersections/
Intersections are common roadway features, whether four-way stops in a neighborhood or traffic-light-filled exchanges on busy multi-lane thoroughfares.Powered by Discourse, best viewed with JavaScript enabled"
1106,sc20-demo-flow-physics-quantification-in-an-aneurysm-using-nvidia-simnet,"Originally published at:			https://developer.nvidia.com/blog/sc20-demo-flow-physics-quantification-in-an-aneurysm-using-nvidia-simnet/
NVIDIA SimNet is a GPU-accelerated simulation toolkit based on the Physics Informed Neural Networks—or PINNs. At SC19, we showed how SimNet could interactively explore the pressure drop and temperature variations in different heatsink designs to quickly converge on an optimal design.  Since then, the technology behind SimNet has advanced significantly in simulation of  advanced physics…Powered by Discourse, best viewed with JavaScript enabled"
1107,synthesizing-high-resolution-images-with-stylegan2,"Originally published at:			Synthesizing High-Resolution Images with StyleGAN2 | NVIDIA Technical Blog
This new project called StyleGAN2, presented at CVPR 2020, uses transfer learning to produce seemingly infinite numbers of portraits in an infinite variety of painting styles.Powered by Discourse, best viewed with JavaScript enabled"
1108,volkswagen-accelerates-aerodynamics-concept-design-with-nvidia-v100-gpus-on-aws,"Originally published at:			https://developer.nvidia.com/blog/volkswagen-accelerates-aerodynamics-concept-design-with-nvidia-v100-gpus-on-aws/
Designing a fuel-efficient and stylish vehicle is a massive undertaking that requires teams from many parts of a manufacturer to work in unison. To accelerate the process, Volkswagen, Altair, and Amazon recently showed a proof of concept that can more rapidly test the aerodynamics of vehicle designs in simulation. “As vehicle manufacturers seek to increase…Powered by Discourse, best viewed with JavaScript enabled"
1109,deep-learning-for-classifying-hotel-aesthetics-photos,"Originally published at:			Deep Learning for Classifying Hotel Aesthetics Photos | NVIDIA Technical Blog
Most people love to travel. The desire to experience something unfamiliar, such as learning about a new culture or meeting different people, is something that probably all of us have felt in our lives. However, travel means making travel arrangements, which usually includes booking a hotel room. Finding a hotel room is however not always…Very cool project. Does EML require many votes per image to be accurate? I imagine one would need more votes per image for using EML compared to regressing on the means. Eg, how many votes did you collect for your dataset composed of 1k images?It would be interesting to commission a crowd sourced labeling task where you give the annotators pairs of images A & B and they are to make a relative judgement on which image is better for quality and aesthetics. This might have an advantage over your point-wise likert scale style approach as you will probably have less disagreement between annotators: cleaner data.Powered by Discourse, best viewed with JavaScript enabled"
1110,experience-immersive-streaming-with-omniverse-xr-remote,"Originally published at:			https://developer.nvidia.com/blog/experience-immersive-streaming-with-omniverse-xr-remote/
Creators and developers can now view their 3D content as it was meant to be experienced, in full immersive detail with NVIDIA Omniverse XR Remote for iPad.Powered by Discourse, best viewed with JavaScript enabled"
1111,generating-synthetic-data-with-transformers-a-solution-for-enterprise-data-challenges,"Originally published at:			https://developer.nvidia.com/blog/generating-synthetic-data-with-transformers-a-solution-for-enterprise-data-challenges/
Data privacy and availability remain an issue for enterprises. Delve into how synthetic tabular data generated by NeMo addresses these challenges.Powered by Discourse, best viewed with JavaScript enabled"
1112,will-there-be-a-ray-tracing-gems-iii,"Any plans on releasing RTGIII?I’m not aware of plans for an RTG3 (yet), but that doesn’t mean it won’t happen. We’re just still catching our breath since RTG2 😊. If you are interested in contributing to a third volume, let us know.Powered by Discourse, best viewed with JavaScript enabled"
1113,dive-into-the-future-of-streaming-with-nvidia-cloudxr,"Originally published at:			https://developer.nvidia.com/blog/dive-into-the-future-of-streaming-with-nvidia-cloudxr/
Recently, at GTC21, the NVIDIA CloudXR team ran a Connect with Experts session about the CloudXR SDK. We shared how CloudXR can deliver limitless virtual and augmented reality over networks (including 5G) to low cost, low-powered headsets and devices, all while maintaining the high-quality experience traditionally reserved for high-end headsets that are plugged into high-performance…Powered by Discourse, best viewed with JavaScript enabled"
1114,new-translator-provides-more-human-like-translations,"Originally published at:			New Translator Provides More Human-Like Translations | NVIDIA Technical Blog
Germany-based startup DeepL launched a neural machine translator that was preferred by professional (human) translators by a 3-to-1 margin over similar tools. The deep learning company claims “that (DeepL Translator) can boast the world’s most accurate and natural-sounding machine translation tool. When users enter a text, DeepL’s artificial intelligence is able to capture even the…Powered by Discourse, best viewed with JavaScript enabled"
1115,gtc-2019-silicon-valley-preview-media-and-entertainment,"Originally published at:			GTC 2019 Silicon Valley Preview: Media and Entertainment | NVIDIA Technical Blog
NVIDIA’s GPU Technology Conference (GTC) is the premier AI and deep learning conference, providing training, insights, and direct access to media and entertainment experts. To give you a preview of the some of the conference sessions you can attend, we’ve put together a list of some the key sessions in each business area. This video…Powered by Discourse, best viewed with JavaScript enabled"
1116,how-to-get-started-with-optix-7,"Originally published at:			https://developer.nvidia.com/blog/how-to-get-started-with-optix-7/
The evolution of a production-tested high performance ray tracing API Image courtesy of © Dabarti Studio, rendered with V-Ray Next GPU There has been a recent shift in high-performance API design towards providing lower-level control of resource management and execution scheduling. This design evolution allows experienced developers to be in full control of their application…Looks like the template parameters (<t> etc.) are missing in the ""Shader Binding Table"" code snippet above, probably mangled by your CMS and interpreted as HTML tags.Can the curve primitives be utilized to create closed shapes?  For instance, using a series of linear segments to define a hexagon-type curve shape?I want to test your denoiser in NVIDIA-OptiX-SDK-7.3.0-linux64-x86_64/SDK/optixDenoiser. However, I don’t know how to generate some basic inputs like soane-BSDF, soane-Normal and soane-Flow. I have tried to produce them in NVIDIA-OptiX-SDK-7.3.0-linux64-x86_64/SDK/optixSphere, but I don’t even know how to draw one more ball.I modified few lines in optixSphere.cpp as below, but I still can’t see another sphere I add.To sum up, I have two questions. First, how to draw multiple objects (including spheres and triangles) in one scene. Second, how to use optix7 to generate soane-BSDF, soane-Normal and soane-Flow in exr format.My account is fangtiancheng@sjtu.edu.cn or 1773701277@qq.com. I would be appreciated if you could answer my question.Would you please write to more beginner centric technical staff on OptiX? There are few on NVIDIA ON DEMAND, but those are of high level. Also, I think a few video tutorials are really missing in OptiX. After SIGGRAPH 2019 Ingo Wald’s talk, nobody actually made any other video tutorial series.Powered by Discourse, best viewed with JavaScript enabled"
1117,real-time-multiple-moving-objects-tracking-for-video-surveillance,"Originally published at:			https://developer.nvidia.com/blog/real-time-multiple-moving-objects-tracking-for-video-surveillance/
Tracking moving objects in the real-time is a complex. A new paper proposes a multi-object visual color tracking algorithm using multi-threading in real-time using GPUs. A professor from The British University in Egypt presents the integration of a proposed enhanced multi-object color tracking, Partitioned Region Matching (PRM), and Spatial Region Graph (adjacency graph) for real-time…How are you? Thanks for your post. Can you share the presentation of this solution proposal?
My email is xt@go2future.com
Best regards@xt11 – This wasn’t an NVIDIA proposal, but a research paper. Try contacting the researchers…Powered by Discourse, best viewed with JavaScript enabled"
1118,evolving-from-network-simulation-to-data-center-digital-twin,"Originally published at:			https://developer.nvidia.com/blog/evolving-from-network-simulation-to-data-center-digital-twin/
Learn about the evolution and value of data center digital twins, which are attracting increasing attention across industries.Powered by Discourse, best viewed with JavaScript enabled"
1119,gtc-2020-high-throughput-3d-image-reconstruction-visualization-and-segmentation-of-large-scale-data-at-the-sirius-synchrotron-light-source,"GTC 2020 S21278
Presenters: Thiago Vallin Spina,Brazilian Synchrotron Light Laboratory / CNPEM
Abstract
We’ll present highly efficient tools for large-scale 3D image reconstruction, visualization, and segmentation being developed for the Sirius synchrotron light source. Sirius will be the second fourth-generation synchrotron in the world, and will acquire 3D/4D images with resolution up to <50 nm using hard X-rays. With NVIDIA, we’re creating integrated pipelines to reconstruct 3D images from modalities such as coherent-diffraction imaging and transmission tomography, visualize the data in streaming mode, and segment the images to provide almost real-time feedback. We rely on multi-GPU/node CUDA programming and machine/deep learning-optimized inference to address the issues, given that each 3D image may be larger than 100 GB and may be acquired down to 1s, resulting in around 50 TB of data of a wide variety of samples (for example, biological and geological) expected to be produced every day.Watch this session
Join in the conversation below.Powered by Discourse, best viewed with JavaScript enabled"
1120,deploying-nvidia-triton-at-scale-with-mig-and-kubernetes,"Originally published at:			https://developer.nvidia.com/blog/deploying-nvidia-triton-at-scale-with-mig-and-kubernetes/
NVIDIA Triton can manage any number and mix of models (limited by system disk and memory resources). It also supports multiple deep-learning frameworks such as TensorFlow, PyTorch, NVIDIA TensorRT, and so on. This provides flexibility to developers and data scientists, who no longer have to use a specific model framework. NVIDIA Triton is designed to integrate easily with Kubernetes for large-scale deployment in the data center.Powered by Discourse, best viewed with JavaScript enabled"
1121,new-on-ngc-nvidia-maxine-nvidia-tlt-3-0-clara-train-sdk-4-0-pytorch-lightning-and-vyasa-layar,"Originally published at:			https://developer.nvidia.com/blog/new-on-ngc-nvidia-maxine-nvidia-tlt-3-0-clara-train-sdk-4-0-pytorch-lightning-and-vyasa-layar/
The NVIDIA NGC catalog is a hub of highly performant software containers, pre-trained models, industry specific SDKs and Helm charts you can simplify and accelerate your end-to-end workflows.Powered by Discourse, best viewed with JavaScript enabled"
1122,gtc-21-5-data-center-networking-and-ecosystem-sessions-you-shouldn-t-miss,"Originally published at:			https://developer.nvidia.com/blog/gtc-21-5-data-center-networking-and-ecosystem-sessions-you-shouldnt-miss/
We at NVIDIA are on a mission to bring the next generation data center vision to reality. Join us at NVIDIA GTC’21 (Apr 12-16, 2021) to witness the data center innovation we are pioneering.Powered by Discourse, best viewed with JavaScript enabled"
1123,meet-the-researcher-dr-emanuel-gull-theoretical-and-computational-condensed-matter-physics,"Originally published at:			https://developer.nvidia.com/blog/meet-the-researcher-dr-emanuel-gull-theoretical-and-computational-condensed-matter-physics/
‘Meet the Researcher’ is a series in which we spotlight different researchers in academia who use NVIDIA technologies to accelerate their work.  This month we spotlight Dr. Emanuel Gull, Associate Professor of Physics at University of Michigan, whose research focuses on the development of theoretical and computational methods for strongly correlated quantum systems.  Gull is…Powered by Discourse, best viewed with JavaScript enabled"
1124,get3d-takes-3d-cad-data-in-training,"Can GET3D takes 3D CAD data in training and generate new 3D CAD models?Can GET3D directly take 3D CAD data for training?
In 3D CAD (computer aided design)  software industry, there are many finished mechanical models. The ideas to use those existing 3D models to training GET3D and then generate new 3D models.Assuming that you have triangulated 3D CAD models in mind, the answer is yes. Get3D could be trained on a dataset of 3D CAD models and then used to generate new ones.Yes, I have triangulated 3D CAD models for training input.
Is there any example/showcase for this application? Thanks.You could follow the examples on how to train Get3D from our Github repository (GitHub - nv-tlabs/GET3D). In principle, you would need to: i) render images of your models in a rendering engine (e.g. Omnvierse or Blender), and then ii) use those images to train Get3D. We provide instructions and example scripts for both in our repository.Do you mean I need to convert 3D CAD models to images and rendering them for GET3D training input? Can I directly feed 3D CAD data (triangulates) to GET3D for training?An example of my use case is like I have a separate 3D car model and a street model. I want to generate a model with the car (or two cars) on the street.Powered by Discourse, best viewed with JavaScript enabled"
1125,exploring-the-new-features-of-cuda-11-3,"Originally published at:			Exploring the New Features of CUDA 11.3 | NVIDIA Technical Blog
CUDA is the software development platform for building GPU-accelerated applications, providing all the components you need to develop applications that use NVIDIA GPUs. CUDA is ideal for diverse workloads from high performance computing, data science analytics, and AI applications. The latest release, CUDA 11.3, and its features are focused on enhancing the programming model and…CUDA 11.3 significantly improves the performance of Ampere/Turing/Volta Tensor Core kernels.298TFLOPS was recorded on A100 when benchmarking FP16 GEMM from CUTLASS, an open source CUDA DL/HPC library (GitHub - NVIDIA/cutlass: CUDA Templates for Linear Algebra Subroutines).  This is 14% higher than CUDA 11.2. FP32(via TF32) GEMM is improved by 39% and can reach 143TFLOPS. The same speedup applies to the CONV kernels.Also, see the discussion here: CUDA 11.3 significantly improved the performance of CUTLASS · Discussion #241 · NVIDIA/cutlass · GitHubMetrics and Performances of Cuda 11.3 made it easy for me to buy GE RTX 3070 for my new desktop…But your website is too confusing for a seamless installation of the drivers, toolkit and CuDNN…I have done cleanup twenty times, and still to see one instance of my model training using the GPU…Powered by Discourse, best viewed with JavaScript enabled"
1126,case-study-resnet50-with-dali,"Originally published at:			https://developer.nvidia.com/blog/case-study-resnet50-dali/
Let’s imagine a situation. You buy a brand-new, cutting-edge, Volta-powered DGX-2 server. You’ve done your math right, expecting a 2x performance increase in ResNet50 training over the DGX-1 you had before. You plug it into your rack cabinet and run the training. That’s when an unpleasant surprise pops up. Even though your math is correct,…Powered by Discourse, best viewed with JavaScript enabled"
1127,gtc-2020-intelligent-end-to-end-ai-chatbot-with-audio-driven-facial-animation,"GTC 2020 D2S39
Presenters: Tech Demo Team,NVIDIA
Abstract
During the GTC 2020 virtual keynote, NVIDIA CEO and founder Jensen Huang interacted with Misty, a conversational AI weather chatbot, to demonstrate an end-to-end pipeline to create an AI driven 3D digital avatar. The NVIDIA Jarvis multimodal application framework includes pre-trained conversational AI models, and optimized end-to-end services for speech, vision, and natural language understanding (NLU) tasks. It also includes Audio2Face, an audio driven AI-based technology, to automatically create the real-time facial animation from the synthesized speech of Jarvis. All of this is rendered in real-time with NVIDIA Omniverse, a powerful, multi-GPU, real-time simulation and collaboration platform for 3D production pipelines.

Learn more about NVIDIA Jarvis
Learn more about NVIDIA Omniverse

Watch the entire GTC 2020 keynoteWatch this session
Join in the conversation below.HeyPowered by Discourse, best viewed with JavaScript enabled"
1128,cuda-spotlight-gpu-accelerated-agent-based-simulation-of-complex-systems,"Originally published at:			https://developer.nvidia.com/blog/cuda-spotlight-gpu-accelerated-agent-based-simulation-complex-systems/
This week’s Spotlight is on Dr. Paul Richmond, a Vice Chancellor’s Research Fellow at the University of Sheffield (a CUDA Research Center). Paul’s research interests relate to the simulation of complex systems and to parallel computer hardware. NVIDIA: Paul, tell us about FLAME GPU. Paul: Agent-Based Simulation is a powerful technique used to assess and…Powered by Discourse, best viewed with JavaScript enabled"
1129,ai-algorithm-monitors-sleep-with-radio-waves,"Originally published at:			https://developer.nvidia.com/blog/ai-algorithm-trained-monitors-sleep-with-radio-waves/
To help diagnose and monitor people with sleep disorders, researchers from MIT and Massachusetts General Hospital developed a device that uses an advanced artificial intelligence algorithm to analyze the radio signals around the person and translate those measurements into sleep stages: light, deep, or rapid eye movement (REM). “Imagine if your Wi-Fi router knows when…Powered by Discourse, best viewed with JavaScript enabled"
1130,welcome-back-parallel-forall,"Originally published at:			https://developer.nvidia.com/blog/welcome-back-parallel-forall/
Well, that was embarrassing. Ever since NVIDIA Developer Zone was compromised by hackers  in July, most of the DevZone content has been offline while NVIDIA’s crack team of web infrastructure and security engineers improve security. Unfortunately for me and you, dear reader, Parallel Forall was sidelined. But we haven’t been resting on our laurels! On…Powered by Discourse, best viewed with JavaScript enabled"
1131,new-machine-learning-model-taps-into-the-problem-solving-potential-of-satellite-data,"Originally published at:			https://developer.nvidia.com/blog/new-machine-learning-model-taps-into-the-problem-solving-potential-of-satellite-data/
New research creates a low-cost and easy-to-use machine learning model to analyze streams of data from earth-imaging satellites.Powered by Discourse, best viewed with JavaScript enabled"
1132,preparing-state-of-the-art-models-for-classification-and-object-detection-with-the-nvidia-transfer-learning-toolkit,"Originally published at:			https://developer.nvidia.com/blog/preparing-state-of-the-art-models-for-classification-and-object-detection-with-tlt/
Accuracy is one of the most important metrics for deep learning models. Greater accuracy is a prerequisite for deploying the trained models to production to solve real-world problems reliably and effectively. Creating highly accurate models from scratch is time-consuming and capital-intensive. As a result, companies with limited data and resources struggle to get their AI…Powered by Discourse, best viewed with JavaScript enabled"
1133,nvidia-backs-data-science-bowl-to-fight-heart-disease,"Originally published at:			NVIDIA Backs Data Science Bowl to Fight Heart Disease | NVIDIA Technical Blog
Doctors diagnose someone with Cardiovascular disease every 43 seconds so finding new and better ways to speed the diagnosis of heart disease couldn’t be more important. In collaboration with Booz Allen Hamilton, Kaggle and the National Institutes of Health, NVIDIA is pleased to support the second annual Data Science Bowl competition. During the 90-day competition, teams…Powered by Discourse, best viewed with JavaScript enabled"
1134,how-nvlink-will-enable-faster-easier-multi-gpu-computing,"Originally published at:			https://developer.nvidia.com/blog/how-nvlink-will-enable-faster-easier-multi-gpu-computing/
Accelerated systems have become the new standard for high performance computing (HPC) as GPUs continue to raise the bar for both performance and energy efficiency.  In 2012, Oak Ridge National Laboratory announced what was to become the world’s fastest supercomputer, Titan, equipped with one NVIDIA® GPU per CPU – over 18 thousand GPU accelerators.  Titan…Hi,I have a query regarding “NVLink Signaling and Protocol Technology” and particularly regarding Atomic operations via NVLink.“The protocol uses a variable length packet with packet sizes ranging from 1 (simple read request command for example) to 18 (write request with data for 256B data transfer with address extension) flits” (https://images.nvidia.com/content/pdf/tesla/whitepaper/pascal-architecture-whitepaper.pdf - Page 35)From the above text I presume NVLink uses the same protocol to perform an atomic-operation on a peer GPU. I could not find any whitepaper or reference that describes the protocol or packets’ in detail. Could you please confirm if my assumption is true? If not then could you please shed some light on how NVLink performs an atomic-operation?Thank you in advance.Hello,
I am very interested in learning and incorporate the advantage(s) of NVLINK within the context of multi GPU applications and algorithms. Very recently I have build my first deep learning workstation with two Geforce RTX 2080TI and hopefully will upgrade to three or four GPUS down the road. I started implementing and benchmarking  platforms (Resnet, CNNs etc) and have noticed that when the two GPUS are connected via an NVLINK bridge, the two GPUs are considered as one device only when i import mult_gpu_model(model, num_gpu) from keras.tensorflow models, execution of the code is halted with an error due to only one gpu is available , not two.
So my question here is how can I  go from here and why the system “sees” only one device , not two?.  The NVIDIA platforms sadly dont offer much support regarding this, and for the most part, expand on multi gpus without NVLINK methodologies. Furthermore, the nvlink-smi displays the two gpus and as such, one would conclude that both devices should be visible to the system when executing the CNN models from tensorflow and keras. I definitely would like to pursue this matter and expose the reason(s) . HEre is the details for my platform: WSL2  ubuntu 20.04 within windows10, AMD threadripper 1950x CPU, 128G DDR4 RAM, Jupyter Notebook with tensorflow 2.3.0 , python 3.7.8
thank you,
George JubranAre we talking the release of this along with Pascal sometime in 2016?I would love to hear more on who and what are going to be ""behind"" this particular hardware.  This is something that warranties a possible revamp of the DeskStar!!So, Pascal card, it looks to be compatible with Pci-e slots (8? 16x?) but limited about 25% compare to using it in a NVLink. To say then, MBoards need updating and while it is stated 'servers' will best be accommodated, I've no idea what chipset design will be necessary to handle a true NVLink for the Pascal Vid card.Consumer level.....?......? When....???Generally, HBM2 memory living on a graphics card should be accessible by the CPU and even unified. At least, it should be possible with the new cards in 2016, what with their estimated 32GB of HBM2 memory, to dispense with traditional motherboard system ram and allow the processor to directly access the graphics card memory as its own with all the advantages of the significantly increased bandwidth, where perhaps DDR4 / 5 mobo memory acts as a swap out before hitting the disk swap file.  NVLink does not say anything about this.NVlink is proprietary, much like G-Sync, this is utterly unattractive. Purely an interim solution before an open standard hits the shelves. This also paints you into a corner with regard to upgradability and component choice, whilst coupling you to NVLink enabled hardware carrying the NVidia price premium.Bad times. All this amounts to an attack on AMD and indeed, Mobo manufactures et'al. Just as is the case with G-Sync, NVidia hope to wreak havoc on the standards based proven model currently in place by imposing costly intellectual property licencing and hardware costs upon anyone who wishes to utilise NVlink, this includes AMD. Since if they want to compete, they will simply have no choice. Very bad times indeed, boycott this proprietary IP which is terrible for innovation and competition.Freesync and PCIe 4   are where we want and need to be in 2016 onward.If NVidia really care about the PC industry, they could merge the PCIe4 spec with an enhanced option which utilities the NVLink IP without licensing costs. So named, PCIe4-enhanced.Storm Lake is for communication between nodes, so it competes with Infiniband.  NVLink is for communication within a node.  It would be helpful to both Nvidia and AMD if AMD includes NVLink on their CPUs and GPUs.  Otherwise, AMD will have to develop AMDLink and there will be no way to connect an Nvidia GPU to an x86 CPU with this type of interconnect.Now is a good time for disruption in the personal computer market because a lot of people are unhappy with Windows 10.  I hope Nvidia has some of their software engineers contributing to Linux desktop projects because that will increase the market for Nvidia's CPU.  In addition to that, I bet there is a startup developing a new commercial operating system for desktops and notebooks.  Nvidia should help fund them.A CPU with NVLink will be great.  Nvidia should license the Mill CPU or acquire the company Mill Computing Inc.  The Mill CPU is a truly impressive new CPU design.  When only one thread is being used, Intel's Haswell-E runs at its turbo frequency of 3.6 GHz.  A good target for Nvidia's CPU would be at least 1.5x or 2x of Intel's single-thread SPECint performance.At this time, NVLink is used only for GPU-to-GPU communication, with the only exception of IBM Power processors. At the consumer level you are unlikely to buy multiple Pascal GPUs per host, and x86 machines don't ""speak"" NVLink, so you won't have access to NVLink.Som'bitch......  Thank you though....Powered by Discourse, best viewed with JavaScript enabled"
1135,designworks-siggraph-2017-announcements,"Originally published at:			DesignWorks SIGGRAPH 2017 Announcements | NVIDIA Technical Blog
Get the latest SDK and tools designed for developers creating professional graphics, advanced rendering, video processing, material design, and 3D printing. We are excited to share a comprehensive update of our innovative and industry leading SDKs and tools for the DesignWorks SIGGRAPH 2017 release. New SDK announcements OptiX 5.0 MDL 2017.1 360 Video (Stereo Stitching)…Powered by Discourse, best viewed with JavaScript enabled"
1136,nvidia-researchers-to-present-groundbreaking-ai-projects-at-eccv-2018,"Originally published at:			https://developer.nvidia.com/blog/nvidia-researchers-to-present-groundbreaking-ai-projects-at-eccv-2018/
NVIDIA Researchers will present 17 accepted papers and posters, one of them an oral, at the biennial European Conference on Computer Vision (ECCV) on September 8-14 in Munich, Germany. Orals ContextVP: Fully Context-Aware Video Prediction Wonmin Byeon, Qin Wang, Rupesh Kumar Srivastava, and Petros Koumoutsakos September 13, 02:45 PM Abstract: Video prediction models based on…Powered by Discourse, best viewed with JavaScript enabled"
1137,gtc-2020-bringing-ai-to-the-classroom-nvidias-deep-learning-teaching-kit,"GTC 2020 S22357
Presenters: Joseph Bungo ,NVIDIA ; Pawel Morkisz,AGH University of Science and Technology
Abstract
The call for AI and deep-learning skills is soaring, and university classrooms are on the front lines of feeding the demand. NVIDIA Teaching Kits lower the barrier of incorporating AI and GPU computing in coursework. NVIDIA’s higher-education leadership and Pawel Morkisz, assistant professor of mathematics at AGH University of Science and Technology in Krakow, will discuss the Deep Learning Teaching Kit, co-developed with Professor Yann LeCun and his team at New York University. The kit was a starting point for preparing materials for the course dedicated for postgraduate students of mathematics. Using NVIDIA’s materials saved many days of work preparing lecture slides and source-level coding projects/solutions.Watch this session
Join in the conversation below.Powered by Discourse, best viewed with JavaScript enabled"
1138,accelerating-climate-change-mitigation-with-machine-learning-the-case-of-carbon-storage,"Originally published at:			https://developer.nvidia.com/blog/accelerating-climate-change-mitigation-with-machine-learning-the-case-of-carbon-storage/
We present a new generation of neural operators, named U-FNO, that empowers a novel technology for solving multiphase flow problems with superior accuracy, speed, and data efficiency.Powered by Discourse, best viewed with JavaScript enabled"
1139,gtc-2020-high-throughput-real-time-data-processing-with-gpus-at-cern,"GTC 2020 S21341
Presenters: Daniel Hugo Cámpora Pérez,NIKHEF and University of Maastricht
Abstract
We’ll present the design and performance considerations and system optimization of a GPU-based, real-time physics selection system at a Large Hadron Collider experiment. Millions of particles collide every second at the LHCb experiment at CERN in Switzerland. To select interesting particle collisions, data must pass through an acquisition system and be filtered with real-time selection software. The throughput processed in the first stage of this streaming data processing application amounts to 40 terabytes per second, and the efficiency of the selection is crucial toward improving our fundamental understanding of the universe. In order to process this massive data throughput in real-time, we developed GPU physics reconstruction software called Allen. The Allen framework hands the raw data to GPU streams, which perform the decoding, reconstruction, and selection of particle collisions.Watch this session
Join in the conversation below.Powered by Discourse, best viewed with JavaScript enabled"
1140,nvidia-announces-jetson-nano-nvidia-cuda-x-ai-computer-that-runs-all-ai-models,"Originally published at:			﻿NVIDIA Announces Jetson Nano: NVIDIA CUDA-X AI Computer That Runs All AI Models | NVIDIA Technical Blog
NVIDIA today announced the Jetson Nano, an AI computer that makes it possible to create millions of intelligent systems. The small but powerful CUDA-X AI computer delivers 472 GFLOPS of compute performance for running modern AI workloads and is highly power -efficient, consuming as little as 5 watts. Jetson Nano supports high-resolution sensors, can process…Powered by Discourse, best viewed with JavaScript enabled"
1141,mocha-jl-deep-learning-for-julia,"Originally published at:			https://developer.nvidia.com/blog/mocha-jl-deep-learning-julia/
Deep learning is becoming extremely popular due to several breakthroughs in various well-known tasks in artificial intelligence. For example, at the ImageNet Large Scale Visual Recognition Challenge, the introduction of deep learning algorithms into the challenge reduced the top-5 error by 10% in 2012. Every year since then, deep learning models have dominated the challenges,…This is beautiful.  Is CUDA for Julia available now?  I thought CUDA is only available for C/C++, Python, and Fortran according to this:  https://developer.nvidia.co...Because Julia has a foreign function interface, you can write and compile libraries that use CUDA in C/C++, and then call the libraries from Julia. Mocha.jl is written this way, but using Mocha.jl does not require you to write any CUDA kernels yourself.You could also use this package https://github.com/JuliaGPU... as an easy wrapper to CUDA APIA list of related projects can be found here: https://github.com/JuliaGPUIs there a C++ binding for this?Powered by Discourse, best viewed with JavaScript enabled"
1142,upcoming-event-level-up-with-nvidia-dlss-dlaa-and-nvidia-image-scaling-in-unreal-engine-5,"Originally published at:			Level Up with NVIDIA
Join us for the second episode of our webinar series, Level Up with NVIDIA. You learn how to use the latest NVIDIA RTX technology in Unreal Engine 5, followed by a live Q&A session where you can ask NVIDIA experts about your game integrations. Powered by Discourse, best viewed with JavaScript enabled"
1143,developing-smart-city-traffic-management-systems-with-openusd-and-synthetic-data,"Originally published at:			https://developer.nvidia.com/blog/developing-smart-city-traffic-management-systems-with-openusd-and-synthetic-data/
End-to-end AI engineering company SmartCow, an NVIDIA Metropolis partner, has created digital twins of traffic scenarios on NVIDIA Omniverse.Powered by Discourse, best viewed with JavaScript enabled"
1144,how-to-get-better-outputs-from-your-large-language-model,"Originally published at:			https://developer.nvidia.com/blog/how-to-get-better-outputs-from-your-large-language-model/
Large language models (LLMs) have generated excitement worldwide due to their ability to understand and process human language at a scale that is unprecedented. It has transformed the way that we interact with technology. Having been trained on a vast corpus of text, LLMs can manipulate and generate text for a wide variety of applications…Powered by Discourse, best viewed with JavaScript enabled"
1145,ai-turns-ui-designs-into-code,"Originally published at:			AI Turns UI Designs Into Code | NVIDIA Technical Blog
Copenhagen-based startup UIzard Technologies trained a neural network to automatically generate code from a graphical user interface screenshot. Currently, a UI designer mocks up the interface and then the front-end developer takes the design and translates it into code – but, what if an AI system can do this for you? The company’s slogan is…Powered by Discourse, best viewed with JavaScript enabled"
1146,upcoming-event-building-and-running-an-end-to-end-machine-learning-workflow-5x-faster,"Originally published at:			https://info.nvidia.com/building-a-machine-learning-app-webinar.html
Join NVIDIA and Google Cloud for a live webinar on May 25 to learn how to build a machine learning application to predict bike rental durations 5X faster.Powered by Discourse, best viewed with JavaScript enabled"
1147,ai-and-drones-help-farmers-detect-crop-needs,"Originally published at:			https://developer.nvidia.com/blog/ai-and-drones-help-farmers-detect-crop-needs/
Researchers from the University of South Australia recently developed a deep learning system that uses drones to detect areas in agricultural land that require additional irrigation or fertilizers. The system allows farmers to precisely plan how much water and nutrients they will need on a given day. The method also has the potential to drastically…Powered by Discourse, best viewed with JavaScript enabled"
1148,new-course-introduction-to-robotic-simulations-in-isaac-sim,"Originally published at:			Courses – NVIDIA
Learn how to use NVIDIA Isaac Sim to tap into the simulation loop of a 3D engine and initialize experiments with objects, robots, and physics logic.Powered by Discourse, best viewed with JavaScript enabled"
1149,ray-tracing-integration-in-substance,"Originally published at:			https://developer.nvidia.com/blog/ray-tracing-integration-in-substance/
We recently caught up with the Substance team about ray tracing integration in Substance. When you first learned that NVIDIA was bringing real-time ray tracing to consumer grade GPUs, what were your team’s initial thoughts? When the RTX initiative was disclosed to Sébastien Deguy (founder of Substance), his reaction was it was the biggest revolution…Powered by Discourse, best viewed with JavaScript enabled"
1150,rapidly-build-ai-streaming-apps-with-python-and-c,"Originally published at:			https://developer.nvidia.com/blog/rapidly-build-ai-streaming-apps-with-python-and-c/
The computational needs for AI processing of sensor streams at the edge are increasingly demanding. Edge devices must keep up with high rates of incoming data streams, processing, displaying, archiving, and streaming results or closing a control loop in real time. This requires powerful, efficient, and accurate hardware and software solutions capable of high performance…Powered by Discourse, best viewed with JavaScript enabled"
1151,how-edge-computing-is-transforming-healthcare,"Originally published at:			https://developer.nvidia.com/blog/healthcare-at-the-edge/
Forward-thinking healthcare organizations are adopting edge computing.Powered by Discourse, best viewed with JavaScript enabled"
1152,better-together-accelerating-ai-model-development-with-lexset-synthetic-data-and-nvidia-tao,"Originally published at:			https://developer.nvidia.com/blog/better-together-accelerating-ai-model-development-with-lexset-synthetic-data-and-nvidia-tao/
Train highly accurate computer vision models with Lexset synthetic data and the NVIDIA TAO Toolkit.Powered by Discourse, best viewed with JavaScript enabled"
1153,tuning-ai-infrastructure-performance-with-mlperf-hpc-v2-0-benchmarks,"Originally published at:			https://developer.nvidia.com/blog/tuning-ai-infrastructure-performance-with-mlperf-hpc-v2-0-benchmarks/
Discover how you can tune AI infrastructure performance with MLPerf HPC v2.0 benchmarks.Powered by Discourse, best viewed with JavaScript enabled"
1154,cuda-pro-tip-write-flexible-kernels-with-grid-stride-loops,"Originally published at:			https://developer.nvidia.com/blog/cuda-pro-tip-write-flexible-kernels-grid-stride-loops/
One of the most common tasks in CUDA programming is to parallelize a loop using a kernel. As an example, let’s use our old friend SAXPY. Here’s the basic sequential implementation, which uses a for loop. To efficiently parallelize this, we need to launch enough threads to fully utilize the GPU. void saxpy(int n, float a,…Thank you! I use this pattern everywhere now.Thank you for the article. I always learn a lot from them.Mark,When running loops of such large size, do we need to copy arrays from global to shared memory to speed calculations up? I have a B x T array in global memory totaling 1.4 GB in size. I need to take chunks of 1 x T size from it and perform a convolution with another 1 x T array taken from a lookup table 16 MB in size also residing in global memory. It makes sense to use T threads to perform the convolution since the calculations don't overlap but somehow I am getting inconsistent results (sometimes it works and sometimes it does not). I will learn you technique and see if it works, but I am a bit lost in loops right now.Thanks for any light you can shed on this matter.Cheers,Fabio.Hi Fabio,I think that these are independent concepts (when to use grid stride loops and when to use shared memory). Shared memory won't help speed up every computation in a loop -- just those that can benefit from reuse among threads of the same block. It's a part of the memory hierarchy.MarkThanks, Mark for your prompt answer!I verified your statement about how independent shared and global memories are. It then dawn on me that within a warp, any operation done on a piece of global memory is actually performed in some fast memory and that I wasn't gaining any extra time by copying it to shared memory first. In any case, what ended up causing the unexpected behavior were unused threads in the warp that I ensured to be inactive by an if statement.Again, many thanks.FabioAny arithmetic instruction is performed in the compute cores (ALUs), reading data from registers. I suppose that yes, registers are ""some fast memory"".Hi Mark, sorry for possible silly question: does it make sense to use inverse external loop. I mean:use i=threadIdx.x*gridDim.x+blockIdx.x instead of i=blockIdx.x * blockDim.x + threadIdx.x; Does it always slower or any reasons don't do so?Thank you, AlexeyThink about which threads are running together. Threads 0 through 31, for example, will get values of `i` that are spread apart by gridDim.x. This means that if you index an array using `i` you will lose locality across parallel threads (this is important even in sequential loops on a CPU). Specifically, you will not get coalescing and each thread is likely to require a separate memory transaction (loading a whole cache line). Performance will suffer.Thanks a lot, Mark!All the best, AlexeyIn the ""grid-stride loop"" example, would it be more efficient to store blockDim.x * gridDim.x in a register and use that register to increment i in the for loop?If the compiler doesn't do that for you automatically then I would consider it a bug. Let me know if you find this is the case.They appear to perform the same. :-)Still a great article (I've come back to it multiple times for reference). I was wondering how one could implement this for Tensors that require more than 3-dimensions. Thanks again! -JJI'm also interested in the answer to this question. Did you figure it out?Great discussion and now please  describe for a 2D case with a simple example.Mr. Mark,1-I feel a bit confused about this statement of yours ""Rather than assume that the thread grid is large enough to cover the entire data array,"". I don't understand the word ""assume"" here because when you launch the grid-block to GPU, you must have the number of threads you need in your mind already (based on the data array), and therefore, know exactly know how many blocks you need for the syntax <<<m,t>>> to cover the entire array, don't you? If so, why ""assume""?2-Furthermore, to my knowledge, thread is a calculation that you create and it is different from a CUDA core (which is a physic element). For example, you create a grid of 512 threads, even though you might have only 256 cores, when you launch the grid, GPU still be able to calculate the whole 512 threads (by monolithic kernel). Not that 256 core will calculate only 256 threads and ignore the others. However, this blog: ""https://alexminnaar.com/201..."" explained in a way that threads and cores are the same. Can you please help to clarify this as well.3-I have tested with nvprof, and seems like monolithic kernel is a bit faster than grid-stride because seems like for each thread, the latter has to calculate 2 more commands below that makes it a bit longer than the mono:int index = blockIdx.x * blockDim.x + threadIdx.x;int stride = blockDim.x * gridDim.x;Maybe the way I tested is not correct, hope you can help to explain more.I am just starting with CUDA, therefore, I am very grateful if you can help me to understand. Thank you,1. It's common to launch fewer threads than you have data items. Hence you need to iterate. You might even rely on (for example) the CUDA occupancy API to choose a block / grid size for you, which means you don't know until launch time how many threads you will launch.2. You are correct. CUDA threads are not the same as CUDA cores. CUDA threads are threads of execution that stay resident and use resources (registers, e.g.) on a single multiprocessor until they finish the kernel. CUDA cores are physical instruction execution units on the GPU.3. If you don't need a loop, then you can write it without a loop. If the index calculation slows the kernel down, then the kernel isn't doing much computation or memory access. :)Mr. Mark,Thank you very much for the fast and informative answers.May I bother you further in the first point. I have tested and confirm that I misunderstood the point earlier. Let say, if I launch <<<1,256>> with the monolith kernel for an  array of 1M as in your tutorial.__global__void saxpy(int n, float a, float *x, float *y){    int i = blockIdx.x * blockDim.x + threadIdx.x;    if (i < n)         y[i] = a * x[i] + y[i];}--> Then GPU only calculates 256 threads, then stop. Therefore, if use monolith, when launch <<<m,t>>> with Array size N. Make sure that m*t >= N.I hope this comment will help other newbies like me to understand more.Powered by Discourse, best viewed with JavaScript enabled"
1155,meet-the-researcher-john-w-garrett-improving-access-to-clinical-data-with-ai,"Originally published at:			Meet the Researcher: John W. Garrett, Improving Access to Clinical Data with AI | NVIDIA Technical Blog
‘Meet the Researcher’ is a new series in which we spotlight different researchers in academia who are using GPUs to accelerate their work. This week, we spotlight Dr. John Garrett, an Assistant Professor in the Departments of Radiology and Medical Physics and the Director of Imaging Informatics for the Department of Radiology at the University…Powered by Discourse, best viewed with JavaScript enabled"
1156,getting-ai-applications-ready-for-cloud-native,"Originally published at:			https://developer.nvidia.com/blog/getting-ai-applications-ready-for-cloud-native/
Cloud-native is one of the most important concepts associated with deploying edge AI applications. Find out how to get AI applications cloud-native ready.Powered by Discourse, best viewed with JavaScript enabled"
1157,streaming-everything-with-nvidia-rivermax,"Originally published at:			Streaming Everything with NVIDIA Rivermax | NVIDIA Technical Blog
In 2020, many of us adopted a work-from-home routine, and this new norm has been stressing IT networks. It shouldn’t be a surprise that the sudden boost in remote working drives the need for a more dynamic IT environment, one that can pull in resources on demand. Over the past few years, we’ve focused on…Powered by Discourse, best viewed with JavaScript enabled"
1158,nasa-and-nvidia-collaborate-to-accelerate-scientific-data-science-use-cases-part-1,"Originally published at:			https://developer.nvidia.com/blog/nasa-and-nvidia-collaborate-to-accelerate-scientific-data-science-use-cases-part-1/
Over the past couple of years, NVIDIA and NASA have been working closely on accelerating data science workflows using RAPIDS and integrating these GPU-accelerated libraries with scientific use cases. In this blog, we’ll share some of the results from an atmospheric science use case, and code snippets to port existing CPU workflows to RAPIDS on…Powered by Discourse, best viewed with JavaScript enabled"
1159,generate-natural-sounding-speech-from-text-in-real-time,"Originally published at:			Generate Natural Sounding Speech from Text in Real-Time | NVIDIA Technical Blog
This post, intended for developers with professional level understanding of deep learning, will help you produce a production-ready, AI, text-to-speech model. Converting text into high quality, natural-sounding speech in real time has been a challenging conversational AI task for decades. State-of-the-art speech synthesis models are based on parametric neural networks1. Text-to-speech (TTS) synthesis is typically…You state ""Our current model synthesizes samples at 125 * 22,050 = 2,756,250, which is 125 times faster than “real-time” at 22,050 samples"", why RTF is then not 125 instead of 1-4 ?Guys, hope you could correct the use of the term RTF(pls do not mix with xRTF which is 1/RTF), we do not like RTF > 1 systems which means it could not be real-time http://dictionary.sensagent...there are two factors that influence the latency results reported here: 1) we are measuring end-to-end text-to-speech inference, i.e., the total of Tacotron2 and WaveGlow latency is reported; in the quoted sentence, the 125 refers to WaveGlow latency only. 2) In this article we were using the slower version of WaveGlow with 512 residual channels; the quoted version uses 256 channels.This is a real good article, thank you.Powered by Discourse, best viewed with JavaScript enabled"
1160,ama-with-the-cugraph-engineering-team-april-12-2023-9am-pdt,"Join us for an AMA with the engineering team behind cuGraph
When: April 12th, 9am (PDT)
Where: Right here in this forum directory
How do I participate?What is cuGraph?
cuGraph is a robust, full feature, suite of graph analytics libraries. The core cuGraph library includes about three dozen algorithms, with most support scaling to massive GPU clusters. The cuGraph PageRank algorithm, for example has been test on a 4.4 trillion edge graph spread over 2,048 GPU. Even at that scale, a single PageRank iteration only took 1.5 seconds. Recently cuGraph has added supports for GNN with accelerated aggregators, accelerated models, and extensions to both DGL and PyG.Just a reminder to join us for the live AMA with the cuGraph Engineering team leads tomorrow at 9am (PDT)
See you tomorrow.Both NVIDIA Register Buttons in the email that was sent, forward to this Forum.
https://forums.developer.nvidia.com/Can you please post here the correct working registration URL for this event?The actual AMA event happens in this sub category:Join us for an AMA with the engineering team behind cuGraph
When: April 12th, 9am (PDT)
Where: Right here in this forum directoryCan cuGraph accommodate sparse adjacency matrix as input? Can I use it as an extension of cuSparse?please repost your question here: AMA cuGraph: Graph analysis and GNN - NVIDIA Developer Forums  I don’t think this post is monitored for questions :)Powered by Discourse, best viewed with JavaScript enabled"
1161,improving-diffusion-models-as-an-alternative-to-gans-part-1,"Originally published at:			https://developer.nvidia.com/blog/improving-diffusion-models-as-an-alternative-to-gans-part-1/
Researchers at NVIDIA have developed methods to improve and accelerate sampling from diffusion models, a novel and powerful class of generative models.Powered by Discourse, best viewed with JavaScript enabled"
1162,using-the-rapids-vm-image-for-google-cloud-platform,"Originally published at:			Using the RAPIDS VM Image for Google Cloud Platform | NVIDIA Technical Blog
NVIDIA’s Ty McKercher and Google’s Viacheslav Kovalevskyi and Gonzalo Gasca Meza jointly authored a post on using the new the RAPIDS VM Image for Google Cloud Platform. Following is a short summary. For the full post, please see the full Google article. If you’re a data scientist, researcher, engineer, or developer using pandas, Dask, scikit-learn,…Powered by Discourse, best viewed with JavaScript enabled"
1163,model-interpretability-using-rapids-implementation-of-shap-on-microsoft-azure,"Originally published at:			https://developer.nvidia.com/blog/model-interpretability-using-rapids-implementation-of-shap-on-microsoft-azure/
Machine Learning (ML) is increasingly used to make decisions across many domains like healthcare, education, and financial services. Since ML models are being used in situations that have a real impact on people, it is critical to understand what features are being considered for the decisions to eliminate or minimize the impact of biases. Model…Powered by Discourse, best viewed with JavaScript enabled"
1164,cugraph-and-entity-alignment-models,"Hello, I wanted to know if implementation of some entity alignment models on graphs e.g. RDGCN are available on cuGraph?We do not support entity alignment models as of now, but we have been gradually adding most requested models from users/customers.Thank you so much. I hope it happens very soon.This topic was automatically closed 14 days after the last reply. New replies are no longer allowed.Powered by Discourse, best viewed with JavaScript enabled"
1165,nvidia-cloudxr-3-0-delivers-support-for-bidirectional-audio-to-enhance-immersive-collaboration,"Originally published at:			https://developer.nvidia.com/blog/nvidia-cloudxr-3-0-delivers-support-for-bidirectional-audio-to-enhance-immersive-collaboration/
NVIDIA CloudXR provides a powerful edge computing platform for extended reality. Built on NVIDIA RTX technology, CloudXR is an advanced streaming technology that delivers VR and AR across 5G and Wi-Fi networks.Powered by Discourse, best viewed with JavaScript enabled"
1166,gtc-2020-smart-cities-in-the-cloud-nvidia-metropolis-on-red-hat-openshift,"GTC 2020 S21994
Presenters: Sujit Biswas ,NVIDIA; Peter MacKinnon,Red Hat Inc.
Abstract
We’ll talk about the collaboration between NVIDIA and Red Hat, and demo the integration between the NVIDIA’s Metropolis application running on EGX and Red Hat OpenShift (Kubernetes) in the public cloud.Watch this session
Join in the conversation below.Powered by Discourse, best viewed with JavaScript enabled"
1167,boosting-ai-model-inference-performance-on-azure-machine-learning,"Originally published at:			Boosting AI Model Inference Performance on Azure Machine Learning | NVIDIA Technical Blog
Learn how to optimize input parameters when deploying AI models for inference on Azure Machine Learning while using Triton Model Analyzer and ONNX Runtime OLive.Powered by Discourse, best viewed with JavaScript enabled"
1168,jetpack-2-3-with-tensorrt-doubles-jetson-tx1-deep-learning-inference,"Originally published at:			https://developer.nvidia.com/blog/jetpack-doubles-jetson-tx1-deep-learning-inference/
Deep Neural Networks (DNNs) are a powerful approach to implementing robust computer vision and artificial intelligence applications. NVIDIA Jetpack 2.3, released today, increases run-time performance of DNNs in embedded applications more than two-fold using NVIDIA TensorRT (formerly called GPU Inference Engine or GIE). With up to 20x higher power efficiency than an Intel i7 CPU during inference workloads, NVIDIA’s 1…Hi,What is the procedure to benchmark Caffe GoogleNet/AlexNet based on TensorRT on Jetson TX1 using Jetpack 2.3?Hello, the Caffe procedure is available from this sticky post   https://devtalk.nvidia.com/...  and TensorRT includes timings example in the samples installed on the Jetson with JetPack 2.3.Thanks for your reply. The only samples I found were for classification and I could run those. https://github.com/dusty-nv... But how do I get forward time or images/sec?Check the timing sample located at /usr/src/gie_samples/samples/sampleGoogleNet , it installed along with TensorRT / JetPack 2.3.Hi,I still can not reproduce the benchmark of TensorRT 21fps detection as paper. I got only 8 fps on this detection./detectnet-camera ped-100ThanksHi VuNguyen, the 21 figure is the number of GoogleNet (ImageNet) images per second per watt.   The raw number of GoogleNet images per second is 203fps.  What you are referring to is DetectNet, which requires additional layers and computation.  GoogleNet is image recognition and DetectNet is multi-class multi-object localization.  Since the publication of this article, there has been additional work and progress on faster DetectNet however for a future release.Hi Dustin Franklin,Good to hear that. Do you know the plan for a future release?ThanksCan artificial neural netowrk problems be solved using above kit?Yes.Powered by Discourse, best viewed with JavaScript enabled"
1169,now-available-nvidia-dlss-3-for-unreal-engine-5,"Originally published at:			https://developer.nvidia.com/blog/now-available-nvidia-dlss-3-for-unreal-engine-5/
NVIDIA DLSS 3 is a neural graphics technology that multiplies performance using AI image reconstruction and frame generation. It’s a combination of three core innovations: Super Resolution uses deep learning algorithms to upscale a lower-resolution input into a higher-resolution output, creating a sharp image with a boosted frame rate. Frame Generation uses AI rendering to…great enhancement! Is Linux support/compiling planned?FANTASTIC!  Is frame generation available on 30 series cards or 40 only?Powered by Discourse, best viewed with JavaScript enabled"
1170,using-a-resnet18-onnx-model,"Hello Dustin,I was using the tutorial “Hello AI WORD” for creating my trained model. Everything was ok until I obtained the onnx model.
I was trying to make an example in python but I get an error that I cannot find the solution.The code is:import jetson.inference
import jetson.utils
import cv2
import numpy as np
import time
width=800
height=600
dispW=width
dispH=heightcam1=cv2.VideoCapture(‘/dev/video0’)
cam1.set(cv2.CAP_PROP_FRAME_WIDTH,dispW)
cam1.set(cv2.CAP_PROP_FRAME_HEIGHT,dispH)
net = jetson.inference.imageNet(‘alexnet’,[‘–model=/home/pc/jetson-inference/python/training/classification/myModel/resnet18.onnx’,‘–input_blob=input_0’,‘–output_blob=output_0’,‘–labels=/home/pc/Documents/jetsonr/mytrain/labels.txt’])font=cv2.FONT_HERSHEY_SIMPLEX
timeMark=time.time()
fpsFilter=0
while True:
_,frame=cam1.read()
img=cv2.cvtColor(frame,cv2.COLOR_BGR2RGBA).astype(np.float32)
img=jetson.utils.cudaFromNumpy(img)
classID, confidence =net.Classify(img, width, height)
item=‘’
item =net.GetClassDesc(classID)
dt=time.time()-timeMark
fps=1/dt
fpsFilter=.95fpsFilter +.05fps
timeMark=time.time()
cv2.putText(frame,str(round(fpsFilter,1))+’ fps '+item,(0,30),font,1,(0,0,255),2)
cv2.imshow(‘recCam’,frame)
cv2.moveWindow(‘recCam’,0,0)
if cv2.waitKey(1)==ord(‘q’):
break
cam1.releast()
cv2.destroyAllWindows()The error of the compilation is:jetson.inference.init.py
jetson.inference – initializing Python 3.6 bindings…
jetson.inference – registering module types…
jetson.inference – done registering module types
jetson.inference – done Python 3.6 binding initialization
jetson.utils.init.py
jetson.utils – initializing Python 3.6 bindings…
jetson.utils – registering module functions…
jetson.utils – done registering module functions
jetson.utils – registering module types…
jetson.utils – done registering module types
jetson.utils – done Python 3.6 binding initialization
Traceback (most recent call last):
File “my-detection4.py”, line 3, in 
import cv2
File “/usr/local/lib/python3.6/dist-packages/cv2/init.py”, line 89, in 
bootstrap()
File “/usr/local/lib/python3.6/dist-packages/cv2/init.py”, line 79, in bootstrap
import cv2
ImportError: /usr/local/lib/libopencv_cudaarithm.so.4.1: undefined symbol: _ZN2cv4cuda14StreamAccessor9getStreamERKNS0_6StreamEDo you know how I can fix this error of cuda?Thank you and best regards,
SalvaPowered by Discourse, best viewed with JavaScript enabled"
1171,nvidia-earns-1st-place-in-recsys-challenge-2021,"Originally published at:			https://developer.nvidia.com/blog/nvidia-earns-1st-place-in-recsys-challenge-2021/
The NVIDIA Merlin and KGMON team earned 1st place in the RecSys Challenge 2021 by effectively predicting the probability of user engagement within a dynamic environment and providing fair recommendations on a multi-million point dataset.Powered by Discourse, best viewed with JavaScript enabled"
1172,gtc-2020-power-intelligent-video-analytics-with-nvidia-ai-and-virtual-compute-server,"GTC 2020 S21407
Presenters: Eric Kana,NVIDIA; J. David ‘TUBA’ Britt, PMP,ManTech
Abstract
Intelligent video analytics is growing as cameras become ubiquitous. Video analytics can provide meaningful insight to help city officials, retail stores, and other entities. However, security and isolation of these streams is becoming a common concern for enterprise IT. We’ll explain how virtualization using NVIDIA Virtual Compute Server software with vGPU technology provides a cost- and operational-efficient solution that offers multi-tenancy and mixed workload options, on top of securing workloads and user isolation. A virtualized environment harnessing the power of NVIDIA GPUs can provide flexibility for users to run AI and deep learning workloads for their video analytics in a highly performing and secure environment.Watch this session
Join in the conversation below.Powered by Discourse, best viewed with JavaScript enabled"
1173,validating-distributed-multi-node-autonomous-vehicle-ai-training-with-nvidia-dgx-systems-on-openshift-with-dxc-robotic-drive,"Originally published at:			https://developer.nvidia.com/blog/validating-distributed-multi-node-av-ai-training-with-dgx-systems-on-openshift-with-dxc-robotic-drive/
Deep neural network (DNN) development for self-driving cars is a demanding workload. In this post, we validate DGX multi-node, multi-GPU, distributed training running on RedHat OpenShift in the DXC Robotic Drive environment. We used OpenShift 3.11, also a part of the Robotic Drive containerized compute platform, to orchestrate and execute the deep learning (DL) workloads.…Powered by Discourse, best viewed with JavaScript enabled"
1174,accelerated-inference-for-large-transformer-models-using-fastertransformer-and-triton-inference-server,"Originally published at:			Accelerated Inference for Large Transformer Models Using NVIDIA Triton Inference Server | NVIDIA Technical Blog
Learn about FasterTransformer, one of the fastest libraries for distributed inference of transformers of any size, including benefits of using the library.Powered by Discourse, best viewed with JavaScript enabled"
1175,ai-reinvents-the-filmmaking-process,"Originally published at:			https://developer.nvidia.com/blog/ai-reinvents-the-filmmaking-process/
A deep learning startup working to automate the creation of digital effects in the motion picture and television industry recently announced they raised over $10 million in funding. Arraiy, a Palo Alto, California-based startup, is building an AI system that can generate and manipulate images in real-time such as changing the type of car, color,…Powered by Discourse, best viewed with JavaScript enabled"
1176,rendering-in-real-time-with-spatiotemporal-blue-noise-textures-part-2,"Originally published at:			https://developer.nvidia.com/blog/rendering-in-real-time-with-spatiotemporal-blue-noise-textures-part-2/
Spatiotemporal blue noise textures add the time axis, providing better convergence of blue noise, without loss of quality of the blue noise error patterns.Powered by Discourse, best viewed with JavaScript enabled"
1177,algorithm-successfully-diagnoses-pneumonia-at-radiologist-level-accuracy,"Originally published at:			Algorithm Successfully Diagnoses Pneumonia at Radiologist-Level Accuracy | NVIDIA Technical Blog
A team of Stanford researchers developed a deep learning-based algorithm that evaluates chest X-rays for signs of disease. “Interpreting X-ray images to diagnose pathologies like pneumonia is very challenging, and we know that there’s a lot of variability in the diagnoses radiologists arrive at,” said Pranav Rajpurkar, a graduate student in the Stanford Machine Learning…Powered by Discourse, best viewed with JavaScript enabled"
1178,latest-nvidia-optix-renders-ray-tracing-faster-than-ever-before,"Originally published at:			Latest NVIDIA OptiX Renders Ray Tracing Faster Than Ever Before | NVIDIA Technical Blog
NVIDIA OptiX 7.4 introduces parallel compilation, temporal denoising of arbitrary values, improvements to the demand loading library, enhancements to ray payloads, Catmull-Rom curves, and decreased memory for curves.My only interest is VR - I no longer play games in flat (or pancake) mode, so whenever Nvidia announces something could you please include a mention to tell us whether this is going to improve the VR experience.  I have a 2080TI which I bought especially for VR and it works well, I would have upgraded to one in the 30 series (if they had been available) but from what I have been able to work out it would not have given a significant improvement.Thank you.hello, thank you for share, but i don’t have idea how get, install and use the program to practice, maybe a tutorial or link to see how works ?
thank youThanks for the input! OptiX has a wide set of use cases from professional rendering to simulation. OptiX can even be used to simulate sound propagation, which can offer a much more immersive experience in VR.The OptiX SDK comes with a number of samples to help you get started. They range from simple to advanced and should serve as a great place for you to start. Additionally, We have a number of talks at GTC this week that would be valuable to check out. “[A31547]: RTX Ray Tracing 101: Learn How to Build Ray-tracing Applications” would be the best one to watch for beginners.First a quick question: Is it possible that the OptiX users guide hasn’t been updated? The examples in there still mention the limit of 8 payload values.A thing I find  surprising about the OptiX 7.4 SDK is how all ray tracing calls (no matter how many payloads) are now mapped to a single PTX intrinsic _optix_trace_typed_32, whereas previous versions used finer-grained wrappers like optix_trace_1 … _optix_trace_8. This makes the wrappers in include/internal/optix_7_device_impl.h super-unwieldy (both in terms of C++ code, and the PTX code that is generated).  Is this a good idea? Even the most basic one reads:My group builds JIT compilers generating OptiX code that often contain many ray tracing calls – it seems scary to generate that many unused/temporary variables when doing a few of these in a kernel.can offer a much more immersive experience in VR.Hi, @akanell ! OptiX is a pure raytracing engine, however not a renderer. Developers can use it for implementation. My main concern is real-time rendering in VR, and unfortunately a sample code/ tutorial is missing for that.Hi @wenzel.jakob, what are the benefits of using JIT compiler over NVCC compiler? Is it faster than nvcc?@_Bi2022. Speed of compilation is not the motivating factor. JIT compilation is useful in applications where you don’t even know what code to execute until at runtime, for example when the user is writing it in a different language like Python, or when there are program transformations like differentiation that change the code at runtime. See this paper for details: RGL | Dr.Jit: A Just-In-Time Compiler for Differentiable RenderingPowered by Discourse, best viewed with JavaScript enabled"
1179,artificial-intelligence-chatbot-remembers-anything-for-you,"Originally published at:			Artificial Intelligence Chatbot Remembers Anything For You | NVIDIA Technical Blog
A new bot called Wonder will remember anything you want, and then return the information you need via a text message. Once you enter your phone number on the Wonder website, the bot will send you a text explaining how it works – and then you just reply back with the information you want it…Powered by Discourse, best viewed with JavaScript enabled"
1180,dialed-into-5g-nvidia-cloudxr-4-0-brings-enhanced-flexibility-and-scalability-for-xr-deployment,"Originally published at:			https://developer.nvidia.com/blog/dialed-into-5g-cloudxr-4-0-brings-enhanced-flexibility-and-scalability-for-xr-deployment/
At GTC 2023, NVIDIA announced the latest release of NVIDIA CloudXR that enables you to customize this SDK for your applications and customers, and scale extended reality (XR) deployment across the cloud, 5G Mobile Compute Edge (MEC), and corporate networks. NVIDIA CloudXR 4.0 introduces new APIs that deliver enhanced flexibility for server and client application…Powered by Discourse, best viewed with JavaScript enabled"
1181,ready-player-me-in-audio2face,"When will it be possible to import and use RPM avatars in A2F?You can check out ACE - Avatar cloud engine. We support Audio2Face microservice that allows you to render with any avatars and any rendering engine. Omniverse Avatar Cloud Engine ACE | NVIDIA Developer | NVIDIA Developer We have showcased a demo with RPM thereCan you export the avatar from readyplayer.me as GLB? (I thought it was possible but could not find it after some quick Google searches.)I managed recently to get a VRoid Studio avatar working with Audio2Face (exported as a VRM file, which is a GLB file), but I had to write scripts to clean up the mesh. VRoid Studio was doing things like generating lots of vertexes that were never used in any mesh.I am still working on putting it all together however (sequencer, use in combination with full body animation clips, hair physics, etc).I fixed your URL typo - the url you accidently typed was a ransom ware site. So please be careful when you share the ready player URL :)Awesome work! If you haven’t yet, you should join our Audio2Face channel on discord: NVIDIA OmniverseIt’s one of our most active channels and there are a lot of people working on similar projects there. I bet they would love to see what you’re doing and might be able to help you as you progress.Just did some googling myself and it looks like you should be able to export GLB from readyplyer.me as noted here: How to convert a GLB avatar file to FBX?. It’s not something I’ve done, however, so I would definitely check in on the discord channel.This topic was automatically closed 14 days after the last reply. New replies are no longer allowed.Powered by Discourse, best viewed with JavaScript enabled"
1182,coming-next-week-in-the-trenches-reports-from-gtc,"Originally published at:			https://developer.nvidia.com/blog/coming-next-week-trenches-reports-gtc/
The GPU Technology Conference starts next week in San Jose, California, featuring hundreds of hours of sessions, tutorials and keynotes. To help you stay current, we’ve invited a group of talented guest bloggers to provide reports from “in the trenches.” Each day next week, check this spot to read our guests’ posts on some of…Powered by Discourse, best viewed with JavaScript enabled"
1183,introducing-nvidia-aerial-research-cloud-for-innovations-in-5g-and-6g,"Originally published at:			https://developer.nvidia.com/blog/introducing-aerial-research-cloud-for-innovations-in-5g-and-6g/
At NVIDIA GTC 2023, NVIDIA introduced Aerial Research Cloud, the first fully programmable 5G and 6G network research sandbox, which enables researchers to rapidly simulate, prototype, and benchmark innovative new software deployed through over-the-air networks. The platform democratizes 6G innovations with a full-stack, C-programmable 5G network, and jumpstarts ML in advanced wireless communications using NVIDIA-accelerated…Hi!
do you have any limitations because of latencies issues in your system?
I mean, 5G 1ms round trip time seems to be too tight to reach for a GPU architectureDo you support 120kHz subcarrier spacing? Thank you!Powered by Discourse, best viewed with JavaScript enabled"
1184,accelerating-hpc-applications-with-nvidia-nsight-compute-roofline-analysis,"Originally published at:			https://developer.nvidia.com/blog/accelerating-hpc-applications-with-nsight-compute-roofline-analysis/
Writing high-performance software is no simple task. After you have code that can compile and run, a new challenge is introduced when you try and understand how it is performing on the available hardware. Different platforms, whether they are CPUs, GPUs, or something else, will have different hardware limitations like available memory bandwidth and theoretical…It was great to collaborate with some of the foremost experts on Roofline Analysis and the Nsight Compute engineering team to create this example. If you have any questions or comments, please let us know.Powered by Discourse, best viewed with JavaScript enabled"
1185,porsches-storm-troopers-and-ray-tracing-how-nvidia-and-epic-are-redefining-graphics,"Originally published at:			Porsches, Storm Troopers, and Ray Tracing: How NVIDIA and Epic are Redefining Graphics | NVIDIA Technical Blog
Throughout 2018, NVIDIA and Epic Games have led the charge in showing what development teams can pull off with real-time ray tracing. During GDC, they worked together to create a short demo entitled “Reflections”   Amazing, isn’t it? We’ve reached a stage where real-time ray tracing techniques can be used to construct a sequence that…Powered by Discourse, best viewed with JavaScript enabled"
1186,new-openacc-online-course-will-help-you-quickly-accelerate-your-code-on-gpus,"Originally published at:			New OpenACC Online Course Will Help You Quickly Accelerate Your Code on GPUs | NVIDIA Technical Blog
In the age of Exascale, scientists are striving to use the latest generation of supercomputers to do more science faster. At the same time many researchers find themselves trapped in new complex technologies and architectures that are not always easy to grasp — they need tools that can help them spend less time on programming…Powered by Discourse, best viewed with JavaScript enabled"
1187,assess-parallelize-optimize-deploy,"Originally published at:			https://developer.nvidia.com/blog/assess-parallelize-optimize-deploy/
When developing an application from scratch it is feasible to design the code, data structures, and data movement to support accelerators. However when facing an existing application it is often hard to know where to start, what to expect, and how best to make use of an accelerator like a GPU. Based on our experience…Powered by Discourse, best viewed with JavaScript enabled"
1188,topic,"So much time wasting on the pre-experience applicationPowered by Discourse, best viewed with JavaScript enabled"
1189,meta-works-with-nvidia-to-build-massive-ai-research-supercomputer,"Originally published at:			https://developer.nvidia.com/blog/meta-works-with-nvidia-to-build-massive-ai-research-supercomputer/
Powered by Discourse, best viewed with JavaScript enabled"
1190,accelerating-winml-and-nvidia-tensor-cores,"Originally published at:			Accelerating WinML and NVIDIA Tensor Cores | NVIDIA Technical Blog
Figure 1. TensorCores. Every year, clever researchers introduce ever more complex and interesting deep learning models to the world. There is of course a big difference between a model that works as a nice demo in isolation and a model that performs a function within a production pipeline. This is particularly pertinent to creative apps…Powered by Discourse, best viewed with JavaScript enabled"
1191,nvidia-announces-cuda-x-hpc,"Originally published at:			NVIDIA Announces CUDA-X HPC | NVIDIA Technical Blog
Giving developers the libraries and tools for their next scientific breakthrough From fluid dynamics and weather simulation, to computational chemistry and bioinformatics, HPC applications span across many domains. Developing these applications requires a robust programming environment with highly optimized domain specific libraries. Announced today, CUDA-X HPC is a collection of libraries, tools, compilers and APIs…Powered by Discourse, best viewed with JavaScript enabled"
1192,introducing-the-nvidia-openacc-toolkit,"Originally published at:			Introducing the NVIDIA OpenACC Toolkit | NVIDIA Technical Blog
Programmability is crucial to accelerated computing, and NVIDIA’s CUDA Toolkit has been critical to the success of GPU computing. Over 3 million CUDA Toolkits have been downloaded since its first launch. However there are many scientists and researchers yet to benefit from GPU computing. These scientists have limited time to learn and apply a parallel…Hi,  Does your OpenACC Toolkit support AMD GPU ?  Specifically old AMD Firetream 9250 Stream Processor CardThanks,ArulFrom the PGI website:https://www.pgroup.com/reso...PGI Accelerator compilers target all NVIDIA Tesla GPU accelerators with compute capability 2.0 or higher. In addition, they support the following accelerators from AMD:AMD Radeon HD Graphics 7700 seriesAMD Radeon HD Graphics 7800 seriesAMD Radeon HD Graphics 7900 seriesAMD APU Family with AMD Radeon HD Graphics R7 seriesPowered by Discourse, best viewed with JavaScript enabled"
1193,prototyping-algorithms-and-testing-cuda-kernels-in-matlab,"Originally published at:			https://developer.nvidia.com/blog/prototyping-algorithms-and-testing-cuda-kernels-matlab/
This guest post by Daniel Armyr and Dan Doherty from MathWorks describes how you can use MATLAB to support your development of CUDA C and C++ kernels. You will need MATLAB, Parallel Computing Toolbox™, and Image Processing Toolbox™ to run the code. You can request a trial of these products. For a more detailed description of this…Dear Mark. Although there are an abundance of information about Cudo computing with Matlab, it fails to mention which cards will work with Matlab. Matlab does not support all cards so one has to be careful. I'm interested in the K6000 to sue with Matlab, but I'm not sure whether its compatible since from other sources you read that its not added . See here for example: http://www3.pny.com/NVIDIA-... They will only mention Java, Python etc.. So will the k6000 work with Matlab please?thanksDanHi dan, thanks for your comment! The answer is yes.  MATLAB ParallelComputing Toolbox supports GPUs of compute capability 1.3 and higher. Formal Kepler support was added a version or two ago. See http://www.mathworks.com/pr...K6000 should work. This page says that GPUs with Compute Capability 2.0 or later are supported. http://au.mathworks.com/dis...Powered by Discourse, best viewed with JavaScript enabled"
1194,cvpr-2020-plotly-dash-with-rapids-gpu-accelerated-2010-census-data-visualization,"CVPR 2020 dcv22
Presenters: Tech Demo Team, NVIDIA
Abstract
Datasets are getting larger and larger. Visualization is key to validate, understand, and communicate important information often hidden among hidden among a massive numbers of rows and columns. Based on Python, this dashboard integrates Plotly Dash, RAPIDS cuDF, and Datashader for real-time cross filtering on NVIDIA GPUs to empower data scientists to extract valuable insights by working independently across the whole stack – from raw data to user interface – and quickly deliver interactive dashboards.Watch this session
Join in the conversation below.Powered by Discourse, best viewed with JavaScript enabled"
1195,nvidia-clara-sdk-now-available,"Originally published at:			NVIDIA Clara SDK Now Available | NVIDIA Technical Blog
Earlier this year, NVIDIA unveiled the NVIDIA Clara platform, an open platform that enables developers and partners to take advantage of NVIDIA’s technology and expertise in artificial intelligence, advanced visualization, and high performance computing,  to build the next generation of medical imaging instruments and workflows. Today, we are announcing the availability of the Clara Software…Powered by Discourse, best viewed with JavaScript enabled"
1196,new-pascal-gpus-accelerate-inference-in-the-data-center,"Originally published at:			https://developer.nvidia.com/blog/new-pascal-gpus-accelerate-inference-in-the-data-center/
Artificial intelligence is already more ubiquitous than many people realize. Applications of AI abound, many of them powered by complex deep neural networks trained on massive data using GPUs. These applications understand when you talk to them; they can answer questions; and they can help you find information in ways you couldn’t before. Pinterest image…Your table shows that GP104/GP102 has 128 KB of shared memory per SM. Can you please confirm that it's not a mistake (Maxwell GPUs had only 64-96 KB)? Is that holds for all CC 6.1 devices including all gaming cards? Can you please say the size of L1/texture cache and how it's shared between warp shedulers? May be there is some Whitepaper describing the 6.1 architecture?The http://docs.nvidia.com/cuda... shows that CC 6.1 has the same capabilities as 5.2 (and answers my question regarding caches), so it's just a typo hereTypo fixed, thanks. 96KB shared memory on P4 and P40 SMs.Powered by Discourse, best viewed with JavaScript enabled"
1197,webinar-limitless-extended-reality-with-nvidia-cloudxr-2-0,"Originally published at:			https://developer.nvidia.com/blog/webinar-limitless-extended-reality-with-nvidia-cloudxr-2-0/
Many people believed delivering extended reality (XR) experiences from cloud computing systems was impossible until now. Join our webinar to learn how NVIDIA CloudXR can be used to deliver limitless virtual and augmented reality over networks (including 5G) to low cost, low-powered headsets and devices—while maintaining the high-quality experience traditionally reserved for high-end headsets that…Powered by Discourse, best viewed with JavaScript enabled"
1198,gpu-pro-tip-fast-great-circle-distance-calculation-in-cuda-c,"Originally published at:			https://developer.nvidia.com/blog/fast-great-circle-distance-calculation-cuda-c/
This post demonstrates the practical utility of CUDA’s sinpi() and cospi() functions in the context of distance calculations on earth. With the advent of location-aware and geospatial applications and geographical information systems (GIS), these distance computations have become commonplace. A great circle divides a sphere into two hemispheres. Image: Jhbdel at en.wikipedia [CC BY-SA 3.0],…Powered by Discourse, best viewed with JavaScript enabled"
1199,gtc-2020-performance-and-model-fidelity-of-bert-training-from-a-single-dgx-through-dgx-superpod,"GTC 2020 S21385
Presenters: Chris Forster,NVIDIA; Thor Johnsen,NVIDIA
Abstract
BERT (Bidirectional Encoder Representations from Transformers) is a natural language processing model that performs well on a wide variety of tasks, including (but not limited to) question answering, natural language inference, and classification. We’ll cover how you can use our open-source code to train BERT models themselves, right from dataset creation to fine-tuning for specific NLP tasks, such as question answering with the SQuAD dataset. We’ll also discuss some of the challenges and solutions to delivering both computational performance and model fidelity on large distributed machines, such as the DGX SuperPod. We’ll offer a brief overview of the model itself, choice of optimizers, performance optimizations, testing methodology, running BERT at scales up to 1,472 GPUs, and we’ll summarize the results that our open-source multi-node BERT examples in Tensorflow and PyTorch can achieve.Watch this session
Join in the conversation below.Powered by Discourse, best viewed with JavaScript enabled"
1200,pix2pixhd-is-now-available-as-open-source-on-ngc,"Originally published at:			Pix2PixHD Is Now Available as Open Source on NGC | NVIDIA Technical Blog
Pix2PixHD is a PyTorch implementation of a deep learning-based method for high-resolution (e.g. 2048×1024) photorealistic image-to-image translation. Today, NVIDIA is releasing the code on NGC for commercial use via the Berkeley Software Distribution (BSD) License. The BSD license will allow developers to use Pix2PixHD in their closed source commercial projects, as long as users accept…Powered by Discourse, best viewed with JavaScript enabled"
1201,improved-interoperability-between-vpi-and-pytorch,"Originally published at:			https://developer.nvidia.com/blog/improved-interoperability-between-vpi-and-pytorch/
NVIDIA VPI is a computer vision and image-processing software library to implement algorithms that are accelerated on different hardware backends.Powered by Discourse, best viewed with JavaScript enabled"
1202,three-things-you-need-to-know-about-ray-tracing-in-vulkan,"Originally published at:			Three Things You Need to Know About Ray Tracing in Vulkan | NVIDIA Technical Blog
In this video, Nuno Subtil, Senior Devtech Engineer at NVIDIA, details the three most important things developers need to know about ray tracing in Vulkan. To learn more, you can attend his talk at GDC: Title: RAY TRACING IN VULKAN Location: Room 205, South Hall Date: Wednesday, March 20 Time: 10:30am – 11:15am Pass Type:…Powered by Discourse, best viewed with JavaScript enabled"
1203,deep-learning-and-data-science-workshops-at-gtc-2022,"Originally published at:			Workshops and Training with DLI at GTC 2022 | NVIDIA
GTC 2022 hands-on workshops in data science and deep learning are just $99 when you register by August 29 (standard price is $500).Powered by Discourse, best viewed with JavaScript enabled"
1204,choosing-the-best-dpu-based-smartnic,"Originally published at:			https://developer.nvidia.com/blog/choosing-the-best-dpu-based-smartnic/
This post defines NICs, SmartNICs, and lays out a cost-benefit analysis for NIC categories and use cases.Powered by Discourse, best viewed with JavaScript enabled"
1205,confidential-compute-on-github,"I saw a new GitHub repo added with Confidential Compute - has this been released? Does it work at the Kernel level?The confidential computing (CC) features of the Hopper H100 are indeed now available in early access; make sure you have driver 535.86 or later!The confidential computing of the H100 is complementary to a system which has a CC enabled CPU (e.g., Intel TDX or AMD’s SEV-SNP). With these style CPUs, you can connect an H100 and begin to keep your code and data secure while in use.Powered by Discourse, best viewed with JavaScript enabled"
1206,nvidia-at-nips-2017,"Originally published at:			NVIDIA at NIPS 2017 | NVIDIA Technical Blog
NVIDIA is headed to NIPS (Neural Information Processing Systems) and we can’t wait to show you our latest AI innovations. Visit our booth (#109) to see cutting-edge technology in action and meet with 40+ members of our AI team focused on research, applied engineering, and solutions engineering. Our partners will also be there, from AI…Powered by Discourse, best viewed with JavaScript enabled"
1207,gtc-2020-quickquery-gpu-based-approximate-query-processing-for-sub-second-exploration-at-scale,"GTC 2020 S21566
Presenters: Larry Rudolph ,Two Sigma Investments, LP; Steven Martin, Two Sigma Investments, LP
Abstract
Exploring data for signals can be viewed as a series of questions, queries, and hypotheses, each of which leads to either deeper exploration or a new line of inquiry. QuickQuery employs sophisticated sampling, exploits memory hierarchy, and leverages the power of GPUs to provide sub-second response times no matter the size of the dataset. Even with trillion-row datasets, researchers can execute many such queries at the speed of thought, all while providing confidence bounds on the results. QuickQuery lets the researcher specify constraints on both accuracy and the response time. It is highly tuned to be very performant; our proof-of-concept implementation on a single GPU server was more than 500 times faster than a 400-core Spark implementation for simple queries, and more than 1,000 times faster for more complex ones.Watch this session
Join in the conversation below.Powered by Discourse, best viewed with JavaScript enabled"
1208,rapids-the-rise-of-notebooks-extended,"Originally published at:			RAPIDS: The Rise of Notebooks Extended | NVIDIA Technical Blog
By Taurean Dyer, Technical Product Manager at NVIDIA As we journey to RAPIDS 1.0, we launched an entire new documentation update (https://docs.rapids.ai/) with our 0.6 release. With our 0.7 release, we are making available a new Notebooks Extended repo on GitHub. You can think of this as the “RAPIDS Community’s” notebooks. Today, Notebooks Extended adds user-centric restructuring, easy…Powered by Discourse, best viewed with JavaScript enabled"
1209,ai-helps-pathologists-detect-prostate-cancer,"Originally published at:			AI Helps Pathologists Detect Prostate Cancer | NVIDIA Technical Blog
According to the American Cancer Society, prostate cancer is the second most common cancer in American men, averaging around 175,000 new cases every year. During the diagnosis process, more than one million men in the U.S. alone undergo a prostate biopsy, a procedure that results in 10-12 needle cores for patients, and more than 10…Powered by Discourse, best viewed with JavaScript enabled"
1210,deep-learning-in-a-nutshell-reinforcement-learning,"Originally published at:			https://developer.nvidia.com/blog/deep-learning-nutshell-reinforcement-learning/
This post is Part 4 of the Deep Learning in a Nutshell series, in which I’ll dive into reinforcement learning, a type of machine learning in which agents take actions in an environment aimed at maximizing their cumulative reward. Deep Learning in a Nutshell posts offer a high-level overview of essential concepts in deep learning.…I'd like to apply this kind of deep machine learning to a problem in human education: the matching of learners' characteristics (background knowledge, learning styles and goals) with learning objects (more properly labeled ""teaching objects"" including Open Educational Resources, free-lance teachers and communities of practice). The machine (AI) would scan the profile of the learner and search online for the most appropriate collection of teaching objects to meet the learner's goals given his/her abilities and background knowledge. Is anyone out there working on this? Please contact me.Liza, the type of problem you are looking to solve is a bit different to the thrust of this article (Reinforcement Learning), which considers how to learning a task based on the feedback received from repeated attempts/experience (e.g. playing a game over and over). The problem you pose could be addressed with Machine Learning IF there was sufficient data that could be analyzed that links the learning techniques that have worked for people with different learning characteristics. ""sufficient"" here typically means a LOT. Various techniques can then be used to determine any pattern between the learner characteristics and the most suitable learning resource. If such data exists or can be obtained then I'm sure someone could/would look into it...Thanks for the comments, Carl. I don't think the data we need exists yet in useable form because a) we don't aggregate user responses to the learning objects they use and b) we don't have well-elaborated learner profiles. But ya' gotta' start somewhere. I'm starting by writing about the idea. Hopefully someone will be able to provide the technical expertise and facilities to begin  data collection. I know a lot about cognitive learning characteristics and how measure them. Others will have to throw in the AI and computing power. We'd have to start small and develop a robust matching model. With those two in place there are plenty of learners out in the world to let the machine rip. Because personalizing instruction is such a core issue in education I think this research is fundable -- with the right team. Are you ready to come on board?It's on the Nvidia site already, anyway I guess the idea is I have is to evolve an aLife using a deep neural network and ""soft"" associative memory (AM.)  The AM only needs an approximately similar input to what it has seen before to produce a meaningful output.  If you don't overload the memory it has repetition code error correction.  Overall that should make it easier for the deep neural network to learn how to use it.Then it should be able to learn any algorithms it needs to survive.https://devtalk.nvidia.com/...I'm working on how to link the two together at the moment.Powered by Discourse, best viewed with JavaScript enabled"
1211,top-5-deep-learning-research-sessions-at-gtc-2019,"Originally published at:			Top 5 Deep Learning & Research Sessions at GTC 2019 | NVIDIA Technical Blog
NVIDIA’s GPU Technology Conference (GTC) is the premier AI and deep learning conference, providing training, insights, and direct access to experts from leading research institutions. Watch the video below to get a preview of some of the deep learning and research sessions at GTC.Powered by Discourse, best viewed with JavaScript enabled"
1212,ai-app-identifies-plants-and-animals-in-seconds,"Originally published at:			AI App Identifies Plants and Animals In Seconds | NVIDIA Technical Blog
The popular website for nature lovers, iNaturalist.org is launching a deep learning-based app that automatically identifies plants and animals down to the species level. Founded in 2008 by students at University of California, Berkeley and recently acquired by the California Academy of Sciences, iNaturalist has until now been solely a crowdsourcing site. Users upload a…Powered by Discourse, best viewed with JavaScript enabled"
1213,join-me-and-other-nvidia-experts-at-the-gpu-technology-conference,"Originally published at:			https://developer.nvidia.com/blog/join-me-and-other-nvidia-experts-gpu-technology-conference/
NVIDIA’s GPU Technology Conference (GTC) 2013, scheduled for March 18-21, is the premier event for accelerated computing. This will be the fourth GTC and it just keeps getting better. If you haven’t been to GTC before you won’t be disappointed. Thousands of developers and research scientists from over 40 countries will converge on the San…Powered by Discourse, best viewed with JavaScript enabled"
1214,a-guide-to-cuda-graphs-in-gromacs-2023,"Originally published at:			https://developer.nvidia.com/blog/a-guide-to-cuda-graphs-in-gromacs-2023/
This post describes how CUDA Graphs have been recently leveraged by GROMACS, a simulation package for biomolecular systems and one of the most highly used scientific software applications worldwide.Following the example in this blog, I receive the gromacs error
Inconsistency in user input:
Bonded interactions can not be computed on a GPU:
None of the bonded types are implemented on the GPU.I assumed graph support enabled these computations. My build is the current version from the gitlab provided. Is there a particular release or branch required to experiment with this?Thanks and great article.Powered by Discourse, best viewed with JavaScript enabled"
1215,gtc-2020-accelerating-linguistically-informed-bert-with-kubeflow-at-linkedin,"GTC 2020 S22163
Presenters: Eddie Weill,NVIDIA; Abin Shahab,LinkedIn
Abstract
Kubeflow at LinkedIn has expanded beyond notebooks, training, and serving in the past year. We have now integrated ML workflows on Kubernetes with the Hadoop Distributed File System (HDFS). We’ll explain why we integrated Kerberized HDFS with Kubernetes, our implementation choices, and current challenges. We’re also working on multi-node, multi-GPU experiments with Kubeflow’s MPIJob operator, pre-training BERT using LinkedIn’s data, and hyper-parameter tuning with Microsoft Neural Network Intelligence (NNI) using Kubeflow to schedule distributed-training trials. We’ll discuss how we trained the models, fine-tuning, knowledge distillation, and model and experiment performance. We’re training models leveraging PyTorch to generate member graph embedding. We’ll discuss link prediction, including member-to-member and member-to-entity such as skill, title, and company.Watch this session
Join in the conversation below.Powered by Discourse, best viewed with JavaScript enabled"
1216,gpu-pro-tip-fast-histograms-using-shared-atomics-on-maxwell,"Originally published at:			GPU Pro Tip: Fast Histograms Using Shared Atomics on Maxwell | NVIDIA Technical Blog
Histograms are an important data representation with many applications in computer vision, data analytics and medical imaging. A histogram is a graphical representation of the data distribution across predefined bins. The input data set and the number of bins can vary greatly depending on the domain, so let’s focus on one of the most common use…Thanks for the interesting article!Are there any optimisation ideas on computing multidimensional histograms?With the size about 32^3 - 32^4 collisions doesn't matter much, but random access to the large hist array does. So I just store one histogram in global memory and fill it with atomics - so simple but found nothing better.Hello, Serge,Yes, multi-dimensional histograms can be represented as linear histograms with the large amount of bins. In case of 32^3 = 32K bins, you won’t be able to keep the full copy of the histogram in the shared memory. If you have a lot of contention to a specific address you can try a multi-pass approach similar to a radix sort. If the data distribution is fairly random, you’re right, just plain simple global atomics to a single histogram array might be the best choice here. In this case the threads won’t be competing with each other for atomic increments so you will be mostly limited by write bandwidth to global memory.Thanks,Nikolay.Ok, thank you!Just wanted to say thank you for this post! It helped tremendously in a research project I was working on which involved measuring the galaxy bispectrum, which is effectively a large histogram. I even cited this post as a reference. Here's the paper on arXiv: https://arxiv.org/abs/1712....Hello - there are a few major errors in your shared atomics kernels.  They are as follows:1.  In the first kernel, out += g * NUM_PARTS; should be out += g * NUM_BINS;2.  In the second kernel, for (int j = 0; j < n; j++) should be written as for (int j = 0; j < NUM_PARTS; j++)     One should be as clear as possible in their presentation when sharing information.3.  In the second kernel, total += in[i + NUM_PARTS * j]; should be total += in[i + NUM_BINS * j];The code as provided cannot achieve the desired objective in all cases as it results in access violations.  It's surprising to me that Nvidia engineers wouldn't have tested code with known input values and CUDA-MEMCHECK prior to posting.Hi Rajeev,1. The original version with out += g * NUM_PARTS is correct since each threadblock will be writing a separate histogram per channel, so up to NUM_BINS * <number of="""" channels=""""> which is basically NUM_PARTS in the example code above. Sorry that it was not clear in the post. I think the subsequent points #2 and #3 in your comment would be resolved by the clarification I provided on #1, but just in case to reiterate:2. In the second kernel, thread i is summing up values from i-th bins across all per-block histograms, so the loop will be from 0 to n where n is the number of threadblocks from the first kernel.3. Each block from the first kernel will be using NUM_PARTS as the offset so we need to use the same offset in the 2nd kernel when merging the per-block histograms.Nikolay.Hi Nikolay,In the first kernel using shared memory, why do you initialize smem[3 * NUM_BINS + 3]? Suppose that NUM_BIN is 256, so the size of smem is 771. But in the loop for writing partial histogram into the global memory, the i value varies from 0 to 255, which means smem at index 256, 513 and 770 is never accessed. Is there any purpose for initializing smem with one extra index for each channel?Hi Cao, the smem size is initialized to 3 * NUM_BINS + 3 because I access channels with an offset, for example, atomicAdd(&smem[NUM_BINS * 2 + b + 2], 1) - the 3rd (blue) channel is accessed with offset 2, so we have to allocate 3 * NUM_BINS + 3 to avoid out of memory errors. My original intention was to skew/shift the channels position in shared memory to avoid bank conflicts, however, I don't think that this matters between two separate atomic instructions, so I think you can reduce the size to just 3 * NUM_BINS and then accordingly update the smem indices in atomicAdds and combining into the global histogram. Thanks for the feedback!Hi Nikolay,It is still unclear to me what NUM_PARTS is. In your comment, you say that NUM_BINS * num_channels = NUM_PARTS, however, in your code (following the link to the CUB repo), NUM_PARTS seems to be arbitrarily set to 1024. Can you explain what NUM_PARTS is?Thanks,ZanWe can further improve performance on reducing the  global bins in the second kernel call by using some popular optimized parallel reduction techniques, but to make a difference, the bin size should be quite large.Powered by Discourse, best viewed with JavaScript enabled"
1217,gpus-help-microsoft-build-record-breaking-image-recognition-system,"Originally published at:			GPUs Help Microsoft Build Record-Breaking Image Recognition System | NVIDIA Technical Blog
Thanks to the use of GPUs, Microsoft researchers achieved record results on ImageNet, a prestigious image-recognition benchmark. Compared to last year, Microsoft’s system cut the top-5 error rate by half, correctly classifying images within 1,000 pre-defined categories more than 96 percent of the time. Their system uses a 152-layer neural network, which is nearly five…Powered by Discourse, best viewed with JavaScript enabled"
1218,upcoming-webinar-performant-multiphase-flow-simulation-at-leadership-class-scale-using-openacc,"Multi-phase and multi-component flow are central to a wide range of engineering problems. Attend an upcoming webinar to learn how Georgia Tech researchers implemented a three-fold approach to accelerate the open-source solver MFC on #GPUs. Register today: Performant Multiphase Flow Simulation at Leadership-Class Scale via OpenACCPowered by Discourse, best viewed with JavaScript enabled"
1219,tips-for-creating-a-meaningful-and-successful-virtual-hackathon,"Originally published at:			https://developer.nvidia.com/blog/tips-for-creating-a-meaningful-and-successful-virtual-hackathon/
Providing key elements of mentoring,  socializing, and specialized training proved successful for the virtually 2021 KISTI GPU Hackathon.Powered by Discourse, best viewed with JavaScript enabled"
1220,webinar-ai-enabled-cybersecurity-for-financial-services,"Originally published at:			AI-Enabled Cybersecurity for Financial Services
Learn how financial firms can build automated, real-time fraud and threat detection solutions with NVIDIA Morpheus.Powered by Discourse, best viewed with JavaScript enabled"
1221,top-ai-video-analytics-sessions-at-nvidia-gtc-2023,"Originally published at:			https://developer.nvidia.com/blog/top-ai-video-analytics-sessions-at-nvidia-gtc-2023/
Explore the latest software and developer tools to build, deploy, and scale vision AI and IoT apps.Powered by Discourse, best viewed with JavaScript enabled"
1222,nvswitch-leveraging-nvlink-to-maximum-effect,"Originally published at:			NVSwitch: Leveraging NVLink to Maximum Effect | NVIDIA Technical Blog
GPUs have been PCIe devices for many generations in client systems, and more recently in servers. The rapid growth in deep learning workloads has driven the need for a faster and more scalable interconnect, as PCIe bandwidth increasingly becomes the bottleneck at the multi-GPU system level. As deep learning neural networks become more sophisticated, their…Powered by Discourse, best viewed with JavaScript enabled"
1223,startup-develops-an-ai-based-system-to-treat-neurological-disorders,"Originally published at:			Startup Develops an AI-based System to Treat Neurological Disorders | NVIDIA Technical Blog
BrainQ is an Israeli-based AI startup that focuses on helping stroke and spinal cord injury patients receive new and innovative treatment, including a deep learning-based system that analyzes a patient’s brain waves and generates a precise treatment plan. The company recently announced it has raised $8.8 million in funding to continue investing in its AI business.…Powered by Discourse, best viewed with JavaScript enabled"
1224,can-machine-learning-help-write-a-hit-musical,"Originally published at:			Can Machine Learning Help Write a Hit Musical? | NVIDIA Technical Blog
Computers can drive, create recipes, even compose rap songs. London audiences will soon find out whether they can write a hit musical, too. The new show, “Beyond the Fence,” is the world’s first musical conceived and substantially crafted by computer. Opening next month in the West End, it’s an experiment to determine how technology affects…Powered by Discourse, best viewed with JavaScript enabled"
1225,analysis-driven-optimization-finishing-the-analysis-with-nvidia-nsight-compute-part-3,"Originally published at:			https://developer.nvidia.com/blog/analysis-driven-optimization-finishing-the-analysis-with-nvidia-nsight-compute-part-3/
In part 1, I introduced the code for profiling, covered the basic ideas of analysis-driven optimization (ADO), and got you started with the NVIDIA Nsight Compute profiler. In part 2, you began the iterative optimization process. In this post, you finish the analysis and optimization process, determine whether you have reached a reasonable stopping point,…Powered by Discourse, best viewed with JavaScript enabled"
1226,build-and-deploy-ai-hpc-and-data-analytics-software-faster-using-ngc,"Originally published at:			Build and Deploy AI, HPC, and Data Analytics Software Faster Using NGC | NVIDIA Technical Blog
NGC, NVIDIA’s GPU-optimized hub for AI, HPC, and data analytics software accelerates end-to-end workflows. The 100+ models and industry-specific SDKs on NGC, help data scientists and developers build solutions, gather insights, and deliver business value faster than ever before. NGC software can deploy on-premise, cloud, or at the edge.  We have recently added new containers,…Powered by Discourse, best viewed with JavaScript enabled"
1227,nvidia-gpus-power-first-self-driving-shuttle,"Originally published at:			NVIDIA GPUs Power First Self-Driving Shuttle | NVIDIA Technical Blog
The six-passenger WEpod shuttle became the world’s first vehicle without a steering wheel to be given license plates. Without any special lanes, magnets or rails, the shuttle successfully navigates between two towns in the Netherlands. Created by a team of researchers from Delft University of Technology, WEpod use NVIDIA GPUs to tackle the massive computing…Powered by Discourse, best viewed with JavaScript enabled"
1228,real-time-facial-expression-transfer,"Originally published at:			Real-time Facial Expression Transfer | NVIDIA Technical Blog
A new GPU-based facial reenactment technique tracks the expression of a source actor and transfers it to a target actor in real-time – which translates into you being able to control another human’s expressions. The project is a collaboration of researchers from Stanford University, Max Planck Institute for Informatics and University of Erlangen-Nuremberg. The novelty…Powered by Discourse, best viewed with JavaScript enabled"
1229,big-data-is-saving-this-little-bird,"Originally published at:			Big Data is Saving this Little Bird | NVIDIA Technical Blog
A Santa Cruz, California based company is using big data and deep learning to improve conservation efforts. California birdwatchers can go a lifetime without seeing the globally endangered marbled murelett bird and with now with remote acoustic sensors and deep learning, biologists are now able to analyze the audio of the bird, and keep better…Powered by Discourse, best viewed with JavaScript enabled"
1230,inside-the-programming-evolution-of-gpu-computing,"Originally published at:			Inside the Programming Evolution of GPU Computing | NVIDIA Technical Blog
In a recent interview, NVIDIA’s VP of Accelerated Computing Ian Buck talks about the history of using GPUs for more than just game graphics. Back in 2000, Buck and a small computer graphics team at Stanford University were watching the steady evolution of computer graphics processors for gaming and thinking about how such devices could…Powered by Discourse, best viewed with JavaScript enabled"
1231,nvidia-jetson-based-robots-excel-in-darpa-underground-competition,"Originally published at:			https://developer.nvidia.com/blog/jetson-robots-darpa-underground-competition/
With NVIDIA Jetson embedded platforms, teams at the DARPA SubT Challenge detected objects with both high accuracy and high throughput.Powered by Discourse, best viewed with JavaScript enabled"
1232,spotscale-uses-ai-to-create-3d-building-models,"Originally published at:			https://developer.nvidia.com/blog/spotscale-uses-ai-to-create-3d-building-models/
Spotscale, a Swedish startup, uses machine learning to transform standard aerial images into realistic 3D models. The company, a member of NVIDIA’s Inception Program, attended the annual Computer Vision and Pattern Recognition conference in Salt Lake City, Utah this week and we caught up with them to learn more about their technology. “We can compute massive…Powered by Discourse, best viewed with JavaScript enabled"
1233,running-the-new-toolkit-on-python-efficiently,"Hello! Hope you’re having a great day, it’s 1AM here :)I’m currently working on creating a high-performance, optimized library with gradient aggregation rules in our ML application for very large tensors/vectors, which is already speeding up our work quite well. However, some parts of the code is in PyTorch, as running CUDA with a C++ script integrated in python often doesn’t provide the best results, it requires lots of data transfer between the CPU and GPU almost every time.However, I still want to utilize CUDA more. I’m wondering, with the new Toolkit, is there a way to efficiently run CUDA in python? And if so, how to do that?Thanks for the answer in advance!
SerhanThere are a few frameworks like Numba that can help you accelerate CUDA code onto the GPU, through just-in-time compilation of either C/C++ code (usually provided through a separate file or Python string), or through compilation of Python functions directly with decorators.This is an area we’re actively investigating, though, so stay tuned for more!Thanks for the answer!I’m excited to hear the development on that side, I think it could be really beneficial in the world of AI!Powered by Discourse, best viewed with JavaScript enabled"
1234,how-well-does-cugraph-scale,"How well does cuGraph scale? Any information you can share would be interestingWe have tested PageRank and Louvain on 1000+ GPUs. Many other algorithms (except for few with legacy implementations) should scale as well even though we haven’t tested every algorithm for 1000+ GPUs. If you need to run very large graph analytics and if your algorithm of interest does not scale well enough, pleae submit an issue in the cuGraph github page. For more academic reference, see Analyzing Multi-trillion Edge Graphs on Large GPU Clusters: A Case Study with PageRank | IEEE Conference Publication | IEEE XploreThis topic was automatically closed 14 days after the last reply. New replies are no longer allowed.Powered by Discourse, best viewed with JavaScript enabled"
1235,accelerate-video-analytics-development-with-deepstream-2-0,"Originally published at:			Accelerate Video Analytics Development with DeepStream 2.0 | NVIDIA Technical Blog
The sheer scale of the smart city boggles the mind. Tens of billions of sensors will be deployed worldwide, used to make every street, highway, park, airport, parking lot, and building more efficient. This translates to better designs of our roadways to reduce congestion, stronger understanding of consumer behavior in retail environments, and the ability…Kaustabh, Very well written. Quick question on the support for other GPUs. I understand that DeepStream is officially supported on Tesla and Jetson. How about Titan Xp? Are there any known limitations? We are looking at creating a multichannel video analytics solution using DeepStream & Titan Xp. Please advise on feasibility.Powered by Discourse, best viewed with JavaScript enabled"
1236,build-better-iva-applications-for-edge-devices-with-nvidia-deepstream-sdk-on-jetson,"Originally published at:			Build Better IVA Applications for Edge Devices with NVIDIA DeepStream SDK on Jetson | NVIDIA Technical Blog
Register Now for Early Access Program NVIDIA’s DeepStream SDK on Jetson makes it easy for developers to create robust, complex intelligent video analytics (IVA) capabilities for edge devices. Rapidly build end-to-end modular and scalable deep learning solutions — from intelligent cameras to appliances — with applications for smart cities, robotics, and industrial automation. The SDK…Powered by Discourse, best viewed with JavaScript enabled"
1237,programming-tensor-cores-in-cuda-9,"Originally published at:			https://developer.nvidia.com/blog/programming-tensor-cores-cuda-9/
Tensor cores provide a huge boost to convolutions and matrix operations. Tensor cores are programmable using NVIDIA libraries and directly in CUDA C++ code. A defining feature of the new Volta GPU Architecture is its Tensor Cores, which give the Tesla V100 accelerator a peak throughput 12 times the 32-bit floating point throughput of the previous-generation…Tensor-Cores - will it also be available within the next Gamer-GPU (Geforce) ?That's a good question !interesting possibilities for crypto currencies using tensor cores...Which NN framework did you use for Figure4 inference?This will really help the 3D porn industry, oh and cancer research, possibly...When is ""Titan W"" coming out? You know, Two Titan-V's in one card?Are you waiting for ""Windows 4K"" to be made?I concur, I think I even know the theoretical solution: For each instance of CUDA algorithm line calculation, it gets stored in Matrix A until 16 instances are filled, then Stored into B, where their computation is multiplied which ought to lower the the net computation time by 4096 per tensor core utilized.... I've already seen this applied with the cryptonight algorithm, in terms of Hash Calculation however it was insufficient to generate a nonce. Create the nonce with that version of cryptonight and you revolutionize cryptomining!10 months later, the answer is yes.Extremely helpful. Thanks as always :)Question: Do the Tensor cores run concurrently with the CUDA cores?  If I were to have my deep learning model cranking away on TCs, could I simultaneously be rendering high quality graphics?Nice blog but it misses the most important statement - ""Tensor Cores require that the Tensors be in NHWC data layout."" So if NCHW is given, it transposes it to NHWC.Can you provide information about relative performance V100 / RTX 2080 TI or V100 / RTX 2080 ?Thank youGEMMs that do not satisfy the above rules will fall back to a non-Tensor Core implementationthis sounds like a silent failure, and a really bad thing
i assume hope theres some function to assert or check that TPUs are being used?I think there is a technical error in this image https://developer-blogs.nvidia.com/wp-content/uploads/2017/12/tensor_cube_white-624x934.png. There should be 16 green layers instead of 12. As tensor core only performing multiplication of a 4x4 data, as a result 16 -4x4 array will be generated.I have a little question for tensor core code example in blog.
Since the threadblock dim3 is (128, 4), namely 16 warps, I think the loop over k is should be 4xWMMA_K which would optimize the performance.
Would this change the loop stride affect the correctness of result ?Powered by Discourse, best viewed with JavaScript enabled"
1238,embedded-machine-learning-with-the-cudnn-deep-neural-network-library-and-jetson-tk1,"Originally published at:			https://developer.nvidia.com/blog/embedded-machine-learning-cudnn-deep-neural-network-library-jetson-tk1/
GPUs have quickly become the go-to platform for accelerating machine learning applications for training and classification. Deep Neural Networks (DNNs) have grown in importance for many applications, from image classification and natural language processing to robotics and UAVs. To help researchers focus on solving core problems, NVIDIA introduced a library of primitives for deep neural…Will CNTK and Tensor Flow work on this?  Is there a sample setup with a screen for output or hooking up a laptop and deploying to it?Powered by Discourse, best viewed with JavaScript enabled"
1239,machine-composes-harmony-that-sounds-like-bach,"Originally published at:			Machine Composes Harmony That Sounds Like Bach | NVIDIA Technical Blog
Researchers from Sony developed a deep learning machine that generated a harmony in the style of Johann Sebastian Bach, one of the great composers of baroque music. Using a GTX 980 Ti GPU, CUDA, and cuDNN with the TensorFlow deep learning framework, the researchers trained their neural network on 352 chorale harmonizations composed by Bach…Powered by Discourse, best viewed with JavaScript enabled"
1240,bidmach-machine-learning-at-the-limit-with-gpus,"Originally published at:			https://developer.nvidia.com/blog/bidmach-machine-learning-limit-gpus/
Deep learning has made enormous leaps forward thanks to GPU hardware. But much Big Data analysis is still done with classical methods on sparse data. Tasks like click prediction, personalization, recommendation, search ranking, etc. still account for most of the revenue from commercial data analysis. The role of GPUs in that realm has been less…Powered by Discourse, best viewed with JavaScript enabled"
1241,accelerating-etl-for-recommender-systems-on-nvidia-gpus-with-nvtabular,"Originally published at:			Accelerating ETL for Recommender Systems on NVIDIA GPUs with NVTabular | NVIDIA Technical Blog
Recommender systems are ubiquitous in online platforms, helping users navigate through an exponentially growing number of goods and services. These models are key in driving user engagement. With the rapid growth in scale of industry datasets, deep learning (DL) recommender models have started to gain advantages over traditional methods by capitalizing on large amounts of…Powered by Discourse, best viewed with JavaScript enabled"
1242,optimizing-bim-workflows-using-usd-at-every-design-phase,"Originally published at:			https://developer.nvidia.com/blog/optimizing-bim-workflows-using-usd-at-every-design-phase/
Siloed data has long been a challenge in architecture, engineering, and construction (AEC), hindering productivity and collaboration. However, new innovative solutions are transforming the way that architects, engineers, and construction managers work together on BIM (building information management) workflows, offering new possibilities for real-time collaboration. The new NVIDIA Omniverse Connector from Vectorworks exemplifies this potential,…Powered by Discourse, best viewed with JavaScript enabled"
1243,best-practices-explainable-ai-powered-by-synthetic-data,"Originally published at:			https://developer.nvidia.com/blog/best-practices-explainable-ai-powered-by-synthetic-data/
Learn how financial institutions are using high-quality synthetic data to validate explainable AI models and comply with data privacy regulations.Powered by Discourse, best viewed with JavaScript enabled"
1244,share-your-science-predicting-healthcare-outcomes-with-artificial-intelligence,"Originally published at:			https://developer.nvidia.com/blog/share-your-science-predicting-healthcare-outcomes-with-artificial-intelligence/
Joel Dudley, an Associate Professor at the Icahn School of Medicine at Mount Sinai in New York shares how he is developing and applying advanced computational methods to integrate the digital universe of information to build better predictive models of diseases and drug response. Using TITAN X GPUs and deep learning, his lab is training…Powered by Discourse, best viewed with JavaScript enabled"
1245,tips-on-scaling-storage-for-ai-training-and-inferencing,"Originally published at:			https://developer.nvidia.com/blog/tips-on-scaling-storage-for-ai-training-and-inferencing/
There are many benefits of GPUs in scaling AI, ranging from faster model training to GPU-accelerated fraud detection. While planning AI models and deployed apps, scalability challenges—especially performance and storage—must be accounted for.  Regardless of the use case, AI solutions have four elements in common:  Training model Inferencing app Data storage  Accelerated compute  Of these elements, data storage…While writing this blog, I spoke with AI solutions creators and IT professionals. As a result, I learned of several important factors that are not always considered in deployments. This includes storage scalability, availability, and adaptability as these are not always fully evaluated. Additionally, I learned that even a well-designed POC does not necessarily address future adaptability and challenges. For those fully aware of the points made in this blog, I’m hopeful this will serve as a good checklist for future reference. For others, it’s my hope this will cause existing plans to be re-evaluated in the light of storage scalability for training and inference. I’d love to hear your comments and/or questions!Powered by Discourse, best viewed with JavaScript enabled"
1246,deploy-ai-models-with-confidence-with-the-new-model-credentials-feature-from-nvidia-ngc,"Originally published at:			Deploy AI Models with Confidence with the New Model Credentials Feature from NVIDIA NGC | NVIDIA Technical Blog
The pre-trained models on the NVIDIA NGC catalog offer state of the art accuracy for a wide variety of use-cases including natural language understanding, computer vision, and recommender systems. NGC models now include the important credentials that help data scientists and developers quickly identify the right model to deploy for their AI software development. These…Powered by Discourse, best viewed with JavaScript enabled"
1247,nvidia-rtx-top-3-week-of-january-16-2018,"Originally published at:			NVIDIA RTX Top 3: Week of January 16, 2018 | NVIDIA Technical Blog
Every week, we’ll be delivering 3 interesting stories coming from the world of RTX Game Development. 1 – Atomic Heart Tech Demo “From NVIDIA’s CES 2019 livestream, watch the latest preview of Atomic Heart, which features improved ray-traced graphics accelerated by the addition of DLSS.” 2 – What is DLSS? In this article, How-to Geek…Powered by Discourse, best viewed with JavaScript enabled"
1248,fast-large-scale-agent-based-simulations-on-nvidia-gpus-with-flame-gpu,"Originally published at:			https://developer.nvidia.com/blog/fast-large-scale-agent-based-simulations-on-nvidia-gpus-with-flame-gpu/
The COVID-19 pandemic has brought the focus of agent-based modeling and simulation (ABMS) to the public’s attention. It’s a powerful computational technique for the study of behavior, whether epidemiological, biological, social, or otherwise. The process is conceptually simple: Behaviors of individuals are described (the model). Inputs to the model are provided. The simulation of many…Author of this work here. So pleased to see it on the NVIDIA blog. If people have questions about it then feel free to ask. We also have a GitHub discussions board for technical questions about using the software: Discussions · FLAMEGPU/FLAMEGPU2 · GitHubPowered by Discourse, best viewed with JavaScript enabled"
1249,gtc-2020-drones-machetes-fire-and-vr-21st-century-tools-for-social-and-sustainable-impact,"GTC 2020 S21515
Presenters: Dace Campbell,McKinstry
Abstract
We’ll describe and compare workflows developed for projects on the islands of Kosrae, Micronesia and Vorovoro, Fiji. These projects—collaboratively executed with indigenous locals, universities, social-impact organizations, and technology corporations—built immersive experiences to support grant proposals, feasibility studies, and mitigation plans. Our climate is changing, and our global population is increasing. Remote, indigenous communities are developing to accommodate growth and responsible tourism in the face of catastrophic environmental and political forces. Technology is now available to support and promote development of remote communities in an environmentally and culturally sustainable manner. Specifically, photogrammetry and virtual reality enable effective decisions about mitigating the effects of climate change and developing sustainably by empowering stakeholders with intuitive, immersive experience of site conditions from new points of view.Watch this session
Join in the conversation below.Powered by Discourse, best viewed with JavaScript enabled"
1250,reducing-the-risk-of-second-breast-cancer-surgery-with-ai,"Originally published at:			Reducing the Risk of Second Breast Cancer Surgery with AI | NVIDIA Technical Blog
Researchers from Lehigh University in Pennsylvania developed a diagnostic technique that combines deep learning and cutting-edge imaging technology to detect in real-time the difference between cancerous and benign cells with over 90% accuracy. “The idea is that one day, if this technique could be used during surgery, it could complement the histopathology, potentially reducing the…Powered by Discourse, best viewed with JavaScript enabled"
1251,future-of-data-science-looks-spectacular,"Originally published at:			Future of Data Science Looks Spectacular | NVIDIA Technical Blog
Australia’s first Data Arena powered by by nine NVIDIA Quadro K6000, with 27,000 CUDA cores lets you literally see, hear and feel data sets through 3D visualization. A few software developers from the University of Technology Sydney built a 3D, 360-degree data visualization room to help researchers and data scientists intuitively explore huge and complex data…Powered by Discourse, best viewed with JavaScript enabled"
1252,top-deep-learning-sessions-at-nvidia-gtc-2023,"Originally published at:			Developer Conference Session Catalog | NVIDIA GTC
Explore the latest tools, optimizations, and best practices for deep learning training and inference.Powered by Discourse, best viewed with JavaScript enabled"
1253,i-need-antennas-for-my-house-where-is-the-best-place-to-find-them-is-it-possible-for-you-to-help-me,"Can anyone help me getting best antenna for me?Powered by Discourse, best viewed with JavaScript enabled"
1254,deploying-xr-applications-in-private-networks-on-a-server-platform,"Originally published at:			https://developer.nvidia.com/blog/deploying-xr-applications-in-private-networks-on-a-server-platform/
Learn how servers can be built to deliver immersive workloads and take advantage of compute power to combine streaming XR applications with AI and other computing functions.Powered by Discourse, best viewed with JavaScript enabled"
1255,workshop-how-to-enable-your-product-with-voice-interface,"Originally published at:			Workshop – NVIDIA Riva
This hands-on workshop guides you through the process of voice-enabling your product, from familiarizing yourself with NVIDIA Riva to assessing the costs and resources required for your project.Powered by Discourse, best viewed with JavaScript enabled"
1256,ai-at-the-edge-challenge-spotlight-sim-to-real-an-effective-robot-navigation-framework,"Originally published at:			https://developer.nvidia.com/blog/ai-at-the-edge-challenge-spotlight-sim-to-real-an-effective-robot-navigation-framework/
Autonomous robot navigation is a hard problem that is best tackled with deep learning frameworks running on powerful AI platforms. Students of the National Tsing Hua University in Taiwan presented just such an easy-to-implement, modular robot navigation framework on the Jetson AI platform and won the second prize in the recently concluded AI at the…Powered by Discourse, best viewed with JavaScript enabled"
1257,applying-federated-learning-to-traditional-machine-learning-methods,"Originally published at:			https://developer.nvidia.com/blog/applying-federated-learning-to-traditional-machine-learning-methods/
In the era of big data and distributed computing, traditional approaches to machine learning (ML) face a significant challenge: how to train models collaboratively when data is decentralized across multiple devices or silos. This is where federated learning comes into play, offering a promising solution that decouples model training from direct access to raw training…Powered by Discourse, best viewed with JavaScript enabled"
1258,nvidia-udacity-can-help-you-build-a-career-creating-robot-software,"Originally published at:			NVIDIA, Udacity Can Help You Build a Career Creating Robot Software | NVIDIA Technical Blog
Building robots isn’t just a hobby anymore. Now you can build a career creating the software that will make bots better. Step one: enroll in Udacity’s Robotics Software Engineer Nanodegree program. NVIDIA’s Deep Learning Institute is now working with the online learning provider to deliver a four-month program that will immerse students in the field…Powered by Discourse, best viewed with JavaScript enabled"
1259,simplifying-ai-model-deployment-at-the-edge-with-nvidia-triton-inference-server,"Originally published at:			https://developer.nvidia.com/blog/simplifying-ai-model-deployment-at-the-edge-with-triton-inference-server/
Learn how to simplify AI model deployment at the edge with NVIDIA Triton Inference Server on NVIDIA Jetson. Triton Inference Server is available on Jetson starting with the JetPack 4.6 release.Powered by Discourse, best viewed with JavaScript enabled"
1260,new-optimizations-to-accelerate-deep-learning-training-on-nvidia-gpus,"Originally published at:			New Optimizations To Accelerate Deep Learning Training on NVIDIA GPUs | NVIDIA Technical Blog
The pace of AI adoption across diverse industries depends on maximizing data scientists’ productivity. NVIDIA releases optimized NGC containers every month with improved performance for deep learning frameworks and libraries, helping scientists maximize their potential. NVIDIA continuously invests in the full data science stack, including GPU architecture, systems, and software stacks. This holistic approach provides the…Powered by Discourse, best viewed with JavaScript enabled"
1261,the-new-parallel-forall,"Originally published at:			The New Parallel Forall | NVIDIA Technical Blog
Today I’m excited to introduce you to the new and improved Parallel Forall blog. For over a year and a half, Parallel Forall has been a part of NVIDIA’s CUDA Zone website, but to better serve the parallel programming community, today we’re launching a dedicated home for Parallel Forall at https://developer.nvidia.com/blog/parallelforall. In addition to a fantastic…Good going...Powered by Discourse, best viewed with JavaScript enabled"
1262,capturing-deep-learning-data-for-neural-network-training,"Originally published at:			Capturing Deep Learning Data for Neural Network Training | NVIDIA Technical Blog
At GDC 2019, NVIDIA presented a talk called “Truly Next Gen: Adding Deep Learning to Games & Graphics”, which detailed the current state-of-the-art in deep learning for games. In the excerpt below, Anjul Patney, Senior Research Scientist at NVIDIA, provides information on how to capture data for neural network training. He also describes common deep…Powered by Discourse, best viewed with JavaScript enabled"
1263,malware-detection-in-executables-using-neural-networks,"Originally published at:			Malware Detection in Executables Using Neural Networks | NVIDIA Technical Blog
The detection of malicious software (malware) is an increasingly important cyber security problem for all of society. Single incidences of malware can cause millions of dollars in damage. The current generation of anti-virus and malware detection products typically use a signature-based approach, where a set of manually crafted rules attempt to identify different groups of known…Indeed. At the risk of sounding like a company shill, this is exactly why Cylance was founded 5 years ago. You can read more about Cylance Protect here.Good article. Whether the data and code of the experiments is available for further research.Excuse me, I have a question about the training data. In the article, you imply that the size of an exe is almost 1-2M. But usually, more exes have a size more than that. If the size is bigger, the memery and calculation problems will appear. How can we reduce the size of an exe to 1-2M?With simple crop would be enought to resolve your problem.Powered by Discourse, best viewed with JavaScript enabled"
1264,model-parallelism-virtual-workshop,"Originally published at:			​ - Model Parallelism: Building and Deploying Large Neural Networks (NALA)
Learn to build and deploy large neural networks to production with this virtual workshop on May 3 from the NVIDIA Deep Learning Institute.Powered by Discourse, best viewed with JavaScript enabled"
1265,nvidia-introduces-precompiled-driver-packages-for-rhel-8-to-streamline-installs,"Originally published at:			https://developer.nvidia.com/blog/nvidia-introduces-precompiled-driver-packages-for-rhel-8-to-streamline-installs/
Nvidia is now offering precompiled drivers that are tested and packaged for targeted kernel versions. Precompiled drivers enable faster, seamless installation and driver updates, faster boot-up, and have reduced dependencies on external tools and repositories. Developers can now take advantage of NVIDIA precompiled drivers quite easily, using our kernel module (kmod) packages and modularity stream…Powered by Discourse, best viewed with JavaScript enabled"
1266,gpus-are-the-unlikely-secret-making-cars-much-smarter,"Originally published at:			https://developer.nvidia.com/blog/gpus-are-the-unlikely-secret-making-cars-much-smarter/
In a recent interview with TIME, NVIDIA’s senior director of automotive Danny Shapiro shares how the company’s innovations in gaming graphics are well-suited to the needs of autonomous vehicles. Driverless cars, which take passengers from A to B with minimal human input, are already hitting American roads. A variety of automakers and technology firms are…Powered by Discourse, best viewed with JavaScript enabled"
1267,accelerating-fortran-do-concurrent-with-gpus-and-the-nvidia-hpc-sdk,"Originally published at:			https://developer.nvidia.com/blog/accelerating-fortran-do-concurrent-with-gpus-and-the-nvidia-hpc-sdk/
Fortran developers have long been able to accelerate their programs using CUDA Fortran or OpenACC. Now with the latest 20.11 release of the NVIDIA HPC SDK, the included NVFORTRAN compiler automatically accelerates DO CONCURRENT, allowing you to get the benefit of the full power of NVIDIA GPUs using ISO Standard Fortran without any extensions, directives,…I heard about this in SC18. Glad that it has finally become reality! super useful. looking forward to full implementation without the limitations mentioned in the blog post.When is it scheduled for release?We appreciate your interest! We are continuing to work on removing limitations, so be sure to keep an eye out for upcoming releases with improvements.Hi. The DO CONCURRENT implementation will be available in our next release of the NVIDIA HPC SDK, version 20.11.This was a great article! It appears that all the discussions and example are based on accelerating Fortran without any need for CUDA programming but only on one single GPU .From my work so far on multi-GPU programming, invoking two GPUs and partitioning the data in between always needs some CUDA related code – for instance, binding an MPI rank or a thread to one of the GPUs, or using CUDA Streams for simultaneous use of multiple GPUs and probably other approaches to enable accelerations on multi-GPUs. all need selecting the device one way or another which needs CUDA.All of these at the very least need selecting the device one way or another, which then needs CUDA and hence is in the opposite direction of “Accelerating Fortran with a GPU Using stdpar”, where the goal is to not change the CPU-based code (with no CUDA runtime API, etc.) and compile the code simply with NVFORTRAN.Perhaps, it would have been ideal in an MPI-based code – that consists of i) simultaneous use of CPUs to solve each sub-domain/array as well as ii) possible CPU-to-CPU communication – compiling with NVFORTRAN and stdpar automatically offloads a Do loop that is inherently within MPI process #0 to GPU0 & offload the same loop within MPI process #1 to GPU1 and so on and so forth. That way, a code platform that is already massively parallelized with MPI on CPUs could run on and utilize multi-GPU environments equally as well with NO change in any of the paradigms. If only this were possible…Indeed, multi-gpu programming is an important use-case and we are already looking into ways to make that easier, including with stdpar. I’ll also mention that we are planning to publish another blog post about more advanced usage of DO CONCURRENT, so keep an eye out for that as well.Is the Tensor Core used in this way?I’m running into an error that is making me scratch my head. when using the DO CONCURRENT command.“NVFORTRAN-F-0000-Internal compiler error. Missing end DO CONCURRENT region block”Anybody else running into this error? And for the record, yes I do have an END DO at the end of my very simple loop.Hi. The DO CONCURRENT feature is accelerated using the compute SMs of the GPU. However, the tensor cores are activated when using ISO Fortran array intrinsics, as described in another developer blog, Bringing Tensor Cores to Standard Fortran.Hi, can you please show me your code and how do you compile it?Here is the test program:program testimplicit noneinteger :: i,j
integer, parameter :: m=10000, n=10000
real :: a(m,n),b(m,n),c(m,n)
a=1
b=1do concurrent(i=1:n, j=1:m)
c(i,j)=b(i,j)+a(i,j)
end doend program testHere is the compile command:
nvfortran -stdpar=gpu,multicore test.f90 -o test
Here’s the execute command:
./testThis is probably a bug. Following change worksnvfortran -stdpar=gpu,multicore forum.f90  -o test -MinfoYep, works on my system too. Thanks!BTW, any timeline on the next update for Do Concurrent?I’ve got another bug for you. I increased the dimensions of the arrays by 1. If the array size becomes too large the code quits with the message “Killed”. The code will run with l=10.program testimplicit noneinteger :: i,j,k
integer, parameter :: m=10000, n=10000, l=100
!real :: a(m,n),b(m,n),c(m,n)
real,allocatable,dimension(:,:,:) :: a,b,c
real :: start, finish
allocate(a(m,n,l),b(m,n,l),c(m,n,l))
a=1
b=1do concurrent(i=1:n, j=1:m, k=1:l)
c(i,j,k)=b(i,j,k)+a(i,j,k)
end doend program testnvfortran -stdpar=gpu,multicore -Minfo test.f90 -o testMea Culpa. It looks like this is a memory limit that has nothing to do with NVFortran and the Do Concurrent loop. It looks like this is a fortran related memory limitation.In terms of the next update - There are bug fixes in every release. However, your original code won’t work unless you have a truly unified memory system or IBM power system with ATS enabled. You allocate arrays on the host stack that cannot leverage CUDA-managed memory. I hope I am clear.Hi,I have a small problem when attempt to write a matrix to a file using nvfortran compiler,I wrote a code that calculate a matrix S(c,c), where c=500 or >500.Then program is compiled:nvfortran -stdpar=gpu -Minfo=accel corr.f90 -o corr_gpuEverything was as expected until a try the fallowing code lines.Instead of an output file with c columns and c rows...I get an output file with 4 columns.Using gfortran compiler, gfortran corr.f90 -o corr_linear, the output file is a matrix of c X cdoes someone have any ideas why this difference occurs?Solution: Write output data file with more than 3 columnsI am beginning my learning in programming NVIDIA GPUs, and just read this very instructive and useful article. I would like to use DO CONCURRENT and the HPC SDK to accelerate several of my Fortran programs, which need to run on Windows, using GPUs. I saw that the HPC SDK is not available yet for Windows. I would like to ask the authors @gozen or @grahamlopez if there is any estimate of when this Windows version will become available.I also saw on another post (WSL and PGI compiler works great!) that one can enable the Windows Subsystem for Linux (WSL) on a Windows 10 machine, install the HPC SDK in WSL, compile Fortran code with the NVFORTRAN compiler, and then (apparently) invoke the resulting executable from Windows. This may provide a solution to accelerate my Fortran code under Windows until a HPC SDK version for Windows is released, but only for Windows 10. Is there any way to run the resulting Linux executable under previous Windows versions, in which WSL is not available?Hello! We are currently working on bringing the HPC SDK and the HPC Compilers to Windows; we hope to make an announcement about this later this year. As to your second question, I do not know of a way to run WSL executables in older versions of Windows that do not support WSL.Powered by Discourse, best viewed with JavaScript enabled"
1268,gpu-technology-conference-2017-call-for-submissions-now-open,"Originally published at:			https://developer.nvidia.com/blog/gpu-technology-conference-2017-call-for-submissions-now-open/
Take part in the world’s top GPU developer event May 8 -11, 2017 in Silicon Valley where artificial intelligence, virtual reality and autonomous vehicles will take center stage. GTC 2017 provides developers and thought leaders with the opportunity to share their work with thousands of the world’s brightest minds. The 2016 event had more than…Powered by Discourse, best viewed with JavaScript enabled"
1269,style-transfer-from-multiple-reference-images,"Originally published at:			Style Transfer From Multiple Reference Images | NVIDIA Technical Blog
Researchers from Microsoft and Hong Kong University of Science and Technology developed a deep learning method that can transfer the style and color from multiple reference images onto another photograph. “Our approach is applicable for cases where images may be very different in appearance but semantically similar,” mentioned the researches in their research paper. “Essentially,…Powered by Discourse, best viewed with JavaScript enabled"
1270,smart-device-brings-ai-control-to-your-camera,"Originally published at:			https://developer.nvidia.com/blog/smart-device-brings-ai-control-to-your-camera/
Arsenal, a camera technology startup from Bozeman, Montana, is an intelligent camera assistant that leverages deep learning to determine the optimal settings for the scene you’re shooting. “Today’s cameras have amazing optics, but they do very little to actually help you take a good photo,” said Ryan Stout, Arsenal’s founder and CEO. “You can go…Powered by Discourse, best viewed with JavaScript enabled"
1271,accelerated-signal-processing-with-cusignal,"Originally published at:			https://developer.nvidia.com/blog/accelerated-signal-processing-with-cusignal/
This post was originally published on RAPIDSai. Why signal processing?  Signal processing is all around us. Broadly defined as the manipulation of signals — or mechanisms of transmitting information from one place to another — the field of signal processing exploits embedded information to achieve a certain goal. In the case of noise cancellation, the…Powered by Discourse, best viewed with JavaScript enabled"
1272,cuda-9-2-now-available,"Originally published at:			CUDA 9.2 Now Available | NVIDIA Technical Blog
CUDA 9.2 includes updates to libraries, a new library for accelerating custom linear-algebra algorithms, and lower kernel launch latency. With CUDA 9.2, you can: Speed up recurrent and convolutional neural networks through cuBLAS optimizations Speed up FFT of prime size matrices through Bluestein kernels in cuFFT Accelerate custom linear algebra algorithms with CUTLASS 1.0 Launch…Powered by Discourse, best viewed with JavaScript enabled"
1273,ai-improves-the-frequency-and-quality-of-mobile-app-notifications,"Originally published at:			AI Improves the Frequency and Quality of Mobile App Notifications | NVIDIA Technical Blog
Researchers from Leopard Mobile, Taiwan’s first mobile internet company, recently developed a deep learning recommendation system that can improve smartphone notifications and pop-up advertisements. One of the big challenges developers and advertisers face is knowing when to deploy push notifications. If you send too many, people may delete your app, send too few,  people might…Powered by Discourse, best viewed with JavaScript enabled"
1274,using-shared-memory-in-cuda-fortran,"Originally published at:			https://developer.nvidia.com/blog/using-shared-memory-cuda-fortran/
CUDA Fortran for Scientists and Engineers shows how high-performance application developers can leverage the power of GPUs using Fortran. In the previous post, I looked at how global memory accesses by a group of threads can be coalesced into a single transaction, and how alignment and stride affect coalescing for various generations of CUDA hardware. For…Powered by Discourse, best viewed with JavaScript enabled"
1275,high-school-student-using-artificial-intelligence-to-fight-breast-cancer,"Originally published at:			https://developer.nvidia.com/blog/high-school-student-using-artificial-intelligence-to-fight-breast-cancer/
What started as a tenth-grade class project, the high school senior believes his technology can take on potentially deadly breast tumors and non-cancerous growths by using a mobile phone or tablet to aid in diagnosis and classification, reduce human error and save the expense of false-positive readings. Abu Qader’s technology can help aid in breast-cancer…Powered by Discourse, best viewed with JavaScript enabled"
1276,optimizing-xgboost-and-random-forest-machine-learning-approaches-on-nvidia-gpus,"Originally published at:			Optimizing XGBoost and Random Forest Machine Learning Approaches on NVIDIA GPUs | NVIDIA Technical Blog
XGBoost and random forest machine learning models have a dizzying array of parameters for data science practitioners to tune to produce the best possible model. Join Rory Mitchell, NVIDIA engineer and primary author of XGBoost’s GPU gradient boosting algorithms, for a clear discussion about how these parameters impact model performance.  This developer blog serves as…Powered by Discourse, best viewed with JavaScript enabled"
1277,boosting-productivity-and-performance-with-the-nvidia-cuda-11-2-c-compiler,"Originally published at:			https://developer.nvidia.com/blog/boosting-productivity-and-performance-with-the-nvidia-cuda-11-2-c-compiler/
The 11.2 CUDA C++ compiler incorporates features and enhancements aimed at improving developer productivity and the performance of GPU-accelerated applications. The compiler toolchain gets an LLVM upgrade to 7.0, which enables new features and can help improve compiler code generation for NVIDIA GPUs. Link-time optimization (LTO) for device code (also known as device LTO), introduced…Powered by Discourse, best viewed with JavaScript enabled"
1278,high-performance-gpu-computing-in-the-julia-programming-language,"Originally published at:			High-Performance GPU Computing in the Julia Programming Language | NVIDIA Technical Blog
Julia is a high-level programming language for mathematical computing that is as easy to use as Python, but as fast as C. The language has been created with performance in mind, and combines careful language design with a sophisticated LLVM-based compiler [Bezanson et al. 2017]. Julia is already well regarded for programming multicore CPUs and large parallel…what about host 2 device memory transfers?Just want to point out that there are a bunch of comments and discussion around this on Hacker Newshttps://news.ycombinator.co...Constructing the CuArray performs a host-to-device memory transfer, whereas converting it back to a regular Array fetches the memory back.is there support for asynchronous transfers? multiple streams? concurrent kernel and memory transfer?Partially, eg. streams are supported and can be used for kernel execution, but asynchronous transfers are not wrapped right now. It isn't much work to add though, and I'm currently redesigning the memory buffer interface so I'll see about adding it: https://github.com/JuliaGPU...If there's similar missing features you'd want to use, don't hesitate to file an issue at CUDAdrv or CUDAnative.Powered by Discourse, best viewed with JavaScript enabled"
1279,learn-gpu-programming-in-your-browser-with-nvidia-hands-on-labs,"Originally published at:			https://developer.nvidia.com/blog/learn-gpu-programming-free-on-demand-gpu-training/
As CUDA Educator at NVIDIA, I work to give access to massively parallel programming education & training to everyone, whether or not they have access to GPUs in their own machines. This is why, in partnership with qwikLABS, NVIDIA has made the hands-on content we use to train thousands of developers at the Supercomputing Conference…Cool payable stuff - looking for closing CUDA openAccess for GTX 10 GPU's... Is not still reproducted=revisited GTX780 6GB with 100$ price enough profitable?Post Scriptum: integrate CPU with HBM2 GPU global memory - I would do check 2core ARMv7. Additionaly one will only need a 40GE QSFP+; single PCIe3.0x16 slot for external RAID controler, few USB's and other standard=typical peripherals. Highly - complicated locksmithing for vaseline oil cooling will be necessary. It should do the standard - puzzle  idea job with comparision to next-generation few-years-buggy hybrid processor.Post Post Scriptum: CUDA is great, publically closing it, wil automate OpenCL switching instead Microsoft-like SDK's enforcing. Why STL container vector<vector<float>> is not runnable on GPU?P.P.P.S.: Java is fast because it is written in C++. But I am only an amateur.Powered by Discourse, best viewed with JavaScript enabled"
1280,microsoft-uses-ai-for-chinese-to-english-translation,"Originally published at:			Microsoft Uses AI for Chinese to English Translation | NVIDIA Technical Blog
Researchers from Microsoft recently announced they’ve created the first deep learning translation system capable of translating sentences of news articles from Chinese to English with the same level of accuracy as a person.   Microsoft used NVIDIA Tesla GPUs and millions of sentences from various online newspapers to train their neural network. The team used a dual learning…Powered by Discourse, best viewed with JavaScript enabled"
1281,ai-helps-protect-endangered-elephants,"Originally published at:			AI Helps Protect Endangered Elephants | NVIDIA Technical Blog
According to the U.N., up to 100 elephants are slaughtered every day in Africa by poachers taking part in the illegal ivory trade. This amounts to around 35,000 elephants killed each year due to poaching.To help fight the problem, Conservation Metrics, a Santa Cruz, California-based startup, is using deep learning to help detect the sounds…Powered by Discourse, best viewed with JavaScript enabled"
1282,ai-models-developed-by-nvidia-and-nih-could-aid-in-the-covid-19-fight-come-fall,"Originally published at:			AI Models Developed by NVIDIA and NIH Could Aid in the COVID-19 Fight Come Fall | NVIDIA Technical Blog
In a new paper published in Nature Communications, researchers at NVIDIA and the National Institutes of Health (NIH) demonstrate how they developed AI models (publicly available on NVIDIA NGC) to help researchers study COVID-19 in chest CT scans in an effort to develop new tools to better understand, measure and detect infections. Chest CT is…Powered by Discourse, best viewed with JavaScript enabled"
1283,inception-spotlight-new-skydio-2-drone-powered-by-nvidia-jetson,"Originally published at:			Inception Spotlight: New Skydio 2 Drone Powered by NVIDIA Jetson | NVIDIA Technical Blog
Redwood City, California-based Skydio and member of NVIDIA’s startup accelerator, Inception, has just released the latest version of their AI capable GPU-accelerated drone, Skydio 2.  Comprised of six 4K cameras, with an NVIDIA Jetson TX2  as the processor for the autonomous system, Skydio 2 is capable of flying for up to 23 minutes at a…Powered by Discourse, best viewed with JavaScript enabled"
1284,top-speech-ai-developer-day-sessions-at-nvidia-gtc-2023,"Originally published at:			Custom Domain by Bitly
Explore the latest advances in accurate and customizable automatic speech recognition, multi-language translation, and text-to-speech.Powered by Discourse, best viewed with JavaScript enabled"
1285,getting-the-best-performance-on-mlperf-inference-2-0,"Originally published at:			https://developer.nvidia.com/blog/getting-the-best-performance-on-mlperf-inference-2-0/
NVIDIA delivered leading results for MLPerf Inference 2.0, including 5x more performance for NVIDIA Jetson AGX Orin, an SoC platform built for edge devices and robotics.Powered by Discourse, best viewed with JavaScript enabled"
1286,accelerating-quantum-circuit-simulation-with-nvidia-custatevec,"Originally published at:			https://developer.nvidia.com/blog/accelerating-quantum-circuit-simulation-with-nvidia-custatevec/
cuStateVec is a library for acceleration of state vector-based quantum circuit simulation. We discuss APIs, integrations, and benchmarks.The step to install requirements is not suitable for the latest version.

image1958×723 81.5 KB
I use the latest qsim version from github repo and cuquantum-22.05.I can successfully make the qsim but failed for qsimcirq installation by pip install .  There is an error “fatal error: pybind11/complex.h: No such file or directory”.Powered by Discourse, best viewed with JavaScript enabled"
1287,using-nvidia-nsight-systems-in-containers-and-the-cloud,"Originally published at:			https://developer.nvidia.com/blog/nvidia-nsight-systems-containers-cloud/
Gone are the days when it was expected that a programmer would “own” all the systems that they needed. Modern computational work frequently happens in shared systems, in the cloud, or otherwise on hardware not owned by the user or even their employer. This is good for developers. It can save time and money by…Powered by Discourse, best viewed with JavaScript enabled"
1288,accelerated-data-analytics-speed-up-data-exploration-with-rapids-cudf,"Originally published at:			https://developer.nvidia.com/blog/accelerated-data-analytics-speed-up-data-exploration-with-rapids-cudf/
This post is part of a series on accelerated data analytics: Accelerated Data Analytics: Faster Time Series Analysis with RAPIDS cuDF walks you through the common steps of time series data processing with RAPIDS cuDF. This post discusses how the pandas library provides efficient, expressive functions in Python. Digital advancements in climate modeling, healthcare, finance,…Powered by Discourse, best viewed with JavaScript enabled"
1289,democratizing-deep-learning-recommenders-resources,"Originally published at:			https://developer.nvidia.com/blog/democratizing-deep-learning-recommenders-resources/
Deep learning work is iterative, experimental, and often time consuming for new and established machine learning practitioners, data scientists, and engineers. Yet, when deep learning is applied to certain application domains, like recommenders, industry members have the potential to provide better predictions at scale over existing commercial recommenders. As recommenders impact daily decisions regarding the…Powered by Discourse, best viewed with JavaScript enabled"
1290,nvidia-open-sources-parsers-and-plugins-in-tensorrt,"Originally published at:			NVIDIA open sources parsers and plugins in TensorRT | NVIDIA Technical Blog
TensorRT is a high performance deep learning inference platform that delivers low latency and high throughput for apps such as recommenders, speech and image/video on NVIDIA GPUs. It includes parsers to import models, and plugins to support novel ops and layers before applying optimizations for inference. Today NVIDIA is open sourcing parsers and plugins in…Powered by Discourse, best viewed with JavaScript enabled"
1291,detecting-and-labeling-diseases-in-chest-x-rays-with-deep-learning,"Originally published at:			Detecting and Labeling Diseases in Chest X-Rays with Deep Learning | NVIDIA Technical Blog
Researchers from the National Institutes of Health in Bethesda, Maryland are using NVIDIA GPUs and deep learning to automatically annotate diseases from chest x-rays. Accelerated by Tesla GPUs, the team trained their convolutional neural networks on a publicly available radiology dataset of chest x-rays and reports to describe the characteristics of a disease, such as…Powered by Discourse, best viewed with JavaScript enabled"
1292,qhack-results-highlight-quantum-computing-applications-and-tools-on-gpus,"Originally published at:			https://developer.nvidia.com/blog/qhack-results-highlight-quantum-computing-applications-and-tools-on-gpus/
Participants in QHack 2023, the world’s largest quantum machine learning hackathon, used the NVIDIA Quantum Platform to create useful tools and innovations, and novel algorithm development.Powered by Discourse, best viewed with JavaScript enabled"
1293,natural-language-processing-first-steps-how-algorithms-understand-text,"Originally published at:			https://developer.nvidia.com/blog/natural-language-processing-first-steps-how-algorithms-understand-text/
This post shows how NLP in Text is converted into vectors to be compatible with ML and other algorithms.Powered by Discourse, best viewed with JavaScript enabled"
1294,gtc-brings-a-new-world-of-ray-traced-possibilities,"Originally published at:			https://developer.nvidia.com/blog/gtc-brings-a-new-world-of-ray-traced-possibilities/
Explore the latest announcements and releases for professional visualization and game development, including some of the newest tools and techniques in the graphics industry.Powered by Discourse, best viewed with JavaScript enabled"
1295,gtc-digital-ai-deep-learning-presentations-demos-and-posters,"Originally published at:			GTC Digital: AI / Deep Learning Presentations, Demos, and Posters | NVIDIA Technical Blog
GTC Digital is all the great training, research, insights, and direct access to the brilliant minds of NVIDIA’s GPU Technology Conference, now online. Join live webinars, training, and Connect with the Experts sessions, or choose from a library of talks, panels, research posters, and demos that you can view on your own schedule, at your own…Powered by Discourse, best viewed with JavaScript enabled"
1296,just-released-cuda-toolkit-12-0,"Originally published at:			https://developer.nvidia.com/cuda-downloads#new_tab
CUDA Toolkit 12.0 supports NVIDIA Hopper architecture and many new features to help developers maximize performance on NVIDIA GPU-based products.Powered by Discourse, best viewed with JavaScript enabled"
1297,pearl-raises-11-million-to-develop-an-ai-based-dental-analysis-technology,"Originally published at:			Pearl Raises $11 Million to Develop an AI-Based Dental Analysis Technology | NVIDIA Technical Blog
Santa Monica, California-based startup Pearl, a new healthcare company focused on the dental industry, has just raised $11 million in series A funding to create a holistic oral health platform. “Pearl will have an immediate positive impact on the dental category,” Ophir Tanz, the company’s CEO told VentureBeat.  “It will streamline tedious, repetitive tasks, enhance…Powered by Discourse, best viewed with JavaScript enabled"
1298,nvidia-isaac-sim-on-omniverse-now-available-in-open-beta,"Originally published at:			https://developer.nvidia.com/blog/nvidia-isaac-sim-on-omniverse-now-available-in-open-beta/
The new Isaac simulation engine not only creates better photorealistic environments, but also streamlines synthetic data generation and domain randomization to build ground-truth datasets to train robots in applications from logistics and warehouses to factories of the future.Powered by Discourse, best viewed with JavaScript enabled"
1299,enabling-predictive-maintenance-using-root-cause-analysis-nlp-and-nvidia-morpheus,"Originally published at:			https://developer.nvidia.com/blog/enabling-predictive-maintenance-using-root-cause-analysis-nlp-and-nvidia-morpheus/
Background Predictive maintenance is used for early fault detection, diagnosis, and prediction when maintenance is needed in various industries including oil and gas, manufacturing, and transportation. Equipment is continuously monitored to measure things like sound, vibration, and temperature to alert and report potential issues. To accomplish this in computers, the first step is to determine…Powered by Discourse, best viewed with JavaScript enabled"
1300,an-introduction-to-large-language-models-prompt-engineering-and-p-tuning,"Originally published at:			https://developer.nvidia.com/blog/an-introduction-to-large-language-models-prompt-engineering-and-p-tuning/
ChatGPT has made quite an impression. Users are excited to use the AI chatbot to ask questions, write poems, imbue a persona for interaction, act as a personal assistant, and more. Large language models (LLMs) power ChatGPT, and these models are the topic of this post.  Before considering LLMs more carefully, we would first like…Powered by Discourse, best viewed with JavaScript enabled"
1301,deep-learning-helps-stitch-fix-dress-customers,"Originally published at:			https://developer.nvidia.com/blog/deep-learning-helps-stitch-fix-dress-customers/
San Francisco-based fashion startup Stitch Fix is applying deep learning to match their customers with personalized clothing recommendations. Using TITAN X GPUs and Tesla K80 GPUs on the Amazon cloud, along with CUDA and cuDNN to train their deep learning models, Stich Fix’s natural language processing algorithms decode written answers from customers’ feedback on what…Powered by Discourse, best viewed with JavaScript enabled"
1302,ai-powered-video-analytics-at-gtc-making-physical-spaces-smarter-and-safer,"Originally published at:			https://developer.nvidia.com/blog/ai-powered-video-analytics-at-gtc-making-physical-spaces-smarter-and-safer/
Find out how to make our important physical spaces smarter using the most widely deployed IoT devices – video cameras. NVIDIA GTC will be hosted on April 12-16. With over 1,400 breakthrough sessions for all technical levels, those registered have access to topic experts, networking events, and a front-row seat to NVIDIA CEO Jensen Huang’s…Powered by Discourse, best viewed with JavaScript enabled"
1303,drive-labs-eliminating-collisions-with-safety-force-field,"Originally published at:			DRIVE Labs: Eliminating Collisions with Safety Force Field | NVIDIA Technical Blog
By: Julia Ng Editor’s note: This is the latest post in our NVIDIA DRIVE Labs series, which takes an engineering-focused look at individual autonomous vehicle challenges and how NVIDIA DRIVE addresses them. Catch up on all of our automotive posts, here. Safety Force Field (SFF) vehicle software is designed specifically for collision avoidance. It acts as an…Powered by Discourse, best viewed with JavaScript enabled"
1304,accelerating-scientific-applications-in-hpc-clusters-with-nvidia-dpus-using-the-mvapich2-dpu-mpi-library,"Originally published at:			https://developer.nvidia.com/blog/accelerating-scientific-apps-in-hpc-clusters-with-dpus-using-mvapich2-dpu-mpi/
High-performance computing (HPC) and AI have driven supercomputers into wide commercial use as the primary data processing engines enabling research, scientific discoveries, and product development. These systems can carry complex simulations and unlock the new era of AI, where software writes software. Supercomputing leadership means scientific and innovation leadership, which explains the investments made by…Powered by Discourse, best viewed with JavaScript enabled"
1305,latest-nsight-compute-2021-2-release-now-available-for-download,"Originally published at:			https://developer.nvidia.com/blog/latest-nsight-compute-2021-2-release-now-available-for-download/
The new release helps identify more performance issues, and makes it easier to understand and fix them.Powered by Discourse, best viewed with JavaScript enabled"
1306,best-practices-using-nvidia-rtx-ray-tracing,"Originally published at:			https://developer.nvidia.com/blog/best-practices-using-nvidia-rtx-ray-tracing/
This post gathers best practices based on our experiences so far on using NVIDIA RTX ray tracing in games. I’ve organized the tips into short, actionable items that give practical tips for developers working on ray tracing today. They aim to give a broad picture of what kind of solutions lead to good performance in…Powered by Discourse, best viewed with JavaScript enabled"
1307,new-course-gpu-acceleration-with-the-c-standard-library,"Originally published at:			Courses – NVIDIA
Learn how to write simple, portable, parallel-first GPU-accelerated applications using only C++ standard language features in this self-paced course from the NVIDIA Deep Learning InstitutePowered by Discourse, best viewed with JavaScript enabled"
1308,ai-helps-robots-navigate-in-hazardous-indoor-spaces,"Originally published at:			https://developer.nvidia.com/blog/ai-helps-robots-navigate-in-hazardous-indoor-spaces/
By Mokshith Voodarla, Josh Hejna, Anish Singhani, Rahul Amara As robots become more integral throughout the world, delivering mail, food, and giving directions, they need to be able to easily navigate through indoor environments. Over the summer four high school NVIDIA interns, Team CCCC (ForeSee),  developed a robust, low-cost solution to tackle this challenge. Indoor…Powered by Discourse, best viewed with JavaScript enabled"
1309,explainer-what-is-path-tracing,"Originally published at:			What Is Path Tracing? | NVIDIA Blog
Path tracing is going real-time, unleashing interactive, photorealistic 3D environments filled with dynamic light and shadow, reflections, and refractions.Powered by Discourse, best viewed with JavaScript enabled"
1310,gtc-2019-silicon-valley-preview-cuda-talks-and-sessions,"Originally published at:			GTC 2019 Silicon Valley Preview: CUDA Talks and Sessions | NVIDIA Technical Blog
Expected to be the biggest yet, NVIDIA’s GPU Technology Conference (GTC) features hundreds of sessions on the most important topics in computing today. From weather prediction and materials science to wind tunnel simulation and genomics, NVIDIA GPU-accelerated computing is at the heart of HPC. Gain insights from experts into updates to the CUDA platform and…Powered by Discourse, best viewed with JavaScript enabled"
1311,nvidia-releases-jarvis-1-0-beta-for-building-real-time-conversational-ai-services,"Originally published at:			https://developer.nvidia.com/blog/nvidia-releases-jarvis-1-0-beta-for-building-real-time-conversational-ai-services/
Jarvis is a flexible application framework for multimodal conversational AI services that delivers real-time performance on NVIDIA GPUs.Powered by Discourse, best viewed with JavaScript enabled"
1312,common-challenges-with-conducting-an-edge-ai-proof-of-concept,"Originally published at:			https://developer.nvidia.com/blog/common-challenges-with-conducting-an-edge-ai-proof-of-concept/
A proof-of-concept (POC) is the first step towards a successful edge AI deployment. Companies adopt edge AI to drive efficiency, automate workflows, reduce cost, and improve overall customer experiences. As they do so, many realize that deploying AI at the edge is a new process that requires different tools and procedures than the traditional data…Powered by Discourse, best viewed with JavaScript enabled"
1313,share-your-science-analyzing-human-brain-connectivity-with-gpus,"Originally published at:			Share Your Science: Analyzing Human Brain Connectivity with GPUs | NVIDIA Technical Blog
Moises Hernandez Fernandez, PhD student at University of Oxford shares how GPUs are being used to accelerate the analysis of the human brain’s underlying anatomical and structural organization, which can lead to a better understanding of neurological disorders like Alzheimer’s or Multiple Sclerosis. Using Tesla K80 GPUs and CUDA, the group at Oxford Centre for…Powered by Discourse, best viewed with JavaScript enabled"
1314,gtc-digital-domino-s-takes-delivery-to-a-whole-new-level-with-nvidia-dgx-1,"Originally published at:			GTC Digital: Domino’s Takes Delivery to a Whole New Level with NVIDIA DGX-1 | NVIDIA Technical Blog
Have you ever wondered what kind of technology is used to get your delicious cheese pizza, and perhaps an order of Domino’s Chocolate Lava Crunch Cake from a store to your home?  In a new video posted to LinkedIn, previewing a talk at GTC Digital, the company reveals the magic behind their AI platform, an…Powered by Discourse, best viewed with JavaScript enabled"
1315,using-a-network-digital-twin-as-an-it-training-tool,"Originally published at:			https://developer.nvidia.com/blog/using-a-network-digital-twin-as-an-it-training-tool/
As organizations rely on complex network systems to support their operations, the need for well-trained network administrators is becoming increasingly important. Data center infrastructure is interconnected in ways that are not always obvious, and the points of intersection between systems are often difficult to design on paper alone.  In order to ensure that network administrators…Powered by Discourse, best viewed with JavaScript enabled"
1316,deep-learning-is-transforming-asr-and-tts-algorithms,"Originally published at:			https://developer.nvidia.com/blog/deep-learning-is-transforming-asr-and-tts-algorithms/
Speech is one of the primary means to communicate with an AI-powered application. From virtual assistants to digital avatars, voice-based interfaces are changing how we typically interact with smart devices. Deep learning techniques for speech recognition and speech synthesis are helping improve the user experience—think human-like responses and natural-sounding tones. If you plan to build…Powered by Discourse, best viewed with JavaScript enabled"
1317,gtc-2020-memory-management-on-modern-gpu-architectures,"GTC 2020 CWE21754
Presenters: Nikolay-Sakharnykh,NVIDIA; Lars Nyland, ; Chirayu Garg, ; William, Sebastian, Cory, ; Mark Hairgrove,
Abstract
Come to our session to discuss any topics about memory management on GPU systems: new memory management APIs, profiling and optimizations for GPU memory subsystem, tips and tricks for managing data across multiple GPUs and CPUs, and compression techniques. Whether you’re interested in the low-level details of the GPU architecture, or software heuristics in the driver, or guidelines and best practices for applications — we have the right experts from multiple NVIDIA teams you can connect with and chat about your topic.Watch this session
Join in the conversation below.Powered by Discourse, best viewed with JavaScript enabled"
1318,aws-announces-new-gpu-accelerated-ec2-instances-and-networking-enhancements,"Originally published at:			AWS Announces New GPU-Accelerated EC2 Instances and Networking Enhancements | NVIDIA Technical Blog
Today At AWS re:Invent in Las Vegas, Amazon Web Services, announced a brand new GPU instance offering for Amazon Elastic Compute Cloud (Amazon EC2).The new P3dn GPU instances are ideal for distributed machine learning and high-performance computing applications. The instances are comprised of NVIDIA Tesla Tensor Core V100 GPUs each with 32GB of memory. “This enables…Powered by Discourse, best viewed with JavaScript enabled"
1319,top-game-development-sessions-at-nvidia-gtc-2023,"Originally published at:			https://developer.nvidia.com/blog/top-game-development-sessions-at-nvidia-gtc-2023/
Join us for the latest on NVIDIA RTX and neural rendering technologies, and learn how they are accelerating game development.Powered by Discourse, best viewed with JavaScript enabled"
1320,world-s-largest-manufacturing-players-tapping-nvidia-ai-platform-for-factory-of-the-future,"Originally published at:			https://developer.nvidia.com/blog/worlds-largest-manufacturing-players-tapping-nvidia-ai-platform-for-factory-of-the-future/
Tapping into the NVIDIA AI platform and Microsoft Azure Infrastructure, Sight Machine accurately maps data to assets at a global scale.Thanks for sharing the informative info.Powered by Discourse, best viewed with JavaScript enabled"
1321,nvidia-on-demand-rapids-sessions-from-gtc-2023,"Originally published at:			Playlist | RAPIDS at GTC 2023 | NVIDIA On-Demand
Get the latest best practices about how to accelerate your data science projects with RAPIDS.Powered by Discourse, best viewed with JavaScript enabled"
1322,nvidia-ffmpeg-transcoding-guide,"Well id doesn't look like nVidia is going to take any notice of you. It seems odd that you need an older card in order to retain features.you're either going to have to get an older card or you're gonna have to use CPU.It may be possible that an older version of FFMPEG or an older version of nv-codec-headers will still work. You will have to experiement. You've waited six months with no solution so maybe set aside a day to regression test older versions until you find one that works again.I'm having trouble building a binary on windows. I'm not a C programmer so it's hard to figure out. I suspect this would be a lot easier under linux with gcc built right in, but I tried using mingw and it works to a point, but I get errors. I posted the problem herehttps://devtalk.nvidia.com/...Thanks again Terence Kearns,I wend back to all older version of ffmpeg and no luck.Older nVidia cards (9 and 10 series) are working like charm.But those cards are no more avalable on the market.My question is why all of a sudden nVidia removde bFrame (interlaced) support in latest cards like Turing.Without interlaced support nVidia is useless.Last I checked, interlaced encoding seems to be disabled on Turing GPUs. Also take a look at this: https://devtalk.nvidia.com/... gHi,Lokking for some help with something that I had working some time ago.The command ""ffmpeg -vsync 0 -i input.mp4 -c:v hevc_nvenc -gpu list -f null -"" I copied from the page above does not work anymore. I get the following error:# ffmpeg -vsync 0 -i input.mp4 -c:v hevc_nvenc -gpu list out.mp4ffmpeg version 4.2.2 Copyright (c) 2000-2019 the FFmpeg developers  built with gcc 4.8.5 (GCC) 20150623 (Red Hat 4.8.5-36)  configuration: --prefix=/usr --arch=x86_64 --disable-shared --disable-debug --enable-nonfree --enable-nvenc --enable-cuda --enable-cuvid --enable-libnpp --enable-gpl --enable-fontconfig --enable-gnutls --enable-gray --enable-libfreetype --enable-version3 --extra-cflags=-Ilocal/include --extra-cflags=-I/home/pcarmo/HARDENING/RPMS/ffmpeg/ffmpeg-4.2.2 --extra-ldflags=-L/home/pcarmo/HARDENING/RPMS/ffmpeg/ffmpeg-4.2.2 --extra-cflags=-I/home/pcarmo/HARDENING/RPMS/ffmpeg/Video_Codec_SDK_7.1.9/Samples/common/inc/ --extra-cflags=-I/usr/local/cuda-10.0/targets/x86_64-linux/include --extra-ldflags=-L/usr/local/cuda-10.0/targets/x86_64-linux/lib --extra-ldflags=-L/usr/lib64/nvidia  libavutil      56. 31.100 / 56. 31.100  libavcodec     58. 54.100 / 58. 54.100  libavformat    58. 29.100 / 58. 29.100  libavdevice    58.  8.100 / 58.  8.100  libavfilter     7. 57.100 /  7. 57.100  libswscale      5.  5.100 /  5.  5.100  libswresample   3.  5.100 /  3.  5.100  libpostproc    55.  5.100 / 55.  5.100Input #0, mov,mp4,m4a,3gp,3g2,mj2, from 'input.mp4':  Metadata:    major_brand     : isom    minor_version   : 512    compatible_brands: isomiso2avc1mp41    encoder         : Lavf58.29.100  Duration: 00:00:27.90, start: 0.000000, bitrate: 374 kb/s    Stream #0:0(und): Video: h264 (Main) (avc1 / 0x31637661), yuv420p, 1366x768 [SAR 1:1 DAR 683:384], 371 kb/s, 30 fps, 30 tbr, 15360 tbn, 60 tbc (default)    Metadata:      handler_name    : VideoHandlerFile 'out.mp4' already exists. Overwrite ? [y/N] yStream mapping:  Stream #0:0 -> #0:0 (h264 (native) -> hevc (hevc_nvenc))Press [q] to stop, [?] for help[hevc_nvenc @ 0x2347e40] [ GPU #0 - < Quadro P2000 > has Compute SM 6.1 ]Error initializing output stream 0:0 -- Error while opening encoder for output stream #0:0 - maybe incorrect parameters such as bit_rate, rate, width or heightConversion failed!...I know hevc_nvenc is avaliable and working as I can successful run the command:...# ffmpeg -vsync 0 -i input.mp4 -c:v hevc_nvenc -gpu all out.mp4ffmpeg version 4.2.2 Copyright (c) 2000-2019 the FFmpeg developers  built with gcc 4.8.5 (GCC) 20150623 (Red Hat 4.8.5-36)  configuration: --prefix=/usr --arch=x86_64 --disable-shared --disable-debug --enable-nonfree --enable-nvenc --enable-cuda --enable-cuvid --enable-libnpp --enable-gpl --enable-fontconfig --enable-gnutls --enable-gray --enable-libfreetype --enable-version3 --extra-cflags=-Ilocal/include --extra-cflags=-I/home/pcarmo/HARDENING/RPMS/ffmpeg/ffmpeg-4.2.2 --extra-ldflags=-L/home/pcarmo/HARDENING/RPMS/ffmpeg/ffmpeg-4.2.2 --extra-cflags=-I/home/pcarmo/HARDENING/RPMS/ffmpeg/Video_Codec_SDK_7.1.9/Samples/common/inc/ --extra-cflags=-I/usr/local/cuda-10.0/targets/x86_64-linux/include --extra-ldflags=-L/usr/local/cuda-10.0/targets/x86_64-linux/lib --extra-ldflags=-L/usr/lib64/nvidia  libavutil      56. 31.100 / 56. 31.100  libavcodec     58. 54.100 / 58. 54.100  libavformat    58. 29.100 / 58. 29.100  libavdevice    58.  8.100 / 58.  8.100  libavfilter     7. 57.100 /  7. 57.100  libswscale      5.  5.100 /  5.  5.100  libswresample   3.  5.100 /  3.  5.100  libpostproc    55.  5.100 / 55.  5.100Input #0, mov,mp4,m4a,3gp,3g2,mj2, from 'input.mp4':  Metadata:    major_brand     : isom    minor_version   : 512    compatible_brands: isomiso2avc1mp41    encoder         : Lavf58.29.100  Duration: 00:00:27.90, start: 0.000000, bitrate: 374 kb/s    Stream #0:0(und): Video: h264 (Main) (avc1 / 0x31637661), yuv420p, 1366x768 [SAR 1:1 DAR 683:384], 371 kb/s, 30 fps, 30 tbr, 15360 tbn, 60 tbc (default)    Metadata:      handler_name    : VideoHandlerFile 'out.mp4' already exists. Overwrite ? [y/N] yStream mapping:  Stream #0:0 -> #0:0 (h264 (native) -> hevc (hevc_nvenc))Press [q] to stop, [?] for helpOutput #0, mp4, to 'out.mp4':  Metadata:    major_brand     : isom    minor_version   : 512    compatible_brands: isomiso2avc1mp41    encoder         : Lavf58.29.100    Stream #0:0(und): Video: hevc (hevc_nvenc) (Main) (hev1 / 0x31766568), yuv420p, 1366x768 [SAR 1:1 DAR 683:384], q=-1--1, 2000 kb/s, 30 fps, 15360 tbn, 30 tbc (default)    Metadata:      handler_name    : VideoHandler      encoder         : Lavc58.54.100 hevc_nvenc    Side data:      cpb: bitrate max/min/avg: 0/0/2000000 buffer size: 4000000 vbv_delay: -1frame=  837 fps=563 q=9.0 Lsize=    1622kB time=00:00:27.86 bitrate= 476.9kbits/s speed=18.7xvideo:1618kB audio:0kB subtitle:0kB other streams:0kB global headers:0kB muxing overhead: 0.259576%...So, why the ""-gpu list"" stoped working? I had it work some time ago before upgrading ffmpeg/cuda...Thank!Excellent tutorial. I've added hw accelerated decodes and cuda_hwupload (for special filter cases) to my encodes.ffmpeg -f lavfi -i nullsrc -c:v hevc_nvenc -gpu list -f null -See if that works?I am trying to encode 10-bit content to 8-bit, is there a way to this in complete GPU.. I am able to do it using CPU/GPU, however performance is not acceptable, I use the following command line::ffmpeg -init_hw_device cuda=hw -filter_hw_device hw -c:v hevc_cuvid –resize 1280x720 -i hevc_10b_4K60p_BroadcastCaptureSample.mkv -vf ""fps=fps=30,format=yuv420p,hwupload"" -c:v hevc_nvenc output.mp4Is there a way to avoid 10-bit to 8-bit conversion on CPU ? Please help.Hi,It does not:# ffmpeg -f lavfi -i nullsrc -c:v hevc_nvenc -gpu list -f null -ffmpeg version 4.2.2 Copyright (c) 2000-2019 the FFmpeg developers  built with gcc 4.8.5 (GCC) 20150623 (Red Hat 4.8.5-36)  configuration: --prefix=/usr --arch=x86_64 --disable-shared --disable-debug --enable-nonfree --enable-nvenc --enable-cuda --enable-cuvid --enable-libnpp --enable-gpl --enable-fontconfig --enable-gnutls --enable-gray --enable-libfreetype --enable-version3 --extra-cflags=-Ilocal/include --extra-cflags=-I/home/pcarmo/HARDENING/RPMS/ffmpeg/ffmpeg-4.2.2 --extra-ldflags=-L/home/pcarmo/HARDENING/RPMS/ffmpeg/ffmpeg-4.2.2 --extra-cflags=-I/home/pcarmo/HARDENING/RPMS/ffmpeg/Video_Codec_SDK_7.1.9/Samples/common/inc/ --extra-cflags=-I/usr/local/cuda-10.1/targets/x86_64-linux/include --extra-ldflags=-L/usr/local/cuda-10.1/targets/x86_64-linux/lib --extra-ldflags=-L/usr/lib64/nvidia  libavutil      56. 31.100 / 56. 31.100  libavcodec     58. 54.100 / 58. 54.100  libavformat    58. 29.100 / 58. 29.100  libavdevice    58.  8.100 / 58.  8.100  libavfilter     7. 57.100 /  7. 57.100  libswscale      5.  5.100 /  5.  5.100  libswresample   3.  5.100 /  3.  5.100  libpostproc    55.  5.100 / 55.  5.100Input #0, lavfi, from 'nullsrc':  Duration: N/A, start: 0.000000, bitrate: N/A    Stream #0:0: Video: rawvideo (I420 / 0x30323449), yuv420p, 320x240 [SAR 1:1 DAR 4:3], 25 tbr, 25 tbn, 25 tbcStream mapping:  Stream #0:0 -> #0:0 (rawvideo (native) -> hevc (hevc_nvenc))Press [q] to stop, [?] for help[hevc_nvenc @ 0x31bf080] [ GPU #0 - < Quadro P2000 > has Compute SM 6.1 ]Error initializing output stream 0:0 -- Error while opening encoder for output stream #0:0 - maybe incorrect parameters such as bit_rate, rate, width or heightConversion failed!I'm sure it worked in the past, but now it just give the error above :(Did you figure it out?No.Munish, could you give me some more detail about your business use case here? I'll try to find some answers for you, though our developers are a little busy with GTC Digital. Thanks!Jen, I need 10-bit to 8-bit transcoding  for casting 10-bit videos to streaming sticks and smart tvs. Additionally, I want to transcode 10-bit HDR (high dynamic range) content to 8-bit SDR content for casting on streaming sticks and smart tvs with SDR display.Munish, may I suggest posting your question at https://forums.developer.nv... It would be great to open this up to the community and see if someone can help.Jen, I have posted the same at https://forums.developer.nv...ffmpeg -vsync 0 -hwaccel cuvid -c:v h264_cuvid -i input.mp4 -c:a copy -c:v h264_nvenc -b:v 5M output.mp4Just throws up errors. @jwitsoe could you update the guide?Hello @rzzbu5l,FFmpeg sources have been updated since the article was published.
Below is the preferable syntax for on-GPU transcoding:ffmpeg -hwaccel cuda -hwaccel_output_format cuda -i input.mp4 -c:v h264_nvenc -v:b 5M output.mp4Relevant video decoder will be selected automatically from the list of codecs supported by cuda hwaccel.@rarzumanyan and @rzzbu5l, I’ve updated the post to use the preferred syntax. Thanks for the feedback!Can you clarify some of the syntax and options described in the blog post?What is the difference between -hwaccel cuda and -hwaccel cuvid?  Are they just different ways to do the same thing?What is the difference between the two scaling methods mentioned:  -resize before -i and scale_npp as a filter?  I’ve also seen the scale_cuda filter mentioned elsewhere on the nvidia web site.  I realize that the filter method can be used if you want two outputs, but for a single output is there any difference in quality or speed between the three methods?  I’m downscaling 4K to 1080p, and want the highest quality.Any thoughts on how -cq and -qp compare when using hardware encoding?Thanks!##issues in installing ffmpegHi All,I have CentOS Stream 8  vm which has 50 gb space and i have tried the below steps for installing ffmpeg on the the vm by following steps present in the nvidia-ffmpeg-transcoding-guide1.Downloaded repository “git.ffmpeg.org Git - ffmpeg.git/summary and cloned with
git clone git.ffmpeg.org Git - ffmpeg.git/summary”
2.Downaloded the nvidia driver for tesla T4 product and linux 64-bit RHEL 8
file name is “nvidia-driver-local-repo-rhel8-510.47.03-1.0-1.x86_64.rpm”
3.Downloaded the cuda tool kit for centos 8 system by running following commands(I)wget https://developer.download.nvidia.com/compute/cuda/11.6.2/local_installers/cuda-repo-rhel8-11-6-local-11.6.2_510.47.03-1.x86_64.rpm
(II)sudo rpm -i cuda-repo-rhel8-11-6-local-11.6.2_510.47.03-1.x86_64.rpm
(III)sudo dnf clean all
(IV)sudo dnf -y module install nvidia-driver:latest-dkms
(V)sudo dnf -y install cuda4.cloned the rep FFmpeg/nv-codec-headers5.After completion of ffmpeg/nv-codec-headers,i have entered to the nv-codec-headers
cd nv-codec-headers
6.i’m  getting below installation messages when i ran the “make install” command and kindly help me in resolving the issue
[root@localhost nv-codec-headers]# make install
sed ‘s#@@PREFIX@@#/usr/local#’ ffnvcodec.pc.in > ffnvcodec.pc
install -m 0755 -d ‘/usr/local/include/ffnvcodec’
install -m 0644 include/ffnvcodec/*.h ‘/usr/local/include/ffnvcodec’
install -m 0755 -d ‘/usr/local/lib/pkgconfig’
install -m 0644 ffnvcodec.pc ‘/usr/local/lib/pkgconfig’Hi，I have a question to ask:Many thanks! Looking forward to hearing from you.Powered by Discourse, best viewed with JavaScript enabled"
1323,nvidia-jetson-x2122-tx1-supercomputer-on-module-drives-next-wave-of-autonomous-machines,"Love to see how VisionWorks works....What are the dimensions of the developer board?Dustin,  Would the pcie x4 support a standard adapter  (ie. http://www.addonics.com/pro... to add a m.2x4 -m key to add larger/ higher speed storageThe Jetson TX1 Developer Kit Carrier Board is 170mm x 170mm.Yes, in theory the PCIe adapter you linked to should work with the devkit.Hi Dustin..and everyone elseExcuse me if I duplicate my post, but I can't see it in this forum. I want to know if it would be possible to run Windows applications on Jetson TX1 using NVIDIA GRID 2.0 in between (it is an VMware layer, isn't it?)SincerelyHi Dustin, I couldn't find any references to an Omnivision OV5694 sensor anywhere. Are you sure that's the right one?You're right, its OV5693 -- http://www.ovt.com/products...Thanks!Hi DustinIs there any M.2 key E 4G/wifi/BT/GNSS data card can be supported on jetson X1?Can you tell me how much memory L4T needs? Absent any user applications, just the OS memory footprint size.hi can i ask to you about ""hetson tx1 developer kit"" datasheet?i want to this struct size. .. please let know me.hi i wish to know about the heat sink part details of the jetson tx1 board do any one know about the heat sink part number from cool masterUsing the recent L4T R24.1 aarch64, after boot the memory consumption is 869 of 3994MB.Hi, here is link to the datasheet:  http://developer.nvidia.com...Here's the CoolerMaster P/N:   DCV-01672-N2-GPYou could develop a VDI (virtual desktop) client for Jetson that receives the H.264-encoded stream from GRID and decompresses it with gstreamer and displays it with OpenGL, then sends back user events to the server application in Windows.Hi i wish to know the  total power consumption of TX1 SOM .It depends on if you use the default clocks or increase the limits with ~/jetson_clocks script, between 6.5-15W for TX1.As far as I do know it is better idea to write narrow-specialized kernels for particular usage directly at C++11 CUDA kernels, instead of libraries (there are some exceptions like cuFFT, Thrust sort, and so on). Quite cool, but only aplicable to expensive uav's for that price.Powered by Discourse, best viewed with JavaScript enabled"
1324,startup-uses-ai-to-generate-sports-highlights,"Originally published at:			Startup Uses AI to Generate Sports Highlights | NVIDIA Technical Blog
At the GPU Technology Conference in San Jose, California this week, Southern California-based startup REELY, showed off their deep learning platform that automatically generates sports highlights in real-time. “We run multiple neural networks in tandem, watching for multiple different things, such as event detection, scores, audio analysis, player tracking, and scene recognition all in real-time,”…Powered by Discourse, best viewed with JavaScript enabled"
1325,speeding-up-deep-learning-training-with-nvidia-v100-tensor-core-gpus-in-the-aws-cloud,"Originally published at:			Speeding Up Deep Learning Training with NVIDIA V100 Tensor Core GPUs in the AWS Cloud | NVIDIA Technical Blog
Training deep learning models on NVIDIA GPUs is the gold standard in artificial intelligence, but the process can still take weeks to complete. To help advance the work, a team from the Amazon Web Services cloud announced today a new scalable way to optimize the AWS infrastructure to minimize deep learning training times from weeks…Powered by Discourse, best viewed with JavaScript enabled"
1326,training-and-optimizing-a-2d-pose-estimation-model-with-the-nvidia-transfer-learning-toolkit-part-1,"Originally published at:			https://developer.nvidia.com/blog/training-optimizing-2d-pose-estimation-model-with-tlt-part-1/
Human pose estimation is a popular computer vision task of estimating key points on a person’s body such as eyes, arms, and legs. This can help classify a person’s actions, such as standing, sitting, walking, lying down, jumping, and so on. Understanding the context of what a person might be doing in a scene has…The model of the training , it can be used at the following article?I know I can use the method to convert to tensorRT, but the model can be used at creating-a-human-pose-estimation-application-with-nvidia-deepstream?https://docs.nvidia.com/tlt/tlt-user-guide/text/deepstream_tlt_integration.html#installation-prerequisitesPowered by Discourse, best viewed with JavaScript enabled"
1327,managing-video-streams-in-runtime-with-the-nvidia-deepstream-sdk,"Originally published at:			https://developer.nvidia.com/blog/managing-video-streams-in-runtime-with-the-deepstream-sdk/
The applications of video analytics are changing right before your eyes. With AI applied to video analytics, it is now possible to keep a watch over hundreds of cameras in real time.Powered by Discourse, best viewed with JavaScript enabled"
1328,gtc-2020-how-nvidia-s-deep-learning-training-examples-have-state-of-the-art-accuracy-and-performance,"GTC 2020 S21765
Presenters: Pablo Ribalta,NVIDIA
Abstract
Convert ideas into fully working solutions with NVIDIA Deep Learning examples. Have you ever scraped the net for a model implementation and ultimately rewritten your own because none would work as you wanted? Get as fast as possible to a working baseline by pulling one of our many reference implementations of the most popular models. They come with a step-by-step guide, can be deployed directly in our latest NVIDIA container, and are optimized to deliver state-of-the-art accuracy with full reproducibility across different GPU setups. Curious about mixed precision or multi-GPU? We have those, too! Implementations are available in Tensorflow and Pytorch, with interactive examples consisting of Jupyter Notebooks.Watch this session
Join in the conversation below.Powered by Discourse, best viewed with JavaScript enabled"
1329,ai-learns-to-lip-sync-from-audio-clips,"Originally published at:			https://developer.nvidia.com/blog/ai-learns-to-lip-sync-from-audio-clips/
University of Washington researchers developed a deep learning-based system that converts audio files into realistic mouth shapes, which are then grafted onto and blended with the head of that person from another existing video. “These type of results have never been shown before,” said Ira Kemelmacher-Shlizerman, an assistant professor at the UW’s Paul G. Allen…Powered by Discourse, best viewed with JavaScript enabled"
1330,runtime-decisions,"Does cuGraph make any runtime decisions based on various graph/input properties to determine the best parallelization strategy for tackling a certain task, or is it up to the user to determine this?Yes, mainly we consider average vertex degree and the number of GPUs to decide whether to cache edge source/destination property values in a contiguous array or (key, value) pairs. More detailed answer is avaliable in Analyzing Multi-trillion Edge Graphs on Large GPU Clusters: A Case Study with PageRank | IEEE Conference Publication | IEEE Xplore If I just briefly outline the key idea, with 2D partitioning and assuming V vertices and P GPUs, the range of edge source/destination values scales V/sqrt(P). Say we have E edges, the number of edges per partition is E/P. And we access E/P or fewer source/destination property values. If E/P is smaller than V/sqrt(P), storing edge source/destination property values in (key, value) pairs saves memory. Otherwise, storing in a contiguous memory is more efficient in both space & time. We also make concurrency/memory footprint trade-offs in multiple places.This topic was automatically closed 14 days after the last reply. New replies are no longer allowed.Powered by Discourse, best viewed with JavaScript enabled"
1331,sony-breaks-resnet-50-training-record-with-nvidia-v100-tensor-core-gpus,"Originally published at:			SONY Breaks ResNet-50 Training Record with NVIDIA V100 Tensor Core GPUs | NVIDIA Technical Blog
Researchers from SONY today announced a new speed record for training ImageNet/ResNet 50 in only 224 seconds (three minutes and 44 seconds) with 75 percent accuracy using 2,100 NVIDIA Tesla V100 Tensor Core GPUs. This achievement represents the fastest reported training time ever published on ResNet-50. The team also achieved over 90% GPU scaling efficiency…Powered by Discourse, best viewed with JavaScript enabled"
1332,working-with-ray-traced-water-caustics-in-dxr,"Originally published at:			Working With Ray Traced Water Caustics in DXR | NVIDIA Technical Blog
NVIDIA’s Holger Gruen enjoys snorkeling, a useful past-time when your day job includes figuring out how to help game developers get the most out of ray-traced water caustics in DXR. His observation of how light actually behaves underwater informed his approach to creating convincing simulations of aquatic environments. Ray Traced Water Caustics with DXR, Holger’s…Powered by Discourse, best viewed with JavaScript enabled"
1333,mysterious-radio-burst-pinpointed-to-distant-galaxy,"Originally published at:			Mysterious Radio Burst Pinpointed to Distant Galaxy | NVIDIA Technical Blog
For the first time, astronomers have tracked down the location of a fast radio burst (FRB), confirming these short but spectacular flashes of radio waves originate in the distant universe. Australian astronomers announced last week that they successfully used Tesla GPUs and CSIRO radio telescopes in eastern Australia and Japan’s Subaru telescope in Hawaii to…Powered by Discourse, best viewed with JavaScript enabled"
1334,airbnb-turns-to-deep-learning-to-supercharge-their-search-rankings,"Originally published at:			Airbnb Turns to Deep Learning to Supercharge Their Search Rankings | NVIDIA Technical Blog
Finding the perfect place to stay when you’re traveling can be a daunting process. The location, price, number of rooms, availability, amenities, type of rental, budget, and many other factors play a major role in the process. To help travelers get the best Airbnb suggestions for their travel plans, developers from the company have developed…Powered by Discourse, best viewed with JavaScript enabled"
1335,simplifying-access-to-large-language-models-with-nvidia-nemo-framework-and-services,"Originally published at:			Simplifying Access to Large Language Models with NVIDIA NeMo Framework and Services | NVIDIA Technical Blog
Learn about recent advances in large language models (LLMs) that have fueled state-of-the-art performance for NLP applications.Powered by Discourse, best viewed with JavaScript enabled"
1336,facebook-ai-researchers-achieve-a-107x-speedup-for-training-virtual-agents,"Originally published at:			Facebook AI Researchers Achieve a 107x Speedup for Training Virtual Agents | NVIDIA Technical Blog
Navigating a new indoor space without any prior knowledge or even a map is a challenging task for a human, let alone a robot.  To help develop intelligent machines that interact more effectively with complex 3D environments, Facebook researchers developed a GPU-accelerated deep reinforcement learning model that achieves near 100 percent success in navigating a variety…Powered by Discourse, best viewed with JavaScript enabled"
1337,simplify-ai-model-development-with-the-latest-tao-toolkit-release,"Originally published at:			https://developer.nvidia.com/blog/simplify-ai-model-development-with-the-latest-tao-toolkit-release/
Boost productivity and model training with new pretrained models and features such as ONNX model weights import, REST APIs, and TensorBoard visualization.Powered by Discourse, best viewed with JavaScript enabled"
1338,new-cloud-applications-simready-assets-and-tools-for-nvidia-omniverse-developers-announced-at-gtc,"Originally published at:			New Cloud Applications, SimReady Assets, and Tools for NVIDIA Omniverse Developers Announced at GTC | NVIDIA Technical Blog
Discover the latest NVIDIA Omniverse releases and capabilities for building virtual worlds and pushing the boundaries of the metaverse.Powered by Discourse, best viewed with JavaScript enabled"
1339,scaling-ai-with-mlops-and-the-nvidia-partner-ecosystem,"Originally published at:			https://developer.nvidia.com/blog/scaling-ai-with-mlops-and-the-nvidia-partner-ecosystem/
AI is impacting every industry, from improving customer service and streamlining supply chains to accelerating cancer research.  As enterprises invest in AI to stay ahead of the competition, they often struggle with finding the strategy and infrastructure for success. Many AI projects are rapidly evolving, which makes production at scale especially challenging. We believe in…Our MLOps partner ecoystem is growing daily, making it easier than ever to deploye production AI at scale. The MLOps sessions and the partner panel at GTC are going to cover so much new ad usefully information to help get enterprises off the ground. I highly recommend joining to hear from the experts!Powered by Discourse, best viewed with JavaScript enabled"
1340,federated-learning-from-simulation-to-production-with-nvidia-flare,"Originally published at:			https://developer.nvidia.com/blog/federated-learning-from-simulation-to-production-with-nvidia-flare/
Learn about the new features of NVIDIA FLARE 2.2 that reduce development time and accelerate deployment for federated learning, helping organizations cut costs for building robust AI.Powered by Discourse, best viewed with JavaScript enabled"
1341,celebrating-the-2-million-innovators-changing-the-world,"Originally published at:			Celebrating the 2 Million Innovators Changing the World | NVIDIA Technical Blog
With the number of registered NVIDIA developers having just hit 2 million, NVIDIA developers are pursuing more breakthroughs than ever. Whether they’re tackling challenges at the cutting edge of physics, trying to tame a worldwide pandemic, or sorting their child’s Lego collection, innovators join NVIDIA’s developer program to help them solve their most challenging problems. It took…Powered by Discourse, best viewed with JavaScript enabled"
1342,easily-colorize-black-and-white-photos-with-ai,"Originally published at:			Easily Colorize Black and White Photos with AI | NVIDIA Technical Blog
Researchers from University of California, Berkeley developed an interactive deep learning-based app that makes it easy to accurately colorize a black and white image in minutes. Building on the researcher’s previous work of a convolutional neural network automatically adding color to black and white photos, their new app uses the same process, but with the…Powered by Discourse, best viewed with JavaScript enabled"
1343,upcoming-event-aria-zero-trust-security-gateway,"Originally published at:			https://developer.nvidia.com/blog/upcoming-event-aria-zero-trust-security-gateway/
Powered by Discourse, best viewed with JavaScript enabled"
1344,gpus-help-cut-siri-s-error-rate-by-half,"Originally published at:			https://developer.nvidia.com/blog/gpus-help-cut-siris-error-rate-by-half/
To make Siri great, Apple employed several artificial intelligence experts three years ago to apply deep learning to their intelligent mobile smart assistant. The team began training a neural net to replace the original Siri. “We have the biggest and baddest GPU farm cranking all the time,” says Alex Acero, who heads the speech team.…Powered by Discourse, best viewed with JavaScript enabled"
1345,delivering-fast-recommendations-from-google-analytics-360-sql-knowledge-graph-with-rapids-cugraph,"Originally published at:			Delivering fast recommendations from Google Analytics 360 SQL Knowledge Graph with RAPIDS cuGraph | NVIDIA Technical Blog
Introduction In part 1 of this blog series, we introduced The GA360 SQL Knowledge Graph that timbr has created, acting as a user-friendly strategic tool that shortens time to value. We discussed how users can conveniently connect GA360 exports to BigQuery in no time with the use of an SQL Ontology Template, which allows users…Powered by Discourse, best viewed with JavaScript enabled"
1346,get-ready-for-the-nvidia-dpu-hackathon-in-north-america,"Originally published at:			https://developer.nvidia.com/blog/get-ready-for-the-nvidia-dpu-hackathon-in-north-america/
Register by November 13 for the NVIDIA DPU Hackathon in North America.Powered by Discourse, best viewed with JavaScript enabled"
1347,gtc-21-top-5-data-science-technical-sessions,"Originally published at:			GTC 21: Top 5 Data Science Technical Sessions | NVIDIA Technical Blog
Join thousands of other practitioners, leaders, and innovators to learn data science from the world’s most advanced data teams. The following are some highlighted data science sessions planned for GTC: 1. GPU-Accelerated Model Evaluation: How we took our offline evaluation process from hours to minutes with RAPIDS In this session, we’ll describe how we utilized…Powered by Discourse, best viewed with JavaScript enabled"
1348,inception-spotlight-assaia-ai-ready-for-takeoff-at-kentucky-airport,"Originally published at:			Inception Spotlight: Assaia AI Ready for Takeoff at Kentucky Airport | NVIDIA Technical Blog
Switzerland-based Assaia International AG is deploying a deep learning solution at Cincinnati/Northern Kentucky International Airport (CVG) to help airport employees monitor the turnaround time between flights.Powered by Discourse, best viewed with JavaScript enabled"
1349,gtc-autonomous-vehicle-presentations,"Originally published at:			https://developer.nvidia.com/blog/gtc-autonomous-vehicle-presentations/
GTC provides the unique opportunity to hear from researchers, engineers, developers and technologists in the autonomous vehicle space.Powered by Discourse, best viewed with JavaScript enabled"
1350,3d-map-of-earths-interior,"Originally published at:			https://developer.nvidia.com/blog/3d-map-of-earths-interior/
A team of researchers led by Jeroen Tromp at Princeton University used a GPU-accelerated supercomputer to create a detailed 3D picture of Earth’s interior. “This is the first global seismic model where no approximations — other than the chosen numerical method — were used to simulate how seismic waves travel through the Earth and how…Powered by Discourse, best viewed with JavaScript enabled"
1351,learn-how-to-build-intelligent-recommender-systems,"Originally published at:			https://developer.nvidia.com/blog/learn-how-to-build-intelligent-recommender-systems/
Deep learning-based recommender systems are the secret ingredient behind personalized online experiences and powerful decision support tools in retail, entertainment, healthcare, finance, and other industries.  Recommender systems work by understanding the preferences, previous decisions, and other characteristics of many people. For example, recommenders can help a streaming media service understand the types of movies an…Powered by Discourse, best viewed with JavaScript enabled"
1352,gtc-2020-modularizing-natural-language-processing,"GTC 2020 S21560
Presenters: Zhengzhong Liu,Carnegie Mellon University; Zecong Hu,Carnegie Mellon University
Abstract
Recent success and growth in natural language processing and artificial intelligence have given the world many new applications, techniques, models, and architectures. We’ll show how appropriate abstraction and modularization can streamline both development and deployment of NLP technologies. We’ll provide a systematic overview of NLP abstractions and breakdown, the insights of machine learning integration, and the designs of NLP systems for fast module development. You’ll learn to use off-the-shelf tools to practice the modularized NLP and build practical applications. Our talk is suitable for researchers and practitioners with an intermediate-level understanding of NLP and ML concepts and applications, and a strong interest in real-world NLP application design and development.Watch this session
Join in the conversation below.Powered by Discourse, best viewed with JavaScript enabled"
1353,inception-nvidias-startup-incubator-at-tensorflow-world-2019,"Originally published at:			Inception, NVIDIA’s Startup Incubator, At TensorFlow World 2019 | NVIDIA Technical Blog
This year at TensorFlow World, startups within NVIDIA’s startup incubator, Inception, will showcase their latest AI-based applications, accelerated by NVIDIA GPUs, and running on the TensorFlow deep learning framework. Here is a spotlight of three Inception startups exhibiting this year:  Cnvrg.io Israel-based cnvrg.io has just released a new application that allows data scientists and developers…Powered by Discourse, best viewed with JavaScript enabled"
1354,intelligent-trash-pick-up-robots-coming-to-an-office-near-you,"Originally published at:			https://developer.nvidia.com/blog/intelligent-trash-pick-up-robots-coming-to-an-office-near-you/
By Mark Theis, Michael Chacko, Ishan Mitra, Shruthi Jaganathan While you’re probably used to throwing away your own trash (hopefully), there’s no reason to with modern artificial intelligence (AI). The Trashformer Team, a group of NVIDIA Jetson interns, created a small humanoid robot that detects and picks up trash autonomously through a neural network model…Powered by Discourse, best viewed with JavaScript enabled"
1355,nvidia-s-gaugan-wins-a-2019-popular-science-best-of-what-s-new-award,"Originally published at:			NVIDIA’s GauGAN Wins a 2019 Popular Science “Best of What’s New Award” | NVIDIA Technical Blog
NVIDIA’s viral real-time AI art sensation GauGan just won a “Best of What’s New Award” in the engineering category, Popular Science magazine announced today. “The Best of What’s New is our celebration of the most impactful and exciting innovations of the year,” says Popular Science Editor-in-Chief Joe Brown. “This expertly vetted collection lays the groundwork…Powered by Discourse, best viewed with JavaScript enabled"
1356,ai-helps-unlock-the-mysteries-of-the-brain,"Originally published at:			AI Helps Unlock the Mysteries of the Brain | NVIDIA Technical Blog
The human brain contains around 86 billion neurons and imaging a single cubic millimeter of it can generate more than 1000 terabytes of data. Because of the sheer size, the process of mapping the internal structure of the nervous system is computationally intensive and tedious. To accelerate the process, researchers from Google and the Max…Powered by Discourse, best viewed with JavaScript enabled"
1357,extending-nvidia-performance-leadership-with-mlperf-inference-1-0-results,"Originally published at:			Extending NVIDIA Performance Leadership with MLPerf Inference 1.0 Results | NVIDIA Technical Blog
Inference is where we interact with AI. Chat bots, digital assistants, recommendation engines, fraud protection services, and other applications that you use every day—all are powered by AI. Those deployed applications use inference to get you the information that you need. Given the wide array of usages for AI inference, evaluating performance poses numerous challenges…This is very exciting, I am a small time developer  just getting on my feet but it seems clear that team green will remain supreme. I really hope I can raise the money to get my hands on this technology when it’s ready.It seems to me when Nvidia choose Arm architecture it snowballed into the biggest jump start on the next level of innovation since x86 started it’s runEven if the above tests were hand picked, the graphs are too impressive to ignore. I want to know EVERYTHING!!!Hi Ronald. We’re pleased with our MLPerf Inference 1.0 results, and we submitted across ALL usages: CV, medical imaging, natural language processing, translation and recommender systems.  Our Triton Inference Server software did very well, as did our MIG technology to show the completeness of our data center platform.Cheers,
DeeGreat to hear about NVIDIA’s performance leadership in MLPerf Inference 1.0 results! They continue to push boundaries in the field.Powered by Discourse, best viewed with JavaScript enabled"
1358,segment-objects-without-masks-and-reduce-annotation-effort-using-the-discobox-dl-framework,"Originally published at:			https://developer.nvidia.com/blog/segment-objects-without-masks-and-reduce-annotation-effort-using-the-discobox-dl-framework/
Discobox: Weakly supervised learning algorithm that allows you to identify objects without costly mask annotations during training.Powered by Discourse, best viewed with JavaScript enabled"
1359,gpu-pro-tip-fast-dynamic-indexing-of-private-arrays-in-cuda,"Originally published at:			https://developer.nvidia.com/blog/fast-dynamic-indexing-private-arrays-cuda/
Sometimes you need to use small per-thread arrays in your GPU kernels. The performance of accessing elements in these arrays can vary depending on a number of factors. In this post I’ll cover several common scenarios ranging from fast static indexing to more complex and challenging use cases. Static indexing Before discussing dynamic indexing let’s…Thanks much for this Maxim! I was thinking about it for a many-small array project I am thinking of. There are several experiments and opinions about occupancy/registers/shared memory/spills around and your post is definitely fresh stuff.Two comments (Let me tell you I am still educating myself about CUDA).1) In your first example of dynamical rays, you write: i * a[start_index + i], while in the assembler instructions, there is no multiplication by ""i"". If the ""i*"" is kept, does the uniform access still hold?2) In your no-bank-conflict indexing, one creates a new array ""A"". In your example, I do not see how ""A"" is actually fed with its actual values. I imagine there is an allocation time. I wonder whether you could post your actual codes to better understand what and how (I found this very useful from other Mark Harris posts). I am really interested to see if this can help me.Hi Sergi,I am glad you found this post useful!1) You are perfectly right. I changed the source code while looking for neat SASS and forgot to change it here. Fixed now.2) Yes, I omitted setting A from the listing for the clarity.Here is the full source code of all the experiments I ran: http://pastebin.com/bZjNZkpZHello Maxim! Thanks for this useful post. I have a comment. The need for the helper function named `no_bank_conflict_index` can be avoided by allocating `A` 2 dimensional. Knowing that linearizing a multi-dimensional representation of the memory starts from the right most dimension, using below declaration of `A`__shared__ float A[ ARRAY_SIZE ][ THREADBLOCK_SIZE ];`val` can be retrieved with:float val = A[ index ][ threadIdx.x ];Although both versions will probably results in the same machine code and have the same performance, this representation might be better for clarity.Hello and Thanks to your post that is so useful.I have one question about below sentence.""Approximately 2.5 replays on average when index is an independent random variable with uniform distribution from 0 to 31;""What is the reason that replay number is 2.5...Hi Farzad, nice trick! This should definitely work.Hi! I did a Monte Carlo experiment with each thread accessing random location with uniform distribution, and I got ~3.5 shared memory banks accessed. Thus we have 1 ""normal"" access and 2.5 replays (in average).Amazing trick, and still work for very well for Pascal (GP102)Thanks for sharing!what is meaning of ""replay"" in section ""Dynamic Indexing with Non-Uniform Access"" ?there are 2 lines  A:   int index = indexbuf[threadIdx.x + blockIdx.x * blockDim.x]; B:   float val = a[index];which line will be replayed, A or B?from the context, I guess ""replay load/store"" means threads need to read from different location. if all threads in a warp read the same location, we do not need replay.But I still have a question:all threads in a warp have same ""index"", but they still need to read ""val"" from different location of local memory, because their ""a"" is different.Powered by Discourse, best viewed with JavaScript enabled"
1360,what-is-a-pretrained-ai-model,"Originally published at:			What Is a Pretrained AI Model? | NVIDIA Blog
A pretrained AI model is a deep learning model that’s trained on large datasets to accomplish a specific task, and it can be used as is or customized to suit application requirements across multiple industries.Powered by Discourse, best viewed with JavaScript enabled"
1361,gtc-2020-inside-nvidias-ai-infrastructure-for-self-driving-cars,"GTC 2020 S22355
Presenters: Clement Farabet,NVIDIA
Abstract
We’ll discuss Project MagLev, NVIDIA’s internal end-to-end AI platform for developing its self-driving car software, DRIVE. We’ll explore the platform that supports continuous data ingest from multiple cars producing terabytes of data per hour. We’ll also cover how the platform enables autonomous AI designers to iterate training of new neural-network designs across thousands of GPU systems and validate the behavior of these designs over multi-petabyte-scale data sets. We’ll talk about our overall architecture for everything from data center deployment to AI pipeline automation, as well as large-scale AI dataset management, AI training, and testing.Watch this session
Join in the conversation below.Powered by Discourse, best viewed with JavaScript enabled"
1362,using-cuda-and-machine-learning-to-detect-colon-cancer,"Originally published at:			Using CUDA and Machine Learning to Detect Colon Cancer | NVIDIA Technical Blog
As part of the GlaS@MICCAI2015 colon gland segmentation challenge, a team of researchers introduced a machine learning-based algorithm to segment glands in tissue of benign and malignant colorectal cancer. The variability of glandular structures in biological tissue poses a challenge to automated analysis of histopathology slides. It has become a key requirement to  quantitative morphology…Powered by Discourse, best viewed with JavaScript enabled"
1363,an-even-easier-introduction-to-cuda,"Originally published at:			https://developer.nvidia.com/blog/even-easier-introduction-cuda/
Learn more with these hands-on DLI courses: Fundamentals of Accelerated Computing with CUDA C/C++ Fundamentals of Accelerated Computing with CUDA Python This post is a super simple introduction to CUDA, the popular parallel computing platform and programming model from NVIDIA. I wrote a previous “Easy Introduction” to CUDA in 2013 that has been very popular over…Hi Sir, thanks for the tutorial. I tried this code on Tesla T4, and found that using 4096 blocks does not improve the performance to us level compare to 1 block 256 threads. What might be the reason ? thanks.Thanks for the post Mark.  To get accurate profiling, is it still a good idea to put cudaDeviceReset() just prior to exiting?  https://devblogs.nvidia.com...Also, is it possible to get this level of timing via code?  cudaEventElapsedTime does not seem to have this same level of precision.Thanks Mark. I tried this and it did not work as a straight copy and paste. The cudaMallocManaged did not appear to do anything. This is on a Titan X with up to date drivers and NSight. I replaced the cudaMallocManaged functionality with the relevant cudaMalloc and cudaMempy, which sorted it. Am I missing something wrt cudaMallocManaged?It's a really good post btw. Thanks - I have learned much.What do you mean did not appear to do anything? Did you get an error? Incorrect results? What CUDA version do you have installed? Is it an ""NVIDIA Titan X"" (Pascal) or ""GeForce GTX Titan X"" (Maxwell)?I think the Windows tools are more dependent on cudaDeviceReset(). I kept it out of this post to keep things simple. cudaEventElapsedTime() should have the same level of precision, but in more complex apps you may get things in your timing that you didn't intend.I believe the most reliable way to accurately time is to run your kernel many times in a loop, followed by cudaDeviceSynchronize() or cudaStreamSynchronize(), and use a high precision CPU timer (like std::chrono) to wrap the whole loop and the sync. Then divide by the number of iterations.CUDA 8 latest. It's the Pascal card. After executing the cudaMallocManaged function the variable pointed to address 0x0. I'll put error checking into your original code in the morning and try to get more diagnostic information.I cannot access variable assigned using 'cudaMallocManaged' on host. It throws '0xC0000005: Access violation writing location 0x00000000' error. I can access them fine on device kernel. Am I missing something ? I am using MS Visual Studio Community 2015 with CUDA Runtime 8.0 on GTX 1070.I think I had the same problem. Var pointed to '0x0' and it was not accessible by host. However accessible by device. I know it works the old was using separate var for host and device. But it would be good if we can make it work without all those memCpy like this example does. Tell me if you have any luck.Did you guys change the program at all? If you share your changes I can try to diagnose.Did you change the code, or are you getting the error in the initialization loop?I didn't change the code at all. Just copied it to visual studio. Yes, I got error on initialization loop. So I put initialization in kernel. Got error at verification as expected. Removed verification and it ran fine. From this I concluded that it's not accessible by host. Further mode in VS debugging watch it shows it the same was as var allocated by cudaMalloc, some can't read or something indicating that it's not accessible to CPU as I understand. And the address is 0x0. Sorry for long response. I really appreciate your help.I just put the unchanged original on a box with a GT 755M and it fails similarly. What have I done wrong?:::#include ""cuda_runtime.h""#include ""device_launch_parameters.h""#include <iostream>#include <math.h>// Kernel function to add the elements of two arrays__global__void add(int n, float *x, float *y){    for (int i = 0; i < n; i++)        y[i] = x[i] + y[i];}int main(void){    int N = 1 << 20;    float *x, *y;    // Allocate Unified Memory ñ accessible from CPU or GPU    cudaMallocManaged(&x, N*sizeof(float));    cudaMallocManaged(&y, N*sizeof(float));    // initialize x and y arrays on the host    for (int i = 0; i < N; i++) {        x[i] = 1.0f;        y[i] = 2.0f;    }    // Run kernel on 1M elements on the GPU    add << <1, 1 >> >(N, x, y);    // Wait for GPU to finish before accessing on host    cudaDeviceSynchronize();    // Check for errors (all values should be 3.0f)    float maxError = 0.0f;    for (int i = 0; i < N; i++)        maxError = fmax(maxError, fabs(y[i] - 3.0f));    std::cout << ""Max error: "" << maxError << std::endl;    // Free memory    cudaFree(x);    cudaFree(y);    return 0;}It is not sufficient to change the the target to x64  in the CUDA properties solution properties and Active(x64) in the properties - you must change it in the solution configuration manager. It then execs fine. The error is simply a 'not supported' one.Thanks Mark - suspected it was my bad.You need to change the solution properties to x64 in the Configuration Manager. Changin it in the CUDA props and in the Active platform dropdown don't get it done.Thanks, that worked !any chance you can post a screenshot of this, @mike_agius:disqus? Thanks!I have used this tech way back and am quite pleased with the results .The speedup is so adictive that i swear by my programI encorage all to give cuda a must try to solve problems even if it requires nvidia specific hardware.This is my code developed when i was grad student http://bit.ly/2ko6rNbSure https://uploads.disquscdn.c...Powered by Discourse, best viewed with JavaScript enabled"
1364,143-new-ancient-carvings-discovered-with-the-help-of-ai,"Originally published at:			143 New Ancient Carvings Discovered with the Help of AI | NVIDIA Technical Blog
For the first time using AI, researchers from Yamagata University in Japan, in collaboration with IBM, have discovered 142 new geoglyphs, which depict people, animals, and other beings, on the ancient motifs in the Nazca Pampa region of Peru.  The new geoglyphs were identified using high-resolution 3D data, taken from on-site surveys and aerial imagery…Powered by Discourse, best viewed with JavaScript enabled"
1365,gpu-inference-momentum-continues-to-build,"Originally published at:			https://developer.nvidia.com/blog/gpu-inference-momentum-continues-to-build/
AI algorithms trained on NVIDIA GPUs have proven their mettle to draw insights from huge swaths of data. They have enabled researchers and companies to gain new, deeper insights, and deliver more insights in less time. This evolution has taken training times from days to minutes, and researchers have invented sophisticated techniques that use multiple…Powered by Discourse, best viewed with JavaScript enabled"
1366,app-helps-fishermen-instantly-id-their-catch,"Originally published at:			https://developer.nvidia.com/blog/app-helps-fishermen-instantly-id-their-catch/
Not sure which fish you caught? FishVerify uses artificial intelligence to help fishermen instantly identify their catch and learn local fishing regulations related to that specific specie. With over 80 years of fishing experience combined, the three founders found themselves still stumped by the mysterious fish they caught in the Florida waters. Using CUDA, Tesla…Powered by Discourse, best viewed with JavaScript enabled"
1367,cusparselt-v0-1-0-now-available-arm-and-windows-support,"Originally published at:			cuSPARSELt v0.1.0 Now Available: Arm and Windows Support | NVIDIA Technical Blog
NVIDIA announced the availability of cuSPARSELt version 0.1.0. This software can be downloaded now free for members of the NVIDIA Developer Program.Powered by Discourse, best viewed with JavaScript enabled"
1368,an-end-to-end-blueprint-for-customer-churn-modeling-and-prediction-part-3,"Originally published at:			https://developer.nvidia.com/blog/an-end-to-end-blueprint-for-customer-churn-modeling-and-prediction-part-3/
This is the third installment in a series describing an end-to-end blueprint for predicting customer churn. In previous installments, we’ve discussed some of the challenges of machine learning systems that don’t appear until you get to production: in the first installment, we introduced our use case and described an accelerated data federation pipeline; in the…Powered by Discourse, best viewed with JavaScript enabled"
1369,top-5-ai-stories-of-the-week,"Originally published at:			https://developer.nvidia.com/blog/top-5-ai-stories-of-the-week/
In this week’s edition of the NVIDIA Developer Top 5 video, we revisit the top AI developer stories of the week. From an AI algorithm that can automatically detect fake news on online platforms to the first study to officially determine that smoking makes people biologically older. Plus, see how AI is helping first responders…Powered by Discourse, best viewed with JavaScript enabled"
1370,boosting-application-performance-with-gpu-memory-access-tuning,"Originally published at:			https://developer.nvidia.com/blog/boosting-application-performance-with-gpu-memory-access-tuning/
In this post, we examine a method programmers can use to saturate memory bandwidth on a GPU.This blog saves the best part for the end. Users interested in tuning performance of their CUDA kernels should always try and use launch bounds first. In this particular case study that would have been enough. But not always, of course.Great walkthrough, thank you! Can you please elaborate on why the duration of all kernel variants suddenly dropped to the same value (~10 ns) starting from ~20th kernel launch (Figure 1)?  Does that mean that for an application with thousands of kernel launches all the described optimizations are actually useless?Good question. The plot of kernel durations shown actually repeats itself in the application from which it was derived. Figure 1 shows one full period of that repetition. After the kernel duration drops, it jumps back up again, and substantial overall application performance improvement is obtained from the optimizations described in the blog.Thank you, very intriguing! Is there an explanation for such a huge variation (3x-5x!) of kernel durations?Great walkthrough, thank you! Can you please elaborate on why the duration of all kernel variants suddenly dropped to the same value (~10 ns) starting from ~20th kernel launch (Figure 1)? Does that mean that for an application with thousands of kernel launches all the described optimizations are actually useless?thanks for the awesome information.Mark, the drop is due to a smaller data set being processed by the kernel. But, as I said, it’s a cyclic process, and the size increases again periodically.Thank you, robv, now it is all clear! I will give it a go for sure.пн, 8 авг. 2022 г. в 18:45, Robv via NVIDIA Developer Forums <notifications@nvidia.discoursemail.com>:Very good, Mark. Please let me know if you have more questions.I think there’s a typo in the third paragraph of the launch bounds section: “each thread can use up to 64 threads”. I think that last word should be “registers”.You are correct again, thank you. We’ll fix the typo.Fixed! Thanks, @dwatersg!Great walkthrough, thank you! Can you please elaborate on why the duration of all kernel variants suddenly dropped to the same value (~10 ns) starting from ~20th kernel launch (Figure 1)? Does that mean that for an application with thousands of kernel launches all the described optimizations are actually useless?Ometv
thanks for the awesome information.thanks my issue has been fixed.Powered by Discourse, best viewed with JavaScript enabled"
1371,gtc-2020-building-autonomous-store-platform-for-everyone,"GTC 2020 S21905
Presenters: Steve Gu,AiFi Inc.
Abstract
Learn how AiFi’s autonomous store platform, coined O.A.S.I.S (Open Autonomous Store Infrastructure and Services), can provide a turnkey solution for retailers who want to implement their own autonomous stores without reinventing the wheel. AiFi’s all-in-one technology platform for retailers includes seamless multi-camera/multi-person tracking, real-time product recognition, and live inventory tracking, as well as analytics for shopper behavior and sales. AiFi interprets all of this shopping and inventory activity simultaneously, in real time, using NVIDIA’s high-performing and power-efficient P4 GPUs, without delays or latency. With its latest platform, AiFi is launching a whole new suite of APIs that can be easily integrated into a retailer’s front end, back end, and point-of-sale systems. Come and hear how AiFi has harnessed the power of GPUs to create a delightful checkout-free experience at lightning speed for retail customers.Watch this session
Join in the conversation below.Powered by Discourse, best viewed with JavaScript enabled"
1372,a-car-that-can-anticipate-your-next-driving-maneuver,"Originally published at:			https://developer.nvidia.com/blog/a-car-that-can-anticipate-your-next-driving-maneuver/
Through a project called Brain4Cars, Stanford and Cornell researchers released a new architecture consisting of Recurrent Neural Networks (RNNs) that use Long Short-Term Memory (LSTM) units to predict driving maneuvers several seconds in advance. This enables assistive cars to alert drivers before they make a dangerous maneuver. Maneuver anticipation complements existing Advance Driver Assistance Systems…Powered by Discourse, best viewed with JavaScript enabled"
1373,upcoming-event-conversational-ai-sessions-at-gtc-2022,"Originally published at:			Conversational AI & NLP Conference Sessions | GTC 2022 | NVIDIA
Learn about the latest tools, trends, and technologies for building and deploying conversational AI.Powered by Discourse, best viewed with JavaScript enabled"
1374,optimize-ray-tracing-with-nvidia-nsight-graphics-2021-5-featuring-windows-11-support,"Originally published at:			Optimize Ray Tracing with NVIDIA Nsight Graphics 2021.5 Featuring Windows 11 Support | NVIDIA Technical Blog
NVIDIA announced the latest release in Nsight Graphics, which supports Direct3D, Vulkan, OpenGL, OpenVR, and the Oculus SDK.Does this Nsight version support OptiX API , or not?Powered by Discourse, best viewed with JavaScript enabled"
1375,n-ways-to-saxpy-demonstrating-the-breadth-of-gpu-programming-options,"Originally published at:			https://developer.nvidia.com/blog/n-ways-to-saxpy-demonstrating-the-breadth-of-gpu-programming-options/
Back in 2012, NVIDIAN Mark Harris wrote Six Ways to Saxpy, demonstrating how to perform the SAXPY operation on a GPU in multiple ways, using different languages and libraries. Since then, programming paradigms have evolved and so has the NVIDIA HPC SDK. In this post, I demonstrate five ways to implement a simple SAXPY computation…Six Ways to SAXPY gets even more expansive when we consider all the ways one can now program NVIDIA GPUs. Since 2012 we’ve seen great work from the NVIDIA HPC SDK team and other open source projects to make NVIDIA GPUs easier to program for. With the latest NVC++ compiler, we even have stdpar acceleration, which was demonstrated in more detail in blogs like Accelerating Standard C++ with GPUs Using stdpar, Accelerating Fortran DO CONCURRENT with GPUs and the NVIDIA HPC SDK, and Accelerating Python on GPUs with nvc++ and Cython
There are also many improvements that have been made to parts of the SDK like Thrust, cuBLAS, OpenACC to improve performance, allow better use of Unified Memory, etc.
Furthermore, there have been more open source projects that allow one to program for NVIDIA GPUs like cuPy, Numba, Tensorflow, Pytorch and more are always showing up thanks to the NVIDIA Compiler SDK
Thanks for reading my blog and leave a comment if you have any questions or letting me know how you program for NVIDIA GPUs.Powered by Discourse, best viewed with JavaScript enabled"
1376,using-cuda-warp-level-primitives,"Originally published at:			https://developer.nvidia.com/blog/using-cuda-warp-level-primitives/
Figure 1: The Tesla V100 Accelerator with Volta GV100 GPU. SXM2 Form Factor. NVIDIA GPUs execute groups of threads known as warps in SIMT (Single Instruction, Multiple Thread) fashion. Many CUDA programs achieve high performance by taking advantage of warp execution. In this blog we show how to use primitives introduced in CUDA 9 to make your…Hi guys,there is a small mistake in Listing 6: it should beint y2 = threadIdx.x % 4;Robin, thanks for pointing out the mistake. Fixed.This is a very helpful article.  I think the information on shuffles in the CUDA C Programming Guide is a bit too brief, especially regarding the mask.In Listing 8, I think the initialisation should be int v = shmem[tid];Do warp primitives work on the pascal architecture?  I have a geforce gtx 1050 ti card and i'm trying to get the __shfl_down_sync function to work, but it just returns 0.  Please let me know if it works on the Pascal architecture.  Thanks!I have a question on the use of __match_all_sync in Listing 9. According to CUDA Programming Guide""__match_all_syncReturns mask if all threads in mask have the same value for value; otherwise 0 is returned. Predicate pred is set to true if all threads in mask have the same value of value; otherwise the predicate is set to false.""So, is it actually what we want there? How about ""__match_any_syncReturns mask of threads that have same value of value in mask""?Thanks.The explanation for Listing 4 says ""all the threads in a warp get the value of val from the thread at lane 0"" -- for this, shouldn't the offset be ""-threadIdx.x"" rather than 0?  When the offset is 0, isn't the code just val+=val?Regarding ""On the latest Volta (and future) GPUs, you can run library functions that use warp synchronous primitives without worrying whether the function is called in a thread-divergent branch.""  -- so the compiler ensures that any mask we supply, such as -1, is ANDed with a properly determined mask of ACTIVE THREADS (while avoiding the error shown in Listing 5 -- ""__active_mask_with_sync()"")?  Otherwise, in Listing 4, FULL_MASK (=-1) would need to be replaced by ""even mask"" and ""odd mask"".      .Do the *_sync() operations (e..g, __shfl_sync()) imply a barrier/memory fence, as __syncwarp()?Is there a porting guide which will show mapping of kepler warp intrinsics to volta warp instrinsics which could be adopted by legacy applications without learning new semantics?Thank you so much for this helpful guide.A question about masks: when you say ""N-th"" bit, is it from the left or from the right? so, if I only want the first thread in the warp to participate, would it be 0x0001 or 0x8000?LSB. So 0x0001 is the first thread. 0xFFFF is threads 0-15, 0x80000000 is thread 31, 0xFFFFFFFF is all 32 threads.Thanks!Awesome functionality and great writing!  Heads up that __activemask() is misspelled as __active_mask() a few times.Fixed! Thanks.In ""Update Legacy Warp-Level Programming"", it says ""Don’t just use FULL_MASK (i.e. 0xffffffff for 32 threads) as the mask value. If not all threads in the warp can reach the primitive according to the program logic, then using FULL_MASK may cause the program to hang.""but in listing 4, if (threadIdx.x % 2) {    val += __shfl_sync(FULL_MASK, val, 0);…}else {val += __shfl_sync(FULL_MASK, val, 0);…}and it says ""On the latest Volta (and future) GPUs, you can run library functions that use warp synchronous primitives without worrying whether the function is called in a thread-divergent branch.""  and You just used FULL_MASK inside branch.I tested ballot_sync myself using FULL_MASK inside nested control flow statements in various cases, it produced unexpected outputs or even deadlocked in some cases. So I guess it is not safe to use FULL_MASK inside arbitrary branch, at least for ballot_sync.So, I wonder how do I interpret listing 4?In listing 4, regardless the thread id is even or odd, the thread in the warp will always execute one of the two __shfl_sync() statements. Therefore, FULL_MASK should be used.The following code may cause a stall.if (threadIdx.x % 2) {   val += __shfl_sync(FULL_MASK, val, 0);   …}else {   …}Thanks for clarifying! Does this mean it is only safe to use FULL_MASK inside non-nested if-else statements, where all threads must execute one or the other path?For example, the below code would not be safe?if (some_condition){  if (threadIdx.x % 2) {   val += __shfl_sync(FULL_MASK, val, 0);  …  }  else {   val += __shfl_sync(FULL_MASK, val, 0);   …  }}I would guess that this is a typo, yes!Given the documentation and the intended use case, it should be __match_any_sync.I don’t understand why __syncwarp() is needed in listing 8.The article said:
“The CUDA programming model does not guarantee that all the reads will be performed before all the writes.”I come up with a situation where sync is needed, like blow:In the code above, as hardware may not re-convergence after else, the threads in same warp may not execute the same instruction. So the final result is undefined.But what if there is no such situation, I mean, there is no warp-level divergency? is it necessary to add __syncwarp()? Assume shared memory array is decorated by volatile.If it’s still necessary, please tell me why some threads may not read data written by other threads.CUDA C++ programs following this programming model are guaranteed to work correctly for all future HWs.  Compiler will remove the __syncwarp if it is not needed on a particular target.Powered by Discourse, best viewed with JavaScript enabled"
1377,speeding-up-numerical-computing-in-c-with-a-python-like-syntax-in-nvidia-matx,"Originally published at:			https://developer.nvidia.com/blog/speeding-up-numerical-computing-in-c-with-a-python-like-syntax-in-nvidia-matx/
MatX is an experimental library that allows you to write high-performance GPU code in C++, with high-level syntax and a common data type across all functions.Powered by Discourse, best viewed with JavaScript enabled"
1378,accelerating-next-generation-cybersecurity-with-nvidia-morpheus-now-with-expanded-early-access,"Originally published at:			Accelerating Next-Generation Cybersecurity with NVIDIA Morpheus, Now with Expanded Early Access | NVIDIA Technical Blog
The NVIDIA Morpheus AI framework enables behavior analysis on a massive scale, with accelerated performance 600X faster than CPU only.Powered by Discourse, best viewed with JavaScript enabled"
1379,meet-the-researcher-antti-honkela-applying-machine-learning-to-preserve-private-data,"Originally published at:			https://developer.nvidia.com/blog/meet-the-researcher-antti-honkela-applying-machine-learning-to-preserve-private-data/
Dr. Honkela is the Coordinating Professor of the Research Program in Privacy-preserving and Secure AI at the Finnish Center for Artificial Intelligence (FCAI).Powered by Discourse, best viewed with JavaScript enabled"
1380,nvidia-partners-with-id-tech-on-new-ai-and-machine-learning-course,"Originally published at:			https://developer.nvidia.com/blog/nvidia-partners-with-id-tech-on-new-ai-and-machine-learning-course/
NVIDIA has partnered with iD Tech to create the Artificial Intelligence and Machine Learning certification program, a boot-camp style course for teens.Hello,
I would like to get in touch with the person managing the partnership with iD Tech for the AI/ML certification program.  I want to share my experience with the program.
Regards,
VinitPlease contact me. I have a first hand experience with this program. They are giving out pre-written programs and asking each student to copy and paste in their Jetson NANO. There is no discussion of basic concepts of AI/ML nor how to program for.nVIDIA’s good name and reputation is being used. nVIDIA is giving out certificates without realizing that these students have not learned anything other than how to copy/paste someone’s code.Hi Vinit,I will send you a message.Best,
ChelseaPowered by Discourse, best viewed with JavaScript enabled"
1381,gpu-accelerated-speech-to-text-with-kaldi-a-tutorial-on-getting-started,"Originally published at:			GPU-Accelerated Speech to Text with Kaldi: A Tutorial on Getting Started | NVIDIA Technical Blog
Recently, NVIDIA achieved GPU-accelerated speech-to-text inference with exciting performance results. That blog post described the general process of the Kaldi ASR pipeline and indicated which of its elements the team accelerated, i.e. implementing the decoder on the GPU and taking advantage of Tensor Cores in the acoustic model. Now with the latest Kaldi container on…I would like to see a link to an article which describes what is needed to use the model in real time.Do you mean as in streaming audio in real time?  How many streams of audio would you have?  This is something we are currently working on.I'm also interested in the about especially in voice related home automationin the WAV format, shouldn't it be 16bit instead of 32bit float ?Hi, I would like to know if the real-time streaming option is out yet ? If not, when is this going to be supported.Hi,Yes, streaming is now fully supported. You can find more details there: https://developer.nvidia.com/gtc/2020/video/s21832-vidThanks,
HugoAssuming this forum is appropriate to discuss KALDI implementation issues. If not, I apologize.I hit a roadblock when trying to use KALDI for a corpus of english-spanish language data using this code which seems to be taylored to Chinese.
More details in this status report There is a paragraph ‘discontinuing the project’ explaining the data preparation issue. I would appreciate any help on thisPowered by Discourse, best viewed with JavaScript enabled"
1382,nvidia-at-gdc-2020,"Originally published at:			NVIDIA at GDC 2020 | NVIDIA Technical Blog
Editors note: GDC has been postponed. Stay tuned for updates. Come join NVIDIA at GDC 2020 in San Francisco from Monday, March 16 to Friday, March 20 to see all the latest breakthroughs in gaming—and beyond. Come join NVIDIA at GDC 2020 in San Francisco from Monday, March 16 to Friday, March 20 to see…Powered by Discourse, best viewed with JavaScript enabled"
1383,nvidia-announces-nsight-aftermath,"Originally published at:			NVIDIA Announces Nsight Aftermath | NVIDIA Technical Blog
NVIDIA announces Nsight Aftermath – a new feature in the Nsight Graphics 2019.1 release. Nsight Aftermath gives developers the tools to solve one of the most frustrating and time consuming problems in the industry – GPU crashes and hangs.   With Nsight Aftermath, developers will save countless hours of debugging by easily inspecting GPU specific…Powered by Discourse, best viewed with JavaScript enabled"
1384,extending-block-cyclic-tensors-for-multi-gpu-with-nvidia-cutensormg,"Originally published at:			https://developer.nvidia.com/blog/extending-block-cyclic-tensors-for-multi-gpu-with-nvidia-cutensormg/
cuTENSOR is now able to distribute tensor contractions across multiple GPUs. This has been released as a new library called cuTENSORMg (multi-GPU).Powered by Discourse, best viewed with JavaScript enabled"
1385,nvidia-rtx-top-3-week-of-january-31-2019,"Originally published at:			NVIDIA RTX Top 3: Week of January 31, 2019 | NVIDIA Technical Blog
Every week, we’ll be delivering three interesting stories coming from the world of RTX Game Development. Quake 2 Adds Real-Time Ray Tracing In this article, Venture Beat shows off Quake 2 running with Real-Time Ray Tracing. Atomic Heart Q&A on NVIDIA RTX/DLSS Metro Exodus: GeForce RTX Real-Time Ray Traced Global Illumination Demo 4A Games are…Powered by Discourse, best viewed with JavaScript enabled"
1386,gtc-2020-shifts-to-online-event-gtc-digital,"Originally published at:			GTC 2020 Shifts to Online Event: GTC Digital | NVIDIA Technical Blog
NVIDIA has decided to shift GTC 2020 to an online event due to growing concern over the coronavirus. What is GTC Digital?  GTC Digital will include all the great training, research, insights, and direct access to the brilliant minds of NVIDIA’s GPU Technology Conference, now online. The best part is — registration for GTC Digital…Powered by Discourse, best viewed with JavaScript enabled"
1387,explore-the-future-of-robotics-at-gtc-2022,"Originally published at:			AI Developer Conference Session Catalog | GTC 2022 | NVIDIA
Discover the latest innovations in AI and robotics, and hear world-renowned roboticist Dr. Henrik Christensen talk about the future of robotics…Powered by Discourse, best viewed with JavaScript enabled"
1388,cuda-spotlight-gpu-accelerated-cancer-detection,"Originally published at:			https://developer.nvidia.com/blog/cuda-spotlight-gpu-accelerated-cancer-detection/
This week’s Spotlight is on Diego Rivera, a senior software engineer at Hologic, Inc. Hologic is a leading developer of medical imaging systems and surgical products, with an emphasis on serving the healthcare needs of women throughout the world. NVIDIA: Diego, tell us about your role at Hologic. Diego: I’m part of a team that has…Powered by Discourse, best viewed with JavaScript enabled"
1389,porting-gpu-accelerated-applications-to-power8-systems,"Originally published at:			https://developer.nvidia.com/blog/porting-gpu-accelerated-applications-power8-systems/
With the US Department of Energy’s announcement of plans to base two future flagship supercomputers on IBM POWER CPUs, NVIDIA GPUs, NVIDIA NVLink interconnect, and Mellanox high-speed networking, many developers are getting started building GPU-accelerated applications that run on IBM POWER processors. The good news is that porting existing applications to this platform is easy. In…Want to do some what-if or drill-down analysis with Cognos BI or other applications? Is it possible ?Hi, we don't have any product announcements in this area, but I agree it is a great example of where POWER & GPUs are relevant. In fact there is a BI acceleration product from Jedox that you might want to check out:  (http://www.jedox.com/en/pro...Powered by Discourse, best viewed with JavaScript enabled"
1390,bring-on-your-questions-we-have-oli-wright-and-filip-strugar-live-answering-questions,"Another 15mins leftPowered by Discourse, best viewed with JavaScript enabled"
1391,improving-gameplay-latency-in-unreal-engine-5-with-nividia-reflex,"The NVIDIA Reflex plug-in reduces system latency, which is key for any title where a responsive experience is required. With native support in Unreal Engine 5, simply navigate to the plug-ins folder in UE5, search for NVIDIA Reflex, and enable.Review the video walkthrough below for information on installing and implementing NVIDIA Reflex.This is part of a series of Unreal 5 related introductory videos first listed in our technical blog on the developer pages. https://developer.nvidia.com/blog/shaping-the-future-of-graphics-with-nvidia-technologies-in-unreal-engine-5/Powered by Discourse, best viewed with JavaScript enabled"
1392,generating-and-editing-high-resolution-synthetic-images-with-gans,"Originally published at:			Generating and Editing High-Resolution Synthetic Images with GANs | NVIDIA Technical Blog
Researchers from NVIDIA, led by Ting-Chun Wang, have developed a new deep learning-based system that can generate photorealistic images from high-level labels, and at the same time create a virtual environment that allows the user to modify a scene interactively. The researchers, Ting-Chun Wang, Ming-Yu Liu, Jun-Yan Zhu, Andrew Tao, Jan Kautz, and Bryan Catanzaro…Powered by Discourse, best viewed with JavaScript enabled"
1393,ai-can-play-it-by-ear,"Originally published at:			AI Can Play It By Ear | NVIDIA Technical Blog
Researchers from Facebook developed a deep learning system that can replicate the music it hears and play it back as if it were Mozart, Beethoven, or Bach. This is the first time researchers have produced high fidelity musical translation between instruments, styles, and genres. “Humans have always created music and replicated it – whether it…Powered by Discourse, best viewed with JavaScript enabled"
1394,developer-spotlight-computational-fluid-dynamics-for-surgical-planning,"Originally published at:			https://developer.nvidia.com/blog/share-your-science-computational-fluid-dynamics-for-surgical-planning/
Todd Raeker, Research Technology Consultant at the University of Michigan shares how a group of 50 researchers at University of Michigan are using GPUs and OpenACC to accelerate the codes for their data-driven physics simulations. The current versions of the codes use MPI and depend on finer and finer meshes for higher accuracy which are…Powered by Discourse, best viewed with JavaScript enabled"
1395,jumpstarting-ai-with-a-covid-19-ct-inference-pipeline-and-the-nvidia-clara-deploy-quickstart-virtual-machine,"Originally published at:			https://developer.nvidia.com/blog/jumpstarting-ai-with-covid-19-ct-inference-pipeline-and-clara-deploy-quickstart-vm/
Getting AI up and running in hospitals has never been more important. Until recently, connecting an inference pipeline to perform analysis has had its challenges and limitations. There is a considerable amount of complexity in setting up and maintaining the hardware and software, deployment, configuration, and all workflow steps in an AI inference research pipeline.…Powered by Discourse, best viewed with JavaScript enabled"
1396,gtc-2020-rapids-gpu-accelerated-data-analytics-machine-learning,"GTC 2020 D2S29
Presenters: Tech Demo Team,NVIDIA
Abstract
The RAPIDS suite of software libraries, built on CUDA-X AI, gives you the freedom to execute end-to-end data science and analytics pipelines entirely on GPUs. It relies on NVIDIA CUDA primitives for low-level compute optimization, but exposes that GPU parallelism and high-bandwidth memory speed through user-friendly Python interfaces.RAPIDS also focuses on common data preparation tasks for analytics and data science. This includes a familiar DataFrame API that integrates with a variety of machine learning algorithms for end-to-end pipeline accelerations without paying typical serialization costs. RAPIDS also includes support for multi-node, multi-GPU deployments, enabling vastly accelerated processing and training on much larger dataset sizes.This demonstration uses RAPIDS, and OmniSci’s GPU-accelerated analytics platform to quickly visualize and run queries on the 1.1 billion New York City taxi ride dataset.Watch this session
Join in the conversation below.Hello.
Could we download the Jupyter Notebook?
ThanksPowered by Discourse, best viewed with JavaScript enabled"
1397,how-nvidia-driveworks-ensures-av-sensors-stay-aligned,"Originally published at:			How NVIDIA DriveWorks Ensures AV Sensors Stay Aligned | NVIDIA Technical Blog
Lidars and cameras alone aren’t enough to put self-driving into action. Sensor diversity is a cornerstone of autonomous driving. However, it only works if every sensor is in alignment. The NVIDIA DriveWorks SDK makes it possible to perform sensor calibration both offline, before the vehicle hits the road, and while the vehicle is driving with…Powered by Discourse, best viewed with JavaScript enabled"
1398,ai-transforms-smartphones-into-laboratory-grade-devices,"Originally published at:			AI Transforms Smartphones Into Laboratory-grade Devices | NVIDIA Technical Blog
Imagine using your smartphone to take images that compare to the quality of laboratory-grade microscopes and can be used by doctors and biomedical professionals to assess a medical sample and even recommend treatment. Researchers at UCLA developed a deep learning technique that can enhance microscopic details in photos taken by smartphones and make them comparable…Powered by Discourse, best viewed with JavaScript enabled"
1399,on-demand-webinar-deep-learning-demystified,"Originally published at:			On Demand Webinar: Deep Learning Demystified | NVIDIA Technical Blog
Artificial Intelligence (AI) is solving problems that seemed well beyond our reach just a few years back. Using deep learning, the fastest growing segment of AI, computers are now able to learn and recognize patterns from data that were considered too complex for expert written software. Today, deep learning is transforming every industry, including automotive,…Powered by Discourse, best viewed with JavaScript enabled"
1400,accelerating-cloud-native-supercomputing-with-magnum-io,"Originally published at:			Accelerating Cloud-Native Supercomputing | NVIDIA Technical Blog
NVIDIA Magnum IO helps to accelerate MPI operations along with NVIDIA SHARP by offloading collective operation from the host CPU.URL in blog is wrong Please fix.from Magnum IO Software Stack for Accelerated Data Centers | NVIDIA to Magnum IO Software Stack for Accelerated Data Centers | NVIDIA@sakaia  – Good catch! URL is now fixed.Powered by Discourse, best viewed with JavaScript enabled"
1401,training-in-nvidia-isaac-sim-closes-the-sim2real-gap,"Originally published at:			Training in NVIDIA Isaac Sim Closes the Sim2Real Gap | NVIDIA Technical Blog
Interested in designing, testing, or training your robot in a virtual environment? This can all be done with Issac Sim on the NVIDIA Omniverse simulation platform.Powered by Discourse, best viewed with JavaScript enabled"
1402,nvidia-sdk-updated-with-new-releases-of-tensorrt-cuda-and-more,"Originally published at:			NVIDIA SDK Updated With New Releases of TensorRT, CUDA, and More | NVIDIA Technical Blog
At NIPS 2017, NVIDIA announced new software releases for deep learning and HPC developers.  The latest SDK updates include new capabilities and performance optimizations to TensorRT, CUDA toolkit and the new project CUTLASS library. Here’s a detailed look at each of the software updates and the benefits they bring to developers and end users: TensorRT…Powered by Discourse, best viewed with JavaScript enabled"
1403,mapd-massive-throughput-database-queries-with-llvm-on-gpus,"Originally published at:			https://developer.nvidia.com/blog/mapd-massive-throughput-database-queries-llvm-gpus/
Note: this post was co-written by Alex Şuhan and Todd Mostak of MapD. At MapD our goal is to build the world’s fastest big data analytics and visualization platform that enables lag-free interactive exploration of multi-billion row datasets. MapD supports standard SQL queries as well as a visualization API that maps OpenGL primitives onto SQL…Hello, very interesting post. Quick question, as I haven't used MapD before, does all the data need to be somehow stored on the GPUs in GPU memory to get these speed ups or do these timings include the standard costs of transferring data from say harddisk to the computation unitHi Rob,One of the authors here.  We don't include costs of transferring data to GPU memory because our architecture focuses on caching the hot or most recently used data on the GPUs, which can have up to 192GB of memory on an 8 K80 server (and likely much more with future generations of GPUs).  Also since we are a column store we can do significant compression on the data as well as only caching queried columns on the GPU.  Whatever data doesn't fit on GPU can be cached in the likely larger CPU RAM.  So in most of our typical use cases the data is only read once at startup from SSD.  The compressed working set typically fits in GPU RAM and queries are blazing fast.  The result sets are usually quite small so the overheads of moving the results back to the CPU are typically negligible.The numbers in figure 2 are pretty disingenuous. You are comparing >$40k in GPUs against what could be <$2k in CPU hardware.A dual-socket Xeon server with decent clocks and at least 200GB of RAM will be at least 5-6K, and with a RAID array of fast SSDs lets say 8K.  And we can basically run as fast (albeit with half the VRAM) on a server with eight $1K Titan X cards.  The point here is not to be deceptive about price but to show the amazing performance that can be achieved on a single server with GPUs.  To match the performance you'd have to get at least a full rack of CPU servers (running fast software like MapD and not your usual in-memory databases) which has lots of extra costs of its own (rack space, extra power) - plus you'd probably not be able to scale linearly due to the network overheads of being distributed.  So we think that for customers who need to support multiple users running interactive analytics on relatively big datasets, our system with GPUs is the both the most performant and cost effective solution available.How does your current system scale beyond one box?Also, if one were to buy a setup like you mention on AWS:A G2.8xlarge comes contains 16GB vram and will cost almost $1,900 per month.  So 192GB of VRAM you mention will cost almost $23,000 per month!An R3.8xlarge system with 244GB ram and 32 cores only costs $2,000 per month.How then does it make sense to run MapD in that case considering the enormous cost difference?if you buy on AWS there is no cost of rack space or power...Interesting!Recently in the PostgreSQL world, there is some talk of GPU acceleration of queries and JIT compilation for something called schema-binding, the former of which is more-or-less possible with an extension called PG-Strom and an approach for the latter was recently discussed at PGCon 2015.Obviously, there is a long way to go...I disagree about linear scaling.  Scaling linearly is easy if you push down aggregation.  I can run many nodes for the star schema benchmark, for example, and there are only ever 600 rows per server at most sent between  the machines (because they are aggregated already) - it is a shared nothing embarrassingly parallel system.For 5B row table, for example, and 20 nodes, that is 250M rows per node, which can easily fit in ram, and I can aggregate it with all say 64 cores in the box, over all 20 boxes. Those nodes don't have to have much disk, CPU and RAM are fairly cheap.  Not sure how that competes with your solution.It is called Shard-Query and it meets/beats RedShift at 200GB SSB (largest tested)Actually those are baked in.  nothing in life is free.There is also Alenka on github, which is interesting.Hi Justin you have a good point.  For low-cardinality group-by the amount of data sent between nodes should be small and sharding between nodes should work quite well as long as the network communication is efficient. (My experience is that existing distributed dbs often see an appreciable slowdown for network I/O even when the data sizes exchanged should be trivial.) As the cardinality goes up though or you start looking at joins it is more advantageous to be on a single super-node like ours.  Of course we're planning distributed support as well, and for that we'll be bound by the same laws of network physics as everyone else. :)Shard-Query looks nice, how does it compare to something like MemSQL for analytics workloads?Hi,Few points:  Even if there are 1 million aggregate rows in a query, it is still much more efficient than transit of billions of rows over network.  Network physics of 10Gbit ethernet hardly matter at those scales.  I doubt there are group by into the billions.Shard-Query duplicates non-sharded tables on all nodes, so you can use a star or snowflake schema.  It does not support joining tables sharded on different keys together.  Use hadoop or redshift for that, it is not my niche.I do analytics with ICE (infobright community edition) which is a compressing column store with a in-memory metadata system that replaces indexes.  It is quite fast, as I said, competes with RedShift, and Shard-Query is just as petabyte scale as RedShift is.I even got congrats on my benchmark against RedShift from the manager of the RedShift team.Shard-Query also implements window functions for MySQL, which are not supported natively.Softlayer has cloud systems available with NVIDIA K80 GPUs, fwiw.  You can get a machine with two K80's = 24 GB of VRAM for under $2K per month.Not the 192 you guys are talking about but enough to get started perhaps.Actually 2 K80s = 48GB of VRAM.Right on -- and I should know, eh?!One K80 = 24 GB folks...I'm just curious if you have any thoughts about trying FPGAs for acceleration on MapD projectcan I run using nvidia geforce 840m which is based on maxwell architecture?Powered by Discourse, best viewed with JavaScript enabled"
1404,gtc-2020-deep-learning-training-with-cudnn,"GTC 2020 CWE22463
Presenters: ,
Abstract
NVIDIA cuDNN is foundational for deep-learning training on GPUs. Learn about the latest and upcoming features in cuDNN 8. Talk to NVIDIA experts about your use-cases and get your cuDNN questions answered.Watch this session
Join in the conversation below.Powered by Discourse, best viewed with JavaScript enabled"
1405,aaa-ray-tracing-as-a-solo-artist,"Originally published at:			AAA Ray Tracing as a Solo Artist | NVIDIA Technical Blog
How Christian Hecht Single-Handedly Built DXR Spotlight Demo “Attack From Outer Space” in One Month Ok, first off, you have to watch the 1:42 minute video below, which showcases Christian Hecht’s winning contribution to NVIDIA’s DXR Spotlight Contest. It’s called “Attack from Outer Space”, and it’s awesome. We talked to Christian Hecht to better understand…Powered by Discourse, best viewed with JavaScript enabled"
1406,visual-components-connector-for-nvidia-omniverse-a-perfect-recipe-for-manufacturing-digitalization,"Originally published at:			Visual Components Connector for NVIDIA Omniverse: a perfect recipe for manufacturing digitalization
Learn about how the Visual Components NVIDIA Omniverse Connector creates a simulation solution for the manufacturing industry to resolve operational and planning challenges.Powered by Discourse, best viewed with JavaScript enabled"
1407,exploring-the-spacenet-dataset-using-digits,"Originally published at:			https://developer.nvidia.com/blog/exploring-spacenet-dataset-using-digits/
DigitalGlobe, CosmiQ Works and NVIDIA recently announced the launch of the SpaceNet online satellite imagery repository. This public dataset of high-resolution satellite imagery contains a wealth of geospatial information relevant to many downstream use cases such as infrastructure mapping, land usage classification and human geography estimation. The SpaceNet release is unprecedented: it’s the first public…Do you know the date of acquisition of these images? thanks.Question: Do you have to / do you input the building rotation into the training/validation label file? Probably into the last column?Awaiting your future post on this subject...The Kitti format (described here https://github.com/NVIDIA/D... has a field for rotation.  This is not currently used within DetectNet - all bounding boxes produced will have edges parallel to the input image.  It would be straightforward to modify DetectNet to estimate rotation, but the DIGITS interface would not currently support drawing rotated bounding boxes.Hi, in reference to 'We modified the default DetectNet network by changing both network architecture and training parameters'.I've read you used different input image dimensions, random cropping and made some parameters changes (min. coverage value, min. box height...), but I compared DetectNet and SpaceNet and I don't see any difference in the architecture... what are the network architecture modifications? Thanks!Hi there, are there any tools or open source code available to create the signed distance label images from vector format labels such as geojson? ThanksSpaceNet is not a neural network architecture - it is the name of the dataset described in the blog post.  The object detection neural network applied to the dataset is DetectNet without modification except to the parameters you mentionedHere's an example of how to apply a Euclidean distance transform to a bitmap image: http://www.logarithmic.net/...The SpaceNet Challenge Utilituies repo provides a number of utiliity functions for converting the geoJSON files to other formats: https://github.com/SpaceNet...hi,In the article ""We binned the signed distance function values into 128 bins ranging in value from 64 to 192"" - how you do it ? is the 0 - building edge - is set to 128 and th2 neg' values are stretched between 64 and 128 and positive 129 to 192 ?hi, why we need to normalize the value between 64 - 192 ?Hello and thanks for interesting material!As it's known the performance is a crucial issue for such tasks, so what time does it take to get a semantic segmentation for tested image via your version of SharpMask CNN (image resolution and time)?Hi, Thanks for explanation of Building detector method. I have calculated distance map(labels) label image is -64 to 64 based on Yuan's approach and next step is how do we setup and train the images?. Initially, I would like to approach Yuan's approach. Please some one help me in this regard. It would be greatly appreciated.Thank youI am still not clear on how you preprocessed the images to convert from TIFF format to another format. If I understand it correctly, DIGITS only accepts PNG, JPEG, JPG, formats and alike. So the original GeoTIFF files had to be converted to one of the approved formats. Second, the original files are about 650x650 pixels, so to resize them to 1280x1280 you had to transform both the original image and the coordinates in the labels as well. Did you just double the coordinates for the boxes, or was there a more precise method?Please see my responses on the github issues:https://github.com/NVIDIA/D...https://github.com/NVIDIA/D...Hello,""[...]we changed the minimum allowable coverage map value for a bounding box candidate to be 0.06[...]and the minimum number of bounding boxes that must be clustered to produce a final output bounding box to 4""Where should I specify this settings?Powered by Discourse, best viewed with JavaScript enabled"
1408,analyzing-cassandra-data-using-gpus-part-1,"Originally published at:			https://developer.nvidia.com/blog/analyzing-cassandra-data-using-gpus-part-1/
Editor’s Note: Watch the Analysing Cassandra Data using GPUs workshop. Organizations keep much of their high-speed transactional data in fast NoSQL data stores like Apache Cassandra®. Eventually, requirements emerge to obtain analytical insights from this data. Historically, users have leveraged external, massively parallel processing analytics systems like Apache Spark for this purpose. However, today’s analytics…Powered by Discourse, best viewed with JavaScript enabled"
1409,latest-updates-to-nvidia-cuda-x-ai-libraries,"Originally published at:			Latest Updates to NVIDIA CUDA-X AI Libraries | NVIDIA Technical Blog
Learn what’s new in the latest releases of NVIDIA’s CUDA-X AI libraries and NGC. For more information on NVIDIA’s developer tools, join live webinars, training, and Connect with the Experts sessions now through GTC Digital.  NVIDIA Collective Communications Library 2.6 NVIDIA Collective Communications Library (NCCL) implements multi-GPU and multi-node collective communication primitives that are performance…Powered by Discourse, best viewed with JavaScript enabled"
1410,hands-on-access-to-vmware-s-vsphere-on-nvidia-bluefield-dpus-with-nvidia-launchpad,"Originally published at:			https://developer.nvidia.com/blog/hands-on-access-to-vmwares-vsphere-on-nvidia-bluefield-dpus-with-nvidia-launchpad/
vSphere Distributed Services Engine (project monterey) on LaunchPad provides access to dedicated hardware and software through, so IT teams can get hands-on experience prior to deployment.Powered by Discourse, best viewed with JavaScript enabled"
1411,optimizing-access-to-parquet-data-with-fsspec,"Originally published at:			https://developer.nvidia.com/blog/optimizing-access-to-parquet-data-with-fsspec/
This post details how the filesystem specification’s new parquet model provides a format-aware byte-cashing optimization.Dear Nvidia engineers. Please advise whether do you have C++ library that supports the same approach for access parquet file. Thanks.Dear Nvidia engineers. Please advise whether do you have C++ library that supports the same approach for access parquet file. Thanks.Thanks for the question @evgenik !I am not currently aware of any C++ library that implements all of the optimizations discussed in this article. However, it is my understanding that both Arrow and libcudf perform a subset of these approaches. For example, I know that Arrow uses a pre-buffering strategy, and libcudf coalesces multiple file-system accesses when possible. I suppose that the primary difference is that these are all “read-time” optimizations. I don’t believe either of these libraries offer an optimized file-opening utility (yet).Powered by Discourse, best viewed with JavaScript enabled"
1412,physx-sdk-4-0-available-now,"Originally published at:			https://developer.nvidia.com/blog/announcing-physx-sdk-4-0-an-open-source-physics-engine/
The engine has been upgraded to provide industrial grade simulation quality at game simulation performance. In addition, the PhysX SDK has gone open source! It is available under the simple 3-Clause BSD license. With access to the source code, developers can debug, customize and extend the PhysX SDK as they see fit. New features: Temporal…Powered by Discourse, best viewed with JavaScript enabled"
1413,ai-assesses-infrastructure-quality-in-africa,"Originally published at:			AI Assesses Infrastructure Quality in Africa | NVIDIA Technical Blog
Monitoring infrastructure quality in developing regions is a major goal for international aid organizations, unfortunately gathering up to date and reliable data is expensive and a time-consuming process. To alleviate the burden on aid organizations, researchers from Stanford University developed a deep learning-based method that can assess infrastructure quality from satellite imagery. “Basic infrastructure availability…Powered by Discourse, best viewed with JavaScript enabled"
1414,new-on-the-nvidia-ngc-catalog-riva-ai-updates-to-tensorflow-and-pytorch-containers-plus-a-new-hpc-quantum-espresso-container,"Originally published at:			https://developer.nvidia.com/blog/new-on-the-nvidia-ngc-catalog-riva-ai-updates-to-tensorflow-and-pytorch-containers-and-new-hpc-quantum-espresso-container/
With highly performant software containers, pre-trained models, industry specific SDKs and Helm Charts, the content available on the catalog helps you simplify and accelerate your end-to-end workflows.Powered by Discourse, best viewed with JavaScript enabled"
1415,researching-and-developing-an-autonomous-vehicle-lane-following-system,"Originally published at:			https://developer.nvidia.com/blog/researching-and-developing-an-autonomous-vehicle-lane-following-system/
Four years ago, a system known as PilotNet became the first NVIDIA system to steer an autonomous car along a roadway. This system represents a departure from the classical approach for self-driving in which the process is manually decomposed into a series of modules, each performing a different task. In contrast, PilotNet is a single…Powered by Discourse, best viewed with JavaScript enabled"
1416,cybert-neural-network-that-s-the-tech-to-free-your-staff-from-bad-regex,"Originally published at:			cyBERT: Neural network, that’s the tech; To free your staff from, bad regex | NVIDIA Technical Blog
Cybersecurity logs are generated across an organization and cover endpoints (e.g, computers, laptops, servers), network communications, and perimeter devices (e.g., VPN nodes, firewalls). Using a conservative estimate for a company of 1000 employee devices, a small organization can expect to generate over 100 GB/day in log traffic with a peak EPS (Event Per Second) of…Powered by Discourse, best viewed with JavaScript enabled"
1417,confidential-computing-on-nvidia-h100-gpus-for-secure-and-trustworthy-ai,"Originally published at:			https://developer.nvidia.com/blog/confidential-computing-on-h100-gpus-for-secure-and-trustworthy-ai/
Hardware virtualization is an effective way to isolate workloads in virtual machines (VMs) from the physical hardware and from each other. This offers improved security, particularly in a multi-tenant environment. Yet, security risks such as in-band attacks, side-channel attacks, and physical attacks can still happen, compromising the confidentiality, integrity, or availability of your data and…Powered by Discourse, best viewed with JavaScript enabled"
1418,an-openacc-example-part-2,"Originally published at:			https://developer.nvidia.com/blog/openacc-example-part-2/
You may want to read the more recent post Getting Started with OpenACC by Jeff Larkin. In my previous post I added 3 lines of OpenACC directives to a Jacobi iteration code, achieving more than 2x speedup by running it on a GPU. In this post I’ll continue where I left off and demonstrate how we can use OpenACC directives clauses…Is there a way to look at the accelerated code generated by openacc?By default, the PGI compiler uses the LLVM backend. However, you can use the options ‘-ta=tesla:nollvm,keepgpu’ and it will generate CUDA C code instead and save the file. It basically looks like assembly written in C, but it’s something you can inspect.Powered by Discourse, best viewed with JavaScript enabled"
1419,easily-build-your-first-movie-recommender-system,"Originally published at:			Easily Build Your First Movie Recommender System | NVIDIA Technical Blog
Recommender systems are being deployed everywhere to deliver personalized experiences. Siraj Raval, a former software engineer at Meetup and CBS Interactive, recently launched an entertaining yet informative YouTube channel called Sirajology aimed to inspire and equip developers to build the future. His recent tutorial video explains how you can create a recommender system in just…Powered by Discourse, best viewed with JavaScript enabled"
1420,how-to-run-ngc-deep-learning-containers-with-singularity,"Originally published at:			How to Run NGC Deep Learning Containers with Singularity | NVIDIA Technical Blog
New scientific breakthroughs are being made possible by the convergence of HPC and AI. It is now necessary to deploy both HPC and AI workloads on the same system.   The complexity of the software environments needed to support HPC and AI workloads is huge. Application software depends on many interdependent software packages. Just getting a…Could I ask for advice on how to make GPUDirect work with Singularity?You pulled down the tensorflow-19.11-tf1-py3.sif container and then used the pytorch-19.11-tf1-py3.sifPowered by Discourse, best viewed with JavaScript enabled"
1421,meet-the-researcher-peerapon-vateekul-deep-learning-solutions-for-medical-diagnosis-and-nlp,"Originally published at:			https://developer.nvidia.com/blog/meet-the-researcher-peerapon-vateekul-deep-learning-solutions-for-medical-diagnosis-and-nlp/
‘Meet the Researcher’ is a series spotlighting researchers in academia who use NVIDIA technologies to accelerate their work.  This month’s spotlight features Peerapon Vateekul, assistant professor at the Department of Computer Engineering, Faculty of Engineering, Chulalongkorn University (CU), Thailand. Vateekul drives collaboration activities between CU and the NVIDIA AI Technology Center (NVAITC) including seminars and workshops on…Powered by Discourse, best viewed with JavaScript enabled"
1422,edge-computing-is-essential-to-building-smarter-and-safer-spaces,"Originally published at:			https://developer.nvidia.com/blog/edge-computing-is-essential-to-building-smarter-and-safer-spaces/
A new generation of AI applications at the edge is driving incredible operational efficiency and safety gains across a broad range of spaces. Read how the power of AI and edge computing is critical to building smarter and safer spaces.Powered by Discourse, best viewed with JavaScript enabled"
1423,nvidia-broadcast-engine-empowers-developers-with-ai-tools-for-live-video,"Originally published at:			NVIDIA Broadcast Engine Empowers Developers With AI Tools for Live Video | NVIDIA Technical Blog
The world of live streaming has expanded rapidly this year: Twitch has seen an 89% increase in the number of streamers, while viewership is up 56%.  Meanwhile more employees are working from home and collaborating by video conference.  Background noise in the home and messy rooms can be an inconvenience for those hosting video meetings,…Powered by Discourse, best viewed with JavaScript enabled"
1424,upgrade-to-the-newest-versions-of-nvidia-cuda-x-libraries,"Originally published at:			Upgrade to the newest versions of NVIDIA CUDA-X libraries | NVIDIA Technical Blog
Learn what’s new in the latest releases of cuDNN, CUDA, TensorRT, DALI, and Nsight Compute. cuDNN 7.5 The NVIDIA CUDA Deep Neural Network library (cuDNN) is a GPU-accelerated library of primitives for deep neural networks. This version of cuDNN includes: Multi-head attention for accelerating popular models such as TransformerImproved depth-wise separable convolution for training models…Powered by Discourse, best viewed with JavaScript enabled"
1425,designing-robots-with-nvidia-isaac-gems-for-ros,"Originally published at:			https://developer.nvidia.com/blog/designing-robots-with-isaac-gems-for-ros/
NVIDIA and Open Robotics have entered into an agreement to accelerate ROS2 performance. As a result, the NVIDIA Jetson edge AI platform now offers new NVIDIA Isaac GEMs for ROS software.So should i forget about using NVidia Isaac the one on the Kaya robot and just go all in with ROS2 since Nvidia is improving on ROS2?   Whats the use case for using ISAAc SDK if NVidia is enabling ROS2?Im guessing NVidias original plan was to create a competitor to ROS but that is optimized for NVidia GPU.  BUt now it seems Nvidia has switched gears and will be improving ROS and creating gems for ROS since the ISAAC sdk uptake was low?Please keep it real :)ISAAC ROS is NVIDIA’s effort to support the open-source ROS community with GPU acceleration, developers and customers are welcome to use the GEMS in their applications.
If you are using ISAAC SDK and developing on Kaya, please continue to develop using SDK.If I want to implement a Visual SLAM algorithm using a Realsense Camera and GPU acceleration, am I pursuing the correct way in trying to plug in the Isaac Ros GEMS (isaac_common, isaac_image_pipeline, isaac_ros_visual_odometry) into an existing ROS2 installation?As a first shot, I ran the Elbrus example
bob@desktop:~/isaac/sdk$ bazel run packages/visual_slam/apps:elbrus_visual_slam_realsense
from the Isaac SDK but to my great appointment, this example at least didn’t touch the GPU at all but was computed on CPU solely.  Was this a mistake on my side, could I make it use the GPU? Initially I was planning to use the Isaac SDK, possibly alongside with its ROS bridge but since this doesn’t seem to involve the GPU I switched to the idea mentioned in the first paragraph of using the GEMs.I want to avoid rewriting the whole ROS Visual SLAM algorithm by hand to convert it into something palatable for my Nvidia GPU.Edit:
Ubuntu 18.04; ROS Noetic; Isaac SDK
or
Ubuntu 20.04; ROS Foxy, Isaac GEMS only
… in either case Nvidia T1000, Cuda 11.6Edit No. 2:I have integrated the GEMS to my ROS2 installation and once more: only CPU is used … no errors/warnings though … how can I force the GEMS to use GPU instead? Isn’t that the whole purpose of this???Edit3:Anybody know what could be wrong here?Hi @user150672ROS2 middleware is limiting performance.
Improvements are coming in June for a new Isaac ROS release. This new release works with ROS2 Humble to reduce the load on the CPU.
There will continue to be a CPU component to Visual Odometry, so it will never be zero CPU.Meanwhile, I suggest registering for the new GTC22 and scheduling the calendar for our robotics sessions: Developer Conference Session Catalog | NVIDIA GTCHi, is there a version of issac ros visual odometry version that supports ros1?Actually you could use the Isaac SDK which currently to my knowledge supports only ROS1, however it is also limited to Ubuntu 18.04 (if this is not your native OS you can run it inside a container).Thanks a heap for the information, I’m looking forward to trying the new release as soon as it’s out!Powered by Discourse, best viewed with JavaScript enabled"
1426,cuda-11-2-introduces-improved-user-experience-and-application-performance,"Originally published at:			CUDA 11.2 Introduces Improved User Experience and Application Performance | NVIDIA Technical Blog
CUDA Toolkit is a complete, fully-featured software development platform for building GPU-accelerated applications, providing all the components needed to develop apps targeting every NVIDIA GPU platform.  CUDA 11 announced support for the new NVIDIA A100 based on the NVIDIA Ampere architecture, and CUDA 11.1 delivered support for NVIDIA GeForce RTX 30 Series and Quadro RTX…Powered by Discourse, best viewed with JavaScript enabled"
1427,jetson-project-of-the-month-deepway-ai-based-navigation-aid-for-the-visually-impaired,"Originally published at:			Jetson Project of the Month: DeepWay, AI-based navigation aid for the visually impaired | NVIDIA Technical Blog
Satinder Singh won the Jetson Project of the Month for DeepWay, an AI-based navigation assistance system for the visually impaired. The project, which runs on an NVIDIA Jetson Nano Developer Kit, monitors the path of a person and provides guidance to walk on the correct side and avoid any oncoming pedestrians.  In addition to the…Powered by Discourse, best viewed with JavaScript enabled"
1428,nvidia-clara-platform-augmenting-radiology-with-ai,"Originally published at:			NVIDIA Clara Platform: Augmenting Radiology with AI | NVIDIA Technical Blog
NVIDIA’s Clara is an open, scalable computing platform that enables developers to build and deploy medical imaging applications into hybrid (embedded, on-premise, or cloud) computing environments to create intelligent instruments and automated healthcare workflows. The Clara platform aims to bring the technological advances that have been proven in other industries, like gaming, autonomous vehicles, and…Powered by Discourse, best viewed with JavaScript enabled"
1429,in-the-trenches-at-gtc-scaling-applications-to-a-thousand-gpus-and-beyond,"Originally published at:			https://developer.nvidia.com/blog/trenches-gtc-scaling-applications-thousand-gpus-and-beyond/
By Adnan Boz (GTC 2012 Guest Blogger) Question: Why would you need 50 petaflops of horsepower and a 500,000 scalar processor capable supercomputer? Answer: You need to simulate dynamics of complex fluid systems! On Day 3 of GTC, HPC architect and Ogden prize winner Dr. Alan Gray from the University of Edinburgh described his use of C,…Powered by Discourse, best viewed with JavaScript enabled"
1430,maximizing-gromacs-throughput-with-multiple-simulations-per-gpu-using-mps-and-mig,"Originally published at:			https://developer.nvidia.com/blog/maximizing-gromacs-throughput-with-multiple-simulations-per-gpu-using-mps-and-mig/
In this post, we demonstrate the benefits of running multiple simulations per GPU for GROMACS and show how MPS can achieve up to 1.8X overall improvement in throughput.Hello Alan and Szilárd,Thanks for the very useful post. I have tried implementing the MPS on V100s and have seen a massive improvement in the performance.I am facing an issue with using MPS on nodes with multiple GPUs (two GPUs). I would like your help with the same:I am using a job scheduler (qsub) to submit a gromacs simulation run. Each job requests 1xV100+4xCPUs. I run one independent simulation on each CPU core. Therefore I have 4 parallel ongoing runs. The sample command is as follows:The above command runs perfect and I am able to see nvidia-cuda-mps-server and four gmx_mpi processes on GPU 0.However, the issue arises when the job scheduler assigns another job on the same node. Note that nodes have 2 GPUs, therefore it assigns the jobs to the other GPU (GPU 1). (The same resources are requested and the same script as above was run).The messages/errors on using gmx mdrun are as follows:I also notice this in the log file:The CUDA runtime shows N/A. While for the first job, it shows 11.0.Please note the following:I would really appreciate it if you can help resolve my query. Let me know if you need additional details.Akshay,
PhD student,Thanks,I am new to parallelization and cannot understand running the same job multiple times on the same GPU. I want to run multiple jobs on the same GPU and thus cannot comprehend $INPUT file manipulation.
I hope you can help me.Hi Akshay, can you please provide me with your simulation.sh scriptHi Akshay,In your simulation.sh script, are you setting the CUDA_VISIBLE_DEVICES environment variable? If so, please can you try removing that. When you launch each job with 1xV100, I expect that each device will be available as (the default) GPU 0 in each of the jobs (you can check this with nvidia-smi), such that setting CUDA_VISIBLE_DEVICES to any other value would case the error your see. If you still get the error, then I’m not sure of the cause at the moment but can try and reproduce internally.One other thing to try is launching jobs with multiple GPUs in each job, and using CUDA_VISIBLE_DEVICES in a similar way to that shown in the script in the blog.Best regards,AlanHi Ravis,The relevant lines in the first script given in the blog are L45-51, where I create a new directory specific to each simulation, and copy the (same) input file into that directory. The directory naming structure I use is gpu${i}_sim${j}, such that e.g. for 2 simulations on each of 2 GPUs we would have 4 directories, gpu0_sim0, gpu0_sim1, gpu1_sim0 and gpu1_sim1.In your case, of course you will want to use a different input file in each directory. I suggest to set up these directories in advance, each with the appropriate input file(s), and then for each simulation, simply “cd” into the appropriate pre-existing directory to run the simulation (i.e. remove lines 47,48 and 51 but keep lines 46 and 49).Best regards,AlanHello Dr Alan,I appreciate your response to my queries.1)
No, I am not setting the CUDA_VISIBLE_DEVICES environment variable.
(Though I had also tried running mdrun after setting this as detailed in your blog.)The simulation.sh file solely consists of:2)
I have tried launching jobs with multiple GPUs and used the CUDA_VISIBLE_DEVICES variable. This had worked as expected without errors.  The simulations were running on GPU_ID 0 or 1 based on our CUDA_VISIBLE_DEVICES variable used with gmx mdrun.Some observations:I am attaching the tpr file, in case you would like to test them at your end.
md.tpr (6.1 MB)Thank you,
Akshay.Hi Akshay,Thanks for the info. It looks like you just need to set a unique MPS pipe directory for each job, before launching MPS.To do this for your first job (using, e.g. /tmp/mps1 for the directory):export CUDA_MPS_PIPE_DIRECTORY=/tmp/mps1
mkdir -p $CUDA_MPS_PIPE_DIRECTORY
nvidia-cuda-mps-control -dThen the second job on the same node should be able to use its GPU OK, and can also use MPS in a similar way, as long as it uses a different directory (e.g. /tmp/mps2).Best regards,AlanHello Dr Alan,Thank you very much for the suggestion. I will update you in case the problem remains unresolved.Regards,AkshayI have almost reproduced the results that are given in this post for launching multiple simulations on the A100 GPU using MPS and MIG. Now, I am trying to use the Nvidia Nsights profiler to get a deeper understanding of the system state, GPUs, memory copies, kernel execution times etc. However I am not finding any resource or article that could help in creating the profiles. I tried browsing through the documentation of the nsight tool and tried a lot of different methods to create profiles, however nvtx events are not captured along with some other events. Is there a blog/article which explains how this can be done? Thanks in advance for the help!Hi Dr. Alan,I am able to run the script and get an output of 32 .xtc files. I want to concatenate these separate files into one xtc file that covers the entire simulation. I’ve tried this using the gromacs trjcat utility as well as concatenating using VMD, but the xtc files do not appear to be in chronological order where gpu0_sim0 is the first portion of the trajectory and gpu0_sim1 is the next and so on. The position of the protein makes a large jump when moving from the last frame of one xtc file to the first frame of the next file.  Is there a way to stitch all the separate xtc files together so it is one contiguous trajectory?Thank you,
ReubenPowered by Discourse, best viewed with JavaScript enabled"
1431,gtc-2020-automating-dnn-design-for-drive-agx-platform-aware-neural-architecture-search,"GTC 2020 S21666
Presenters: Arash Vahdat,NVIDIA; Le An,NVIDIA
Abstract
In the past few years, wide applications of deep neural networks (DNN) have contributed to significant progress in various fields such as image classification, object detection, and segmentation. Most of the successful DNNs, such as VGG and ResNet, are designed by humans, which requires in-depth domain expertise and effort. While DNNs have become deeper and wider, the need for fast inference is increasing on (edge) computing devices, while accuracy must be maintained. Therefore, developing state-of-the-art neural networks for resource-constrained applications has become challenging. We’ll present our progress on the automated design of neural networks using hardware-aware neural architecture search (NAS) techniques. We show concrete end-to-end examples from differentiable and latency-reflected search of optimal network architectures to their deployment on NVIDIA’s DRIVE AGX platforms using TensorRT for autonomous-driving-related applications.Watch this session
Join in the conversation below.Powered by Discourse, best viewed with JavaScript enabled"
1432,gtc-2020-inter-gpu-communication-with-nccl,"GTC 2020 CWE21698
Presenters: Sylvain Jeaugey,NVIDIA; Sreeram Potluri, NVIDIA; Ke Wen, NVIDIA; Anton Korzh, NVIDIA; Nathan Luehr, NVIDIA
Abstract
NCCL (NVIDIA Collective Communication Library) optimizes inter-GPU communication on PCI, NVIDIA NVLink, and Infiniband, powering large-scale training for most DL frameworks, including Tensorflow, PyTorch, MXNet, and Chainer. Come discuss NCCL’s performance, features, and latest advances.Connect directly with NVIDIA Experts to get answers to all of your questions on GPU programming and code optimization, share your experience, and get guidance on how to achieve maximum performance on NVIDIA’s platform.Watch this session
Join in the conversation below.Powered by Discourse, best viewed with JavaScript enabled"
1433,gtc-graphics-technology-presentations,"Originally published at:			GTC Graphics Technology Presentations | NVIDIA Technical Blog
Starting on October 5, this fall’s GPU Technology Conference (GTC) will run continuously for five days, across seven time zones. The conference will showcase the latest breakthroughs in graphics technology, as well as many other GPU technology interest areas. Explore topics like AI-enabled applications for content creation or learn about the advanced features for virtualized…Powered by Discourse, best viewed with JavaScript enabled"
1434,turing-h-264-video-encoding-speed-and-quality,"Originally published at:			Turing H.264 Video Encoding Speed and Quality | NVIDIA Technical Blog
All NVIDIA GPUs starting with Kepler support fully-accelerated hardware video encoding;  GPUs starting with Fermi support fully-accelerated hardware video decoding. The recently released Turing hardware delivered Tensor Cores and better machine learning performance, but the new GPU also incorporated new multimedia features such as an improved NVENC unit to deliver better compression and image quality…Turing NVENC is very good, we also did tests and see higher quality than libx264 at slow and veryslow presets, bigger difference is for H264 at High profile, where NVENC is better than libx264 by 10-15%!!! Only think sad is that Pascal generation had 2 NVENC engines so performance were two times better :(While this is a good point, you have to also consider that a single Turing NVENC can outperform a single Pascal NVENC in certain applications.  Looking at NVIDIA's initial v9 SDK Tests, the single Turing NVENC H.264 1080p encoding performance is between 22% (High Quality) and 30% (Low Latency) faster than a single Pascal NVENC would be.  However, as you pointed out, in the 4K HEVC test, the single NVENC encoding performance is the same between both Pascal and Turing (and I would assume Volta as well).NVIDIA's customers will have to weigh the features and functions they need before deciding which generation of card to purchase. Taking what you pointed out, those that need more NVENC's and do NOT need HEVC B Frame / Ray Tracing / Tensor support would be better off purchasing one or more refurbished Quadro P5000/P6000's or Tesla P4/P40's (with 2x NVENC's).  And if they don't need 8K HEVC, even refurbished Quadro GP100's or Tesla P100's (with 3x NVENC's) might be a good choice if the price is justified.  For others that want a mix of the newer technologies though, I'd likely recommend at least one Turing-based card, but the others in a system could be Pascal, again, depending on the need for NVENC's.The main problem with speed is when you use HQ preset for HEVC (which is needed for low resolution channels as it will enable 8x8 CU instead of 16x16 CU for Medium), it will give you only 1/4 performance of P5000 on any Turing Card (600 fps vs 150 fps at 1080p).Second problem is that if you wan't only NVENC there is no need to buy anything better than Quadro RTX 2000 (which is not yet released) as all Quadro GPU has same NVENC speed. We liked model when we pay more for P5000 to have 2xNVENC instead of one in P4000.Currently we use Supermicro servers with 4xGPU, but with this new generation we will need 4x times more GPU, yes they could be cheaper (RTX 4000 or RTX 2000 when released), but we will need to change all our servers to something like SuperServer 6049GP-TRT which can handle 20 GPU or have 4x more servers, this will introduce other problems, from our internal tests we find that it is not very stable to use more than 4 GPU in one server.Quality increase was expected as it is now year 2019, but we didn't expect such drop in performance.This will make GPU NVENC solution much more expensive and when AMD will release new Epyc 2 CPUs there will be no difference between GPU and CPU transcoding performance, speed of 1 Turing NVENC HQ preset =< 1 AMD Epyc2 32cores at libx265 Medium preset.These are all valid points.  You would think NVIDIA would consider making cards similar to Teslas (but specialized just for video applications) that offer multiple NVENC's / NVDEC's without all the other features at a lower price point.  NVIDIA really needs to consider your point about the cost of purchasing Epyc 2's versus Quadro RTX 2000's / 4000's.  While it might make sense at low-scale (mobile / desktop), as you said the cost isn't justified for workstations / servers, especially beyond 4 GPUs in a single system.In my case, I use a video switcher application that only supports Intel QuickSync and NVIDIA NVENC/NVDEC.  I'm considering the purchase of one or more refurbished P5000's (for around US$ 1,250 each), and adding a Turing GPU after the Quadro RTX 2000 is released once I have a justified need for the features Turing offers.Great discussion team, which is the best transcoding card I could buy to install in my super micro server for transcoding? Right now am using M6000 and want to upgrade so I can transcode more AVC services in 1080p.Dear Roman, thanks for the interesting results. A couple of questions: 1. Why did you run x264 without lookahead option for ""High quality? Hard to compare quality of encoders when one of them is started with different options 2. For x264 you set -threads 4. But your CPU is ""Dual Intel Xeon E5-2660v3 @ 2.6 GHz"" where CPU has 10 physical cores. I'd say that ""-threads 10"" looks more appropriate here for performance compassion.Hello Vasily,Thank you for the kind talk.>Why did you run x264 without lookahead option for ""High quality?libx264 uses 40 frames lookahead by default in medium preset, so there's no need to specify that.>I'd say that ""-threads 10"" looks more appropriateWe've observed some time ago that for bigger amount of threads, libx264 sometimes produce bitstream with bitrate being lower than it's set from CLI. It's not a big deal for the desktop CPUs, but for, say, 20 threads on a server-grade CPU it really becomes an issue. I've not checked if this is fixed in more recent libx264 releases, however.OK, I see, thanks. I've raised threading question since you should see another FPS with 10-20 threads what impacts the diagram about number of simultaneous streams for x264.Hello, possible to use h264_nvenc with -profile:v baseline -level 3.0 ?Powered by Discourse, best viewed with JavaScript enabled"
1435,nvidia-s-new-ampere-data-center-gpu-in-full-production,"Originally published at:			NVIDIA’s New Ampere Data Center GPU in Full Production | NVIDIA Technical Blog
NVIDIA today introduced the first GPU based on the NVIDIA Ampere architecture, the NVIDIA A100, is in full production and shipping to customers worldwide. The A100 draws on design breakthroughs in the NVIDIA Ampere architecture — offering the company’s largest leap in performance to date within its eight generations of GPUs — to unify AI…Powered by Discourse, best viewed with JavaScript enabled"
1436,nvidia-omniverse-replicator-generates-synthetic-training-data-for-robots,"Originally published at:			https://developer.nvidia.com/blog/generating-synthetic-datasets-isaac-sim-data-replicator/
Synthetic Data generation cockpit for machine learning (ML) engineers.@jwitsoe
I have done quit a few tutorials on TAO (TLT3) and I would like to do one that uses images produced by NVIDIA Omniverse Replicator for Isaac Sim. I like the Forklift example.
Can you point me in the direction of resources on how to use the NVIDIA Omniverse Replicator for Isaac SimThe upcoming release of Isaac Sim will contain all of the new synthetic data features used in the forklift example. Stay tuned for that release which is scheduled to happen soon.I attempted to install Omniverse and the Issac sim but since I only have a GTX 1080TI it doesn’t work.
Do you have any recommendations for a cloud computing service has the RTX 3070 capability’sHave you seen these docs?[https://docs.omniverse.nvidia.com/app_isaacsim/app_isaacsim/setup.html#docker-cloud-deployment]https://docs.omniverse.nvidia.com/app_isaacsim/app_isaacsim/setup.html#cloud-deploymentExactly what I was looking forThanksIs the synthetic data generated by ISSAC Synthetic data generator annotated and in the KITTI format?Will omniverse run on a RTX 4000The minimum specs say RTX 3070. Does that include the Quadro RTX seriesYes, Isaac Sim can generate data in the KITTI format. Read More: https://docs.omniverse.nvidia.com/app_isaacsim/app_isaacsim/sample_syntheticdata.htmlNow that Issac Sim 2021.2.0 is out.
Yeah
Where can I find the tutorial associated to this Developer Blog post?
"" NVIDIA Omniverse Replicator Generates Synthetic Training Data for Robots""Replicator Composer was used to generate the datasets referred to in the blog post. Documentation on this tool can be found here. 5. Replicator Composer — Omniverse Robotics documentationPowered by Discourse, best viewed with JavaScript enabled"
1437,ai-can-detect-open-parking-spaces,"Originally published at:			AI Can Detect Open Parking Spaces | NVIDIA Technical Blog
With as many as 2 billion parking spaces in the United States, finding an open spot in a major city can be complicated. To help city planners and drivers more efficiently manage and find open spaces, MIT researchers developed a deep learning-based system that can automatically detect open spots from a video feed.    “Parking spaces…Powered by Discourse, best viewed with JavaScript enabled"
1438,automated-analysis-of-disaster-damage,"Originally published at:			Automated Analysis of Disaster Damage | NVIDIA Technical Blog
Researchers from Purdue University are using deep learning to dramatically reduce the time it takes for engineers to assess damage to buildings after disasters. Engineers need to quickly document the damage to buildings, bridges and pipelines after a disaster. “These teams of engineers take a lot of photos, perhaps 10,000 images per day, and these…Powered by Discourse, best viewed with JavaScript enabled"
1439,scaling-language-model-training-to-a-trillion-parameters-using-megatron,"Originally published at:			Scaling Language Model Training to a Trillion Parameters Using Megatron | NVIDIA Technical Blog
Natural Language Processing (NLP) has seen rapid progress in recent years as computation at scale has become more available and datasets have become larger. At the same time, recent work has shown large language models to be effective few-shot learners, with high accuracy on many NLP datasets without additional finetuning. As a result, state-of-the-art NLP…Happy to answer questions on the post or the work more broadly! More details are in our arXiv paper: [2104.04473] Efficient Large-Scale Language Model Training on GPU Clusters Using Megatron-LM.Our work is open sourced at GitHub - NVIDIA/Megatron-LM: Ongoing research training transformer models at scale and we would love for people to try it out!Powered by Discourse, best viewed with JavaScript enabled"
1440,airborne-sensors-accurately-monitor-crops-in-real-time,"Originally published at:			https://developer.nvidia.com/blog/airborne-sensors-accurately-monitor-crops-in-real-time/
Researchers use advanced remote sensing and machine-learning algorithms to quickly monitor crop nitrogen levels, central to informing sustainable agriculture.Powered by Discourse, best viewed with JavaScript enabled"
1441,maximize-performance-and-portability-of-hpc-apps-with-hpc-sdk-v21-11,"Originally published at:			Maximize Performance and Portability of HPC Apps with HPC SDK v21.11 | NVIDIA Technical Blog
The latest NVIDIA HPC SDK includes a variety of tools to maximize developer productivity, as well as the performance and portability of HPC applications.Powered by Discourse, best viewed with JavaScript enabled"
1442,nvidia-drive-os-5-2-6-linux-sdk-now-available,"Originally published at:			NVIDIA DRIVE OS 5.2.6 Linux SDK Now Available | NVIDIA Technical Blog
NVIDIA DRIVE OS 5.2.6 Linux SDK is now available on the NVIDIA DRIVE Developer site, providing developers with the latest operating system and development environment purpose-built for autonomous vehicles. As the foundation of the NVIDIA DRIVE SDK, NVIDIA DRIVE OS is designed specifically for accelerated computing and artificial intelligence. It includes NVIDIA CUDA for efficient…Powered by Discourse, best viewed with JavaScript enabled"
1443,upcoming-event-deep-learning-framework-sessions-at-gtc-2022,"Originally published at:			AI Developer Conference Session Catalog | GTC 2022 | NVIDIA
Join us for these featured sessions to learn about optimizing PyTorch models, accelerating graph neural networks, improving GPU performance with automated code generation, and more.Powered by Discourse, best viewed with JavaScript enabled"
1444,nvidia-research-flip-a-difference-evaluator-for-alternating-images,"Originally published at:			NVIDIA Research: FLIP: A Difference Evaluator for Alternating Images | NVIDIA Technical Blog
NVIDIA introduces FLIP: A Difference Evaluator for Alternating Images, published as an HPG 2020 research paper. Image quality measures are becoming increasingly important in the field of computer graphics. For example, there is currently a major focus on generating photorealistic images in real time by combining path tracing with denoising, for which such quality assessment…Powered by Discourse, best viewed with JavaScript enabled"
1445,generating-ray-traced-caustic-effects-in-unreal-engine-4-part-2,"Originally published at:			https://developer.nvidia.com/blog/generating-ray-traced-caustic-effects-in-unreal-engine-4-part-2/
In the first post on ray-traced caustic effects, we introduced mesh caustics and its usages in Unreal Engine 4. In this second post, we describe water caustics. The beta version of the source code and sample assets have been released in the UE4 NVRTX_Caustics repository. For more information, see the Release Information section at the…Is there a sample map available with an example of the water caustics?Hello, here’s the sample map of the swimming pool mentioned in the blog : WaterCausticsDemo_4.25 - Google Drive .Hi! The caustics effects (especially the Mesh caustics) are fantastic, do you have any timeframe on when we could have a 4.26 update on those? I tried to make a rework of the engine code myself to combine 4.26+dlss and caustics-4.25.3 but so much seems to have changed that had to quit after a few days of struggle. Thanks already!Hi, we are working on the 4.26.1 update and try to complete the update in a few weeks.Thanks for posting the link to the demo project!  When I tried to open it, I got an error saying ""This project requires the “GraphicsCardInfo” plugin.  I synced the branch named “NvRTX_Caustics” from your git repository and built that, is that the correct branch?This is excellent news, thank you already!Sorry for the delay, NvRTX_Caustics is the correct branch so far, we are going to upload a new branch soon. We will check the code and give you feedback ASAPHi, here’s the caustics branch based on 4.26.1, https://github.com/NvRTX/UnrealEngine/commits/NvRTX_Caustics-4.26, please have a try.Please try the new caustics branch based on 4.26.1, https://github.com/NvRTX/UnrealEngine/tree/NvRTX_Caustics-4.26, the issue should be solved.Thank you so much, compiled already and got my test scene working! This is exciting, currently trying to understand the DiaphragmDOF shadermagic a bit better to push the prism effects even further.Can’t wait to see 4.26.2 rolling out :D because of metahuman :D
and I really should say your work is masterpiece … keep it going guysThank you! We are upgrading it to 4.26.2 and will release it ASAP :)Hello and thank you for sharing this very exciting work!I’ve read everything I could find, but have been struggling for a week to build this.  All of the commands under Building and running end with similar errors. I documented build errors from 4.25 here.  Also with both windows and linux I’m receiving this make error when building the NvRTX_Caustics-4.26 branch. Windows VS showed 5 build errors in total.  Searching the forums I don’t see anyone else with this issue, and this isn’t my first rodeo.  I’ve built many branches of UE, and other programs all the way down to custom Linux kernels for diskless nodes that boot from NFS drives.What could be the fix?[2/613] Compile Module.Engine.35_of_48.cpp
In file included from /mnt/2.26/Engine/Intermediate/Build/Linux/B4D820EA/UE4Editor/Development/Engine/Module.Engine.11_of_48.cpp:11:
/mnt/2.26/Engine/Source/Runtime/Engine/Private/Components/LightComponent.cpp:321:4: error: field ‘bTiledDeferredLightingSupported’ will be initialized after field ‘bAffectWaterCaustics’ [-Werror,-Wreorder-ctor]
, bTiledDeferredLightingSupported(false)
^Ubunbtu Linux Kernel 5.4.0-66-generic
NVIDIA Driver Version: 460.73.01
CUDA Version: 11.2, and an RTX 3090Thanks for any insight :)Hello, sorry for the delay. I think you get this error because you are turning on the “Treat warnings as errors” option in your compiler. To fix this issue, you could turn off the “Treat warnings as errors” option or relocate the variable initializing position in the code. Could you move the WaterCausticsPrecision,  NumWaterCausticsMapCascades & WaterCausticsMapCascadeScale ahead of the IESTexture and move the bAffectWaterCaustics ahead of bCastModulatedShadows ? This should fix the compiling warnings. And we will submit a change to our branch to fix it later, thank you for pointing out this problem !Here comes the NVRTX 4.26.2 and now I’m assuming an update for this branch will come soon. so excited :DHi there again:D
Sorry for asking too much but I just wanted to know what your plan is for uneral engine support?
do you guys have any plan for UE5  or not? cause I’m just thinking about Lumen with caustics and its blowing my mind.
please inform me if it is going to be in your plan or not thanks a lotCurrently we don’t have a clear plan for UE5. But I think we will continue our support for caustics.That is the most relaxing “not so clear plan” I’ve ever heard :D
thanks a lot and I hope you guys the best for letting the magic continue its wizardry.Huge thanks to developers at Nvidia, this ray trace looks really nice, hope UE5 will keep ray trace as an option, if not, just keep using UE4 is still not a bad choose.Powered by Discourse, best viewed with JavaScript enabled"
1446,cvpr-2020-ai-inferencing-at-the-speed-of-light,"CVPR 2020 dcv05
Presenters: Tech Demo Team, NVIDIA
Abstract
NVIDIA A100 GPUs deliver significantly faster performance for HPC, AI, and data analytics workloads.Watch this session
Join in the conversation below.Powered by Discourse, best viewed with JavaScript enabled"
1447,cudacasts-episode-4-single-gpu-debugging-with-cuda-5-5,"Originally published at:			https://developer.nvidia.com/blog/cudacasts-episode-4-single-gpu-debugging-cuda-5-5/
Even if you’ve already watched CUDACasts episode 3 on creating your first OpenACC program, you’ll want to go watch the new version which includes a clearer, animated introduction. So check it out! In the next few CUDACasts we’ll be exploring some of the new features available in CUDA 5.5, which is available as a release candidate now…Any chance single GPU debugging is this going to be  supported for 3.0 devices ?Powered by Discourse, best viewed with JavaScript enabled"
1448,this-ai-can-automatically-remove-the-background-from-a-photo,"Originally published at:			https://developer.nvidia.com/blog/this-ai-can-automatically-remove-the-background-from-a-photo/
Researchers from MIT’s Computer Science and Artificial Intelligence Lab (CSAIL), ETH Zurich, and Adobe recently introduced a new deep learning-based tool that can automatically extract objects or people in the foreground from the background. The solution offers an alternative to manually selecting an object in a photo and attempting to remove it the old-fashioned way.…Powered by Discourse, best viewed with JavaScript enabled"
1449,edge-computing-considerations-for-security-architects,"Originally published at:			https://developer.nvidia.com/blog/edge-computing-considerations-for-security-architects/
Learn about considerations organizations must take to secure AI models at the edge when incorporating edge computing into their strategy.I really enjoyed the article. I do have a few questions. Background - I have over 20 years experience starting with protecting mobile devices that are in the wild to securing infrastructure in the cloud. I am working with the Nano, TX, NX and have an AG on back order. I work with TAO and DeepStream. The article shared a great deal of information on goals and partnerships. I understand the need to encrypt, zero trust, trusted boot,leveraging Microsoft’s IOT service,etc.
What I did not see are references to how to address each of the security topics and map them to NVidia devices. I would appreciate another article on explicitly mapping the security area of concerns to NVidia edge devices.
I will go even further. I will implement recommended Nvidia edge device security controls and write a post on the implementation - the pros and the cons. I would need to see a list of the recommended security controls  from NVidia that can be applied to NVidia edge devices.
Hope this helps.Powered by Discourse, best viewed with JavaScript enabled"
1450,klm-royal-dutch-airlines-using-ai-to-boost-customer-service,"Originally published at:			KLM Royal Dutch Airlines Using AI to Boost Customer Service | NVIDIA Technical Blog
With the increasing volume of interactions with customers over social media channels, KLM Royal Dutch Airline is the first airline to test how artificial intelligence could assist customer service agents. “We have 100,000 mentions a week on social media,” says Tjalling Smit, senior vice president of Digital at KLM Royal Dutch Airlines. “We handle around…Powered by Discourse, best viewed with JavaScript enabled"
1451,explainer-what-is-direct-and-indirect-lighting,"Originally published at:			What Is Direct and Indirect Lighting? | NVIDIA Blog
In computer graphics, the right balance between direct and indirect lighting elevates the photorealism of a scene.Powered by Discourse, best viewed with JavaScript enabled"
1452,ai-research-holds-the-key-to-affordable-and-accessible-drug-development,"Originally published at:			https://developer.nvidia.com/blog/ai-research-holds-the-key-to-affordable-and-accessible-drug-development/
Published in Nature Machine Intelligence, a panel of experts shares a vision for the future of biopharma featuring collaboration between ML and drug discovery powered by GPUs.Powered by Discourse, best viewed with JavaScript enabled"
1453,ai-helps-doctors-diagnose-the-coronavirus,"Originally published at:			AI Helps Doctors Diagnose the Coronavirus | NVIDIA Technical Blog
At the epicenter of the Coronavirus in Wuhan China, a team of physicians in China are using GPU-accelerated AI software to detect visual signs of the coronavirus (Covid 19).  Physicians there say the AI-based software, which relies on NVIDIA GPUs for both training and inference, has helped overworked staff screen patients and prioritize those likely…Powered by Discourse, best viewed with JavaScript enabled"
1454,fast-spectral-graph-partitioning-on-gpus,"Originally published at:			https://developer.nvidia.com/blog/fast-spectral-graph-partitioning-gpus/
Graphs are mathematical structures used to model many types of relationships and processes in physical, biological, social and information systems. They are also used in the solution of various high-performance computing and data analytics problems. The computational requirements of large-scale graph processing for cyberanalytics, genomics, social network analysis and other fields demand powerful and efficient computing performance that only accelerators…When will this be available?Likely first half of 2017.Thank youPowered by Discourse, best viewed with JavaScript enabled"
1455,expedia-ranking-hotel-images-with-deep-learning,"Originally published at:			Expedia Ranking Hotel Images with Deep Learning | NVIDIA Technical Blog
Nuno Castro, the Director of Data Science at Expedia gave a talk at PyData London 2017 on how they’re using GPUs and deep learning to rank hotel images. Castro explains that humans first glance at the image within a hotel listing before considering the price or hotel name and this is why it’s crucial to…Powered by Discourse, best viewed with JavaScript enabled"
1456,occupancy-flow-4d-reconstruction-by-learning-particle-dynamics,"Originally published at:			Occupancy Flow: 4D Reconstruction by Learning Particle Dynamics | NVIDIA Technical Blog
The story below is a guest post from researchers at the Max Planck Institute for Intelligent Systems. In this article, the researchers describe a new method for dense 4D reconstruction from images or sparse point clouds. By Michael Niemeyer An intelligent agent that interacts and navigates in our world has to be able to reason in 3D. Therefore,…Powered by Discourse, best viewed with JavaScript enabled"
1457,dgx-2-server-virtualization-leverages-nvswitch-for-faster-gpu-enabled-virtual-machines,"Originally published at:			DGX-2 Server Virtualization Leverages NVSwitch for Faster GPU Enabled Virtual Machines | NVIDIA Technical Blog
NVIDIA Kernel-based Virtual Machine (KVM) takes open source KVM and enhances it to support the unique capabilities of the NVIDIA DGX-2 server, creating a full virtualization solution for NVIDIA GPUs and NVIDIA NVSwitch devices with PCI passthrough. The DGX-2 server incorporates 16 NVIDIA Tesla V100 32GB GPUs, 12 NVSwitch chips, two 24 core Xeon CPUs,…Hi, does NVIDIA Kernel-based Virtual Machine (KVM) supports DGX-1 with 8 GPU NVlink without NVswitch?Powered by Discourse, best viewed with JavaScript enabled"
1458,upcoming-event-healthcare-life-sciences-developer-summit-november-10-2022,"Originally published at:			EMEA HCLS Dev Summit
A virtual event designed for healthcare developers and startups, this summit on November 10, 2022 offers a full day of technical talks to reach developers and technical leaders in the EMEA region. Get best practices and insights for applications, from biopharma to medical imaging.Powered by Discourse, best viewed with JavaScript enabled"
1459,covariant-unveils-ai-powered-warehouse-robots,"Originally published at:			https://developer.nvidia.com/blog/covariant-unveils-ai-powered-warehouse-robots/
Pieter Abbeel’s new robotics startup Covariant this week deployed their AI-equipped robot at customer facilities in North America and Europe in the apparel, pharmaceutical, and electronics industries.  The company is backed by some of the biggest names in AI, including Geoffrey Hinton, Jeff Dean, Yann LeCun, Fei-Fei Li, and many others.   Their GPU-accelerated platform consists of…Powered by Discourse, best viewed with JavaScript enabled"
1460,net-cloud-computing-with-alea-gpu,"Originally published at:			https://developer.nvidia.com/blog/net-cloud-computing-with-alea-gpu/
Cloud computing is all about making resources available on demand, and its availability, flexibility, and lower cost has helped it take commercial computing by storm. At the Microsoft Build 2015 conference in San Francisco Microsoft revealed that its AzureC cloud computing platform is averaging over 90 thousand new customers per month; contains more than 1.4…Powered by Discourse, best viewed with JavaScript enabled"
1461,isc20-featured-demo-running-multiple-workloads-on-a-single-a100-gpu,"Originally published at:			ISC20 Featured Demo: Running Multiple Workloads on a Single A100 GPU | NVIDIA Technical Blog
The NVIDIA A100 Tensor Core GPU includes a groundbreaking feature called Multi-Instance GPU (MIG), which partitions the GPU into as many as seven instances, each with dedicated compute, memory, and bandwidth.  This allows multiple users to run their workloads on the same GPU, maximizing per-GPU utilization and user productivity.  This demo runs an HPC simulation,…Powered by Discourse, best viewed with JavaScript enabled"
1462,dynamic-graphs,"Have you come across any scenarios where users need their graph to change dynamically? What are those scenarios? For example, the user has a large existing graph and modifications in the form of edge insertions or deletions are streamed in?A number of customers have been discussing the notion of a dynamic graph.  There are two main thrusts we have seen:
•        A fixed graph with time attributes where we run algorithms that filter data based on different time windows
•        Graphs where the graph is actually being updated over time.  That is, we want to create a graph, run some algorithms against that graph and then add/update/delete edges from the graph before running more algorithms.We have been looking at options for implementing these.Thank you : ).Can you please say something more about the second bullet? What are the applications and/or what are the algorithms being run?We have customers in the Financial sector and in the Cyber Security sector that have been interested in dynamic graphs.  Most of these are for running GNN workflows… do some inferencing, append some new financial or cyber transactions and then do more inferencing.Powered by Discourse, best viewed with JavaScript enabled"
1463,an-introduction-to-edge-computing-common-questions-and-resources-for-success,"Originally published at:			https://developer.nvidia.com/blog/an-introduction-to-edge-computing-common-questions-and-resources-for-success/
During a recent webinar, participants outlined common edge computing questions and challenges. This post provides NVIDIA resources to help beginners on their journey.Powered by Discourse, best viewed with JavaScript enabled"
1464,building-an-automatic-speech-recognition-model-for-the-kinyarwanda-language,"Originally published at:			Building an Automatic Speech Recognition Model for the Kinyarwanda Language | NVIDIA Technical Blog
Learn how an ASR model was trained on the Kinyarwanda language dataset that achieved state-of-the-art performance.Powered by Discourse, best viewed with JavaScript enabled"
1465,develop-ray-traced-games-using-unreal-engine-4,"Originally published at:			https://developer.nvidia.com/blog/develop-ray-traced-games-using-unreal-engine-4/
Today we are releasing the NVRTX Example Project, which provides some practical guidance in the world of ray tracing, in addition to showing you how NVIDIA technologies can enhance the result.  This scene demonstrates ray traced lighting that is fully dynamic and can handle indoor and outdoor scenes running on our NVRTX branch of Unreal…Powered by Discourse, best viewed with JavaScript enabled"
1466,ai-assists-doctors-monitor-icu-patients,"Originally published at:			https://developer.nvidia.com/blog/ai-helps-doctors-monitor-icu-patients/
Researchers at the University of Florida developed a deep learning system that analyzes movement and expressions, as well as environmental factors, to better serve hospital patients in the intensive care unit.  This study is the first that uses an autonomous system for patient monitoring in the ICU. The system can detect elements such as faces,…Powered by Discourse, best viewed with JavaScript enabled"
1467,rover-trained-on-gpus-wins-750k-at-nasa-s-autonomous-robotics-challenge,"Originally published at:			Rover Trained on GPUs Wins $750k at NASA’s Autonomous Robotics Challenge | NVIDIA Technical Blog
The team from West Virginia University took home the largest prize awarded in the five-year long NASA Sample Return Robot Challenge. This challenge began in 2012 with more than 50 teams and to qualify for the final level, the team’s autonomous robot had to return a single sample in 30 minutes. Using CUDA, and a…Powered by Discourse, best viewed with JavaScript enabled"
1468,nagoya-university-to-install-new-15-petaflop-gpu-accelerated-supercomputer,"Originally published at:			Nagoya University to Install New 15 petaFLOP GPU-Accelerated Supercomputer | NVIDIA Technical Blog
To advance research and development, Nagoya University just announced plans to build a new 15 petaFLOP GPU-accelerated supercomputer. The system will be used to develop new weather modeling systems, new drugs, advance conventional simulation of numerical computations, apply AI to healthcare, and develop AI-powered autonomous machines. “The digitization of university education and research activities has…Powered by Discourse, best viewed with JavaScript enabled"
1469,latest-nvidia-graphics-research-advances-generative-ai-s-next-frontier,"Originally published at:			Latest NVIDIA Graphics Research Advances Generative AI’s Next Frontier | NVIDIA Blog
NVIDIA will present around 20 research papers at SIGGRAPH, the year’s most important computer graphics conference.Powered by Discourse, best viewed with JavaScript enabled"
1470,denoising-and-filtering-part-v-of-ray-tracing-gems,"Originally published at:			Denoising and Filtering: Part V of Ray Tracing Gems | NVIDIA Technical Blog
In this installment of Ray Tracing Gems, we take a look at the process of denoising and filtering scenes. Real-time ray tracing always begins with noisy imagery; the solutions described in this text will get your scenes as close to ground truth as possible.  “Ray Tracing Gems Part V” can be downloaded at NVIDIA Developer…Powered by Discourse, best viewed with JavaScript enabled"
1471,scalable-gpu-accelerated-supercomputer-in-the-microsoft-azure-cloud,"Originally published at:			Scalable GPU-Accelerated Supercomputer in the Microsoft Azure Cloud | NVIDIA Technical Blog
At Supercomputing 2019 in Denver, Colorado, NVIDIA announced the availability of a new kind of GPU-accelerated supercomputer in the cloud on Microsoft Azure. Built to handle the most demanding AI and high performance computing applications, the largest deployments of Azure’s new NDv2 instance rank among the world’s fastest supercomputers, offering up to 800 NVIDIA V100…Powered by Discourse, best viewed with JavaScript enabled"
1472,sc20-demo-maximizing-performance-for-distributed-machine-learning-and-deep-learning-with-sharp,"Originally published at:			https://developer.nvidia.com/blog/sc20-demo-maximizing-performance-for-distributed-machine-learning-and-deep-learning-with-sharp/
Today’s modern-day machine learning data centers require complex computations and fast, efficient data delivery. The NVIDIA Mellanox Scalable Hierarchical Aggregation and Reduction Protocol (SHARP) takes advantage of the in-network computing capabilities in the NVIDIA Mellanox Quantum switch, dramatically improving the performance of distributed machine learning workloads. SHARP technology improves upon the performance of MPI and…Powered by Discourse, best viewed with JavaScript enabled"
1473,accelerating-ai-modules-for-ros-and-ros-2-on-nvidia-jetson-platform,"Originally published at:			https://developer.nvidia.com/blog/accelerating-ai-modules-for-ros-and-ros-2-on-jetson/
NVIDIA Jetson developer kits serve as a go-to platform for roboticists because of its ease of use, system support, and its comprehensive support for accelerating AI workloads. In this post, we showcase our support for open-source robotics frameworks including ROS and ROS 2 on NVIDIA Jetson developer kits. Figure 1. ROS and ROS 2 with…Hello! I would love to know, what is this robot on the left of the lead image (the biggest one, with 4 wheels and LiDAR on high support)? Is it kinda unique Carter-like robot, or is there a possibility to acquire such robot / construct from specs?Hello, yes we are building one of those Robot using Xavier NX.  In fact we have built tons of Robotics-AI stuffs based on Jetson Nano and continue to post tutorial videos for enthu community.  Take a look at our channel https://youtube.com/user/anbukumar73Our Website - https://www.codingscientist.comI think it is a carter. You may follow the building requirements shown here: https://docs.nvidia.com/isaac/isaac/doc/tutorials/carter_hardware.htmlThis looks great. Feel free to put it on Jetson Projects - NVIDIA Developer Forums if you have not already.Hello everyone, I was wondering if there is a chance to extend this project with custom gestures that I would like to add. I am not really familiar with ROS, but I would like to use ros2_trt_pose_hand to control robot in my master thesis. The idea is to use ML to teach detection of some extra gestures that will be used to control some arm movements.Thanks in advance for help :)Powered by Discourse, best viewed with JavaScript enabled"
1474,cuda-7-release-candidate-feature-overview-c-11-new-libraries-and-more,"Originally published at:			https://developer.nvidia.com/blog/cuda-7-release-candidate-feature-overview/
It’s almost time for the next major release of the CUDA Toolkit, so I’m excited to tell you about the CUDA 7 Release Candidate, now available to all CUDA Registered Developers. The CUDA Toolkit version 7 expands the capabilities and improves the performance of the Tesla Accelerated Computing Platform and of accelerated computing on NVIDIA…do you support any c++14 features yet? like auto-deduced return types?I've already used some c++11 features in device code before CUDA 7, (CUDA 6.5 with VS2010 just worked), does that only mean that it wasn't officially supported until CUDA 7? Can I pass lambdas to global function now?You can use lambda in device code (as a functor or otherwise) as long as its definition is in device code. You can't (yet) pass a lambda from host code to device code (i.e. as a kernel argument).I'm curious which C++11 features you were able to use in device code in the past?  There was an undocumented option (--std=c++11) in CUDA 6.5, but not before that. But mvcc does use the EDG C++ front end so it's possible some features that require front-end compilation only may have worked if they were supported by the version of EDG used.In any case, CUDA 7 is the first version with official support. Note that not everything in C++11 is supported on the device at this stage. It's mostly language features, not standard library features, like std::thread or STL. We plan to provide more detailed information in a future blog post.All the features you mentioned except range-based for loop & variadic templates, etc which aren't supported by the host compiler (VC10), all work fine in CUDA 6.5 with VS2010, no special compiler flags needed.Mark: Happy days! Looks like I can use libc++ on OS X. This means that I no longer have to maintain other dependencies because of libstdc++ dependency!CUDA 7 does not officially support C++14 features, and from my quick tests, features like auto-deduced return types, generic lambdas, etc., are not working yet.are the 3D FFT improvements only on the K20 or other GPUs as well?This is because MSVC enables C++11 support without any flags / options specified. But CUDA 6.5 does not officially support C++11The cuFFT improvements are not limited to K20 (I fixed the confusing wording in the post).  Also, they are not limited to 3D FFTs!  I've added a graph showing speedups for 1D FFTs.Once again: that's a fantastic feature set in this release! We are really looking into the constexpr support on the device side (and to throw out a huge amount of self-written auto, lambda features).One unrelated quick question: was the support for the PGI compiler on the host-side added (#439486 -> #1449951)?Yes, the PGI C++ compiler is supported as a host compiler for nvcc in CUDA 7.0 on Linux.Great! Do you know which pgi version(s)? I could not find anything in the header files nor the announcements.cuSOLVER is great news for the signal processing community ! Is it possible to stream cuSOLVER functions in order to use them in batch mode (to compute many medium size matrices) ? It would be interesting to compare cuSOLVER with the batched solver sample code available in the registered dev website.Does someone happen to know whether this new release of cuFFT does support callbacks (cufftXTSetCallback etc.) on Windows?No, cufftXTSetCallback is not supported on Windows in this release.Yes, cuSOLVER supports CUDA streams.  Also, cuSolver contains some batched operations: batched sparse QR and batched refactorization. The cuSOLVER PDF documentation included with the CUDA Toolkit v7 RC download provides full details.Well, it's a pity. Nevertheless, thanks a lot for your reply!Hi,apparently I am too blind and cannot find the setting in CUDA NSight 7.0 which enables c++11 standard. Can you please help me and tell me where to enable this option? I am using NSight to compile/link the project.Will CUDA 7 support 32-bit Windows?The release notes document is rather unclear (says that CUDA Toolkit wold be 64-nit only)...Powered by Discourse, best viewed with JavaScript enabled"
1475,nvidia-deepens-commitment-to-streamlining-recommender-workflows-with-gtc-spring-sessions,"Originally published at:			https://developer.nvidia.com/blog/nvidia-deepens-commitment-to-streamlining-recommender-workflows-with-gtc-spring-sessions/
Here a few key sessions from industry leaders in media, delivery-on-demand, and retail at GTC Spring 2021.Powered by Discourse, best viewed with JavaScript enabled"
1476,accelerated-solution-of-sparse-linear-systems,"Originally published at:			https://developer.nvidia.com/blog/accelerated-solution-sparse-linear-systems/
Fresh from the NVIDIA Numeric Libraries Team, a white paper illustrating the use of the CUSPARSE and CUBLAS libraries to achieve a 2x speedup of incomplete-LU- and Cholesky-preconditioned iterative methods. The paper focuses on the Bi-Conjugate Gradient and stabilized Conjugate Gradient iterative methods that can be used to solve large sparse non-symmetric and symmetric positive definite linear…Hi.. Mark Harris Can you suggest me which Ebook to start myself with the concept of graphics and to a Never ending graphics engineer Or designer  and help my family whose striving to eat Pls help me..Powered by Discourse, best viewed with JavaScript enabled"
1477,upcoming-event-join-nvidia-at-microsoft-inspire-2022,"Originally published at:			https://developer.nvidia.com/blog/upcoming-event-join-nvidia-at-microsoft-inspire-2022/
Powered by Discourse, best viewed with JavaScript enabled"
1478,how-to-implement-performance-metrics-in-cuda-fortran,"Originally published at:			https://developer.nvidia.com/blog/how-implement-performance-metrics-cuda-fortran/
CUDA Fortran for Scientists and Engineers shows how high-performance application developers can leverage the power of GPUs using Fortran. In the first post of this series we looked at the basic elements of CUDA Fortran by examining a CUDA Fortran implementation of SAXPY. In this second post we discuss how to analyze the performance of…Hi, in the first code block `<span="""" class=""hiddenSpellError"" pre="""">` is probably a typo?I've fixed this.  Thanks!The 'next post' link in the last paragraph is broken. It links back to the current pageThanks! Fixed.Powered by Discourse, best viewed with JavaScript enabled"
1479,addressing-cybersecurity-in-the-enterprise-with-ai,"Originally published at:			Addressing Cybersecurity in the Enterprise with AI | NVIDIA Technical Blog
Traditional cybersecurity approaches are falling short. See how AI can help you manage risks, identify vulnerabilities, and stop breaches in your enterprise.Powered by Discourse, best viewed with JavaScript enabled"
1480,enhancing-sample-efficiency-in-reinforcement-learning-with-nonparametric-methods,"Originally published at:			Enhancing Sample Efficiency in Reinforcement Learning with Nonparametric Methods | NVIDIA Technical Blog
Recent developments in artificial intelligence and autonomous learning have shown impressive results in tasks like board games and computer games. However, the applicability of learning techniques remains mainly limited to simulated environments. One of the major causes of this inapplicability to real-world scenarios is the general sample-inefficiency and inability to guarantee the safe operation of…For further information read the full paper, check out the code, or simply comment here below.Powered by Discourse, best viewed with JavaScript enabled"
1481,building-cloud-native-ai-powered-avatars-with-nvidia-omniverse-ace,"Originally published at:			Building Cloud-Native, AI-Powered Avatars with NVIDIA Omniverse ACE | NVIDIA Technical Blog
Explore the AI technology that powers Violet, the cloud-native interactive avatar showcased at GTC, along with new details about NVIDIA Omniverse ACE and NVIDIA Tokkio.Got a question about NVIDIA Omniverse ACE? Let us know below!Hi. How can I start to use NVIDIA Omniverse ACE?Thanks for your interest in NVIDIA Omniverse ACE! To learn more about the platform and apply for early access, visit: https://developer.nvidia.com/omniverse-platform/aceYou can also explore related NVIDIA AI technology including Omniverse Audio2Face(Omniverse Audio2Face AI Powered Application | NVIDIA) and Riva TTS(https://developer.nvidia.com/riva). These SDKs and tools are well-suited for avatar applications that don’t need to be optimized for the cloud and are a great way to start getting familiar with the technology.Hi thank you for the answer. I have submitted form to get the ACE early access. Status show that my application is being reviewed. I guess I need to wait until it get approved then after that I will able to try the platform.Hi! Can I deploy Violet showcase or maybe some other showcase, based on Omniverse ACE or Tokkio, to my environment, e.g. AWS?Powered by Discourse, best viewed with JavaScript enabled"
1482,gtc-2020-software-based-compression-for-analytical-workloads,"GTC 2020 S21597
Presenters: Nikolay Sakharnykh,NVIDIA; Rene Mueller, NVIDIA
Abstract
Real-world analytical pipelines have very large memory requirements and stress the CPU-GPU and GPU-GPU interconnects. The GPU memory size is limited, and the data is often offloaded to CPU memory for further processing on the GPU later. That can present a significant bottleneck for the end-to-end pipeline. Fast compression and decompression can improve performance by reducing the amount of data to be sent over the interconnect, or even completely eliminate the need to offload data by storing it in GPU memory in compressed format and performing subsequent operations on the compressed data. We’ll survey various parallel compression algorithms, from LZ-based to run-length encoding, dictionary, and bit-packing. We’ll discuss efficient GPU implementations and integration in data-science frameworks such as RAPIDS, and also highlight compression mechanisms in hardware. Our best approaches can achieve a 50x compression ratio and maintain 50 GB per second compression/decompression speed on Tesla T4.Watch this session
Join in the conversation below.Powered by Discourse, best viewed with JavaScript enabled"
1483,top-edge-ai-sessions-at-gtc-2022,"Originally published at:			Conversational AI & NLP Conference Sessions | GTC 2022 | NVIDIA
Join us September 19-22 for a deep dive into the latest advances in edge AI, from reimagined shopping experiences to industrial automation.Powered by Discourse, best viewed with JavaScript enabled"
1484,nvidia-announces-cuquantum-beta-availability-record-quantum-benchmark-and-quantum-container,"Originally published at:			NVIDIA Announces cuQuantum Beta Availability, Record Quantum Benchmark, and Quantum Container | NVIDIA Technical Blog
Learn about the new NVIDIA cuQuantum beta release, benchmark record, and forthcoming containerized software for a DGX Quantum Appliance.Powered by Discourse, best viewed with JavaScript enabled"
1485,advanced-api-performance-barriers,"Originally published at:			https://developer.nvidia.com/blog/advanced-api-performance-barriers/
This post tells you about tips and caveats for barriers usage for the performance when you’re developing with DX12 or Vulkan.If you have any questions or comments, let us knowPowered by Discourse, best viewed with JavaScript enabled"
1486,nvidia-deepstream-sdk-4-0-now-available,"Originally published at:			NVIDIA DeepStream SDK 4.0 Now Available | NVIDIA Technical Blog
There’s a tremendous opportunity to bring efficiency in our cities, in retail operations, manufacturing lines, shipping and routing in warehouses. The groundwork has already been laid out with billions of sensors and cameras installed worldwide, that are rich sources of data.  Yet the ability to extract insights from this information has been challenging, and today’s…Powered by Discourse, best viewed with JavaScript enabled"
1487,how-to-build-a-gpu-accelerated-research-cluster,"Originally published at:			https://developer.nvidia.com/blog/how-build-gpu-accelerated-research-cluster/
Some of the fastest computers in the world are cluster computers. A cluster is a computer system comprising two or more computers (“nodes”) connected with a high-speed network. Cluster computers can achieve higher availability, reliability, and scalability than is possible with an individual computer. With the increasing adoption of GPUs in high performance computing (HPC), NVIDIA GPUs…hi, myself medha, Ph.D studentWorking in area of publish subscribe distributed system . I am interested in building GPU accelerated research cluster for my research in the area of design of high performance pub/sub using MPI and CUDA. Can u give specification of infrastructure like node or GPU for purchase. Also I wanted to discuss with u my research area  .can u help?thanks for ur valuable post.Thanks for your interest in building a research cluster. The basic inputs about choosing the Nodes (Workstation or server) and GPUs are given in point 1 of my blog above. You can choose either to buy any standard OEM machine or assemble any machine which fulfills the specs given. Please let me know if you have any specific questions about choosing the hardware, I will be happy to answer. Please let me know about your research area and points of discussion, I would be happy to discuss more on that.thanks for the reply and the interest shown. I am working in the area of publish subscribe system where publishers publishes the work and subscriber subscribes the things of his interest. EXample is stock trading, where subscriber can subscribe to any stock when some conditions satisfies. Matching of subscriptions with publications is called matching algorithm . I am trying to  port this pub/sub system on HPC platform, I want to perform hybrid parallelism by using MPI and CUDA . My idea is one node will do the task of clustering and send the subscriptions according to clusters formed to individual work stations. Every work node will have cuda card. Matching will be done by GPGPU. As the publications arrived , the node who does the clustering will approximately choose the node where subscriptions can be found. If this cluster is formed then I can check about latency  bandwidth , MPI communications bandwidth etc, Now my questions are:-No one has done the porting of pub/sub system on MPI and CUDA.yet. I haven't found any IEEE paper on it. can I go with this idea of forming research cluster and deploying pub/sub system on that? or my concept is itself wrong?I am pursuing Ph.D and my work is to  make pub/sub system parallel and scalable by using HPC.I have implemented CUDA content matching algorithm and results are promising. Now I want to make it distributed with combination of MPI and cuda.Also I want to test this system on hadoop and storm which is event processing system. and then conclude about which architecture is suitable for pub/sub systemPls guide me regarding this. Thanks for everything.medhaPlease drop an email to CUDA-Technology-IN@nvidia.com, we can discuss in detail on that about your research work.Will this cluster provide any acceleration for molecular dynamics (or docking) software (Amber, schrodinger, MOE etc)If your application is getting better performance with GPUs and also scales well across nodes, cluster can help you in getting a good accelerationI wanna build a low cost GPU +CPU cluster , im very much confused in selecting the right board . can any help?Hi, Hung from Hong Kong.Teacher in a middle school.I find the link has been removed. Can you tell from where I can watch your video record and slide for your talk?Thanks.E-mailschrodingeriap@yahoo.com.hkPlease see recording - search at http://on-demand-gtc.gputec...Search GTC, 2013 and with Title, it will take you a page that will show this talk and will have recording link.Slides are at http://on-demand.gputechcon...Get it.Thanks for your kindness.Chun Hung------------------------------2015年8月7日週五中國標準時間上午8:49 Disqus 的來信﹕Can I use GeForce GTX card instead of TeslaSir,     I Karishma Bansole.I am doing Mtech.My dessertation work in Parallel computing.I need to establish MPI-GPU Cluster.Uptill now I have made rock cluster of one node.Now I want to add Cuda roll on rock cluster How should I do? And I dont have infiband.So I would like to know how to established MPI-GPU cluster without infiband?.Or Infiniband is needed for making the MPI-GPU ClusterCould you please email to CUDA-Technology-IN@nvidia.com about your requirements. We will get back to you.Multiple GPU's on a single node (Ex: 4-in-one/8-in-one) OR one/two GPU per node. What is the trade-off? Where does it actually make a difference?Hi Pradeep,Any thoughts on how to install a computing cluster for Matlab distributed computing?Powered by Discourse, best viewed with JavaScript enabled"
1488,gtc-2020-named-tensors-model-quantization-and-the-latest-pytorch-features,"GTC 2020 S22145
Presenters: Joseph Spisak,Facebook; James Reed,Facebook AI
Abstract
PyTorch, the popular open-source ML framework, has continued to evolve rapidly since the introduction of PyTorch 1.0, which brought an accelerated workflow from research to production. We’ll deep dive on some of the most important new advances, including the ability to name tensors, support for quantization-aware training and post-training quantization, improved distributed training on GPUs, and streamlined mobile deployment. We’ll also cover new developer tools and domain-specific frameworks including Captum for model interpretability, Detectron2 for computer vision, and speech extensions for Fairseq.Watch this session
Join in the conversation below.Powered by Discourse, best viewed with JavaScript enabled"
1489,transforming-ipsec-deployments-with-nvidia-doca-2-0,"Originally published at:			https://developer.nvidia.com/blog/transforming-ipsec-deployments-with-nvidia-doca-2-0/
Announced in March 2023, NVIDIA DOCA 2.0, the newest release of the NVIDIA SDK for BlueField DPUs, is now available. Together, NVIDIA DOCA and BlueField DPUs accelerate the development of applications that deliver breakthrough networking, security, and storage performance with a comprehensive, open development platform. NVIDIA DOCA 2.0 includes newly added support for the BlueField-3…Powered by Discourse, best viewed with JavaScript enabled"
1490,get-improved-real-time-ray-tracing-results-with-unreal-engine-4-23-available-now,"Originally published at:			Get Improved Real-Time Ray Tracing Results with Unreal Engine 4.23, Available Now | NVIDIA Technical Blog
Last April, Unreal Engine added beta support for ray tracing and path tracing, letting developers get the most out of DirectX 12 and DirectX Raytracing (DXR). Since then, we’ve seen teams of all sizes achieve incredible results, adding dynamic global illumination, pixel perfect reflections, and physically accurate refractions to their applications.  Unreal Engine’s real-time ray…Powered by Discourse, best viewed with JavaScript enabled"
1491,enhanced-image-analysis-with-multidimensional-image-processing,"Originally published at:			Enhanced Image Analysis with Multidimensional Image Processing | NVIDIA Technical Blog
Many times two dimensions are insufficient for analyzing image data. cuCIM is an open-source, accelerated, computer vision and image-processing software library for multidimensional images.Yeah- sometimes you need more than two (2).  What else helps you in multidimensional image i/o and n-dimensional image processing?Powered by Discourse, best viewed with JavaScript enabled"
1492,deep-learning-super-sampling-v2-in-unreal-engine-5,"NVIDIA Deep Learning Super Sampling (DLSS) is a plug-in that uses deep learning algorithms to upscale or “super sample” an image, and helps during GPU heavy workloads like ray tracing. NVIDIA DLSS takes a lower resolution image and increases its resolution.For a visual walkthrough on how to install and implement DLSS view the video below.This is part of a series of Unreal 5 related introductory videos first listed in our technical blog on the developer pages. https://developer.nvidia.com/blog/shaping-the-future-of-graphics-with-nvidia-technologies-in-unreal-engine-5/Powered by Discourse, best viewed with JavaScript enabled"
1493,cudacasts-episode-18-cuda-6-0-unified-memory,"Originally published at:			https://developer.nvidia.com/blog/cudacasts-episode-18-cuda-6-0-unified-memory/
CUDA 6 introduces Unified Memory, which dramatically simplifies memory management for GPU computing. Now you can focus on writing parallel kernels when porting code to the GPU, and memory management becomes an optimization. The CUDA 6 Release Candidate is now publicly available. In today’s CUDACast, I will show you some simple examples showing how easy…Nice!Hi Mark, I read somewhere that Maxwell GPUs can directly access system main memory. But I couldn't find how this access is performed or any benchmarking about it. Do you know any document about it?Fermi, Kepler, and Maxwell GPUs can all access host memory directly via what is known as ""Zero Copy"". Zero copy basically maps a host pointer into the device address space and then the device accesses the memory over PCI-e. This is different from Unified Memory, which is available on Kepler and later GPUs.  Zero copy performance is always limited to PCI-e throughput speeds. There is a bit of discussion in my post on Unified Memory. http://devblogs.nvidia.com/...  You may also want to look at the ""Simple Zero-Copy"" sample included with the CUDA Toolkit package, and the documentation of page-locked host memory and mapped memory here: https://docs.nvidia.com/cud...Powered by Discourse, best viewed with JavaScript enabled"
1494,gtc-2020-vmd-on-arm,"GTC 2020 D2S22
Presenters: Tech Demo Team,NVIDIA
Abstract
Interactive All-Atom Molecular Visualization - Analyzing Large Biomolecular Systems with GPUs and ArmWatch this session
Join in the conversation below.Powered by Discourse, best viewed with JavaScript enabled"
1495,adobe-adds-shazam-for-fonts-tool-to-photoshop,"Originally published at:			Adobe Adds ‘Shazam for Fonts’ Tool to Photoshop | NVIDIA Technical Blog
Designers have thousands of fonts to choose from, but what do you when you see the perfect font on a building or on a flyer you found on the street? Photoshop is using artificial intelligence to help designers identify typefaces that they have seen elsewhere. Revealed last year by Adobe researchers, DeepFont uses NVIDIA GPUs…Powered by Discourse, best viewed with JavaScript enabled"
1496,ai-detects-seizures-with-superb-accuracy,"Originally published at:			AI Detects Seizures with Superb Accuracy | NVIDIA Technical Blog
Researchers from the University of British Columbia and Microsoft developed a deep learning algorithm to automatically detect epileptic seizures in realistic situations with 90% accuracy. Epilepsy is a chronic neurological disorder of the brain that affects approximately 70 million people of all ages worldwide, making it the second most common neurological disease after migraines. Detecting…Powered by Discourse, best viewed with JavaScript enabled"
1497,sc20-demo-cinematic-climate-visualization-with-omniverse,"Originally published at:			https://developer.nvidia.com/blog/sc20-demo-cinematic-climate-visualization-with-omniverse/
Climate simulations are among the most challenging problems in scientific computing due to their size and complexity. They produce large amounts of inherently 3D data, and yet the analysis is often limited to 2D projections.  Visualization is an incredibly useful tool that allows us to find patterns and trends and extract meaningful insights from massive…Powered by Discourse, best viewed with JavaScript enabled"
1498,caffe2-portable-high-performance-deep-learning-framework-from-facebook,"Originally published at:			https://developer.nvidia.com/blog/caffe2-deep-learning-framework-facebook/
Yesterday Facebook launched Caffe2, an open-source deep learning framework made with expression, speed, and modularity in mind. It is a major redesign of Caffe: it inherits a lot of Caffe’s design while addressing the bottlenecks observed in the use and deployment of Caffe over the years. As a result, Caffe2 opens the gate for algorithm experimentation…Caffe2 https://www.facebook.com/gr...Great work! Will Caffe2 be compatible with Caffe projects, i.e. How easy to transfer the previous projects based on Caffe to the new framework?On the first graph ""Scaling Out with Multi-GPU machines"" you used log-space for x axis and normal space for y axis. It completely contradicts to the content of graph. If you want to pull the wool over our eyes, you can go deeper: for example use loglog-space for x axis and square-space for y axis. Classical marketing, If i can say so. Shame on you.Great, yet another CNN library...My apologies. This is oversight from our side - will definitely fix.I would respectfully decline the ""classical marketing"" comment though - Caffe is the first deep learning library to release *every single bit of the implementation detail* of a full CNN training as early as 2013, for free, full BSD. We intend to keep this spirit fully.Well, not another, but more of a brainchild and a continued contribution of Caffe :)Yup, if you would like to translate a model from Caffe to Caffe2 please try it out , and please do let us know if it has bugs.Specifically, the script that allows you to convert a Caffe model to Caffe2 is here: https://github.com/caffe2/c...The graph has been updated in the post to use a log scale for the y-axis.Here’s a slimmed down sorted extract of the top three results from that 1000-long tensor. The results were sorted by the probability of a match, 0.98222 (98%) being the highest. Good jobs!Noob question: how does this compare to Tensorflow?On Windows I ran into a minor issue with the model downloader. The command ""python -m caffe2.python.models.download -i squeezenet"" fails because it uses ""os.symlink"" which is problematic on windows. To complete the install, just open an admin prompt and create the symlink by hand similar to this:mklink <your-build-root>\caffe2\python\models\squeezenet\__init__.py <your-build-root>\caffe2\python\models\__sym_init__.pyWill it be integrated in a near future with DIGITS ?Great article.Can you please provide benchmarks with ImageNet training used on a standard PC with 1,2,3 and 4 NVIDIA GTX 1080 Ti GPUs (PCIE 3.0 x16) and the ResNet-50 neural network architecture?Update : I found a useful graph in an article on DGX - 1, where performance and scalability are presented for NVLink and PCIe.https://devblogs.nvidia.com...With respect to the scaling graph ? Was it up to 64 GPUs on a single machine ? How was the MEMORY IO dealt with ? Can I get more details on how exactly the benchmark was generated ?Powered by Discourse, best viewed with JavaScript enabled"
1499,experience-the-latest-breakthroughs-in-game-development-with-nvidia-at-gdc,"Originally published at:			https://developer.nvidia.com/blog/experience-the-latest-breakthroughs-in-game-development-with-nvidia-at-gdc/
The Game Developer Conference (GDC) is here, and NVIDIA will be showcasing how our latest technologies are driving the future of  game development and graphics. Check out our list of sessions now.Powered by Discourse, best viewed with JavaScript enabled"
1500,why-there-is-no-ideal-data-center-network-design,"Originally published at:			https://developer.nvidia.com/blog/why-there-is-no-ideal-data-center-network-design/
Is there an ideal network design that always works? Let’s find out. This blog covers the pros and cons of pure layer 3, layer 2 only, and VXLAN and EVPN.Powered by Discourse, best viewed with JavaScript enabled"
1501,rapids-accelerator-for-apache-spark-release-v21-10,"Originally published at:			https://developer.nvidia.com/blog/rapids-accelerator-for-apache-spark-release-v21-10/
This post details the latest functionality of RAPIDS Accelerator for Apache Spark.Powered by Discourse, best viewed with JavaScript enabled"
1502,ai-algorithm-quickly-diagnoses-heart-failure,"Originally published at:			https://developer.nvidia.com/blog/ai-algorithm-quickly-diagnoses-heart-failure/
The research creates a deep learning model that accurately predicts whether a patient is experiencing heart failure by detecting subtle changes in EKGs, often undetectable to the human eye.Powered by Discourse, best viewed with JavaScript enabled"
1503,creating-custom-ai-models-using-nvidia-tao-toolkit-with-azure-machine-learning,"Originally published at:			https://developer.nvidia.com/blog/creating-custom-ai-models-using-nvidia-tao-toolkit-with-azure-machine-learning/
Learn how to accelerate your vision AI model development using NVIDIA TAO Toolkit and deploy it for inference with NVIDIA Triton Inference Server—all on the Azure platform.Powered by Discourse, best viewed with JavaScript enabled"
1504,new-reference-applications-for-edge-ai-developers-on-holohub-with-nvidia-holoscan-v0-5,"Originally published at:			https://developer.nvidia.com/blog/new-reference-applications-for-edge-ai-developers-on-holohub-with-nvidia-holoscan-v0-5/
Edge AI applications, whether in airports, cars, military operations, or hospitals, rely on high-powered sensor streaming applications that enable real-time processing and decision-making. With its latest v0.5 release, the NVIDIA Holoscan SDK is ushering in a new wave of sensor-processing capabilities for the next generation of AI applications at the edge. This release also coincides…Powered by Discourse, best viewed with JavaScript enabled"
1505,state-of-the-art-real-time-multi-object-trackers-with-nvidia-deepstream-sdk-6-2,"Originally published at:			https://developer.nvidia.com/blog/state-of-the-art-real-time-multi-object-trackers-with-nvidia-deepstream-sdk-6-2/
When you observe something over a period of time, you can find trends or patterns that enable predictions. With predictions, you can, for example, proactively alert yourself to take appropriate action. More specifically, when you observe moving objects, the trajectory is one of the most important ways to understand the target object behavior, through which…Very impressive work! Likeee!Very cool, I am assuming this could work very well with player tracking for an AI camera recording a sporting event for example?If you have any info related to motion tracking and sports please share.ThanksLooks very impressive! Is it possible to run that vehicle tracking example from the article with the NvDCF tracker on Jetson Nano in real time? And what maximum input resolution is it able to process?Powered by Discourse, best viewed with JavaScript enabled"
1506,accelerating-innovation-in-multi-messenger-astrophysics-with-gpu-accelerated-computing,"Originally published at:			Accelerating Innovation in Multi-Messenger Astrophysics with GPU-Accelerated Computing | NVIDIA Technical Blog
To help accelerate multi-messenger (MMA) astrophysics with deep learning, dozens of researchers from multiple communities including HPC, AI, physics, data analytics, and astronomy have written a new paper published in Nature Reviews Physics that looks into the best techniques for bringing AI-based processing to multi-messenger astrophysics.  MMA astrophysics is an interdisciplinary field that combines data…Powered by Discourse, best viewed with JavaScript enabled"
1507,interview-with-the-founder-of-advanced-image-recognition-startup-clarifai,"Originally published at:			Interview with the Founder of Advanced Image Recognition Startup, Clarifai | NVIDIA Technical Blog
Matthew Zeiler is the founder and CEO of Clarifai, which uses machine learning and deep neural networks to develop the world’s advanced image recognition system. Zeiler received his Ph.D. in machine learning and image recognition, and his research produced the top 5 results in the 2013 ImageNet classification competition. In a recent interview with BigData-MadeSimple.com…Powered by Discourse, best viewed with JavaScript enabled"
1508,optimizing-system-latency-with-nvidia-reflex-sdk-available-now,"Originally published at:			Optimizing System Latency with NVIDIA Reflex SDK - Available Now | NVIDIA Technical Blog
Measuring and optimizing system latency is one of the hardest challenges during game development and the NVIDIA Reflex SDK helps developers solve that issue. NVIDIA Reflex is an easy to integrate SDK that provides API to both measure and reduce system latency – giving players a more responsive experience.  Epic, Bungie, Respawn, Activision Blizzard, and…Powered by Discourse, best viewed with JavaScript enabled"
1509,gtc-digital-crafting-a-real-time-path-tracer-for-minecraft-rtx,"Originally published at:			GTC Digital: Crafting a Real-Time Path-Tracer for Minecraft RTX | NVIDIA Technical Blog
At GTC Digital, NVIDIA engineers, in collaboration with Microsoft, revealed how they crafted a real-time path tracer in Minecraft with NVIDIA RTX technology. The talk gives a high level overview of the implementation, as well as more details about the denoising techniques used in the project.  “We can see that the algorithm is conceptually very…Powered by Discourse, best viewed with JavaScript enabled"
1510,nvidias-top-5-ai-stories-of-the-week-4-22,"Originally published at:			NVIDIA’s Top 5 AI Stories of the Week: 4/22 | NVIDIA Technical Blog
Every week we highlight NVIDIA’s Top 5 AI stories of the week. In this week’s edition we cover a new deep learning-based algorithm from OpenAI that can automatically generate new music. Plus, an automatic speech recognition model that could improve Alexa’s algorithm by 15%. Watch below: 5 – AI Model Can Recommend the Optimal Workout…Powered by Discourse, best viewed with JavaScript enabled"
1511,photomath-app-uses-a-tesla-gpu-to-solve-math-problems,"Originally published at:			PhotoMath App Uses a Tesla GPU to Solve Math Problems | NVIDIA Technical Blog
The smart calculator reads and solves mathematical problems by using the camera of your mobile device in real-time – even providing the solving steps. As students are heading back to school, the PhotoMath app was the #1 iPhone Free US downloads in early September – and now has been downloaded over 11 million times. Parents…Powered by Discourse, best viewed with JavaScript enabled"
1512,upcoming-webinar-low-code-ai-model-development-with-the-nvidia-tao-toolkit,"Originally published at:			https://developer.nvidia.com/blog/upcoming-webinar-low-code-ai-model-development-with-the-nvidia-tao-toolkit/
Powered by Discourse, best viewed with JavaScript enabled"
1513,ai-composer-creates-music-for-films-and-games,"Originally published at:			https://developer.nvidia.com/blog/ai-composer-creates-music-for-films-and-games/
AIVA Technologies, one of the leading startups in the field of AI music composition, developed a deep learning-based system that is the world’s first non-human to officially acquire the worldwide status of Composer. AIVA is registered under the France and Luxembourg authors’ right society (SACEM), where all of its works reside with a copyright to…Powered by Discourse, best viewed with JavaScript enabled"
1514,considerations-for-deploying-ai-at-the-edge,"Originally published at:			https://developer.nvidia.com/blog/considerations-for-deploying-ai-at-the-edge/
There are a number of factors businesses should consider to ensure an optimized edge computing strategy and deployment.Powered by Discourse, best viewed with JavaScript enabled"
1515,run-state-of-the-art-nlp-workloads-at-scale-with-rapids-huggingface-and-dask,"Originally published at:			https://developer.nvidia.com/blog/run-state-of-the-art-nlp-workloads-at-scale-with-rapids-huggingface-and-dask/
This post was originally published on the RAPIDS AI Blog. TLDR: Learn how to use RAPIDS, HuggingFace, and Dask for high-performance NLP. See how to build end-to-end NLP pipelines in a fast and scalable way on GPUs. This covers feature engineering, deep learning inference, and post-inference processing. Introduction Modern natural language processing (NLP) mixes modeling,…I think link to the workflow and code is brokenThanks for flagging! You can check out the workflow code on the rapidsai/gpu-bdb GitHub repo.This topic was automatically closed 14 days after the last reply. New replies are no longer allowed.Powered by Discourse, best viewed with JavaScript enabled"
1516,get-the-best-performance-for-your-neural-networks-with-tensorrt,"Originally published at:			https://developer.nvidia.com/blog/get-the-best-performance-for-your-neural-networks-with-tensorrt/
NVIDIA TensorRT is a high-performance deep learning inference library for production environments. Power efficiency and speed of response are two key metrics for deployed deep learning applications, because they directly affect the user experience and the cost of the service provided. Tensor RT automatically optimizes trained neural networks for run-time performance, delivering up to 16x higher energy…Powered by Discourse, best viewed with JavaScript enabled"
1517,gtc-2020-fastspeech-and-its-acceleration-of-training-and-inference-on-gpu,"GTC 2020 S21420
Presenters: Dabi Ahn,NVIDIA
Abstract
We’ll focus on the concept of FastSpeech, and how it can be accelerated during inference. FastSpeech is a state-of-the-art text-to-speech model developed by Microsoft Research Asia and accepted in Neurips 2019. It achieved much faster inference speed than Tacotron2. Fast inference is one of the most important requirements in industry because all kinds of conversational AI, including AI speaker, requires low latency in a production setting. You’ll need basic knowledge of deep learning and TTS.Watch this session
Join in the conversation below.Powered by Discourse, best viewed with JavaScript enabled"
1518,nvidia-leading-the-ray-tracing-discussion-at-siggraph-2019,"Originally published at:			NVIDIA Leading the Ray Tracing Discussion at SIGGRAPH 2019 | NVIDIA Technical Blog
SIGGRAPH provides developers and researchers an opportunity to share information on cutting-edge technologies, including next-generation graphical rendering. NVIDIA is leading the discussion on ray-tracing, offering a wide range of sessions, listed below. Topics include real-time path tracing and denoising, dynamic diffuse global illumination, RTX-accelerated ray tracing with Optix, and Ray Tracing at 240Hz. NVIDIA is…Powered by Discourse, best viewed with JavaScript enabled"
1519,developer-blog-developing-robotics-applications-in-python-with-nvidia-isaac-sdk,"Originally published at:			https://developer.nvidia.com/blog/developer-blog-developing-robotics-applications-in-python-with-nvidia-isaac-sdk/
The modular and easy-to-use perception stack of NVIDIA Isaac SDK continues to accelerate the development of various mobile robots. Isaac SDK 2020.1 introduces the Python API, making it easier to build robotic applications for those who are familiar with Python. In this blog, published on the NVIDIA Developer Blog, we explore this feature and share a step-by-step…Powered by Discourse, best viewed with JavaScript enabled"
1520,gtc-2020-running-multi-messenger-astrophysics-with-icecube-across-all-available-gpus-in-the-cloud,"GTC 2020 S22206
Presenters: Igor Sfiligoi ,UC San Diego - San Diego Supercomputer Center; Benedikt Riedel,University of Wisconsin-Madison and Wisconsin IceCube Particle Astrophysics Center
Abstract
We’ll report on a computational experiment that marshaled all globally available for-sale NVIDIA GPUs across AWS, Azure, and GCP. The net result was a peak of about 51,000 GPUs of eight different kinds, with an aggregate peak of about 380 PFLOPS fp32.
The experiment used simulations for the IceCube Neutrino Observatory, an array of some 5,000 optical sensors buried deep within a cubic kilometer of ice at the South Pole. The sensors detect the signatures of shock waves created by particles from neutrino interactions passing through the ancient ice sheets. Simulation is needed to properly account for natural ice imperfections, and photon propagation codes are a natural fit for GPGPU computing. We’ll provide both a summary overview and technical details of the infrastructure needed to create a supercomputer-like environment across multiple cloud providers, as well as an overview of the science behind IceCube and how GPU compute helps in advancing the scientific goals.Watch this session
Join in the conversation below.Powered by Discourse, best viewed with JavaScript enabled"
1521,nvidia-nsight-tools-video-showcase-for-ampere-gpus,"Originally published at:			https://developer.nvidia.com/blog/nvidia-nsight-tools-video-showcase-for-ampere-gpus/
NVIDIA’s RTX 30 series GPUs deliver the world’s fastest graphics, powered by 3rd generation Tensor Cores and 2nd generation RT Cores. Nsight Tools enable developers to get the most out of this powerful new hardware, visualizing the state of the GPU and making clear what steps can be taken to achieve peak performance. This showcase…Powered by Discourse, best viewed with JavaScript enabled"
1522,detecting-objects-in-point-clouds-using-ros-2-and-tao-pointpillars,"Originally published at:			https://developer.nvidia.com/blog/detecting-objects-in-point-clouds-using-ros-2-and-tao-pointpillars/
Use this ROS 2 node for object detection from lidar in 3D scenes, an important task for robotic navigation and collision avoidance.Hello!I’m trying to use this ROS 2 node but I’m having some issues building the package. Is there any place where I can get any help or documentation to solve those problems?Thank you.Hi @francisco.cruz1,You can find ROS documentation here: ROS & ROS2 Bridge — Omniverse Robotics documentationFor posting questions, please visit the Isaac ROS forum here: Isaac ROS - NVIDIA Developer ForumsPowered by Discourse, best viewed with JavaScript enabled"
1523,diagnosing-network-issues-faster-with-nvidia-wjh,"Originally published at:			https://developer.nvidia.com/blog/diagnosing-network-issues-faster-with-wjh/
AI has seamlessly integrated into our lives and changed us in ways we couldn’t even imagine just a few years ago. In the past, the perception of AI was something futuristic and complex. Only giant corporations used AI on their supercomputers with HPC technologies to forecast weather and make breakthrough discoveries in healthcare and science.…Powered by Discourse, best viewed with JavaScript enabled"
1524,jetson-project-of-the-month-detecting-acute-lymphoblastic-leukemia-with-nvidia-jetson,"Originally published at:			https://developer.nvidia.com/blog/detecting-leukemia-jetson-nano/
NVIDIA Jetson Nano is paving the way to detect certain types of cancer sooner.Powered by Discourse, best viewed with JavaScript enabled"
1525,bmw-brings-together-art-artificial-intelligence-for-virtual-installation-using-nvidia-stylegan,"Originally published at:			https://developer.nvidia.com/blog/bmw-brings-together-art-artificial-intelligence-for-virtual-installation-using-nvidia-stylegan/
BMW today unveiled a virtual art installation that projects AI-generated artwork onto a virtual rendition of the automaker’s 8 Series Gran Coupe.  Dubbed “The Ultimate AI Masterpiece,” the installation harnessed NVIDIA StyleGAN — a generative model for high-resolution images — to create original artwork projection-mapped onto the virtual vehicle. The project debuts in conjunction with…Powered by Discourse, best viewed with JavaScript enabled"
1526,coffee-break-series-ray-tracing-in-games-with-nvidia-rtx,"Originally published at:			Coffee Break Series: Ray Tracing in Games with NVIDIA RTX | NVIDIA Technical Blog
Ray tracing will soon revolutionize the way video games look. Ray tracing simulates how rays of light hit and bounce off of objects, enabling developers to create stunning imagery that lives up to the word “photorealistic”. Ignacio Llamas and Edward Liu from NVIDIA’s real-time rendering software team will introduce you to real-time ray tracing in…Glad you liked the content, though I'm not sure what you're seeing in terms of resolution. These were actually posted as 720p.Will they be able to use this with overdraw or rendering partially transparent objects with increased effeciency? I want to render tons of grass planes with little performacne loss because of overdraw.I've been a programmer and developer for 35 years .. mostly business software. Ray tracing technology has convinced me to try to jump onboard. Anyone willing to give me an internship? I am fascinated by the potential and I would like to get involved and learn about the tech and perhaps help improve ray tracing toolkits.Hey guys I tried to download the MDL SDK and got some kind of weird problem, either the mime type is wrong for the download or perhaps the file extension is missing. I would be able to tell you but windows appears to be giving me an automatic (default) extension and due to the name I am paranoid about renaming the extension in an attempt to determine the true/proper extension name .. [exe/zip/tar] .. my downloaded file is named etcmdl-sdk-307800.2890.solitairetheme8URL: https://developer.nvidia.co...Powered by Discourse, best viewed with JavaScript enabled"
1527,accelerating-digital-pathology-pipelines-with-nvidia-clara-deploy,"Originally published at:			https://developer.nvidia.com/blog/accelerating-digital-pathology-pipelines-with-nvidia-clara-deploy/
As an undergraduate student excited about AI for healthcare applications, I was thrilled to be joining the NVIDIA Clara Deploy team for an internship. It was the perfect combination: the opportunity to work at a leading technology company enabling the acceleration and adoption of AI while contributing to a team building the future (and the…I greatly enjoyed working on this project at NVIDIA! I would love to hear any thoughts or feedback. Feel free to also reach out to chat more about AI in healthcare, digital pathology, or the future of health!Hi,Thank you for introducing the very useful topic.
I’m so interested in GPU-accelerated stain normalization.
When will it be available in MONAI or Clara?Thank you,
DaisukeHi @kdais-prm ,Neha’s work is integrated into MONAI by the following pull requests.Stain normalization by drbeh · Pull Request #2666 · Project-MONAI/MONAI (github.com)And APIs are available atApplications — MONAI 0.6.0 DocumentationBased on the MONAI roadmap (Project MONAI), I expect those methods would be available in the next version of MONAI (0.7.0 in Sep), and/or the next version of Clara Train that is based on MONAI (TBD).The availability of the stain normalization in cuCIM is not determined but we expect those methods would be available by the December release of cuCIM (v21.12.00, RAPIDS Maintainer Docs - RAPIDS Docs).Thank you!Hi @gigony ,Thank you very much!Powered by Discourse, best viewed with JavaScript enabled"
1528,new-real-time-smartnic-technology-5t-for-5g-optimizes-5g-access-and-edge-networks,"Originally published at:			New Real-Time SmartNIC Technology 5T-for-5G Optimizes 5G Access and Edge Networks | NVIDIA Technical Blog
NVIDIA announced a new technology embedded in its NVIDIA Mellanox ConnectX-6 Dx SmartNIC and BlueField-2 I/O Processing Unit to optimize 5G networks.Powered by Discourse, best viewed with JavaScript enabled"
1529,introduction-to-gpu-accelerated-python-for-financial-services,"Originally published at:			Introduction to GPU Accelerated Python for Financial Services | NVIDIA Technical Blog
By Yi Dong, Alex Volkov, Miguel Martinez, Christian Hundt, Alex Qi, and Patrick Hogan – Solution Architects at NVIDIA.  Quantitative finance is commonly defined as the use of mathematical models and large datasets to analyze financial markets and securities. This field requires massive computational effort to extract knowledge from raw data. Many scientific toolkits are available for processing…Powered by Discourse, best viewed with JavaScript enabled"
1530,gtc-2020-pytorch-from-research-to-production,"GTC 2020 S21928
Presenters: Grzegorz Karch,NVIDIA
Abstract
Learn how to get your neural network from the PyTorch framework into production. Explore ways to handle complex neural network architectures during deployment. We’ll show how to transform a neural network developed in PyTorch into a model ready for a production environment and exemplify the workflow on a conversational AI system. For full understanding, you should be familiar with PyTorch framework and have some interest in model deployment for inference. We’ll demonstrate the neural network system on TensorRT Inference Server (TRTIS).Watch this session
Join in the conversation below.Hi! Cannot play the video. It says “The media could not be loaded, either because the server or network failed or because the format is not supported”Hi, please try again, it should be working now.Thanks for the talk with convesrtaionalAI as example (even though it’s not my area of research).Questions:
1: Slide 42: where can I find deployer.py? Is the deployer specific to your conversationAI models? Or is it general, and theoretically could work for any model?2: Slide 32: so, if I’ve nn.Conv1d in my model, I should convert to 2d. But, does it impact the performance/quality of the model originally designed with 1d?3: Your presentation is tailored to deployment on Triton Inference Server. I guess the basic steps required to deploy for example on any other cloud platform is not much different. Correct?Powered by Discourse, best viewed with JavaScript enabled"
1531,nvidia-deepstream-sdk-for-iot-and-real-time-streaming-analytics-debuts-on-microsoft-azure-marketplace,"Originally published at:			NVIDIA DeepStream SDK for IoT and Real-Time Streaming Analytics Debuts on Microsoft Azure Marketplace | NVIDIA Technical Blog
NVIDIA has partnered with Microsoft Azure IoT in transforming and enabling advanced AI innovations for our developers and customers, by making DeepStream; the multi-purpose streaming analytics SDK available on Azure IoT Edge Marketplace.  DeepStream enables a broad set of use cases and industries, to unlock the power of NVIDIA GPUs for smart retail and warehouse…Powered by Discourse, best viewed with JavaScript enabled"
1532,register-cache-caching-for-warp-centric-cuda-programs,"Originally published at:			Register Cache: Caching for Warp-Centric CUDA Programs | NVIDIA Technical Blog
Figure 1: Execution and Memory hierarchy in CUDA GPUs. In this post we introduce the “register cache”, an optimization technique that develops a virtual caching layer for threads in a single warp. It is a software abstraction implemented on top of the NVIDIA GPU shuffle primitive. This abstraction helps optimize kernels that use shared memory…thanks, this was fun!I've been avoiding shared memory likes its the plague since 2009, was really stoked when warp shuffling came along :-)There are a surprising amount of kernels where people use shared memory when they actually don't need it nor warp-shuffles, they can just do more elements per thread instead.//JimmyThe use of __activemask() here is wrong and highly unsafe. https://devblogs.nvidia.com... especially mentions not  to ""just use __activemask() as the mask"" for the *_sync operations.Reasoning would be: You want to shuffle/publish the value of thread 0 of the warp. For whatever reason this thread is blocked (e.g. cache miss while the others had a cache hit) so it doesn't get included into __activemask() and doesn't participate in the __shfl_sync. So the result is undefined!On Pascal this does not matter as we have lock-step execution, but on Volta you might run into this. Please update your examples to use e.g. FULL_MASK and mention this problem so others won't fall in the same trap.Powered by Discourse, best viewed with JavaScript enabled"
1533,gtc-hpc-presentations,"Originally published at:			GTC HPC Presentations | NVIDIA Technical Blog
Starting on October 5-9, This fall’s GTC will run continuously for five days, across seven time zones. The conference will showcase the latest breakthroughs in HPC, and many other GPU technology interest areas.  Attend live events in the time zone that works best for you, or browse an extensive catalog of on-demand content showcasing innovative…Powered by Discourse, best viewed with JavaScript enabled"
1534,gtc-2020-simplifying-gpu-access-a-polyglot-binding-for-gpus-with-graalvm,"GTC 2020 S21269
Presenters: Rene Mueller,NVIDIA; Lukas Stadler,Oracle Labs
Abstract
GPU computing accelerates workloads and fuels breakthroughs across industries. There are many GPU-accelerated libraries developers can leverage, but integrating these libraries into existing software stacks can be challenging. Programming GPUs typically requires low-level programming, while high-level scripting languages have become very popular. Accelerated computing solutions are heterogeneous and inherently more complex. We’ll present an open-source prototype called grCUDA that leverages Oracle’s GraalVM and exposes GPUs in polyglot environments. While GraalVM can be regarded as the “one VM to rule them all,” grCUDA is the “one GPU binding to rule them all.” Data is efficiently shared between GPUs and GraalVM languages (R, Python, JavaScript) while GPU kernels can be launched directly from those languages. Precompiled GPU kernels can be used, as well as kernels that are generated at runtime. We’ll also show how to access GPU-accelerated libraries such as RAPIDS cuML.Watch this session
Join in the conversation below.Powered by Discourse, best viewed with JavaScript enabled"
1535,diagnosing-brain-tumors-quicker-and-with-higher-accuracy,"Originally published at:			https://developer.nvidia.com/blog/diagnosing-brain-tumors-quicker-and-with-higher-accuracy/
Neurosurgeons and pathologists from University of Michigan Medicine developed a new imaging technique that can be used in the operating room to diagnose brain tumors more efficiently. Today’s workflow for determining a diagnosis during an operation requires the surgeon wait for 30 to 40 minutes while tissue is sent to a dedicated pathology lab for…Powered by Discourse, best viewed with JavaScript enabled"
1536,new-sensor-partners-expand-surgical-ultrasound-and-data-acquisition-capabilities-in-the-nvidia-clara-holoscan-platform,"Originally published at:			https://developer.nvidia.com/blog/new-sensor-partners-expand-surgical-ultrasound-and-data-acquisition-capabilities-in-the-clara-holoscan-platform/
NVIDIA Clara Holoscan offers an expanded selection of third-party interface options for video capture, ultrasound research, data acquisition, and connection to legacy medical devices.Powered by Discourse, best viewed with JavaScript enabled"
1537,navigating-the-global-supply-chain-with-networking-digital-twins,"Originally published at:			Navigating the Global Supply Chain with Networking Digital Twins | NVIDIA Technical Blog
Supply chain shortages are impacting many industries, with semiconductors feeling the crunch in particular. With networking digital twins, you don’t have to wait on the hardware. Get started with infrastructure simulation in NVIDIA Air to stage deployments, test out tools, and enable hardware-free training.Powered by Discourse, best viewed with JavaScript enabled"
1538,cvpr-2020-researchers-develop-ai-solutions-to-improve-transportation-systems,"Originally published at:			https://developer.nvidia.com/blog/ai-city-challenge-cvpr-2020-transportation-systems/
To accelerate research and development of techniques to help make transportation systems smarter, researchers at the annual Computer Vision and Pattern Recognition Conference (CVPR) competed in the fourth annual AI City Challenge.  “The opportunity for insights from these sensors to make transportation systems smarter is immense,” said Milind Naphade, CTO of Metropolis at NVIDIA. The…Powered by Discourse, best viewed with JavaScript enabled"
1539,upcoming-webinar-using-gpus-to-accelerate-hd-mapping-and-location-based-services,"Originally published at:			https://developer.nvidia.com/blog/upcoming-webinar-using-gpus-to-accelerate-hd-mapping-and-location-based-services/
Powered by Discourse, best viewed with JavaScript enabled"
1540,how-gpus-are-revolutionizing-machine-learning,"Originally published at:			How GPUs are Revolutionizing Machine Learning | NVIDIA Technical Blog
NVIDIA announced that Facebook will accelerate its next-generation computing system with the NVIDIA Tesla Accelerated Computing Platform which will enable them to drive a broad range of machine learning applications. Facebook is the first company to train deep neural networks on the new Tesla M40 GPUs – introduced last month – this will play a…Powered by Discourse, best viewed with JavaScript enabled"
1541,grcuda-a-polyglot-language-binding-for-cuda-in-graalvm,"Originally published at:			grCUDA: A Polyglot Language Binding for CUDA in GraalVM | NVIDIA Technical Blog
Integrating GPU-accelerated libraries into existing software stacks can be challenging, in particular, for applications that are written in high-level scripting languages. Although CUDA-bindings already exist for many programming languages, they have different APIs and vary in functionality. Some are simple wrappers around the CUDA Runtime, others provide higher-level abstractions.    Figure 1: Architecture of grCUDA…Powered by Discourse, best viewed with JavaScript enabled"
1542,developers-show-off-amazing-real-time-ray-traced-projects-in-new-dxr-spotlight-contest,"Originally published at:			https://developer.nvidia.com/blog/new-dxr-spotlight-contest-ray-tracing/
The winners of the latest DXR Spotlight contest have been selected.Powered by Discourse, best viewed with JavaScript enabled"
1543,gtc-2020-cuda-on-nvidia-ampere-gpu-architecture-taking-your-algorithms-to-the-next-level-of-performance,"GTC 2020 S21170
Presenters: Carter Edwards,NVIDIA
Abstract
NVIDIA Ampere GPU Architecture delivers exciting new capabilities to take your algorithms to the next level of performance. Learn how to load shared memory at the speed of light, exert control over cache residency, and configure flexible synchronization patterns. Be delighted by how easily shared memory is prefetched while computing. If you can cudaMemcpyAsync host to device memory, then you will know how to memcpy_async device to shared memory. To make the most of the new 48Mb cache, you can prioritize what data should persist in cache for frequent fast access, or what data should perturb cache as little as possible as it streams from device memory. For algorithms that have been limited by only having __syncthreads() and __syncwarp(), there is the new fully configurable barrier type. Configure your own groups of threads to coordinate through your own barrier objects. Coordinate groups into producer/consumer patterns. Will you combine these building blocks to craft a persistent systolic array kernel?Watch this session
Join in the conversation below.Powered by Discourse, best viewed with JavaScript enabled"
1544,how-ai-is-helping-consumer-brands-detect-and-eliminate-counterfeit-products,"Originally published at:			https://developer.nvidia.com/blog/how-ai-is-helping-consumer-brands-detect-and-eliminate-counterfeit-products/
Seattle-based startup DataWeave, a competitive intelligence service provider for retailers and consumer brands, recently launched a counterfeit product detection system that uses deep learning to detect and help eliminate fake products from e-commerce websites. “As online retailers and marketplaces aggressively add thousands of merchants on their platforms, unauthorized white labeling, listing fake products, and image…Powered by Discourse, best viewed with JavaScript enabled"
1545,one-question-about-the-camera-model-in-ray-tracing-program,"In a simple ray-tracing application, we usually emit ray from camera to each pixel. In this condition the range of illumination information received by each pixel is limited to the solid angle that each pixel extent to camera’s position. As show in pictures below.
p1969×540 25.3 KB


p2969×540 25.5 KB
In first picture, pixel (b) can only receive the indirect illumination about object2. But in real life, pixel (b) should be able to receive direct illumination of object2. As shown in second picture. But this direct illumination is not accounted for in current ray-tracing methods.Is there really something wrong with the usual calculations model or is there something wrong with my understanding?The second figure shows light coming in from object 2 “directly” to the image plane. But, when you look in a direction, you only get light coming from that direction. I understand the confusion. A better mental model is to think of a ray from the eye looking at a little square in a screen door - think of that as the “pixel”. What do we see through that “pixel”? Say we see a table. Now, light coming from all different directions can hit that table, and some (usually tiny fraction of) that light will bounce off the table toward our eye. So, that’s how light from light sources and from other objects (which is turn are lit by light sources) affects the surface we see through our “pixel” in the screen. The “camera plane” is just a convenience for how to form eye rays and assign their results to specific pixels.That said, there are complications, e.g., if the surface is glass or some other semitransparent material. Or the atmosphere itself, such as fog, can affect how much light travels back to the eye. But light does not travel as shown in the second figure - it does not travel to this image plane, which is just a convenience and does not normally exist in real life, e.g., in our eyes it’s the retina behind the lens that captures the image. You could put a glass diffuser plate in place of the image plane, like how showers have, just for fun. This would give you a blurry image. In this case Figure 2 would be correct, in that the diffuse plate would be affected by light from many different directions.Powered by Discourse, best viewed with JavaScript enabled"
1546,step-into-omniverse-the-inaugural-nvidia-omniverse-user-group,"Originally published at:			https://developer.nvidia.com/blog/step-into-omniverse-the-inaugural-nvidia-omniverse-user-group/
Watch the event recording to learn more about Omniverse from some of the Omniverse teams and users, and start imagining what it can do for you and your ideas.Powered by Discourse, best viewed with JavaScript enabled"
1547,building-hpc-containers-demystified,"Originally published at:			Building HPC Containers Demystified | NVIDIA Technical Blog
What’s New with HPC Container Maker Whether you are a HPC research scientist, application developer, or IT staff, NVIDIA has solutions to help you use containers to be more productive. NVIDIA is enabling easy access and deployment of HPC applications by providing tuned and tested HPC containers on the NGC registry. Many commonly used HPC…Powered by Discourse, best viewed with JavaScript enabled"
1548,build-tools-for-the-3d-world-with-the-extend-the-omniverse-contest,"Originally published at:			https://developer.nvidia.com/blog/build-tools-for-the-3d-world-with-the-extend-the-omniverse-contest/
Announcing our first Omniverse developer contest for building an Omniverse Extension. Show us how you’re extending Omniverse to transform 3D workflows and virtual worldbuilding.Powered by Discourse, best viewed with JavaScript enabled"
1549,mit-develops-ai-that-handles-speech-and-object-recognition-all-at-once,"Originally published at:			MIT Develops AI That Handles Speech and Object Recognition All at Once | NVIDIA Technical Blog
MIT researchers have developed a deep learning system that can identify objects within an image, based on a spoken description of the picture, in real time. “We wanted to do speech recognition in a way that’s more natural, leveraging additional signals and information that humans have the benefit of using, but that machine learning algorithms…Powered by Discourse, best viewed with JavaScript enabled"
1550,silicon-valley-meetup-baidu-researchers-to-talk-on-whats-new-in-deep-learning,"Originally published at:			Silicon Valley Meetup: Baidu Researchers to talk on ‘What’s New in Deep Learning’ | NVIDIA Technical Blog
The HPC and GPU Supercomputing Group of Silicon Valley will be hosting two researchers from Baidu on Tuesday, October 6, 2015 from 6:30PM to 9:30PM at the NVIDIA Headquarters in Santa Clara, Ca.   Awni Hannun, left, and Erich Elsen, right We are very excited to have Awni Hannun and Erich Elsen from Baidu Research…Powered by Discourse, best viewed with JavaScript enabled"
1551,predicting-credit-defaults-using-time-series-models-with-recursive-neural-networks-and-xgboost,"Originally published at:			https://developer.nvidia.com/blog/predicting-credit-defaults-using-time-series-models-with-recursive-neural-networks-and-xgboost/
Today’s machine learning (ML) solutions are complex and rarely use just a single model. Training models effectively requires large, diverse datasets that may require multiple models to predict effectively. Also, deploying complex multi-model ML solutions in production can be a challenging task. A common example is when compatibility issues with different frameworks can lead to…Powered by Discourse, best viewed with JavaScript enabled"
1552,latest-nvidia-jetpack-developer-tools-will-double-your-deep-learning-performance,"Originally published at:			Latest NVIDIA JetPack Developer Tools Will Double Your Deep Learning Performance | NVIDIA Technical Blog
Today NVIDIA released a major update of the JetPack SDK with new developer tools and libraries that doubles the performance of deep learning applications on the Jetson TX1 Developer Kit, the world’s highest performance platform for deep learning on embedded systems. JetPack 2.3 is available as a free download and is focused on making it…Powered by Discourse, best viewed with JavaScript enabled"
1553,upcoming-event-how-onecup-ai-created-betsy-the-ai-ranch-hand,"Originally published at:			https://developer.nvidia.com/blog/upcoming-event-how-onecup-ai-created-betsy-the-ai-ranch-hand/
Powered by Discourse, best viewed with JavaScript enabled"
1554,create-realistic-robotics-simulations-with-ros-2-moveit-and-nvidia-isaac-sim,"Originally published at:			https://developer.nvidia.com/blog/create-realistic-robotics-simulations-with-ros-2-moveit-and-nvidia-isaac-sim/
Learn how to integrate MoveIt 2 with a robot simulated in NVIDIA Isaac Sim.there is a problem with this command :it should clone main branch , there is no docker compose on humble branchPowered by Discourse, best viewed with JavaScript enabled"
1555,nvidia-nsight-systems-2022-1-introduces-vulkan-1-3-and-linux-backtrace-sampling-and-profiling-improvements,"Originally published at:			https://developer.nvidia.com/blog/nvidia-nsight-systems-2022-1-introduces-vulkan-1-3-and-linux-backtrace-sampling-and-profiling-improvements/
The latest Nsight Systems 2022.1 release introduces several improvements aimed to enhance the profiling experience.Powered by Discourse, best viewed with JavaScript enabled"
1556,cutlass-fast-linear-algebra-in-cuda-c,"Originally published at:			CUTLASS: Fast Linear Algebra in CUDA C++ | NVIDIA Technical Blog
Matrix multiplication is a key computation within many scientific applications, particularly those in deep learning. Many operations in modern deep neural networks are either defined as matrix multiplications or can be cast as such. As an example, the NVIDIA cuDNN library implements convolutions for neural networks using various flavors of matrix multiplication. Matrix multiplication is…Powered by Discourse, best viewed with JavaScript enabled"
1557,cuda-7-5-pinpoint-performance-problems-with-instruction-level-profiling,"Originally published at:			https://developer.nvidia.com/blog/cuda-7-5-pinpoint-performance-problems-instruction-level-profiling/
[Note: Thejaswi Rao also contributed to the code optimizations shown in this post.] Today NVIDIA released CUDA 7.5, the latest release of the powerful CUDA Toolkit. One of the most exciting new features in CUDA 7.5 is new Instruction-Level Profiling support in the NVIDIA Visual Profiler. This powerful new feature, available on Maxwell (GM200) and…i cant setup cuda toolkit. My gtx 980ti is not vga cuda compatible? ??You'll need to provide more information on the problems you are having. All NVIDIA GPUs are CUDA-compatible.I upgraded to 7.5 on my Ubuntu host, but now I can't debug on Jetson TK1 target due to error ""cuda-gdb version (7.5.123) is not compatible with cuda-gdbserver version (6.5.121)"". Is there some way to get 7.5 on the TK1?nvvp not shows the information in columns and rows (for example, Utilization (column) and stacks in ""Kernel Performance is Bound By Instruction And Memory Latency"". Why?I don't fully understand the question. Is there a figure from this post where you see something different? Which figure? Can you link to a screenshot showing what you see instead?  Thanks!In attached screenshots, you could see the differenceWe tried with fermi GPU on win7 and could not reproduce this issue. It seems you have taken both the screenshots on the same platform with same GPU, is that correct?If you can give detailed steps to reproduce the issue along with the platform/operating system you are working on, it will be helpful for us to reproduce the issue quicker.Yes, both screenshots are taken on the same computer and the same GPU, one running cuda 7.0 (OK) and the other running cuda 7.5 (not OK).System is running Scientific Linux 6.7 x86_64 in a i7 processor with 8 GB RAMWe are unable to reproduce this behaviour onCentOS-7/GTX 480 setup with the CUDA 7.5 Production release(7.5.18).""Scientific Linux 6.7 x86_64"" is not supported officially in CUDA 7.5.Well... Scientific Linux is ""not"" supported officially in CUDA, but is very similar to CentOS... so... I suppose CentOS will return the same problem... But, if I have free time now, I will install a CentOS 6.x machine with CUDA 7.0 and 7.5Found the same problem on a CentOS 6.6 machine with K80s. Have you fixed the problem?Now, in a CentOS 7.0, both Cuda 7.0 and Cuda 7.5 runs OK and nvvp shows correctly the information in columns and rows (for example, Utilization (column) and stacks in ""Kernel Performance is Bound By Instruction And Memory Latency"".So, in CentOS 7.x we could say ""OK"", but in CentOS 6.x (and SL-6.x) the problem persists...Thanks for the tip!Would you be able to post the modified source code (estimated_combined4.cu)?great explanation. but, how can i do this Instruction-Level Profiling on command line via nvprof?Powered by Discourse, best viewed with JavaScript enabled"
1558,latest-releases-and-resources-march-3-9,"Originally published at:			https://developer.nvidia.com/blog/latest-releases-and-resources-march-3-9/
Register for the Game Developer Conference; join  DRIVE Developer Days; get DLI training at GTC; learn how Metropolis can grow your vision AI business; meet the Shell.AI Hackathon winners.Powered by Discourse, best viewed with JavaScript enabled"
1559,researchers-using-tesla-gpus-to-reduce-aircraft-noise,"Originally published at:			https://developer.nvidia.com/blog/researchers-using-tesla-gpus-to-reduce-aircraft-noise/
Taking advantage of the 27 petaflop Titan Supercomputer at Oak Ridge National Laboratory, researchers from Imperial College London’s Department of Aeronautics are attempting to reduce aircraft noise by visualizing how air is forced through engines when planes are in flight. Noise pollution from aircraft is a global policy and health issue. In fact, the scientists…Powered by Discourse, best viewed with JavaScript enabled"
1560,gtc-2020-jetson-xavier-nx-developer-kit-the-next-leap-in-edge-computing,"GTC 2020 S22706
Presenters: Dustin Franklin,NVIDIA
Abstract
This webinar will deep-dive into the new NVIDIA Jetson Xavier NX Developer Kit, the world’s smallest embedded edge AI supercomputer with over 21 TeraOPS of performance.
During this webinar, we’ll cover:Watch this session
Join in the conversation below.Powered by Discourse, best viewed with JavaScript enabled"
1561,nvidia-offers-siggraph-2019-attendees-a-closer-look-at-mdl,"Originally published at:			NVIDIA Offers SIGGRAPH 2019 Attendees a Closer Look at MDL | NVIDIA Technical Blog
At SIGGRAPH 2019, NVIDIA’s Lutz Kettner and Jan Jordan will discuss the basics of NVIDIA’s material definition language, showing how a single material can be used to define matching appearances between different renderers and rendering techniques. End users will learn how physically based definitions can be defined, while developers will learn what’s entailed in supporting…Powered by Discourse, best viewed with JavaScript enabled"
1562,introducing-nvidia-isaac-gym-end-to-end-reinforcement-learning-for-robotics,"Originally published at:			https://developer.nvidia.com/blog/introducing-isaac-gym-rl-for-robotics/
For several years, NVIDIA’s research teams have been working to leverage GPU technology to accelerate reinforcement learning (RL). As a result of this promising research, NVIDIA is pleased to announce a preview release of Isaac Gym – NVIDIA’s physics simulation environment for reinforcement learning research. RL-based training is now more accessible as tasks that once…Powered by Discourse, best viewed with JavaScript enabled"
1563,autodmp-optimizes-macro-placement-for-chip-design-with-ai-and-gpus,"Originally published at:			https://developer.nvidia.com/blog/autodmp-optimizes-macro-placement-for-chip-design-with-ai-and-gpus/
Macro placement has a tremendous impact on the landscape of the chip, directly affecting many design metrics, such as area and power consumption.Powered by Discourse, best viewed with JavaScript enabled"
1564,nvidia-delivers-doca-sdk-to-accelerate-and-secure-next-generation-data-centers,"Originally published at:			NVIDIA Delivers DOCA SDK to Accelerate and Secure Next Generation Data Center | NVIDIA Technical Blog
NVIDIA DOCA SDK, a Data Center-on-a-Chip Architecture that provides developers with an easy way to program the BlueField DPU.Powered by Discourse, best viewed with JavaScript enabled"
1565,cuda-pro-tip-do-the-kepler-shuffle,"Originally published at:			https://developer.nvidia.com/blog/cuda-pro-tip-kepler-shuffle/
When writing parallel programs, you will often need to communicate values between parallel threads. The typical way to do this in CUDA programming is to use shared memory. But the NVIDIA Kepler GPU architecture introduced a way to directly share data between threads that are part of the same warp. On Kepler, threads of a…What happens if I warp shuffle in a thread block of size 1024? Are there 32 warps or just 1?32. Warps on all current and past architectures have 32 threads.Thank you. Would it decrease warp instruction bottleneck (32 shuffles throughput) if I pack x,y,z,w variables into a struct and shuffle it instead, in a tight loop, rather than do x,y,z,w shuffles one after another? Does the 32 shuffles per cycle throughput limited by bandwidth or instruction isssue throughput?I'm optimizing an nbody algorithm with 20 flops in its unit work. If I add x,y,z,m shuffles, it gets bottlenecked. I mean, there is a loading part that 32 warps (in a 1024-thread block) load same 32 x,y,z,m values then they all start processing by their own shuffles for 32 times by the broadcasting shuffle 0xffffffff, value, counter parameters.Powered by Discourse, best viewed with JavaScript enabled"
1566,explainer-what-is-extended-reality,"Originally published at:			https://developer.nvidia.com/blog/explainer-what-is-extended-reality/
Extended reality, or XR, is a collective term that refers to immersive technologies, including virtual reality, augmented reality, and mixed reality.Powered by Discourse, best viewed with JavaScript enabled"
1567,google-cloud-makes-nvidia-gpus-available-for-first-time-in-brazil-india-tokyo-and-singapore,"Originally published at:			https://developer.nvidia.com/blog/nvidia-t4-gpus-on-google-cloud-platform/
Expansion Comes with Today’s Public Beta of NVIDIA T4 GPUs on Google Cloud Platform.  Google Cloud, with its public beta launch of NVIDIA Tesla T4 GPU across eight regions worldwide, announced the broadest availability yet of NVIDIA GPUs on Google Cloud Platform. Starting today, NVIDIA T4 GPU instances are available in public beta on GCP in…Powered by Discourse, best viewed with JavaScript enabled"
1568,new-release-nvidia-rtx-global-illumination-1-3,"Originally published at:			NVIDIA RTXGI SDK - Get Started | NVIDIA Developer
NVIDIA RTX Global Illumination (RTXGI) 1.3 includes highly requested features such as dynamic library support, an increased maximum probe count per DDGI volume by 2x, support for Shader Model 6.6 Dynamic Resources in D3D12, and more.Powered by Discourse, best viewed with JavaScript enabled"
1569,gtc-2020-exterminating-buffer-overflows-and-other-embarrassing-vulnerabilities-with-spark-ada-on-tegra,"GTC 2020 S21122
Presenters: Quentin Ochem,AdaCore
Abstract
Since 2018, NVIDIA has been actively investigating the SPARK Ada programming language to develop their most sensitive pieces of firmware. We’ll explain how users of the NVIDIA hardware can also benefit from this language choice when developing applications for the Tegra SoC. The benefits of the technology, from a cyber security point of view, will be demonstrated through the use of formal methods, allowing trivial proof of properties, such as absence of buffer overflows. We’ll describe using this technology on top of ARM processor cores, as well as methodologies for applications leveraging the GPU, either through existing libraries interfaces/CUDA code, or through an experiential port of Ada/OpenACC, which allows applications directly written in Ada or SPARK to offload to the GPU.Watch this session
Join in the conversation below.Powered by Discourse, best viewed with JavaScript enabled"
1570,gtc-2020-creating-an-intelligent-cockpit-with-drive-ix-and-driveworks,"GTC 2020 CWE21186
Presenters: Aaraadhya Narra,Oliver Knieps,  Anshul Jain,  Niranjan Avadhanam,  NVIDIA
Abstract
The NVIDIA DRIVE IX intelligent experience software stack runs on the DriveWorks middleware layer to enhance the driver’s situational awareness, assists in driving functions and provides natural language interactions between the vehicle and its occupants. Learn from our Developer Zone Forum experts how to create a safer, AI-powered cockpit experience using DriveWorks in this hour-long Q&A session.Watch this session
Join in the conversation below.Powered by Discourse, best viewed with JavaScript enabled"
1571,improving-gpu-utilization-in-kubernetes,"Originally published at:			https://developer.nvidia.com/blog/improving-gpu-utilization-in-kubernetes/
To improve NVIDIA GPU utilization in K8s clusters, we offer new GPU time-slicing APIs, enabling multiple GPU-accelerated workloads to time-slice and run on a single NVIDIA GPU.Hi all,I was wondering if it it is possible to enable time-slicing after having installed the NVIDIA’s operator chart.I have a fully-working k8s cluster with GPUs and I prefer not to “break” it. So I am trying the following:I create a configmap containing the configuration:(the same .yaml you use in your example)And I upgrade the operator release (after having seen its Helm’s values structure to apply config):I can see the .yaml file mounted from the configmap in /available-configs both in config-manager and nvidia-device-plugin pods of the StatefulSet. However time-slicing configuration is not applied yet.I noticed nvcr.io/nvidia/k8s-device-plugin:v0.12.2-ubi8  image is used for those pods instead of just v0.12.2.Am I missing something? Is any other approach available for running environments?Many thanks in advance!SergioHi @sergio.garcia - thanks for reading the blog and your comment!The gpu-operator Helm chart provides a default value on the devicePlugin. To set a default config across the cluster, you would need to specify a parameter of  devicePlugin.config.default=<config-name> or in your case, devicePlugin.config.default=time-slicing. If no config is set as default, then node labeling is required so that those nodes get the new plugin configuration.Also - you can also file a GitHub issue on the gpu-operator project in the future at Issues · NVIDIA/gpu-operator · GitHub for questions or issues.Hi @P_Ramarao,Many thanks for your response. I’ve been on vacation and hadn’t been able to try it.I managed to enable time-slicing using Operator’s chart. However I think the devicePlugin.config.default value (that I was leaving blank) must include the actual name of the .yaml included in the ConfigMap (dp-example-config.yaml in my previous example). Don’t you agree?
In chart’s values this option is described as “# Default config name within the ConfigMap”.Best,SergioDoes Triton also provide the same oversubscription functionality as the device plugin? It is able to run multiple models on the same device concurrently, so seems quite similar. What are the differences between the two approaches and how to choose between them?Hi @P_Ramarao  is the replica in the sharing configuration of time-slicing split the GPU memory equally, i.e for a 16 GB GPU if I specify a replica of 2 does the memory get split into half like 8GB for a single subscription by a Pod.and if the replicas is 5 would the memory be split into 16\5 for each subscription of the GPU.
Would be really great if you could clear this out.
Thankshi @adnTriton also provide the same oversubscription functionality as the device plugin? It is able to run multiple models on the same device concurrentlyTriton does not use time-slicing for oversubscription. Triton does allow multiple models to be executed concurrently - but it uses the CUDA streams API to do so (i.e. each model is executed via a different CUDA stream concurrently on the GPU). We also detailed CUDA streams in the blog - so there are tradeoffs to using CUDA streams.Hope that helps.Hi @sam137is the replica in the sharing configuration of time-slicing split the GPU memory equallyNo - the time-slicing capability does not partition memory. Each process running on the GPU has full access to the GPU - only execution contexts are swapped in & out by the scheduler. We mention in the blog that the tradeoff with using time-slicing is that you (as the application developer or devops) needs to be sure that one of the process doesn’t end up allocating all the memory on the GPU (so the other process may suffer from an OOM).The time-slicing support in the device plugin simply provides an oversubscription model on the number of GPU devices available - so that two different containers can land on the same GPU and thus time-slice (from an execution perspective).Hope that clarifies.Thanks @P_Ramarao  that clears it.Powered by Discourse, best viewed with JavaScript enabled"
1572,3-methods-for-speeding-up-ai-model-development-with-tao-toolkit,"Originally published at:			https://developer.nvidia.com/blog/3-methods-speeding-up-ai-model-development-tao-toolkit-whitepaper/
How do you shorten your AI application’s TTM? Here are 3 methods to eliminate framework complexity and cut your training time in half using TAO Toolkit.How Do I use the data produced by the ""Process&Train_Helmet.ipynb "" jupyter notebook to make the TAO export function produce and OUTPUT folder that contains the info to build an engine on my Xavier NX
tao911×318 11 KB
Hi @adventuredaisy ,You can use the exported .etlt model and then use the tao-converter on NX to convert (ref link for the jetpack version you have on NX: https://developer.nvidia.com/tao-toolkit-get-started ) this .etle model to TensorRT engine.Here is a video walk thru of the jupyter notebook
“Experiment 3 Add new classes of objects to an existing AI model”
from the Nvidia Tech blog
"" 3 Methods for Speeding up AI Model Development with TAO Toolkit""
There is also a link with the video to my github repo that contains the jupyter notebook
from this example with additional cells added so you can export to run on Xavier NXPowered by Discourse, best viewed with JavaScript enabled"
1573,nvidia-rtx-top-3-week-of-november-21-2018,"Originally published at:			NVIDIA RTX Top 3: Week of November 21, 2018 | NVIDIA Technical Blog
Every week, we’ll be delivering 3 interesting stories coming from the world of RTX Game Development. 1 – Battlefield V Now Supports Real-Time Ray Tracing on GeForce RTX graphics cards “It’s a watershed moment in many ways and a phenomenal technological achievement – not just from the RTX hardware that makes it possible, but also…Powered by Discourse, best viewed with JavaScript enabled"
1574,scaling-quantum-circuit-simulation-with-nvidia-cutensornet,"Originally published at:			https://developer.nvidia.com/blog/scaling-quantum-circuit-simulation-with-cutensornet/
We present benchmarks and usage of cuTensorNet, a cuQuantum library providing high-performance tensor network computations for quantum circuit simulation.I have played with the cuquantum sdk + google qsim a bit and managed to get the statevector simulation work with NVIDIA’s cuStateVec.
Do I understand it correctly that Google’s quantum libraries (qsim + cirq) do not work with cuTensorNet?cuTensorNet does not have a fully integrated solution with Google’s quantum libraries similar to cuStateVec. However, we do offer open source converter (translator) that translate a Cirq or Qsim circuit into cutensorNet format.
If this is what you are looking for, then please check our converter  and we will be happy to answer any further question/comments suggestions to improve.main/python/cuquantum/cutensornetHome for cuQuantum Python &amp; NVIDIA cuQuantum SDK C++ sampleshttps://github.com/NVIDIA/cuQuantum/tree/main/python/samples/circuit_converterThanksPowered by Discourse, best viewed with JavaScript enabled"
1575,accelerating-ai-development-pipelines-for-industrial-inspection-with-the-nvidia-tao-transfer-learning-toolkit,"Originally published at:			https://developer.nvidia.com/blog/accelerating-ai-development-pipelines-for-industrial-inspection-with-the-nvidia-tao-transfer-learning-toolkit/
There is an increasing demand for manufacturers to achieve high-quality control standards in their production processes. Traditionally, manufacturers have relied on manual inspection to guarantee product quality. However, manual inspection is expensive, often only covers a small sample of production, and ultimately results in production bottlenecks, lowered productivity, and reduced efficiency. By automating defect inspection…Powered by Discourse, best viewed with JavaScript enabled"
1576,gtc-2020-accelerate-and-autoscale-deep-learning-inference-on-gpus-with-kfserving,"GTC 2020 S22459
Presenters: David Goodwin,NVIDIA ; Dan Sun,Bloomberg
Abstract
Large-scale language models, such as BERT and GPT-2, have brought about exciting leaps in state-of-the-art accuracy for many NLP tasks. Due to its multi-head attention network, BERT requires significant compute during inference, which poses challenges for real-time application performance. KFServing provides model serving interfaces for common ML frameworks like TensorFlow, XGBoost, SKLearn, PyTorch, ONNX and NVIDIA’s TensorRT. Built on Kubernetes CRDs and KNative, KFServing enables hardware acceleration and autoscaling of Bloomberg’s own BERT models trained on a corpora of specialized financial news data. We’ll discuss how the Bloomberg Data Science Platform uses KFServing to address latency and scalability in a production application. In addition to its scalability features, KFServing provides a standardized data plane across model frameworks and servers. We’ll also present the community proposal for a v2 REST/gRPC data plane, along with its integration in TRTIS.Watch this session
Join in the conversation below.Powered by Discourse, best viewed with JavaScript enabled"
1577,lighting-scenes-with-millions-of-lights-using-rtx-direct-illumination,"Originally published at:			https://developer.nvidia.com/blog/lighting-scenes-with-millions-of-lights-using-rtx-direct-illumination/
At GTC Fall 2020, NVIDIA announced an upcoming developer SDK, RTX Direct Illumination (RTXDI), which enables and accelerates the rendering of dynamic direct lighting and shadows from many light sources. In this post, I highlight the RTXDI capabilities, ahead of the initial public release planned for spring 2021. You can request early access to RTXDI…Powered by Discourse, best viewed with JavaScript enabled"
1578,popular-open-source-thrust-and-cub-libraries-updated,"Originally published at:			Popular Open Source Thrust and CUB Libraries Updated | NVIDIA Technical Blog
Thrust 1.11.0 is a major release providing bug fixes and performance enhancements. It includes a new sort algorithm that provides up to 2x more performance from thrust::sort when used with certain key types and hardware. The new thrust::shuffle algorithm has been tweaked to improve the randomness of the output.  CUB 1.11.0 is a major release…Powered by Discourse, best viewed with JavaScript enabled"
1579,ai-helps-measure-properties-of-stars,"Originally published at:			AI Helps Measure Properties of Stars | NVIDIA Technical Blog
Scientists from Australia and Denmark trained a deep learning system to classify and predict the ages of red giant stars. “Automated methods do exist, however considerable effort is required for defining and acquiring features,” mentioned the researchers in their paper in reference to why they leveraged deep learning for their work over conventional methods. “Furthermore,…Powered by Discourse, best viewed with JavaScript enabled"
1580,now-available-nvidia-nemo-guardrails,"Originally published at:			Nemo Framework for Generative AI - Get Started | NVIDIA Developer
Develop safe and trustworthy LLM conversational applications with NVIDIA NeMo Guardrails, an open-source toolkit that enables programmable guardrails for defining desired user interactions within an application.Powered by Discourse, best viewed with JavaScript enabled"
1581,microsoft-trains-turing-nlg-world-s-largest-transformer-language-model,"Originally published at:			https://developer.nvidia.com/blog/using-nvidia-dgx-2-systems-microsoft-trains-worlds-largest-transformer-language-model/
Microsoft today announced a breakthrough in conversational AI, training the largest transformer-based language generation model with 17 billion parameters, using NVIDIA DGX-2 systems. Also today, Microsoft open-sourced DeepSpeed, a deep learning library that can help developers with latency and inference. Named Turing Natural Language Generation (T-NLG), the model is the largest transformer model available that…Powered by Discourse, best viewed with JavaScript enabled"
1582,ai-accelerates-efforts-to-develop-clean-fusion-energy,"Originally published at:			https://developer.nvidia.com/blog/ai-accelerates-efforts-to-develop-clean-fusion-energy/
Researchers from Harvard University, Princeton University’s Plasma Physics Laboratory, and the U.S. Department of Energy are working towards replicating the energy of the sun in the form of fusion energy. The process involves heating nuclei of light atoms to create plasma comprised of ionized particles to millions of degrees, however, the plasma created is simply…Powered by Discourse, best viewed with JavaScript enabled"
1583,ai-finds-smoking-affects-the-biological-clock,"Originally published at:			AI Finds Smoking Affects the Biological Clock | NVIDIA Technical Blog
According to the centers for disease control, cigarette smoking causes more than 480,000 deaths every year in the United States. That is more deaths than HIV, illegal drug use, alcohol use, motor vehicle injuries, and firearm-related incidents combined.   There are numerous studies that show an association between smoking and cancer, cardiovascular disease and all-cause…Powered by Discourse, best viewed with JavaScript enabled"
1584,pinterest-introduces-the-future-of-visual-search,"Originally published at:			https://developer.nvidia.com/blog/pinterest-introduces-the-future-of-visual-search/
The popular social photo sharing site is adding the ability for people to take a photo of things in the real-world and get recommendations for similar products on Pinterest. For example, If you take a photo of a room with a purse and a lamp on a table that you like, Pinterest will show you…Powered by Discourse, best viewed with JavaScript enabled"
1585,new-vulkan-device-generated-commands,"Originally published at:			New: Vulkan Device Generated Commands | NVIDIA Technical Blog
Connect directly with NVIDIA Developer Technology Engineers on OpenGL and Vulkan-related topics to get answers to your questions, from regular graphics use to compute shaders, ray tracing, or interop between the APIs. Register for this GTC Connect with Experts session, Vulkan and OpenGL Q&A, May 27, 9AM PT. This post was updated substantially in March…Powered by Discourse, best viewed with JavaScript enabled"
1586,end-to-end-ai-for-workstation-onnx-runtime-and-optimization,"Originally published at:			https://developer.nvidia.com/blog/end-to-end-ai-for-workstation-onnx-runtime-and-optimization/
This post is the third in a series about optimizing end-to-end AI for workstations. For more information, see part 1, End-to-End AI for Workstation: An Introduction, and part 2, End-to-End AI for Workstation: Transitioning AI Models with ONNX. When your model has been converted to the ONNX format, there are several ways to deploy it,…Powered by Discourse, best viewed with JavaScript enabled"
1587,multi-gpu-programming-with-standard-parallel-c-part-2,"Originally published at:			https://developer.nvidia.com/blog/multi-gpu-programming-with-standard-parallel-c-part-2/
By developing applications using MPI and standard C++ language features, it is possible to program for GPUs without sacrificing portability or performance.Powered by Discourse, best viewed with JavaScript enabled"
1588,fast-track-adas-and-av-research-with-nvidia-drive-agx,"Originally published at:			https://developer.nvidia.com/blog/fast-track-av-research-nvidia-drive-agx/
Driver assistance technology is an incredibly active research domain – from supervised assistance functions all the way to fully autonomous driving. The best way to showcase the capabilities of novel AV approaches is to demonstrate them in a real car, but there are significant challenges to this type of deployment.   Getting to the point where a new approach for multi-agent prediction, camera-based localization…Powered by Discourse, best viewed with JavaScript enabled"
1589,ray-tracing-updates-available-through-nvidia-sdks-and-the-nvidia-branch-of-unreal-engine,"Originally published at:			https://developer.nvidia.com/blog/ray-tracing-updates-available-through-nvidia-sdks-and-the-nvidia-branch-of-unreal-engine/
Developers can access the latest versions of RTXGI, RTXDI, NRD, and OptiX through our SDKs and select technologies through the NVIDIA Branch of Unreal Engine.Powered by Discourse, best viewed with JavaScript enabled"
1590,call-for-papers-real-time-ray-tracing,"Originally published at:			Call for Papers: Real-Time Ray Tracing | NVIDIA Technical Blog
Real-time ray tracing – the holy grail of graphics, considered unattainable for decades – is now possible for video games. Thanks to advances in GPU hardware and integration in standards like DirectX, game developers will eagerly add ray tracing to take the next step in visual quality and ease of content creation. To help game…Powered by Discourse, best viewed with JavaScript enabled"
1591,gtc-2020-interactive-deep-learning-using-the-gpu-for-visual-insight-into-training-and-inference,"GTC 2020 S21313
Presenters: Don Brittain,NVIDIA
Abstract
The GPU was originally created to drive interactive, highly-visual applications. Because of their massive computation power, GPUs are also excellent for deep-learning training and inference. By capitalizing on both of these strengths during DL development, we can gain insight into model design, loss-function performance, and inference optimization opportunities. We’ll show some ways that interactive visual processing can help streamline DL development, from model design through final deployment. Our session’s geared toward application programmers and other AI practitioners, but it should also be accessible to program designers and project managers interested in incorporating AI features into your products.Watch this session
Join in the conversation below.Powered by Discourse, best viewed with JavaScript enabled"
1592,cuda-refresher-the-gpu-computing-ecosystem,"Originally published at:			CUDA Refresher: The GPU Computing Ecosystem | NVIDIA Technical Blog
This is the third post in the CUDA Refresher series, which has the goal of refreshing key concepts in CUDA, tools, and optimization for beginning or intermediate developers. Ease of programming and a giant leap in performance is one of the key reasons for the CUDA platform’s widespread adoption. The second biggest reason for the…Powered by Discourse, best viewed with JavaScript enabled"
1593,gtc-2020-accelerating-sparsity-in-the-nvidia-ampere-architecture,"GTC 2020 S22085
Presenters: Jeff Pool,NVIDIA
Abstract
We’ll cover the sparsity features of NVIDIA Ampere hardware, as well as techniques for taking advantage of them. We’ll describe how you can train your network to take advantage of sparsity features to accelerate their inference and maintain the accuracy of the original dense network model.Watch this session
Join in the conversation below.Powered by Discourse, best viewed with JavaScript enabled"
1594,does-get3d-have-image-guided-shape-generation-feature,"Hey there!
in your press release (https://developer.nvidia.com/blog/rapidly-generate-3d-assets-for-virtual-worlds-with-generative-ai/) there is a phrase: “GET3D is a new generative AI model that generates 3D shapes with topology, rich geometric details, and textures from data like 2D images, text, and numbers.” - so, it seems GET3D can somehow  generate a 3D model from a picture… I mean something like that: at the Inference stage, GET3D takes a png image as a parameter, and in response generates not a bunch of random 3D models, but a 3D model similar to png picture. Is it possible to use GET3D this way, if yes - how exactly (it would be great to get any docs or advices)…
Vasil.Hi, Thanks for the questions! Due to time constraints, we are not working on image-guided shape generation for GET3D, we might explore that in the future.thanks for the answer!
is there any understanding when there will be a version with the ability to upload an image?We do not have a specific timeline for it but will be actively looking into thisFrom the point of view of technology, is it not possible to upload a picture and get a 3D model from it now? or is there such an option, but not available to us (developers)?Hi, we do not have this option at the moment, and we would encourage developers/researchers to try PTI inversion if having one image as input and wants to generate 3D shape from the image GitHub - danielroich/PTI: Official Implementation for ""Pivotal Tuning for Latent-based editing of Real Images"" (ACM TOG 2022) https://arxiv.org/abs/2106.05744This topic was automatically closed 14 days after the last reply. New replies are no longer allowed.Powered by Discourse, best viewed with JavaScript enabled"
1595,improving-landmark-localization-with-a-new-deep-learning-architecture,"Originally published at:			Improving Landmark Localization with a New Deep Learning Architecture | NVIDIA Technical Blog
Researchers from NVIDIA, together with collaborators from academia, developed a new deep learning-based architecture for landmark localization, which is the process of finding the precise location of specific parts of an image. Additionally they proposed a novel training procedure based on semi-supervised learning that allows exploring images without ground-truth landmarks to improve accuracy of the…Powered by Discourse, best viewed with JavaScript enabled"
1596,gtc-2020-accelerated-data-science-on-gpus-using-rapids,"GTC 2020 CWE21728
Presenters: ,
Abstract
Parallelizing ML workloads on NVIDIA GPUs helps to analyze data and make decisions more efficiently. Come to this session to learn how to use GPUs to accelerate your ML workloads using the cuML RAPIDS project.Watch this session
Join in the conversation below.Powered by Discourse, best viewed with JavaScript enabled"
1597,seven-things-you-might-not-know-about-numba,"Originally published at:			Seven Things You Might Not Know about Numba | NVIDIA Technical Blog
One of my favorite things is getting to talk to people about GPU computing and Python. The productivity and interactivity of Python combined with the high performance of GPUs is a killer combination for many problems in science and engineering. There are several approaches to accelerating Python with GPUs, but the one I am most…Wow, this is super cool!!! Thank you for sharing!Hi All,Here is what I did with Numba:I just got published an blog article ""High Performance Big Data Analysis Using NumPy, Numba and Python Asynchronous Programming"" in Dataconomy media (http://dataconomy.com/). Here is the link: http://dataconomy.com/2017/...Let me know what you think?ThanksErnest Bonat, Ph.D.Senior Software EngineerSenior Data ScientistDid you run it on a GPU, or just the CPU?Hi Mark,I run it in CPU laptop! (32 RAMs)ThanksErnest Bonat, Ph.D.Senior Software EngineerSenior Data ScientistDid you consider compiling it to run on GPU?Yes, I would like to do that. I wish I can have a GPU laptop for it!ThanksErnest Bonat, Ph.D.Senior Software EngineerSenior Data ScientistHi Stanley,Looks like a typo snuck into your Cuda C++ clamp example, patched thusly:{code} __host__ __device__ float clamp(float x, float xmin, float xmax) {     if (x < xmin){         return xmin;-    } else if (x > xmin) {+    } else if (x > xmax) {         return xmax;     } else {         return x;{code}Thanks! Fixed.Probably a very late response but:
Regarding “Numba + Jupyter”: I find that Jupyter somehow limits the sched_affinity such that Numba is only using 2 cores when 96 are available on my cluster node.  I’d love to know how to get around that.  Just setting “os.environ[‘NUMBA_NUM_THREADS’] = ‘96’” within the notebook doesn’t seem to do it; still only uses 2.Powered by Discourse, best viewed with JavaScript enabled"
1598,running-multiple-applications-on-the-same-edge-devices,"Originally published at:			https://developer.nvidia.com/blog/running-multiple-applications-on-the-same-edge-devices/
Partition GPUs to give applications dedicated resources at the edge with Multi-Instance GPU on Fleet Command.Powered by Discourse, best viewed with JavaScript enabled"
1599,cuda-9-features-revealed-volta-cooperative-groups-and-more,"Originally published at:			https://developer.nvidia.com/blog/cuda-9-features-revealed/
Figure 1: CUDA 9 provides a preview API for programming Tesla V100 Tensor Cores, providing a huge boost to mixed-precision matrix arithmetic for deep learning. At the 2017 GPU Technology Conference NVIDIA announced CUDA 9, the latest version of CUDA’s powerful parallel computing platform and programming model. CUDA 9 is now available as a free…The Gtx 1050 Ti Laptop, will be supported?What's exactly new, in terms of assembly programming ?New libraries and software isn't exactly what you would call ""new"".Also, could you drop that idea of threads you promoted so long ?Initially it was used for marketing purposes, to claim that GPU is capable of running 32x more threads than it actually can. I see no point in getting things worse and worse just for marketing lies. It would be easier for developers to adjust their code to the actual operations performed by GPU, that is array operations.CUDA programs execute thousands of parallel threads. The threads are not as heavyweight as CPU threads, and they are created and run in parallel, but they are still threads, free to branch and take different execution paths. They are not limited to just array operations. Volta and CUDA 9 make this even more flexible.Yes.I didn't know there were SIMT deniers.How come ? If one divides CPU cache into 64 byte-sized private areas and run 1 thread, one could claim that there are 512 threads per L1 cache. Such ""parallel"" threads would be still 1 thread actually. If they can't branch independently and the branches are emulated by having the rest of them wait till the branch is performed, they are not threads, in actual meaning.I've always thought the following blog post was a thoughtful, unbiased, well-explained analysis by someone who gets it. http://yosefk.com/blog/simd...Is there a possibility that NVIDIA will publish the data on the following:Instruction format (encoding) and timing for the new architecture,possibility to use registers as temporary up to (before) the time they are written by the result of the operation (I wanted to know if one could use them as temporary until a write is performed)tagging instructions for pseudo-threads and their effect, theoretical, if any,register file write and read operation description, as well as addressing and throughput for register groups,extension for tensor operation to the instruction set, if any.@Mark_Harris:disqus I enjoyed reading that blog post, thanks for sharing.Thanks for the share Mark, the SIMT vs SIMD article is excellent.Looks like the recorded sessions won't be available until June 8th for people who did not attend GTC 2017.Hello Mark!when i read this blog, 2 things catch my eyes:1) in your screenshots you use a Titan card, but talking ""as usual""extensively about Tesla (okay only tesla exist now with volta engine, but onmany other nvidia paper you ""unintentionally"" talk about telsa or quadro, and veryfew about titan)since this blog is about CUDA 9, can you make a completelist of all GPU compatible with CUDA 9?2) ""These new meta packages provide simple and clean installation of CUDAlibraries for deep learning and scientific computing""my question**********I’m trying to clearly understand Nvidia positioning about the titan-line ofcards:[+] Titan are simply the best choice for ""any"" CUDA developer : theyare (since their introduction) the fastest CUDA FP32 hardware available [titan XP even better than GP100], they are versatile (can works in any machine, actively cooled, supportboth WDDM and TCC mode), they support almost the biggest VRAM, they have astandard-positioned power plug, they provide -by around 6x- much more $ perCUDA core ratio than any other ""professional"" card[+] i think they were introduced as this, a perfect CUDA card for high endworkstations + Computenow, since the most differentiation between all CUDA oriented cards is mostlyinside the driver (allowing or not TCC mode, optimized for openGL/CAD, allowingor not virtualization, allowing or not 10bit output, allowing or notremote/grid, only for servers, ...)[-] why does titan card are using consumer/gaming drivers? With the famous""unintentional virtualized bug"" and the windows code 43 result + notcompatible with quadro/tesla drivers.[-] i think it's time that Nvidia create a specific driver for “titan-for-compute”,focusing on TCC mode + virtualization support - the perfect companion of anyCUDA developerAlso, this new driver will then indeed ""provide simple and cleaninstallation of CUDA libraries for deep learning and scientific computing"",without requiring to install an huge driver package.fredHere's complete list of CUDA compatible GPUs: https://developer.nvidia.co...thanks for this list, however i guess that :- volta does require CUDA9 to work- older chipset (i will say from kelpler to pascal) will all works ? i guess that some features will not be enabled? so a simple drivers update will bring cuda9 to all cards?for question 2 :I'm ""dreaming"" of a day that Nvidia will propose a clean, efficient and global driver for compute and deep learning. so no more ""artificial"" separation (from a driver perspective) between titan, quadro and tesla. i will be up to nvidia to cleary differentiate each product (eg : titan for fast and affordable CUDA development without compromise -specifically in the new virtualisation area-, telsa for datacenter and supercomputer and scientific compute/fp64, quadro for engineering and design). also we should be able to freely mix any of those card on same system, with unified driver.basically, buy and use the most efficient card for each usage, without any driver limitation as today...fredWill we one day teach AI how to code and let the software developers die out?Have you planned to make the matrices Row-Major? O Any direct conversion?Can you clarify your question?Existing CUDA codes should run on Volta. To build new codes for Volta, you need CUDA 9. CUDA 9 will support older GPUs as well. Your ""question 2"" is not a question.Powered by Discourse, best viewed with JavaScript enabled"
1600,an-exclusive-invitation-peek-behind-the-omniverse-curtain-at-the-inaugural-omniverse-user-group,"Originally published at:			https://developer.nvidia.com/blog/an-exclusive-invitation-peek-behind-the-omniverse-curtain-at-the-inaugural-omniverse-user-group/
Join the first NVIDIA Omniverse User Group, an exclusive event hosted by the lead engineers, designers, and artists of Omniverse on August 12, during the virtual SIGGRAPH conference.Powered by Discourse, best viewed with JavaScript enabled"
1601,take-a-deep-dive-into-ray-tracing-machine-learning-and-neural-networks-through-siggraph-frontiers,"Originally published at:			https://developer.nvidia.com/blog/take-a-deep-dive-into-ray-tracing-machine-learning-and-neural-networks-through-siggraph-frontiers/
SIGGRAPH 2021 is the premier conference and exhibition in computer graphics and interactive techniques. This year, participants can join a series of exciting pre-conference webinars called SIGGRAPH Frontiers Interactions.Powered by Discourse, best viewed with JavaScript enabled"
1602,introduction-to-neural-machine-translation-with-gpus-part-2,"Hi Kyunghyun,Thank you very much for a clear explanation. Just one point I don't completely understand is what would be the value of z_0 when decoding (assuming that the first recurrent state we need is z_1). Because I can see that z_1 only has 2 inputs instead of 3 inputs like other z.Thanks a lot.ChienHi Chien,We often initialize z_0 based on the output of the encoder. See, e.g., https://github.com/nyu-dl/d...Best,- Ksuppose my utterance is :  "" How are you"" response is : "" Iam Fine""the final hidden state of the encoder has the information of the whole sentence ""How are you"" and we are sending that to the decoder.In the paragraph,""Now we have a probability distribution over the target words, which we can use to select a word by sampling the distribution (see here), as Figure 9 shows. After choosing the i-th word, we go back to the first step of computing the decoder’s internal hidden state (Figure 7), scoring and normalizing the target words (Figure 8) and selecting the next (i+1)-th word (Figure 9), repeating until we select the end-of-sentence word (<eos>).""you are saying that ""After choosing i th word"" ,Do you mean, you will just choose that word with highest probability and then convert it into a hot vector and then give it to the decode as the input (or) Do you mean, you will choose the word ""Iam"" (as here in example) with it's probability and convert this to a hot vector and input to the decoder?Hi Harsha,It'll depend on whether you're training or testing. If you're training to maximize the log-likelihood, you simply use the ground-truth word (e.g., ""Iam"" in your example.) If you're testing, you will either choose the most likely word so far (greedy decoding), sample from the conditional distribution (ancestral sampling) or use another more sophisticated decoding algorithm (e.g., beam search.)For more about this, please refer to my lecture note athttps://github.com/nyu-dl/N...Best,- KHello Sir,your answer greatly helped.Thank you HarshaHi,could you kindly explain what is an E matrix and how to get it? And one more point to regard the z equation: you wrote that its non-linear function was described in previous post. Indeed, there were twi equations for RNN hidden state and for GRU hidden state. But i didn't find any appropriate description on how to obtain values for z. And, therefore, i didin't understand, how to compute z.did i miss something? Please clarify how to calculate z.Thanks in advance.E is a word embedding matrix, of which each row corresponds to a word vector. z is the hidden activation vector of a recurrent net and computed based on your choice of activation function (e.g., GRU).Also kindly explain why it is necessary to sample from multinomial distribution? Why not to select just element with maximum probability? Also I've tried np.random.multinomial(...) for this purpose and it causes the whole model to fail gradient check.Yes, this is clear to me. But I've asked about how does h_t corresponds to z_i? All illustrations show that h is passed to z at each decoder iteration. And every recurrent cell  receives only two parameters: input vector (u in our case) and it's previous hidden vector (z_i-1 in our case). And I still can't realize how h is taken into account at each decoder step.You can think of concatenating u and h_T every time step in the decoder.sampling or greedy-max is only for decoding. for MLE training, you do not need to do either.And therefore concatenated pair (u, h_T) is also concatenated with z_i-1 and passed to the decoder. Am i right?Does decoder must be trained separate from encoder?they are not trained separately but jointly.depends on which type of recurrent activation function you use.Well, as i understand, decoding occurs during training and work. Could you please clarify what is MLE training?you do decoding during test, but don't during training if maximum likelihood training is used. mle is explained in this article already.I completely confused. Encoder and decoder are trained jointly. But how to train a decoder without decoding?Thank you for this informative post. Although I don't understand the part for calculating output scores. I think you meant the output layer's Weight, by W. (just under Figure. 8)Hi, I am also a reader of this post.Mr. Cho mentioned W as a 'target word vector' that appears to be weight parameters in the softmax output layer. Since we need the number of outputs from the softmax ouput layer (for each time unit, i.e., for each word in the target sentence) to be the same as the total number of unique target words (i.e., the size of the target words' vocabulary), we may consider that each combination of weight parameters to a specific output node (here, indexed by k) as target word vector......Powered by Discourse, best viewed with JavaScript enabled"
1603,get-hands-on-ai-data-science-and-accelerated-computing-training-at-gtc,"Originally published at:			https://developer.nvidia.com/blog/get-hands-on-ai-data-science-and-accelerated-computing-training-at-gtc/
Register now for instructor-led workshops from the NVIDIA Deep Learning Institute at GTC.Powered by Discourse, best viewed with JavaScript enabled"
1604,creating-voice-based-virtual-assistants-using-nvidia-jarvis-and-rasa,"Originally published at:			https://developer.nvidia.com/blog/creating-voice-based-virtual-assistants-using-jarvis-and-rasa/
Virtual assistants have become part of our daily lives. We ask Siri almost anything that we wonder about or order groceries through Alexa. In addition to providing convenience to our daily lives, virtual assistants are of tremendous help when it comes to enterprise applications. For example, we use online chatbots to help navigate complex technical…Hi there! I’m following the weather chatbot demo from this link and I cannot seem to get microphone to work. The chatbot is only working via text (basically rasa capabilities with Jarvis UI). Any suggestions? I’m working on an EC2 instance powered by NVIDIA and running the UI locally from my Chrome browser. Thanks in advance!Hi, were you able to install RASA on jetson nano? if yes then what version are you using?Hi Saleem
We have not tried running RASA on jetson nano.
For our work, we ran it on the same hardware that we ran Jarvis on - a V100 GPUHi HannaSorry for reaching out to so late. I seem to have missed this comment.Were you able to get your mic working? Just want to make sure that you granted the website access to the mic.Hi! The dialogue management component of the chatbot is developed using RASA. I was wondering if NVIDIA has this development tool too so that we can build a chatbot without relying on RASA?Hi Saeed
Thats a great question.
NVIDIA is currently working on working on a Chatbot Maker framework, built on top of Riva, which includes Dialog Management. This is currently under Early Access - You can apply for it here https://developer.nvidia.com/riva-studio-early-accessPowered by Discourse, best viewed with JavaScript enabled"
1605,nsight-graphics-the-three-things-you-need-to-know,"Originally published at:			Nsight Graphics: The Three Things you Need to Know | NVIDIA Technical Blog
In this video, Aurelio Reis, Director of Graphics Tools at NVIDIA, details the three most important things developers need to know about Nsight Graphics. To learn more, you can attend his talk at GDC:Title: Advanced Graphics Techniques Tutorial: “Surfing the Wave(front)s with Radeon GPU Profiler” & “Debugging and Profiling DXR & Vulkan Ray Tracing”Location: Room…Powered by Discourse, best viewed with JavaScript enabled"
1606,share-your-science-scaling-deep-learning-with-gpus,"Originally published at:			Share Your Science: Scaling Deep Learning with GPUs | NVIDIA Technical Blog
Bryan Catanzaro, Senior Researcher at Baidu, shares how his company is Tesla and TITAN GPUs for a variety of Artificial Intelligence applications, including speech recognition, and natural language understanding.   Watch Bryan’s talk in the NVIDIA GPU Technology Theater at SC15: Watch Now Learn more about their research at Baidu USA Share your GPU-accelerated science with…Powered by Discourse, best viewed with JavaScript enabled"
1607,monash-university-upgrades-massive-gpu-accelerated-supercomputer,"Originally published at:			Monash University Upgrades MASSIVE GPU-Accelerated Supercomputer | NVIDIA Technical Blog
To accelerate biomedical research, Australia’s Monash University boosted its research infrastructure with a third GPU-accelerated supercomputer called MASSIVE-3. MASSIVE-3 is equipped with both Tesla GPUs and Quadro GPUs for data processing and visualization, driving the new system nearly four times faster than MASSIVE-2. Over the past five years, MASSIVE has played a key role in…Powered by Discourse, best viewed with JavaScript enabled"
1608,nvidia-apex-tools-for-easy-mixed-precision-training-in-pytorch,"Originally published at:			NVIDIA Apex: Tools for Easy Mixed-Precision Training in PyTorch | NVIDIA Technical Blog
Most deep learning frameworks, including PyTorch, train using 32-bit floating point (FP32) arithmetic by default. However, using FP32 for all operations is not essential to achieve full accuracy for many state-of-the-art deep neural networks (DNNs). In 2017, NVIDIA researchers developed a methodology for mixed-precision training in which a few operations are executed in FP32 while…Powered by Discourse, best viewed with JavaScript enabled"
1609,analysis-driven-optimization-analyzing-and-improving-performance-with-nvidia-nsight-compute-part-2,"Originally published at:			https://developer.nvidia.com/blog/analysis-driven-optimization-analyzing-and-improving-performance-with-nvidia-nsight-compute-part-2/
In part 1, I introduced the code for profiling, covered the basic ideas of analysis-driven optimization (ADO), and got you started with the Nsight Compute profiler. In part 2, you apply what you learned to improve the performance of the code and then continue the analysis and optimization process. Refactoring To refactor the code based…Powered by Discourse, best viewed with JavaScript enabled"
1610,ampme-connects-your-phone-with-others-to-make-one-sound-system,"Originally published at:			AmpMe Connects Your Phone with Others to Make One Sound System | NVIDIA Technical Blog
After crossing 1.5M downloads, the free music-syncing app AmpMe now allows users to party without an internet connection using a new machine learning-powered predictive syncing technology. AmpMe streams music to multiple devices in perfect sync so friends can enjoy music together anywhere, without setting up a sound system. In the latest version, the new Predictive Sync”…Powered by Discourse, best viewed with JavaScript enabled"
1611,how-to-evaluate-ai-in-your-vendors-cybersecurity-solution,"Originally published at:			https://developer.nvidia.com/blog/how-to-evaluate-ai-in-your-vendors-cybersecurity-solution/
Considering new security software? AI and security experts Bartley Richardson and Daniel Rohrer from NVIDIA have advice: Ask a lot of questions.Powered by Discourse, best viewed with JavaScript enabled"
1612,fingerprinting-every-network-user-and-asset-with-nvidia-morpheus,"Originally published at:			Fingerprinting Every Network User and Asset with NVIDIA Morpheus | NVIDIA Technical Blog
Use unsupervised AI and time series modeling to create microtargeted models for every user and account; as humans and machine/account combinations running on your network.Powered by Discourse, best viewed with JavaScript enabled"
1613,nvidia-announces-modulus-a-framework-for-developing-physics-ml-models-for-digital-twins,"Originally published at:			NVIDIA Announces Modulus: A Framework for Developing Physics ML Models for Digital Twins | NVIDIA Technical Blog
Learn how NVIDIA Modulus blends physics and AI to deliver higher fidelity models, enabling more sophisticated and interactive digital twin applications.Powered by Discourse, best viewed with JavaScript enabled"
1614,meet-the-researcher-alina-zare-advancing-machine-learning-and-sensing,"Originally published at:			Meet The Researcher: Alina Zare, Advancing Machine Learning and Sensing | NVIDIA Technical Blog
‘Meet the Researcher’ is a series in which we feature different researchers in academia who are using GPUs to accelerate their work. This month we spotlight Alina Zare who conducts research and teaches in the areas of machine learning, artificial intelligence, computer vision and image analysis, and remote sensing.  Zare directs the Machine Learning and…Powered by Discourse, best viewed with JavaScript enabled"
1615,changing-ctc-rules-to-reduce-memory-consumption-in-training-and-decoding,"Originally published at:			Changing CTC Rules to Reduce Memory Consumption in Training and Decoding | NVIDIA Technical Blog
See how your ASR pipeline can benefit from modifying some of the Connectionist Temporal Classification (CTC) loss rules for training and decoding.Powered by Discourse, best viewed with JavaScript enabled"
1616,microsoft-leverages-the-power-of-nvidia-gpus-to-enhance-speech-recognition-algorithms,"Originally published at:			Microsoft Leverages the Power of NVIDIA GPUs to Enhance Speech Recognition Algorithms | NVIDIA Technical Blog
To enhance the capability of text-to-speech and automatic speech recognition algorithms, Microsoft researchers developed a deep learning model that uses unsupervised learning, an approach not commonly used in this field,  to improve the accuracy of the two speech tasks. By using the Transformer model, which is based on a sequence-to-sequence architecture, the team achieved a…Powered by Discourse, best viewed with JavaScript enabled"
1617,teaching-machines-to-read-lego-manuals-with-computer-vision,"Originally published at:			Teaching Machines to Read LEGO Manuals with Computer Vision | NVIDIA Technical Blog
LEGO lovers scratching their heads reading assembly instructions could soon have help with complicated builds thanks to a new study from Stanford University, MIT, and Autodesk.Powered by Discourse, best viewed with JavaScript enabled"
1618,nvidia-isaac-sdk-now-available-for-download-to-kickstart-robotics-development,"Originally published at:			NVIDIA Isaac SDK Now Available for Download to Kickstart Robotics Development | NVIDIA Technical Blog
Last month at GTC 2019, NVIDIA presented its Isaac Software Development Kit (SDK), a developer toolbox for accelerating the development and deployment of AI-powered robots. This week at TechCrunch’s Robotics + AI event at UC Berkeley, NVIDIA’s VP of Engineering, Claire Delaunay, was joined on stage with the company’s Kaya and Carter robots to showcase…Powered by Discourse, best viewed with JavaScript enabled"
1619,nvidia-to-host-ai-tech-summit-at-neurips,"Originally published at:			NVIDIA To Host AI Tech Summit at NeurIPS | NVIDIA Technical Blog
At the thirty-second Conference on Neural Information Processing Systems (NeurlPS) in Montreal, Canada, NVIDIA will host the NVIDIA AI Tech Summit, a half-day workshop about how GPU-accelerated computing is transforming the landscape of computational science and AI. We’ll explore the future of GPU computing, give you the latest analysis of NVIDIA’s AI platforms, and demo…Powered by Discourse, best viewed with JavaScript enabled"
1620,rendering-in-real-time-with-spatiotemporal-blue-noise-textures-part-1,"Originally published at:			https://developer.nvidia.com/blog/rendering-in-real-time-with-spatiotemporal-blue-noise-textures-part-1/
Blue noise textures are used in a variety of real time–rendering techniques to hide noise in a perceptually pleasing wayPowered by Discourse, best viewed with JavaScript enabled"
1621,omniverse-top-5-resources-from-gtc-21,"Originally published at:			https://developer.nvidia.com/blog/omniverse-top-5-resources-from-gtc-21/
NVIDIA Omniverse is an open platform built for virtual collaboration and real-time physically accurate simulation.como puedo colavorar?Powered by Discourse, best viewed with JavaScript enabled"
1622,gtc21-top-5-higher-education-and-research-technical-sessions,"Originally published at:			https://developer.nvidia.com/blog/gtc21-top-5-higher-education-and-research-technical-sessions/
Join speakers and panelists considered to be pioneers of AI, technologists, and creators who are re-imagining what is possible in higher education and research.Thanks for sharing such an informative piece of information.
I will definitely get registered for this free event to get some
benefit from the sessions.Powered by Discourse, best viewed with JavaScript enabled"
1623,av1-encoding-and-fruc-video-performance-boosts-and-higher-fidelity-on-the-nvidia-ada-architecture,"Originally published at:			AV1 Encoding and FRUC: Video Performance Boosts and Higher Fidelity on the NVIDIA Ada Architecture | NVIDIA Technical Blog
AV1 Encoding comes to Video Codec SDK 12.0 on NVIDIA Ada, while frame rate up conversion (FRUC) doubles video frame rates with interpolation in Optical Flow 4.0.Hi
Can you tell us what is the underlying algorithms behind nvidia optical flow method.
Thanks
IlanHi, is it possible to use greater than 2X framerate enhancement using FRUC? I.e. generate, say, 2 or more intermediate frames, instead of just 1.You would only need to do the forward / back motion vector estimates once, plus filling in the missing gaps, and use those end points to generate several intermediary frames instead of just 1. I’d love to see a 5x or even 10x enhancement (like 24 FPS → 120 FPS or even 240).thanks!Any anticipated release date for NVOF SDK 4.0? Front page said “in October” ;)Thanks.I would welcome an adjustable interpolation. Having a 0-1 parameter as the distance between the frames. 0 would be completely first frame and 1 the second one. 0.5 would be the interpolated exactly in between.Available Now!!! :)Hi,
We do not provide these details publicly but are happy working under NDA with companies requiring this information when needed. Please reach out to your NVIDIA contact if this is needed.Current implementation of FRUC library support only 2x framerate in one run.
FRUC library is capable to interpolate a frame anywhere between two frames e.g. given two frames at time stamp 0 and 1 you can interpolate frame at any arbitrary time stamp like 0.25, 0.33, 0.75 and so on.
Frame rate of more than 2x can be achieved using FRUC library multiple times on same input stream.  This solution is not optimal and efficient but will allow you to test and check on your end.Powered by Discourse, best viewed with JavaScript enabled"
1624,nvidia-partners-with-hackster-for-ai-at-the-edge-challenge,"Originally published at:			NVIDIA Partners with Hackster for AI at the Edge Challenge | NVIDIA Technical Blog
NVIDIA has partnered with Hackster.io to launch the AI at the Edge Challenge – a competition where developers use the NVIDIA Jetson Nano Developer Kit to build unique and creative projects and earn a chance to win from $100K in prizes. Using the Jetson Nano Developer Kit developers should create a new application or solution…Powered by Discourse, best viewed with JavaScript enabled"
1625,saving-endangered-birds-with-deep-learning-and-gpus,"Originally published at:			Saving Endangered Birds with Deep Learning and GPUs | NVIDIA Technical Blog
Ornithologists study every aspect of birds, including bird songs, flight patterns, physical appearance, and migration patterns – and to do so, they use acoustic sensors and cameras placed in remote areas. Conservation Metrics, a California-based company, is using deep learning accelerated with NVIDIA GPUs to help capture the immense amounts of data that would be…Powered by Discourse, best viewed with JavaScript enabled"
1626,accelerating-hyperscale-data-center-applications-with-tesla-gpus,"Originally published at:			Accelerating Hyperscale Data Center Applications with Tesla GPUs | NVIDIA Technical Blog
This week NVIDIA announced two new Tesla GPUs and a suite of software to help web services companies accelerate huge machine learning workloads and keep up with challenging computational demands. The NVIDIA Tesla M40 lets researchers more quickly innovate and design new deep neural networks for each of the increasing number of applications they want…Powered by Discourse, best viewed with JavaScript enabled"
1627,constructing-cuda-graphs-with-dynamic-parameters,"Originally published at:			Constructing CUDA Graphs with Dynamic Parameters | NVIDIA Technical Blog
Applying CUDA graphs to existing CUDA code is a great way to achieve performance on the latest NVIDIA GPUs, with dynamic parameters and minimal code changes.CUDA Graph is a useful tool to achieve maximum performance on the latest NVIDIA GPUs and this blog introduces one way to make applying CUDA graphs to existing codes easier. If you have any questions or comments, let us know by commenting here or in the github code example repo.Powered by Discourse, best viewed with JavaScript enabled"
1628,solving-ai-inference-challenges-with-nvidia-triton,"Originally published at:			https://developer.nvidia.com/blog/solving-ai-inference-challenges-with-nvidia-triton/
Understand the challenges in AI inference and how Triton Inference Server helps address them. The blog also discusses the recently added features to Triton and new customer success stories.Powered by Discourse, best viewed with JavaScript enabled"
1629,getting-started-with-cuda-graphs,"Originally published at:			https://developer.nvidia.com/blog/cuda-graphs/
The performance of GPU architectures continue to increase with every new generation. Modern GPUs are so fast that, in many cases of interest, the time taken by each GPU operation (e.g. kernel or memory copy) is now measured in microseconds. However, there are overheads associated with the submission of each operation to the GPU –…Hi Alan,Interesting post. I tried using the manual mode with some applications, and noticed the graph also can explore concurrent streams for the kernel nodes automatically. This is an interesting feature that applies beyond short runtime kernels, since I don't need the partitioning work anymore. I am wondering what other optimizations I can expect from such a graph implementation?That’s a very good point. To achieve optimal performance for an application, you need to expose the parallelism inherent to that application as fully as possible. When using the manual method to create a CUDA graph, you do this by explicitly specifying dependencies, and the graph will be built with as many parallel branches as possible given these dependencies. When capturing a graph from CUDA streams, the parallelism will be the same as that of your original stream based code. So if your stream-based code was already fully exposing the available parallelism, the graph would be exactly the same and there would be no benefit from building it manually. But in many cases, the manual approach may end up exposing extra available parallelism (as you found), possibly at the expense of more effort and code disruption (depending on the application). The best approach should be decided on a case by case basis, given the costs and benefits for that specific case.What if I need to modify the kernel's parameter first before calling another kernel? What if I need to call cudaDeviceSynchronize before executing another child graph?Hi Alan, I can't see the benefit in your example, and as I´ve understood the CUDAGraph purpose is to implement a ""circuit"" of kernels as an alternative of dynamic parallel processing. In the source of simpleCUDAGraphs sample it is much more clarify, but still I have not found a sufficiently instructive example. Could you please post a simple example of how to implement a Graph with different kernels and having graphs as nodes aswell as kernells? Thanks.Hi Alan, is graph executor thread-safe? Can I have a centralized executor with multiple threads to submit graphs at the same time? I know graph is not thread-safe.Can the CUDA stream record and capture build a graph that includes an Optix 7 optixLaunch() call? Optix 7 is CUDA compatible, but launches its own kernels, in a user selected stream.Pat, just wanted to let you know that we're working on an answer. We'll get back to you soon.In general, there is scope to apply CUDA graphs to any CUDA compatible API, but doing so relies on the internal functionality of that API only performing activities that are supported by graphs. We are not aware of anyone else having tried this combination so far, so we had to investigate. Unfortunately it looks like OptiX is not currently capturable into a graph. When OptiX launches work, it adds incoming/outgoing events around the work items which are not yet supported by graphs, and this type of “eager” resource assignment needs some rework to be made fully asynchronous. But this question has highlighted to us that we need to bring the different teams together to make this happen. So many thanks for bringing up this issue, and we hope to support interoperability between graphs and Optix in a future release.Is it possible to run the same graph in multiple devices? Will child graphs nodes always run in the same stream?Powered by Discourse, best viewed with JavaScript enabled"
1630,algorithm-turns-any-picture-into-the-work-of-a-famous-artist,"Originally published at:			Algorithm Turns Any Picture Into the Work of a Famous Artist | NVIDIA Technical Blog
Using Caffe and Tesla K40s, a group of German researchers have created an algorithm that could be the most amazing Instagram filter ever conceived: a convolutional neural network that can convert any photograph into a work of fine art. Last week the researchers released a paper detailing how a computer algorithm could be used to…Powered by Discourse, best viewed with JavaScript enabled"
1631,achieving-fp32-accuracy-for-int8-inference-using-quantization-aware-training-with-nvidia-tensorrt,"Originally published at:			https://developer.nvidia.com/blog/achieving-fp32-accuracy-for-int8-inference-using-quantization-aware-training-with-tensorrt/
○	TensorRT is an SDK for high-performance deep learning inference and with TensorRT 8.0, you can import models trained using Quantization Aware Training (QAT) to run inference in INT8 precision without losing FP32 accuracy. QAT significantly reduces compute required and storage overhead for efficient inference.Powered by Discourse, best viewed with JavaScript enabled"
1632,build-train-and-deploy-ai-in-the-cloud-with-nvidia-gpu-cloud,"Originally published at:			Build, Train and Deploy AI in the Cloud with NVIDIA GPU Cloud | NVIDIA Technical Blog
NVIDIA just announced the NVIDIA GPU Cloud (NGC) — a GPU-accelerated cloud platform that makes it easy to get started with the top deep learning frameworks on-premises or on Amazon Web Services. The cloud-based service is available immediately to users of the just-announced Amazon Elastic Compute Cloud (Amazon EC2) P3 instances featuring NVIDIA Tesla V100…Powered by Discourse, best viewed with JavaScript enabled"
1633,icymi-new-and-updated-ai-workflows-announced-at-nvidia-gtc-2023,"Originally published at:			https://developer.nvidia.com/blog/icymi-new-and-updated-ai-workflows-announced-at-gtc-2023/
At NVIDIA GTC 2023, NVIDIA showed how AI workflows can be leveraged to help you accelerate the development of AI solutions to address a range of use cases. AI workflows are cloud-native, packaged reference examples showing how NVIDIA AI frameworks can be used to efficiently build AI solutions such as intelligent virtual assistants, digital fingerprinting…Powered by Discourse, best viewed with JavaScript enabled"
1634,shaping-the-future-of-graphics-with-nvidia-technologies-in-unreal-engine-5,"Originally published at:			https://developer.nvidia.com/blog/shaping-the-future-of-graphics-with-nvidia-technologies-in-unreal-engine-5/
With the launch of Unreal Engine 5, NVIDIA announces support with key RTX technologies for developers to propel their games and experiences to the next level.Powered by Discourse, best viewed with JavaScript enabled"
1635,deep-learning-study-could-spark-new-dinosaur-discoveries,"Originally published at:			https://developer.nvidia.com/blog/deep-learning-study-could-spark-new-dinosaur-discoveries/
Researchers combine CT imaging with deep learning to evaluate dinosaur fossils. The approach could change how paleontologists study ancient remains.Powered by Discourse, best viewed with JavaScript enabled"
1636,gtc-2020-interactive-8k-video-editing-on-rtx-studio-laptops,"GTC 2020 D2S24
Presenters: Tech Demo Team,NVIDIA
Abstract
Blackmagic Design has integrated GPU acceleration for advanced video editing and visual effects,
including several exciting AI-based features in DaVinci Resolve Studio 16 creative app.Video editors can now create high-quality, smooth slow motion without high-speed camera footage
using Speed Warp. They can color and refine facial features using Face Refinement and easily remove
objects from video using Object Removal and much more functions. These effects and tools are
accelerated with NVIDIA CUDA and Tensor Cores empowering editors to produce high quality videos in
seconds instead of hours and iterate faster than ever before. With RTX laptops video professionals can
take advantage of these powerful features wherever they need to be.Watch this session
Join in the conversation below.Hi, Nicely demonstrated. Thanks for the demo. By the way which laptop is used for this video editing and from what storage the 8k raw video is playing? Which RTX card is used for the demo? Thank you.Powered by Discourse, best viewed with JavaScript enabled"
1637,top-3-favorite-graphics-papers,"What are every editors top 3 favorite graphics papers of all time?I will be curious to see if we make it past the 1984 SIGGRAPH proceedings!These guys have so many papers - this is going to be a fun answer - lets see what they say. Thanks for asking !I was 10 at the time - and thought I was supposedly an early reader I somehow doubt I read the Siggraph proceedings back then … :-)1984 was my first SIGGRAPH, memorable for seven papers from Pixar, some seminal. Really, it’s hard to pick just three papers overall!Ray Tracing Jell-O Brand Gelatin, Paul Heckbert, Siggraph 87
Veach Dissertation
Point-Based Rendering (UNC Tech Report)I’m into oldies. Kajiya’s “The Rendering Equation.” Whitted’s “An Improved Illumination Model for Shaded Display.” Veach’s chapter 9, “Multiple Importance Sampling.”  (basically same as Pete’s 1995 reference, only longer)Distribution Ray Tracing 1984
The Rendering Equation 1986
Optimally Combining Sampling Techniques for Monte Carlo Rendering 1995Memory Coherent Ray Tracing (Matt Pharr)Powered by Discourse, best viewed with JavaScript enabled"
1638,getting-started-with-nvidia-nvue-api,"Originally published at:			NVUE API | Cumulus Linux 5.5
Learn how NVIDIA NVUE API automates data center network operations with sample code for Curl commands, Python Code, and NVUE CLI.Powered by Discourse, best viewed with JavaScript enabled"
1639,how-nerfs-helped-me-re-imagine-the-world,"Originally published at:			https://developer.nvidia.com/blog/how-nerfs-helped-me-re-imagine-the-world/
Why have images not evolved past two dimensions yet? Why are we satisfied with century-old technology? What if the technology already exists that’s ready to push the content world forward and only requires a camera? That technology is neural radiance fields (NeRFs). Although this technology was created only a few years ago (2020), the pace…Powered by Discourse, best viewed with JavaScript enabled"
1640,achieving-high-quality-search-and-recommendation-results-with-deepnlp,"Originally published at:			https://developer.nvidia.com/blog/achieving-high-quality-search-and-recommendation-results-with-deepnlp/
Speech and natural language processing (NLP) have become the foundation for most of the AI development in the enterprise today, as textual data represents a significant portion of unstructured content. As consumer internet companies continue to improve the accuracy of conversational AI, search, and recommendation systems, there is an increasing need for processing rich text…Great article! I tried to pull the docker container and got an authorisation error:Would be great if you could fix this.Thanks for your interest. You need to signup for NGC (ngc.nvidia.com) and get the access key before pulling the container.
here are more details:
https://docs.nvidia.com/ngc/ngc-catalog-user-guide/index.html#accessing_registryHope this helps.Powered by Discourse, best viewed with JavaScript enabled"
1641,taking-ai-into-clinical-production-with-monai-deploy,"Originally published at:			https://developer.nvidia.com/blog/taking-ai-into-clinical-production-with-monai-deploy/
MONAI Deploy provides a set of open source tools for developing, packaging, testing, deploying, and running medical AI applications.Powered by Discourse, best viewed with JavaScript enabled"
1642,accelerated-ray-tracing-in-one-weekend-in-cuda,"Thank you for your contribution.  It is greatly appreciated by the CUDA community!I was able to run your code successfully!  On my CPU it takes 30 minutes to generate the image, but with GPU assistance using CUDA I'm down to approx 3 minutes.  That's a really really big deal!  I have a nvidia GeForce 1050 ti graphics card.One last thing in regards to the code.  On the later chapters, the CUDA code uses M_PI constant in the camera.h code.  Please add the following snippet before the camera.h include so there are no build-time errors:#ifndef M_PI#define M_PI 3.14159265358979323846#endifAgain, thanks for your contribution.  I will tinker with the code so I can learn ray tracing better.  Have a great day!-CUDA EducationI'm happy to hear you were successful in translating the code & your suggestion is a good one.  I have found that I need that #define if I try out the code on Windows.  Thanks!Great post!Somehow on my GTX 1050 and CUDA 9.2, my machine freezes completely at the end of the rendering. After a lot of trials I figured it happens when accessing fb on cpu while writing the image to disk. Copying the buffer to a host array resolved the issue for me.Any ideas what could be causing this ?The only thing I can think of is that you somehow removed the cudaDeviceSynchronize() after the render call & before the first read from fb.  That could cause an issue like this.If that wasn't it, suggest you try to find the simplest version of the code that doesn't work (try going back to Chapter 1 to see if that passes) and we could attempt to debug via an Issue on the github repository.Let my try to simplify the code and still reproduce the issue, will open a github issue when I do. Thanks again!Thanks very much for this !! I was hoping someone could give some insight into some issues I'm having in running this using VS 2017 on Windows10. I basically started a blank CUDA 9.1 Runtime project in VS 2017, then copied and pasted the header files and main.cu from the Chapter 12 code. I also selected the VS 2015 v140 platform, and under CUDA C/C++/Device in the Configuration Properties changed Code Generation to ""compute_60,sm_60"" to presumably match my GTX 1080ti. I then ran the debugger, and aside from the Intellisense warnings for the kernel launcher (""<<< >>>"") and not recognizing ""blockidx"", it ran fine. And I sent the output to a .ppm file and dragged that into Gimp and the rendered image looked fine.My problem is that it seems that it never utilized the GTX-1080ti, since the render (10 samples) took about 46 seconds, and Task Manager showed no significant GPU activity with either the 3D or CUDA (or any other) engines. Instead it seemed to bounce between using 2 of my CPU cores (8 - core Ryzen).My other concern is that it never opened a window showing the realtime GPU render as it progressed, only printing the RGB values in the command window.I'm guessing I missed a step in configuring the GPU in Visual Studio. Anyone have any insights? Thanks much.Oh, and if anyone has any additional insight into adding a simple UI to this like ImGUI so I can manually tweak settings during the render I'd be thrilled....I am not sure what could be happening with your VS 2017 project.  If you see CUDA calls in the code it will use the GPU--there's no way around that. I've done something similar and gotten sub-10-second speeds with a GTX 1070.  Maybe debug string output is super-slow?  Make sure you create a Release executable would be one suggestion.Note that there is no GUI for this project.  It only outputs to the console.BINGO !!!! 4.5 seconds !!! It was the Release vs. Debug as you mentioned. DOH !!!  Thanks much.Fantastic!Dear Roger Allen,Thank you very muchfor your excellent implementation or ray tracing/casting in CUDA. Iwas really surprised that you manged to use virtual functions fromand the abstract class hitable to create other classes. You manged toderive hitable_list and sphere classes from the hitable abstractclass. All manuals on CUDA published by NVIDIA claim that it isimpossible to use virtual functions and abstract classes in order tocreate other classes for usage in global kernel functions. Followingyou steps I derived from hitable class other classes, like Plane, Instance (for moving and rotation), Compound (for drawing complexobjects). All of them can be nested in each other.  I even managed toimplement the class Grid for acceleration of computations calledregular grids described in the book Ray Tracing from the Ground up byKeven Suffern.  You employed the global kernel called Create World inorder to fill in GPU memory with objects.   But when the number ofobject was greater than 4000 spheres packed in the acceleration classGrid I received  the mistakeCUDA error = 700 atmain5_add_obj_grid_init.cu:256 'cudaDeviceSynchronize()' (program exited withcode: 99)Then I decided touse unified memory  cudaMallocManaged((void**)&d_world,sizeof(hitable *)); and employed host function to instead of thekernel function create_world and immediately received the errormessage   number of objects=34CUDA error = 700 atmain5_add_obj_grid_init.cu: 270 'cudaDeviceSynchronize()'I have Quadro P4000card with 8 gigabytes of memory. My host device has 32  gigabytes. Iuse Ubuntu operating system and CUDA 10.1Potentially, one canuse host functions to access and modify unified memory and then useglobal kernel functions to process that unified memory . If I amright  my comp has in total 40 gigabytes  of unified memory. I wasable to ray trace even several millions of spheres on my modestlaptop with 8 gigabytes without any CUDA card for this purpose. So ,what was wrong in my case?Best Regards,Valery Pavlov,SpainHi Valery,I am going to have to look closer into my usage of virtual functions and the CUDA documentation and I will try to find some internal experts to help me understand.  It was an oversight and not my intention to go against a CUDA rule.  It looks like I may have stumbled into some undefined behavior.  So, it may well be the case that my usage does not trigger errors, but your more elaborate usage does trigger errors as you seem to have found.Now that I have read some posts like this one, I could see how the second case of creating objects on the host and copying them to the device would lose virtual function information and cause crashes.For debugging your first issue that creates the objects on the device, since you have made significant changes, I think the best plan would be for you to post to the CUDA Programming forum so that others can try to reproduce and investigate the error.Hi Roger I created the following topic: https://devtalk.nvidia.com/...in which I provide the small modification of your code provided by you for chapter 5:Dear all ,I have read a veryinteresting post Accelerated Ray Tracing in One Weekend in CUDA by ByRoger Allen(https://devblogs.nvidia.com....In this post virtual functions were employed to create ray-tracedpictures of spheres. I modified the code provided in Chapter 5 (https://github.com/rogerall...a little bit inorder to  create_world on host instead of device.  In other words Ireplaced __device__ functions with __host__ __device__  functions  in headers ray.h;  hitable.h;  hitable_list.h; sphere.h; and finallyI added host function in main.cu filevoidcreate_world_host(hitable **d_list, hitable **d_world) {*(d_list)= new sphere(vec3(0,0,-1), 0.5);*(d_list+1)= new sphere(vec3(0,-100.5,-1), 100);        *d_world   = new hitable_list(d_list,2);}and used unifiedmemory    // make ourworld of hitables    //but usingunified memory    hitable**d_list;checkCudaErrors(cudaMallocManaged((void **)&d_list,2*sizeof(hitable *)));    hitable**d_world;checkCudaErrors(cudaMallocManaged((void **)&d_world,sizeof(hitable *)));)I found it isimpossible to fill in the unified memory using my host function  create_world_host instead of similar __global__ void create_world . Ihave Quadro P4000 card with Pascal architecture.Why does  usage ofthe host function  create_world_host instead of __global__ voidcreate_world lead to the error : enderinga 1200x600 image in 8x8 blocks.CUDA error = 700 atmain.cu:114 'cudaDeviceSynchronize()'  ????My version of ray.his#ifndef RAYH#define RAYH#include ""vec3.h""class ray{    public:        __host____device__ ray() {}        __host____device__ ray(const vec3& a, const vec3& b) { A = a; B = b;}        __host____device__ vec3 origin() const       { return A; }        __host____device__ vec3 direction() const    { return B; }        __host____device__ vec3 point_at_parameter(float t) const { return A + t*B; }        vec3 A;        vec3 B;};#endifMy version ofhitable.h is#ifndef HITABLEH#define HITABLEH#include ""ray.h""struct hit_record{    float t;    vec3 p;    vec3 normal;};class hitable  {    public:        __host____device__ virtual bool hit(const ray& r, float t_min, floatt_max, hit_record& rec) const = 0;};#endifMy version ofhitable_list.h is: #ifndefHITABLELISTH#define HITABLELISTH#include ""hitable.h""class hitable_list:public hitable  {    public:        __host____device__ hitable_list() {}        __host____device__ hitable_list(hitable **l, int n) {list = l; list_size = n;}        __host____device__ virtual bool hit(const ray& r, float tmin, float tmax,hit_record& rec) const;        hitable**list;        intlist_size;};__host__ __device__bool hitable_list::hit(const ray& r, float t_min, float t_max,hit_record& rec) const {        hit_recordtemp_rec;        boolhit_anything = false;        floatclosest_so_far = t_max;        for (int i =0; i < list_size; i++) {            if(list[i]->hit(r, t_min, closest_so_far, temp_rec)) {hit_anything = true;closest_so_far = temp_rec.t;                rec= temp_rec;            }        }        returnhit_anything;}#endifMy version ofsphere.h is:#ifndef SPHEREH#define SPHEREH#include ""hitable.h""class sphere: publichitable  {    public:        __host____device__ sphere() {}        __host____device__ sphere(vec3 cen, float r) : center(cen), radius(r)  {};        __host____device__ virtual bool hit(const ray& r, float tmin, float tmax,hit_record& rec) const;        vec3 center;        floatradius;};__host__ __device__bool sphere::hit(const ray& r, float t_min, float t_max,hit_record& rec) const {    vec3 oc =r.origin() - center;    float a =dot(r.direction(), r.direction());    float b =dot(oc, r.direction());    float c =dot(oc, oc) - radius*radius;    floatdiscriminant = b*b - a*c;    if (discriminant> 0) {        float temp =(-b - sqrt(discriminant))/a;        if (temp <t_max && temp > t_min) {            rec.t =temp;            rec.p =r.point_at_parameter(rec.t);rec.normal = (rec.p - center) / radius;            returntrue;        }        temp = (-b +sqrt(discriminant)) / a;        if (temp <t_max && temp > t_min) {            rec.t =temp;            rec.p =r.point_at_parameter(rec.t);rec.normal = (rec.p - center) / radius;            returntrue;        }    }    return false;}#endifand finally myversion of main.cu:#include <iostream>#include <time.h>#include <float.h>#include ""vec3.h""#include ""ray.h""#include ""sphere.h""#include""hitable_list.h""// limited versionof checkCudaErrors from helper_cuda.h in CUDA examples#definecheckCudaErrors(val) check_cuda( (val), #val, __FILE__, __LINE__ )voidcheck_cuda(cudaError_t result, char const *const func, const char*const file, int const line) {    if (result) {        std::cerr <<""CUDA error = "" << static_cast<unsigned int="""">(result) << "" at "" <<            file <<"":"" << line << "" '"" << func <<""' \n"";        // Make surewe call CUDA Device Reset before exitingcudaDeviceReset();        exit(99);    }}__device__ vec3color(const ray& r, hitable **world) {    hit_record rec;    if((*world)->hit(r, 0.0, FLT_MAX, rec)) {        return0.5f*vec3(rec.normal.x()+1.0f, rec.normal.y()+1.0f,rec.normal.z()+1.0f);    }    else {        vec3unit_direction = unit_vector(r.direction());        float t =0.5f*(unit_direction.y() + 1.0f);        return(1.0f-t)*vec3(1.0, 1.0, 1.0) + t*vec3(0.5, 0.7, 1.0);    }}__global__ voidrender(vec3 *fb, int max_x, int max_y,  vec3 lower_left_corner, vec3 horizontal, vec3 vertical, vec3origin,  hitable **world) {    int i =threadIdx.x + blockIdx.x * blockDim.x;    int j =threadIdx.y + blockIdx.y * blockDim.y;    if((i >=max_x) || (j >= max_y)) return;    int pixel_index= j*max_x + i;    float u =float(i) / float(max_x);    float v =float(j) / float(max_y);    ray r(origin,lower_left_corner + u*horizontal + v*vertical);    fb[pixel_index]= color(r, world);}__global__ voidcreate_world(hitable **d_list, hitable **d_world) {    if (threadIdx.x== 0 && blockIdx.x == 0) {        *(d_list)  = new sphere(vec3(0,0,-1), 0.5);        *(d_list+1)= new sphere(vec3(0,-100.5,-1), 100);        *d_world   = new hitable_list(d_list,2);    }}voidcreate_world_host(hitable **d_list, hitable **d_world) {        *(d_list)  = new sphere(vec3(0,0,-1), 0.5);        *(d_list+1)= new sphere(vec3(0,-100.5,-1), 100);        *d_world   = new hitable_list(d_list,2);}__global__ voidfree_world(hitable **d_list, hitable **d_world) {    delete*(d_list);    delete*(d_list+1);    delete *d_world;}int main() {    int nx = 1200;    int ny = 600;    int tx = 8;    int ty = 8;    std::cerr <<""Rendering a "" << nx << ""x"" <<ny << "" image "";    std::cerr <<""in "" << tx << ""x"" << ty <<"" blocks.\n"";    int num_pixels =nx*ny;    size_t fb_size =num_pixels*sizeof(vec3);    // allocate FB    vec3 *fb;checkCudaErrors(cudaMallocManaged((void **)&fb, fb_size));    // make ourworld of hitables    //but usingunified memory    hitable**d_list;checkCudaErrors(cudaMallocManaged((void **)&d_list,2*sizeof(hitable *)));    hitable**d_world;checkCudaErrors(cudaMallocManaged((void **)&d_world,sizeof(hitable *)));    //***Creation ofworld on device using unified memory works***//create_world<<<1,1>>>(d_list,d_world);   //***Creation ofworld on host using unified memory fails***//   //create_world_host(d_list,d_world);checkCudaErrors(cudaGetLastError());checkCudaErrors(cudaDeviceSynchronize());    clock_t start,stop;    start = clock();    // Render ourbuffer    dim3blocks(nx/tx+1,ny/ty+1);    dim3threads(tx,ty);    render<<<blocks, threads="""">>>(fb, nx, ny,           vec3(-2.0, -1.0, -1.0),           vec3(4.0, 0.0, 0.0),           vec3(0.0, 2.0, 0.0),           vec3(0.0, 0.0, 0.0),           d_world);checkCudaErrors(cudaGetLastError());checkCudaErrors(cudaDeviceSynchronize());    stop = clock();    doubletimer_seconds = ((double)(stop - start)) / CLOCKS_PER_SEC;    std::cerr <<""took "" << timer_seconds << "" seconds.\n"";    // Output FB asImage    std::cout <<""P3\n"" << nx << "" "" << ny <<""\n255\n"";    for (int j =ny-1; j >= 0; j--) {        for (int i =0; i < nx; i++) {            size_tpixel_index = j*nx + i;            int ir =int(255.99*fb[pixel_index].r());            int ig =int(255.99*fb[pixel_index].g());            int ib =int(255.99*fb[pixel_index].b());std::cout << ir << "" "" << ig << """" << ib << ""\n"";        }    }    // clean upcheckCudaErrors(cudaDeviceSynchronize());free_world<<<1,1>>>(d_list,d_world);checkCudaErrors(cudaGetLastError());checkCudaErrors(cudaFree(d_list));checkCudaErrors(cudaFree(d_world));checkCudaErrors(cudaFree(fb));    // useful forcuda-memcheck --leak-check fullcudaDeviceReset();I'm happy to see this is already getting replies.  Suggest further followups go there for clarity.To clarify, I did discuss a bit with some internal experts and found for certain that my code is not relying on any undefined behavior (UB).I was worried about this statement:  from https://docs.nvidia.com/cud...Passing an object by value could trigger that UB, but since I am passing a pointer to an object that is both created & used on the device, the code works as expected.  Internally, we are also discussing how to best update the programming guide for better clarity.Further discussion should probably go on the devtalk forum post that Valery created in his response to the previous post.Not sure if anyone experienced this but with a higher image resolution (larger fb size) using pixel_index number as the sequence number slows curand_init() to a crawl. I've run into similar problem in the following thread and suggested solution in this comment, suggesting to vary seed instead of sequence. Quoting Cuda toolkit: ""Sequences generated with different seeds usually do not have statistically correlated values"".Yes, indeed, you can get a significant speedup!  See the issue here: https://github.com/rogerall...I noticed in your color function that you don’t handle the issue of thread divergence, do you have any suggestions for minimizing that issue?No, for simplicity I did not do anything special for divergence.  I don’t have any ready suggestions either, sorry.  Perhaps others who are reading can help?Hi Roger,I recently applied to the Senior Content Developer position at NVIDIA.  I worked with you briefly a while back to get the Ray Tracing tutorial running on my machine (and also fix a bug in the code).  I have a plethora of CUDA and Vulkan API tutorials on my website at cudaeducation.com and https://www.youtube.com/channel/UCzpwNg0Ai8zCzbsEtozkfFQ/videosAnyway, I was hoping that you could put in a good word for me since you are the big man on campus.  NVIDIA sorely needs help getting their technology user-friendly and approachable for the masses, and I’m your guy to do so.  I’ve put in serious time and effort creating these tutorials, so I’m hoping that will suffice.  Not sure the best way to communicate my name as I don’t want it publicly available here, but you can reach out to me at cudaeducation@gmail.com.  I am based in the United States.  Thanks for your time and consideration.Also I hope the moderators don’t have a problem with this post.  I’m just trying to reach out to all my “links”.Powered by Discourse, best viewed with JavaScript enabled"
1643,nvidia-gtc-a-complete-overview-of-nsight-developer-tools,"Originally published at:			NVIDIA GTC: A Complete Overview of Nsight Developer Tools | NVIDIA Technical Blog
Read a complete overview of the Nsight suite of developer tools with new features and capabilities.Powered by Discourse, best viewed with JavaScript enabled"
1644,nvidia-highlights-unity-tutorial,"Originally published at:			NVIDIA Highlights Unity Tutorial | NVIDIA Technical Blog
  NVIDIA Highlights enables players to capture in-game moments automatically based on events occurring during gameplay. Highlights represents a key feature in NVIDIA’s ShadowPlay automated screen capture software. (You can check out more information about ShadowPlay here). This blog introduces basic concepts a developer needs to implement the NVIDIA Highlights plugin for the Unity game…Very interesting feature but not working, neither in Unity 2017.4 nor 2018.1... Everything looks fine and messages pops up saying this or that video was captured but nothing there! ...Unless it is a GeForce Experience version issue (I'm using 3.14.0.139 with driver 397.93).That's very interesting, but throws an error on startup -1001 GFGSDK_ERR_INVALID_PARAMETERS, also tried with Sample scene.""Invalid handle pointer provided, or no app name provided.""What is wrong with that?Powered by Discourse, best viewed with JavaScript enabled"
1645,accelerated-data-analytics-a-guide-to-data-visualization-with-rapids,"Originally published at:			https://developer.nvidia.com/blog/accelerated-data-analytics-a-guide-to-data-visualization-with-rapids/
Learn how to use RAPIDS to integrate powerful visualizations into your workflows.Powered by Discourse, best viewed with JavaScript enabled"
1646,drink-up-beer-tasting-robot-uses-ai-to-assess-quality,"Originally published at:			https://developer.nvidia.com/blog/drink-up-beer-tasting-robot-uses-ai-to-assess-quality/
Can a beer tasting robot do a better job than humans in judging a beer? Researchers in Australia developed a robot that uses machine learning to assess the quality of the beer. “RoboBEER can handle repetitive sampling and does not suffer from fatigue as human panelists do, which helps to obtain more consistent, representative, and…Powered by Discourse, best viewed with JavaScript enabled"
1647,ansel-coffee-break-what-is-nvidia-ansel-5-20-minutes,"Originally published at:			Ansel Coffee Break: What is NVIDIA Ansel? (5:20 minutes) | NVIDIA Technical Blog
Learn how CD Projekt Red integrated 150 lines of code, and wound up with more than 2 million Ansel photos of The Witcher III: Wild Hunt published! In this video, we explain the value of Ansel, and describes the product’s benefits to players.   Five Things to Remember: Ansel was created because it became clear…Powered by Discourse, best viewed with JavaScript enabled"
1648,gtc-2020-rtx-accelerated-raytracing-with-optix-7,"GTC 2020 S21888
Presenters: Tony Kanell,NVIDIA; Ingo Wald,NVIDIA
Abstract
This session is for programmers interested in using OptiX to write RTX-accelerated raytracing applications. We’ll start with the general concepts behind the OptiX 7 API, and then build up to more advanced topics such as how to properly use, and optimize for, the hardware raytracing cores of modern GPUs.Watch this session
Join in the conversation below.Great talk!
Excited to start writing my first OptiX ray-tracer! :)Few feebacks/thoughts though:The cuts in the video are really garring and make it more difficult to follow - looks like either the un-cut version should have been published instead, however longer it might be, or the talk should have been split into 2.For the SBT section, given it’s multi-dimentional structure/nature, seems like some 2D/3D disgramming would have gone a long way into clarifying that structure in the presentation. The conveience of information in the presentation appears like a serialisation of a multidimentional story into a single dimentional stream of verbal and textual descriptions - that rarely works well for multi-dimentional stories.I have been working as a developer in VFX for many years, and shader binding is always a complex multi-dimentional story. Packing all these dimensions into a single dimentional array of structurs, while surely efficient, leaves a lot to be desired from a developer perspective. I don’t know how OptiX 6 was like, as I haven’t studied it, but the whole packing/unpacking story seems like a very low-level implementation detail that should never have been exposed the way it is. The combination of having to construct these multi-dimentional story manually and coordinate it against it’s indexing story on the shader side, all while accounting for a an implicit formula that’s embedded in the API, screams of “abstraction leakage” to me, if I ever saw one. The likelihood that any developer would get all of it correct is very low. The prevalance of confusions and bugs surrounding this is very telling, and should be considered strong evidence for there being a design issue there.
A better design might have been to provide a much wider-surface-area API with per-dimension functionality for describing the multi-dimentional graph-like structure of the binding, with some identifiers per-dimention. The actual construction of the packed-form of the data-structure would then be done internally to the API. On the querying side, one could imagine then extracting the multi-dimentional identifiers out of the hit-data, then providing it to some shader-domain utility procedures that internally generate the proper “final” indices/offsets, and just produce the actual relevant data for the shaders as their output. Such higher-level abstraction would not be taking away any level of flexibility or control from developers. If anything, it would expose better visibility into that control more explicitly. Performance cost could proably be made easilly acceptable.
The currently existing approach could still be left accessible for “advanced use-cases” or as a performance optimisation choice, but it seems wrong for it to be the default. I can’t imagine any production-level solution using it as-is ad-hock like that. My guess would be that they would all end-up devising some abstraction layer on-top of it, of the like that I’ve described here (perhapse that’s what OWL is…?)Just my un-enlightened 2 cents on that :)Powered by Discourse, best viewed with JavaScript enabled"
1649,directx-12-ultimate-preview-driver-enables-dxr-1-1-mesh-shaders-variable-rate-shading-and-more,"Originally published at:			DirectX 12 Ultimate Preview Driver Enables DXR 1.1, Mesh Shaders, Variable Rate Shading, and More | NVIDIA Technical Blog
To get the most out of DirectX 12 Ultimate, we’ve provided early public access to the NVIDIA DirectX Ultimate Developer Preview Driver [450.82] on our DirextX developer page, for both NVIDIA GeForce and NVIDIA Quadro. This new driver will let you go hands-on with DirectX 12 Ultimate’s exciting new features:  DirectX Raytracing 1.1 DXR traces…Powered by Discourse, best viewed with JavaScript enabled"
1650,essential-ray-tracing-sdks-for-game-and-professional-development,"Originally published at:			https://developer.nvidia.com/blog/essential-ray-tracing-sdks-for-game-and-professional-development/
Game and professional visualization developers need the best tools to create the best games and real-time interactive content. Read this article to find out what NVIDIA technologies will provide developers optimal real time ray tracing within their workflows.Any word about when RTXGI plugin for UE4 (4.25 - 4.27) will become public ?Powered by Discourse, best viewed with JavaScript enabled"
1651,calculating-and-synchronizing-time-with-the-precision-timing-protocol-on-the-nvidia-spectrum-switch,"Originally published at:			Calculating and Synchronizing Time with the Precision Timing Protocol on the NVIDIA Spectrum Switch | NVIDIA Technical Blog
The NVIDIA Spectrum switch supports accurate and scalable Precision Timing Protocol (PTP).Powered by Discourse, best viewed with JavaScript enabled"
1652,nvidia-clara-agx-sdk-3-0-goes-public-and-includes-new-application-container,"Originally published at:			https://developer.nvidia.com/blog/nvidia-clara-agx-sdk-3-0-goes-public-and-includes-new-application-container/
The Clara AGX SDK runs on the NVIDIA Jetson and Clara AGX platform and provides developers with capabilities to build end-to-end streaming workflows for medical imaging.Powered by Discourse, best viewed with JavaScript enabled"
1653,fast-ai-assisted-annotation-and-transfer-learning-powered-by-the-clara-train-sdk,"Originally published at:			Fast AI Assisted Annotation and Transfer Learning Powered by the Clara Train SDK | NVIDIA Technical Blog
The growing volume of clinical data in medical imaging slows down identification and analysis of specific features in an image. This reduces the annotation speed at which radiologists and imaging technicians capture, screen, and diagnose patient data. The demand for artificial intelligence in medical image analysis has drastically grown in the last few years. AI-assisted…Hello! Actually, I am trying to reproduce the above blog at my end. I am able to run example.cpp successfully after making some changes to the code that was shared in this link (https://docs.nvidia.com/cla... and was able to get this output shown in the attachment.Can you please let me know how does MITK interact with AIAA or is there any beginner level guide for us to kind of understand how this interaction works? Or is there any tutorial end-end process? From loading images in MITK to running annotation server etc.Currently, I don't have a clear idea as to what the flow is?You can watch this video to see how to add new segmentation in MITKhttps://drive.google.com/fi...For more details on MITK workbench can be found at:http://docs.mitk.org/2018.0...You can find NVIDIA plugin in http://mitk.org/wiki/MITK_R...Hello @sachidanandalle:disqus - Thanks for the response. I have installed the MITK workbench in my desktop. We have annotation server running in remote server. As an end-user, I would like to seek your inputs on few things. Can you please help?1) Once I establish connection with the AIAA in MITK, I should be able to see the data folder under ""Data Manager"" tab. Let's say I have data segregated as training datasets and validation datasets. So what do I do from now on?Is it like I open an image file and mark certain points/annotate labels for certain images? Which part of this is automated?Once I do this annotation as shown in the video above, what happens? Can you please help us understand how does this work after loading images in MITK.Data Manager tab is nothing related to connection between AIAA and MITK.  Data Manager concept can be read over MITK User guide.For annotation, using MITK kind of tool, you pick some points as input labels for annotation.. you start with some random labels, and based on the result, you keep iterating over the different slices to complete the annotation...it's like this, if you pick the good slices/points, you are certain to see quick results and less manual work in defining the full segmentation over the organ.  it's not automation here.. we can call it as semi supervised processyou can go through the video carefully, how to load the image, how to create a label for segmentation and then shift+click points and then call the annotation actionHello @sachidanandalle:disqus - Thanks for your response. This certainly helps. I believe call to AIAA server is made each time we ""confirm points"".So, radiologist inspects the patient image and marks certain points as input labels for annotation. In this case, we saw these input points were used to identify ""spleen"", which is the label for this image. So Radiologist repeats this process for a significant portion of their dataset which can be used as training dataset. Rest of the unseen images can then be used for prediction? Is my understanding of the flow right? Radiologist may decide to repeat this procedure for creating his/her own training dataset or he/she can use NVIDIA's spleen model as well. Could you kindly correct my understanding if it's not right?In addition, I have a query regarding Nvidia-ai-assisted annotation client (code error) ?Is it the right forum to ask?https://drive.google.com/op...Possibly a good video to show-case how to select extreme points and get better results for annotation.Nvidia-ai-assisted annotation client error-code are little generic; let me know if you are facing any issue regarding.Hi Sachidanand,Can you please help me understand what does ""Nvidia SmartPoly"" does? I am not sure whether I am using it the right way. As attached in the screenshot, I am able to see some colored slices in 3D view but unable to interpret it. Can you share any resource/example on how to interpret or kindly guide me to use this the right way?https://uploads.disquscdn.c...Its like any other polygon editing.. you switch 2D and click on Nvidia SmartPoly action.This will show the polygon points for the segmented region.  If something is wrong (say for some slice) then you can do manual correction by dragging the point correctly.I followed tutorial_cxr for training classification model. But, but I'm failing at running tlt-train command.2019-05-22 02:08:14.053192: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1115] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:1 with 9944 MB memory) -> physical GPU (device: 1, name: GeForce RTX 2080 Ti, pci bus id: 0000:03:00.0, compute capability: 7.5)Traceback (most recent call last):  File ""/usr/lib/python3.5/runpy.py"", line 184, in _run_module_as_main    ""__main__"", mod_spec)  File ""/usr/lib/python3.5/runpy.py"", line 85, in _run_code    exec(code, run_globals)  File ""common/scripts/train.py"", line 25, in <module>  File ""common/scripts/train.py"", line 20, in main  File ""classification/scripts/train_classification.py"", line 210, in train_classificationTypeError: 'NoneType' object is not subscriptableFaced similar issue when I ran tutorial_brats.Traceback (most recent call last):=========]  train_loss: 0.9822  train_dice_et: 0.0073  train_dice_tc: 0.0178  train_dice_wt: 0.0063  train_dice: 0.0105  time: 32.52s          File ""/usr/lib/python3.5/runpy.py"", line 184, in _run_module_as_main    ""__main__"", mod_spec)  File ""/usr/lib/python3.5/runpy.py"", line 85, in _run_code    exec(code, run_globals)  File ""common/scripts/train.py"", line 25, in <module>  File ""common/scripts/train.py"", line 18, in main  File ""segmentation/scripts/train_segmentation.py"", line 316, in train_segmentation  File ""common/trainers/fitter.py"", line 500, in standard_fit  File ""common/metrics/val_classes.py"", line 165, in getTypeError: object of type 'NoneType' has no len()Thanks in Advance.TypeError: ‘NoneType’ object is not subscriptableIn general, the error means that you attempted to index an object that doesn’t have that functionality. You are trying to subscript an object which you think is a list or dict, but actually is None. NoneType is the type of the None object which represents a lack of value, for example, a function that does not explicitly return a value will return None. ‘NoneType’ object is not subscriptable is the one thrown by python when you use the square bracket notation object[key] where an object doesn’t define the getitem method .A subscriptable object is any object that implements the getitem special method (think lists, dictionaries). It is an object that records the operations done to it and it can store them as a “script” which can be replayed. You are trying to subscript an object which you think is a list or dict, but actually is None. NoneType is the type of the None object which represents a lack of value, for example, a function that does not explicitly return a value will return None.‘NoneType’ object is not subscriptable is the one thrown by python when you use the square bracket notation object[key] where an object doesn’t define the getitem method . You might have noticed that the method sort() that only modify the list have no return value printed – they return the default None. This is a design principle for all mutable data structures in Python.Powered by Discourse, best viewed with JavaScript enabled"
1654,inception-spotlight-subtle-medical-s-ai-based-medical-imaging-software-receives-fda-clearance,"Originally published at:			Inception Spotlight: Subtle Medical’s AI-based Medical Imaging Software Receives FDA Clearance | NVIDIA Technical Blog
To help denoise and enhance the resolution of medical images from existing scanners, Menlo Park, California-based Subtle Medical, a member of NVIDIA’s startup accelerator, Inception,  announced this week approval from the U.S. Food and Drug Administration to market their SubtleMR imaging processing software.  “We are pleased to receive FDA clearance for SubtleMR, and we look forward…Powered by Discourse, best viewed with JavaScript enabled"
1655,accelerating-ai-and-ml-workflows-with-amazon-sagemaker-and-nvidia-ngc,"Originally published at:			Accelerating AI and ML Workflows with Amazon SageMaker and NVIDIA NGC | NVIDIA Technical Blog
AI is going mainstream and is quickly becoming pervasive in every industry—from autonomous vehicles to drug discovery. However, developing and deploying AI applications is a challenging endeavor. The process requires building a scalable infrastructure by combining hardware, software, and intricate workflows, which can be time-consuming as well as error-prone. To accelerate the end-to-end AI workflow,…Powered by Discourse, best viewed with JavaScript enabled"
1656,finite-difference-methods-in-cuda-fortran-part-1,"Originally published at:			https://developer.nvidia.com/blog/finite-difference-methods-cuda-fortran-part-1/
CUDA Fortran for Scientists and Engineers shows how high-performance application developers can leverage the power of GPUs using Fortran. In the last CUDA Fortran post we investigated how shared memory can be used to optimize a matrix transpose, achieving roughly an order of magnitude improvement in effective bandwidth by using shared memory to coalesce global…Powered by Discourse, best viewed with JavaScript enabled"
1657,advancing-autonomous-valet-functionality-with-parking-sign-assist,"Originally published at:			https://developer.nvidia.com/blog/advancing-autonomous-valet-functionality-with-parking-sign-assist/
Autonomous parking involves complex perception algorithms. We present an AI-based parking sign assist system relying on live perception that can fuse to map systems.We are now expanding the work to support/cover more signs (especially supplemental signs) in more areas. We are introducing OCR in our system to better handle complex text information on signs.If you have any questions, suggestions or comments, feel free to ping us. Thanks.Powered by Discourse, best viewed with JavaScript enabled"
1658,nvidia-announces-nsight-graphics-2018-6,"Originally published at:			NVIDIA Announces Nsight Graphics 2018.6 | NVIDIA Technical Blog
NVIDIA is proud to announce Nsight Graphics 2018.6! In this release, we finalized our Linux support for full release, revamped the user documentation, added full Windows Redstone 5 API support, and added support for 15 new Vulkan extensions. Nsight Graphics for Linux has been promoted from beta to full release. This release supports Vulkan and…Powered by Discourse, best viewed with JavaScript enabled"
1659,what-is-cugraph-dgl,"Thanks for answering my other question.
I have another newbie question what is cugraph-dgl?cugraph-dglcugraph-dgl is particularly useful for graph analytics tasks that require both graph operations and GNN modeling, such as node classification, link prediction, and graph classification. The combination of efficient graph operations in cuGraph and scalable GNN modeling in DGL allows for fast and accurate training of GNNs on large-scale graph data.Overall, cugraph-dgl is a powerful tool for graph analytics and GNN modeling on GPU-accelerated hardware, providing a comprehensive solution for graph-based machine learning applications.Thanks !Thanks, I shall have to read up on that - thanks.This topic was automatically closed 14 days after the last reply. New replies are no longer allowed.Powered by Discourse, best viewed with JavaScript enabled"
1660,deep-learning-for-computer-vision-with-caffe-and-cudnn,"Originally published at:			https://developer.nvidia.com/blog/deep-learning-computer-vision-caffe-cudnn/
Deep learning models are making great strides in research papers and industrial deployments alike, but it’s helpful to have a guide and toolkit to join this frontier. This post serves to orient researchers, engineers, and machine learning practitioners on how to incorporate deep learning into their own work. This orientation pairs an introduction to model structure…HiIs this work with  Jetson TK1Caffe runs on the Jetson TK1. See this blog post guide by Pete Warden: http://petewarden.com/2014/...What is the difference between the first set of grayscale looking gabors and the second set of color looking gabors in Figure 3? Is the color supposed to represent sensitivity of those filters for those color difference? Does the same apply to higher visualization of the network, or does color in higher layers encode orientation sensitivity?The feature extraction and visualization link appears to be deadFigure 3 shows the filter weights for the first layer so these filters are interpretable as image patches representing what the filter responds to most whether oriented edges, colors, or so on. In this model certain layers' filters are learned in two separate groups and as it happens this makes the first layer learn sets of grayscale edge features and color features. The higher layers are not so interpretable due to the nonlinearity of the network but research continues. See http://cs231n.github.io/und... for a summary of some approaches.Fixed now. Thanks!how I can know the kernels formula or the kernels that computed for the trained vesion.Lets say I am just interested in trained filtering kernels, is there anyways that I can have them?Powered by Discourse, best viewed with JavaScript enabled"
1661,nvidia-bluefield-european-hackathon-fuels-data-center-innovation-with-pioneering-dpu-based-applications-demonstrations,"Originally published at:			https://developer.nvidia.com/blog/nvidia-bluefield-european-hackathon-fuels-data-center-innovation-with-pioneering-dpu-based-applications-demonstrations/
At NVIDIA where non-stop innovation is our culture, we are hosting a global series of regional Data Processing Unit (DPU) software hackathons over the next 12 months, aimed at advancing research and development in data center and AI technologies.Powered by Discourse, best viewed with JavaScript enabled"
1662,new-on-ngc-security-reports-latest-containers-for-pytorch-tensorflow-hpc-and-more,"Originally published at:			https://developer.nvidia.com/blog/new-on-ngc-security-reports-latest-containers-for-pytorch-tensorflow-hpc-and-more/
This month the NGC catalog added new containers, model resumes, container security scan reports, and more to help identify and deploy AI software faster.Powered by Discourse, best viewed with JavaScript enabled"
1663,developing-end-to-end-real-time-applications-with-the-nvidia-clara-agx-developer-kit,"Originally published at:			https://developer.nvidia.com/blog/developing-end-to-end-real-time-applications-with-clara-agx-dev-kit/
The NVIDIA Clara AGX development kit with the us4R ultrasound development system makes it possible to quickly develop and test a real-time AI processing system for ultrasound imaging.The Clara AGX team greatly enjoyed working with us4us on this collaboration. If you’re interested in developing ultrasound AI applications for research or production and are interested in the us4r-lite + AGX real-time systems, please reach out. We’d be happy to work with you.Powered by Discourse, best viewed with JavaScript enabled"
1664,the-cuda-team-is-back-and-ready-to-answer-questions-live-now,"Thanks for joining us - please post your CUDA questions and the team will be happy to answer.Powered by Discourse, best viewed with JavaScript enabled"
1665,explainer-what-is-conversational-ai,"Originally published at:			What Is Conversational AI? | NVIDIA Blog
Real-time natural language understanding will transform how we interact with intelligent machines and applications.Powered by Discourse, best viewed with JavaScript enabled"
1666,the-fluid-dynamics-revolution-driven-by-gpu-acceleration,"Originally published at:			https://developer.nvidia.com/blog/the-fluid-dynamics-revolution-driven-by-gpu-acceleration/
The end of 2021 and beginning of 2022 saw the two largest commercial CFD tool vendors, Ansys and Siemens, both launch versions of their flagship CFD tools with support for GPU acceleration.Interesting !
For a fair comparison, it would be great to know about the precise amount of CPU cores and CPU references for both CFD tools.
Here I read for Star-CCM+ that you compare A100 GPU over “100 cores of CPU” (I assume Intel Platinum 8380), whereas you compare to 80 Intel Xeon CPU cores (which CPU model ?) for Fluent.
Could you provide more details about this ?Thanks !CharlesHello Charles,
The Intel Xeon 80 cores referenced is Xeon Platinum 8380. AMD Rome is EPYC 7742 and AMD Milan is EPYC 7763.Baskar.I have a question about CFD, does anyone know which CFD software is compatible with the jetson AGX Xavier module?Powered by Discourse, best viewed with JavaScript enabled"
1667,reducing-power-plant-greenhouse-gasses-using-ai-and-digital-twins,"Originally published at:			https://developer.nvidia.com/blog/reducing-power-plant-greenhouse-gasses-using-ai-and-digital-twins/
Learn how the physics-informed machine learning framework, NVIDIA Modulus, is being used to develop power plant digital twins that can help move towards net-zero carbon emissions.Is there code for this that can be share? IPowered by Discourse, best viewed with JavaScript enabled"
1668,using-compute-or-meshlets-with-accelerationstructureindirectbuild-to-implement-continuous-lod-via-displacement-maps,"Hi Eric, I asked your colleague at Nvidia, Christoph Kubisch a while back about how one might build BLASes entirely on the GPU as an output from a previous stage, such as a meshlet (or compute) shader, and he suggested it may be possible to do so via the accelerationStructureIndirectBuild Vulkan extension.Sadly, it is not supposed by Nvidia GPUs yet:Vulkan hardware capability database.Does Nvidia have any plans to support this extension any time soon? If so, would it be restricted to a certain generation of RTX cards? i.e. Will it require Lovelace or Ampere, or could it be packported to Turing.I was hoping to use such an indirect method of generating new meshes every frame programmatically, then be able to trace rays against them in order to exploit ray-triangle intersection hardware (instead of custom intersection shaders which are an order of magnitude slower) in order to do something akin to the recent displacement mapping paper by Adobe. This would enable a continuous LOD path-tracer without requiring many tens of gigabytes of VRAM and pre-generated LODs stored in triangle meshes / BLASes:Théo Thonat François Beaune Xin Sun Nathan Carr Tamy Boubekeur ACM Transactions on Graphics (SIGGRAPH Asia 2021)Also, related question re: using Async Compute to generate / update BLASes, as mentioned in the Best Practices doc here:Optimize your use of NVIDIA RTX with these in-depth ray tracing tips.Are there any code samples illustrating how best to do animate or generate triangle meshes meant to be ray traced using compute? The synchronization code in Vulkan is a bit complex and a reference implementation would be very helpful to me. Thanks!Great question! We are aware of this use case but cannot comment on future releases.Powered by Discourse, best viewed with JavaScript enabled"
1669,multilingual-and-code-switched-automatic-speech-recognition-with-nvidia-nemo,"Originally published at:			https://developer.nvidia.com/blog/multilingual-and-code-switched-automatic-speech-recognition-with-nvidia-nemo/
Multilingual automatic speech recognition (ASR) models have gained significant interest because of their ability to transcribe speech in more than one language. This is fueled by the growing multilingual communities as well as by the need to reduce complexity. You only need one model to handle multiple languages. This post explains how to use pretrained…Powered by Discourse, best viewed with JavaScript enabled"
1670,isc-2020-boosting-performance-and-utilization-with-multi-instance-gpu,"ISC 2020 disc01
Presenters: DemoTeam, NVIDIA
Abstract
Multi-Instance GPU (MIG) on the NVIDIA A100 Tensor Core GPU can guarantee performance for up to seven jobs running concurrently on the same GPU—and each GPU instance is fully isolated with its own compute, memory, and bandwidth. This unique capability of the A100 GPU offers the right-sized GPU for every job and maximizes data center utilization. This demo shows inference performance on a single slice of MIG and then scales linearly across the entire A100.Watch this session
Join in the conversation below.Powered by Discourse, best viewed with JavaScript enabled"
1671,visualizing-depth-precision,"Originally published at:			https://developer.nvidia.com/blog/visualizing-depth-precision/
Discussion and diagrams to help you understand how nonlinear depth mapping works in different situations, intuitively and visually.Powered by Discourse, best viewed with JavaScript enabled"
1672,i-am-bit-confused-about-discriminator,"How are you utilizing the Discriminator in your work? Does it receive randomly rendered outputs for fake signal and real images for real signal, or does it receive the same position rendered output each time for fake signal?Yes, we’re utilizing the discriminator in GET3D. At every training iteration, it receives a rendered image (rendered at a random camera position, which is sampled from the training dataset) and classifies whether this image is real or fake. The real and fake images are not rendered in the same camera position, all of the images & cameras are randomly sampled from the training setThank You !This topic was automatically closed 14 days after the last reply. New replies are no longer allowed.Powered by Discourse, best viewed with JavaScript enabled"
1673,jetson-project-of-the-month-dragon-eye-an-electronic-glider-race-judging-assistant,"Originally published at:			https://developer.nvidia.com/blog/jetson-project-of-the-month-dragon-eye-an-electronic-glider-race-judging-assistant/
The project, which runs on an NVIDIA Jetson Nano Developer Kit, helps count completed laps of a radio controlled slope glider on a course.Powered by Discourse, best viewed with JavaScript enabled"
1674,introducing-low-level-gpu-virtual-memory-management,"If you could further elaborate in what way do they interact with the OS, it would be incredible. We could try to fine-tune our OS to try and reduce this jittering. We’re currently looking at such solutions, but we’re doing so blindly.So, the ioctls involved on your platform in cuMemSetAccess (and in this case cuMemUnmap, as it internally calls cuMemSetAccess(PROT_NONE) to unmap the memory) are the UVM ioctls, which is open source (look for UvmMapExternalMemory when extracting the nvidia display driver package, the full source of this part of the kernel mode driver is readily available), so you’re free to inspect these paths yourself.  Unfortunately, this path in particular ends up jumping into the proprietary binary blob, for which there’s not much I can give insight on in a public forum.If this was an issue in our actual use-case, I would expect to see jittering in various other ioctls, not only these three.It really depends on what other ioctls you’re looking at, as some ioctls have different locking semantics than others (some have read/write semantics, others are plain exclusive).  The fact that you said nvidia-smi makes your application slower is an indication this could possibly be your issue, but I can’t be sure without looking at your use case directly, and/or seeing a timeline profile.You had asked about paid consulting before, and that might be the direction you might want to take in order to have someone dedicated to look into your particular use case and see what we can do to help.  I’ve sent you a direct message with a contact that you can use to describe your problem in more detail with a representative more suited to guide your application development.  I’ll also be in touch with them and see if we can’t resolve your issue so we can hopefully post the solution for others to enjoy.Hey Cory,
Sorry for the late response. You’ve been extremely helpful. I’ll contact the representative.— OmriHey @jmak, I reached out but received some error from your exchange server. It couldn’t be delivered to some specific mail address (because it’s restricted). Not sure if my email arrived properly.Hi!Is there support for these APIs in Windows systems as well? I’m having trouble with mine.
I’m using Windows Server 2022 with a single Tesla T4 GPU and NVIDIA driver version 516.01 installed.At first I saw that the API cuMemAddressReserve succeeds but returns the address 0x0.
I then encountered the device attribute CU_DEVICE_ATTRIBUTE_VIRTUAL_ADDRESS_MANAGEMENT_SUPPORTED and querying the device for this attribute returned 0 as if this is not supported by the GPU.As mentioned above, I’m using a Tesla T4 GPU which should support this feature.
I even verified that querying the same attribute for the same GPU type in a Linux machine returns 1.I haven’t found any note that these APIs do not support Windows, and it also seems that the memMapIPCDrv CUDA sample supports Windows.I wonder if this is something related to Windows or the GPU itself, and if I can somehow add support for this in my system.Thanks in advance!Hi razrotenberg!Is there support for these APIs in Windows systems as well?Yes, but unfortunately only for WDDM driver model GPUs, which your Tesla T4 being a Tesla card only supports TCC driver model IIRC (more information about WDDM/TCC driver model can be found in the link below).  This feature is supported on this GPU on native Linux platforms though if that’s an option for you.  As to when or if this feature will be supported on TCC, I cannot say at this time, but I can say we’re actively looking into some options!NVIDIA-SMI Documentation - Look for the Driver Model section.Hope this helps, good luck!Thanks @Cory.Perry for the super fast answer. Much appreciated!.Is there any place where I could see which GPUs support the WDDM driver model? Is there a general rule or should I check per GPU type?
From peeking at the nvidia-smi documentation you sent, it seems that TCC is more for compute-related GPUs while WDDM is more for graphics. Does it mean that any data-center GPU will not support the WDDM driver model?
It also seems that WDDM driver model has some disadvantages compared to TCC in performance.I really hope you’ll manage to add support for these amazing features to the TCC driver model as well.Thanks again and I’ll be waiting for any news regarding this in the future!Thanks @Cory.Perry for the super fast answer. Much appreciated!.No worries, happy to help!Is there any place where I could see which GPUs support the WDDM driver model?If you look at the nvidia-smi output, you can see the current driver model.  With admin priviledges you can change the driver model on some GPUs, but IIRC all “Tesla” branded models do not support changing the driver model, but I don’t recall if that’s entirely accurate.  I do know Quadro GPUs support switching driver models, and I believe drivers today require the GPU to have a display head (ability to connect to a screen, though an actual connection to a screen is not required) in order to use WDDM.It also seems that WDDM driver model has some disadvantages compared to TCC in performance.Most of the documentation surrounding TCC and WDDM touting lower performance are typically fairly old in my experience.  Current drivers in WDDM can achieve comparative performance on par with TCC, and newer CUDA features like WSL support, these VMM apis, and many many more are only available on WDDM.  For more up-to-date information on performance of WDDM, check out this other blog post by one of our lead CUDA Windows engineers, linked below!  This blog post is mostly related to WSL, but there is information here about WDDM performance as well, and much of the performance benefits (and more) apply to native Windows WDDM as well.  That all said, WDDM is a very different driver model from what most are used to with TCC and native Linux, and there are some caveats and pitfalls that one might run into.  Most of these issues fall outside our CUDA Programming Model as defined (like buffering launches until synchronization rather than immediately).  See the programming guide below for more information.  I believe there’s also a few GTC presentations that go over WDDM performance tips as well that might be beneficial in this regard.I really hope you’ll manage to add support for these amazing features to the TCC driver model as well.Keep an eye out :)Hope this helps, good luck and let us know if you have any other questions!Hey Cory!I am testing VMM random copy performance, and get in a strange performance problem when  I initialize requestedHandleTypes in CUmemAllocationProp  with CU_MEM_HANDLE_TYPE_POSIX_FILE_DESCRIPTOR .Considering we have two array, I want to read the src array randomly and copy to the dst array.The testing kernel looks like the following:And src, dst are  1GB 1D arrays allocated by vmm api. Say their CUmemAllocationProp is prop, when intializing prop I set:Case1: prop.requestedHandleTypes = 0
Case2: prop.requestedHandleTypes = CU_MEM_HANDLE_TYPE_POSIX_FILE_DESCRIPTORI find that Case1 performance is better than Case2 4 times.  But I think they should have same performance. So my questions are:Hi User 2944419175!I am guessing you don’t hit the same issue with sequential copies?  And when you run your random copy through one of our profilers, my guess is your TLB miss  counters is unusually high? If so, then yeah, this is unfortunately a known issue with allocations that need to be importable with other user mode drivers like Vulkan.  Some of these require a smaller page size than cuda’s default internal page size, and due some driver limitations we don’t support being able to use different page sizes with different mappings of the same allocation.  This means CUDA is forced to map with a smaller page size, which puts more pressure on the gpu’s TLB.The good news is, this should be fixed in an upcoming driver update very soon, so look forward to it!  Hope this helps, good luck!Hey, thanks for you fast answer, much appreciated!I am guessing you don’t hit the same issue with sequential copies?Yes, this problem will not occur with sequential copies.And when you run your random copy through one of our profilers, my guess is your TLB miss counters is unusually high?I see nvprof document, but I didn’t find some metrics about TLB miss counter.  And I do a pointer chasing to test access latency , comfirm that TLB pressure is actually higher in Case2, and result shows that the first access (every cache miss) VMM with CU_MEM_HANDLE_TYPE_POSIX_FILE_DESCRIPTOR seems to have higher latency than VMM without CU_MEM_HANDLE_TYPE_POSIX_FILE_DESCRIPTOR or memory allocated by cudaMalloc?Thanks again!I see nvprof document, but I didn’t find some metrics about TLB miss counterI’m sorry, I’m not much of an expert with nvprof or nsys that I can direct you to the exact counters, but I believe many of our devblogs should be able to help guide you.  If anything, I believe PC sampling and guided analysis should be able to indicate the large access times, give details into some cache analysis, and help you confirm this is the case.  That said, based on the fact that sequential access is working fine, I would count on the TLB thrashing as being the the issue, unfortunately.So, yes, by requesting a shareable handle type, you’re opting to create this allocation such that it is compatible with other UMDs and thus needs a smaller page size.  Thus you will likely get higher latency accesses if you thrash the TLB (with random accesses across 1GiB for example) due to the smaller page size.  As I mentioned, this should be fixed in an upcoming driver release (I’m not sure which one just yet, I can try to let you know when it does).  Even so, it is always recommended, just as it is with CPUs though much more so with GPUs, to try to align your accesses in order to achieve maximum bandwidth and cache utilization whenever possible.Hope this helps, please let us know if you have further questions or if we can help further!  Good luck!Hi, We are developing a Windows application using the Virtual Memory Management APIs along with the CU_MEM_HANDLE_TYPE_WIN32 shareable handle. We have followed the memMapIpcDrv code sample and have successfully accessed the same CUDA memory on separate processes.The issue we are experiencing is that after any shared handles have been imported on the child process, the physical memory is not released until the child process exits.Do you happen to know if this is a known issue or limitation?We are making sure to unmap/release/close all device-pointers/allocation-handles/shareable-handles on both processes. All function calls exit without errors. Please see the following summary for the sequence of the api calls (error handling and minor details omitted for clarity).Any help would be greatly appreciated. Thanks!Parent process:Child process:Parent process:Hi, I try some features about cuMemSetAccess, I find if one device can not peer access another, the cuMemSetAccess will raise an cuda error when I set the access to this device. Is more detail information about the limitation of Driver API cuMemSetAccess?Hi jearnshaw, welcome back to the forums!It looks like everything you’re doing is correct… could you post a reproducer of the problem?  Do you see the same issue with the sample code or just your application?  I believe there used to be a bug in older drivers (I don’t remember what versions sorry) that did have a leak in this path, but this should be fixed in newer drivers IIRC.  Can you post what version driver you’re using?Hi 182yzh,I’m sorry, I’m not quite following your question.  I believe you’re asking if cuMemSetAccess can be used to enable access to memory physically located on a different device but does not have peer access enabled.  As described in the blog post above, this is a primary use case for this API, but it still requires that the two communicating device be peer-capable (i.e. cuDeviceCanAccessPeer or the runtime equivalent cudaDeviceCanAccessPeer returns true for the two devices).  Memory physically present on one device cannot be accessed by another device without the peer-capability between the two, whatever the underlying backend may be (PCI-E, NVLINK, NVSWITCH, etc).  Hope that answers your question, let me know if it doesn’t!  Good luck!Thanks for your quick reply @Cory.Perry. Yes I can reproduce the issue with the sample code as well.I’ve just modified memMapIpcDrv in a forked cuda-samples here to demonstrate the issue.The modifications increase the memory allocated to 2GB and add in some delays to inspect the GPU memory usage with nvidia-smi and Task Manager. Please see the following screenshots of the sample running. Our application reallocates memory and the physical memory continues to increase until the child process is exited.
Screen Shot 2022-12-06 at 1.14.38 PM1920×416 69.3 KB

Screen Shot 2022-12-06 at 1.15.17 PM1920×417 75.5 KB
The test machine is running CUDA 11.8 with nvidia driver 527.37 on Windows 10 Pro 22H2.Thanks for your reply. Through your reply I find out that the cuMemSetAccess need the capacity among devices(In other words, the cuDeviceCanAccessPeer result is true). Thanks a lot！@jearnshaw please file a support ticket following the instructions on our support site and we can look into diagnosing the issue as this is likely a bug.  Here’s a link to the nvidia support site for your convenience:
NVIDIA Support SiteHi @Cory.Perry , was the reason for @jearnshaw s problems resolved? I am experiencing a similar issue with a straightforward POC in which I continuously allocate (cuMemCreate) and free memory (cuMemUnmap/cuMemRelease/cuMemAddressFree), and I see that the memory usage keeps growing indefinitely, as if the memory is never freed.PS: I just updated my drivers to 535.86.05.@moises.jimenez Unfortunately I don’t have visibility into the exact support ticket that was filed.  If you’re seeing the same issue with the latest release drivers available, I would file your own support ticket.  It never hurts to have duplicated tickets, and in fact, helps us better prioritize issues such as this in order to build a better product.  Hope this helps!Powered by Discourse, best viewed with JavaScript enabled"
1675,gtc-21-top-5-arm-computing-and-ecosystem-sessions,"Originally published at:			https://developer.nvidia.com/blog/gtc-21-top-5-arm-computing-and-ecosystem-sessions/
NVIDIA and Arm are working together to open new opportunities for partners, users, and developers, driving a new wave of computing around the world. Explore all the Arm accelerated computing and ecosystem sessions at GTC.Powered by Discourse, best viewed with JavaScript enabled"
1676,7-things-you-might-not-know-about-numba,"Originally published at:			7 Things You Might Not Know about Numba | NVIDIA Technical Blog
Numba is a Python compiler from Anaconda that can compile Python code for execution on CUDA-capable GPUs or multicore CPUs. Numba allows automatic just-in-time (JIT) compilation of Python functions, which can provide orders of magnitude speedup for Python and Numpy data processing. Numba also provides CUDA Python, an API for writing CUDA kernels in the…Powered by Discourse, best viewed with JavaScript enabled"
1677,building-a-speech-enabled-ai-virtual-assistant-with-nvidia-riva-on-amazon-ec2,"Originally published at:			Building a Speech-Enabled AI Virtual Assistant with NVIDIA Riva on Amazon EC2 | NVIDIA Technical Blog

Learn how to get started with NVIDIA Riva, a fully accelerated speech AI SDK, on AWS EC2 using Jupyter Notebooks and a sample virtual assistant application.Powered by Discourse, best viewed with JavaScript enabled"
1678,nvidia-dgx-1-the-fastest-deep-learning-system,"Originally published at:			https://developer.nvidia.com/blog/dgx-1-fastest-deep-learning-system/
Figure 1: NVIDIA DGX-1. One year ago today, NVIDIA announced the NVIDIA® DGX-1™, an integrated system for deep learning. DGX-1 (shown in Figure 1) features eight Tesla P100 GPU accelerators connected through NVLink, the NVIDIA high-performance GPU interconnect, in a hybrid cube-mesh network. Together with dual socket Intel Xeon CPUs and four 100 Gb InfiniBand…Very good article and very useful comparisons between NVLink and PCIe performance scalability.The 8x M40 and 8x P100 PCIe server is an SMC 4028GR with dual Intel Xeon E5-2698v4 CPUs and 256GB DDR4-2133 RAM (DGX-1 has 512GB DDR4-2133).Any reason not having same RAM size as DGX-1 while comparing?Powered by Discourse, best viewed with JavaScript enabled"
1679,the-louvre-museum-unveils-the-mona-lisa-in-vr,"Originally published at:			The Louvre Museum Unveils the Mona Lisa in VR | NVIDIA Technical Blog
Last year, over ten million people visited the Mona Lisa at the Louvre Museum in Paris, France, making it the world’s most visited museum. For anyone that’s ever visited though, seeing the Mona Lisa in real life, the iconic Leonardo da Vinci masterpiece, can be a daunting experience due to the sheer number of people…Powered by Discourse, best viewed with JavaScript enabled"
1680,fixing-ray-traced-shadow-content-issues-with-nvidia-rtx-dev,"Originally published at:			Fixing Ray-traced Shadow Content Issues with NVIDIA RTX-Dev | NVIDIA Technical Blog
Custom NVIDIA RTX branches for Unreal Engine 4 (RTX-Dev) may substantially help your development effort by providing enhanced compatibility with certain types of common content. While UE4 has substantially improved its rendering capabilities with ray tracing, ray tracing effects often have expectations on how the scene was constructed. Today, real-time content, such as games, is often…Powered by Discourse, best viewed with JavaScript enabled"
1681,google-s-ai-can-fill-in-the-missing-frames-in-a-video-sequence,"Originally published at:			Google’s AI Can Fill In the Missing Frames in a Video Sequence | NVIDIA Technical Blog
With just the beginning and end frames, this 3D convolutional neural network can fill in the gaps. The process, known as “inbetweening,” can generate intermediate frames between two given points. The technique is normally executed by training and running recurrent neural networks. In this work, using NVIDIA V100 GPUs for training, Google AI researchers instead…Powered by Discourse, best viewed with JavaScript enabled"
1682,ai-writes-believable-fake-yelp-reviews,"Originally published at:			AI Writes Believable Fake Yelp Reviews | NVIDIA Technical Blog
Researchers from University of Chicago developed a deep learning-based text generation system that can develop sophisticated reviews that are nearly impossible to detect. In reference to how artificial intelligence can automatically write fake reviews, Ben Y. Zhao, a professor at the University of Chicago mentioned, “In general, the threat is bigger. I think the threat…Powered by Discourse, best viewed with JavaScript enabled"
1683,gtc-2020-using-video-codec-sdk-and-optical-flow-sdk-on-nvidia-gpus-effectively,"GTC 2020 CWE21120
Presenters: Abhijit-Patait,NVIDIA; Roman-Arzumanyan, ; Stefan-Schoenefeld, ; Mandar-Godse,
Abstract
NVIDIA GPUs, starting with the Turing generation, feature an optical-flow hardware accelerator that enhances several applications, including AI/DL, object tracking, video frame interpolation, and video analytics. The optical-flow functionality is available for software developers using NVIDIA’s optical-flow SDK. Bring your questions, suggestions, and feature requests related to optical-flow hardware and SDK to this session. Discover how it can be used in the applications above, and how you can leverage the optical flow with GPU-inferencing capabilities to build amazing applications for various industries. Plus, you’ll learn about new features and the roadmap of optical-flow hardware and software. NVIDIA’s technical staff responsible for multimedia software will staff this session.Watch this session
Join in the conversation below.In the presentation, Mr. Abhijit mentions that details for the questions like low latency encoding is in “the document”.
Can you please share the document or the name of the document ?Kind Regards,I found the low latency encoding answer in SDK documents.How to do true CBR encoding ? same parameters but this time true CBR (not CVBR) encoding ?Powered by Discourse, best viewed with JavaScript enabled"
1684,brightness-toggle-on-desktop,"Hi everyone !I just wanna know why it’s just impossible to adujst brightness on a desktop from a slider toggle ?
Have to go to the control panel all night is just so boring, by hotkey that feature could really save billions eyes…
Windows night mode is inefficient to my mind, my eyes don’t rest by changing a bit the color.
So many software can add hotkeys for their own features so I wish to understand why Nvidia can’t ? What’s the Desktop/Laptop variance that don’t allow it ?Thanks for your answer and please tell if any way exist !Thanks for this question. This one is best on the general GeForce forums : https://www.nvidia.com/en-us/geforce/forums/discover/
Your response will get the attention of the teams who manage this product.Powered by Discourse, best viewed with JavaScript enabled"
1685,a-startup-s-guide-to-success-in-central-and-eastern-europe,"Originally published at:			https://developer.nvidia.com/blog/a-startups-guide-to-success-in-central-and-eastern-europe/
Central and Eastern Europe (CEE) is quickly gaining recognition as one of the world’s most important rising technology ecosystems. A highly skilled workforce, government support, proximity to key markets, and a history of entrepreneurship are all factors that have led to a significant increase in funding to the region over the past several years. In…Powered by Discourse, best viewed with JavaScript enabled"
1686,nvidia-drive-developer-days-at-gtc-2021-now-open-to-the-entire-auto-industry,"Originally published at:			NVIDIA DRIVE Developer Days at GTC 2021 Now Open to the Entire Auto Industry | NVIDIA Technical Blog
This year, everyone can learn how to develop safe, robust autonomous vehicles on NVIDIA DRIVE. The annual DRIVE Developer Days is taking place April 20-22 during GTC 2021, featuring a series of specialized sessions on autonomous vehicle hardware and software, including perception, mapping, simulation and more, all led by NVIDIA experts. And now, registration is…Powered by Discourse, best viewed with JavaScript enabled"
1687,building-a-real-time-redaction-app-using-nvidia-deepstream-part-2-deployment,"Originally published at:			Building a Real-time Redaction App Using NVIDIA DeepStream, Part 2: Deployment | NVIDIA Technical Blog
This post is the second in a series (Part 1) that addresses the challenges of training an accurate deep learning model using a large public dataset and deploying the model on the edge for real-time inference using NVIDIA DeepStream. In the previous post, you learned how to train a RetinaNet network with a ResNet34 backbone…i got a problem when i executed the “make” command on the nano, the error is "" no cub/device/device_radix_sort.cub"", how to fix it.hi,how can i show landmarks points of face , i have written my custom parser for facedetection with landmarks. If i try to add any parameter to NvDsInferObjectDetectionInfo(nvdsinfer.h) i am getting segmentation fault.For questions about extending the app, I recommend posting in the DeepStream SDK forum. Good luck!thanks.I try to use my webcam feed instead of sample videos. I am getting the error : passed ‘0’ as denominator for `GstFraction’
** ERROR: <create_camera_source_bin:160>: Failed to link ‘src_elem’I replace the function in .txt file:
[source0]
enable=1
#Type - 1=CameraV4L2 2=URI 3=MultiURI 4=RTSP
type=1
CameraV4L2 = /dev/video0Please help me to solve this issue !!Sorry to hear that you’re getting this error! I can suggest posting this question in the DeepStream developer forum. There are lots of helpful people there. Good luck!Powered by Discourse, best viewed with JavaScript enabled"
1688,cuda-spotlight-gpu-accelerated-deep-neural-networks,"Originally published at:			https://developer.nvidia.com/blog/cuda-spotlight-gpu-accelerated-deep-neural-networks/
This week’s Spotlight is on Dr. Dan Ciresan, a senior researcher at IDSIA in Switzerland and a pioneer in using CUDA for Deep Neural Networks (DNNs). His methods have won international competitions on topics such as classifying traffic signs and recognizing handwritten Chinese characters. Dan presented a session on Deep Neural Networks for Visual Pattern Recognition at GTC…How do you start implementing a Neural Network with CUDA? What about OpenCL ?Typically you use a deep learning framework, rather than developing in CUDA or OpenCL directly. I suggest you check out the NVIDIA Deep Learning Institute courses, and look at the deep learning posts here: https://devblogs.nvidia.com...Powered by Discourse, best viewed with JavaScript enabled"
1689,rapids-release-0-13-is-live-and-packed-with-new-features,"Originally published at:			RAPIDS Release 0.13 is Live and Packed with New Features | NVIDIA Technical Blog
By Joshua Patterson As I look back on release 0.13, I feel a fraction of how the astronauts on Apollo 13 must have felt: relieved to have landed and grateful for rooms of quick thinking engineers. Early space missions can teach you a lot about what to do when things don’t go according to plan.…Powered by Discourse, best viewed with JavaScript enabled"
1690,nvidia-cloudxr-2-1-delivers-support-for-ios,"Originally published at:			NVIDIA CloudXR 2.1 Delivers Support for iOS | NVIDIA Technical Blog
The upcoming NVIDIA CloudXR 2.1 release will deliver support for Apple iOS AR devices, including iPads and iPhones.Powered by Discourse, best viewed with JavaScript enabled"
1691,mps-error-isolation,"AFAIK, MPS doesn’t provide error isolation. Are there plans to improve that?This is a complex topic because some of the error isolation and trapping happens at hardware level, so there’s an interplay between MPS and the available reporting from the GPU itself. We are exploring ways we can get better error isolation through MPS, but they’ll likely come with performance tradeoffs so they may not always be desirable.Ultimately, NVIDIA is putting a lot of engineering focus onto improving both MPS and MIG to bring better error isolation and scheduling control to MPS, and more flexibility to MIG.Powered by Discourse, best viewed with JavaScript enabled"
1692,exploring-unique-applications-of-text-to-speech-technology,"Originally published at:			https://developer.nvidia.com/blog/exploring-unique-applications-of-text-to-speech-technology/
When interacting with a virtual assistant, you give a command and receive a verbal response. The technology powering this generated voice response is known as text-to-speech (TTS). TTS applications are highly useful as they enable greater content accessibility for those who use assistive devices. With the latest TTS techniques, you can generate a synthetic voice…Powered by Discourse, best viewed with JavaScript enabled"
1693,gtc-digital-data-science-presentations-demos-and-posters,"Originally published at:			GTC Digital: Data Science Presentations, Demos, and Posters | NVIDIA Technical Blog
See how innovations are driving new data science capabilities, applications, and bridges to open source communities. Learn the latest breakthroughs in data manipulation, machine learning, spatial analysis, graph analytics, data visualization, and more.Powered by Discourse, best viewed with JavaScript enabled"
1694,introduction-to-turing-mesh-shaders,"Originally published at:			Introduction to Turing Mesh Shaders | NVIDIA Technical Blog
The Turing architecture introduces a new programmable geometric shading pipeline through the use of mesh shaders. The new shaders bring the compute programming model to the graphics pipeline as threads are used cooperatively to generate compact meshes (meshlets) directly on the chip for consumption by the rasterizer. Applications and games dealing with high-geometric complexity benefit…""Optimizing the vertex locations along is also beneficial,""I assume the word ""along"" is meant to be ""alone"".""Vertex cache optimizers that help classic rendering also help improving meshlet packing efficiency.""I assume should be either:""Vertex cache optimizers that help classic rendering also helps, improving meshlet packing efficiency.""or""Vertex cache optimizers that help classic rendering also help improve meshlet packing efficiency.""Overall an interesting and informative read, thanks.Good read!To make sure I understand how work can be done once and re-used, is it just that you'd have a run-once mesh shader that does work and writes the meshlet data to a buffer, then on subsequent frames, you'd get the processed data from that buffer instead of doing the work again?I don't think it maintains the memory from frame to frame. It sounds like the memory is maintained for each of the task and mesh threads per dispatch. What this means is that when you dispatch it a bunch of task shader instances are spawned and the only input is the id for each instance. The task shader then outputs how many mesh shader instances to spawn. Each of these mesh shader instances have a single input of an id. My guess is that these ids are used to figure out what ""piece"" or ""meshlet"" or ""submesh"" of the larger mesh needs to be processed. Once a mesh shader instance is done it writes out the indices and vertices that then passed to the rasterizer which then launches the pixel shader. After this I believe the memory is reused by the next dispatch call. I'm looking forward to a sample application that will hopefully explain this better.Hi, thanks for this! Any chance to get our hands on some code? Maybe you could put the code for this asteroids sample on github? Many thanks!Powered by Discourse, best viewed with JavaScript enabled"
1695,improving-real-time-communication-experiences-with-nvidia-maxine,"Originally published at:			https://developer.nvidia.com/blog/improving-real-time-communication-experiences-with-maxine/
For developers who want to quickly build or improve real-time virtual collaboration and content creation applications, NVIDIA Maxine offers GPU-accelerated Video Effects, Audio Effects, and Augmented Reality SDKs.Powered by Discourse, best viewed with JavaScript enabled"
1696,create-3d-virtual-worlds-with-new-releases-expansions-and-toolkits-from-nvidia-omniverse,"Originally published at:			https://developer.nvidia.com/blog/create-3d-virtual-worlds-with-new-releases-expansions-and-toolkits-from-nvidia-omniverse/
Learn about all of the new applications, features, and functions released for developers to build, extend, and connect 3D tools and platforms to the Omniverse ecosystem seamlessly.Powered by Discourse, best viewed with JavaScript enabled"
1697,automatically-identify-wild-animals-in-camera-trap-images,"Originally published at:			https://developer.nvidia.com/blog/automatically-identify-wild-animals-in-camera-trap-images/
A research team led by University of Wyoming developed a deep learning-based system to identify species in the Serengeti National Park in Tanzania that could make it easier for ecologists to track animals in the wild. Camera traps automatically take pictures of passing animals when triggered by heat and motion which produce millions of images…Powered by Discourse, best viewed with JavaScript enabled"
1698,nuance-accelerates-conversational-ai-training-by-50,"Originally published at:			Nuance Accelerates Conversational AI Training by 50% | NVIDIA Technical Blog
The article below is a guest post by Nuance, a company focused on conversational AI. In this post, Nuance engineers describe their use of NVIDIA’s automatic mixed precision to speed up their AI models in the healthcare industry. By Wenxuan Teng, Ralf Leibold, and Gagandeep Singh Nuance’s ambient clinical intelligence (ACI) technology is an example…Powered by Discourse, best viewed with JavaScript enabled"
1699,accelerating-your-network-with-adaptive-routing-for-nvidia-spectrum-ethernet,"Originally published at:			https://developer.nvidia.com/blog/accelerating-your-network-with-adaptive-routing-for-spectrum-ethernet/
This post introduces the adaptive routing technology for NVIDIA Spectrum Ethernet and provides preliminary performance benchmarks.Hello.The post says that “NVIDIA is introducing adaptive routing to Spectrum switches”.Let me ask please:Thank you.Hi,
Thank you for interest and questions.Hope this is helpful for you, feel free to continue this thread or reach out personaly.Thanks,
YonatanPowered by Discourse, best viewed with JavaScript enabled"
1700,deploying-healthcare-ai-workflows-with-the-nvidia-clara-deploy-application-framework-updated,"Originally published at:			Deploying Healthcare AI Workflows with the NVIDIA Clara Deploy Application Framework (updated) | NVIDIA Technical Blog
This is an updated version of Deploying Healthcare AI Workflows with the NVIDIA Clara Deploy Application Framework. The new version adds information about configuring the DICOM adapter and three new reference pipelines. Figure 1. NVIDIA Clara Deploy SDK in the healthcare ecosystem. The adoption of AI in hospitals is accelerating rapidly. There are many reasons…Powered by Discourse, best viewed with JavaScript enabled"
1701,nvidia-announces-new-omniverse-educational-programs,"Originally published at:			https://developer.nvidia.com/blog/nvidia-announces-new-omniverse-educational-programs/
NVIDIA Omniverse is key to bringing advanced architectural design concepts combining AI, simulation and collaborative workflows to the next generation of students.Powered by Discourse, best viewed with JavaScript enabled"
1702,deep-learning-system-improves-breast-cancer-detection,"Originally published at:			Deep Learning System Improves Breast Cancer Detection | NVIDIA Technical Blog
Researchers from Beth Israel Deaconess Medical Center (BIDMC) and Harvard Medical School have developed a deep learning approach to read and interpret pathology images. Trained on Tesla K80 GPUs with the cuDNN-accelerated Caffe deep learning framework, their system achieved 92 percent accuracy at identifying breast cancer in images of lymph nodes which earned them the…Powered by Discourse, best viewed with JavaScript enabled"
1703,meet-the-researcher-alan-fern-machine-learning-and-automated-planning-for-sequential-decision-making,"Originally published at:			Meet the Researcher: Alan Fern, Machine Learning and Automated Planning for Sequential Decision Making | NVIDIA Technical Blog
‘Meet the Researcher’ is a new series in which we spotlight different researchers in academia who are using GPUs to accelerate their work. This week, we spotlight Alan Fern, a Professor of Computer Science and Associate Head of Research in the School of Electrical Engineering and Computer Science at Oregon State University.  Fern received a…Powered by Discourse, best viewed with JavaScript enabled"
1704,gtc-2020-new-features-in-optix-7,"GTC 2020 S21904
Presenters: David Hart,NVIDIA; Mark Leone,NVIDIA
Abstract
We’ll discuss the newest features in the OptiX SDK, a software development kit for achieving high-performance ray tracing on GPUs in areas ranging from film rendering to acoustic modeling to scientific visualization. We’ll examine OptiX 7, which introduces a new low-level CUDA-centric API, giving application developers direct control of memory, compilation, and launches while maintaining the OptiX programming model and shader types. It also includes a library that provides helper functions to load textures on demand. We’ll do a deep dive into some of the new features, providing code samples and integration tips.Watch this session
Join in the conversation below.Powered by Discourse, best viewed with JavaScript enabled"
1705,gtc-2020-autonomous-anomaly-detection-for-dense-sensor-iot-prognostics,"GTC 2020 S21621
Presenters: Akshay Subramaniam,NVIDIA ; Guang Wang,Oracle
Abstract
Low latencies and high throughputs are critical to achieving real-time streaming prognostics, particularly in cloud environments. Learn how NVIDIA GPU acceleration of MSET2, Oracle’s advanced machine learning pattern-recognition innovation, yields substantial reductions in latency and enhancements to throughput rates on-premises and in cloud environments. This work will enable customers in dense-sensor internet-of-things industries such as utilities, smart manufacturing, oil and gas, government, and commercial aviation to harness vast amounts of data from plants and physical assets to gain valuable prognostic insights. We’ll discuss our collaboration on accelerating MSET2 and the substantial (upwards of 200x) acceleration results we achieved.Watch this session
Join in the conversation below.Powered by Discourse, best viewed with JavaScript enabled"
1706,experimenting-with-novel-distributed-applications-using-nvidia-flare-2-1,"Originally published at:			https://developer.nvidia.com/blog/experimenting-with-novel-distributed-applications-using-nvidia-flare-2-1/
In this post, I introduce new features of NVIDIA FLARE v2.1 and walk through proof-of-concept and production deployments of the NVIDIA FLARE platform.Actually I am trying to configure multiple clients and server machines in order to use NVIDIA FLARE but i didnt find any documentation regarding that. Where can we actually configure clients ip address, server ip address, or admin ip address so that server will send updates to only configured clients ?Powered by Discourse, best viewed with JavaScript enabled"
1707,gtc-2020-simnet-toolkit-accelerating-scientific-engineering-simulation-workflows-with-ai,"GTC 2020 D2S30
Presenters: Tech Demo Team,NVIDIA
Abstract
Simulations form an integral part of product design to reduce significant iterations in physical prototyping and testing to improve quality, cost and time-to-market. However, this process is very time consuming and can take weeks to months since, in a typical simulation workflow, several iterations are involved if the results are not satisfactory for a given design. Typically, there is never enough time or compute power to examine all the design variations.The NVIDIA SimNet Toolkit is an end-to-end AI-driven simulation framework based on a novel Physics Informed Neural Network (PINN) architecture. This demonstration of SimNet is solving a multi-Physics problem to perform automatic design space exploration, a thousand times faster than traditional simulation, with the accuracy of numerical solvers.Such unprecedented throughput enables optimized design selection, which we show using SimNet. Completing these design tasks take seconds not hours, and complex design optimization can be completed in days instead of months.Watch this session
Join in the conversation below.Powered by Discourse, best viewed with JavaScript enabled"
1708,nvidia-metropolis-partners-showcase-vision-ai-traffic-optimization-at-ces-2022,"Originally published at:			https://developer.nvidia.com/blog/nvidia-metropolis-partners-showcase-vision-ai-traffic-optimization-at-ces-2022/
Explore NVIDIA Metropolis partners showcasing new technologies to improve city mobility at CES 2022.Powered by Discourse, best viewed with JavaScript enabled"
1709,using-deepspeed-and-megatron-to-train-megatron-turing-nlg-530b-the-world-s-largest-and-most-powerful-generative-language-model,"Originally published at:			https://developer.nvidia.com/blog/using-deepspeed-and-megatron-to-train-megatron-turing-nlg-530b-the-worlds-largest-and-most-powerful-generative-language-model/
MT-NLG has 3x the number of parameters compared to the existing largest model of this type and demonstrates unmatched accuracy in a broad set of natural language tasksAwesome. How can end-user get access to this model? Is it integrated with Azure cognitive services - given it is a joint effort by Microsoft?Powered by Discourse, best viewed with JavaScript enabled"
1710,cudacasts-episode-16-thrust-algorithms-and-custom-operators,"Originally published at:			https://developer.nvidia.com/blog/cudacasts-episode-16-thrust-algorithms-custom-operators/
Continuing the Thrust mini-series (see Part 1), today’s episode of CUDACasts focuses on a few of the algorithms that make Thrust a flexible and powerful parallel programming library. You’ll also learn how to use functors, or C++ “function objects”, to customize how Thrust algorithms process data. In the next CUDACast in this Thrust mini-series, we’ll…the ""square"" example has a small bug (see around 6:00) -- ""init"" should be initialized differently, namely by applying kernel to the first element.You're absolutely right Vladimir.  Thanks for catching the bug!  I've added an annotation to the video to point out the bug and fix.Powered by Discourse, best viewed with JavaScript enabled"
1711,building-a-computer-vision-application-to-recognize-human-activities,"Originally published at:			https://developer.nvidia.com/blog/building-a-computer-vision-application-to-recognize-human-activities/
This walkthrough shares how a user can quickly build and deploy a computer vision application with the NVIDIA NGC catalog and Google Vertex AI.Train and test an action recognition model in few steps using NVIDIA NGC models and quick deploy feature. If you have any questions or comments, let us knowPowered by Discourse, best viewed with JavaScript enabled"
1712,building-an-active-digital-twin-using-nvidia-omniverse-and-project-gemini,"Originally published at:			Building an Active Digital Twin Using NVIDIA Omniverse and Project Gemini | NVIDIA Technical Blog
Discover an active digital twin framework and toolkit that uses Universal Scene Description to fully connected to NVIDIA Omniverse.Powered by Discourse, best viewed with JavaScript enabled"
1713,getting-the-most-out-of-nvidia-t4-on-aws-g4-instances,"Originally published at:			https://developer.nvidia.com/blog/getting-the-most-out-of-nvidia-t4-on-aws-g4-instances/
With the continued growth of AI models and data sets and the rise of real-time applications, getting optimal inference performance has never been more important. In this post, you learn how to get the best natural language inference performance from AWS G4dn instance powered by NVIDIA T4 GPUs, and how to deploy BERT networks easily using NVIDIA Triton Inference Server.Powered by Discourse, best viewed with JavaScript enabled"
1714,the-future-of-edge-ai-is-cloud-native,"Originally published at:			https://developer.nvidia.com/blog/the-future-of-edge-ai-is-cloud-native/
Accelerated edge AI uses cloud-native architecture to deliver resilience and performance.Powered by Discourse, best viewed with JavaScript enabled"
1715,meet-the-researcher-raphael-frank-optimizing-autonomous-perception,"Originally published at:			Meet the Researcher, Raphaël Frank, Optimizing Autonomous Perception | NVIDIA Technical Blog
Meet the Researcher is a series in which we spotlight different researchers in academia who are using GPUs to accelerate their work. This week, we spotlight Raphaël Frank, Senior Research Scientist at the SnT Research Center of the University of Luxembourg. The research featured in this article is part of the project of François Robinet…Powered by Discourse, best viewed with JavaScript enabled"
1716,gtc-2020-federated-learning-for-medical-imaging-collaborative-ai-without-sharing-patient-data,"GTC 2020 S21536
Presenters: Yuhong Wen,NVIDIA; Nicola Rieke, NVIDIA
Abstract
While deep neural networks have shown promising results in various medical applications, they highly depend on the amount and diversity of the training data. In the context of medical imaging, this poses a major challenge because patient data needs to be protected and cannot easily be shared. The training data that is required to train a reliable and robust algorithm may not be available in a single institution due to the low incidence rate of some pathologies and limited numbers of patients. At the same time, it is often not feasible to collect and share patient data in a centralized data lake due to patient privacy concerns and regulations. Federated learning — as a collaborative machine learning paradigm — combined with an advanced privacy-preserving mechanism has the potential of solving this issue: models can be trained across several institutions without explicitly sharing patient data. When implementing and deploying a federated learning system into the real-world medical imaging ecosystem, participants can authenticate and communicate securely, and exchange model weights efficiently, enabling model training to be successful. In this talk, we present an introduction to the core concepts of federated learning and discuss the benefits as well as the unique considerations and challenges of implementing a federated-learning system in the context of health care.Watch this session
Join in the conversation below.Powered by Discourse, best viewed with JavaScript enabled"
1717,simplifying-ai-development-with-nvidia-base-command-platform,"Originally published at:			https://developer.nvidia.com/blog/simplifying-ai-development-with-base-command-platform/
The NVIDIA Base Command Platform enables an intuitive, fully featured development experience for AI applications. It was built to serve the needs of the internal NVIDIA research and product development teams. Now, it has become an essential method for accessing on-demand compute resources to train neural network models and execute other accelerated computing experiments.  Base Command Platform…Hi! I’m Joe, one of the authors of this blog - we’ve been working with Base Command Platform internally for a while now, and we’re excited to share it outside of NVIDIA! I’m happy to answer any questions that come up - please give the blog a read and let us know what you think, we’d be happy to hear from anyone interested in Base Command Platform!Powered by Discourse, best viewed with JavaScript enabled"
1718,high-performance-next-generation-deep-learning-clusters,"GTC 2020 S22047
Presenters: Julie Bernauer, NVIDIA
Abstract
From climate modeling to drug design, AI models are not fully part of scientific modeling, and AI models are getting more complex and larger every year. Until recently, system design for HPC and AI were often done in isolation as the requirements for the platforms were different, making large scientific experimentation difficult. To overcome these gaps, systems are now designed with AI software in mind, and scale is introduced in the software design from the ground up so that each model running at the edge can be trained in minutes at scale. The largest supercomputers in the world are now designed with AI in mind, and enterprise and AI research systems are being designed more like supercomputers. Last year, we showcased the Superpod as an example of rapid time-to-floor for an AI performance infrastructure; today, we’ll cover where the next generation is moving and show how we think about design and infrastructure that can be leveraged to support the needs of AI research and development teams, and how modern AI frameworks and models are built to leverage these systems.Watch this session
Join in the conversation below.Powered by Discourse, best viewed with JavaScript enabled"
1719,fabula-ai-develops-a-new-algorithm-to-stop-fake-news,"Originally published at:			https://developer.nvidia.com/blog/fabula-ai-develops-a-new-algorithm-to-stop-fake-news/
London based startup Fabula AI has developed a deep learning-based system that can help identify fake news across online platforms.  “Automatically detecting fake news poses challenges that defy existing approaches based on linguistic content analysis,” the company stated in a blog post. “News is often highly nuanced and their interpretation requires the knowledge of political…Powered by Discourse, best viewed with JavaScript enabled"
1720,predicting-your-brain-age-from-mri-scans,"Originally published at:			https://developer.nvidia.com/blog/predicting-your-brain-age-from-mri-scans/
Researchers from King’s College London developed a deep learning system to measure brain age using raw data from an MRI scanner that can help reveal the onset of conditions such as dementia. Using CUDA, TITAN X GPUs and cuDNN with the Torch deep learning framework, the researchers trained their models on 1,600 MRI brain scans…Powered by Discourse, best viewed with JavaScript enabled"
1721,gpus-and-dsls-for-life-insurance-modeling,"Originally published at:			https://developer.nvidia.com/blog/gpus-dsls-life-insurance-modeling/
The Solvency II EU Directive came into effect at the beginning of the year. It harmonizes insurance regulation in the European Union with an economic and risk based approach, which considers the full balance sheet of insurers and re-insurers. In the case of life insurers and pension funds, this requires the calculation of the economic value…Very interesting, thank you for this -complete- post. As a math person, it's always good to see a nice description and modeling of the problem.I'm currently working with a bank that uses GPUs and develop in a C# framework (nvcc & VS C# compiler), but relies on a tool provided by another company to generate multithreaded or GPU code automatically : I wasn't aware of such alternatives, or of F#.A question, though, on the C# code snippet. This simulation relies on a stochastic model which means we probably cannot just ""compute"" the result for a given contract by simply integrating the equation.Is that stochasticity somewhat hidden under the outer loop (under ""results""), that accounts for different scenarii / states of the Markov chain, or has this ""expectation"" been already computed and translated into the ""bj_ii"" factor ?As these loops are sequential by nature, I'd then infer that it's the work of a single thread. Then, what is mapped to thread blocks / grid (a contract, a single simulation, a portfolio) ?ValentinThe model directly incorporates the stochasticity of unknown remaining lifetime (through a death intensity function) but not the stochasticity of unknown death intensity (eg. longevity shocks) or unknown interest rates, nor of unknown interest rates: it assumes deterministic future interest rates, often mandated by financial regulation. A more complicated model could account for these unknowns too, or more simply, one could run the given model with a range of death intensities and interest rate curves -- which of course would require more computation time.Your are right that (the reserve of) each contract is computed in a single thread. An entire portfolio of similar contracts is mapped to  thread blocks or grids, though we are also experimenting with mapping a single many-state contract to a thread block.Powered by Discourse, best viewed with JavaScript enabled"
1722,turn-your-selfie-into-realistic-vr-avatar,"Originally published at:			https://developer.nvidia.com/blog/turn-your-selfie-into-realistic-vr-avatar/
California-based startup ObEN uses artificial intelligence to create a personalized and realistic 3D virtual self with just one smartphone image, and transports your virtual self into any AR and VR environment. ObEN, who was also chosen as one of 33 companies to participate in HTC’s VIVE X accelerator program, raised nearly $8 million for their…Powered by Discourse, best viewed with JavaScript enabled"
1723,debugging-cuda-more-efficiently-with-nvidia-compute-sanitizer,"Originally published at:			https://developer.nvidia.com/blog/debugging-cuda-more-efficiently-with-nvidia-compute-sanitizer/
Debugging code is a crucial aspect of software development but can be both challenging and time-consuming. Parallel programming with thousands of threads can introduce new dimensions to the already complex debugging process. There are various tools and techniques available to developers to help make debugging simpler and more efficient. This post looks at one such…Great news!  I look forward to using this.  Does this also now work with address sanitizer and thread sanitizer?No, the compute sanitizer tools use binary patching at runtime, so they work independently from compiler-assisted tools such as asan or tsan.This looks very useful! Do you know if these tools will detect problems within user Cuda code which are compiled into larger OptiX kernels? Thanks.Yes, the compute-sanitizer tools support OptiX applications since CUDA 11.6.See Compute Sanitizer User Manual :: Compute Sanitizer Documentation for more information.Powered by Discourse, best viewed with JavaScript enabled"
1724,migrating-from-range-profiler-to-gpu-trace-in-nsight-graphics,"Originally published at:			https://developer.nvidia.com/blog/migrating-from-range-profiler-to-gpu-trace-in-nsight-graphics/
Starting in Nsight Graphics 2023.1, the GPU Trace Profiler is the best way to profile your graphics application at the frame level. The Frame Profiler activity, and the Range Profiler tool window, have been removed. Don’t worry! The key profiling information is still available, only in a different form. This post guides you through the…Powered by Discourse, best viewed with JavaScript enabled"
1725,insilico-medicine-identifies-therapeutic-targets-for-als-with-ai,"Originally published at:			Insilico Medicine Identifies Therapeutic Targets for ALS With AI | NVIDIA Technical Blog
Drug discovery startup Insilico Medicine collaborated with academic researchers to identify over two dozen gene targets related to ALS with AI.Powered by Discourse, best viewed with JavaScript enabled"
1726,upcoming-webinar-building-ai-based-simulations-with-nvidia-simnet,"Originally published at:			https://developer.nvidia.com/blog/upcoming-webinar-building-ai-based-simulations-with-nvidia-simnet/
Simulations are prevalent in science and engineering fields and have been recently advanced by physics-driven AI. Join this webinar to learn how NVIDIA SimNet addresses a wide range of use cases involving coupled forward simulations without any training data, as well as inverse and data assimilation problems. SimNet is integrated with parameterized constructive solid geometry…Powered by Discourse, best viewed with JavaScript enabled"
1727,end-to-end-ai-for-workstation-an-introduction,"Originally published at:			https://developer.nvidia.com/blog/end-to-end-ai-for-workstation-an-introduction/
This post is the first in a series about optimizing end-to-end AI for workstations. For more information, see part 2, End-to-End AI for Workstation: Transitioning AI Models with ONNX, and part 3, End-to-End AI for Workstation: ONNX Runtime and Optimization. The great thing about the GPU is that it offers tremendous parallelism; it allows you…We hope everyone finds this introduction to optimizing workstation AI a helpful starting point. If you have any questions or comments, let us knowPowered by Discourse, best viewed with JavaScript enabled"
1728,new-nvidia-webinar-series-healthcare-on-tap-with-nvidia-sdks,"Originally published at:			New NVIDIA Webinar Series: Healthcare on Tap with NVIDIA SDKs | NVIDIA Technical Blog
To help developers, engineers, and startups get started with NVIDIA healthcare tools, NVIDIA has published a new webinar series called Healthcare on Tap. The webinars are designed to help developers accelerate their work in areas such as image analysis, scientific research, and drug discovery.  The first 12 episodes of the webinar are available on-demand. Three…Powered by Discourse, best viewed with JavaScript enabled"
1729,access-the-latest-in-vision-ai-model-development-workflows-with-nvidia-tao-toolkit-5-0,"Originally published at:			https://developer.nvidia.com/blog/access-the-latest-in-vision-ai-model-development-workflows-with-nvidia-tao-toolkit-5-0/
NVIDIA TAO Toolkit 5.0 features include source-open architecture, transformer-based pretrained models, AI-assisted data annotation, and the capability to deploy models on any platform.Looks promising, especially the transformer-based models and the ONNX export option.When does 5.0 become available and where can it be found?
Curious to test it on a Jetson Orin NX 16GB.Hi, TAO Toolkit 5.0 will be released mid 2023 . You can sign up here to be notified when the new version is available - Log in | NVIDIA Developer. To get started with TAO Toolkit, checkout this page - Get Started with TAO Toolkit | NVIDIA Developer.Good morning June!Any news?will any custom model from BYOM converter can be usable as pretrained model in TAO?Hello, 4th of July!!! no plan for releasing TAO 5.0? or any update on when it will be released? @jwitsoe @Fiona.Chenthe 5.0 is already there, can’t wait to use it.Powered by Discourse, best viewed with JavaScript enabled"
1730,pretraining-bert-with-layer-wise-adaptive-learning-rates,"Originally published at:			Pretraining BERT with Layer-wise Adaptive Learning Rates | NVIDIA Technical Blog
Training with larger batches is a straightforward way to scale training of deep neural networks to larger numbers of accelerators and reduce the training time. However, as the batch size increases, numerical instability can appear in the training process. The purpose of this post is to provide an overview of one class of solutions to…Powered by Discourse, best viewed with JavaScript enabled"
1731,facebook-trains-imagenet-in-1-hour,"Originally published at:			Facebook Trains ImageNet in 1 Hour | NVIDIA Technical Blog
Facebook published a paper today detailing how they are able to train nearly 1.3 million images in under an hour using 256 Tesla P100 GPUs that previously took days on a single system.      The team reduced the training time of a ResNet-50 deep learning model on ImageNet from 29 hours to one – which…Powered by Discourse, best viewed with JavaScript enabled"
1732,inception-member-wrnch-and-proplayai-unveil-new-ai-biomechanics-app-for-baseball,"Originally published at:			Inception Member wrnch and ProPlayAI Unveil New AI Biomechanics App for Baseball | NVIDIA Technical Blog
To help aspiring baseball players analyze and improve their technique, NVIDIA Inception member wrnch, in collaboration with ProPlayAI, developed a GPU-powered biomechanics app for pitchers. The app is designed to help people stay fit, improve performance, and coach players on the technical details. “Our approach allows athletes to perfect their pitching mechanics, even when they…Powered by Discourse, best viewed with JavaScript enabled"
1733,machine-learning-acceleration-in-vulkan-with-cooperative-matrices,"Originally published at:			Machine Learning Acceleration in Vulkan with Cooperative Matrices | NVIDIA Technical Blog
Machine learning harnesses computing power to solve a variety of ‘hard’ problems that seemed impossible to program using traditional languages and techniques. Machine learning avoids the need for a programmer to explicitly program the steps in solving a complex pattern-matching problem such as understanding speech or recognizing objects within an image. NVIDIA aims to bring machine learning to…This is great news. I especially applaud this line:""if that developer desires to access state-of-the-art GPU rendering and compute functionality in a way that doesn’t lock them to a single platform, then that API is Vulkan!""But then why call it ""VK_NV_cooperative_matrix"". IE why is the ""NV"" in there. Doesn't this dis-incentivize other vendors such as Intel or AMD from creating their own vendor extension that exposes the same function, but with vendor-neutral nomenclature? What am I missing here? Please dissuade me from my suspicion that this is actually an attempt at quasi vendor lockin?  Why not just call it VK_cooperative_matrix? Then it can be implemented by everyone, because right now they sure won't do so with ""NV"" in the function name. We really don't want fragmentation.the NV in the name only indicates that nvidia authored the extension without explicit input from other software or hardware vendors. Other hardware vendors can implement the extension, and/or ask for a KHR version.Very good news and initiative to expose the tensor cores in VK. How perfs compare against CuDNN or CuBlas? Tk.Powered by Discourse, best viewed with JavaScript enabled"
1734,record-edit-and-rewind-in-virtual-reality-with-nvidia-vr-capture-and-replay,"Originally published at:			https://developer.nvidia.com/blog/record-edit-and-rewind-in-virtual-reality-with-nvidia-vr-capture-and-replay/
Developers and users can capture and replay VR sessions for performance testing and scene troubleshooting with early access to NVIDIA Virtual Reality Capture and Replay.Powered by Discourse, best viewed with JavaScript enabled"
1735,nvidia-dli-teaches-supervised-and-unsupervised-anomaly-detection,"Originally published at:			https://developer.nvidia.com/blog/nvidia-dli-teaches-supervised-and-unsupervised-anomaly-detection/
Learn about multiple ML and DL techniques to detect anomalies in your organization’s data.Powered by Discourse, best viewed with JavaScript enabled"
1736,developer-spotlight-defending-the-planet-against-asteroids-with-artificial-intelligence,"Originally published at:			Developer Spotlight: Defending the Planet Against Asteroids with Artificial Intelligence | NVIDIA Technical Blog
James Parr, co-director of the NASA Frontier Development Lab (FDL) shares how NVIDIA GPUs and deep learning can help detect, characterize and deflect asteroids. The FDL hosted 12 standout graduate students for an internship to take on the White House’s Asteroid Grand Challenge, an ongoing program that aims to get researchers to “find all asteroid…Powered by Discourse, best viewed with JavaScript enabled"
1737,storage-performance-basics-for-deep-learning,"Originally published at:			Storage Performance Basics for Deep Learning | NVIDIA Technical Blog
Introduction When production systems are not delivering expected levels of performance, it can be a challenging and time-consuming task to root-cause the issue(s). Especially in today’s complex environments, where the workload is comprised of many software components, libraries, etc, and rely on virtually all of the underlying hardware subsystems (CPU, memory, disk IO, network IO)…Great write up ! I enjoyed reading that...Thanks for the article, it was a nice read.For CUDA developers that need very low latency disk access and do not require a file system, I have made a library for creating CUDA storage applications: https://github.com/enfiskut...I've also made a synthetic benchmark for it, comparing it to among other things memory mapping a file. It's still very much a work in progress, so don't expect too much from it, but it shows some interesting concepts like directly accessing a disk using GPUDirect RDMA/Async.Thanks very much Tim. Much more to come!Thanks very much. Having a look at your code this afternoon - very interesting!On a related note, applying some basic sanity checking on several white-box storage nodes we have in our lab is time well spent. These nodes each have 6 NVMe SSD's, and on one of the storage nodes, one of the NVMe devices gets less than half the random 4k read IOPS as the other five NVMe SSD's. I have not yet root-caused this, but it's one of those things that would potentially cause a lot of hair-pulling once in production. The NVMe SSD's are getting near 500k random 4k reads, but the 'bad' NVMe SSD sustains less than 200k. Huge difference, and something that would have dragged a RAID group down for sure.Thanks James. Really help us in our Testing of NVMe drives.Powered by Discourse, best viewed with JavaScript enabled"
1738,new-releases-of-nvidia-nsight-systems-and-nsight-graphics-debut-at-siggraph-2022,"Originally published at:			https://developer.nvidia.com/blog/new-releases-of-nvidia-nsight-systems-and-nsight-graphics-debut-at-siggraph-2022/
NVIDIA Developer Tools bring new utility with Vulkan Video support for Nsight Systems, OpenGL support in Nsight Graphics, and feature updates.Powered by Discourse, best viewed with JavaScript enabled"
1739,nvidia-ai-inference-performance-milestones-delivering-leading-throughput-latency-and-efficiency,"Originally published at:			https://developer.nvidia.com/blog/nvidia-ai-inference-performance-milestones-delivering-leading-throughput-latency-and-efficiency/
Inference is where AI-based applications really go to work. Object recognition, image classification, natural language processing, and recommendation engines are but a few of the growing number of applications made smarter by AI. Recently, TensorRT 5, the latest version of NVIDIA’s inference optimizer and runtime,  became available. This version brings new features including support for…Powered by Discourse, best viewed with JavaScript enabled"
1740,nvidia-rapids-accelerates-kubeflow-pipeline-with-gpus-on-kubernetes,"Originally published at:			NVIDIA RAPIDS Accelerates Kubeflow Pipeline with GPUs on Kubernetes | NVIDIA Technical Blog
Data science workflows are inherently complex. They scale across clusters of servers running software from different parts of the workflow and they are often compute-intensive. All this results in slow machine learning model development and deployment cycles.   To help accelerate end-to-end data science training, NVIDIA developed  RAPIDS, an open-source data analytics and machine learning…Powered by Discourse, best viewed with JavaScript enabled"
1741,ray-tracing-gems-ii-available-today-as-free-digital-download,"Originally published at:			https://developer.nvidia.com/blog/ray-tracing-gems-ii-available-today-as-free-digital-download/
Ray Tracing Games II is now available to download for free via Apress and Amazon. This Open Access book is a must-have for anyone interested in real-time rendering. Ray tracing is the holy grail of gaming graphics, simulating the physical behavior of light to bring real-time, cinematic-quality rendering to even the most visually intense games.Powered by Discourse, best viewed with JavaScript enabled"
1742,writing-portable-rendering-code-with-nvrhi,"Originally published at:			https://developer.nvidia.com/blog/writing-portable-rendering-code-with-nvrhi/
Modern graphics APIs, such as Direct3D 12 and Vulkan, are designed to provide relatively low-level access to the GPU and eliminate the GPU driver overhead associated with API translation. This low-level interface allows applications to have more control over the system and provides the ability to manage pipelines, shader compilation, memory allocations, and resource descriptors…Thank you for your interest in NVRHI. It’s been used internally by NVIDIA DevTech for years, and now it’s available as open source for everyone else. I hope someone finds it a useful library for their project. If you have any comments or questions, you can post them here.This looks very interesting, but I have a few questions:a) Looks like this will work on Intel and AMD GPUs too, as long as they support Vulkan or Direct3d 11/12. Is that correct?b) Could you comment on how this compares with libraries like gfx-rs ?Hi,
Does NVHRI support multiple GPUs ? I mean, in the case of an application with two windows, running on a system with two GPU (each GPU having one output), where each output contain one single window, would NVHRI support rendering to each window with its respective GPU ?
I haven’t seen any code that highlight that feature, but I’m not really sure I haven’t missed it !
Cheers,
GregNVRHI does work with AMD GPUs, that’s right.Comparison with gfx-rs: first, gfx-rs is written in Rust and for Rust, while NVRHI is in C++. Beyond that, the APIs seem somewhat similar - but they’re similar to the backend APIs as well. Gfx-rs has some functionality for adapter creation and windowing system interop, NVRHI does not. NVRHI has automatic resource state tracking and barriers, supports ray tracing.
That’s what I could find from a quick look at the gfx-rs documentation, anyway.Yes, NVRHI should support multiple GPUs in that scenario. You can create multiple NVRHI device objects, one from each device, and interact with them separately.I checked out the donut samples and NVRHI seems to be a very nice abstraction layer and donut itself is also very good if you want to avoid reinventing all the redundant work like a TAA pass.
Is there a specific reason why it doesn’t handle runtime compilation of shaders? I’m tinkering with the idea to write another client for Quake 3 where I translate every material to a custom shader that returns diffuse, specular and emmissive colors and handles all the texture rotations and blending operations.Thank you for the positive feedback, Robert!There is no single, strong reason why NVRHI/Donut do not handle runtime shader compilation; they just use a different path. One reason is that they are designed for shipping demos, among other things, and compiling shaders in shipping code is… well, let’s say frowned upon. Unsupported on some platforms even. Also, when you compile shaders offline and list all necessary permutations, you are less likely to leave bugs in the code, as all code paths will be at least somewhat checked by the compiler. Finally, you don’t need to handle situations in the engine when a shader reload is requested but the shaders cannot be compiled, and you don’t need to implement multi-threaded compilation to speed things up - and it can be tricky if the code requests shaders one by one.That said, it’s certainly possible to implement a custom mechanism for runtime shader compilation in an app that uses NVRHI, as the library just takes a shader binary and doesn’t care where it came from.Powered by Discourse, best viewed with JavaScript enabled"
1743,nvidia-rtx-top-3-week-of-december-14-2018,"Originally published at:			NVIDIA RTX Top 3: Week of December 14, 2018 | NVIDIA Technical Blog
Every week, we’ll be delivering 3 interesting stories coming from the world of RTX Game Development. 1 – NVIDIA Announces Nsight Graphics 2018.7 This week, we rolled out Nsight Graphics 2018.7. In this release, we enhanced our Ray Tracing support by adding the ability to view the scene Bounding Volume Hierarchy: the standardized acceleration structure…Powered by Discourse, best viewed with JavaScript enabled"
1744,simplifying-ai-inference-with-nvidia-triton-inference-server-from-nvidia-ngc,"Originally published at:			https://developer.nvidia.com/blog/simplifying-ai-inference-with-nvidia-triton-inference-server-from-nvidia-ngc/
Seamlessly deploying AI services at scale in production is as critical as creating the most accurate AI model. Conversational AI services, for example, need multiple models handling functions of automatic speech recognition (ASR), natural language understanding (NLU), and text-to-speech (TTS) to complete the application pipeline. To provide real-time conversation to users, such applications should be…Try building your own AI application leveraging Triton Inference Server today, and let us know of any questions or concerns!Hi, do you have any materials giving a comparison with TensorFlow TFX model serving ?Hi, I’m looking for an inference server to provide access to experimental models in our R&D department. Therefore, speed is not the top priority.
Do all models have to fit in the GPU memory at the same time? Or are models unloaded and reloaded if necessary?
In our scenario, many AI models (and old versions) are provided which in total would require more GPU memory than available, but a reload delay of a model which isn’t active would be tolerable at inference time.Powered by Discourse, best viewed with JavaScript enabled"
1745,cuda-and-nvcc,"i have installed cuda 10.2
but my nvidia-smi shows 11.1 version
and NVCC --version shows 10.2nvrtc: error: failed to open libnvrtc-builtins.so.11.1.
Make sure that libnvrtc-builtins.so.11.1 is installed correctly.Powered by Discourse, best viewed with JavaScript enabled"
1746,san-diego-supercomputer-center-celebrates-30-years-supporting-the-long-tail-of-science,"Originally published at:			San Diego Supercomputer Center Celebrates 30 Years Supporting the Long Tail of Science | NVIDIA Technical Blog
For the past 30 years, users of the San Diego Supercomputer Center (SDSC) systems have achieved major scientific breakthroughs spanning many domains, from earth sciences and biology to astrophysics, bioinformatics, and health IT. A few milestones include: 1987: Scientists take a major step in the new arena of rational drug design, determining the relative free…Powered by Discourse, best viewed with JavaScript enabled"
1747,using-ai-based-emulators-to-speed-up-simulations-by-billions-of-times,"Originally published at:			Using AI-Based Emulators to Speed Up Simulations by Billions of Times | NVIDIA Technical Blog
To simulate how subatomic particles interact, or how haze affects climate, scientists from Stanford University and the University of Oxford developed a deep learning-based method that can speed up simulations by billions of times.  Normally, a typical computer simulation calculates how physical forces affect atoms, clouds, galaxies, or whatever else is being modeled. This approach…Powered by Discourse, best viewed with JavaScript enabled"
1748,using-gans-to-improve-chair-design,"Originally published at:			https://developer.nvidia.com/blog/using-gans-to-improve-chair-design/
Developers Philipp Schmitt and Steffen Weiss recently unveiled a new system that uses a generative adversarial network (GAN) to generate classic 20th-century chair designs. “Many famous designers have designed chairs, and one could argue that there have been enough chair designs in the world for quite a while. Yet, designers still design chairs. They are…Are there any chairs made using this technology?Powered by Discourse, best viewed with JavaScript enabled"
1749,tips-for-optimizing-gpu-performance-using-tensor-cores,"Originally published at:			Tips for Optimizing GPU Performance Using Tensor Cores | NVIDIA Technical Blog
Our most popular question is “What can I do to get great GPU performance for deep learning?” We’ve recently published a detailed Deep Learning Performance Guide to help answer this question. The guide explains how GPUs process data and gives tips on how to design networks for better performance. We also take a close look at Tensor Core…Thanks for the post!I have a question about enabling Tensor cores. I wonder where should we set the value for ""the batch size and number of inputs and outputs, for a fully-connected layer and channels in and out, for a convolutional layer"" ?Glad you enjoyed the post!That depends on how you are running your network.  In our APIs and most frameworks, you can specify these parameters when you define a layer and its inputs and outputs.  Are you using cuBLAS or cuDNN, or a particular framework?Hi  Valerie, this blog said, ""Earlier versions of cuDNN required the channel dimension of all tensors  be a multiple of 8. That constraint no longer applies to packed NCHW data; cuDNN now automatically pads the tensors as needed."" but in this paper: ""We recommend ensuring all such parameters are multiples of 8 when training with FP16 and multiples of 16 when training with INT8. These include batch size and number of inputs and outputs, for a fully-connected layer and channels in and out, for a convolutional layer.""For a convolutional layer, is it necessary to ensure channel dimensions are multiples of 8 ?This is a very good question!  The blog you linked to is correct: with data in the NCHW layout, cuDNN performs automatic padding of channel in and out counts of convolutional layers, so in that case Tensor Cores will activate even when channels in and out are not set to multiples of 8.For brevity, this post focused on the strictest version of these rules: when using data in the NHWC layout, automatic padding won't occur. We talk about the difference between these formats in the Tensor Layouts section of the Deep Learning Performance Guide, if you'd like to read more.  The Channels In and Out section of the guide also explains in more detail how this affects the rules for channel counts. (Channels In and Out describes special case kernels for layers with four input channels as well, which may be of interest!)Thanks for the reply. I am using Caffe, I see that we can define a layer and its outputs, I guess the value of inputs in this case will be the outputs from last layer. But I am not sure I can define the batch size.Hi,I wonder if Tensor cores have wave quantization problem? Since different GPUs have different numbers of Tensor cores.I'm a little rusty with Caffe, but if memory serves, the batch size is controlled by the shape of the tensor you use as input during training or inference, which is probably defined in the data layer.  So the first dimension of your net.blobs, or other form of data tensor, would be the batch size for all layers.Which framework works best for Tensor cores?The overall Tensor Core count of a GPU doesn't have a separate wave quantization effect.  SM-based wave quantization, the sort that we talk about in this post, occurs because layer parameters can be set to any value.  So we can choose an inefficient batch size such that the training work can't be divided evenly among SMs once split into tiles / thread blocks.  You don't need to worry about this issue at the Tensor Cores level because the tile sizes available are designed to allow efficient work by the set of Tensor Cores in an SM!That question is very complex!  There isn't any single preferred framework.  Our Training With Mixed Precision guide, and in particular this section explaining how to set up and optimize for Tensor Cores in various frameworks, might be a good place to start.I see. Thank you!Do you mean that the tie sizes of Tensor cores are more flexible than the tie size options in cuBLAS?My wording wasn't precise, sorry!  By tile sizes, I mean those available in cuBLAS.  This sort of tiling doesn't occur at the Tensor Cores level.To illustrate, consider our feed-forward layer example from the post again.  With a batch size of 2048, the equivalent output matrix would have dimensions of 4096 x 2048.  Assuming the 256 x 128 tile size is used, 16 x 16 = 256 total tiles are created.  These tiles can't be split evenly between the 80 SMs on a Tesla V100 GPU, so this case suffers from wave quantization.With a tile size of 256 x 128, each SM handles the thread block for one tile at a time.  The amount of work done by this thread block is controlled by the tile size, and we design the available tile sizes such that the corresponding thread blocks can be calculated jointly by the Tensor Cores on an SM with maximum efficiency.  So you don't need to worry about wave quantization at this level.Put another way: a Tesla V100 GPU has 80 SMs, and each SM has 8 Tensor Cores, for a total of 640 Tensor Cores on the GPU.  However, wave quantization depends directly on the number of SMs and the tile size; the number of Tensor Cores isn't itself relevant.  (Your intuition that the number of Tensor Cores affects quantization is correct in that the number of Tensor Cores is 8 times the number of SMs on this GPU; the information is already being taken into account!)Hope this makes it clearer!I see it now. Thank you so much for the reply! This answer is great!Thanks for your reply! I have another question. In the cuDNN Developer Guide: ""For algorithms other than *_ALGO_WINOGRAD_NONFUSED, when the following requirements are met, the cuDNN library will trigger the Tensor Core operations: The number of input and output feature maps is a multiple of 8."" Question: For algorithms  *_ALGO_WINOGRAD_NONFUSED, what are the requirements ? Because the TF_ENABLE_WINOGRAD_NONFUSED variable is enabled by defaultPowered by Discourse, best viewed with JavaScript enabled"
1750,gtc-2020-learning-to-route-using-multi-agent-reinforcement-learning,"GTC 2020 T22125
Presenters: Amine Kerkeni,Instadeep; Alexandre Laterre, Instadeep
Abstract
Learn about deep multi-agent reinforcement learning on GPUs. We’ll schedule trains in the Flatland routing environment, although such scheduling is relevant to a wide range of industries. We’ll present different reinforcement learning (RL) solutions to address this specific challenge. Our step-by-step approach will help you discover some of the main difficulties in designing multi-agent deep RL systems and help you solve these challenges using GPU compute. We’ll present best practices for overcoming these hurdles. We’ll also compare centralized and decentralized multi-agent deep RL solutions. Specifically, we’ll look at the Deep Deterministic Policy Gradient as Independent Learner and Multi-Agent algorithms as a centralized learner. We’ll provide working code, and you’ll undertake a series of tasks evaluating how different solutions handle problem complexity and efficiency in GPU usage.Watch this session
Join in the conversation below.Powered by Discourse, best viewed with JavaScript enabled"
1751,share-your-science-simulating-smoke-propagation-in-real-time-with-openacc,"Originally published at:			https://developer.nvidia.com/blog/simulating-smoke-propagation-in-real-time-with-openacc/
Anne Severt, PhD student at Forschungszentrum Jülich in Germany shares how she is using NVIDIA Tesla K80s and OpenACC with complex geometries to create real-time simulations of smoke propagation to better prepare firefighters for real-life situations – such as where smoke will be propagating from underground metro stations over time. To learn more, view Anne’s…Powered by Discourse, best viewed with JavaScript enabled"
1752,deep-reinforcement-learning-agent-beats-atari-games,"Originally published at:			Deep Reinforcement Learning Agent Beats Atari Games | NVIDIA Technical Blog
Stanford researchers developed the first deep reinforcement learning agent that learns to beat Atari games with the aid of natural language instructions. “Humans do not typically learn to interact with the world in a vacuum, devoid of interaction with others, nor do we live in the stateless, single-example world of supervised learning,” mentioned the researchers…Powered by Discourse, best viewed with JavaScript enabled"
1753,tackling-ebola-with-gpus,"Originally published at:			Tackling Ebola With GPUs | NVIDIA Technical Blog
A team of researchers from University of Illinois at Urbana-Champaign and Stanford University are using the Blue Waters supercomputer at the National Center for Supercomputing Applications (NCSA) to predict what antibody would most likely pair best with a protein that coats a virus. The work focuses on two strains of the Ebola virus, and multiple…Powered by Discourse, best viewed with JavaScript enabled"
1754,data-science-best-practices-for-an-intelligent-edge-solution,"Originally published at:			https://developer.nvidia.com/blog/data-science-best-practices-for-an-intelligent-edge-solution/
Learn industry insights and best practice when implementing data science and AI at the edge.Powered by Discourse, best viewed with JavaScript enabled"
1755,amazon-expands-nvidia-gpu-backed-eks-availability,"Originally published at:			Amazon Expands NVIDIA GPU-backed EKS Availability | NVIDIA Technical Blog
If you are using GPU-enabled instances on the Amazon Elastic Container Service for Kubernetes (EKS), the company today announced expanded availability in the following cities, Frankfurt, Seoul, Singapore, Sydney, and Tokyo. Amazon EKS makes it easy to deploy, manage, and scale containerized applications using Kubernetes on AWS. The tool allows developers to use existing plugins…Powered by Discourse, best viewed with JavaScript enabled"
1756,cuda-spotlight-gpu-accelerated-speech-recognition,"Originally published at:			https://developer.nvidia.com/blog/cuda-spotlight-gpu-accelerated-speech-recognition/
This week’s Spotlight is on Dr. Ian Lane of Carnegie Mellon University. Ian is an Assistant Research Professor and leads a speech and language processing research group based in Silicon Valley. He co-directs the CUDA Center of Excellence at CMU with Dr. Kayvon Fatahalian. NVIDIA: Ian, what is Speech Recognition? Ian: Speech Recognition refers to…Powered by Discourse, best viewed with JavaScript enabled"
1757,nvidia-deep-learning-institute-public-workshop-summer-series-extended,"Originally published at:			https://developer.nvidia.com/blog/nvidia-deep-learning-institute-public-workshop-summer-series-extended/
Workshops are conducted live in a virtual classroom environment with expert guidance from NVIDIA-certified instructors.Powered by Discourse, best viewed with JavaScript enabled"
1758,autoscaling-nvidia-riva-deployment-with-kubernetes-for-speech-ai-in-production,"Originally published at:			https://developer.nvidia.com/blog/autoscaling-nvidia-riva-deployment-with-kubernetes-for-speech-ai-in-production/
Learn how to deploy NVIDIA Riva servers on a large scale with Kubernetes for autoscaling and Traefik for load balancing.Powered by Discourse, best viewed with JavaScript enabled"
1759,graphcuts-using-npp,"Originally published at:			https://developer.nvidia.com/blog/graphcuts-using-npp/
Have you ever wished you could change the background on an existing photo with you and your friends for fun or on a professional photo for publishing? If so, you’ll want to read on and learn about Graph Cut running on a GPU. This blog post will talk about the exciting new features and improvements that…Is there an implementation that can work on graphs with connection patterns other than 4- and 8-connected?Powered by Discourse, best viewed with JavaScript enabled"
1760,favorite-path-tracing-color,"What’s your favorite path tracing color?Green. Obvs. Pantone #76B900Powered by Discourse, best viewed with JavaScript enabled"
1761,monai-leaps-forward-with-automl-powered-model-development-and-cloud-native-deployments,"Originally published at:			MONAI Leaps Forward with AutoML-Powered Model Development and Cloud-Native Deployments | NVIDIA Technical Blog
Project MONAI continues to expand its end-to-end workflow with new releases and a new component called MONAI Deploy Inference Service.Powered by Discourse, best viewed with JavaScript enabled"
1762,new-ai-technologies-introduced-at-gtc-2020-keynote,"Originally published at:			https://developer.nvidia.com/blog/conversational-ai-recommenders-software-2020/
At GTC 2020 NVIDIA unveiled new state-of-the-art conversational AI frameworks and recommender systems. They include NVIDIA Riva, an application framework for multimodal conversational AI services that runs deep learning models under 300 milliseconds vs 25 seconds on CPUs. Also introduced, NVIDIA Merlin, a deep recommender framework that reduces training time from days to minutes on GPUsPowered by Discourse, best viewed with JavaScript enabled"
1763,adding-more-support-in-nvidia-gpu-operator,"Originally published at:			https://developer.nvidia.com/blog/adding-more-support-in-nvidia-gpu-operator/
Reliably provisioning servers with GPUs can quickly become complex as multiple components must be installed and managed to use GPUs with Kubernetes. The GPU Operator simplifies the initial deployment and management and is based on the Operator Framework. NVIDIA, Red Hat, and others in the community have collaborated on creating the GPU Operator. To provision…Powered by Discourse, best viewed with JavaScript enabled"
1764,advanced-api-performance-command-buffers,"Originally published at:			https://developer.nvidia.com/blog/advanced-api-performance-command-buffers/
Achieve performance gains on both the CPU and the GPU by maximizing parallelism, avoiding bottlenecks, and reducing idle times on the GPU.I hope you find those recommendations useful. I surely learned quite a bit while collecting this information. Command buffer preparation and scheduling involves knowledge of many aspects: GPU, CPU, multi-threading and OS scheduling.If you have any questions or comments, please let us know!Cheers!Powered by Discourse, best viewed with JavaScript enabled"
1765,dyndrite-unveils-first-gpu-accelerated-geometry-kernel-to-tackle-data-explosion-in-additive-manufacturing,"Originally published at:			Dyndrite Unveils First GPU-Accelerated Geometry Kernel to tackle Data Explosion in Additive Manufacturing | NVIDIA Technical Blog
To help manufacturers and developers manage the growing data problem in additive manufacturing, while streamlining production workflows, the team at Dyndrite has developed a new GPU-based platform: Accelerated Computation Engine (ACE), the world’s first GPU-accelerated geometry kernel.  Dyndrite is funded by Google’s Gradient Ventures, Carl Bass, ex-CEO of Autodesk, and a number other VC’s.  The…Powered by Discourse, best viewed with JavaScript enabled"
1766,revealing-new-features-in-the-cuda-11-5-toolkit,"Originally published at:			https://developer.nvidia.com/blog/revealing-new-features-in-the-cuda-11-5-toolkit/
CUDA 3D image to help represent it as a system platform, development environment and libraries.Powered by Discourse, best viewed with JavaScript enabled"
1767,accelerating-python-on-gpus-with-nvc-and-cython,"Originally published at:			https://developer.nvidia.com/blog/accelerating-python-on-gpus-with-nvc-and-cython/
The C++ standard library contains a rich collection of containers, iterators, and algorithms that can be composed to produce elegant solutions to complex problems. Most importantly, they are fast, making C++ an attractive choice for writing highly performant code. NVIDIA recently introduced stdpar: a way to automatically accelerate the execution of C++ standard library algorithms…Nice post – it’s great to see access to GPU from Python/Cython.I’m trying to use the jacobi_solver example in the post as a starting point to write a GPU-accelerated function to perform convolution using the for_each algorithm from standard C++ library.I don’t want to reinvent the wheel and was wondering if basic convolution of a kernel with a 2D array (e.g., using two 1D convolutions)  might already be implemented on GPU in a way similar to the example given in the jacobi_solver.Thanks!Thanks for your message!What you’re proposing sounds very much similar to the Jacobi example. You would need to write a functor (similar to avg) that encapsulates your kernel.In fact, if I’m understanding correctly, couldn’t the Jacobi solver be thought of as repeatedly applying the following kernel?@ashwint -  Thanks for the reply – that clarifies it a lot.In trying to write the code, I’ve noticed  NVIDIA HPC SDK doesn’t appear to be available for Windows yet.  Any idea when it will reach Windows?Thanks!Hi! Thanks again for your interest. We plan to have Windows support for the HPC SDK later this year.Hi @ashwint, thanks a lot for this great post! I have a quick question regarding the Figure showing the speedup over numpy sort: why is serial CPU processing doing better at smaller sample sizes? I understand that GPU parallel processing capacities are not enfolding their power at small sample sizes - but what creates the overhead?Thanks, @boehmvanessa. Likely, it’s the cost of transferring data from the host (GPU) to the device (GPU) and back.stdpar:Hi, thanks for the article!
I’m facing following task: I have cuda code in a .cu file which relies on the cublas library, a wrapper.pyx file and a setup.py which is based on this repository. Unfortunately, I don’t have any knowledge about setuptools or creating modules in general.
I want to create a python module, however I do not know how to link my cuda code with the cublas library in this case.
Could you give me a hint how to build a module from the .pyx and the .cu file which includes the cublas funtionalities? Manual build is also fine.
Thanks for your help!Hi - the setup.py file you linked to should largely work. I think you would need to add the cublas to the library_dirs, libraries, runtime_library_dirs, and  include_dirs arguments to the Extension constructor on line 112.Thanks Ashwin for this nice article.
I am trying to run code from GitHub - anuga-community/anuga_core: ANUGA for the simulation of the shallow water equation which is having compute intensive code written in C and using cython to create extension for Python.
The memory allocation is happening through numpy.
We also have an openmp version of compute intensive code running on CPU.The challenge im seeing is about memory allocation on GPU.
Can you give us some suggestion with latest trends which will simplify memory management?Cheers,
SamirPowered by Discourse, best viewed with JavaScript enabled"
1768,learn-from-nvidia-experts-at-gtc-drive-developer-day,"Originally published at:			https://developer.nvidia.com/blog/learn-from-nvidia-experts-at-gtc-drive-developer-day/
NVIDIA DRIVE solutions span from end-to-end—they are the tools NVIDIA engineers themselves use to build autonomous vehicle technology. Which is why, at NVIDIA GTC, developers have the chance to learn the latest features and how they can be applied to AV development from NVIDIA experts.Powered by Discourse, best viewed with JavaScript enabled"
1769,open-source-fleet-management-tools-for-autonomous-mobile-robots,"Originally published at:			https://developer.nvidia.com/blog/open-source-fleet-management-tools-for-autonomous-mobile-robots/
The newest Isaac ROS release includes new cloud– and edge-to-robot task management and monitoring software for autonomous mobile robot fleets.Powered by Discourse, best viewed with JavaScript enabled"
1770,transform-flat-images-into-high-resolution-3d-models,"Originally published at:			https://developer.nvidia.com/blog/transform-flat-images-into-high-resolution-3d-models/
Researchers from University of California, Berkeley developed a deep learning-based method that creates a 3D reconstruction from a single 2d color image. “Humans have the ability to effortlessly reason about the shapes of objects and scenes even if we only see a single image,” mentioned Christian Häne of the Berkeley Artificial Intelligence Research lab. “The question…Powered by Discourse, best viewed with JavaScript enabled"
1771,integrating-dynamic-diffuse-global-illumination-ddgi,"Originally published at:			Integrating Dynamic Diffuse Global Illumination (DDGI) | NVIDIA Technical Blog
With Ray-Traced Irradiance Fields Global illumination light leaking is a persistent problem for artists in the game industry. Fixing this issue typically requires tasks such as creating manual clipping geometry, moving individual probes, or changing lightmap parameterizations. These are time consuming, limit lighting and geometry iteration time, and only work for static geometry. The arrival…Powered by Discourse, best viewed with JavaScript enabled"
1772,georgia-tech-uc-davis-texas-a-m-join-nvail-program-with-focus-on-graph-analytics,"Originally published at:			https://developer.nvidia.com/blog/graph-technology-leaders-combine-forces-to-advance-graph-analytics/
NVIDIA is partnering with three leading universities — Georgia Tech, the University of California, Davis, and Texas A&M — as part of our NVIDIA AI Labs program, to build the future of graph analytics on GPUs.Powered by Discourse, best viewed with JavaScript enabled"
1773,new-online-course-offers-hands-on-machine-learning-using-aws-and-nvidia,"Originally published at:			https://developer.nvidia.com/blog/new-online-course-offers-hands-on-machine-learning-using-aws-and-nvidia/
AWS and NVIDIA have collaborated to develop an online course that introduces Amazon SageMaker with EC2 Instances powered by NVIDIA GPUs.the link on coursera
https://www.coursera.org/learn/machine-learning-aws-nvidia   is not working please fix the issuePowered by Discourse, best viewed with JavaScript enabled"
1774,ai-can-convert-black-and-white-clips-into-color,"Originally published at:			https://developer.nvidia.com/blog/ai-can-convert-black-and-white-clips-into-color/
Manually colorizing black and white video is labor intensive and a tedious process. But now, a new deep learning based algorithm developed by NVIDIA researchers promises to make the process a lot easier — the new framework allows visual artists to simply colorize one frame in a scene and the AI goes to work by…Powered by Discourse, best viewed with JavaScript enabled"
1775,an-interview-with-carbon-machina-on-real-time-ray-tracing,"Originally published at:			An Interview with Carbon Machina on Real-Time Ray Tracing | NVIDIA Technical Blog
An interview with Carbon Machina, the team behind the creation of Scavenger, a DXR Spotlight Contest 2020 Winner Carbon Machina believes in video games as an art form. Scavenger, their environmental demo, proves this; it uses real-time ray tracing to draw out a broad range of emotions in its audience. Warm yellow hues create a sense…Powered by Discourse, best viewed with JavaScript enabled"
1776,nvidia-achieves-4x-speedup-on-bert-neural-network,"Originally published at:			NVIDIA Achieves 4X Speedup on BERT Neural Network | NVIDIA Technical Blog
Among the many knotty problems that AI can help solve, speech and natural language processing (NLP) represent areas poised for significant growth in the coming years. Recently, a new language representation model called BERT (Bidirectional Encoder Representations from Transformers) was described by Google Research in a paper published on arXiv. According to the paper’s authors,…Powered by Discourse, best viewed with JavaScript enabled"
1777,gtc-2020-optimization-strategies-for-large-scale-dl-training-workloads-case-study-with-rn50-on-dgx-clusters,"GTC 2020 S21733
Presenters: Mohammad Zulfiqar,NVIDIA; Joshua Mora Acosta, NVIDIA
Abstract
Our tutorial will expose a list of optimizations for large-scale DL training workloads. We’ll give performance metrics and performance modeling of the deep-learning neural network as we scale the run, details on the executions at large scale, hardware subsystem’s performance and software layers, paired with profiling tools (NVPROF,NSYS), NVTX tagging, profile logging considerations, profile parsing, visualizing and analyzing (for example, tradeoffs) the profiled information to identify the opportunities to improve the performance at large scale and to guide and prioritize the optimization efforts. We’ll showcase those optimization strategies on training RN50 on large clusters of DGX1 and DGX2 machines up to 1,500 GPUs, which delivered a 2x performance improvement on the same amount of hardware. You need to be familiar with HW, SW, clusters, MPI, NCCL, profiling, deep-learning training, HPC, and performance metrics.Watch this session
Join in the conversation below.Feel free to post your feedback and questions here for everyone to learn. I’ll do my best to answer them promptly.Powered by Discourse, best viewed with JavaScript enabled"
1778,nvidia-ai-on-5g-for-enterprise-a-converged-platform-for-ai-and-5g-at-the-edge,"Originally published at:			https://developer.nvidia.com/blog/nvidia-ai-on-5g-for-enterprise-a-converged-platform-for-ai-and-5g-at-the-edge/
Learn how the NVIDIA AI-on-5G platform enables enterprises to deploy AI applications over their own private 5G network.Powered by Discourse, best viewed with JavaScript enabled"
1779,speech-recognition-generating-accurate-transcriptions-using-nvidia-riva,"Originally published at:			Speech Recognition: Generating Accurate Domain-Specific Audio Transcriptions Using NVIDIA Riva | NVIDIA Technical Blog
Thousands of companies are using speech AI to interact with customers. Learn the benefits of using speech AI with the NVIDIA Riva end-to-end pipeline.Jen, can you point me in the right direction to find a developer to integrate NVIDIA Riva with our existing transcription platform.I can help you direct to the correct person at NVIDIA. Let’s take this conversation offline to know more about your transcription platform.Powered by Discourse, best viewed with JavaScript enabled"
1780,building-generally-capable-ai-agents-with-minedojo,"Originally published at:			https://developer.nvidia.com/blog/building-generally-capable-ai-agents-with-minedojo/
NVIDIA is helping push the limits of training AI generalist agents with a new open-sourced framework called MineDojo.Powered by Discourse, best viewed with JavaScript enabled"
1781,how-to-read-research-papers-a-pragmatic-approach-for-ml-practitioners,"Originally published at:			https://developer.nvidia.com/blog/how-to-read-research-papers-a-pragmatic-approach-for-ml-practitioners/
This article presents an effective systematic method to approach reading research papers to be used as a resource for Machine Learning practitioners.Thanks for sharing this informative article! What would you suggest to stay on a single research paper?  I get lost  in reference papers or material while trying to understand unfamiliar terms/concept mentioned in a paper.Hi Yousaf1951,I too get lost in reference papers, especially when I feel the need to understand intrinsic details during my study. The problem with this is that there are a lof of reference papers to get through, and those reference paper also have their own set of reference paper, it’s almost never ending.Firstly in order to not get lost in reference papers, you have to admit to yourself that there will be some limit to your understanding. Secondly, if you must go through some reference papers, simply limit yourself to 2 and also go through the titles, abstract and conclusions of the reference paper to get some context.Again only you can determine how deep you want to go in your studies and research. I would advise sticking to the goals you set prior to starting to research.Regards,Richmond AlakePowered by Discourse, best viewed with JavaScript enabled"
1782,nvidia-helping-developers-get-started-with-deep-learning,"Originally published at:			NVIDIA Helping Developers Get Started with Deep Learning | NVIDIA Technical Blog
From self-driving cars to medical diagnostics, deep learning powered artificial intelligence is impacting nearly every industry. In 2015, NVIDIA’s Deep Learning Institute delivered more than 16,000 hours of training to help data scientists and developers master this burgeoning field of AI – and the need for deep learning training is rapidly growing. In the next…Powered by Discourse, best viewed with JavaScript enabled"
1783,catching-up-with-unreal-engine-about-the-unreal-e3-awards-2019,"Originally published at:			Catching up with Unreal Engine about the Unreal E3 Awards 2019 | NVIDIA Technical Blog
Every year, Unreal Engine celebrates the hard work of its development community with the Unreal E3 Awards. With E3 in full swing, we reached out to Unreal Engine to see how the nominating process was going. NVIDIA: Could you please provide us with an idea on how nominees and winners are determined? With so many…Powered by Discourse, best viewed with JavaScript enabled"
1784,creating-visualizations-of-large-molecular-systems-using-nvidia-omniverse,"Originally published at:			https://developer.nvidia.com/blog/creating-visualizations-of-large-molecular-systems-using-omniverse/
Wouldn’t it be amazing if you could create beautiful and immersive scientific visualizations of large and dynamic simulations like Folding@Home’s simulation of COVID-19 spikes? In this post, we share our recipe to show that you can use NVIDIA Omniverse to create powerful cinematic visualizations from scientific data. Figure 1. Bring in large dynamic simulation data…Powered by Discourse, best viewed with JavaScript enabled"
1785,release-nvidia-rtx-branch-of-unreal-engine-5-2,"Originally published at:			RTX Branch of Unreal Engine 4 (NvRTX) | NVIDIA Developer
This release offers Unreal Engine, NVIDIA RTX, and neural rendering advancements. Powered by Discourse, best viewed with JavaScript enabled"
1786,speech-recognition-deploying-models-to-production,"Originally published at:			Speech Recognition: Deploying Models to Production | NVIDIA Technical Blog
Deploy optimized services that can run in real-time using Riva, a GPU-accelerated SDK for developing speech applications.Powered by Discourse, best viewed with JavaScript enabled"
1787,ai-helps-predict-the-structure-of-any-protein,"Originally published at:			AI Helps Predict the Structure of Any Protein | NVIDIA Technical Blog
A new paper published in Cell systems by Harvard researcher Mohammed AlQuraishi describes a new deep learning-based approach for predicting the 3D structure of a protein based on its amino acid sequence. The work achieves high accuracy at speeds upward of a million times faster than previous methods. “Protein folding has been one of the…Powered by Discourse, best viewed with JavaScript enabled"
1788,gtc-2020-visualizing-exploring-and-analyzing-terabytes-of-multi-channel-image-volumes,"GTC 2020 D2S23
Presenters: Tech Demo Team,NVIDIA
Abstract
NVIDIA IndeX, a 3D volumetric, interactive visualization SDK running on NVIDIA GPUs and arivis5D, a web-based image management solution, seamlessly integrate to allow life science researchers to explore, visualize, and analyze terabytes of microscopy data, enabling faster and more confident insights.Watch this session
Join in the conversation below.Powered by Discourse, best viewed with JavaScript enabled"
1789,gtc-2020-making-visualization-hpc-and-machine-learning-workflows-efficient-on-nvidia-vgpu-on-vsphere,"GTC 2020 S21287
Presenters: Raj Rao,NVIDIA; Uday Kurkure,VMware
Abstract
We’ll discuss how to combine the performance of GPUs with manageability and scalability features to maximize GPU utilization for
the visualization, HPC, and machine learning workflows that are emerging in modern data centers, which thrive on the virtualization features of vSphere and NVIDIA virtual GPUs. We’ll outline end-to-end ML for training, deploying for inferencing, and managing a production environment using vSphere and Pivotal Kubernetes Service. We’ll describe ways to deploy GPU workloads developed with ML frameworks like TensorFlow & Caffe2 by using VMware DirectPathIO and NVIDIA vGPU, and we’ll provide case studies that leverage NVIDIA vGPU scheduling options such as Equal Share, Fixed Share, and Best Effort and illustrate their benefits.Watch this session
Join in the conversation below.Powered by Discourse, best viewed with JavaScript enabled"
1790,nvidia-deep-learning-sdk-update-for-volta-now-available,"Originally published at:			NVIDIA Deep Learning SDK Update for Volta Now Available | NVIDIA Technical Blog
At GTC 2017, NVIDIA announced Volta optimized updates to the NVIDIA Deep Learning SDK. Today, we’re making these updates available as free downloads to members of the NVIDIA Developer Program. Deep learning frameworks using NVIDIA cuDNN 7 and NCCL 2 can take advantage of new features and performance benefits of the Volta architecture. cuDNN 7…Powered by Discourse, best viewed with JavaScript enabled"
1791,path-tracing-vs-ray-tracing,"What is actually the difference between path tracing and ray tracing? Mostly they seem to be the same?Ray tracing refers to the mechanical process of tracing a ray through a scene and figuring out what it hits. There are many applications of ray tracing. Ray traced reflections, ray traced shadows, ray traced AO etc.Path tracing is also an application of ray tracing - but unlike those individual ‘effects’, path tracing addresses the whole of the rendering process in a single algorithm.
Specifically, in a path tracer, ray tracing is at least used to advance each path segment, and often for NEE (Next Event Estimation) light sampling visibility (shadow) tracing and similar. As an example, for a single pixel path traced sample, the path tracer might use anything from 5 to 50 or more ray traces depending on the properties of the encountered surfaces.Thank you!Powered by Discourse, best viewed with JavaScript enabled"
1792,speech-ai-technology-enables-natural-interactions-with-service-robots,"Originally published at:			https://developer.nvidia.com/blog/speech-ai-technology-enables-natural-interactions-with-service-robots/
From taking your order and serving you food in a restaurant to playing poker with you, service robots are becoming increasingly prevalent. Globally, you can find these service robots at hospitals, airports, and retail stores. According to Gartner, by 2030, 80% of humans will engage with smart robots daily, due to smart robot advancements in…Powered by Discourse, best viewed with JavaScript enabled"
1793,inline-gpu-packet-processing-with-nvidia-doca-gpunetio,"Originally published at:			https://developer.nvidia.com/blog/inline-gpu-packet-processing-with-nvidia-doca-gpunetio/
Learn how the new NVIDIA DOCA GPUNetIO Library can overcome some of the limitations found in the previous DPDK solution, moving a step closer to GPU-centric packet processing applications.A very interesting article. If the payloads of the packets were encrypted (e.g. they represent a TCP connection with TLS enabled) is there anything in DOCA to facilitate decryption/encryption. If not how would this be done in a CUDA kernel? ThxThanks for the feedback! I’m happy to answer your questions, however, this would currently require an NDA. You can register for Early Access here and I’ll connect with you then.Hi I am facing some difficulty.I want to use DPDK to perform burst transfer to My RTX 3090 GPU. I am unable to find any  methods. I would appreciate if someone can help me with thatPowered by Discourse, best viewed with JavaScript enabled"
1794,new-features-and-applications-make-deploying-edge-ai-easy-with-nvidia-fleet-command,"Originally published at:			New Features and Applications Make Deploying Edge AI Easy with NVIDIA Fleet Command | NVIDIA Technical Blog
New trial program enables users to experience brand new Fleet Command features and applications for free.Hi folks. I’m very excited about all of the momentum that Fleet Command has gathered since launch. I’m happy to answer any questions here if you have them.Hello Sir,Can Fleet Command support Jetson?Hey aliu1,Not yet, but it is something we want to support in the future. Fleet Command currently works with NVIDIA-Certified Systems, a catalog of which can be found here: Qualified System Catalog | NVIDIAHope this helps,
-THi Sir,Thanks for your prompt reply. No problem at all, there are few software partners in Jetson ecosystem who can provide Fleet Command-like service for our cases. Thank you.Best RegardsPowered by Discourse, best viewed with JavaScript enabled"
1795,reinventing-the-hearing-aid-with-deep-learning,"Originally published at:			https://developer.nvidia.com/blog/reinventing-the-hearing-aid-with-deep-learning/
Researchers at Ohio State University developed a GPU-accelerated program that can isolate speech from background noise and automatically adjust the volumes of each separately. Less than 25 percent of people who need a hearing aid actually use one – with the greatest frustration among users is the hearing aid cannot distinguish sounds that occur at…Powered by Discourse, best viewed with JavaScript enabled"
1796,reducing-acceleration-structure-memory-with-nvidia-rtxmu,"Originally published at:			https://developer.nvidia.com/blog/reducing-acceleration-structure-memory-with-nvidia-rtxmu/
RTXMU (RTX Memory Utility) combines both compaction and suballocation techniques to optimize and reduce memory consumption of acceleration structures for any DXR or Vulkan Ray Tracing application.Powered by Discourse, best viewed with JavaScript enabled"
1797,nvidia-waveworks-2-0-debuts-in-grapeshot-games-atlas,"Originally published at:			NVIDIA WaveWorks 2.0 Debuts in Grapeshot Games’ ATLAS | NVIDIA Technical Blog
Atlas is a new multiplayer pirate adventure from Grapeshot Games. The spectacular oceans in the game have been made possible by NVIDIA WaveWorks 2.0, the next generation of water simulation technology. We sat down and talked to Jesse Rapczak, the co-founder of Grapeshot Games, to find out what it was like to work with WaveWorks…Powered by Discourse, best viewed with JavaScript enabled"
1798,ai-helps-researchers-unlock-mysteries-of-vatican-archives,"Originally published at:			AI Helps Researchers Unlock Mysteries of Vatican Archives | NVIDIA Technical Blog
Researchers in Italy recently published the findings of a project called Codice Ratio, or “The Code System,” which demonstrates how deep learning is helping the Vatican transcribe a portion of their archives. The Vatican Secret Archives are made up of some fifty-three miles of shelving, thirty-five thousand volumes of the catalog, and twelve centuries worth…Powered by Discourse, best viewed with JavaScript enabled"
1799,nvidia-ai-generating-motion-capture-animation-without-hardware-or-motion-data,"Originally published at:			https://developer.nvidia.com/blog/nvidia-ai-generating-motion-capture-animation-without-hardware-or-motion-data/
NVIDIA researchers developed a framework to build motion capture animation without the use of hardware or motion data by simply using video capture and AI.NVDA Shareholder with an idea for development:
Holograms:Picture this and then imagine…A special occasion
is coming up, (Birthday,anniverary,wedding,graduation)
and you make a reservation for dinner at a personalized
venue with multiple choices for your entertainment.
An evening at this dinner event goes like this:
A pre selected famous figure chosen by you (a hologram of
John Wayne) guides you to your parking space.
Upon entering the venue, a Hostess greets you and asks for your
reservation name which includes your list of options
for the evening. A hologram of Frankenstein appears and
escorts you to your dinner room of choice.
A hologram of Star Wars R2D2 shows you to your table.
You have chosen the Storm room venue. A hologram of an
approaching thunderstorm begins to appear and fills the
room with clouds, thunder and lightning complete with
sound effects.
A hologram of a much larger than life flying hummingbird slowly
materializes above your table to take your order.
After dinner you all decide to stop by the wild west bar
for a drink. Various holograms of wild west characters having
gun fights appear in different areas of the room.
You sit down at the bar with a full mirrored back bar and in
the mirror various figures (chosen by you beforehand) appear
in the mirror on the bar stool next to you.Imagine:
The ultimate 3D experience…Hologram fantasies of choice.
nvidia is one of very few company’s with the technological
abilities to make this happen.
Sincerely,
Bryan Mailliard
B &C Mailliard
480 694 1367Looks great! Any plans to release source code or a implementation of this Motion Capture research?Hey @Sephiroth_FF, thanks for jumping into the forums! Currently the researchers are not planning on releasing the source code, the method currently relies on an unreleased project that will be presented a bit later this year. Keep an eye out for new research from the authors and a potential source code release afterwards!I have been watching for updates for this project on github and on the research project page. Any idea when we will get updates and a source code release? Thanks!Unfortunately, no additional plans have been made to progress with releasing the code. I will update this thread when I have any news related to this project.Powered by Discourse, best viewed with JavaScript enabled"
1800,using-tensor-cores-for-mixed-precision-scientific-computing,"Originally published at:			Using Tensor Cores for Mixed-Precision Scientific Computing | NVIDIA Technical Blog
Double-precision floating point (FP64) has been the de facto standard for doing scientific simulation for several decades. Most numerical methods used in engineering and scientific applications require the extra precision to compute correct answers or even reach an answer. However, FP64 also requires more computing resources and runtime to deliver the increased precision levels. Problem complexity…Really Nice that tensor cores speed-up simulations of ITER researchers! And thank you for the computational tricks to solve x.This is over my head! too much info...hehehePowered by Discourse, best viewed with JavaScript enabled"
1801,nvidia-launches-morpheus-early-access-program-to-enable-advanced-cybersecurity-solution-development,"Originally published at:			https://developer.nvidia.com/blog/nvidia-launches-morpheus-early-access-program-to-enable-advanced-cybersecurity-solution-development/
NVIDIA Morpheus gives security teams complete visibility into security threats with unmatched AI processing and real-time monitoring to protect every server and screen every packet in the data center.Powered by Discourse, best viewed with JavaScript enabled"
1802,optimizing-game-development-with-gpu-performance-events,"Originally published at:			GPU Performance Events: Best Practices | NVIDIA Technical Blog
GPU performance events can be used to instrument your game by labeling regions and marking important occurrences. A performance event represents a logical, hierarchical grouping of work, consisting of a begin/end marker pair. There are best practices for GPU performance events that are universally used by profiling tools such as NVIDIA Nsight Graphics and NVIDIA…Powered by Discourse, best viewed with JavaScript enabled"
1803,gtc-2020-from-shop-floor-to-engineering-workspaces-rich-immersive-experiences-across-the-enterprise-nvidia-cloudxr-and-esi-immersive-cloud-tech,"GTC 2020 S21432
Presenters: Jan Wurster,ESI Group
Abstract
We’ll explore the challenges in designing user-centric, collaborative enterprise extended-reality experiences, and the architectural considerations in deploying a scalable virtualized infrastructure to deliver democratized job-to-be-done applications to all of the transformed digital enterprise. By tethering to complex software and immobile hardware, high maintenance and complexity are still major obstacles to the ubiquitous adoption of virtual and augmented reality, especially in challenging environments such as production and manufacturing. Decoupling hardware and experience through high-efficiency streaming, NVIDIA CloudXR offers independence from physical workstations. By adding CloudXR capabilities to ESI’s immersive data center concept, enterprises can now democratize the benefits of AR and VR, optimizing the use of computing resources and data.Watch this session
Join in the conversation below.Powered by Discourse, best viewed with JavaScript enabled"
1804,analyizing-fabric-wide-network-latency-with-netq-4-1-0,"Originally published at:			https://developer.nvidia.com/blog/analyizing-fabric-wide-network-latency-with-netq-4-1-0/
NetQ 4.1.0 introduces fabric-wide network latency and buffer occupancy analysis, along with many other enhancements.Powered by Discourse, best viewed with JavaScript enabled"
1805,deepstream-next-generation-video-analytics-for-smart-cities,"Originally published at:			DeepStream: Next-Generation Video Analytics for Smart Cities | NVIDIA Technical Blog
Imagine every parent’s worst nightmare: a child lost in a crowded mall. Now imagine a scenario where that child is located and rescued in a matter of minutes using a network of cameras deployed within the building—and all the video is recorded, retrieved and analyzed in real time. This represents just one of many possibilities offered…Hello? Thanks for uploading this article. I have downloaded DeepStream SDK and found there is an example of resnet-18 that seemed to be modified to detect object boundaries. Since the resnet-18 of DeepStream SDK is quite different from the original resnet-18, I wish to download train.prototxt for the resnet-18 of DeepStream SDK; the DeepStream SDK isn't shipped with it. Where can I get it? Thanks,Are these interfaces (DeviceWorker,Models, and Tensors) supported on the DeepStreamSDK for Jetson?Hi Vivek,This article relates to DeepStream SDK version 1.5 for Tesla. The SDK applies to the Tesla family of GPUs and not Jetson. Therefore the DeviceWorker (and other classes included in deepstream.h) would not apply to DeepStream for Jetson. However the models and their output tensors processing logic are based on TensorRT, and hence can be used with the Jetson SDK as well.Please refer to DeepStream 2.0 for Tesla for commonality to the DeepStreamSDK for Jetson. They are both GStreamer based and share much of the interfaces and software architecture. We plan to continue to improve compatibility between our Jetson and dGPU platforms. Thanks.Thanks for the response!Powered by Discourse, best viewed with JavaScript enabled"
1806,learn-how-to-build-transformer-based-natural-language-processing-applications,"Originally published at:			https://developer.nvidia.com/blog/learn-how-to-build-transformer-based-natural-language-processing-applications/
Deep learning models have gained widespread popularity for natural language processing (NLP) because of their ability to accurately generalize over a range of contexts and languages. Transformer-based models, such as Bidirectional Encoder Representations from Transformers (BERT), have revolutionized NLP by offering accuracy comparable to human baselines on benchmarks like SQuAD for question-answer, entity recognition, intent…Powered by Discourse, best viewed with JavaScript enabled"
1807,top-ai-researchers-receive-first-nvidia-tesla-v100s,"Originally published at:			Top AI Researchers Receive First NVIDIA Tesla V100s | NVIDIA Technical Blog
At this week’s Computer Vision and Pattern Recognition conference in Honolulu, NVIDIA CEO Jensen Huang surprised a group of elite deep learning researchers at CVPR to unveil the NVIDIA Tesla V100, our latest GPU, based on our Volta architecture, by presenting it to 15 participants in our NVIDIA AI Labs program. “AI is the most…Powered by Discourse, best viewed with JavaScript enabled"
1808,upcoming-webinar-build-modern-recommender-systems-using-nvidia-merlin,"Originally published at:			https://info.nvidia.com/Recommender-Systems-Using-NVIDIA-Merlin-webinar.html
Join this webinar on January 17, or catch it on-demand, for a technical overview and a demo of NVIDIA Merlin—an end-to-end framework for building a modern recommenders pipeline.Powered by Discourse, best viewed with JavaScript enabled"
1809,announcing-the-nvidia-texture-tools-exporter-2020-1,"Originally published at:			Announcing the NVIDIA Texture Tools Exporter 2020.1 | NVIDIA Technical Blog
Today, we’re releasing the free NVIDIA Texture Tools Exporter, the new version of our DDS texture compression tool, available both as a standalone application and as a plugin for Adobe Photoshop. This all-new release adds support for modern, CUDA-accelerated Texture Tools 3.0 compression (including ASTC, BC7, and BC6s), support for more than 130 DXGI and…Powered by Discourse, best viewed with JavaScript enabled"
1810,oci-accelerates-hpc-ai-and-database-using-roce-and-nvidia-connectx,"Originally published at:			https://developer.nvidia.com/blog/oci-accelerates-hpc-ai-and-database-using-roce-and-nvidia-connectx/
Oracle Cluster Infrastructure uses an innovative approach to deliver scalable, RDMA-powered networking on Ethernet for a multitude of distributed workloads, providing higher performance and value.Powered by Discourse, best viewed with JavaScript enabled"
1811,gtc-2020-photoreal-design-workflows-with-nvidia-iray-the-siemens-experience,"GTC 2020 S22454
Presenters: Alexander Fuchs,NVIDIA ; Patti Longwinter, Siemens
Abstract
Learn how Siemens integrated raytracing into their NX CAD/CAM system using the Iray SDK and how their customers use this technology to communicate their products and ideas. The easy integration of the Iray SDK allowed Siemens to focus on their core competence in CAD/CAM, while adding photoreal visualization to their solution using the NVIDIA Material Definition Language. This permits their customers from multiple industries to collaboratively visualize their ideas with the push of a button, increasing productivity and decision quality.Watch this session
Join in the conversation below.Powered by Discourse, best viewed with JavaScript enabled"
1812,innovators-researchers-industry-leaders-meet-the-women-headlining-at-gtc,"Originally published at:			Innovators, Researchers, Leaders: Women Headlining at GTC | NVIDIA Blog
Hundreds of women speakers will present research and insights across industries at the upcoming GPU Technology Conference.Powered by Discourse, best viewed with JavaScript enabled"
1813,virtual-ai-challenge-aims-to-help-prosthetic-patients,"Originally published at:			https://developer.nvidia.com/blog/virtual-ai-challenge-aims-to-help-prosthetic-patients/
Stanford University recently announced a crowdsourcing competition that aims to solve problems in biomechanics. Participants in the “AI for Prosthetics” challenge will build models that can predict how patients will walk after getting a prosthesis. “Last year was more of a proof of concept,” said Łukasz Kidziński, a postdoctoral fellow in bioengineering at Stanford University…Powered by Discourse, best viewed with JavaScript enabled"
1814,streaming-interactive-deep-learning-applications-at-peak-performance,"Originally published at:			Streaming Interactive Deep Learning Applications at Peak Performance | NVIDIA Technical Blog
Imagine that you have just finished implementing an awesome, interactive, deep learning pipeline on your NVIDIA-accelerated data science workstation using OpenCV for capturing your webcam stream and rendering the output. A colleague of yours mentions that exploiting the novel TF32 compute mode of the Ampere microarchitecture third-generation Tensor Cores might significantly accelerate your application. So…I have written this library to make it easier to showcase deep learning solutions when I am on business travel. If you have questions or comments, please let me know.excelente justo lo que buscabaPowered by Discourse, best viewed with JavaScript enabled"
1815,fujitsu-upgrades-japan-s-ai-research-supercomputer-to-54-petaflops,"Originally published at:			Fujitsu Upgrades Japan’s AI Research Supercomputer to 54 Petaflops | NVIDIA Technical Blog
Fujitsu announced today they upgraded the RAIDEN deep learning supercomputers at RIKEN, Japan’s largest research institution, from four petaflops to 54 petaflops, making it one of the most powerful DGX-1 supercomputer installations in the world. Today’s announcement more than doubles the number of DGX-1 servers at the research institution which went from having 24 to…Powered by Discourse, best viewed with JavaScript enabled"
1816,upcoming-webinar-designing-efficient-vision-transformer-networks-for-autonomous-vehicles,"Originally published at:			https://info.nvidia.com/designing-efficient-vision-transformer-networks-for-autonomous-vehicles.html?nvid=nv-int-unbr-434072-vt03#cid=av02_nv-int-unbr_en-us
Explore design principles for efficient transformers in production and how innovative model design can help achieve better accuracy in AV perception.Powered by Discourse, best viewed with JavaScript enabled"
1817,hpl-ai-now-runs-2x-faster-on-nvidia-dgx-a100,"Originally published at:			https://developer.nvidia.com/blog/hpl-ai-now-runs-2x-faster-on-nvidia-dgx-a100/
NVIDIA announced its latest update to the HPL-AI Benchmark version 2.0.0, which will reside in the HPC-Benchmarks container version 21.4.Powered by Discourse, best viewed with JavaScript enabled"
1818,increasing-the-luminosity-of-beam-dynamics-with-gpus,"Originally published at:			https://developer.nvidia.com/blog/increasing-luminosity-beam-dynamics-gpus/
What is dark matter? We can neither see it nor detect it with any instrument. CERN is upgrading the LHC (Large Hadron Collider), which is the world’s largest and most powerful particle accelerator ever built, to explore the new high-energy frontier. The most technically challenging aspects of the upgrade cannot be done by CERN alone and…Powered by Discourse, best viewed with JavaScript enabled"
1819,gtc-2020-real-time-ray-traced-ambient-occlusion-of-complex-scenes-using-spatial-hashing,"GTC 2020 S22170
Presenters: Pascal Gautron,NVIDIA
Abstract
Ambient occlusion is an effective way to approximate global illumination: in essence, the closer a point is to its surroundings, the darker it gets. For real-time rendering, this effect is often approximated using screen-space techniques, leading to visible artifacts. Ray tracing provides a unique way to increase the rendering fidelity by accurately computing the distance to the surrounding objects, but it introduces sampling noise. Using the NVIDIA RTX technology available with Vulkan, we propose a real-time ray-traced ambient occlusion technique in which noise is removed in world space. Using spatial hashing for efficient storage, we’ll cover all the technical challenges to make ambient occlusion a production feature usable in CAD viewports with scenes comprising thousands of instances and hundreds of millions of polygons.Watch this session
Join in the conversation below.Powered by Discourse, best viewed with JavaScript enabled"
1820,gtc-2020-training-financial-language-models-on-bloombergs-kubernetes-data-science-platform,"GTC 2020 S21961
Presenters: Ian Hummel,Bloomberg ; CJ Zheng,Bloomberg
Abstract
We’ll explain how we leverage the Bloomberg Data Science Platform to accelerate training large neural networks on our enormous in-house corpora. We’ll discuss how we integrate open-source solutions, such as Kubernetes and KubeFlow, and review our results from pretraining BERT entirely from scratch. The Bloomberg Terminal provides data, analytics, news, and communication for professionals in business, finance, government, and philanthropy. Natural language processing (NLP) plays a growing role in our toolkit due to the increasing importance of textual data in modern finance. Transformer models such as BERT, XLNet and OpenAI’s GPT series achieve superior performance on a wide variety of NLP tasks by replacing recurrence of neurons with self-attention mechanisms. At Bloomberg, we train customized transformer LMs on finance-related corpora to improve performance of production tasks like named entity recognition, document classification, question answering, and more.Watch this session
Join in the conversation below.Powered by Discourse, best viewed with JavaScript enabled"
1821,gtc21-top-5-public-sector-technical-sessions,"Originally published at:			https://developer.nvidia.com/blog/gtc21-top-5-public-sector-technical-sessions/
This year at GTC you will join speakers and panelists considered to be the pioneers of AI who are transforming AI possibilities in government and beyond.Powered by Discourse, best viewed with JavaScript enabled"
1822,fighting-disease-carrying-mosquitoes-with-neural-networks,"Originally published at:			https://developer.nvidia.com/blog/fighting-disease-carrying-mosquitoes-with-neural-networks/
Targeting areas populated with disease-carrying mosquitoes just got easier thanks to a new study. The research, recently published in IEEE Explore, uses deep learning to recognize tiger mosquitoes from images taken by citizen scientists with near perfect accuracy. “Identifying the mosquitoes is fundamental, as the diseases they transmit continue to be a major public health…Powered by Discourse, best viewed with JavaScript enabled"
1823,unifying-the-cuda-python-ecosystem,"Originally published at:			Unifying the CUDA Python Ecosystem | NVIDIA Technical Blog
Python plays a key role within the science, engineering, data analytics, and deep learning application ecosystem. NVIDIA has long been committed to helping the Python ecosystem leverage the accelerated massively parallel performance of GPUs to deliver standardized libraries, tools, and applications. Today, we’re introducing another step towards simplification of the developer experience with improved Python…So even after cuda python release. We still have to write kernel code in c++? If so, PyCuda is already does the same right.Hi Maney,Yes, with this release requires writing kernels, with C++ in a string, to be compiled by NVRTC. This release is to build the foundation, by provide wrappers for CUDA Driver and Runtime API. There will be more functionality and flexibility to come. Currently, partner products such as Numba and CuPy write their own CUDA layer. This release removes the need for them to do this and provide an industry standard.If you’re interested in writing “Python” kernels, please take a look at Numba (Writing CUDA Kernels — Numba 0.50.1 documentation) and CuPy (User-Defined Kernels — CuPy 11.1.0 documentation) functionality.Hello,will PyCuda be useless with this new unified system ?AlainI wouldn’t consider PyCUDA useless. PyCUDA will have the ability to utilize our new infrastructure.Couple questions: If I have a program which is currently implemented in PyCUDA how difficult do you anticipate the switch, to CUDA Python, will be? Also will CUDA Python have support for GPU Arrays? (specifically for reductions and generating arrays of random numbers)Super excited to have an in house NVIDIA vetted CUDA Python package, and even more excited to try it out! ThanksHi morgjack,I very glad your excited. I haven’t use PyCUDA in a minute, but IIRC the conversion should be straight forward. You would need to use the corresponding Driver/Runtime API.As far as question 2, I’m not sure at the moment. I’m pretty sure that functionality currently exists in CuPy.This is great news, I have a lot of legacy code for Python extensions that I wish to port to PyCUDA, but this can provide now an officially supported approach to integrate advanced kernels. Is there any ETA when this will be officially released or when a preview will be available?The best timeline I can give you at the moment is 2H’21.I’m trying to use the new platform, but only importing some library, for example, I try to run the vectorAddDrv.py program and just include the library #include <cuComplex.h> in the cuda string and the compiler fails. :
/cuda-python-main/examples/0_Simple$ CUDA_HOME=/opt/cuda/11.4 python3 vectorAddDrv.py
Vector Addition (API Driver)
sourceCode.cu(10): catastrophic error: cannot open source file “cuComplex.h”1 catastrophic error detected in the compilation of “sourceCode.cu”.
Compilation terminated.CUDA error code=6(b’NVRTC_ERROR_COMPILATION’)Hi Emanuel,When that program passes the string to NVRTC, it sets --include-path=$CUDA_HOME/include. If the resulting directory doesn’t have cuComplex.h then we expect an error.Check that your /opt/cuda/11.4/include/cuComplex.h exist. Take a look at how this program uses the KernelHelper (link) to pass the --include-path.Hi, vzhurba,Thanks, I managed to solve it, the error was on my way CUDA_HOME, now it’s working perfectly. I tested a program of mine that I’m developing in my Master’s Thesis in pure Cuda-C and translated it to the new platform and realized that it still doesn’t achieve the same performance, but, the new platform proved to be much faster than PyCuda, which left me very much pleased. Congratulations.Powered by Discourse, best viewed with JavaScript enabled"
1824,performance-portability-for-gpus-and-cpus-with-openacc,"Originally published at:			https://developer.nvidia.com/blog/pgi-15-10-delivers-performance-portability-from-gpus-to-cpus-for-openacc/
New PGI compiler release includes support for C++ and Fortran applications to run in parallel on multi-core CPUs or GPU accelerators. OpenACC gives scientists and researchers a simple and powerful way to accelerate scientific computing applications incrementally. With the PGI Compiler 15.10 release, OpenACC enables performance portability between accelerators and multicore CPUs. The new PGI Fortran,…Powered by Discourse, best viewed with JavaScript enabled"
1825,can-we-use-nvidia-omniverse-to-build-virtual-assistant-3d-human-like-metahuman-and-talking-to-us-by-using-chatgpt-api,"Can we use NVIDIA Omniverse to build Virtual Assistant 3D human like MetaHuman and talking to us by using ChatGPT API work as custom agent?While you definitely can build your onw solution in NVIDIA Omniverse to animate virtual assistants, using technologies like Audio2Face in combination with speech synthesis tech like Riva and combine that with GPT-4, there are also 3rd party solutions in Omniverse that allow you to easily create Virtual Assistants using generative AI. One is Convai, you can check it out here: https://convai.com/ They have recently released an Extension for Omniverse that allows youto easily create your characters on their website and then interact with the in Omniverse. They were recently featured in the GTC keynote!This topic was automatically closed 14 days after the last reply. New replies are no longer allowed.Powered by Discourse, best viewed with JavaScript enabled"
1826,gtc-2020-future-of-iso-and-cuda-c,"GTC 2020 CWE21284
Presenters: Bryce Lelbach,NVIDIA; Michal Dominiak, ; David Olsen,
Abstract
Curious about the future of C++? Interested in learning about the C++ committee’s roadmap for safety critical, concurrent, parallel, and heterogeneous programming?Come join Olivier Giroux (chair of the C++ committee’s Concurrency and Parallelism group), Bryce Adelstein Lelbach (chair of the C++ committee’s Library Evolution Incubator and Tooling groups), and the rest of NVIDIA’s ISO C++ committee delegation for a Q&A session about the future of the C++ programming language.Connect directly with NVIDIA Experts to get answers to all of your questions on GPU programming and code optimization, share your experience, and get guidance on how to achieve maximum performance on NVIDIA’s platform.Watch this session
Join in the conversation below.Can you share the link for the YouTube playlist that Bryce was referring at beginning of the session (at 3:33 in the video)?I think he was referring to this:
https://www.youtube.com/watch?v=zoMZAV6FEbc&list=PLVFrD1dmDdvcOwDsNigchYDgBKIWFpcXKPowered by Discourse, best viewed with JavaScript enabled"
1827,featured-sessions-for-startups-at-gtc-2022,"Originally published at:			AI Developer Conference Session Catalog | GTC 2022 | NVIDIA
Learn how startups use AI to build solutions faster and accelerate their growth with these recommended sessions at GTC.Powered by Discourse, best viewed with JavaScript enabled"
1828,high-performance-matlab-with-gpu-acceleration,"Originally published at:			https://developer.nvidia.com/blog/high-performance-matlab-gpu-acceleration/
In this post, I will discuss techniques you can use to maximize the performance of your GPU-accelerated MATLAB® code. First I explain how to write MATLAB code which is inherently parallelizable. This technique, known as vectorization, benefits all your code whether or not it uses the GPU. Then I present a family of function wrappers—bsxfun, pagefun, and arrayfun—that…Vectorisation is important in Matlab whether working on the GPU or CPU if your interested in code execution speed.Did some quick tests on a CPU only Matlab without the Parallel Computing Toolbox addon:Taking your code, removing all the GPU references (gpuArray, gather), the vectorisation still give about a 400x speedup on my CPU.It seems Matlab's arrayfun is different between the standard built-in version and the version included in the Parallel Computing Toolbox. The standard version does not support the automatic expansion of variables (i.e. variables need to be the same size). It is only the Parallel Computing Toolbox that adds that feature. If I manually expand the data and then call the standard version it unfortunately results in about a 100x slow-down on my CPU. Not sure if this is due to the manual expansion or due to using arrayfun only on the CPU.pagefun is only available in the Parallel Computing Toolbox, so couldn't do any tests on that.Thanks for an interesting articleThat's true about arrayfun and pagefun. arrayfun on the CPU is just a convenience function and provides no benefit over a loop.Powered by Discourse, best viewed with JavaScript enabled"
1829,nvidia-nvail-partners-present-their-research-at-cvpr-2019,"Originally published at:			https://developer.nvidia.com/blog/nvidia-nvail-partners-present-their-research-at-cvpr-2019/
Many of our NVAIL partners are at CVPR this week presenting their top-tier research.Powered by Discourse, best viewed with JavaScript enabled"
1830,building-intelligent-video-analytics-apps-using-nvidia-deepstream-5-0-updated-for-ga,"Originally published at:			https://developer.nvidia.com/blog/building-iva-apps-using-deepstream-5-0-updated-for-ga/
Whether it’s a warehouse looking to balance product distribution and optimize traffic, a factory assembly line inspection, or hospital management, making sure that employees and caregivers use personal protection equipment (PPE) while attending to patients, advanced intelligent video analytics (IVA) turn out to be useful. At the foundational layer, there are billions of cameras and…Powered by Discourse, best viewed with JavaScript enabled"
1831,top-telecommunications-sessions-at-gtc-2022,"Originally published at:			AI Developer Conference Session Catalog | GTC 2022 | NVIDIA
Join us for sessions from AT&T, Verizon, T-Mobile, Ericsson, and more to discover the latest innovations in telecom.Powered by Discourse, best viewed with JavaScript enabled"
1832,how-do-dlss-and-ser-fit-in-with-path-tracing,"On your path-tracing developer page you list e.g. DLSS and SER as part of the “RTX Path Tracing core technologies”. How are they connected to path tracing?Technically, they’re unrelated to path tracing. But they are both accelerating technologies that help to make real time path tracing possible. SER in particular is designed to accelerate GPU workloads that are divergent in nature. Path tracing is by its nature a very divergent workload, so SER happens to be particularly beneficial for path tracing.Powered by Discourse, best viewed with JavaScript enabled"
1833,mlops-made-simple-cost-effective-with-google-kubernetes-engine-and-nvidia-a100-multi-instance-gpus,"Originally published at:			https://developer.nvidia.com/blog/mlops-made-simple-cost-effective-with-google-kubernetes-engine-and-nvidia-a100-multi-instance-gpus/
Google Cloud and NVIDIA collaborated to make MLOps simple, powerful, and cost-effective by bringing together the solution elements to build, serve and dynamically scale your end-to-end ML pipelines with the right-sized GPU acceleration in one place.Powered by Discourse, best viewed with JavaScript enabled"
1834,explainer-what-is-a-pod-what-is-a-cluster,"Originally published at:			What Is a Pod? What Is a Cluster? | NVIDIA Blog
Our digital lives run on collections of computers tightly linked on high-speed networks, and the latest one is an AI supercomputer called NVIDIA DGX SuperPOD.Powered by Discourse, best viewed with JavaScript enabled"
1835,enhancing-robotic-applications-with-the-nvidia-isaac-sdk-3d-object-pose-estimation-pipeline,"Originally published at:			https://developer.nvidia.com/blog/enhancing-robotic-applications-with-the-nvidia-isaac-sdk-3d-object-pose-estimation-pipeline/
In robotics applications, 3D object poses provide crucial information to downstream algorithms such as navigation, motion planning, and manipulation. This helps robots make intelligent decisions based on surroundings. The pose of objects can be used by a robot to avoid obstacles and guide safe robotic motion or to interact with objects in the robot’s surroundings.…Powered by Discourse, best viewed with JavaScript enabled"
1836,accelerating-large-scale-object-detection-with-tensorrt,"Originally published at:			Accelerating Large-Scale Object Detection with TensorRT | NVIDIA Technical Blog
Detecting the presence of humans accurately is critical to a variety of applications, ranging from medical monitoring in nursing homes to large-scale video analytics in various environments. High performance for deep learning training makes it possible to create robust and generalizable models for objects, humans, animals, and machines. Maintaining real-time inference performance in production environments…Is there a way to implement Yolov3 (maybe yolov3-tiny) with TensorRT?Sorry for late reply. Definitely you could! All you need is to implement neural-net layers which are not supported in TensorRT(v5 now) as custom plug-in layers.when can I use this on my phone?It could run on mobile phone, which just not full real-time for now. And this work is mainly focus on scale-out for large-scale application on GPU server side.@shounanan:disqus :  Where can I find the code to try this out?  ThanksNice Article. And also can you tell me what is the Precision of YOLOV2 Model ( 100 FPS YOLOV2) . It didn't mentioned anywhere.Sir, I want the customise the faster rcnn with the tensorrt plugin. I have read from the documentation that faster rcnn with the object detection with tensorrt plugin works with the fixed size 3 channel 375x500 images as input. When I input  image of size 5000 *600 faster rcnn does not gave any results. I am struck with this and I don't know how to cope with this error.Any ideas are welcomed here.Powered by Discourse, best viewed with JavaScript enabled"
1837,boost-your-ai-workflows-with-federated-learning-enabled-by-nvidia-flare,"Originally published at:			https://developer.nvidia.com/blog/boost-your-ai-workflows-with-federated-learning-enabled-by-nvidia-flare/
NVIDIA FLARE 2.3.0 enables you to quickly deploy to multi-cloud and explore NLP examples for LLMs, and demonstrates split learning capability.Powered by Discourse, best viewed with JavaScript enabled"
1838,neural-network-generates-global-tree-height-map-reveals-carbon-stock-potential,"Originally published at:			https://developer.nvidia.com/blog/neural-network-generates-global-tree-height-map-reveals-carbon-stock-potential/
Using remote sensing and an ensemble of convolutional neural networks, the study could guide sustainable forest management and climate mitigation efforts.Powered by Discourse, best viewed with JavaScript enabled"
1839,accelerating-materials-discovery-with-cuda,"Originally published at:			https://developer.nvidia.com/blog/accelerating-materials-discovery-cuda/
In this post, we discuss how CUDA has facilitated materials research in the Department of Chemical and Biomolecular Engineering at UC Berkeley and Lawrence Berkeley National Laboratory. This post is a collaboration between Cory Simon, Jihan Kim, Richard L. Martin, Maciej Haranczyk, and Berend Smit. Engineering Applications of Nanoporous Materials Figure 1: The repeating crystal structure…Powered by Discourse, best viewed with JavaScript enabled"
1840,understanding-aesthetics-with-deep-learning,"Originally published at:			https://developer.nvidia.com/blog/understanding-aesthetics-deep-learning/
To me, photography is the simultaneous recognition, in a fraction of a second, of the significance of an event. — Henri larartier Bresson As a child I waited anxiously for the arrival of each new issue of National Geographic Magazine. The magazine had amazing stories from around the world, but the stunningly beautiful photographs were more important…Great post! I can't find aesthetic sorting on EyeEM's search results today (March 8th, 2016) - where can I try this?Hi Appu Shaji.Great article and very interesting work. You may remember me from IIT days :)I have a technical question here. Why use triplet hinge loss ? Why not use a classifier to separate images as aesthetic vs not. May be the idea is to also find similar images to high quality images using the embedding and allow image search.VineethHi Vineeth,Two major reason in using triplet hinge loss:1. Often there is no correct classification if an image is aesthetics or not, and bucketing it into a class is non-trivial. Further, our understanding of aesthetics is perceptual and relative, so this has to be approached as a ranking problem, rather than a classification problem. Hence the motivation for using implicit regression/ranking framework. This is true for other cases, like image similarity, where the boundaries are fuzzy. Further aesthetics is deeply rooted to context/story embedded in a photo. Our sampling scheme is based on similarity based on keyword detections, this indirectly enforces a context; so aesthetics is judged relatively (ranked ) to other samples within a context.2. Further, there is an order of O(N^3) samples we can generate from a training set of N images. For example, a dataset of 2N images, split into N good and N bad ones, there are Combination(N,2) * Combination(N,1) samples available.And of course, as you mentioned, they are interesting applications of discovery ( like similarity ) inside a embedding space.-appuGreat post! may I ask why you use three images as input instead of two images as input?Thanks!Photography is artMore than ArtI love itkecabang.blogspot.comHi Appu Shaji,Thank you for sharing information. Could you please answer these questions regarding to the steps when you generating training dataset?1. How do judges label the images in detail? Is it 1.1 or 1.2?    1.1 Given 10 images having the same context (by a similarity measure), judges label the images by giving score in range 1-5.     1.2 Given a pair of images having the same context, judges label the images by selecting a better one.2. How does the process of sampling works? below is my understanding, is that correct?   - Let's say we have total 100K images. Firstly, images are grouped based on keyword detection.    - We then sample the images for each group separately because we need the examples to have the same context.EkkalakInteresting !! do you know how AI can Assist Radiologists in Pneumonia Detection? https://blog.skyl.ai/how-ai... this article is very interesting relater to healthcare.Powered by Discourse, best viewed with JavaScript enabled"
1841,automatic-defect-inspection-using-the-nvidia-end-to-end-deep-learning-platform,"Originally published at:			Automatic Defect Inspection Using the NVIDIA End-to-End Deep Learning Platform | NVIDIA Technical Blog
Quality requirements for manufacturers are increasing to meet customer demands. Manual inspection is usually required to guarantee product quality, but this requires significant cost and can result in production bottlenecks, lowered productivity, and reduced efficiency.  Defect inspection for industrial applications has unique characteristics and challenges compared to other computer vision problems for consumer applications: Lack…Powered by Discourse, best viewed with JavaScript enabled"
1842,5-startups-betting-their-future-on-deep-learning,"Originally published at:			https://developer.nvidia.com/blog/5-deep-learning-startups-betting-on-the-future-with-gpu-computing/
A generation of startups are now putting artificial intelligence into the hands of millions. NVIDIA GPUs and deep learning software power much of this work. A billboard ad that guesses your age. A photo app that recognizes your face. A digital math aide that solves quadratic equations. A billboard ad that guesses your age. A…Powered by Discourse, best viewed with JavaScript enabled"
1843,ray-tracing-gems-ii-available-august-4th,"Originally published at:			Ray Tracing Gems II: Available August 4th | NVIDIA Technical Blog
We are pleased to announce that Ray Tracing Gems II, the follow up to 2019’s Ray Tracing Gems, will be available for digital download and print on August 4th, 2021.Is there a preliminary table of contents for the accepted gems?@billkatz – Thanks for expressing interest in the contents of Ray Tracing Gems 2! We’ll provide information on the contents of the book later this year.Powered by Discourse, best viewed with JavaScript enabled"
1844,choosing-a-server-for-deep-learning-training,"Originally published at:			https://developer.nvidia.com/blog/choosing-a-server-for-deep-learning-training/
Learn about the characteristics of various accelerated workload categories and the system features needed to run them.Powered by Discourse, best viewed with JavaScript enabled"
1845,gtc-2020-edge-ai-meets-5g-nvidia-and-mavenir-transform-edge-ai,"Originally published at:			https://developer.nvidia.com/blog/gtc-2020-edge-ai-meets-5g-nvidia-and-mavenir-transform-edge-ai/
At GTC, NVIDIA and Mavenir will present an end-to-end 5G Edge IVA solution running on the NVIDIA EGX platform.Powered by Discourse, best viewed with JavaScript enabled"
1846,gtc-2020-accelerating-apache-spark-3-0-with-rapids-and-gpus,"GTC 2020 S22674
Presenters: Robert Evans,NVIDIA; Jason Lowe, NVIDIA
Abstract
Utilizing accelerators in Apache Spark presents opportunities for significant speedup of ETL, ML and DL applications. In this talk we give an overview of accelerator aware task scheduling, columnar data processing support, fractional scheduling, and stage level resource scheduling and configuration. Furthermore, we dive into the Apache Spark 3.x RAPIDS plugin, which enables applications to take advantage of GPU acceleration with no code change. An explanation of how the Catalyst optimizer physical plan is modified for GPU aware scheduling is reviewed. The talk touches upon how the plugin can take advantage of the RAPIDS specific libraries, cudf, cuio and rmm to run tasks on the GPU. Optimizations were also made to the shuffle plugin to take advantage of the GPU using UCX, a unified communication framework that addresses GPU memory intra and inter node.Watch this session
Join in the conversation below.Powered by Discourse, best viewed with JavaScript enabled"
1847,analysis-driven-optimization-preparing-for-analysis-with-nvidia-nsight-compute-part-1,"Originally published at:			https://developer.nvidia.com/blog/analysis-driven-optimization-preparing-for-analysis-with-nvidia-nsight-compute-part-1/
In this three-part series, you discover how to use NVIDIA Nsight Compute for iterative, analysis-driven optimization. Part 1 covers the background and setup needed, part 2 covers beginning the iterative optimization process, and part 3 covers finishing the analysis and optimization process and determining whether you have reached a reasonable stopping point. Nsight Compute is…I hope you found the post interesting and useful.  If you have specific questions about the topics in the post, this is a good place to ask them.  For more general questions about profiler usage, or running the profiler on your own code, I encourage you to ask those in the appropriate profiler section(s) on our forums:Nsight Compute is an interactive kernel profiler for CUDA applications. It provides detailed performance metrics and API debugging via a user interface and command line tool.Powered by Discourse, best viewed with JavaScript enabled"
1848,ai-app-brings-your-photos-to-life-with-3d-animation,"Originally published at:			AI App Brings Your Photos to Life with 3D Animation | NVIDIA Technical Blog
A new innovative mobile app called Mug Life leverages deep learning to let you instantly create 3D animations from any uploaded photo. “The ability to manipulate photos in Mug Life is tremendously fun and addicting,” explains Rob Cohen, Co-Founder and CEO of Mug Life. “We spent years lovingly creating Mug Life to bring people an…Powered by Discourse, best viewed with JavaScript enabled"
1849,get-certified-on-the-fundamentals-of-deep-learning,"Originally published at:			Get Certified on the Fundamentals of Deep Learning | NVIDIA Technical Blog
The NVIDIA Deep Learning Institute (DLI) is offering instructor-led, hands-on training on how deep learning works through hands-on exercises in computer vision and natural language processing. Deep learning is a powerful AI approach that uses multi-layered artificial neural networks to deliver state-of-the-art accuracy in tasks such as object detection, speech recognition, and language translation. Using…Powered by Discourse, best viewed with JavaScript enabled"
1850,expedite-the-development-testing-and-training-of-ai-robots-with-isaac-sim,"Originally published at:			https://developer.nvidia.com/blog/expedite-the-development-testing-and-training-of-ai-robots-with-isaac-sim/
This release of Isaac Sim adds more tools for AI-based robotics including Isaac Gym support for RL, Isaac Cortex for cobot programming, and much more.Powered by Discourse, best viewed with JavaScript enabled"
1851,ai-algorithm-diagnoses-rare-eye-condition,"Originally published at:			https://developer.nvidia.com/blog/ai-algorithm-diagnoses-rare-eye-condition/
A group of Chinese ophthalmologists and scientists developed a deep learning algorithm to identify congenital cataracts, a rare eye disease responsible for nearly 10 percent of all vision loss in children worldwide. The researchers suggest the algorithm would assist humans, instead of replacing them. “For doctors, technology is not sufficient to determine the best course…Powered by Discourse, best viewed with JavaScript enabled"
1852,delivering-one-click-vr-streaming-using-innoactive-portal-and-nvidia-cloudxr,"Originally published at:			https://developer.nvidia.com/blog/delivering-one-click-vr-streaming-using-innoactive-portal-and-nvidia-cloudxr/
Building one-click delivery of PC-based VR applications to standalone devices by streaming from the Cloud using NVIDIA CloudXR SDK and Innoactive Portal.Powered by Discourse, best viewed with JavaScript enabled"
1853,coffee-break-ray-plus-raster-era-begins,"Originally published at:			Coffee Break: Ray Plus Raster Era Begins | NVIDIA Technical Blog
After decades of research, NVIDIA has unearthed the holy grail of video game graphics: real-time ray tracing! This series of videos will explain why you need to add ray tracing to your pipeline now. The idea isn’t to use ray tracing as the only rendering technique, but to combine it with traditional rasterization to generate…Powered by Discourse, best viewed with JavaScript enabled"
1854,volta-tensor-core-gpu-achieves-new-ai-performance-milestones,"Originally published at:			Volta Tensor Core GPU Achieves New AI Performance Milestones | NVIDIA Technical Blog
Artificial intelligence powered by deep learning now solves challenges once thought impossible, such as computers understanding and conversing in natural speech and autonomous driving. Inspired by the effectiveness of deep learning to solve a great many challenges, the exponentially growing complexity of algorithms has resulted in a voracious appetite for faster computing. NVIDIA designed the…Too fast. I'm starting to feel ""lacking"" with my quadro k420.Can you share the resnet50 settings you used please so I can compare on my V100s?Which version of Tensorflow we can use to reproduce this performance?Hi All,I have some Query regarding GPU Utility1. what is GPU Utility.2.what are the feature and benefits of GPU Utility.3.How can we configure in our environments.4.how we monitor user_login,GPU_POWER consuming,GPU core clock speed.Please i need in depth understanding in above points.It will be great if you could share the implementation as well as settings since the Nvidia guide only reports 660 img/sec: https://docs.nvidia.com/dee...Is the Data Augmentation Library available for download?Why would I want anything to do with clouds?Clouds are the biggest scam since carbon taxHi, When will these great optimization be included in CUDNN etc. and included in the standard Tensorflow package? Appreciate and Thanks!How can I replicate the ""Time to Train Facebook’s Fairseq"" data on DGX-2 system?Loved the post! Thanks so much.Any plans implementing fused kernels for Pytorch and Tensorflow in the near future?Powered by Discourse, best viewed with JavaScript enabled"
1855,cuda-spotlight-gpu-accelerated-high-energy-physics,"Originally published at:			https://developer.nvidia.com/blog/cuda-spotlight-gpu-accelerated-high-energy-physics/
This week’s Spotlight is on Valerie Halyo, assistant professor of physics at Princeton University. Researchers in the field of high energy physics, such as Valerie, are exploring the most fundamental questions about the nature of the universe, looking for the elementary particles that constitute matter and its interactions. One of Valerie’s goals is to extend…Powered by Discourse, best viewed with JavaScript enabled"
1856,cuda-pro-tip-fast-and-robust-computation-of-givens-rotations,"Originally published at:			https://developer.nvidia.com/blog/cuda-pro-tip-fast-robust-computation-givens-rotations/
A Givens rotation [1] represents a rotation in a plane represented by a matrix of the form , where the intersections of the th and th columns contain the values and . Multiplying a vector by a Givens rotation matrix represents a rotation of the vector in the plane by radians. According to Wikipedia, the…One of the first investigations of the performance of a Fast Givens Rotation algorithm to calculate a QR factorization can be found in, ""Benchmarking the NVIDIA 8800GTX with the CUDA Development Platform"" (McGraw-Herdeg, et al, 2007 MIT/Lincoln Labs HPEC Workshop).  QR factorization is part of the HPEC Challenge Benchmark Suite.  Having a reciprocal hypotenuse function will be of great benefit to a lot of signal and image processing applications.   The HPEC abstract can be found here: http://www.ll.mit.edu/HPEC/...Cool stuff. Any plans to make a complex given rotation implementation?Powered by Discourse, best viewed with JavaScript enabled"
1857,gtc-2020-fostering-a-strong-ecosystem-for-ai-in-medical-imaging,"GTC 2020 S22631
Presenters: Geraldine McGinty,Weill Cornell Medicine
Abstract
In order to fully leverage the possibilities that AI offers to drive higher-value health care, we need to create an effective collaboration between physicians and developers, policymakers and payers, and most importantly the patients we will serve. We’ll explore ways in which those collaborations are already happening and highlight gaps and opportunities. The speaker, a practicing radiologist specializing in breast imaging, will discuss the ways in which she sees AI impacting her practice now and in the future.Watch this session
Join in the conversation below.Powered by Discourse, best viewed with JavaScript enabled"
1858,accelerating-io-in-the-modern-data-center-network-io,"Originally published at:			https://developer.nvidia.com/blog/accelerating-io-in-the-modern-data-center-network-io/
This is the second post in the Explaining Magnum IO series, which described the architecture, components, and benefits of Magnum IO, the IO subsystem of the modern data center. The first post in this series introduced the Magnum IO architecture and positioned it in the broader context of CUDA, CUDA-X, and vertical application domains. Of…We’d love to hear how these technologies have helped with your applications, and we’d appreciate getting concrete usage models that motivate extensions to current offerings.  Thanks for being part of Magnum IO.Powered by Discourse, best viewed with JavaScript enabled"
1859,gtc-2020-optimizing-cuda-kernels-in-hpc-simulation-and-visualization-codes-using-nvidia-nsight-compute,"GTC 2020 S21771
Presenters: Magnus Strengert,NVIDIA; John Stone, University of Illinois at Urbana Champaign
Abstract
Do you want to analyze and tune the performance of your CUDA kernels? We’ll show you how NVIDIA Nsight Compute can maximize their performance. NVIDIA engineers, and the developers of molecular modeling tools at University of Illinois, will share their experiences using NVIDIA Nsight Compute to analyze and optimize several CUDA/Optix kernels in HPC applications, such as VMD and NAMD. We’ll highlight several intermediate and advanced kernel profiling techniques and show you how to iteratively identify bottlenecks and improve your kernel performance.Watch this session
Join in the conversation below.Powered by Discourse, best viewed with JavaScript enabled"
1860,video-series-mixed-precision-training-techniques-using-tensor-cores-for-deep-learning,"Originally published at:			Video Series: Mixed-Precision Training Techniques Using Tensor Cores for Deep Learning | NVIDIA Technical Blog
Neural networks with thousands of layers and millions of neurons demand high performance and faster training times. The complexity and size of neural networks continue to grow. Mixed-precision training using Tensor Cores on Volta and Turing architectures enable higher performance while maintaining network accuracy for heavily compute- and memory-intensive Deep Neural Networks (DNNs). This post will get you…Powered by Discourse, best viewed with JavaScript enabled"
1861,scaling-data-pipelines-at-t-optimizes-speed-cost-and-efficiency-with-gpus,"Originally published at:			Scaling Data Pipelines: AT&T Optimizes Speed, Cost, and Efficiency with GPUs | NVIDIA Technical Blog
See how AT&T’s data teams used NVIDIA RAPIDS Accelerator for Apache Spark to quickly process trillions of records in large datasets on GPUs.Powered by Discourse, best viewed with JavaScript enabled"
1862,download-deepstream-sdk-2-0-today-to-develop-scalable-video-analytics-applications,"Originally published at:			Download DeepStream SDK 2.0 Today to Develop Scalable Video Analytics Applications | NVIDIA Technical Blog
NVIDIA has released the DeepStream Software Development Kit (SDK) 2.0 for Tesla GPUs, which is a key part of the NVIDIA Metropolis platform. The technology enables developers to design and deploy scalable AI applications for intelligent video analytics (IVA). DeepStream SDK 2.0 includes TensorRT and CUDA to incorporate the latest AI techniques and accelerate video analytics workloads. DeepStream…Powered by Discourse, best viewed with JavaScript enabled"
1863,video-tutorial-introduction-to-recurrent-neural-networks-in-tensorrt,"Originally published at:			Video Tutorial: Introduction to Recurrent Neural Networks in TensorRT | NVIDIA Technical Blog
NVIDIA TensorRT is a high-performance deep learning inference optimizer and runtime that delivers low latency and high-throughput. TensorRT can import trained models from every deep learning framework to easily create highly efficient inference engines that can be incorporated into larger applications and services. This video demonstrates how to configure a simple Recurrent Neural Network (RNN)…Powered by Discourse, best viewed with JavaScript enabled"
1864,get-started-with-ai-on-the-nvidia-jetson-nano-dli-course,"Originally published at:			https://developer.nvidia.com/blog/get-started-with-ai-on-the-nvidia-jetson-nano-dli-course/
Looking to get started with AI but don’t know how? NVIDIA has just published a new self-paced Deep Learning Institute course that uses the newly released Jetson Nano Developer Kit to get up and running fast.  In the course, students will learn to collect image data and use it to train, optimize, and deploy AI…Powered by Discourse, best viewed with JavaScript enabled"
1865,learn-how-to-use-deep-learning-for-industrial-inspection,"Originally published at:			https://developer.nvidia.com/blog/learn-how-to-use-deep-learning-for-industrial-inspection/
Register now for the Sept. 21 instructor-led training from DLI covering training, accelerating, and optimizing a defect detection classifier.Powered by Discourse, best viewed with JavaScript enabled"
1866,ai-system-helps-detect-and-manage-traffic-incidents,"Originally published at:			https://developer.nvidia.com/blog/ai-system-helps-detect-and-manage-traffic-incidents/
Iowa State University researchers are developing a deep learning-based system to help the Iowa Department of Transportation improve incident detection and support operator decision-making. “There is more data than you could ever imagine coming out of this system,” said Neal Hawkins, the associate director of Iowa State’s Institute for Transportation. “We’re getting data every 20…Powered by Discourse, best viewed with JavaScript enabled"
1867,the-technology-behind-the-viral-prisma-photo-app,"Originally published at:			The Technology Behind the Viral Prisma Photo App | NVIDIA Technical Blog
The new Prisma mobile app that transforms your photos into a work of art has been downloaded nearly 17 million times since it was released last month on iOS devices and is now surpassing 2 million a day after it was made available for Android users this week. People of all ages and celebrities around…Powered by Discourse, best viewed with JavaScript enabled"
1868,icymi-new-ai-tools-and-technologies-announced-at-nvidia-gtc-keynote,"Originally published at:			ICYMI: New AI Tools and Technologies Announced at NVIDIA GTC Keynote | NVIDIA Technical Blog
New AI software tools include Riva Customer Voice, TensorRT, Triton Inference Server, Merlin, NeMo Megatron, and DeepStream.Powered by Discourse, best viewed with JavaScript enabled"
1869,new-features-to-dlss-coming-through-nvidia-rtx-unreal-engine-4-branch,"Originally published at:			New Features to DLSS Coming through NVIDIA RTX Unreal Engine 4 Branch | NVIDIA Technical Blog
Powered by dedicated AI processors on RTX GPUs called Tensor Cores, the latest update to DLSS introduces a new ultra performance mode that accelerates performance to support 8K gaming.This DLSS update provides the following key benefits: New ultra performance mode for 8K gaming. Deliver 8K gaming on GeForce RTX 3090 with DLSS.Improved VR support. Maintaining…Powered by Discourse, best viewed with JavaScript enabled"
1870,securing-llm-systems-against-prompt-injection,"Originally published at:			https://developer.nvidia.com/blog/securing-llm-systems-against-prompt-injection/
This post explains prompt injection and shows how the NVIDIA AI Red Team identified vulnerabilities where prompt injection can be used to exploit three plug-ins included in the LangChain library.Powered by Discourse, best viewed with JavaScript enabled"
1871,accelerate-opencv-optical-flow-algorithms-with-nvidia-turing-gpus,"Originally published at:			Accelerate OpenCV: Optical Flow Algorithms with NVIDIA Turing GPUs | NVIDIA Technical Blog
OpenCV is a popular open-source computer vision and machine learning software library with many computer vision algorithms including identifying objects, identifying actions, and tracking movements. The tracking algorithms use optical flow to compute motion vectors that represent the relative motion of pixels (and hence objects) between images. Computation of optical flow vectors is a computationally…Thank you for your post! I tried to read NVIDIA Optical Flow Programming Guide in References.But, I think that this URL is wrong.Powered by Discourse, best viewed with JavaScript enabled"
1872,american-option-pricing-with-monte-carlo-simulation-in-cuda-c,"Originally published at:			https://developer.nvidia.com/blog/american-option-pricing-monte-carlo-simulation/
In finance, an option (or derivative) is the common name for a contract that, under certain conditions, gives a firm the right or obligation to receive or supply certain assets or cash flows.  A financial firm uses options to hedge risks when it operates in the markets. It is critical for a firm to be…Could you provide a slightly more detailed,and possibly simplified working example for Jetson TK1?My feeling is: this is a useful conceptual introduction, and might be a great working example.JonathanPowered by Discourse, best viewed with JavaScript enabled"
1873,datascience-raises-22m-to-predict-customer-value-reduce-traffic-deaths-and-more,"Originally published at:			DataScience Raises $22M To Predict Customer Value, Reduce Traffic Deaths And More | NVIDIA Technical Blog
Using the NVIDIA Tesla Accelerated Computing Platform in the Amazon Web Services cloud, this Southern California startup is supporting the City of Los Angeles with an initiative to eliminate traffic-related deaths. The company uses a combination of in-house data scientists, proprietary technology and custom data models to help businesses do things like predict a customer’s…Powered by Discourse, best viewed with JavaScript enabled"
1874,gtc-2020-building-msoe-s-gpu-powered-infrastructure-for-ai-instruction-and-research,"GTC 2020 S21423
Presenters: Bradley Palmer ,NVIDIA ; Derek Riley,Milwaukee School of Engineering
Abstract
Last September, Milwaukee School of Engineering, with NVIDIA, deployed a new hybrid cluster to serve multiple AI and HPC computing demands including instruction, student and faculty research, and industry collaborations. We’ll describe the optimized accelerated computing architecture of this cluster and the software stack enabling these uses. We’ll explain the lessons learned through the design, build, and deploy processes. We’ll provide a blueprint for other institutions that seek to build GPU-accelerated computing infrastructure that supports similar diverse requirements.Watch this session
Join in the conversation below.Powered by Discourse, best viewed with JavaScript enabled"
1875,real-time-noise-suppression-using-deep-learning,"Originally published at:			Real-Time Noise Suppression Using Deep Learning | NVIDIA Technical Blog
Imagine waiting for your flight at the airport. Suddenly, an important business call with a high profile customer lights up your phone. Tons of background noise clutters up the soundscape around you —  background chatter, airplanes taking off, maybe a flight announcement. You have to take the call and you want to sound clear. We…The Krisp page says it works for IoT applications. In these scenarios, is it limited to single-mic devices? Many IoT devices are multi-mic, for instance smart speakers? Can these be quickly integrated with Krisp, or do they have to be redesigned to single-mic devices?If I already have existing noisy sound files, what is the best way to go about de-noising them and extracting the speech per party for use?I know of two startups that can enhance audio files - https://babblelabs.com/prod... and https://2hz.ai/Thanks for sharingThis article is all about noise suppression in headphones and telephones. I wonder if there are any experiments about noise cancellation in a room or in a car when you are not wearing a headset? when you are just sitting there and hearing the noise with your plain ears.Is there a solution for that kind of problems?Powered by Discourse, best viewed with JavaScript enabled"
1876,new-nvidia-optix-enhancements-that-improve-your-ray-tracing-applications,"Originally published at:			New NVIDIA OptiX Enhancements That Improve Your Ray Tracing Applications | NVIDIA Technical Blog
OptiX 7.3 brings temporal denoising and improvements to OptiX Curves primitives and new features to the OptiX Demand Loading libraryPowered by Discourse, best viewed with JavaScript enabled"
1877,share-your-science-training-a-machine-to-answer-questions-about-images,"Originally published at:			Share Your Science: Training a Machine to Answer Questions About Images | NVIDIA Technical Blog
Aishwarya Agrawal, PhD student at Virginia Tech shares how her team is using NVIDIA GPUs and deep learning to automatically answer a wide range of questions about arbitrary images. According to Agrawal and her collaborators, the system may one day be used by the visually impaired to help navigate real-world environments, such as informing the…Powered by Discourse, best viewed with JavaScript enabled"
1878,meet-the-researcher-gregory-a-voth-developing-new-methods-for-molecular-dynamics-simulations,"Originally published at:			Meet the Researcher: Gregory A. Voth, Developing New Methods for Molecular Dynamics Simulations | NVIDIA Technical Blog
‘Meet the Researcher’ is a monthly series in which we spotlight different researchers in academia who are using NVIDIA technology to accelerate their work. This month, we spotlight Gregory A. Voth, Distinguished Professor at the Department of Chemistry, The University of Chicago.  Voth received a Ph.D. in Theoretical Chemistry from the California Institute of Technology…Powered by Discourse, best viewed with JavaScript enabled"
1879,end-to-end-deep-learning-for-self-driving-cars,"What happen if there is no road?This car drives on decided road.Each steering decision is based solely on the current frame? Also, I didn't see the gas or brake pedal mentioned -- is the human still responsible for these?That's correct. Each steering decision is based solely on the current frame. Speed is controlled by the car's adaptive cruise control.That's very impressive! Have you considered using a LRCN (https://arxiv.org/abs/1411.... Also, the ACC, is this what is available currently to consumers?I agree, it's quite amazing what the network can do with a single frame. We are working on a number of advancements whose results we will publish at some time in the future.Yes, it's the standard ACC in the car, available to consumers.I'm still not clear how a self-driving car deals with situations which have not been seen in the pre-trained image model?Self driving cars used to be the thing of the future. But they are no longer something of the future. Companies like Tesla, Mercedes Benz and BMW are working towards making self driving cars a reality. You can also check http://conceptcarsuae.com/b...Hi, in the last layer (Fig. 5), after flatten, should not be 64*1*18 = 1152 neurons? ThanksHello!Are there any reports on how good the network converges? What is the minimal loss achieved?Hi all!Is the model available somewhere? and the dataset you generated?I have a question about section 6:""Since human drivers might not be driving in the center of the lane all the time, we manually calibratethe lane center associated with each frame in the video used by the simulator. We call this positionthe “ground truth”.""What do you mean with ""manually calibrate the lane center""? What did you do exactly?Many thanks!'Our system has no dependencies on any particular vehicle make or model.' How you make it not depended on any make or model? İf you are recording steering wheel angle it should be work only in that car which you drive to get data.HiI was wondering how the YUV form of the image is helping the network. I am assuming that it might speed the process. But it is hard to get how would YUV would work faster over RGB as there is just a linear transformation between the two.It would be a great help if someone could answer !Thank youHelloSomeone please guide me on how to collect data for self driving car using raspberry pi.This is a forum for NVIDIA developers. If you need support with Raspberry Pi, you'll need to find another source, since we don't make or support that platform.Hi, this is a powerful approach that requires minimum training data.But recently, I find the end-to-end self-driving model can be manipulated by adding imperceivable perturbations to the input image. Adversarial Driving that attacks Autonomous Driving may raise some concern.Adversarial Driving v.s. Autonomous Driving. Contribute to wuhanstudio/adversarial-driving development by creating an account on GitHub.I’ll further investigate some defence strategies to improve the robustness of end-to-end driving models.How does the network return the correct turn radius when the vehicle is trying to return to the center of the road without a speed input?Does anyone know which chip /SOC  mercedes are missing supply of for mbux 549 high with augmented reality.   Basically the main holdup for mercedes cars is the augmented reality chip would like to know which it is and if anyone has XENTRY PASSTHRU  WIS/ ASRA ESP CAN U link pdf of the details or part number if possible.Details pleaseThanks for your question! We recommend contacting Mercedes for any questions about their products.Powered by Discourse, best viewed with JavaScript enabled"
1880,curating-data-for-transfer-learning-with-the-nvidia-tao-toolkit-and-innotescus,"Originally published at:			https://developer.nvidia.com/blog/curating-data-for-transfer-learning-with-the-nvidia-tao-toolkit-and-innotescus/
Using NVIDIA TAO Toolkit and Innotescus’ data curation and analysis platform to improve a popular object detection model’s performance on the person class by over 20%.Powered by Discourse, best viewed with JavaScript enabled"
1881,gtc-2020-scaling-hyper-parameter-optimization-using-rapids-dask-and-kubernetes,"GTC 2020 S21578
Presenters: Eric Harper,NVIDIA; Miro Enev, NVIDIA
Abstract
We’ll show you how to scale end-to-end data-science workflows on GPUs using RAPIDS, Dask, and Kubernetes. Specifically, we build a dynamically sized dataset and use it to run XGBoost hyper-parameter optimization using a particle-swarm strategy while scaling the number of GPUs in our cluster. We’ll highlight best practices for scaling within a node (Dask) and across nodes (Dask plus Kubernetes), and demonstrate that the entire data-science workflow can be done on the GPU.The session content is presented as a Jupyter notebook enabling:Having walked through these ideas, participants will be able to further explore and extend the content in their own environments.Watch this session
Join in the conversation below.Powered by Discourse, best viewed with JavaScript enabled"
1882,nvidia-and-tencent-cloud-demonstrate-xr-streaming-from-the-cloud,"Originally published at:			https://developer.nvidia.com/blog/tencent-xr-streaming-cloud/
At GTC China, NVIDIA announced that Tencent Cloud demonstrated CloudXR streaming an immersive high-rise office building. NVIDIA CloudXR platform uses Tencent Cloud’s stable and efficient cloud GPU computing power to turn any end device, including head-mounted displays (HMD) and connected Windows and Android devices, into a high-fidelity XR display that can showcase professional-quality graphics. The…Powered by Discourse, best viewed with JavaScript enabled"
1883,ai-predicts-athletes-on-field-decision-making,"Originally published at:			https://developer.nvidia.com/blog/ai-predicts-athletes-on-field-decision-making/
Researchers at Disney, California Institute of Technology and STATS, a supplier of sports data developed a deep learning-based technique that provides coaches and teams with a quicker tool to help assess defensive athletic performance in any game situation. The method analyzes game data on player and ball positions to create models, or “ghosts,” of how…Powered by Discourse, best viewed with JavaScript enabled"
1884,gtc-2020-visualize-your-data,"GTC 2020 S21946
Presenters: Peter Messmer,NVIDIA
Abstract
Visualization is a key component of many computational science disciplines. In the past, only typical HPC domains like computational fluid dynamics or cosmology had dataset sizes large enough to require sophisticated multi-node visualization tools. But with increasing detector sizes and large neural networks or higher resolution models, application domains that typically used single-node visualization suddenly need performance that goes beyond a single node. In addition, rendering techniques using ray tracing or VR can dramatically increase the visual cues of visualizations, offering novel ways to investigate the data. We’ll look at a range of large-data problems and a palette of GPU-accelerated visualization tools, and see how they can help in a range of use cases.Watch this session
Join in the conversation below.Powered by Discourse, best viewed with JavaScript enabled"
1885,how-can-i-downgrade-cuda-version-to-10-1-or-lower,"Hi im new here, and facing a problem for several days now. I started to do some tutorials and later on projects with tensorflow. problem here is my version of CUDA which is v11 atm. Tensorflow is just working with CUDA 10.1 … sadly i onlyfind game ready driver with cuda 10.2 as oldest version for my gtx1050 ti (notebook - m version) … anyoneone has a clue or do i have to wait for tensoflow 2.4?
greetings fellasPowered by Discourse, best viewed with JavaScript enabled"
1886,accelerating-medical-image-segmentation-with-nvidia-tensor-cores-and-tensorflow-2,"Originally published at:			Accelerating Medical Image Segmentation with NVIDIA Tensor Cores and TensorFlow 2 | NVIDIA Technical Blog
Figure 1. Example of a serial section Transmission Electron Microscopy image (ssTEM) and its corresponding segmentation. Medical image segmentation is a hot topic in the deep learning community. Proof of that is the number of challenges, competitions, and research projects being conducted in this area, which only rises year over year. Among all the different…Powered by Discourse, best viewed with JavaScript enabled"
1887,emotional-ai-solution-won-texas-children-s-hospital-healthcare-hackathon,"Originally published at:			https://developer.nvidia.com/blog/emotional-ai-solution-won-texas-childrens-hospital-healthcare-hackathon/
NVIDIA Inception member, IPMD won the Touchless Experience category of Texas Children’s Hospital Healthcare Hackathon with its Project M emotional AI solution.Powered by Discourse, best viewed with JavaScript enabled"
1888,nvidia-showcases-software-and-hardware-at-neurips-expo,"Originally published at:			NVIDIA Showcases Software and Hardware at NeurIPS Expo | NVIDIA Technical Blog
At the thirty-second Conference on Neural Information Processing Systems (NeurlPS) in Montreal, Canada, NVIDIA hosted the NVIDIA AI Tech Workshop, a workshop about how GPU-accelerated computing is transforming the landscape of computational science and AI. “This workshop gives us an opportunity to show the AI community what we’re doing as a company, to help researchers…Powered by Discourse, best viewed with JavaScript enabled"
1889,optimizing-fraud-detection-in-financial-services-with-graph-neural-networks-and-nvidia-gpus,"Originally published at:			https://developer.nvidia.com/blog/optimizing-fraud-detection-in-financial-services-with-graph-neural-networks-and-nvidia-gpus/
Learn an end-to-end workflow showcasing best practices for detecting financial services fraud using GNNs and GPUs.What batch sizes were used while scaling from 1 to 8 GPUs on the MAG240M dataset?We used batch size of 8192 and this batch size gave us the best classification accuracy. We see similar speedups with lower batch sizes as well.Hi!
Can you share the full end-to-end code for fraud detection (including R-GCN building, training and downstream XGBoost applying)?Hi I know I am a bit late to the topic, but I have some questions I was wondering if you could answer.So far, I have conducted preprocessing, and the dataset now contains 20 numerical features. As the article suggests, I have saved the bulk of the data on the edges between nodes, leaving the nodes featureless, besides their distinct IDs. Moreover, from my perspective, it would seem that these transactions only have one relationship, which is “Credit card purchases from Merchant”. Now, I have some questions regarding the suggestions of the articleI am having a pretty hard time understanding this article, and the methods for that matter, and I would really appreciate some clarification.Hi there nthqhai2002!Thanks for reading this blog and for your awesome questions!it would seem that these transactions only have one relationship, which is “Credit card purchases from Merchant”In order to make the edge undirected and to allow message propogation between both classes of the graph, we also add a second reverse edge type, in your cases “Merchant has purchase from Credit Card”As such, wouldn’t it be ineffective to conduct node embeddings?The IDs themselves in a transductive setting are valuable as well, as a learned user embedding encodes the generalized structural behavioral embedding of the user. You can also aggregate adjacent edge features per node to user as a node featureYou are correct that an architecture that propogates edge information would likely be useful here. The purpose of the blog post was mainly to show baseline usefulness, but if you’re interested in edge-inclusive papers, you can refer to: Exploiting Edge Features for Graph Neural Networks | IEEE Conference Publication | IEEE XploreI have also seen the article suggest using Link Prediction as part of the approach, but I do not understand how it helps with detecting fraudulent transactions.Link Prediction in this case is used to generate robust representations of nodes, which can be used downstream in the direct prediction of the fraud label. Often in the fraud detection domain, labels are noisy and generally weak. Training representations on non-noisy labels (transaction presence) often has more consistent convergence properties.Hi kkranen!
Thanks a lot for the response, your answers have helped me a lot !
I have another question regarding this workflow, if you do not mind. Suppose I have trained the model on data from 2015 until 2021, and new influx of data from 2022 comes in. At this stage, I would follow the workflow in order to generate robust node embeddings, then attach them to the 2022 tabular data according to the unique node IDs, then conduct predictions. My question is should the node embeddings of 2022 be generated by submitting the entire dataset from 2015 to 2022 to the workflow, or do I only need to exclusively use data of 2022?Hello,
Could you find the reproducible source code?Powered by Discourse, best viewed with JavaScript enabled"
1890,achieving-a-cloud-scale-architecture-with-dpus,"Originally published at:			https://developer.nvidia.com/blog/achieving-a-cloud-scale-architecture-with-dpus/
This post explains why you need a DPU-based SmartNIC and discusses some Smart NIC use cases.Powered by Discourse, best viewed with JavaScript enabled"
1891,meet-laconia-michigan-states-new-supercomputer,"Originally published at:			https://developer.nvidia.com/blog/meet-laconia-michigan-states-new-supercomputer/
MSU President Lou Anna K. Simon cutting the ceremonial ribbon on Laconia. The Institute for Cyber-Enabled Research (iCER) at Michigan State University recently unveiled a new $3 million supercomputer named Laconia with double the speed of its previous supercomputer. Partnering with Lenovo as the system OEM, powered by 200 NVIDIA Tesla K80 GPUs, Laconia ranks…Powered by Discourse, best viewed with JavaScript enabled"
1892,model-parallelism-and-conversational-ai-workshops,"Originally published at:			​ - DLI Public Workshops June 2023
Join these upcoming workshops to learn how to train large neural networks, or how to build and deploy a conversational AI pipeline.Powered by Discourse, best viewed with JavaScript enabled"
1893,advanced-api-performance-async-copy,"Originally published at:			https://developer.nvidia.com/blog/advanced-api-performance-async-copy/
Using async copy on your NVIDIA GPU effectively can significantly increase your performance. These quick recommendations give guidelines on how to arrange your work queues to maximize your gains.I hope this article was helpful, I also learned a lot when consulting my fellow Nvidians while writing it. While Async Copy and Async Compute may seem like almost the same thing at first glance, they do have key differences to master when putting into practice. If you have any questions or need any clarifications I’ll be more than happy to help!Powered by Discourse, best viewed with JavaScript enabled"
1894,fraud-detection-top-resources-from-gtc-21,"Originally published at:			https://developer.nvidia.com/blog/fraud-detection-top-resources-from-gtc-21/
AI can help banking firms better detect and prevent payment fraud and improve processes for anti-money laundering (AML) and know-your-customer (KYC) systems. With NVIDIA GPU-accelerated machine learning and deep learning platforms, data scientists can deliver results.Powered by Discourse, best viewed with JavaScript enabled"
1895,nvidia-drive-10-0-now-available,"Originally published at:			https://developer.nvidia.com/blog/nvidia-drive-10-0-now-available/
DRIVE Software 10.0 brings advanced perception capabilities and new sensor functionalities to the NVIDIA autonomous vehicle software stack.Powered by Discourse, best viewed with JavaScript enabled"
1896,autonomous-robot-helps-keep-construction-projects-on-track,"Originally published at:			https://developer.nvidia.com/blog/autonomous-robot-helps-keep-construction-projects-on-track/
California-based startup Doxel developed an autonomous robot that captures 3D scans of construction sites and then sends that data to their deep learning algorithms to inspect the quality of the work installed and measure quantities in real-time. “You can’t improve what you can’t measure. Without real-time visibility into quality and progress, managers simply can’t boost…Powered by Discourse, best viewed with JavaScript enabled"
1897,gtc-2020-containers-runtime-orchestration-and-monitoring,"GTC 2020 CWE22195
Presenters: ,
Abstract
Interactive session to answer any questions you might have regarding:We will also share tips on how to tune containers for high-performance applications.Watch this session
Join in the conversation below.Hi NVIDIA,Is there any way to update Jetpack 3.3 to Jetpack 4.3 remotely so I can update TensorRT? When I tried to run the L4T container with the updated drivers on the machine with Jetpack 3.3 installed I keep running into driver run time issues, is there a way I can address this problem with new containers?Hi,Unfortunately the OTA method we present now only support to upgrade between jetpack4.x → 4.x. There is no method to upgrade jetpack3.3 to 4.3 remotely.  Need to flash with sdkmanager.Powered by Discourse, best viewed with JavaScript enabled"
1898,cvpr-2020-meet-misty-interactive-3d-chatbot,"CVPR 2020 dcv03
Presenters: Tech Demo Team, NVIDIA
Abstract
NVIDIA Jarvis is an application framework for multimodal conversational AI services that delivers real-time performance on GPUs.Watch this session
Join in the conversation below.Powered by Discourse, best viewed with JavaScript enabled"
1899,ecommerce-and-open-ethernet-criteo-clicks-with-sonic,"Originally published at:			https://developer.nvidia.com/blog/ecommerce-and-open-ethernet-criteo-clicks-with-sonic/
When you see a browser ad for a new restaurant, or the perfect gift for that hard-to-please family member, you probably aren’t thinking about the infrastructure used to deliver that ad. However, that infrastructure is what allows advertising companies like Criteo to provide these insights. The NVIDIA networking portfolio is essential to Criteo technology stack.…Hello, and thanks for reading!  We are just witnessing the beginning of the open networking revolution, and it’s great to have this partnership with pioneers like Criteo. Let me know if you have any questions about NVIDIA, SONiC, Criteo, or anything else. Thanks again!Best,
TaylorIt was a good to read.Powered by Discourse, best viewed with JavaScript enabled"
1900,new-zealand-building-gpu-accelerated-petaflop-supercomputer,"Originally published at:			New Zealand Building GPU-Accelerated Petaflop Supercomputer | NVIDIA Technical Blog
New Zealand’s National Institute of Water and Atmospheric Research (NIWA) is upgrading their supercomputers with three new energy efficient systems that will be used for weather forecasting and climate research. $18 million will be used to purchase three Cray systems, the most powerful being the 1.4 petaflop Cray XC50 supercomputer equipped with NVIDIA Tesla P100…Powered by Discourse, best viewed with JavaScript enabled"
1901,upcoming-webinar-building-a-computer-vision-service-using-nvidia-ngc-and-google-cloud,"Originally published at:			https://developer.nvidia.com/blog/upcoming-webinar-building-a-computer-vision-service-using-nvidia-ngc-and-google-cloud/
The NGC team is hosting a webinar and live Q&A. Topics include how to use containers from the NGC catalog deployed from Google Cloud Marketplace to GKE, a managed Kubernetes service on Google Cloud, that easily builds, deploys, and runs AI solutions. Building a Computer Vision Service Using NVIDIA NGC and Google CloudAugust 25 at…how to login@g.brown1 – If you register for the webinar, you’ll get a login link. Let me know if that doesn’t answer your question.Yes I would like to register.  Sounds great. thanks.Powered by Discourse, best viewed with JavaScript enabled"
1902,jetson-agx-xavier-module-now-available-a-major-leap-forward-for-autonomous-machines,"Originally published at:			Jetson AGX Xavier Module Now Available – A Major Leap Forward for Autonomous Machines | NVIDIA Technical Blog
The Jetson AGX Xavier production module is now available from distributors globally, joining the Jetson TX2 and TX1 family of products. Developers can use Jetson AGX Xavier to build autonomous machines that will solve some of the world’s toughest challenges and transform a wide range of industries—including manufacturing, logistics, retail, agriculture, smart cities, healthcare and…Powered by Discourse, best viewed with JavaScript enabled"
1903,improving-apache-spark-performance-and-reducing-costs-with-amazon-emr-and-nvidia,"Originally published at:			https://developer.nvidia.com/blog/improving-apache-spark-performance-and-reducing-costs-with-amazon-emr-and-nvidia/
Apache Spark has emerged as the standard framework for large-scale, distributed, data analytics processing. NVIDIA worked with the Apache Spark community to accelerate the world’s most popular data analytics framework and to offer revolutionary GPU acceleration on several leading platforms, including Google Cloud, Databricks, and Cloudera. Now, Amazon EMR joins the list of leading platforms, making it easy and…Powered by Discourse, best viewed with JavaScript enabled"
1904,openacc-summit-2020-goes-digital,"Originally published at:			OpenACC Summit 2020 Goes Digital | NVIDIA Technical Blog
This year’s OpenACC 2020 Summit is going digital. Scheduled from August 31st to September 4th, the OpenACC Summit brings together users of the OpenACC programming model and members of OpenACC organization across national laboratories, research institutions, and industry.  This year the Summit will be completely online and feature a keynote from Martijn Marsman from the…Powered by Discourse, best viewed with JavaScript enabled"
1905,gtc-21-5-cant-miss-sessions-in-ai-and-deep-learning,"Originally published at:			https://developer.nvidia.com/blog/gtc-21-5-cant-miss-sessions-in-ai-and-deep-learning/
Register now for AI and deep learning GTC sessions focused on topics such as training, inference, frameworks, and tools.Powered by Discourse, best viewed with JavaScript enabled"
1906,nvidia-jetson-takes-to-the-course-and-classroom,"Originally published at:			NVIDIA Jetson Takes to the Course and Classroom | NVIDIA Technical Blog
Embedded Computing took center stage last week in Pittsburgh, PA. The Jetson team sponsored an F1/10 autonomous racing competition at Carnegie Mellon and participated in Embedded Systems Week (ESWeek), a premier event covering all aspects of embedded systems and software, where we showed the Robotics Teaching Kit with ‘Jet’. F1/10 – Shift Into High Gear…Powered by Discourse, best viewed with JavaScript enabled"
1907,facial-recognition-as-a-service-for-businesses,"Originally published at:			Facial Recognition as a Service for Businesses | NVIDIA Technical Blog
Russia’s NTechLab, one of the world’s leading facial recognition companies, released a new product called FindFace.Pro that allows businesses to easily integrate their cloud-based REST API into existing products. NVIDIA products are used extensively in the technology. The team is using CUDA, GeForce GTX 1080 GPUs, TITAN X GPUs and cuDNN-accelerated deep learning frameworks to…Powered by Discourse, best viewed with JavaScript enabled"
1908,ovh-becomes-first-cloud-provider-in-europe-to-be-validated-for-ngc,"Originally published at:			OVH Becomes First Cloud Provider In Europe to be Validated for NGC | NVIDIA Technical Blog
This week OVH, a leading cloud computing company headquartered in France, became the first European cloud provider to be a validated platform partner for the NVIDIA GPU Cloud (NGC). “OVH users can now run software from the NGC container registry, which provides a comprehensive catalogue of AI software optimized for NVIDIA GPUs, on OVH infrastructure,”…Powered by Discourse, best viewed with JavaScript enabled"
1909,ai-to-improve-structure-modeling-for-protein-interactions,"Originally published at:			AI to Improve Structure Modeling for Protein Interactions | NVIDIA Technical Blog
To help better understand how proteins interact in the body, Purdue University researchers developed a deep learning-based approach for modeling protein interactions, a first for AI the researchers say.  The work has the potential to help fight various diseases and to design better drugs that directly target protein interactions.  “Our work represents a major advancement…Powered by Discourse, best viewed with JavaScript enabled"
1910,building-an-autonomous-vehicle-camera-pipeline-with-nvidia-driveworks-sdk,"Originally published at:			Building an Autonomous Vehicle Camera Pipeline with NVIDIA DriveWorks SDK | NVIDIA Technical Blog
Autonomous vehicles rely on cameras for a visual representation of the world around them. To safely drive without a human, these vehicles must be able to process image data from cameras quickly and accurately.  The NVIDIA DriveWorks SDK provides all the middleware developers need to efficiently build autonomous vehicle software, including reference applications, tools and…Powered by Discourse, best viewed with JavaScript enabled"
1911,introducing-apex-pytorch-extension-with-tools-to-realize-the-power-of-tensor-cores,"Originally published at:			https://developer.nvidia.com/blog/introducing-apex-pytorch-extension-with-tools-to-realize-the-power-of-tensor-cores/
Today at the Computer Vision and Pattern Recognition Conference in Salt Lake City, Utah, NVIDIA is kicking off the conference by demonstrating an early release of Apex, an open-source PyTorch extension that helps users maximize deep learning training performance on NVIDIA Volta GPUs. Inspired by state-of-the-art mixed precision training in translational networks, sentiment analysis, and…Powered by Discourse, best viewed with JavaScript enabled"
1912,build-a-fully-interactive-dashboard-in-a-few-lines-of-python,"Originally published at:			https://developer.nvidia.com/blog/build-a-fully-interactive-dashboard-in-a-few-lines-of-python/
Work continues on improving the UX and capabilities of our GPU cross-filter dashboard library, cuxfilter. Here is a quick recap of its latest features.  First, it is as easy as ever to access cuxfilter. Just run a standard RAPIDS install as shown in the getting started page. Additionally, you can try it online at PaperSpace.…Powered by Discourse, best viewed with JavaScript enabled"
1913,upcoming-workshop-applications-of-ai-for-anomaly-detection-emea,"Originally published at:			Personal Information - Applications of AI for Anomaly Detection (EMEA)
Learn to detect data abnormalities before they impact your business by using XGBoost, autoencoders, and GANs.Powered by Discourse, best viewed with JavaScript enabled"
1914,nvidia-jetson-agx-xavier-delivers-32-teraops-for-new-era-of-ai-in-robotics,"Originally published at:			https://developer.nvidia.com/blog/nvidia-jetson-agx-xavier-32-teraops-ai-robotics/
The world’s ultimate embedded solution for AI developers, Jetson AGX Xavier, is now shipping as standalone production modules from NVIDIA. A member of NVIDIA’s AGX Systems for autonomous machines, Jetson AGX Xavier is ideal for deploying advanced AI and computer vision to the edge, enabling robotic platforms in the field with workstation-level performance and the ability to operate…HelloI am a student engineer in the course of project end of study. My end of study project is focused on the autonomous bus for that we chose as solution the card NVIDIA AGX Xavier. Please if possible I need:* schematic Altuim Designer or Orcad* PCB (routing) Altuim Designer or Orcade-mail: gassmimohamed974 @ gmail.comthank you in advanceIs it possible to game on this machine?Like installing steam on ubuntu and...Or even with a CPU of this size it could probably run win10, if it gets ported.How to connect 4 USB 3.0 to Xavier dev kit? I need to connect 4 depth camera using  USB3.0 ports. Can the camera lanes used to connect depth cameras?In comparison to xavier does TX2 support 4 usb 3.0 ports?Other than using the available USB ports on the Xavier devkit (e.g. via USB-C dongles, and the built-in hybrid USB3/SATA port), you can try a USB3 hub.  If you run out of bandwidth through the hub, I would try attaching a PCIe-based USB3 card to the Xavier devkit's PCIe slot.   The MIPI CSI camera lanes would require some kind of converter to use with USB or a native CSI depth camera (I think E-ConSystems makes one of those).   With the TX2 devkit, you could attach a PCIe-based USB3 card too.I just ordered the AGX Developer. Is there an affordable adapter to connect to the CSI camera port?  I am surprised that no cable is included. I may return the AGX, if I cannot find a cheap adapter.Camera: (16x) MIPI CSI-2 lanes, (8x) SLVS-EC lanes; up to 6 active sensor streams and 36 virtual channels
†MIPI CSI-2, up to 40 Gbps in D-PHY V1.2 or 109 Gbps in CPHY v1.1Hi @m_lanham, the multi-camera CSI header is typically used with camera kits like these:https://elinux.org/Jetson_AGX_Xavier#CamerasIf you had intended to use it with a 15-pin CSI cable (i.e. like the Raspberry Pi Camera Module v2), then the Jetson Xavier NX devkit has those headers.I will return the AGX to get an NX. Thanks, but why not add a CSI adapter cable for full functionality on your top end machine!!! After paying for the best, I have to spend another couple of hundred to use it!!!Which power bank/battery do you recommend for Jetson AGX Xavier?The VLIW Accelerator is named as “Programmable”. Does it mean it is available to run custom algorithms? If so, is there an SDK I can use to build my code for the VLIW Accelerator?Hi @eugene.panich, its programmable in the sense that you can create pipelines and chain algorithms together, however there isn’t a public-facing SDK for implementing custom operations.Powered by Discourse, best viewed with JavaScript enabled"
1915,speed-up-machine-learning-models-with-accelerated-weka,"Originally published at:			Speed Up Machine Learning Models with Accelerated WEKA | NVIDIA Technical Blog

Accelerated WEKA integrates the WEKA workbench with Python and Java libraries that support GPU to speedup the training and prediction time of Machine Learning models.Powered by Discourse, best viewed with JavaScript enabled"
1916,gpu-accelerated-supercomputer-assists-firefighters-in-california,"Originally published at:			GPU-Accelerated Supercomputer Assists Firefighters in California | NVIDIA Technical Blog
The recent wildfires in California spread at an alarming rate — forcing people to evacuate within minutes of first learning about it. To help battle future fires and possibly save lives, California firefighters are leveraging the power of GPU-accelerated supercomputers to predict the possible trajectories a wildfire could take. Using the Comet supercomputer at the San…Powered by Discourse, best viewed with JavaScript enabled"
1917,thanks-everyone-for-joining-ama-is-now-closed,"Thanks everyone for their interest, if there are any follow on questions please add them to the posts quickly and we will respond offline.
Thanks !Powered by Discourse, best viewed with JavaScript enabled"
1918,new-public-workshops-now-available-from-the-nvidia-deep-learning-institute,"Originally published at:			https://developer.nvidia.com/blog/new-public-workshops-now-available-from-the-nvidia-deep-learning-institute/
For the first time ever, the NVIDIA Deep Learning Institute (DLI) is making its popular instructor-led workshops available to the general public.  With the launch of public workshops this week, enrollment will be open to individual developers, data scientists, researchers, and students. NVIDIA is increasing accessibility and the number of courses available to participants around…Powered by Discourse, best viewed with JavaScript enabled"
1919,the-dxr-spotlight-contest-is-back,"Originally published at:			The DXR Spotlight contest is back! | NVIDIA Technical Blog
We’re bringing back the DXR Spotlight contest for 2020. Last year’s contest was a tremendous success, garnering dozens of entrants and positive press throughout the game and content creation community. Format for the contest is the same as last year – we’re asking content creators and game developers to deliver tech demos showcasing real-time ray tracing. …Powered by Discourse, best viewed with JavaScript enabled"
1920,top-video-streaming-and-conferencing-sessions-at-nvidia-gtc-2023,"Originally published at:			Applied AI Session Catalog | NVIDIA GTC
Learn about advancements in video conferencing that have transformed how we communicate.Powered by Discourse, best viewed with JavaScript enabled"
1921,using-turing-mesh-shaders-nvidia-asteroids-demo,"Originally published at:			Using Turing Mesh Shaders: NVIDIA Asteroids Demo | NVIDIA Technical Blog
The NVIDIA Asteroids demo showcases how the mesh shading technology built into NVIDIA’s Turing GPU architecture can dramatically improve performance and image quality when rendering a substantial number of very complex objects in a scene. The following video highlights the capabilities of the mesh shader in the Asteroids demo. This video shows off the Asteroids demo in…Looks great, performed better. Dat tris count..This is amazing, when it comes to video games? Is it a driver solution for all games running on Turing or its a per-game solution?Like most things NVidia does, this will almost certainly be an SDK or added as an update to be part of the already existing NVidia SDK (Gameworks?). From there, it'll be implemented into game engines that developers use. Some developers might go ahead and add it themselves on a per game basis (requiring downloading the 20GB+ UE4 source code, NVidia SDK, and integrating manually + hooking up all of the relevant stuff - a pretty heavy undertaking for most when it comes to such a feature) but otherwise it'll just be enabled once added by the engine teams (Unreal and Unity basically).Mind you, this is just a great culling tool. Rushing a ray traced game to market, no matter how bad, would sell like mad right now and no one's done it so don't expect it to happen tbh. Even an update to a pre-existing game to add Ray tracing.Nv extensions for OpenGL and Vulkan have been released and one can write software using mesh shaders.https://developer.nvidia.co...Hi. It would be great if we can get the sources of this demo as well, no only the executable.While the full source code for this particular demo won't be released, we are prepping a getting-started package which includes some code samples from the demo.These code samples plus additional tutorial information will first be presented at a session at GDC and will be available on developer.nvidia.com after that.Why not release the source code? I truly don't get it... just remove music and art if that is an issue. I really don't care if it is programmer art. This ""holding back"" thing doesn't sit well. If anything putting things like this into peoples hands is a great starting point. It's hardly a game (or whatever) at this stage.This demo should be released for common avaiablitlity .People would want to see RTX Graphics card capability .I dont think login to dev works is requiredThis demo is available with DevZone registration. It's free with a valid email address. Hope that helps.would be nice if this was on Nvidias ""Demos"" page for free without the need of an emailPowered by Discourse, best viewed with JavaScript enabled"
1922,validating-nvidia-drive-sim-camera-models,"Originally published at:			https://developer.nvidia.com/blog/validating-drive-sim-camera-models/
This post covers the validation of camera models in DRIVE Sim, assessing the performance of each element, from rendering of the world scene to protocol simulation.Powered by Discourse, best viewed with JavaScript enabled"
1923,meet-the-researcher-lokman-abbas-turki-applying-hpc-to-computationally-complex-mathematical-finance-problems,"Originally published at:			Meet the Researcher: Lokman Abbas Turki, Applying HPC to Computationally Complex Mathematical Finance Problems | NVIDIA Technical Blog
‘Meet the Researcher’ is a monthly series in which we spotlight researchers who are using GPUs to accelerate their work. This month we spotlight Lokman Abbas Turki, lecturer and researcher at Sorbonne University in Paris, France. What area of research is your lab focused on? If I had to sum it up in a few…Powered by Discourse, best viewed with JavaScript enabled"
1924,nvidia-triton-inference-server-boosts-deep-learning-inference,"Originally published at:			https://developer.nvidia.com/blog/nvidia-triton-inference-server-boosts-deep-learning-inference/
The NVIDIA Triton Inference Server, previously known as TensorRT Inference Server, is now available from NVIDIA NGC or via GitHub.  The NVIDIA Triton Inference Server helps developers and IT/DevOps easily deploy a high-performance inference server in the cloud, in on-premises data center or at the edge. The server provides an inference service via an HTTP/REST…Powered by Discourse, best viewed with JavaScript enabled"
1925,improving-cancer-treatment-with-gpu-accelerated-supercomputers,"Originally published at:			https://developer.nvidia.com/blog/improving-cancer-treatment-with-gpu-accelerated-supercomputers/
Researchers from The University of Texas MD Anderson Cancer Institute in Houston, Texas developed a deep learning process that automatically identifies the precise size and location of tumors and determines how much radiation should be administered. The process, known as contouring, is a time-intensive approach that varies from doctor to doctor. The problem, in the…Powered by Discourse, best viewed with JavaScript enabled"
1926,nsight-graphics-1-1-now-available,"Originally published at:			Nsight Graphics 1.1 Now Available | NVIDIA Technical Blog
NVIDIA Nsight Graphics 1.1 is now available for download for members of the NVIDIA Registered Developer Program. Nsight Graphics is a suite of debugging and profiling tools for graphics applications. It provides insights into your application’s operation and optimal performance, and reduces your time spent debugging. Built from the same core features and codebase as…Powered by Discourse, best viewed with JavaScript enabled"
1927,detecting-concussions-with-a-smartphone-and-deep-learning,"Originally published at:			Detecting Concussions with a Smartphone and Deep Learning | NVIDIA Technical Blog
Researchers from University of Washington developed a smartphone app that can detect concussions and other brain injuries — whether on the sidelines of a sports game or at an accident site. “Having an objective measure that a coach or parent or anyone on the sidelines of a game could use to screen for concussion would…Powered by Discourse, best viewed with JavaScript enabled"
1928,fast-multi-gpu-collectives-with-nccl,"Originally published at:			Fast Multi-GPU collectives with NCCL | NVIDIA Technical Blog
Today many servers contain 8 or more GPUs. In principle then, scaling an application from one to many GPUs should provide a tremendous performance boost. But in practice, this benefit can be difficult to obtain. There are two common culprits behind poor multi-GPU scaling. The first is that enough parallelism has not been exposed to…Hi Nathan, I've looking forward to this project to come out!I have a question: If I want to perform a 2D domain decomposition (using a Cartesian domain for simplicity) with NCCL, how would you advice to perform the halo regions exchange using only the 5 collectives in the project?This is an important library for our work.  We use numerous multi-GPU applications.  Having a topology-aware library should ease our programming burden.  Does this tool support the P8NVLink system with P100 GPU?  From the blog below, this tool need the GPUDirect but OpenPOWER system doesn't have GPUDirect. That is why I ask if this tool is ready for OpenPOWER P8NVLink system with P100 GPU.Nvidia blog=========https://devblogs.nvidia.com...""NCCL makes extensive use of GPUDirect Peer-to-Peer direct access to push data between processors.""NCCL will automatically fall back to pinned buffers if P2P direct access is not available. That said, the GitHub version of NCCL is not optimized for NVLink, so the achieved performance will be similar to PCIe rather than NVLink bandwidths.Does NCCL provide benefits when using a heterogeneous set of NVIDIA cards or must they be a homogeneous matched set?  For example, will I see benefits if I have a GTX 980ti and GTX 1070 in my machine?For best performance, NCCL uses peer-to-peer direct access between GPUs. When this feature is not available (e.g., when different models of GPU are used together), it stages transfers through pinned host memory instead. This definitely impacts performance, but should still outperform memcpy-based solutions.Interesting library, which I find useful for my multi-GPU scheduling library. Are there mechanisms in place that can be used to identify when a particular GPU has received all of the data it needs to begin processing?NCCL uses CUDA Streams to track dependencies while executing GPU tasks asynchronously with respect to the host thread. When calling NCCL you provide a stream handle into which NCCL enqueues it's kernels. If you use this same stream both to populate the send buffers before the NCCL call and to consume the collected results afterward, the CUDA runtime will ensure that the steps are executed in the correct order.I am evaluating NCCL in our DL training. Great work! I run some quick test such as NCCLv1 across 4 P100 (PCIe) in the same node, but I didn't find any d2d/p2p API invoke when profile using NVProf. so any internal APIs used for cross-dev copy? how to profile and monitor such traffic e.g., through nvprof?I actually post the question with more details in Nvidia tech forum (below URL), but nobody answered, could u help me out? thanks. https://devtalk.nvidia.com/...To avoid launching interleaved kernels and memcpys, NCCL uses peer direct writes between the GPUs located on a common PCIe root hub (and zero-copy memory when traversing QPI). This means no cudaMemcpy calls will appear in the profile. Instead there will be a single kernel launched on each GPU. To gauge raw NCCL perf, you can use the minimum kernel time from all GPUs (the collectives need all GPUs to be present, so early arrivals spend time spin-waiting for the last device to arrive).thanks for the reply.May I confirm that those internal data moving are triggered in the kernel and hence underneath CUDA driver? Is such moving traffic included in nvidia-smi when query PCIe rd/wr? if yes, I guess it is shown in both src.GPU read and dst.GPU write? pls confirm.In cross-node case, if GPUDirect (RDMA) not available, will it goto CPU via memcpyD2H then go to networking? if with GPUDirect, it could be RDMA (NCCLv2)?thanks again.I am not familiar with nvidia-smi's PCIe query. As far as I am aware these are not available on x86 which is the platform I predominantly use. I would expect more ""writes"" than ""reads"" since NCCL tries to push rather than pull data.For multi-node you are correct. GDRDMA is used, but a host-memory fall-back is used when the GDRDMA is not available.Thanks Nathan for writing the blog.I have an orthogonal question regarding the method you used to measure the bandwidth. What tool can I use to measure bandwidth between various links on a server with 8 GPUs?Note: I came across bandwidthTest which is part of the CUDA samples but I would like something that can help look at bandwidth usage for all links over time.Hi Nathan, what is the profiling tool you used to get the performance link bandwidth achieved by various NCCL collective?Powered by Discourse, best viewed with JavaScript enabled"
1929,deep-learning-and-satellite-data-helping-map-poverty,"Originally published at:			Deep Learning and Satellite Data Helping Map Poverty | NVIDIA Technical Blog
Stanford scientists developed a solution combining high-resolution satellite imagery with deep learning to accurately predict poverty levels at the village level. Using TITAN X and Tesla K40 GPUs with the cuDNN-accelerated Caffe and Tensorflow deep learning frameworks to train their convolutional neural networks, the researchers used the “nightlight” satellite data to identify features in the…Powered by Discourse, best viewed with JavaScript enabled"
1930,creating-an-object-detection-pipeline-for-gpus,"Originally published at:			Creating an Object Detection Pipeline for GPUs | NVIDIA Technical Blog
Earlier this year in March, we showed retinanet-examples, an open source example of how to accelerate the training and deployment of an object detection pipeline for GPUs. We presented the project at NVIDIA’s GPU Technology Conference in San Jose. This post discusses the motivation for this work, a high-level description of the architecture, and a brief look…Powered by Discourse, best viewed with JavaScript enabled"
1931,encoding-for-directx-12-with-nvidia-video-codec-sdk-11-1,"Originally published at:			https://developer.nvidia.com/blog/encoding-for-directx-12-with-video-codec-sdk-11-1/
DirectX 12 is a low-level programming API from Microsoft that reduces driver overhead in comparison to its predecessors. DirectX 12 provides more flexibility and fine-grained control on the underlying hardware using command queues, command lists, and so on, which results in better resource utilization. You can take advantage of these functionalities and optimize your applications…Powered by Discourse, best viewed with JavaScript enabled"
1932,speedup-end-to-end-vision-ai-using-transfer-learning-toolkit-2-0-deepstream-sdk-5-0,"Originally published at:			https://developer.nvidia.com/blog/deepstream-5-0-tlt-2-0-ga/
Advanced video analysis solutions are in great demand across multiple industries. Some of the popular use-cases include retail aisles to understand customer brand sentiment, occupancy analytics for crowd management in mass transit locations, optimizing vehicle traffic patterns in cities, hospitals, and malls for social distancing protocols, and defect detection in manufacturing facilities. Building and deploying…Powered by Discourse, best viewed with JavaScript enabled"
1933,nvidia-hpc-sdk-21-3-now-available,"Originally published at:			NVIDIA HPC SDK 21.3 Now Available | NVIDIA Technical Blog
The NVIDIA HPC SDK is a comprehensive suite of compilers, libraries, and tools enabling developers to program the entire HPC platform from the GPU foundation to the CPU, and through the interconnect.Powered by Discourse, best viewed with JavaScript enabled"
1934,an-introduction-to-cuda-aware-mpi,"Originally published at:			https://developer.nvidia.com/blog/introduction-cuda-aware-mpi/
MPI, the Message Passing Interface, is a standard API for communicating data via messages between distributed processes that is commonly used in HPC to build applications that can scale to multi-node computer clusters. As such, MPI is fully compatible with CUDA, which is designed for parallel computing on a single computer or node. There are many reasons…Hello,I have a question about the MPI communication, I didn't understand if it can be used to transfer data between gpu's in the same machine.Thank you in advance.Hi Miguel,Yes. MPI can be used for communication between GPUs, both within a node and across nodes. MPI supports Intra-node (within the node) and Inter-node (across cluster nodes) communication. MVAPICH2 is a CUDA-Aware MPI library (mvapich.cse.ohio-state.edu) that you can use to perform communication for GPUs in the same machine as well across machines.Hi, Can it be used to split GPU computing power for smaller pieces for example to rent someone? how about mpi4py in python is that pycuda aware?Hi, Jiri, A new question. With CUDA aware MPI, in MPI_Send, if the send buffer is a device pointer and the data is produced by a previous compute kernel (might be  on a non-default stream, might be results of a cuBLAS call, or in other words, as a library developer, I don't know where the data is from), do I need to call cudaDeviceSynchronize() before the send? After MPI_Recv(), if the recv buffer is a device pointer, can I access it immediately in a kernel on a new stream?Thanks.Powered by Discourse, best viewed with JavaScript enabled"
1935,gtc-2020-nvidia-ngc-for-deep-learning-machine-learning-and-hpc,"GTC 2020 CWE21722
Presenters: Deep Learning Team,NVIDIA
Abstract
Meet with NVIDIA experts one-on-one for questions on using GPU-accelerated software from NGC for deep learning, machine learning, and HPC. Ask us about such topics as strategies for using NGC in your workflows; running NGC containers on different platforms; and using pre-trained models and industry SDKs with Docker, Singularity, and Kubernetes.Who would benefit: Data scientist, developers, devops, system admins supporting DL, ML, data analytics, and HPC workloadsHere are just a few topics we will cover. Feel free to bring your questions to the session:• Getting started with AI
• Best practices to train, infer, and deploy AI solutions
• Utilizing pre-trained models for NLP, TTS, Recommender and more
• Running NGC software on-premeses, cloud, and edge with Kubernetes
• Deploying DL frameworks and HPC app containers in Singularity
• Building performant and lean containers with NVIDIA tools
• Maximizing application performance and running applications at scaleWatch this session
Join in the conversation below.Powered by Discourse, best viewed with JavaScript enabled"
1936,data-center-interconnect-reference-design-guide,"Originally published at:			Data Center Interconnect Reference Design Guide | Cumulus Networks Guides
This reference guide on Data Center Interconnect with NVIDIA Cumulus Linux covers L2 and L3 extension and design aspects.Powered by Discourse, best viewed with JavaScript enabled"
1937,top-5-ray-tracing-sessions-for-graphics-developers-from-gtc-21,"Originally published at:			https://developer.nvidia.com/blog/top-5-ray-tracing-sessions-for-graphics-developers-from-gtc-21/
Industry luminaries joined us to introduce the fundamentals of real-time ray tracing, and how current developers such as Autodesk, Dassault, Chaos and ESI have integrated ray traced technologies into their most popular apps.Powered by Discourse, best viewed with JavaScript enabled"
1938,evaluating-applications-using-the-nvidia-arm-hpc-development-kit,"Originally published at:			https://developer.nvidia.com/blog/evaluating-applications-using-the-nvidia-arm-hpc-development-kit/
The Oak Ridge National Laboratory Leadership Computing Facility integrated the NVIDIA Arm HPC Developer Kit into their Wombat test cluster and tested different HPC applications.Powered by Discourse, best viewed with JavaScript enabled"
1939,nvidia-rtx-top-3-week-of-november-30-2018,"Originally published at:			https://developer.nvidia.com/blog/nvidia-rtx-top-3-week-of-november-30-2018/
Every week, we’ll be delivering 3 interesting stories coming from the world of RTX Game Development. 1 – Ray Tracing Optimizations Coming to Battlefield V “People can expect us to keep improving our ray tracing as time goes, as both we at DICE and Nvidia have a bunch of optimizations coming in from the engine…Powered by Discourse, best viewed with JavaScript enabled"
1940,extension-for-chatgpt,"
jcass358
Posted this question as a reply - so I am re-posting it here so everyone can see.Hello, I am definitely interested in using a tool like ChatGPT, would this event show us how to buld the extension? also how feasible would it be to perform transfer learning on the LLM with all of the NVIDIA information and software publicly available to finetune a model to help out beginners like myself.
thank youthank youYour are most welcome - sorry for any confusion - our guys will have a response for you very soonHi, while we will not be presenting today, there is a GitHub project that demonstrates how to implement the ChatGPT + Omniverse integration. You can find it here: GitHub - NVIDIA-Omniverse/kit-extension-sample-airoomgenerator: A tool used to create 3D content for rooms by calling OpenAI's API. As per the question on fine-tuning the model, while you can’t fine-tune GPT-4, you can fine tune other LLMs and then connect your fine-tuned model into Omniverse in a similar way we did in our Github project.This topic was automatically closed 14 days after the last reply. New replies are no longer allowed.Powered by Discourse, best viewed with JavaScript enabled"
1941,open-source-healthcare-ai-innovation-continues-to-expand-with-monai-v1-0,"Originally published at:			https://developer.nvidia.com/blog/open-source-healthcare-ai-innovation-continues-to-expand-with-monai-1-0/
Learn about MONAI, the domain-specific, open-source medical AI framework that drives research breakthroughs and accelerates AI into clinical impact.Powered by Discourse, best viewed with JavaScript enabled"
1942,explainer-what-is-edge-ai-and-how-does-it-work,"Originally published at:			What Is Edge AI and How Does It Work? | NVIDIA Blog
Edge AI is the deployment of AI applications in devices throughout the physical world. It’s called “edge AI” because the AI computation is done near the user at the edge of the network, close to where the data is located, rather than centrally in a cloud computing facility or private data center.Powered by Discourse, best viewed with JavaScript enabled"
1943,smart-mirror-monitors-your-face-for-telltale-signs-of-disease,"Originally published at:			Smart Mirror Monitors Your Face for Telltale Signs of Disease | NVIDIA Technical Blog
Wize Mirror looks like a mirror, but incorporates GPUs, 3D scanners, multispectral cameras and gas sensors to assess the health of someone looking into it. Mirror, can you please tell me if I have signs of diabetes? Researchers from the National Research Council of Italy present a novel multisensory device, the Wize Mirror, which is…Powered by Discourse, best viewed with JavaScript enabled"
1944,simplifying-cuda-upgrades-for-nvidia-jetson-users,"Originally published at:			https://developer.nvidia.com/blog/simplifying-cuda-upgrades-for-nvidia-jetson-users/
With CUDA Toolkit 11.8 and NVIDIA JetPack 5.0, you can upgrade to the latest CUDA release without updating NVIDIA JetPack or Jetson Linux BSP software.Powered by Discourse, best viewed with JavaScript enabled"
1945,post-your-questions-in-this-category,"You are at the right place to post questions for the AMAPost your questions to this category - NOT as a reply to this posting - thanksGoodnight.
I would like to know if it is possible to animate the mouth of a realistic photo and synchronize it with the output of an audio in real time, as in a conversation.
I also wanted to know how I would be charged for this functionality.
Thank you very much.Yes, we have a product called Maxine. One of the feature is Live portrait. We have the ability to use video and audio to animate the 2D portrait. See more here NVIDIA Maxine | NVIDIA DeveloperPowered by Discourse, best viewed with JavaScript enabled"
1946,nvidia-healthcare-on-tap-with-nvidia-sdks-season-2-now-live,"Originally published at:			https://developer.nvidia.com/blog/nvidia-healthcare-on-tap-with-nvidia-sdks-season-2-now-live/
This series is designed for developers in areas such as image analysis, scientific research, and drug discovery to learn more about AI in the healthcare and life sciences sector.Powered by Discourse, best viewed with JavaScript enabled"
1947,webinar-performance-measurement-of-robotics-applications-with-ros2-benchmark,"Originally published at:			Isaac ROS Webinar Series
Register now for this Isaac ROS webinar on May 4th to learn how to run and customize ros2_benchmark to measure your robotics application graphs of nodes.Powered by Discourse, best viewed with JavaScript enabled"
1948,tensorrt-5-ga-now-available,"Originally published at:			TensorRT 5 GA Now Available | NVIDIA Technical Blog
NVIDIA announced the latest version of the TensorRT’s high-performance deep learning inference optimizer and runtime. Today we are releasing the general availability TensorRT.  TensorRT 5 supports the new Turing architecture, provides new optimizations, and INT8 APIs achieving up to 40x faster inference over CPU-only platforms. This latest version also dramatically speeds up inference of recommenders, neural…Powered by Discourse, best viewed with JavaScript enabled"
1949,accelerate-your-edge-ai-journey-with-nvidia-igx-orin-developer-kit,"Originally published at:			https://developer.nvidia.com/blog/accelerate-your-edge-ai-journey-with-nvidia-igx-orin-developer-kit/
NVIDIA IGX Orin is the first platform to combine industrial-grade hardware with enterprise-level software and support for edge AI management.Powered by Discourse, best viewed with JavaScript enabled"
1950,cuda-pro-tip-kepler-texture-objects-improve-performance-and-flexibility,"Originally published at:			https://developer.nvidia.com/blog/cuda-pro-tip-kepler-texture-objects-improve-performance-and-flexibility/
The Kepler architecture introduces texture objects, a new feature that makes textures easier to use and higher performance. Texture References Textures are likely a familiar concept to anyone who’s done much CUDA programming. A feature from the graphics world, textures are images that are stretched, rotated and pasted on polygons to form the 3D graphics…Hi,There are a couple of bugs in this code: viz. the case of token cudatextureObject_t should actually be cudaTextureObject_t, there are invalid symbols such as a double quote in kernel launch code.Importantly, upon trying this code my compiler complains :error: more than one instance of overloaded function ""tex1Dfetch"" matches the argument list:Making a similar setup for 2D texture, I get:error : more than one instance of overloaded function ""tex2D"" matches the argument list:My platform is:Windows 7, Visual Studio 2012, NVIDIA Nsight 3.2, CUDA 5.5 Runtime Project, Debug Compile, compute capability set to compute_35,sm_35GK110 Card (GTX 780), Intel i7.Hi,After I added a template type to ""tex1Dfetch"", I could compile the code above.I mean,  float x = tex1Dfetch(tex, i);should be  float x = tex1Dfetch<float>(tex, i);I hope this helps you :-)My platform is:Windows 7, Visual Studio 2012, CUDA 5.5, Release Compile, compute capability set to compute_35,sm_35The code snippets in the blog post have now been rectified.  Thanks for posting.Hey,We just bought a PC with NVIDIA card and are trying to run different example code from different sources. We ran into trouble now with the command:cudaCreateTextureObject(&tex, &resDesc, &texDesc, NULL);It seems that in our computer the function fails to assign a value to ""tex"". When adding an assertion in the next line:assert(tex != 0);the assertion fails. We have also tried to surround the object creation command with checkCudaErrors()  in hope to get more information, but it gives no error message (so the program thinks everything is fine).Could you help us sort it out? We really have run out of ideas here and we have not found any hints online either.Our system specs:Ubuntu 14.04GeForce GTX 750Driver Version: 331.62Cuda toolkit 6.0compiling with parameter  ""--gpu-architecture sm_50""PS! Also worth mentioning that we have a large-scale example using Cuda via python/pylearn2/Theano running without problems. So in other cases out Cuda works fine and thus it seems a very specific issue.I have the same problem on MacBook Pro with GTX 750M. Cuda 6.5Submitted a bug report in Cuda Zone, also mentioned that it does not work on your system. I will let you know if we get help from there. Hope for the best.Can you provide the bug ID number?Bug ID is 1562891I looked at the bug, and our QA was able to reproduce the problem. It is now being investigated.  Sorry for the inconvenience, we'll resolve it as soon as we can.Please inform us as soon as any progress is made on fixing the bug, so we could plan our work.This “old” article on texture usage and how to migrate from the texture template reference format to the (for many years) recommended usage will be much appreciated now that the texture template is obsolete (as of CUDA 12.0). Posting this message in the hope that it will help others finding it in the search function.
I needed to be referred to it my a human.  :-)
[https://developer.nvidia.com/blog/cuda-pro-tip-kepler-texture-objects-improve-performance-and-flexibility](Texture article)Powered by Discourse, best viewed with JavaScript enabled"
1951,optimizing-enterprise-it-workloads-with-nvidia-certified-systems,"Originally published at:			https://developer.nvidia.com/blog/optimizing-enterprise-it-workloads-with-nvidia-certified-systems/
Choose from a range of workload-specific validated configurations for GPU-accelerated servers and workstations.Powered by Discourse, best viewed with JavaScript enabled"
1952,accelerating-robotics-simulation-with-nvidia-omniverse-isaac-sim,"Originally published at:			Accelerating Robotics Simulation with NVIDIA Omniverse Isaac Sim | NVIDIA Technical Blog
NVIDIA released a new NVIDIA Omniverse Experience for Robotics for NVIDIA Isaac Sim to help support the accelerating need for accurate, reliable, easy-to-use simulation tools in robotics. With many research labs and universities closed indefinitely, roboticists around the world are separated from the physical hardware and environments that they need for developing, testing, and deploying…Powered by Discourse, best viewed with JavaScript enabled"
1953,developing-an-autonomous-bot-is-a-walk-in-the-park,"Originally published at:			https://developer.nvidia.com/blog/developing-an-autonomous-bot-is-a-walk-in-the-park/
Each November for the last decade, Tsukuba City in Japan has run a 2,000-meter race unlike just about any other in the world. What’s unusual is not the terrain – spanning parks, city streets, even indoor shopping malls – or the speed, the pace is a leisurely stroll. It’s the participants: each is an autonomous…Powered by Discourse, best viewed with JavaScript enabled"
1954,implementing-high-performance-matrix-multiplication-using-cutlass-v2-8,"Originally published at:			Implementing High Performance Matrix Multiplication Using CUTLASS v2.8 | NVIDIA Technical Blog
High performance CUTLASS template abstractions support matrix multiply operations (GEMM), Convolution AI, and improved Strided-DGrad.Powered by Discourse, best viewed with JavaScript enabled"
1955,nvidia-research-at-c-mimi-understanding-speech-to-automate-charting-for-telemedicine-and-beyond,"Originally published at:			https://developer.nvidia.com/blog/c-mimi-research-nlp-telemedicine/
In a new research paper, NVIDIA researchers deploy a state-of-the-art pretrained speech architecture to help clinicians augment patient experience with key extracts of symptoms, diagnosis, and recommended therapy.Powered by Discourse, best viewed with JavaScript enabled"
1956,job-statistics-with-nvidia-data-center-gpu-manager-and-slurm,"Originally published at:			Job Statistics with NVIDIA Data Center GPU Manager and SLURM | NVIDIA Technical Blog
Resource management software, such as SLURM, PBS, and Grid Engine, manages access for multiple users to shared computational resources. The basic unit of resource allocation is the “job”, a set of resources allocated to a particular user for a period of time to run a particular task. Job level GPU usage and accounting enables both users…Using -c allgpus the solution proposed does not work for non exclusive nodes.You may want to build upon these instead:prolog:… it would be great if dcgmi group -c would do json as well.epilog:… this requires jp.Powered by Discourse, best viewed with JavaScript enabled"
1957,how-to-implement-performance-metrics-in-cuda-c-c,"Originally published at:			https://developer.nvidia.com/blog/how-implement-performance-metrics-cuda-cc/
In the first post of this series we looked at the basic elements of CUDA C/C++ by examining a CUDA C/C++ implementation of SAXPY. In this second post we discuss how to analyze the performance of this and other CUDA C/C++ codes. We will rely on these performance measurement techniques in future posts where performance optimization…When the runtime is too high, a problem happens. The output is zero or very low!::::MY CODE::::cudaEvent_t start, stop;float elapsedTime;cudaEventCreate(&start);    cudaEventRecord(start,0);    Mult(A, B, C, blockSize, gridSize);    cudaEventCreate(&stop);    cudaEventRecord(stop,0);    cudaEventSynchronize(stop);    cudaEventElapsedTime(&elapsedTime, start,stop);    printf(""Elapsed time : %.2f ms\n"" ,elapsedTime);    printf(""Elapsed time : %.2f s\n"" ,elapsedTime/1000);    printf(""Elapsed time : %.2f min\n"" ,(elapsedTime/1000)/60);    printf(""Elapsed time : %.2f h\n"" ,((elapsedTime/1000)/60)/60);::::OUTPUT (A[16384x16384], B[16384x16384])::::Elapsed time : 0.00 msElapsed time : 0.00 sElapsed time : 0.00 minElapsed time : 0.00 hI counted 8 seconds to run!!!!!!!Would it make sense to use ""N*sizeof(float)"" rather than ""N*4"" for the bandwidth calculation?Hay,If i want to calculate the bandwidth for double precison is enough to change N*4*3 with N*8*3 ? In my opinion bandwidth in double precison shoud be lower then float. I have try this test and :bandwidth in single precison is :30GBbandwidth in double precison is: 45 GBI don't think this is correct. Has somebody any idea? Thanks:)You would also need to change the code to use double instead of float. Memory bandwidth is not directly related to datatype. It may be that your array is not large enough to saturate memory bandwidth, so by changing from float to double you may be better utilizing memory bandwidth. But the computational throughput of double vs. single may also come into play.  I would need to know what GPU you are running on and how big N is to reason about this better.Yes.You are probably hitting the OS watchdog timer. If you are on a system where the GPU is attached to a display, it will not allow a kernel to run for more than a couple of seconds before killing it.Hey, thanks for your answer.I have a GeForce GT 750M. About the N is the same number like in your example (N = 20 * (1 20);). In the code I didn't make other changes, just I have replaced float with a double.My board has a BUS with 128 bits I think (in the code show me 128 bits and online say 2x128 bits) so my theoretical bandwidth alter your formula will be 30GB (and that is the output of the program) but for double I don't see what is the problem.Thanks for this great tutorial .. I've a question, I'm preparing to give a training in an automotive company, can I use this tutorial examples and other tutorials here with mentioning the source? Can you also refer me to other tutorial by Nvidea that I can use ?Hei,Why is such a big difference between THEORETICAL BANDWIDTH and EFFECTIVE BANDWIDTH.This should be almost the same, but also the difference is about 30GB.hi,*my first question:cudaEventSynchronize() blocks CPU execution until the specified event is recorded. In the example shown, the cudaEventRecord(end)  comes before the  cudaMemcpy() instruction. Both instructions were issued to the same stream (stream 0), so cudaMemcpy() will be exec after cudaEventRecord(end) from the device perspective. In this case, i suppose we don't need to add the cudaEventSynchronize() after the cudaMemcpy(), because cudaMemcpy() is synchronous with the host therefor after the copy back from device we are certin that the event was recorded.*for my second question:when measuring the Effective Bandwidth, i suppose we should place the cudaEventRecord() around the cudaMemcpy()  and not around the kernel execution !Hi Azri, neither of those are questions, but I'll try to answer anyway.1. Correct.2. That depends. If you want to measure GPU device memory bandwidth achieved by the kernel, then time the kernel. If you want to measure host-device interconnect bandwidth achieved, then time the memcpy.thanks ^^I suppose not all instructions  within a period of time involve memory transfers?Are badwidth and throughput increased when we increase input array size? Can someone please explain?This is really helpful! Thank you about your tips!By the way, I have a stupid question to you with my code.=================================__global__void HelloGPU() { printf(""hello Worlds.\n"");}int main() { float ms; cudaEvent_t start, end; cudaEventCreate(&start); cudaEventCreate(&end); cudaEventRecord(start, 0); HelloGPU << <10, 1 >> > ();            //or HelloGPU << <1, 10 >> > (); cudaEventRecord(end, 0); cudaEventSynchronize(end); cudaEventElapsedTime(&ms, start, end); printf(""GPU = %.20lf\n"", ms);=================================This is code for confiming a speed between CPU and GPU process.However, I just got a 0.153 sec in GPU rather than 0.003 sec in CPU which was made of 'for' loop and sentences.I thought GPU process is much faster than CPU process but the result of mine is different.Can I get a some hit or can you tell me what i miss?Thank you!Hi ! My answer maybe not definitely correct . For your reference, there are many cores in GPU. On the other hand , there is just few cores in CPU, but they are much stronger than GPU cores. So, GPU is suitable for intensively computing program.I am sorry about my late reply. BUT I can understand why!Thank you!^^I wonder if there is a way to measure the runtime of a kernel inside the device code. To be a little more specific, I have a kernel that I run on N blocks each with M threads, like this:My_Kernel<<<n, m="""">>>();I'm running N embarrassingly parallel simulations that are each assigned to one block of M threads. So, computations in each block are independent of other blocks. I need to measure the runtime of each block (simulation).I would be so thankful if you can kindly suggest a way to do so.Apart from GPU device memory bandwidth, are we not also counting the time taken for the multiply-add operation?Powered by Discourse, best viewed with JavaScript enabled"
1958,game-developers-new-resources-from-gtc-21,"Originally published at:			https://developer.nvidia.com/blog/game-developers-new-resources-from-gtc-21/
NVIDIA RTX enables developers to create breathtaking, interactive worlds with performance that exceeds gamers expectations. Integrating RTX has never been easier - gain access through popular game engines such as Unreal Engine or through standalone SDKs made available at GTC.Powered by Discourse, best viewed with JavaScript enabled"
1959,choosing-the-right-speed-for-your-leaf-spine-data-center-network,"Originally published at:			https://developer.nvidia.com/blog/choosing-the-right-speed-for-your-leaf-spine-data-center-network/
This post was originally published on the Mellanox blog in April 2020. People generally assume that faster network interconnects maximize endpoint performance. In this post, I examine the key factors and considerations when choosing the right speed for your leaf-spine data center network. To establish a common ground and terminology, Table 1 lists the five…Powered by Discourse, best viewed with JavaScript enabled"
1960,gtc-digital-iva-iot-presentations-demos-and-posters,"Originally published at:			https://developer.nvidia.com/blog/gtc-digital-iva-iot/
GTC Digital is all the great training, research, insights, and direct access to the brilliant minds of NVIDIA’s GPU Technology Conference, now online. Join live webinars, training, and Connect with the Experts sessions, or choose from a library of talks, panels, research posters, and demos that you can view on your own schedule, at your own…Powered by Discourse, best viewed with JavaScript enabled"
1961,vrworks-360-video-sdk-2-0-adds-features-turing-support,"Originally published at:			VRWorks 360 Video SDK 2.0 Adds Features, Turing Support | NVIDIA Technical Blog
An ecosystem of camera systems and video processing applications surround us today for professional and consumer use, be it, for film or home video. The ability to enhance and optimize this omnipresent stream of videos and photos has become an important focus in consumer and prosumer circles. The real-world use cases associated with 3DoF 360 video have…I recently downloaded VR Works 360 for app developers but I am unable to find header files required for video stitching, i.e. nvstich.h, etc. Could you help me find the right SDK for video stitching?NVIDIA VRWorks 360 is no longer available or supported. For more information about other VRWorks products, see https://developer.nvidia.com/vrworks.Powered by Discourse, best viewed with JavaScript enabled"
1962,upcoming-webinar-learn-how-to-build-conversational-ai-applications,"Originally published at:			https://developer.nvidia.com/blog/upcoming-webinar-learn-how-to-build-conversational-ai-applications/
Sign up for a workshop on building and deploying production quality conversational AI applications, hosted by NVIDIA DLI.Powered by Discourse, best viewed with JavaScript enabled"
1963,arrayfire-a-portable-open-source-accelerated-computing-library,"Originally published at:			https://developer.nvidia.com/blog/arrayfire-portable-open-source-accelerated-computing-library/
The ArrayFire library is a high-performance software library with a focus on portability and productivity. It supports highly tuned, GPU-accelerated algorithms using an easy-to-use API. ArrayFire wraps GPU memory into a simple “array” object, enabling developers to process vectors, matrices, and volumes on the GPU using high-level routines, without having to get involved with device…Nice!!! what about bindings for python? Is there other library for this programming language?Powered by Discourse, best viewed with JavaScript enabled"
1964,caustics-available-this-week-on-experimental-nvidia-branch-of-unreal-engine-4,"Originally published at:			Caustics Available This Week on Experimental NVIDIA Branch of Unreal Engine 4 | NVIDIA Technical Blog
NVIDIA’s branch of Unreal Engine 4 (NvRTX UE4 Branch) is popular with developers because it allows teams to maximize the performance of games running on NVIDIA RTX GPUs with cutting-edge features. 8K Ultra Performance Mode Available Now A great example of this is the new Ultra Performance Mode, which allows for 8K gaming on compatible…Powered by Discourse, best viewed with JavaScript enabled"
1965,nvidia-ceo-shares-current-state-of-artificial-intelligence,"Originally published at:			https://developer.nvidia.com/blog/nvidia-ceo-shares-current-state-of-artificial-intelligence/
In an interview with Fortune, NVIDIA CEO Jen-Hsun Huang weighs in on data centers, driverless cars, and deep learning technology. Below are some excerpts from the interview: Fortune: What’s the current status of artificial intelligence? Jen-Hsun Huang: 2015 was a big year. Artificial intelligence is moving into the commercial world. AI has been worked on…Powered by Discourse, best viewed with JavaScript enabled"
1966,ultra-realism-made-accessible-with-nvidia-ai-and-path-tracing-technologies,"Originally published at:			https://developer.nvidia.com/blog/ultra-realism-made-accessible-with-ai-and-path-tracing-technologies/
At GDC 2023, NVIDIA released new tools that make real-time path tracing more accessible to developers while accelerating the creation of ultra-realistic game worlds. Video 1. NVIDIA at GDC 2023: Frame Generation and Path Tracing Tools Now Available Generate frames with the latest breakthrough in AI rendering Announced with the NVIDIA Ada Lovelace architecture, DLSS…Powered by Discourse, best viewed with JavaScript enabled"
1967,drive-labs-how-localization-helps-vehicles-find-their-way,"Originally published at:			https://developer.nvidia.com/blog/drive-labs-how-localization-helps-vehicles-find-their-way/
Localization is a critical capability for autonomous vehicles, making it possible to pinpoint their location within centimeters inside a map.Powered by Discourse, best viewed with JavaScript enabled"
1968,ai-helps-nba-players-dance-on-the-jumbotron,"Originally published at:			AI Helps NBA Players Dance on the Jumbotron | NVIDIA Technical Blog
So you think you can dance? Earlier this month, the Dallas Mavericks of the NBA showed off a new deep learning in-game entertainment application that synthesized the dance moves of one of their star players on the team’s jumbotron. The Texas-based startup Xpire AI who was co-founded by the team’s owner Mark Cuban, based their…Powered by Discourse, best viewed with JavaScript enabled"
1969,cuda-pro-tip-minimize-the-tail-effect,"Originally published at:			https://developer.nvidia.com/blog/cuda-pro-tip-minimize-the-tail-effect/
When I work on the optimization of CUDA kernels, I sometimes see a discrepancy between Achieved and Theoretical Occupancies. The Theoretical Occupancy is the ratio between the number of threads which may run on each multiprocessor (SM) and the maximum number of executable threads per SM (2048 on the Kepler architecture). This value is estimated…You say the GPU arranges blocks in a grid into waves, and allocates them to SMs on a per-wave basis, not a block-by-block basis.Does this mean an idle SM with free resources will not be assigned a ready block until every SM on the device is able to accept a new block?Waves are an easy abstraction but the work is launched on a block-by-block basis (so, the answer to your question is no). If you have a grid of blocks which leads to a couple of full waves. You may still have a strong tail effect if a few blocks are significantly longer than the others. It's a rather classical scheduling problem.Powered by Discourse, best viewed with JavaScript enabled"
1970,deep-learning-in-a-nutshell-history-and-training,"Originally published at:			https://developer.nvidia.com/blog/deep-learning-nutshell-history-training/
This series of blog posts aims to provide an intuitive and gentle introduction to deep learning that does not rely heavily on math or theoretical constructs. The first part in this series provided an overview over the field of deep learning, covering fundamental and core concepts. The third part of the series covers sequence learning…I would like to know when did NVIDIA began production on AI products?Hi,
I am a research scholar at IIT Kanpur. I am working towards my PhD thesis on the architecture optimization of deep learning models.I was going through brief history on deep learning. I came across this blog article which has mentioned about Ivakhnenko and Lapa in 1965 (Figure 1).Can you help me to get the digital copy of this article.I am expecting your help in this regard.Thanking you,
Arun.@arun.iitkgp2k9  – You can find Ivakhnenko and Lapa’s work in the following book:  Cybernetic predicting devices. Good luck with your thesis!Hi,
I was going through brief history on deep learning. I came across this blog article which has mentioned about Ivakhnenko and Lapa in 1965 (Figure 1).Can you help me to get the digital copy of this article.I am expecting your help in this regard.Thanking you,As I mentioned to @arun.iitkgp2k9 earlier, you can find Ivakhnenko and Lapa’s work in the following book: Cybernetic predicting devices . Good luck!Powered by Discourse, best viewed with JavaScript enabled"
1971,a-new-stac-a2-record,"Originally published at:			A New STAC-A2 Record | NVIDIA Technical Blog
The results are in, and GPUs are still the fastest solution on the planet for financial risk management. This is according to the latest STAC-A2 audited test results. STAC-A2 is a risk management benchmark created by leading global banks working with the Securities Technology Analysis Center (STAC) in order to allow assessment of financial compute…Powered by Discourse, best viewed with JavaScript enabled"
1972,ai-helps-discover-hit-songs-for-record-labels,"Originally published at:			AI Helps Discover Hit Songs for Record Labels | NVIDIA Technical Blog
How does a top of the charts song sound? This new AI system can tell you. Musiio, a new music tech company based in Singapore, is using deep learning to identify the best songs for record labels and streaming services. The service can listen to over 30,000 songs a day, around the same number of…Powered by Discourse, best viewed with JavaScript enabled"
1973,nvidia-cuda-toolkit-12-2-unleashes-powerful-features-for-boosting-applications,"Originally published at:			https://developer.nvidia.com/blog/nvidia-cuda-toolkit-12-2-unleashes-powerful-features-for-boosting-applications/
The latest release of NVIDIA CUDA Toolkit 12.2 introduces a range of essential new features, modifications to the programming model, and enhanced support for hardware capabilities accelerating CUDA applications. Now out through general availability from NVIDIA, CUDA Toolkit 12.2 includes many new capabilities, both major and minor.  The following post offers an overview of many…Powered by Discourse, best viewed with JavaScript enabled"
1974,cudacasts-episode-15-introduction-to-thrust,"Originally published at:			https://developer.nvidia.com/blog/cudacasts-episode-15-introduction-thrust/
Whenever I hear about a developer interested in accelerating his or her C++ application on a GPU, I make sure to tell them about Thrust. Thrust is a parallel algorithms library loosely based on the C++ Standard Template Library. Thrust provides a number of building blocks, such as sort, scans, transforms, and reductions, to enable…Powered by Discourse, best viewed with JavaScript enabled"
1975,nvidia-research-nerf-tex-neural-reflectance-field-textures,"Originally published at:			https://developer.nvidia.com/blog/nvidia-research-nerf-tex-neural-reflectance-field-textures/
NVIDIA researchers collaborated with ETH Zurich and Disney Research|Studios on a new paper, “NeRF-Tex: Neural Reflectance Field Textures” that will be presented at the Eurographics Symposium on Rendering (EGSR) June 29 – July 2, 2021. The paper introduces a new modeling primitive: neural reflectance field textures, or NeRF-Tex for short. NeRF-Tex is inspired by recent…Powered by Discourse, best viewed with JavaScript enabled"
1976,gtc-2020-latest-advancements-for-production-rendering-with-v-ray-gpu-and-real-time-raytracing-with-project-lavina,"GTC 2020 S22197
Presenters: Alexander Soklev,Chaos Software Ltd
Abstract
We’ll cover the latest improvements in V-Ray GPU, including out-of-core rendering and RTX support through OptiX 7, as well as real-time raytracing with DXR and Project Lavina.Watch this session
Join in the conversation below.Powered by Discourse, best viewed with JavaScript enabled"
1977,create-a-3d-caricature-in-minutes-with-deep-learning,"Originally published at:			Create a 3D Caricature in Minutes with Deep Learning | NVIDIA Technical Blog
Researchers from The University of Hong Kong developed a deep learning-based sketching system that lets users easily create a 3D face in minutes. The low-cost interactive face modeling system allows the user to draw freehand imprecise 2D lines and then with the help of deep learning, they’re able to further manipulate the expressions of the…Powered by Discourse, best viewed with JavaScript enabled"
1978,parallel-direct-solvers-with-cusolver-batched-qr,"Originally published at:			https://developer.nvidia.com/blog/parallel-direct-solvers-with-cusolver-batched-qr/
[Note: Lung Sheng Chien from NVIDIA also contributed to this post.] A key bottleneck for most science and engineering simulations is the solution of sparse linear systems of equations, which can account for up to 95% of total simulation time. There are two types of solvers for these systems: iterative and direct solvers.  Iterative solvers are…Fantastic development -- the cuSOLVER documentation has quite some potential to be updated and corrected but the library is a great project! We are using cuSOLVER to develop a 1D / 2D / 3D particle in cell code with a sparse Poisson solver for beam dynamics and collective effects in accelerator physics here at CERN.I want to use cuSOLVER to find the eigenvalues of a large matrixPowered by Discourse, best viewed with JavaScript enabled"
1979,accelerating-wide-deep-recommender-inference-on-gpus,"Originally published at:			Accelerating Wide & Deep Recommender Inference on GPUs | NVIDIA Technical Blog
Recommendation systems drive engagement on many of the most popular online platforms. As the growth in the volume of data available to power these systems accelerates rapidly, data scientists are increasingly turning from more traditional machine learning methods to highly expressive deep learning models to improve the quality of their recommendations. Google’s Wide & Deep…Powered by Discourse, best viewed with JavaScript enabled"
1980,now-available-nvidia-drive-agx-orin-developer-kit-with-drive-os-6,"Originally published at:			Now Available: NVIDIA DRIVE AGX Orin Developer Kit with DRIVE OS 6 | NVIDIA Technical Blog
With Orin and the latest DRIVE OS release, you now have access to flexible and scalable platforms to build safer, more efficient transportation.Powered by Discourse, best viewed with JavaScript enabled"
1981,cuda-11-features-revealed,"Originally published at:			CUDA 11 Features Revealed | NVIDIA Technical Blog
The new NVIDIA A100 GPU based on the NVIDIA Ampere GPU architecture delivers the greatest generational leap in accelerated computing. The A100 GPU has revolutionary hardware capabilities and we’re excited to announce CUDA 11 in conjunction with A100. CUDA 11 enables you to leverage the new hardware capabilities to accelerate HPC, genomics, 5G, rendering, deep…Hi! I have problems with executing nvidia-smi commands on Ampere which were fully working on Turing.
Clocks, caches and other gives Unknown error.
Failed to initialize NVML: Unknown ErrorI have newest drivers up to date.
Do I have to have CUDA applet installed to manipulate the clocks?CheersPeterNone of the links at the bottom of the article are working.  They all 404.@a13ph.fsal  – Sorry about that! The links have now been updated to the GTC On-Demand site. Let me know if you have any other comments about our posts…Powered by Discourse, best viewed with JavaScript enabled"
1982,gtc-2020-training-biomedical-clinical-language-models-using-bert,"GTC 2020 S21108
Presenters: Raghav Mani,NVIDIA; hoo chang shin,NVIDIA; Anthony Costa,Icahn School of Medicine at Mount Sinai; Eric Oermann,Icahn School of Medicine at Mount Sinai
Abstract
Pre-trained language models like BERT that are built using Transformer networks have produced greatly improved performance in a wide variety of NLP tasks. However, making BERT perform as well on other domain-specific text corpora, such as in the biomedical domain, is not straightforward. The NVIDIA team will describe the general trends in the evolution of these language models, and the tools they’ve created to efficiently train large domain specific language models like BioBERT.The Mt. Sinai team will then talk about how they’re applying these tools and techniques to build clinical language models using what’s potentially the largest corpus of medical text to date, almost 8 times larger than the Wikipedia Corpus. Given the difficulty in accessing massive clinical datasets, approaches to improving masked language model performance on clinical tasks have focused on transfer learning and ever expanding parameter sizes. The Mount Sinai team will discuss the implications of increasing pretraining data availability by orders of magnitude to mass language models, evaluating these pre-trained models on established clinical tasks, such as named entity recognition and 30 day readmissions. They’ll delve into the model architecture, NLP experiments, and GPU configuration they used to facilitate this study.Watch this session
Join in the conversation below.Powered by Discourse, best viewed with JavaScript enabled"
1983,tips-tricks-getting-the-best-ray-tracing-performance-out-of-unreal-engine-4-23,"Originally published at:			Tips & Tricks: Getting the Best Ray Tracing Performance Out of Unreal Engine 4.23 | NVIDIA Technical Blog
Roughly five months ago, we introduced you to the new ray tracing support (via DirectX Raytracing) in the 4.22 release of Unreal Engine. Recently, Epic Games released version 4.23 which brings a number of upgrades for those working with ray tracing. Even better, many of these new and improved features, such as enhancements to performance,…Powered by Discourse, best viewed with JavaScript enabled"
1984,what-are-your-favorite-parallel-programming-references,"Originally published at:			https://developer.nvidia.com/blog/what-are-your-favorite-parallel-programming-references/
Recently a colleague asked if I could recommend a good parallel programming textbook. Since it isn’t the first time I’ve been asked this question, and it comes up from time to time, I thought it would be interesting to share the question with you, dear reader. What are your favorite books about parallel programming (or…Powered by Discourse, best viewed with JavaScript enabled"
1985,announcing-latest-nsight-graphics-2021-4-download-now,"Originally published at:			https://developer.nvidia.com/blog/announcing-latest-nsight-graphics-2021-4-download-now/
Learn about the latest release of Nsight Graphics 2021.4, an all-in-one graphics debugger and profiler to help game developers get the most out of NVIDIA hardware.Thanks for sharing!Powered by Discourse, best viewed with JavaScript enabled"
1986,using-mesh-shaders-for-professional-graphics,"Originally published at:			https://developer.nvidia.com/blog/using-mesh-shaders-for-professional-graphics/
Mesh shaders were introduced with the Turing architecture and are shipping with Ampere as well. In this post, I offer a detailed look over mesh shader experiences for these hardware architectures so far. The context of these results was primarily CAD and DCC viewport or VR-centric. However, some of it may be applicable to games…One more thing to add, while the blog shows mostly GLSL in the context of OpenGL and Vulkan, the tips do apply to DirectX 12 Ultimate as well. The main difference is that in DirectX one would use shared memory for primitive culling and then allocate the mesh via SetMeshOutputCounts and do the write out after that.If you have any questions or comments, let us know.Great work and very inspiring. I really enjoy whenever you come along with a new mesh shader post :-)I have a couple of question on Table 2.  Maybe I missed something while reading.How many triangles does the Lucy Modell have? I am little bit confused, because the site you link suggest that it is more like 28 million triangles. You can provide the exact triangle and vertex count, please?Can you also provide byte exact numbers for that table?Were those numbers achieved with the 64 vertices/84 triangles meshlet?To how many bits where the vertex position quantized. That text only states a few bits?Thank you very much in advance!RegardsQuirinhi @quirin.meyer1 Here are some statistics from a slightly different version of the model (there a slightly different variants given different vertex merging setups, and I didn’t remember exactly the one used in the article)Size of vertex data:   224808928 (128 bit per vertex, 3 x fp32 pos, 2x unorm16 octant normal)
Size of index data:    336668864 (32 bit)
triangles total:  2805573864 vertices 84 primitives meshletsmeshlets;  ;
prim;  <number of total triangles, removed degenerated>; ;
vertex;  <number of vertices within meshlets, how much fetching/transforms etc. we will do>; ;each meshlet always has a 128-bit header (stored in a dense array), and then a variable amount of raw data (primitive, vertex, indices etc.), whose offset is stored within the header. The raw data is typically aligned to 32-bit sometimes 64-bit as well to aid decoding logic and get appropopriate load instructions.meshlet basic packing (1)
meshlets;  342266; prim;  28055736; 1.00; vertex;  19784789; 0.90;  168644 KBmeshlet with delta indices
meshlets;  334288; prim;  28055736; 1.00; vertex;  19718844; 0.92;  117053 KBmeshlet with quantized position (10-bit unorm components, packed in single 32-bit, relative to meshlet cluster’s fp32 bbox also stored within meshlet)
meshlets;  334288; prim;  28055736; 1.00; vertex;  19718844; 0.92;  165128 KBmeshlet with quantized position and delta indices for other attributes
meshlets;  334288; prim;  28055736; 1.00; vertex;  19718844; 0.92;  243869 KB(1) https://github.com/nvpro-samples/gl_vk_meshlet_cadscene/blob/master/nvmeshlet_packbasic.hppPowered by Discourse, best viewed with JavaScript enabled"
1987,gtc-2020-accelerating-sptag-library-on-the-gpus-for-approximate-nearest-neighborhood-search,"GTC 2020 S21561
Presenters: Murat Guney,NVIDIA
Abstract
The Approximate Nearest Neighborhood (ANN) search algorithm is essential to many machine-learning applications. For instance, vector similarity search, multimedia search, and duplicate entry search all employ ANN for handling very large datasets efficiently. There are two main approaches to implementing ANN: space partitioning trees and locality sensitive hashing. Although hashing methods are accelerated on the GPUs (RAPIDS and FAISS), to our knowledge there is no GPU-accelerated space partitioning ANN algorithm in the literature. We’ll present the GPU acceleration of the SPTAG library, where both space partition tree and neighborhood graph construction are accelerated on GPUs. We’ll discuss the data structures and algorithms developed for efficient GPU implementation. Finally, we’ll discuss the performance characteristics and compare the execution times against a single-socket GPU.Watch this session
Join in the conversation below.Powered by Discourse, best viewed with JavaScript enabled"
1988,how-to-overlap-data-transfers-in-cuda-fortran,"Originally published at:			https://developer.nvidia.com/blog/how-overlap-data-transfers-cuda-fortran/
CUDA Fortran for Scientists and Engineers shows how high-performance application developers can leverage the power of GPUs using Fortran. In my previous CUDA Fortran post I discussed how to transfer data efficiently between the host and device.  In this post, I discuss how to overlap data transfers with computation on the host, computation on the device, and…Powered by Discourse, best viewed with JavaScript enabled"
1989,three-things-you-need-to-know-about-nsight-systems,"Originally published at:			Three Things You Need to Know About Nsight Systems | NVIDIA Technical Blog
In this video, Seth Schneider, Nsight Program Manager at NVIDIA, details the three most important things developers need to know about Nsight Systems. Watch below: 3: Nsight Systems is an Extremely Low Overhead Profiling Tool Nsight Systems is your first stop in the profiling workflow. Inspect your application’s algorithm timings and GPU interactions to identify…Powered by Discourse, best viewed with JavaScript enabled"
1990,ai-at-the-edge-challenge-spotlight-saving-bandwidth-with-anomaly-detection,"Originally published at:			https://developer.nvidia.com/blog/ai-at-the-edge-challenge-spotlight-saving-bandwidth-with-anomaly-detection/
Most smart city applications today rely on analyzing large amounts of video data from cameras. The ability to identify and reason over the most relevant events within a video is essential to build efficient and scalable applications. In the recently concluded AI at the Edge Challenge, team SmellslikeML proposed an NVIDIA Jetson Nano-based application and…Powered by Discourse, best viewed with JavaScript enabled"
1991,metropolis-spotlight-viisights-uses-ai-to-understand-behavior-and-predict-what-might-happen-next,"Originally published at:			https://developer.nvidia.com/blog/metropolis-spotlight-viisights-uses-ai-to-understand-behavior-and-predict-what-might-happen-next/
Developers can use the new viisights wise application powered by GPU-accelerated AI technologies to help organizations and municipalities worldwide avoid operational hazards and risk in their most valuable spaces.Powered by Discourse, best viewed with JavaScript enabled"
1992,accelerating-spark-3-0-and-xgboost-end-to-end-training-and-hyperparameter-tuning,"Originally published at:			https://developer.nvidia.com/blog/accelerating-spark-3-0-and-xgboost-end-to-end-training-and-hyperparameter-tuning/
At GTC Spring 2020, Adobe, Verizon Media, and Uber each discussed how they used Spark 3.0 with GPUs to accelerate and scale ML big data pre-processing, training, and tuning pipelines. Adobe Intelligent Services achieved a 7x performance improvement and 90 percent cost savings with a GPU-based Spark 3.0 and XGBoost intelligent email solution to optimize…If you have any questions about Spark 3.0, RAPIDS , XGBoost or hyperparameter tuning let me knowPowered by Discourse, best viewed with JavaScript enabled"
1993,achieving-supercomputing-scale-quantum-circuit-simulation-with-the-nvidia-dgx-cuquantum-appliance,"Originally published at:			Achieving Supercomputing-Scale Quantum Circuit Simulation with the NVIDIA DGX cuQuantum Appliance | NVIDIA Technical Blog
Scale quantum circuit simulations on the largest supercomputers with the DGX cuQuantum Appliance.Powered by Discourse, best viewed with JavaScript enabled"
1994,analyzing-scientific-data-from-mars-with-deep-learning,"Originally published at:			https://developer.nvidia.com/blog/analyzing-scientific-data-from-mars-with-deep-learning/
Researchers from University of Massachusetts Amherst and Mount Holyoke College received a four-year grant from the National Science Foundation to analyze images and data on the chemical composition of rocks and dust from NASA’s Curiosity rover. The rover has been exploring a crater on Mars since 2012 and sends back large amounts of data collected…Powered by Discourse, best viewed with JavaScript enabled"
1995,accelerate-ai-training-faster-than-ever-with-new-nvidia-omniverse-replicator-capabilities,"Originally published at:			https://developer.nvidia.com/blog/accelerate-ai-training-faster-than-ever-with-new-nvidia-omniverse-replicator-capabilities/
Announced at GTC, technical artists, software developers, and ML engineers can now build custom, physically accurate, synthetic data generation pipelines in the cloud with NVIDIA Omniverse Replicator. Omniverse Replicator is a highly extensible framework built on the NVIDIA Omniverse platform that enables physically accurate 3D synthetic data generation to accelerate the training and accuracy of…Powered by Discourse, best viewed with JavaScript enabled"
1996,explainer-what-s-the-difference-between-level-2-and-level-5-autonomy,"Originally published at:			What’s the Difference Between Level 2, Level 3, Level 4, and Level 5 Autonomy?
To define the path to fully realized autonomy, SAE International detailed six categories of autonomous capability to establish clear benchmarks.Powered by Discourse, best viewed with JavaScript enabled"
1997,gtc-2020-can-gpus-transform-earth-system-models-and-help-mitigate-impacts-of-weather-and-climate-on-society,"GTC 2020 S22384
Presenters: Richard Loft,Computational and Information Systems Laboratory, National Center for Atmospheric Research
Abstract
Storm-resolving Earth System models capable of making statistically-based predictions on seasonal to decadal scales are urgently needed to aid long-range planning, which will help mitigate the impacts of weather and climate on society. To meet this need, our team at NCAR has built a partnership with universities and the private sector to develop and test effective and innovative ways to use of exascale architectures and machine learning algorithms to accelerate and improve Earth System Modeling. These efforts are already showing encouraging results.Watch this session
Join in the conversation below.Powered by Discourse, best viewed with JavaScript enabled"
1998,simulate-and-create-smart-indoor-robots-with-the-new-isaac-sdk-version-2019-3,"Originally published at:			Simulate and Create Smart Indoor Robots with the new Isaac SDK Version 2019.3 | NVIDIA Technical Blog
Today NVIDIA announced the availability of Isaac SDK 2019.3 with new simulation capabilities, new DNNs, and much more. The NVIDIA Isaac Software Development Kit (SDK) is the Industry’s first Robotic AI Development Platform with Simulation, Navigation, and Manipulation. The SDK includes Isaac Engine, high-performance robotics algorithm packages (GEMs), hardware reference applications and Isaac Sim for…Powered by Discourse, best viewed with JavaScript enabled"
1999,turbocharging-generative-ai-workloads-with-nvidia-spectrum-x-networking-platform,"Originally published at:			https://developer.nvidia.com/blog/turbocharging-ai-workloads-with-nvidia-spectrum-x-networking-platform/
NVIDIA Spectrum-X networking platform is an end-to-end solution that combines AI-optimized networking hardware and software to provide predictable, consistent performance required by AI workloads.Powered by Discourse, best viewed with JavaScript enabled"
2000,using-ai-to-detect-damage-in-nuclear-reactors,"Originally published at:			https://developer.nvidia.com/blog/using-ai-to-detect-damage-in-nuclear-reactors/
Designing materials that can withstand the force of nuclear power is pivotal to maintaining the integrity of nuclear reactors. However, performing manual inspections of materials is time-consuming, prone to error, inconsistent, and does not scale well. To solve the problem, researchers from the University of Wisconsin-Madison and the Oak Ridge National Laboratory in Tennessee developed…Powered by Discourse, best viewed with JavaScript enabled"
2001,10-minutes-to-data-science-transitioning-between-rapids-cudf-and-cupy-libraries,"Originally published at:			https://developer.nvidia.com/blog/10-minutes-to-data-science-transitioning-between-rapids-cudf-and-cupy-libraries/
This post was originally published on the RAPIDS AI Blog. RAPIDS is about creating bridges, connections, and clean handoffs between GPU PyData libraries. Interoperability with functionality is our goal. For example, if you’re working with RAPIDS cuDF but need a more linear-algebra oriented function that exists in CuPy, you can leverage the interoperability of the…Powered by Discourse, best viewed with JavaScript enabled"
2002,cuda-refresher-getting-started-with-cuda,"Originally published at:			CUDA Refresher: Getting started with CUDA | NVIDIA Technical Blog
This is the second post in the CUDA Refresher series, which has the goal of refreshing key concepts in CUDA, tools, and optimization for beginning or intermediate developers. Advancements in science and business drive an insatiable demand for more computing resources and acceleration of workloads. Parallel programming is a profound way for developers to accelerate…Powered by Discourse, best viewed with JavaScript enabled"
2003,atcom-real-time-enhancement-of-long-range-imagery,"Originally published at:			https://developer.nvidia.com/blog/atcom-real-time-enhancement-long-range-imagery/
Imaging over long distances is important for many defense and commercial applications. High-end ground-to-ground, air-to-ground, and ground-to-air systems now routinely image objects several kilometers to several dozen kilometers away; however, this increased range comes at a price. In many scenarios, the limiting factor becomes not the quality of your camera but the atmosphere through which…Impressive work. How does this approach compare to naive multi-frame registration? I.e. when stills from successive frames are aligned (warped) and blended, without the atmospheric correction kernel.Good question. We have actually spent a good amount of time looking at this approach as well. In some cases, particularly very light turbulence, its hard to notice much of a difference. Where the bispectrum-based approach seems to shine is when the turbulence becomes significant. That said, when processing in color and depending on the colorspace being used, we may process one or two of the channels as you suggest to save computations. We also leave the option in our software to process completely that way to increase speed when working with low turbulence data.And not to go too far on a tangent but you raise the more subtle point of what actually is better. How do you objectively compare two enhanced images and say which is better? People have written entire dissertations on this topic. We have even co-authored a paper with one of the leading experts in this space and I still don't think there is a categorical conclusion.Since our interest is pushing the limits of the technology, we have pursued the bispectrum approach. In our experience, there are scenarios where a more naive approach is sufficient but they are a small subset of cases a more robust method could address (at least for the kinds of data sets we're used to seeing).I wish I could give you a more quantitative answer but how to arrive at one is still a somewhat philosophical question at this point. (And based on the length of this answer you can see why I had to limit my post and leaving a lot of material on the cutting room floor.)testEven semi-naive registration is far off grom what multiframe blind deconvolution can archive. You can find a comparison (for use with astronomy) in the following dissertation: http://hdl.handle.net/10900...I btw. am currently working on porting it to CUDA with some changes to remove data dependent and convergence critical parameters. It though no longer is online, but still scale O(n log n) as of it's tree-like reduction. Powered by Discourse, best viewed with JavaScript enabled"
2004,cuda-spotlight-gpu-accelerated-earthquake-simulations,"Originally published at:			https://developer.nvidia.com/blog/cuda-spotlight-gpu-accelerated-earthquake-simulations/
This week’s Spotlight is on Yifeng Cui, director of the High Performance GeoComputing Lab (HPGeoC) at the San Diego Supercomputer Center (and adjunct professor in the Department of Geological Sciences at San Diego State University). HPGeoC was recently named a winner of the HPC Innovation Excellence Award by IDC for developing a highly scalable computer…Powered by Discourse, best viewed with JavaScript enabled"
2005,ai-generates-songs-to-resemble-kurt-cobain,"Originally published at:			https://developer.nvidia.com/blog/ai-generates-songs-to-resemble-kurt-cobain/
creAIted, a Russian startup, released an EP with four songs entirely written by a neural network that resemble the lyrics of Nirvana frontman Kurt Cobain.  The startup who is also a member of the NVIDIA Inception program is focused on projects to show the creative potential of AI, and the possibilities of AI and human…Powered by Discourse, best viewed with JavaScript enabled"
2006,massively-parallel-path-space-filtering-in-game-development,"Originally published at:			Massively Parallel Path Space Filtering in Game Development | NVIDIA Technical Blog
At SIGGRAPH 2019, NVIDIA explained how to harness massively parallel path space filtering in order to achieve high quality real-time ray traced global illumination in games.  Imagine seeing a point on the ceiling through a mirror. With offline rendering, you could collect the radiance over a hemisphere to determine how bright it is there. This…Powered by Discourse, best viewed with JavaScript enabled"
2007,accelerating-inference-with-sparsity-using-the-nvidia-ampere-architecture-and-nvidia-tensorrt,"Originally published at:			https://developer.nvidia.com/blog/accelerating-inference-with-sparsity-using-ampere-and-tensorrt/
○	TensorRT is an SDK for high-performance deep learning inference, and TensorRT 8.0 introduces support for sparsity that uses sparse tensor cores on NVIDIA Ampere GPUs. It can accelerate networks by reducing the computation of zeros present in GEMM operations in neural networks. You get a performance gain compared to dense networks by just following the steps in this post.Hi ,I test resnext101_32x8d_sparse_onnx_v1/resnext101_32x8d_dense_onnx_v1 model with the trtexec as the blog said.
The sparse result as:
[07/28/2021-08:45:44] [I] Throughput: 501.618 qps
[07/28/2021-08:45:44] [I] Latency: min = 1.81982 ms, max = 3.1843 ms, mean = 2.05464 ms, median = 2.02905 ms, percentile(99%) = 2.29761 msThe dense result as:
[07/28/2021-08:48:35] [I] Throughput: 498.367 qps
[07/28/2021-08:48:35] [I] Latency: min = 1.78882 ms, max = 2.14081 ms, mean = 2.06509 ms, median = 2.06612 ms, percentile(99%) = 2.10205 msI also use nsys to profile the kernel trace, but fail to see any different with the kernel used by sparse with the one used by dense model…I am doing the experiment over 3090 with nvcr.io/nvidia/pytorch:21.07-py3
docker image. Any idea?ThxHello @leiwen, thanks for your comment. There was a mistake in the code snippet. ngc registry model download-version nvidia/resnext101_32x8d_dense_onnx:1 command downloads the dense model and NOT the sparse model. You can change this to ngc registry model download-version nvidia/resnext101_32x8d_sparse_onnx:1 and then run the trtexec command with sparsity enabled for exporting the onnx model to trt engine. Please let me know if that works thanks!Hi @asawarkar ,I redownload the onnx file, and here are the md5sum of the two files:
c962aeafd8a7000f3c72bbfcd2165572  resnext101_32x8d_sparse_onnx_v1/resnext101_32x8d_pyt_torchvision_sparse.onnx
49beb2920f6f6e42eb20b874a30eab98  resnext101_32x8d_dense_onnx_v1/resnext101_32x8d_pyt_torchvision_dense.onnxBut still I cannot see any different for the performance improve for the sparse onnx model.When trt build the sparse one, it print below message:
[08/07/2021-09:14:02] [I] [TRT] (Sparsity) Layers eligible for sparse math: Conv_3 + Relu_4, Conv_7, Conv_8 + Add_9 + Relu_10, Conv_11 + Relu_12, Conv_15 + Add_16 + Relu_17, Conv_18 + Relu_19, Conv_22 + Add_23 + Relu_24, Conv_25 + Relu_26, Conv_29, Conv_30 + Add_31 + Relu_32, Conv_33 + Relu_34, Conv_37 + Add_38 + Relu_39, Conv_40 + Relu_41, Conv_44 + Add_45 + Relu_46, Conv_47 + Relu_48, Conv_51 + Add_52 + Relu_53, Conv_54 + Relu_55, Conv_58, Conv_59 + Add_60 + Relu_61, Conv_62 + Relu_63, Conv_66 + Add_67 + Relu_68, Conv_69 + Relu_70, Conv_73 + Add_74 + Relu_75, Conv_76 + Relu_77, Conv_80 + Add_81 + Relu_82, Conv_83 + Relu_84, Conv_87 + Add_88 + Relu_89, Conv_90 + Relu_91, Conv_94 + Add_95 + Relu_96, Conv_97 + Relu_98, Conv_101 + Add_102 + Relu_103, Conv_104 + Relu_105, Conv_108 + Add_109 + Relu_110, Conv_111 + Relu_112, Conv_115 + Add_116 + Relu_117, Conv_118 + Relu_119, Conv_122 + Add_123 + Relu_124, Conv_125 + Relu_126, Conv_129 + Add_130 + Relu_131, Conv_132 + Relu_133, Conv_136 + Add_137 + Relu_138, Conv_139 + Relu_140, Conv_143 + Add_144 + Relu_145, Conv_146 + Relu_147, Conv_150 + Add_151 + Relu_152, Conv_153 + Relu_154, Conv_157 + Add_158 + Relu_159, Conv_160 + Relu_161, Conv_164 + Add_165 + Relu_166, Conv_167 + Relu_168, Conv_171 + Add_172 + Relu_173, Conv_174 + Relu_175, Conv_178 + Add_179 + Relu_180, Conv_181 + Relu_182, Conv_185 + Add_186 + Relu_187, Conv_188 + Relu_189, Conv_192 + Add_193 + Relu_194, Conv_195 + Relu_196, Conv_199 + Add_200 + Relu_201, Conv_202 + Relu_203, Conv_206 + Add_207 + Relu_208, Conv_209 + Relu_210, Conv_213 + Add_214 + Relu_215, Conv_216 + Relu_217, Conv_220, Conv_221 + Add_222 + Relu_223, Conv_224 + Relu_225, Conv_228 + Add_229 + Relu_230, Conv_231 + Relu_232, Conv_235 + Add_236 + Relu_237
[08/07/2021-09:14:02] [I] [TRT] (Sparsity) TRT inference plan picked sparse implementation for layers: Conv_3 + Relu_4, Conv_7, Conv_8 + Add_9 + Relu_10, Conv_11 + Relu_12, Conv_15 + Add_16 + Relu_17, Conv_18 + Relu_19, Conv_22 + Add_23 + Relu_24, Conv_25 + Relu_26, Conv_29, Conv_30 + Add_31 + Relu_32, Conv_33 + Relu_34, Conv_37 + Add_38 + Relu_39, Conv_40 + Relu_41, Conv_44 + Add_45 + Relu_46, Conv_47 + Relu_48, Conv_51 + Add_52 + Relu_53, Conv_54 + Relu_55, Conv_58, Conv_59 + Add_60 + Relu_61, Conv_62 + Relu_63, Conv_66 + Add_67 + Relu_68, Conv_69 + Relu_70, Conv_73 + Add_74 + Relu_75, Conv_76 + Relu_77, Conv_80 + Add_81 + Relu_82, Conv_83 + Relu_84, Conv_87 + Add_88 + Relu_89, Conv_90 + Relu_91, Conv_94 + Add_95 + Relu_96, Conv_97 + Relu_98, Conv_101 + Add_102 + Relu_103, Conv_104 + Relu_105, Conv_108 + Add_109 + Relu_110, Conv_111 + Relu_112, Conv_115 + Add_116 + Relu_117, Conv_118 + Relu_119, Conv_122 + Add_123 + Relu_124, Conv_125 + Relu_126, Conv_129 + Add_130 + Relu_131, Conv_132 + Relu_133, Conv_136 + Add_137 + Relu_138, Conv_139 + Relu_140, Conv_143 + Add_144 + Relu_145, Conv_146 + Relu_147, Conv_150 + Add_151 + Relu_152, Conv_153 + Relu_154, Conv_157 + Add_158 + Relu_159, Conv_160 + Relu_161, Conv_164 + Add_165 + Relu_166, Conv_167 + Relu_168, Conv_171 + Add_172 + Relu_173, Conv_174 + Relu_175, Conv_178 + Add_179 + Relu_180, Conv_181 + Relu_182, Conv_185 + Add_186 + Relu_187, Conv_188 + Relu_189, Conv_192 + Add_193 + Relu_194, Conv_195 + Relu_196, Conv_199 + Add_200 + Relu_201, Conv_202 + Relu_203, Conv_206 + Add_207 + Relu_208, Conv_209 + Relu_210, Conv_213 + Add_214 + Relu_215, Conv_216 + Relu_217, Conv_220, Conv_224 + Relu_225, Conv_231 + Relu_232I assume after enabling structual sparsity, it would at last gain twice speed up with the non sparse kernel?
But from the nsys profile, no big improment is seen.
Could you help list the performance you assume that  resnext101_32x8d_pyt_torchvision_sparse.onnx could reach over 3090 platform with sparsity turn on or off? And would twice speed up assumption could be hold for this case?Thx,
LeiHi @leiwenThe assumption of double the performance gain due to structured sparsity is incorrect. We don’t have numbers for 3090 but on A100, the performance gain for ResNeXt101 32x8d should be in the range of 1% to 8% end to end in INT8. If FP16 is used, then sparse vs dense perf gap is larger.I think to compare the performance shall take single kernel as example. In previous experience, when switch from fp16 to int8, the same shape convolution would be accelerated upto twice of the origin speed.As the article also mention that, in ampere, dense int8 has 624Tops, while sparse has 1248 Tops, I think if the kernel is implemented corrected, its performance also shall be twice speed up?Thx@asawarkar I am planning to test sparsity gains for bert models and want to know that bert-large onnx also available in the ngc registry like below resnet model ?
ngc registry model download-version nvidia/resnext101_32x8d_dense_onnx:1Hi,As described in the slides, I used the following script to covert pretrained resnet50 model to prune the weights but it’s been more than 24 hours and still pruning process hasn’t completed. Could someone help on this ?Script used:
import torch
import torchvision
import torch.optim as optim
from torchvision import datasets, transforms, models
try:
from apex.contrib.sparsity import ASP
except ImportError:
raise RuntimeError(“Failed to import ASP. Please install Apex from https:// GitHub - NVIDIA/apex: A PyTorch Extension: Tools for easy mixed precision and distributed training in Pytorch .”)
device = torch.device(‘cuda’)
print(‘Is cuda available: ’ + str(torch.cuda.is_available()))
model = models.resnet50(pretrained=True) # Define model structure
model.load_state_dict(torch.load(’/home/ubuntu/.cache/torch/hub/checkpoints/resnet50-0676ba61.pth’))
optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.9) # Define optimizer
ASP.prune_trained_model(model, optimizer)
torch.save(model.state_dict(), ‘pruned_resnetmodel.pth’) # checkpoint has weights and masksI have also attached the log file.
resnet50_asp_pruning_log.txt (386.3 KB)Hi ,  I have run the resnext101_32x8d dense and sparse models for inference for  fp16 as described in the following paper and I don’t see any improvement in the inference for sparse model. Could some one look in to it ?Dense model:
trt generation: tuser@fde6b05b597a:/workspace/TensorRT/build/out/trtexec --onnx=resnext101_32x8d_pyt_torchvision_dense.onnx --saveEngine=resnext101_dense_engine_pytorch.trt --explicitBatch --inputIOFormats=fp16:chw --outputIOFormats=fp16:chw  --fp16
Batch size: 32000
Precision: fp16
Processing time for 1 loop:4.3sSparse model:
trt generation: trtuser@fde6b05b597a:/workspace/TensorRT/build/out/trtexec --onnx=resnext101_32x8d_pyt_torchvision_sparse.onnx --saveEngine=resnext101_sparse_engine_pytorch.trt --explicitBatch --inputIOFormats=fp16:chw --outputIOFormats=fp16:chw --fp16 --sparsity=enable
Batch size: 32000
Precision: fp16
Processing time for 1 loop:4.2sHi, I’m using TensorRT to test with the ResNeXt101. But the problem is as follows:[07/19/2022-09:37:20] [I] [TRT] (Sparsity) Layers eligible for sparse math: Conv_3 + Relu_4, Conv_7, Conv_8 + Add_9 + Relu_10, Conv_11 + Relu_12, Conv_15 + Add_16 + Relu_17, Conv_18 + Relu_19, Conv_22 + Add_23 + Relu_24, Conv_25 + Relu_26,
Conv_29, Conv_30 + Add_31 + Relu_32, Conv_33 + Relu_34, Conv_37 + Add_38 + Relu_39, Conv_40 + Relu_41, Conv_44 + Add_45 + Relu_46, Conv_47 + Relu_48, Conv_51 + Add_52 + Relu_53, Conv_54 + Relu_55, Conv_58, Conv_59 + Add_60 + Relu_61, Conv
_62 + Relu_63, Conv_66 + Add_67 + Relu_68, Conv_69 + Relu_70, Conv_73 + Add_74 + Relu_75, Conv_76 + Relu_77, Conv_80 + Add_81 + Relu_82, Conv_83 + Relu_84, Conv_87 + Add_88 + Relu_89, Conv_90 + Relu_91, Conv_94 + Add_95 + Relu_96, Conv_97 + Relu_98, Conv_101 + Add_102 + Relu_103, Conv_104 + Relu_105, Conv_108 + Add_109 + Relu_110, Conv_111 + Relu_112, Conv_115 + Add_116 + Relu_117, Conv_118 + Relu_119, Conv_122 + Add_123 + Relu_124, Conv_125 + Relu_126, Conv_129 + Add_130 +
Relu_131, Conv_132 + Relu_133, Conv_136 + Add_137 + Relu_138, Conv_139 + Relu_140, Conv_143 + Add_144 + Relu_145, Conv_146 + Relu_147, Conv_150 + Add_151 + Relu_152, Conv_153 + Relu_154, Conv_157 + Add_158 + Relu_159, Conv_160 + Relu_161,
Conv_164 + Add_165 + Relu_166, Conv_167 + Relu_168, Conv_171 + Add_172 + Relu_173, Conv_174 + Relu_175, Conv_178 + Add_179 + Relu_180, Conv_181 + Relu_182, Conv_185 + Add_186 + Relu_187, Conv_188 + Relu_189, Conv_192 + Add_193 + Relu_194,
Conv_195 + Relu_196, Conv_199 + Add_200 + Relu_201, Conv_202 + Relu_203, Conv_206 + Add_207 + Relu_208, Conv_209 + Relu_210, Conv_213 + Add_214 + Relu_215, Conv_216 + Relu_217, Conv_220, Conv_221 + Add_222 + Relu_223, Conv_224 + Relu_225,
Conv_228 + Add_229 + Relu_230, Conv_231 + Relu_232, Conv_235 + Add_236 + Relu_237, MatMul_240[07/19/2022-09:37:20] [I] [TRT] (Sparsity) TRT inference plan picked sparse implementation for layers:
[07/19/2022-09:37:20] [I] [TRT] [MemUsageChange] Init cuBLAS/cuBLASLt: CPU +0, GPU +8, now: CPU 3117, GPU 1787 (MiB)
[07/19/2022-09:37:20] [I] [TRT] [MemUsageChange] Init cuDNN: CPU +0, GPU +10, now: CPU 3117, GPU 1797 (MiB)
[07/19/2022-09:37:20] [I] [TRT] [MemUsageChange] TensorRT-managed allocation in building engine: CPU +8, GPU +339, now: CPU 8, GPU 339 (MiB)
[07/19/2022-09:37:20] [W] [TRT] The getMaxBatchSize() function should not be used with an engine built from a network created with NetworkDefinitionCreationFlag::kEXPLICIT_BATCH flag. This function will a
lways return 1.
[07/19/2022-09:37:20] [W] [TRT] The getMaxBatchSize() function should not be used with an engine built from a network created with NetworkDefinitionCreationFlag::kEXPLICIT_BATCH flag. This function will a
lways return 1.There are all the layers that are eligible for sparse math but none in TRT inference plan picked sparse implementation for layers. Could you help me check why this could happen?I’ve solved this problem by adding enable FP16 flag.
However, I’ve noticed that the sparsity module only support convolution operation but barely linear layers(fully connected), is that the case or we need other flag to enable linear layer acceleration?I tested on A100, latest TensorRT and I got:
sparse model qps 885
dense mode qps 870It seems that the advantage of using sparsity is very little.Hi,
I tested the inference speed but I did not find how to test GPU power consumption.(RTX-A6000)
Could you tell me how to check the power consumption?ThxI am trying to prune the model to 2:4 sparsity and convert it to tensorRT and run on orin nano Ampere architecture. But Iam stuck at the ONNX conversion step.
I was able to prune the model using the ‘ASP.prune_trained_model(model, optimizer)’ command. But the model I got as the output was a mix of pruned weights and masks. Then when I tried to convert it to onnx its not working as the model. eval() is not working. can you please provide the instructions to convert it to onnx?These are the things I tried.
#. Prune the model
print(“Pruning the model”)
ASP.prune_trained_model(model, optimizer)
for epoch in range(1, args.epochs+1):
print(“\n---- Training Model ----”)torch.save(model.state_dict(), “yolov3_pruned.pth”)#ONNX conversionimport torchpruned_model = torch.load(‘yolov3_pruned.pth’)
dummy_input=torch.randn(1, 3, 224, 224)
torch.onnx.export(pruned_model , dummy_input, “yolov3.onnx”, verbose=False)But when converting to ONNX this is the error Iam getting this error.
File “/home/ashish/code/pruning/PyTorch-YOLOv3/test.py”, line 5, in 
torch.onnx.export(pruned_model , dummy_input, “yolov3.onnx”, verbose=False)
File “/home/ashish/venvs/py310/lib/python3.10/site-packages/torch/onnx/utils.py”, line 506, in export
_export(
File “/home/ashish/venvs/py310/lib/python3.10/site-packages/torch/onnx/utils.py”, line 1525, in _export
with exporter_context(model, training, verbose):
File “/usr/lib/python3.10/contextlib.py”, line 135, in enter
return next(self.gen)
File “/home/ashish/venvs/py310/lib/python3.10/site-packages/torch/onnx/utils.py”, line 178, in exporter_context
with select_model_mode_for_export(
File “/usr/lib/python3.10/contextlib.py”, line 135, in enter
return next(self.gen)
File “/home/ashish/venvs/py310/lib/python3.10/site-packages/torch/onnx/utils.py”, line 139, in disable_apex_o2_state_dict_hook
for module in model.modules():
AttributeError: ‘collections.OrderedDict’ object has no attribute ‘modules’Powered by Discourse, best viewed with JavaScript enabled"
2008,sc20-demo-revolutionizing-supercomputing-with-nvidia-mellanox-ufm-cyber-ai,"Originally published at:			SC20 Demo: Revolutionizing Supercomputing with NVIDIA Mellanox UFM Cyber AI | NVIDIA Technical Blog
NVIDIA Mellanox UFM (Unified Fabric Manager) Cyber AI – an advanced InfiniBand system powered by deep learning that can proactively predict failures in data centers. By combining rich telemetry information from network adapters, switches, cables and other components with deep learning algorithms — in real-time — Cyber AI can empower IT managers to take action…Powered by Discourse, best viewed with JavaScript enabled"
2009,ai-study-predicts-alzheimers-six-years-before-diagnosis,"Originally published at:			AI Study Predicts Alzheimer’s Six Years Before Diagnosis | NVIDIA Technical Blog
A new study published in Radiology describes how deep learning can improve the ability of brain imaging to predict Alzheimer’s disease years before an actual diagnosis. The research has the potential to help millions of people and could be a useful tool for radiologists. “If we diagnose Alzheimer’s disease when all the symptoms have manifested,…Powered by Discourse, best viewed with JavaScript enabled"
2010,accelerating-openvdb-on-gpus-with-nanovdb,"Originally published at:			https://developer.nvidia.com/blog/accelerating-openvdb-on-gpus-with-nanovdb/
OpenVDB is the Academy award–winning, industry standard library for sparse dynamic volumes. It is used throughout the visual effects industry for simulation and rendering of water, fire, smoke, clouds, and a host of other effects that rely on sparse volume data. The library includes a hierarchical, dynamic data structure and a suite of tools for…How does it compare to a solution like GVDB ? Will NanoVDB replace GVDB ?AndréNanoVDB primarily targets non-dynamic workflows like simulation-sourcing, rendering and collision-detection, as the VDB structure is streamed into a pointerless buffer.However, GVDB’s dynamic functionality, such as points-to-grid and voxelization, will be handled by an, as yet unannounced, project which will be compatible with NanoVDB workflows. Stay tuned.Will GVDB no longer be supported moving forward? GVDB had a few useful features that I’m curious if NanoVDB or this unannounced project will support, such as explicit raytracing (3DDA raytracing from any point to another point, custom kernels to pass into ray traces, etc). Are there plans to support these in the future?NanoVDB supports raytracing with custom kernels. Because it is a C++ library, you can simply construct a Ray and write a simple algorithm to trace and sample the volume however you choose.For levelsets, helper functions already exist, e.g. nanovdb::ZeroCrossing(…)There are some minimal examples doing both of these techniques in the library.Any forthcoming project will also support this kind of functionality, and it will be a superset of GVDB features. Thanks for your interest.When can we expect the unannounced project will be available?is NeuralVDB the new solution ?Powered by Discourse, best viewed with JavaScript enabled"
2011,gtc-2020-visual-anomaly-detection-using-nvidia-deepstream-iot,"GTC 2020 S22675
Presenters: Emmanuel Bertrand,Microsoft; Ian Davis,
Abstract
In this workshop, you’ll discover how to build a solution that can process up to 8 real-time video streams with an AI model on a $100 device, how to remotely operate your device, and demonstrate how you can deploy custom AI models to it.With this solution, you can transform pixels from a camera into insights to know when there is an available parking spot, a missing product on a retail store shelf, an anomaly on a solar panel, a worker approaching a hazardous zone., etc.We’ll build this solution using NVIDIA DeepStream on a NVIDIA Jetson Nano device connected to Azure via Azure IoT Edge. DeepStream is a highly-optimized video processing pipeline, capable of running deep neural networks. It is a must-have tool whenever you have complex video analytics requirements like real-time object detection or when employing cascading AI models. IoT Edge gives you the possibility to run this pipeline next to your cameras, where the video data is being generated, thus lowering your bandwidth costs and enabling scenarios with poor internet connectivity or privacy concerns.We’ll operate this solution with an aesthetic UI provided by IoT Central and customize the objects detected in video streams using Custom Vision, a service that automatically generates computer vision AI models from pictures.Watch this session
Join in the conversation below.Powered by Discourse, best viewed with JavaScript enabled"
2012,nvidia-gpus-to-accelerate-microsoft-azure-cloud,"Originally published at:			https://developer.nvidia.com/blog/nvidia-gpus-to-accelerate-microsoft-azure-cloud/
Tesla GPUs will enable Microsoft Azure to deliver accelerated computing capabilities to customers worldwide through its cloud platform, Microsoft Azure. The Tesla Accelerated Computing Platform is designed from the ground up for power-efficient, HPC, computational science, supercomputing, data analytics and deep learning applications. Powering some of the world’s highest performance supercomputers, the Tesla platform delivers…Powered by Discourse, best viewed with JavaScript enabled"
2013,pro-tip-linking-opengl-for-server-side-rendering,"Originally published at:			Pro Tip: Linking OpenGL for Server-Side Rendering | NVIDIA Technical Blog
Visualization is a great tool for understanding large amounts of data, but transferring the data from an HPC system or from the cloud to a local workstation for analysis can be a painful experience. It’s increasingly popular to avoid  the transfer by analyzing and visualizing data in situ: right where it is generated. Moreover, using server-side rendering lets…It sound good, but is there any examples, e.g. Draw a triangle?Have a look at this post for an example of rendering a triangle with EGL.Powered by Discourse, best viewed with JavaScript enabled"
2014,in-the-trenches-at-gtc-languages-apis-and-development-tools-for-gpu-computing,"Originally published at:			https://developer.nvidia.com/blog/trenches-gtc-languages-apis-and-development-tools-gpu-computing/
By Michael Wang, The University Of Melbourne, Australia (GTC ’12 Guest Blogger) It’s 9 am, the first morning session of the pre-conference Tutorial Day. The atmosphere in the room is one of quiet anticipation. NVIDIA’s Will Ramey takes the stage and says: “this is going to be a great week.” I couldn’t agree more. A…Powered by Discourse, best viewed with JavaScript enabled"
2015,deep-learning-in-robotic-automation-and-warehouse-logistics,"Originally published at:			https://developer.nvidia.com/blog/deep-learning-in-robotic-automation-and-warehouse-logistics/
Read about how deep learning models are used for an automated pick and place system, a feature more and more advanced warehouses are implementing.Powered by Discourse, best viewed with JavaScript enabled"
2016,ai-mind-reading-technology-can-decode-your-brain,"Originally published at:			AI Mind-Reading Technology Can Decode Your Brain | NVIDIA Technical Blog
Researchers from Purdue University developed a model that can decode what the human brain is seeing by using deep learning to interpret fMRI scans from people watching videos, representing a sort of mind-reading technology. “That type of network (convolutional neural network) has made an enormous impact in the field of computer vision in recent years,”…Powered by Discourse, best viewed with JavaScript enabled"
2017,lazy-loading,"This is a question sent directly to me, so repeating as a post:What is lazy loading and what benefits should I expect from it.Lazy loading is a catch-all term we use to describe a few different techniques for lowering memory consumption by CUDA applications. Ultimately, they all boil down to not loading functions either into host memory or into the GPU’s memory until the first time the application needs to call them.If you think about some of our larger libraries, like cuDNN or cuBLAS, there are tens of thousands of kernels you could run although a typical application calls maybe 5% of them. By not loading the other 95%, you can see substantial savings in both the time it takes to load the application (less data transfer to the GPU) and lower memory utilization (functions that aren’t called aren’t loaded). In some applications this can be very substantial.It’s worth noting that because we don’t load functions until you call them, it does change the latency of the functions at the first invocation. That’s usually un-noticeable for most applications and the net effect will be a performance increase, but if you have an application that’s particularly latency sensitive you can switch it off wiht the CUDA_MODULE_LOADING environment variable (valid settings are EAGER to turn it off, and LAZY which will turn it on).Lazy loading is enabled by default in CUDA 12.2 for Linux and 12.3 for Windows.Powered by Discourse, best viewed with JavaScript enabled"
2018,gtc-ai-deep-learning-presentations,"Originally published at:			GTC: AI / Deep Learning Presentations | NVIDIA Technical Blog
Starting on October 5, this fall’s GPU Technology Conference (GTC) will run continuously for five days, across seven time zones. The conference will showcase the latest breakthroughs in AI, as well as many other GPU technology interest areas. Here’s a preview of some of the AI sessions at GTC.  Recommendation Systems A Deep Dive into…Powered by Discourse, best viewed with JavaScript enabled"
2019,just-released-lightning-fast-simulations-with-pennylane-and-the-nvidia-cuquantum-sdk,"Originally published at:			Lightning-fast simulations with PennyLane and the NVIDIA cuQuantum SDK
Learn how the PennyLane lightning.gpu device uses the NVIDIA cuQuantum software development kit to speed up the simulation of quantum circuits.Powered by Discourse, best viewed with JavaScript enabled"
2020,nvidia-webinars-hello-ai-world-and-learn-with-jetbot,"Originally published at:			NVIDIA Webinars: Hello AI World and Learn with JetBot | NVIDIA Technical Blog
We recently announced two exciting upcoming webinars about the new Jetson Nano. Each presentation will be followed by a live Q&A session where you can ask questions in real-time with the NVIDIA Jetson team. We look forward to you joining us! Hello AI World – Meet Jetson NanoDate: Thursday, May 2, 2019 Time: 10:00 –…Powered by Discourse, best viewed with JavaScript enabled"
2021,nvidia-inception-partners-won-veterans-affairs-ai-tech-sprint-awards-with-latest-ai-technologies,"Originally published at:			https://developer.nvidia.com/blog/nvidia-inception-partners-won-veterans-affairs-ai-tech-sprint-awards-with-latest-ai-technologies/
Hosted by the Department of Veterans Affairs (VA), the sprint is designed to foster collaboration with industry and academic partners on AI-enabled tools that leverage federal data to address a need for veterans.Powered by Discourse, best viewed with JavaScript enabled"
2022,share-your-science-combating-cancer-with-deep-learning,"Originally published at:			Share Your Science: Combating Cancer with Deep Learning | NVIDIA Technical Blog
Le Lu, staff scientist at the National Institutes of Health (NIH) shares how they are applying artificial intelligence techniques to assist cancer clinicians make better diagnostic decisions.  Using NVIDIA Tesla GPUs and the cuDNN-accelerated Caffe deep learning framework, Lu and his team trained their model on nearly one million patient cases which helped them develop…Powered by Discourse, best viewed with JavaScript enabled"
2023,webinar-build-your-next-deep-learning-application-for-nvidia-jetson-in-matlab,"Originally published at:			Webinar: Build Your Next Deep Learning Application for NVIDIA Jetson in MATLAB | NVIDIA Technical Blog
Learn how you can use MATLAB to build your computer vision and deep learning applications and deploy them on NVIDIA Jetson. MATLAB auto-generates portable CUDA code that leverages CUDA libraries like cuBLAS and cuDNN from the MATLAB algorithm, which is then cross-compiled and deployed to Jetson. The generated code is highly optimized and benchmarks will…Hi @jwitsoe, any idea how I could access the webinar ? Tried registering on this link : https://info.nvidia.com/build-your-next-nvidia-jetson-deep-learning-application-in-matlab-reg-page.html, but the webinar page won’t load.
Thanks for your help.@AlexandreJ  – It looks like the webinar is no longer available, sorry! Maybe this video might be helpful instead?Deep Learning with MATLAB, NVIDIA Jetson, and ROS VideoGood find ! I’ll have a look :) Thank you very much for your helpPowered by Discourse, best viewed with JavaScript enabled"
2024,nvidia-research-fast-uncertainty-quantification-for-deep-object-pose-estimation,"Originally published at:			https://developer.nvidia.com/blog/nvidia-research-fast-uncertainty-quantification-for-deep-object-pose-estimation/
Researchers from NVIDIA, University of Texas at Austin and Caltech developed a simple, efficient, and plug-and-play uncertainty quantification method for the 6-DoF object pose estimation task, using an ensemble of K pre-trained estimators with different architectures and/or training data sources.Is it possible to use this on Jetson Platform ?
Could you show me its procedure for Jetson if possible ?Our pose estimator models are based on DOPE (GitHub - NVlabs/Deep_Object_Pose: Deep Object Pose Estimation (DOPE) – ROS inference (CoRL 2018)). We have tested it on Ubuntu 20.04 with ROS Noetic with an NVIDIA Titan X and RTX 2080ti with Python 3.8. We expect this to work on the Jetson platform as well. You can check out the Issues on DOPE for pointers (e.g., Build Deep Object Pose on AGX · Issue #105 · NVlabs/Deep_Object_Pose · GitHub).Powered by Discourse, best viewed with JavaScript enabled"
2025,creating-medical-imaging-models-with-nvidia-clara-train-4-0,"Originally published at:			Creating Medical Imaging Models with NVIDIA Clara Train 4.0 | NVIDIA Technical Blog
In the field of medicine, advancements in artificial intelligence are constantly evolving. To keep up with the pace of innovation means adapting and providing the best experience to researchers, clinicians, and data scientists. NVIDIA Clara Train, an application framework for training medical imaging models, has undergone significant changes for its upcoming release at the beginning…We’re really excited about Clara Train 4.0 coming out soon!  If you’re looking for access today, sign up for our Early Acces program for this upcoming release and check back for our Early Access program for future releases. NVIDIA Clara Imaging | NVIDIA DeveloperWe are also continually updating and providing new Jupyter Notebooks and always looking for feedback or suggestions for new topics to cover.  GitHub - NVIDIA/clara-train-examples: Example notebooks demonstrating how to use Clara Train to build Medical Imaging Deep Learning models.  Feel free to reach out if you have any questions or comments!Powered by Discourse, best viewed with JavaScript enabled"
2026,edge-computing-fuels-a-sustainable-future-for-energy,"Originally published at:			https://developer.nvidia.com/blog/edge-computing-fuels-a-sustainable-future-for-energy/
Learn how edge computing is powering efficient energy operations, protecting worker health and safety, and improving power grid resiliency.Powered by Discourse, best viewed with JavaScript enabled"
2027,making-apache-spark-more-concurrent,"Originally published at:			https://developer.nvidia.com/blog/making-apache-spark-more-concurrent/
Apache Spark provides capabilities to program entire clusters with implicit data parallelism. With Spark 3.0 and the open source RAPIDS Accelerator for Spark, these capabilities are extended to GPUs. However, prior to this work, all CUDA operations happen in the default stream, causing implicit synchronization and not taking advantage of concurrency on the GPU. In…This was an interesting project going up and down the stack to get everything working in a multi-threaded environment on the host side. It was particularly challenging to get the memory pool to work efficiently with per-thread default stream without causing too much fragmentation. Inspirations from jemalloc, Hoard, tcmalloc, and the classic Paul R. Wilson paper from 1995.If you have any questions or comments, let us know.Powered by Discourse, best viewed with JavaScript enabled"
2028,the-power-of-c-11-in-cuda-7,"Originally published at:			https://developer.nvidia.com/blog/power-cpp11-cuda-7/
Today I’m excited to announce the official release of CUDA 7, the latest release of the popular CUDA Toolkit. Download the CUDA Toolkit version 7 now from CUDA Zone! CUDA 7 has a huge number of improvements and new features, including C++11 support, the new cuSOLVER library, and support for Runtime Compilation. In a previous…This looks really great! However, I am using fedora so I cannot try it out myself yet :-( Do you know when the fedora edition will be available?Hi Kenneth. The .run file installer *might* work with Fedora 20, though we haven't tested it.  It's worth a try. We're working on getting the Fedora 21 installer available soon (only 21 will be *officially* supported with CUDA 7 -- please see the release notes).Hi Mark, thanks, nice and clear explanations. I have found a typo in the first code snippet, when calling count_if, there should be text instead of data.Good catch; I fixed it. Thanks!Hi Mark, which parts of the STL of C++11 can be used on the device with cuda 7? NB: I was thinking in defining classes that make use of std::vector, etc ... that would incorporate unified memory managed class, as in a previous posting of you, and then having __device__ __host__ functions using those classes. I have tried but it seems not to work. Does one need to use something like Thrust?, or am I mistaken? Thank you.CUDA 7 adds support for C++11 language features in device code, but not the standard template library, I'm afraid. You can use a thrust::device_vector.  You could indeed write your own vector class that uses managed memory, but existing STL headers won't ""just work"" because of the need to annotate all functions called on the device with ""__host__ __device__""That makes sense. Thanks for the clarification. I had started to do so, about the '__host__ __device__' with a ifdef/ifndef, but, clearly, I encountered problems using the STL methods. In order to use the implicit methods of STL, the closest and best thing to do seems to use Thrust. I had not looked at it before and it is clearly very good, as well, and probably close to what an adapted C++ for the device memory would be.Powered by Discourse, best viewed with JavaScript enabled"
2029,cuda-toolkit-11-1-introduces-support-for-geforce-rtx-30-series-and-quadro-rtx-series-gpus,"Originally published at:			CUDA Toolkit 11.1 Introduces Support for GeForce RTX 30 Series and Quadro RTX Series GPUs | NVIDIA Technical Blog
CUDA 11 announced support for the new NVIDIA A100 based on the NVIDIA Ampere architecture. Today CUDA 11.1 introduces support for NVIDIA GeForce RTX 30 Series and Quadro RTX Series GPU platforms.  CUDA is the most powerful software development platform for building GPU-accelerated applications, providing all the components needed to develop applications targeting every GPU…Powered by Discourse, best viewed with JavaScript enabled"
2030,nvidia-nsight-eclipse-edition-for-jetson-tk1,"Originally published at:			https://developer.nvidia.com/blog/nvidia-nsight-eclipse-edition-for-jetson-tk1/
NVIDIA® Nsight™ Eclipse Edition is a full-featured, integrated development environment that lets you easily develop CUDA® applications for either your local (x86) system or a remote (x86 or ARM) target. In this post, I will walk you through the process of remote-developing CUDA applications for the NVIDIA Jetson TK1, an ARM-based development kit. Nsight supports…Thanks for this post, Satish! I'm still waiting to get my Jetson TK1. When I get it, I intend on developing on it from my Mac (OS X Mavericks). Do you know if that setup is supported, or should I use a Ubuntu partition instead?Is there any way to compile the program inside the Jetson itself?! I couldn't find any instruction for that?Mr Alexander...i am looking for a solution to a challenge..i want to use the jetson board to encode an analog video using h.264 and then output it at ethernet port of the board...i couldnt find any analog video input port...is there any expansion slot..?Hi, I followed the same steps but I keep on running into Xlib : extension ""GLX"" missing on display "":0"" error... Can anybody guide me.Josh good to know that you have a board on the way. Please use Ubuntu 12.04 LTS on the host for cross development. MAC OSX is also a supported host platform but “synchronize-projects” remote development mode is the way to go on MAC, I’ll add more details on MAC in a future post.You would usually see this error if you don't have a active desktop running on Jetson TK1. Do you have a panel connected to Jetson TK1?Thanks Alexander.Do you want to capture an analog video and then use Jetson to encode it? Have you looked at Analog Video USB solutions? There is a LinuxTV project which supports many different USB video capture devices: http://www.linuxtv.org/wiki...With kernel and driver requirements listed @ http://www.linuxtv.org/wiki...Please note none of these are validated on Jetson Tk1.All my earlier replies on these questions/comments were made from the blog portal and were thus lost. So if you seeing late replies you know why:-) I am now using disqus for the replies.Thank you. That helped.Hello, I followed the above instructions and everything worked very good. Thank you for your great tutorial. I have one problem though. I am using Nsight in order to debug and profile my code and when I time the output lets say that I get x seconds. If I take the exact same code and compile it on the board, I am getting y seconds, with y secs being smaller than x. So the algorithms run faster if I compile them on the board and without using cross-compilation. Does anybody have any idea for that?Thank you.The generated GPU(SASS) code will be the same whether cross compiled or natively compiled. Please check the GPU code generation options (I mentioned above in the blog) is the same in both the cross compile scenario and the native compile case, they both need to be SM32 for code and SM30 for PTX. Also check if you are using any debug options -G, make sure any such flags are same across both the compile paths.Can I upgrade graphics driver in Jetson TK1 platform?- I am having NVIDIA Jetson TK1 kit and I am having Linux ubuntu inside. Now I need to try latest ES3.1 extension like tesselation shader or draw indirect but I am getting linker error as those functions are not available in the library.- I am assuming NVIDIA is working on new ES3.1 extension with google. So, I believe there must be new version of drivers for that toolkit.Thanks.No you should never update just the driver on JetsonTK1 since the driver is part of the L4T OS image. ES3.1 is supported in the upcoming Rel21 to be announced soon.""sudo apt-get update"" has problem something like this. Is there any suggestions for this?Err http://archive.ubuntu.com precise-security/universe armhf Packages  404 Not Found [IP: 2001:67c:1360:8c01::19 80]Graham, this is a known issue after adding ""foreign-architecturearmhf"" to multiarch file and we're working to fix it.However, this shouldn't have any effect on your system (it's a harmless error).  Are you seeing other problems?So far it is good! Thank you Mark!!!Ok, Thanks Satish.Hi,The .deb file for the cross compilers installs the 6.5 version of CUDA's cross compilers. On the otherhand, Jetson TK1 is at CUDA 6.0. This causes a version mismatch between the gdb server on TK1 and the gdb client on the host. Is there a way to resolve this? I am running ubuntu 12.04.ArchithPowered by Discourse, best viewed with JavaScript enabled"
2031,bringing-cloud-native-agility-to-edge-ai-devices-with-the-nvidia-jetson-xavier-nx-developer-kit,"Originally published at:			Bringing Cloud-Native Agility to Edge AI Devices with the NVIDIA Jetson Xavier NX Developer Kit | NVIDIA Technical Blog
Figure 1. NVIDIA Jetson Xavier NX Developer Kit and production compute module. Today, NVIDIA announced the NVIDIA Jetson Xavier NX Developer Kit , which is based on the Jetson Xavier NX module. Delivering up to 21 TOPS of compute in a compact form factor with under 15W of power, Jetson Xavier NX brings server-level performance…Powered by Discourse, best viewed with JavaScript enabled"
2032,latest-updates-to-nvidia-cuda-x-libraries,"Originally published at:			Latest Updates to NVIDIA CUDA-X Libraries | NVIDIA Technical Blog
Learn what’s new in the latest releases of NVIDIA’s CUDA-X Libraries and NGC. Neural Modules NVIDIA Neural Modules is a new open-source toolkit for researchers to build state-of-the-art neural networks for AI accelerated speech applications. Early release of the toolkit includes: Base modules for automatic speech recognition and natural language processingGPU acceleration with mixed precision…Powered by Discourse, best viewed with JavaScript enabled"
2033,nvidia-data-scientists-take-top-spots-in-miccai-2021-brain-tumor-segmentation-challenge,"Originally published at:			https://developer.nvidia.com/blog/nvidia-data-scientists-take-top-spots-in-miccai-2021-brain-tumor-segmentation-challenge/
NVIDIA data scientists this week took three of the top 10 spots in a brain tumor segmentation challenge validation phase at the prestigious MICCAI 2021 medical imaging conference.Powered by Discourse, best viewed with JavaScript enabled"
2034,automatically-segmenting-brain-tumors-with-ai,"Originally published at:			Automatically Segmenting Brain Tumors with AI | NVIDIA Technical Blog
Each year tens of thousands of people in the United States are diagnosed with a brain tumor. To help physicians more effectively analyze, treat, and monitor tumors, NVIDIA researchers have developed a robust deep learning-based technique that uses 3D magnetic resonance images to automatically segment tumors. Segmentation provides tumor boundary definition of the affected region.…Powered by Discourse, best viewed with JavaScript enabled"
2035,gtc-2020-mlperf-a-benchmark-suite-for-machine-learning-from-an-academic-industry-cooperative,"GTC 2020 S22099
Presenters: David Kanter,Independent
Abstract
The ML field requires systematic benchmarking that represents real-world uses and enables fair comparisons across different software and hardware platforms. MLPerf is a machine-learning benchmark standard, and suite, driven by the industry (50+ companies) and researchers (800+) at large. The suite comprises a set of key machine-learning training and inference workloads representing important production-use cases, ranging from image classification and object detection to recommendation. MLPerf has released two rounds of training results and a new inference benchmark suite.Watch this session
Join in the conversation below.Powered by Discourse, best viewed with JavaScript enabled"
2036,implementing-a-real-time-ai-based-face-mask-detector-application-for-covid-19,"Originally published at:			Implementing a Real-time, AI-Based, Face Mask Detector Application for COVID-19 | NVIDIA Technical Blog
  Businesses are constantly overhauling their existing infrastructure and processes to be more efficient, safe, and usable for employees, customers, and the community. With the ongoing pandemic, it’s even more important to have advanced analytics apps and services in place to mitigate risk. For public safety and health, authorities are recommending the use of face…Powered by Discourse, best viewed with JavaScript enabled"
2037,introduction-to-neural-machine-translation-with-gpus-part-3,"Originally published at:			https://developer.nvidia.com/blog/introduction-neural-machine-translation-gpus-part-3/
Note: This is the final part of a detailed three-part series on machine translation with neural networks by Kyunghyun Cho. You may enjoy part 1 and part 2. In the previous post in this series, I introduced a simple encoder-decoder model for machine translation. This simple encoder-decoder model is excellent at English-French translation. However, in this…Thank you! Very good post!!""Furthermore, we can consider this set of context-dependent word representations as a mechanism by which we store the source sentence as a variable-length representation, as opposed to the fixed-length, fixed-dimensional summary from the simple encoder-decoder model.""1. Does that mean the representation length of the source sentence is determined by the number of annotation vector h and the length of h?2. In addition, I think  expected vector c_i can be seen as the final representations of the source sentence, whose length is fixed (the dimension of h). Is that right?Hi Minglei,1. Ues and no, I meant that the length of the representation is proportional to the length of the source sentence, which is proportional to the number of annotation vectors.2. Only at time i. The representation c_i changes w.r.t. i according to the attention weights which are determined by the decoder's hidden state (which again changes for each and every target word generated.)Cheers- KHey, thanks for the great post! NMT is one of the coolest recent applications of NNets out there.I have two questions, however: - Can Your code be used to train an actual working NMT model, or it's just a toy example for education purposes?- In general, can a good NMT model be trained on a GTX (980) with 4Gibs of RAM, or these models need so many parameters they require a cluster of Titan Xs?Edit:I've got one more question. When we use decoder to produce translation, it is obvious that we feed the output back to the input (like in character-level RNN language model). During training however, what is the input to the decoder? Also it's own output, or rather the groundtruth translation? When training the char-RNN we input the true letters, not the sampled ones.Hi Marcin,- ""Can Your code be used to train an actual working NMT model, or it's just a toy example for education purposes?""Yes, it can. It misses some post-processing routine for replacing unknown tokens and very large target vocabulary extension at the moment, but even without them, you can get a decent NMT model with the very code in those repos.- ""In general, can a good NMT model be trained on a GTX (980) with 4Gibs of RAM, or these models need so many parameters they require a cluster of Titan Xs?""Yes, you can, but each model needs to be quite small and won't perform well. Instead, you can train multiple small models (each on GTX980) and make an ensemble of them.- ""During training however, what is the input to the decoder?""It's the ground truth you feed in, because that is how it's supposed to be if the log-likelihood is maximized (check out the first post.) However, this does not mean that this is the only or best way. See, for instance, http://arxiv.org/abs/1506.0...Cheers,- KThank You so much for the reply and the link.It's too bad my new high-end GPU is still not good enough for deep learning, but I hope it'll suffice for educational purposes.Thanks again, Regards, MarcinVery great post! I have two questions.1. We find NMT gets very good performance compared with SMT for only several years. But we know much linguistic knowledge play a key role in NLP and SMT. How do you think about gaining profit by integrating the linguistic knowledge into NMT?2. Another question is that whether NMT can not limited by the expensive parallel corpus, but fully utilizing many comparable corpus? Because NMT models the source sentence as an abstractive representation, and doesn't actually model the word or  phrase relationship between languages like SMT.Hi Xiang,1. ""How do you think about gaining profit by integrating the linguistic knowledge into NMT?""I believe that the described NMT models already have captured those linguistic knowledge that's necessary for well performing translation. However, I also believe that the existing linguistic knowledge can be used as guiding signal during training. For instance, it may speed up the convergence of training by augmenting the encoder with additional classifiers or structured output predictors to predict certain linguistic properties of a source sentence that are deemed important. This kind of giving out hints to make learning easier has been tried in, e.g., http://arxiv.org/abs/1301.4083. So, not as input but as additional target.2. ""Another question is that whether NMT can not limited by the expensive parallel corpus, but fully utilizing many comparable corpus?""Yes, I agree with you that it's important to utilize (almost infinite) monolingual corpora. However, it is not clear how it should be done in NMT. In fact, we here at Montreal jointly with our collaborators in France proposed one such way, called 'deep fusion', in http://arxiv.org/abs/1503.0.... Though, I need to tell you: this paper has been rejected twice already from ACL'15 and EMNLP'15.. :(Cheers,- KDo You think that in the nearest future it will be possible to utilize ""somewhat parallel"" corpora, like translated books? Perhaps by forcing two language models to produce similar paragraph vectors or something like this?Well, this I have no answer to. Of course, I hope one day there will be no need for strict sentence/paragraph alignment, but it's quite unclear how it'll happen.Hi Kyunghyun, thanks for sharing the codes. I've some questions on these lines: https://github.com/kyunghyu... . What is `n_words_source` and what is the list comprehension and line 48 - 50 doing. Could you care to explain in brief?Also, what does `source` at https://github.com/kyunghyu... refer to? Is that the list of frequencies of the words?Once again, thanks for sharing the code. It's great to learn from the code and I've learnt much and still learning more from them! I wish everyone releases their code for any paper/tutorials no matter how ""raw"" they are. They are valuable documentation and great learning points!Hi Liling,1. ""What is `n_words_source`""That is the maximum size of the source vocabulary. Any word with an index larger than this max size will be considered an unknown word (too rare.)2. ""what does `source` at https://github.com/kyunghyunch... refer to?""that's a list of sentences in a minibatch.Cheers,- KThanks for the great post!In figure 7, why RNNsearch-30 still suffer from the problem of performance degradation with respect to the sentence length?HI Kyunghyun, thanks for the specific explanations to the code!!Thanks for the posts. I am trying to replicate your sessions from the git repo, the datasets you are using seems to be missing from the repo. Do you know where I can get these sets?ThanksThanks for great post, and excellent codes.I am trying to apply attention model to other NLP tasks.What should I do If i want to specify or constraint number of target sequence in prediction? (say 1 or same number as input sequence)Hello! Thanks for great introduction!I have a question:What is the exact form of mixing ""glimpses"" generated by attention network and recurrent ( GRU for example ) decoding network?Thanks for very nice post! I am very interested in your opinion about beyond modeling sentences. You said, ""Learning should probably be local, and weights should be updated online while processing a sequence."" I have a question, what's ""weights should be updated online while processing a sequence""? Could you explain it more?Hi, Thanks so much for your very interesting illustrations. However, I wish you could answer my questions below.1) In the ""encoder"" phase of the network, what are the targets used during training to bring up the fixed sentence representation? Do you use for example a fixed special word as a target to indicate ""null"" output at this phase, or you also train to produce the next word in sequence like in language modeling ? or something else? I am really very interested to know.2) For the simpler model without attention mechanism, or even for the more complicated ones with the attention mechanism, is the encoder and decoder trained at the same time in one shot? or indeed they are two separate phases? I assume they are trained in one shot to optimize the log likelihood criterion that imposes the probability of target seq given the source seq, is that right?3) When using the encoder-decoder network in translation mode (I mean not in training mode) in order to produce a target translation of a given source sentence.We start by reading the source sentence sequentially until we produce its representation, then we get into the decoder phase, here the input that we need to feed in is a sample of the output, right? do you select highest probable word from the output layer of the decoder? or you employ multiple N best words in a beam search strategy? please explain, and what is the stopping condition during this search?4) can you please give an idea how much accuracy you got using your approachescompared to the best SMT and the best earlier NMT method?Thank you so so much for your great contribution!!All the best,Amr Mousa""In the ""encoder"" phase of the network, what are the targets used during training to bring up the fixed sentence representation?""There is no target in the encoder network.""For the simpler model without attention mechanism, or even for the more complicated ones with the attention mechanism, is the encoder and decoder trained at the same time in one shot?""They are trained jointly without any pretraining. The separation between the encoder and decoder is rather conceptual.""3)""During training, you feed in the correct words from the training data set (as dictated by maximum likelihood estimation.) During test time, you feed in the word selected from the previous time step. In order to do better (approximate) decoding, it is usual to use beam search.""can you please give an idea how much accuracy you got using your approaches compared to the best SMT and the best earlier NMT method?""I suggest you to Table 1 of http://arxiv.org/abs/1507.0...Powered by Discourse, best viewed with JavaScript enabled"
2038,top-sessions-for-game-developers-at-gtc-2022,"Originally published at:			AI Developer Conference Session Catalog | GTC 2022 | NVIDIA
Learn about the latest RTX and neural rendering technologies and how they are accelerating game development.Powered by Discourse, best viewed with JavaScript enabled"
2039,gtc-2020-ray-traced-virtual-reality-in-omniverse,"GTC 2020 S22029
Presenters: Jeroen Stinstra,NVIDIA; Kevin Parker, NVIDIA
Abstract
Ominverse is a new platform developed by NVIDIA to share scenes and models between different editors and viewers. Ray tracing is used to accurately visualize content within the Omniverse Kit viewer. As quality ray-tracing effects (such as reflections, soft shadows, and ambient occlusion) are expensive to compute, we’ll discuss how we were able to use eye-tracked foveation and warped-space rendering to achieve sufficient performance and quality gains for a virtual reality viewer. We’ll also show how adding multi-frame explicit history reprojection to our de-noising strategy better handles the motion of VR interactions. To further improve performance, we’ll discuss our strategies for dividing work between multiple GPUs. Streaming allows us to decouple the multi-GPU rendering server from the headset. Finally, we’ll demonstrate our application to allow you to experience first-hand the benefits of eye-tracked foveation and ray tracing.Watch this session
Join in the conversation below.Powered by Discourse, best viewed with JavaScript enabled"
2040,gtc-2020-nvidia-drive-labs-an-inside-look-at-autonomous-vehicle-software,"GTC 2020 S22159
Presenters: Neda Cvijetic,NVIDIA
Abstract
We’ll take an engineering-focused look at a range of open autonomous vehicle challenges — from perceiving drivable paths to handling intersections — and how they are being solved at NVIDIA to create safe and robust self-driving car software.Watch this session
Join in the conversation below.Powered by Discourse, best viewed with JavaScript enabled"
2041,actress-kristen-stewart-co-authored-ai-style-transfer-paper,"Originally published at:			Actress Kristen Stewart Co-Authored AI Style Transfer Paper | NVIDIA Technical Blog
The Twilight actress made her directorial debut in a short film Come Swim, shown yesterday at the Sundance Film Festival and features the use of the popular neural “style transfer” technique to build its story. The project, co-authored with an Adobe research engineer, is a great case study on the ability to use style transfer…Powered by Discourse, best viewed with JavaScript enabled"
2042,troubleshooting-networks-with-netq,"Originally published at:			https://developer.nvidia.com/blog/troubleshooting-networks-with-netq/
EVPN has become the standard solution for modern data center fabrics. To end users, EVPN offers the flexibility of extending their broadcast domains while benefiting from the stability of BGP-based control planes. However, these extra benefits come at a price of increased configuration complexity. No longer are we dealing with relatively flat, simple network configurations…Powered by Discourse, best viewed with JavaScript enabled"
2043,thinking-parallel-part-i-collision-detection-on-the-gpu,"Originally published at:			https://developer.nvidia.com/blog/thinking-parallel-part-i-collision-detection-gpu/
This series of posts aims to highlight some of the main differences between conventional programming and parallel programming on the algorithmic level, using broad-phase collision detection as an example. The first part will give some background, discuss two commonly used approaches, and introduce the concept of divergence. The second part will switch gears to hierarchical…Hi,  Nice post! I'm a beginner of parallel computing. Your post is very useful for me. But I failed in implementing the BVH building algorithm described in part III. For me, those related papers published on ACM HPG  are too difficult to understand. Do you have available source code?Powered by Discourse, best viewed with JavaScript enabled"
2044,machine-learning-frameworks-interoperability-part-2-data-loading-and-data-transfer-bottlenecks,"Originally published at:			https://developer.nvidia.com/blog/machine-learning-frameworks-interoperability-part-2-data-loading-and-data-transfer-bottlenecks/
Introduction Efficient pipeline design is crucial for data scientists. When composing complex end-to-end workflows, you may choose from a wide variety of building blocks, each of them specialized for a dedicated task. Unfortunately, repeatedly converting between data formats is an error-prone and performance-degrading endeavor. Let’s change that! Figure 1: Interoperability between data science and machine…Powered by Discourse, best viewed with JavaScript enabled"
2045,gtc-2020-fully-exploiting-a-gpu-supercomputer-for-seismic-imaging,"GTC 2020 S21451
Presenters: Lionel Boillot,Total SA; Long Qu,Total SA
Abstract
We’ll show you how we ported modern seismic applications like Reverse Time Migration, Full Wave Inversion, and One-Way Migration to the GPU-accelerated Pangea III supercomputer. We’ll explain decisive code transformations to take full advantage of the computing power brought by NVIDIA V100, as well as Power9-enhanced multi-GPU support (NVLink/GPUDirect). We’ll describe different CUDA optimization techniques to achieve an asynchronous implementation entirely overlapping communications with the propagation kernels. We’ll also compare OpenACC and CUDA programming models, and outline a new hybrid GPU-CPU data compression algorithm, developed with the support of NVIDIA, that vastly outperforms the CPU version.Watch this session
Join in the conversation below.Powered by Discourse, best viewed with JavaScript enabled"
2046,just-released-tensorrt-8-4,"Originally published at:			https://developer.nvidia.com/tensorrt-getting-started
Powered by Discourse, best viewed with JavaScript enabled"
2047,gtc-2020-espresso-a-fast-end-to-end-neural-speech-recognition-toolkit,"GTC 2020 S21239
Presenters: Yiming Wang,Johns Hopkins University
Abstract
We’ll introduce Espresso, an open-source, modular, extensible, end-to-end neural automatic speech recognition (ASR) toolkit based on the deep learning library PyTorch and the popular neural machine translation toolkit fairseq. Espresso supports distributed training across GPUs and computing nodes, and features various decoding approaches commonly employed in ASR, including look-ahead word-based language model fusion, for which a fast, parallelized decoder is implemented. Espresso achieves state-of-the-art ASR performance on the WSJ, LibriSpeech, and Switchboard datasets, among other end-to-end systems without data augmentation, and is up to 11x faster for decoding than similar systems, such as ESPnet.Watch this session
Join in the conversation below.Powered by Discourse, best viewed with JavaScript enabled"
2048,massively-scale-your-deep-learning-training-with-nccl-2-4,"Originally published at:			Massively Scale Your Deep Learning Training with NCCL 2.4 | NVIDIA Technical Blog
Imagine using tens of thousands of GPUs to train your neural network. Using multiple GPUs to train neural networks has become quite common with all deep learning frameworks, providing optimized, multi-GPU, and multi-machine training. Allreduce operations, used to sum gradients over multiple GPUs, have usually been implemented using rings [1] [2] to achieve full bandwidth. The…I wonder whether the Flat Ring is composed of a reduce-scatter then an all-gather? Or just a reduce and broadcast?Powered by Discourse, best viewed with JavaScript enabled"
2049,deep-learning-experts-named-to-mits-innovators-under-35-list,"Originally published at:			Deep Learning Experts Named to MIT’s ‘Innovators Under 35’ List | NVIDIA Technical Blog
Two winners in the Visionary category are harnessing the computing power of NVIDIA GPUs to drive their artificial intelligence applications. MIT Technology Review recently revealed its annual “35 Innovators Under 35,” which lists young technologists usingtoday’s emerging technologies to transform tomorrow’s world. Adam Coates, left, and Ilya Sutskever, right Ilya Sutskever, 29, is a key…Powered by Discourse, best viewed with JavaScript enabled"
2050,cuda-toolkit-11-8-new-features-revealed,"Originally published at:			https://developer.nvidia.com/blog/cuda-toolkit-11-8-new-features-revealed/
Updated news on latest CUDA Toolkit 11.8 software release.These are some incredible features!So, I guess no changes to the core APIs…? Anyway, I’ve updated the Modern-C++ API wrappers with the information about Hopper and Lovelace, but only on the development branch for now.One of the 11.8 features, “CUDA upgrades on Jetson” has an upcoming webinar here : https://info.nvidia.com/cuda-on-jetson-webinar.html
Do register if you’d like to get more details on this feature.Thanks! We appreciate the feedback! :)Powered by Discourse, best viewed with JavaScript enabled"
2051,new-ai-courses-in-data-science-iva-and-robotics-at-gtc-2019,"Originally published at:			New AI Courses in Data Science, IVA, and Robotics at GTC 2019 | NVIDIA Technical Blog
At GTC 2019 next week in San Jose, California, the NVIDIA Deep Learning Institute (DLI) will launch new hands-on courses — both instructor-led and self-paced online — in AI for data science, intelligent video analytics, and robotics. The DLI will offer six full-day workshops, more than 75 instructor-led training sessions, and dozens of self-paced trainings…Powered by Discourse, best viewed with JavaScript enabled"
2052,porting-scientific-applications-to-gpus-at-the-olcf-openacc-hackathon,"Originally published at:			https://developer.nvidia.com/blog/porting-scientific-applications-gpus-olcf-openacc-hackathon/
Dr. Misun Min of the Argonne National Laboratory Six scientific computing teams from around the world spent an intense week late last year porting their applications to GPUs using OpenACC directives. The Oak Ridge Leadership Computing Facility (OLCF) hosted its first ever OpenACC Hackathon in Knoxville, Tennessee. Paired with two GPU mentors, each team of…Powered by Discourse, best viewed with JavaScript enabled"
2053,metropolis-spotlight-marshallai-optimizes-traffic-management-while-reducing-carbon-emissions,"Originally published at:			https://developer.nvidia.com/blog/metropolis-spotlight-marshallai-optimizes-traffic-management-while-reducing-carbon-emissions/
MarshallAI is using NVIDIA GPU accelerated technologies to help cities improve their traffic management, reduce carbon emissions, and save drivers time.Powered by Discourse, best viewed with JavaScript enabled"
2054,ray-tracing-essentials-part-1-basics-of-ray-tracing,"Originally published at:			Ray Tracing Essentials Part 1: Basics of Ray Tracing | NVIDIA Technical Blog
In February 2019, NVIDIA published Ray Tracing Gems, a deep-dive into best practices for real-time ray tracing. The book was made free-to-download, in an effort to help all developers embrace the bleeding edge of rendering technology. Ray Tracing Essentials is a seven-part video series hosted by the editor of Ray Tracing Gems, NVIDIA’s Eric Haines.…Powered by Discourse, best viewed with JavaScript enabled"
2055,meet-the-ai-choreographer-this-new-model-can-help-you-with-your-next-dance-video,"Originally published at:			Meet the AI Choreographer: This New Model Can Help You With Your Next Dance Video | NVIDIA Technical Blog
To help automatically create a dance video, NVIDIA researchers in collaboration with University of California, Merced developed a deep learning-based model that can automatically compose new dance moves that are diverse, style-consistent, and match the beat. “This is a challenging but interesting generative task with the potential to assist and expand content creations in arts…Powered by Discourse, best viewed with JavaScript enabled"
2056,video-tutorial-flex-for-unity-plugin,"Originally published at:			Video Tutorial: Flex for Unity Plugin | NVIDIA Technical Blog
NVIDIA FleX, a particle based simulation tool for real-time visual effects, has just made its debut as a plugin on the Unity Asset Store. FleX for Unity solves a significant physics challenge in game development: getting different simulated substances (rigid bodies, fluids, clothing, etc.) to interact with each other seamlessly in real time. FleX harnesses…Hi,thanks for the great work! Btw. why does it say DX11 or CUDA only? I thought FleX also works with OpenGL and Linux?Most important: is it possible to use NvFlexRegisterD3DBuffer to directly copy Flex Data to a ComputeShader (without using the CPU) ?Question. For VR I want to create a newspaper pickup that actually feels bendy. I want to be able to move it via code but also release it and let it fall and do it's thing. I tried using a softbody but I had to constraint the bottom, left and right edges of the newspaper for it to feel right (even with max stiffness the paper felt more like a cloth than a thick cluster of paper). The issue with that is that if I constraint the edges then the newspaper sticks in midair. Generally adding a parent with a rigidbody allows it to fall but the result is rather odd even if the collider I add to allow the rigidbody to fall is tiny. it would be cool if we could constraint parts of th simulation but at the same time not have them constrained to the world (like local constraints?). Anyway, maybe I'm just not using the right component for the job. Any better ideas? Also, how would you go about adding sound on collision? are there any events or something we can use to do that?Hi Alvaro, Did you try to work with a thin softbody, and increase the cluster size? This way you can make the simulation really stiff at the expense of the amount of the degrees of freedom you'll get in the motion.Flex on Linux uses CUDA, there's no Flex version which uses OpenGL compute shaders. Flex CUDA version can work with both D3D11 and OpenGL. Though, Unity plugin has only native libs for Windows64.It is possible, in theory, to use NvFlexRegisterD3DBuffer() function with Flex DX11 version, though we didn't try it.Hi,Thanks for the reply, I already tried NvFlexRegisterD3DBuffer it is great, but I get some memory leaks from the buffer maybe I am doing something wrong, an simple example would be great!Hi i am doing a mechanism for softbody cutting in flex. Can you tell me how change the shapeindices and shapeoffsets values at runtime. i have tired using asset.m_shapeIndices and change the values but it is changing only after the scene gets restartedHi!Using this plugin in Unity 2018.3, we are trying to scale this Fluids Particles (we are trying to simulate water) but once we do that, the particle source ""explodes"". So, we can't reduce the size of the fluid source at the scene.Is there a way to modify the scale of the source without breaking it?Hi!Why do my flex particles pass through * .obj objects?Could you give me a demo about using NvFlexRegisterD3DBuffer . I tried ,but when I use NvFlexGetParticles ,It shows error 0xC0000005.Powered by Discourse, best viewed with JavaScript enabled"
2057,upcoming-event-retail-edge-computing-101-an-introduction-to-the-edge-for-retail,"Originally published at:			https://developer.nvidia.com/blog/upcoming-event-retail-edge-computing-101-an-introduction-to-the-edge-for-retail/
​​Join us November 9 for the Retail Edge Computing 101: An Introduction to the Edge for Retail webinar to get everything you wanted to know about the edge, from the leader in AI and accelerated computing.Powered by Discourse, best viewed with JavaScript enabled"
2058,new-courses-for-building-metaverse-tools-on-nvidia-omniverse,"Originally published at:			https://developer.nvidia.com/blog/new-courses-for-building-metaverse-tools-in-omniverse/
Check out three new NVIDIA Omniverse self-paced, hands-on courses for developers and technical artists who build tools to create virtual worlds.Powered by Discourse, best viewed with JavaScript enabled"
2059,building-xr-applications-on-pc-for-high-quality-graphics,"Originally published at:			https://developer.nvidia.com/blog/building-xr-applications-on-pc-for-high-quality-graphics/
When it comes to creating immersive, virtual environments, users want the experience to look as realistic and lifelike as possible. And while AIO headsets provide mobility and freedom for VR users, the headsets don’t always have enough power to render photorealistic scenes with accurate physics and lighting. Using the cloud and professional GPUs, you can…Powered by Discourse, best viewed with JavaScript enabled"
2060,top-data-science-sessions-at-nvidia-gtc-2023,"Originally published at:			Data Science Conference Sessions | GTC 2023 Spring | NVIDIA
Learn about the latest AI and data science breakthroughs from leading data science teams at NVIDIA GTC 2023.Powered by Discourse, best viewed with JavaScript enabled"
2061,accelerating-the-suricata-ids-ips-with-nvidia-bluefield-dpus,"Originally published at:			https://developer.nvidia.com/blog/accelerating-the-suricata-ids-ips-with-nvidia-bluefield-dpus/
Deep packet inspection (DPI) is a critical technology for network security that enables the inspection and analysis of data packets as they travel across a network. By examining the content of these packets, DPI can identify potential security threats such as malware, viruses, and malicious traffic, and prevent them from infiltrating the network. However, the…Are you interested in developing solutions for Suricata or have any questions? If so, drop a note below.Powered by Discourse, best viewed with JavaScript enabled"
2062,cuda-11-6-toolkit-new-release-revealed,"Originally published at:			CUDA 11.6 Toolkit New Release Revealed | NVIDIA Technical Blog
New CUDA 11.6 Toolkit is focused on enhancing the programming model and performance of your CUDA applications.Powered by Discourse, best viewed with JavaScript enabled"
2063,gpus-accelerating-higher-energy-exploration-at-cern,"Originally published at:			GPUs Accelerating Higher Energy Exploration at CERN | NVIDIA Technical Blog
CERN is upgrading the LHC (Large Hadron Collider), which is the world’s largest and most powerful particle accelerator ever built, to explore the new high-energy frontier. A researcher describes how his team is working to increase the luminosity of beam dynamics with GPUs. The most technically challenging aspects of the upgrade cannot be done by…Powered by Discourse, best viewed with JavaScript enabled"
2064,on-demand-session-deploying-highly-accurate-retail-applications-using-a-digital-twin,"Originally published at:			https://developer.nvidia.com/blog/deploying-highly-accurate-retail-applications-using-a-digital-twin/
A new session from GTC shares how to use synthetic data and Fleet Command to deploy highly accurate and scalable models.Powered by Discourse, best viewed with JavaScript enabled"
2065,gtc-2020-audio-effects-sdk-implementing-state-of-the-art-real-time-audio-effects-using-the-power-of-nvidia-rtx-gpus,"GTC 2020 S21409Sorry folks - this session was cancelled.The video is not working: https://developer.nvidia.com/gtc/2020/video/s21409+1 Please fix thisplease fixSorry this session was cancelled - and this posting and the assets on the developer site were created in error.
Sorry for any confusion.Powered by Discourse, best viewed with JavaScript enabled"
2066,nvidia-clara-deploy-adds-new-pipelines-operators-and-improved-developer-productivity,"Originally published at:			https://developer.nvidia.com/blog/clara-deploy-adds-new-pipelines/
At SIIM 2020, the annual meeting of the Society for Imaging Informatics in Medicine, NVIDIA announced updates to the Clara Deploy Application Framework to expand its capabilities to address COVID-19, digital pathology and improved developer experience.Powered by Discourse, best viewed with JavaScript enabled"
2067,adding-mig-preinstalled-drivers-and-more-to-nvidia-gpu-operator,"Originally published at:			https://developer.nvidia.com/blog/adding-mig-preinstalled-drivers-and-more-to-nvidia-gpu-operator/
Learn about the latest GPU Operator releases which include support for multi-instance GPU Support, pre-installed NVIDIA drivers, Red Hat OpenShift 4.7, and more.Hey folks,Hope you enjoyed the blog. Feel free to ask any follow-up questions here and Erik and I can answer whatever is on your mind!Thanks,
Troy Estes
Product Marketing Manager, Edge AIPowered by Discourse, best viewed with JavaScript enabled"
2068,inside-volta-the-world-s-most-advanced-data-center-gpu,"Originally published at:			Inside Volta: The World’s Most Advanced Data Center GPU | NVIDIA Technical Blog
Today at the 2017 GPU Technology Conference in San Jose, NVIDIA CEO Jen-Hsun Huang announced the new NVIDIA Tesla V100, the most advanced accelerator ever built. From recognizing speech to training virtual personal assistants to converse naturally; from detecting lanes on the road to teaching autonomous cars to drive; data scientists are taking on increasingly…AyyyMD.AMDeadDon't worry guys the RX580 is still faster!Your bank account is dead too.Yes yes just gotta overclock it a bit, the gains are ayyymazing!815mm^2 on a 12nm process. That is a humongous GPU.Reticle limit of TSMCThe independent thread scheduling in a warp looks very interesting. With this feature, is it possible to have threads in different branches participate in warp intrinsics like __ballot or __shfl?If the intention is to use 8 GV100 on a DGX-1 why 6 NVlink? should be 7 NVlinks, or i miss something?At least i can buy a RX580 even crossfire it to beat a 1080ti, but i have to admit i don't have $140K to expend on nvidia new card solution.""Each Tensor Core performs 64 floating point FMA mixed-precision operations per clock (FP16 input multiply with full-precision product and FP32 accumulate, as Figure 8 shows) and 8 Tensor Cores in an SM perform a total of 1024 floating point operations per clock.""How many 4 x 4 matrix-matrix multiplications is that per clock?I think it's 16 or 64 FMAs per matrix multiplication, so either 4 or 1 MMMs per clock, but the article doesn't say.The fp16 format is dismayingly approximate, with an infinity that starts above 65,504, and a minimum value above 1 of only a bit less than 1.001. Resolution between 0 and 1 is less cramped, though ( ~13-14 bits, I think) and is often all the application requires, especially dealing with probabilities and data normalized to the [0,1] or [-1, 1]range.The Tensor cores look like they might be useful for doing 4D Geometric Algebra (GA)/ Clifford Algebra calculations, which would be extremely cool, since GA is the best way to do math representing physics, whether classical mechanics, EM, QM, SR or GR - too many advantages to list here, but I'll point out Geomerics, (the British company that brought real-time radiosity lighting to games, bought by ARM) was the work of the worlds top GA physicists, particularly Cambridge's Chris Doran.There are only  2 Clifford algebras that can be represented with real-valued 4 x 4  matrices, Cl(3,1) (signature (+++-)) and Cl(2,2) (signature (++- -)). The other 4D signatures require 2 x 2 matrices of quaternions. The 2D + 2 Conformal Geometric Algebra (CGA) has a (+++-) Minkoski signature that can also be used for relativistic EM, (though the (+- - -) ""space-time algebra"" is more common for that use).The 2D + 2 CGA represents 2D lines, circles, and points as points in a 4D space. It's like extending first to homogeneous coordinates, as in conventional graphics: the extra dimension allows constructing subspaces (e.g. lines) that don't pass through the origin on the 2D plane. In CGA that extra homogeneous dimension is called ""origin"" for that reason. In addition, CGA adds another extra dimension called ""infinity"", which allows representing points, circles and lines  (and planes, spheres in 3D +2 CGA) as unified entities - a point has zero infinity component, a circle has some, and a line is a circle passing through the ""point at infinity"", a circle with infinite radius. Taking the outer product of any 3 points gives the circle passing through those points. An easy ""dualization"" converts the circle to a representation as a center point and a radius. There are some other primatives such as point-pairs (0D spheres) in CGA as well that are very useful  All sorts of geometric operations such as unions, intersections  are much easier in CGA.Obviously the 3D +2 CGA is more useful for 3D graphics, but it is a 5D algebra which doesn't fit in the Tensor units. (Cl(4,1) needs 4x4 complex matrices) It would be useful to find out if there are practical ways to make GA calculation on GPUs easier and faster because that would make physics simulations in general much easier to program. GA gives a single, unified representation to areas that now are a vast collection of ad-hoc hacks that often don't work well together. Chris Doran would be the person to talk to about what would make GPUs better for GA and physics simulation in general.Yes. Note that you have to use the new ""sync"" versions these builtin functions, which take an additional parameter to specify which threads participate in the operation.For scale, 35mm camera sensors are 864mm^2 (but they're way lower res. lithography.)I thought mullti-chip modules had gotten to the point where it was possible to divide such monster chips into several pieces with up to thousands of bus lines going through through-silicon vias then running a just a mm or two across the carrier between chips. I guess not, since if you could restrict the pieces to less than a couple 100 mm^2 then yields could be something like an order of magnitude higher.These are not going to be competitive with the TPUs having 65535 MACs with 8 bits which is ideal.I assume the __syncwarp() is heavily added by the compiler as well whenever safe? ... but does this give away reconvergence of sub-warps in nested divergence or do the compiler still enforces this in an implicit way (when safe)?The compiler uses a different set of instructions for convergence optimizations. You should expect the same convergence as Pascal (for code that both architectures can run) at no additional effort on your part.Tesla M40 Peak FP64?Table 1 above:TFLOP/s: 2.1 (about 2100 GFLOPs)https://images.nvidia.com/c...page 11 Table 1:GFLOPs: 210Where does the factor 10 come from?96 FP64 Cores * 1114 MHz * ? FLOP/cyc107 GHz * 2 (e.g. single FMA) would be about 210 GFLOPsalso cmp. https://en.wikipedia.org/wi...so it converges (roughly?) at IPDOM unless synchronization is detected, it converges at the safest reconvergence point the compiler can detect?If so then, ""You should expect the same convergence as Pascal (for code that both architectures can run) at no additional effort on your part.""  sounds like the compiler can never ""falsely"" detect synchronization, which does not sound realistic?MPS(Multi-Process Service) has a few restrictions. One of the most mysterious one is unsupport of dynamic parallelism. Is it still prohibited on the Volta generation?Powered by Discourse, best viewed with JavaScript enabled"
2069,accelerating-recommender-systems-training-with-nvidia-merlin-open-beta,"Originally published at:			https://developer.nvidia.com/blog/accelerating-recommender-systems-training-with-nvidia-merlin-open-beta/
NVIDIA Merlin is an open beta application framework and ecosystem that enables the end-to-end development of recommender systems, from data preprocessing to model training and inference, all accelerated on NVIDIA GPU. We announced Merlin in a previous post and have been continuously making updates to the open beta. In this post, we detail the new…Powered by Discourse, best viewed with JavaScript enabled"
2070,beginners-guide-to-gpu-accelerated-event-stream-processing-in-python,"Originally published at:			https://developer.nvidia.com/blog/beginners-guide-to-gpu-accelerated-event-stream-processing-in-python/
This tutorial is the seventh installment of introductions to the RAPIDS ecosystem. The series explores and discusses various aspects of RAPIDS that allow its users solve ETL (Extract, Transform, Load) problems, build ML (Machine Learning) and DL (Deep Learning) models, explore expansive graphs, process geospatial, signal, and system log data, or use SQL language via…Powered by Discourse, best viewed with JavaScript enabled"
2071,laying-the-foundation-for-better-object-manipulation-in-robotics,"Originally published at:			Laying the Foundation for Better Object Manipulation in Robotics | NVIDIA Technical Blog
Imagine a robot that can efficiently model clay, push ice cream onto a cone, or mold the rice for your sushi roll. MIT researchers developed a deep learning-based algorithm that improves a robot’s ability to mold materials into shapes, as well as enabling it to interact with liquids and solid objects. The work draws on…Powered by Discourse, best viewed with JavaScript enabled"
2072,realizing-the-power-of-real-time-network-processing-with-nvidia-doca-gpunetio,"Originally published at:			https://developer.nvidia.com/blog/realizing-the-power-of-real-time-network-processing-with-nvidia-doca-gpunetio/
NVIDIA DOCA GPUNetIO library can be adopted in a wide range of applications from different contexts, providing huge improvements for latency, throughput, and system resource utilization.Powered by Discourse, best viewed with JavaScript enabled"
2073,share-your-science-the-future-of-facial-recognition-for-video-surveillance,"Originally published at:			https://developer.nvidia.com/blog/share-your-science-the-future-of-facial-recognition-for-video-surveillance/
Javier Rodriguez Saeta, CEO of Herta Security, shares how they’re using NVIDIA GPUs to train deep neural networks for pattern recognition to help with security at airports, stadiums and train stations. Herta Security’s high performance video-surveillance solution for facial recognition is designed to identify people in crowded and changing environments in real-time. Their technology makes…Powered by Discourse, best viewed with JavaScript enabled"
2074,upcoming-event-accelerate-yolov5-and-custom-ai-models-in-ros-with-nvidia-isaac,"Originally published at:			Isaac ROS Webinar Series
Learn about the NVIDIA Isaac ROS DNN Inference pipeline and how to use your own models with a YOLOv5 example in this December 1 webinar.Powered by Discourse, best viewed with JavaScript enabled"
2075,webinar-learn-how-nvidia-driveworks-gets-to-the-point-with-lidar-sensor-processing,"Originally published at:			https://developer.nvidia.com/blog/webinar-learn-how-nvidia-driveworks-gets-to-the-point-with-lidar-sensor-processing/
With NVIDIA DriveWorks SDK, autonomous vehicles can bring their understanding of the world to a new dimension. The SDK enables autonomous vehicle developers to easily process three-dimensional lidar data and apply it to specific tasks, such as perception or localization. You can learn how to implement this critical toolkit in our expert-led webinar, Point Cloud…Powered by Discourse, best viewed with JavaScript enabled"
2076,finite-difference-methods-in-cuda-c-part-2,"Originally published at:			https://developer.nvidia.com/blog/finite-difference-methods-cuda-c-part-2/
In the previous CUDA C++ post we dove in to 3D finite difference computations in CUDA C/C++, demonstrating how to implement the x derivative part of the computation. In this post, let’s continue by exploring how we can write efficient kernels for the y and z derivatives. As with the previous post, code for the examples in this post…Hi, Dr. Mark Harris, I wanted to share my benchmarks with you and the rest of the NVIDIA CUDA dev community my benchmarks for 3-dim. finite difference derivatives.  I had a few questions, which I wanted to throw out: beyond this 64^3 grid, how does this implementation and the concepts of pencils, extend to arbitrarily (large) sized grids?  Naively, if you wanted a ""big"" grid to do 3-dim. finite difference derivatives on, e.g. 2560^3, does the ""pencil"" extend to size 2560 (entries)? Can we go even larger?  More in general, are there any implementations out there for 3-dim. Navier-Stokes equation solvers using this finite difference with CUDA C/C++?I asked this in part 1, but it may pertain here: Arbitrarily (large) sized grids - naively, I changed mx=my=mz for the grid size (originally 64^3) to 92^3 (i.e. mx=my=mz=92) and anything above 92, I obtain a Segmentation fault (core dumped).  I was simply curious what was happening; is it a limitation on the GPU hardware?  If so, which parameter? I'm on a NVIDIA GeForce GTX 980Ti.Quite useful could be real-measures approximation vial finite sum of improper polynomial. Note improper integral as sum of improper integrals - trivially to parallelize, and less computational costly than f.e. rectangles method. I strongly feel that simple cost function of RMSE and LUT of precomputated polynomial integrals should do the job on GTX 1060 and Core2Duo.Powered by Discourse, best viewed with JavaScript enabled"
2077,upcoming-event-nvidia-jetson-edge-ai-developer-days,"Originally published at:			Jetson Developer Day at GTC 2023
At NVIDIA GTC 2023 join robotics, edge AI, and computer vision experts for a deep dive into building next-generation AI-powered applications and autonomous machines.Powered by Discourse, best viewed with JavaScript enabled"
2078,data-attached-to-a-graph,"I’m sure there’s a huge range, but what kind / what sizes of data do you see attached to vertices and edges? In the academic world, we rarely have more than, say, something like a weight (float) per edge, but I can imagine in practice, there might be much larger / much more interesting auxiliary data.We support any type of data, from simple integers to full tensors.
Our algorithm is generic enough to support all kind of edge and node properties and multiple properties at the same time.Is that data stored separately from the connectivity information? What kind of data structure do you use for attached data?In the generic case, data is typically stored in dataframes such as cudf or dask_cudf. If this is edge data, then connectivity is typically provided with src id and dst id, which follows the familiar “SRO” model (source, relation, object).  cuGraph also natively supports additional edge properties such as weight, edge id, and edge type.  We also offer a property graph, which can store both features and connectivity prior to extracting a structural cugraph graph to run various algorithms on.For GNNs specifically, we recommend using our feature store that can efficiently store tensors.This topic was automatically closed 14 days after the last reply. New replies are no longer allowed.Powered by Discourse, best viewed with JavaScript enabled"
2079,using-network-graphs-to-visualize-potential-fraud-on-ethereum-blockchain,"Originally published at:			Using Network Graphs to Visualize Potential Fraud on Ethereum Blockchain | NVIDIA Technical Blog

This article provides a guided project to access, analyze, and identify potential fraud using blockchain data via python.Powered by Discourse, best viewed with JavaScript enabled"
2080,gtc-2020-opening-up-the-black-box-model-understanding-with-captum-and-pytorch,"GTC 2020 S22147
Presenters: Narine Kokhlikyan,Facebook AI; Ludwig Schubert,OpenAI
Abstract
PyTorch, the popular open-source ML framework, has continued to evolve rapidly since the introduction of PyTorch 1.0, which brought an accelerated workflow from research to production. We’ll deep dive on some of the most important new advances, including the ability to name tensors, support for quantization-aware training and post-training quantization, improved distributed training on GPUs, and streamlined mobile deployment. We’ll also cover new developer tools and domain-specific frameworks including Captum for model interpretability, Detectron2 for computer vision, and speech extensions for Fairseq.Watch this session
Join in the conversation below.Hi @nadeemm, hope you are well.
It will be great to know more about this popular open-source ML framework  :)Thanks for watching.
You can read more about PyTorch here : https://pytorch.org/
and on our developer site@bayangp0, specifically about captum you can read more here: https://captum.ai/
Introduction to Captum — A model interpretability library for PyTorch | by PyTorch | PyTorch | Medium@nadeemm , Thank you :)@narine.kokhlikyan,  Thank you  :)Powered by Discourse, best viewed with JavaScript enabled"
2081,lockheed-martin-and-usc-to-launch-jetson-based-nanosatellite-for-scientific-research-into-orbit,"Originally published at:			https://developer.nvidia.com/blog/lockheed-martin-usc-jetson-nanosatellite/
This week Lockheed Martin, in a joint collaboration with the University of Southern California, announced plans to launch an AI, GPU-accelerated nanosatellite into orbit.  As part of a program called La Jument, Lockheed Martin and the Space Engineering Research Center at the University of Southern California’s Information Sciences Institute plan to launch its first satellite…Powered by Discourse, best viewed with JavaScript enabled"
2082,fast-int8-inference-for-autonomous-vehicles-with-tensorrt-3,"Originally published at:			Fast INT8 Inference for Autonomous Vehicles with TensorRT 3 | NVIDIA Technical Blog
Autonomous driving demands safety, and a high-performance computing solution to process sensor data with extreme accuracy. Researchers and developers creating deep neural networks (DNNs) for self driving must optimize their networks to ensure low-latency inference and energy efficiency. Thanks to a new Python API in NVIDIA TensorRT, this process just became easier. TensorRT optimizes trained…Powered by Discourse, best viewed with JavaScript enabled"
2083,nvidia-researchers-introduce-robot-that-automatically-adapts-to-different-terrains,"Originally published at:			NVIDIA Researchers Introduce Robot that Automatically Adapts to Different Terrains | NVIDIA Technical Blog
At GTC, NVIDIA researchers introduced a robotics framework that combines model-based control and reinforcement learning to adaptively change contact sequences in real time.  The system has the potential to help delivery robots and other autonomous machines function more effectively in environments and terrains the robot is not familiar with.  The controller adapts to environmental changes…Powered by Discourse, best viewed with JavaScript enabled"
2084,fast-track-production-ai-with-pretrained-models-and-transfer-learning-toolkit-3-0,"Originally published at:			https://developer.nvidia.com/blog/fast-track-your-production-ai-with-pre-trained-models-and-transfer-learning-toolkit-3-0/
NVIDIA announced new pre-trained models and general availability of Transfer Learning Toolkit (TLT) 3.0, a core component of NVIDIA’s Train, Adapt and Optimize (TAO) platform guided workflow for creating AI.Powered by Discourse, best viewed with JavaScript enabled"
2085,jetson-project-of-the-month-blinkr-blink-detection-and-reminder,"Originally published at:			https://developer.nvidia.com/blog/jetson-project-of-the-month-blinkr-blink-detection-and-reminder/
Thirteen-year-old Adrit Rao, was awarded the Jetson Project of the Month for his Blink Detection and Reminder (Blinkr). The project, which runs on a NVIDIA Jetson Nano 2GB Developer Kit, monitors the eyes of the user and voices a prompt when their blink rate is less than the recommended rate of 10 blinks per minute. …Powered by Discourse, best viewed with JavaScript enabled"
2086,dli-training-deep-learning-for-autonomous-vehicles,"Originally published at:			DLI Training: Deep Learning for Autonomous Vehicles | NVIDIA Technical Blog
Artificial intelligence (AI) is taking the automotive industry by storm. AI-powered robotaxis are expected to create a $2 trillion market worldwide by 2030, according to global financial services firm UBS.  One of the most prominent ways AI is revolutionizing the industry is the development of autonomous driving technology. Self-driving cars use camera-based machine vision systems…Powered by Discourse, best viewed with JavaScript enabled"
2087,unlocking-operational-consistency-with-the-nvidia-user-experience-cli-object-model,"Originally published at:			https://developer.nvidia.com/blog/unlocking-operational-consistency-with-the-nvidia-user-experience-cli-object-model/
Cumulus Linux 4.4 introduces a new CLI, NVUE, that is more than just a CLI. NVUE provides a complete object model for Linux, unlocking incredible operational potential.Powered by Discourse, best viewed with JavaScript enabled"
2088,advanced-api-performance-memory-and-resources,"Originally published at:			https://developer.nvidia.com/blog/advanced-api-performance-memory-and-resources/
This post covers best practices for memory and resources on NVIDIA GPUs. To get a high and consistent frame rate in your applications, see all Advanced API Performance tips. Memory Optimal memory management in DirectX 12 is critical to a performant application. The following advice should be followed for the best performance while avoiding stuttering.…Hi, I hope you find this post useful. If you have any questions or comments please get in touchPowered by Discourse, best viewed with JavaScript enabled"
2089,how-to-access-global-memory-efficiently-in-cuda-fortran-kernels,"Originally published at:			https://developer.nvidia.com/blog/how-access-global-memory-efficiently-cuda-fortran-kernels/
CUDA Fortran for Scientists and Engineers shows how high-performance application developers can leverage the power of GPUs using Fortran. In the previous two posts we looked at how to move data efficiently between the host and device.  In this sixth post of our CUDA Fortran series we discuss how to efficiently access device memory, in…Powered by Discourse, best viewed with JavaScript enabled"
2090,gtc-2020-case-study-of-deploying-text-to-speech-services-on-gpu,"GTC 2020 S21465
Presenters: Peter Huang,NVIDIA
Abstract
Based on collaboration with customers, we’ll go through the key phases to deploy text-to-speech services including use-case survey, model selection, data preparation, model training, and, most importantly, optimizing model inference on Tesla GPU products. After introducing the background, related models, and tricks for training the model, we’ll take a deep dive into TensorRT based Tacotron and WaveGlow acceleration work-study, and then touch upon methods to accelerate other VoCoders, such as WaveRNN and BERT’s potential usage for TTS.Watch this session
Join in the conversation below.Powered by Discourse, best viewed with JavaScript enabled"
2091,gtc-2020-from-hours-to-minutes-the-journey-of-optimizing-mask-rcnn-and-bert-using-mxnet,"GTC 2020 S22483
Presenters: Haibin Lin,Amazon; Lin Yuan, Amazon
Abstract
Training large deep learning models like Mask R-CNN and BERT takes lots of time and compute resources. Using MXNet, the Amazon Web Services deep learning framework team has been working with NVIDIA to optimize many different areas to cut the training time from hours to minutes.Watch this session
Join in the conversation below.Powered by Discourse, best viewed with JavaScript enabled"
2092,top-synthetic-data-sessions-at-nvidia-gtc-2023,"Originally published at:			https://nvda.ws/3RN2wJz
 Discover how 3D synthetic data generation is accelerating AI and simulation workflows.Powered by Discourse, best viewed with JavaScript enabled"
2093,startup-babblelabs-uses-ai-to-enhance-speech,"Originally published at:			Startup BabbleLabs Uses AI to Enhance Speech | NVIDIA Technical Blog
California-based startup BabbleLabs is working to enhance speech quality, accuracy, and personalization. The company recently announced a new deep learning product that relies on GPUs end-to-end to perform tasks such as speech enhancement, noise reduction, as well as audio and video processing from standard video or audio. “Our first product, Clear Cloud, brings to market…Powered by Discourse, best viewed with JavaScript enabled"
2094,jetson-projects-of-the-month-ai-thermometer-and-self-navigating-robot-for-search-and-rescue,"Originally published at:			Jetson Projects of the Month: AI Thermometer and Self-Navigating Robot for Search and Rescue | NVIDIA Technical Blog
In this edition of our posts about the Jetson Community, we’re featuring two projects and their developers who won the NVIDIA Jetson Project of the Month award recently.  Tomasz Lewicki’s open-source AI fever screening thermometer was awarded the Jetson Project of the Month for June and Maggie Xu’s Self-Navigating Robot for Search and Rescue won…Powered by Discourse, best viewed with JavaScript enabled"
2095,nvidia-makes-3d-deep-learning-research-easy-with-kaolin-pytorch-library,"Originally published at:			NVIDIA Makes 3D Deep Learning Research Easy with Kaolin PyTorch Library | NVIDIA Technical Blog
Research efforts in 3D computer vision and AI have been rising side-by-side like two skyscrapers. But the trip between these formidable towers has involved clambering up and down dozens of stairwells. To bridge that divide, NVIDIA recently released Kaolin, which in a few steps moves 3D models into the realm of neural networks.  Implemented as…Powered by Discourse, best viewed with JavaScript enabled"
2096,maximizing-deep-learning-inference-performance-with-nvidia-model-analyzer,"Originally published at:			Maximizing Deep Learning Inference Performance with NVIDIA Model Analyzer | NVIDIA Technical Blog
Figure 1. Screenshot of Model Analyzer. You’ve built your deep learning inference models and deployed them to NVIDIA Triton Inference Server to maximize model performance. How can you speed up the running of your models further? Enter NVIDIA Model Analyzer, the soon-to-be-released tool for gathering the compute requirements of your models. Without this information, there…We’re really excited to share this work with you – we know this will help you get the most out of your inference models! We have plans to make this open-source shortly. The post will be updated when this happens.If you have any questions or comments, or want to share how you’re using this for your models, please let us know!Hi,The project is so exciting and looks so promising! That said, I wanted to try out by creating the model analyzer Docker container using the Dockerfile in it’s github page. However, I have couple problems using it.I am currently trying to run model analyzer following the steps mentioned in this link: Maximizing Deep Learning Inference Performance with NVIDIA Model AnalyzerHowever, when I download both chest_xray and segmentation_liver models from NGC and give their paths to model analyzer, it throws the following error:Here is the docker command that I use to start model analyzer with.How can I use this tool properly? Documentation is not enough.Hi Doruk,Thanks for giving Model Analyzer a try and asking for clarification!Since Model Analyzer is specifically meant to be used on models prepared for Triton, it expects them in the same format as Triton does.If you’re looking to try it with pre-trained Clara models from NGC, the best bet is to install Clara Deploy and pull that model’s pipeline. That will install a folder with the model to your chosen directory, and you can then use it for Model Analyzer. Alternatively, you can download the model directory directly from the pipeline in NGC (e.g. app_chestxray-model_v1.zip under the chest x-ray pipeline). Also, please ensure the name matches the model name in the configuration file used (config.pbtxt). For example, I believe the chest x-ray you are using is named classification_chestxray_v1. Hope that helps!Hi David,Thanks for your reply. I actually wanted to try out Model Analyzer on Triton Server’s example models first but I got the same error with those models too. Since you already have a post where you try out Model Analyzer on Clara models, I wanted to give it a shot. So the Model Analyzer supports models in the following formats right?However, when I try to give two of Triton Server’s example models to the Model Analyzer with following two methods, Model Analyzer throws the same error.Method 1
Renaming the model file names and config.pbtxt file names to model’s own name and gathering all of them under the same directory.
Screenshot from 2020-09-09 09-43-15845×226 17.1 KB
Method 2
Leaving file names as what they already are and seperating them under the respective directories.
 None of the above two methods working for me right now. I am using the below command to run Model Analyzer’s Docker image.And this is the final output:
Screenshot from 2020-09-09 09-51-371533×169 24.3 KB
Hi Doruk,Thanks for providing such detail. Your method 2 is correct. Essentially, if the repository works with Triton Inference Server via the --model-repository flag, it works with Model Analyzer. So a good test is to load it into Triton.I went through the steps you provided. The issue is happening from the mapped model folder not being the same as the absolute path to it on your computer, due to how Docker opens other Docker containers. The container tries to access /home/models on your local machine and cannot find the models there, so it skips them.Apologies if the sample command did not make this point explicit. We are updating the documentation as feedback is provided.If you run the below command, it should work.Hi all,Thanks for providing useful information and instructions.I built an image from Dockerfile of Github.StepsThen I used this command that I got error.Error message:Here is my normal TRTIS command:It can work very well. (It can also do inference on client side)Questions:Thank you so much!BR,
ChiehHi Chieh,Apologies, I’m seeing this a few weeks late.This error means that your system is not compatible with DCGM. Model Analyzer uses DCGM under the hood to capture the metrics. Please find system requirements in the documentation here: Welcome — NVIDIA DCGM Documentation latest documentationKind regards,
DavidHi Chieh,Apologies, I’m seeing this a few weeks late.This error means that your system is not compatible with DCGM. Model Analyzer uses DCGM under the hood to capture the metrics. Please find system requirements in the DCGM documentation. I would post a link, but the forum is flagging it.Kind regards,
DavidDear @david.yastremsky,Thanks for your reply and your important information!
I saw the link of your provided from previous comment that I will study more about that.Thank you again!Sincerely,
ChiehPowered by Discourse, best viewed with JavaScript enabled"
2097,developing-applications-with-nvidia-bluefield-dpu-and-dpdk,"Originally published at:			https://developer.nvidia.com/blog/developing-applications-with-nvidia-bluefield-dpu-and-dpdk/
Developing and application and offloading it in two ways, via DPDK and via DOCA libraries, to run on the BlueField Data Processing UnitHi,
Is it possible to program DPU using rte_flow from a dpdk application running on x86 VM ? I can offload flows with action type RSS (steering to another queue) but can’t able to program transfer rules like if packet ingress on vf portX , match five tuple and egress to vf port Y (used RTE_FLOW_ACTION_TYPE_PORT_ID ). I have tried with dpdk 19.11 and 20.11, ofed 5.4 from my vm and my bf2 is with embedded switch mode ( also tried in separated mode in connectx6 as well). It returns failure with no error message and error code unspecified. Is it always required to use representator ports from dpu and dpdk program to be run on arm to offload such flow rules? Or am I missing some configuration here ?
Thanks
SubhajitBy default the DPU Arm controls the hardware accelerators (this is the embedded mode that you are referring to). And typically the control plane is offloaded to the Arm.However you can still run your control plane on the X86-host and access the hardware accelerators in one of two ways -The NIC mode is similar to the separated mode (in that the host can configure to the embedded switch) so I am not sure what is missing. I will suggest installing the DOCA 1.3 metapackage on your host and DPU and trying with those DPDK libraries. And maybe also try the NIC mode (instead of the separated mode).Thanks. I have not tried with NIC mode in BF2. However I checked with connectx6DX as well, where there is no internal cpu model as such. In both the cards I tried below thingsPowered by Discourse, best viewed with JavaScript enabled"
2098,explore-the-rtx-platform-within-game-engines-at-new-level-up-with-nvidia-webinars,"Originally published at:			Explore the RTX Platform within Game Engines at New ‘Level Up with NVIDIA’ Webinars | NVIDIA Technical Blog
Learn more about the NVIDIA RTX platform, ask questions about game integrations, and connect with NVIDIA experts at our new webinar series.Powered by Discourse, best viewed with JavaScript enabled"
2099,merge-sort-explained-a-data-scientist-s-algorithm-guide,"Originally published at:			https://developer.nvidia.com/blog/merge-sort-explained-a-data-scientists-algorithm-guide/
The article includes a step by step explanation of the merge sort algorithm and code snippets illustrating the implementation of the algorithm itself.Powered by Discourse, best viewed with JavaScript enabled"
2100,uvm-support,"Does cuGraph have UVM support?Yes, through RMM (rapids memory manager) we support use of managed memory for both single-GPU and multi-GPU workflows. Obviously, performance with managed memory will not be as high as performance with device memory. In addition, cuDF has a spilling feature that can be very helpful when working with RMM pools, which are pre-allocated chunks of memory. And finally, for multi-GPU workflows, dask has its own spilling features that can be configured. There is also additional research going on for further expanding use of host memory in GPU workflows.This topic was automatically closed 14 days after the last reply. New replies are no longer allowed.Powered by Discourse, best viewed with JavaScript enabled"
2101,epfl-plans-to-install-a-gpu-accelerated-supercomputer-for-brain-research,"Originally published at:			EPFL Plans to Install a GPU-Accelerated Supercomputer for Brain Research | NVIDIA Technical Blog
The Ecole Polytechnique Fédérale de Lausanne’s (EPFL) Blue Brain Project, a Swiss brain research initiative, recently announced plans to install a new GPU-accelerated supercomputer designed for attaining a deeper understanding of the brain, in particular for simulation-based research, analysis and visualization. Named, Blue Brain S, the new supercomputer will deliver 1.06 petaflops of peak performance.…Powered by Discourse, best viewed with JavaScript enabled"
2102,nvidias-top-10-ai-developer-stories-of-2019,"Originally published at:			NVIDIA’s Top 10 AI Developer Stories of 2019 | NVIDIA Technical Blog
From a Jetson-based service robodog, to an algorithm that can instantaneously detect cancer in the blood, these are the top 10 AI developer stories that we covered this year on the NVIDIA Developer News Center. All of the developers featured in this video are using NVIDIA GPUs for both training and inference. 10 – Deep…Powered by Discourse, best viewed with JavaScript enabled"
2103,upcoming-event-join-nvidia-at-automate-2022,"Originally published at:			https://developer.nvidia.com/blog/upcoming-event-join-nvidia-at-automate-2022/
Powered by Discourse, best viewed with JavaScript enabled"
2104,government-of-india-nvidia-and-openacc-hackathon-helps-develop-covid-19-solutions,"Originally published at:			Government of India, NVIDIA, and OpenACC Hackathon Helps Develop COVID-19 Solutions | NVIDIA Technical Blog
The Government of India’s Center for Development of Advanced Computing (C-DAC) under Ministry of Electronics and IT (MeitY) in association with NVIDIA, and OpenACC, organized the SAMHAR-COVID19 Hackathon to help researchers combat ongoing COVID-19 pandemic and help the scientific community predict future outbreaks. Through C-DAC’s program, Supercomputing using artificial intelligence, and Healthcare Analytics-based Research for…Powered by Discourse, best viewed with JavaScript enabled"
2105,cvpr-2020-synthesizing-high-resolution-images-with-gans,"CVPR 2020 dcv20
Presenters: Tech Demo Team, NVIDIA
Abstract
Developed by NVIDIA Researchers, StyleGAN2 yields state-of-the-art results in data-driven unconditional generative image modeling.Watch this session
Join in the conversation below.Supposing this proposed method will eventually work well on photographic inputs, artists going forward will eventually no longer make a profit from generating new subjects from their initial style.Now, I understand the intention here is to speed up time-consuming processes in order to optimize profit margins at the enterprise level. I help automate processes for companies as well. That is the essential value we all provide as employees.I’m curious if the intention of the creators of this project is to annihilate the essence of the phrase “artistic style” and reduce stylistic families to a meditation as representational art has become in the place of photography for all of time?And if this wasn’t the intention because the thought was not raised, what can we all do now for all the artists who will be affected?It’s always a good idea to have constructive debate on how new technologies might be used, for art or any other application. Other research into new capabilities for creative applications has drawn widespread praise (Over Half-Million Images Created with GauGAN AI Art Tool | NVIDIA Blog) from both the research and artistic communities, and artists have commented positively that new tools have allowed them to think differently and create their visions in new ways. We are certainly open to input from our developer community, so if others have questions or concerns you can contact me directly at gestes@nvidia.comThe GAN you are referring to is a concept-generator tool as opposed to the tool this post is presenting which will directly replace existing job functions. Moreover, Creative Directors (who were the ones who have commented positively in your article) are the project manager equivalents to development teams who are not themselves the artists doing the work, and can hire and fire artists to do work as they please.I do not mean at all to oppose but to raise the question that when a tool is released, what kind of instruction manual will be released with it? What resources will be created alongside your development of these tools for those who will eventually be impacted to let them know to pivot their job functions?A lot of people complain about the rich getting richer and the poor getting poorer. If we are to continue to complain, I hope that when we create tools we are not waving it around legitimizing the greatness of it only because of the technical achievements it provides, but we are also thinking about the long term impacts.This concludes all that I would like to express on the topic.Hello. Is it possible to play around with the paintings GAN tool demonstrated in the video ? I’m not referring to GauGAN, but this specific one. Is it very curious, but I can’t find any additional information on it except this video.Hi,Last year I was playing with GauGAN and I had an idea to generate animated input maps to create an AI visualization in animated format. I made a YouTube video detailing my process and in the last week have had almost 1 million views! The output was fascinating but I had to click the load, process and save buttons 700 times to create a 30 second video. I’m excited to do more but could anyone create a method to batch process PNGs into the web client or a desktop client?Check out my video at around the 6 minute mark to really be amazed!Thanks,
Jonjonwarlick@gmail.comAssuming this proposed method becomes successful with photographic inputs, it is possible that artists may no longer generate profits from creating new subjects using their initial style. While I understand the desire to optimize profit margins at the enterprise level and to automate processes, as someone who also automates processes for companies, I am curious if the creators of this project intend to erode the meaning of “artistic style” and reduce stylistic diversity to a mere representation of art, akin to photography throughout history. If this was not the intended outcome, what steps can we take to support the artists who will inevitably be impacted?Powered by Discourse, best viewed with JavaScript enabled"
2106,analyzing-cassandra-data-using-gpus-part-2,"Originally published at:			https://developer.nvidia.com/blog/analyzing-cassandra-data-using-gpus-part-2/
Learn how GPU-accelerated analytics queries on Cassandra and sstable-to-arrow works with RAPIDS.Powered by Discourse, best viewed with JavaScript enabled"
2107,gtc-2020-next-gen-rendering-technology-at-pixar,"GTC 2020 S22266
Presenters: Max Liani,Pixar
Abstract
We’re continuing on the journey to develop RenderMan XPU, our next-generation photorealistic production rendering technology to deliver the animation and film visual effects of tomorrow. We’ll update you on our techniques leveraging heterogeneous compute across CPUs and GPUs, how we adopted the unique features offered by NVIDIA RTX hardware, and how we combine that with our own solutions to make RenderMan the path tracer of choice for so many productions.Watch this session
Join in the conversation below.The term “Sort” seems to be the wrong term to use here, leading to confusion:
The open problem discussed here is about bunching “like-things” together in time and space (sequential in memory to improve coherency, and executed together to reduce context-switching latency), countering the random nature of path tracing (random in terms of types of rays, geometries materials, etc.).
Sorting is about “order” and dependencies, not about “categories”.
In virtually all the cases discussed here, the order within each category within a pipeline-step, is irrelevant - there are no dependencies - each ray/computation is independent of the others within each category, and even between categories in a pipeline step.
Literally any instance of the term “Sorty by ” in the slides would be better substituted by the term “Group by ”.
So it’s not a “Sorting” problem of the kind that transparency is in rasterisation, it’s a “Grouping” problem, so the comparison furthers the confusion. The ordering in rasterisation is a bout “correctness”, whereas the grouping in path-tracing is about performance. The problems are not related.Powered by Discourse, best viewed with JavaScript enabled"
2108,nvidia-academic-partners-and-inception-members-present-ai-research-at-cvpr-2020,"Originally published at:			https://developer.nvidia.com/blog/nvidia-partners-present-ai-research-at-cvpr-2020/
CVPR is one of the main conferences which provide researchers and engineers with the opportunity to meet and discuss their amazing work. This year, with CVPR and other conferences going virtual, we take the opportunity to recognize our academic and Inception industry partners’ work at CVPR 2020 through this post.Powered by Discourse, best viewed with JavaScript enabled"
2109,preview-for-cuda-on-wsl-updated-for-performance,"Originally published at:			Preview for CUDA on WSL Updated for Performance | NVIDIA Technical Blog
Recently, Microsoft announced their public preview program for their Windows Subsystem for Linux (WSL) capability on Microsoft Windows platforms. NVIDIA co-released GPU-accelerated support via CUDA on WSL, where AI frameworks run as Linux executables on Microsoft Windows platforms.  We are happy to announce the release of the latest version of the preview driver with enhancements that…Powered by Discourse, best viewed with JavaScript enabled"
2110,gtc-2020-optimized-image-classification-on-the-cheap,"GTC 2020 S21598
Presenters: Meghana Ravikumar,SigOpt
Abstract
We’ll anchor on building an image classifier trained on the Stanford Cars dataset to evaluate two approaches to transfer learning — fine tuning and feature extraction — and the impact of hyperparameter optimization on these techniques. Once we define the most performant transfer-learning technique for Stanford Cars, we’ll explore Bayesian Optimization as a black-box optimization technique to tune image-transformation parameters required to augment the model, using the downstream image classifier’s performance as the guide. Drawing on a rigorous set of experimental results can help us answer the question: How can resource-constrained teams make tradeoffs between efficiency and effectiveness using pre-trained models?Watch this session
Join in the conversation below.Powered by Discourse, best viewed with JavaScript enabled"
2111,scikit-learn-tutorial-beginners-guide-to-gpu-accelerating-ml-pipelines,"Originally published at:			https://developer.nvidia.com/blog/scikit-learn-tutorial-beginners-guide-to-gpu-accelerating-ml-pipelines/
This tutorial is the fifth installment of the series of articles on the RAPIDS ecosystem. The series explores and discusses various aspects of RAPIDS that allow its users solve ETL (Extract, Transform, Load) problems, build ML (Machine Learning) and DL (Deep Learning) models, explore expansive graphs, process signal and system log, or use SQL language…Powered by Discourse, best viewed with JavaScript enabled"
2112,hands-on-lab-learn-to-build-digital-twins-for-free-with-nvidia-modulus,"Originally published at:			Physics-Informed Machine Learning with Modulus | NVIDIA
NVIDIA Modulus is now available on NVIDIA LaunchPad. Sign-up for a free, hands-on lab that will teach you how to develop physics-informed machine-learning solutions.Powered by Discourse, best viewed with JavaScript enabled"
2113,nvidia-announces-nsight-systems-2019-6-and-nsight-graphics-2019-6,"Originally published at:			NVIDIA announces Nsight Systems 2019.6 and Nsight Graphics 2019.6 | NVIDIA Technical Blog
The Nsight family of tools includes Nsight Systems, Nsight Graphics and Nsight Compute. The typical workflow for a graphics developer would start with Nsight Systems to analyze and profile at the big picture to avoid picking less efficient optimizations based on assumptions and false-positive indicators. Next, if the issue is CPU dependent they may continue…Powered by Discourse, best viewed with JavaScript enabled"
2114,clemson-university-to-install-an-nvidia-dgx-2,"Originally published at:			Clemson University to Install An NVIDIA DGX-2 | NVIDIA Technical Blog
Clemson University in South Carolina just announced they will soon receive a new NVIDIA DGX-2 supercomputer to help solve tomorrow’s greatest challenges using big data approaches. “With the addition of the DGX-2 server, the computational abilities of Clemson University will be boosted with the most advanced computing power available,” the university wrote in a blog…Powered by Discourse, best viewed with JavaScript enabled"
2115,teaching-a-computer-to-see-galaxies-in-hubble-pics,"Originally published at:			Teaching a Computer to ‘See’ Galaxies in Hubble Pics | NVIDIA Technical Blog
UK researchers are teaching computers to see and label galaxies using unsupervised machine learning. The group from the University of Hertfordshire, Hatfield, presents a novel unsupervised learning approach to automatically segment and label images in astronomical surveys. Automation of this procedure will be essential as next-generation surveys enter the petabyte scale: data volumes will exceed…Powered by Discourse, best viewed with JavaScript enabled"
2116,turn-your-selfies-into-3d-models,"Originally published at:			Turn Your Selfies Into 3D Models | NVIDIA Technical Blog
Researchers from The University of Nottingham and Kingston University developed a deep learning-based method that automatically converts two-dimensional images of faces into 3D. “3D face reconstruction is a fundamental Computer Vision problem of extraordinary difficulty,” mentioned the researchers in their paper. “Current systems often assume the availability of multiple facial images (sometimes from the same…Powered by Discourse, best viewed with JavaScript enabled"
2117,gtc-2020-accelerated-data-etl-and-analytics-implementations-and-algorithms,"GTC 2020 CWE21753
Presenters: Nikolay-Sakharnykh,NVIDIA; Jake-Hemstad, NVIDIA; David-Wendt, NVIDIA; Olivier-Lapicque, NVIDIA; Benjamin-Zaitlen, NVIDIA; Rui-Lan, NVIDIA; Mads-Kristensen, NVIDIA
Abstract
Modern data science and analytics applications have high memory bandwidth and computational demands. GPUs are well equipped for this challenge, processing large amounts of data at high speed. This session focuses on implementations and algorithms for data analytics, such as parallel joins, aggregations, and other data manipulation techniques, as well as string operations, distributed systems, and more. We have a team of experts who architect, develop, and maintain core RAPIDS libraries ready to discuss the nuts and bolts of GPU-accelerated Data ETL & Data Analytics.Watch this session
Join in the conversation below.Powered by Discourse, best viewed with JavaScript enabled"
2118,webinar-accelerate-ai-model-inference-at-scale-for-financial-services,"Originally published at:			Accelerate AI Model Inference at Scale for Financial Services
Learn how AI is transforming financial services across use cases such as fraud detection, risk prediction models, contact centers, and more.Powered by Discourse, best viewed with JavaScript enabled"
2119,facebook-ai-model-translates-between-100-languages-without-english-data,"Originally published at:			Facebook AI Model Translates Between 100 Languages Without English Data | NVIDIA Technical Blog
Facebook AI this week announced they are open sourcing a deep learning model called M2M-100 that can translate any language pair, among 100 languages, without relying on English data. For example, when translating from Chinese to French, previous models would train on Chinese to English to French. M2M-100 directly trains on Chinese to French to better preserve…Powered by Discourse, best viewed with JavaScript enabled"
2120,nature-supercharge-your-research-with-a-gpu,"Originally published at:			Nature: Supercharge your Research with a GPU | NVIDIA Technical Blog
NVIDIA GPUs power the world’s fastest supercomputer, and 20 of the 100 most powerful supercomputing clusters in the world are also powered by them too. If you follow NVIDIA closely, you are probably not surprised by this, but in a new article published in Nature this week, the leading scientific publication explains why so many…Powered by Discourse, best viewed with JavaScript enabled"
2121,ai-learns-to-play-dota-2-with-human-precision,"Originally published at:			AI Learns to Play Dota 2 with Human Precision | NVIDIA Technical Blog
Developers from the California-based non-profit OpenAI announced today that their five deep learning neural networks they call “OpenAI Five” beat amateur human teams at the popular battle arena game Dota 2. “OpenAI Five plays 180 years worth of games against itself every day, learning via self-play. It trains using a scaled-up version of Proximal Policy…Powered by Discourse, best viewed with JavaScript enabled"
2122,the-need-for-speed-edge-ai-with-nvidia-gpus-and-smartnics-part-2,"Originally published at:			https://developer.nvidia.com/blog/the-need-for-speed-edge-ai-with-nvidia-gpus-and-smartnics-part-2/
The NVIDIA Network Operator includes an RDMA Shared Device Plug-In and OFED Driver.The NVIDIA GPU Operator has NVIDIA GPU Monitoring, NVIDIA Container Runtime, NVIDIA Driver, and NVIDIA Kubernetes Device Plug-In.When deployed together, they automatically enable the GPU Direct RDMA Driver.The NVIDIA EGX Operators are part of the NVIDIA EGX stack which contain Kubernetes, Container engine and Linux Distribution. They run bare metal virtualization.Powered by Discourse, best viewed with JavaScript enabled"
2123,securing-and-accelerating-cloud-computing-platforms-with-nvidia-bluefield-2-dpus,"Originally published at:			https://developer.nvidia.com/blog/securing-and-accelerating-cloud-computing-platforms-with-bluefield-2-dpus/
Cloud technologies are increasingly taking over the worldwide IT infrastructure market. With offerings that include elastic compute, storage, and networking, cloud service providers (CSPs) allow customers to rapidly scale their IT infrastructure up and down without having to build and manage it on their own. The increasing demand for differentiated and cost-effective cloud products and…GTC 2020 Fall has come to an end, but the NVIDIA DPU journey has just begun!We’re super-excited about the recently-announced BlueField-2 DPU and DOCA SDK, and how they are set to revolutionize cloud data-centers.This post by Barbara and I is intended for IT professionals and software developers to learn how you can build high- performance and secure data-centers by taking advantage of the NVIDIA BlueField-2 DPUs.Of course, we welcome any questions you may have.Thank you!
Itay.Powered by Discourse, best viewed with JavaScript enabled"
2124,learn-more-about-reservoir-sampling-in-free-ray-tracing-gems-ii-chapter,"Originally published at:			https://developer.nvidia.com/blog/learn-more-about-reservoir-sampling-in-free-ray-tracing-gems-ii-chapter/
As the August 4 release date for Ray Tracing Gems II nears, NVIDIA is offering another free chapter from the book to celebrate. This time, the topic is reservoir sampling, which is a family of algorithms that, given a stream of N elements, randomly select a K-element subset in a single pass. Usually, K is defined as a small constant, but N need not be known in advance.Powered by Discourse, best viewed with JavaScript enabled"
2125,gtc-2020-developing-iva-software-using-nvidia-deepstream-sdk,"GTC 2020 CWE22241
Presenters: Kaustubh Purandare,NVIDIA; Prashant Gaikwad, ; Paul Shin,
Abstract
NVIDIA’s DeepStream SDK delivers a complete streaming analytics toolkit for AI-based video and image understanding, as well as multi-sensor processing. DeepStream (DS) is an integral part of NVIDIA Metropolis, the platform for building end-to-end services and solutions for transforming pixels and sensor data to actionable insights. Ask us about developing intelligent video analytics software using DS, the basics of pipeline creation and design, Python bindings using DS, optimizing the DS SDK pipeline, and DS SDK internet-of-things cases.Watch this session
Join in the conversation below.Powered by Discourse, best viewed with JavaScript enabled"
2126,getting-to-know-autonomous-vehicles,"Originally published at:			https://developer.nvidia.com/blog/getting-to-know-autonomous-vehicles/
The future is autonomous, and AI is already transforming the transportation industry. But what exactly is an autonomous vehicle and how does it work? Autonomous vehicles are born in the data center. They require a combination of sensors, high-performance hardware, software, and high-definition mapping to operate without a human at the wheel. While the concept…Powered by Discourse, best viewed with JavaScript enabled"
2127,rapids-accelerator-for-apache-spark-release-v21-08,"Originally published at:			https://developer.nvidia.com/blog/rapids-accelerator-for-apache-spark-release-v21-08/
NVIDIA Decision Support (NDS) is our adaptation of an industry-standard data science benchmark often used in the Apache Spark community. NDS consists of the same 105 SQL queries as the industry standard benchmark TPC-DS, but has modified parts for dataset generation and execution scripts.Powered by Discourse, best viewed with JavaScript enabled"
2128,ai-helps-create-new-manga-reflecting-osamu-tezuka-s-legendary-works,"Originally published at:			AI Helps Create New Manga Reflecting Osamu Tezuka’s Legendary Works | NVIDIA Technical Blog
Born in 1928 in Osaka, Japan, Osamu Tezuka is known in Japan and around the world as the “Father of Manga.” He’s also known as one of the world’s greatest illustrators, akin to Walt Disney, for his legendary comics that include Astro Boy, Princess Knight, Kimba the White Lion, Black Jack, and many more.  Tezuka…Powered by Discourse, best viewed with JavaScript enabled"
2129,develop-robotics-applications-top-5-resources-from-gtc-21,"Originally published at:			https://developer.nvidia.com/blog/develop-robotics-applications-top-5-resources-from-gtc-21/
NVIDIA Isaac is a developer toolbox for accelerating the development and deployment of AI-powered robots. The SDK includes Isaac applications, GEMs (robot capabilities), a Robot Engine, and Isaac Sim.Powered by Discourse, best viewed with JavaScript enabled"
2130,gtc-2020-nvidia-tool-to-visualize-and-interact-with-feature-maps,"GTC 2020 D2S17
Presenters: Tech Demo Team,NVIDIA
Abstract
This demo shares how you can leverage the powerful new tool named ‘NVIDIA Feature Map Explorer’ to visualize 4-dimensional image-based feature map data in a fluid and interactive fashion. The tool provides developers with a rich set of views into feature map data that range from high-level summary to low-level channel slices, as well as detailed statistics information. The visualization and statistics enable deep learning developers to easily peer into the deep learning processing “black box” to find intimate information about what the model is learning, where the model is failing to use resources efficiently, and what is changing as a model is learning during training.Watch this session
Join in the conversation below.Would anyone be willing to share a tensorflow/keras code snippet where
feature maps are dumped from a model ?I still find the vocabulary confusing, and a working example would be very
much appreciated.I guess nobody is …Powered by Discourse, best viewed with JavaScript enabled"
2131,getting-immediate-speedups-with-nvidia-a100-tf32,"Originally published at:			https://developer.nvidia.com/blog/getting-immediate-speedups-with-a100-tf32/
The NVIDIA A100 brought the biggest single-generation performance gains ever in our company’s history. These speedups are a product of architectural innovations that include Multi-Instance GPU (MIG), support for accelerated structural sparsity, and a new precision called TF32, which is the focus of this post. TF32 is a great precision to use for deep learning…NVIDIA official open source library github/nvidia/cutlass contains all the details of the tf32 data type, including storage, rounding, conversion, arithmetic operations, etc.Powered by Discourse, best viewed with JavaScript enabled"
2132,training-neural-networks-with-tensor-core,"GTC 2020 S22082
Presenters: Dusan Stosic, NVIDIA
Abstract
Mixed-precision training of deep neural networks enables faster training and reduces memory requirements, enabling the use of larger batch sizes, larger models, or larger inputs. Tensor Cores in Volta provide an order of magnitude more throughput when compared to FP32. We will first present considerations and techniques when training with reduced precision, and review results from networks of varying tasks and model architectures. Then we will discuss real-world training in mixed precision using popular deep learning frameworks. We will conclude with guidelines and recommendations on maximizing training performance with mixed precision.Watch this session
Join in the conversation below.Powered by Discourse, best viewed with JavaScript enabled"
2133,gtc-2020-scalable-speech-recognition-with-gpus-from-cloud-to-edge,"GTC 2020 S21263
Presenters: Vitaly Lavrukhin ,NVIDIA; Jocelyn Huang, NVIDIA
Abstract
We’ll present our latest automatic speech recognition models that reach a state-of-the-art accuracy while having almost 10x fewer parameters than Jasper, our previous flagship model. The small model size enables deployment on a broad spectrum of GPU-accelerated devices, from DGX servers all the way up to tiny Jetson Nano. We’ll give architecture details and training recipes in NeMo. Finally, we’ll discuss different approaches to transfer learning and domain adaptation.Watch this session
Join in the conversation below.Powered by Discourse, best viewed with JavaScript enabled"
2134,introducing-qoda-the-platform-for-hybrid-quantum-classical-computing,"Originally published at:			https://developer.nvidia.com/blog/introducing-qoda-the-platform-for-hybrid-quantum-classical-computing/
NVIDIA introduces QODA, a new platform for hybrid quantum-classical computing, enabling easy programming of integrated CPU, GPU, and QPU systems.Hi, any timeframe, when QODA will be available?Am member of NVIDIA developer program, and when I click “apply for early interest” button on QODA for Hybrid Quantum-Classical Computing | NVIDIA Developer page, it says “membership required”.  So, is there some kind of problem here, or you’re just not ready to accept applications for this program yet?  I’d like to be able at least to browse docs and read some tutorials, if available, even if SDK is not yet released.Thanks.I’m experiencing the same thing too.QODA will be available early next yearThanks for bringing this to our attention. The early access application page should now be working again with your developer login.Powered by Discourse, best viewed with JavaScript enabled"
2135,about-tesla-k80-compute-capability,"Dear developer：Hello, I ran into the problem, my graphics card is  Tesla K80 , and the official website shows that its Compute Capability is 3.7. However, I tried to use deviceQuery and found that the  compute capability is 3.5 .  This causes me to be unable to use pytorch version 1.3 or higher (compiling from source code may solve). What is the cause of this?Here is the log:Powered by Discourse, best viewed with JavaScript enabled"
2136,explainer-what-is-robotics-simulation,"Originally published at:			What Is Robotics Simulation? | NVIDIA Blog
Robotics simulation enables virtual training and programming that can use physics-based digital representations of environments, robots, machines, objects, and other assets.Powered by Discourse, best viewed with JavaScript enabled"
2137,artificial-intelligence-could-help-diagnose-tuberculosis,"Originally published at:			https://developer.nvidia.com/blog/artificial-intelligence-could-help-diagnose-tuberculosis/
Researchers from Thomas Jefferson University Hospital in Philadelphia are training deep learning models to identify tuberculosis (TB) in an effort to help patients in regions with limited access to radiologists. TB is one of the top ten causes of death worldwide with nearly two million deaths in 2016. TB can be identified on chest imaging,…Powered by Discourse, best viewed with JavaScript enabled"
2138,inference-the-next-step-in-gpu-accelerated-deep-learning,"Originally published at:			https://developer.nvidia.com/blog/inference-next-step-gpu-accelerated-deep-learning/
At 45 images/s/W, Jetson TX1 is super efficient at deep learning inference. Read the whitepaper.Deep learning is revolutionizing many areas of machine perception, with the potential to impact the everyday experience of people everywhere. On a high level, working with deep neural networks is a two-stage process: First, a neural network is trained: its parameters…Hello Michael,I’m Alice Lai from National Taipei University of Technology in Taiwan (Official website: https://www.ntut.edu.tw).We and other 3 schools in Taiwan are producing a MOOC (Massive Open Online Course) course and the content is about AI.The abstract is as below:Autonomous vehicles and drones are coming and will change our life inevitably. In this short course, you will learn the big picture of AI (deep learning specifically), and how AI is employed in autonomous vehicles and drones. Moreover, we will demonstrate how to train a self-flying drone with Virtual Reality.During this course, to make students better understand the concept of Backpropagation, we hope to use your flow chart (Figure 1)(https://devblogs.nvidia.com...)as an example. Therefore, we would like to ask for your kind authorization for the right to use the images in the videos of our course.This course is nonprofit, it will be provided to all students worldwide on Ewant (www.ewant.org) and Future Learn (www.futurelearn.com), or other online learning platforms, hope we could have your permission on this proposal.Sincerely,Alice LaiPowered by Discourse, best viewed with JavaScript enabled"
2139,nvidia-cloudxr-now-integrated-in-vmware-workspace-one-xr-hub,"Originally published at:			https://developer.nvidia.com/blog/nvidia-cloudxr-now-integrated-in-vmware-workspace-one-xr-hub/
NVIDIA and VMware are helping enterprises take XR streaming to the cloud with Workspace ONE XR Hub, which includes an NVIDIA CloudXR integration for accessing high quality XR experiences.Powered by Discourse, best viewed with JavaScript enabled"
2140,ai-powered-piano-allows-anyone-to-compose-music-by-pressing-a-few-buttons,"Originally published at:			AI-Powered Piano Allows Anyone to Compose Music by Pressing a Few Buttons | NVIDIA Technical Blog
Think of it as predictive text but for your piano. A new deep learning based system developed by Google researchers enables anyone to play the piano like a trained musician. The system dubbed Piano Genie automatically predicts the next most probable note in a song, enabling a non-musician to compose new and original music in…Powered by Discourse, best viewed with JavaScript enabled"
2141,detecting-out-of-band-malware-with-nvidia-bluefield-dpu,"Originally published at:			https://developer.nvidia.com/blog/detecting-out-of-band-malware-with-bluefield-dpu/
In an era where cyberthreats are around every corner and with increasing attacks on data centers, security has become an essential element to include in every machine guarding user data. However, many security offerings are defenseless in the presence of malware. Furthermore, software-based security consumes compute and memory resources that should be allocated to users.…We are experimenting with OOB and wireline fingerprinting. As part of the Morpheus programme, we are keen to explore further the Bluefield capability with V3.Powered by Discourse, best viewed with JavaScript enabled"
2142,nvidia-announces-availability-for-cunumeric-public-alpha,"Originally published at:			https://developer.nvidia.com/blog/nvidia-announces-availability-for-cunumeric-public-alpha/
Read how NVIDIA cuNumeric delivers accelerated computing to the NumPy ecosystem via a drop-in library replacement, scaling applications to over 1000 GPUs.Powered by Discourse, best viewed with JavaScript enabled"
2143,developing-new-materials-with-gpu-accelerated-supercomputers,"Originally published at:			https://developer.nvidia.com/blog/developing-new-materials-with-gpu-accelerated-supercomputers/
Research Area Specialist Dr. Joshua A. Anderson at University of Michigan was an early user of GPU computing technology. He began his career developing software on the first CUDA capable GPU, and now runs simulations on one of the world’s most powerful supercomputers. His “contributions to the development and dissemination of the open source, GPU-enabled…Powered by Discourse, best viewed with JavaScript enabled"
2144,gtc-2020-rtcore-for-compute-exploiting-computational-patterns-using-nvidia-rtx,"GTC 2020 S21138
Presenters: Vishal Mehta,NVIDIA
Abstract
Learn how to exploit NVIDIA RTCore and NVIDIA RTX for general-purpose compute. We’ll provide a deep dive into computational patterns that can benefit from RTCore hardware; discuss code samples in domain of HPC/machine learning that can leverage them, and also discuss important metrics to consider while optimizing.Watch this session
Join in the conversation below.Powered by Discourse, best viewed with JavaScript enabled"
2145,imperial-college-leveraging-deep-learning-in-cybathlon-s-brain-computer-interface-race,"Originally published at:			https://developer.nvidia.com/blog/imperial-college-leveraging-deep-learning-in-cybathlons-brain-computer-interface-race/
58 teams from 29 countries are participating in the world’s first ever “bionic Olympics” in Zurich, Switzerland where athletes with physical disabilities will compete side-by-side in six demanding disciplines, using the latest technologies. Researchers from Imperial College in London teamed up with volunteer athletes to compete in four events — Brain-Computer Interface (BCI) race, Powered…Powered by Discourse, best viewed with JavaScript enabled"
2146,nvidias-top-5-ai-stories-of-the-week-4-8,"Originally published at:			https://developer.nvidia.com/blog/nvidias-top-5-ai-stories-of-the-week-4-8/
Every week we highlight the top 5 stories in Artificial Intelligence using NVIDIA technology. This week’s Top 5 features an #AI trained on #Starcraft, a humanistic call center agent, and a new AI-invented sport. Watch below: 5 – Solving AI Challenges by Playing StarCraft f you follow science news, you’ve probably heard about the latest…Powered by Discourse, best viewed with JavaScript enabled"
2147,ray-tracing-tips-optimizing-the-performance-of-path-roughness-and-clearcoat-bdrf-in-ue4,"Originally published at:			https://developer.nvidia.com/blog/ray-tracing-tips-optimizing-the-performance-of-path-roughness-and-clearcoat-bdrf-in-ue4/
At SIGGRAPH2019, NVIDIA Engineers helped the development community get up-to-speed on the newest graphics techniques during an advanced real-time ray tracing tutorial. They shared their experiences integrating, debugging, and profiling ray tracing while working with commercial engines and their content pipelines. This session can be viewed in its entirety here.  Below is an excerpt from…Powered by Discourse, best viewed with JavaScript enabled"
2148,nsight-systems-exposes-new-gpu-optimization-opportunities,"Originally published at:			Nsight Systems Exposes New GPU Optimization Opportunities | NVIDIA Technical Blog
As GPU performance steadily ramps up, your application may be overdue for a tune-up to keep pace. Developers have used independent CPU profilers and GPU profilers in search of bottlenecks and optimization opportunities across their disjointed datasets for years. Using these independent tools can result in picking small optimizations based on false positive indicators or…I downloaded the Linux version run file for NVIDIA nsights system and ran the .run file. It doesn't seem to install anything, just extract. How do I run the thing? I was hoping it would be like nvvp.Powered by Discourse, best viewed with JavaScript enabled"
2149,what-kind-of-output-format-does-get3d-create,"I might have overlooked some details in the ReadMes or the research paper, but what  kind of format does Get3D output? Is it some special interchange format or raw triangle meshes? And can I use the output directly in tools like Blender, 3DSMax or Unreal Engine?Thanks!Get3D outputs triangular meshes and texture maps so it is directly usable in standard rendering engines such as Blender, 3DSMax and Unreal Engine, OmniverseThis topic was automatically closed 14 days after the last reply. New replies are no longer allowed.Powered by Discourse, best viewed with JavaScript enabled"
2150,oak-ridge-national-laboratory-scientists-develop-neural-network-to-extract-cancer-data-in-record-time,"Originally published at:			Oak Ridge National Laboratory Scientists Develop Neural Network to Extract Cancer Data in Record Time | NVIDIA Technical Blog
Every year, more than 17 million people around the world are diagnosed with cancer.  To better identify trends in cancer diagnoses and treatment responses, scientists at the Oak Ridge National Laboratory (ORNL) developed an AI-based, natural language processing tool to improve information extraction from textual pathology reports.  The work has the potential to help better…Powered by Discourse, best viewed with JavaScript enabled"
2151,building-and-deploying-conversational-ai-models-using-the-nvidia-transfer-learning-toolkit,"Originally published at:			https://developer.nvidia.com/blog/building-and-deploying-conversational-ai-models-using-the-transfer-learning-toolkit/
Conversational AI is a set of technologies enabling human-like interactions between humans and devices based on the most natural interfaces for us: speech and natural language. Systems based on conversational AI can understand commands by recognizing speech and text, translating on-the-fly between different languages, understanding our intents, and responding in a way that mimics human…Hello, for the fine tuning part of this exercise, is the data available?  When I stepped through the example I hit an error there.  Also, for the KEY bash variable,  i set this to my own user specified value, but I kept getting errors telling me I had an incorrect format.  In a different TLT example, i found the authors set key to “tlt_encode” and then I was able to make progress.Hi @dvanstee ,Generally we/NVIDIA don’t distribute data(sets) that we don’t own.  Still, for the Text Classification task, TLT  supports two public datasets out-of-the-box (SST-2 and IMBD) that you can download on your own. Then simply run dataset_convert script with a proper dataset name (please refer to TLT Text Classification user guide). Those datasets can be used for both training (from scratch) and/or fine-tuning.When it comes to models used as a starting point for fine-tuning, I guess you downloaded them from NGC. If so, then please follow the instructions regarding usage of a given model (and key in particular) provided in the associated NGC Model Card.Hope it helps,
TomPowered by Discourse, best viewed with JavaScript enabled"
2152,get-your-nvidia-rtx-games-ready-for-epic-megajam,"Originally published at:			2022 Epic MegaJam - itch.io
Create a project in Unreal Engine and submit it by September 1 for your chance to win an NVIDIA GeForce RTX 3080 GPU.Powered by Discourse, best viewed with JavaScript enabled"
2153,nvidia-at-sc15-to-describe-path-forward-for-accelerated-computing,"Originally published at:			NVIDIA at SC15 to Describe Path Forward for Accelerated Computing | NVIDIA Technical Blog
Accelerated computing is the path forward to faster discoveries and more insights in science, technology, and industry. At SC15, we will describe our vision for what’s possible today in the world of computation, machine learning and visualization to 10,000 of the world’s leading researchers and technologists attending the supercomputing conference in Austin, Texas. On Monday,…Powered by Discourse, best viewed with JavaScript enabled"
2154,oil-giant-launches-supercomputer-to-analyze-subsoil-data,"Originally published at:			https://developer.nvidia.com/blog/oil-giant-launches-supercomputer-to-analyze-subsoil-data/
The Italian multinational oil giant Eni deployed a 18.6 petaflops GPU-accelerated supercomputer, making it the most powerful industrial system in the world. Located outside Milan, the new HPC4 machine will scan for oil and gas reservoirs deep below the Earth. “This is where the company’s heart is, where we hold our most delicate data and…Powered by Discourse, best viewed with JavaScript enabled"
2155,fast-track-deploying-machine-learning-models-with-octoml-cli-and-nvidia-triton-inference-server,"Originally published at:			https://octoml.ai/blog/deploying-ml-models-with-octoml-cli-and-nvidia-triton/
Read how OctoML CLI and NVIDIA Triton automate model optimization and containerization to run models on any cloud or data center, at scale, and at much lower cost.Powered by Discourse, best viewed with JavaScript enabled"
2156,new-gpu-computing-model-for-artificial-intelligence,"Originally published at:			New GPU Computing Model for Artificial Intelligence | NVIDIA Technical Blog
Yann LeCun, Director of Facebook AI Research, invited NVIDIA CEO Jen-Hsun Huang to speak at “The Future of AI” symposium at NYU, where industry leaders discussed the state of AI and its continued advancement. Jen-Hsun published a blog on his talk that coverstopics such as how deep learning is a new software model that needs…Powered by Discourse, best viewed with JavaScript enabled"
2157,jetson-project-of-the-month-smart-social-distancing-with-ai,"Originally published at:			Jetson Project of the Month: Smart Social Distancing with AI | NVIDIA Technical Blog
Social distancing is one of the most important defenses against the spread of COVID-19. The team at Galliot was awarded the Jetson Project of the Month for their ”Smart Social Distancing with AI” application. This open-source application based on Jetson Nano helps businesses monitor social distancing practices on their premises and take corrective action in…Powered by Discourse, best viewed with JavaScript enabled"
2158,gtc-2020-improve-ml-training-performance-with-amazon-sagemaker-debugger-presented-by-amazon-web-services,"GTC 2020 S22493
Presenters: Shashank Prasanna,Amazon Web Services; Satadal Bhattacharjee, Amazon Web Services
Abstract
During ML model training, it’s challenging to ensure that models are progressively learning the correct values for different parameters and to analyze and debug model characteristics without building additional tools, making the process time-consuming and cumbersome. With Amazon SageMaker Debugger, developers can get complete insights into the training process by automating data capture and analysis from training runs without code changes. We’ll take a closer look at how you can define rules to monitor and analyze tensors and watch for issues in your model. By monitoring training flow, developers can improve GPU utilization, reduce troubleshooting time during training, and build high-quality models.Watch this session
Join in the conversation below.Powered by Discourse, best viewed with JavaScript enabled"
2159,upcoming-event-speech-ai-summit-2022,"Originally published at:			https://www.nvidia.com/en-us/events/speech-ai-summit/?nvid=nv-int-tblg-675595#cid=dl20_nv-int-tblg_en-us
Join experts from Google, Meta, NVIDIA, and more at the first annual NVIDIA Speech AI Summit. Register now!Powered by Discourse, best viewed with JavaScript enabled"
2160,deep-learning-helps-reconstruct-and-improve-optical-microscopy,"Originally published at:			Deep Learning Helps Reconstruct and Improve Optical Microscopy | NVIDIA Technical Blog
Researchers from UCLA developed a deep learning approach that could quickly produce more accurate images to aid diagnostic medicine. In two related papers, the researchers used a GTX 1080 GPU and the cuDNN-accelerated TensorFlow deep learning framework to train their models. For the first study, the engineers used CUDA and the GPU-accelerated libraries cuFFT and…Powered by Discourse, best viewed with JavaScript enabled"
2161,nvidia-steps-up-network-security-with-research-in-quantum-keys,"Originally published at:			https://developer.nvidia.com/blog/nvidia-steps-up-network-security-with-research-in-quantum-keys/
As part of NVIDIA efforts to advance research towards a more secure data center, the NVIDIA Advanced Development Goup is conducting research on quantum key distribution (QKD) technologies, along with other top organizations in Europe and in Israel. The initiatives are funded by the European Union’s Horizon 2020 program and the Israel Innovation Authority. QKD…Powered by Discourse, best viewed with JavaScript enabled"
2162,how-wolfenstein-youngblood-integrated-ray-tracing-for-next-generation-visuals,"Originally published at:			How Wolfenstein Youngblood Integrated Ray Tracing for Next-Generation Visuals | NVIDIA Technical Blog
Earlier this year at GTC Digital, engineers from MachineGames and NVIDIA presented how they leveraged the latest in ray traced reflections to create bleeding edge visuals in MachineGame’s title, Wolfenstein Youngblood. Wolfenstein Youngblood is one of the first Vulkan-based game engines to bring RTX technology to market, and their findings can help game developers bring…Powered by Discourse, best viewed with JavaScript enabled"
2163,nvidia-announces-nsight-graphics-2019-1,"Originally published at:			NVIDIA Announces Nsight Graphics 2019.1 | NVIDIA Technical Blog
NVIDIA announces Nsight Graphics 2019.1.In this release, we add a new and exciting capability – GPU crash debugging with the alpha release of Nsight Aftermath. Additionally, we have added the capability to compare traces in GPU Trace, greatly enhanced the user experience in the debugging and profiling activities, and added support for D3D12!  For full…Powered by Discourse, best viewed with JavaScript enabled"
2164,cuda-spotlight-gpu-accelerated-fdtd-simulations-for-applications-in-photonics,"Originally published at:			https://developer.nvidia.com/blog/cuda-spotlight-gpu-accelerated-fdtd-simulations-applications-photonics/
This week’s Spotlight is on Pierre Wahl, a PhD student at Vrije Universiteit Brussel. As a member of the Brussels Photonics Team (B-PHOT), he designs energy-efficient optical interconnects and works closely with the NVIDIA Application Lab at the Forschungszentrum Jülich. Pierre used CUDA to develop B-CALM, a GPU-accelerated Finite Difference Time Domain (FDTD) simulator. NVIDIA:…Powered by Discourse, best viewed with JavaScript enabled"
2165,deploying-a-scalable-object-detection-inference-pipeline-optimization-and-deployment-part-3,"Originally published at:			https://developer.nvidia.com/blog/deploying-a-scalable-object-detection-inference-pipeline-optimization-and-deployment-part-3/
This post is the third in a series on Autonomous Driving at Scale, developed with Tata Consultancy Services (TCS). The previous posts provided a general overview of deep learning inference for object detection and covered the object detection inference process and object detection metrics. In this post, we conclude with a brief look at the…Hi,
Couple of questions please.thanks
EyalPowered by Discourse, best viewed with JavaScript enabled"
2166,deep-learning-may-soon-assist-pro-football-coaches,"Originally published at:			Deep Learning May Soon Assist Pro Football Coaches | NVIDIA Technical Blog
An Oregon State University professor is training his GPU-accelerated neural network to understand and coach football. “AI will revolutionize sports,” said Alan Fern, lead researcher and computer science professor at Oregon State University. Using CUDA, Tesla K80 GPUs, and the cuDNN versions of Caffe and Torch deep learning frameworks, Fern trained his model on video…Powered by Discourse, best viewed with JavaScript enabled"
2167,control-of-prompt-based-3d-content-creation,"Hi,Does creating these 3D environment using prompt can adhere to strict constraints like scale of an obj file has to be a specific value. And can we ask the prompt use a specific obj file? How much control do we have on the domain.There are two places in the process where you can add constraints. The first is in your prompt engineering and the second is in your post-processing of the prompt results.When you craft your prompt, you could give GPT a list of acceptable responses and then ask it to only use those responses. You can see an example of this in prompts.py starting on line 32 where it says:""For each object you need to store:You can use this type of pattern to apply other constraints to any of those properties.When you get the results back from GPT you can enforce any other constraints you might want. The limit here is only what you can code in python!Thanks for your response!This topic was automatically closed 14 days after the last reply. New replies are no longer allowed.Powered by Discourse, best viewed with JavaScript enabled"
2168,conversational-ai-and-nlp-top-resources-from-gtc-21,"Originally published at:			Conversational AI and NLP: Top Resources from GTC 21 | NVIDIA Technical Blog
NVIDIA announced several major breakthroughs in conversational AI for building and deploying ASR, NLP and TTS applications.Powered by Discourse, best viewed with JavaScript enabled"
2169,automatic-mixed-precision-for-nvidia-tensor-core-architecture-in-tensorflow,"Originally published at:			Automatic Mixed Precision for NVIDIA Tensor Core Architecture in TensorFlow | NVIDIA Technical Blog
Whether to employ mixed precision to train your TensorFlow models is no longer a tough decision. NVIDIA’s Automatic Mixed Precision (AMP) feature for TensorFlow, recently announced at the 2019 GTC, features automatic mixed precision training by making all the required model and optimizer adjustments internally within TensorFlow with minimal programmer intervention. Performance increases using automatic mixed precision…I tried it and it failed. I posted in the developers forum, not sure if this is the best place. https://devtalk.nvidia.com/...If it doesn't work, then try to use dimensions of your layers divisible by 8 . In our case it helped a lot.Hi @jwitsoeI run AMP using both Python and C++.
I found that there is the difference between number of converted nodes in C++ and Python.
(no of converted nodes of Python > C++ => Python AMP run faster than C++)
Why does this happen?Thanks.Hi @nha.tuan84,Can you provide more details about your use case? What version of TensorFlow are you using, are you using AMP for training or inference? In the C++ case, are you building up the model from scratch in C++, or executing a graph saved from python? If you are training in the C++ API, have you manually implemented loss scaling? And what APIs are you using to enable AMP?Thanks.Thanks @nluehrI am rebuilding TF for xavier to check this.Powered by Discourse, best viewed with JavaScript enabled"
2170,share-your-science-accelerating-digital-rock-physics-with-gpus,"Originally published at:			Share Your Science: Accelerating Digital Rock Physics with GPUs | NVIDIA Technical Blog
James McClure, a Computational Scientist with Advanced Research Computing at Virginia Tech describes how they are using the NVIDIA Tesla GPU-accelerated Titan Supercomputer at Oak Ridge National Laboratory. Their project involves mathematical models combined with 3D visualization to provide insight on how fluids move below the surface of the earth. This can ultimately be used…Powered by Discourse, best viewed with JavaScript enabled"
2171,gtc-2020-precise-ultra-hd-map-data-as-basis-for-nvidia-drive-sim-virtual-testing-and-simulation,"GTC 2020 S22273
Presenters: Dr.-Ing. Gunnar Gräfe,3D Mapping Solutions GmbH
Abstract
Digital road data is the basis for virtual testing and simulation. The roads used for virtual testing and simulation have to be identical digital twins of the real-world roads. 3D mapping presents the technical solution for digitizing test tracks, race tracks, and public roads with high-end mobile surveying using high-resolution scanners and multiple cameras. The technology is used to produce precise high-definition reference maps in OpenDRIVE format, which are either used as basis for virtual simulation and testing or as reference map in the car for autonomous driving development. We’ll show various project examples that were completed as the basis for NVIDIA DRIVE Sim through 2019.Watch this session
Join in the conversation below.Powered by Discourse, best viewed with JavaScript enabled"
2172,nvidia-announces-cuda-x-ai-sdk,"Originally published at:			https://developer.nvidia.com/blog/nvidia-announces-cuda-x-ai-sdk/
At GTC Silicon Valley in San Jose, NVIDIA released CUDA-X AI, a collection of NVIDIA’s GPU acceleration libraries built on CUDA that accelerate deep learning, machine learning, and data analysis. CUDA-X AI includes cuDNN for accelerating deep learning primitives, cuML from RAPIDS.ai for accelerating machine learning algorithms, NVIDIA TensorRT for optimizing trained models for inference,…Powered by Discourse, best viewed with JavaScript enabled"
2173,gtc-2020-thrust-cub-and-libcu-users-forum,"GTC 2020 CWE21285
Presenters: Bryce Lelbach,NVIDIA; Michal Dominiak, ; Olivier Giroux, ; Jared Hoberock,
Abstract
Come join NVIDIA’s CUDA C++ Core Libraries team for a Q&A session on:Usage questions, feature requests, and bug reports are most welcome!Connect directly with NVIDIA Experts to get answers to all of your questions on GPU programming and code optimization, share your experience, and get guidance on how to achieve maximum performance on NVIDIA’s platform.Watch this session
Join in the conversation below.Hello All, Would appreciate pointers to set intersection examples and use cases for the NVIDIA GPU and heterogenous programming. Interested in optimizations such as all or nothing [1, 2, 3, 4, 5, 6] п [3, 4, 5, 6, 7, 8] = {} but
[2, 3, 4, 5, 6, 7] п [2, 3, 4, 5, 6, 7] = [2, 3, 4, 5, 6, 7]Powered by Discourse, best viewed with JavaScript enabled"
2174,microsoft-and-tempoquest-accelerate-wind-energy-forecasts-with-acecast,"Originally published at:			https://developer.nvidia.com/blog/microsoft-and-tempoquest-accelerate-wind-energy-forecasts-with-acecast/
Accurate weather modeling is essential for companies to properly forecast renewable energy production and plan for natural disasters. Ineffective and non-forecasted weather cost an estimated $714 billion in 2022 alone. To avoid this, companies need faster, cheaper, and more accurate weather models. In a recent GTC session, Microsoft, and TempoQuest detailed their work with NVIDIA…GPU-accelerated climate and weather simulation enables utilities to proactively plan for extreme weather events, while improving forecast accuracy of renewable energy generation. Great to see the simulation speedups and reduced costs with TempoQuest and Microsoft Azure. If you have any questions or comments, let us know.Powered by Discourse, best viewed with JavaScript enabled"
2175,jetson-agx-orin-32gb-module-now-available,"Originally published at:			Jetson AGX Orin 32GB Module Now Available | NVIDIA Technical Blog
Delivering up to 200 TOPS of AI performance, the NVIDIA Jetson AGX Orin 32GB system-on-module is available now.Hi,Is the (NVIDIA) HPC SDK supported? In other words, can we install the hpc sdk, compile and run, for example, CUDA Fortran?Thanks!!@emaill6pub HPC SDK is not supported on Jetson but many of the libraries in HPC SDK like cuBLAS, cuSPARSE, cuFFT, etc and CUDA is supported in JetPack.Powered by Discourse, best viewed with JavaScript enabled"
2176,upcoming-event-recommender-systems-summit-2022,"Originally published at:			https://developer.nvidia.com/blog/upcoming-event-recommender-systems-summit-2022/
Powered by Discourse, best viewed with JavaScript enabled"
2177,upcoming-webinar-jetpack-5-0-with-new-kernel-bootloader-trusted-os-and-support-for-jetson-agx-orin,"Originally published at:			https://info.nvidia.com/jetpack5-0-deep-dive-webinar.html
Join NVIDIA Jetson experts on Tuesday, May 10 for a webinar and Q&A about JetPack 5.0, the latest release supporting the Jetson platform.Powered by Discourse, best viewed with JavaScript enabled"
2178,ai-helps-manufacturers-identify-product-defects,"Originally published at:			AI Helps Manufacturers Identify Product Defects | NVIDIA Technical Blog
A California-based startup called Instrumental developed an intelligent AI inspection system to help manufactures identify product defects on the assembly line. The California-based startup, founded by two form Apple engineers have raised over $10 million to make it easier to manufacture electronics and head off complicated problems before they start costing companies thousands of dollars…Powered by Discourse, best viewed with JavaScript enabled"
2179,announcing-nvidia-nsight-visual-studio-code-edition-new-addition-to-the-nsight-developer-tools-suite,"Originally published at:			Announcing NVIDIA Nsight Visual Studio Code Edition: New Addition to the Nsight Developer Tools Suite | NVIDIA Technical Blog
NVIDIA Nsight Visual Studio Code Edition, an application development environment for heterogeneous platforms which brings CUDA development for GPUs into Microsoft Visual Studio Code.How I can get  NInsight integration for visual studio code?. I prefer visual studio code instead of visual studio, it is faster and very flexible to add custom snipples in compare with visual studio. I am big fan on visual studio code.Please shared.@victormtzc  – Thanks for your interest! To get the latest on the release date, I suggest watching the Introducing NVIDIA® Nsight™ Visual Studio Code Edition thread.Is there any more documentation available on setup or examples.I’m having an issue with references.Powered by Discourse, best viewed with JavaScript enabled"
2180,nvidia-vulkan-top-3-june-2019,"Originally published at:			https://developer.nvidia.com/blog/nvidia-vulkan-top-3-june-2019/
Every month, we’ll be delivering three interesting stories coming from the world of Vulkan Development.Powered by Discourse, best viewed with JavaScript enabled"
2181,gtc-2020-hugectr-high-performance-click-through-rate-estimation-training,"GTC 2020 S21455
Presenters: Minseok Lee,NVIDIA ;Zehuan Wang ,NVIDIA
Abstract
We’ll introduce HugeCTR, an open source, GPU-accelerated click-through rate training framework. CTR estimation is a large-scale learning problem that’s essential to the online advertising and content feed industry. Unlike traditional deep learning tasks, CTR training requires terabyte-sized embedding tables, massive I/O, and dynamic feature vocabulary. HugeCTR addresses all of these challenges while achieving 12-44x speedup over existing CPU and hybrid CPU/GPU solutions. HugeCTR is an end-to-end training solution with all computations performed on the GPU, leaving I/O to the CPU. The GPU hash tables support dynamic feature vocabulary. It leverages message-passing interface for multi-node training to support arbitrary large embedding sizes. It also supports mixed precision training, exploiting tensor cores on Volta and its successors.Watch this session
Join in the conversation below.Powered by Discourse, best viewed with JavaScript enabled"
2182,optimizing-warehouse-operations-with-machine-learning-on-gpus,"Originally published at:			https://developer.nvidia.com/blog/optimizing-warehouse-operations-machine-learning-gpus/
Recent advances in deep learning have enabled research and industry to master many challenges in computer vision and natural language processing that were out of  reach until just a few years ago. Yet computer vision and natural language processing represent only the tip of the iceberg of what is possible. In this article, I will demonstrate…Looks like a great source of information that would enhance someone's understanding about machine learning. At least, they can use those things on their studies and would even apply this in the future.Powered by Discourse, best viewed with JavaScript enabled"
2183,gpu-accelerated-model-reveals-details-of-nuclear-fission,"Originally published at:			GPU-Accelerated Model Reveals Details of Nuclear Fission | NVIDIA Technical Blog
Scientists from University of Washington, Warsaw University of Technology in Poland, Pacific Northwest National Laboratory, and Los Alamos National Laboratory, have developed a model that provides a detailed look at what happens during the last stages of the fission process. According to their research paper, nuclear fission has almost reached the venerable age of 80…Powered by Discourse, best viewed with JavaScript enabled"
2184,unified-memory-for-cuda-beginners,"Hello,I just tested the Prefetching method and i got an avg time of 27.682 us on a RTX 2080. I didn't expect such a jump from Pascal to Turing.Thank you for the tutorial.Hi Mark,I tried to test this code with double precision. it worked with the original code as well as the prefetching code. However, the GPU init function does not work with double precision. Even if I simply put the __global__ void init(int n, double *x, double *y) definition in the code without even calling it in main(), the code would have Segmentation Fault when running. if I change it back to  __global__ void init(int n, float *x, float *y) (again without calling it in main()), but keep all other parts of the code with double precision, it will run without a problem. Any idea what was the problem? Thanks!Can you provide a link to your modified code so I can take a quick look?The udacity link to CS344 is broken.  Is the class still available?  Thanks for the many nice write-ups.Robert, unfortunately that course is no longer offered from Udacity. However, you might be interested in the self-paced Fundamentals of Accelerated Computing with CUDAC/C++ course, which covers Unified Memory. Hope that helps!For future readers, the videos are still on Youtube (Not sure how much they reflect the actual course) at Using NVVP Part1 - Intro to Parallel Programming - YouTubeHi Mark,
It was a wonderful article about unified memory.
I would just like to have one clarification. Let’s say a kernel running on the GPU accesses a page, which is not resident in the GPU memory and so it faults. Now it tries to get the corresponding page from CPU. Lets assume that the corresponding CPU page is not available in CPU RAM ,i.e., it was swapped out to disk previously on CPU. Now, there are following two possibilities in this scenario.1. CPU swaps in the corresponding CPU page, which will be then migrated to GPU.
2. Since, the corresponding CPU page was not present in CPU RAM, the GPU request will not be satisfied.My doubt is which of these 2 possibilities occur in the above described scenario?Continuing with the previous doubt, I would also like to ask whether the CPU memory part in the unified memory will be page-locked (or pinned) or it will not be pinned memory.Pascal and post-Pascal GPUs have the capability for hardware page-faulting. I would like to ask here whether size of a page of GPU is same as the size of a page of CPU.Hello,cudaMallocManaged allocations are never swapped out to disk. They are “pinned” by the GPU driver, so OS cannot swap that memory to disk.Nikolay.Yes, Pascal+ GPUs support HW page faulting and on-demand migration. GPU supports multiple different page sizes, but we don’t document publicly what are these sizes.Hello,
My system has one CPU and one V100-32 GPU. My code uses two unified memory arrays, X and Y, which are allocated using cudaMallocManaged(). Elements of both arrays, X and Y, are only accessed in GPU kernel and not in the CPU host side. I have provided the screenshot of my code for your reference herewith.
Screenshot from 2021-11-27 13-08-51828×428 68.2 KB
The total execution time of the above program using unified memory is 54 times faster than the time required to just allocate arrays X and Y as pinned memory using cudaMallocHost() as follows:cudaMallocHost(&x, N*sizeof(float));cudaMallocHost(&y, N*sizeof(float));My doubt is that, if cudaMallocHost() as well as cudaMallocManaged(), both allocate pinned memory on the CPU host side, then why just pinned memory allocation for arrays, X and Y, using cudaMallocHost() is 54 times slower than the whole program execution using unified memory for the same arrays X and Y.SIDE NOTE- I have used clock() function to measure the execution times. Also cudaDeviceSynchronize() was used after the kernel to get correct time measurement.cudaMallocManaged call itself does not allocate any memory. It only reserves the VA, and physical backing will be allocated on first touch, depending on the accessing processor. In your example above, memory will be allocated during add kernel execution: SMs will try to access the VA, trigger faults, Unified Memory driver will process the faults, and allocate GPU memory and will use larger GPU page sizes. It you were to touch the memory first on the host, the physical backing will be allocated there and may use different page sizes.cudaMallocHost actually allocates physical memory on the CPU, so it’s more expensive than cudaMallocManaged call alone. Also, depending on your system, CPU may use smaller page sizes, so allocating and pinning memory on the CPU may take longer than doing the same on the GPU.@user34605 hope that clears things up!Hello,
Thanks for the reply. That was quite helpful. I just had one more small doubt regarding unified memory.I completely understand the point that cudaMallocManaged() only reserves the VA, and physical backing will be allocated on first touch, depending on the accessing processor.Now, lets say that I allocate one array X, whose size is equal to 10 CPU pages, using cudaMallocManaged(). And in my whole program, I access only first element of X ,i.e., X[0]. As per my understanding, CPU memory for whole array X will be allocated, when I access X[0], and not when I call cudaMallocManaged(X) (More specifically, on the first touch by CPU).Now, which of the following two conditions will hold, when I first access X[0] on CPU?The reason why I have asked this doubt is because of the following observations:The total execution time for the below program, which uses array X as unified memory and which accesses only 0th index element of X, takes 0.10 seconds.Whereas, allocating array X as pinned memory using cudaMallocHost(), as shown below, takes 11.26 seconds.cudaMallocHost(&x, N*sizeof(float));If first touch to X using program statement  - X[0] = 3.0;, page-locks (or pins) the whole CPU memory for X, then above referenced unified memory program’s execution time should have been at least 11.26 seconds.The following is not correct:As per my understanding, CPU memory for whole array X will be allocated, when I access X[0]Only the first page will be allocated when you access X[0]. The pages are allocated and migrated “on-demand”, so Unified Memory driver will allocate and page-lock only pages that you actually access.Therefore, in your example, the cudaMallocManaged + accessing x[0] performs much faster than cudaMallocHost of the whole array.Consider a hypothetical scenario, where my GPU global memory size is 4 bytes and all the four bytes are usable. We assume that GPU page size is constant and is 1 byte, i.e., we have 4 usable GPU pages.  Also, assume that GPU uses LRU(Least recently used) page replacement mechanism.I allocate two char arrays X and Y as unified memory using cudaMallocManaged(). X and Y are each of size 30 bytes.cudaMallocManaged((void ** )&X,30)
cudaMallocManaged((void ** )&X,30)Now 4 elements are initialized in GPU in the following orderX[0], Y[0], X[1], Y[1]After allocation of 4 GPU pages for these elements, GPU global memory will look like as shown below.
Now, the GPU global memory is fully occupied. If we now try to initialize X[2] in the GPU, the first page which contains X[0] should be replaced (as it was used least recently) and X[2] should occupy the first page. After this operation, GPU global memory should like as shown below
I would just like to confirm whether my understanding of GPU page replacement mechanism for unified memory arrays for above example case is correct or not.I would also like to ask whether the policy of allocating the free GPU page with the least index is used, when the GPU pages are allocated for unified memory array elements in the GPU.For example - again consider a scenario where where my GPU global memory size is 4 bytes and all the four bytes are usable. We assume that GPU page size is constant and is 1 byte, i.e., we have 4 usable GPU pages.I allocate a char array X  as unified memory using cudaMallocManaged(). X is of size 30 bytes.cudaMallocManaged((void ** )&X,30)Now, I launch a kernel K1, which initializes only X[0]. Since the free GPU page with the least index is the first GPU page,  X[0] should occupy the first GPU page. Then my global memory should look like this .
Then, I launch a kernel K2, which initializes only X[4]. Since the free GPU page with the least index now is the second GPU page,  X[4] should occupy the second GPU page.Then, my global memory should look like this.
I would just like to confirm whether my understanding of GPU page allocation policy for A UNIFIED MEMORY ARRAY is correct or not through this example.Hello @user34605,Your understanding is generally correct for the first case with X and Y arrays and how eviction works. I would clarify though that “GPU global memory” view you’re showing is some abstract view of GPU physical memory. Regarding the second case with only X array, x[4] virtual address can be possibly mapped to any free region of GPU physical memory, i.e. it’s not necessarily will be placed right after x[0], but what you’re showing is a possibility.If you’re interested to learn more about Unified Memory and have deeper discussion with experts, I would also recommend to attend GTC 2022 virtual connect with experts session we’ll be holding on “CUDA Memory Management” this year in March (similarly to last year’s CUDA Memory Management | NVIDIA On-Demand).Nikolay.Hello,
Thanks for the clarification. I would just like to clarify one more thing regarding second case.As you have said that when we access any virtual address in the GPU which is not currently mapped in the GPU, a new page can be allocated from anywhere in the free GPU memory space for this virtual address. So, is there any rule which is followed for selecting a page from the free GPU memory for the corresponding virtual address or is it selected randomly?Powered by Discourse, best viewed with JavaScript enabled"
2185,support-for-cuda-unified-memory-now-available-in-thrust,"Originally published at:			Support for CUDA Unified Memory Now Available in Thrust | NVIDIA Technical Blog
Thrust 1.12.0 is a major release providing bug fixes and performance enhancements. It includes a new thrust::universal_vector which holds data that is accessible from both host and device. This enables the use of CUDA unified memory with Thrust. Also added are new asynchronous versions of thrust::async:exclusive_scan and inclusive_scan algorithms. The synchronous versions of these have…Powered by Discourse, best viewed with JavaScript enabled"
2186,accelerating-solution-development-with-doca-on-nvidia-bluefield-dpus,"Originally published at:			Accelerating Solution Development with DOCA on NVIDIA BlueField DPUs | NVIDIA Technical Blog
DOCA is a software framework for developing applications on BlueField DPUs. By using DOCA, you can offload infrastructure workloads from the host CPU and accelerate them with the BlueField DPU. This enables an infrastructure that is software-defined yet hardware accelerated, maximizing both performance and flexibility in the data center. DOCA is here! NVIDIA first introduced…Powered by Discourse, best viewed with JavaScript enabled"
2187,diagnosing-cancer-with-deep-learning-and-gpus,"Originally published at:			Diagnosing Cancer with Deep Learning and GPUs | NVIDIA Technical Blog
Using GPU-accelerated deep learning, researchers at The Chinese University of Hong Kong pushed the boundaries of cancer image analysis in a way that could one day save physicians and patients precious time. The team used a TITAN X GPU to win the 2015 Gland Segmentation Challenge held at the Medical Image Computing and Computer conference,…Powered by Discourse, best viewed with JavaScript enabled"
2188,rtx-coffee-break-ray-traced-light-area-shadows-and-denoising-7-12-minutes,"Originally published at:			https://developer.nvidia.com/blog/rtx-coffee-break-ray-traced-light-area-shadows-and-denoising-712-minutes/
Why would you use ray tracing for shadows, instead of shadow meshes? What are the different kinds of denoisers used for ray tracing? We give you the answers.   Five Things to Remember: Why do we bother to use ray tracing to render shadows? Because ray tracing gives you better visual quality for large area…Powered by Discourse, best viewed with JavaScript enabled"
2189,improve-human-connection-in-video-conferences-with-nvidia-maxine-eye-contact,"Originally published at:			https://developer.nvidia.com/blog/improve-human-connection-in-video-conferences-with-nvidia-maxine-eye-contact/
Video conferencing is at the heart of several streaming use cases such as vlogging, vtubing, webcasting, and even video streaming for remote work. To create an increased sense of presence and pick up on both verbal and non-verbal cues video conferencing technology must enable users to see and hear clearly. Eye contact plays a key…Powered by Discourse, best viewed with JavaScript enabled"
2190,allen-institute-for-ai-announces-bert-breakthrough-passing-a-12th-grade-science-exam,"Originally published at:			Allen Institute for AI Announces BERT-Breakthrough: Passing a 12th-Grade Science Exam | NVIDIA Technical Blog
Recently the Allen Institute for Artificial Intelligence announced a breakthrough for a BERT-based model, passing a 12th-grade science test. The GPU-accelerated system called Aristo can read, learn, and reason about science, in this case emulating the decision making of students. For this milestone, Aristo answered more than 90 percent of the questions on an eighth-grade…Powered by Discourse, best viewed with JavaScript enabled"
2191,blast-from-the-past-design-a-retro-scene-in-the-new-omniverse-design-contest,"Originally published at:			https://developer.nvidia.com/blog/blast-from-the-past-design-a-retro-scene-in-the-new-omniverse-design-contest/
Recreate the past with the new NVIDIA Omniverse retro design challenge.Powered by Discourse, best viewed with JavaScript enabled"
2192,research-unveils-breakthrough-deep-learning-tool-for-understanding-neural-activity-and-movement-control,"Originally published at:			Research Unveils Breakthrough Deep Learning Tool for Understanding Neural Activity and Movement Control | NVIDIA Technical Blog
A primary goal in the field of neuroscience is understanding how the brain controls movement. By improving pose estimation, neurobiologists can more precisely quantify natural movement and in turn, better understand the neural activity that drives it. This enhances scientists’ ability to characterize animal intelligence, social interaction, and health.  Columbia University researchers recently developed a…There is nothing more rewarding than seeing how helping one of many DALI users can lead to close cooperation between open-source projects resulting in breakthroughs in neuroscience.
If you have any questions or comments, let us know.Powered by Discourse, best viewed with JavaScript enabled"
2193,faster-text-classification-with-naive-bayes-and-gpus,"Originally published at:			https://developer.nvidia.com/blog/faster-text-classification-with-naive-bayes-and-gpus/
Dealing with a sparse dataset? A technical expert’s guide on how to use Naive Bayes algorithms with GPUs to speed up the text classification process.Powered by Discourse, best viewed with JavaScript enabled"
2194,improving-the-nvidia-isaac-3d-pose-estimation-model-with-feature-map-explorer,"Originally published at:			https://developer.nvidia.com/blog/improving-the-isaac-3d-pose-estimation-model-with-feature-map-explorer/
As many would probably agree, the development of a winning deep learning model is an iterative process of fine-tuning both the model architecture and hyperparameters. The tuning is often based on insights about the network behavior gained by examining various data obtained in training. One such data is the output from each layer of a…Hello,It is mentioned in the post that evaluation metrics have been run on the simulated data for the dolly object. Is there any way I can implement the same for custom objects? Where can I find the code used for evaluation?It will be part of the next Isaac SDK release which is towards the end of next month.Powered by Discourse, best viewed with JavaScript enabled"
2195,fast-ai-breaks-imagenet-record-with-nvidia-v100-tensor-core-gpus,"Originally published at:			Fast.AI Breaks ImageNet Record with NVIDIA V100 Tensor Core GPUs | NVIDIA Technical Blog
Researchers from fast.ai announced a new speed record for training ImageNet to 93 percent accuracy in only 18 minutes. Fast.ai alumni Andrew Shaw, and Defense Innovation Unit Experimental (DIU) researcher Yaroslav Bulatov achieved the speed record using 128 NVIDIA Tesla V100 Tensor Core GPUs on the Amazon Web Services (AWS) cloud, with the fastai and…Powered by Discourse, best viewed with JavaScript enabled"
2196,jetson-project-of-the-month-ml-based-home-security-platform-mavis,"Originally published at:			https://developer.nvidia.com/blog/jetson-project-of-the-month-ml-based-home-security-platform-mavis/
A new machine learning-based security platform automatically monitors and detects people in a scene and sends captured footage to the user.Powered by Discourse, best viewed with JavaScript enabled"
2197,gtc-2020-generating-diverse-and-photorealistic-synthetic-data-for-real-world-perception-tasks,"GTC 2020 S21321
Presenters: Nikita Jaipuria,Ford Motor Company; Rohan Bhasin,  Ford Motor Company
Abstract
We’ll cover these two related topics: First, leveraging generative adversarial networks for style transfer to diversify simulated images rendered in simple domains (that is, easier to render realistically, such as daytime) into photorealistic images in different weather and lighting conditions, using domain translation models (such as day-to-night, clear-to-rainy, clear-to-snowy) learned once from generic real-world datasets; and second, investigating the role of the discriminator’s receptive field in unsupervised sim-to-real image translation. We’ll show that reducing the discriminator’s receptive field is directly proportional to improved structural coherence during translation in scenarios where the real and simulated images used for training have mismatched content — a situation often encountered in real-world deployment. Prior knowledge in computer vision and deep learning will help you get the most out of this session.Watch this session
Join in the conversation below.Powered by Discourse, best viewed with JavaScript enabled"
2198,monai-bridges-the-gap-from-innovative-research-to-clinical-production,"Originally published at:			https://developer.nvidia.com/blog/monai-bridges-the-gap-from-innovative-research-to-clinical-production/
MONAI announces new releases and a new Deployment Framework for Medical Imaging.Powered by Discourse, best viewed with JavaScript enabled"
2199,neural-machine-translation-inference-with-tensorrt-4,"Originally published at:			Neural Machine Translation Inference with TensorRT 4 | NVIDIA Technical Blog
Neural machine translation exists across a wide variety consumer applications, including web sites, road signs, generating subtitles in foreign languages, and more. TensorRT, NVIDIA’s programmable inference accelerator, helps optimize and generate runtime engines for deploying deep learning inference apps to production environments. NVIDIA released TensorRT 4 with new features to accelerate inference of neural machine…Hi, which GPU this blog used. I found that this blog said at the beginning 'Google’s Neural Machine Translation (GNMT) model  performed inference 60x faster using TensorRT on Tesla V100 GPUscompared to CPU-only platforms' but '1 GPU: Tesla P4(GP104), Driver=r384.125, CPU = E5-2690 v4@2.60GHz 3.5GHz Turbo (Broadwell) HT On, Threads=56, Sockets=2, FP32. CPU-only configuration: Skylake Gold 6140@2.30GHz 3.7GHz Turbo (Skylake); HT Off; Sockets: 2; Threads: 36, FP32' at the end of this blog.Detailed machine spec at the end of the blog corresponds to sampleNMT measurement, which was performed on a Tesla P4 GPU. GNMT performance was on a Tesla V100 GPU as you mentioned above. Hope that clarifies.Powered by Discourse, best viewed with JavaScript enabled"
2200,ai-helps-monitor-cancer-treatment,"Originally published at:			https://developer.nvidia.com/blog/ai-helps-monitor-cancer-treatment/
Measuring how tumors react to cancer treatment plays a major role in determining a patient’s outcome. The process, normally performed by trained radiologists, is labor-intensive, subjective and prone to inconsistency. To help alleviate the problem, researchers at the National Institutes of Health, the Ping An Insurance company, and a researcher presently at NVIDIA developed a…Powered by Discourse, best viewed with JavaScript enabled"
2201,share-your-science-simulating-reionization-of-the-universe-witnessing-our-own-cosmic-dawn,"Originally published at:			Share Your Science: Simulating Reionization of the Universe - Witnessing Our Own Cosmic Dawn | NVIDIA Technical Blog
Dr. Paul Shapiro, Professor at University of Texas at Austin shares how his team is using the Tesla-accelerated Titan Supercomputer at Oak Ridge National Laboratory to analyze and visualize new cosmological simulations. Watch Paul’s talk in the NVIDIA GPU Technology Theater at SC15: Watch Now Learn more about their research at [1511.00011] Cosmic Dawn (CoDa): the First Radiation-Hydrodynamics Simulation of Reionization and Galaxy Formation in the Local Universe Share your GPU-accelerated…Powered by Discourse, best viewed with JavaScript enabled"
2202,catboost-enables-fast-gradient-boosting-on-decision-trees-using-gpus,"Originally published at:			CatBoost Enables Fast Gradient Boosting on Decision Trees Using GPUs | NVIDIA Technical Blog
Machine Learning techniques are widely used today for many different tasks. Different types of data require different methods. Yandex relies on Gradient Boosting to power many of our market-leading products and services including search, music streaming, ride-hailing, self-driving cars, weather prediction, machine translation, and our intelligent assistant among others. Gradient boosting on decision trees is a form of…Powered by Discourse, best viewed with JavaScript enabled"
2203,building-medical-3d-image-segmentation-using-jupyter-notebooks-from-the-ngc-catalog,"Originally published at:			https://developer.nvidia.com/blog/building-medical-3d-image-segmentation-using-jupyter-notebooks-from-the-ngc-catalog/
The NVIDIA NGC team is hosting a webinar with live Q&A to dive into this Jupyter notebook available from the NGC catalog. Learn how to use these resources to kickstart your AI journey. Register now: NVIDIA NGC Jupyter Notebook Day: Medical Imaging Segmentation. Image segmentation partitions a digital image into multiple segments by changing the…I tried to run the demo from below link. I think below python command line argument --data_dir, should be “/data/preprocessed” instead of “/data/preprocessed_test” at “Prediction” section. Please confirm it. This demo trained with Brain Tumor 2019 data. Can I simply set Brain Tumor 2020 data to --data_dir after conversion of data by execution of dataset/preprocess_data.py? or Does it cause a problem?command: python main.py --model_dir /results --exec_mode predict --data_dir /data/preprocessed_testURL: https://developer.nvidia.com/blog/building-medical-3d-image-segmentation-using-jupyter-notebooks-from-the-ngc-catalog/I am trying to download the data set does anyone have the exact url? I went to the url given: Cancer Imaging Phenomics Toolkit (CaPTk) | CBICA | Perelman School of Medicine at the University of Pennsylvania but cannot find the data itsself.Hi @thironaka in order to have the data set you need to register in that website and ask for dataset. The format of 2020 dataset is different from the one that we used (2019). If you want to use 2020 you need to adjust the format of dataset or adjust the preprocess code to work with new format. the name of folders inside the 2020 dataset folder is not similar to 2019.
In demo and in the notebook I mentioned that after preprocessing we need to select part of data as test data so I made a folder called it preprocess-test and put the test data in it and I use this folder for testing the model not the data in preprocessed that I used for training.The dataset is not available for downloading. you need to register in the website and ask them to send you the dataset.@skouchak I have already downloaded both 2019 and 2020 datasets. As you mentioned that 2020 data was the different format. However, I could possibly convert 2020 data into .tfrecord  data format up to data number 49 by executing “preprocess_data.py” with “–single_data_dir” option, but this caused the problem at data number 50. See below error message.
Although, the rest of data after data number 50 needed to be converted. I could test the first data up to data 49 from 2020 to get segmentation. I segmented the first data and it looked different from the last one. It seemed to segmentation of the first data from 2020 was successful. Bit, I still need to confirm the segmentation data was correct or not by overlapping the segmentation mask with the original data.50/93 tfrecord files created
Traceback (most recent call last):
File “/usr/local/lib/python3.6/dist-packages/nibabel/loadsave.py”, line 42, in load
stat_result = os.stat(filename)
NotADirectoryError: [Errno 20] Not a directory: ‘/data/MICCAI_BraTS2020_TrainingData/name_mapping.csv/name_mapping.csv_t1.nii.gz’During handling of the above exception, another exception occurred:Traceback (most recent call last):
File “dataset/preprocess_data.py”, line 160, in 
main()
File “dataset/preprocess_data.py”, line 124, in main
features = load_features(folder)
File “dataset/preprocess_data.py”, line 44, in load_features
vol = load_single_nifti(os.path.join(path, name+modality)).astype(np.float32)
File “dataset/preprocess_data.py”, line 58, in load_single_nifti
data = nib.load(path).get_fdata().astype(np.int16)
File “/usr/local/lib/python3.6/dist-packages/nibabel/loadsave.py”, line 44, in load
raise FileNotFoundError(f""No such file or no access: ‘{filename}’"")
FileNotFoundError: No such file or no access: ‘/data/MICCAI_BraTS2020_TrainingData/name_mapping.csv/name_mapping.csv_t1.nii.gz’@Peita the model built and trained using 2019 dataset. I didn’t try 2020 dataset for training and testing but as long as you have the same format, same directory names the scripts should work fine. the errors that I can see here is not founding specific file which are related to the name of directories in dataset .Removing name_mapping.csv and survival_info.csv from MICCAI_BraTS2020_TrainingData, solved above “NotADirectoryError: [Errno 20] Not a directory.” I could convert all 2020 data into .tfrecord.I got below error while running: !bash examples/unet3d_train_single.sh 10 /data/preprocessed /results 2[1,0]:[ce2491b123f8:13974] Read -1, expected 16521, errno = 1
[1,0]:[ce2491b123f8:13974] Read -1, expected 13185, errno = 1
[1,0]:[ce2491b123f8:13974] Read -1, expected 9553, errno = 1
[1,0]:[ce2491b123f8:13974] Read -1, expected 19801, errno = 1@ythacker
I visit below URL. You need to scroll down to the bottom of page and register. CBICA will take for a couple of days to approval you. I think I received my login approval for 2 dyas. Then, you login, browse datasets, and click to request for download. After then, you receive a email from CBCIA with a zip file. Unzip it and open PDF file contains download links.URL: https://ipp.cbica.upenn.edu/Registration:
image737×577 86.5 KB
Awesome thank you!@ythacker no problem.having hard time install tensorflow for NGC container,
i run this command
“sudo nvidia-docker run -it --rm nvcr.io/nvidia/tensorflow:22.03-tf1-py3”
and after about hr or so of downloading various stuff, it crashes spiting out this error message.Status: Downloaded newer image for nvcr.io/nvidia/tensorflow:22.02-tf1-py3
docker: Error response from daemon: failed to create shim: OCI runtime create failed: container_linux.go:380: starting container process caused: process_linux.go:545: container init caused: Running hook #0:: error running hook: exit status 1, stdout: , stderr: nvidia-container-cli: initialization error: load library failed: libnvidia-ml.so.1: cannot open shared object file: no such file or directory: unknown.I simply don’t understand whats going wrong, any suggestion will help -thank youhi @krishnakumar . Please try docker instead of nvidia-docker and it will work fine. “docker run -it --rm nvcr.io/nvidia/tensorflow:22.03-tf1-py3”Powered by Discourse, best viewed with JavaScript enabled"
2204,in-the-trenches-at-gtc-faster-finite-elements-for-wave-propagation,"Originally published at:			https://developer.nvidia.com/blog/trenches-gtc-faster-finite-elements-wave-propagation/
By Kenneth A. Lloyd (GTC 2012 Guest Blogger) Geophysical wave propagation is really interesting because you can’t actually see the phenomenon, and you can only feel a small part of it (that is, unless you are unlucky and find yourself in the path of an earthquake). The only way we have of understanding the causes…Powered by Discourse, best viewed with JavaScript enabled"
2205,share-your-science-detecting-tumor-cells-in-blood-samples,"Originally published at:			Share Your Science: Detecting Tumor Cells in Blood Samples | NVIDIA Technical Blog
Dr. Diego Rossinelli, an ETH Zurich researcher and 2015 Gordon Bell Finalist, shares how they are relying on CUDA and Tesla GPUs to track down tumor cells that are markers of metastatic cancer.   Watch Diego’s talk “18,688 K20X’s Running after a Tumor Cell” in the NVIDIA GPU Technology Theater at SC15: Watch Now Read…Powered by Discourse, best viewed with JavaScript enabled"
2206,hierarchical-risk-parity-on-rapids-an-ml-approach-to-portfolio-allocation,"Originally published at:			https://developer.nvidia.com/blog/hierarchical-risk-parity-on-rapids-an-ml-approach-to-portfolio-allocation/
Read on step by step guide how hierarchical risk parity can used used in portfolio optimization to manage risk.Powered by Discourse, best viewed with JavaScript enabled"
2207,new-ai-based-approach-helps-identify-at-risk-patients-from-a-single-x-ray,"Originally published at:			New AI-based Approach Helps Identify At-Risk Patients from a Single X-Ray | NVIDIA Technical Blog
Massachusetts General Hospital, Brigham and Women’s Hospital, and Harvard Medical School researchers have developed a deep learning model that can predict long-term mortality from a single chest X-ray. The new findings were recently published in the JAMA Network Open journal.  The research has the potential to help identify patterns in chest radiographs not linked to…Powered by Discourse, best viewed with JavaScript enabled"
2208,dxr-spotlight-contest,"Originally published at:			DXR Spotlight contest | NVIDIA Technical Blog
Win a NVIDIA RTX TITAN GPU! Today NVIDIA announced a partnership with Microsoft and Epic Games on the DXR Spotlight contest. We’re looking for game developers and content creators to create tech demos in Unreal Engine 4.22  featuring Microsoft DirectX 12 and DirectX Raytracing for a chance to win NVIDIA RTX TITAN GPUs. Microsoft DirectX…Powered by Discourse, best viewed with JavaScript enabled"
2209,cudacasts-episode-5-install-cuda-5-5-with-a-linux-package-manager,"Originally published at:			https://developer.nvidia.com/blog/cudacasts-episode-5-install-cuda-55-linux-package-manager/
Today, CUDA 5.5 has been officially released! To continue with our CUDACasts mini-series on new CUDA 5.5 features, we will be exploring a new method for installing the CUDA platform on a supported Linux OS. In previous versions of CUDA, you would have used the run-file installer, a utility that handled installing the CUDA Toolkit,…There is an error with the video, it is supposed to be a tutorial on installing Cuda on Linux. But instead it's doing a tutorial for Windows. Please upload the Linux tutorial.Thanks for letting us know, kl!  The embedded URL was incorrect and it was showing CUDACast Episode 1. I've fixed this and you should see the correct CUDACast (Episode 5) here now.Powered by Discourse, best viewed with JavaScript enabled"
2210,new-ai-research-ranks-cities-fighting-climate-change-with-sustainable-rooftops,"Originally published at:			https://developer.nvidia.com/blog/new-ai-research-ranks-cities-fighting-climate-change-with-sustainable-rooftops/
A new study creates a deep convolutional neural network using global satellite imagery to detect sustainable roofscapes—a promising strategy for climate mitigation.Powered by Discourse, best viewed with JavaScript enabled"
2211,vulkan-1-3-broadens-cross-platform-functionality-with-developer-requested-features,"Originally published at:			https://developer.nvidia.com/blog/vulkan-1-3-broadens-cross-platform-functionality-with-developer-requested-features/
The latest release of Vulkan 1.3 simplifies render passes and makes synchronization easier.why not start this from 495/510 driver barch? 470 not include linux GBM wayland support :(greetings@jwitsoe Would this be helpful for those of us on Pascal and using Linux to game on, or do other VKD3D/DXVK tasks that require UBO and other extensions?Seems like workarounds are available for DX12 on Windows but fall short in current Linux drivers :/Hopefully attention is given to possible EXT workarounds for both Devs & End-users especially given the shortages and supply issues that have prevented many people from upgrading past Pascal. (Currently stuck on GTX 1080 myself) 🙏🏼🤞🏼Powered by Discourse, best viewed with JavaScript enabled"
2212,using-vulkan-sc-for-safety-critical-graphics-and-real-time-gpu-processing,"Originally published at:			Using Vulkan SC for Safety-Critical Graphics and Real-time GPU Processing | NVIDIA Technical Blog
Vulkan SC (Safety Critical) is a newly-released open standard to streamline the use of GPUs in markets where functional safety and hitch-free performance are essential.Many of the NVIDIA DRIVE processes as well as hardware and software components have been recently assessed or certified compliant to ISO 26262 by TÜV SÜD.
Est. reading time: 3 minutes

This blog states: "" * Development of NVIDIA DRIVE OS 6.x is in progress and will be assessed by TÜV SÜD. This follows the recent [certification of DRIVE OS 5.2]""  If possible, can you comment on the impact of Vulkan SC being supported for 6.x impacting the certification by TUV?  This is interesting since it sounds like Vulkan SC was not possible in Drive OS 5.x, but it appears that the TUV 26262 cert was performed on 5.x.   Thank you for your insights.Powered by Discourse, best viewed with JavaScript enabled"
2213,share-your-science-machine-learning-helps-fortune-500-companies-hunt-cyber-attacks-and-outages,"Originally published at:			https://developer.nvidia.com/blog/share-your-science-machine-learning-helps-fortune-500-companies-hunt-cyber-attacks-and-outages/
Leo Meyerovich, CEO of Graphistry Inc., shares how GPUs and machine learning are protecting the largest companies and organizations in the world by visually alerting them of attacks and big outages. Using NVIDIA GPUs and CUDA, the graph analysis cloud platform is able to help the company’s response and hunting team sift through 100M+ alerts…Powered by Discourse, best viewed with JavaScript enabled"
2214,nvail-partners-present-ai-research-at-iclr,"Originally published at:			https://developer.nvidia.com/blog/nvail-partners-present-ai-research-at-iclr/
This week top AI researchers are gathered in New Orleans, LA, to present their cutting edge research.Powered by Discourse, best viewed with JavaScript enabled"
2215,diagnose-heart-disease-with-ai,"Originally published at:			Diagnose Heart Disease with AI | NVIDIA Technical Blog
Stanford researchers developed a deep learning algorithm that can diagnose 14 types of heart rhythm abnormalities with cardiologist-level accuracy. They are able to identify irregular heartbeats, called arrhythmias, from sifting through hours of heart rhythm using electrocardiogram (ECG) signals generated by wearable monitors – which can be used to speed diagnosis and improve treatment for…Powered by Discourse, best viewed with JavaScript enabled"
2216,using-hybrid-physics-informed-neural-networks-for-digital-twins-in-prognosis-and-health-management,"Originally published at:			https://developer.nvidia.com/blog/using-hybrid-physics-informed-neural-networks-for-digital-twins-in-prognosis-and-health-management/
Simulations are pervasive in every domain of science and engineering, but they often have constraints such as large computational times, limited compute resources, tedious manual setup efforts, and the need for technical expertise. Neural networks not only accelerate simulations done by traditional solvers, but also simplify simulation setup and solve problems not addressable by traditional…Powered by Discourse, best viewed with JavaScript enabled"
2217,preferring-compile-time-errors-over-runtime-errors-with-vulkan-hpp,"Originally published at:			https://developer.nvidia.com/blog/preferring-compile-time-errors-over-runtime-errors-with-vulkan-hpp/
One of the most important aspects in professional software development is to detect errors as early as possible. Of course, the best case would be if we couldn’t even write erroneous code. The next best thing is errors that the compiler can detect. The worst cases are runtime errors. The hardest ones are hidden in…Great bindings, great article. Are there limitations to try to use Vulkan-Hpp with Vulkan 1.0.160?Thanks for reading the post. Glad if you like it.
Regarding the limitations on using it: of course there are issues every now and then (see https://github.com/KhronosGroup/Vulkan-Hpp/issues), but there are no principal limitations. So please feel free to give it a try!This is my very first blog on Vulkan-hpp.
If you have any questions or comments, please let us know.Powered by Discourse, best viewed with JavaScript enabled"
2218,jetson-project-of-the-month-exploring-human-robot-interactions-with-pretrained-models,"Originally published at:			Jetson Project of the Month: Exploring Human-Robot Interactions with Pretrained Models | NVIDIA Technical Blog
Learn how developers used pretrained machine learning models and the Jetson Nano 2GB to create Mariola, a robot that can mimic human actions, from arm and head movements to making faces.Powered by Discourse, best viewed with JavaScript enabled"
2219,speed-up-new-models-with-tensorrt-updates,"Originally published at:			Speed Up New models with TensorRT Updates | NVIDIA Technical Blog
At GTC Silicon Valley in San Jose, NVIDIA released the latest version of TensorRT 5.1 which includes 20+ new operators and layers, integration with Tensorflow 2.0, ONNX Runtime, and TensorRT Inference Server 1.0. TensorRT 5.1 TensorRT 5.1 includes support for 20+ new Tensorflow and ONNX operations, ability to update model weights in engines quickly, and…Powered by Discourse, best viewed with JavaScript enabled"
2220,metropolis-spotlight-sighthound-enhances-traffic-safety-with-nvidia-gpu-accelerated-ai-technologies,"Originally published at:			https://developer.nvidia.com/blog/metropolis-spotlight-sighthound-enhances-traffic-safety-with-nvidia-gpu-accelerated-ai-technologies/
Using NVIDIA pretrained models and the Jetson edge AI platform, a computer vision innovator accelerates game-changing traffic management in Denver.Powered by Discourse, best viewed with JavaScript enabled"
2221,openai-presents-gpt-3-a-175-billion-parameters-language-model,"Originally published at:			OpenAI Presents GPT-3, a 175 Billion Parameters Language Model | NVIDIA Technical Blog
OpenAI researchers recently released a paper describing the development of GPT-3, a state-of-the-art language model made up of 175 billion parameters.  For comparison, the previous version, GPT-2, was made up of 1.5 billion parameters. The largest Transformer-based language model was released by Microsoft earlier this month and is made up of 17 billion parameters.  “GPT-3…Powered by Discourse, best viewed with JavaScript enabled"
2222,ai-research-detects-glaucoma-with-94-percent-accuracy,"Originally published at:			AI Research Detects Glaucoma with 94 Percent Accuracy | NVIDIA Technical Blog
Glaucoma affects more than 2.7 million people in the U.S. and is one of the leading causes of blindness in the world. To study how deep learning can help doctors more efficiently diagnose the disease, researchers from IBM and New York University have developed a deep learning framework that automatically detects the disease with 94 percent…Powered by Discourse, best viewed with JavaScript enabled"
2223,nvidia-boosts-ai-performance-in-mlperf-v0-6,"Originally published at:			https://developer.nvidia.com/blog/nvidia-boosts-ai-performance-mlperf-0-6/
The relentless pace of innovation is most apparent in the AI domain. Researchers and developers discovering new network architectures, algorithms and optimizations, drive these innovations, with much of the work done on leading-edge NVIDIA data center platforms. Tracking this progress requires standardized performance metrics, with MLPerf representing the flagship benchmarking tool for AI. Backed by…Powered by Discourse, best viewed with JavaScript enabled"
2224,hybrid-approaches-and-systems-part-vi-of-ray-tracing-gems,"Originally published at:			Hybrid Approaches and Systems: Part VI of Ray Tracing Gems | NVIDIA Technical Blog
Learn More about Hybrid Approaches and Systems for Ray Tracing for FREE in Part VI of Ray Tracing Gems. In this installment of Ray Tracing Gems, we take a look at hybrid approaches and systems for ray tracing. Rasterization is still a critical technique to employ, alongside ray tracing.  “Ray Tracing Gems Part VI” can…Powered by Discourse, best viewed with JavaScript enabled"
2225,an-easy-introduction-to-cuda-fortran,"Originally published at:			https://developer.nvidia.com/blog/easy-introduction-cuda-fortran/
CUDA Fortran for Scientists and Engineers shows how high-performance application developers can leverage the power of GPUs using Fortran. This post is the first in a series on CUDA Fortran, which is the Fortran interface to the CUDA parallel computing platform.  If you are familiar with CUDA C, then you are already well on your…Hi dear allI am a beginner to CUDA (CUDA FORTRAN) . I have installed PGI 13.9 but the problem is, when I try to debug even a very simple CUDA Fortran code I get plenty of errors such as below:Error    1    unresolved external symbol cudaSetupArgument referenced in function mathops_saxpy_    saxp.obj     Error    2    unresolved external symbol cudaLaunch referenced in function mathops_saxpy_    saxp.obj     Error    3    unresolved external symbol __cudaRegisterFatBinary referenced in function mathops_saxpy_    saxp.obj     Error    4    unresolved external symbol __cudaRegisterFunction referenced in function mathops_saxpy_    saxp.obj     Error    5    unresolved external symbol __cudaUnregisterFatBinary referenced in function mathops_saxpy_    saxp.obj     Error    6    unresolved external symbol pgf90_dev_auto_alloc04 referenced in function MAIN_    saxp.obj     Error    7    unresolved external symbol pgf90_dev_copyin referenced in function MAIN_    saxp.obj     ....Error    12    unresolved external symbol CUDAFOR    saxp.objFurthermore, when I check the project properties I see that the CUDA FOR is not even enabled when I enable it and debug again I get the same errors. I will be appreciated if someone help me with this problem.Thanks,RezaHi Reza:I sounds like you are using the free version of PVF where CUDA Fortran is not enabled.  If so, information on how to enable CUDA Fortran can be found at https://www.pgroup.com/prod....Hi, i tried to run this script and it returned 'Max error: 2.0000'. Where this error come from? from the cudaDeviceProp in Fortran CUDA i got Device Number: 0   Device name: GeForce GTX 1060 3GB   Memory Clock Rate (KHz): 4004000   Memory Bus Width (bits): 192   Peak Memory Bandwidth (GB/s): 192.19this is work when I use pgf90 -Mcuda=cc60 -o saxpy saxpy.cuf to compileThanks! I have exact same problem with you. And it can be solved by your solution.What we really need is to discuss why the different in ""results"" depending upon the compute capability  (or compiler version)Hi, I am experiencing the same problem as you did. It seems to me that the kernel subroutine never return any value to the host. I have tried your suggestion but still not workingPowered by Discourse, best viewed with JavaScript enabled"
2226,gtc-2020-accelerating-gnmt-inference-on-gpu,"GTC 2020 S21180
Presenters: Maxim Milakov,NVIDIA; Jeremy Appleyard,NVIDIA
Abstract
Google Neural Machine Translation (GNMT) is one of the benchmarks in the MLPerf inference benchmark suite, representing Seq2Seq models. The benchmark measures throughput under latency constraints. We’ll go through the challenges that we, at NVIDIA, faced when implementing the GNMT benchmark and how we solved them with NVIDIA GPUs using the optimized and customizable TensorRT library. You’ll learn the tricks we used to optimize the GNMT model, many of which are applicable to other auto-regressive models and to DL inference on GPU in general.Watch this session
Join in the conversation below.Powered by Discourse, best viewed with JavaScript enabled"
2227,ibm-donates-a-12-million-gpu-accelerated-supercomputer-powered-by-nvidia-v100-gpus-to-mit,"Originally published at:			https://developer.nvidia.com/blog/ibm-donates-a-12-million-gpu-accelerated-supercomputer-powered-by-nvidia-v100-gpus-to-mit/
To help MIT researchers run more elaborate AI models, IBM is donating a $12 million GPU-accelerated supercomputer, powered by the latest NVIDIA V100 GPUs, to the university.  The new cluster named Satori, a Zen Buddhism term for “sudden enlightenment,” will come online this fall in conjunction with the opening of the university’s new College of Computing.  The…Powered by Discourse, best viewed with JavaScript enabled"
2228,nvidia-s-thomas-kernen-named-2020-smpte-fellow-for-acts-of-good-timing,"Originally published at:			NVIDIA’s Thomas Kernen Named 2020 SMPTE Fellow For Acts of Good Timing | NVIDIA Technical Blog
On Tuesday this week, the Society of Motion Picture and Television Engineers (SMPTE) named NVIDIA employee Thomas Kernen as one of their new Fellows for 2020. Each year, SMPTE recognizes several technical experts for outstanding contributions to the creation, management, and transmission of video for the professional broadcast industries. The new fellows are recognized at…Powered by Discourse, best viewed with JavaScript enabled"
2229,saving-earth-s-coral-reefs-with-deep-learning,"Originally published at:			https://developer.nvidia.com/blog/saving-earths-coral-reefs-with-deep-learning/
Global warming and pollution are causing severe stress to coral reefs across the world. Researchers from the University of California Berkeley and University of Queensland developed a deep learning process that automatically analyzes reef photos that will help measure reef health and changes over time. Reefs provide food and shelter for more than a quarter…Powered by Discourse, best viewed with JavaScript enabled"
2230,gtc-2020-ai-innovation-success-stories-in-retail-and-consumer-products-industries,"GTC 2020 S21682
Presenters: Alex Sabatier,NVIDIA; Paul Hendricks, NVIDIA
Abstract
AI is driving success in many areas of the retail and consumer industries. Learn more about use cases, customer references, and compelling value propositions from GPU-enabled technology. Topics include AI, computer vision, and machine learning. We’ll discuss success stories with production-grade business impacts that cultivate an innovative fast-fail approach to driving consumer engagement, brand awareness, and operational efficiency.Watch this session
Join in the conversation below.Powered by Discourse, best viewed with JavaScript enabled"
2231,accelerating-blender-python-using-cuda,"Originally published at:			https://developer.nvidia.com/blog/accelerating-blender-python-using-cuda/
Simulated or synthetic data generation is an important emerging trend in the development of AI tools. Classically, these datasets can be used to address low-data problems or edge-case scenarios that might now be present in available real-world datasets. Emerging applications for synthetic data include establishing model performance levels, quantifying the domain of applicability, and next-generation…Powered by Discourse, best viewed with JavaScript enabled"
2232,cudacasts-episode-12-programming-gpus-using-cuda-python,"Originally published at:			https://developer.nvidia.com/blog/cudacasts-episode-12-programming-gpus-cuda-python/
So far in the CUDA Python mini-series on CUDACasts, I introduced you to using the @vectorize decorator and CUDA libraries, two different methods for accelerating code using NVIDIA GPUs.  In today’s CUDACast, I’ll be demonstrating how to use the NumbaPro compiler from Continuum Analytics to write CUDA Python code which runs on the GPU. In CUDACast…Hello Mark,I really enjoy the Cudacasts. I was wondering if the script you're using is available anywhere. Copying it from the video can be difficult. Particularly, when I try to run my version of your code, I get the error that the device strides and host strides don't match at the d_next.copy_to_host command.-TPowered by Discourse, best viewed with JavaScript enabled"
2233,explainer-what-is-a-smart-city,"Originally published at:			What’s a Smart City? | NVIDIA Blogs
Examples of what a smart city is can be found in metro IoT deployments from Singapore to Seat Pleasant, Maryland.Powered by Discourse, best viewed with JavaScript enabled"
2234,nvidia-cloudxr-1-0-sdk-now-available,"Originally published at:			https://developer.nvidia.com/blog/nvidia-cloudxr-1-0-sdk-now-available/
NVIDIA announced CloudXR 1.0 software development kit, which enables streaming augmented reality, mixed reality and virtual reality content over 5G, Wi-Fi and other high-performance networks.  The NVIDIA CloudXR platform enhances immersive experiences by turning end devices such as head-mounted displays, smartphones, or tablets into a high-fidelity XR display, so users can power design reviews, speed…Powered by Discourse, best viewed with JavaScript enabled"
2235,nvidia-ar-sdk-available-now-brings-ai-to-live-streaming,"Originally published at:			NVIDIA AR SDK, Available Now, Brings AI to Live Streaming | NVIDIA Technical Blog
Technology Debuts in StreamFX, a Popular OBS Plugin The NVIDIA AR SDK opens up creative new options for live streaming, video conferencing, and gameplay. The SDK lets developers detect a face or the face landmarks, and obtain a 3D mesh of the face through a regular webcam.  Now available as an open beta, it enables…Powered by Discourse, best viewed with JavaScript enabled"
2236,optix-a-new-look-for-gpu-ray-tracing-in-film-and-design,"Originally published at:			OptiX: A New Look for GPU Ray Tracing in Film and Design | NVIDIA Technical Blog
At SIGGRAPH 2019, NVIDIA showed how GPU-accelerated ray tracing provides artists with new levels of interactivity and boosts rendering speeds for film and design. NVIDIA OptiX API is evolving rapidly, and many designers are using OptiX to get the best performance for production. In this talk, Steven Parker, Vice President of Professional Graphics at NVIDIA…Powered by Discourse, best viewed with JavaScript enabled"
2237,gtc-2020-nvidia-drive-sim-autonomous-urban-and-highway-drive-around-silicon-valley,"GTC 2020 D2S44
Presenters: Tech Demo Team,NVIDIA
Abstract
During the GTC virtual keynote, NVIDIA CEO and founder Jensen Huang demonstrated how NVIDIA DRIVE technology is currently being developed and tested in simulation. While physical testing is currently paused, the cloud-based NVIDIA DRIVE Constellation platform makes it possible to dispatch virtual vehicles in virtual worlds to progress in self-driving technology. The 17-mile route shown recreates a testing loop around San Jose, Calif., including highways 101, 87 and interstate 280, with traffic lights, on-ramps, off-ramps and merges as well as changes to the time of day, weather and traffic.Learn more about DRIVE Sim
Watch the entire GTC 2020 keynoteWatch this session
Join in the conversation below.Hi I want know about,
ROS integration, scenario recreation from collected data,ease of sensor configuration and tools for test automation?Powered by Discourse, best viewed with JavaScript enabled"
2238,accelerate-recommender-systems-with-gpus,"Originally published at:			https://developer.nvidia.com/blog/accelerate-recommender-systems-with-gpus/
Wei Tan, Research Staff Member at IBM T. J. Watson Research Center shares how IBM is using NVIDIA GPUs to accelerate recommender systems, which use ratings or user behavior to recommend new products, items or content to users. Recommender systems are important in applications such as recommending products on retail sites, recommending movies or music on…I am just slightly disappointed our work did not get this kind of attention http://dx.doi.org/10.1109/B...I looked at the code and am wondering how to choose the optimal number for iterations of conjugate gradient. I see that the default is 6. Is there any experimentation or guidance for how many is best?It would seem to me that a CG iteration that gives a relative reduction in the residual that is much less than the first reduction for the next 'side' is likely to get wiped out. Therefore it might make sense to stop CG iterations when the fractional reduction in residual norm is less than that of the last one done for the other side.On the other hand, I'm guessing that it is more computationally-efficient to run multiple iterations on the same matrix than switching between them.Also, minor edit, I think: ""where n_{x_u} and n_{\theta_v} denote the number of total ratings on user u and item v, respectively.""Thanks!Thanks for the comment John and apologize for the late reply.As for CG iteration, we tested on three data sets (netflix, yahoomusic and hugewiki) and found that <=10 iterations is good enough. I am not very sure what do you mean ""a CG iteration that gives a relative reduction in the residual that is much less than the first reduction for the next 'side' is likely to get wiped out."" Could you explain more?""I'm guessing that it is more computationally-efficient to run multiple iterations on the same matrix than switching between them."" -- solving one matrix at a time is not able to saturate GPU so we run batch solvers. At the end of the day, CG kernel is memory bandwidth bounded and computation is very light (vector inner product and vector times scalar).Thanks for identifying the typo!WeiPowered by Discourse, best viewed with JavaScript enabled"
2239,hacking-ansel-to-slash-vr-rendering-times,"Originally published at:			Hacking Ansel to Slash VR Rendering Times | NVIDIA Technical Blog
Warrior9 VR team members started working on The PhoenIX – a sci-fi animated series in virtual reality (VR) — two years ago. The tools and technology used to create CG imagery have rapidly evolved during those two years, and our production process needed to evolve with it. VR rendering techniques can often be complex and…nice hack, although im pretty sure this macro approach was solved already here - https://www.youtube.com/wat...Powered by Discourse, best viewed with JavaScript enabled"
2240,gpu-accelerated-json-data-processing-with-rapids,"Originally published at:			https://developer.nvidia.com/blog/gpu-accelerated-json-data-processing-with-rapids/
JSON is a widely adopted format for text-based information working interoperably between systems, most commonly in web applications. While the JSON format is human-readable, it is complex to process with data science and data engineering tools. To bridge that gap, RAPIDS cuDF provides a GPU-accelerated JSON reader (cudf.read_json) that is efficient and robust for many…Powered by Discourse, best viewed with JavaScript enabled"
2241,real-time-ray-tracing-s-rapid-evolution,"Originally published at:			Real-Time Ray Tracing’s Rapid Evolution | NVIDIA Technical Blog
In March 2018, Epic Games, ILMxLAB, and NVIDIA unveiled “Reflections”, a Star Wars real-time ray tracing cinematic demo running on a $60,000 DGX Workstation. Just one year later, that same photoreal demo can be run on a laptop equipped with a single NVIDIA RTX GPU. “The visual quality of that demo came from ray tracing;…Powered by Discourse, best viewed with JavaScript enabled"
2242,best-approach-for-text-to-3d-scene-assembly,"I was trying out ChatGPT to generate scene assembly based on the NVIDIA demo app, but it would often get the height wrong. E.g. putting a TV on a TV stand if I understand the code, it would find a TV stand from the assets then add a TV, but the “add TV” step did not use the height of the TV stand - everything ended up at height zero.Are there best practices or a better way to generate a scene description so the geometry of objects put into a scene are taken into account?There are a few ways to achieve this, and include prompt engineering, result control and pre-object disposition checks. In the demo AI Room Generator we just take the XYZ position of objects and don’t provide specific instructions on how the Y axis should behave. You can be more strict in your prompt engineering about it, for example you could try adding specific behaviour requirements like “If you are placing a tabletop object or an object that usually is on a wall or on top of furniture, make sure the Y axis value is not 0”. Also you can provide a few examples of desired result, for example provide an example where a TV is placed on top of a TV stand. Once you receive the final results, you can also run your check to make sure that the desired objects are correctly placed with the right offsets ahead of placing them in the scene.This topic was automatically closed 14 days after the last reply. New replies are no longer allowed.Powered by Discourse, best viewed with JavaScript enabled"
2243,accelerating-conversational-ai-research-with-new-cutting-edge-neural-networks-and-features-from-nemo-1-0,"Originally published at:			https://developer.nvidia.com/blog/accelerating-conversational-ai-research-with-new-cutting-edge-neural-networks-and-features-from-nemo-1-0/
NVIDIA NeMo is a conversational AI toolkit built for researchers working on automatic speech recognition (ASR), natural language processing (NLP), and text-to-speech synthesis (TTS). The primary objective of NeMo is to help researchers from industry and academia to reuse prior work (code and pretrained models and make it easier to create new conversational AI models.…it doesnt support nvidia Jetson embedded devices, does it?Nowhere in the new User guide does it mention the platforms it does or does not run under…One would expect it to be under Prerequisites.  Since there has not been any new Jetson downloads in many months don’t hold your breath!@Andrey1984 and @Danieljyu2n, we currently don’t have official support of Nemo for Jetson embedded devices.yes, it works somehow but it seems devs need to do support themselves to get it on armGood news and nice job Andrey…Did you need to do anything special that you want to share.  If you say it “works” and I will take that with a grain of salt, then I will give it a go myself and report back my findings. It may help the Jetson team.@Danieljyu2n
You may like to check NeMo installation - done; performance to be evaluated
However, for Jetson there is jetson-voice container at NGC that is poorly supported for recent Jetson versions though, rather not supported at all, but it is kind of based on not-supported at at all for Jetsons Jarvis that is a production version of NeMo that doesn’t supportt officially Jetson arch either.Powered by Discourse, best viewed with JavaScript enabled"
2244,inception-spotlight-synvivia-developing-a-protein-switch-for-covid-19,"Originally published at:			https://developer.nvidia.com/blog/inception-spotlight-synvivia-developing-a-protein-switch-for-covid-19/
This week at GTC 2020, synthetic biology startup Synvivia showcased protein switches being developed to control engineered organisms and aid in drug discovery for COVID-19. The full session is available in the GTC catalog to view on-demand. Synvivia uses GPU-accelerated molecular dynamics simulations to design protein molecule interactions and was able to observe potential additive…Powered by Discourse, best viewed with JavaScript enabled"
2245,what-is-limiting-your-rendering-performance-using-nsight-graphics-gpu-trace-and-the-peak-performance-percentage-method,"Originally published at:			What is Limiting Your Rendering Performance? Using ‘Nsight Graphics: GPU Trace’ and the Peak-Performance-Percentage Method | NVIDIA Technical Blog
Game development is complicated, and even the most mature pipelines can hit snags that will bring performance to a crawl. ‘Nsight Graphics: GPU Trace’ helps developers identify GPU inefficiencies as they crop up, taking the guesswork out of the process. NVIDIA’s Louis Bavoil provides a useful tip to consider when using Nsight Graphics: GPU Trace…Powered by Discourse, best viewed with JavaScript enabled"
2246,problem-with-motion-vectors-with-nrd,"We are developing a full ray tracing engine for VR, using SDKs like DLSS, RTXDI and NRD. By using our own variable resolution technique, we have been able to render two eyes of 3k x 3k pixels at 90Hz using two RTX 3090 GPUs. Our goal is to eventually get these same results on a single RTX 4090. This technique however, causes some problems with NRD.What we do:When shooting the primary rays we transform them non linearly so that areas we want to render at lower resolution are compressed. We adjust our motion vectors so that they correctly represent the non-linear pixel movement and then we compute diffuse/specular using RTXDI. At the end of the pipeline, we upscale the image while transforming it back to a linear view; stretching the lower resolution areas back. The current use for this technique is a form of fixed foveated rendering. In the future we’ll want to extend this into dynamic foveated rendering.The problem:To make this technique work with NRD we tried using the compensated motion vectors, this works perfect except for one thing; The specular reflections are not correct when moving the camera. A part of NRD does its own form of temporal reprojection without considering our motion vectors, which causes visual errors since it does not account for the non-linear transformation. We’ve tried modifying NRD ourselves, but we did not succeed in doing so.Do you have suggestions how to cope with this?And would it be possible for future versions of NRD to have a user-defined shader function that allows for non-linear transformations to be applied?This is a neat use case that you should definitely talk to the NRD team about. They should be able to help you (better than I can) to get your motion vectors playing nice with the denoiser(s) in the short term.I don’t know specifics about RTXDI’s feature roadmap, but you can ask the RTXDI team about non-linear transform support by posting in the forum Raytracing - NVIDIA Developer ForumsOk, I will do. How can I contact the NRD team?
Thx.Hi mechamania, we can just copy this post into the Forum category Adam suggested and I will add one of the NRD team to the thread.Ok, Thx a lot!Powered by Discourse, best viewed with JavaScript enabled"
2247,nih-develops-an-ai-to-help-with-cervical-cancer-screening,"Originally published at:			NIH Develops an AI to Help With Cervical Cancer Screening | NVIDIA Technical Blog
According to the National Institutes of Health (NIH), cervical cancer is third most common cancer among women and the second most frequent cause of cancer-related death, with 80% of cervical cancer cases occurring in developing nations with limited access to cervical cancer screening. To help alleviate the problem, researchers from the NIH and intellectual Ventures Global…Powered by Discourse, best viewed with JavaScript enabled"
2248,real-time-natural-language-understanding-with-bert-using-tensorrt,"Originally published at:			Real-Time Natural Language Understanding with BERT Using TensorRT | NVIDIA Technical Blog
Large scale language models (LSLMs) such as BERT, GPT-2, and XL-Net have brought about exciting leaps in state-of-the-art accuracy for many natural language understanding (NLU) tasks. Since its release in Oct 2018, BERT1 (Bidirectional Encoder Representations from Transformers) remains one of the most popular language models and still delivers state of the art accuracy at…Great work! Does this also work on other GPUs like V100 and K80?Also, what if I have a PyTorch model?I got an error when pulling the docker container:root@ubuntu-gpu-7-200gb:/home/ubuntu/TensorRT/demo/BERT# sh python/create_docker_container.shSending build context to Docker daemon  265.7kBStep 1/17 : FROM nvcr.io/nvidia/tensorrt:19....19.05-py3: Pulling from nvidia/tensorrt7e6591854262: Pulling fs layer089d60cb4e0a: Pull complete9c461696bc09: Pull complete45085432511a: Pull complete6ca460804a89: Pull complete2631f04ebf64: Pull complete86f56e03e071: Pull complete234646620160: Downloading [====================================>              ]  447.9MB/615.2MB7f717cd17058: Download completee69a2ba99832: Download completebc9bca17b13c: Download complete1870788e477f: Download complete603e0d586945: Downloading [=============================================>     ]  452.2MB/492.7MB717dfedf079c: Download complete1035ef613bc7: Download completec5bd7559c3ad: Download completed82c679b8708: Download complete059d4f560014: Download completef3f14cff44df: Download complete96502bde320c: Download completebc5bb9379810: Download completee4d8bb046bc2: Download complete4e2187010a7c: Download complete9d62684b94c3: Download completee70e61e48991: Download completeadecb91612fe: Download completeba27dafb70e8: Download complete16bde716c9b2: Download complete476faeed0740: Download complete5af7c8a6b101: Download complete960591fee98d: Download complete0dd138c184ff: Download complete7ef953567062: Downloadingbd9a54f5a193: Waiting144852c40661: Waiting171a26eec2d4: Waiting999acb71c4df: Waiting3f301e4ba386: Waiting3fc30e0f9cba: Waiting38d1459042f4: Waitingaafa1a9d16eb: Waitingunauthorized: authentication requiredUnable to find image 'bert-tensorrt:latest' locallydocker: Error response from daemon: pull access denied for bert-tensorrt, repository does not exist or may require 'docker login': denied: requested access                                                     to the resource is denied.See 'docker run --help'.root@ubuntu-gpu-7-200gb:/home/ubuntu/TensorRT/demo/BERT#Any ideas ?good one so helpfulThe instructions include:python python/bert_builder.py -m /workspace/models/fine-tuned/bert_tf_v2_base_fp16_384_v2/model.ckpt-8144 -o bert_base_384.engine -b 1 -s 384 -c /workspace/models/fine-tuned/bert_tf_v2_base_fp16_384_v2however, the defaults appear to be configured to work with BERT large. The following change allows the steps to all complete without error:+++ b/demo/BERT/python/build_examples.sh@@ -16,9 +16,9 @@ # Setup default parameters (if no command-line parameters given)-MODEL='large'+MODEL='base' FT_PRECISION='fp16'-SEQ_LEN='128'+SEQ_LEN='384' SCRIPT=$(readlink -f ""$0"") SCRIPT_DIR=$(dirname ${SCRIPT})Great work.  I ran into the following problem, running the fourth step above:FileNotFoundError: [Errno 2] No such file or directory: '/workspace/models/fine-tuned/bert_tf_v2_base_fp16_384_v2/bert_config.json'Ted.  Hi !  It seemed you got past my issue if ""/workspace/.."" directory not being found.  How did you get past that?OK, I solved my own problem.   It works great now!I had 2 issues.  1) The example script downloads a different model, so you might need to adjust it 2) It can take a while to create the ""engine"" file - at least it did for me :)I solved it by downloading the right file and fixing the example.get your API key from ngc.nvidia.com and then try `docker login nvcr.io`.Hi,Really nice work done guys. In the explanation it is stated that the input and output of the fully connected layers is B x S x (N * H). However i have the PyTorch implementation of BERT from NVIDIA and it seems that the input and output of the Fully connected layers is just B x S x H. Below is a part of output of print(model).(encoder): BertEncoder(      (layer): ModuleList(        (0): BertLayer(          (attention): BertAttention(            (self): BertSelfAttention(              (query): Linear(in_features=1024, out_features=1024, bias=True)              (key): Linear(in_features=1024, out_features=1024, bias=True)              (value): Linear(in_features=1024, out_features=1024, bias=True)              (dropout): Dropout(p=0.1, inplace=False)              (softmax): Softmax(dim=-1)            )Also the BERT config file is .  1 {  2   attention_probs_dropout_prob: 0.1,  3   hidden_act: gelu,  4   hidden_dropout_prob: 0.1,  5   hidden_size: 1024,  6   initializer_range: 0.02,  7   intermediate_size: 4096,  8   max_position_embeddings: 512,  9   num_attention_heads: 16, 10   num_hidden_layers: 24, 11   type_vocab_size: 2, 12   vocab_size: 30522 13 }When run the “cd TensorRT/demo/BERT && sh python/build_examples.sh” got the problem: ""Error: ‘nvidia/bert_tf_v2_base_fp16_384:2’ could not be found. ""And on the ngc I didn’t find the model with this name. AI Models - Computer Vision, Conversational AI, and More | NVIDIA NGCAny one can  help? Where can I download the fine tuned weight for t his now?  Thanks in advance.Powered by Discourse, best viewed with JavaScript enabled"
2249,upcoming-webinar-how-to-integrate-isaac-sim-with-ros,"Originally published at:			Isaac Sim series
Join this webinar on January 26 and learn how to integrate Isaac Sim into your ROS workflows to support robotics apps including navigation, manipulation, and more.Powered by Discourse, best viewed with JavaScript enabled"
2250,new-gan-can-lipread-and-synthesize-speech,"Originally published at:			New GAN Can Lipread and Synthesize Speech | NVIDIA Technical Blog
Current audio speech recognition models normally do not perform well in noisy environments. To help solve the problem, researchers from Samsung and Imperial College in London developed a deep learning solution that uses computer vision for visual speech recognition.  The model is capable of lipreading, as well as synthesizing audio it sees from the video.  …Powered by Discourse, best viewed with JavaScript enabled"
2251,upcoming-event-nvidia-at-acm-recsys-2022,"Originally published at:			RecSys22: ACM Conference on Recommender Systems 2022 | NVIDIA
Join NVIDIA at the 16th annual ACM Conference on Recommender Systems to see how recommender systems are driving our future.Powered by Discourse, best viewed with JavaScript enabled"
2252,gtc-2020-high-performance-distributed-deep-learning-a-beginners-guide,"GTC 2020 S21546
Presenters: Ammar Ahmad Awan,Ohio State University; Dhabaleswar K (DK) Panda, Ohio State University; Hari Subramoni, Ohio State University
Abstract
Learn the current wave of advances in AI and HPC technologies to improve the performance of deep neural network training on NVIDIA GPUs. We’ll discuss many exciting challenges and opportunities for HPC and AI researchers. Several modern DL frameworks (Caffe, TensorFlow, CNTK, PyTorch, and others) that offer ease-of-use and flexibility to describe, train, and deploy various types of DNN architectures have emerged. We’ll provide an overview of interesting trends in DL frameworks from an architectural/performance standpoint. Most DL frameworks have utilized a single GPU to accelerate the performance of DNN training/inference. However, approaches to parallelize training are being actively explored. We’ll highlight new challenges for message-passing interface runtimes to efficiently support DNN training, and how efficient communication primitives in MVAPICH2-GDR can support scalable DNN training. Finally, we’ll discuss how we scale training of ResNet-50 using TensorFlow to 1,536 GPUs for MVAPICH2-GDR.Watch this session
Join in the conversation below.Powered by Discourse, best viewed with JavaScript enabled"
2253,emulating-an-nvidia-jetson-orin-nx-using-the-nvidia-jetson-agx-orin-developer-kit,"Originally published at:			https://developer.nvidia.com/blog/emulating-an-nvidia-jetson-orin-nx-using-the-nvidia-jetson-agx-orin-developer-kit/
Discover how to use the NVIDIA Jetson AGX Orin Developer Kit to emulate natively any of the NVIDIA Jetson Orin modules, including Jetson Orin NX and Jetson Orin Nano.Powered by Discourse, best viewed with JavaScript enabled"
2254,simplifying-hpc-workflows-with-nvidia-ngc-container-environment-modules,"Originally published at:			Simplifying HPC Workflows with NVIDIA NGC Container Environment Modules | NVIDIA Technical Blog
Many system administrators use environment modules to manage software deployments. The advantages of environment modules are that they allow you to load and unload software configurations dynamically in a clean fashion, providing end users with the best experience when it comes to customizing a specific configuration for each application. However, robustly supporting HPC and deep…Hi,I tried to set the NGC_IMAGE_DIR variable to path that had all .sif file from NGC container. However, when I run singularity with just file name it try to look at .sif file in home directory or working directory. Do I need to explicitly append the $NGC_IMAGE_DIR to sif file ?ThanksVijayHi Vijay,The NGC container environment modules expect the SIF files in $NGC_IMAGE_DIR to have specific names.You didn’t mention a particular NGC container.  For instance, the tensorflow/20.06-tf2-py3 module expects the SIF file to be named nvcr.io_nvidia_tensorflow:20.06-tf2-py3.sif.  (These are the file names you would get if you used to NGC Container Replicator tool to download the container image files.)Alternatively, you can customize the environment modules for your site to use a different SIF file name.  The relevant line, again for the TensorFlow container, is
local image = ""nvcr.io_nvidia_tensorflow:20.06-tf2-py3.sif""Thanks,
ScottPowered by Discourse, best viewed with JavaScript enabled"
2255,democratizing-and-accelerating-genome-sequencing-analysis-with-nvidia-clara-parabricks-v4-0,"Originally published at:			https://developer.nvidia.com/blog/democratizing-and-accelerating-genome-sequencing-analysis-with-clara-parabricks-v4-0/
Learn about NVIDIA Clara Parabricks v4.0, which brings significant improvements to how genomic researchers and bioinformaticians deploy and scale genome sequencing analysis pipelines.Hello, Parabricks 3.2 had a RNA pipeline. This is not present in the new Parabricsk 4.0 version. Any suggestions on how to analyze RNA data for variant identification with the 4.0 version? Thanks!3.2:
https://docs.nvidia.com/clara/parabricks/v3.2/text/rna_pipeline.htmlPowered by Discourse, best viewed with JavaScript enabled"
2256,3d-real-time-video-hair-coloration,"Originally published at:			3D Real-Time Video Hair Coloration | NVIDIA Technical Blog
Augmented Reality technology startup ModiFace unveiled their deep learning-based live video hair tracking and hair color simulation that is ideal for the beauty industry. “We have been working on deep learning architectures for a long time now, and recent advances in both the neural network architectures, basic hardware level optimizations, as well as the availability…Powered by Discourse, best viewed with JavaScript enabled"
2257,nvidia-delivers-high-quality-xr-streaming-on-microsoft-azure,"Originally published at:			NVIDIA Delivers High-Quality XR Streaming on Microsoft Azure | NVIDIA Technical Blog
The NVIDIA CloudXR platform will be available on NVIDIA GPU-powered virtual machine instances on Azure.Powered by Discourse, best viewed with JavaScript enabled"
2258,simplify-the-deployment-of-hpc-applications-with-nvidia-gpu-cloud,"Originally published at:			Simplify the Deployment of HPC Applications with NVIDIA GPU Cloud | NVIDIA Technical Blog
NVIDIA announced the availability of HPC application and visualization containers on NVIDIA GPU Cloud (NGC). Together, these offerings make NGC a single source for researchers seeking access to deep learning frameworks, HPC applications, and visualization tools essential for their scientific workflows. Deep Learning NGC features all the top deep learning frameworks tuned, tested, and certified…Powered by Discourse, best viewed with JavaScript enabled"
2259,facebook-s-ai-model-outmatches-competitors-in-poker,"Originally published at:			Facebook’s AI Model Outmatches Competitors in Poker | NVIDIA Technical Blog
Facebook researchers developed a reinforcement learning model that can outmatch human competitors in heads-up, no-limit Texas hold’em, and turn endgame hold’em poker.  At the heart of the model is how software-agents handle perfect-information games such as chess, versus imperfect-information games like poker.  Instead of just deciding on its next move, a reinforcement learning software agent…Powered by Discourse, best viewed with JavaScript enabled"
2260,training-your-nvidia-jetbot-to-avoid-collisions-using-nvidia-isaac-sim,"Originally published at:			https://developer.nvidia.com/blog/training-your-nvidia-jetbot-to-avoid-collisions-using-nvidia-isaac-sim/
Collecting a variety of data is important for AI model generalization. A good dataset consists of objects with different perspectives, backgrounds, colors, and sometimes obstructed views. The model should learn how to handle outliers or unseen scenarios. This makes the data collection and labeling process hard. In this post, we showcase sim2real capabilities of NVIDIA…
image1842×1727 265 KB

Hi Expert,
I am learning to use Isaac SIM . from this blog, I found no DR selection in Isaac SIM version 2022.2.0, could you help to teach me how to do this data generation like author  described in this blog?  thanks.Allan (ahou@nvidia.com)Powered by Discourse, best viewed with JavaScript enabled"
2261,solving-automatic-speech-recognition-deployment-challenges,"Originally published at:			Solving Automatic Speech Recognition Deployment Challenges | NVIDIA Technical Blog
Top speech recognition challenges developers face when building and deploying an ASR application and how to solve them using a speech AI-optimized workflow.Powered by Discourse, best viewed with JavaScript enabled"
2262,share-your-science-microsoft-developing-applications-for-the-visually-impaired,"Originally published at:			Share Your Science: Microsoft Developing Applications for the Visually Impaired | NVIDIA Technical Blog
Ken Tran, Senior Research Engineer at Microsoft Research shares how they are using deep learning to create applications for people who are blind or visually impaired. Using Tesla M40 and TITAN X GPUs with the cuDNN-accelerated Caffe deep learning framework, they have trained their language model to describe images or scenes in natural language. The…Powered by Discourse, best viewed with JavaScript enabled"
2263,securing-and-accelerating-end-to-end-ai-workflows-with-the-ngc-private-registry,"Originally published at:			Securing and Accelerating End-to-End AI Workflows with the NGC Private Registry | NVIDIA Technical Blog
AI is moving from research to production and enterprises are embracing its power to develop and deploy it across a wide variety of applications, including retail analytics, medical imaging, autonomous driving, and smart manufacturing.  However, developing and deploying open source AI software inherently has its own challenges. Data scientists need optimized software, developers need the…Powered by Discourse, best viewed with JavaScript enabled"
2264,top-recommender-system-sessions-at-nvidia-gtc-2023,"Originally published at:			Restaurants & Quick-Service Conference Session Catalog | NVIDIA GTC
Get training, insights, and access to experts for the latest in recommender systems.Powered by Discourse, best viewed with JavaScript enabled"
2265,cuda-x-accelerated-dgl-containers-for-large-graph-neural-networks,"Originally published at:			https://developer.nvidia.com/blog/cuda-x-accelerated-dgl-containers-for-large-graph-neural-networks/
NVIDIA partnered with the DGL team to provide containers with the latest DGL, PyTorch, and SE(3)-Transformer for GPU-accelerated performance optimization.It was fun collaborating with the team in creating this post. I’m looking forward to seeing DGL containers being downloaded and in great use.Powered by Discourse, best viewed with JavaScript enabled"
2266,using-sketches-to-search-for-products-online,"Originally published at:			https://developer.nvidia.com/blog/using-sketches-to-search-for-products-online/
Sometimes drawing a quick sketch is easier than using text to describe a particular product you have in mind. Researchers at Queen Mary University of London created a tool that recognizes hand-drawn sketches on a smartphone or tablet and uses them to search for products online. Using two Tesla K80 GPUs and the cuDNN-accelerated Caffe…Powered by Discourse, best viewed with JavaScript enabled"
2267,ai-model-matches-radiologists-accuracy-identifying-breast-cancer-in-mris,"Originally published at:			https://developer.nvidia.com/blog/ai-model-matches-radiologists-accuracy-identifying-breast-cancer-in-mris/
Researchers from NYU Langone Health aim to improve breast cancer diagnostics with a new AI model. Recently published in Science Translational Medicine, the study outlines a deep learning framework that predicts breast cancer from MRIs as accurately as board-certified radiologists. The research could help create a foundational framework for implementing AI-based cancer diagnostic models in…Powered by Discourse, best viewed with JavaScript enabled"
2268,accelerating-space-exploration-with-ai-at-fdl,"Originally published at:			Accelerating Space Exploration with AI at FDL | NVIDIA Technical Blog
By Alison Lowndes With the world focused on the 50th Anniversary of Man’s first footsteps on the Moon, the periphery is filled with preparation and innovation for the launch of an entirely “New Space” industry, which will land the planet’s first Woman onto the Moon’s surface by 2024. Named after Apollo’s twin sister, the Greek…Powered by Discourse, best viewed with JavaScript enabled"
2269,gtc-2020-ai-ml-with-vgpu-on-openstack-or-rhv-using-kubernetes,"GTC 2020 S22106
Presenters: Erwan Gallen,Red Hat; Michael Xiaomin Shen,NVIDIA; Martin Tessun,Red Hat
Abstract
By sharing GPU resources for AI/ML, you can better utilize on-premises hardware and gain flexibility without moving sensitive workflows into the cloud. Learn how Red Hat Openstack Platform and Red Hat Virtualization are bringing agility to AI/ML accelerated workloads with vGPU. We’ll describe how Red Hat contributes to new vGPU capabilities like SR-IOV and live migration support. This makes setting up AI/ML within a Kubernetes system even simpler and more reliable, as the workloads can be migrated and SR-IOV functionality boosts the performance and usability of the vGPU device.Watch this session
Join in the conversation below.Powered by Discourse, best viewed with JavaScript enabled"
2270,latest-releases-and-resources-feb-3-10,"Originally published at:			https://developer.nvidia.com/blog/latest-releases-and-resources-feb-3-10/
Sharpen your conversational AI, vehicle routing, or CUDA Python skills; learn how Metropolis boosts go-to-market efforts; find solutions for AI inference deployment.Powered by Discourse, best viewed with JavaScript enabled"
2271,video-series-practical-real-time-ray-tracing-with-rtx,"Originally published at:			Video Series: Practical Real-Time Ray Tracing With RTX | NVIDIA Technical Blog
RTX introduces an exciting and fundamental shift in the way lighting systems work in games and applications. In this video series, NVIDIA Engineers Martin-Karl Lefrancois and Pascal Gautron help you get started with real-time ray tracing. You’ll learn how data and rendering is managed, how acceleration structures and shaders work, and what new components are…RTX: Awesomeness. Looking forward to apply my CUDA knowledge to this new tech.Small misleading mistake in the last video, for shadow rays, given that we're only interested to know if there's ANY intersection happening : I guess we should use anyHit shader (and abort further intersections, I think there's an intrinsic for that?) instead of closestHit shader.Powered by Discourse, best viewed with JavaScript enabled"
2272,nvidia-to-host-digital-gtc-in-october,"Originally published at:			NVIDIA to Host Digital GTC in October | NVIDIA Technical Blog
NVIDIA will host its GPU Technology Conference, running Oct. 5-9, featuring a recorded keynote by CEO and founder Jensen Huang. GTC will feature the latest innovations in AI, data science, graphics, high-performance and edge computing, networking, autonomous machines and VR for a broad range of industries. Seven separate programming streams will run across North America, Europe,…Powered by Discourse, best viewed with JavaScript enabled"
2273,reducing-application-build-times-using-cuda-c-compilation-aids,"Originally published at:			https://developer.nvidia.com/blog/reducing-application-build-times-using-cuda-c-compilation-aids/
This technical walkthrough on the CUDA C++ compiler toolchain complements the programming guide and provides a broad overview of new features being introduced in the CUDA 11.5 toolkit release.Hello authors,First, thank you for recognizing the importance of compilation times, particularly dynamic compilation times. This can be quite critical in systems whose computational workloads change depending on user input.Second, some general feedback:When you compile anything in NVRTC, you guys are pulling in this huge header, __nv_nvrtc_builtin_header.h . It has more than 140000 lines! It must have an atrocious effect on compilation time. You really must make it optional; and probably break it up into multiple include files. Let us decide whether we want any of that at all, or just parts of it, or none of it.I hope you’re working on this issue for OpenCL compilation as well as for CUDA.You guys should open-source libnvrtc, and probably the CUDA driver. The best thing in terms of allowing customization and maximizing performance is to make that transparent and manipulable by us. NVIDIA is a hardware company, don’t distribute so much closed-source software.Now for some section-specific feedback:NVRTC concurrent compilationYou write:Some of these stages are not thread-safeWhy? There’s no reason they shouldn’t be. Now, sure, it’s much better that you’ve broken the faux critical section into 3 critical sections, but why not just make it thread-safe?PTX concurrency compilationYou write:PTX compilation … proceeds through multiple internal phases. The previous implementation … the PTX compiler used a global lock to serialize concurrent compilations … In CUDA 11.5 and the R495 driver, the PTX compiler implementation now uses finer-grained local locksAgain, why should it lock anything?Eliminating unused kernelsNeat :-)Pragma diagnostic controlCan you please put up, and maintain, a complete list of all of the warnings/errors supported by NVCC and by NVRTC, and their numbers?Powered by Discourse, best viewed with JavaScript enabled"
2274,cuda-team-will-be-live-at-9am,"Bring your questions about CUDA 12Powered by Discourse, best viewed with JavaScript enabled"
2275,announcing-the-winners-of-the-dxr-spotlight-contest,"Originally published at:			https://developer.nvidia.com/blog/announcing-the-winners-of-the-dxr-spotlight-contest/
When combined with Microsoft DirectX® Raytracing and NVIDIA RTX GPUs, Unreal Engine 4’s ray tracing tools allow individual artists to deliver AAA results. Each of our finalists are single-person teams, but you’d never know it by looking at their demos.Powered by Discourse, best viewed with JavaScript enabled"
2276,how-to-speed-up-deep-learning-inference-using-tensorrt,"Originally published at:			https://developer.nvidia.com/blog/speed-up-inference-tensorrt/
Looking for more? Check out the hands-on DLI training course: Optimization and Deployment of TensorFlow Models with TensorRT The new version of this post, Speeding Up Deep Learning Inference Using TensorRT, has been updated to start from a PyTorch model instead of the ONNX model, upgrade the sample application to use TensorRT 7, and replaces…Thanks for this extremely informative post - I am attempting to replicate some of the numbers for throughput for inference attained here: https://developer.nvidia.co...This example targets a Resnet-50, but the performance, even when enabling the mode for FP16, seems to not match the latencies/throughputs reported there. How should I be building the model differently?Got errors while attempting make inside TensorRT-introduction/Solved them by adding <math.h> and <numeric> in sampleOnnx*.cppPlease keep in mind that this blog post sample is oriented towards new users. It does not include all possible optimizations. You might achieve better results with our existing benchmark tool ""trtexec"". It's  possible to further optimize the code e.g. by using CUDA Graphs, but I think the scope of such optimizations is beyond this post.Reference: https://docs.nvidia.com/dee...I think I ended up figuring this out. Initially, I got better results by increasing the workspace size (to around 14GB), which seemed to increase the compile time and generate more tactics options. Looking at what trtexec did, I stopped measuring the copy overhead of copying real inputs to the GPU (comment out the cudaMemCpy lines in simpleOnnx), and that got me closer to the correct number. This seems to be what is meant by results on a ""synthetic"" dataset.However, we realized that it turns out that the example is actually running a different version of Resnet-50! The example asks you to download Resnet-50v2, but TensorRT seems to be much better optimized for earlier versions of Resnet-50. I tested on release 1.1 at https://github.com/onnx/mod.... Curiously, workspace size seems to have no impact on this older version of Resnet-50 at all, so decreasing to 1GB produces the same benchmark.@disqus_mD6AGHAfPt:disqus , is there a guide to these further optimizations? I would like to get the maximum throughput in a real, non-synthetic use-case, but the memcpy for batch size 41 seems to add almost 3ms to the latency, landing a pretty heavy impact on throughput. How can I learn to make this come down even further?@ankmathur96:disqus Did you try to overlap copy with compute operations? Here is an inspirational presentation by Stephen Jones:http://on-demand.gputechcon...Got the same error. The script might need to be fixed.I was so confused that why you have // Read input tensor from ONNX file    if (readTensor(inputFiles, inputTensor) != inputTensor.size())    {        cout << ""Couldn't read input Tensor"" << endl; return 1; }until I realize that the input files you are using have only one sample in each pb file.Hi,Thanks very much for the example sharing. But I got error when I attempted to compile the code on the TensorRT 5:""simpleOnnx_1.cpp:54:16: error: ‘IParser’ is not a member of ‘nvonnxparser’ ""Is this caused by the different version of TensorRT? Could you tell me how to fix it?Thanks again.Where can I get the CMakeLists.txt files to debug this code?I follow the guide and when I run make in TensorRT-introduction, it shows:g++ -std=c++11 -Wall -I/usr/local/cuda/include   -c -o ioHelper.o ioHelper.cppIn file included from ioHelper.cpp:33:0:/usr/local/include/onnx/onnx_pb.h:52:26: fatal error: onnx/onnx.pb.h: No such file or directorycompilation terminated.<builtin>: recipe for target 'ioHelper.o' failedmake: *** [ioHelper.o] Error 1do you have any suggestion? I only find onnx_ml.pb.h, this maybe the problem of onnx.Please check  onnx installation log for errors. This might be a problem with protobuf compiler; missing file onnx.pb.h is generated using protoc.Sample comes with Makefile, which can be tweaked for debugging flags. Could you please elaborate on the issue?This is likely due to using older TensorRT version. This sample should work with TensorRT 5.0 or newer.thansks reply, I solved it by add  -DONNX_ML in makefile, CXXFLAGS=-std=c++11 -Wall -I$(CUDA_INSTALL_DIR)/include -DONNX_MLHello,I’ve a code in MXNET which I exported to ONNX, then from ONNX imported to TensorRT.I’m using onnx-tensorrt(https://github.com/onnx/onn... in order to run the inference.I’ve got an output after usingtrt_outputs = common.do_inference(context, bindings=bindings, inputs=inputs, outputs=outputs, stream=stream)Also I’ve got an output when I do a forward pass in my MXNET (on that output I find the bboxes values for the face).Question: How can I convert the TensorRT’s inference output to match the MXNET’s inference output so I can classify the faces with the bboxes?Also, maybe I don’t look at the right place and I need to ignore MXNET’s output and interpret ONNX’s output and use that instead? (I also verified that ONNX has the same output)This comment doesn't tie in directly with the topic of the post. You might try asking your question in the Deep Learning section of the NVIDIA Developer Talk Forum.Hello,Is it possible to batch inputs in Python API?Yes.Powered by Discourse, best viewed with JavaScript enabled"
2277,nvidia-engineers-apply-ai-advances-in-engineering-physics-problems,"Originally published at:			https://developer.nvidia.com/blog/nvidia-engineers-apply-ai-advances-in-fluid-mechanics/
An article in the latest edition of Science magazine, Hidden fluid mechanics: Learning velocity and pressure fields from flow visualizations, describes an emerging branch of fluid dynamics at the intersection of scientific computing and deep learning. It starts with imaging data and discovers the underlying physics and/or properties of the fluid using the partial differential equations…Powered by Discourse, best viewed with JavaScript enabled"
2278,mit-researchers-use-ai-to-capture-silent-speech,"Originally published at:			https://developer.nvidia.com/blog/mit-researchers-use-ai-to-capture-silent-speech/
From personal assistant applications to helping people with disabilities speak, voice and speech recognition is one of the most researched areas in AI. Last week, researchers from MIT announced a breakthrough, a deep learning based wearable device that can transcribe words people internally verbalize but do not actually speak out loud. “Our idea was: Could…Powered by Discourse, best viewed with JavaScript enabled"
2279,introducing-the-clara-agx-high-performance-ai-development-kit-for-medical-devices,"Originally published at:			Introducing the Clara AGX High-Performance AI Development Kit for Medical Devices | NVIDIA Technical Blog
This combination of hardware and software creates a unique platform that allows medical device manufacturers, software developers, and the medical research community to bring AI advancements to medical instruments more easily.Do you know of any benchmarks for running guppy_basecaller on data generated by the MinION sequencer from Oxford Nanopore Technology on the Clara AGX platform? Thanks.Powered by Discourse, best viewed with JavaScript enabled"
2280,jetson-project-of-the-month-automate-your-household-duties-with-nvidia-jetson-and-deepstack,"Originally published at:			https://developer.nvidia.com/blog/jetson-project-of-the-month-automate-duties-jetson-deepstack/
The Jetson Project of the Month simplifies home automation projects using a combination of DeepStack and Home Assistant along with the NVIDIA Jetson.Powered by Discourse, best viewed with JavaScript enabled"
2281,nvidia-tensorrt-inference-server-available-now,"Originally published at:			NVIDIA TensorRT Inference Server Available Now | NVIDIA Technical Blog
The NVIDIA TensorRT inference server GA version is now available for download in a container from the NVIDIA GPU Cloud container registry. Announced at GTC Japan and part of the NVIDIA TensorRT Hyperscale Inference Platform, the TensorRT inference server is a containerized microservice for data center production deployments. As more and more applications leverage AI,…Powered by Discourse, best viewed with JavaScript enabled"
2282,share-your-science-extracting-information-from-images,"Originally published at:			Share Your Science: Extracting Information from Images | NVIDIA Technical Blog
Anton van den Hengel, director of The University of Adelaide’s Australian Centre for Visual Technologies shares how his research group is working on Visual Question Answering (VQA) which uses deep learning to understand the contents of an image. “The data has been around for a while, but really the GPU technology coming in and allowing…Powered by Discourse, best viewed with JavaScript enabled"
2283,end-to-end-ai-for-nvidia-based-pcs-cuda-and-tensorrt-execution-providers-in-onnx-runtime,"Originally published at:			https://developer.nvidia.com/blog/end-to-end-ai-for-nvidia-based-pcs-cuda-and-tensorrt-execution-providers-in-onnx-runtime/
This post is the fourth in a series about optimizing end-to-end AI. The last post described the higher-level idea behind ONNX and ONNX Runtime. As explained in the previous post in the End-to-End AI for NVIDIA-Based PCs series, there are multiple execution providers (EPs) in ONNX Runtime that enable the use of hardware-specific features or optimizations…Thanks for the great blog post. Assuming a previously generated TRT engine, will ONNX with a TensorRT EP achieve the same runtime performance as running the engine directly through TensorRT APIs? In other words, is there any performance penalty to use TensorRT through ONNX runtime?If your engine is not split up by ONNX Runtime the performance should be the same. Essentially if an ONNX file is not able to compile to a single engine ONNXRuntime will slice up the network and fallback to CUDA Execution provider for unsupported ops.
There are a few things to watch out for:Powered by Discourse, best viewed with JavaScript enabled"
2284,cuda-pro-tip-understand-fat-binaries-and-jit-caching,"Originally published at:			https://developer.nvidia.com/blog/cuda-pro-tip-understand-fat-binaries-jit-caching/
As NVIDIA GPUs evolve to support new features, the instruction set architecture naturally changes. Because applications must run on multiple generations of GPUs, the NVIDIA compiler tool chain supports compiling for multiple architectures in the same application executable or library. CUDA also relies on the PTX virtual GPU ISA to provide forward compatibility, so that already…Another potential problem: when a 2-parameter kernel function is in cache, and you're developing a 3-parameter version of this function, if your test suite still calls the obsolete 2-parameter function, it still works even it should not.My Pro Tip: disable CUDA cache during the development process, and enable it in production only.Powered by Discourse, best viewed with JavaScript enabled"
2285,c-11-in-cuda-variadic-templates,"Originally published at:			https://developer.nvidia.com/blog/cplusplus-11-in-cuda-variadic-templates/
CUDA 7 adds C++11 feature support to nvcc, the CUDA C++ compiler. This means that you can use C++11 features not only in your host code compiled with nvcc, but also in device code. In my post “The Power of C++11 in CUDA 7” I covered some of the major new features of C++11, such…How about OpenCL 2.1 support ?Useful examples Mark! I had written a C pre-processor variadic macro for something similar as your first cudaLaunch example. It works fine. Variadic Kernels look very useful. I believe this could help simplify heterogenous programming and portability. Let's read quite a few more times your posting ... It seems the clearest exposition about it ... One question: is the CUDA toolkit updated with what you tell us? More examples? After a quick search, I only found E.2.9.8. __global__ functions and function templates and PTX ISA stuff.""is the CUDA toolkit updated with what you tell us""?  Not sure what you mean. As the post says this is supported in CUDA 7, and the documentation link in the post links to the restrictions on variadic __global__ function template parameters. So I think the answer is ""yes"".I would like to also mention perfect forwarding of arguments, as no discussion of function templates in C++11 is complete without it: http://eli.thegreenplace.ne...How does the compiler resolve lambda functions in CUDA 7.0? Are they inline or is it really a function call?I need this information in order to estimate the performance. When all lambda functions are resolved as real function calls then this will affect very negative the performance.Hi Peter,The compiler uses the same strategy to inline lambdas that it uses for any other function call. Where the lambda is called directly the inliner will try to inline as usual. But if the lambda is stored into an instance of nvstd::function (from the ""nvfunctional"" header), and the compiler is unable to figure out the underlying function at the call site, it will not be inlined.Powered by Discourse, best viewed with JavaScript enabled"
2286,create-speech-ai-applications-in-multiple-languages-and-customize-text-to-speech-with-riva,"Originally published at:			https://developer.nvidia.com/blog/create-speech-ai-applications-in-multiple-languages-and-customize-text-to-speech-with-riva/
This month, NVIDIA Riva released world-class speech-to-text in Spanish, German, and Russian, empowering enterprises to deploy speech AI applications globally.When will you have speech-to-text in Asian languages like
Mandarin
Japanese
Korean
VietnameseHello, we appreciate your interest :)
Though we cannot comment on our roadmap, the Riva team plans to add Asian languages in the coming months.
Stay tuned: we’ll share more as they become available.Thank youhello i would like to know if there a possibility to have french language for TTS ??Hi Raied,  Thank you for the interest! We are planning to add French. Stay tined :)Powered by Discourse, best viewed with JavaScript enabled"
2287,new-dli-course-will-show-you-how-to-build-containerized-hpc-applications-using-hpccm,"Originally published at:			New DLI Course Will Show You How to Build Containerized HPC Applications Using HPCCM | NVIDIA Technical Blog
To build advanced high-performance computing (HPC) applications efficiently, it’s critical to leverage HPC container technologies like Docker and Singularity to access many application components – libraries, software stacks, and more – in a way that is portable and easy to reproduce.  In the new NVIDIA Deep Learning Institute (DLI) online course on High-Performance Computing with…Powered by Discourse, best viewed with JavaScript enabled"
2288,ai-helps-detect-irregular-heartbeats,"Originally published at:			AI Helps Detect Irregular Heartbeats | NVIDIA Technical Blog
According to the Centers for Disease Control, an estimated 2.7 to 6.1 million people in the U.S. have Atrial Fibrillation, the most common type of an irregular heartbeat. To help distinguish between harmless rhythm irregularities and a life-threatening condition, researchers from Stanford University, UCSF, and Rhythm Technologies developed a deep learning-based system that can automatically…Powered by Discourse, best viewed with JavaScript enabled"
2289,10-ways-cuda-6-5-improves-performance-and-productivity,"Cool story bro!The static libraries are currently only available on Linux and Mac OS. They have names such as libcublas_static.a.  The Windows toolkit currently only has dynamic libraries.Dear Mark,If I use Unified Memory Programming in Cuda C version 6.5, program as follows:char *s;cudaMallocManaged(&s, size);1. Will the total number of bytes for size limited by RAM size of my PC or RAM size of the graphics card that contains the GPU?2. Originally, my program use CPU commands like malloc() to allocate RAM that's used by my program, in order to such RAM locations to be accessed by GPU in additional to CPU access, should I just change the malloc to cudaMallocManaged(), such that CPU can still make use of the original code to access that part of RAM. Will the change of this command affect the availability of memory resource in GPU?Thanks.Regards, Simon1. Limited by the GPU memory size on current implementation.2. If you never will touch the data on the GPU, stick with malloc(). If you never touch the data on the CPU, stick with cudaMallocManaged or cudaMalloc.  If you share the data between GPU and CPU, use cudaMallocManaged.Unified Memory allocations with cudaMallocManaged() do use GPU memory -- the allocation is initially owned by the GPU.ThanksHow can I integrate Cuda 6.5 in Visual Studio 2013 using the Visual C++ Compiler Nov 2013 CTP (CTP_Nov2013) tool set?Thanks,A BI have installed Cuda 5.5 and then upgraded to Cuda 6.5 in my PC. As I am using Visual Studio 2008, I need to use back Cuda 5.5. However, I found that after I installed Cuda 6.5 in my PC, the Nsight pull down manual and corresponding debug function disappeared when I open Visual Studio 2008. How can I restore back the pull down manual and the debug functions?After you uninstalled the 6.5 toolkit, did you rerun the 5.5 toolkit installer?  Doing so may restore NSight VSE.just try to download CUDA 6.5.14 on my MacPro...true the system preference on the mac but keep saying it fail..running  OSX 10.9.4...is this not support on this version of OSX 10.9.4 yet?Hi Dara -- thanks very much for pointing this out.  There was a problem with our content delivery network that hosts the files, which was causing these failures. This should be fixed now; can you try again?  Thanks!Thanks Mark! Just download and install perfectly.Hi Mark, as stated in other forum CUDA 6.5 apparently understands C++11 even it is not documented. When can we expect CUDA with full C++11 support which will be officially supported?Thank you, PeterHi Mark, as stated in other forum CUDA 6.5 apparently understands C++11 even it is not documented. When can we expect CUDA with full C++11 support which will be officially supported?Thank you, PeterI have a Quadro K1000M on my laptop.  Will Cuda 6.5 work with this card?Yes!I'm not sure I fully understand the question.  I installed Visual Studio 2013, and then installed CUDA 6.5, and integration is configured by the CUDA TK installer automatically.  This should work for you also.There is a question：How did it install NVIDIA diaplay drive 340.62 for my GF 320M display card? The drive only for Tesla platforms, Look forward to your reply，Thanks。http://www.nvidia.com/driversCUDA 7 supports C++11 in device code. http://devblogs.nvidia.com/...I run a cuda program to generate random numbers in linux,I have included ""curand.h"" in the program,but why it tells me ""undefined reference to `curandCreateGenerator'""?thanks.Powered by Discourse, best viewed with JavaScript enabled"
2290,nvidia-deep-learning-institute-instructor-led-training-now-available-remotely,"Originally published at:			NVIDIA Deep Learning Institute Instructor-Led Training Now Available Remotely | NVIDIA Technical Blog
Starting this month, NVIDIA’s Deep Learning Institute is offering instructor-led workshops that are delivered remotely via a virtual classroom. DLI provides hands-on training in AI, accelerated computing and accelerated data science to help developers, data scientists and other professionals solve their most challenging problems. These in-depth classes are taught by experts in their respective fields, delivering industry-leading…Powered by Discourse, best viewed with JavaScript enabled"
2291,optimizing-the-deep-learning-recommendation-model-on-nvidia-gpus,"Originally published at:			Optimizing the Deep Learning Recommendation Model on NVIDIA GPUs | NVIDIA Technical Blog
Recommender systems help people find what they’re looking for among an exponentially growing number of options. They are a critical component for driving user engagement on many online platforms. With the rapid growth in scale of industry datasets, deep learning (DL) recommender models, which capitalize on large amounts of training data, have started to show…Powered by Discourse, best viewed with JavaScript enabled"
2292,powering-ultra-high-speed-frame-rates-in-ai-medical-devices-with-the-nvidia-clara-holoscan-sdk,"Originally published at:			https://developer.nvidia.com/blog/powering-ultra-high-speed-frame-rates-in-ai-medical-devices-with-clara-holoscan-sdk/
NVIDIA Clara Holoscan SDK 0.3 now provides a lightning-fast frame rate of 240 Hz for 4K video, enabling the next generation of medical devices.Powered by Discourse, best viewed with JavaScript enabled"
2293,harnessing-the-caffe-framework-for-deep-visualization,"Originally published at:			https://developer.nvidia.com/blog/harnessing-caffe-framework-deep-visualization/
Jeff Clune, Lab Director, Evolving Intelligence Laboratory at The University of Wyoming. The need to train their deep neural networks as fast as possible led the Evolving Artificial Intelligence Laboratory at the University of Wyoming to harness the power of NVIDIA Tesla GPUs starting in 2012 to accelerate their research. “The speedups GPUs provide for…Powered by Discourse, best viewed with JavaScript enabled"
2294,nvidia-merlin-accelerates-recommender-workflows-with-4-release,"Originally published at:			https://developer.nvidia.com/blog/nvidia-merlin-accelerates-recommender-workflows-with-4-release/
Relevant recommenders have the potential to impact millions of human decisions each day and build trust. Today, data scientists and machine learning engineers responsible for building relevant and impactful recommenders face challenges including slow pipelines, large embedding tables exceeding memory, and maintaining high throughput while maintaining low latency. These challenges are not inconsequential and can…Powered by Discourse, best viewed with JavaScript enabled"
2295,prototyping-faster-with-the-newest-udf-enhancements-in-the-nvidia-cudf-api,"Originally published at:			https://developer.nvidia.com/blog/prototyping-faster-using-udfs-and-new-cudf-features/
This post highlights helpful new cuDF features that allow you to think about a single row of data and write code faster.Powered by Discourse, best viewed with JavaScript enabled"
2296,usb-capture-device-errors,"I recently bought a Jetson Nano, in hopes it could improve performance from the Raspberry Pi 3B + board. I am trying to run a script that runs two processes, one that is a continuous loop reading audio from a capture device attached to the jetson nano, and the other a continuous loop reading video from a capture device:
Amazon.ca.There seems to be trouble with the video process. I am using JetPack 4.2 and I have followed the guide located at:How to configure your NVIDIA Jetson Nano for Computer Vision and Deep Learning - PyImageSearch.In order to initialize a video stream i am using the command cv2.videoCapture(-1, cv2.v4l_CAP). When running audio and video each individually (and not together) they work fine, but together they work fine for a few minutes as expected, and then execution comes to a hault, giving a select timout error, as well as a “VIDIOC_DQBUF: Resource temporarily unavailable” error.  After a bit more digging I noticed that the videoCapture process seems to spike the CPU usage when executed without the audio portion to sometimes 300%.I have tried initializing the video stream using a gstreamer pipeline :‘v4l2src device=/dev/video{} ! video/x-raw, width=(int){}, height=(int){}, framerate=(fraction){}/1 ! videoconvert !  video/x-raw, format=(string)BGR ! appsink’.format(dev, width, height, fps),but this led to the same CPU usage spike.I figure this is a driver problem or i am not initializing the video stream optimally, can anyone shed light on this issue, or give me any advice on how to optimize the video stream and reduce the cpu usage?Powered by Discourse, best viewed with JavaScript enabled"
2297,icymi-nvidia-tensorrt-and-triton-in-healthcare,"Originally published at:			https://developer.nvidia.com/blog/icymi-nvidia-tensorrt-and-triton-in-healthcare/
In this update, we look at the ways NVIDIA TensorRT and the Triton Inference Server can help your business deploy high-performance models with resilience at scale.Powered by Discourse, best viewed with JavaScript enabled"
2298,reinforcement-learning-algorithm-helps-train-thousands-of-robots-simultaneously,"Originally published at:			Reinforcement Learning Algorithm Helps Train Thousands of Robots Simultaneously | NVIDIA Technical Blog
In recent years, model-free deep reinforcement learning algorithms have produced groundbreaking results. However, the current algorithms require a large number of training samples as well as an enormous amount of computing power to achieve the desired results. To help make training more accessible, a team of researchers from NVIDIA developed a GPU-accelerated reinforcement learning simulator…Powered by Discourse, best viewed with JavaScript enabled"
2299,boosting-application-performance-with-gpu-memory-prefetching,"Originally published at:			https://developer.nvidia.com/blog/boosting-application-performance-with-gpu-memory-prefetching/
This CUDA post examines the effectiveness of methods to hide memory latency using explicit prefetching.The work described in this post is derived from a real application in computational finance. Please feel free to ask questions about any details that may be unclear.Hello, normally how to decide PDIST per application to hide the memory latency?Thanks for the question. It is difficult to derive an analytical expression for the proper value of PDIST, because it depends, among others, on the occupancy of the Streaming Multiprocessors (SMs), which in turn is a function of the number of registers used per thread, and the total amount of shared memory used by the kernel, as well as the memory latency. The easiest strategy would be to vary PDIST until optimal performance is achieved. A slightly more focused approach would be to compute how much shared memory there is to spare, using the occupancy view in Nsight Compute, and choosing PDIST such that it is all used for the prefetch buffer. But this is not foolproof, because sometimes it helps to reduce the number of thread blocks per SM somewhat to free up more shared memory.Hello, shared memory padding strategy is not economic for some circumstances. Does #define vsmem(index) v[threadIdx.x + PDIST*index] works better for this post?
Besides, according to cuda programming guide, for Compute Capability 5.x and later, shared memory has 32 banks with 32-bit word. So there is no way to make a conflict-free read for double type?Yes, it would work better. As I wrote in the blog: “We could actually have arrived at this performance improvement without resorting to padding by changing the indexing scheme of the array in shared memory, which is left as an exercise for the reader.” You did the exercise!
It is indeed impossible to avoid conflicts with 64b words, but the point is that the indexing you proposed minimizes conflicts.The indexing into v should be threadIdx.x + blockDim.x*index right? Each thread essentially gets its own column (would equate to a bank for 32b words).Yes, you are right, I was too quick to respond to respondent liuws’s suggestion. Thank you for pointing out my error.Powered by Discourse, best viewed with JavaScript enabled"
2300,improve-accuracy-and-robustness-of-vision-ai-apps-with-vision-transformers-and-nvidia-tao,"Originally published at:			https://developer.nvidia.com/blog/improve-accuracy-and-robustness-of-vision-ai-apps-with-vision-transformers-and-nvidia-tao/
Vision Transformers (ViTs) are taking computer vision by storm, offering incredible accuracy, robust solutions for challenging real-world scenarios, and improved generalizability. The algorithms are playing a pivotal role in boosting computer vision applications and NVIDIA is making it easy to integrate ViTs into your applications using NVIDIA TAO Toolkit and NVIDIA L4 GPUs. How ViTs…Powered by Discourse, best viewed with JavaScript enabled"
2301,top-ai-video-analytics-sessions-at-gtc-2022,"Originally published at:			Developer Conference Session Catalog | NVIDIA GTC
Register now to experience the most advanced developer tools and learn how experts across industries are using vision AI to increase operational efficiency.Powered by Discourse, best viewed with JavaScript enabled"
2302,watch-the-gtc-keynote,"Originally published at:			Keynote by NVIDIA CEO Jensen Huang | GTC 2022 | NVIDIA
Catch the latest announcements from NVIDIA CEO Jensen HuangPowered by Discourse, best viewed with JavaScript enabled"
2303,gtc-21-top-5-professional-visualization-sessions,"Originally published at:			https://developer.nvidia.com/blog/gtc-21-top-5-professional-visualization-sessions/
Learn how you can take advantage of the latest NVIDIA technology to enable the creation of beautiful worlds quicker and easier than ever before.Powered by Discourse, best viewed with JavaScript enabled"
2304,cutting-edge-parallel-algorithms-research-with-cuda,"Originally published at:			https://developer.nvidia.com/blog/cutting-edge-parallel-algorithms-research-cuda/
Leyuan Wang, a Ph.D. student in the UC Davis Department of Computer Science, presented one of only two “Distinguished Papers” of the 51 accepted at Euro-Par 2015.  Euro-Par is a European conference devoted to all aspects of parallel and distributed processing held August 24-28 at Austria’s Vienna University of Technology. Leyuan’s paper Fast Parallel Suffix Array on the GPU,…She does so goodInteresting work! In cudpp 2.2 I could only find cudppCompress and not cudppDecompress. Am I missing something obvious?Hi Anders, currently we don't have cudppDecompress as a primitive in cudpp2.2, but it's promising to be added in the next version of cudpp.Powered by Discourse, best viewed with JavaScript enabled"
2305,gtc-2020-performance-analysis-and-optimization,"GTC 2020 CWE21720
Presenters: Peng-Wang,NVIDIA; Akshay-Subramaniam, NVIDIA; Evan-Weinberg, NVIDIA; Markus-Tavenrath, NVIDIA; Sven-Middelberg, NVIDIA; Lars-Nyland, NVIDIA; Sebastian-Jodlowski, NVIDIA; Bill-Fiser, NVIDIA
Abstract
Connect directly with NVIDIA Experts to get answers to all of your questions on GPU programming and code optimization, share your experience, and get guidance on how to achieve maximum performance on NVIDIA’s platform.Watch this session
Join in the conversation below.Powered by Discourse, best viewed with JavaScript enabled"
2306,changing-cybersecurity-with-natural-language-processing,"Originally published at:			https://developer.nvidia.com/blog/changing-cybersecurity-with-natural-language-processing/
NLP can be leveraged in cybersecurity workflows to assist in breach protection, identification, and scale and scope analysis.Powered by Discourse, best viewed with JavaScript enabled"
2307,new-on-ngc-richer-seamless-experience-with-latest-ngc-catalog-user-interface,"Originally published at:			New on NGC: Richer, Seamless Experience with Latest NGC Catalog User Interface | NVIDIA Technical Blog
The NVIDIA NGC catalog, a GPU-optimized hub for HPC, ML and AI applications, now has a new look and we’re really excited about it! Over the past few months we’ve been working with our community, design and research teams to bring you an enhanced user experience, engineered to deliver the most relevant content and features,…Powered by Discourse, best viewed with JavaScript enabled"
2308,introduction-to-neural-machine-translation-with-gpus-part-1,"Originally published at:			https://developer.nvidia.com/blog/introduction-neural-machine-translation-with-gpus/
Note: This is the first part of a detailed three-part series on machine translation with neural networks by Kyunghyun Cho. You may enjoy part 2 and part 3. Neural machine translation is a recently proposed framework for machine translation based purely on neural networks. This post is the first of a series in which I will explain a simple…Nice post! Thanks!One equation is missing. Looks like latex error. Thanks, I've fixed it. Wordpress latex is tricky...This is fantastic. Thanks! Great to include the extra papers as jumping off points.Awesome. I can't wait for the next posts!Had one question, there is a function g_theta specified towards the end of the post to model the conditional probability of p(x|x_(less_than(t))), but it is not defined anywhere. Is g_theta the soft-max function? Also is g_theta used at any point in the training?I found this post so interesting, thank you for sharing! If I want to cite this (or the next two posts), what's the best thing to do?Cite the NVIDIA Developer Blog, along with authors and title, as you would any source.Powered by Discourse, best viewed with JavaScript enabled"
2309,power-up-your-skills-and-credentials-at-nvidia-gtc-2023,"Originally published at:			https://developer.nvidia.com/blog/power-up-your-skills-and-credentials-at-nvidia-gtc-2023/
Last August, I wrote a post about GTC that asked, ‘What if you could spend 8 hours with an AI legend while getting hands-on experience using some of the most advanced GPU and DPU technology available?”  My point still stands: This is exactly why you should attend training at GTC. The virtual conference offers hands-on…Powered by Discourse, best viewed with JavaScript enabled"
2310,top-hpc-sessions-at-gtc-2022,"Originally published at:			AI Developer Conference Session Catalog | GTC 2022 | NVIDIA
Learn about new CUDA features, digital twins for weather and climate, quantum circuit simulations, and much more with these GTC 2022 sessions.Powered by Discourse, best viewed with JavaScript enabled"
2311,facebook-training-ai-bots-to-negotiate-with-humans,"Originally published at:			Facebook Training AI Bots to Negotiate with Humans | NVIDIA Technical Blog
Researchers at Facebook Artificial Intelligence Research (FAIR) published a paper introducing AI-based dialog agents that can negotiate and compromise. In a new blog post, Facebook explains how existing chatbots can hold short conversations and perform simple tasks such as booking a restaurant – but building machines that can hold meaningful conversations with people is challenging…Powered by Discourse, best viewed with JavaScript enabled"
2312,accelerating-sequential-python-user-defined-functions-with-rapids-on-gpus-for-100x-speedups,"Originally published at:			https://developer.nvidia.com/blog/accelerating-sequential-python-user-defined-functions-with-rapids-on-gpus-for-100x-speedups/
Motivation Custom “row-by-row” processing logic (sometimes called sequential User-Defined Functions) is prevalent in ETL workflows. The sequential nature of UDFs makes parallelization on GPUs tricky. This blog post covers how to implement the same UDF logic using RAPIDS to parallelize computation on GPUs and unlock 100x speedups. Introduction Typically, sequential UDFs revolve around records with…Powered by Discourse, best viewed with JavaScript enabled"
2313,gtc-2020-multi-gpu-programming-with-message-passing-interface,"GTC 2020 S21067
Presenters: Jiri Kraus,NVIDIA
Abstract
Learn how to program multi-GPU systems or GPU clusters using the message-passing interface (MPI) and OpenACC or NVIDIA CUDA. We’ll start with a quick introduction to MPI and how it can be combined with OpenACC or CUDA. Then we’ll cover advanced topics like CUDA-aware MPI and how to overlap communication with computation to hide communication times. We’'ll also cover the latest improvements with CUDA-aware MPI, interaction with unified memory, the multi-process service, and MPI support in NVIDIA performance analysis tools.Watch this session
Join in the conversation below.Hi Jiri, thanks for the great video.  I have a question regarding implementation with CUDA-aware MPI.As per the OpenMPI page, they suggest that you set the device prior to calling MPI_Init (FAQ: Running CUDA-aware Open MPI)  Is that outdated information?  If not, how do we go about doing that when we determine the GPU using the local rank?Thanks.Hi Todd,thanks for the feedback on my talk. Regarding your question: Yes for most implementations that is outdated information it is no longer needed to set the device before calling MPI_Init. However it still should be selected before calling MPI routines that require a GPU context and not change after that. What exact MPI routines require a GPU context depends on your MPI implementation. A mental model that works for me is: All MPI communication routines that accept a pointer or reference to data that should be communicated need a GPU context. Creating a new communicator with e.g. MPI_Comm_split_type and querying the size or rank of a communicator does not require GPU context and can thus be used before selecting a device.However I think the FAQ entry you are referring to is “10. What are some guidelines for using CUDA and Open MPI with Omni-Path?” which is specific to Omni-Path. So if you are using the GPUDirect RDMA support of Omni-Path it is probably still required to select a device before calling MPI_Init. I will try to get a clarification on that.If not, how do we go about doing that when we determine the GPU using the local rank?You can use getenv and query a environment variable set to the local rank on a node. All launchers I am aware of set a variable like this. E.g. the OpenMPI launcher set OMPI_COMM_WORLD_LOCAL_RANK and the MVAPICH2 launcher sets MV2_COMM_WORLD_LOCAL_RANK.I hope this helps.ThanksJiriThanks for the response, Jiri.  That answered my question.I did have one more question regarding MPI datatypes with CUDA-aware MPI.  I know this is probably implementation-specific, but I was wondering about your thoughts regarding using MPI_Type_indexed for communicating between GPU’s.Would it be better to pack/unpack my own buffers on the device and pass those to Isend/Irecv?  (as opposed to using the MPI_Type_indexed on a GPU data pointer).Hi Todd,as you say the support for MPI datatype processing with CUDA-aware MPI is implementation specific, e.g. MVAPICH2-GDR has support for GPU side packing: MVAPICH :: GDR Userguide. However beside varying support for this in different CUDA-aware MPI implementations there are also some general considerations. MPI internal packing of data can be more efficiently pipelined with inter GPU data movement via the network or inter node. However it requires the MPI to launch CUDA kernels and you application has no control when they are launched and in which streams. In case your are overlapping MPI communication with kernel execution that can cause performance issues so you might be better of doing the packing application side.Hope this helpsJiriThanks, Jiri, that helps a lot.Thanks again for taking the time to answer my questions.Excited by mention of WRF (Weather Research & Forecasting) Model acceleration with OpenACC directives in the newly released NVIDIA HPC SDK, I am seeking any advice or direction towards a collaboration. Would you perhaps know where I could ask more about this?Thanks Bennet for reaching out to us. A colleague of mine will follow up with you offline.Powered by Discourse, best viewed with JavaScript enabled"
2314,bridging-the-divide-between-cli-and-automation-it-teams-with-nvidia-nvue,"Originally published at:			https://developer.nvidia.com/blog/bridging-the-divide-between-cli-and-automation-it-teams-with-nvue/
Learn more about the NVIDIA NVUE object-oriented, schema-driven model of a complete Cumulus Linux system. The API enables you to configure any system element.Powered by Discourse, best viewed with JavaScript enabled"
2315,demystifying-enterprise-mlops,"Originally published at:			https://developer.nvidia.com/blog/demystifying-enterprise-mlops/
Learn about enterprise MLOps with this introductory overview.Great overview from William Benton on MLOps! I think this is recommended reading for anyone just getting started/curious about what’s required for production AI at scale. Can’t wait for the GTC session!Powered by Discourse, best viewed with JavaScript enabled"
2316,parallel-shader-compilation-for-ray-tracing-pipeline-states,"Originally published at:			Parallel Shader Compilation for Ray Tracing Pipeline States | NVIDIA Technical Blog
In ray tracing, a single pipeline state object (PSO) can contain any number of shaders. This number can grow large, depending on scene content and ray types handled with the PSO; construction cost of the state object can significantly increase. The DXR API makes it possible to distribute part of the creation work to multiple…Powered by Discourse, best viewed with JavaScript enabled"
2317,large-language-models-trained-on-nvidias-omniverse-kit-api,"Hi!I’ve been using Omniverse a lot over the past 18 months. The release of LLMs has assisted quite a bit with parsing large and sometimes confusing APIs like Pixar USD.My question is: Is there a LLM that has been trained on the omniverse APIs? As a developer having a tool like that would increase my productivity greatly.I am specifically concerned with things like omni.kit and omni.ui for extension development. I find myself spending quite a bit of time parsing documentation (and often even finding the correct documentation as kit versions update and mutate - see my other posts on the forum for reference). The developer community and Mati have been incredibly helpful in resolving many of these roadblocks I’ve encountered, but a conversational AI would be invaluable for me. Obviously the infrastructure already exists, but things like OpenAI’s ChatGPT had their public-facing model’s training data cut off back in 2021, and there have been significant updates/changes to omniverse APIs since then.Thank you for your time!-MatthewWonderful idea - makes sense. Could you share some example questions that you might want to ask an LLM that’s trained in this way?Hi Paul,Sure! Here are some questions I’ve brought to the forums or asked other experts before that a LLM would probably make quick work of:‘Can you show me how to generate extra viewports in an Omniverse Kit extension using the kit viewport API? I would also like these viewports to be capable of popping out into their own windows outside of the currently running base kit application.’‘In the context of an NVIDIA Omniverse Kit extension, can you show me how to add text labels on world-space object that scale consistently to screen space?’ (this one may be outside LLM capability)‘I would like to generate an Omniverse Kit extension GUI that has the following elements and layout: (arbitrary description). Can you help me generate this using the omni.ui library?’In addition to some of these more complex things, it would also be very helpful to have it reply with the relevant library for a given task, i.e.Omniverse has so many powerful capabilities and use cases but for things like scene manipulation I find myself relying very heavily on coding directly with the USD API even if it is more verbose or low-level than omni.usd. Pixar has very elaborate (albeit dense) documentation, and though many of the omniverse libraries do have decent docs, it seems they are more subject to version changes/mismatches between the most modern codebase and the current web-based documentation. Pixar also has some very helpful wiki-like descriptions for computer graphics theory built into some of the top level classes. That kind of description goes a long way for keeping experimenting developers engaged and not lost. A LLM (to a certain degree) can circumvent the need for such details and if trained on every kit version separately, could even resolve the version confusion issue.If Pixar USD knowledge, Omniverse Kit knowledge, and generative AI capabilities were concatenated into one tool I think there would be some remarkable results. It would also make the platform more attractive to independent developers and beginners.Cheers!Mati (from Discord) said the Python bindings to all the API functions currently do not include all the argument lists for functions. It won’t be ready for 105, but will be released some time after that. I suspect that will help a lot as well. Teach it the python APIs (not the C++ ones).Its a great tool for learning. E.g. “Using the Python API, how do you bind a material to a geometry given a prim path to the material”. Sometimes you need to use UsdSkel instead of a prim node (a wrapper class) to get type safe APIs, and for Python you often just use a string instead of a token.I would train it up on Mati’s git repo of NVIDIA code samples from his live streams as well. But I shelved trying to “fine tune” a ChatGPT model until all the function prototypes were fully available. After that, I think this would be a fantastic resource for learning the APIs.“How do I load a model and make sure that it is rotated upwards and scaled to the same scale factor as the rest of the project?”
“What is the “st” property?”
“Write python code to retarget an animation clip from one character to another”
“How can I create a sequencer from python, add the UsdSkelRoot under /World/Characters/Sam to an animation track, then add the animation clip for walking from the Hank character to Sam.”Thanks, this is a great list. I’ll take this back to team and explore it.Powered by Discourse, best viewed with JavaScript enabled"
2318,beating-sota-inference-performance-on-nvidia-gpus-with-gpunet,"Originally published at:			Beating SOTA Inference Performance on NVIDIA GPUs with GPUNet | NVIDIA Technical Blog
GPUNet is a new family of convolutional neural networks designed to maximize the performance of NVIDIA GPUs and TensorRT. Crafted by AI, GPUNet is up to 2x faster than EfficientNet-X and FBNet-V3.Powered by Discourse, best viewed with JavaScript enabled"
2319,ai-model-rapidly-identifies-structures-damaged-by-wildfires,"Originally published at:			https://developer.nvidia.com/blog/ai-model-rapidly-identifies-structures-damaged-by-wildfires/
New research develops a deep learning algorithm to detect wildfire damage remotelyPowered by Discourse, best viewed with JavaScript enabled"
2320,develop-a-multi-robot-environment-with-nvidia-isaac-sim-ros-and-nimbus,"Originally published at:			https://developer.nvidia.com/blog/develop-a-multi-robot-environment-with-nvidia-isaac-sim-ros-and-nimbus/
Developing a high-fidelity multi-robot simulated environment is complex and takes time, but it can be simplified with NVIDIA Isaac Sim and Nimbus.Powered by Discourse, best viewed with JavaScript enabled"
2321,upcoming-event-jetpack-5-0-2-walkthrough-for-jetson-orin-based-modules,"Originally published at:			JetPack 5 Webinar Series
Join us on October 4 to learn how to develop for any Jetson Orin module using emulation support on the Jetson AGX Orin Developer Kit.Powered by Discourse, best viewed with JavaScript enabled"
2322,new-question,"This is a repost of a question which was in the wrong location:Do you have any collection of information about CUDA with recipe for various computing solutions?I’m new to CUDA. Open to ideas of use cases for the followings:thanks@kcsham Here is where we can continue this discussion! Could you please give a little bit more information regarding what you mean regarding “computing solutions”?Currently, one thing that comes to mind is our Hopper H100 Confidential Computing Solution (NVIDIA Confidential Computing - NVIDIA Docs). This information goes over how you can ensure confidentiality (encryption and authentication) of your data from prying eyes (physically or remotely)We also have our High Performance Computing (HPC) for things like high-complexity simulations, etc.Happy to keep talking based on what you’re looking for!You can take a look at our computer vision section: Explore Computer Vision Software & Cloud Platform | NVIDIA . Personally for a hobby/getting started I recommend looking at DeepStream SDK; it is generally best for ‘getting it done fast’ as it has many pre-built plugins for you to use.For Audio transcription, you could take a look at Audio Transcription from NVIDIA LaunchPad This is a free lab where you can try our NVIDIA Riva speech recognition!For video transcoding, we have FFmpeg plugins, which can be found, built, and integrated into this open-source, industry-standard video tool. Please see this for instructions (Using FFmpeg with NVIDIA GPU Hardware Acceleration - NVIDIA Docs). This plugin you might notice is part of our larger Video Codec SDK, which is our main area for doing transcoding/modification. The main document page for that can be found here: NVIDIA Video Codec SDK v12.1 (Latest Release) - NVIDIA DocsPowered by Discourse, best viewed with JavaScript enabled"
2323,to-help-with-animal-conservation-efforts-ai-can-now-help-identify-chimpanzees,"Originally published at:			To Help With Animal Conservation Efforts, AI Can Now Help Identify Chimpanzees | NVIDIA Technical Blog
To help with animal conservation efforts, University of Oxford researchers developed a deep learning-based model that can identify individual chimpanzees with 93% accuracy and correctly classify their sex with 96% accuracy.  “Automating the process of individual identification could represent a step change in our use of large image databases from the wild to open up…Powered by Discourse, best viewed with JavaScript enabled"
2324,can-dlss-be-used-for-denoising,"I’m probably too late to the party but I’ll leave this here anyways…Somebody on Twitter wrote that DLSS would be great for denoising. Is this true?Hello @codemonkkkey and welcome to the NVIDIA developer forums!I saw that you added a couple of posts to the forums over the last couple of weeks, I’ll make sure to at least give some replies if I can, or get some more experts involved.Starting of with this one it is pretty easy. If you look inside the actual technology of DLSS you will see that de-noising is an essential part of the overall deep learning model of DLSS.But if you would want to only denoise and not upscale, you might rather consider NRD in terms of performance.I recommend this GDC session about DLSS for more information: NVIDIA DLSS Overview & Game Integrations | NVIDIA On-DemandI hope this helps!Hi @MarkusHoHo !Interesting. I was about to try it out (just finished my DLSS integration). I’ll look into NRD as well.Thanks!This topic was automatically closed 14 days after the last reply. New replies are no longer allowed.Powered by Discourse, best viewed with JavaScript enabled"
2325,peer-to-peer-multi-gpu-transpose-in-cuda-fortran-book-excerpt,"Originally published at:			https://developer.nvidia.com/blog/peer-to-peer-multi-gpu-transpose-cuda-fortran/
This post is an excerpt from Chapter 4 of the book CUDA Fortran for Scientists and Engineers, by Gregory Ruetsch and Massimiliano Fatica. In this excerpt we extend the matrix transpose example from a previous post to operate on a matrix that is distributed across multiple GPUs. The data layout is shown in Figure 1 for an nx × ny =…Hi Greg,I just thought I would let you know that I found your posts on CUDA Fortran extremely useful and easy to follow. A great introduction that is informative and enjoyable.Maybe I'll take a peek at CUDA Fortran for Scientists and Engineers next.Thank you for your great work.ManaHi Mana,Thanks for your feedback, I'm glad you enjoy the posts.  There are several new CUDA Fortran features slated for 2014 that I'll be writing about in upcoming posts, I hope you'll find those useful as well.Powered by Discourse, best viewed with JavaScript enabled"
2326,six-ways-to-saxpy,"Originally published at:			https://developer.nvidia.com/blog/six-ways-saxpy/
This post is a GPU program chrestomathy. What’s a Chrestomathy, you ask? In computer programming, a program chrestomathy is a collection of similar programs written in various programming languages, for the purpose of demonstrating differences in syntax, semantics and idioms for each language. [Wikipedia] There are several good examples of program chrestomathies on the web, including Rosetta Code and NBabel, which demonstrates gravitational N-body simulation in…Hi Mark,I have just started looking into CUDA and downloaded and installed Visual Studio on my Windows machine (GPU: Nvidia Geforce 940M CUDA:7.5 MS VS 2013). I have started with vector add and then read about SAXPY.Where is the definition of the SAXPY code?cublas_v2.h contains:#define cublasSaxpy          cublasSaxpy_v2cublas_api.h containsCUBLASAPI cublasStatus_t CUBLASWINAPI cublasSaxpy_v2 (..);but where is the actual code where processing occurs?I am confused as all blogs just use the api call but I wanted to see the actual code. I am new to cpp as well, can you help me with the location of its definition and not declaration in CUDA samplesHi Deepak, cuBLAS is not an open-source library, so you can't see the implementation code for its routines.Hi Mark,Thanks for your reply.Yes I realised that after a while. Actually I am working on a project about kernel fusions especially CUDAs. Wanted to start small like DAXPY and DGEMM. Will try writing the code myself now and use other Cuda samples like BlackScholes(full implementation is present) or Rodinia benchmarks.Thanks for all the elaborate tutorials.Deepak.Hello Mark,Thank you for providing such a beautiful article on this topic. Recently I have started learning about parallel computing and gpu programming and after reading your article and trying out the code I stumbled over an error:void saxpy(int n, float a, float *x, float *y){#pragma acc kernels  for (int i = 0; i < n; ++i)      y[i] = a*x[i] + y[i];}The following snippet of code produces an error at compilation if *x and *y are dinamically allocated using mallocThe error:PGC-S-0155-Cannot determine bounds for array x (saxpy.c: 26)PGC-S-0155-Cannot determine bounds for array y (saxpy.c: 26)main:     26, Generating copy(x[:],y[:])PGC/x86-64 Linux 16.5-0: compilation completed with severe errorsHi Christian, the following code works for me with PGI 16.5:void saxpy(int n, float a, float *restrict x, float *restrict y){#pragma acc kernels    for (int i = 0; i < n; ++i)        y[i] = a*x[i] + y[i];}int main(void){    const int n = 1 << 20;    float *x = new float[n];    float *y = new float[n];    for (int i = 0; i < n; ++i) {        x[i] = (float)i / (float)n;        y[i] = (float)i;    }    // Perform SAXPY on 1M elements    saxpy(n, 2.0, x, y);    delete [] x;    delete [] y;}Note that I needed to add the ""restrict"" flags to tell the compiler that there is no aliasing in the function on these pointers. I updated the post text to reflect this. See https://devblogs.nvidia.com...Compiled with:    pgc++ -acc -ta=tesla -Minfo=accel saxpy_acc.cppCompiler output:saxpy(int, float, float *, float *):    2, Generating copyin(x[:n])        Generating copy(y[:n])    4, Loop is parallelizable        Accelerator kernel generated        Generating Tesla code         4, #pragma acc loop gang, vector(128) /* blockIdx.x threadIdx.x */Hi there Mark,Thank you for your reply, it works like a charm.Inspired by your article, I've done and benchmarked many more ways (currently 24) to SAXPY: https://github.com/bennylp/...Fun! I can't say SAXPY is really a good computation benchmark. It's bandwidth bound. However, in your comparison you are basically demonstrating which frameworks have more or less overhead.Thank you!But anyway, does it mean that the complexity of walking through an array is about O(1)? Yhank you!on C++ loop [cpu] (src/saxpy_cpu.cpp) you've got 40 ms with Intel i7-6700 CPU @ 3.40GHz but I've got 3.3 ms with Intel i3-2370M CPU @ 2.40GHz https://github.com/Evegen55... Why your code is so slow?https://uploads.disquscdn.c... Here is an imageI use N=2^26 not 2^20Change yours to use N=2^26Thank you! I've got 220 ms :-)The question is a bit off-topic, but is the Boost library the original source for this syntax wherever it appears? I always wondered if Scala had been inspired by another language/library for this argument syntax as well.Not sure which syntax you mean. As mentioned in the post, SAXPY comes from BLAS.Powered by Discourse, best viewed with JavaScript enabled"
2327,nvidia-announces-nsight-systems-2019-4,"Originally published at:			NVIDIA announces Nsight Systems 2019.4 | NVIDIA Technical Blog
NVIDIA Nsight Systems 2019.4 is now available for download. This release aims to provide a more detailed data collection, exploration, and collection control for all markets ranging from high performance computing to visual effects. 2019.4 introduces new data sources, improved visual data navigation, expanded CLI capabilities, extended export coverage and statistics.   Nsight Systems is an…Powered by Discourse, best viewed with JavaScript enabled"
2328,proving-superior-cloud-ai-and-storage-performance-with-nvidia-spectrum-3-switches,"Originally published at:			https://developer.nvidia.com/blog/proving-superior-cloud-ai-and-storage-performance-with-spectrum-3-switches/
Independent IT lab The Tolly Group compared the cloud, AI, and storage performance of an NVIDIA Ethernet switch to the performance of a comparable switch built with commodity silicon.Powered by Discourse, best viewed with JavaScript enabled"
2329,autonomous-robots-to-deliver-your-shopping-in-2016,"Originally published at:			https://developer.nvidia.com/blog/self-driving-delivery-robots-coming-in-2016/
Skype co-founders have launched a company that plans to send GPU-based robots to the homes of consumers for local deliveries. The idea is that the deliveries will be sent to a central hub from which the six-wheeled, electric-powered autonomous robots, outfitted with a hollow center and a lockable carrying case, will make deliveries to customers’…Powered by Discourse, best viewed with JavaScript enabled"
2330,bring-ai-to-market-fast-with-pre-trained-models-and-transfer-learning-toolkit-3-0,"Originally published at:			https://developer.nvidia.com/blog/bring-ai-to-production-faster-with-nvidia-pre-trained-models-ai-toolkit/
Today, NVIDIA released several production-ready, pre-trained models and a developer preview of Transfer Learning Toolkit (TLT) 3.0, along with DeepStream SDK 5.1.Powered by Discourse, best viewed with JavaScript enabled"
2331,new-video-light-resampling-in-practice-with-rtxdi,"Originally published at:			New Video: Light Resampling In Practice with RTXDI | NVIDIA Technical Blog
In this video, NVIDIA’s Alexey Panteleev explains the key details needed to add performant resampling to modern game engines. He also discusses roadmap plans for the recently announced RTXDI SDK, which allows easy experimentation and integration of direct illumination.  With RTXDI, lighting artists can render scenes with millions of dynamic area lights in real-time without…Powered by Discourse, best viewed with JavaScript enabled"
2332,ai-helps-accelerate-the-drug-development-process,"Originally published at:			https://developer.nvidia.com/blog/ai-helps-further-the-drug-development-process/
University of California, Irvine researchers developed a deep learning-based approach to accelerate drug discovery and cancer research. “We have developed a convolutional neural network to improve the data analysis processes for high-throughput drug screening using our microphysiological system (MPS),” the researchers stated in their paper. A microphysiological system is an interconnected set of 2 or…Powered by Discourse, best viewed with JavaScript enabled"
2333,what-s-new-to-ngc-hpc-containers-for-a100-and-arm-systems,"Originally published at:			What’s New to NGC: HPC Containers for A100 and Arm Systems | NVIDIA Technical Blog
Researchers are harnessing the power of NVIDIA GPUs more than ever before to find a cure for COVID-19. Leveraging popular molecular dynamics and quantum chemistry HPC applications, they are running thousands of experiments to predict which compounds can effectively bind with protein and block the virus from affecting our cells.  NGC has recently introduced updated…Powered by Discourse, best viewed with JavaScript enabled"
2334,deploying-a-scalable-object-detection-pipeline-the-inferencing-process-part-2,"Originally published at:			https://developer.nvidia.com/blog/deploying-a-scalable-object-detection-pipeline-the-inferencing-process-part-2/
This post is the second in a series on Autonomous Driving at Scale, developed with Tata Consultancy Services (TCS). The previous post in this series provided a general overview of the deep learning inference for object detection. In this post, we dive deep into the object detection inference process. We cover an explanation of the…Powered by Discourse, best viewed with JavaScript enabled"
2335,ai-can-help-anyone-become-a-beatbox-champion,"Originally published at:			https://developer.nvidia.com/blog/ai-can-help-anyone-become-a-beatbox-champion/
To help up-and-coming musicians create the best beats for their song, developers from a Japanese-based AI startup developed a deep learning system called Neural Beatboxer that can convert everyday sounds into hours of automatically compiled rhythms. Users can visit their website, feed it some sounds, and the neural network automatically produces a custom drum kit…Powered by Discourse, best viewed with JavaScript enabled"
2336,teaching-autonomous-machines-to-see-transparent-objects,"Originally published at:			Teaching Autonomous Machines to See Transparent Objects | NVIDIA Technical Blog
To help autonomous machines better sense transparent objects, Google researchers, in collaboration with Columbia University and Synthesis AI, developed ClearGrasp, an algorithm that can accurately estimate the 3D data of clear objects, like a glass container or plastic utensil, from standard RGB images.  “Enabling machines to better sense transparent surfaces would not only improve safety…Powered by Discourse, best viewed with JavaScript enabled"
2337,learn-to-build-real-time-video-ai-applications,"Originally published at:			https://developer.nvidia.com/blog/learn-to-build-real-time-video-ai-applications/
Learn the skills to transform raw video data from cameras into deep learning-based insights in real-timePowered by Discourse, best viewed with JavaScript enabled"
2338,deploy-ai-workloads-at-scale-with-bottlerocket-and-nvidia-powered-amazon-ec2-instances,"Originally published at:			https://developer.nvidia.com/blog/deploy-ai-workloads-at-scale-with-bottlerocket-and-nvidia-powered-amazon-ec2-instances/
AWS and NVIDIA collaborated on Bottlerocket, a container-optimized OS, to support all NVIDIA powered Amazon EC2 instances like the P4d, P3, G4dn, and G5 instances.Powered by Discourse, best viewed with JavaScript enabled"
2339,nvidia-announces-availability-for-arm-hpc-developer-kit-with-new-hpc-sdk-v21-7,"Originally published at:			https://developer.nvidia.com/blog/nvidia-announces-availability-for-arm-hpc-developer-kit-with-new-hpc-sdk-v21-7/
The DevKit is an integrated hardware-software platform for creating, evaluating, and benchmarking HPC, AI, and scientific computing applications for Arm server based accelerated platforms.Where can I find information about GIGABYTE G242-P32?Hi,You can find more information about the NVIDIA Arm HPC Developer KIt can be found one the Developer Zone here: https://developer.nvidia.com/arm-hpc-devkitThanks, I’ve emailed sam.hsieh@gigabyte.com as requested on the bottom of the page. Will post the spec here once I get it :)The answer was simply “G242-P32 is not ready yet”.Hardware-level root of trust supportUp to 2 x NVIDIA ® A100 PCIe Gen4 GPU cardsUp to 2 x NVIDIA ® BlueField-2 DPUsSingle socket Ampere® Altra® Max or Altra®...spec/ manual is upPowered by Discourse, best viewed with JavaScript enabled"
2340,why-automatic-augmentation-matters,"Originally published at:			https://developer.nvidia.com/blog/why-automatic-augmentation-matters/
Deep learning models require hundreds of gigabytes of data to generalize well on unseen samples. Data augmentation helps by increasing the variability of examples in datasets. The traditional approach to data augmentation dates to statistical learning when the choice of augmentation relied on the domain knowledge, skill, and intuition of the engineers that set up…Powered by Discourse, best viewed with JavaScript enabled"
2341,human-like-character-animation-system-uses-ai-to-navigate-terrains,"Originally published at:			https://developer.nvidia.com/blog/human-like-character-animation-system-uses-ai-to-navigate-terrains/
Researchers from University of Edinburgh and Method Studios developed a real-time character control mechanism using deep learning that can help virtual characters walk, run and jump a little more naturally.  “Data-driven motion synthesis using neural networks is attracting researchers in both the computer animation and machine learning communities thanks to its high scalability and runtime…Powered by Discourse, best viewed with JavaScript enabled"
2342,upcoming-event-level-up-with-nvidia-nsight-graphics-and-optimize-your-game,"Originally published at:			Level Up with NVIDIA
Learn how to use the latest NVIDIA RTX technology in NVIDIA Nsight Graphics and get your questions answered in a live Q&A session with experts.Powered by Discourse, best viewed with JavaScript enabled"
2343,accelerating-multiorgan-rendering-for-radiology-and-radiation-therapy-with-nvidia-clara-holoscan,"Originally published at:			https://developer.nvidia.com/blog/accelerating-multi-organ-rendering-for-radiology-and-radiation-therapy-with-clara-holoscan/
NVIDIA Clara  Holoscan is the AI computing platform for medical devices that combines hardware systems for low-latency sensor and network connectivity.Powered by Discourse, best viewed with JavaScript enabled"
2344,using-ai-to-encrypt-messages-in-plain-sight,"Originally published at:			Using AI to Encrypt Messages in Plain Sight | NVIDIA Technical Blog
Researchers from Columbia University developed a deep learning technique that allows people to embed and later retrieve hidden information in ordinary looking text. The technique, called FontCode, works by altering text glyphs (the particular shape designs of fonts) to encode information. The method preserves the text and the hidden information and is almost impossible to…Powered by Discourse, best viewed with JavaScript enabled"
2345,wef-s-2020-technology-pioneers-list-ai-startups-to-know,"Originally published at:			WEF’s 2020 Technology Pioneers List: AI Startups to Know | NVIDIA Technical Blog
The World Economic Forum (WEF) announced its 2020 Technology Pioneers list, highlighting 100 startups driving innovation through technological advancement.  Numerous NVIDIA Inception startups were featured, including: ABC Technology, ABEJA, GuanData, Lunit, Fiddler Labs, Metawave, Roadbotics, and Veo Robotics.  ABC Technology ABC Technology uses natural language processing (NLP) to help financial institutions and large enterprises draw…Powered by Discourse, best viewed with JavaScript enabled"
2346,high-performance-python-communication-with-ucx-py,"Originally published at:			https://developer.nvidia.com/blog/high-performance-python-communication-with-ucx-py/
This post was originally published on the RAPIDS AI Blog. TL;DR UCX/UCX-Py is an accelerated networking library designed for low-latency high-bandwidth transfers for both host and GPU device memory objects.  You can easily get started by installing through conda (limited to linux-64): > conda install -c rapidsai ucx-py ucx Introduction RAPIDS is committed to delivering…Powered by Discourse, best viewed with JavaScript enabled"
2347,cudacasts-episode-8-accelerate-fftw-apps-with-cufft-5-5,"Originally published at:			https://developer.nvidia.com/blog/cudacasts-episode-8-accelerate-fftw-apps-cufft-55/
GPU libraries provide an easy way to accelerate applications without writing any GPU-specific code. With the new CUDA 5.5 version of the NVIDIA CUFFT Fast Fourier Transform library, FFT acceleration gets even easier, with new support for the popular FFTW API. It is now extremely simple for developers to accelerate existing FFTW library calls on the GPU, sometimes…interesting feature , i wasn't aware of it , thank for the infoHello sir, This code has one intersting featues Cuda from one makefile. I would like to see how this is done. In the video, you didn't show the kernel in main function. Could you please show the full version of this code ?Powered by Discourse, best viewed with JavaScript enabled"
2348,nvidia-driver-symbol-server-now-available,"Originally published at:			https://developer.nvidia.com/blog/nvidia-driver-symbol-server-now-available/
NVIDIA is making available a repository of driver binaries. So why do we need this? Problem: During application development you may find yourself looking at a crash report with an attached crash dump – unfortunately there’s no guarantee you’ll have the same driver installed on your local system as is used in the dump (typically…Powered by Discourse, best viewed with JavaScript enabled"
2349,detecting-malware-with-purple-team-collaboration,"Originally published at:			https://developer.nvidia.com/blog/detecting-malware-with-purple-team-collaboration/
The NVIDIA Security Team worked with an open source developer within the information security field to help bolster the defensive capabilities of the broader community.Powered by Discourse, best viewed with JavaScript enabled"
2350,develop-for-all-six-nvidia-jetson-orin-modules-with-the-power-of-one-developer-kit,"Originally published at:			Develop for All Six NVIDIA Jetson Orin Modules with the Power of One Developer Kit | NVIDIA Technical Blog
All NVIDIA Jetson Orin modules share one SoC architecture, enabling the Jetson AGX Orin Developer Kit to emulate any of them. This makes it easy to start developing your next product.Hi jwitsoe,Does the emulation apply to all HW components within the SoC such as ISP capability… Or to put it another way, is the ISP the ‘same’ for each module and do they therefore all have the same capability.Please note i am referring to the ISP processing pipeline only and not the actual camera connectivity, details regarding the C-PHY DPHY capability of modules seems well explained elsewhere.All the best
ChrisPowered by Discourse, best viewed with JavaScript enabled"
2351,ai-system-trained-to-recognize-new-galaxies,"Originally published at:			AI System Trained to Recognize New Galaxies | NVIDIA Technical Blog
Researchers from the University of Western Australia have developed a deep learning system that can identify galaxies in deep space. The system, called ClaRAN, presents a system that scans images taken by radio telescopes and spots radio galaxies that emit powerful radio jets from their black holes. Dr. Ivy Wong, an astronomer from the University…Powered by Discourse, best viewed with JavaScript enabled"
2352,gtc-2020-supercomputing-seamlessly-interactive-hpc-and-ai-with-gpus-via-open-ondemand,"GTC 2020 S21461
Presenters: Doug Holt ,NVIDIA; Alan Chalker,Ohio Supercomputer Center
Abstract
Discover how Open OnDemand (OOD) can help lower the barrier to entry and ease access to clusters of HPC resources for all users of scientific and AI workloads. We’ll touch upon the capabilities and architecture of OOD, installation experiences, the priority of upcoming features such as customized workflows, training users, integration with other science gateways, and growing the community. We’ll emphasize the joint efforts between NVIDIA and the OOD team to provide GPU-specific metrics, accessibility, and workflows to facilitate utilization of GPUs in multi-system HPC environments. OOD, already used in dozens of HPC centers, is a National Science Foundation-funded open-source portal that provides an easy way for system administrators to offer round-the-clock web access to their resources from desktop or mobile devices.Watch this session
Join in the conversation below.Powered by Discourse, best viewed with JavaScript enabled"
2353,deep-learning-helps-ucla-scientists-identify-cancer-cells-in-the-blood-instantaneously,"Originally published at:			Deep Learning Helps UCLA Scientists Identify Cancer Cells in the Blood Instantaneously | NVIDIA Technical Blog
UCLA researchers have just developed a deep learning, GPU-powered device that can detect cancer cells in a few milliseconds, hundreds of times faster than previous methods.  “improvement in computational efficiency enables low-latency inference and makes this pipeline suitable for cell sorting via deep learning,” the researchers stated in a newly published paper in Nature. “Our…Powered by Discourse, best viewed with JavaScript enabled"
2354,startup-launches-ai-app-that-helps-fix-bad-photos,"Originally published at:			https://developer.nvidia.com/blog/startup-launches-ai-app-that-helps-fix-bad-photos/
Relonch AI, a California-based startup, recently introduced a new deep learning app that drastically enhances photos shot in low light. The purpose of the application is to introduce users to some of the company’s capabilities in the digital imaging sector. The app, named Relonch Alfred, corrects photos by creating exposure and color maps of each…Powered by Discourse, best viewed with JavaScript enabled"
2355,cvpr-2020-nvidia-research-supercharges-autonomous-vehicle-perception-testing,"Originally published at:			CVPR 2020: NVIDIA Research Supercharges Autonomous Vehicle Perception Testing | NVIDIA Technical Blog
GPU parallel computing is delivering high performance to autonomous vehicle evaluation. In a research paper presented at the Computer Vision and Pattern Recognition Conference (CVPR) this week, NVIDIA GPUs were found to drastically reduce the time it takes to evaluate perception models using a new sophisticated evaluation metric, named Planning Kullback-Leibler Divergence (PKL).  PKL evaluates…Powered by Discourse, best viewed with JavaScript enabled"
2356,advanced-real-time-visualization-for-robotic-heart-surgery,"Originally published at:			Advanced Real-Time Visualization for Robotic Heart Surgery | NVIDIA Technical Blog
Researchers at the Harvard Biorobotics Laboratory are harnessing the power of GPUs to generate real-time volumetric renderings of patients’ hearts. The team has built a robotic system to autonomously steer commercially available cardiac catheters that can acquire ultrasound images from within the heart. They tested their system in the clinic and reported their results at…Powered by Discourse, best viewed with JavaScript enabled"
2357,researchers-at-videogorillas-use-ai-to-remaster-archived-content-to-4k-resolution-and-above,"Originally published at:			https://developer.nvidia.com/blog/researchers-at-videogorillas-use-ai-to-remaster-archived-content-to-4k-resolution-and-above/
Over the past few years, film and video standards have continued to evolve. There is a growing demand for higher fidelity imagery and resolutions to deliver a more immersive viewing experience.  With 4K as the current standard and 8K experiences becoming the new norm, older content doesn’t meet today’s visual standard. The remastering process aims…Powered by Discourse, best viewed with JavaScript enabled"
2358,experimental-ai-powered-hearing-aid-automatically-amplifies-who-you-want-to-hear,"Originally published at:			Experimental AI Powered Hearing Aid Automatically Amplifies Who You Want to Hear | NVIDIA Technical Blog
To help people who suffer from hearing loss, Researchers from Columbia University just developed a deep learning-based system that can help amplify specific speakers in a group, a breakthrough that could lead to better hearing aids.  “The brain area that processes sound is extraordinarily sensitive and powerful; it can amplify one voice over others, seemingly…Powered by Discourse, best viewed with JavaScript enabled"
2359,gpu-accelerated-r-in-the-cloud-with-teraproc-cluster-as-a-service,"Originally published at:			https://developer.nvidia.com/blog/gpu-accelerated-r-cloud-teraproc-cluster-service/
Analysis of statistical algorithms can generate workloads that run for hours, if not days, tying up a single computer. Many statisticians and data scientists write complex simulations and statistical analysis using the R statistical computing environment. Often these programs have a very long run time. Given the amount of time R programmers can spend waiting…Powered by Discourse, best viewed with JavaScript enabled"
2360,upcoming-event-manufacturing-workshops-at-gtc-2022,"Originally published at:			Conference Session Catalog | GTC 2022 | NVIDIA
Join us for manufacturing sessions at GTC 2022, including an expert-led Deep Learning Institute workshop on Computer Vision for Industrial Inspection.Powered by Discourse, best viewed with JavaScript enabled"
2361,first-emotionally-intelligent-speaker-trained-on-gpus,"Originally published at:			https://developer.nvidia.com/blog/first-emotionally-intelligent-speaker-trained-on-gpus/
Created by researchers at the Hong Kong University of Science and Technology, the MoodBox speaker is billed as the first ever high quality wireless speaker that senses human emotions. Using NVIDIA Tesla GPUs and deep learning, the speaker operates with cutting edge sensory recognition technology named “Emi”. Emi collects and analyzes audio signals and music…Powered by Discourse, best viewed with JavaScript enabled"
2362,nvidia-announces-full-open-source-of-material-definition-language-to-streamline-graphics-pipelines,"Originally published at:			NVIDIA Announces Full Open Source of Material Definition Language to Streamline Graphics Pipelines | NVIDIA Technical Blog
At SIGGRAPH 2022, NVIDIA announced the open sourcing of the MDL Distiller and GLSL backend technologies to further expand the MDL ecosystem.Powered by Discourse, best viewed with JavaScript enabled"
2363,vpf-hardware-accelerated-video-processing-framework-in-python,"Originally published at:			VPF: Hardware-Accelerated Video Processing Framework in Python | NVIDIA Technical Blog
Support for accelerated hardware video encoding began with the Kepler generation of NVIDIA GPUs, and all GPUs since the Fermi generation support hardware video acceleration decoding through the NVIDIA Video Codec SDK.  While showing great performance and flexibility, it requires knowledge of C/C++. Another option is to use third party libraries and applications like FFmpeg…Is VPF compatible with Python 3.8 ?Hey there, first of all, thank you for putting the effort to make this available for free, it’s really helpul for us who use python for AI projects, I’m working on a project that needs to decode 4 rtsp streams simultaneously and process them using some AI object detection algorithms, I’d like to know if you have a little more detailed docs on how to implement this framework, API reference maybe? I’d like to use it to decode the H264 high profile streams into opencv compatible frames, your samples run fine on my system but I’d like to know more so I can customize it better to suit my project’s needsThank you
Best
Fernando,Hi, there, I think the documentation is not up to date anymore. Could anyone there update the documentation ?Powered by Discourse, best viewed with JavaScript enabled"
2364,deploying-ai-applications-with-nvidia-egx-on-nvidia-jetson-xavier-nx-microservers,"Originally published at:			https://developer.nvidia.com/blog/deploying-ai-apps-with-egx-on-jetson-xavier-nx-microservers/
Modern expectations for agile capabilities and constant innovation—with zero downtime—calls for a change in how software for embedded and edge devices are developed and deployed. Adopting cloud-native paradigms like microservices, containerization, and container orchestration at the edge is the way forward but complexity of deployment, management, and security concerns gets in the way of scaling.…All things good up until running command:
helm install --name-template deepstream deepstream-helmchart/error:
Error: failed to download “deepstream-helmchart/” (hint: running helm repo update may help)~/deepstream-helmchart/templates$ helm repo update
Hang tight while we grab the latest from your chart repositories…
…Successfully got an update from the “stable” chart repository
Update Complete. ⎈ Happy Helming!⎈error persisted.Missing an import step for newbie:helm create deepstream-helmchartTac,You don’t need to run helm create as we provided instructions how to create a helm directory and how to add the files to helm directory.
image1168×468 51.1 KB
Thanks,
Anurag GI ran into a small issue following the doc. The doc has you name the Chart file chart.yaml. For me on a cluster of Jetson Nanos, I got the following error. I then renamed the file to Chart.yaml (capital C is key) and the deployment worked as planned.Thank you for the feedback, updated the doc with Chart.yaml. it should be good now.Small nit – the instructions say to install calico pod-network addon but the example commands deploy flannel.Thank You, Jacob will update according to that.Thanks for the detailed instructions, I was able to make it to the end. Can you add some explanation on what the sample output (below, from blog) means. Also is there any image/video output (since supposed to be IVA example)? I didn’t see any.$ kubectl logs deepstream-9f8b6b68d-rc5lq
**PERF: 101.42 (100.29) 101.42 (100.29)      101.42 (100.29)   101.42 (100.29)Hi Sudesh,Thanks for the following this blog. We will update the instructions about the video output for IVA.Powered by Discourse, best viewed with JavaScript enabled"
2365,new-video-visualizing-census-data-with-rapids-cudf-and-plotly-dash,"Originally published at:			https://developer.nvidia.com/blog/new-video-visualizing-census-data-with-rapids-cudf-and-plotly-dash/
Gathering business insights can be a pain, especially when you’re dealing with countless data points.  It’s no secret that GPUs can be a time-saver for data scientists. Rather than wait for a single query to run, GPUs help speed up the process and get you the insights you need quickly. In this video, Allan Enemark,…If you’re looking for the accompanying Notebook to try out these data viz techniques with RAPIDS cuDF, check out the RAPIDS Visualization Guide Notebook on GitHub.Powered by Discourse, best viewed with JavaScript enabled"
2366,researchers-take-steps-towards-autonomous-ai-powered-exoskeleton-legs,"Originally published at:			Researchers Take Steps Towards Autonomous AI-Powered Exoskeleton Legs | NVIDIA Technical Blog
University of Waterloo researchers are using deep learning and computer vision to develop autonomous exoskeleton legs to help users walk, climb stairs, and avoid obstacles.  The ExoNet project, described in an early-access paper on “Frontiers in Robotics and AI“, fits users with wearable cameras. AI software processes the camera’s video stream, and is being trained…Powered by Discourse, best viewed with JavaScript enabled"
2367,ai-legend-gill-pratt-of-toyota-to-keynote-at-gpu-technology-conference,"Originally published at:			AI Legend Gill Pratt of Toyota to Keynote at GPU Technology Conference | NVIDIA Technical Blog
Gill Pratt, CEO of the Toyota Research Institute and one of the world’s leading figures in artificial intelligence will be a keynote speaker at the GPU Technology Conference in Silicon Valley on April 7. Pratt joined Toyota in 2015 from the U.S. Defense Advanced Research Projects Agency where he led the DARPA Robotics Challenge. His…Powered by Discourse, best viewed with JavaScript enabled"
2368,apache-airflow-for-authoring-workflows-in-nvidia-base-command-platform,"Originally published at:			Apache Airflow for Authoring Workflows in NVIDIA Base Command Platform | NVIDIA Technical Blog
Integrating Apache Airflow with an AI platform such as NVIDIA Base Command, which leverages GPU acceleration, streamlines the process of training and deploying AI models.Powered by Discourse, best viewed with JavaScript enabled"
2369,real-time-natural-language-processing-with-bert-using-nvidia-tensorrt-updated,"Originally published at:			https://developer.nvidia.com/blog/real-time-nlp-with-bert-using-tensorrt-updated/
Today, NVIDIA is releasing TensorRT 8.0, which introduces many transformer optimizations. With this post update, we present the latest TensorRT optimized BERT sample and its inference latency benchmark on A30 GPUs. Using the optimized sample, you can execute different batch sizes for BERT-base or BERT-large within the 10 ms latency budget for conversational AI applications.Powered by Discourse, best viewed with JavaScript enabled"
2370,dancing-dna-revealed-in-high-res-hpc-simulations,"Originally published at:			https://developer.nvidia.com/blog/dancing-dna-revealed-in-high-res-hpc-simulations/
Using the highest-resolution images of a single DNA molecule captured to date, researchers in the U.K. discovered that coiled strands of genetic material twist and writhe while crammed in a cell. This previously unobserved movement was simulated on GPU-based systems including JADE, a University of Oxford supercomputer made up of NVIDIA DGX systems. Published in…Powered by Discourse, best viewed with JavaScript enabled"
2371,on-demand-webinar-limitless-capabilities-of-nvidia-cloudxr-2-0,"Originally published at:			https://developer.nvidia.com/blog/on-demand-webinar-limitless-capabilities-of-nvidia-cloudxr-2-0/
Learn how NVIDIA CloudXR can be used to deliver limitless virtual and augmented reality over networks (including 5G) to low cost, low-powered headsets and devicesPowered by Discourse, best viewed with JavaScript enabled"
2372,remote-access-of-terminal-of-nano-jetson,"Hi team,Please suggest any way to access nano jetson remotely on different network without static IP. Is there any software available like dataplicity which provide access of Raspberry pi, similar way for nano jetson.
I tried nomachine and vnc both works in same network.Powered by Discourse, best viewed with JavaScript enabled"
2373,gtc-2020-using-ai-algorithms-to-help-battle-the-global-wildfire-crisis,"GTC 2020 S22405
Presenters: Jacci Cenci-McGrody,NVIDIA ; Douglas O’Flaherty,IBM; Bogdan Pugach,AeroVironment, Inc.; Eric Rowell,Tall Timbers Research Station & Land Conservancy
Abstract
Building an AI pipeline is emerging as a critical need across many industries and applications. See and learn how this is being applied when building a smoke detection model using UAS Video Imagery for Prescribed Fire Management. Drones or unmanned aerial systems are likely to be one of the next big changes in fire service and many other use cases. Learn about building a collaborative AI data pipeline to address:• Thermal imaging for hot spots, structural or large commercial fires, natural disaster response;
• Applied AI through the lens of seeing how hazard reduction happens through fire authorities, national park staff, and business individual property owners who are using AI to battle the global wildfire crisis;
• Algorithms, and how they are deployed to fight future wildfires; and
• Smoke detection in UAS Video Imagery for Prescribed Fire Management.Watch this session
Join in the conversation below.If anyone wants to know more about the technology used, data points, or any details, please feel free to ask here. We are monitoring this site for a month. Sorry we could not do this live.Powered by Discourse, best viewed with JavaScript enabled"
2374,explainer-what-is-explainable-ai,"Originally published at:			What Is Explainable AI (XAI)? | NVIDIA Blog
Our trust in AI will largely depend on how well we understand it — explainable AI, or XAI, helps shine a flashlight into the “black box” of complexity in AI models.Powered by Discourse, best viewed with JavaScript enabled"
2375,new-ray-tracing-ai-cloud-and-virtual-world-tools-simplify-game-development-at-gdc-2022,"Originally published at:			https://developer.nvidia.com/blog/new-ray-tracing-ai-cloud-and-virtual-world-tools-streamline-game-development-at-gdc-2022/
NVIDIA announced a number of new tools for game developers at this year’s GDC to help you save time, more easily integrate RTX, and simplify the creation of virtual worlds.Powered by Discourse, best viewed with JavaScript enabled"
2376,gtc-2020-training-models-20x-faster-on-gpu-in-medical-image-analysis,"GTC 2020 S21590
Presenters: Dong Yang,NVIDIA
Abstract
Analyzing high-dimensional medical images (2D/3D/4D CT, MRI, histopathological images, and so on) plays an important role in many biomedical applications, such as anatomical pattern understanding, disease diagnosis, and treatment planning. The AI-assisted models have been widely adopted in the domain of medical image analysis with great successes. However, training such models with large-size data is expensive in terms of computation and memory consumption. We’ll provide solutions for improving model-training efficiency, which will speed up the training of AI models by 20x and enable researchers and radiologists to improve the efficiency in their clinical studies. The overall efficiency improvement comes from both improved algorithms and engineering advance. Needless to say, this work is also an excellent showcase for the collaboration among different NVIDIA teams.Watch this session
Join in the conversation below.Powered by Discourse, best viewed with JavaScript enabled"
2377,gridspace-presents-an-ai-based-call-center-agent,"Originally published at:			Gridspace Presents an AI-Based Call Center Agent | NVIDIA Technical Blog
Gridspace, a southern California-based company, recently presented an end-to-end deep learning based-solution that can allow businesses to automate the call center process by using NVIDIA GPUs on the cloud. In an example shown at GTC Silicon Valley, the company presented a video of an AI generated voice interacting with a customer as if it were…Powered by Discourse, best viewed with JavaScript enabled"
2378,kitware-paraview-omniverse-connector-now-available-in-open-beta,"Originally published at:			https://developer.nvidia.com/blog/kitware-paraview-omniverse-connector-now-available-in-open-beta/
NVIDIA Omniverse has expanded to address the scientific visualization community with aConnector to Kitware ParaView, one of the world’s most popular scientific visualization applications.Powered by Discourse, best viewed with JavaScript enabled"
2379,nvidia-merlin-deepens-commitment-to-deep-learning-recommenders-with-latest-beta-update,"Originally published at:			NVIDIA Merlin Deepens Commitment to Deep Learning Recommenders with Latest Beta Update | NVIDIA Technical Blog
Recently, NVIDIA CEO Jensen Huang announced NVIDIA Merlin, an end-to-end deep learning recommender framework, entered open beta during his GTC Keynote. When data scientists and machine learning engineers seek to build and scale their recommenders, they often face challenges with feature engineering, preprocessing, training, and performance. Merlin is designed to address these challenges and enables…Powered by Discourse, best viewed with JavaScript enabled"
2380,deep-learning-to-help-preserve-privacy-from-wearable-cameras,"Originally published at:			Deep Learning to Help Preserve Privacy from Wearable Cameras | NVIDIA Technical Blog
Wearable cameras are opening up exciting new applications, but will also require new techniques to help people preserve their privacy. A group of researchers from Indiana University and Olin College used the Caffe deep learning framework and a Tesla K20 GPU to automatically detect private content on monitors that people may not want to be…Powered by Discourse, best viewed with JavaScript enabled"
2381,gpus-help-find-a-massive-new-reef-hiding-behind-great-barrier-reef,"Originally published at:			GPUs Help Find a Massive New Reef Hiding Behind Great Barrier Reef | NVIDIA Technical Blog
Australian scientists made a significant discovery hiding behind the world-famous Great Barrier Reef. The discovery was made using cutting-edge surveying technology, which revealed vast fields of doughnut-shaped mounds measuring up to 300 meters across and up to 10 meters deep. “We’ve known about these geological structures in the northern Great Barrier Reef since the 1970s and…Powered by Discourse, best viewed with JavaScript enabled"
2382,gtc-2020-ultra-fast-radar-simulation-for-radar-system-design-and-automotive-applications,"GTC 2020 S21966
Presenters: Jeff Decker,ANSYS
Abstract
Ultra-fast, physics-based radar simulations are required to design and deploy hardware-in-the-loop systems for applications such as autonomous vehicles and driver assistance systems (ADAS/AV). Rapid radar image generation is also an enabling technology for AI algorithms, vastly expanding the data sets to train and test the algorithms. We’re developing an ultra-fast, end-to-end, GPU-accelerated radar image simulation engine for automotive, AI, and other applications. The shooting and bouncing rays (SBR) technique generates physics-based range-Doppler images of dynamic driving scenarios in urban settings. GPU kernels using CUDA, OptiX, and cuFFT propagate radar energy from the radar, through the scene, and back to the receiver to generate range-Doppler images displaying distance and relative velocity of surrounding objects. ANSYS will discuss the new solver and show results of automotive scenarios in busy, complex environments. The solver shows promise toward our goal of ultra-fast radar simulation.Watch this session
Join in the conversation below.Powered by Discourse, best viewed with JavaScript enabled"
2383,ask-me-anything-series-nvidia-experts-answer-your-questions-live,"Originally published at:			Ask Me Anything Series: NVIDIA Experts Answer Your Questions Live | NVIDIA Technical Blog
Join NVIDIA experts and the developer community on July 28 for our first Ask Me Anything.Hi, i use a nvidia G210M and i have a problem with my graphics driverI recently installed the latest version of my graphics driver, then it zoomed in my screen and showed an error code 43 on device managerI tried reinstalling the driver’s again but still didn’t workCan you please help me resolve this issueHi, i use a nvidia G210M and i have a problem with my graphics driverI recently installed the latest version of my graphics driver, then it zoomed in my screen and showed an error code 43 on device managerI tried reinstalling the driver’s again but still didn’t workCan you please help me resolve this issueHi @mariodemodder,Welcome to the NVIDIA Developer forums. You posted in the blogs section of the developper community. There is no support given at this section. If this is a consumer card issue, you should post to the Geforce forums. For enterprise and developer issues, please post in the drivers category.Best regards,
Tom Kwhile  running this command “!pkexec dpkg -i ~/Downloads/cuda-repo-ubuntu1804-10-0-local-10.0.130-410.48_1.0-1_amd64.deb”:
i am getting this and it is staying as it isSelecting previously unselected package cuda-repo-ubuntu1804-10-0-local-10.0.130-410.48.
(Reading database … 279400 files and directories currently installed.)
Preparing to unpack …/cuda-repo-ubuntu1804-10-0-local-10.0.130-410.48_1.0-1_amd64.deb …
Unpacking cuda-repo-ubuntu1804-10-0-local-10.0.130-410.48 (1.0-1) …
Setting up cuda-repo-ubuntu1804-10-0-local-10.0.130-410.48 (1.0-1) …
what to do?Powered by Discourse, best viewed with JavaScript enabled"
2384,gtc-2020-deep-learning-profiling-technologies,"GTC 2020 CWE21282
Presenters: Poonam Chitale,NVIDIA; Timothy Gerdes, NVIDIA; Chris Vinson, NVIDIA; Elias Bermudez, NVIDIA; Jonathan Dekhtiar, NVIDIA
Abstract
n this session, we’ll provide guidance to data scientists and deep learning researchers who are trying to optimize their networks to take advantage of the high performance that GPUs have to offer. NVIDIA has been working on profiling tools and technologies that make profiling part of the workflow. To construct high-quality models that train faster, you can profile them and understand which operations are taking up the most time and which iterations contribute to maximum utilization of tensor cores, then get recommendations on where performance can be improved. There are several technologies and they vary in the output reports they provide, as well as the visualization provided. Talk to experts to understand which tools to use when.Connect directly with NVIDIA Experts to get answers to all of your questions on GPU programming and code optimization, share your experience, and get guidance on how to achieve maximum performance on NVIDIA’s platform.Watch this session
Join in the conversation below.Powered by Discourse, best viewed with JavaScript enabled"
2385,exploring-a-career-in-ai-with-sophia-abraham,"Originally published at:			Exploring a Career in AI with Sophia Abraham | NVIDIA Technical Blog
Get an inside look at the pursuit of a career in AI from the perspective of a computer science Ph.D. student.Powered by Discourse, best viewed with JavaScript enabled"
2386,how-to-manage-virtual-environments-and-automate-testing-with-tox,"Originally published at:			https://developer.nvidia.com/blog/how-to-manage-virtual-environments-and-automate-testing-with-tox/
Learn how to use tox to automate workflows and manage virtual environments, as well as standardize and automate tests in Python.Powered by Discourse, best viewed with JavaScript enabled"
2387,improving-video-quality-and-performance-with-av1-and-nvidia-ada-lovelace-architecture,"Originally published at:			https://developer.nvidia.com/blog/improving-video-quality-and-performance-with-av1-and-nvidia-ada-lovelace-architecture/
NVIDIA NVENC AV1 offers substantial compression efficiency with respect to H.264 and HEVC at better performance.Would it be possible to add AV1 information to NVML?Just wondering if its possible to add support for 4:4:4 unless this is a hardware limitation?AV1 4:4:4 encoding is not supported at present. We may consider it in future hardware generations.We are investigating adding AV1 information in the next update of NVML.That would be great. I don’t have a 40 series card myself but I’m sure as they gain adoption people will be looking for that information.A suggestion regarding the API design: it would be great if all of the encoders/decoders information could be aligned behind a well-documented single function as much as possible so that application developers who don’t have the hardware are more confident that AV1 information reporting works. It would also be nice if existing NVML functions could be better documented on how they respond to different video encoders/decoders being used.Looks like work was done to fix NVML’s broken nvmlDeviceGetEncoderSessions function but nvmlEncoderType_t still only lists H264 and HEVC encoder types.  Is AV1 support still being worked on?Powered by Discourse, best viewed with JavaScript enabled"
2388,making-python-data-science-enterprise-ready-with-dask,"Originally published at:			https://developer.nvidia.com/blog/making-python-data-science-enterprise-ready-with-dask/
At NVIDIA, we are driving change in data science, machine learning, and artificial intelligence. Some of the key trends that drive us are as follows: The rise of  Python as the most-used language for data analyticsIncreased demand for highly usable distributed computingThe need for more computational powerOpen-source software becoming mainstream in industry At the intersection…Powered by Discourse, best viewed with JavaScript enabled"
2389,gtc-2020-decoding-the-dna-of-companies-achieving-highly-successful-ai-presented-by-ibm,"GTC 2020 S22349
Presenters: Elenita Elinon ,JP Morgan Chase; Michael Gale,Inc.Digital; Rania Khalaf,IBM; Gregor Stewart,Medallia, Inc.
Abstract
AI is becoming table stakes when it comes to IT transformation, but the reality is that over half of all AI programs generate no real measurable ROI. The companies that achieve AI results haven’t just embraced the idea of AI, they’ve woven it into the core of their business. New industry research commissioned by IBM illuminates the DNA of successful AI programs and investments. Learn more in our presentation and panel discussion about the key traits that separate those who thrive with AI from those who lag behind.Watch this session
Join in the conversation below.Powered by Discourse, best viewed with JavaScript enabled"
2390,ibm-watson-cto-to-keynote-at-gpu-technology-conference,"Originally published at:			IBM Watson CTO to Keynote at GPU Technology Conference | NVIDIA Technical Blog
Rob High, IBM Fellow, VP and chief technology officer for Watson, will deliver a keynote at the GPU Technology Conference in Silicon Valley on April 6. Watson was an overnight sensation when it competed on Jeopardy! against two former winners to win $1 million. Watson is among the first of a new generation of cognitive…Powered by Discourse, best viewed with JavaScript enabled"
2391,overview-of-zero-shot-multi-speaker-tts-systems-top-q-as,"Originally published at:			https://developer.nvidia.com/blog/overview-of-zero-shot-multi-speaker-tts-systems-top-qas/
The Speech AI Summit is an annual conference that brings together experts in the field of AI and speech technology to discuss the latest industry trends and advancements. This post summarizes the top questions asked during Overview of Zero-Shot Multi-Speaker TTS System, a recorded talk from the 2022 summit featuring Coqui.ai. Synthesizing a voice with…Powered by Discourse, best viewed with JavaScript enabled"
2392,open-source-simulation-expands-with-nvidia-physx-5-release,"Originally published at:			Open Source Simulation Expands with NVIDIA PhysX 5 Release | NVIDIA Technical Blog
Learn about the latest version of the NVIDIA PhysX SDK, the primary physics engine and a key foundational technology pillar of NVIDIA Omniverse.Powered by Discourse, best viewed with JavaScript enabled"
2393,beginners-guide-to-querying-data-using-sql-on-gpus-in-python,"Originally published at:			https://developer.nvidia.com/blog/beginners-guide-to-querying-data-using-sql-on-gpus-in-python/
This beginner’s guide, is the third installment of the series of articles on the RAPIDS ecosystem. The series explores and discusses various aspects of RAPIDS that allow its users solve ETL (Extract, Transform, Load) problems, build ML (Machine Learning) and DL (Deep Learning) models, explore expansive graphs, process signal and system log, or use SQL…Powered by Discourse, best viewed with JavaScript enabled"
2394,new-sdks-accelerating-ai-research-computer-vision-data-science-and-more,"Originally published at:			https://developer.nvidia.com/blog/new-sdks-accelerating-ai-research-computer-vision-data-science-and-more/
At GTC 2022, NVIDIA revealed major updates to its suite of NVIDIA AI software for developers. The updates accelerate computing in several areas, such as machine learning research with NVIDIA JAX, AI imaging and computer vision with NVIDIA CV-CUDA, and data science workloads with RAPIDS. To learn about the latest SDK advancements from NVIDIA, watch…Powered by Discourse, best viewed with JavaScript enabled"
2395,tensorrt-3-faster-tensorflow-inference-and-volta-support,"Originally published at:			TensorRT 3: Faster TensorFlow Inference and Volta Support | NVIDIA Technical Blog
NVIDIA TensorRT™ is a high-performance deep learning inference optimizer and runtime that delivers low latency, high-throughput inference for deep learning applications. NVIDIA released TensorRT last year with the goal of accelerating deep learning inference for production deployment. A new NVIDIA Developer Blog post introduces TensorRT 3, which improves performance over previous versions and adds new features that make it easier…Powered by Discourse, best viewed with JavaScript enabled"
2396,understanding-natural-language-with-deep-neural-networks-using-torch,"Originally published at:			https://developer.nvidia.com/blog/understanding-natural-language-deep-neural-networks-using-torch/
This post was co-written by Soumith Chintala and Wojciech Zaremba of Facebook AI Research. Language is the medium of human communication. Giving machines the ability to learn and understand language enables products and possibilities that are not imaginable today. One can understand language at varying granularities. When you learn a new language, you start with…The performance comparison section could use some clarification. Both the CPU type as well as the means of parallelization used in the CPU code are omitted. One can just hope that it's not some low-end several years old i7 CPU (mobile? do desktop i7 with 2.7 GHz exist even?) that you compare the latest GeForce cards against. The means of parallelization in the CPU code is not mentioned either. And than we have not even talked about power consumption and other aspects.Given that past, even very recent, posts on this blog have made highly questionable and rather unfair performance comparisons, I can not help thinking that the same is happening here.I'm afraid that in a more honest comparison, e.g. i7-5930K vs GTX 980 or 2x 4790K vs GTX 980 the 5-10x difference shown above will change drastically.It is time for changing the culture of sloppy comparisons - and here I'm giving the authors the benefit of the doubt that this was just a mistake.Hi pSz!Soumith should respond to your specific comments, but I'd like to discuss your claim that ""past, even very recent, posts on this blog have made highly questionable and rather unfair performance comparisons.""I think this claim is quite untrue, but if you can provide some pointers to specific examples, I'd be happy to investigate.I think that the ""culture of sloppy comparisons"" actually changed years ago, and at NVIDIA we are very careful with accelerated computing comparisons. Again, if you can point out specific examples on this blog that you think are ""sloppy"" I'd be happy to look into the details.MarkMark, coincidentally, it is your recent 7.0 RC overview blog article that compares performance of the new solver library performance on K40 against some 4-5 years old Sandy Bridge desktop CPU.Hi pSz. Hope you are enjoying GTC! I have updated the post with a more recent comparison to a quite expensive recent Haswell-based Xeon CPU. http://devblogs.nvidia.com/...Thanks for fixing it, I really appreciate your responsiveness to criticism! I have not seen any comment by Soumith, though.I hope the cuSOLVER home page also get gets updated soon: https://developer.nvidia.co... !Hey pSz,While your skepticism is good, I've given the source code (and instructions) on how to run the code right in the blogpost, there is nothing to really hide here.I've rerun the benchmark on the Digits box which has: 6-core Intel(R) Core(TM) i7-5930K CPU @ 3.50GHz. Hopefully that satisfies your constraints for a fair comparison. It takes 207 mins in total on CPU and 29 mins on GPU (which is actually an even more speedup than the benchmark I've given in the blogpost). The BLAS used is OpenBLAS (latest trunk) with multiple cores enabled for all blas operations.A full gist is here for you:https://gist.github.com/sou...p.s.: sorry for the delayed reply. This is the earliest that I could take time to rerun the benchmarks on something more appropriate.--SoumithThanks Soumith for the clarification.I still believe that to have a complete description of what your ""Table 2"" compares at least the following information is necessary:- exact model number of the CPU;- compiler, flags, etc. as well as means of parallelization used in the CPU code (or a reference to what the code used is capable of, e.g. threading, SIMD);- amount of resources used on the CPU to execute the experiment (number of threads, HT on/off, turbo boost, etc.)The same applies to the GPU code and experiments done with it! Without all of that, IMHO such comparisons belong to the blogs of the specialist communities who may not care much about such ""irrelevant"" technicalities.> I've given the source code (and instructions) on how to run the code right in the blogpost, there is nothing to really hide here.I suspect that you're missing my point, after this comment even more so. This is the ""Parallel Forall blog"" and not a machine learning specialist one. Based on the description (http://devblogs.nvidia.com/... the blog advertises itself as focused on ""detailed technical information on a variety of massively parallel programming topics"" and mentions ""high-performance programming techniques"" as one of the topics discussed. Finally, in the last paragraph it highlights how GPU computing claims its space in the world of HPC/scientific computing.To live up to these ideals, I believe at least this blog (but preferably the entire HPC/computing division at NVIDIA, including marketing) needs to become (even) better at being more ""C"" and less ""H"". To be successful with a ""parent and provider"" living off of the gamer community and opponents as well as partners deeply embedded in the scientific and technical computing world (and the minds of those part of it), I think it is highly beneficial, if not necessary, for new players like GPUs and NVIDIA to be as honest as possible to the coders visiting such a technical blog. And if the competition pulls dirty tricks, disprove their numbers - or ask the community to do it. I'm sure many will gladly contribute to the extent possible!BTW, Mark: it just occurred to me that you explicitly refer to the particularly high price the CPU you compare against.That reminded me of something... so I did a quick search - and although I'm fine with comparing socket-vs-board -, to be fair I should share what I have found:- The Xeon E5-2697v3 costs $2,769.99*, has a 145W TDP;- The Tesla K40 costs $3,499.99*, has a 235W TDP.I can only wonder what would a comparison between a pair of CPUs that match the price and TDP of the K40, e.g. two E5-2680v3: 2x$1,799.99* / 2x120W (capped at 117.5W each :).*Prices from newegg.compSz, have you considered the cost of memory? The Xeon CPU price does not include 12GB of GDDR5 RAM with SSE, while the K40 price does. The Xeon CPU TDP does not include the power for the memory, while the K40 TDP does.Good point! However, the GPU does need a host too, doesn't it?Today this host would most often have at least as much memory as the GPU and in fact you will rarely have drastically less memory in GPU-equipped servers than in non-GPU equipped ones (given a fixed set of use-cases with a certain memory requirement). Plugging the K40 into a desktop box defeats the purpose of the Tesla (among other things its ECC), so the actual difference between the CPU-only and the CPU+GPU server platform that we should be comparing will likely boil down to two cases. Either single-socket + GPU vs dual socket without GPU comparison if density is not a concern. However, as a single-socket machine will have only so much memory bandwidth and PCI lanes on the CPU-side, quite likely a very realistic comparison is dual-socket lower-end CPU + GPU (e.g. 2x2620v3 + 2xK40) vs dual-socket higher-end CPU (e.g. 2x2680v3).PS: You could of course throw in a third class of systems into the comparison: exotic stuff like the One Stop Systems 16-way 3U HDCA box which gives 14*16=224 GPUs/42U rack if we don't count switches and hosts, so more realistically 12*16=192 GPUs/rack (if feeding this beast with power and dissipating its heat is even possible in this setup). For workloads that are *very* parallel and very GPU-friendly such a system can do miracles. However, even this density is not unheard of in the CPU-only world. Take the Dell PowerEdge FX2 (up to four half-wide 1U 2s server module into 2U) which allows ~164 sockets in 42U = a rack, or the Supermicro MicroBlade (up to 28 2s modules in 7U) and based on specs up to 192 sockets/42U rack.I've given you the exact model number of CPU in the first comment. It is multi-thread capable (using OpenMP where appropriate) and it is SIMD enabled for BLAS operations (the sigmoid and softmax are not SIMD, understandably so, as the instructions to do SIMD for those operations are not obvious or universal). Amount of resources used on CPU is not recorded.Perhaps I'm confused, but where exactly does your comment state what your article's ""Table 2"" compares against? And in my humble opinion, your article needs amending rather than comments that provide _additional_ data rather than fixing the existing stuff.Please consider Mark's actions as inspiration. Instead of posting additional random info in the comments section, he actually posted new data and complete benchmark info *first* and foremost.Let me say it again: I applaud his prompt and effective actions. At the same time we are instead arguing here (you and me) about something simple and straightforward: that the benchmark data in your article is incomplete (and possible bogus).Soumith, thanks for the wonderful post! It was very interesting to read your blog. At any rate, I have a few questions about the source code for the LSTM.  How can I get help / advice understanding the code?  Specifically, I am wondering about the lines that look like the following:local i2h = nn.Linear(params.rnn_size, 4*params.rnn_size)(x)What exactly is the variable x doing there?  Why is the function in the foillowing format?foo (parm1, param2) (param3)  ?This would make sense if foo returns a function, but it doesn't seem that way...Thanks in advance for your help!-HidekazuHi Hidekazu,I wouldn't do justice to explaining this, compared to this excellent post by Adam Paszke who tears apart that piece of code and explains it with the help of nice diagrams and math:http://apaszke.github.io/ls...Thanks very much for the reference!  This is very helpful!Hi Walt. What do you mean by: ""how do I talk to it""?Thanks a lot for such easily understandable example of LSTMHi Soumith,In the TDNN/CNN example, I believe there's some issue with this line:m:add(nn.TemporalConvolution(sentenceLength, 150, embeddingSize))From the nn readme for Temporal Convolution:module = nn.TemporalConvolution(inputFrameSize, outputFrameSize, kW, [dW])In above example, I guess the input-frame size is embeddingSize, and  outputFrameSize is 150.Hence, this seems more appropriate:m:add(nn.TemporalConvolution(embeddingSize, 150))Powered by Discourse, best viewed with JavaScript enabled"
2397,new-ai-approach-automatically-creates-maps-from-satellite-images,"Originally published at:			New AI Approach Automatically Creates Maps from Satellite Images | NVIDIA Technical Blog
There are over 20 million miles of roads across the globe, and many of them have not yet been mapped. This problem creates many roadblocks for digital maps, particularly for systems developed for autonomous vehicles. To solve the problem, researchers at MIT developed a deep learning method called RoadTracer to build roadmaps from aerial images…Powered by Discourse, best viewed with JavaScript enabled"
2398,introduction-to-nvidia-rtx-and-directx-ray-tracing,"Originally published at:			Introduction to NVIDIA RTX and DirectX Ray Tracing | NVIDIA Technical Blog
“Ray tracing is the future, and it always will be!” has been the tongue-in-cheek phrase used by graphics developers for decades when asked whether real-time ray tracing will ever be feasible. Everyone seems to agree on the first part: ray tracing is the future. That’s because ray tracing is the only technology we know of…I'm glad to see this is finally getting traction. I'm looking forward to videogames that aren't high frames per second shooters to using this, hopefully across the board. I play a lot of adventure games and similar that would look beautiful with this. It'll be a great day if developers can hit a switch in Unity or UE and just have it work after defining some light sources.Can you tell us if Volta tensor unit is required for DXR acceleration? Wondering what I would lose by using this on a P6000 for example.Linux support?Did the ""DirectX"" part just fly under your radar?Coool! Combine this with IBM's photon based super computer circuitry and we'll be onto sumthin' for real.Might not say anything about Vulkan, but I'm pretty sure that Nvidia's raytracing technology will also affect that. Therefore, Linux support.I'm really glad that Nvidia is finally getting more serious about DX12. We can finally start seeing games being released on that API rather than the lackluster of DX11. Hopefully we also see some Vulkan titles come out more rather than just DX.https://www.phoronix.com/sc... part just fly under your radar?NVidia's general lack of gains on DX12 vs DX11 was more of a reflection of their superior DX11 drivers and better utilization of their GPU's. So many people seem confused by this.ASync Compute on AMD GPU's helped games like DOOM more fully utilize the processors as they were otherwise having difficulty utilizing the GPU fully.That doesn't make one product better or worse.I believe NVidia said we MAY see DXR support in products without Tensor Cores but that the difference in performance would be huge thus you'd need Tensor Cores for any real-time applications like games.Ray-Tracing may have been announced recently but don't expect much working software for a while.We will see a MIXTURE of traditional rasterization and ray-tracing techniques for many years to come. This will be a slow process, though I would imagine a 2028 game console would be designed specifically with ray-tracing in mind.I suspect we may not see any games or mainstream GPU's with ray-tracing until 2020. I think it will be mostly development hardware/software before that point then we'll see some games with again a mixture that may only make sense on the more expensive cards.DX12 only.DXR runs on top of DX12, and there's zero chance they will integrate this with DX11 since that would be a coding nightmare.Not sure why it would matter since you'd need a video card with TENSOR CORES and that will support DX12 by default. The only reason you'd care is if  you had W8.1 or W7 as your OS.Of course NVidia's RT code will end up in applications based on Vulkan. Which in turn means they can be used in Linux. It's simply a matter of which applications and when.I'm more curious whether 2020 game consoles like the PS5 will have anything like Tensor Cores to support a similar AMD method. I'm a PC gamer but adding ray-tracing (even if just a mix of 90% rasterization and say 10% ray-tracing) would push ray-tracing on PC much, much faster.amd's rapid packed math does fp16 at twice the rate of fp32 so in a sense they already have tensor cores....kinda.  well not exaclty but tensor cores do fp16 ops at twice the rate and so a 12 tflop console power by amd hardware would have 24 tflops of fp 16 so quite a bit below the titan v at 110 i think it was but the titan v is 3000 dolllars are 800mm^2 or so.Is this global illumination made in two passes? First pass should improve only baking lightning information into vertexes. Second pass with rays interpolate this baked lightning for accurate one-pounce reflections. Finally everything is mixed up with material properties. However i do not know how soft shadows are calculated.With pure ray-tracing shadows come crisp. Have these new shading languages something for shadow calculation? Something special.Do you plan to make ray-triangle intersection builtins and BVH-acceleration routines disposable for developers directly from CUDA without OptiX or other media involved?Powered by Discourse, best viewed with JavaScript enabled"
2399,optimizing-large-scale-sparse-volumetric-data-with-nvidia-neuralvdb-early-access,"Originally published at:			https://developer.nvidia.com/blog/optimizing-large-scale-sparse-volumetric-data-with-nvidia-neuralvdb-early-access/
Building on the past decade’s development of OpenVDB, the introduction of NVIDIA NeuralVDB is a game-changer for developers and researchers working with extremely large and complex datasets. The pre-release version of NVIDIA NeuralVDB brings AI and GPU optimization to OpenVDB, delivering up to a 100x reduction in memory footprint for smoke, clouds, and other sparse…This is incredible. Posted to twitter.Powered by Discourse, best viewed with JavaScript enabled"
2400,announcing-megatron-for-training-trillion-parameter-models-nvidia-jarvis-availability,"Originally published at:			Announcing Megatron for Training Trillion Parameter Models & NVIDIA Riva Availability | NVIDIA Technical Blog
NVIDIA announced several major breakthroughs in conversational AI that will bring in a new wave of conversational AI applications.Powered by Discourse, best viewed with JavaScript enabled"
2401,advanced-api-performance-variable-rate-shading,"Originally published at:			https://developer.nvidia.com/blog/advanced-api-performance-variable-rate-shading/
This post covers best practices for variable rate shading on NVIDIA GPUs.Thank you for reading the post.  For more details on the NVIDIA Adaptive Shading (NAS) algorithm, check out this page which has more information along with links to the technical paper and GDC presentation.
Please let us know if you have any questions or comments.Powered by Discourse, best viewed with JavaScript enabled"
2402,gtc-2020-toward-int8-inference-deploying-quantization-aware-trained-networks-using-tensorrt,"GTC 2020 S21664
Presenters: Dheeraj Peri,NVIDIA; Jhalak Patel,NVIDIA
Abstract
We’ll describe how TensorRT can optimize the quantization ops and demonstrate an end-to-end workflow for running quantized networks. Accelerating deep neural networks (DNN) is a critical step in realizing the benefits of AI for real-world use cases. The need to improve DNN inference latency has sparked interest in lower precision, such as FP16 and INT8 precision, which offer faster inference time. Two prevalent techniques to convert FP32 DNNs to INT8 precision are post-training quantization and quantization-aware training (QAT). TensorRT, a platform for high-performance deep learning inference, supports post-training quantization by performing calibration on the trained model, which quantizes the weights and activations. However, in some cases post-training quantization can degrade accuracy when converting a FP32 model to its INT8 counterpart. QAT introduces quantization ops to achieve higher accuracy by simulating the process for lower-precision quantization during training.Watch this session
Join in the conversation below.Powered by Discourse, best viewed with JavaScript enabled"
2403,nvidia-volta-gpus-power-the-world-s-fastest-supercomputer,"Originally published at:			NVIDIA Volta GPUs Power the World’s Fastest Supercomputer | NVIDIA Technical Blog
The U.S. Department of Energy’s Oak Ridge National Laboratory (ORNL)  today announced Summit as the world’s most powerful supercomputer. The GPU-accelerated supercomputer, equipped with 27,648 NVIDIA Volta GPUs and configured using the high-speed NVLink interconnect, is designed for research in energy, advanced materials, and artificial intelligence. The system can perform more than three exaops, or…Powered by Discourse, best viewed with JavaScript enabled"
2404,iclone-connector-adds-breakthrough-character-animation-workflow-to-nvidia-omniverse,"Originally published at:			iClone Connector Adds Breakthrough Character Animation Workflow to NVIDIA Omniverse | NVIDIA Technical Blog
Learn about iClone Connector, which easily and efficiently adds digital character animation capabilities to NVIDIA Omniverse.Powered by Discourse, best viewed with JavaScript enabled"
2405,share-your-science-calculating-high-accuracy-molecular-energies-with-gpus,"Originally published at:			Share Your Science: Calculating High-Accuracy Molecular Energies With GPUs | NVIDIA Technical Blog
Janus Juul Eriksen, a Ph.D. fellow at Aarhus University in Denmark, shares how he is using OpenACC to optimize and accelerate the quantum chemistry code LSDalton on the Titan Supercomputer at Oak Ridge National Laboratory. “OpenACC makes GPU computing approachable for domain scientists,” said Eriksen. “Initial OpenACC implementation required only minor effort, and more importantly,…Powered by Discourse, best viewed with JavaScript enabled"
2406,nasa-and-nvidia-collaborate-to-accelerate-scientific-data-science-use-cases-part-2,"Originally published at:			https://developer.nvidia.com/blog/nasa-and-nvidia-collaborate-to-accelerate-scientific-data-science-use-cases-part-2/
Over the past couple of years, NVIDIA and NASA have been working closely on accelerating data science workflows using RAPIDS, and integrating these GPU-accelerated libraries with scientific use cases. This is the second post in a series that will discuss the results from an air pollution monitoring use case conducted during the COVID-19 pandemic, and…Powered by Discourse, best viewed with JavaScript enabled"
2407,build-your-own-ai-powered-q-amp-a-service,"Originally published at:			https://developer.nvidia.com/blog/build-your-own-ai-powered-qa-service/
You can now build your own AI-powered Q&A service with the step-by-step instructions provided in this four-part blog series.Powered by Discourse, best viewed with JavaScript enabled"
2408,nvidia-simnet-v20-12-released,"Originally published at:			https://developer.nvidia.com/blog/nvidia-simnet-v20-12-released/
NVIDIA recently announced the release of SimNet v20.12 with support for new physics such as Fluid Mechanics, Linear Elasticity and Conductive as well as Convective Heat Transfer. Systems governed by Ordinary Differential Equations (ODEs) as well as Partial Differential Equations (PDEs) can now be solved.  Previously announced in September, NVIDIA SimNet is a Physics Informed Neural Networks (PINNs) toolkit for students and researchers who are either…Powered by Discourse, best viewed with JavaScript enabled"
2409,this-ai-drone-can-be-controlled-by-your-eyes,"Originally published at:			This AI Drone Can Be Controlled by Your Eyes | NVIDIA Technical Blog
Piloting a drone or an unmanned vehicle by only using your gaze sounds like a scene out of a science fiction movie, but now it’s a reality. Researchers from the University of Pennsylvania, New York University, and collaborators developed a deep learning system that uses NVIDIA GPUs to enable a user to control a drone by…Powered by Discourse, best viewed with JavaScript enabled"
2410,gtc-2020-multi-gpu-programming,"GTC 2020 CWE21103
Presenters: Jiri-Kraus,NVIDIA; Akshay-Venkatesh, ; Davide-Rossetti, ; Mathias-Wagner,
Abstract
Wondering how to scale your code to multiple GPUs in a node or cluster? Need to discuss NVIDIA CUDA-aware MPI details? This is the right session for you to ask your beginner or expert questions on multi-GPU programming, GPUDirect, NVSHMEM, and MPI. Connect with the Experts offers a range of informal sessions where you can ask experts from NVIDIA and other organizations your burning questions about a specific subject.Watch this session
Join in the conversation below.Powered by Discourse, best viewed with JavaScript enabled"
2411,ama-is-live-now-please-bring-your-questions-now,"We are excited to answer any questions about the exciting new features of CUDA 12Do you have any collection of information about CUDA with recipe for various computing solutions?Please post questions  as new topics not as replys.
I will repost this question for you.
Thanks for joining usPowered by Discourse, best viewed with JavaScript enabled"
2412,new-pgi-release-supports-v100-tensor-cores-full-c-17-language-openacc-printf,"Originally published at:			New PGI Release Supports V100 Tensor Cores, Full C++ 17 Language, OpenACC printf() | NVIDIA Technical Blog
PGI Compilers & Tools are used by scientists and engineers developing applications for high-performance computing (HPC). PGI products deliver world-class multicore CPU performance, an easy on-ramp to GPU computing with OpenACC directives, and performance portability across all major HPC platforms. New version 19.1 is available now for users with current PGI Professional support. The latest…Powered by Discourse, best viewed with JavaScript enabled"
2413,creating-robust-and-generalizable-ai-models-with-nvidia-flare,"Originally published at:			Creating Robust and Generalizable AI Models with NVIDIA FLARE | NVIDIA Technical Blog
NVIDIA FLARE v2.0 is an open-source federated learning SDK that is making it easier for data scientists to collaborate to develop more generalizable robust AI models by just sharing model weights rather than private data.Hello everyone, I have recently came across NVIDIA FLARE framework, and I like it. Unfortunately, when I was trying to simulate an experiment with NVIDIA FLARE on kubernetes (k8s), I faced “CrashLoopBackOff”  error from k8s. Also, I could not find any promising results on google.Additional Info: I was trying this on my private cluster.Also, I suspect that “CrashLoopBackoff” is due to the fl process was in sleeping state  instead of running state. To be confident, I have deployed a hello world flask app along with fl_server using the same pod and by exposing an extra port. This time, there is no “CrashLoopBackoff” error, and I deduce that this is because there is at least one process (Flask App) which is in running state. Similarly, I have followed the same workaround for the client pods too, and it worked. Now, the clients authenticated to server, but I could not perform federation using FLAdminAPI. upload_app method returned “{“details”:{“message”:“Exception: Failed to communicate with Admin Server admin on 3002: [Errno 2] No such file or directory”}” response. On top of this, all this  has been merely trail and error but could not find any promising resources that explain how to use Nvidia FLARE with k8s, so I am hoping for some resources.Powered by Discourse, best viewed with JavaScript enabled"
2414,latest-releases-and-resources-march-10-16,"Originally published at:			https://developer.nvidia.com/blog/latest-releases-and-resources-march-10-16/
Join experts from the NVIDIA at the Healthcare and Life Sciences Developer Summit at GTC; attend Omniverse sessions at GDC; and get hands-on NVIDIA Deep Learning Institute training at GTC.Powered by Discourse, best viewed with JavaScript enabled"
2415,end-to-end-ai-for-nvidia-based-pcs-optimizing-ai-by-transitioning-from-fp32-to-fp16,"Originally published at:			https://developer.nvidia.com/blog/end-to-end-ai-for-nvidia-based-pcs-optimizing-ai-by-transitioning-from-fp32-to-fp16/
This post is part of a series about optimizing end-to-end AI. The performance of AI models is heavily influenced by the precision of the computational resources being used. Lower precision can lead to faster processing speeds and reduced memory usage, while higher precision can contribute to more accurate results. Finding the right balance between precision…Powered by Discourse, best viewed with JavaScript enabled"
2416,expanding-accelerated-genomic-analysis-to-rna-gene-panels-and-annotation,"Originally published at:			https://developer.nvidia.com/blog/expanding-accelerated-genomic-analysis-to-rna-gene-panels-and-annotation/
Clara Parabricks v3.7 supports gene panels, RNA-Seq, short tandem repeats, and updates to GATK (4.2) and DeepVariant (1.1), and PON support for Mutect2.Powered by Discourse, best viewed with JavaScript enabled"
2417,new-suite-of-nsight-tools-for-gaming-and-graphics-developers,"Originally published at:			https://developer.nvidia.com/blog/new-suite-of-nsight-tools-for-gaming-and-graphics-developers/
Nsight developer tools is a suite of powerful tools and SDKs for profiling, debugging and optimizing applications focused on improving performance for graphics, gaming and other use cases. Identifying bottlenecks, highlighting code (multi-threading operations, event timing ) to improve efficiency and the unique features offerings for refined user experience.Powered by Discourse, best viewed with JavaScript enabled"
2418,accelerating-data-center-security-with-bluefield-2-dpu,"Originally published at:			https://developer.nvidia.com/blog/accelerating-data-center-security-with-bluefield-2-dpu/
BlueField-2 offers protection, while delivering high security, integrity, and reliability for the new hybrid cloud era.Powered by Discourse, best viewed with JavaScript enabled"
2419,doubling-all2all-performance-with-nvidia-collective-communication-library-2-12,"Originally published at:			https://developer.nvidia.com/blog/doubling-all2all-performance-with-nvidia-collective-communication-library-2-12/
The NCCL 2.12 release significantly improves all2all communication collective performance, with the PXN feature.Powered by Discourse, best viewed with JavaScript enabled"
2420,ai-helps-instantly-copy-your-car-or-home-keys,"Originally published at:			AI Helps Instantly Copy Your Car or Home Keys | NVIDIA Technical Blog
New York-based startup KeyMe recently raised $20 million in a Series B funding for their technologically advanced in-store kiosks that allows you to instantly print keys. Founder and CEO Greg Marsh said his goal is “disrupting the locksmith industry” by creating a key duplication service that’s more convenient, more affordable and even more accurate than…Powered by Discourse, best viewed with JavaScript enabled"
2421,texas-a-amp-m-launches-new-gpu-accelerated-supercomputer,"Originally published at:			https://developer.nvidia.com/blog/texas-am-launches-new-gpu-accelerated-supercomputer/
Texas A&M installed a new $2.1 million supercomputer with 10 times the processing power of their previous system Eos, which was launched in 2009. Nicknamed “Terra,” the new supercomputer will support projects that include developing new materials, discovering new drugs, forecasting storm surges and managing energy resources. “Terra represents a new supercomputer iteration deployed at…Powered by Discourse, best viewed with JavaScript enabled"
2422,will-pt-sdk-support-subsurface-scattering-for-human-skins,"Will PT SDK support Subsurface scattering for human skins?
if yes, in realtime mode or in reference mode?Thanks for your question, this is something we are considering and can prioritize based on feedback! It would require upgrading the BSDF and potentially denoising modifications.
If/when we implement subsurface scattering materials, will aim for support in both reference and real-time modes.This topic was automatically closed 14 days after the last reply. New replies are no longer allowed.Powered by Discourse, best viewed with JavaScript enabled"
2423,gtc-2020-raytracing-for-deep-neural-networks-training,"GTC 2020 T22124
Presenters: Team,NVIDIA
Abstract
Learn how to prepare virtual scene and render images dedicated to train deep convolutional neural networks. Make sure that the model trained in virtual reality can be efficient and accurate in the real world. We present Sky Engine, the very first raytracer designed for data generation for deep neural networks training. Our NVIDIA OptiX based raytracer is tightly integrated with the PyTorch framework and can render images, semantic masks, and depth fields on the fly. Sky Engine can change scene parameters during the training process to balance the dataset automatically and detect most scene configurations that will confuse the model.Watch this session
Join in the conversation below.Powered by Discourse, best viewed with JavaScript enabled"
2424,share-your-science-visualizing-protein-structures-with-high-performance-computing,"Originally published at:			Share Your Science: Visualizing Protein Structures with High Performance Computing | NVIDIA Technical Blog
Dr. Wojtek Goscinski, High Performance Computing Manager at the Monash University eResearch Center, shares how his team is using supercomputers accelerated by NVIDIA Tesla GPUs to process and analyze big data to create visualizations of large-scale molecular datasets. Dr. Goscinski mentions they are able to process their data within hours with GPUs as opposed to…NVDA Shareholder with an idea for development:
Holograms:Picture this and then imagine…A special occasion
is coming up, (Birthday,anniverary,wedding,graduation)
and you make a reservation for dinner at a personalized
venue with multiple choices for your entertainment.
An evening at this dinner event goes like this:
A pre selected famous figure chosen by you (a hologram of
John Wayne) guides you to your parking space.
Upon entering the venue, a Hostess greets you and asks for your
reservation name which includes your list of options
for the evening. A hologram of Frankenstein appears and
escorts you to your dinner room of choice.
A hologram of Star Wars R2D2 shows you to your table.
You have chosen the Storm room venue. A hologram of an
approaching thunderstorm begins to appear and fills the
room with clouds, thunder and lightning complete with
sound effects.
A hologram of a much larger than life flying hummingbird slowly
materializes above your table to take your order.
After dinner you all decide to stop by the wild west bar
for a drink. Various holograms of wild west characters having
gun fights appear in different areas of the room.
You sit down at the bar with a full mirrored back bar and in
the mirror various figures (chosen by you beforehand) appear
in the mirror on the bar stool next to you.Imagine:
The ultimate 3D experience…Hologram fantasies of choice.
nvidia is one of very few company’s with the technological
abilities to make this happen.
Sincerely,
Bryan Mailliard
B &C Mailliard
480 694 1367Powered by Discourse, best viewed with JavaScript enabled"
2425,winning-mlperf-inference-0-7-with-a-full-stack-approach,"Originally published at:			Winning MLPerf Inference 0.7 with a Full-Stack Approach | NVIDIA Technical Blog
Three trends continue to drive the AI inference market for both training and inference: growing data sets, increasingly complex and diverse networks, and real-time AI services. MLPerf Inference 0.7, the most recent version of the industry-standard AI benchmark, addresses these three trends, giving developers and organizations useful data to inform platform choices, both in the datacenter…Powered by Discourse, best viewed with JavaScript enabled"
2426,assisting-farmers-with-artificial-intelligence,"Originally published at:			https://developer.nvidia.com/blog/assisting-farmers-with-artificial-intelligence/
With our planet getting warmer and warmer, and carbon dioxide levels steadily creeping up, companies are using deep learning to help cope with the effects that climate change is having on their crops.    An article on MIT Technology Review highlights PEAT, a German company using CUDA, TITAN X GPUs and the cuDNN-accelerated Caffe deep…Powered by Discourse, best viewed with JavaScript enabled"
2427,share-your-science-orthopedic-virtual-reality-surgical-simulation-platform,"Originally published at:			https://developer.nvidia.com/blog/share-your-science-orthopedic-virtual-reality-surgical-simulation-platform/
Justin Barad, CEO of OSSO VR shares how they are using NVIDIA GPUs and the latest virtual reality technologies to create a low-cost immersive surgical simulator designed to train surgeons to use orthopedic devices. Barad, a trained orthopedic surgeon, shares how students are graduating from medical schools without learning how to do certain procedures and…Powered by Discourse, best viewed with JavaScript enabled"
2428,gtc-21-top-5-automotive-technical-sessions,"Originally published at:			https://developer.nvidia.com/blog/gtc-21-top-5-automotive-technical-sessions/
Register for the free conference to hear talks from Audi Board Member Hildegard Wortmann, Zoox CTO Jesse Levinson, University of Toronto Professor Raquel Urtasun, and Cruise SVP of Engineering Mo ElShenawy.Powered by Discourse, best viewed with JavaScript enabled"
2429,accelerating-edge-computing-with-a-smarter-network,"Originally published at:			https://developer.nvidia.com/blog/accelerating-edge-computing-with-a-smarter-network/
As computing power moves to edge computing, NVIDIA announces the EGX platform and ecosystem, including server-vendor certifications, hybrid cloud partners, and new GPU Operator software.Powered by Discourse, best viewed with JavaScript enabled"
2430,state-of-rapids-bridging-the-gpu-data-science-ecosystem,"GTC 2020 S22181
Presenters: Joshua Patterson, NVIDIA
Abstract
See how RAPIDS and the open-source ecosystem are advancing data science. We’ll explore RAPIDS, the end to end open-source data science accelerator from NVIDIA. You’ll learn how to start leveraging RAPIDS and the libraries it interacts with for faster performance and easier development on GPUs. See the latest engineering work, new release features, and future roadmap. Finally, see how RAPIDS works with leading OS tools like BlazingSQL, Dask, Plot.ly, PyTorch, Ray, SpaCy, and others to deliver end-to-end data science benchmarks on GPUs that shatter CPU equivalents.Watch this session
Join in the conversation below.Powered by Discourse, best viewed with JavaScript enabled"
2431,new-dli-training-accelerating-cuda-c-applications-with-multiple-gpus,"Originally published at:			https://developer.nvidia.com/blog/new-dli-training-accelerating-cuda-c-applications-with-multiple-gpus/
Computationally-intensive CUDA C++ applications in high performance computing, data science, bioinformatics, and deep learning can be accelerated by using multiple GPUs, which can increase throughput and/or decrease your total runtime. When combined with the concurrent overlap of computation and memory transfers, computation can be scaled across multiple GPUs without increasing the cost of memory transfers.…Powered by Discourse, best viewed with JavaScript enabled"
2432,mlperf-v1-0-training-benchmarks-insights-into-a-record-setting-nvidia-performance,"Originally published at:			https://developer.nvidia.com/blog/mlperf-v1-0-training-benchmarks-insights-into-a-record-setting-nvidia-performance/
MLPerf v1.0 showcases the continuous innovation that is happening in the AI domain. In the last two-and-a-half years since the first MLPerf training benchmark launched, NVIDIA performance has increased by nearly 7x. In this post, we describe some of the major optimizations that enabled such improvements.Hi I am trying to reproduce RNNT distributed training. Do you have any specific throughput/step time numbers that I can refer to? Thanks!Powered by Discourse, best viewed with JavaScript enabled"
2433,upcoming-webinar-introduction-to-building-conversational-ai-applications,"Originally published at:			https://developer.nvidia.com/blog/upcoming-webinar-introduction-to-building-conversational-ai-applications/
Join us as we introduce conversational AI concepts and how to build efficient pipelines.Powered by Discourse, best viewed with JavaScript enabled"
2434,optimizing-vk-vkr-and-dx12-dxr-applications-using-nsight-graphics-gpu-trace-advanced-mode-metrics,"Originally published at:			https://developer.nvidia.com/blog/optimizing-vk-vkr-and-dx12-dxr-applications-using-nsight-graphics-gpu-trace-advanced-mode-metrics/
Many GPU performance analysis tools are based on a capture and replay mechanism, where a frame is first captured (either in-memory or to disk), and then replayed multiple times to be profiled. Nsight Graphics: GPU Trace differs in that it directly profiles the frames emitted by a live application, with no constraint on subsequent frames…Powered by Discourse, best viewed with JavaScript enabled"
2435,nvidia-showcases-the-latest-in-graphics-ai-and-virtual-collaboration-at-siggraph,"Originally published at:			https://developer.nvidia.com/blog/nvidia-showcases-the-latest-in-graphics-ai-and-virtual-collaboration-at-siggraph/
Developers, researchers, graphics professionals, and others from around the world will get a sneak peek at the latest innovations in computer graphics at the SIGGRAPH 2021 virtual conference, taking place August 9-13.Powered by Discourse, best viewed with JavaScript enabled"
2436,23-000-atoms-mapped-for-first-time,"Originally published at:			23,000 Atoms Mapped for First Time | NVIDIA Technical Blog
Scientists used an extremely high-resolution transmission electron microscope to capture 2D projections of the nanoparticle’s structure, and used an algorithm to stitch those together into a 3D reconstruction. The unprecedented detail sheds light on the material’s properties at the single-atom level and the insights gained from the particle’s structure could lead to new ways to…Powered by Discourse, best viewed with JavaScript enabled"
2437,ai-model-could-predict-which-bills-get-passed,"Originally published at:			AI Model Could Predict Which Bills Get Passed | NVIDIA Technical Blog
State legislatures play a significant role in setting the laws and policies that affect citizens of a state, however, due to the decline of state political coverage in the news, many of the decisions at this level are largely ignored by the public. To help people become more familiar with voting behavior in their home…Powered by Discourse, best viewed with JavaScript enabled"
2438,neural-network-pinpoints-artist-by-examining-a-painting-s-brushstrokes,"Originally published at:			https://developer.nvidia.com/blog/neural-network-pinpoints-artist-by-examining-a-paintings-brushstrokes/
Researchers developed a new AI algorithm that can identify a painter based on brushstrokes, with precision down to a single bristle.I have a painting to be examined using your AI algorithm. Can I learn how to do that or have it done?Thanks for your interest! This story highlights amazing work from researchers at Case Western Reserve University, Cleveland Institute of Art, and the Cleveland Museum of Art. They have made the data and code associated with the research available through GitHub.I do not believe there is opportunity to submit paintings for examination at this time.Powered by Discourse, best viewed with JavaScript enabled"
2439,accelerating-quantized-networks-with-the-nvidia-qat-toolkit-for-tensorflow-and-nvidia-tensorrt,"Originally published at:			https://developer.nvidia.com/blog/accelerating-quantized-networks-with-qat-toolkit-and-tensorrt/
Introduction to the NVIDIA Quantization-Aware Training toolkit for TensorFlow 2 for model quantization for TensorRT acceleration on NVIDIA GPUs.Powered by Discourse, best viewed with JavaScript enabled"
2440,nlp-and-text-processing-with-rapids-now-simpler-and-faster,"Originally published at:			https://developer.nvidia.com/blog/__trashed-4/
This post was originally published on the RAPIDS AI Blog. TL;DR: Google famously noted that “speed isn’t just a feature, it’s the feature,” This is not only true for search engines but all of RAPIDS. In this post, we will showcase performance improvements for string processing across cuDF and cuML, which enables acceleration across diverse…Powered by Discourse, best viewed with JavaScript enabled"
2441,breaking-data-silos-by-integrating-mlops-platforms-and-edge-solutions,"Originally published at:			Breaking Data Silos by Integrating MLOps Platforms and Edge Solutions | NVIDIA Technical Blog
Seamless workflow from development to deployment with Domino Data Lab Enterprise MLOps Platform and NVIDIA Fleet Command.Powered by Discourse, best viewed with JavaScript enabled"
2442,gtc-2020-virtual-validation-of-automated-vehicle-safety-challenges-lessons-learned-and-opportunities,"GTC 2020 S21658
Presenters: Jim Cherian,CETRAN (Center for Testing and Research on Autonomous Vehicles - NTU), Nanyang Technological University, Singapore
Abstract
We’ll explain the need for virtual validation (VV) of automated vehicles (AV), identify the general strengths and weaknesses of VV, briefly review the present landscape of tools and solutions available to perform AV simulations, and list the major building blocks of a good virtual testing process (VTP). We’ll also discuss the potential of NVIDIA DRIVE Constellation toward implementing a meaningful VTP for developers, regulators, and service providers. We’ll outline some key challenges and opportunities to motivate further research. We’ll also introduce two research projects at CETRAN (Nanyang Technological University, Singapore): first, modeling the sensing and perception (S&P) errors using a Perception Error Model (PEM) and studying their impact on decision-making (AV behavior), and second, developing methods to evaluate the holistic fidelity of simulation toolchain and judge its effectiveness in representing reality. We’ll present some preliminary findings and a research roadmap.Watch this session
Join in the conversation below.Powered by Discourse, best viewed with JavaScript enabled"
2443,accelerating-load-times-for-directx-games-and-apps-with-gdeflate-for-directstorage,"Originally published at:			https://developer.nvidia.com/blog/accelerating-load-times-for-directx-games-and-apps-with-gdeflate-for-directstorage/
Load times. They are the bane of any developer trying to construct a seamless experience. Trying to hide loading in a game by forcing a player to shimmy through narrow passages or take extremely slow elevators breaks immersion. Now, developers have a better solution. NVIDIA collaborated with Microsoft and IHV partners to develop GDeflate for DirectStorage…Powered by Discourse, best viewed with JavaScript enabled"
2444,gtc-digital-hpc-presentations-demos-and-posters,"Originally published at:			GTC Digital: HPC Presentations, Demos, and Posters | NVIDIA Technical Blog
GTC Digital is all the great training, research, insights, and direct access to the brilliant minds of NVIDIA’s GPU Technology Conference, now online. Join live webinars, training, and Connect with the Experts sessions, or choose from a library of talks, panels, research posters, and demos that you can view on your own schedule, at your own…Powered by Discourse, best viewed with JavaScript enabled"
2445,steps-to-getting-started-with-edge-ai,"Originally published at:			https://developer.nvidia.com/blog/steps-to-getting-started-with-edge-ai/
Learn how to roll out a successful edge AI solution across your organization in just 5 steps.Powered by Discourse, best viewed with JavaScript enabled"
2446,accelerating-embedding-with-the-hugectr-tensorflow-embedding-plugin,"Originally published at:			https://developer.nvidia.com/blog/accelerating-embedding-with-the-hugectr-tensorflow-embedding-plugin/
Recommender systems are the economic engine of the Internet. It is hard to imagine any other type of applications with more direct impact in our daily digital lives: Trillions of items to be recommended to billions of people. Recommender systems filter products and services among an overwhelming number of options, easing the paradox of choice…Powered by Discourse, best viewed with JavaScript enabled"
2447,how-kaggle-makes-gpus-accessible-to-5-million-data-scientists,"Originally published at:			https://developer.nvidia.com/blog/how-kaggle-makes-gpus-accessible-to-5-million-data-scientists/
Kaggle is a great place to learn how to train deep learning models using GPUs. Engineers and designers at Kaggle work hard behind the scenes to make it easy for over 5 million data scientist users to focus on learning and improving their deep learning models.Powered by Discourse, best viewed with JavaScript enabled"
2448,fast-fine-tuning-of-ai-transformers-using-rapids-machine-learning,"Originally published at:			https://developer.nvidia.com/blog/fast-fine-tuning-of-ai-transformers-using-rapids-machine-learning/
Find out how RAPIDS and the cuML support vector machine can achieve faster training time and maximum accuracy when fine-tuning transformers.Powered by Discourse, best viewed with JavaScript enabled"
2449,new-data-science-client-and-wsl2-for-data-science-development-on-workstations,"Originally published at:			https://developer.nvidia.com/blog/how-to-jumpstart-data-science-workflows/
Data science development faces many challenges in the areas of: Exploration and model developmentTraining and evaluationModel scoring and inference Some estimates point to 70%-90% of the time is spent on experimentation – much of which will run fast and efficiently on GPU-enabled mobile and desktop workstations. Running on a Linux mobile workstation, for example, presents…Powered by Discourse, best viewed with JavaScript enabled"
2450,data-storytelling-best-practices-for-data-scientists-and-ai-practitioners,"Originally published at:			https://developer.nvidia.com/blog/data-storytelling-best-practices-for-data-scientists-and-ai-practitioners/
Learn how you can use data storytelling to more effectively communicate with clients, project stakeholders, team members, and other business entities.Hi Guys,Do have a read of this article and let me know if you have any questions regards to this content.Interested in hearing what topics around Data Storytelling you would like me to cover next.Regards,
RichmondPowered by Discourse, best viewed with JavaScript enabled"
2451,developing-nlp-applications-to-enhance-clinical-experiences-and-accelerate-drug-discovery,"Originally published at:			Developing NLP Applications for Healthcare | NVIDIA Technical Blog
Discover tools to translate unstructured data to structured data to help healthcare organizations harness relevant insights and improve healthcare delivery and patient experiences.Powered by Discourse, best viewed with JavaScript enabled"
2452,the-need-for-speed-edge-ai-with-nvidia-gpus-and-smartnics-part-1,"Originally published at:			The Need for Speed: Edge AI with NVIDIA GPUs and SmartNICs, Part 1 | NVIDIA Technical Blog
How to integrate the NVIDIA GPU and Network OperatorsPowered by Discourse, best viewed with JavaScript enabled"
2453,rtx-for-indies-stunning-ray-traced-lighting-achieved-with-rtxgi-in-action-platformer-escape-from-naraka,"Originally published at:			https://developer.nvidia.com/blog/rtx-for-indies-stunning-ray-traced-lighting-achieved-with-rtxgi-in-action-platformer-escape-from-naraka/
Developed by XeloGames, an indie studio of just three, and published by Headup Games, Escape from Naraka achieves eye-catching ray-traced lighting using RTXGI and significant performance boosts from DLSS.With RTX looks worse to me, yes it darker but you can fix that, but you loose reflection , and soft shadows, aslo water too clean and bright with rtx i cannot say what it’s better it’s just different shader
adjust levels on the right

2587×723 715 KB
Thank you for the post, and your keen observation! Yes, sometimes the benefits of RTX don’t align with the artistic vision of the game. A title that wants to have dark, murky water wouldn’t benefit too much from ray traced reflections, for example. In this case, the Escape from Naraka developers wanted to implement more pronounced illumination and natural lighting, and we believe RTX was able to provide a great help to them!Powered by Discourse, best viewed with JavaScript enabled"
2454,learn-gpu-computing-with-hands-on-labs-at-gtc-2015,"Originally published at:			https://developer.nvidia.com/blog/learn-gpu-computing-with-hands-on-labs-gtc-2015/
Every year NVIDIA’s GPU Technology Conference (GTC) gets bigger and better. One of the aims of GTC is to give developers, scientists, and practitioners opportunities to learn with hands-on labs how to use accelerated computing in their work. This year we are nearly doubling the amount of hands-on training provided from last year, with almost…Powered by Discourse, best viewed with JavaScript enabled"
2455,adapting-p-tuning-to-solve-non-english-downstream-tasks,"Originally published at:			https://developer.nvidia.com/blog/adapting-p-tuning-to-solve-non-english-downstream-tasks/
The scarcity of non-English labeled datasets poses serious challenges in applying large NLP models to specific tasks. Learn how we use p-tuning to solve them.Powered by Discourse, best viewed with JavaScript enabled"
2456,new-nvidia-metropolis-microservices-fast-tracks-cloud-native-vision-ai-development,"Originally published at:			https://developer.nvidia.com/blog/new-nvidia-metropolis-microservices-fast-tracks-cloud-native-vision-ai-development/
Vision AI-powered applications are exploding in terms of value and adoption across industries. They’re being developed both by sophisticated AI developers and those totally new to AI.  Both types of developers are being challenged with more complex solution requirements and faster time to market. Building these vision AI solutions requires a scalable, distributed architecture and…Does Metropolis support on-premise processing?Powered by Discourse, best viewed with JavaScript enabled"
2457,practical-tips-for-optimizing-ray-tracing,"Originally published at:			https://developer.nvidia.com/blog/practical-tips-for-optimizing-ray-tracing/
To achieve high efficiency with ray tracing, you must build a pipeline that scales well at every stage. This starts from mesh instance selection and their data processing towards optimized tracing and shading of every hit that you encounter. Instance data generation In a common scene, there can be far more static than dynamic objects.…Powered by Discourse, best viewed with JavaScript enabled"
2458,simple-portable-parallel-c-with-hemi-2-and-cuda-7-5,"Originally published at:			https://developer.nvidia.com/blog/simple-portable-parallel-c-hemi-2/
The last two releases of CUDA have added support for the powerful new features of C++. In the post The Power of C++11 in CUDA 7 I discussed the importance of C++11 for parallel programming on GPUs, and in the post New Features in CUDA 7.5 I introduced a new experimental feature in the NVCC CUDA C++ compiler:…Hi Mark,Nice article!Can you compare Hemi and OpenACC? If I want to develop an algorithm, some part might be suitable for my current CPU, other part might be suitable for my current GPU, which tool shall I choose, so I can easily switch one function/algorithm between CPU and GPU to get the best performance?Thanks!Thanks! OpenACC is a compiler-based approach to generating parallel code from directives used to annotate loops and data. Hemi is a wrapper API that makes it easy to write custom parallel functions so that they can be compiled either for the host or the device.  Hemi is meant to complement CUDA C++, not replace it. So OpenACC is potentially higher level and easier to use with existing code, while Hemi, like CUDA C++ is lower-level and therefore may afford a greater level of control.If the loops you want to accelerate with GPUs are mostly C, you may have very good luck with OpenACC (especially combined with Unified Memory), because you may be able to let the compiler do more for you.If you have complex C++ code, you may have better luck with a lower-level approach like Hemi, or using the parallel_for feature in Hemi.the article has formatting error: ""CUDA’s lt;lt;lt; gt;gt;gt; triple-angle-bracket""Powered by Discourse, best viewed with JavaScript enabled"
2459,gtc-2020-optuna-an-eager-hyperparameter-optimization-library,"GTC 2020 S21291
Presenters: Shotaro Sano,Preferred Networks
Abstract
We’ll present Optuna, an open-source, next-generation hyperparameter optimization framework with three novel design criteria: (1) an eager API that allows users to concisely construct dynamic, nested, or conditional search spaces; (2) efficient implementation of both sampling and early-stopping strategies; and (3) an easy-to-set-up, versatile architecture that can be deployed for various purposes, ranging from simple scaling on distributed GPUs to lightweight experimentation, conducted via interactive interface. Optuna is the first optimization software designed to run in Eager mode. We’ll present the basic usage of Optuna, describe the design techniques that we needed to meet the above criteria during development, and share our experiences leveraging Optuna to tune hyperparameters on single and multiple GPUs.Watch this session
Join in the conversation below.Powered by Discourse, best viewed with JavaScript enabled"
2460,enabling-enterprise-ai-transformations-for-telcos-with-nvidia-and-vmware,"Originally published at:			https://developer.nvidia.com/blog/enabling-enterprise-ai-transformations-for-telcos-with-nvidia-and-vmware/
AI has the power to transform every industry, but transformation takes time, and it’s rarely easy. For enterprises across industries to be as successful as possible in their own transformations, they need access to AI-ready technology platforms. They also must be able to use 5G connectivity at the edge to harness valuable data and inform…Powered by Discourse, best viewed with JavaScript enabled"
2461,accelerating-high-volume-manufacturing-for-inverse-lithography-technology,"Originally published at:			https://developer.nvidia.com/blog/accelerating-high-volume-manufacturing-for-inverse-lithography-technology/
Learn about our joint effort in porting the ILT engine to GPU. We also identify future areas for EDA and mask inspection tool vendors to address for ILT to be adopted at logic foundries.Powered by Discourse, best viewed with JavaScript enabled"
2462,tensorrt-4-accelerates-neural-machine-translation-recommenders-and-speech,"Originally published at:			TensorRT 4 Accelerates Neural Machine Translation, Recommenders, and Speech | NVIDIA Technical Blog
NVIDIA has released TensorRT 4 at CVPR 2018. This new version of TensorRT, NVIDIA’s powerful inference optimizer and runtime engine provides: New Recurrent Neural Network (RNN) layers for Neural Machine Translation apps New Multilayer perceptron (MLP) operations and optimizations for Recommender Systems Native ONNX parser to import models from popular deep learning frameworks Integration with TensorFlow…Powered by Discourse, best viewed with JavaScript enabled"
2463,theia-interactive-generates-realistic-and-interactive-scenes-with-dlss-and-rtx-global-illumination,"Originally published at:			https://developer.nvidia.com/blog/theia-interactive-generates-realistic-and-interactive-scenes-with-dlss-and-rtx-global-illumination/
Learn how by integrating NVIDIA technologies, Theia Interactive has reduced the time it takes for ray tracing and rendering.Powered by Discourse, best viewed with JavaScript enabled"
2464,gtc-2020-a-faster-radix-sort-implementation,"GTC 2020 S21572
Presenters: Andrey Adinets,NVIDIA
Abstract
We’ll present a faster implementation of the least-significant-digit radix sort. Decoupled look-back is used to reduce the number of memory operations per partition pass from 3N to 2N. A faster partition implementation inside the thread block is used. For 32-bit keys, we use four partition passes, with 8 bits per digit. On V100 sorting 64 million random UInt32 keys, our implementation achieves the speed of 16 Gkey/s, which is more than 2x faster than cub::DeviceRadixSort.Watch this session
Join in the conversation below.Nice content, thanks for sharing! I’m thinking about implementing this in compute shader,
But I found this presentation a little bit confusing…, is there any source code or paper about this algorithm?Powered by Discourse, best viewed with JavaScript enabled"
2465,boosting-data-ingest-throughput-with-gpudirect-storage-and-rapids-cudf,"Originally published at:			https://developer.nvidia.com/blog/boosting-data-ingest-throughput-with-gpudirect-storage-and-rapids-cudf/
Learn how RAPIDS cuDF accelerates data science with the help of GPUDirect Storage. Dive into the techniques that minimize the time to upload data to the GPUThank you for checking out our article. Since the publication, we added a “read throughput” analysis based on the benchmarks in this report. The analysis shows that for high-cardinality data with simple data types, we exceed 5 GiB/s end-to-end read throughput with GDS.

image1167×681 125 KB
Powered by Discourse, best viewed with JavaScript enabled"
2466,creating-more-engaging-sports-broadcasts-with-ai,"Originally published at:			Creating More Engaging Sports Broadcasts With AI | NVIDIA Technical Blog
In order to expand the scope of possibilities for augmented reality applications and player performance tracking in professional sports, researchers at Stats Perform, a sports analytics company, have developed an AI-based method that makes camera calibration faster and more flexible. Sports teams use vision-based tracking systems to analyze player performance, and broadcasters deploy AR to…Powered by Discourse, best viewed with JavaScript enabled"
2467,nvidia-gtc-computer-vision-sessions-span-retail-ag-healthcare-and-more,"Originally published at:			https://developer.nvidia.com/blog/nvidia-gtc-computer-vision-sessions-span-retail-ag-healthcare-and-more/
Check out the lineup of IVA sessions covering applications in smart spaces such as stadium operations, manufacturing, retail, smart traffic systems, healthcare, agriculture, and autonomous machines.Powered by Discourse, best viewed with JavaScript enabled"
2468,nvidia-researchers-use-ai-to-teach-robots-how-to-improve-human-to-robot-interactions,"Originally published at:			NVIDIA Researchers Use AI to Teach Robots How to Improve Human-to-Robot Interactions | NVIDIA Technical Blog
To continue to build robots that can safely and effectively collaborate with humans in warehouses and the home, NVIDIA researchers in the Seattle AI Robotics Research Lab developed a human-to-robot handover method in which the robot meets the human halfway.  The system, a proof of concept, results in more fluent handovers compared to previous approaches,…Powered by Discourse, best viewed with JavaScript enabled"
2469,mlperf-inference-nvidia-innovations-bring-leading-performance,"Originally published at:			MLPerf Inference: NVIDIA Innovations Bring Leading Performance | NVIDIA Technical Blog
New TensorRT 6 Features Combine with Open-Source Plugins to Further Accelerate Inference  Inference is where AI goes to work. Identifying diseases. Answering questions. Recommending products and services. The inference market is also diffuse, and will happen everywhere from the data center to edge to IoT devices across multiple use-cases including image, speech and recommender systems…Powered by Discourse, best viewed with JavaScript enabled"
2470,tips-and-tricks-vulkan-dos-and-donts,"Originally published at:			Tips and Tricks: Vulkan Dos and Don’ts | NVIDIA Technical Blog
The increased performance potential of modern graphics APIs is coupled with a dramatically increased level of developer responsibility. Optimal use of Vulkan is not a trivial concept, especially in the context of a large engine, and information about how to maximize performance is still somewhat sparse. The following document is a compilation of wisdom from…When you say ""Switching tessellation and geometry shaders on/off is an expensive operation,"" does that apply to the situation where you bind two consecutive pipelines per frame, one with a shader that uses the geometry or tesselation stage, and the other that doesn't? To clarify, each frame you'd bind the first, draw, then the second, draw, present, and repeat. Is there a better way to do this if you have some scene geometry that requires those stages and others scene geometry that doesn't? Surely it wouldn't be better to have those stages active as passthroughs even on the geometry that doesn't use them?What about the number of pipelines: Is it better to make one big pipeline with many DS bindings and draw calls, or should we make many little pipelines with minimal draw calls? How many pipelines per frame are reasonable?Sorry but I find this confusing:  ""Work Submission"" section    DO (last item) Reuse command buffers when possible.    DON'T (4th item) Don't design around lots of command buffer reuse.Maybe it's the ""design around"" term.  Is the correct meaning: ""Don't avoid, in your design, lots of command buffer reuse""?That's what I'm assuming!Reuse command buffers when possible, but do not base system design decisions around enabling reuse.Hello there. What a nice article. The markup is broken here btw:best regards,
Johannes@cmdfreak  – Good catch, thanks! It’s now fixed.Powered by Discourse, best viewed with JavaScript enabled"
2471,gtc-2020-some-understandings-and-new-designs-of-convolutional-networks,"GTC 2020 S21783
Presenters: Fuxin Li,Oregon State University
Abstract
Convolutional networks (CNNs) have driven the great engineering success of deep learning in recent years. However, as academics, we still wonder whether they are indeed the ultimate models of choice. It’s been especially hard to understand where CNN predictions come from; they seem unable to characterize predictive uncertainty, and they are highly dependent on small filters on small, rectangular neighborhoods. Can we understand CNN predictions better and explore alternative designs to improve upon them?Watch this session
Join in the conversation below.Powered by Discourse, best viewed with JavaScript enabled"
2472,accelerate-net-applications-with-alea-gpu,"Originally published at:			https://developer.nvidia.com/blog/accelerate-net-applications-alea-gpu/
Today software companies use frameworks such as .NET to target multiple platforms from desktops to mobile phones with a single code base to reduce costs by leveraging existing libraries and to cope with changing trends. While developers can easily write scalable parallel code for multi-core CPUs on .NET with libraries such as the task parallel…Wow this is great. I'm just thinking about some applications for this on our existing software. We have servers with Nvidia cards and now even the smaller firms can leverage this hardware. I hope that the licencing costs for Alea GPU are reasonable.Powered by Discourse, best viewed with JavaScript enabled"
2473,analyzing-video-content-to-display-related-advertisements,"Originally published at:			Analyzing Video Content to Display Related Advertisements | NVIDIA Technical Blog
Taiwanese startup Viscovery recently raised $10M to further develop their Artificial Intelligence system that scans the video content you’re watching and then brings up a relative ad. Using CUDA, TITAN X Pascal GPUs and cuDNN to train their deep learning models, their Video Discovery Service is able to detect the content in videos, including face,…Powered by Discourse, best viewed with JavaScript enabled"
2474,limit-order-book-dataset-generation-for-accelerated-short-term-price-prediction-with-rapids,"Originally published at:			https://developer.nvidia.com/blog/limit-order-book-dataset-generation-for-accelerated-short-term-price-prediction-with-rapids/
Hardware acceleration using GPUs reduces the time required for financial ML researchers to obtain prediction results.Powered by Discourse, best viewed with JavaScript enabled"
2475,top-generative-ai-sessions-at-nvidia-gtc-2023,"Originally published at:			https://register.nvidia.com/events/widget/nvidia/gtcspring2023/1674855518111001o2Ur/?nvid=nv-int-bnr-463583
See how recent breakthroughs in generative AI are transforming media, content creation, personalized experiences, and more. Powered by Discourse, best viewed with JavaScript enabled"
2476,cuda-refresher-the-cuda-programming-model,"Originally published at:			CUDA Refresher: The CUDA Programming Model | NVIDIA Technical Blog
This is the fourth post in the CUDA Refresher series, which has the goal of refreshing key concepts in CUDA, tools, and optimization for beginning or intermediate developers. The CUDA programming model provides an abstraction of GPU architecture that acts as a bridge between an application and its possible implementation on GPU hardware. This post…Seems that the link for "" CUDA sample code deviceQuery"" will show a 404 page for readers.Yeah, it should be: deviceQueryPowered by Discourse, best viewed with JavaScript enabled"
2477,gpu-accelerated-cosmological-analysis-on-the-titan-supercomputer,"Originally published at:			https://developer.nvidia.com/blog/gpu-accelerated-cosmological-analysis-titan-supercomputer/
Ever looked up in the sky and wondered where it all came from? Cosmologists are in the same boat, trying to understand how the Universe arrived at the structure we observe today. They use supercomputers to follow the fate of very small initial fluctuations in an otherwise uniform density. As time passes, gravity causes the…Powered by Discourse, best viewed with JavaScript enabled"
2478,meet-the-experts-leading-innovative-dli-workshops-at-nvidia-gtc,"Originally published at:			https://developer.nvidia.com/blog/nvidia-certified-instructors-you-might-meet-during-dli-workshop-at-gtc/
Sign up now for DLI workshops at GTC and learn new technical skills from  top experts across a range of fields including NLP, data science, deep learning, and more.Powered by Discourse, best viewed with JavaScript enabled"
2479,speech-ai-spotlight-visualizing-spoken-language-and-sounds-on-ar-glasses,"Originally published at:			https://developer.nvidia.com/blog/speech-ai-spotlight-visualizing-spoken-language-and-sounds-on-ar-glasses/
Audio can include a wide range of sounds, from human speech to non-speech sounds like barking dogs and sirens. When designing accessible applications for people with hearing difficulties, the application should be able to recognize sounds and understand speech. Such technology would help deaf or hard-of-hearing individuals with visualizing speech, like human conversations and non-speech…I really enjoyed the article!! Making a generalized sound AI system that covers both speech and non-speech sounds is definitely a challenging goal, but we were able to demonstrate this futuristic goal using NVIDIA Riva with Cochl.Sense!Powered by Discourse, best viewed with JavaScript enabled"
2480,nvidia-titan-rtx-helps-artist-generate-original-paintings,"Originally published at:			https://developer.nvidia.com/blog/nvidia-titan-rtx-helps-artist-generate-original-paintings/
Wondering how AI can inspire artists to create their best work? Renowned artist Chris Peters recently purchased a new NVIDIA TITAN RTX GPU with the intention of using it to create art. The results are stunning compositions generated by the AI, and actual oil paintings painted by Peters himself. “The AI Muse produces digital images, but a…Powered by Discourse, best viewed with JavaScript enabled"
2481,cvpr-2020-dramatic-gains-in-performance-and-utilization-with-multi-instance-gpus,"CVPR 2020 dcv04
Presenters: Tech Demo Team, NVIDIA
Abstract
Multi-Instance GPU (MIG) mode on the NVIDIA A100 Tensor Core GPU can guarantee performance for up to seven jobs running concurrently on the same GPU.Watch this session
Join in the conversation below.Powered by Discourse, best viewed with JavaScript enabled"
2482,deploying-edge-ai-in-nvidia-headquarters,"Originally published at:			https://developer.nvidia.com/blog/deploying-edge-ai-in-nvidia-headquarters/
Learn how the NVIDIA IT team deployed an edge AI solution in NVIDIA headquarters.Powered by Discourse, best viewed with JavaScript enabled"
2483,introducing-nvidia-dgx-a100-the-universal-ai-system-for-enterprise,"GTC 2020 S21702
Presenters: Charlie Boyle, NVIDIA; Christopher Lamb, NVIDIA; Rajeev Jayavant, NVIDIA
Abstract
Today’s enterprise needs a platform that can help business embrace AI-powered transformation to thrive in challenging times. NVIDIA DGX A100 delivers breakthrough computing performance while enabling organizations to unify previously siloed resources that were optimized for single AI workload types. Built on the revolutionary NVIDIA A100 Tensor Core GPU, DGX A100 unifies data center AI infrastructure, flexibly adapting to training, inference, and analytics workloads with ease. More than a server, DGX A100 is the foundational building block of AI infrastructure and part of the NVIDIA end-to-end data center solution created from over a decade of AI leadership by NVIDIA. Attend this session to learn how DGX A100 can help you fast-track AI transformation.Watch this session
Join in the conversation below.Powered by Discourse, best viewed with JavaScript enabled"
2484,building-a-dream-home-with-real-time-ray-tracing,"Originally published at:			https://developer.nvidia.com/blog/building-a-dream-home-with-real-time-ray-tracing/
This article covers an interview with OIa Stalmach, co-creator of House or Crickets, a 2020 DXR Spotlight Contest Winner.Powered by Discourse, best viewed with JavaScript enabled"
2485,developing-the-next-generation-of-extended-reality-applications-with-speech-ai,"Originally published at:			Developing the Next Generation of Extended Reality Applications with Speech AI | NVIDIA Technical Blog
Technical tutorial for extended reality (XR) developers adding speech AI skills, like ASR and TTS, to AR/VR/MR applications using Riva SDK as an example.Powered by Discourse, best viewed with JavaScript enabled"
2486,gtc-sessions-now-available-on-nvidia-on-demand,"Originally published at:			NVIDIA On-Demand
With over 1600 sessions on the latest in AI, data center, accelerated computing, healthcare, intelligent networking, game development, and more - there is something for everyone.Powered by Discourse, best viewed with JavaScript enabled"
2487,physics-informed-machine-learning-platform-nvidia-modulus-is-now-open-source,"Originally published at:			https://developer.nvidia.com/blog/physics-ml-platform-modulus-is-now-open-source/
Physics-informed machine learning (physics-ML) is transforming high-performance computing (HPC) simulation workflows across disciplines, including computational fluid dynamics, structural mechanics, and computational chemistry. Because of its broad applications, physics-ML is well suited for modeling physical systems and deploying digital twins across industries ranging from manufacturing to climate sciences. NVIDIA Modulus is a state-of-the-art physics-ML platform that…Powered by Discourse, best viewed with JavaScript enabled"
2488,share-your-science-transforming-drug-design-with-molecular-dynamics,"Originally published at:			Share Your Science: Transforming Drug Design with Molecular Dynamics | NVIDIA Technical Blog
Ross Walker, AMBER developer at San Diego Computing Super Center and University of California, San Diego discusses how they are accelerating molecular dynamics to transform drug design.   Read more about his research at the Walker Molecular Dynamics lab - Main Share your GPU-accelerated science with us: http://nvda.ly/Vpjxr Watch more scientists and researchers share how accelerated computing is #thepathforward: Watch NowPowered by Discourse, best viewed with JavaScript enabled"
2489,deploying-time-sensitive-5g-networks-at-the-dawn-of-ai-for-telcos,"Originally published at:			https://developer.nvidia.com/blog/deploying-time-sensitive-5g-networks-at-the-dawn-of-ai-for-telcos/
Telecommunication (telco) providers are undergoing a business transformation. They’re replacing the traditional network infrastructure that lacks agility, flexibility, and efficiency with commercial off-the-shelf (COTS) white box servers to assist in implementing 5G and modernizing data centers. 5G is the foundation for boosting network capacity and bandwidth but will overwhelm current network architectures. A considerable challenge…Powered by Discourse, best viewed with JavaScript enabled"
2490,emotional-chatting-chatbot,"Originally published at:			Emotional Chatting Chatbot | NVIDIA Technical Blog
Computers lack empathy, but researchers from China are looking to change that with their deep learning-based chatbot capable of assessing the emotional content of a conversation and responding accordingly. The work opens the door to a new generation of chatbots that are emotionally aware. “To the best of our knowledge, this is the first work…Powered by Discourse, best viewed with JavaScript enabled"
2491,how-to-successfully-integrate-nvidia-dlss-3,"Originally published at:			https://developer.nvidia.com/blog/how-to-successfully-integrate-dlss-3/
NVIDIA DLSS Frame Generation is the new performance multiplier in DLSS 3 that uses AI to create entirely new frames. This breakthrough has made real-time path tracing—the next frontier in video game graphics—possible. NVIDIA has made it easier for you to take full advantage of this technology with the release of the Unreal Engine 5.2…Powered by Discourse, best viewed with JavaScript enabled"
2492,overcoming-communication-congestion-for-hpc-applications-with-nvidia-nvtags,"Originally published at:			Overcoming Communication Congestion for HPC Applications with NVIDIA NVTAGS | NVIDIA Technical Blog
HPC applications are critical to solving the biggest computational challenges to further scientific research. There is a constant need to drive efficiencies in hardware and software stacks to run larger scientific models and speed up simulations. High communication costs between GPUs prevents you from maximizing performance from your existing hardware. To address this, we’ve built…Hi, Does NVTAGS support running jobs through LSF (Load Sharing Facility)? Also, NVTAGS seems to be for distributed MPI+GPU, would it work without MPI? Currently, for my application, it seems that both the device and the socket are randomly and independently selected – Is there a way to grab / bind the closest socket automatically without setting CUDA_VISIBLE_DEVICES using NVTAGS? Thanks!Hi,At the moment NVTAGS has support for Slurm and direct MPI calls (with mpirun/mpiexec). NVTAGS is also only supported with MPI. It relies on detecting MPI ranks of processes to assign selected resources.Setting CUDA_VISIBLE_DEVICES is necessary as otherwise NVTAGS will not know which devices are assigned to each process, so that it can select CPU socket/cores and NICs that are within the affinity of the selected device.Running NVTAGS in run mode without prior tuning will set CUDA_VISIBLE_DEVICES to GPUs 0,1,2,… The assumption here is that process with rank 0,1,2,… will pick them in order(which is the typical default GPU selection). Next, CPU sockets/cores and NIC within the affinity of selected devices will be selected for each process.In short, you need to know which GPU is selected by each process so that you can select resources that are within its affinity. Hope this helpsHi @imanfaraji
Regarding “Running NVTAGS in run mode without prior tuning will set CUDA_VISIBLE_DEVICES to GPUs 0,1,2,…” – is this due to the fact that this environment variable is set to FASTEST_FIRST by default? Then, it could potentially generate different IDs for the devices compared to PCI_BUS_ID if the devices happen to have different speeds. Is there a way to set the IDs so that they are consistent with the IDs in nvidia-smi? Also, if we were to run with IBM LSF spectrum, is there a way to set CUDA_VISIBLE_DEVICES?
Thanks!Yes, FASTEST_FIRST is default behavior but what tools like NVTAGS does is trying to resolve is optimizing total GPU communication time. So, this ordering doesn’t mean GPU3<->GPU4 connection is faster than GPU4<->GPU5 connection. BW/Latency benchmarking needs to be done here to figure out connection speed.Moreover, let’s say GPU1<->GPU2 and GPU5<->GPU6 are fastest GPU pairs in your applications, however what matters here most is that GPU processing pairs that are communicating the most to use them. Let’s say Process1<->Process2 and Process5<->Process6 have little or no GPU communication, then default GPU ordering will be a bad GPU selection for your application. What NVTAGS does it reorder GPUs so highly communicating processes use fastest GPU pairs.Again, you need Latency/BW benchmarks to know which processing pairs are the fastest.Please, note that you first need to know that your application GPU communication pattern is non-uniform. If all of the GPUs are communicating the same amount of data with each other, there won’t any benefit in reordering GPUs. Having said that, you can still pick the NIC and CPU cores/socket within the affinity of your selected GPU and that should help.I am not sure how setting affinity can be done in IBML LSF, but I find setting CUDA_VISIBLE_DEVIES env var and using numactl/tasket best option for this. Please take a look at nvtags_set_env.sh and nvtags_run scripts. This will give you an idea how this is done, I think you should be able to replicate that for IBM LSF.Powered by Discourse, best viewed with JavaScript enabled"
2493,tips-acceleration-structure-compaction,"Originally published at:			https://developer.nvidia.com/blog/tips-acceleration-structure-compaction/
In ray tracing, more geometries can reside in the GPU memory than with the rasterization approach because rays may hit the geometries out of the view frustum. You can let the GPU compact acceleration structures to save memory usage. For some games, compaction reduces the memory footprint for a bottom-level acceleration structure (BLAS) by at…Feel free to ask if you have any questions! Thanks.Powered by Discourse, best viewed with JavaScript enabled"
2494,multi-gpu-programming-with-standard-parallel-c-part-1,"Originally published at:			https://developer.nvidia.com/blog/multi-gpu-programming-with-standard-parallel-c-part-1/
By developing applications using MPI and standard C++ language features, it is possible to program for GPUs without sacrificing portability or performance.Powered by Discourse, best viewed with JavaScript enabled"
2495,enabling-enterprise-cybersecurity-protection-with-a-dpu-accelerated-next-generation-firewall,"Originally published at:			http://127.0.0.1:8089/enabling-enterprise-cybersecurity-protection-with-a-dpu-accelerated-next-generation-firewall/
Palo Alto Networks and NVIDIA have developed an Intelligent Traffic Offload (ITO) solution to solve the scaling, efficiency, and economic challenges this creates.Powered by Discourse, best viewed with JavaScript enabled"
2496,nvdla-deep-learning-inference-compiler-is-now-open-source,"Originally published at:			NVDLA Deep Learning Inference Compiler is Now Open Source | NVIDIA Technical Blog
Designing new custom hardware accelerators for deep learning is clearly popular, but achieving state-of-the-art performance and efficiency with a new design is a complex and challenging problem. Two years ago, NVIDIA opened the source for the hardware design of the NVIDIA Deep Learning Accelerator (NVDLA) to help advance the adoption of efficient AI inferencing in…and yet the NVIDIA's driver stack and CUDA compilers are not open source. I advise anyone who cares about software Freedom to avoid CUDA and do not use NVIDIA hardware until they stop their monopolistic policies.Powered by Discourse, best viewed with JavaScript enabled"
2497,gtc-2020-accelerating-applications-for-the-nersc-perlmutter-supercomputer-using-openmp,"GTC 2020 S21387
Presenters: Annemarie Southwell ,NVIDIA ; Christopher Daley,Lawrence Berkeley National Laboratory
Abstract
Learn about the NERSC/NVIDIA effort to support OpenMP target offload on the forthcoming NERSC-9 Perlmutter supercomputer with next-generation NVIDIA GPUs. NVIDIA’s HPC compilers for C, C++, and Fortran will support a subset of OpenMP 4.5/5.0 for GPU offload on Perlmutter. We’ll review the primary features included in this subset, walk through the reasons some features were included and others were left out, and highlight the characteristics that make some OpenMP applications better candidates for GPU acceleration than others. Selected teams from the NERSC Exascale Science Applications Program (NESAP) are porting their applications to NVIDIA GPUs using this subset of OpenMP for target offload. We’ll share their early experiences and those of other NERSC developers working with early releases of NVIDIA’s OpenMP-offload-enabled compilers.Watch this session
Join in the conversation below.Powered by Discourse, best viewed with JavaScript enabled"
2498,optimizing-end-to-end-memory-networks-using-sigopt-and-gpus,"Originally published at:			Optimizing End-to-End Memory Networks Using SigOpt and GPUs | NVIDIA Technical Blog
Natural language systems have become the go-between for humans and AI-assisted digital services. Digital assistants, chatbots, and automated HR systems all rely on understanding language, working in the space of question answering. So what are question answering (QA) systems and why do they matter? In general, QA systems take some sort of context in the…Powered by Discourse, best viewed with JavaScript enabled"
2499,startup-uses-nvidia-gpus-to-analyze-sports-broadcasts-in-real-time,"Originally published at:			Startup Uses NVIDIA GPUs to Analyze Sports Broadcasts in Real Time | NVIDIA Technical Blog
Sportlogiq is a Montreal, Canada-based startup that uses deep learning and NVIDIA GPUs to collect sports analytics from a standard video in real time. The company, a member of the NVIDIA Inception Program, recently showed off their technology at the annual Computer Vision and Pattern Recognition Conference in Salt Lake City, Utah this week. “Sportlogiq…Powered by Discourse, best viewed with JavaScript enabled"
2500,a-cuda-dynamic-parallelism-case-study-panda,"Originally published at:			https://developer.nvidia.com/blog/a-cuda-dynamic-parallelism-case-study-panda/
This post concludes an introductory series on CUDA Dynamic Parallelism. In my first post, I introduced Dynamic Parallelism by using it to compute images of the Mandelbrot set using recursive subdivision, resulting in large increases in performance and efficiency. The second post is an in-depth tutorial on the ins and outs of programming with Dynamic…Cool. Why step tutorials are not common?Can you clarify your question?Powered by Discourse, best viewed with JavaScript enabled"
2501,programming-the-entire-data-center-infrastructure-with-the-nvidia-doca-sdk,"Originally published at:			https://developer.nvidia.com/blog/programming-the-entire-data-center-infrastructure-with-the-nvidia-doca-sdk/
Today, in his NVIDIA GTC Fall keynote, CEO Jensen Huang introduced a new kind of processor, the BlueField-2 data processing unit (DPU), a powerful new software development kit for the DPU, DOCA, along with a three year roadmap of DPU and AI innovation. The NVIDIA BlueField-2 DPU is the world’s first data center infrastructure on…Hi everyone! The NVIDIA DOCA team is looking forward to working with you to support integration with and programming of the NVIDIA BlueField family of DPUs. Please let us know how you would like to program the DPUs (SDN, storage, security, remote management, or all of the above) and which APIs or toolkits you’d like to use.Hello, I’m a postgraduate student and I am intertesred in boosting storage performance with NVIDIA DPU. I applied for DOCA SDK early access a few days ago with my school email and the site shows ‘Approval pending’. Can I get an approval to do further research? My work is used for reaseach only and does not raise any ethical issues.Thank you for your interest in DOCA, DOCA developer SDK first release for early access is not available yet, we will let you know once it is available.
In the meanwhile I would recommend to download the BlueField package located in the downloader and review the SPDK framework and storage features which are accelerated on the BlueFIeld DPU, you’ll be able to use SPDK to accelerate your storage application with the DPU.We will be keen to learn which type of acceleration you considered to use and what APIs or drivers you will find useful for your development.Hi,
today our distributor confirmed that we will get the first delivery of ConnectX-6DX SmartNICs by the end of this week. We want to start the integrating the SmartNICs in our appliances as soon as possible, therefore we are very eager to get the DOCA SDK in our hands. Do you have an official timeline when the SDK will be available? If not, where can we apply for early access besides https://developer.nvidia.com/nvidia-doca-sdk-early-access ? Thank you!Dear Gernot,
Thanks for reaching out, we share your excitement about starting your journey with SmartNICs.
DOCA developer SDK first release for early access is not available yet, we will let you know once it is available and it’s coming soon.I’d like to emphasize that at the moment DOCA SDK is only supported for our DPUs, in the case of ConnectX-6Dx please refer to our OFED package information on the web and our support teams will be happy to assist you with integration and trying out all the networking, security and storage accelerators we offer.Let us know if you have any further questions and hoping to see you join the DOCA community once starting to use our DPUs.Powered by Discourse, best viewed with JavaScript enabled"
2502,optimizing-and-accelerating-ai-inference-with-the-tensorrt-container-from-nvidia-ngc,"Originally published at:			Optimizing and Accelerating AI Inference with the TensorRT Container from NVIDIA NGC | NVIDIA Technical Blog
Natural language processing (NLP) is one of the most challenging tasks for AI because it needs to understand context, phonics, and accent to convert human speech into text. Building this AI workflow starts with training a model that can understand and process spoken language to text. BERT is one of the best models for this…Powered by Discourse, best viewed with JavaScript enabled"
2503,build-custom-synthetic-data-generation-pipelines-with-omniverse-replicator,"Originally published at:			https://developer.nvidia.com/blog/build-custom-synthetic-data-generation-pipelines-with-omniverse-replicator/
Overcome data challenges and build high-quality synthetic data to accelerate the training and accuracy of AI perception networks with Omniverse Replicator, available in beta.Hi @jwitsoe Bhumin Pathak,
I really enjoyed the conference you gave about replicator around the 22nd of june, thank you!During the Q&A, I asked if we could generate thermal images with replicator.
You replied it is possible to generate thermal images if we coded a sensor.
Could you shed some light on this?
We are using the NVIDIA replicator for a project to help firefighters detect fire starts faster.Thank you!
JoséphinePowered by Discourse, best viewed with JavaScript enabled"
2504,just-released-new-updates-to-nvidia-riva,"Originally published at:			Riva Getting Started | NVIDIA Developer
Build better GPU-accelerated Speech AI applications with the latest NVIDIA Riva updates, including enterprise support.Powered by Discourse, best viewed with JavaScript enabled"
2505,nvidia-accelerates-apache-spark-world-s-leading-data-analytics-platform,"Originally published at:			https://developer.nvidia.com/blog/nvidia-accelerates-apache-spark-worlds-leading-data-analytics-platform/
NVIDIA today announced that it is collaborating with the open-source community to bring end-to-end GPU acceleration to Apache Spark 3.0.Powered by Discourse, best viewed with JavaScript enabled"
2506,share-your-science-exploring-the-wonders-of-the-world-in-virtual-reality,"Originally published at:			https://developer.nvidia.com/blog/share-your-science-exploring-the-wonders-of-the-world-in-virtual-reality/
Dominic Eskofier, co-founder of realities.io shares how they are using GPUs for their photogrammetric virtual reality app, Realities on Steam. The app transports users to some of the most mesmerizing places around the world – like ancient Mayan temples in Guatemala – with the push of a button. Using CUDA with NVIDIA GeForce GTX 980…Powered by Discourse, best viewed with JavaScript enabled"
2507,maximizing-hpc-cluster-ethernet-fabric-performance-with-mlag,"Originally published at:			https://developer.nvidia.com/blog/maximizing-hpc-cluster-ethernet-fabric-performance-with-mlag/
MLAG can help provide physical switch-level redundancy, avoid single-point failure, and maximize overall utilization of the total available bandwidth in your Ethernet fabric.Powered by Discourse, best viewed with JavaScript enabled"
2508,get-started-on-nlp-and-conversational-ai-with-nvidia-dli-courses,"Originally published at:			https://developer.nvidia.com/blog/get-started-on-nlp-and-conversational-ai-with-free-courses-from-nvidia-dli/
Building an NLP model or AI-powered chatbot? Developers can learn how to create, train and deploy sample models with free NVIDIA DLI courses.Powered by Discourse, best viewed with JavaScript enabled"
2509,innovate-automate-and-scale-with-cumulus-4-4,"Originally published at:			https://developer.nvidia.com/blog/innovate-automate-and-scale-with-cumulus-4-4/
The recently released Cumulus Linux 4.4. In CL 4.4 provides innovation, advanced features and scale enhancements based on the guiding principles of simplicity.Powered by Discourse, best viewed with JavaScript enabled"
2510,choosing-a-server-for-deep-learning-inference,"Originally published at:			https://developer.nvidia.com/blog/choosing-a-server-for-deep-learning-inference/
Learn about the characteristics of inference workloads and systems features needed to run them, particularly at the edge.Powered by Discourse, best viewed with JavaScript enabled"
2511,optimizing-robot-route-planning-with-nvidia-cuopt-for-isaac-sim,"Originally published at:			https://developer.nvidia.com/blog/optimizing-robot-route-planning-with-nvidia-cuopt-for-isaac-sim/
Mailroom workers pick up mail and parcels from different stations and deliver them to various recipients. They know that some envelopes are time-sensitive so they use their knowledge to plan routes with the shortest possible delivery time. This mail delivery puzzle can be mathematically addressed by using techniques from operations research, a discipline that deals…Powered by Discourse, best viewed with JavaScript enabled"
2512,optimizing-compute-shaders-for-l2-locality-using-thread-group-id-swizzling,"Originally published at:			Optimizing Compute Shaders for L2 Locality using Thread-Group ID Swizzling | NVIDIA Technical Blog
As part of my GDC 2019 session, Optimizing DX12/DXR GPU Workloads using Nsight Graphics: GPU Trace and the Peak-Performance-Percentage (P3) Method, I presented an optimization technique named thread-group tiling, a type of thread-group ID swizzling. This is an important technique for optimizing 2D, full-screen, compute shader passes that are doing widely spread texture fetches and…Hello, thank you for the great article! I’ve used this approach in my pet project and it really improves performance. But also I’ve found an issue in code snippet, I’ve already created pull request  and it will be great if you can merge it.Powered by Discourse, best viewed with JavaScript enabled"
2513,belgium-supercomputer-installed-to-further-ai-research,"Originally published at:			Belgium Supercomputer Installed to Further AI Research | NVIDIA Technical Blog
Research university KU Leuven in Belgium just installed a new Hewlett Packard Enterprise supercomputer equipped with NVIDIA GPUs, to further the university’s deep learning research. The deployment makes this one of the largest supercomputers in northern Belgium. The new machine called Genius, will be used to further AI research in molecular modeling, engineering, physics, chemistry,…Powered by Discourse, best viewed with JavaScript enabled"
2514,conquering-noisy-images-in-ray-tracing-with-next-event-estimation,"Originally published at:			Conquering Noisy Images in Ray Tracing with Next Event Estimation | NVIDIA Technical Blog
At SIGGRAPH 2019, NVIDIA presented a talk entitled “Light at the End of the Ray,” which explained importance sampling, and why it is critical for real-time ray tracing applications. The full talk can be found here. The seven-minute video below is an excerpt from the full 30 minute talk. If you’re using uniform random sampling,…Powered by Discourse, best viewed with JavaScript enabled"
2515,nvidia-cloudxr-now-available-on-google-cloud-marketplace,"Originally published at:			https://developer.nvidia.com/blog/nvidia-cloudxr-now-available-on-google-cloud-marketplace/
NVIDIA CloudXR is now publicly available through the Google Cloud Marketplace with NVIDIA RTX Virtual Workstations as a Virtual Machine Image.Powered by Discourse, best viewed with JavaScript enabled"
2516,classifying-tattoos-with-neural-networks,"Originally published at:			Classifying Tattoos with Neural Networks | NVIDIA Technical Blog
With nearly 1.5 billion monthly visitors and 346,000 pictures of tattoos, Tattoodo is taking advantage of deep learning to help categorize the growing number of uploaded images. “At Tattoodo, we spend a lot of time and effort on classifying the tattoo pictures that are uploaded,” mentioned Goran Vuksic, a developer at Tattoodo. “A community member…Powered by Discourse, best viewed with JavaScript enabled"
2517,announcing-the-nvidia-hpc-sdk,"Originally published at:			Announcing the NVIDIA HPC SDK | NVIDIA Technical Blog
The NVIDIA HPC SDK is a comprehensive suite of compilers and libraries enabling HPC developers to program the entire HPC platform from the GPU foundation to the CPU and through the interconnect. It is the only comprehensive, integrated SDK for programming accelerated computing systems. The NVIDIA HPC SDK C++ and Fortran compilers  are the first and…Powered by Discourse, best viewed with JavaScript enabled"
2518,run-rapids-on-microsoft-windows-10-using-wsl-2-the-windows-subsystem-for-linux,"Originally published at:			https://developer.nvidia.com/blog/run-rapids-on-microsoft-windows-10-using-wsl-2-the-windows-subsystem-for-linux/
This post was originally published on the RAPIDS AI Blog. A tutorial to run your favorite Linux software, including NVIDIA CUDA, on Windows RAPIDS is now more accessible to Windows users! This post walks you through installing RAPIDS on Windows Subsystem for Linux (WSL). WSL is a Windows 10 feature that enables users to run…The DEV channel is now moving onto Windows 11 builds. I just got access at work to the WIP DEV channel, so I’m assuming these instructions will work with Win11.
If I have time … :P I might be able to test on my home laptop … but would be nice to have the blog post updated if possible.reporting back … Win11 DEV channel worked fine with the instructions provided. Just don’t follow the ‘CUDA on WSL guide’ too far … ;)I followed the procedure in the article to install the Rapids on windows 11 with WSL2 and Nvidia driver.but failed to import cudf.I also carefully followed ‘CUDA on WSL guide’, and I can run docker …does Rapids 22 support Nvidia T500 GPU on WSL2?Powered by Discourse, best viewed with JavaScript enabled"
2519,mixed-precision-programming-with-cuda-8,"Originally published at:			https://developer.nvidia.com/blog/mixed-precision-programming-cuda-8/
Update, March 25, 2019: The latest Volta and Turing GPUs now incoporate Tensor Cores, which accelerate certain types of FP16 matrix math. This enables faster and easier mixed-precision computation within popular AI frameworks. Making use of Tensor Cores requires using CUDA 9 or later. NVIDIA has also added automatic mixed precision capabilities to TensorFlow,  PyTorch, and MXNet. Interested in learning more…1. cuDNN 6 will add support for INT8 inference convolutions. Does the INT8 converlution will use DP4A?2.When cudnn 6 can be download?3.Can I get some exsamples in github about the ""INT8 convolution""?Powered by Discourse, best viewed with JavaScript enabled"
2520,nvidia-data-center-gpu-manager-simplifies-cluster-administration,"Originally published at:			https://developer.nvidia.com/blog/nvidia-data-center-gpu-manager-cluster-administration/
Today’s data centers demand greater agility, resource uptime and streamlined administration to deal with the ever-increasing computational requirements of HPC, hyperscale and enterprise workloads. IT administrators depend on robust data center management tools to proactively monitor resource health, increase efficiency and lower operational costs. In this blog post, I’ll tell you about a new tool…Where would I obtain the latest Release Candidate of Data Center GPU Manager?  We are running dcgmi 1.3.3 but need more verbose output from the diag command to troubleshoot a PCIe ""Fail - All"" error.It appears the python binding is only for python 2?  Will Py3 be supported soon?Powered by Discourse, best viewed with JavaScript enabled"
2521,nvidia-announces-nsight-graphics-2018-4,"Originally published at:			https://developer.nvidia.com/blog/nvidia-announces-nsight-graphics-2018-4/
Today, NVIDIA announced Nsight Graphics 2018.4, the first public release of GPU Trace. This release also adds D3D12 Pixel history, supports NVIDIA’s Vulkan ray tracing extension, completes support for the D3D12 RS3 SDK, and improves performance for D3D11 and Vulkan debugging and profiling. Additionally, with this release, the Nsight family of tools is being re-versioned…Powered by Discourse, best viewed with JavaScript enabled"
2522,jetson-project-of-the-month-dr-spaam-person-detector-for-2d-range-data,"Originally published at:			Jetson Project of the Month: DR-SPAAM, Person Detector For 2D Range Data | NVIDIA Technical Blog
Dan Jia, Alexander Hermans, and Bastian Leibe of RWTH Aachen University were awarded the Jetson Project of the Month for DR-SPAAM Detector. The Distance Robust Spatial Attention and Auto-regressive Model (DR-SPAAM), which runs on a Jetson AGX Xavier, is a deep learning model for detecting persons in 2D range sequences from a laser scanner.  Sensors…Powered by Discourse, best viewed with JavaScript enabled"
2523,gtc-2019-conference-sessions-preview,"Originally published at:			GTC 2019 Conference Sessions Preview | NVIDIA Technical Blog
NVIDIA’s GPU Technology Conference (GTC) is the premier AI and deep learning conference, providing training, insights, and direct access to experts on various fields. To give you a preview of the some of the conference sessions you can attend, we’ve put together a list of some the key sessions in each business area. Network &…Powered by Discourse, best viewed with JavaScript enabled"
2524,isc-2020-taking-a-closer-look-at-nvidia-mellanox-ufm,"ISC 2020 disc05
Presenters: DemoTeam, NVIDIA
Abstract
The NVIDIA Mellanox Unified Fabric Manager (UFM) provides enhanced network management, monitoring, optimizations, security, and more. The newly announced UFM Cyber-AI applies AI to learning a data center’s operational cadence and network workload patterns, drawing on both real-time and historic telemetry and workload data. Against this baseline, it tracks the system’s health and network modifications and detects performance degradations, usage, and profile changes. The new platform will provide alerts of abnormal system behavior and potential system failures and threats, as well as perform corrective actions. View this demonstration to learn how system administrators can take advantage of UFM to improve their operation’s costs and minimize downtime.Watch this session
Join in the conversation below.Powered by Discourse, best viewed with JavaScript enabled"
2525,new-dli-online-courses-for-hands-on-training-in-accelerated-computing,"Originally published at:			NEW DLI Online Courses for Hands-on Training in Accelerated Computing | NVIDIA Technical Blog
The NVIDIA Deep Learning Institute (DLI) has launched two new self-paced courses giving accelerated computing enthusiasts hands-on training on how to scale workloads across multiple GPUs with CUDA C++ and accelerate CUDA C++ applications with concurrent streams.   Each course will give you access to a GPU-accelerated server in the cloud and the opportunity to earn…Powered by Discourse, best viewed with JavaScript enabled"
2526,nvidia-research-at-cvpr-2020,"Originally published at:			NVIDIA Research at CVPR 2020 | NVIDIA Technical Blog
Researchers, developers, and engineers from all over the world are gathering virtually this year for the 2020 Conference on Computer Vision and Pattern Recognition (CVPR). NVIDIA Research will present its research through oral presentations, posters, and interactive Q&As.  NVIDIA’s accepted papers at this year’s online CVPR feature a range of groundbreaking research in the ﬁeld…Powered by Discourse, best viewed with JavaScript enabled"
2527,announcing-nvidia-physx-sdk-5-0,"Originally published at:			https://developer.nvidia.com/blog/announcing-nvidia-physx-sdk-5-0/
Available soon in 2020, we’ll be introducing support for a unified constrained particle simulation framework.Is PhysX 5 still under development? Anyone know?I wish I knew. I hope that it is GPU accelerated. Some game dvelopers are just lazy.I wish I had their source code.Sorry for the delay! Adam had a great answer to this question on another thread.Jeez, can you guys hurry up? I’m using Flex on the old version of the engine in VR and it’s already amazing. There’s so much to do around soft body mechanics exactly in VR. With all this hype around meta you have to do something. Please give it into our hands ASAP! I’m literally begging you, my project won’t move much further without proper SDK. Please hit me up if it is possible to participate in early testing or something. I can share some of my project and you 100% will get as exited as I am.Powered by Discourse, best viewed with JavaScript enabled"
2528,new-mlperf-inference-network-division-showcases-nvidia-infiniband-and-gpudirect-rdma-capabilities,"Originally published at:			https://developer.nvidia.com/blog/new-mlperf-inference-network-division-showcases-infiniband-and-gpudirect-rdma-capabilities/
In MLPerf Inference v3.0, NVIDIA made its first submissions to the newly introduced Network division, which is now part of the MLPerf Inference Datacenter suite. The Network division is designed to simulate a real data center setup and strives to include the effect of networking—including both hardware and software—in end-to-end inference performance. In the Network…Powered by Discourse, best viewed with JavaScript enabled"
2529,gtc-2020-democratizing-conversational-ai-with-square-assistant,"GTC 2020 S22190
Presenters: Arun Tejasvi Chaganty,Square Inc; Gabor Angeli, Square Inc.
Abstract
In today’s omnichannel world, businesses need to adapt to how their customers operate, which includes the expectation of prompt replies at any time of day via multiple modes of communication. At Square, we are developing Square Assistant, a conversational AI that empowers small businesses to communicate with their customers more efficiently. We’ll talk about the smorgasbord of deep-learning models we employ to understand and respond to messages at scale, and how they exemplify our user-centric approach to AI. We’ll also touch on how we collect data for this complex task, and how our system continues to learn and improve over time.Watch this session
Join in the conversation below.Powered by Discourse, best viewed with JavaScript enabled"
2530,nvidia-cuquantum-sdk-introduces-quantum-circuit-simulation-acceleration,"Originally published at:			https://developer.nvidia.com/blog/nvidia-cuquantum-sdk-introduces-quantum-circuit-simulation-acceleration/
Developers can use cuQuantum to speed up quantum circuit simulations based on state vector, density matrix, and tensor network methods by orders of magnitude.Powered by Discourse, best viewed with JavaScript enabled"
2531,nvidia-leaders-honored-at-venturebeat-transform-women-in-ai-awards,"Originally published at:			NVIDIA Leaders Honored at VentureBeat Transform ‘Women in AI Awards’ | NVIDIA Technical Blog
At VentureBeat’s (VB) second annual Women in AI Awards, Anima Anandkumar, Director of Machine Learning Research at NVIDIA, and Bren professor and director of ML research at the California Institute of Technology, received the VB ‘Women in AI’ award in the AI Research category.  Anandkumar and fellow NVIDIA researchers Sanja Fidler, Director of AI, and Sifei…Powered by Discourse, best viewed with JavaScript enabled"
2532,cuda-10-1-now-available,"Originally published at:			https://developer.nvidia.com/blog/cuda-10-1-now-available/
CUDA 10.1 is now available for download. This version includes a new lightweight GEMM library, new functionality and performance updates to existing libraries,  and improvements to the CUDA Graphs API. With CUDA 10.1, you get: cuBLASLt, a new lightweight GEMM library with a flexible API and tensor core support for INT8 inputs and FP16 CGEMM…Powered by Discourse, best viewed with JavaScript enabled"
2533,develop-ray-traced-games-in-unreal-engine-4,"Originally published at:			Develop Ray Traced Games in Unreal Engine 4 | NVIDIA Technical Blog
Ray tracing in Unreal Engine 4 (UE4) is powerful and flexible, allowing game developers, designers, and artists to render visuals in real-time. The engine also enables developers to mix and match ray tracing and raster techniques. Creating content for ray tracing involves some new considerations, especially when fine tuning. During this talk presented at GTC…Powered by Discourse, best viewed with JavaScript enabled"
2534,new-ai-technologies-announced-at-gtc-2020-keynote,"Originally published at:			https://developer.nvidia.com/blog/new-ai-technologies-gtc-102020/
At GTC 2020, NVIDIA announced updates to 80 SDKs, including tools to help developers build AI-powered video streaming solutions, conversational AI, recommendation systems and more.Powered by Discourse, best viewed with JavaScript enabled"
2535,unlocking-a-simple-extensible-and-performant-video-pipeline-at-fyma-with-nvidia-deepstream,"Originally published at:			Unlocking a Simple, Extensible, and Performant Video Pipeline at Fyma with NVIDIA DeepStream | NVIDIA Technical Blog
Discover how Fyma used NVIDIA DeepStream to improve the performance of its vision AI applications while speeding up development time.Great piece. Ive been sending a ton of messages to the helpdesk without avail. But this even though the live feed is from custom made stock charts to feed CV primed image of all my custom Charting tools/algorithm’s. So if I  am understanding right I wanna use this structure you defined here?  ultimately The CV primed data will inferences out to a front end in the Metaverse and IRL all using 3d omni assets so I am really pushing the limits of what I am capable of producing with very minimal experience especially with NV’s full solution arch I am going for. I am nervous my infrastructure may get to sloppy and I am stuck having to come back. My urge is overwhelming to dive into Omniverse so I can make the Ark. I am also a bit to over zealous with the passion sometimes so I have been all over forums and YouTube spamming my objectives to try and piece together the knowledge base to program all of with this from scratch. Thank you for the piece I didnt know about deepsort and the way you explained the work flow of the company I was able to connect some dots. But I still would like to n=know for sure I have chosen all the right solutions so I am not running circles for a year. Thank you for any info replied.JamesPowered by Discourse, best viewed with JavaScript enabled"
2536,an-aiot-solution-for-visual-blockage-detection-at-culverts,"Originally published at:			https://developer.nvidia.com/blog/an-aiot-solution-for-visual-blockage-detection-at-culverts/
One of the key contributors in originating flash floods is the blockage of cross-drainage hydraulic structures, such as culverts, by unwanted, flood-borne debris being transported. The accumulation and interaction of debris with culverts often result in reduced hydraulic capacity, diversion of upstream flows, and structural failure. For example, the Newcastle, Australia floods in 2007, Wollongong,…Powered by Discourse, best viewed with JavaScript enabled"
2537,understanding-the-visualization-of-overhead-and-latency-in-nvidia-nsight-systems,"Originally published at:			https://developer.nvidia.com/blog/understanding-the-visualization-of-overhead-and-latency-in-nsight-systems/
Recently, a user came to us in the forums. They sent a screenshot of a profiling result using NVIDIA Nsight Systems on a PyTorch program. A single launch of an element-wise operation gave way to questions about the overheads and latencies in CUDA code, and how they are visualized with the Nsight Systems GUI. This…Powered by Discourse, best viewed with JavaScript enabled"
2538,enabling-matrix-product-state-based-quantum-circuit-simulation-with-nvidia-cuquantum,"Originally published at:			https://developer.nvidia.com/blog/enabling-matrix-product-state-based-quantum-circuit-simulation-with-nvidia-cuquantum/
Quantum circuit simulation is the best means to design quantum-ready algorithms so you can take advantage of powerful quantum computers as soon as they are available. NVIDIA cuQuantum is an SDK that enables you to leverage different ways to perform quantum circuit simulation. cuStateVec, a high-performance library built for state vector quantum simulators, relies on…Be sure to reach out to us if you have any questions, comments or concerns. We’d love to hear from you!Powered by Discourse, best viewed with JavaScript enabled"
2539,accelerating-bare-metal-kubernetes-workloads-the-right-way,"Originally published at:			Accelerating Bare Metal Kubernetes Workloads, the Right Way | NVIDIA Technical Blog
This post was originally published on the Mellanox blog. In my previous Kubernetes post, Provision Bare-Metal Kubernetes Like a Cloud Giant!, I discussed the benefits of using BlueField DPU-programmable SmartNICs to simplify provisioning of Kubernetes clusters in bare-metal infrastructures. A key takeaway from this post was the current rapid shift toward bare metal Kubernetes, for…Powered by Discourse, best viewed with JavaScript enabled"
2540,nvidia-announces-tensorrt-8-slashing-bert-large-inference-down-to-1-millisecond,"Originally published at:			https://developer.nvidia.com/blog/nvidia-announces-tensorrt-8-slashing-bert-large-inference-down-to-1-millisecond/
NVIDIA announced TensorRT 8.0 which brings BERT-Large inference latency down to 1.2 ms with new optimizations.Powered by Discourse, best viewed with JavaScript enabled"
2541,an-interactive-2010-census-plotly-dash-visualization-accelerated-by-rapids,"Originally published at:			https://developer.nvidia.com/blog/an-interactive-2010-census-plotly-dash-visualization-accelerated-by-rapids/
The COVID-19 pandemic brings the efforts of the data science community to the forefront. Real-time, interactive visualizations of the novel coronavirus’ spread across populations help researchers, scientists, health officials and governments understand, validate, and communicate important insights hidden among hundreds of millions of rows of records. NVIDIA and Plot.ly, a premier member of NVIDIA Inception,…Powered by Discourse, best viewed with JavaScript enabled"
2542,cuda-10-1-update-2-now-available,"Originally published at:			https://developer.nvidia.com/blog/cuda-10-1-update-2-now-available/
CUDA 10.1 Update 2 is now available for download. This version is a compatible update to CUDA 10.1 and includes updates to libraries, developer tools and bug fixes. Features: Multi-GPU symmetric eigensolvers in cuSOLVER, provided via the cuSOLVERMg library extensionNsight Compute (command-line tools) now add support for POWER, allowing developers to do kernel level profiling…Powered by Discourse, best viewed with JavaScript enabled"
2543,on-demand-session-deploying-edge-ai-in-manufacturing,"Originally published at:			https://developer.nvidia.com/blog/on-demand-session-deploying-edge-ai-in-manufacturing/
At GTC '21, Data Monsters, who builds AI solutions for production and packaging, discussed the growth of AI in manufacturing and how AI is being used to optimize every part of the supply chain, from forecasting and production planning to quality control.Powered by Discourse, best viewed with JavaScript enabled"
2544,how-monai-fuels-open-research-for-medical-ai-workflows,"Originally published at:			How MONAI Fuels Open Research for Medical AI Workflows | NVIDIA Technical Blog
MONAI is fueling open innovation for medical imaging with tools to accelerate image annotation, train state-of-the-art deep learning models, and create AI applications that drive research innovation.Powered by Discourse, best viewed with JavaScript enabled"
2545,nih-uses-ai-to-detect-multiple-sclerosis-with-human-level-accuracy,"Originally published at:			NIH Uses AI to Detect Multiple Sclerosis with Human Level Accuracy | NVIDIA Technical Blog
Researchers from the National Institute of Health and Johns Hopkins University developed a deep learning algorithm to detect multiple sclerosis (MS) from magnetic resonance imaging (MRI). MS is a disease of the central nervous system, in which the body launches a defensive attack against its own tissues. MS can range from relatively benign to somewhat…Powered by Discourse, best viewed with JavaScript enabled"
2546,artificial-intelligence-system-predicts-how-you-will-look-with-different-hair-styles,"Originally published at:			Artificial Intelligence System Predicts How You Will Look With Different Hair Styles | NVIDIA Technical Blog
A new personalized search engine helps you explore what you would look like with brown hair, curly hair or in a different time period. Upload a selfie to Dreambit and type in a term like “curly hair” or “1930 woman”, and the software’s algorithm searches through photo collections for similar images and seamlessly maps your…Powered by Discourse, best viewed with JavaScript enabled"
2547,webinar-nvidia-dlss-3-and-unreal-engine-5-2,"Originally published at:			Level Up with NVIDIA
On July 26, walkthrough DLSS 3 features within Unreal Engine 5.2 and learn how to best use the latest updates.Powered by Discourse, best viewed with JavaScript enabled"
2548,deep-learning-predicts-the-look-of-cells,"Originally published at:			Deep Learning Predicts the Look of Cells | NVIDIA Technical Blog
The Allen Institute for Cell Science launched a one-of-a-kind online portal of 3D cell images called Allen Cell Explorer that were produced using deep learning. The website combines large-scale 3D imaging data, the first application of deep learning to create predictive models of cell organization, and a growing suite of powerful tools. “This is the…Powered by Discourse, best viewed with JavaScript enabled"
2549,jump-start-ai-training-with-ngc-pretrained-models-on-premises-and-in-the-cloud,"Originally published at:			https://developer.nvidia.com/blog/jump-start-ai-training-with-ngc-pretrained-models-on-premises-and-in-the-cloud/
Figure 1. NGC software stack. The process of building an AI-powered solution from start to finish can be daunting. First, datasets must be curated and pre-processed. Next, models need to be trained and tested for inference performance, and then finally deployed into a usable, customer-facing application.  At each step along the way, developers are constantly…Powered by Discourse, best viewed with JavaScript enabled"
2550,5-can-t-miss-gtc-sessions-for-game-developers,"Originally published at:			https://developer.nvidia.com/blog/5-cant-miss-gtc-sessions-for-game-developers/
With the GPU Technology Conference (GTC) just around the corner, below are the sessions we recommend Game Developers attend to learn how to use our new software. Attendees will learn how to create stunning in-game visuals, ask our engineering team questions directly, and find out what’s in the pipeline.Powered by Discourse, best viewed with JavaScript enabled"
2551,nvidia-gpu-accelerated-vasp-6-uses-openacc-to-deliver-15x-more-performance,"Originally published at:			NVIDIA GPU Accelerated VASP 6 uses OpenACC to Deliver 15X More Performance | NVIDIA Technical Blog
Developers of the world’s leading HPC application for atomic scale modelling, Vienna Ab initio Simulation Package (VASP), rolled out VASP 6.1.0 which ports new and expanded acceleration in NVIDIA GPUs through OpenACC. VASP is one of the most widely used codes for electronic-structure calculations and first-principles molecular dynamics. Senior scientist and VASP lead developer Dr.…Powered by Discourse, best viewed with JavaScript enabled"
2552,vrworks-2-6-graphics-sdk-release,"Originally published at:			https://developer.nvidia.com/blog/vrworks-2-6-graphics-sdk-release/
NVIDIA has released the new VRWorks SDK V2.6 for application and headset developers along with the NVIDIA display drivers 397.31(Windows) and 396.18(Linux). Release Highlights Added sample application ‘dx11_smp_assist’ to demonstrate the new feature SMP Assist. SMP Assist driver extension provides an easier alternative for integrating MRS and LMS into an application by offloading much of…Powered by Discourse, best viewed with JavaScript enabled"
2553,profiling-dxr-shaders-with-timer-instrumentation,"Originally published at:			Profiling DXR Shaders with Timer Instrumentation | NVIDIA Technical Blog
Optimizing real-time graphics applications for maximum performance can be a challenging endeavor, and ray tracing is no exception. Whether you want to make your graphics engine more efficient overall or find a specific performance bottleneck, profiling is the most important tool to achieve your goals.  Despite constantly improving support for ray tracing APIs in profilers…Hi there,I’ve been trying to implement this in our codebase. On my RTX 3080 GPU, I find that instead of getting sensible per-pixel execution times, I seem to be getting blocks of the same value. And the value itself looks wrong!Could anyone advise me on whether (a) this feature is known to work on the latest GPUs and (b) are there any additional steps I can use to debug/validate what I have done beyond this document? Sample code, perhaps? Some way to check the generated asm?Thanks.Seeing blocks of pixels with the same timing value is expected because that is how the GPU divides the workload.  You won’t see individual pixels.Answers for your questions:
a) I just ran a test on a RTX 3090 and it functioned properly.
b) There isn’t any asm you can see, but if you followed the article’s sample code and you see a colored heatmap similar to the article then the feature is most likely working and you just need to tweak your heatmap scaling to get a visualization that is appropriate for your workload.  Implementing a dynamic scale value that you can change at runtime could help if your workload changes from scene to scene and find a heatmap that looks more useful.If the values (or colors) look wrong you should try doubling or halving the time scale value since this can depend on the workload of your scene.  The article above was written for a RTX 2080 and the RTX 3080 will certainly finish work faster.  Try modifying your heatmapScale to 5000, 10000, and so on to see if that reflects in your heat map colors:
Here’s the sample code from the article:Let us know if that helps.Hi there,On closer inspection, what I’m getting does seem to match what you describe.I have managed to get per-pixel timing detail on another RT hardware platform I’m working with. I’m guessing that’s what I was expecting here. I do see timing data, but more at a threadgroup level. Which is still useful.I guess when I looked at the images in the post they looked more pixel like and less threadgroup like.Thanks for the help!Powered by Discourse, best viewed with JavaScript enabled"
2554,rtx-global-illumination-part-i,"Originally published at:			RTX Global Illumination Part I | NVIDIA Technical Blog
RTX Global Illumination (RTX GI) creates changing, realistic rendering for games by computing diffuse lighting with ray tracing. It allows developers to extend their existing light probe tools, knowledge, and experience with ray tracing to eliminate bake times and avoid light leaking. Hardware-accelerated programmable ray tracing is now accessible through DXR, VulkanRT, OptiX, Unreal Engine,…Really very nice information for the global illumination and i'm waiting for the more info about it regards -  GHDSPORTS We're planning on posting part 2 in the next couple of weeks.this is a nice primer, did you ever get round to Part 2?Powered by Discourse, best viewed with JavaScript enabled"
2555,gtc-2020-jetson-embedded-platform-experts,"GTC 2020 CWE21703
Presenters: Phil-Lawrence,NVIDIA; Eric-Brower
Winnie-Hsu
Christoph-Fritsch, NVIDIA; Sebastien-Domine
Rafael-Campana, NVIDIA; Gilbert-Yeung
Saurabh-Maniktala, NVIDIA; Stuart-Yates
John-Hsu, NVIDIA; Parthasarathy-Sriram
Kaustubh-Purandare, NVIDIA; Eric-Work
Ashok-Kelur, NVIDIA
Abstract
NVIDIA Jetson is the world’s leading computing platform for AI at the edge. High in performance and low in power, it’s ideal for compute-intensive embedded applications like robots, drones, mobile medical imaging, and intelligent video analytics. Manufacturers, independent developers, makers, and hobbyists can use Jetson developer kits and modules to explore the future of embedded computing and artificial intelligence. Have questions? Jetson experts will be available to discuss the platform capabilities, SDKs, and development tools, and answer questions to help you rapidly deploy AI at the edge. Connect directly with NVIDIA engineers and experts from other organizations to get answers to all of your questions on topics ranging from AI and deep learning to accelerated data science. Visit us at the pods in the exhibit hall and ask a question.Watch this session
Join in the conversation below.Powered by Discourse, best viewed with JavaScript enabled"
2556,nvidia-at-gdc-2021-rtx-and-ai-sees-new-platforms-new-development-tools-new-updates-and-a-new-sdk,"Originally published at:			https://developer.nvidia.com/blog/nvidia-at-gdc-2021-rtx-and-ai-sees-new-platforms-new-development-tools-new-updates-and-a-new-sdk/
An impressive array of NVIDIA GDC announcements elevates game development to the next level. Real-time ray tracing comes to Arm and Linux, DLSS gets an expansive update, the newly announced RTX Memory Utility enables efficient memory allocation, and Omniverse supercharges the development workflow.Powered by Discourse, best viewed with JavaScript enabled"
2557,improving-enterprise-it-fraud-prevention,"Originally published at:			https://developer.nvidia.com/blog/improving-enterprise-it-fraud-prevention/
This post discusses infrastructure factors to consider, such as performance, hardware, and types of AI software for implementing a fraud prevention strategy.Powered by Discourse, best viewed with JavaScript enabled"
2558,gtc-2020-running-unmodified-numpy-programs-on-hundreds-of-gpus-with-legate-numpy,"GTC 2020 S21762
Presenters: Mike Bauer,NVIDIA
Abstract
Learn how you can run unmodified NumPy programs on hundreds of GPUs with Legate NumPy. We’ll describe the implementation of Legate NumPy, a drop-in replacement for NumPy that lets you transparently distribute and accelerate NumPy programs across machines ranging from a single DGX box to the world’s top supercomputers. To illustrate the power of Legate NumPy, we’ll present performance results on a combination of HPC and traditional machine-learning applications written in NumPy running at scale. In addition to describing how to use Legate NumPy, we’ll also describe its underlying architecture and how it leverages the Legion runtime system to make distributed and accelerated NumPy possible. Finally, we’ll cover the underlying Legate interface on which Legate NumPy is built and how you can construct your own accelerated and distributed Python libraries using Legate to seamlessly compose and interoperate with Legate NumPy.Watch this session
Join in the conversation below.Powered by Discourse, best viewed with JavaScript enabled"
2559,sharing-cuda-resources-through-interoperability-with-nvscibuf-and-nvscisync,"Originally published at:			https://developer.nvidia.com/blog/sharing-cuda-resources-through-interoperability-with-nvscibuf-and-nvscisync/
Figure 1. Various hardware engines on the NVIDIA embedded platform. There is a growing need among embedded and HPC applications to share resources and control execution for pipelined workflows spanning multiple hardware engines and software applications. The following diagram gives an insight into the number of engines that can be supported on NVIDIA embedded platforms.…CUDA interoperability with NvSciSync / NvSciBuf has been implemented in CUDA 10.2 release with a focus on usability on safety critical applications and we have seen good performance gains with this as well as elaborated in the blog. If you have any questions or comments, let us know.As stated in this thread, nvsci does not work properly with the dGPUs of the DriveAGX Pegasus with Drive10. As this issue should be fixed in the next release, I was wondering if there is any information on when this will happen.hello @rekhamukund,
i want to know if i allocate NvSciBuf on orin, if cpu and gpu can both can access the buffer.
thank you.Can we use NvSciSync / NvSciBuf to implement CUDA and OpenGL interop? Can we use it in a headless environment like an AWS EC2 instance?Please advise.OpenGL is not a supported UMD (User Mode Driver) for NvSciBuf/NvSciSync, so CUDA-OpenGL interop cannot achieved. Other interops which can be used instead are CUDA-OpenGL (CUDA Runtime API :: CUDA Toolkit Documentation) or EGL interop (CUDA Driver API :: CUDA Toolkit Documentation).CPU access to NvSciBuf allocated buffer can be achieved with NvSciBufObjGetCpuPtr() API as described in https://docs.nvidia.com/drive/drive_os_5.1.6.1L/nvvib_docs/index.html#page/DRIVE_OS_Linux_SDK_Development_Guide/Graphics/nvsci_nvscibuf.html#wwpID0E0OK0HAPowered by Discourse, best viewed with JavaScript enabled"
2560,gtc-2020-nvidia-research-prescription-embedded-ar-display,"GTC 2020 D2R15
Presenters: Tech Demo Team,NVIDIA
Abstract
This demo presents “Prescription AR,” an NVIDIA Research project that won a SIGGRAPH 2019 ‘Best in Show Emerging Technology’ award. Vision-correction is a big hurdle for daily AR display applications since more than 40% of the population requires additional eyewear in their everyday life. Prescription AR is a prescription-embedded fully customized AR display that is optimized for each user’s vision, facial structure and taste of fashion. Based on Prescription AR optics, we present AR-convertible prescription glasses, dubbed as ModulAR. Since the user needs eyewear anyway while not watching AR contents, ModulAR can be switched from glasses-only mode (36.5g) to display mode (61.6g, without cable) easily without screws. The prototype provides binocular FHD images with 40°X20° FOV in a 6mm X 4mm eyebox.Watch this session
Join in the conversation below.Powered by Discourse, best viewed with JavaScript enabled"
2561,leveraging-the-hardware-jpeg-decoder-and-nvidia-nvjpeg-library-on-nvidia-a100-gpus,"Originally published at:			Leveraging the Hardware JPEG Decoder and NVIDIA nvJPEG Library on NVIDIA A100 GPUs | NVIDIA Technical Blog
According to surveys, the average person produces 1.2 trillion images that are captured by either a phone or a digital camera. The storage of such images, especially in high-resolution raw format, uses lots of memory. JPEG refers to the Joint Photographic Experts Group, which celebrated its 25th birthday in 2017. The JPEG standard specifies the…Even JPEG  can be further compressed. I have found the below 2 sites which can help to further reduce the size of imagesCompress JPEG Image to 100KB, 50KB. Compress PDF or Covert PDF to DOCX,. Extract text from images with 80+ language supportAn app to support <b> Digital India </b>.   Compress  JPEG.   Compress Image  with an intelligent Image Optimizer  or JPEG Compressor  to  compress jpeg further.  Image compressor which can compress jpeg  without loss of quality. For e.g. compress...Powered by Discourse, best viewed with JavaScript enabled"
2562,cuda-12-0-new-features-and-beyond-on-youtube-premiere,"Originally published at:			CUDA 12 New Features and Beyond - YouTube
Learn about the newest CUDA features such as release compatibility, dynamic parallelism, lazy module loading, and support for the new NVIDIA Hopper and NVIDIA Ada Lovelace GPU architectures.Powered by Discourse, best viewed with JavaScript enabled"
2563,get-hands-on-training-from-nvidia-experts-at-gtc,"Originally published at:			Get Hands-on Training from NVIDIA Experts at GTC | NVIDIA Technical Blog
GTC has 20 full-day workshops covering a range of deep learning, data science, and accelerated computing topics. Register now!Powered by Discourse, best viewed with JavaScript enabled"
2564,windows-10-october-2018-update-available-now-includes-directx-raytracing,"Originally published at:			Windows 10 October 2018 Update: Available Now - Includes DirectX Raytracing | NVIDIA Technical Blog
Real-time ray tracing has been the holy grail of graphics for decades. At this year’s GDC, NVIDIA and Microsoft joined forces, announcing plans to make this incredible technology accessible to all Windows developers. Since GDC, we’ve gotten glimpses of what Epic Games and DICE have been able to achieve with real-time ray tracing techniques. NVIDIA’s…Powered by Discourse, best viewed with JavaScript enabled"
2565,accelerating-jpeg-2000-decoding-for-digital-pathology-and-satellite-images-using-the-nvjpeg2000-library,"Originally published at:			https://developer.nvidia.com/blog/accelerating-jpeg-2000-decoding-for-digital-pathology-and-satellite-images-using-the-nvjpeg2000-library/
JPEG 2000 (.jp2, .jpg2, .j2k) is an image compression standard defined by the Joint Photographers Expert Group (JPEG) as the more flexible successor to the still popular JPEG standard. Part 1 of the JPEG 2000 standard, which forms the core coding system, was first approved in August 2002. To date, the standard has expanded to…nvJPEG2000 library provides decoding for JPEG 2000 formatted images used by researcher in digital pathology, remote sensing applications and medical imaging.
Key Featuresis there encoding part?Hi, nvJPEG2000 support encoder, refer details nvJPEG2000 Documentation — nvJPEG2000 0.6.0 documentationHi, I can use nVJPEG2000 to decode j2k image (JPEG 2000 - Part1) encoded by Kakadu / OpenJpeg libraries successfully, but fail to decode j2k image (JPEG 2000 - Part1) encoded by Pegasus library of Accusoft with error code NVJPEG2K_STATUS_JPEG_NOT_SUPPORTED. Any suggestions to check which parts are missed or not matched?We are facing the same error (NVJPEG2K_STATUS_JPEG_NOT_SUPPORTED) trying to encode using the GPU in DCP-o-Matic free software.You can see the discussion  and track the bug here.@mkhadatare do you know how to get this solved? It would be great to move it forward. Nvjpeg2k enable_custom_precinctsThanks everyone :)enable_custom_precincts member of nvjpeg2kEncodeConfig_t). It looks like this isn’t currently supported (I get a NVJPEG2K_STATUS_JPEG_NOT_SUPPORTED error when I try it). Do you have plans to add this?In passing, perhaps I can report that the arrays precint_width and precint_height in nvjpeg2kEncodeConfig_t appear to have mis-spelt names (I think they should be precinct_width and precinct_height)Powered by Discourse, best viewed with JavaScript enabled"
2566,accelerating-automated-and-explainable-machine-learning-with-rapids-and-nvidia-gpus,"Originally published at:			https://developer.nvidia.com/blog/accelerating-automated-and-explainable-machine-learning-with-rapids/
RAPIDS aims to democratize accelerated data science through accessibility and innovation. The most recent release reflects major strides in these efforts through integrations with TPOT, a popular tool for AutoML, and innovations in SHAP, a method for providing deep interpretability to machine learning. Faster AutoML for Data-Driven Enterprises AutoML makes machine learning accessible for non-experts…Powered by Discourse, best viewed with JavaScript enabled"
2567,nvidia-announces-riva-speech-ai-and-large-language-modeling-software-for-enterprise,"Originally published at:			https://developer.nvidia.com/blog/nvidia-announces-riva-speech-ai-and-large-language-modeling-software-for-enterprise/
At GTC, NVIDIA unveiled breakthroughs making it simpler for enterprise and research organizations to build state-of-the-art, customizable conversational AI.Powered by Discourse, best viewed with JavaScript enabled"
2568,amazon-elastic-kubernetes-services-now-offers-native-support-for-nvidia-a100-multi-instance-gpus,"Originally published at:			https://developer.nvidia.com/blog/amazon-elastic-kubernetes-services-now-offers-native-support-for-nvidia-a100-multi-instance-gpus/
Deployment and integration of trained machine learning (ML) models in production remains a hard problem, both for application developers and the infrastructure teams supporting them. How do you ensure you have the right-sized compute resources to support multiple end-users, serve multiple disparate workloads at the highest level of performance, automatically balancing the load, scale up…Powered by Discourse, best viewed with JavaScript enabled"
2569,nvidia-introduces-nsight-graphics-1-0,"Originally published at:			NVIDIA Introduces Nsight Graphics 1.0 | NVIDIA Technical Blog
NVIDIA Nsight Graphics 1.0 is now available for download for members of the NVIDIA Registered Developer Program. Nsight Graphics is a suite of debugging and profiling tools for graphics applications, providing understanding of your application’s operation and insight to achieve optimal performance. Nsight Graphics is built with many of the same tools as Nsight Visual…Powered by Discourse, best viewed with JavaScript enabled"
2570,train-your-reinforcement-learning-agents-at-the-openai-gym,"Originally published at:			Train Your Reinforcement Learning Agents at the OpenAI Gym | NVIDIA Technical Blog
Today OpenAI, a non-profit artificial intelligence research company, launched OpenAI Gym, a toolkit for developing and comparing reinforcement learning algorithms. It supports teaching agents everything from walking to playing games like Pong or Go. John Schulman is a researcher at OpenAI. OpenAI researcher John Schulman shared some details about his organization, and how OpenAI Gym will make it easier for AI researchers to…Thanks Mark for such a great blog on RL. I think RL will be the coming DL hotspot. For openai gym, does it support existing DL frameworks? such as Caffe, Torch, Tensorflow, Theano, etc..?Yes, OpenAI Gym doesn't make any restrictions on how you write your algorithms, so you can use any of these frameworks.Does it have a ROS interface?http://erlerobotics.com/doc...Here you will find a toolkit extending the OpenAI Gym for robotics, which supports ROS and Gazebo.Powered by Discourse, best viewed with JavaScript enabled"
2571,event-jensen-huang-nvidia-keynote-at-siggraph-2023,"Originally published at:			NVIDIA at SIGGRAPH 2023 Conference
 On Aug. 8, Jensen Huang features new NVIDIA technologies and award-winning research for content creation.Powered by Discourse, best viewed with JavaScript enabled"
2572,personalized-aesthetics-recording-the-visual-mind-using-machine-learning,"Originally published at:			https://developer.nvidia.com/blog/personalized-aesthetics-machine-learning/
Visual aesthetics are very personal, often subconscious, and hard to express.  In a world with an overload of photographic content, a lot of time and effort is spent manually curating photographs, and it’s often hard to separate the good images from the visual noise. The question we put forward at EyeEm is: can a machine…I'm having fun wading through the pedantic pseudo-analysis here. The topic is similar to my undergrad thesis, which suggested that we're a long way off from using standard CNN models to partition images based on single user's preferences in a art space detached from reality like abstract art. Maybe we just need more training data.https://docs.google.com/pre...Great work. Finally this is picking up. Worked on Computational Aesthetics about 12 years ago but limited it to Early Vision and dropping semantics.Great work! We are trying to solve a similar problem with Cornea Ai (https://cornea.ai/) albeit more from the point of view of virality and popularity than aesthetics. Trick is to keep adding to the dataset more and more variety of photos accounting for demographics, age and gender apart from events and places.nice post i already bookmarked ithttps://www.imperial-car-re...Powered by Discourse, best viewed with JavaScript enabled"
2573,nvidia-clara-train-sdk-now-available,"Originally published at:			NVIDIA Clara Train SDK Now Available | NVIDIA Technical Blog
At the Society for Imaging Informatics in Medicine, 2019, NVIDIA released the Clara Train SDK for General Availability. The SDK is available for download now from NGC. This version includes: APIs to add AI-assisted  annotation to any medical viewer with new capabilities like Auto-Annotation and interactive annotation modes. Also includes Annotation Server that makes pre-trained models available to…Powered by Discourse, best viewed with JavaScript enabled"
2574,validate-applications-for-secure-edge-deployments-with-the-expanded-nvidia-metropolis-partner-program,"Originally published at:			https://developer.nvidia.com/blog/validate-applications-for-secure-edge-deployments-with-the-expanded-nvidia-metropolis-partner-program/
The Metropolis Partner Program expanded to include a certification that ensures partner applications can be securely deployed to any location with NVIDIA Fleet Command.Powered by Discourse, best viewed with JavaScript enabled"
2575,transforming-noisy-low-resolution-into-high-quality-videos-for-captivating-end-user-experiences,"Originally published at:			https://developer.nvidia.com/blog/transforming-noisy-low-resolution-into-high-quality-videos-for-captivating-end-user-experiences/
The NVIDIA Maxine Video Effects SDK offers AI-based visual features that transform noisy, low-resolution video streams into pleasant user experiences. This post demonstrates how you can run these effects with standard webcam input and easily integrate them into video conference and content creation pipelines.Powered by Discourse, best viewed with JavaScript enabled"
2576,performance-portability-from-gpus-to-cpus-with-openacc,"Originally published at:			https://developer.nvidia.com/blog/performance-portability-gpus-cpus-openacc/
OpenACC gives scientists and researchers a simple and powerful way to accelerate scientific computing applications incrementally. The OpenACC API describes a collection of compiler directives to specify loops and regions of code in standard C, C++, and Fortran to be offloaded from a host CPU to an attached accelerator. OpenACC is designed for portability across operating…What is the cost for this compiler?Academic can get a free PGI compiler license with the NVIDIA OpenACC Toolkit download. Non-academic users can get a free trial and full PGI pricing information is available here: http://www.pgroup.com/prici...Does this compiler support vectorization or only multi-threading?Vector clauses do not affect code generation at this point, but the compiler will vectorize and generate SIMD loops automatically where it can.  In other words, the existing PGI auto-vectorizer is not disabled when OpenACC for multicore is used. There are also some vectorization improvements in the works for future releases.Powered by Discourse, best viewed with JavaScript enabled"
2577,gtc-2020-from-training-to-inference-maximizing-resource-usage-and-reducing-cost-with-gpu-virtualization-on-vmware-vsphere,"GTC 2020 S21339
Presenters: Raj Rao,NVIDIA; Uday Kurkure,VMware; Lan Vu,VMware
Abstract
As machine learning and artificial intelligence are increasingly adopted across all industries, their workload share in data centers is growing. We’ll present use cases to optimize the cost and resource of your data center for ML on VMware vSphere with GPU virtualization, especially with NVIDIA GRID. We’ll discuss the differences in resource utilization between training and inference, and showcase techniques to maximize the benefits of GPU for your deep-learning workloads. These techniques include sharing GPU by multiple concurrent users or workloads, using GPU scheduling policies, and optimizing for training and inference in cloud environment. We’ll demonstrate how we applied these techniques in our real-world ML/AI applications at VMware and how they help us further improve the performance of these applications, enabling real-time analytics while reducing the cost of deployment with the latest Volta/Turing GPUs and NVIDIA GRID.Watch this session
Join in the conversation below.Powered by Discourse, best viewed with JavaScript enabled"
2578,12-things-you-should-know-about-the-tesla-accelerated-computing-platform,"Originally published at:			https://developer.nvidia.com/blog/12-things-tesla-accelerated-computing-platform/
You may already know NVIDIA Tesla as a line of GPU accelerator boards optimized for high-performance, general-purpose computing. They are used for parallel scientific, engineering, and technical computing, and they are designed for deployment in supercomputers, clusters, and workstations. But it’s not just the GPU boards that make Tesla a great computing solution. The combination of…Udacity course about CUDA parallel programming is great, but instead of tutorial character it aims at advanced developers. For newbie's there is a great book of ""CUDA by example"" by Jason Sanders and Edward Kandrot.Why not use this for Seti or a cure for cancer?Good idea. ""Compute the Cure"" grants: https://www.nvidia.com/en-u...CUDA Seti@Home: https://setiathome.berkeley...Had no idea. Sorry I was ignorant, but it isn't well known. Thanks for being cool.I do apollogize.Powered by Discourse, best viewed with JavaScript enabled"
2579,accelerating-io-in-the-modern-data-center-magnum-io-architecture,"Originally published at:			https://developer.nvidia.com/blog/accelerating-io-in-the-modern-data-center-magnum-io-architecture/
Previously the boundary of the unit of computing, sheet metal no longer constrains the resources that can be applied to a single problem or the data set that can be housed. The new unit is the data center. A single box can hold not just one GPU, CPU, and NIC, but a host of cooperating…Bringing these many parts together has been an exhilarating labor of  love.   But the fun part is collaborating with others to build a community around the innovations so they fit together to solve end to end problems.  We look forward to your insights and feedback.Just a heads-up: The link for GPUDirect Storage download points to an restricted page. The public link should be Magnum IO GPUDirect Storage | NVIDIA Developer.Thanks for the tip! I’ve made this change.Powered by Discourse, best viewed with JavaScript enabled"
2580,automating-sports-highlights-with-ai,"Originally published at:			Automating Sports Highlights with AI | NVIDIA Technical Blog
California-based startup REELY and a member of the NVIDIA Inception Program developed a deep learning platform that automatically generates sports highlights in real-time. Using NVIDIA Tesla GPUs on the Amazon cloud with the cuDNN-accelerated Caffe deep learning framework, the team trained their recurrent neural networks to understand sports broadcast commonalities across a wide array of…Powered by Discourse, best viewed with JavaScript enabled"
2581,hpc-top-resources-from-gtc-21,"Originally published at:			https://developer.nvidia.com/blog/hpc-top-resources-from-gtc-21/
Get the latest resources and news about the NVIDIA technologies that are accelerating the latest innovations in HPC from industry leaders and developers.Powered by Discourse, best viewed with JavaScript enabled"
2582,ai-helps-generate-speech-from-brain-recordings,"Originally published at:			AI Helps Generate Speech From Brain Recordings | NVIDIA Technical Blog
To help people who have lost the ability to speak, researchers from the University of California, San Francisco developed a deep learning method that can decode and convert brain signals into speech. “Neurological conditions that result in the loss of communication are devastating,” the researchers wrote in their paper. “Technology that translates neural activity into…Powered by Discourse, best viewed with JavaScript enabled"
2583,rtx-coffee-break-ray-traced-reflections-and-denoising-9-52-minutes,"Originally published at:			https://developer.nvidia.com/blog/rtx-coffee-break-ray-traced-reflections-and-denoising-952-minutes/
Reproducing accurate reflections has always been a big challenge in game development. Artists and engineers traditionally needed to apply a complex set of connected solutions – and even then, the results can be jarring. Edward explains how ray tracing and denoising solves this. Five Things to Remember: Prior solutions such as SSR (Screen Space Reflection)…Powered by Discourse, best viewed with JavaScript enabled"
2584,ai-for-a-scientific-computing-revolution,"Originally published at:			https://developer.nvidia.com/blog/ai-for-a-scientific-computing-revolution/
AI and its newest subdomain generative AI are dramatically accelerating the pace of change in scientific computing research. From pharmaceuticals and materials science to astronomy, this game-changing technology is opening up new possibilities and driving progress at an unprecedented rate. In this post, we explore some new and exciting applications of generative AI in science,…Tom’s presentation on Gen AI in science to a group of supercomputing facility leaders led to this post.  I researched all the papers on the subject and learned how Gen AI is being applied to these diverse scientific research projects.  His years of experience in the field gives him the perspective to say Gen AI is driving the fastest pace of change in scientific research he has seen in his career!This report from HPCWire on some talks at the ISC Conference on AI for Science using Digital Twins was generating a LOT of buzz at the showhttps://www.hpcwire.com/2023/05/25/the-grand-challenge-of-simulating-nuclear-fusion-an-overview-with-ukaeas-rob-akers/Powered by Discourse, best viewed with JavaScript enabled"
2585,gtc-2020-few-shot-adaptive-video-to-video-synthesis,"GTC 2020 S21142
Presenters: Ting-Chun Wang,NVIDIA
Abstract
Learn about GPU acceleration for Random Forest. We’ll focus on how to use high performance RF from RAPIDS, describe the algorithm in detail, and show benchmarks on different datasets. We’ll also focus on performance optimizations done along the way and disseminate the lessons learned.Watch this session
Join in the conversation below.Powered by Discourse, best viewed with JavaScript enabled"
2586,announcing-the-nvidia-clara-platform-bringing-ai-to-the-next-generation-of-medical-instruments,"Originally published at:			Announcing the NVIDIA Clara Platform: Bringing AI to the Next Generation of Medical Instruments | NVIDIA Technical Blog
At GTC Japan in Tokyo, NVIDIA unveiled the Clara platform, a revolutionary computing architecture based on the NVIDIA Xavier AI computing module and NVIDIA Turing GPUs. The platform, which is a combination of hardware and software brings AI to the next generation of medical instruments. “The Clara platform addresses the great challenge of medical instruments:…Powered by Discourse, best viewed with JavaScript enabled"
2587,new-ai-imaging-technique-reconstructs-photos-with-realistic-results,"Originally published at:			https://developer.nvidia.com/blog/new-ai-imaging-technique-reconstructs-photos-with-realistic-results/
Researchers from NVIDIA, led by Guilin Liu, introduced a state-of-the-art deep learning method that can edit images or reconstruct a corrupted image, one that has holes or is missing pixels. The method can also be used to edit images by removing content and filling in the resulting holes. The method, which performs a process called…Powered by Discourse, best viewed with JavaScript enabled"
2588,archigan-a-generative-stack-for-apartment-building-design,"Originally published at:			ArchiGAN: a Generative Stack for Apartment Building Design | NVIDIA Technical Blog
Figure 1. GAN-Generated masterplan AI will soon massively empower architects in their day-to-day practice. This potential is around the corner and my work provides a proof of concept. The framework used in my work offers a springboard for discussion, inviting architects to start engaging with AI, and data scientists to consider Architecture as a field of investigation.…Interesting - I am also working on a 3D-gan space planing, in particular to generate small modular dwellings but I got stucked when I generate vectors in a given space. As oposed to learning creatively from images, i want to develop a network of boundaries but i dont know how to represent them in 3d. In 2d space makes sense, you touched a very sensitive part of space planning. Congrats!  https://uploads.disquscdn.c...Hi, Can you please provide a link to the dataset used for training here?would be happy to see it implemented on FOSS tools like Blender…
see osarch.orgTake (i) a floor plate (Gross Square footage; length and width of building; assume same square footage of both sides of center-line); and, (ii) a set of standardized apartment floor plans:Use a modular approach to Unit design:A Module’s dimensions are (must be):
(i) length equal to the length of 1/2 of the building width
(ii) width - a number that when multiplied by the width is divisible by 1/4 and 1/2, each of which result in planned room or space usage
(iii) that is, all room spaces consume either: 100%, are some combination of quarters (75%; 50% or 25%) of a standard module dimension
EX: 40 foot width by 16.26 foot = a 325 SF module
(a) Studio - one module
(b) 1BR 1 Bath - two modules
(c) 2BR 1 Bath - three modules
(d) many variations using 1/2 and 1/4 modules added to the above
Then:Use a given for (a) corridor space, (b) elevator shaft space, (c) stairwell space
Use the set of Apt Units selected for the building
Factor Two Story units for Town-homes (if selected)Requirement - Determine the Unit mix that consumes/maximizes the floor plate square footageHow can I train myself to use AI for architecture? Where do I even start?  I was behind the curve when Revit gained the edge over AutoCAD. It has been a painful process to catch up.  I would love to be in front of the curve with adapting AI. Thank youPowered by Discourse, best viewed with JavaScript enabled"
2589,tensor-core-programming-using-cuda-fortran,"Originally published at:			Tensor Core Programming Using CUDA Fortran | NVIDIA Technical Blog
The CUDA Fortran compiler from PGI now supports programming Tensor Cores with NVIDIA’s Volta V100 and Turing GPUs. This enables scientific programmers using Fortran to take advantage of FP16 matrix operations accelerated by Tensor Cores. Let’s take a look at how Fortran supports Tensor Cores. Tensor Cores Tensor Cores offer substantial performance gains over typical CUDA…Powered by Discourse, best viewed with JavaScript enabled"
2590,employing-cuda-graphs-in-a-dynamic-environment,"Originally published at:			Employing CUDA Graphs in a Dynamic Environment | NVIDIA Technical Blog
Many workloads can be sped up greatly by offloading compute-intensive parts onto GPUs. In CUDA terms, this is known as launching kernels. When those kernels are many and of short duration, launch overhead sometimes becomes a problem. One way of reducing that overhead is offered by CUDA Graphs. Graphs work because they combine arbitrary numbers…Hello thank You for sharing very intresting blog post! Is it possible to use CUDA graphs in a while loop - I mean I will execute the kernel multiple times until some condition will be met - Hence I do not know in advance how long should be sequence of kernel lounches in the graph - currently I manage by running loop inside the cooperative kernel and syncgrid(), but it would be more convinient to separate logic into multiple smaller kernels and avoid gridsync .Thank you for your comment and question. It does not matter whether the kernels that you would like to put in a CUDA graph are executed in a for loop, a while loop, or any other construct, as long as the conditions for CUDA graphs are met. That means that the topology of the resulting graph does not change from one execution of your while loop to the next (this would allow you to use the graph update API), or that the exact same graph is encountered multiple times, so that it can be retrieved from a container in which it was stored upon first encounter.Thank You for a reply ! Hovewer I probably did not made it clear I have a while loop where  set of 4 kernels inside may be launched for example 200 or 10 or 300 - depending on data, and I am looking for a way to avoid penalty for launching kernel multiple times - so use CUDA graph.
Obviously I can create a graph with 4 kernel nodes and then launch it in while loop - still it will lead to starting couple hundred graphs - and I would like to make it one graph.
Still in order to execute it I suppose I would need some graph in form of a loop and be able to stop execution of this graph when given condition read from global memory will be present. I had seen in CUDA 11.6 cudaGraphNodeSetEnabled function can it be used to stop execution of a graph from inside of the kernel?Powered by Discourse, best viewed with JavaScript enabled"
2591,using-nvidia-nsight-compute-in-containers,"Originally published at:			https://developer.nvidia.com/blog/using-nsight-compute-in-containers/
Containers are now ubiquitous, and for good reason; the portability and productivity enhancements they provide have made them a standard component in HPC and many other computing fields. The NVIDIA Nsight family of developer tools for analyzing performance of CUDA applications are supported in container environments. For more information about the environmental landscape and Nsight…Powered by Discourse, best viewed with JavaScript enabled"
2592,cugraph-and-sparse-matrices,"Thanks for being here! I am curious on the relationship between cuGraph and other CUDA libraries. My work now is highly dependent on cuSparse and cuSolver for geometry surface meshes.Can cuGraph accommodate sparse adjacency matrix as input? Can I use it as an extension of cuSparse?Good to know that! Thank you for the reply.This topic was automatically closed 14 days after the last reply. New replies are no longer allowed.Powered by Discourse, best viewed with JavaScript enabled"
2593,ai-system-can-detect-objects-around-corners,"Originally published at:			AI System Can Detect Objects Around Corners | NVIDIA Technical Blog
To help autonomous vehicles and robots potentially spot objects that lie just outside a system’s direct line-of-sight, Stanford, Princeton, Rice, and Southern Methodist universities researchers developed a deep learning-based system that can detect objects, including words and symbols, around corners. “Compared to other approaches, our non-line-of-sight imaging system provides uniquely high resolutions and imaging speeds,”…Powered by Discourse, best viewed with JavaScript enabled"
2594,competition-and-community-insights-from-nvidia-s-kaggle-grandmasters,"Originally published at:			https://developer.nvidia.com/blog/competition-and-community-insights-from-nvidias-kaggle-grandmasters/
In this post, we summarize questions and answers from GTC sessions with NVIDIA’s Kaggle Grandmaster team.  Additionally, we answer audience questions we did not get a chance during these sessions. Q: How do you decide which competitions to join? Ahmet: I read the competition description and evaluation metric. Then I give myself several days to…Powered by Discourse, best viewed with JavaScript enabled"
2595,transform-the-data-center-for-the-ai-era-with-nvidia-dpus-and-nvidia-doca,"Originally published at:			https://developer.nvidia.com/blog/transform-the-data-center-for-the-ai-era-with-nvidia-dpus-and-nvidia-doca/
NVIDIA BlueField-3 DPUs are now in full production, and have been selected by Oracle Cloud Infrastructure (OCI) to achieve higher performance, better efficiency, and stronger security.Powered by Discourse, best viewed with JavaScript enabled"
2596,new-release-of-nvidia-rtxgi-1-1-with-unreal-engine-4-support,"Originally published at:			New Release of NVIDIA RTXGI 1.1 with Unreal Engine 4 Support | NVIDIA Technical Blog
Earlier this year, we released RTX Global Illumination (RTXGI) – an SDK that provides scalable solutions to compute multi-bounce global illumination in real-time with ray tracing. Today, we are pleased to announce the release of RTXGI 1.1, an updated version of the SDK with performance and image quality improvements, bug fixes, new features, and support…Powered by Discourse, best viewed with JavaScript enabled"
2597,ai-helps-improve-tumor-diagnosis-in-the-operating-room,"Originally published at:			AI Helps Improve Tumor Diagnosis in the Operating Room | NVIDIA Technical Blog
To help neurosurgeons diagnose brain tumors more efficiently, researchers from the University of Michigan developed a deep learning-based imaging technique that can reduce the tumor diagnosis process during surgery from 30-40 minutes to less than three minutes.  First unveiled in 2017, the technique called stimulated Rama histology (SRH) helps neurosurgeons more rapidly assess tumor tissue…Powered by Discourse, best viewed with JavaScript enabled"
2598,gpu-accelerated-graph-analytics-in-python-with-numba,"Originally published at:			https://developer.nvidia.com/blog/gpu-accelerated-graph-analytics-python-numba/
Numba is an open-source just-in-time (JIT) Python compiler that generates native machine code for X86 CPU and CUDA GPU from annotated Python Code. (Mark Harris introduced Numba in the post Numba: High-Performance Python with CUDA Acceleration.) Numba specializes in Python code that makes heavy use of NumPy arrays and loops. In addition to JIT compiling…Thank you for this great insight into cuda/ python mashup. Is the complete source for the second example available by any chance? Thank you very much.I have extracted the CPU and GPU random-walk implementations into a Gist: https://gist.github.com/skl...I have replaced the webgraph with a randomly generated graph so that you don't have to download the massive webgraph to test it out.Thank you very much, for the model and the fast response.Please help me. It's urgent.https://stackoverflow.com/q...Powered by Discourse, best viewed with JavaScript enabled"
2599,nvidia-clara-train-and-deploy-brings-new-features-to-medical-imaging-developers,"Originally published at:			https://developer.nvidia.com/blog/clara-train-deploy-medical-imaging-developers/
NVIDIA released its latest version of Clara Train and Deploy application frameworks. From AutoML to workflow manager for priority scheduling these releases bring state-of-the-art capabilities to AI development and deployment in medical imaging.Powered by Discourse, best viewed with JavaScript enabled"
2600,question-about-reference-rtx-3080-fan-header,"Hello. I’m wondering what type of connector is used on for the cooling fan on the reference pcb. Is it by chance a JST-PH?Powered by Discourse, best viewed with JavaScript enabled"
2601,icymi-new-ai-tools-and-technologies-announced-at-gtc-2021-keynote,"Originally published at:			ICYMI: New AI Tools and Technologies Announced at GTC 2021 Keynote | NVIDIA Technical Blog
At GTC 2021, NVIDIA announced new software tools to help developers build optimized conversational AI, recommender, and video solutions.Powered by Discourse, best viewed with JavaScript enabled"
2602,using-physics-informed-deep-learning-for-transport-in-porous-media,"Originally published at:			Using Physics-Informed Deep Learning for Transport in Porous Media | NVIDIA Technical Blog
Simulations are pervasive in every domain of science and engineering, but they are often constrained by large computational times, limited compute resources, tedious manual setup efforts, and the need for technical expertise. NVIDIA SimNet is a simulation toolkit that addresses these challenges with a combination of AI and physics.  A success story of SimNet’s application…I do not see where the neural network comes into play, this looks like a PDE problemPowered by Discourse, best viewed with JavaScript enabled"
2603,kubernetes-on-nvidia-gpus-release-candidate-now-available,"Originally published at:			Kubernetes on NVIDIA GPUs Release Candidate Now Available | NVIDIA Technical Blog
Today at the Computer Vision and Pattern Recognition (CVPR) conference, we’re making the release candidate Kubernetes on NVIDIA GPUs freely available to developers for feedback and testing. Kubernetes on NVIDIA GPUs enables enterprises to scale up training and inference deployment to multi-cloud GPU clusters seamlessly. It lets you automate the deployment, maintenance, scheduling and operation…Powered by Discourse, best viewed with JavaScript enabled"
2604,google-open-sources-image-captioning-intelligence,"Originally published at:			Google Open-Sources Image Captioning Intelligence | NVIDIA Technical Blog
Google released the latest version of their automatic image captioning model that is more accurate, and is much faster to train compared to the original system. “The TensorFlow implementation released today achieves the same level of accuracy with significantly faster performance: time per training step is just 0.7 seconds in TensorFlow compared to 3 seconds…Powered by Discourse, best viewed with JavaScript enabled"
2605,new-update-to-the-nvidia-deep-learning-sdk-now-help-accelerate-inference,"Originally published at:			https://developer.nvidia.com/blog/new-updates-to-the-nvidia-deep-learning-sdk-now-helps-accelerate-inference/
The latest update to the NVIDIA Deep Learning SDK includes the NVIDIA TensorRT deep learning inference engine (formerly GIE) and the new NVIDIA Deep Stream SDK. TensorRT delivers high performance inference for production deployment of deep learning applications. The latest release delivers up to 3x more throughput, using 61% less memory with new INT8 optimized…Powered by Discourse, best viewed with JavaScript enabled"
2606,gtc-2020-more-powerful-secure-ai-at-the-edge-with-nvidia-egx,"GTC 2020 S22704
Presenters: Erik Bohnhorst,NVIDIA; Jacob Liberman, NVIDIA
Abstract
The newest addition to the NVIDIA EGX platform, NVIDIA EGX A100 combines the NVIDIA Ampere architecture and NVIDIA Mellanox ConnectX-6 Dx SmartNIC on a single board. The powerful combination of the NVIDIA Mellanox SmartNIC and NVIDIA’s eighth-generation GPU architecture creates an enhanced security platform for end to end AI processing.Watch this session
Join in the conversation below.Powered by Discourse, best viewed with JavaScript enabled"
2607,build-scalable-immersive-experiences-with-networking-apis-swift-support-and-more-using-nvidia-cloudxr-3-2,"Originally published at:			https://developer.nvidia.com/blog/build-scalable-immersive-experiences-with-networking-apis-swift-support-and-more-using-nvidia-cloudxr-3-2￼/
The latest release of NVIDIA CloudXR includes some of the most requested features from the developer community, such as support for pre-configuring a remote server.Does it offer support to Mobile VR as in Cardboard?No, Cardboard is not supported.Powered by Discourse, best viewed with JavaScript enabled"
2608,5-omniverse-sessions-you-can-t-miss-at-gtc-21,"Originally published at:			https://developer.nvidia.com/blog/gtc-21-top-5-omniverse-sessions-for-developers/
Discover how next-generation collaboration and simulation are bringing the future closer for developers across industries.Powered by Discourse, best viewed with JavaScript enabled"
2609,ai-app-enhances-the-songwriting-process,"Originally published at:			AI App Enhances the Songwriting Process | NVIDIA Technical Blog
Think of it as music sampling but with the help of AI. Developers from Japan-based startup Amadeus Code recently unveiled a new app that uses deep learning to transform your favorite songs into new compositions. The app, which is available on the iTunes store, can create new songs in a matter of seconds. Think of…Powered by Discourse, best viewed with JavaScript enabled"
2610,nvidia-research-an-unbiased-ray-marching-transmittance-estimator,"Originally published at:			https://developer.nvidia.com/blog/nvidia-research-an-unbiased-ray-marching-transmittance-estimator/
NVIDIA researchers will present their paper “An Unbiased Ray-Marching Transmittance Estimator” at SIGGRAPH 2021, August 9-13, showing a new way to compute visibility in scenes with complex volumetric effects.Powered by Discourse, best viewed with JavaScript enabled"
2611,cudacasts-episode-7-nvidia-smi-accounting,"Originally published at:			https://developer.nvidia.com/blog/cudacasts-episode-7-nvidia-smi-accounting/
The NVIDIA System Management Interface, nvidia-smi, is a command-line interface to the NVIDIA Management Library, NVML. nvidia-smi provides Linux system administrators with powerful GPU configuration and monitoring tools. HPC cluster system administrators need to be able to monitor resource utilization (processor time, memory usage, etc.) on their systems. This resource monitoring is typically called accounting. New in…Powered by Discourse, best viewed with JavaScript enabled"
2612,achieving-100x-faster-single-cell-modality-prediction-with-nvidia-rapids-cuml,"Originally published at:			https://developer.nvidia.com/blog/achieving-100x-faster-single-cell-modality-prediction-with-nvidia-rapids-cuml/
Single-cell measurement technologies have advanced rapidly, revolutionizing the life sciences. We have scaled from measuring dozens to millions of cells and from one modality to multiple high dimensional modalities. The vast amounts of information at the level of individual cells present a great opportunity to train machine learning models to help us better understand the…Powered by Discourse, best viewed with JavaScript enabled"
2613,loading-data-fast-with-dali-and-the-new-hardware-jpeg-decoder-in-nvidia-a100-gpus,"Originally published at:			Loading Data Fast with DALI and the New Hardware JPEG Decoder in NVIDIA A100 GPUs | NVIDIA Technical Blog
Today, smartphones, the most popular device for taking pictures, can capture images as large as 4K UHD (3840×2160 image), more than 25 MB of raw data. Even considering the embarrassingly low HD resolution (1280×720), a raw image requires more than 2.5 MB of storage. Storing as few as 100 UHD images would require almost 3…Powered by Discourse, best viewed with JavaScript enabled"
2614,oregon-state-university-installs-six-nvidia-dgx-2-systems,"Originally published at:			Oregon State University Installs Six NVIDIA DGX-2 Systems | NVIDIA Technical Blog
The Oregon State University College of Engineering has just purchased six NVIDIA DGX-2 systems to help accelerate their work in AI, robotics, driverless vehicles, and other research areas that require powerful GPU compute.  “The computing power we now possess will accelerate our research in artificial intelligence and machine learning, while exposing our computer science students…Powered by Discourse, best viewed with JavaScript enabled"
2615,nvidia-boosts-academic-ai-research,"Originally published at:			NVIDIA Boosts Academic AI Research for Business Development
To help AI research like this make the leap from academia to commercial or government deployment, NVIDIA today announced the Applied Research Accelerator Program. The program supports applied research on NVIDIA platforms for GPU-accelerated application deployments.Powered by Discourse, best viewed with JavaScript enabled"
2616,nvidia-omniverse-available-for-early-access-customers,"Originally published at:			NVIDIA Omniverse Available for Early Access Customers | NVIDIA Technical Blog
NVIDIA Omniverse, a computer graphics and simulation platform that enables artists to collaborate seamlessly in real time, is now available for early access customers in the architecture, engineering and construction (AEC) market. Using Pixar’s Universal Scene Description and NVIDIA RTX technology, Omniverse allows multiple people to easily work with popular content creation applications and collaborate…Powered by Discourse, best viewed with JavaScript enabled"
2617,deep-learning-in-a-nutshell-sequence-learning,"Originally published at:			https://developer.nvidia.com/blog/deep-learning-nutshell-sequence-learning/
This series of blog posts aims to provide an intuitive and gentle introduction to deep learning that does not rely heavily on math or theoretical constructs. The first part of this series provided an overview of the field of deep learning, covering fundamental and core concepts. The second part of the series provided an overview of…Would u talk the principles behind Alphago in your next post on RL ?could this help to compress, say, wikipedia any better than current techniques? or, perhaps, help to find similarities in DNA sequences better than existing algorithms?can deep learning, specifically LSTM can be used for topic modeling purpose? Deep neural networks are commonly used for classification purposes. I want to produce cluster of words that represent a topic, is it possible with deep learning?Powered by Discourse, best viewed with JavaScript enabled"
2618,massively-improved-multi-node-nvidia-gpu-scalability-with-gromacs,"Originally published at:			https://developer.nvidia.com/blog/massively-improved-multi-node-nvidia-gpu-scalability-with-gromacs/
GROMACS, a scientific software package widely used for simulating biomolecular systems, plays a crucial role in comprehending important biological processes important for disease prevention and treatment. GROMACS can use multiple GPUs in parallel to run each simulation as quickly as possible. Over the past several years, NVIDIA and the core GROMACS developers have collaborated on…Powered by Discourse, best viewed with JavaScript enabled"
2619,gtc-digital-demo-assessing-property-damage-with-ai,"Originally published at:			GTC Digital Demo: Assessing Property Damage with AI | NVIDIA Technical Blog
The critical task of damage claim processing is typically labor-intensive and requires a significant amount of time.  This demo, released at GTC Digital 2020, shows the workflow USAA used to perform damage assessment after the Woolsey fire, a wildfire that damaged thousands of homes and burned over 97,000 acres in parts of Los Angeles and…Powered by Discourse, best viewed with JavaScript enabled"
2620,nvidia-tensorrt-inference-server-now-open-source,"Originally published at:			https://developer.nvidia.com/blog/nvidia-tensorrt-inference-server-now-open-source/
In September 2018, NVIDIA introduced NVIDIA TensorRT Inference Server, a production-ready solution for data center inference deployments. TensorRT Inference Server maximizes GPU utilization, supports all popular AI frameworks, and eliminates writing inference stacks from scratch. You can learn more about TensorRT Inference Server in this NVIDIA Developer blog post. Today we are announcing that NVIDIA…Powered by Discourse, best viewed with JavaScript enabled"
2621,gtc-ai-video-analytics-presentations,"Originally published at:			GTC: AI Video Analytics Presentations | NVIDIA Technical Blog
From creating AI models to building a real-time seamless video analytics pipeline, developers need powerful tools to simplify their workflows while achieving high performance and highly accurate results. The GPU Technology Conference, Oct. 5-9, will feature NVIDIA technologies that help developers overcome all these challenges. Here are some top technical sessions that will help developers…Powered by Discourse, best viewed with JavaScript enabled"
2622,accelerating-medical-image-processing-with-nvidia-dali,"Originally published at:			https://developer.nvidia.com/blog/accelerating-medical-image-processing-with-dali/
Deep learning models require a lot of data to produce accurate predictions. Here’s how to solve the data processing problem for the medical domain with NVIDIA DALI.Powered by Discourse, best viewed with JavaScript enabled"
2623,gtc-ai-art-gallery-using-ai-to-recreate-lost-da-vinci-painting,"Originally published at:			GTC AI Art Gallery: Using AI to Recreate Lost da Vinci Painting | NVIDIA Technical Blog
We first told you about their work in 2019 when they recreated a lost Picasso painting.  Now as part of the GTC AI Art Gallery, artists George Cann and Anthony Bourached, and their company Oxia Palus, are debuting a recreated a masterpiece, a lost Leonardo da Vinci painting.  The painting seen below, and shown publicly…Powered by Discourse, best viewed with JavaScript enabled"
2624,building-a-benchmark-for-human-level-concept-learning-and-reasoning,"Originally published at:			https://developer.nvidia.com/blog/building-a-benchmark-for-human-level-concept-learning-and-reasoning/
Humans have an inherent ability to learn novel concepts from only a few samples and generalize these concepts to different situations. Even though today’s machine learning models excel with an abundance of training data on standard recognition tasks, a considerable gap exists between machine-level pattern recognition and human-level concept learning. Over 50 years ago, M.…From early drawings of horses in the Lascaux Cave to the advanced Egyptian Hieroglyphic Alphabet - the ability for humankind to covey ideas through images and symbols persists across history. When will machine-level pattern recognition and human-level concept learning intersect? This latest benchmark works to close the gap. Share your thoughts on this work with the team.Powered by Discourse, best viewed with JavaScript enabled"
2625,manufacturing-the-future-of-ai-with-edge-computing,"Originally published at:			https://developer.nvidia.com/blog/manufacturing-the-future-of-ai-with-edge-computing/
Read how the power of AI and edge computing is critical to driving operational efficiencies and productivity gains.Powered by Discourse, best viewed with JavaScript enabled"
2626,reducing-costs-with-one-pass-reverse-time-migration,"Originally published at:			https://developer.nvidia.com/blog/reducing-costs-with-one-pass-reverse-time-migration/
Reverse time migration (RTM) is a powerful seismic migration technique, providing geophysicists with the ability to create accurate 3D images of the subsurface. Steep dips? Complex salt structure? High velocity contrast? No problem. By splitting the upgoing and downgoing wavefields and combining them with an accurate velocity model, RTM can image even the most complex…While a one-pass TTI RTM is much more computationally efficient, it adds code complexity. NVIDIA has a collection of very well documented sample code that lowers the barrier to a one-pass TTI RTM. This sample code includes other tricks, such as compression on GPUs, which alleviates bandwidth bottleneck caused by snapshots. All code is freely available under NDA and our team can help with implementation/optimization. If interested, please reach out to me at reynaldog@nvidia.comCan I get a code sample? I mainly want to see the effect of A100 Cache Data Compression。Hello Sazc,Are you with a university or company? We can share the code once an NDA is in place. Email me at reynaldog@nvidia.com.Thanks,
ReyPowered by Discourse, best viewed with JavaScript enabled"
2627,cudacasts-episode-11-gpu-libraries-for-cuda-python,"Originally published at:			https://developer.nvidia.com/blog/cudacasts-episode-11-gpu-libraries-cuda-python/
In the previous episode of CUDACasts I introduced you to NumbaPro, the high-performance Python compiler from Continuum Analytics, and demonstrated how to accelerate simple Python functions on the GPU. Continuing the Python theme, today’s CUDACast demonstrates NumbaPro’s support for CUDA libraries. The optimized algorithms in GPU-accelerated libraries often provide the easiest way to accelerate applications. NumbaPro…Hi Mark, Where can I obtain the python script(s) for this cast?I took a peak at Github but I see no Python examples under CUDACasts and Mark Ebersole, only examples written in C.Mark,  either there's an error in what you wrote or I'm missing something fundamental.    When you write d_next =  step(....)   python should just overwrite the name d_next with whatever step returns.  Did you mean to write  d_next[:] = step(....)    instead.   if not could you please explain why the way you wrote it works.    I'm confused!Powered by Discourse, best viewed with JavaScript enabled"
2628,cuda-accelerates-computational-discovery-of-new-nanoporous-materials,"Originally published at:			CUDA Accelerates Computational Discovery of New Nanoporous Materials | NVIDIA Technical Blog
While GPUs were originally developed for computer graphics, they are now being used by scientists to help solve important engineering problems. The performance gains from parallelizing molecular simulation codes in CUDA have facilitated efforts to computationally evaluate large databases of nanoporous material structures for several applications. Researchers from UC Berkeley and Lawrence Berkeley National Laboratory…Powered by Discourse, best viewed with JavaScript enabled"
2629,mayflower-autonomous-ship-to-retrace-the-famous-historical-voyage-powered-by-nvidia-jetson,"Originally published at:			Mayflower Autonomous Ship to Retrace the Famous Historical Voyage, Powered by NVIDIA Jetson | NVIDIA Technical Blog
In 1620, the Mayflower sailed from Plymouth, England to Plymouth, Massachusetts changing the history of the world. Four hundred years later, a fully autonomous ship will cross the Atlantic and retrace the route of the original Mayflower – this time with no human captain, or onboard crew. Not only will the ship commemorate the anniversary…Powered by Discourse, best viewed with JavaScript enabled"
2630,gtc-2020-creating-in-camera-vfx-with-real-time-workflows,"GTC 2020 S22160
Presenters: David Morin,Epic Games
Abstract
We’ll cover advancements in “in-camera visual effects” and how this technique is changing the film and TV industry. With software developments in real-time game engines, combined with hardware developments in GPUs and on-set video equipment, filmmakers can now capture final-pixel visual effects while still on set — enabling new levels of creative collaboration and efficiency during principal photography. These new developments allow changes to digital scenes, even those at final-pixel quality, to be seen instantly on high-resolution LED walls — an exponential degree of time savings over a traditional CG rendering workflow. This is crucial, as there is a huge demand for more original film and TV content, and studios must find a way to efficiently scale production and post-production while maintaining high quality and creative intent.Watch this session
Join in the conversation below.Powered by Discourse, best viewed with JavaScript enabled"
2631,nvidia-announces-a100-80gb-gpu-world-s-most-powerful-gpu-for-ai-supercomputing,"Originally published at:			NVIDIA Doubles Down: Announces A100 80GB GPU, Supercharging World’s Most Powerful GPU for AI Supercomputing | NVIDIA Newsroom
NVIDIA unveiled the NVIDIA A100 80GB GPU, providing researchers and engineers unprecedented speed and performance to unlock the next wave of AI and scientific breakthroughs.Powered by Discourse, best viewed with JavaScript enabled"
2632,harnessing-the-power-of-nvidia-ai-enterprise-on-azure-machine-learning,"Originally published at:			https://developer.nvidia.com/blog/harnessing-the-power-of-nvidia-ai-enterprise-on-azure-machine-learning/
NVIDIA AI Enterprise and Azure Machine Learning together create a powerful combination of GPU-accelerated computing and a comprehensive cloud-based machine learning platform.Powered by Discourse, best viewed with JavaScript enabled"
2633,introducing-nvidia-video-codec-sdk-10-presets,"Originally published at:			Introducing NVIDIA Video Codec SDK 10 Presets | NVIDIA Technical Blog
All NVIDIA GPUs starting with the Kepler generation support fully accelerated hardware video encoding, and all GPUs starting with the Fermi generation support fully accelerated hardware video decoding through the NVIDIA Video Codec SDK. The NVIDIA NVENC presets design in Video Codec SDK 9.1 and earlier evolved based on various NVENC use cases, which have…I can’t seem to find a compatibility matrix for Video Codec SDK versions prior to 11.0. I have 750M hardware and the last supported drivers are 418xx, but 390xx on NVidia. If I am going to create an AUR package to try to apply patches to the NVidia 410xx and/or 418xx drivers, just to find out that my device can’t support the Video Codec 9.0/10.0 functionality, I’m going to be pissed.About 3 months ago, for the first time ever in Linux, i was able to stream using OBS and NVidia’s NVENC codecs, which for me is the difference b/w being about to stream with 0% frames dropped and streaming with 30% frames dropped … no matter what I do. Which is wierd because in 2016, I was streaming with OBS in MacOS at 1080/3500kbs with zero problems.Please see this thread on the Garuda linux forum to understand what i’m going through to use my device. I have spent about 80 hours on this in the past two weeks.Since the 750 desktop chipset is supported by 455xx, if I find out that there is no good reason that my 750m chipset is not supported by the 455xx drivers, I am going to be seriously pissed off. And from someone who just looked through all of the available function calls to find references to NVENC, trust me, I will know.Powered by Discourse, best viewed with JavaScript enabled"
2634,explore-the-latest-in-graphics-with-professional-visualization-sessions-at-nvidia-gtc,"Originally published at:			https://developer.nvidia.com/blog/explore-the-latest-in-graphics-with-professional-visualization-sessions-at-nvidia-gtc/
There are several GTC sessions for professional content creators, engineers, and developers looking to explore new tools and techniques accelerated by NVIDIA.Powered by Discourse, best viewed with JavaScript enabled"
2635,drive-software-9-0-now-available-for-download,"Originally published at:			https://developer.nvidia.com/blog/drive-software-9-0-now-available-for-download/
DRIVE Software provides developers with an open platform for autonomous vehicle development comprised of SDKs, powerful tools and AV applications.Powered by Discourse, best viewed with JavaScript enabled"
2636,nvidia-biobert-for-domain-specific-nlp-in-biomedical-and-clinical-applications,"Originally published at:			NVIDIA BioBERT for Domain Specific NLP in Biomedical and Clinical Applications | NVIDIA Technical Blog
At GTC DC in Washington DC, NVIDIA announced NVIDIA BioBERT, an optimized version of BioBERT. BioBERT is an extension of the pre-trained language model BERT, that was created specifically for biomedical and clinical domains.  For context, over 4.5 billion words were used to train BioBERT, compared to 3.3 billion for BERT. BioBERT was built to…Powered by Discourse, best viewed with JavaScript enabled"
2637,ai-helps-doctors-detect-ms-in-the-spinal-cord,"Originally published at:			AI Helps Doctors Detect MS In the Spinal Cord | NVIDIA Technical Blog
A team of researchers from some of the top medical institutions in the world, developed a fully automatic deep learning-based system to detect multiple sclerosis (MS) lesions in the spinal cord and intramedullary from conventional MRI data. MS is a chronic immune disease of the central nervous system which appears in areas of the brain…Powered by Discourse, best viewed with JavaScript enabled"
2638,nvidia-research-unveils-flowtron-an-expressive-and-natural-speech-synthesis-model,"Originally published at:			NVIDIA Research Unveils Flowtron, an Expressive and Natural Speech Synthesis Model | NVIDIA Technical Blog
Many of today’s speech synthesis models lack emotion and human-like expression. To help tackle this problem, a team of researchers from the NVIDIA Applied Deep Learning Research group developed a state-of-the-art model that generates more realistic expressions and provides better user control than previously published models.  Named “Flowtron”, the model debuted publicly for the first…Powered by Discourse, best viewed with JavaScript enabled"
2639,build-neural-networks-for-self-driving-cars-with-matlab,"Originally published at:			Build Neural Networks for Self-Driving Cars with MATLAB | NVIDIA Technical Blog
Autonomous cars must locate and classify all the relevant objects on the road (such as other vehicles) so that they can brake or safely maneuver around vehicles or pedestrians. They must also detect lane markers in order to center the car within its lane. Deep learning enables machines to detect  and classify objects of interest more…Powered by Discourse, best viewed with JavaScript enabled"
2640,gtc-2020-cooperative-neural-networks,"GTC 2020 S21871
Presenters: Harsh Shrivastava,Georgia Tech
Abstract
We’ll walk you through a novel approach to come up with domain-specific deep-learning architectures called cooperative neural networks (CoNN). CoNN incorporates the known prior information of the domain in its architecture by exploiting the structure of the underlying probabilistic graphical model describing the domain. We’ll demonstrate our approach for the document classification task, where we transfer the independence structure of the popular Latent Dirichlet Allocation (LDA) model to a cooperative neural network, CoNN-sLDA. We’ll show that the CoNN-sLDA model outperforms existing state-of-the-art techniques. CoNN-sLDA model has considerably fewer parameters and gives a significant runtime improvement compared to existing deep-learning models.Pre-requisites: Basic familiarity with Deep Learning and Probabilistic Graphical Models.
Nvidia Hardware details: Experiments were ran on the Tesla V100 GPUs using Nvidia DGX Workstation.Watch this session
Join in the conversation below.Powered by Discourse, best viewed with JavaScript enabled"
2641,webinar-create-gesture-based-interactions-with-a-robot,"Originally published at:			Webinar: Create Gesture-Based Interactions with a Robot | NVIDIA Technical Blog
In this webinar, you will learn how to train your own gesture recognition deep learning pipeline. We’ll start with a pre-trained detection model, repurpose it for hand detection, and use it together with the purpose-built gesture recognition model. NVIDIA pre-trained deep learning models and the Transfer Learning Toolkit (TLT) give you a rapid path to…Powered by Discourse, best viewed with JavaScript enabled"
2642,maximize-network-automation-efficiency-with-digital-twins-on-nvidia-air,"Originally published at:			https://developer.nvidia.com/blog/maximize-network-automation-efficiency-with-digital-twins/
NVIDIA Air automates your network through a digital twin to increase efficiencies along with other benefits.Powered by Discourse, best viewed with JavaScript enabled"
2643,isc20-featured-demo-taking-a-closer-look-at-nvidia-mellanox-ufm,"Originally published at:			ISC20 Featured Demo: Taking a Closer Look at NVIDIA Mellanox UFM | NVIDIA Technical Blog
The NVIDIA Mellanox Unified Fabric Manager (UFM) provides enhanced network management, monitoring, optimizations, security, and more.  The newly announced UFM Cyber-AI applies AI to learning a data center’s operational cadence and network workload patterns, drawing on both real-time and historic telemetry and workload data. Against this baseline, it tracks the system’s health and network modifications…Powered by Discourse, best viewed with JavaScript enabled"
2644,rapidly-generate-3d-assets-for-virtual-worlds-with-generative-ai,"Originally published at:			https://developer.nvidia.com/blog/rapidly-generate-3d-assets-for-virtual-worlds-with-generative-ai/
To accelerate the development of 3D worlds and the metaverse, NVIDIA has launched numerous AI research projects to help creators across industries unlock new possibilities with generative AI. Generative AI will touch every aspect of the metaverse and it is already being leveraged for use cases like bringing AI avatars to life with Omniverse ACE.…Powered by Discourse, best viewed with JavaScript enabled"
2645,streamline-your-model-builds-with-pycaret-rapids-on-nvidia-gpus,"Originally published at:			https://developer.nvidia.com/blog/streamline-your-model-builds-with-pycaret-rapids-on-nvidia-gpus/
PyCaret is a low-code Python machine learning library based on the popular Caret library for R. It automates the data science process from data preprocessing to insights, such that short lines of code can accomplish each step with minimal manual effort. In addition, the ability to compare and tune many models with simple commands streamlines…This is a real power hose. As a R user, I am trying to find some similar for R that can help build models with R on NVIDIA GPUsHow do you run the PyCaret Models using cuML when you did not load the data using cuDF?I am a bit confused… Will PyCaret load the dataset on GPU behind the scene?
I ran your code, and used nvtop to monitor my GPU activities, I could see some models running on GPUPowered by Discourse, best viewed with JavaScript enabled"
2646,new-on-ngc-new-and-updated-hpc-containers-on-the-ngc-catalog,"Originally published at:			New on NGC: New and Updated HPC Containers on the NGC Catalog | NVIDIA Technical Blog
There are more than a hundred containers spanning HPC, deep learning and machine applications available in the NGC catalog, NVIDIA’s hub of GPU-optimized HPC and AI applications.Powered by Discourse, best viewed with JavaScript enabled"
2647,developing-and-deploying-ai-powered-robots-with-nvidia-isaac-sim-and-nvidia-tao,"Originally published at:			https://developer.nvidia.com/blog/developing-and-deploying-ai-powered-robots-with-nvidia-isaac-sim-and-nvidia-tao/
Learn how to develop an end-to-end workflow starting with synthetic data generation in NVIDIA Isaac Sim, fine-tuning with the TAO Toolkit and deploying model with NVIDIA Isaac ROS.Powered by Discourse, best viewed with JavaScript enabled"
2648,siggraph-2019-highlight-finding-a-practical-hybrid-anti-aliasing-algorithm,"Originally published at:			SIGGRAPH 2019 Highlight: Finding a Practical Hybrid Anti-Aliasing Algorithm | NVIDIA Technical Blog
With the recent introduction of DirectX Raytracing and NVIDIA RTX, interoperability between the rasterization pipeline and the ray tracing pipeline can be applied in real-time applications. The new technology does a lot of heavy lifting, but developers can’t just throw rays everywhere. They still have a limited ray budget to work with. They need to…Powered by Discourse, best viewed with JavaScript enabled"
2649,nvidia-and-google-cloud-deliver-high-quality-xr-streaming,"Originally published at:			https://developer.nvidia.com/blog/nvidia-and-google-cloud-deliver-high-quality-xr-streaming/
NVIDIA CloudXR is coming to NVIDIA RTX Virtual Workstation instances on Google Cloud. Built on NVIDIA RTX GPUs, NVIDIA CloudXR enables streaming of immersive AR, VR, or mixed reality experiences from anywhere. Organizations can easily set up and scale immersive experiences from any location, to any VR or AR device.  By streaming from Google Cloud,…Powered by Discourse, best viewed with JavaScript enabled"
2650,nvidia-ax800-delivers-high-performance-5g-vran-and-ai-services-on-one-common-cloud-infrastructure,"Originally published at:			https://developer.nvidia.com/blog/nvidia-ax800-delivers-high-performance-5g-vran-and-ai-services-on-one-common-cloud-infrastructure/
The NVIDIA AX800 converged accelerator offers a new architectural approach to deploying 5G on commodity hardware on any cloud.Powered by Discourse, best viewed with JavaScript enabled"
2651,sharing-physically-based-materials-between-renderers-with-mdl,"Originally published at:			Sharing Physically Based Materials between Renderers with MDL | NVIDIA Technical Blog
At SIGGRAPH 2019, NVIDIA shared the latest on Material Definition Language (MDL), a technology developed to define physically based materials for rendering solutions.  In their talk, NVIDIA’s Lutz Kettner and Jan Jordan present how MDL can be used to match the appearance of a single material within different rendering techniques.  “With RTX technology, materials that…Powered by Discourse, best viewed with JavaScript enabled"
2652,video-series-real-time-ray-tracing-for-interactive-global-illumination-workflows-in-frostbite,"Originally published at:			Video Series: Real-Time Ray Tracing for Interactive Global Illumination Workflows in Frostbite | NVIDIA Technical Blog
Real-time ray tracing is upon us. Electronic Arts leads the charge in integrating the technology on an engine level. This video series features Sebastien Hillaire, Senior Rendering Engineer at EA/Frostbite. Sebastian discusses real-time raytracing and global illumination (GI) workflows in Frostbite. He explains the context and the current workflows Frostbite uses, then describes the process…Powered by Discourse, best viewed with JavaScript enabled"
2653,jetson-project-of-the-month-using-pretrained-models-to-predict-bus-arrival-times,"Originally published at:			Jetson Project of the Month: Using Pretrained Models to Predict Bus Arrival Times | NVIDIA Technical Blog
Using pretrained models with an NVIDIA Jetson Nano and Vertex AI, a developer created a machine learning solution to accurately predict bus arrival times.Powered by Discourse, best viewed with JavaScript enabled"
2654,gtc-2020-accelerating-ai-workflows-with-ngc,"GTC 2020 S22421
Presenters: Adel El Hallak,NVIDIA; Philip Rogers, NVIDIA
Abstract
AI has moved beyond research into mission-critical production. AI is now solving real-world problems for organizations around the globe, who are looking to move faster and do more with their data. NVIDIA provides a range of SDKs that simplify training, inference, and deployment of AI for industries including health care, smart cities, robotics, and telecommunications. We’ll cover how NGC, through containers, pre-trained models, helm charts, and SDKs, allows data scientists and developers to build AI solutions faster, DevOps to streamline the development-to-production process, and IT teams to quickly provide compute platforms that the users need. We’ll demo how you can take advantage of an SDK to easily build and deploy your AI solution on-premises, at the edge, or in the cloud.Watch this session
Join in the conversation below.Powered by Discourse, best viewed with JavaScript enabled"
2655,improving-robot-motion-generation-with-motion-policy-networks,"Originally published at:			https://developer.nvidia.com/blog/improving-robot-motion-generation-with-motion-policy-networks/
Collision-free motion generation in unknown environments is a core building block for robotic applications. Generating such motions is challenging. The motion generator must be fast enough for real-time performance and reliable enough for practical deployment.  Many methods addressing these challenges have been proposed, ranging from using local controllers to global planners. However, these traditional motion…Powered by Discourse, best viewed with JavaScript enabled"
2656,share-your-science-using-virtual-reality-to-optimize-user-experience,"Originally published at:			https://developer.nvidia.com/blog/share-your-science-using-virtual-reality-to-optimize-user-experience/
EASE VR Co-Founders Prithvi Kandanda, CEO and Fred Spencer, CTO share how they are using NVIDIA GPUs and real-time analytics to understand a user’s behavior in virtual environments. Use cases and applications vary across industries and including retail, sports, social, manufacturing, and entertainment. Share your GPU-accelerated science with us at http://nvda.ly/Vpjxr and with the world…Powered by Discourse, best viewed with JavaScript enabled"
2657,real-time-path-tracing-and-denoising-in-quake-ii-rtx,"Originally published at:			Real Time Path-Tracing and Denoising in Quake II RTX | NVIDIA Technical Blog
Q2VKPT – short for “Quake II Vulkan Path Tracing” – was a research project led by Christoph Schied, an engineer eager to bend the possibility space of computer graphics. Using id’s 22-year-old game as a foundation, Christoph was able to make real-time path tracing a reality on consumer hardware. His project caught the attention of…Powered by Discourse, best viewed with JavaScript enabled"
2658,separate-compilation-and-linking-of-cuda-c-device-code,"Originally published at:			https://developer.nvidia.com/blog/separate-compilation-linking-cuda-device-code/
Managing complexity in large programs requires breaking them down into components that are responsible for small, well-defined portions of the overall program. Separate compilation is an integral part of the C and C++ programming languages which allows portions of a program to be compiled into separate objects and then linked together to form an executable or…Above ""main.cpp"" is missing some lines.You're right!  Thanks for catching that.  I added in the missing lines.Hi TonyI am trying to run cuda codes on Amazon AWS. I am using the Kmeans cuda implementation by Serban Giuroiu where different functions are written in different files.I just need to alter few parameters and run the code with my data. Now, the default object file is running fine. But when I am trying to compile .cu files after changing the parameters. I am getting errors. I can give more details about the errors. nvcc -c command is creating objects but these objects are not running and giving errors.I have a file cuda_main.cu from which it is calling a function cuda_kmeans() which is written in cuda_kmeans.cu file, where I need to change a few values. Any suggestion how should I compile them using command line ? I'll appreciate any suggestion.I assume you're talking about https://github.com/serban/k.... I checked this out and as it is in the repo, the makefile is not setup to use separate compilation units.  Make sure to add the -dc option when building the objects . If you're linking with nvcc that should cover it. My suspicion is that you are calling one of the __device__ routines in cuda_kmeans.cu from a kernel in cuda_main.cu. If this is not the problem comment back and I'll help you dig deeper. Thanks!-Tony ScudieroNVIDIAThanks Tony for your reply. After changing the code if I 'make' it again, it seems to be working fine. Thanks agian.How do you set this build up with Nsight Eclipse edition?  I can build with your makefile but not from Eclipse.  I posted all the details on SO: http://stackoverflow.com/qu...Great. Nice reference for novice of CUDA programming.Just for completeness, in the section 'Advanced Usage: Using a Different Linker', the interested reader should perform the following commands:# object filesnvcc -x cu -arch=sm_20 -I. -dc main.cpp -o main.onvcc -x cu -arch=sm_20 -I. -dc particle.cpp -o particle.onvcc -x cu -arch=sm_20 -I. -dc v3.cpp -o v3.o# gpu object filenvcc -arch=sm_20 -dlink v3.o particle.o main.o -o gpuCode.o# final linkg++ -L/usr/local/cuda-7.5/lib64 gpuCode.o main.o particle.o v3.o -lcudart -o appThe last command need the -L option to define the location of libcudart.so in order to avoid problems.I try to run the code in MVS 2013 and it turns out to have unresolved extern function for the advance in the particle, why is that? Thank you.That error indicates that you haven't told the CUDA compiler to generate relocatable device code - i.e. linkable device code. In your project property pages, go to CUDA C/C++ ->Common and look for the field ""Generate Relocatable Device Code"" and use the pull-down menu to set this to ""Yes (-rdc=true)."" You should then be able to build and run.It wasn't evident to me at first, but after reading it again, I wanted to note, for the NVIDIA CUDA developer community, that you can use EITHER __host__ or __device__ decorations (prefixes) before your class method.  It was useful in my code that I was working on to use a class from __global__ and I successfully compiled that when the class method had a __device__ decoration (prefix).  Also, what I found is that if I wanted, on the device, for the class to ""instantiate"" or ""contain"" some arrays, objects, ints (integers) etc., then that ""instantiation"" method (function) in the class needs to be decorated with __device__ as well.  I noted this on my README.md to get the good word out: https://github.com/ernestya...Thanks Murphy and Scudiero for the article!  And I find Harris' articles and github repo to be super useful as well.You can also combine `__host__` and `__device__` on a single method.I obtain this warning when I combine __host__ and __device__  when I run my make file (makefile):physlib/dev_R3grid.cu(8): warning: a __device__ function(""dev_Grid3d::dev_Grid3d"") redeclared with __host__ __device__, hence treated as a __host__ __device__ functionphyslib/dev_R3grid.cu(12): warning: a __device__ function(""dev_Grid3d::flatten"") redeclared with __host__ __device__, hence treated as a __host__ __device__ functionMy program is in this subdirectory, as I implemented finite volume upwind method for convection in 3-dimensions on CUDA C/C++: https://github.com/ernestya...and the C++ class that I wanted to run on the device is here:https://github.com/ernestya...In general for good programming practice, when obtaining compiler WARNINGS, is it always best to make changes until they go away?  They weren't errors and my executable did what I wanted.* tangent side note 1 - Dr. Harris, as I was interested (passionate) about implementing on the GPU combustion CFD, I was going through, from the graduate school level up, the ""standard"" (""canonical"") computational methods for aerospace engineering (finite difference, finite volume, etc.), and I found most solvers implemented in 2-dims. - why not in 3-dim. since we obviously live in a 3-dim. world?*tangent side note 2 - @Mark_Harris:disqus I tried increasing from 64 to 92 and above in your blog post about finite difference methods (3-dim.) for the grid size dimension and obtain Segmentation Faults.  Why doesn't it scale?  I don't think it's a RAM (memory) problem. cf. https://devblogs.nvidia.com...Yes, it's important to fix warnings. In this case you need to make the declaration (in .h) and definition (in .cu) of these methods match (__host__ __device__ on both).Tangent 1: Probably because it's simpler to explain and diagram 2D implementations.Tangent 2: I will look at the error. Please don't  cross-post.Thanks!It's my fault : __host__ __device__ works for a single method and you were right @Mark_Harris:disqus , __host__ __device__ has to be BOTH in the declaration and definition.Thanks for all your help!Hi,I have a question about the program. I have done a change on the number of steps in the main function from 100 to 500000 and the program crashed.Why the program crash? Is there a kernel launch limit?I have a Tesla k20m card.Thank you.Hi Albert, there shouldn't be any limit that would cause it to crash under that change. I tried it myself on my laptop and it seems to run fine -- I could only wait for about 50,000 iterations but I see nothing in the code that could cause it to crash after more, other than random bit errors corrupting memory. It took several minutes to run 50,000 iterations on my laptop GPU (a few years old).Hi Mark, thanks for reply. I tried the program in other workstation with different SO and different GPU and seems that the problem is on my Tesla or SO. I'm investigating about this. Thank you!Hi Mark,I found the problem, it's a temperature problem... Do know if it is possible buy the heat-sink for the Tesla K20m?Tank you.Powered by Discourse, best viewed with JavaScript enabled"
2659,customize-cuda-fortran-profiling-with-nvtx,"Originally published at:			https://developer.nvidia.com/blog/customize-cuda-fortran-profiling-nvtx/
The NVIDIA Tools Extension (NVTX) library lets developers annotate custom events and ranges within the profiling timelines generated using tools such as the NVIDIA Visual Profiler (NVVP) and NSight. In my own optimization work, I rely heavily on NVTX to better understand internal as well as customer codes and to spot opportunities for better interaction…Great post, it is great to have access to these features in Fortran.I modified your module to comply with the Fortran standard requirements that arguments to c_loc have attribute TARGET or POINTER. This allows the module to work for the XLF compiler as well as PGI.Here is a gist to the modified module: https://gist.github.com/dap...Thanks for sharing your modifications, David!Sorry to revive an old thread.It seems that David’s modification caused PGI/NVHPC to generate labels that are repeated characters of the first character in the string. For instance, when I pass “Total integral”, the NVTX marker becomes “TTTTTTTTT …”, that is the letter “T” repeated 256 times.To fix this, I have changed that line fromtoI guess this is because in Fortran an array of 256 characters isn’t the same as a string of length 256, unlike in C.Try to use this version:How to call NVTX from Fortran. Contribute to maxcuda/NVTX_example development by creating an account on GitHub.It will work with PGI,XLF and gfortranPowered by Discourse, best viewed with JavaScript enabled"
2660,gtc-2020-real-time-gpu-accelerated-data-analytics,"GTC 2020 D2S19
Presenters: Tech Demo Team,NVIDIA
Abstract
Real-Time GPU-Accelerated Data Analytics - Analyze, Visualize and Turn Data Into Insights with AIWatch this session
Join in the conversation below.Powered by Discourse, best viewed with JavaScript enabled"
2661,creating-faster-molecular-dynamics-simulations-with-gromacs-2020,"Originally published at:			https://developer.nvidia.com/blog/creating-faster-molecular-dynamics-simulations-with-gromacs-2020/
GROMACS logo GROMACS—one of the most widely used HPC applications— has received a major upgrade with the release of GROMACS 2020. The new version includes exciting new performance improvements resulting from a long-term collaboration between NVIDIA and the core GROMACS developers. As a simulation package for biomolecular systems, GROMACS evolves particles using the Newtonian equations…Dear all,I have tried to apply all the suggestions of this super interesting article but I got some problems.
I perform simulations with Gromacs 2020.2 on an HPC which has 4xV100 for each node (Driver Version: 450.51.06; CUDA Version: 11.0).as reported in the article, before starting the run I used the commands:export GMX_GPU_DD_COMMS=true
export GMX_GPU_PME_PP_COMMS=true
export GMX_FORCE_UPDATE_DEFAULT_GPU=trueand in the mdrun I specified:-nb gpu -bonded gpu -pme gpu -npme 1Moreover, the tpr file has been built specifying in the .mdp “constraints = h-bonds”.When the simulation starts I have these messages:Update task on the GPU was required, by the GMX_FORCE_UPDATE_DEFAULT_GPU environment variable, but the following condition(s) were not satisfied:
Domain decomposition without GPU halo exchange is not supported.
With separate PME rank(s), PME must use direct communication.
Will use CPU version of update.Any idea to solve the problem?Thank you so muchBest regards,FedericoHi Federico,I can see that the “GPU Update” feature is not being activated because the “GPU communication” features are not active, even though you are correctly setting the environment variables. My best guess therefore is that you are using an external MPI library rather than the GROMACS-internal thread MPI library - GPU communications are only supported in this release for the latter (where we now have support for the former merged into the master branch, to be included in GROMACS 2022). If so, please can you re-build with thread MPI and try again. If that’s not the case, please can you provide the full “md.log” file and I’ll take a look.AlanDear Alan,thank you so much for your reply! Your guess is right, the error appears when I use the Spectrum MPI library for multi-node/multi-replica simulations. I re-build a thread MPI version of GROMACS for a single-node run, and in that case everything works flawless.Thank youbest regards,FedericoHi Alan,Can you please give some pointers on the 3 systems you use (like how to get them/download them)? Basically we want to reproduce the same result in our system.Thanks!Maybe give the PDB ID or something like that?Hi,
Please can you provide your email address in this temporary form and I’ll  get in touch to help you get set up with this
GROMACS query - Google FormsThanks,AlanI followed the same steps but it throws the following error. Even though I increased the equilibration time but did not work.
Step 100: The total potential energy is nan, which is not finite. The LJ and
electrostatic contributions to the energy are 0 and -1.18497e+07,
respectively. A non-finite potential energy can be caused by overlapping
interactions in bonded interactions or very large or Nan coordinate values.
Usually this is caused by a badly- or non-equilibrated initial configuration,
incorrect interactions or parameters in the topology.Hi,Please can you re-try with the latest GROMACS version (2022). If you still get the error, please can you isolate which of the options described in the blog is triggering it, and then create an issue at GROMACS / GROMACS · GitLab (including your input files and command line options), and the dev team will take a look.Best regards,Alanhi, do you have similar benchmarks for Gromacs 2022 for comparison with 2020? thanksHi,Please see slide 4 of my presentation available at Presentation – PASC Program for some comparisons across versions 2019, 2020, 2021 and 2022, each running on the same A100 hardware. Please note that for the multi-GPU results, we tune for the optimal number of MPI tasks (in particular, running 2 MPI tasks per GPU can be often faster than a single task).Best regards,Alanthank you!Hi,I tried gromacs 2021.6 built using gcc-10.3.0 and cuda-11.1.
The system consisted of 81,743 atoms (slightly smaller than ADHD).
I exploited AMD EPYC™ 7763 (64 cores) and 4 x NVIDIA A100-SXM4.With simple:
$gmx -nt 64 -pin on -v -deffnm mdtest
performance: 93ns/dayWith your setup/parameters, using 64 physical cores:
$gmx -v -pin on -ntmpi 4 -ntomp 16 -nb gpu -bonded gpu -pme gpu -npme 1 -nstlist 400 -deffnm mdtest 
-nsteps 100000 -resetstep 90000 -noconfout
perfomance: 341ns/dayBased on your benchmark I expect no less than 450ns/day.Would you be so kind to commentThanks for your help and suggestions,
TamasHi,GROMACS performance is not simply influenced by the system size, but also quite strongly by specific details of the scientific system such and geometry and atom arrangement. So it’s possible that you are already getting good performance for your specific system. Here are some things to try/check:Mapping of GPU IDs to the 4 GPU tasks in the 4 ranks on this node:
PP:0,PP:1,PP:2,PME:3
PP tasks will do (non-perturbed) short-ranged and most bonded interactions on the GPU
PP task will update and constrain coordinates on the GPU
PME tasks will do all aspects on the GPU
GPU direct communication will be used between MPI ranks.Best regards,AlanDear Alan,Thanks for the suggestions.I have not thought about the effects of simulation system properties on performance.
Are not your tpr files available for benchmarking? I think that this would be very helpful not only for me but also for the community.All other issues (e.g. setting the environment variables, output of GPU task assignments) seem to be OK.
NVIDIA Nsight Systems profiler tool: I might try it. However, its application seems to be complicated for me, especially in an HPC environment.Best regards,
TamasHi,The STMV and ADHD  input files are available for download as the Supplementary Information archive for the paper Heterogeneous parallelization and acceleration of molecular dynamics simulations in GROMACSBest Regards,AlanPowered by Discourse, best viewed with JavaScript enabled"
2662,explainer-what-is-a-digital-twin,"Originally published at:			What Is a Digital Twin? | NVIDIA Blog
A digital twin is a virtual representation synchronized with physical things, people or processes.Powered by Discourse, best viewed with JavaScript enabled"
2663,startup-uses-ai-to-develop-ar-system-that-understands-human-movement,"Originally published at:			Startup Uses AI to Develop AR system that Understands Human Movement | NVIDIA Technical Blog
Startup Octi recently raised $7.5 million to create a deep learning-based augmented reality system that understands human movement. The company says their technology allows their users to create social content and communicate through videos in a fun and unique way. “We are a technology-focused company that has created a proprietary method for re-creating 3D space…Powered by Discourse, best viewed with JavaScript enabled"
2664,just-released-nvidia-hpc-sdk-v23-7,"Originally published at:			NVIDIA HPC SDK 23.7 Downloads | NVIDIA Developer
NVIDIA HPC SDK version 23.7 is now available and provides minor updates and enhancements.Powered by Discourse, best viewed with JavaScript enabled"
2665,gtc-data-science-presentations,"Originally published at:			GTC Data Science Presentations | NVIDIA Technical Blog
Starting on October 5-9, This fall’s GTC will run continuously for five days, across seven time zones. The conference will showcase the latest breakthroughs in data science, and many other GPU technology interest areas.  Attend live events in the time zone that works best for you, or browse an extensive catalog of on-demand content showcasing…Powered by Discourse, best viewed with JavaScript enabled"
2666,accelerating-data-center-and-hpc-performance-analysis-with-nvidia-nsight-systems,"Originally published at:			https://developer.nvidia.com/blog/accelerating-data-center-and-hpc-performance-analysis-with-nvidia-nsight-systems/
NVIDIA Nsight Systems 2023.2 previews profiling for multinode systems alongside support for profiling Python, networking hardware metrics, and a new analysis framework.Powered by Discourse, best viewed with JavaScript enabled"
2667,just-released-nvidia-drive-os-6-0-5-now-available,"Originally published at:			https://developer.nvidia.com/drive/downloads#?tx=$product,drive_orin
The latest NVIDIA DRIVE OS release includes customization and safety updates for supercharging autonomous vehicle development.Powered by Discourse, best viewed with JavaScript enabled"
2668,optimizing-memory-with-nvidia-nsight-systems,"Originally published at:			https://developer.nvidia.com/blog/optimizing-memory-with-nvidia-nsight-systems/
NVIDIA Nsight Systems is a comprehensive tool for tracking application performance across CPU and GPU resources. It helps ensure that hardware is being efficiently used, traces API calls, and gives insight into inter-node network communication by describing how low-level metrics sum to application performance and finding where it can be improved. Nsight Systems can scale…Thanks for reading! Questions? Comments? Need help getting started? Let us know!Powered by Discourse, best viewed with JavaScript enabled"
2669,metropolis-spotlight-inex-is-revolutionizing-toll-road-systems-with-real-time-video-processing,"Originally published at:			https://developer.nvidia.com/blog/metropolis-spotlight-inex-is-revolutionizing-toll-road-systems-with-real-time-video-processing/
INEX Technologies, an NVIDIA Metropolis partner, designs, develops, and manufactures comprehensive hardware and software solutions for license plate recognition and vehicle identification.Powered by Discourse, best viewed with JavaScript enabled"
2670,best-in-class-quantum-circuit-simulation-at-scale-with-nvidia-cuquantum-appliance,"Originally published at:			https://developer.nvidia.com/blog/best-in-class-quantum-circuit-simulation-at-scale-with-nvidia-cuquantum-appliance/
Performance evaluations highlight ultra-fast, full state vector quantum circuit simulations at scale using the NVIDIA cuQuantum Appliance on AIST’s ABCI 2.0 Supercomputer.Powered by Discourse, best viewed with JavaScript enabled"
2671,do-i-need-to-update-my-data-center-network,"Originally published at:			https://developer.nvidia.com/blog/do-i-need-to-update-my-data-center-network/
Is your network getting long in the tooth and are you thinking about an upgrade? This blog will cover three areas to consider when updating your data center network.Powered by Discourse, best viewed with JavaScript enabled"
2672,getting-started-with-real-time-ray-tracing-and-dlss-in-unreal-engine-4,"Originally published at:			Getting Started with Real-Time Ray Tracing and DLSS in Unreal Engine 4 | NVIDIA Technical Blog
The RTX Developer User Guide is a living knowledge document that details all the latest improvements and optimizations to ray tracing in Unreal Engine 4 (UE4). Figure 1. Reflection, refraction and translucency in a ray traced setting. Note the soft shadows and ambient skylighting ALT beveled cubes reflecting light and being translucent with a sun…Powered by Discourse, best viewed with JavaScript enabled"
2673,delivering-up-to-9x-the-throughput-with-namd-v3-and-nvidia-a100-gpu,"Originally published at:			Delivering up to 9X the Throughput with NAMD v3 and NVIDIA A100 GPU | NVIDIA Technical Blog
NAMD, a widely used parallel molecular dynamics simulation engine, was one of the first CUDA-accelerated applications.  Throughout the early evolution of CUDA support in NAMD, NVIDIA GPUs and CUDA grew tremendously in both performance and capabilities. For more information, see Accelerating Molecular Modeling Applications with Graphics Processors and Adapting a Message-driven Parallel Application to GPU-Accelerated Clusters.…Powered by Discourse, best viewed with JavaScript enabled"
2674,webinar-gpu-accelerated-analysis-of-dna-sequencing-data,"Originally published at:			Webinar: GPU-Accelerated Analysis of DNA Sequencing Data | NVIDIA Technical Blog
Personalizing treatments based on patients’ genetic variation will revolutionize how medicine cures diseases; but time to analysis has become a major bottleneck. Join Mehrzad Samadi, CEO of Parabricks on Thursday, March 1, 2018, from 11am-12pm PST as he discusses the use of GPU-acceleration to speed the analysis of DNA sequencing data. Parabricks has accelerated secondary…Powered by Discourse, best viewed with JavaScript enabled"
2675,evaluating-the-security-of-jupyter-environments,"Originally published at:			https://developer.nvidia.com/blog/evaluating-the-security-of-jupyter-environments/
The NVIDIA AI Red Team has developed a JupyterLab extension called jupysec to automatically assess the security of Jupyter environments.Powered by Discourse, best viewed with JavaScript enabled"
2676,accelerating-apache-spark-3-0-with-gpus-and-rapids,"Originally published at:			Accelerating Apache Spark 3.0 with GPUs and RAPIDS | NVIDIA Technical Blog
Given the parallel nature of many data processing tasks, it’s only natural that the massively parallel architecture of a GPU should be able to parallelize and accelerate Apache Spark data processing queries, in the same way that a GPU accelerates deep learning (DL) in artificial intelligence (AI). NVIDIA has worked with the Apache Spark community…Powered by Discourse, best viewed with JavaScript enabled"
2677,gtc-digital-demo-blender-cycles-rtx-on-highlights-new-ai-features,"Originally published at:			GTC Digital Demo: “Blender Cycles: RTX On” Highlights New AI Features | NVIDIA Technical Blog
Blender Cycles, an open-source production renderer, is now super-charged with the latest generation of NVIDIA RTX GPUs.Powered by Discourse, best viewed with JavaScript enabled"
2678,using-machine-learning-to-optimize-warehouse-operations,"Originally published at:			Using Machine Learning to Optimize Warehouse Operations | NVIDIA Technical Blog
With thousands of orders placed every hour and each order assigned to a pick list, Europe’s leading online fashion retailer Zalando is using GPU-accelerated deep learning to identify the shortest route possible to products in their 1.3 million-square-foot distribution center. Two schematics of a rope ladder warehouse zone with picks. The blue shelves denote shelves…Powered by Discourse, best viewed with JavaScript enabled"
2679,gtc-2020-nvidia-marbles-rtx,"GTC 2020 D2S38
Presenters: Tech Demo Team,NVIDIA
Abstract
During the GTC 2020 virtual keynote, NVIDIA CEO and founder Jensen Huang showcased a piece created by NVIDIA to illustrate the power of #RTX on the Omniverse Platform. Marbles was created by a distributed team of artists and engineers in Omniverse, assembling VFX+ quality assets into a fully physically simulated game level. There was no sacrifice to quality and fidelity that typically results from “gamifying” art assets to run in real-time.  Marbles runs on a single Quadro RTX 8000 simulating complex physics in a real-time ray traced world.Learn more about the NVIDIA Omniverse Platform: https://developer.nvidia.com/nvidia-omniverse-platform
Watch the entire GTC 2020 keynote: Keynote by NVIDIA CEO Jensen Huang | GTC 2022 | NVIDIAWatch this session
Join in the conversation below.Will you release this demo publicly? Lots of people wanna play with it.Same. Where to download it?Why is the shadow of the marble not refracting light?Still not released.I want to download it so much… But I dont know if my 2080Ti will be enough…I’ve downloaded the demo from the Omniverse,  but I get this error:|---------------------------------------------------------------------------------------------|
| Driver Version: 466.27      | Graphics API: D3D12
|=============================================================================================|
| GPU | Name                             | Active | LDA | GPU Memory | Vendor-ID | LUID       |
|     |                                  |        |     |            | Device-ID | UUID       |
|---------------------------------------------------------------------------------------------|
| 0   | NVIDIA GeForce RTX 2060 SUPER    |        |     | 8031    MB | 10de      | d6070000… |
|     |                                  |        |     |            | 1f06      | 0          |
|=============================================================================================|
| OS: Windows, Version: 10.0 (20H2), Build: 19042
| Processor: Intel(R) Core™ i9-9900KF CPU @ 3.60GHz | Cores: 8 | Logical: 16
|---------------------------------------------------------------------------------------------|
| Total Memory (MB): 65465 | Free Memory: 54827
| Total Page/Swap (MB): 75193 | Free Page/Swap: 59952
|---------------------------------------------------------------------------------------------|
2021-05-07 00:12:39 [712ms] [Error] [gpu.foundation.plugin] No device could be created.Oh looking at the log file I see why:C:/Users/me/.nvidia-omniverse/logs/Kit/omni.app.marbles/2021.1/kit_20210506_171239.log2021-05-07 00:21:50 [708ms] [Warning] [gpu.foundation.plugin] Skipping GPU NVIDIA GeForce RTX 2060 SUPER with 8031 MB of GPU memory. Requested min 11000 of GPU memoryLooks like the docs are wrong as it says marbles demo will work with any RTX card but it seems to have a min memory requirement of 11GB.The minimum video memory requirement can be changed in:C:\Users\me\AppData\Local\ov\pkg\marbles_rtx-2021.1.5_build\windows-x86_64\release\apps\omni.app.marbles.kityou should find renderer.minGpuMemoryMB = 11000 on line 37Lowering the minimum should allow the application to run on something like a RTX 3080 or 3070, but I can’t guarantee that you would get at least reasonable performance, nor can I test it for you because I only have a RTX 3090.Hope this helps!Can some one paste link to download it? Or maybe private share?Yes updating that kit file worked! Thanks Dusty!  It runs dog slow on my RTX 2060 Super though.  That extra 3GB of memory must really help.  Konrad, you can download the marbles demo by installing NVIDIA Omniverse and installing the Marbles RTX app.i have RTX 3080 i5 9600 and 32 GB of Ram and the game does not exceed 12 FPS in 3440 x 1440 and 1080p max 20fps min 7fps.How can open this file? If I open it via doubleclick it crashesif windows: right click, ‘open with’ and select notepad. once edited, save, exit and try run marbles again. Worked for me after changing value to 10000 for my 3080 so thanks to the author. Beautiful tech; to think that quality will be mainstream in years to come, wowon a RTX2080 setting it to 720p it kind of runs but is no where near 60FPS wondering why there are no dynamic lights considering its a raytracing demo. Isn’t that the whole point??? Setting the ball on fire has the effect of adding a light but its not raytracing the fire. Is this just me? Any how Impressive demo maybe If I had a 3080 it would run as intended.Thanks Dusty, it worked for me, I have the Rtx 2080, I had to bring it down to 7000tried today but i loads  only to the instructions screen but pressing any button doesnt doo anyhthing it just stays ther in the instructions screen foreverOkay i finally figured it out. its working now.Marble Nvidia 12559×1516 441 KBMarble Nvidia 22560×1525 455 KBMarble Nvidia2560×1510 443 KBPowered by Discourse, best viewed with JavaScript enabled"
2680,gtc-2020-integer-quantization-for-dnn-inference-acceleration,"GTC 2020 S22075
Presenters: Patrick Judd,NVIDIA
Abstract
While neural networks are typically trained in floating-point formats, inference/serving can often use integer arithmetic after neural networks are quantized. Benefits of quantized inference include reduced memory requirements, as well as the use of faster math pipelines. For example, NVIDIA’s Tensor Cores provide int8, int4, and int1 math units, which have 4x, 8x, and 32x more math bandwidth than fp32. We’ll detail various options for quantizing a neural network for inference while maintaining model accuracy. We’ll review results for networks trained for various tasks (computer vision, language, speech) and varying architectures (CNNs, RNNs, Transformers).Watch this session
Join in the conversation below.Powered by Discourse, best viewed with JavaScript enabled"
2681,share-your-science-predicting-earthquakes-with-supercomputers,"Originally published at:			https://developer.nvidia.com/blog/share-your-science-predicting-earthquakes-with-supercomputers/
Tom Jordan, Director, Southern California Earthquake at University of Southern California and a team of researchers are using the 27 petaflop Titan Supercomputer at Oak Ridge National Lab with 18,688 Tesla GPUs to develop physics-based earthquake simulations. Their work is helping advance our understanding of earthquake systems, including potential seismic hazards from known faults and…Powered by Discourse, best viewed with JavaScript enabled"
2682,nvidia-and-red-hat-simplifying-nvidia-gpu-driver-deployment-on-red-hat-enterprise-linux,"Originally published at:			NVIDIA and Red Hat: Simplifying NVIDIA GPU Driver Deployment on Red Hat Enterprise Linux | NVIDIA Technical Blog
By Pramod Ramarao, Senior Product Manager, NVIDIA NVIDIA GPUs are transforming enterprises by accelerating enterprise computing from inference, data science to large scale AI training, to VDI. Red Hat and NVIDIA have been working together for over 10 years to accelerate Red Hat Enterprise Linux (RHEL) workloads on NVIDIA GPU enabled servers – across the…Powered by Discourse, best viewed with JavaScript enabled"
2683,nvidia-announces-aerial-a-software-defined-stack-for-telco-systems,"Originally published at:			https://developer.nvidia.com/blog/cubb-software-defined-stack-for-telco/
At Mobile World Congress Los Angeles, NVIDIA unveiled Aerial, a set of SDKs that enables GPU-accelerated, software defined 5G wireless radio access network (RAN). Today, NVIDIA Aerial provides two critical new SDKs: CUDA Virtual Network Functions (cuVNF) and CUDA Baseband (cuBB). These SDKs simplify building highly programmable and scalable software-defined 5G RANs using commercial off-the-shelf…Powered by Discourse, best viewed with JavaScript enabled"
2684,gtc-2020-advanced-optimizations-of-persistent-recurrent-neural-networks,"GTC 2020 S21691
Presenters: Vasily Volkov,NVIDIA; Jeremy Appleyard,NVIDIA
Abstract
Recurrent Neural Networks (RNNs) with small batch sizes tend to be bandwidth-bound when implemented naively. Persisting the majority of the inputs in low-level GPU memory can turn the problem back into a compute-bound one and see order-of-magnitude speedups. We’ll dive into our methods to achieve performance in cuDNN’s persistent RNN implementation, many of which are applicable to other persistent methods.Watch this session
Join in the conversation below.Powered by Discourse, best viewed with JavaScript enabled"
2685,combine-openacc-and-unified-memory-for-productivity-and-performance,"Originally published at:			https://developer.nvidia.com/blog/combine-openacc-unified-memory-productivity-performance/
The post Getting Started with OpenACC covered four steps to progressively accelerate your code with OpenACC. It’s often necessary to use OpenACC directives to express both loop parallelism and data locality in order to get good performance with accelerators. After expressing available parallelism, excessive data movement generated by the compiler can be a bottleneck, and correcting this by…Powered by Discourse, best viewed with JavaScript enabled"
2686,getting-started-with-pgi-compilers-on-aws,"Originally published at:			Getting Started with PGI Compilers on AWS | NVIDIA Technical Blog
PGI Community Edition compilers and tools for Linux/x86-64 provide a low-cost option for people interested in GPU-accelerated computing. These tools are now available as an Amazon Machine Image (AMI) on the AWS Marketplace, extending this low-cost paradigm for doing GPU-accelerated computing to using Amazon’s extensive cloud computing resources. You can create your own personal virtualized NVIDIA Volta V100…Powered by Discourse, best viewed with JavaScript enabled"
2687,ai-powered-flying-camera-to-replace-your-selfie-stick,"Originally published at:			AI-Powered Flying Camera to Replace Your Selfie Stick | NVIDIA Technical Blog
After raising $25 million in funding, Beijing-based ZeroZero Robotics came out of stealth mode and launched their Hover Camera just days before the GMIC Beijing 2016 trade show, the ‘CES of China.’ Co-founder MQ Wang, a Stanford PhD alum focused on machine learning and natural language processing, was inspired to create the autonomous Hover Camera…Powered by Discourse, best viewed with JavaScript enabled"
2688,nvidia-research-at-miccai-2021,"Originally published at:			https://developer.nvidia.com/blog/nvidia-research-at-miccai-2021/
NVIDIA focuses on Medical imaging research at MICCAI 2021 with multiple papers at the conference.Powered by Discourse, best viewed with JavaScript enabled"
2689,explainer-what-is-computer-vision,"Originally published at:			What Is Computer Vision? | NVIDIA Blog
Computer vision is achieved with convolutional neural networks that can use images and video to perform segmentation, classification and detection for many applications.Powered by Discourse, best viewed with JavaScript enabled"
2690,designing-an-optimal-ai-inference-pipeline-for-autonomous-driving,"Originally published at:			https://developer.nvidia.com/blog/designing-an-optimal-ai-inference-pipeline-for-autonomous-driving/
Electric vehicle manufacturer NIO optimized its AI inference pipeline with NVIDIA Triton on GPUs.Powered by Discourse, best viewed with JavaScript enabled"
2691,develop-the-future-of-virtual-worlds-with-omniverse-code-app,"Originally published at:			https://developer.nvidia.com/blog/develop-the-future-of-virtual-worlds-with-omniverse-code-app/
Dive into the Omniverse Code app—an integrated development environment for users to easily build their own Omniverse extensions, apps, or microservices.Powered by Discourse, best viewed with JavaScript enabled"
2692,upcoming-event-rethinking-zero-trust-an-ai-based-approach-to-cybersecurity,"Originally published at:			https://info.nvidia.com/rethinking-zero-trust-an-ai-based-approach-to-cybersecurity.html?nvid=nv-int-txtad-883906-vt43#cid=dl24_nv-int-txtad_en-us
Join us on May 26 to learn how you can leverage accelerated AI frameworks to build high performance zero-trust solutions with reduced friction and fewer lines of code.Powered by Discourse, best viewed with JavaScript enabled"
2693,from-rasterization-to-full-real-time-path-tracing-the-evolution-of-graphical-rendering-techniques,"Originally published at:			From Rasterization to Full Real-Time Path Tracing: The Evolution of Graphical Rendering Techniques | NVIDIA Technical Blog
Video game artists have become highly capable at pushing the limits of rasterization, but it remains a complicated, time-consuming process, ubiquitous only because it’s computationally cheap. There are some challenges that simply can’t be overcome through sophisticated work-arounds. “The problem here is that most lighting interactions are not local. Most interesting lighting interactions turn out…Powered by Discourse, best viewed with JavaScript enabled"
2694,nvidia-opening-core-ai-and-ml-research-lab-in-santa-clara,"Originally published at:			NVIDIA Opening Core AI and ML Research Lab in Santa Clara | NVIDIA Technical Blog
To help accelerate AI research and further discovery across deep learning sciences, the NVIDIA Research team is announcing the opening of a core AI/ML lab in Santa Clara, California. The new lab will be led by computer scientist Anima Anandkumar and will complement existing NVIDIA labs run by several world-renowned researchers.  “This lab will push…Powered by Discourse, best viewed with JavaScript enabled"
2695,nvidia-research-learning-modular-scene-representations-with-neural-scene-graphs,"Originally published at:			https://developer.nvidia.com/blog/nvidia-research-learning-modular-scene-representations-with-neural-scene-graphs/
NVIDIA researchers will present their paper “Neural Scene Graph Rendering” at SIGGRAPH 2021, August 9-13, which introduces a neural scene representation inspired by traditional graphics scene graphs.  Recent advances in neural rendering have pushed the boundaries of photorealistic rendering; take StyleGAN as an example of producing realistic images of fictional people. The next big challenge…Powered by Discourse, best viewed with JavaScript enabled"
2696,ai-remotely-detects-parkinson-s-disease-during-sleep,"Originally published at:			AI Remotely Detects Parkinson’s Disease During Sleep | NVIDIA Technical Blog
Doctors could soon evaluate Parkinson’s disease by having patients do one simple thing—sleep. A new study led by MIT researchers trains a neural network to analyze a person’s breathing patterns while sleeping and determine whether the subject has Parkinson’s. Recently published in Nature Medicine, the work could lead to earlier detection and treatment. “Our goal…Powered by Discourse, best viewed with JavaScript enabled"
2697,gtc-2020-xlnet-optimization-using-cuda,"GTC 2020 S21478
Presenters: Christina Zhang,NVIDIA
Abstract
XLNet, a generalized autoregressive pretraining method, achieved great results on several natural language processing tasks. Compared to the previous language model, XLNET has advantages like being able to process long sentences, and avoids the disadvantage of using special tokens. However, as far as we know, there still isn’t proper performance optimization for XLNet using CUDA, which would demand more inference time and hinder XLNET’s wide deployment. We first ran the performance analysis of XLNet using its Tensorflow code. Then we optimized XLNet with these aspects:Watch this session
Join in the conversation below.Powered by Discourse, best viewed with JavaScript enabled"
2698,deepwave-digital-develops-the-first-deep-learning-spectrum-sensor-for-a-5g-network,"Originally published at:			Deepwave Digital Develops the First Deep Learning Spectrum Sensor for a 5G Network | NVIDIA Technical Blog
The article below is a guest post by Deepwave Digital, a technology company working to incorporate AI into radio frequency and wireless technology. In the post below, Deepwave engineers describe how they developed the first deep learning-based sensor for a 5G network.  By John D. Ferguson, Peter Witkowski, William Kirschner, Daniel Bryant Deepwave Digital is…Powered by Discourse, best viewed with JavaScript enabled"
2699,open-reproducible-computational-chemistry-with-python-and-cuda,"Originally published at:			https://developer.nvidia.com/blog/open-reproducible-computational-chemistry-python-cuda/
Increasingly, computational chemistry researchers use GPUs to push the boundaries of discovery. This motivated Christopher Cooper, an Instructor at Universidad Técnica Federico Santa María in Chile, to move to a Python-based software stack. Cooper’s recent paper, “Probing protein orientation near charged nanosurfaces for simulation-assisted biosensor design,” was recently accepted in J. Chemical Physics. Brad: Can you talk a…Powered by Discourse, best viewed with JavaScript enabled"
2700,building-an-accelerated-data-science-ecosystem-rapids-hits-two-years,"Originally published at:			https://developer.nvidia.com/blog/building-an-accelerated-data-science-ecosystem-rapids-hits-two-years/
GTC Fall 2020 marked the second anniversary of the initial release of RAPIDS. Created out of the GPU Open Analytics Initiative (GoAi) aimed at making accelerated, end-to-end analytics on GPUs easy, RAPIDS has proven GPUs are performant, easy to use, and transformative to the future of data analytics. By thinking about the relationship between software…Powered by Discourse, best viewed with JavaScript enabled"
2701,distributed-deep-learning-made-easy-with-spark-3-4,"Originally published at:			https://developer.nvidia.com/blog/distributed-deep-learning-made-easy-with-spark-3-4/
With the release of Spark 3.4, users now have access to built-in APIs for both distributed model training and model inference at scale.Powered by Discourse, best viewed with JavaScript enabled"
2702,implementing-usd-for-game-development-pipelines-an-interview-with-polyphony-digital,"Originally published at:			https://developer.nvidia.com/blog/implementing-usd-for-game-development-pipelines-an-interview-with-polyphony-digital/
Polyphony Digital, a subsidiary of Sony Interactive Entertainment Inc and the creators of Gran Turismo, has exceeded 90M copies of cumulative sell-through sales of PlayStation software titles over three decades. Gran Turisimo 7, released in 2022, marked the 25th anniversary of the series’ beginning, and included implementation of Universal Scene Description (USD). USD is an…Powered by Discourse, best viewed with JavaScript enabled"
2703,accelerating-product-development-with-physics-informed-neural-networks-and-nvidia-simnet,"Originally published at:			Accelerating Product Development with Physics-Informed Neural Networks and NVIDIA Modulus | NVIDIA Technical Blog
INNs are the latest digital twin application of AI for design and manufacturing. In this blog, we’ll explore how the interactivity of a trained model using SimNet can greatly impact the product development process, covering details that include work processes, results validation, and exciting innovations for the future.Hi my name is Garrett Wyatt, my background is in design engineering.I’m very fascinated by the approach the Kinetic Vision team is taking to improve design simulation. I noticed in the article that Solidworks was used to build the parametric CAD. Would the team by willing to benchmark NX against Solidworks on the same simulation activity? I believe there would be significant advantages with NX. I would love to partner with the team and help set up this proof of concept up. Please contact me at garrett.wyatt@siemens.com or call me at 480-710-9479.Thanks Garrett for reaching out and great to see that you are interested in partnering. I will reach out to you to discuss this further.Powered by Discourse, best viewed with JavaScript enabled"
2704,improving-network-performance-of-hpc-systems-using-nvidia-magnum-io-nvshmem-and-gpudirect-async,"Originally published at:			https://developer.nvidia.com/blog/improving-network-performance-of-hpc-systems-using-nvidia-magnum-io-nvshmem-and-gpudirect-async/
Today’s leading-edge high performance computing (HPC) systems contain tens of thousands of GPUs. In NVIDIA systems, GPUs are connected on nodes through the NVLink scale-up interconnect, and across nodes through a scale-out network like InfiniBand. The software libraries that GPUs use to communicate, share work, and efficiently operate in parallel are collectively called NVIDIA Magnum…Thanks Jim, Seth, Pak, Sreeram for this thoughtful piece addressing situations when applications use smaller message sizes as the workload scales to larger numbers of GPUs, its nice to see MagnumIO (GPUDIrect Async, NVSHMEM ) helps NICs to sustain high throughput on NVIDIA InfiniBand networks. Hint: GPU initiated communications bypassing the GPU bottleneck.Thank you for the interesting article.
Do you intend to publish the lower-level RDMA api for GPU (i.e., InfiniBand GPUDirect Async) for users that wish to use RDMA without the shared-memory abstraction?
All I found is the libgdsync (GitHub - gpudirect/libgdsync: GPUDirect Async support for IB Verbs) which doesn’t seem to have had any development for the last years.Best,
LassePowered by Discourse, best viewed with JavaScript enabled"
2705,gtc-2020-cuda-developer-tools-overview-and-exciting-new-features,"GTC 2020 S22043
Presenters: Rafael Campana,NVIDIA
Abstract
We’ll provide an overview of the wide range of developer tool products and technologies across platforms, as well as exciting new features in these products provided by the NVIDIA Developer Tools team. Learn about integrations into other tools, building, debugging, and profiling products, and other new features.Watch this session
Join in the conversation below.Powered by Discourse, best viewed with JavaScript enabled"
2706,power-your-ai-inference-with-new-nvidia-triton-and-nvidia-tensorrt-features,"Originally published at:			https://developer.nvidia.com/blog/power-your-ai-inference-with-new-nvidia-triton-and-nvidia-tensorrt-features/
NVIDIA Triton now offers native Python support with PyTriton, model analyzer support for model ensembles, and more.Powered by Discourse, best viewed with JavaScript enabled"
2707,accelerating-k-nearest-neighbors-600x-using-rapids-cuml,"Originally published at:			Accelerating k-nearest Neighbors 600x Using RAPIDS cuML | NVIDIA Technical Blog
This post was originally published on the RAPIDS AI Blog. k-Nearest Neighbors classification is a straightforward machine learning technique that predicts an unknown observation by using the k most similar known observations in the training dataset. In the second row of the example pictured above, we find the seven digits 3, 3, 3, 3, 3,…Powered by Discourse, best viewed with JavaScript enabled"
2708,boosting-ultra-rapid-nanopore-sequencing-analysis-on-nvidia-dgx-a100,"Originally published at:			https://developer.nvidia.com/blog/boosting-ultra-rapid-nanopore-sequencing-analysis-on-nvidia-dgx-a100/
Ultra rapid nanopore sequencing is bringing us one step closer to same-day whole genome genetic diagnosis.Powered by Discourse, best viewed with JavaScript enabled"
2709,get-ready-for-the-future-boost-your-skills-with-hands-on-training-at-gtc,"Originally published at:			Get Ready for the Future - Boost Your Skills with Hands-on Training at GTC | NVIDIA Technical Blog
The NVIDIA Deep Learning Institute is offering nine instructor-led workshops at this year’s GTC from April 12-16 on a wide range of advanced software development topics in AI, accelerated computing, and data science. Developers, data scientists, researchers, and students can get practical experience powered by GPUs in the cloud. Upon completion, participants earn an NVIDIA…Powered by Discourse, best viewed with JavaScript enabled"
2710,scientists-capture-first-image-of-a-black-hole,"Originally published at:			https://developer.nvidia.com/blog/scientists-capture-first-image-of-a-black-hole/
Astronomers from around the world pointed their powerful telescopes towards a supermassive black hole that lies in the center of the Milky Way (nearly 26,000 light years from Earth) and believe they have snapped the first-ever picture of a black hole. It will take months to develop the image, but if the scientists succeed the…Powered by Discourse, best viewed with JavaScript enabled"
2711,turing-variable-rate-shading-in-vrworks,"Originally published at:			Turing Variable Rate Shading in VRWorks | NVIDIA Technical Blog
NVIDIA Turing GPUs enable a new, easily implemented rendering technique, Variable Rate Shading (VRS). VRS increases rendering performance and quality by applying varying amount of processing power to different areas of the image. VRS works by changing the number of pixels that can be processed by a single pixel shader operation. Single pixel shading operations can…Regarding performance gains, will DLSS work in VR?since  september 2015 the sony z5 premium gave you 8 mpixel resolution in VR (cardboard like)Very cool... I hope that eye tracking vendors will apply it immediately to VR!I hope flight simulators will apply these kind of features as soon as possible. We really need those 90fps to enjoy flying.Powered by Discourse, best viewed with JavaScript enabled"
2712,gtc-2020-pytorch-tensorrt-accelerating-inference-in-pytorch-with-tensorrt,"GTC 2020 S21671
Presenters: Josh Park,NVIDIA; Naren Dasan,NVIDIA
Abstract
TensorRT is a deep-learning inference optimizer and runtime to optimize networks for GPUs and the NVIDIA Deep Learning Accelerator (DLA). Typically, the procedure to optimize models with TensorRT is to first convert a trained model to an intermediary format, such as ONNX, and then parse the file with a TensorRT parser. This works well for networks using common architectures and common operators; however, with the rapid pace of model development, sometimes a DL framework like Tensorflow has ops that are not supported in TensorRT. One solution is to implement plugins for these ops. Another is to use a tool like TF-TRT, which will convert supportable subgraphs to TensorRT and use Tensorflow implementations for the rest. We’ll demonstrate the same ability with PyTorch with our new tool PTH-TRT, as well leveraging the PyTorch API’s great composability features to allow users to reuse their TensorRT-compatible networks in larger, more complex ones.Watch this session
Join in the conversation below.Powered by Discourse, best viewed with JavaScript enabled"
2713,new-nvidia-path-tracing-breakthrough-interactively-renders-7-000-dynamic-lights,"Originally published at:			https://developer.nvidia.com/blog/turning-up-the-lights-interactive-path-tracing-scenes-from-a-short-film/
Learn how NVIDIA researchers stretched the capabilities of real-time path tracing to highly complex scenes with thousands of dynamic light sources.Quick question, was the denoiser used to denoise this scene the same or similar one published by Hasselgren, et. al. here?:Despite recent advances in Monte Carlo path tracing at interactive rates, denoised image sequences generated with few samples per-pixel often yield temporally unstable results and loss of high-frequency details. We present a novel adaptive rendering...Thanks!JasonHello Jason and welcome back to the NVIDIA Developer forums![…] (Text removed)That said, I will check if I can get confirmation on this internally.Thanks!
MarkusHi @jason.rodriguez !I stand corrected, those two works are indeed connected, you interpreted this correctly.In 2019 the authors were not quite ready to publish their research, so they refined what they demoed at SIGGRAPH 2019 and published their work later in the paper you cited.I hope we could help you!MarkusThank you for looking into that for me, that was exactly the information I was looking for!You are very welcome!Powered by Discourse, best viewed with JavaScript enabled"
2714,advanced-api-performance-mesh-shaders,"Originally published at:			https://developer.nvidia.com/blog/advanced-api-performance-mesh-shaders/
Mesh shaders are designed to overcome the bottlenecks of the fixed layout used by the classical geometry pipeline.Powered by Discourse, best viewed with JavaScript enabled"
2715,unleash-the-power-of-turing-with-nvidia-driver-418,"Originally published at:			Unleash the Power of Turing with NVIDIA Driver 418 | NVIDIA Technical Blog
NVIDIA SDKs provide an API to the hardware and libraries delivered with the NVIDIA drivers. These APIs allow developers to leverage new features and performance by building their applications on the NVIDIA platform. End users of these applications benefit from frequent updates and incremental performance boosts as drivers are optimized. Starting today, the NVIDIA Release…Powered by Discourse, best viewed with JavaScript enabled"
2716,nvidia-releases-new-clara-models-to-help-fight-covid-19,"Originally published at:			NVIDIA Releases New Clara Models to Help Fight COVID-19 | NVIDIA Technical Blog
NVIDIA today announced new AI models to help the medical community better track, test and treat COVID-19. Available today, AI models developed jointly with the National Institutes of Health (NIH) can help researchers study the severity of COVID-19 from chest CT scans and develop new tools to better understand, measure and detect infections. The models…Powered by Discourse, best viewed with JavaScript enabled"
2717,nvidia-supercharges-precision-timing-for-facebook-s-next-generation-time-keeping,"Originally published at:			https://developer.nvidia.com/blog/nvidia-supercharges-precision-timing-for-facebooks-next-generation-time-keeping/
Facebook is open-sourcing the Open Compute Project Time Appliance Project (OCP TAP), which provides very precise time keeping and time synchronization across data centers in a cost-effective manner.Powered by Discourse, best viewed with JavaScript enabled"
2718,automatically-remove-backgrounds-from-images,"Originally published at:			Automatically Remove Backgrounds From Images | NVIDIA Technical Blog
Researchers from Adobe, the Beckman Institute for Advanced Science and Technology and University of Illinois at Urbana-Champaign developed a deep learning-based method that clips objects from photos and videos. Researchers have developed a number of different artificially intelligent programs to automatically subtract a background from an image, but most are based on colors. When presented…Powered by Discourse, best viewed with JavaScript enabled"
2719,converting-ct-scans-into-2d-mris-with-ai,"Originally published at:			Converting CT Scans into 2D MRIs with AI | NVIDIA Technical Blog
Magnetic resonance imaging (MRI), is the gold standard in medical imaging. Unfortunately, it is not a viable option for patients with metal implants, as the metal in the machine could interfere with the results and the patients’ safety. To help solve the problem researchers in South Korea are using deep learning to convert CT scans to…Powered by Discourse, best viewed with JavaScript enabled"
2720,how-will-machine-learning-change-ecommerce,"Originally published at:			How Will Machine Learning Change ECommerce? | NVIDIA Technical Blog
Sentient Technologies is using machine learning and GPUs to create a Sales Associate with artificial intelligence for online stores. The company recently announced a collaboration with Shoes.com that uses a ‘Visual Filter’ and machine learning to make it the world’s first artificial intelligence-powered shopping experience. The new technology called “Sentient Aware” is being deployed on…Powered by Discourse, best viewed with JavaScript enabled"
2721,algorithm-identifies-skin-cancer-as-accurately-as-a-dermatologist,"Originally published at:			Algorithm Identifies Skin Cancer as Accurately as a Dermatologist | NVIDIA Technical Blog
Stanford researchers developed a deep learning-based algorithm to visually diagnose potential cancer. “We realized it was feasible, not just to do something well, but as well as a human dermatologist,” said Sebastian Thrun, an adjunct professor in the Stanford Artificial Intelligence Laboratory. “That’s when our thinking changed. That’s when we said, ‘Look, this is not…Powered by Discourse, best viewed with JavaScript enabled"
2722,gtc-2020-workstation-inference-with-tensorrt-cudnn-and-winml,"GTC 2020 CWE21166
Presenters: ,
Abstract
Our experts are highly experienced with moving AI Inference models from research to production environments and are happy to share these experiences, tools, and techniques with you, including topics such as:Join us to learn more about the constraints related to deployment of AI inference models on Windows workstations using a local GPU.Watch this session
Join in the conversation below.Powered by Discourse, best viewed with JavaScript enabled"
2723,train-your-ai-model-once-and-deploy-on-any-cloud-with-nvidia-and-run-ai,"Originally published at:			https://developer.nvidia.com/blog/train-your-ai-model-once-and-deploy-on-any-cloud-with-nvidia-and-runai/
Organizations are increasingly adopting hybrid and multi-cloud strategies to access the latest compute resources, consistently support worldwide customers, and optimize cost. However, a major challenge that engineering teams face is operationalizing AI applications across different platforms as the stack changes. This requires MLOps teams to familiarize themselves with different environments and developers to customize applications…Powered by Discourse, best viewed with JavaScript enabled"
2724,transforming-the-future-of-mobility-at-its-america-with-nvidia-metropolis-partners,"Originally published at:			Transforming the Future of Mobility at ITS America with NVIDIA Metropolis Partners | NVIDIA Technical Blog
Explore NVIDIA Metropolis partners showcasing new technologies to improve city mobility at the ITS America 2021.Powered by Discourse, best viewed with JavaScript enabled"
2725,new-course-get-started-with-highly-accurate-custom-asr-for-speech-ai,"Originally published at:			Courses – NVIDIA
Learn how to build, train, customize, and deploy a GPU-accelerated automatic speech recognition service with NVIDIA Riva in this self-paced course.Powered by Discourse, best viewed with JavaScript enabled"
2726,pgi-community-edition-19-4-now-available,"Originally published at:			PGI Community Edition 19.4 Now Available | NVIDIA Technical Blog
PGI Compilers & Tools are used by scientists and engineers developing applications for high-performance computing (HPC). PGI products deliver world-class multicore CPU performance, an easy on-ramp to GPU computing with OpenACC directives, and performance portability across all major HPC platforms. Available for free download. New Features in PGI 19.4 CUDA Fortran Support for NVIDIA V100…Powered by Discourse, best viewed with JavaScript enabled"
2727,behind-the-scenes-of-kingsoft-seasun-s-jx3-online,"Originally published at:			Behind the scenes of Kingsoft Seasun’s JX3 Online | NVIDIA Technical Blog
At GTC CHINA – taking place December 16-19 at Suzhou Jinji Lake International Conference Center – Young Yang from NVIDIA, and Ming Dong from Kingsoft, will explain how JX3 Online is using ray tracing for the first time to deliver a top-tier visual experience. JX3 Online combines world-building with martial arts to provide a game…Powered by Discourse, best viewed with JavaScript enabled"
2728,delivering-server-class-performance-at-the-edge-with-nvidia-jetson-orin,"Originally published at:			https://developer.nvidia.com/blog/delivering-server-class-performance-at-the-edge-with-nvidia-jetson-orin/
Get started with developing all four Jetson Orin modules for a new era of robotics.Powered by Discourse, best viewed with JavaScript enabled"
2729,developing-portable-cuda-c-c-code-with-hemi,"Originally published at:			https://developer.nvidia.com/blog/developing-portable-cuda-cc-code-hemi/
Software development is as much about writing code fast as it is about writing fast code, and central to rapid development is software reuse and portability. When building heterogeneous applications, developers must be able to share code between projects, platforms, compilers, and target architectures. Ideally, libraries of domain-specific code should be easily retargetable. In this…Powered by Discourse, best viewed with JavaScript enabled"
2730,justice-adds-nvidia-rtx-global-illumination-expanding-its-roster-of-ray-traced-effects,"Originally published at:			https://developer.nvidia.com/blog/justice-adds-nvidia-rtx-global-illumination-expanding-its-roster-of-ray-traced-effects/
By Andrew Burnes Justice, NetEase’s martial arts MMO, leads the way in its native China, with engrossing gameplay, a robust player base, and a continually-upgraded internally-developed engine called Night Blaze that’s at the forefront of tech. In 2018, Justice’s Night Blaze engine added real-time ray-traced reflections, shadows and caustics, and NVIDIA DLSS. And now, Justice…Powered by Discourse, best viewed with JavaScript enabled"
2731,validating-ai-models-collaboratively-with-nvidia-clara-imaging-and-md-ai,"Originally published at:			Validating AI Models Collaboratively with NVIDIA Clara Imaging and MD.ai | NVIDIA Technical Blog
NVIDIA Clara medical AI models can now run natively on MD.ai in the cloud, enabling collaborative model validation and rapid annotation projects using modern web browsers.Powered by Discourse, best viewed with JavaScript enabled"
2732,new-video-what-runs-chatgpt,"Originally published at:			https://developer.nvidia.com/blog/new-video-what-runs-chatgpt/
Some years ago, Jensen Huang, founder and CEO of NVIDIA, hand-delivered the world’s first NVIDIA DGX AI system to OpenAI. Fast forward to the present and OpenAI’s ChatGPT has taken the world by storm, highlighting the benefits and capabilities of artificial intelligence (AI) and how it can be applied in every industry and business, small…Powered by Discourse, best viewed with JavaScript enabled"
2733,performing-the-vran-benchmark-with-gpus-using-the-nvidia-aerial-sdk,"Originally published at:			https://developer.nvidia.com/blog/performing-the-vran-benchmark-with-gpus-using-the-nvidia-aerial-sdk/
Virtualization is key to making networks flexible and data processing faster, better, and highly adaptive with network infrastructure from Core to RAN. You can achieve flexibility in deploying 5G services on commercial off-the-shelf (COTS) systems. However, 5G networks bring support for ultra-low latency, high-bandwidth applications, and scalable networks with network slicing and software defined networking…Powered by Discourse, best viewed with JavaScript enabled"
2734,nvidia-nsight-systems-adds-vulkan-support,"Originally published at:			NVIDIA Nsight Systems Adds Vulkan Support | NVIDIA Technical Blog
Vulkan is a low-overhead, cross-platform 3D graphics and compute API targeting a wide variety of devices from cloud gaming servers, to PCs and embedded platforms. The Khronos Group manages and defines the Vulkan API. Introduction to NVIDIA Nsight Systems NVIDIA Nsight™Systems provides developers with a unified timeline view which displays how applications use computer resources. This low-overhead performance…This sounds very interesting! Good job!!I have one question. Is there any article on how to run vulkan applications on Nvidia Nsight Systems ? Because I have already tried to run some Vulkan compute applications that I have but the nsight systems doesn't even show the command buffer creation. In Diagnosis summary it says ""Zero Vulkan events were collected. Does the application use Vulkan?"" and I don't really know why...Thank you!Powered by Discourse, best viewed with JavaScript enabled"
2735,autonomous-vehicle-radar-perception-in-360-degrees,"Originally published at:			Autonomous Vehicle Radar Perception in 360 Degrees | NVIDIA Technical Blog
Our radar perception pipeline delivers 360-degree surround perception around the vehicle, using production-grade radar sensors operating at the 77GHz automotive microwave band.     Signals transmitted and received at microwave wavelengths are resistant to impairment from poor weather (such as rain, snow, and fog), and active ranging sensors do not suffer reduced performance during night time…Very interesting reading.Most of the heavy lifting here is done by our radar signal processing libraries-> are these libs restricted to Drive AGX plateforms or will be available on all cuda-capable devices ?Thank you for your interest. I do not believe there is any plan beyond Drive AGX at this stage.Hello sir,We are trying to integrate radar with enforcement camera,we need your support on this.can you provide us your mail Id,so we discuss more about on this topics,we are waiting for your replythanks in advanceIt may be useful for the 360 degree radar outputs to be shared between cars via  V2V communications.  Although processing all the data will be a challenge in real time.  - Ben @ EdlavitchLaw.comThanks for your comment – we recommend checking out the DRIVE AGX forums for more conversations on self-driving development!Powered by Discourse, best viewed with JavaScript enabled"
2736,gtc-2020-ctr-inference-optimization-on-gpu,"GTC 2020 S21416
Presenters: Chandler Zhou ,NVIDIA; David Wu,NVIDIA
Abstract
The CTR (click-through-rate) model is one of the most important models in internet businesses such as search, recommendation, and advertising. The performance of CTR online service has become a critical impact on user experience/enterprise revenue. With the development of deep learning technology, the CTR model began to adopt deeper and more complex DNN structure, and the computation scale and complexity continued to rise, which demanded more computing power. With the evolution of the CTR model in recent years and the promotion of NVIDIA GPU computing platform, more and more companies have begun to use NVIDIA GPU to accelerate the CTR online inference model, and achieved significant acceleration and got commercial benefits. We’ll introduce how to profile and locate the issues when doing optimization CTR inference model on NVIDIA GPU, and provide the general methods on how to solve the issues and get satisfying speedup.Watch this session
Join in the conversation below.Powered by Discourse, best viewed with JavaScript enabled"
2737,training-your-jetbot-in-nvidia-isaac-sim,"Originally published at:			https://developer.nvidia.com/blog/training-your-jetbot-in-isaac-sim/
How do you teach your JetBot new tricks? In this post, we highlight NVIDIA Isaac Sim simulation and training capabilities by walking you through how to train the JetBot in Isaac Sim with reinforcement learning (RL) and test this trained RL model on NVIDIA Jetson Nano with the real JetBot. Figure 1. Which one is…Hi there, for training on omniverse, besides the isaac sim, do we need any other libs? i check your post and the source code in github, the readme.txt in the isaccsim_patch says:
Please copy these files over to your Isaac Sim 2020.2 folder before training.
Specifically, syntheticdata.py goes to source/extensions/omni.isaac/synthetic_utils/python/scripts/
The other 4 files go to source/python_samples/jetbotso where is the omniverse extension, where can we download the extension?ls/python/scripts/
The other 4 files go to source/python_samples/jetbotso where is the omniverse extension, where can we download the extension?You can register and download Isaac Sim here, as stated in the blog post.
https://developer.nvidia.com/isaac-sim“First, download Isaac Sim. Running Isaac Sim requires the following resources:”Hi HaiLocLu,
thanks for the reply, I already downloaded the Isaac Sim, and could not find the extensions in this location: extensions/omni.isaac.
Instead , i found out another directory, looks like it is what I want:
isaac-sim/_build/linux-x86_64/release/exts
I will do the sample training later.isaac-sim/_build/linux-x86_64/release/extsThanks much. You’re right. I was referring to the internal path, while the path for your public build should be in the _build folder. I updated the instructions on my github.Thanks
image1626×1000 81.3 KB
maybe gym version error…how to solve it?Looks like a gym version error. Which version of stable_baselines did you use? You can also check the Isaac Sim’s python_samples/requirements.txt.
At the time of the blogpost published, it was
python3 -m pip install stable_baselines3==0.8.0So the versions in Isaac Sim need to match what’s installed on the real JetBot. Thanks.I am struggling to train using given material from issac sim patch to isaac simulation windows 11. When I trained its saying omnikithelper errorHiya Naresh!Thank you for your interest in our blog post and RL in Omniverse with Isaac-sim! Unfortunately, much of the codebase has changed since this post was released and it is no longer compatible with current versions of Isaac-sim.  In short, addressing this issue would not only mean maintaining depreciated code, but also maintaining a demonstration of a workflow that may not even be valid anymore.That said, we are still developing tools for building and running RL environments in Omniverse with Isaac-sim!  Keep your eyes peeled for new developments in the near future! ;)Sorry I couldn’t be of more help.  Good luck in your work!
GusThanks for the reply, I am happy to collaborate with you guys to make new blog post based on the latest information on the deployment of Deep Reinforcement learning to robotics kit like jetbotThanks for the offer, @ac.nareshkumar1993! While Gus might not be able to work on a new post right now (GTC is coming right up!), we’re always willing to get new posts from other people who have success stories or solutions. Let us know if you’d like to submit a post and we’ll work with you on the process.I am willing to work with you , and write a post soonGreetings! I found the blog and started to follow along, but ran into some snags. Is it still currently possible to train the Jetbot in Isaac Sim through the Omniverse, or is this no longer supported?If it is supported, is it possible to do on Windows? I tried placing the patch files in a few different directories but no luck getting Jetson options to show up in the software.If it’s not supported, is there a roadmap for when support will return? Or alternatives to use? I am looking to teach AI + robotics and would like to leverage simulation to speed up some elements like model training.Hiya jbflot!  welcome to the forums :DThis is a question I get asked a lot! The bad news first: Unfortunately that code is depreciated.The good news is we have Omni Isaac Gym Environments (OIGE) GitHub - NVIDIA-Omniverse/OmniIsaacGymEnvs: Reinforcement Learning Environments for Omniverse Isaac Gym
Which leverages the RL games library for training, and might meet your immediate teaching needs.This might actually make another good topic for a live stream :3Thanks for the response and the link to the Gym Environments. They’re very cool, but after further investigation they seem out-of-reach for the graphics cards in our computers. The Viewport wasn’t loading in Isaac Sim and the console was throwing an error that said I needed to do a clean install of my drivers - I did, but the problem persisted. I have a Quadro T2000 which seems like it’s not supported. Thanks again.Edit: Maybe update the blog that this is based on, if possible, to reflect the deprecated support? It’s still quite high in search rankings and makes it seem like it’s something supported until you get into the weeds.You are not the only person who has pointed this out! Thank you!  I want to do an updated version of this RL blog post with updated techniques, but modern techniques are very different.  There’s a lot to discuss actually…   I will at the very least add a depreciation warning at the top of the postPowered by Discourse, best viewed with JavaScript enabled"
2738,new-on-ngc-one-click-deploy-ai-models-for-speech-and-computer-vision-and-more,"Originally published at:			https://developer.nvidia.com/blog/new-on-ngc-one-click-deploy-ai-models-for-speech-and-computer-vision-and-more/
This month the NGC catalog added a new one-click deploy feature, new speech and computer vision models, and sample speech training data to help simplify your AI app development.Powered by Discourse, best viewed with JavaScript enabled"
2739,nvidia-releases-updates-to-cuda-x-ai-software,"Originally published at:			https://developer.nvidia.com/blog/nvidia-releases-updates-to-cuda-x-ai-software/
NVIDIA CUDA-X AI are deep learning libraries for researchers and software developers to build high performance GPU-accelerated applications for conversational AI, recommendation systems and computer vision.Powered by Discourse, best viewed with JavaScript enabled"
2740,cvpr-2020-watch-an-autonomous-vehicle-navigate-around-silicon-valley,"CVPR 2020 dcv02
Presenters: Tech Demo Team, NVIDIA
Abstract
NVIDIA DRIVE Constellation enables high-fidelity, end-to-end simulation for development and validation of autonomous vehicles.Watch this session
Join in the conversation below.Where can I find more details about this demo? What onboard computer is used for this demo?Powered by Discourse, best viewed with JavaScript enabled"
2741,please-note-we-can-not-comment-on-unannouced-features,"Thanks for your understandingPowered by Discourse, best viewed with JavaScript enabled"
2742,see-the-light-dnn-based-perception-for-automatic-high-beam-control,"Originally published at:			https://developer.nvidia.com/blog/dnn-based-perception-for-automatic-high-beam-control/
By JC Li Editor’s note: This is the latest post in our NVIDIA DRIVE Labs series, which takes an engineering-focused look at individual autonomous vehicle challenges and how NVIDIA DRIVE addresses them. Catch up on all of our automotive posts, here. AI can now make it easier for cars to see in the dark, while ensuring other…Powered by Discourse, best viewed with JavaScript enabled"
2743,serving-ml-model-pipelines-on-nvidia-triton-inference-server-with-ensemble-models,"Originally published at:			https://developer.nvidia.com/blog/serving-ml-model-pipelines-on-nvidia-triton-inference-server-with-ensemble-models/
Learn the steps to create an end-to-end inference pipeline with multiple models using NVIDIA Triton Inference Server and different framework backends.Thanks for the detailed tutorial, very useful!However, this doesn’t seem like an apple-to-apple comparison. What if we do the pre and post processing locally using GPU, then the latency should be the same?Powered by Discourse, best viewed with JavaScript enabled"
2744,gtc-21-top-5-nvidia-ai-dl-technical-sessions,"Originally published at:			https://developer.nvidia.com/blog/gtc-21-top-5-nvidia-ai-dl-technical-sessions/
With more than 1,400 sessions including the latest deep learning technologies in conversational AI, recommender systems, computer vision, and video streaming, here is a preview of some of the top AI/DL sessions.Powered by Discourse, best viewed with JavaScript enabled"
2745,libcu-open-source-gpu-enable-c-standard-library-updated,"Originally published at:			libcu++ Open-Source GPU-enable C++ Standard Library Updated | NVIDIA Technical Blog
libcu++, the NVIDIA C++ Standard Library, provides a C++ Standard Library for your entire system which can be used in and between CPU and GPU codes. The NVIDIA C++ Standard Library is an open source project.  Version 1.4.0 of libcu++ is a major release providing several feature enhancements and bug fixes. It adds support for…Powered by Discourse, best viewed with JavaScript enabled"
2746,event-cuda-12-2-youtube-premiere,"Originally published at:			CUDA Toolkit 12.2: New Accelerated Computing and Security Enhancements Revealed - YouTube
On July 6, join experts for a deep dive into CUDA 12.2, including support for confidential computing.Powered by Discourse, best viewed with JavaScript enabled"
2747,nsight-developer-tools-unleash-performance-advantages-of-nvidia-ampere-architecture,"Originally published at:			Nsight Developer Tools Unleash Performance Advantages of NVIDIA Ampere Architecture | NVIDIA Technical Blog
To help unleash the performance advantages of the NVIDIA Ampere Architecture, the  CUDA Toolkit 11 and Nsight Systems 2020.3 and Nsight Compute 2020.1 developer tools have been enhanced and scheduled for general availability at the end of May.Powered by Discourse, best viewed with JavaScript enabled"
2748,how-to-access-global-memory-efficiently-in-cuda-c-c-kernels,"Originally published at:			https://developer.nvidia.com/blog/how-access-global-memory-efficiently-cuda-c-kernels/
In the previous two posts we looked at how to move data efficiently between the host and device. In this sixth post of our CUDA C/C++ series we discuss how to efficiently access device memory, in particular global memory, from within kernels. There are several kinds of memory on a CUDA device, each with different…In your description, you discussed that ""Arrays allocated in device memory are aligned to 256-byte memory segments by the CUDA driver."" Why are the arrays allocated in memory aligned to 256-byte memory segments? Is it limited by the CUDA Driver? Are there other alignments?Minor mistake:The post is tagged as ""CUDA C++"" while the rest of the post are refered as ""CUDA C/C++""Fixed. Thanks!Hi Mark,Could you explain this statement?""For the C870 or any other device with a compute capability of 1.0, any misaligned access by a half warp of threads (or aligned access where the threads of the half warp do not access memory in sequence) results in 16 separate 32-byte transactions.""And then why were there 4 bytes requested per 32-byte transaction?Thank you!if i'm not mistaken, then :* 4 bytes are requested per 32-byte transaction because the testes presented are for single precision, so elements are coded by 4 bytes* in the C870, if accesses are not aligned, then each access causes a NEW TRANSACTION, and i suppose that the hardware choses -- from 32,64,128 byte  available transactions -- the smallest transaction in term of bytes fetched to satisfy the requested element (in this case 4 bytes, so 32 is more than enough)Are all those observation and best practices explained in the post still valid today, 5 years after the original post? Does this article equally applies to the newer cards with, say, compute capability 6.0?https://docs.nvidia.com/cud... has information about compute capability of 6.0 and above.Powered by Discourse, best viewed with JavaScript enabled"
2749,nvidia-unveils-new-reinforcement-learning-research-at-icra-2019,"Originally published at:			NVIDIA Unveils New Reinforcement Learning Research at ICRA 2019 | NVIDIA Technical Blog
This week, NVIDIA researchers from the newly opened robotics research lab  in Seattle, Washington are presenting a new proof of concept reinforcement learning approach that aims to enhance how robots trained in simulation will perform in the real world. The work will be presented at the International Conference on Robotics and Automation (ICRA) in Montreal,…Powered by Discourse, best viewed with JavaScript enabled"
2750,cooking-up-new-network-models-with-nvidia-linux-switch,"Originally published at:			https://developer.nvidia.com/blog/cooking-up-new-network-models-with-linux-switch/
With Linux Switch on Spectrum, Yandex gained transparency and control over the network, and disaggregated networking hardware and software, and lowered costs.Powered by Discourse, best viewed with JavaScript enabled"
2751,unified-memory-now-for-cuda-fortran-programmers,"Originally published at:			https://developer.nvidia.com/blog/unified-memory-cuda-fortran-programmers/
Unified Memory is a CUDA feature that we’ve talked a lot about on Parallel Forall. CUDA 6 introduced Unified Memory, which dramatically simplifies GPU programming by giving programmers a single pointer to data which is accessible from either the GPU or the CPU. But this enhanced memory model has only been available to CUDA C/C++…Hello, I'm not using the unified memory yet, but I plan to test it soon. Meanwhile I have to solve a problem I'm having: I'm parallelizing a code written in Fortran and I've been using the cublas and cusparse wrappers to call the subroutines I need from Fortran and for that I need to use the flag ""-Miface=cref"" in PGI compiler due to some conventions on calling subroutines from different languages. The problem is that now I need to introduce my own kernels and the subroutine with attributes(global) that I introduced is not recognized, giving the error “error LNK2019:unresolved external symbol mykernelname referenced in function MAIN_ “ . I think that it might be related to some missing underscore or some additional flag that I should use or change the given cublas and cusparse wrappers to avoid using the ""-Mifac=cref"" flag, but I've been looking and testing different scenarios in the last two days and still without success. Any clue?Greetings !Thanks for the excellent article !I am (and as I believe, many of us are) facing simple problem: this is commercial PGI suite.Could we please combine the GNU Fortran with the  CUDA unified memory feature ?Best, MiroPowered by Discourse, best viewed with JavaScript enabled"
2752,enter-the-emerging-companies-summit-vr-showcase-at-the-gpu-technology-conference,"Originally published at:			Enter the Emerging Companies Summit VR Showcase at the GPU Technology Conference | NVIDIA Technical Blog
Share your creative, cutting-edge virtual reality innovations with the world. The VR Showcase, taking place at the GPU Technology Conference April 4-7, 2016 in San Jose, CA, is an opportunity for 8 teams to present their innovative work using Virtual Reality. The winning team will win $30,000 USD in cash and prizes. Each team will…Powered by Discourse, best viewed with JavaScript enabled"
2753,upcoming-event-deep-learning-sessions-at-gtc-2022,"Originally published at:			AI Developer Conference Session Catalog | GTC 2022 | NVIDIA
Join our deep learning sessions at GTC 2022 to learn about real-world use cases, new tools, and best practices for training and inference.Powered by Discourse, best viewed with JavaScript enabled"
2754,nvidia-launches-storefront-in-aws-marketplace-to-accelerate-and-simplify-ai-workflows,"Originally published at:			https://developer.nvidia.com/blog/aws-marketplace-storefront-simplify-ai-workflows/
Enterprises across industries are adopting AI to drive business growth and they’re relying on cloud infrastructure to develop and deploy their solutions. To help data scientists and developers simplify their AI workflows, we have collaborated with Amazon Web Services (AWS) to bring NVIDIA NGC software resources directly to the AWS Marketplace. The AWS Marketplace is…Powered by Discourse, best viewed with JavaScript enabled"
2755,transforming-standard-video-into-slow-motion-with-ai,"Originally published at:			Transforming Standard Video Into Slow Motion with AI | NVIDIA Technical Blog
Researchers from NVIDIA developed a deep learning-based system that can produce high-quality slow-motion videos from a 30-frame-per-second video, outperforming various state-of-the-art methods that aim to do the same.  The researchers will present their work at the annual Computer Vision and Pattern Recognition (CVPR) conference in Salt Lake City, Utah this week.  “There are many memorable…Powered by Discourse, best viewed with JavaScript enabled"
2756,nvidia-merlin-latest-enhancements-streamlines-recommender-workflows-with-5-release,"Originally published at:			NVIDIA Merlin Latest Enhancements Streamlines Recommender Workflows with .5 Release | NVIDIA Technical Blog
The latest Merlin .5 update includes a data generator for training, multi-GPU dataloader, and initial support for session-based recommenders.Powered by Discourse, best viewed with JavaScript enabled"
2757,gtc-2020-integrating-the-nvidia-material-definition-language-in-your-application,"GTC 2020 S21222
Presenters: Moritz Kroll,NVIDIA; Sandra Pappenguth,NVIDIA
Abstract
The NVIDIA MDL SDK provides a rich toolset to integrate MDL in a wide range of renderers, from physically based ray tracing to real-time applications. In this tutorial-like session, we’ll show how MDL materials and texturing functions can be compiled for OptiX/CUDA, DirectX, x86, and OpenGL target platforms. We’ll also discuss how the MDL Distiller can be used to simplify MDL materials for use with real-time rendering solutions. Developers will learn about the available APIs and example code.Watch this session
Join in the conversation below.Powered by Discourse, best viewed with JavaScript enabled"
2758,pinterest-uses-ai-to-enhance-its-recommendations-system,"Originally published at:			Pinterest Uses AI to Enhance its Recommendations System | NVIDIA Technical Blog
Developers from Pinterest, along with collaborators from Stanford University, recently announced PinSage, an advanced recommendation deep learning-based framework used for ad and shopping recommendations within the social network. “As the number of people using Pinterest grows beyond 200M+ MAU, and the number of objects saved has crossed 100B, we must continuously build technology to not…Powered by Discourse, best viewed with JavaScript enabled"
2759,deploying-models-from-tensorflow-model-zoo-using-nvidia-deepstream-and-nvidia-triton-inference-server,"Originally published at:			Deploying Models from TensorFlow Model Zoo Using NVIDIA DeepStream and NVIDIA Triton Inference Server | NVIDIA Technical Blog
If you’re building unique AI/DL application, you are constantly looking to train and deploy AI models from various frameworks like TensorFlow, PyTorch, TensorRT, and others quickly and effectively. Whether it’s deployment using the cloud, datacenters, or the edge, NVIDIA Triton Inference Server enables developers to deploy trained models from any major framework such as TensorFlow,…Hi, this is Dhruv. Hope the blog was instructional. Triton Inference Server is something I find myself using very often to deploy models for simple tests as well as production. Being framework agnostic, it’s also really useful for testing off the shelf models for latency/performance and accuracy to make sure it’ll meet my needs. With the integration of Triton with DeepStream, these abilities are now available on NVIDIA dGPU and NVIDIA Jetson with streaming video and edge-to-cloud features. While this blog focuses on deepstream-app as a turnkey solution for IVA, the nvinferserver gstreamer plugin can be used for most models. Furthemore, TF-TRT allows for easy performance optimization with minimal time spent in creating a TensorRT plan so you can prototype and see what kind low hanging fruit can be used to improve performance. Good luck with your IVA projects!hi @dsingalNV, I see in your blog  at Deploying Models from TensorFlow Model Zoo Using NVIDIA DeepStream and NVIDIA Triton Inference Server | NVIDIA Technical Blog the sample was conducted using FasterRCNN-InceptionV2, but the results are shown for FasterRCNN-InceptionV3, is that correct?, If so, what is the expected performance for FasterRCNN-InceptionV2?. currently for FP32 with no optimization (4 streams and BS=4) I am getting around 4 fps on T4@virsg the results are for FasterRCNN-InceptionV2, thanks for bringing that issue to my attention. Wrt your performance, with 4 streams and BS=4, if you’re getting 4fps per stream before any optimization, then that performance is in line with what we expect(our single stream example obtained 12.8 fps total). On the other hand, if you’re observing 4fps in all, aka 1 fps for each of your 4 streams, then some things to investigate would be:Thanks @dsingalNV, I am using the docker image  deepstream:5.0.1-20.09-triton. It seems that my results are ok then?, see below:With num-sources=1 I am getting *PERF:  17.31 (16.85)
With num-sources=4 I am getting **PERF:  4.33 (4.32)    4.33 (4.35)     4.33 (4.32)     4.33 (4.32)Also some extra questions here:1- What is the best technique to maximize the gpu utilization with DeepStream and Triton?. I have increased the instance count to 2 but average gpu utilization was ~60% and 5fps per stream, I was expecting to double the gpu utilization as well as the throughput since there were 2 instances of the model loaded into the T42- How can I launch Deepstream-Triton server and client separately?Yes, the performance results youre’ seeing are okay.hi @dsingalNV, thanks for your detailed explanation. Here are some extra questions:1- With the current deepstream-app, how can I control the two instances running in parallel or in sequence?
2- How ca the Deepstream-Triton be separated from the client in the case of running the inference on a data center server?Thanks for the blog. this is really impressive.I have followed the steps and installed deepstream in my local system as well as Jetson nano. I am able to run the deepstream on my local system, but when I tried to run nano, I am getting below error, could you help me to resolve this.
deepstream-app: error while loading shared libraries: libnvinfer.so.7: cannot open shared object file: No such file or directoryyou can look into translating the model into a TRT-only engine. This get around quite a lot of overhead from TF-TRT. Currently, only two subgraphs of the model have TRT engines generated for them.Hi @dsingalNV, what scripts do you recommend to convert the model to TF-TRT INT8, and also to Native TRT INT8?This is amazing. All the Nvdia products are well designed and provide good performance.Hi @virsg, sorry for the late reply. There are three methods I know of to convert your model to TF-TRT or TRT. Some support INT8 and some don’t.@sk.ahmed401 that looks like an error with the DeepStream installation. Was it resolved or are you still looking for help?@dsingalNV there’s a way to have such architecture :thankyou, WilliamYes, you can have the triton server hosted elsewhere and communicate with it through gRPC if you use the gRPC option for nvdsinferserver.
https://docs.nvidia.com/metropolis/deepstream/dev-guide/text/DS_plugin_gst-nvinferserver.htmlPowered by Discourse, best viewed with JavaScript enabled"
2760,ai-in-endoscopy-improving-detection-rates-and-visibility-with-real-time-sensing,"Originally published at:			AI in Endoscopy: Improving Detection Rates and Visibility with Real-Time Sensing | NVIDIA Technical Blog
With the assistance of AI, surgical procedures like endoscopy are becoming safer and more consistent while reducing surgeon workload.Powered by Discourse, best viewed with JavaScript enabled"
2761,explore-the-latest-in-omniverse-create-from-material-browsers-to-the-animation-sequencer,"Originally published at:			https://developer.nvidia.com/blog/explore-the-latest-in-omniverse-create-from-material-browsers-to-the-animation-sequencer/
NVIDIA Omniverse Create 2021.3 is now available in open beta, delivering a new set of features for Omniverse artists, designers, developers, and engineers to enhance graphics and content creation workflows. We sat down with Frank DeLise, Senior Director of Product Management for Omniverse, to get a tour of some of the exciting new features. Get…Powered by Discourse, best viewed with JavaScript enabled"
2762,just-released-cusparselt-v0-3,"Originally published at:			Release Notes — NVIDIA cuSPARSELt 0.3.0 documentation
The NVIDIA cuSPARSELt update expands the high-performance CUDA library support for vectors of alpha and beta scalars, GeLu scaling, Split-K Mode, and more.Powered by Discourse, best viewed with JavaScript enabled"
2763,nsight-visual-studio-edition-2019-1-released,"Originally published at:			Nsight Visual Studio Edition 2019.1 Released | NVIDIA Technical Blog
NVIDIA Nsight Visual Studio Edition 2019.1 is now available for download in the NVIDIA Registered Developer Program.   The NVIDIA Nsight Visual Studio Edition 2019.1 is now available for download. Version 2019.1 extends support to the latest Turing GPUs and Win10 RS5. The Graphics Debugger adds Vulkan Pixel History as well as OpenGL + Vulkan…Powered by Discourse, best viewed with JavaScript enabled"
2764,synchronizing-present-calls-between-applications-on-distributed-systems-with-directx-12,"Originally published at:			Synchronizing Present Calls Between Applications on Distributed Systems with DirectX 12 | NVIDIA Technical Blog
Present barrier provides an easy way of synchronizing present calls between application windows on the same machine, as well as on distributed systems.Is there any drawback with this approach when Synchronizing multiple computers with Quadro Sync II and each one having a Mosaic enabled?
In other words: is ist possible to sync across multiple Mosaic computers?Thanks for the post!
Cheers,
LucasPowered by Discourse, best viewed with JavaScript enabled"
2765,nvidia-announces-tensorrt-8-2-and-integrations-with-pytorch-and-tensorflow,"Originally published at:			NVIDIA Announces TensorRT 8.2 and Integrations with PyTorch and TensorFlow | NVIDIA Technical Blog
Learn about TensorRT 8.2 and the new TensorRT framework integrations, which accelerate inference in PyTorch and TensorFlow with just one line of code.Powered by Discourse, best viewed with JavaScript enabled"
2766,cuda-pro-tip-optimize-for-pointer-aliasing,"Originally published at:			https://developer.nvidia.com/blog/cuda-pro-tip-optimize-pointer-aliasing/
Often cited as the main reason that naïve C/C++ code cannot match FORTRAN performance, pointer aliasing is an important topic to understand when considering optimizations for your C/C++ code. In this tip I will describe what pointer aliasing is and a simple way to alter your code so that it does not harm your application…Hi Jeremy,If I use plain (restrict annotated) pointer arguments for a kernel directly, the behavior is what I expect and what you described. Still, if I use restrict annotated pointers inside a struct, it’s compiling, but the compiler doesn’t seem to take advantage of this.
See __restrict__ seems to be ignored for base pointers in structs. having base pointers with restrict as kernel arguments directly works as expected for tiny examples.So to put it in short: Is there a trick to specify such annotation using struct of arrays in a way that nvcc takes advantage of this?Thanks you and kind regards,
KlausPS: It’s not only nvcc, clang behaves very similar. So I might be a front end problem in clang which nvcc inherits, I am just speculating here.I do love the restrict keyword (and const, despite being told in a previous life that you didn't really need the const keyword). The Restrict Contract is an amusing way of putting down what the keyword means.  A revised version was on my door for a while: http://cellperformance.beyo...Why use pointers anyway if L2 is big enough?Is there a pragma to specify a loop has no loop-dependencies instead?When you write""By giving a pointer the restrict property, the programmer is promising the compiler that any data accessed through that pointer is not accessed in any other way. In other words, the compiler doesn’t have to worry about aliasing when using a pointer with the restrict property.""This is not true.Here is a trivial example: void example1(restrict float *a, restrict float *b, float *c, int i) {     c[i] = a[i] + b[i];  }In this example, a and b may alias (and a variant of this example is given in the C99 standard as how two restricted variables may still alias)C99/C11 require that the restricted storage be modified in the restrict block for any guarantee to hold.In fact, there are all sorts of messy/strange requirements on restrict that make it almost impossible for users to reason about whether it will still make their pointers alias. This is one reason that  C++ does not have restrict in the standard yet. After implementing conforming support in GCC/LLVM, the number of issues where the compiler could not do what the user wanted, and the user became seriously confused as to why two restricted pointers were thought aliasing, made them sit down and think about whether it was really the right model.I've been trying to understand pointer aliasing and performance, and am glad I came across this article. However, tried compiling example1 with and without __restrict from Visual Studio (Microsoft Optimizing Compiler Version 18.00.30723.0). Admittedly I'm very new to attempting to read x86 assembly, but here's what I get (both are in release configuration for optimization).In both cases we have:_TEXTSEGMENT_a$ = 8; size = 4_b$ = 12; size = 4_c$ = 16; size = 4_i$ = 20; size = 4Original; 3    : void example1(float* a, float* b, float* c, int i) {pushebpmovebp, esp; 4    : a[i] = a[i] + c[i];movedx, DWORD PTR _i$[ebp]movecx, DWORD PTR _c$[ebp]moveax, DWORD PTR _a$[ebp]movssxmm0, DWORD PTR [ecx+edx*4]addssxmm0, DWORD PTR [eax+edx*4]movssDWORD PTR [eax+edx*4], xmm0; 5    : b[i] = b[i] + c[i];moveax, DWORD PTR _b$[ebp]movssxmm0, DWORD PTR [ecx+edx*4]addssxmm0, DWORD PTR [eax+edx*4]movssDWORD PTR [eax+edx*4], xmm0; 6    : }popebpret0__restrict; 3    : void example1(float* __restrict a, float* __restrict b, float* __restrict c, int i) {pushebpmovebp, esp; 4    : a[i] = a[i] + c[i];movedx, DWORD PTR _i$[ebp]moveax, DWORD PTR _c$[ebp]movecx, DWORD PTR _a$[ebp]movssxmm1, DWORD PTR [eax+edx*4]; 5    : b[i] = b[i] + c[i];moveax, DWORD PTR _b$[ebp]movssxmm0, DWORD PTR [ecx+edx*4]addssxmm0, xmm1addssxmm1, DWORD PTR [eax+edx*4]movssDWORD PTR [ecx+edx*4], xmm0movssDWORD PTR [eax+edx*4], xmm1; 6    : }popebpret0In either case there are the same total operations (1 push, 1 pop, 5 mov, 4 movss, 2 addss), just in a slightly different order. Not obvious to me how one is advantageous compared to the other.I've updated those two sentences to hopefully be true. The fact is that using __restrict does help performance in both the CPU and GPU examples demonstrated here, on two different compilers. Unlike the example you give, our examples all write to a restricted pointer.Pointer aliasing and loop dependencies are two different, but related issues. Even if there are no loop dependencies, pointers may alias, and vice versa.Did you compare wall clock run time?I did not. The blog post implies that pointer aliasing will result in inefficient machine code because additional load operations are necessary (one additional operation in example1, if I read it correctly). However, the assembly code I'm seeing doesn't support that. That's more what I'm trying to understand - why or why aren't there additional loads, rather than if a re-ordering of the same operations is any different.In any event it would be really insightful if this blog entry included some assembly language examples to really drive the point home.Thanks for the reply.It's worth remembering that the compiler is free to optimize however it likes, and additional information may not be used. The actual instructions generated are going to depend on both your compiler and your target architecture.For example: if I compile the first example for a compute capability 3.5 GPU using nvcc on linux, I get the following SASS assembler (viewed using: cuobjdump --dump-sass):Without restrict:MOV R1, c[0x0][0x44];                      MOV R9, c[0x0][0x158];                     MOV32I R10, 0x4;                           IMAD.U32.U32 R4.CC, R9, R10, c[0x0][0x140];IMAD.HI.X R5, R9, R10, c[0x0][0x144];      IMAD.U32.U32 R6.CC, R9, R10, c[0x0][0x150];LD.E R3, [R4];IMAD.HI.X R7, R9, R10, c[0x0][0x154];      LD.E R0, [R6];                             IMAD.U32.U32 R8.CC, R9, R10, c[0x0][0x148];IMAD.HI.X R9, R9, R10, c[0x0][0x14c];      FADD R2, R3, R0;                           ST.E [R4], R2;                             LD.E R0, [R6];LD.E R3, [R8];                             FADD R0, R3, R0;                           ST.E [R8], R0;--With restrict:MOV R1, c[0x0][0x44];                     MOV R7, c[0x0][0x158];                    MOV32I R8, 0x4;                           IMAD.U32.U32 R2.CC, R7, R8, c[0x0][0x150];IMAD.HI.X R3, R7, R8, c[0x0][0x154];      LDG.E R3, [R2];                           IMAD.U32.U32 R4.CC, R7, R8, c[0x0][0x140];IMAD.HI.X R5, R7, R8, c[0x0][0x144];      LD.E R0, [R4];                            IMAD.U32.U32 R6.CC, R7, R8, c[0x0][0x148];IMAD.HI.X R7, R7, R8, c[0x0][0x14c];      TEXDEPBAR 0x0;                            FADD R2, R0, R3;                          ST.E [R4], R2;LD.E R0, [R6];                            FADD R0, R0, R3;                          ST.E [R6], R0;-----In this case the compiler generates four loads in the non-restrict version, and two normal loads and one texture cached load in the restrict version. The latter is clearly preferred.It's really a cool stuff about gpu..Thank u very muchThis is really interesting and helpful. Thank you so much.Powered by Discourse, best viewed with JavaScript enabled"
2767,create-manage-and-deploy-ai-enhanced-clinical-workflows-with-clara-deploy-sdk,"Originally published at:			Create, Manage, and Deploy AI-Enhanced Clinical Workflows with Clara Deploy SDK | NVIDIA Technical Blog
The medical imaging industry is undergoing a dramatic transformation driven by two technology trends: Artificial Intelligence and software-defined solutions are redefining the medical imaging workflow. Artificial Intelligence, specifically deep learning, demonstrates great potential within radiology for disease detection, localization, and classification. It has already shown it can augment humans by increasing their efficiency and effectiveness by…This is amazing! it helps two of the biggest bottlenecks for AI in healthcare: annotation and deployment.Thanks for the feedback. Please share any successes or failures with us. We want to learn with you.Hello,I am playing around with Clara Train AI and MITK. Do we have any documentation that lists detailed procedure as to how to use etc. Currently, it seems to be kind of black box for me. And I am stuck in kind of installation/setup phase. I have posted in Dev forum as well. Am new to DL, hence looking forward to learn from you allHere are my questions1) Nvidia registry has 13 models for different organs. After setting up the annotation server with 13 models, can you let me know what happens?2) As an end-user, let's say I have an MITK application.3) Connection is established between MITK and AIAA server4) Once connection is established, I will be able to see the data in MITK5) I have unlabelled patient data in MITK now6) I select one of those images, mark certain points based on inspectionCan you let us know what happens after this? What does these point marking in MITK do to AIAA. How will I know the label/class of the image?It would really be helpful if we could find any extensive and easy to understand doc.ThanksSelvaI believe it's the similar question/doubts.. please follow the videohttps://drive.google.com/op...to understand more about MITK + AIAA workflow for annotationHi, I've a problem running the 'ai-vnet' workflow instance with './clara-wf test'. Since I've got the output dicom files from stage dicom-writer, I guess it launched correctly. But I can't see any job on Clara dashboard nor any result from render server,  during the whole time.Got no error logs from dashboard container, dose I missing anything?Powered by Discourse, best viewed with JavaScript enabled"
2768,cudacasts-episode-13-clock-power-and-thermal-profiling-with-nsight-eclipse-edition,"Originally published at:			https://developer.nvidia.com/blog/cudacasts-episode-13-clock-power-thermal-profiling-nsight-eclipse-edition/
In the world of high-performance computing, it is important to understand how your code affects the operating characteristics of your HW.  For example, if your program executes inefficient code, it may cause the GPU to work harder than it needs to, leading to higher power consumption, and a potential slow-down due to throttling. A new profiling…This is awesome, thanks for the vid!Powered by Discourse, best viewed with JavaScript enabled"
2769,monai-reaches-1-million-download-milestone-driven-by-research-breakthroughs-and-clinical-adoption,"Originally published at:			https://developer.nvidia.com/blog/monai-reaches-1-million-download-milestone-driven-by-research-breakthroughs-and-clinical-adoption/
MONAI, the domain-specific, open-source medical imaging AI framework that drives research breakthroughs and accelerates AI into clinical impact, has now been downloaded by over 1M data scientists, developers, researchers, and clinicians. The 1M mark represents a major milestone for the medical open network for AI, which has powered numerous research breakthroughs and introduced new developer…Powered by Discourse, best viewed with JavaScript enabled"
2770,nvidia-enables-trustworthy-safe-and-secure-large-language-model-conversational-systems,"Originally published at:			https://developer.nvidia.com/blog/nvidia-enables-trustworthy-safe-and-secure-large-language-model-conversational-systems/
Large language models (LLMs) are incredibly powerful and capable of answering complex questions, performing feats of creative writing, developing, debugging source code, and so much more. You can build incredibly sophisticated LLM applications by connecting them to external tools, for example reading data from a real-time source, or enabling an LLM to decide what action…Powered by Discourse, best viewed with JavaScript enabled"
2771,gtc-2020-deep-learning-for-efficient-modeling-of-high-dimensional-spatiotemporal-physics,"GTC 2020 S22094
Presenters: Arvind Mohan,Los Alamos National Laboratory
Abstract
Several research problems in physical sciences are exceptionally complex and high-dimensional, exhibiting spatio-temporal dynamics, non-linearity, and chaos. In an era when vast quantities of such scientific data are generated, building practical, physics-driven reduced-order models (ROM) of such phenomena is crucial. While deep neural networks for spatio-temporal data have shown considerable promise, they face severe computational bottlenecks in learning extremely high-dimensional datasets, often with greater than 10^9 degrees of freedom. These application-agnostic networks may also lack physical constraints and interpretability that is desired in scientific ROMs. We’ll present our efforts in leveraging the strong mathematical and physical foundations underlying wavelet theory with the learning capacity of deep neural nets. We’ll demonstrate computationally efficient, partially interpretable learning with some embedded physics constraints for modeling large scientific datasets.Watch this session
Join in the conversation below.Powered by Discourse, best viewed with JavaScript enabled"
2772,gtc-2020-deep-learning-demystified,"GTC 2020 S22555
Presenters: Will Ramey,NVIDIA
Abstract
Artificial intelligence has evolved and improved methods for data analysis and complex computations, solving problems that seemed well beyond our reach only a few years ago. Today, deep learning is transforming every industry, from health care and retail to automotive and financial services. Join us to understand the fundamentals of accelerated data analytics, high-level use cases, and problem-solving methods. We’ll cover:• demystifying artificial intelligence, machine learning, and deep learning;
• understanding the key challenges organizations face in adopting this new approach and how to address them; and
• learning about the latest tools and technologies, along with training resources, that can help deliver breakthrough results.Watch this session
Join in the conversation below.Powered by Discourse, best viewed with JavaScript enabled"
2773,announcing-containerd-support-for-the-nvidia-gpu-operator,"Originally published at:			Announcing containerd Support for the NVIDIA GPU Operator | NVIDIA Technical Blog
For many years, docker was the only container runtime supported by Kubernetes. Over time, support for other runtimes has not only become possible but often preferred, as standardization around a common container runtime interface (CRI) has solidified in the broader container ecosystem. Runtimes such as containerd and cri-o have grown in popularity as docker has…The Cloud Native team at NVIDIA is excited to announce the new containerd support in the GPU Operator. If you have any questions or comments, please let us know!Where would I go to see the resulting containerd config toml from successfully running this?Hi rcvanvo,By default the containerd config is located at /etc/containerd/config.toml, and that is the one that gets updated if you don’t specify a different one.Hey,
Will it work for Nvidia Jetson as well?I would also love to know if it will work with Jetson Nano’s? Setting up a GPU enabled k3’s cluster with Jetson Nano’s is turning out to be quite the headache!Hi @alon2 and @josiaseHere is a link to the page listing the supported platforms for the GPU Operator:
https://docs.nvidia.com/datacenter/cloud-native/gpu-operator/platform-support.htmlAt present, Jetson is not supported unfortunately as mentioned in the comment:The GPU Operator only supports platforms using discrete GPUs - Jetson or other embedded products with integrated GPUs are not supported.Thank you for the response @kklues. With this in mind, I don’t see a way to access GPU’s from Jetson Nano worker nodes when running a K3 cluster :(We try to install the GPU Operator in our containerd-based Cluster.The DaemonSet for installing the nvidia driver exists with the following error message:What is our error here? Do you need more information? Hope you can help us. :)We are running Kubernetes v.1.19.7, GPU-Operator Helm Chart v.1.6.0 and containerd in v.1.4.3.Please use https://github.com/NVIDIA/gpu-operator/issues to file any issues related to the GPU Operator.Thanks @kklues. I will try my luck there.@kklues  bummer… any way to use contained on Jetson instead of docker ?Is there a way - even without the GPU operator - to make nvidia-runtime work with containerd instead of docker with k3s on a Jetson (ARM64)?@klein.shaked
Please see my response here for some guidance:
https://github.com/containerd/containerd/issues/4834#issuecomment-786854732Hello,
is this the correct place to ask a technical question?We have previously used the “Nvidia-device-plugin” which adds GPUs as a ressource to Kubernetes. Our Kubernetes-Jobs are scheduled based on percentage of GPU required (just like it is available for CPUs and memory).With the Kubernetes Upgrade beyond 1.20 and docker being removed, I found that the preferred installation now uses the “GPU Operator” according to (1). Which seems to work very well. However, I have not been able to get GPUs show up as schedule-able ressources yet (2). As such, our Jobs currently cannot execute.Is there a part I am missing? Can someone please point me to the instructions I need?Thank you very much.(1) https://docs.nvidia.com/datacenter/cloud-native/gpu-operator/getting-started.html
(2) Excerpt from kubectl get nodes:
Allocated resources:
Resource           Requests   Limitscpu                100m (0%)  100m (0%)
memory             50Mi (0%)  50Mi (0%)
ephemeral-storage  0 (0%)     0 (0%)
hugepages-1Gi      0 (0%)     0 (0%)
hugepages-2Mi      0 (0%)     0 (0%)
nvidia.com/gpu     0          0                              ← missing from GPU-operator setupPowered by Discourse, best viewed with JavaScript enabled"
2774,upcoming-webinar-optimizing-dnn-inference-with-nvidia-tensorrt-on-drive-orin,"Originally published at:			https://info.nvidia.com/optimizing-dnn-inference-with-nvidia-tensorrt-on-drive-orin.html?ncid=em-webi-523029#cid=_em-webi_en-us
Join an upcoming webinar highlighting the newest features of NVIDIA TensorRT and learn how to optimize inference engines for production on the Orin AI platform.Powered by Discourse, best viewed with JavaScript enabled"
2775,idc-business-value-white-paper-the-business-value-of-nvidia-ethernet-switch-solutions-for-managing-and-optimizing-network-performance,"Originally published at:			https://developer.nvidia.com/blog/idc-business-value-white-paper-the-business-value-of-nvidia-ethernet-switch-solutions-for-managing-and-optimizing-network-performance/
IDC analysts Brad Casemore and Harsh Singh interviewed IT organizations with real world experience deploying and managing Cumulus Linux and NVIDIA Spectrum switches in mission critical data centers over a significant time period.Powered by Discourse, best viewed with JavaScript enabled"
2776,top-conversational-ai-sessions-at-nvidia-gtc-2023,"Originally published at:			https://developer.nvidia.com/blog/top-conversational-ai-sessions-at-nvidia-gtc-2023/
Learn about the latest tools, trends, and technologies for building and deploying conversational AI.Powered by Discourse, best viewed with JavaScript enabled"
2777,clara-parabricks-3-7-brings-optimized-and-accelerated-workflows-for-gene-panels,"Originally published at:			https://developer.nvidia.com/blog/clara-parabricks-3-7-brings-optimized-and-accelerated-workflows-for-gene-panels/
The GPU-accelerated Clara Parabricks v3.7 release brings support for gene panels, RNA-Seq, short tandem repeats, and updates to GATK 4.2 and DeepVariant 1.1.Powered by Discourse, best viewed with JavaScript enabled"
2778,gtc-2019-silicon-valley-preview-cuda-training-and-posters,"Originally published at:			GTC 2019 Silicon Valley Preview: CUDA Training and Posters | NVIDIA Technical Blog
Expected to be the biggest yet, NVIDIA’s GPU Technology Conference (GTC 2019) features hundreds of sessions on the most important topics in computing today. At GTC, NVIDIA DLI offers an array of self-paced courses and instructor-led workshops for developers, data scientists, and researchers looking to solve the world’s most challenging problems with accelerated computing. Get…Powered by Discourse, best viewed with JavaScript enabled"
2779,creating-optimal-meshes-for-ray-tracing,"Originally published at:			Creating Optimal Meshes for Ray Tracing | NVIDIA Technical Blog
When you are creating triangle meshes for ray tracing or reusing meshes that have been successfully used in rasterization-based rendering, there are some pitfalls that can cause surprising performance issues. Some mesh properties that have been acceptable in rasterization can be problematic in ray tracing or require specific handling to work as expected. This post…Powered by Discourse, best viewed with JavaScript enabled"
2780,cross-compiling-robot-operating-system-nodes-for-nvidia-drive-agx,"Originally published at:			https://developer.nvidia.com/blog/cross-compiling-robot-operating-system-nodes-for-nvidia-drive-agx/
The Robot Operating System (ROS) is an extremely popular middleware used by roboticists and autonomous vehicle researchers around the globe. It offers a great deal of flexibility and a plethora of software libraries and tools to be leveraged by its users. As a result, there is a great deal of interest in understanding how ROS…Hi, i am working on running ROS as the main Middleware and run NVIDIA lanedetection as a ros node,  i have found Isaac frameworks and I also I have seen this blog for cross compilation,  what is the difference between the two methods and which one would be more efficient for this purposePowered by Discourse, best viewed with JavaScript enabled"
2781,gtc-2020-visualizing-150tb-of-data,"GTC 2020 D2S20
Presenters: Tech Demo Team,NVIDIA
Abstract
Simulating Retropulsion for Landing Humans on Mars - Modeling the Vehicle’s Martian Descent with the NASA FUN3D 150 TB DatasetWatch this session
Join in the conversation below.Powered by Discourse, best viewed with JavaScript enabled"
2782,is-nvidia-also-thinking-about-actual-3d-images,"Hello,Are generative or predictive models being considered for images that are actually 3D in size? For example, 3D scans of objects or human bodies (e.g., gray-scale 3D bodies of [256, 256, 256, 1])? Such problems usually require fully 3D-aware models.If so, what are the challenges on top of those of 2D images?This will have significant applications in scientific fields such as medicine and geoscience.Many thanks,
RezaHi, GET3D is able to genearte 3D meshes as it output, but generating actual 3D voxels is not considered at the moment. It would also be possible try to use similar idea from GET3D for this task (e.g. rendering the 3D bodies into 2D images and apply discriminator on the 2D images for supervision) we’d love to see how this can work!This topic was automatically closed 14 days after the last reply. New replies are no longer allowed.Powered by Discourse, best viewed with JavaScript enabled"
2783,global-ai-weather-forecaster-makes-predictions-in-seconds,"Originally published at:			https://developer.nvidia.com/blog/global-ai-weather-forecaster-makes-predictions-in-seconds/
Using convolutional neural networks researchers create an algorithm that can quickly calculate global forecasts 4 to 6 weeks into the future.Powered by Discourse, best viewed with JavaScript enabled"
2784,ray-tracing-essentials-part-2-rasterization-versus-ray-tracing,"Originally published at:			Ray Tracing Essentials Part 2: Rasterization versus Ray Tracing | NVIDIA Technical Blog
NVIDIA recently published Ray Tracing Gems, a deep-dive into best practices for real-time ray tracing. The book was made free-to-download, to help all developers embrace the bleeding edge of rendering technology. Ray Tracing Essentials is a seven-part video series hosted by the editor of Ray Tracing Gems, NVIDIA’s Eric Haines. The aim of this program is to make developers…Powered by Discourse, best viewed with JavaScript enabled"
2785,community-spotlight-democratizing-computer-vision-and-conversational-ai-in-kenya,"Originally published at:			https://developer.nvidia.com/blog/community-spotlight-democratizing-computer-vision-and-conversational-ai-in-kenya/
Jacques Khisa, community leader at Africa Data School Emerging Chapters Nairobi, shares his experience on getting started in AI in Africa.Powered by Discourse, best viewed with JavaScript enabled"
2786,flexible-cuda-thread-programming,"Originally published at:			Flexible CUDA Thread Programming | NVIDIA Technical Blog
In efficient parallel algorithms, threads cooperate and share data to perform collective computations. To share data, the threads must synchronize. The granularity of sharing varies from algorithm to algorithm, so thread synchronization should be flexible. Making synchronization an explicit part of the program ensures safety, maintainability, and modularity. CUDA 9 introduces Cooperative Groups, which aims to…Powered by Discourse, best viewed with JavaScript enabled"
2787,removing-aliasing-artifacts-in-ultrasound-color-doppler-imaging-with-nvidia-clara-holoscan-and-the-nvidia-clara-developer-kit,"Originally published at:			Removing Aliasing Artifacts in Ultrasound Color Doppler Imaging with NVIDIA Clara Holoscan and the NVIDIA Clara Developer Kit | NVIDIA Technical Blog
The NVIDIA Clara developer kit, NVIDIA Clara Holoscan, and us4us front end help build AI models on streaming data for ultrasounds, to remove artifacts like aliasing.Powered by Discourse, best viewed with JavaScript enabled"
2788,analyzing-genome-sequence-data-on-aws-with-wekafs-and-nvidia-clara-parabricks-pipelines,"Originally published at:			https://developer.nvidia.com/blog/analyzing-genome-sequence-data-on-aws-with-wekafs-and-clara-parabricks-pipelines/
Whole genome sequencing has become an important and foundational part of genomic research, enabling researchers to identify genetic signatures associated with diseases, differentiate sequencing errors from biological signals, and better characterize the genomes of various organisms. With the ongoing COVID-19 pandemic threatening the globe, characterizing, and understanding genomes is now more crucial than ever. Commercially…Thanks @jwitsoe! Great work by Robert and Bob! In particular very interesting how data management is made easier and especially how much better performance is over CPU and with the upgrade to newer version of Parabricks!I greatly enjoyed working on this solution to accelerate genome analysis, especially given the ongoing pandemic as I feel it has a very tangible and relevant impact on the world. If you have any questions or comments, let us know. Thanks Jen for your wonderful work as well!Powered by Discourse, best viewed with JavaScript enabled"
2789,ai-model-can-recommend-the-optimal-workout,"Originally published at:			AI Model Can Recommend the Optimal Workout | NVIDIA Technical Blog
Planning a workout that is specific to a user’s needs can be challenging. To help deliver more personalized workout recommendations, University of California, San Diego researchers developed a deep learning-based system to better estimate a runner’s heart rate during a workout and predict a recommended route. The work has the potential to help fitness tracking…Thanks for sharing! I think this can be helpful for all people who are starting to exercise. There are many ways to improve your workouts, but treating yourself right and keeping track of changes in your health is very importantPowered by Discourse, best viewed with JavaScript enabled"
2790,cuda-pro-tip-improve-nvidia-visual-profiler-loading-of-large-profiles,"Originally published at:			https://developer.nvidia.com/blog/cuda-pro-tip-improve-nvvp-loading-large-profiles/
Some applications launch many tiny kernels, making them prone to very large (100s of megabytes or larger) nvprof timeline dumps, even for application runs of only a handful of seconds. Such nvprof files may fail to even load when you try to import them into the NVIDIA Visual Profiler (NVVP). One symptom of this problem…thanks cliff. i had this issue with a large profile recently. your post is just in time for meOn at least some platforms, these options can also be triggered via the JAVA_TOOL_OPTIONS environment variable .  This is in particular useful where you don't have root privileges.For example, for 200gb memory, in a bash shell, you would use:$ export JAVA_TOOL_OPTIONS="" -Xmx200g""Or alternatively, set a temporary environment variable:$ nvvp JAVA_TOOL_OPTIONS="" -Xmx200g""I installed the most recent version of Cuda Toolkit on Mac. There is no nvvp.ini file in /Developer/NVIDIA/CUDA-10.0/libnvvp. Is that where it's supposed to be?Hi, Jacob:I suggest you take your issue to the NVIDIA developer forums at devtalk.nvidia.com.Powered by Discourse, best viewed with JavaScript enabled"
2791,how-speech-recognition-improves-customer-service-in-telecommunications,"Originally published at:			https://developer.nvidia.com/blog/how-speech-recognition-improves-customer-service-in-telecommunications/
The telecommunication industry has seen a proliferation of AI-powered technologies in recent years, with speech recognition and translation leading the charge. Multi-lingual AI virtual assistants, digital humans, chatbots, agent assists, and audio transcription are technologies that are revolutionizing the telco industry. Businesses are implementing AI in call centers to address incoming requests at an accelerated…Powered by Discourse, best viewed with JavaScript enabled"
2792,generating-high-quality-labels-for-speech-recognition-with-label-studio-and-nvidia-nemo,"Originally published at:			Generating High-Quality Labels for Speech Recognition with Label Studio and NVIDIA NeMo | NVIDIA Technical Blog
Save time and produce a more accurate result when processing audio data with automated speech recognition (ASR) models from NVIDIA NeMo and Label Studio.Powered by Discourse, best viewed with JavaScript enabled"
2793,gtc-2020-gpu-accelerated-deep-learning-for-weather-climate-and-space,"GTC 2020 S21255
Presenters: David Hall,NVIDIA
Abstract
We’ll demonstrate how to use deep learning to tackle important challenges in weather forecasting, climate modeling, and the processing of satellite observations. We’ll present recent results from ongoing research collaborations with the National Oceanic and Atmospheric Administration, NASA, and various universities, and explain how accurate results were achieved. We’ll show how to automate feature detection to identify threats from severe weather, solar storms, and near-earth objects, and we’ll discuss how to accelerate weather/climate models and data assimilation techniques to produce more accurate predictions. Finally, we’ll illustrate how to better use satellite observations by enhancing, transforming, interpolating, fusing, and repairing multispectral data.Watch this session
Join in the conversation below.Powered by Discourse, best viewed with JavaScript enabled"
2794,nvidia-announces-tensorrt-6-breaks-10-millisecond-barrier-for-bert-large,"Originally published at:			NVIDIA Announces TensorRT 6; Breaks 10 millisecond barrier for BERT-Large | NVIDIA Technical Blog
Today, NVIDIA released TensorRT 6 which includes new capabilities that dramatically accelerate conversational AI applications, speech recognition, 3D image segmentation for medical applications, as well as image-based applications in industrial automation.  TensorRT is a high-performance deep learning inference optimizer and runtime that delivers low latency, high-throughput inference for AI applications.  With today’s release, TensorRT continues…Powered by Discourse, best viewed with JavaScript enabled"
2795,rtx-path-tracing-sdk-multi-gpu-support,"Hi, I was wandering if there are any plan to add support for DirectX 12 explicit multi-GPU support to the RTX Path Tracing SDK. To my knowledge, the only mGPU capable real-time raytracing and real-time path tracing engine is Nvidia Omniverse but, it can not be used to ship a packaged game and there are no games, to my knowledge, that support mGPU with even classic real-time raytracing. Will the RTX Path Tracing SDK change this in the future and bring mGPU support to real-time path tracing?
Thank youThis is something we could consider and can prioritize based on feedback! Main path tracing is relatively trivial to parallelize to multiple GPUs via tiling or similar approach, but denoising and the rest of the post process pipeline would take more effort.Use the Like button to indicate your interestPowered by Discourse, best viewed with JavaScript enabled"
2796,alibaba-s-alicloud-partners-with-nvidia-for-artificial-intelligence,"Originally published at:			Alibaba’s AliCloud Partners with NVIDIA for Artificial Intelligence | NVIDIA Technical Blog
Alibaba Group’s cloud computing business, AliCloud, signed a new partnership with NVIDIA to collaborate on AliCloud HPC, the first GPU-accelerated cloud platform for high performance computing (HPC) in China. AliCloud will work with NVIDIA to broadly promote its cloud-based GPU offerings to its customers — primarily fast-growing startups – for AI and HPC work. “Innovative companies…Powered by Discourse, best viewed with JavaScript enabled"
2797,gtc-2020-nvidia-vulkan-features-update-including-vulkan-1-2-and-ray-tracing,"GTC 2020 S21770
Presenters: Christoph Kubisch,NVIDIA
Abstract
Learn about new features available in Vulkan. We’ll cover various new Vulkan extensions and their use-cases. We’ll also address some of our open-source samples published so far.Watch this session
Join in the conversation below.Powered by Discourse, best viewed with JavaScript enabled"
2798,meet-the-researcher-weichung-wang-using-ai-for-intelligent-analytics-in-clinical-workflows,"Originally published at:			Meet the Researcher, Weichung Wang: Using AI for Intelligent Analytics in Clinical Workflows | NVIDIA Technical Blog
‘Meet the Researcher’ is a series in which we spotlight different researchers in academia who are using GPUs to accelerate their work. This week we spotlight Weichung Wang, Professor at the Institute of Applied Mathematical Sciences at the National Taiwan University. Wang is a recipient of the Nian-Chih Award and has also received the Outstanding…Powered by Discourse, best viewed with JavaScript enabled"
2799,nvail-partners-present-robotics-research-at-icra-2019,"Originally published at:			https://developer.nvidia.com/blog/nvail-partners-present-robotics-research-at-icra-2019/
This week top Robotics researchers have gathered in Montreal, Canada to present their cutting edge research.Powered by Discourse, best viewed with JavaScript enabled"
2800,pgi-community-edition-19-10-now-available,"Originally published at:			PGI Community Edition 19.10 Now Available | NVIDIA Technical Blog
New PGI Community Edition supports NVIDIA V100 Tensor Cores in CUDA Fortran, the full C++17 language, PCAST CPU/GPU auto-compare directives, OpenACC 2.6 and more.  PGI Compilers & Tools are for scientists and engineers developing high-performance computing (HPC) applications. PGI products deliver world-class multicore CPU performance, an easy on-ramp to GPU computing with OpenACC directives, and…Powered by Discourse, best viewed with JavaScript enabled"
2801,calibrating-stitched-videos-with-nvidia-vrworks-360-video-sdk,"Originally published at:			https://developer.nvidia.com/blog/calibrating-stitched-videos-with-nvidia-vrworks-360-video-sdk/
The quality of 360 videos varies widely depending on the resolution of the cameras used to capture them. The need for higher quality captures has given rise to many solutions, ranging from 3D printed rigs for GoPro cameras all the way up to professional custom-built 360 camera systems. One particularly difficult task is to align multiple videos from an array of cameras and stitch…Powered by Discourse, best viewed with JavaScript enabled"
2802,combating-adversarial-attacks-with-a-barrage-of-random-transforms-bart,"Originally published at:			Combating Adversarial Attacks with a Barrage of Random Transforms (BaRT) | NVIDIA Technical Blog
Wherever you look these days, you can find AI affecting your life in one way or another. Whether it’s the Netflix recommendation system or self driving cars, the use of deep learning is becoming ever more prevalent throughout our lives and is starting to make increasingly more crucial decisions. Since AI is becoming ingrained in…Powered by Discourse, best viewed with JavaScript enabled"
2803,fast-track-ai-model-adaptation-with-tao,"Originally published at:			https://developer.nvidia.com/blog/fast-track-ai-model-adaptation-with-tao/
NVIDIA Train, Adapt, and Optimize (TAO) is an AI model adaptation platform that simplifies and accelerates the creation of enterprise AI applications.Powered by Discourse, best viewed with JavaScript enabled"
2804,using-the-nvidia-cuda-stream-ordered-memory-allocator-part-1,"Originally published at:			https://developer.nvidia.com/blog/using-the-nvidia-cuda-stream-ordered-memory-allocator-part-1/
Most CUDA developers are familiar with the cudaMalloc and cudaFree API functions to allocate GPU accessible memory. However, there has long been an obstacle with these API functions: they aren’t stream ordered. In this post, we introduce new API functions, cudaMallocAsync and cudaFreeAsync, that enable memory allocation and deallocation to be stream-ordered operations. In part…Powered by Discourse, best viewed with JavaScript enabled"
2805,the-ama-is-now-over-thanks-for-everyone-for-participating,"Thanks everyone, Please feel free to continue to comment on the responses.
At some point we’ll move these conversations over to general forums.
Thanks again !Powered by Discourse, best viewed with JavaScript enabled"
2806,accelerate-machine-learning-with-the-cudnn-deep-neural-network-library,"Originally published at:			https://developer.nvidia.com/blog/accelerate-machine-learning-cudnn-deep-neural-network-library/
Machine Learning (ML) has its origins in the field of Artificial Intelligence, which started out decades ago with the lofty goals of creating a computer that could do any work a human can do.  While attaining that goal still appears to be in the distant future, many useful tools have been developed and successfully applied…Very exciting! Thanks Larry and nVidia! It will be interesting to see if there are any architectural changes that can be made to support deep learning and other new AI architectures.Awesome!Hi, all, there is a similar library that can be found from http://libnn.com. It is totally free.There is a caffe version, optimized for CPU (""openmp"" branch). Imagenet training on this CPU-version (MKL + openmp, dual-socket E5-2680 ) is < 2x ( not 11x! )  slower than caffe-GPU (cuBLAS, K40).This is awesome!This is greatDr. Niu, why cuDNN is awesome?that's great!Hi Larry ,I want to register and download cuDNN but I could not be able to download. When I pressed on register then submit no thing is happen and when I pressed the downlaod I recieved this message ""n Error message You do not have permission to view this form."". So please any help.Dear Larry and othersI want to register and download cuDNN but I could not be able to download or register. When I had pressed on register then submit no thing was happen and when I pressed the download I received this message ""n Error message You do not have permission to view this form."". So please any help.Best regards,SalemIt turns out that cuDNN comes with windows version. My windows porting of Caffe can be hopefully accelerated as well.Given that cuDNN seems to be about adding DNN primitives, what exactly can be expect of the ""support for splitting computation across multiple GPUs on the same node""?What sort of computations will be split?It would be great if the library worked with the Jetson TK1 board. Are there any plans to provide binaries for ARM?is there ANY working example of this thing ? ANY documentation (besides the PDF file bundled with the library) ?It would be great if the library worked.=)Stay tuned...all I can say is you won't have to wait too long...What are you trying to accomplish?  You can post questions on the NVIDIA Developer Forums and we will do our best to answer and help.  cuDNN is integrated with development branch of CAFFE right now, and you should be able to post on the CAFFE forums to get help with that.  Once CAFFE v1.0 is officially launched, there will be easy to follow instructions on how to enable cuDNN.  cuDNN is also rapidly being incorporated into other frameworks as well.  The cuDNN User Guide and the article are what exists at the moment, but that seems to be enough for many folks to do successful integration.Those folks are much smarter then me. I am just a humble developer trying to see if this library is of any use for me. I am trying to have Boltzmann machine running on a GPU cluster, but convolutional network is also great.ANY working examples anyone ?Powered by Discourse, best viewed with JavaScript enabled"
2807,nvidia-healthcare-to-host-clara-developer-day-at-gtc-2020,"Originally published at:			NVIDIA Healthcare to Host Clara Developer Day at GTC 2020 | NVIDIA Technical Blog
At GTC 2020 in Silicon Valley, NVIDIA’s team of healthcare software experts will walk you through tools for federated learning, medical imaging, genomics, and natural language processing (NLP).Powered by Discourse, best viewed with JavaScript enabled"
2808,meet-the-researcher-lorenzo-baraldi-artificial-intelligence-for-vision-language-and-embodied-ai,"Originally published at:			Meet the Researcher: Lorenzo Baraldi, Artificial Intelligence for Vision, Language and Embodied AI | NVIDIA Technical Blog
‘Meet the Researcher’ is a monthly series in which we spotlight different researchers in academia who are using NVIDIA technologies to accelerate their work. This month, we spotlight Lorenzo Baraldi, Assistant Professor at the University of Modena and Reggio Emilia in Italy. Before working as a professor, Baraldi was a research intern at Facebook AI…Powered by Discourse, best viewed with JavaScript enabled"
2809,deep-speech-accurate-speech-recognition-with-gpu-accelerated-deep-learning,"Originally published at:			https://developer.nvidia.com/blog/deep-speech-accurate-speech-recognition-gpu-accelerated-deep-learning/
Speech recognition is an established technology, but it tends to fail when we need it the most, such as in noisy or crowded environments, or when the speaker is far away from the microphone. At Baidu we are working to enable truly ubiquitous, natural speech interfaces. In order to achieve this, we must improve the…Is there any APIs available for developers to start experimenting/using Deep Speech?Hi Anuroop - We are working on making a public API for Deep Speech. We'll make an announcement when it is ready. Thanks for your interest.Any update and/or timeline?Hey Pat, Not 1000% sure but I think https://api.ai/ may be helpful for you hereIs there any way to know more specific about this project?I mean workloads, hardware you use, amount of data and how long did it take to complete process successfully?What is the biggest speech\text database available?Powered by Discourse, best viewed with JavaScript enabled"
2810,world-s-fastest-commercial-drone-powered-by-jetson-tx1,"Originally published at:			World’s Fastest Commercial Drone Powered by Jetson TX1 | NVIDIA Technical Blog
Records were made to be broken, and drones are no exception. Teal Drones unveiled its first production product, Teal, a Jetson TX1-powered drone capable of flight speeds in excess of 70 mph. That makes Teal the world’s fastest production drone.  But Teal is as much a flying supercomputer platform as it is “just” a drone.…Powered by Discourse, best viewed with JavaScript enabled"
2811,implementing-stochastic-levels-of-detail-with-microsoft-directx-raytracing,"Originally published at:			Implementing Stochastic Levels of Detail with Microsoft DirectX Raytracing | NVIDIA Technical Blog
Level-of-detail (LOD) refers to replacing high-resolution meshes with lower-resolution meshes in the distance, where details may not be significant. This technique can help reduce memory footprint and geometric aliasing. Most importantly, it has long been used to improve rasterization performance in games. But does that apply equally to ray tracing? The render time for rasterization…I got a crash right away trying to run this sample.DxrLod.exe!DxrLod::UpdateConstantParamsBuffer() Line 1304	C++
DxrLod.exe!DxrLod::InitShaders() Line 785	C++
DxrLod.exe!DxrLod::PopulateCommandList() Line 800	C++
DxrLod.exe!DxrLod::OnRender() Line 260	C++
DxrLod.exe!Win32Application::WindowProc(HWND__ * hWnd, unsigned int message, unsigned int64 wParam, int64 lParam) Line 137	C++
[External Code]	
DxrLod.exe!Win32Application::Run(DXSample * pSample, HINSTANCE * hInstance, int nCmdShow) Line 70	C++
DxrLod.exe!WinMain(HINSTANCE * hInstance, HINSTANCE__ * __formal, char * __formal, int nCmdShow) Line 19	C++
[External Code]This turned out to be an application bug. The application worked initially, but then a later driver update introduced an optimization that exposed the bug. The code/executable has been updated.Powered by Discourse, best viewed with JavaScript enabled"
2812,more-science-less-programming-with-free-openacc-online-course,"Originally published at:			More Science, Less Programming with FREE OpenACC Online Course | NVIDIA Technical Blog
Interactive lectures, hands-on labs, and live office hours. Learn everything you need to start accelerating your code on GPUs and CPUs. Join HPC industry’s OpenACC experts for a free online course. This course is comprised of four instructor-led classes that include interactive lectures, hands-on exercises, and office hours with the instructors. You’ll learn everything you…Powered by Discourse, best viewed with JavaScript enabled"
2813,detecting-threats-faster-with-ai-based-cybersecurity,"Originally published at:			Detecting Threats Faster with AI-Based Cybersecurity | NVIDIA Technical Blog
The latest release of NVIDIA Morpheus includes new visualization capabilities enabling cybersecurity analysts to more quickly pinpoint and react to threats.Powered by Discourse, best viewed with JavaScript enabled"
2814,nvidia-announcements-at-the-2016-gpu-technology-conference,"Originally published at:			https://developer.nvidia.com/blog/nvidia-announcements-at-2016-gpu-technology-conference/
If you missed the opening keynote by NVIDIA CEO Jen-Hsun Huang, here’s a recap of today’s announcements at the GPU Technology Conference in Silicon Valley. In the video, NVIDIA VP Greg Estes covers the new NVIDIA SDK, Iray VR, Pascal-based Tesla P100 GPU, DGX-1 deep learning supercomputer in a box, and DriveWorks platform for self-driving…Powered by Discourse, best viewed with JavaScript enabled"
2815,openeye-scientific-s-omega-generates-3d-molecular-conformers-for-drug-design-30x-faster-with-nvidia,"Originally published at:			OpenEye Scientific’s OMEGA Generates 3D Molecular Conformers for Drug Design 30X Faster with NVIDIA | NVIDIA Technical Blog
Learn how the updated OpenEye OMEGA software uses NVIDIA GPUs for significantly faster conformer generation, with no loss in accuracy.Powered by Discourse, best viewed with JavaScript enabled"
2816,upcoming-webinar-a-deep-dive-into-monai,"Originally published at:			An Introduction to MONAI: The Essential Framework For The Medical AI Ecosystem
Join us on October 24 for a deep dive into MONAI, the essential framework for AI workflows in healthcare—including use cases, building blocks, and more.Powered by Discourse, best viewed with JavaScript enabled"
2817,top-smart-spaces-sessions-at-nvidia-gtc-2023,"Originally published at:			Developer Conference Session Catalog | NVIDIA GTC
Learn how AI is enabling safer, more sustainable cities and improving operational efficiency in public spaces for our communities.Powered by Discourse, best viewed with JavaScript enabled"
2818,explainer-what-is-a-machine-learning-model,"Originally published at:			What Is a Machine Learning Model? | NVIDIA Blogs
Fueled by data, ML models are the mathematical engines of AI, expressions of algorithms that find patterns and make predictions faster than a human can.Powered by Discourse, best viewed with JavaScript enabled"
2819,nvidia-certified-next-generation-computing-platforms-for-ai-video-and-data-analytics-performance,"Originally published at:			https://developer.nvidia.com/blog/nvidia-certified-next-generation-computing-platforms-for-ai-video-and-data-analytics-performance/
A new generation of computing technologies designed to address increasingly complex compute demands is emerging.Powered by Discourse, best viewed with JavaScript enabled"
2820,gtc-2020-inside-the-nvidia-hpc-sdk-the-compilers-libraries-and-tools-for-accelerated-computing,"GTC 2020 S21766
Presenters: Timothy Costa,NVIDIA
Abstract
NVIDIA opened the GPU to general-purpose programming with the release of CUDA in 2007. The CUDA programming model has steadily evolved to include support for C++, Fortran, Unified Memory, Tensor Cores and many other features which expand the base of applications that can be accelerated, simplify GPU programming and offer even better performance. In addition to proving effective as a parallel programming model, CUDA serves as a platform for building higher-level interfaces such as accelerated libraries, directive-based programming models, and GPU communication libraries like NCCL and NVSHMEM. In this talk we’ll introduce the NVIDIA HPC SDK, a comprehensive, integrated suite of compilers, libraries and tools for the NVIDIA HPC Platform with new developments that continue to open GPU computing to a wider audience of developers and users, including automatic acceleration and tensor core programmability in standard languages and novel libraries for compute and communication.Watch this session
Join in the conversation below.Powered by Discourse, best viewed with JavaScript enabled"
2821,streaming-and-monitoring-xr-experiences-using-nvidia-cloudxr-sdk-and-pcoip-solutions-from-teradici,"Originally published at:			https://developer.nvidia.com/blog/streaming-and-monitoring-xr-experiences-using-cloudxr-and-pcoip-solutions-from-teradici/
Discover how NVIDIA CloudXR with Teradici’s PCoIP protocol enables XR users to experience seamless streaming. CloudXR uses Teradici’s ability to support high-fidelity video and use of flexible integration options for enterprise IT systems.Powered by Discourse, best viewed with JavaScript enabled"
2822,integrating-ray-tracing-into-an-existing-engine-three-things-you-need-to-know,"Originally published at:			https://developer.nvidia.com/blog/integrating-ray-tracing-into-an-existing-engine-three-things-you-need-to-know/
In this video, Alex Dunn – Principal Devtech Engineer at NVIDIA, and Pawel Kozlowski – Developer Technology Engineer at NVIDIA, detail the three most important things developers need to know about integrating ray tracing into an existing engine. Watch below: 3: Ray Tracing Will Transform Your Engine Ray tracing introduces a couple of new concepts…Powered by Discourse, best viewed with JavaScript enabled"
2823,dlss-image-reconstruction-for-real-time-rendering-with-deep-learning,"Originally published at:			https://developer.nvidia.com/blog/dlss-image-reconstruction-for-real-time-rendering-with-deep-learning/
First presented as part of GTC Digital, NVIDIA Senior Research Scientist Edward Liu discussed the latest research progress on Deep Learning Super Sampling (DLSS), which uses deep learning and NVIDIA Tensor Cores to reconstruct super sampled frames in real-time. “Over the last year, we have improved every aspect of DLSS – from the training methodology,…Powered by Discourse, best viewed with JavaScript enabled"
2824,ai-can-smell-illnesses-in-human-breath,"Originally published at:			AI Can Smell Illnesses in Human Breath | NVIDIA Technical Blog
Researchers from Loughborough University, Western General Hospital, the University of Edinburgh, and the Edinburgh Cancer Centre in the United Kingdom, recently developed a deep learning-based method that can analyze compounds in the human breath and detect illnesses, including cancer, with better than-human average performance. “The sense of smell is used by animals and even plants…Powered by Discourse, best viewed with JavaScript enabled"
2825,ai-can-interpret-and-translate-american-sign-language-sentences,"Originally published at:			AI Can Interpret and Translate American Sign Language Sentences | NVIDIA Technical Blog
According to the World Health Organization (WHO), there are an estimated 360 million people worldwide with disabling hearing loss. To help with sign language translation,  Researchers from Michigan State University developed a deep learning-based system that can automatically interpret individual signs of the American Sign Language (ASL) as well as translate full ASL sentences without…Powered by Discourse, best viewed with JavaScript enabled"
2826,openai-creates-a-gym-to-train-your-ai,"Originally published at:			https://developer.nvidia.com/blog/openai-creates-a-gym-to-train-your-ai/
Open AI, a non-profit artificial intelligence research company backed by Elon Musk, launched a toolkit for developing and comparing reinforcement learning algorithms. OpenAI Gym is a suite of environments that include simulated robotic tasks and Atari games as well as a website for people to post their results and share code. OpenAI researcher John Schulman…Powered by Discourse, best viewed with JavaScript enabled"
2827,accelerating-lossless-gpu-compression-with-new-flexible-interfaces-in-nvidia-nvcomp,"Originally published at:			https://developer.nvidia.com/blog/accelerating-lossless-gpu-compression-with-new-flexible-interfaces-in-nvidia-nvcomp/
Use the high-level nvCOMP API for easy compression and decompression and the low-level API for more advanced workflows.Powered by Discourse, best viewed with JavaScript enabled"
2828,new-on-ngc-sdks-for-large-language-models-digital-twins-digital-biology-and-more,"Originally published at:			https://developer.nvidia.com/blog/new-on-ngc-sdks-for-llms-metaverse-computer-vision-and-more/
NVIDIA announces new SDKs available in the NGC catalog, a hub of GPU-optimized deep learning, machine learning, and HPC applications. With highly performant software containers, pretrained models, industry-specific SDKs, and Jupyter notebooks available, AI developers and data scientists can simplify and reduce complexities in their end-to-end workflows. This post provides an overview of new and…Powered by Discourse, best viewed with JavaScript enabled"
2829,upcoming-webinars-learn-about-the-new-features-of-jetpack-4-5-and-vpi-api-for-jetson,"Originally published at:			https://developer.nvidia.com/blog/jetpack-4-5-vpi-api-jetson-webinars/
JetPack SDK 4.5 is now available. This production release features enhanced secure boot, disk encryption, a new way to flash Jetson devices through Network File System, and the first production release of Vision Programming Interface.  For AI embedded and edge developers, the latest update for NVIDIA JetPack is available. It includes the first production release…Powered by Discourse, best viewed with JavaScript enabled"
2830,gtc-2020-hybrid-cloud-ecosystem-a-deep-dive-into-private-and-public-cloud-software-stacks,"GTC 2020 S22118
Presenters: Edward Richards,NVIDIA
Abstract
We’ll cover how NVIDIA GPUs are used in virtual machines and containers on private and public clouds. We’ll explain the different services and what their software stack looks like and how to best use them. Kubernetes is a common container orchestrator among them, but how each cloud deploys the Kubernetes pods is slightly different for each vendor. We’ll explore how GPU workloads execute, either within a container created in a virtual machine running on a hypervisor or created directly on top of the base operating system.Watch this session
Join in the conversation below.Powered by Discourse, best viewed with JavaScript enabled"
2831,building-an-intelligent-cockpit-with-nvidia-drive-ix,"Originally published at:			https://developer.nvidia.com/blog/building-an-intelligent-cockpit-with-nvidia-drive-ix/
The transition to intelligent vehicles is more than just autonomous driving — it extends to the in-cabin experience as well. With centralized, high-performance compute and open, scalable software platforms, developers can build a wide variety of innovative applications for a safer, more convenient ride for all occupants. NVIDIA DRIVE IX is an open software platform…Powered by Discourse, best viewed with JavaScript enabled"
2832,yamaha-motor-selects-nvidia-jetson-agx-xavier,"Originally published at:			https://developer.nvidia.com/blog/yamaha-motor-selects-nvidia-jetson-agx-xavier/
Yamaha Motor just announced they selected the NVIDIA Jetson AGX Xavier platform as the development system to power their upcoming lineup of autonomous machines in agriculture, logistics, marine products, and last mile-transportation. NVIDIA Founder and CEO Jensen Huang made the announcement at GTC Japan in Tokyo. “At Yamaha we want our products to move people,…Powered by Discourse, best viewed with JavaScript enabled"
2833,accelerating-cloud-native-applications-at-china-mobile-bigcloud,"Originally published at:			http://127.0.0.1:8089/accelerating-cloud-native-applications-at-china-mobile-bigcloud/
China Mobile uses NVIDIA technology to overcome network performance problemsPowered by Discourse, best viewed with JavaScript enabled"
2834,accelerating-interpretable-machine-learning-for-diversified-portfolio-construction,"Originally published at:			https://developer.nvidia.com/blog/accelerating-interpretable-machine-learning-for-diversified-portfolio-construction/
Learn how Munich Re markets developed an interpretable machine learning to increase performance on diversified portfolio construction.Powered by Discourse, best viewed with JavaScript enabled"
2835,nvidia-captures-top-spots-on-world-s-first-industry-wide-ai-benchmark,"Originally published at:			NVIDIA Captures Top Spots on World’s First Industry-Wide AI Benchmark | NVIDIA Technical Blog
Today, the MLPerf consortium published its first results for the seven tests that currently comprise this new industry-standard benchmark for machine learning. For the six test categories where NVIDIA submitted results, we’re excited to tell you that NVIDIA platforms have finished with leading single-node and at-scale results for all six, a testament to our total…Powered by Discourse, best viewed with JavaScript enabled"
2836,building-a-question-and-answering-service-using-natural-language-processing-with-nvidia-ngc-and-google-cloud,"Originally published at:			https://developer.nvidia.com/blog/building-a-qa-service-using-nlp-with-ngc-and-google-cloud/
NVIDIA GTC provides training, insights, and direct access to experts. Join us for breakthroughs in AI, data center, accelerated computing, healthcare, game development, networking, and more. Invent the future with us: April 12-16, 2021. Enterprises across industries are leveraging natural language process (NLP) solutions—from chatbots to audio transcription—to improve customer engagement, increase employee productivity, and…I’m looking forward to learning how you are leveraging the content to create your own service deployed on Google Cloud AI Platform with Triton. Don’t feel obligated to follow the entire steps for your work. Use the last section with Triton to deploy the service if you had trained model somewhere else already, or use the first two parts to train your model on CAIP. Don’t forget that you can explore many GPU-optimized models across different domains at NGC which you can plug into the steps shared in this blog. Let us know of any questions or concerns.Powered by Discourse, best viewed with JavaScript enabled"
2837,artificial-intelligence-helps-the-blind-see-facebook,"Originally published at:			Artificial Intelligence Helps the Blind ‘See’ Facebook | NVIDIA Technical Blog
Today, Facebook introduced a new feature that automatically generates text descriptions of pictures using advanced object recognition technology. Until now, people using screen readers would only hear the name of the person who shared the photo, followed by the term “photo” when they came upon an image in News Feed. Now they will get a…Powered by Discourse, best viewed with JavaScript enabled"
2838,7-powerful-new-features-in-openacc-2-0,"Originally published at:			https://developer.nvidia.com/blog/7-powerful-new-features-openacc-2-0/
OpenACC is a high-level programming model for accelerators, such as NVIDIA GPUs, that allows programmers to accelerate applications using compiler directives to specify loops and regions of code in standard C, C++ and Fortran to be offloaded to an accelerator. Through the use of compiler directives, OpenACC allows programmers to maintain a single source code for the CPU and GPU…The fact the compilers are commercial isn't really gonna help its adoption much.Open compiler support is in progress: http://www.hpcwire.com/2013...Powered by Discourse, best viewed with JavaScript enabled"
2839,sc20-demo-accelerate-hpc-application-performance-with-nvtags,"Originally published at:			SC20 Demo: Accelerate HPC Application Performance with NVTAGS | NVIDIA Technical Blog
Many GPU-accelerated HPC applications spend a substantial portion of their time in non-uniform, GPU-to-GPU communications, resulting in an increased solution times.  To ensure that GPU-to-GPU communication is as efficient as possible for HPC applications with non-uniform communication, it is crucial that these applications make informed decisions when assigning MPI processes to GPUs, ensuring processes requiring…Powered by Discourse, best viewed with JavaScript enabled"
2840,accelerating-se-3-transformers-training-using-an-nvidia-open-source-model-implementation,"Originally published at:			https://developer.nvidia.com/blog/accelerating-se3-transformers-training-using-an-nvidia-open-source-model-implementation/
SE(3)-Transformers are versatile graph neural networks unveiled at NeurIPS 2020. NVIDIA just released an open-source optimized implementation that uses 9x less memory and is up to 21x faster than the baseline official implementation. SE(3)-Transformers are useful in dealing with problems with geometric symmetries, like small molecules processing, protein refinement, or point cloud applications. They can…A lot of work went into accelerating these equivariant neural networks, and I hope it will be useful to you. These networks are promising in a variety of physics- and biology-based problems, and their performance should not be an issue preventing them from being used!If you have any questions or comments, let us know.Powered by Discourse, best viewed with JavaScript enabled"
2841,robotics-at-gtc-jetson-tutorials-ai-in-stem-and-commercial-apps,"Originally published at:			Robotics at GTC: Jetson tutorials, AI in STEM, and Commercial Apps | NVIDIA Technical Blog
From Jetson 101 fundamental walk-throughs, to technical deep dive tutorials, GTC is hosting over 1,400 sessions for all technical abilities and applications. Free registration provides access to topic experts, meet-and-greet networking events, and a keynote loaded with breakthrough announcements from NVIDIA CEO Jensen Huang.  If you’re looking for a curated list of Edge AI sessions…Powered by Discourse, best viewed with JavaScript enabled"
2842,cleaning-up-radioactive-waste-from-world-war-ii-with-supercomputing,"Originally published at:			https://developer.nvidia.com/blog/cleaning-up-radioactive-waste-from-world-war-ii-with-supercomputing/
The Handford site in southeastern Washington is the largest radioactive waste site in the United Sates and is still awaiting cleanup after more than 70 years. Cleaning up radioactive waste is extremely complicated since some elements stay radioactive for thousands of years. Scientists from Lawrence Berkeley National Laboratory and six universities: The State University of…Powered by Discourse, best viewed with JavaScript enabled"
2843,transferring-industrial-robot-assembly-tasks-from-simulation-to-reality,"Originally published at:			https://developer.nvidia.com/blog/transferring-industrial-robot-assembly-tasks-from-simulation-to-reality/
Simulation is an essential tool for robots learning new skills. These skills include perception (understanding the world from camera images), planning (formulating a sequence of actions to solve a problem), and control (generating motor commands to change a robot’s position and orientation).  Robotic assembly is ubiquitous in the automotive, aerospace, electronics, and medical device industries.…Powered by Discourse, best viewed with JavaScript enabled"
2844,nvidia-announces-the-transfer-learning-toolkit-and-ai-assisted-annotation-sdk-for-medical-imaging,"Originally published at:			https://developer.nvidia.com/blog/nvidia-announces-the-transfer-learning-toolkit-and-ai-assisted-annotation-sdk-for-medical-imaging/
The NVIDIA Transfer Learning Toolkit is now NVIDIA TAO Toolkit. Deep learning based annotation and segmentation can drastically speed up model development and medical image analysis. However, developing high performance and accurate deep neural networks from scratch is challenging and time-consuming. The sheer cost and quality of data sets needed are often two of the biggest…Powered by Discourse, best viewed with JavaScript enabled"
2845,researchers-harness-gans-for-super-resolution-of-space-simulations,"Originally published at:			https://developer.nvidia.com/blog/researchers-harness-gans-for-super-resolution-of-space-simulations/
Carnegie Mellon University and University of California researchers developed a deep learning model that upgrades cosmological simulations from low to high resolution, allowing scientists to create a complex simulated universe within a day.Powered by Discourse, best viewed with JavaScript enabled"
2846,gtc-2020-khronos-cross-platform-standards-update-vulkan-spir-v-openxr-gltf-and-opencl,"GTC 2020 S22158
Presenters: Neil Trevett,NVIDIA
Abstract
Discover how over 150 companies cooperate at the Khronos Group to create open, royalty-free standards that enable developers to access the power of the GPU to accelerate demanding compute, graphics, and AR/VR applications. This session includes the very latest updates on several Khronos cross-platform standards, including the new Analytic Rendering group for scientific visualization, OpenXR for portable AR and VR, the new-generation Vulkan GPU API, the SPIR-V standard intermediate language for parallel compute and graphics, glTF for efficient transmission of 3D assets, and OpenCL for parallel heterogeneous programming. The session also provides insights into how these open standards are supported across NVIDIA’s product familiesWatch this session
Join in the conversation below.Hi Neil,Thank you for the presentation on the new developments with Khronos. While it has been a couple months since you mentioned Nvidia’s support for OpenCL 1.2, I’m excited to learn Nvidia GPUs are beginning to support OpenCL 2.0 and wanted to confirm with you if thats actually the case? As shown in the TechPowerUp GPU Database Tesla A100 SXM4 and Quadro RTX 8200.Best Regards,
Abdo BabukrHi Abdo - thank you for the question. NVIDIA is planning to implement and ship OpenCL 3.0 after the final specification is released by the Khronos Group. Any updates will be posted here: OpenCL Overview - The Khronos Group Inc. Let me know if you have any more detailed questions - happy to try and answer if I can!Best regards,
NeilHi Neil,Exciting news! It’s amazing to see this portability continue to mature in the GPU space. Can’t wait!Best regards,
AbdoPowered by Discourse, best viewed with JavaScript enabled"
2847,developer-blog-improving-computer-vision-with-nvidia-a100-gpus,"Originally published at:			Improving Computer Vision with NVIDIA A100 GPUs | NVIDIA Technical Blog
During the 2020 NVIDIA GPU Technology Conference keynote address, NVIDIA founder and CEO Jensen Huang introduced the new NVIDIA A100 GPU based on the NVIDIA Ampere GPU architecture. In this post, we detail the exciting new features of the A100 that make NVIDIA GPUs an ever-better powerhouse for computer vision workloads. We also showcase two recent CV…Powered by Discourse, best viewed with JavaScript enabled"
2848,mixed-precision-resnet-50-using-tensor-cores-with-tensorflow,"Originally published at:			Mixed-Precision ResNet-50 Using Tensor Cores with TensorFlow | NVIDIA Technical Blog
Mixed-Precision combines different numerical precisions in a computational method. Using precision lower than FP32 reduces memory usage, allowing deployment of larger neural networks. Data transfers take less time, and compute performance increases, especially on NVIDIA GPUs with Tensor Core support for that precision. Mixed-precision training of DNNs achieves two main objectives: Decreases the required amount of…thanks for this tutorial, great video.Hi Shiva, I was looking at sparse_softmax_cross_entropy_with_logits and it seems like they actually automatically upcast fp16 tensors to fp32. I filed an issue for this - feels very sneaky behavior. https://github.com/tensorfl...Thus, might no longer need the logits upcast that you suggested.Powered by Discourse, best viewed with JavaScript enabled"
2849,estimating-visual-odometry-with-prerecorded-stereo-images-and-the-nvidia-isaac-sdk,"Originally published at:			https://developer.nvidia.com/blog/estimating-visual-odometry-with-prerecorded-stereo-images-and-the-nvidia-isaac-sdk/
The NVIDIA Isaac SDK includes a production-ready, real-time visual odometry solution. According to the KITTI visual odometry algorithm comparison benchmark, it is 11th for accuracy and the fastest for performance. Figure 1. Performance of Isaac SDK visual odometry solution, #32 IsaacElbrus. Figure 2 shows that Isaac provides better speed and accuracy than the widely used…I have tried the tutorial however, it does not publish anything on the Isaac websight tool. I also check via the “lsof -i -P -n | grep LISTEN” command if TCP port 3000 is being used, however, its not being used. Can you explain what the problem might be? I am able to view output on Isaac websight tool from applications for e.g. the “apps/samples/stereo_dummy”.The “sight” module is missing from both BUILD and json file which causes this problem - I was able to solve itI am trying to output (print on console) the Odom pose.x, pose.y and pose.z values for the KITTI dataset, but I get 0 for x,y and z, although I can see the values change on a plot on Isaac Websight. Below is my code - can you tell me what I am doing wrong? The below code snippet is included within the tick() function.ThanksPose3d robo_pose;
::Pose3dProto::Reader reader;
robo_pose.rotation = FromProto(reader.getRotation());
robo_pose.translation = FromProto(reader.getTranslation());
LOG_INFO(“X is %f”, robo_pose.translation);
LOG_INFO(“Y is %f”, robo_pose.translation);
LOG_INFO(“Z is %f”, robo_pose.translation);NOTE: If i try to use the “reader = rx_left_camera_vio_pose().getProto();” it gives me the error shown below.

image1454×329 37.3 KB
Powered by Discourse, best viewed with JavaScript enabled"
2850,machine-learning-in-practice-build-an-ml-model,"Originally published at:			https://developer.nvidia.com/blog/machine-learning-in-practice-build-an-ml-model/
This series looks at the development and deployment of machine learning (ML) models. In this post, you train an ML model and save that model so it can be deployed as part of an ML system. Part 1 gave an overview of the ML workflow, considering the stages involved in using machine learning and data science…Powered by Discourse, best viewed with JavaScript enabled"
2851,ai-technology-automatically-records-soccer-matches,"Originally published at:			AI Technology Automatically Records Soccer Matches | NVIDIA Technical Blog
A Copenhagen startup developed a deep learning-based camera system that can detect where the action is on the soccer field, and automatically zoom and follow the ball – just like how a camera operator would do. “Today, less than 1 percent of all football matches are recorded,” says Veo co-founder and CEO Henrik Teisbæk. “This…Powered by Discourse, best viewed with JavaScript enabled"
2852,generating-photorealistic-images-of-fake-celebrities-with-artificial-intelligence,"Originally published at:			Generating Photorealistic Images of Fake Celebrities with Artificial Intelligence | NVIDIA Technical Blog
Researchers from NVIDIA recently published a paper detailing their new methodology for generative adversarial networks (GANs) that generated photorealistic pictures of fake celebrities. One of the hottest topics in deep learning is GANs, which have the potential to create systems that learn more with less help from humans. Rather than train a single neural network…Powered by Discourse, best viewed with JavaScript enabled"
2853,share-your-science-mapping-the-earth-s-interior-with-gpus,"Originally published at:			Share Your Science: Mapping the Earth’s Interior with GPUs | NVIDIA Technical Blog
Jeroen Tromp, Professor at Princeton University shares how his team is using the Tesla GPU-accelerated Titan Supercomputer at Oak Ridge National Laboratory to image the earth’s interior on a global scale. Tromp and his team are simulating seismic wave propagation by analyzing hundreds of earthquakes recorded by thousands of stations across the world to create…Powered by Discourse, best viewed with JavaScript enabled"
2854,gtc-explore-600-sessions-connect-with-global-experts,"Originally published at:			GTC: Explore 600+ Sessions - Connect with Global Experts | NVIDIA Technical Blog
NVIDIA’s GPU Technology Conference (GTC) is the must attend digital event for developers, researchers, engineers, and innovators looking to enhance their skills, exchange ideas, and gain a deeper understanding of how AI will transform their work. Discover the latest breakthroughs in AI, HPC, graphics, data science and more.  Starting on October 5 – October 9, 2020,…Powered by Discourse, best viewed with JavaScript enabled"
2855,free-chapters-from-the-upcoming-ray-tracing-gems-ii-every-week-in-july,"Originally published at:			https://developer.nvidia.com/blog/free-chapters-from-the-upcoming-ray-tracing-gems-ii-every-week-in-july/
Every Wednesday in July, we are making pre-print PDFs of full chapters from the upcoming book Ray Tracing Gems II available free for download.Powered by Discourse, best viewed with JavaScript enabled"
2856,gtc-2020-jetson-development-tips-tricks-and-avoiding-pitfalls,"GTC 2020 S22653
Presenters: Jim Benson,JetsonHacks
Abstract
Getting started with Jetson Development, or want to know some extra pearls of wisdom? Here are lessons learned over the past 5 years of Jetson Development at JetsonHacks that can save you a lot of time, energy and effort. Setting up your Jetson thoughtfully and leveraging the large number of libraries and resources freely available makes your development life much easier.Watch this session
Join in the conversation below.Powered by Discourse, best viewed with JavaScript enabled"
2857,deep-speech-by-baidu-now-recognizes-mandarin,"Originally published at:			https://developer.nvidia.com/blog/deep-speech-by-baidu-now-recognizes-mandarin/
Chinese search giant Baidu recently presented a new GPU-based Deep Speech deep learning system that has 94% accuracy when handling voice queries in Mandarin. Originally unveiled in December 2014, the speech recognition system was only able to recognize the English language. Baidu senior research engineer Awni Hannun was interviewed by Medium to share why Mandarin…Powered by Discourse, best viewed with JavaScript enabled"
2858,modeling-fruit-fly-brains-to-better-understand-alzheimer-s,"Originally published at:			Modeling Fruit Fly Brains to Better Understand Alzheimer’s | NVIDIA Technical Blog
Researchers at the University of Sheffield (UK) and Columbia University (USA) have launched a GPU-accelerated project to simulate a complete model of the adult fruit fly brain for the first time. The project aims to develop a better understanding of Alzheimer’s disease and new drug targets. The team is developing an open software platform to…Powered by Discourse, best viewed with JavaScript enabled"
2859,nvidia-sharing-new-details-about-mesh-shading-at-siggraph-2019,"Originally published at:			NVIDIA Sharing New Details about Mesh Shading at SIGGRAPH 2019 | NVIDIA Technical Blog
For the first time, developers can have complete control over geometry processing. With programmable mesh shaders, NVIDIA reinvents the GPU rasterization pipeline. At SIGGRAPH 2019, as a practical application showcase, NVIDIA’s Rahul Sathe and Manuel Kraemer will use the advanced culling and level-of-detail techniques that were used in the making of the Asteroids demo. They…Powered by Discourse, best viewed with JavaScript enabled"
2860,getting-started-on-jetson-top-resources-from-gtc-21,"Originally published at:			https://developer.nvidia.com/blog/getting-started-on-jetson-top-resources-from-gtc-21/
Hands-on learning is key for anyone new to AI and robotics. Priced for everyone, the Jetson Nano Developer Kit is the best way to get started learning how to create AI projects.Powered by Discourse, best viewed with JavaScript enabled"
2861,multi-gpu-workflows-for-training-ai-models-in-academic-research,"Originally published at:			https://developer.nvidia.com/blog/multi-gpu-academic-research/
We highlight a few research areas by our NVAIL, NVIDIA’s academic partners, who are leveraging multi-GPU training in their research.Powered by Discourse, best viewed with JavaScript enabled"
2862,accelerating-autonomous-vehicle-safety,"Originally published at:			https://developer.nvidia.com/blog/accelerating-autonomous-vehicle-safety/
In the field of computer vision (CV), an ongoing challenge is to build systems that can extract meaningful information from images as humans do, like distinguishing a street sign from a stop sign, or identifying lane boundaries.  Evolving CV applications such as autonomous vehicles, like any deep learning problem, need a relevant dataset with which…Powered by Discourse, best viewed with JavaScript enabled"
2863,global-illumination-in-metro-exodus-an-artist-s-point-of-view,"Originally published at:			Global Illumination in Metro Exodus: An Artist’s Point of View | NVIDIA Technical Blog
Why is real-time raytracing the future of lighting in game development? Because it allows artists to create more realistic visuals while simplifying the content preparation process. Color bleeding is a byproduct of global illumination that makes scenes look more life-like. Global illumination is a technique made possible with real-time ray tracing. It simulates the way…Powered by Discourse, best viewed with JavaScript enabled"
2864,ai-transforms-recorded-soccer-games-into-3d-holograms,"Originally published at:			AI Transforms Recorded Soccer Games Into 3D Holograms | NVIDIA Technical Blog
With the FIFA World Cup kicking off in just a few days – do you ever wonder what it would be like to have Cristiano Ronaldo, Lionel Messi or Neymar play a match on your kitchen table? Researchers from the University of Washington, Facebook, and Google developed the first end-to-end deep learning-based system that can…Powered by Discourse, best viewed with JavaScript enabled"
2865,pgi-17-7-delivers-openacc-and-cuda-fortran-for-volta-gpus,"Originally published at:			PGI 17.7 Delivers OpenACC and CUDA Fortran for Volta GPUs | NVIDIA Technical Blog
PGI compilers & tools are used by scientists and engineers who develop applications for high-performance computing (HPC) systems. They deliver world-class multicore CPU performance, an easy on-ramp to GPU computing with OpenACC directives, and performance portability across all major HPC platforms. 17.7 is available now for users with current PGI Professional support. Performance measured August,…Powered by Discourse, best viewed with JavaScript enabled"
2866,vrworks-audio-dials-up-the-immersion-with-rtx-acceleration,"Originally published at:			https://developer.nvidia.com/blog/vrworks-audio-dials-up-the-immersion-with-rtx-acceleration/
The field of Virtual Reality (VR) and its usage has evolved significantly over the past years. Technical advances in the field of VR continue to provide an ever-increasing degree of immersion, thus enabling its proliferation into new use cases and verticals. Today, VR is used not only to provide interactive gaming experience but also immersion…Powered by Discourse, best viewed with JavaScript enabled"
2867,gancraft-turning-gamers-into-3d-artists,"Originally published at:			https://developer.nvidia.com/blog/gancraft-turning-gamers-into-3d-artists/
GANcraft is a hybrid neural rendering pipeline to represent large and complex scenes using Minecraft.Powered by Discourse, best viewed with JavaScript enabled"
2868,nvidia-sponsors-learning-to-run-ai-competition-at-nips-2017,"Originally published at:			NVIDIA Sponsors “Learning to Run” AI Competition at NIPS 2017 | NVIDIA Technical Blog
Participants in the Neural Information Processing Systems (NIPS) conference “Learning to Run” competition are vying for the chance to win an NVIDIA DGX Station, the fastest personal supercomputer for researchers and data scientists. Using Deep Reinforcement Learning and open-source tools, competitors are tasked to see who can develop a controller to enable a physiologically-based human model…Powered by Discourse, best viewed with JavaScript enabled"
2869,supercharge-ai-powered-robotics-prototyping-and-edge-ai-applications-with-the-jetson-agx-orin-developer-kit,"Originally published at:			https://developer.nvidia.com/blog/supercharge-ai-powered-robotics-prototyping-and-edge-ai-applications-with-the-jetson-agx-orin-developer-kit/
The Jetson AGX Orin Developer Kit offers 8X the performance of the last generation, offering the most powerful AI supercomputer for advanced robotics, and embedded and edge computing.That dev kit price 😢😢😢Another product from NVIDIA priced way out of range of most experimenters.Powered by Discourse, best viewed with JavaScript enabled"
2870,adapting-llms-to-downstream-tasks-using-federated-learning-on-distributed-datasets,"Originally published at:			https://developer.nvidia.com/blog/adapting-llms-to-downstream-tasks-using-federated-learning-on-distributed-datasets/
Learn how LLMs can be adapted to downstream tasks using distributed datasets and federated learning to preserve privacy and enhance model performance.Powered by Discourse, best viewed with JavaScript enabled"
2871,video-series-path-tracing-for-quake-ii-in-two-months,"Originally published at:			Video Series: Path Tracing for Quake II in Two Months | NVIDIA Technical Blog
You wouldn’t know Quake II is now more than 20 years old when looking at the new RTX version. Path-traced reflections, shadows, and dynamic light sources bring the game’s cavernous environments to life. These new lighting techniques produce a more grounded and convincing aesthetic than the fully rasterized look we’ve all become accustomed to in modern…Powered by Discourse, best viewed with JavaScript enabled"
2872,gtc-2020-nvidia-tensorrt-workflows-importing-from-frameworks-into-your-inference-solution,"GTC 2020 CWE21825
Presenters: Craig-Wittenbrink,NVIDIA; Pravnav-Marathe, NVIDIA; Rajeev-Rao, NVIDIA; Kevin-Chen, NVIDIA; Dilip Sequeira, NVIDIA
Abstract
TensorRT Inference Library is most easily used by importing trained models through ONNX. In this session, we plan to go over fundamentals of the workflow to import and put into production deep learning models using TensorRT’s Parsing. We’ll discuss end-to-end solutions from training to export and import into TensorRT and deployment with TensorRT-Inference Server.Watch this session
Join in the conversation below.Powered by Discourse, best viewed with JavaScript enabled"
2873,production-deep-learning-inference-with-tensorrt-inference-server,"Originally published at:			Production Deep Learning Inference with TensorRT Inference Server | NVIDIA Technical Blog
In the video below, watch how TensorRT Inference server can improve deep learning inference performance and production data center utilization. TensorRT inference server: Simplifies deploying AI inference.Maximizes GPU utilization with concurrent execution of AI modelsIncreases high inference throughout and scales to peak loads Whether it’s performing object detection in images or video, recommending restaurants, or…Powered by Discourse, best viewed with JavaScript enabled"
2874,introducing-nvidia-jarvis-a-framework-for-gpu-accelerated-conversational-ai-applications,"Originally published at:			https://developer.nvidia.com/blog/introducing-jarvis-framework-for-gpu-accelerated-conversational-ai-apps/
Real-time conversational AI is a complex and challenging task. To allow real-time, natural interaction with an end user, the models need to complete computation in under 300 milliseconds. Natural interactions are challenging requiring multimodal sensory integration. Model pipelines are also complex and require coordination across multiple services: Automatic speech recognition (ASR)Natural language understanding (NLU)Domain-specific fulfillment…When will Jarvis be publicly available?When will Jarvis be publicly available?Thank you for your interest in Jarvis. At GTC 2020, NVIDIA announced Jarvis public beta for building and deploying conversational AI applications. You can sign up for Jarvis beta and you will notified via email once Jarvis is available for download.helloq1- does SDK for MISTY included in jarvis SDK.
(voice to face)=(text to speech to 3d model)=(controlling movements of a 3 d model with text input, speech)  =(the stuff that makes MISTY facial movements)q2- i have a few 3D modelling software that i can build my model. can i use them and export the model to (SOMETHING in jarvis sdk) that MISTY uses?q3- is there any specific program/application that generates model to be used in MISTY? is it included un jarvis sdk? else, should i buy it? i prefer not to. learning maya and strata did waste enough time, not so enthusiastic to learn another 3d app.Powered by Discourse, best viewed with JavaScript enabled"
2875,fast-flexible-allocation-for-nvidia-cuda-with-rapids-memory-manager,"Originally published at:			https://developer.nvidia.com/blog/fast-flexible-allocation-for-cuda-with-rapids-memory-manager/
When I joined the RAPIDS team in 2018, NVIDIA CUDA device memory allocation was a performance problem. RAPIDS cuDF allocates and deallocates memory at high frequency, because its APIs generally create new Series and DataFrames rather than modifying them in place. The overhead of cudaMalloc and synchronization of cudaFree was holding RAPIDS back. My first…Dear readers: RAPIDS Memory Manager is not just for RAPIDS – we designed it to be a general and flexible framework for efficient and flexible memory management for CUDA applications.  One of my favorite parts about the project is the ways in which it has been adapted for use with other CUDA-accelerated libraries and applications, such as Numba and CuPy, as well as all of the RAPIDS libraries.We look forward to hearing your questions and comments on this post and on RMM!Thanks,
MarkVery interesting and informative article. Many thanks!!Couple of questions if I may.thanks a lot
EyalHi Eyal,I’m not sure I perfectly follow your questions, but:Hi @Mark_Harris,
Thanks a lot for the prompt answer.
In a previous project I was involved, we used pinned memory instead of regular malloced host memory.
On a IBM machine, along with NVLink, we saw very nice speedups, up to about 50GB/s from pinned to device, if I recall correctly. Intel showed nice speedups as well.We used CUB’s allocator with some changes to make use of pinned memory. We had lot of obstacles mainly because we had to deal with many different scenarios, the amount of allocations, the variance between them (from 1K to 3GB), NUMA stuff, fragmentation etc.Since we were based on CUB’s implementation, the allocation was using a binning mechanism. However, we too, kinda reached the conclusion that maybe a hybrid solution would work best.What I asked was whether you have tested the RMM for  pinned memory and have insights as to how well RMM pools/approach would work with pinned memory.As for the large allocations, say we had to allocate 10 distinct 1GB allocations, that takes time, causes fragmentation, NUMA stuff etc… but then the work that required the 10 1GB buffers ended, and another one came along which required 20 buffers of 512MB, or 5 of 2GB…what then? you split the bin? split the allocation? free and re-allocate (which would take tons of time)… and above all NUMA was a pain in the …Thanks
EyalLet me clarify terms first. By “pinned” you mean “pinned host” memory. (There is such a thing as pinned device memory – that’s what cudaMalloc allocates, and it is different from pageable memory as allocated using cudaMallocManaged.)In any case, we have not yet provided a pool for pinned host memory because we didn’t want to have to rewrite our pool machinery. However with the current interface design, we cannot reuse pool_memory_resource for host memory because it is a device_memory_resource.But I have good news. We are, as I type this, beginning a redesign of our base MR interfaces to enable reuse of allocator machinery for different kinds of memory. Stay tuned.MarkThanks @Mark_Harris. Yes I was referring to “pinned host” memory.
Looking forward to the new interfaces then :)thanks
EyalThanks for nice posting.
I have some questions.For rmm::device_vector or rmm::device_uvector, what is the best way to retrieve the data to the host memory? It seems value() function of rmm::device_scalar supports it but the other two do not have it. cudaMemcpy would be the only way?vector of vector (or matrix-like data structure) can be realized with rmm? I am afraid not tho…Great questions!To retrieve the whole vector or part of it, you could use cudaMemcpy or cudaMemcpyAsync, or thrust::copy. For the latter, I think you’ll need to be careful to use the appropriate pointer types so that Thrust can figure out the memory kinds for the the source and destination. Copying to/from host is something I ran into recently and I agree we could add better support for this.Like std::vector, RMM vector classes are not really designed to represent matrices. Technically rmm::device_vector is just a thrust::vector. And device_uvector only supports trivially moveable/copyable types, so it’s probably not possible to have a uvector of uvectors!Thank you so much!Powered by Discourse, best viewed with JavaScript enabled"
2876,speedgate-world-s-first-sport-generated-by-ai,"Originally published at:			Speedgate: World’s First Sport Generated by AI | NVIDIA Technical Blog
If you like playing or watching team sports, you’ll probably find this AI’s latest creation fascinating. Developers from AKQA, a global innovation agency most known for working with some of the hottest brands and public figures, trained a recurrent neural network and a deep convolutional generative adversarial network on over 400 sports with the aim…Powered by Discourse, best viewed with JavaScript enabled"
2877,accelerating-intelligent-video-analytics-with-transfer-learning-toolkit,"Originally published at:			Accelerating Intelligent Video Analytics with Transfer Learning Toolkit | NVIDIA Technical Blog
Over the past several years, NVIDIA has been developing solutions to make AI and its benefits accessible to every industry. NVIDIA Transfer Learning Toolkit specifically allows developers looking into faster implementation of Intelligent Video Analytics (IVA) systems use deep learning and take their application from prototype to production in the fastest and most efficient way. Neural networks…hello, i have some questions:How to tune my hyperparameter better?Powered by Discourse, best viewed with JavaScript enabled"
2878,explore-the-latest-nvidia-siggraph-breakthroughs-with-the-demos,"Originally published at:			https://developer.nvidia.com/blog/explore-the-latest-nvidia-siggraph-breakthroughs-with-the-demos/
From award-winning research demos to photorealistic graphics created with NVIDIA RTX and Omniverse, see how NVIDIA is breaking boundaries in AI, graphics, and virtual collaboration.Powered by Discourse, best viewed with JavaScript enabled"
2879,share-your-science-scalable-molecular-dynamics,"Originally published at:			Share Your Science: Scalable Molecular Dynamics | NVIDIA Technical Blog
Jim Phillips, Senior Research Programmer at University of Illinois at Urbana-Champaign is using the Tesla-accelerated supercomputers, Titan and Blue Waters for his parallel molecular dynamics code, NAMD, designed for high-performance simulation of large biomolecular systems.   Watch Jim’s talk “Petascale Biomolecular Simulation with NAMD on Titan, Blue Waters, and Summit” in the NVIDIA GPU Technology…Powered by Discourse, best viewed with JavaScript enabled"
2880,explore-and-test-experimental-models-for-dlss-research,"Originally published at:			https://developer.nvidia.com/blog/explore-and-test-experimental-models-for-dlss-research/
Developers are encouraged to download, explore, and evaluate experimental AI models for Deep Learning Super Sampling.Powered by Discourse, best viewed with JavaScript enabled"
2881,behind-the-scenes-of-tencent-s-synced-off-planet,"Originally published at:			Behind the scenes of Tencent’s SYNCED: OFF-PLANET | NVIDIA Technical Blog
Tencent’s NEXT Studios is focused on delivering game experiences that take advantage of the most advanced graphics technology on the market. At GTC CHINA – taking place December 16-19 at Suzhou Jinji Lake International Conference Center – Yuan Xie (Tencent) and Mi Wang (Epic Games) will explain how SYNCED: Off-Planet, NEXT’s upcoming team shooter, will…Powered by Discourse, best viewed with JavaScript enabled"
2882,gtc-2019-silicon-valley-preview-ai-in-telecoms,"Originally published at:			GTC 2019 Silicon Valley Preview: AI In Telecoms | NVIDIA Technical Blog
NVIDIA GTC is the premier AI and deep learning conference, providing training, insights, and direct access to experts on telecommunications. To give you a preview of some of the conference sessions that you can attend, we’ve put together a list of some of the key sessions in each business area. The following video focuses on…Powered by Discourse, best viewed with JavaScript enabled"
2883,accelerating-matrix-multiplication-with-block-sparse-format-and-nvidia-tensor-cores,"Hi, my first suggestion is to take a look at the figure in the documentation https://docs.nvidia.com/cuda/cusparse/index.html#cusparse-generic-spmat-create-blockedell that highlights the memory layout. You can also find an example of usage in the Github samples page https://github.com/NVIDIA/CUDALibrarySamples/tree/master/cuSPARSE/spmm_blockedell. Finally, we are going to provide in the upcoming release a conversion routine from dense to BlockedEll by using the routine DenseToSparse https://docs.nvidia.com/cuda/cusparse/index.html#cusparse-generic-function-dense2sparseThank you! I have seen those resources, I guess what I am struggling with is how the column-idx array for Blocked-ELL is created, and the memory layout or the very small example doesn’t seem super sufficient to understand how I can take an existing dense matrix, write values in this blocked-format to a values array, and somehow generate the column indices for those.The most useful thing I found was the figure in this blog, any more context or open-source code that you have available that I can read/look through?Also looking forward to DenseToSparse support for ELL! Really cool stuff, thank you so much!Just to further add to that, even if you had a larger example hand-coded with a visualization to go along with it (like the one in the blog) that will be great!I also had some other questions:Again, thanks!thanks for the feedback. Batched Ell-SpMM is currently not supported. We will consider this feature in the future. While about bached SpMM for standard format, it is supported but the sample is not available. I will add it in the next few days.Hi @fbusato , I wonder do we support SpMM on transposed blocked-ell format as well?Hi. No, it is not supported. You can set the op(B) operation and change the layout of B and C matrices, but op(A) must be non-transposedI see, but in back-propagation, we need to transpose A, which is not necessarily a blocked-ellpack.transpose A is supported by all other formats for SpMMThe API reference guide for cuSPARSE, the CUDA sparse matrix library.Yeah I understand, but they do not support acceleration w/ TensorCore, is that correct?yes, TensorCore-Accelerated SpMM with op(A) == T is not available. Maybe you can take a look at cuSPARSELt cuSPARSELt: A High-Performance CUDA Library for Sparse Matrix-Matrix Multiplication — NVIDIA cuSPARSELt 0.3.0 documentation. Otherwise, the straightforward solution is to transpose A in a preprocessing stepThanks @fbusato , I think cuSPARSELt is a toolkit designed for leveraging Ampere architecture’s Sparse TensorCore (which requires a 2:4 sparse pattern) to accelerate general GeMM workloads (by pruning A to be 2:4 sparse).Transpose A would break the requirement that each row needs to have an equal number of blocks, but A^T is still a BSR matrix.btw, do you have any plans to accelerate SpmmBsr with TensorCore? I think it’s feasible but I don’t know how good it is compared to block-ellpack (which looks more load-balancing).Yes, we have a plan to accelerate BSR SpMM with Tensor Cores, probably in one of the next releases.awsome, looking forward to seeing that.Hi @fbusato ,I was evaluating the use of “cusparseSpMM”, coupled with the blocked ELLPACK format, to see if it was able to outperform the cuBLAS gemm counterpart.The results I’ve got in terms of performance are not what I expected. Let me present the problem setting first.My goal is to compute AB + C.Now, given A’s structure as well as the figures shown at Accelerating Matrix Multiplication with Block Sparse Format and NVIDIA Tensor Cores | NVIDIA Technical Blog, Figure 3, I was expecting cusparseSpMM + blocked ELL to yield a noticeable speedup w.r.t. cuBLAS gemm on the Tesla V100.Consequently, what I did was to compute the “sparse” multiplication with cusparseSpMM (and A compressed with the blocked ELL format), while the “dense” one was computed with cublasHgemm.Well, in the best case scenario, i.e., cusparseSpMM() + Blocked ELLPACK with block size = 32, I was able to get the same execution time to that I get from cublasHgemm.This is not what I expected, as the problem setting I’ve introduced above should heavily favor cusparseSpMM() + Blocked ELLPACK (and thus observe a noticeable speedup vs cublasHgemm).I have double checked the results generated both from cusparseSpMM and cublasHgemm, and they are the same, so it doesn’t appear I’m doing something wrong…it simply seems that cusparseSpMM() + Blocked ELLPACK is not delivering in terms of promised performance.Am I missing something?Thanks for any reply!Hi, my feeling is that you are doing everything in the right way. There are two points to consider for the performance.There is nothing wrong with these results. We didn’t claim that Blocked-ELL SpMM is faster than cuBLAS in all cases. We will optimize this functionality in the future but I don’t expect to substitute cuBLAS. The user should always consider both alternatives depending on the given problemHi All,
if you want to use Nvidia Tensor cores and CuBLAS implementation for Sparse Matrix times dense Matrix product, you may check our compression algorithm to build a sparse block format.
It is transparent and it basically works for any arbitrary sparse matrix in CSR format.
Paper → [2202.05868] Blocking Techniques for Sparse Matrix Multiplication on Tensor Accelerators
Code → GitHub - LACSFUB/SPARTA: SParse AcceleRation on Tensor Architecture
Please reach us if you will experience any issueHi. I am wondering is there any way to set a Stridedbatch to the Blocked_Ell format? Like ‘cusparseCsrSetStridedBatch’ or ‘cusparseCooSetStridedBatch’ API.
Thank you!Hi, unfortunately, we don’t provide batch computation for BlockedELL format at the moment. Is there any particular application that you want to support?Not particularly, I was just trying to apply to Transformer model which has block shaped sparse matrix like BlockedELL format. If I cannot use this BlockedELL format with batches, is there any other way to do block shaped sparse-dense matrix multiplication with batches?No, right now we support only unstructured sparsity for batched computation, e.g. CUDALibrarySamples/cuSPARSE/spmm_csr_batched at master · NVIDIA/CUDALibrarySamples · GitHub. Batched Block computation will be definitely considered in future releases.Powered by Discourse, best viewed with JavaScript enabled"
2884,ai-helps-detect-disaster-damage-from-satellite-imagery,"Originally published at:			https://developer.nvidia.com/blog/ai-helps-detect-disaster-damage-from-satellite-imagery/
Manually assessing damage in areas most affected by a disaster is challenging and time-consuming. To help produce more accurate and faster data for rescue workers and aid organizations, researchers from Facebook and CrowdAI developed a deep learning-based algorithm that can automatically estimate the level of damages an area has suffered. “The goal of this research…Powered by Discourse, best viewed with JavaScript enabled"
2885,gtc-2020-assessing-property-damage-with-ai,"GTC 2020 D2S18
Presenters: Tech Demo Team,NVIDIA
Abstract
The critical task of​​damage claim processing is typically labor-intensive and requires a significant amount of time. The deep learning tools within Esri ArcGIS sped up the process to provide ​aid to those affected by the Woolsey fire. This demo shows the workflow used; from training the deep learning model to inferring which automated the detection of damaged homes. For this demo, we used a client-server architecture which gives a clean separation of the roles of a Geographic Information System (GIS) Analyst and a Data Scientist. The GIS Analyst uses NVIDIA Quadro Virtual Data Center Workstation (Quadro vDWS) software to create, edit and explore spatial data. The Data Scientist uses NVIDIA Virtual Compute Server (vComputeServer) software to train/build a model which will then be used by the GIS Analyst to execute object detection inferencing.Watch this session
Join in the conversation below.Hello there! what an AWESOME demo… here at Vale we use ArcGis for several applications and teams. How can apply this demo at our company? Could someone from NVIDIA please help us?Thank you.Powered by Discourse, best viewed with JavaScript enabled"
2886,developer-spotlight-applying-deep-learning-to-aerospace-technologies-and-integrated-systems,"Originally published at:			https://developer.nvidia.com/blog/developer-spotlight-applying-deep-learning-to-aerospace-technologies-and-integrated-systems/
Vivek Venugopalan, a staff research scientist at the United Technologies Research Center  (UTRC) shares how they are using deep learning and GPUs to understand the life of an aircraft engine and predictive maintenance for elevators in high-rise buildings. “GPUs have helped us arrive at solutions quickly for computationally intensive challenges across all UTRC platforms, especially…Powered by Discourse, best viewed with JavaScript enabled"
2887,ai-based-mobile-app-tests-for-parkinsons-in-minutes,"Originally published at:			AI-based Mobile App Tests for Parkinson’s in Minutes | NVIDIA Technical Blog
UK researchers developed a smartphone app using deep learning that lets people with Parkinson’s disease test their symptoms at home in just four minutes. “There’s very little understanding as to how Parkinson’s arises, and patients say that every day the condition is different,” says George Roussos at Birkbeck, University of London and co-author of the…Powered by Discourse, best viewed with JavaScript enabled"
2888,similarity-in-graphs-jaccard-versus-the-overlap-coefficient,"Originally published at:			Similarity in graphs: Jaccard versus the Overlap Coefficient | NVIDIA Technical Blog
There are a wide range of graph applications and algorithms that I hope to discuss through this series of blog posts, all with a bias toward what is in RAPIDS cuGraph. I am assuming that the reader has a basic understanding of graph theory and graph analytics. If there is interest in a graph analytic…Powered by Discourse, best viewed with JavaScript enabled"
2889,gtc-2020-do-it-yourself-automatic-speech-recognition-with-nvidia-technologies,"GTC 2020 S21513
Presenters: Adriana Flores,NVIDIA; Ananth Sankarasubramanian, NVIDIA; Purnendu Mukherjee, NVIDIA
Abstract
Ever wondered how to get started on automatic speech recognition? How to make that generic speech-to-text (STT) model work for your domain-specific use case? We’ll teach you how to create and evaluate ASR models that can accurately transcribe speech to text for your use case, including domain-specific words! We’ll do this using the latest NVIDIA technologies for STT.Watch this session
Join in the conversation below.Thank you it was amazing session and well organised notebooks.Powered by Discourse, best viewed with JavaScript enabled"
2890,simplifying-realistic-character-creation-with-nvidia-omniverse-reallusion-connector,"Originally published at:			https://developer.nvidia.com/blog/simplifying-realistic-character-creation-with-nvidia-omniverse-reallusion-connector/
Combining Omniverse and Reallusion software accelerates the creation of realistic and stylized characters with a library of high-quality character assets and motions.Powered by Discourse, best viewed with JavaScript enabled"
2891,sc20-demo-exascale-simulation-of-covid19-using-folding-home-and-nvidia-omniverse,"Originally published at:			https://developer.nvidia.com/blog/sc20-demo-exascale-simulation-of-covid19-using-foldinghome-and-nvidia-omniverse/
NVIDIA is working closely with a wide range of organizations and researchers to help accelerate the creation of treatments and therapeutics for COVID-19. Identifying how and where the virus binds can help to disrupt this propagation chain. Numerical simulations can help researchers understand these processes, but the scale of the problem is enormous, keeping even…Powered by Discourse, best viewed with JavaScript enabled"
2892,language-learning-app-uses-ai-to-adapt-its-teaching-methods,"Originally published at:			Language Learning App Uses AI to Adapt Its Teaching methods | NVIDIA Technical Blog
Memrise, a UK based startup, whose language-learning application uses AI to adapt to its customers’ language learning needs, just raised $15.5 million in funding to expand their products. The application, which aims to help you become fluent in a foreign language, has both a web and mobile app. The system uses machine learning to adapt…Powered by Discourse, best viewed with JavaScript enabled"
2893,accelerated-molecular-simulation-using-deep-potential-workflow-with-ngc,"Originally published at:			Accelerated Molecular Simulation Using Deep Potential Workflow with NGC | NVIDIA Technical Blog
AI neural network deep potential to combine classical MD simulation with DFT calculation accuracy.Powered by Discourse, best viewed with JavaScript enabled"
2894,exploiting-and-securing-jenkins-instances-at-scale-with-groovywaiter,"Originally published at:			https://developer.nvidia.com/blog/exploiting-and-securing-jenkins-instances-at-scale-with-groovywaiter/
Learn how to secure Jenkins instances with the GroovyWaiter Python script.Powered by Discourse, best viewed with JavaScript enabled"
2895,gtc-2020-tuning-gpu-server-for-dl-performance,"GTC 2020 S21501
Presenters: Frank Han,Dell ; Rengan Xu,Dell
Abstract
MLPerf training benchmark is a software suite for measuring how fast systems can train models to a target quality metric. Its version 0.6 has good coverage of deep-learning models in image classification, object detection, translation, and reinforcement learning. We’ll use those subtests to demonstrate how different hardware configurations (CPU core counts vs frequency, memory frequency 2666 vs 2933Mhz, PCIe vs NVLink) and storage (local SSD, U.2 NVMe, Isilon and Lustre) impacts those DL training workloads. We’ll also discuss our work to characterize MLPerf benchmark performance using profiling tools (GPU, CPU, memory, and I/O), our hyperparameter-tuning work (batch size, learning rate, SGD optimizer), software environments study (OS versions, CUDA drivers, docker versions, NCCL P2P levels, NCCL tree vs ring, etc.) on MLPerf performance of both single and distributed systems.Watch this session
Join in the conversation below.The PDF link is dead (404 not found), please fix it.Powered by Discourse, best viewed with JavaScript enabled"
2896,nvidia-real-time-denoiser-delivers-best-in-class-denoising-in-ubisoft-s-watch-dogs-legion,"Originally published at:			https://developer.nvidia.com/blog/nvidia-real-time-denoiser-delivers-best-in-class-denoising-in-watch-dogs-legion/
NVIDIA Real-Time Denoiser (NRD) is a spatio-temporal API agnostic denoising library that’s designed to work with low rpp (ray per pixel) signals, and is the only denoiser variable that works with 0.5 or 1 ray per pixel.Powered by Discourse, best viewed with JavaScript enabled"
2897,giphy-releases-an-open-source-ai-celebrity-detector,"Originally published at:			GIPHY Releases an Open Source AI Celebrity Detector | NVIDIA Technical Blog
Trying to figure out who is in that celebrity GIF? This AI-based algorithm can help. Researchers from GIPHY, the online search database for GIFs, recently developed an open source deep learning model that can recognize over 2,300 celebrity faces with high accuracy.Powered by Discourse, best viewed with JavaScript enabled"
2898,cloudera-and-nvidia-collaborate-to-accelerate-data-analytics-and-ai-at-scale,"Originally published at:			Cloudera and NVIDIA Collaborate to Accelerate Data Analytics and AI at Scale | NVIDIA Technical Blog
With Cloudera CDP and the power of NVIDIA computing, customers like IRS and Commerzbank can accelerate data processing and model training at a lower cost across any on-premises, public cloud, or hybrid cloud deployment.Powered by Discourse, best viewed with JavaScript enabled"
2899,gpudirect-rdma-on-nvidia-jetson-agx-xavier,"Originally published at:			https://developer.nvidia.com/blog/gpudirect-rdma-nvidia-jetson-agx-xavier/
Remote Direct Memory Access (RDMA) allows computers to exchange data in memory without the involvement of a CPU. The benefits include low latency and high bandwidth data exchange. GPUDirect RDMA extends the same philosophy to the GPU and the connected peripherals in Jetson AGX Xavier. GPUDirect RDMA enables a direct path for data exchange between…Hello, Is there any benchmark between using GPUDirect RDMA or Standard DMA Transfer regarding the latency on Xavier?  I am asking this because I know that in Jetson family devices, the memory is accessible with pointers between CPU and GPU (no memcopy operation is required) which means that we can use zero-copy.thank youPowered by Discourse, best viewed with JavaScript enabled"
2900,rtx-coffee-break-reference-in-engine-path-tracer-6-22-minutes,"Originally published at:			RTX Coffee Break: Reference In-Engine Path Tracer (6:22 minutes) | NVIDIA Technical Blog
A new video tutorial explains why it’s important to have a path tracer in your rendering engine, and detail how the Optix 5.0 AI Denoiser can improve the quality of a rendered image.   Five Things to Remember: Having a path tracer in your rendering engine is valuable because it allows you to quickly generate…Powered by Discourse, best viewed with JavaScript enabled"
2901,grpico-takes-too-long-to-install-then-fails-after-wheel-is-built-when-installing-tensorflow,"This error is spotted on Jetpack version 4.4This is due to the installation of setuptools(latest version) when the script on the developer blog run for installation of tensorflow is run instaed of downloading the latest version download version 49.6.0 instead
sudo pip3 install setuptools==49.6.0
to check version of setuptools type easyinstall --versionPowered by Discourse, best viewed with JavaScript enabled"
2902,researchers-using-gpus-to-monitor-underage-drinking-on-instagram,"Originally published at:			Researchers Using GPUs to Monitor Underage Drinking on Instagram | NVIDIA Technical Blog
Instagram could offer a novel way of monitoring the drinking habits of teenagers. Using photos and text from Instagram, a team of researchers from the University of Rochester has shown that this data can not only expose patterns of underage drinking more cheaply and faster than conventional surveys, but also find new patterns, such as…Powered by Discourse, best viewed with JavaScript enabled"
2903,pac-man-recreated-with-ai-by-nvidia-researchers,"Originally published at:			https://developer.nvidia.com/blog/pac-man-recreated-with-ai-by-nvidia-researchers/
Forty years to the day since PAC-MAN first hit arcades in Japan, and went on to munch a path to global stardom, the retro classic has been reborn, delivered courtesy of AI. Trained on 50,000 episodes of the game, a powerful new AI model created by NVIDIA Research, called NVIDIA GameGAN, can generate a fully functional…Powered by Discourse, best viewed with JavaScript enabled"
2904,accelerating-ai-training-with-nvidia-tf32-tensor-cores,"Originally published at:			https://developer.nvidia.com/blog/accelerating-ai-training-with-tf32-tensor-cores/
NVIDIA Ampere GPU architecture introduced the third generation of Tensor Cores, with the new TensorFloat32 (TF32) mode for accelerating FP32 convolutions and matrix multiplications. TF32 mode is the default option for AI training with 32-bit variables on Ampere GPU architecture. It brings Tensor Core acceleration to single-precision DL workloads, without needing any changes to model…Open source library CUTLASS (GitHub - NVIDIA/cutlass: CUDA Templates for Linear Algebra Subroutines) has all the details of TF32 (numeric representation, rounding, math operation, etc.) as well as TF32 GEMM/CONV source code.Powered by Discourse, best viewed with JavaScript enabled"
2905,nvcomp-v2-0-0-now-available-with-new-compressors,"Originally published at:			https://developer.nvidia.com/blog/nvcomp-v2-0-0-now-available-with-new-compressors/
Today, NVIDIA is announcing the availability of nvCOMP version 2.0.0. This software can be downloaded now free for members of the NVIDIA Developer Program.Powered by Discourse, best viewed with JavaScript enabled"
2906,grandmasters-series-learning-from-the-bengali-character-recognition-kaggle-challenge,"Originally published at:			https://developer.nvidia.com/blog/grandmasters-series-learning-from-bengali-character-recognition-kaggle-challenge/
Handwritten character recognition is one of the most quintessential deep learning (DL) problems. One of the oldest and still widely used benchmark datasets for machine learning (ML) tasks is the MNIST dataset, which consists of 70,000 handwritten digits. MNIST was released in 1995. To this day, it is one of the best studied and understood…Powered by Discourse, best viewed with JavaScript enabled"
2907,nvidia-clara-and-xnat-join-forces-to-easily-deliver-ai-in-a-box-to-hospitals-on-nvidia-edge-computing-platform,"Originally published at:			https://developer.nvidia.com/blog/nvidia-clara-and-xnat-join-forces-to-bring-ai-in-a-box-to-hospitals/
NVIDIA today introduced the integration of XNAT, the most widely-used informatics platform for imaging research, and NVIDIA Clara.Powered by Discourse, best viewed with JavaScript enabled"
2908,this-jetson-based-robodog-will-always-listen-to-its-owner,"Originally published at:			This Jetson-based Robodog Will Always Listen to Its Owner | NVIDIA Technical Blog
This robot not only looks like a dog but he learns like one too! Researchers from Florida Atlantic University’s Machine Perception and Cognitive Robotics Laboratory have just developed Astro the robot dog.  “Astro doesn’t operate based on preprogrammed code. Instead, Astro is being trained using inputs to a deep neural network – a computerized simulation…Powered by Discourse, best viewed with JavaScript enabled"
2909,accelerating-inference-up-to-6x-faster-in-pytorch-with-torch-tensorrt,"Originally published at:			Accelerating Inference Up to 6x Faster in PyTorch with Torch-TensorRT | NVIDIA Technical Blog
Torch-TensorRT is a PyTorch integration for TensorRT inference optimizations on NVIDIA GPUs. With just one line of code, it speeds up performance up to 6x on NVIDIA GPUs.Does this one line of code need to be run on the target machine for optimization like previous conversions?Hi, thank you for your post.
Can I expect the speedup by using Toch-TensorRT in case of my low performance laptop GPU such as Geforce GTX 1060 as well?“Torch-TensorRT” shouldn’t be used for higher precision inference, right?Yes, Torch-TensorRT should be used on the target machine since TensorRT optimizations are dependent on the system’s configuration.You can expect some speedup (not always) but that’d depend on the DNN too.DNN’s are typically trained in FP32 or in mixed precision (FP32 + FP16) and inferenced in FP32/FP16/INT8/INT4.
A higher precision (like FP64) is typically not used for inference.Except using ngc pytorch docker images, can we install Torch-TensorRT in our custom env, like using pip install?I was able to manually download wheels to install everything so you should be able to.  I don’t have sudo access on the cloud machine so i had to do everything without sudo, which was a pain. Otherwise there should be easier instructions to followWhere did you download wheel file? I just found torch-tensorrt 0.0.0 in pypi.Any docs about Torch-TensorRT? How I know these APIs?Hi Ashish
Thank you for the article.I found one typo in your code in:] at the end is not neededAlso, the last benchmark fails with errorI think you need to use dtype=""fp32"".
The model was compiled with half precision, but the input_data should still be float32Tested in docker pull nvcr.io/nvidia/pytorch:22.04-py3@asardana @anishm @ashishsardanaTorch-TensorRT has recently moved inside the PyTorch project. You’d find the documentation here - Torch-TensorRT — Torch-TensorRT master documentationHi Alexander, thanks for trying out the code for Torch-TensorRT and pointing out the typo!When publishing this blog (+ code), we tested it in the pytorch:19.11 environment (docker image - nvcr.io/nvidia/pytorch:19.11-py3). This docker image contains Torch-TensorRT v0.4.1
The environment you are in (pytorch:20.04) contains Torch-TensorRT v1.1, hence the difference in the output you’re observing.@apivovarov  – Fixed the code typo, thanks!Ashish,If you blog was updated to use Torch-TensorRT v1.0.0+ API then the code also should be updated.
It would be nice if you fix the code and replacewith@jwitsoe @ashishsardana @anishmIf we want to build torch_tensorrt outside the docker, does it work with the new agx orin?Hi, thanks for your support.I use the python api guide from Using Torch-TensorRT in Python — Torch-TensorRT master documentationmy code:
def load_torchmodel():
…model = load_torchmodel()inputs = [torch_tensorrt.Input(
min_shape=[1, 1, 320, 320],
opt_shape=[1, 1, 640, 640],
max_shape=[1, 1, 1280, 1280],
dtype=torch.half,
)]
enabled_precisions = {torch.float, torch.half} # Run with fp16trt_ts_module = torch_tensorrt.compile(model, inputs=inputs, enabled_precisions=enabled_precisions)input_data = input_data.to(‘cuda’).half()
result = trt_ts_module(input_data)
torch.jit.save(trt_ts_module, “trt_ts_module.ts”)Traceback (most recent call last):
File “trt_infer3.py”, line 236, in 
trt_ts_module = torch_tensorrt.compile(model, inputs=inputs, enabled_precisions=enabled_precisions)
File “/usr/local/lib/python3.6/dist-packages/torch_tensorrt/_compile.py”, line 96, in compile
ts_mod = torch.jit.script(module)
File “/usr/local/lib/python3.6/dist-packages/torch/jit/_script.py”, line 1258, in script
obj, torch.jit._recursive.infer_methods_to_compile
File “/usr/local/lib/python3.6/dist-packages/torch/jit/_recursive.py”, line 451, in create_script_module
return create_script_module_impl(nn_module, concrete_type, stubs_fn)
File “/usr/local/lib/python3.6/dist-packages/torch/jit/_recursive.py”, line 513, in create_script_module_impl
script_module = torch.jit.RecursiveScriptModule._construct(cpp_module, init_fn)
File “/usr/local/lib/python3.6/dist-packages/torch/jit/_script.py”, line 587, in _construct
init_fn(script_module)
File “/usr/local/lib/python3.6/dist-packages/torch/jit/_recursive.py”, line 491, in init_fn
scripted = create_script_module_impl(orig_value, sub_concrete_type, stubs_fn)
File “/usr/local/lib/python3.6/dist-packages/torch/jit/_recursive.py”, line 513, in create_script_module_impl
script_module = torch.jit.RecursiveScriptModule._construct(cpp_module, init_fn)
File “/usr/local/lib/python3.6/dist-packages/torch/jit/_script.py”, line 587, in _construct
init_fn(script_module)
File “/usr/local/lib/python3.6/dist-packages/torch/jit/_recursive.py”, line 491, in init_fn
scripted = create_script_module_impl(orig_value, sub_concrete_type, stubs_fn)
File “/usr/local/lib/python3.6/dist-packages/torch/jit/_recursive.py”, line 463, in create_script_module_impl
method_stubs = stubs_fn(nn_module)
File “/usr/local/lib/python3.6/dist-packages/torch/jit/_recursive.py”, line 732, in infer_methods_to_compile
stubs.append(make_stub_from_method(nn_module, method))
File “/usr/local/lib/python3.6/dist-packages/torch/jit/_recursive.py”, line 66, in make_stub_from_method
return make_stub(func, method_name)
File “/usr/local/lib/python3.6/dist-packages/torch/jit/_recursive.py”, line 51, in make_stub
ast = get_jit_def(func, name, self_name=“RecursiveScriptModule”)
File “/usr/local/lib/python3.6/dist-packages/torch/jit/frontend.py”, line 264, in get_jit_def
return build_def(parsed_def.ctx, fn_def, type_line, def_name, self_name=self_name, pdt_arg_types=pdt_arg_types)
File “/usr/local/lib/python3.6/dist-packages/torch/jit/frontend.py”, line 315, in build_def
build_stmts(ctx, body))
File “/usr/local/lib/python3.6/dist-packages/torch/jit/frontend.py”, line 137, in build_stmts
stmts = [build_stmt(ctx, s) for s in stmts]
File “/usr/local/lib/python3.6/dist-packages/torch/jit/frontend.py”, line 137, in 
stmts = [build_stmt(ctx, s) for s in stmts]
File “/usr/local/lib/python3.6/dist-packages/torch/jit/frontend.py”, line 287, in call
return method(ctx, node)
File “/usr/local/lib/python3.6/dist-packages/torch/jit/frontend.py”, line 550, in build_Return
return Return(r, None if stmt.value is None else build_expr(ctx, stmt.value))
File “/usr/local/lib/python3.6/dist-packages/torch/jit/frontend.py”, line 287, in call
return method(ctx, node)
File “/usr/local/lib/python3.6/dist-packages/torch/jit/frontend.py”, line 988, in build_DictComp
raise NotSupportedError(r, “Comprehension ifs are not supported yet”)
torch.jit.frontend.NotSupportedError: Comprehension ifs are not supported yet:
File “/home/Project/YOLOX/yolox/models/darknet.py”, line 179
x = self.dark5(x)
outputs[“dark5”] = x
return {k: v for k, v in outputs.items() if k in self.out_features}Powered by Discourse, best viewed with JavaScript enabled"
2910,object-detection-and-lane-segmentation-using-multiple-accelerators-with-drive-agx,"Originally published at:			Object Detection and Lane Segmentation Using Multiple Accelerators with DRIVE AGX | NVIDIA Technical Blog
DRIVE AGX is NVIDIA’s platform for autonomous driving Autonomous vehicles require fast and accurate perception of the surrounding environment in order to accomplish a wide set of tasks concurrently in real time. Systems need to handle the detection of obstacles, determine the boundaries of lanes, intersection detection, and sign recognition among many more functions over…Powered by Discourse, best viewed with JavaScript enabled"
2911,deploying-gpudirect-rdma-on-egx-stack-with-the-mellanox-network-operator,"Originally published at:			https://developer.nvidia.com/blog/deploying-gpudirect-rdma-on-egx-stack-with-the-mellanox-network-operator/
Edge computing takes place close to the data source to reduce network stress and improve latency. GPUs are an ideal compute engine for edge computing because they are programmable and deliver phenomenal performance per dollar. However, the complexity associated with managing a fleet of edge devices can erode the GPU’s favorable economics. In 2019, NVIDIA…Powered by Discourse, best viewed with JavaScript enabled"
2912,improving-japanese-language-asr-by-combining-convolutions-with-attention-mechanisms,"Originally published at:			https://developer.nvidia.com/blog/improving-japanese-language-asr-by-combining-convolutions-with-attention-mechanisms/
Learn how to improve the accuracy and speed of Asian language ASR with two novel ways of combining convolutions with attention mechanisms.Powered by Discourse, best viewed with JavaScript enabled"
2913,new-nvidia-deep-learning-software-tools-for-developers,"Originally published at:			New NVIDIA Deep Learning Software Tools for Developers | NVIDIA Technical Blog
Aided by developers’ requests, NVIDIA announced a significant update to the NVIDIA SDK, which includes tools, libraries and enhancements to the CUDA programming model to help developers accelerate and build the next generation of AI and HPC applications. The level of interest in GPU computing has exploded, fueled by advancements in AI. The latest SDK…Powered by Discourse, best viewed with JavaScript enabled"
2914,developer-spotlight-opening-a-new-era-of-drug-discovery-with-amber,"Originally published at:			https://developer.nvidia.com/blog/developer-spotlight-opening-a-new-era-of-drug-discovery-with-amber/
Amber is a suite of biomolecular simulation programs that began in the late 1970’s and is maintained by an active development community.Powered by Discourse, best viewed with JavaScript enabled"
2915,cuda-pro-tip-optimized-filtering-with-warp-aggregated-atomics,"Originally published at:			CUDA Pro Tip: Optimized Filtering with Warp-Aggregated Atomics | NVIDIA Technical Blog
Note: This post has been updated (November 2017) for CUDA 9 and the latest GPUs. The NVCC compiler now performs warp aggregation for atomics automatically in many cases, so you can get higher performance with no extra effort. In fact, the code generated by the compiler is actually faster than the manually-written warp aggregation code.…If 64-bit integer atomics are used, but the increment value is the same across all threads in a warp, then you don't really need warp reduction: you can use the same trick with __popc(), and do this part in 32 bits. The performance with 64-bit atomics in global case (both non-aggregated and aggregated) is very similar (roughly, no more than 1% different) to 32-bit case.If double-precision atomics must be used globally, but increment value is the same, warp reduction is again unnecessary. However, you will have to model atomicAdd() through compare-and-swap (e.g., like here: http://docs.nvidia.com/cuda... ), and its performance will be low, due to high cost of each conflict. In the example from the post, for 50% values passing the filter on K40, it's 0.001 GiB/s and 0.108 GiB/s for global non-aggregated and aggregated cases, respectively. Obviously, aggregation improves performance by a factor of 100, but absolute numbers are very low compared to using atomics supported in hardware. Shared memory version in this case is faster: it achieves 0.637 GiB/s and 1.168 GiB/s for non-aggregated and aggregated cases, respectively.If increment values are different, and you need to do a reduction across the warp, then there will be additional overhead. I expect it, however, to be relatively low in the case of 64-bit floating-point atomics when compared to the costs of atomics themselves. I have no specific performance numbers here, and believe that this is well beyond the scope of this blog post. An example of warp-level reduction for 32-bit integers is available here: http://docs.nvidia.com/cuda... , and it can be easily extended to 64-bit case, though you'll need two shuffle instructions to exchange 64-bit values between threads in the same warp.What is the license of the provided source code? Andrew, many thanks for this algorithm, I was coding something like it (using ballot and popc), and you saved me many hours of work.I've read some old (2009) papers attempting to use this approach before ballot came to CUDA.  Can you name me some paper related to your algorithm?We made it for OpenGL.Well, in the graphs you posted, why I feel everything is slowing down when cooperative group technique is implemented..? On the contrast, shared memory seems to be the best approach?The sample code in the Performance Comparison, 'nres' is undefined. It should be passed in the function argument.I am using Jetson Xavier, which is Volta Arch to test the perfromance of atomicAdd() to shared vs. global as you descirebed in blog. I found the performance of adding in shared in even 10% slower than global. Any possible reasons?Hi wen14211124,Not sure if this is Jetson AGX Xavier related issue, but we recommend you to raise it to the respective platform from the below link
Latest Jetson & Embedded Systems/Jetson AGX Xavier topics - NVIDIA Developer Forums
Thanks!Powered by Discourse, best viewed with JavaScript enabled"
2916,new-ebook-a-beginners-guide-to-large-language-models,"Originally published at:			An Enterprises’ Guide to Large Language Models | NVIDIA
Download this free eBook to learn more about LLMs and how they are powering use cases such as chatbots, global translation, and summarization.Powered by Discourse, best viewed with JavaScript enabled"
2917,does-cugraph-support-graph-convolutional-networks-gcns,"I’d like to know whether cuGraph supports Graph Convolutional Networks (GCNs).Yes, cuGraph support GNN, we have GPU sampling methods and cuGraph-ops also support message passing/aggregation functions as well.
We provide accelerated GNN conv layers via a separate package called pylibcugraphops (cugraph-ops). So far, we support GraphSAGE, GAT and RGCN models.This topic was automatically closed 14 days after the last reply. New replies are no longer allowed.Powered by Discourse, best viewed with JavaScript enabled"
2918,mobile-app-helps-find-furniture-you-see-in-the-real-world,"Originally published at:			Mobile App Helps Find Furniture You See in the Real World | NVIDIA Technical Blog
Take a photo of a chair and a new mobile app will tell you where to buy it, and show you pictures of how it will look in various rooms. “It seems a lot of people want to buy things they see in someone else’s home or in a photo, but they don’t know where…I’m excited to see how this app develops and how it can help make the furniture-buying process easier for people.Powered by Discourse, best viewed with JavaScript enabled"
2919,share-your-science-gpus-help-provide-insight-on-the-secrets-of-solar-wind,"Originally published at:			https://developer.nvidia.com/blog/share-your-science-gpus-help-provide-insight-on-the-secrets-of-solar-wind/
Shahab Fatemi, PhD Fellow at the University of California Berkeley Space Sciences Laboratory, shares how his team is using CUDA, TITAN X and Tesla K80 GPUs to develop a three-dimensional plasma model that can provide a better understanding of how solar wind could affect Earth.  “GPUs have been revolutionary in space and plasma physics modeling,”…Powered by Discourse, best viewed with JavaScript enabled"
2920,upcoming-webinar-learn-how-to-use-nvidia-ngc-jupyter-notebook-for-medical-imaging,"Originally published at:			https://developer.nvidia.com/blog/upcoming-webinar-learn-how-to-use-nvidia-ngc-jupyter-notebook-for-medical-imaging/
Learn how NVIDIA NGC and NVIDIA Clara (NVIDIA’s healthcare AI Platform) can help accelerate medical imaging workflows.Powered by Discourse, best viewed with JavaScript enabled"
2921,customizing-nsight-graphics-for-vulkan-applications,"Originally published at:			Customizing Nsight Graphics for Vulkan Applications | NVIDIA Technical Blog
If you are building Vulkan applications, Nsight Graphics is an indispensable tool in your utility belt. At GDC 2019, NVIDIA’s Jeff Kiel explained how Nsight Graphics can help solve the most puzzling graphics rendering and performance problems. Getting The Most From Your Vulkan Applications with NVIDIA Nsight Graphics, Jeff’s full GDC presentation, can be found…Powered by Discourse, best viewed with JavaScript enabled"
2922,insect-inspired-drone-uses-ai-to-fly-through-narrow-gaps,"Originally published at:			Insect-Inspired Drone Uses AI to Fly Through Narrow Gaps | NVIDIA Technical Blog
Can a bee teach an autonomous drone how to fly through gaps? Researchers from the University of Maryland’s Perception and Robotics Group recently developed a deep learning-based system that allows a drone to fly through a small and completely unknown gap automatically. They call it GapFlyt. “We propose this framework of bio-inspired perceptual design for…Powered by Discourse, best viewed with JavaScript enabled"
2923,rtx-coffee-break-introduction-to-real-time-ray-tracing-5-41-minutes,"Originally published at:			RTX Coffee Break: Introduction to Real-Time Ray Tracing (5:41 minutes) | NVIDIA Technical Blog
Ray tracing will soon revolutionize the way video games look. Ray tracing simulates how rays of light hit and bounce off of objects, enabling developers to create stunning imagery that lives up to the word “photorealistic”. This video provides an overview of how the technology works. Five Things to Remember: Ray tracing defined: a primitive…Powered by Discourse, best viewed with JavaScript enabled"
2924,ai-faces-track-your-movements,"Originally published at:			AI Faces Track Your Movements | NVIDIA Technical Blog
Artist and researcher Branislav Ulicny (“AlteredQualia”) developed an online demo that combines two different deep learning-based projects into spooky cursor-tracking faces. The first is a hobby project by Mike Tyka, who works on machine learning at Google, makes use of GANs to generate realistic new faces. Using a TITAN X GPU, Tyke trained his generative…Powered by Discourse, best viewed with JavaScript enabled"
2925,adobe-uses-ai-to-auto-remove-background-in-photos,"Originally published at:			Adobe Uses AI To Auto Remove Background in Photos | NVIDIA Technical Blog
This week at GTC 2018 in San Jose, California, engineers from Adobe Applied Research and Tech (ART) and NVIDIA demonstrated a deep learning-based method that extracts the foreground content from its background without the use of a green screen. During their demo, the team used a version of Photoshop with the “Deep Matte” algorithm. The…Powered by Discourse, best viewed with JavaScript enabled"
2926,safeguarding-networks-and-assets-with-digital-fingerprinting,"Originally published at:			https://developer.nvidia.com/blog/safeguarding-networks-and-assets-with-digital-fingerprinting/
Use of stolen or compromised credentials remains at the top of the list as the most common cause of a data breach. Because an attacker is using credentials or passwords to compromise an organization’s network, they can bypass traditional security measures designed to keep adversaries out. When they’re inside the network, attackers can move laterally…Powered by Discourse, best viewed with JavaScript enabled"
2927,new-video-creation-and-streaming-features-accelerated-by-the-nvidia-video-codec-sdk,"Originally published at:			https://developer.nvidia.com/blog/new-video-creation-and-streaming-features-accelerated-by-the-nvidia-video-codec-sdk/
For over a decade, NVIDIA GPUs have been built with dedicated encoders and decoders called NVENC and NVDEC. They have a highly parallelized architecture, support popular codec formats, and provide direct access to GPU memory for optimized encode and decode operations. GPU-accelerated video means offloading your video processing to NVENCs and NVDECs, reducing CPU cycles…Powered by Discourse, best viewed with JavaScript enabled"
2928,deep-learning-helps-doctors-classify-breast-cancer-tumors,"Originally published at:			Deep Learning Helps Doctors Classify Breast Cancer Tumors | NVIDIA Technical Blog
In the United States, there are over 3.1 million women with a history of breast cancer.  This year, over 266,000 women are expected to be diagnosed with invasive breast cancer. To help doctors better detect and treat  this disease, researchers are turning to artificial Intelligence. In a recently published study from the University of North…Powered by Discourse, best viewed with JavaScript enabled"
2929,dense-graphs,"Have you come across any interesting examples of dense graphs (e.g., a high edge to vertex ratio : most vertices are connected each other)?Our implementation is mainly tailored towards sparse graph therefore, we recommend using dense matrix libraries instead. In fact, our implementation incurs additional overhead when processing and storing low degree vertices and their properties. We did at some point support Hungrarian wich is for dense graphs but it was pushed to a more suitable package (raft)This topic was automatically closed 14 days after the last reply. New replies are no longer allowed.Powered by Discourse, best viewed with JavaScript enabled"
2930,ai-helps-physicians-deliver-precise-radiotherapy-faster-than-real-time,"Originally published at:			https://developer.nvidia.com/blog/ai-helps-physicians-deliver-precise-radiotherapy-faster-than-real-time/
According to the Centers for Disease Control and Prevention, lung cancer is the leading cause of cancer death in the United States, accounting for 27% of deaths. To treat patients with lung cancer, doctors often use radiation therapy. However, the respiratory motion causes uncertainty in tumor location, leading to complications in radiation therapy techniques. To…Powered by Discourse, best viewed with JavaScript enabled"
2931,gdc-showcase-highlights-top-nvidia-technologies,"Originally published at:			https://developer.nvidia.com/blog/gdc-showcase-highlights-top-nvidia-technologies/
From March 15-19, GDC Showcase will introduce a wide range of new content for game developers to explore. NVIDIA will be there with a new talk, covering how to best harness the power NVIDIA RTX GPUs with a suite of SDKs and tools custom built for the job.Powered by Discourse, best viewed with JavaScript enabled"
2932,speeding-up-seismic-processing-and-interpretation-with-gpus,"Originally published at:			Speeding up Seismic Processing and Interpretation with GPUs | NVIDIA Technical Blog
Large reservoirs are found at greater depths and in sediments that are much harder to analyze, like the recent Jack Field discovery in the Gulf of Mexico which was found at more than 20,000 feet under the sea floor. To interpret and discover these reservoirs it is necessary to acquire and process huge amounts of…Powered by Discourse, best viewed with JavaScript enabled"
2933,meet-the-researcher-rommie-amaro-simulating-the-sars-cov-2-virus-with-ai-and-hpc,"Originally published at:			Meet the Researcher: Rommie Amaro, Simulating the SARS-CoV-2 virus with AI and HPC | NVIDIA Technical Blog
‘Meet the Researcher’ is a new series in which we spotlight different researchers in academia who are using GPUs to accelerate their work. This month we spotlight Rommie Amaro, professor and endowed chair in the Department of Chemistry and Biochemistry at the University of California, San Diego. Amaro is also the principal investigator of the…Powered by Discourse, best viewed with JavaScript enabled"
2934,nvidia-developer-blog-2019-highlights,"Originally published at:			NVIDIA Developer Blog 2019 Highlights | NVIDIA Technical Blog
We published nearly 100 technical blogs this year on the NVIDIA Developer Blog to help developers across a variety of industries develop their GPU-accelerated applications and the site had millions of page views. Below is a list of the most viewed posts from 2019 and some that you might have missed.   Announced early 2019,…Powered by Discourse, best viewed with JavaScript enabled"
2935,gtc-2020-productionizing-gpu-accelerated-iot-workloads-at-the-edge,"GTC 2020 S21118
Presenters: Paul DeCarlo,Microsoft; Ian Davis,Microsoft
Abstract
In this session, we will cover a variety of techniques, tools, and services to assist in the production of Computer Vision Solutions deployed in Edge environments that run on NVIDIA Jetson hardware.Topics will cover:
Usage of Azure DevOps tooling and workflows to produce deployable container-based GPU-Accelerated modules for use with Azure IoT Edge
The Azure IoT Central - Software as a Service offering for fleet management and runtime-configuration of IoT Edge devices
Deepstream compatible custom object detection models built with the Azure Custom Vision AI service offering
Jetson-containers tooling for producing CUDA compatible container images and flashable CUDA-capable base OS images targeted to NVIDIA Jetson hardware.The presentation will consist of live demonstrations of these topics and include resources for reproduction.At the end of the session, you will leave understanding how to develop production grade GPU-accelerated container workloads backed by deployment, management, and configuration capabilities in Microsoft Azure.Watch this session
Join in the conversation below.Powered by Discourse, best viewed with JavaScript enabled"
2936,live-public-preview-and-blog-for-cuda-on-wsl-now-available,"Originally published at:			Live Public Preview and Blog for CUDA on WSL Now Available | NVIDIA Technical Blog
Back at the Build developers conference in May, Microsoft announced the intent for a Public Preview program for the latest version of their Windows Subsystem for Linux (WSL) capability on Microsoft Windows platforms. The live keynote demonstrated NVIDIA GPU-accelerated support via CUDA on WSL, and AI frameworks run as Linux executables on Microsoft Windows platforms. …Powered by Discourse, best viewed with JavaScript enabled"
2937,nvidia-day-1-game-ready-driver-support-for-windows-10-may-2020-update,"Originally published at:			https://developer.nvidia.com/blog/nvidia-day-1-game-ready-driver-support-for-windows-10-may-2020-update/
Microsoft officially launched the Windows 10 May 2020 update today. NVIDIA has a Game Ready Driver available so that developers can deliver the ultimate gaming experience on the latest OS right away. Within this new update, there are a wide variety of new features and functionality ranging from small additions (like GPU temperature being shown…Powered by Discourse, best viewed with JavaScript enabled"
2938,extracting-features-from-multiple-audio-channels-with-kaldi,"Originally published at:			https://developer.nvidia.com/blog/extracting-features-from-multiple-audio-channels-with-kaldi/
In automatic speech recognition (ASR), one widely used method combines traditional machine learning with deep learning. In ASR flows of this type, audio features are first extracted from the raw audio. Features are then passed into an acoustic model. The acoustic model is a neural net trained on transcribed data to extract phoneme probabilities from…Powered by Discourse, best viewed with JavaScript enabled"
2939,gpudirect-storage-early-access-program-availability,"Originally published at:			GPUDirect Storage - Early Access Program Availability | NVIDIA Technical Blog
GPUDirect Storage uses the cuFile API to enable a direct path to transfer data between GPU memory and local or remote storage devices.Powered by Discourse, best viewed with JavaScript enabled"
2940,using-mesh-shading-to-optimize-your-rasterization-pipeline,"Originally published at:			Using Mesh Shading to Optimize Your Rasterization Pipeline | NVIDIA Technical Blog
At GDC 2019, NVIDIA’s Manuel Kraemer took to the stage to discuss the practical implementation of “in-pipe” GPU culling and level of detail algorithms with Turing’s new mesh shading technology. In the full GDC video, which can be seen here, Manuel uses the context of the DX12 Asteroids Demo to demonstrate how programming shading for…Powered by Discourse, best viewed with JavaScript enabled"
2941,automate-network-monitoring-and-reduce-downtime-with-the-latest-release-of-nvidia-netq,"Originally published at:			https://developer.nvidia.com/blog/automate-network-monitoring-and-reduce-downtime-with-the-latest-release-of-nvidia-netq/
Monitor DPUs, validate RoCE deployments, gain network insights through flow-based telemetry analysis, and centrally view network events with NetQ 4.2.0.Powered by Discourse, best viewed with JavaScript enabled"
2942,ai-researchers-pave-the-way-for-translating-brain-waves-into-speech,"Originally published at:			https://developer.nvidia.com/blog/translating-brain-waves-into-speech/
Researchers from Columbia University used deep learning to enhance speech neuroprosthesis technologies, that can result in accurate and intelligible reconstructed speech from the human auditory cortex.  This research has the potential to one day help patients who have lost their ability to speak, communicate with their loved ones.  “Our approach takes a step toward the next…Powered by Discourse, best viewed with JavaScript enabled"
2943,using-nsight-systems-for-fixing-stutters-in-games,"Originally published at:			Using Nsight Systems for Fixing Stutters in Games | NVIDIA Technical Blog
While working with game developers on pre-release games, NVIDIA has had a steady flow of bugs reported where a game stutters for multiple milliseconds during gameplay. These stutter bugs can ruin the experience of the gamer, possibly making the game unplayable (as with the release of Batman Arkham Knight on PC), so they should be…Powered by Discourse, best viewed with JavaScript enabled"
2944,migrating-to-nvidia-nsight-tools-from-nvvp-and-nvprof,"Originally published at:			https://developer.nvidia.com/blog/migrating-nvidia-nsight-tools-nvvp-nvprof/
If you use the NVIDIA Visual Profiler or the nvprof command line tool, it’s time to transition to something newer: NVIDIA Nsight Tools. Don’t worry! The new tools still offer the same profiling / optimization / deployment workflow. The type of data you need to look at is the same. The commands have changes and…Powered by Discourse, best viewed with JavaScript enabled"
2945,run-rapids-on-google-colab-for-free,"Originally published at:			Run RAPIDS on Google Colab — For Free | NVIDIA Technical Blog
Google Colab is a hosted Jupyter-Notebook like service which has long offered free access to GPU instances. This week Colab got even sweeter. The GPUs powering Colab were upgraded to the new NVIDIA T4 GPUs. This upgrade unlocks new software packages; which means you can now experiment with RAPIDS on Colab for free! Check out our RAPIDS cuDF and…Powered by Discourse, best viewed with JavaScript enabled"
2946,using-gpus-to-accelerate-epidemic-forecasting,"Originally published at:			https://developer.nvidia.com/blog/gpus-accelerate-epidemic-forecasting/
Originally trained as a veterinary surgeon, Chris Jewell, a Senior Lecturer in Epidemiology at Lancaster Medical School in the UK became interested in epidemics through his experience working on the foot and mouth disease outbreak in the UK in 2001. His work so far has been on livestock epidemics such as foot and mouth disease,…Powered by Discourse, best viewed with JavaScript enabled"
2947,a-conversation-with-epic-ray-tracing-in-unreal-engine-4-22-and-beyond,"Originally published at:			A Conversation with Epic: Ray tracing in Unreal Engine 4.22 and beyond | NVIDIA Technical Blog
We sat down with Epic Games to discuss what Unreal Engine 4.22 brings to the table for developers. Nick Penwarden, Director of Engineering, Unreal Engine, Epic Games Marcus Wassmer, Director of Engineering, Rendering, Epic Games Juan Cañada, Lead Ray Tracing Engineer, Epic Games How will ray tracing change a 3D artist’s content creation workflow? Juan:…Powered by Discourse, best viewed with JavaScript enabled"
2948,nvidia-announces-gtc-2020-keynote-with-ceo-jensen-huang,"Originally published at:			https://developer.nvidia.com/blog/nvidia-announces-gtc-2020-keynote-with-ceo-jensen-huang/
NVIDIA will release its GTC 2020 keynote address, featuring founder and CEO Jensen Huang, on YouTube on May 14, at 6 a.m. Pacific time. Huang will highlight the company’s latest innovations in AI, high performance computing, data science, autonomous machines, healthcare and graphics during the recorded keynote. Participants will be able to view the keynote…Powered by Discourse, best viewed with JavaScript enabled"
2949,boost-ai-medical-device-streaming-workflows-with-the-clara-holoscan-sdk,"Originally published at:			https://developer.nvidia.com/blog/boost-ai-medical-device-streaming-workflows-with-the-clara-holoscan-sdk/
Clara Holoscan SDK 0.2 offers real-time AI inference capabilities and fast I/O for high-performance streaming applications in medical devices.Powered by Discourse, best viewed with JavaScript enabled"
2950,graphics-top-resources-from-gtc-21,"Originally published at:			https://developer.nvidia.com/blog/graphics-top-resources-from-gtc-21/
Engineers, product developers and designers worldwide attended GTC to learn how the latest NVIDIA technologies are accelerating real-time, interactive rendering and simulation workflows.Powered by Discourse, best viewed with JavaScript enabled"
2951,upcoming-event-accelerating-the-creation-of-custom-production-ready-ai-models-for-edge-aiaccelerating-the-creation-of-custom-production-ready-ai-models-for-edge-ai,"Originally published at:			https://developer.nvidia.com/blog/upcoming-event-accelerating-the-creation-of-custom-production-ready-ai-models-for-edge-aiaccelerating-the-creation-of-custom-production-ready-ai-models-for-edge-ai/
Powered by Discourse, best viewed with JavaScript enabled"
2952,cuda-101-get-ahead-of-the-cuda-curve-with-practice,"Originally published at:			https://developer.nvidia.com/blog/cuda-101-get-ahead-cuda-curve-practice/
After a recent talk I gave called “CUDA 101: Intro to GPU Computing”, a student asked “What’s the best way for me to get experience in parallel programming and CUDA?”. This is a question I struggled a lot with when I was in college and one I still ask myself about various topics today. The first…Powered by Discourse, best viewed with JavaScript enabled"
2953,volvo-cars-selects-nvidia-drive-agx-xavier-for-production-cars,"Originally published at:			https://developer.nvidia.com/blog/volvo-cars-selects-nvidia-drive-agx-xavier-for-production-cars/
At GTC Europe in Munich, Germany this week, NVIDIA and Volvo cars announced that the Swedish automaker has selected the NVIDIA DRIVE AGX Xavier computer for its next generation of vehicles. “As a world leader in safety technology and innovation, Volvo understands there is a direct connection between safety, comfort and the computing capability inside…Powered by Discourse, best viewed with JavaScript enabled"
2954,gtc-2020-optimizing-tensorrt-conversion-for-real-time-inference-on-autonomous-vehicles,"GTC 2020 S22198
Presenters: Dheeraj Peri,NVIDIA; Josh Park,NVIDIA; Zejia Zheng,Zoox; Jeff Pyke,Zoox
Abstract
TensorRt optimizes neural-network computation for deployment on GPU, but not all operations are supported. Reduced precision inference speeds up computation, but can cause regressions in accuracy. We’ll introduce Zoox TensorRt conversion pipeline that addresses these problems. TensorRt compatibility checks are involved at the early stages of neural-network training to ensure that incompatible ops are discovered before wasting time and resources on full-scale training. Inference accuracy checks can be invoked at each layer to identify operations not friendly to reduced-precision computation. Detailed profiling reveals unnecessary computations that aren’t optimized inside TensorRt, but can be optimized by simple code changes during graph construction. With this pipeline, we’ve successfully provided TensorRt conversion support to neural networks performing various perception tasks on the Zoox autonomous driving platform.Watch this session
Join in the conversation below.Powered by Discourse, best viewed with JavaScript enabled"
2955,accelerating-the-wide-deep-model-workflow-from-25-hours-to-10-minutes-using-nvidia-gpus,"Originally published at:			https://developer.nvidia.com/blog/accelerating-the-wide-deep-model-workflow-from-25-hours-to-10-minutes-using-nvidia-gpus/
Recommender systems drive engagement on many of the most popular online platforms. As data volume grows exponentially, data scientists increasingly turn from traditional machine learning methods to highly expressive, deep learning models to improve recommendation quality. Often, the recommendations are framed as modeling the completion of a user-item matrix, in which the user-item entry is…The fast preprocessing and model training all done on GPU now enables data scientists to include feature engineering and selection into a normal HP search for the model. This is crucial for recommenders.  If you have any questions or comments, let us know.Powered by Discourse, best viewed with JavaScript enabled"
2956,new-deep-learning-method-enhances-your-selfies,"Originally published at:			New Deep Learning Method Enhances Your Selfies | NVIDIA Technical Blog
Researchers from Adobe Research and The Chinese University of Hong Kong created an algorithm that automatically separates subjects from their backgrounds so you can easily replace the background and apply filters to the subject. A highly accurate automatic portrait segmentation method allows many portrait processing tools to be fully automatic. Their research paper mentions there…Powered by Discourse, best viewed with JavaScript enabled"
2957,gtc-2020-nvidia-tensorrt-applications-conversational-ai-recommenders-and-object-detection,"GTC 2020 CWE21821
Presenters: Craig-Wittenbrink,NVIDIA; Jhalak-Patel, NVIDIA; Micah-Villmow, NVIDIA; Ashwin-Nanjappa, NVIDIA; Po-Han-Huang, NVIDIA; Dilip-Sequeira, NVIDIA
Abstract
We’ll have experts consult on how to deploy and use TensorRT for Conversational AI. TensorRT has features specifically for low-latency language processing such as automatic speech recognition, speech to text, and question-answer capabilities. Come meet with the engineering team that develops TensorRT to learn how best to utilize our libraries and tools for your applications.Watch this session
Join in the conversation below.Powered by Discourse, best viewed with JavaScript enabled"
2958,improving-player-performance-with-low-latency-as-evident-from-fps-aim-trainer-experiments,"Originally published at:			https://developer.nvidia.com/blog/improving-player-performance-with-low-latency-as-evident-from-fps-aim-trainer-experiments/
In two aim training experiments, results show that lower latency improves player aiming performance, and gives skilled players a better chance to stand out.These experiments are the largest scale human performance latency study I’m aware of. I was particularly excited to see the bigger spread of performance enabled at lower latency.If you have any questions or comments, I’d be happy to do my best to answer them. And of course, you can download KovaaK’s from Steam and give the experiments a try yourself. You might be surprised how easy it is to notice an increase of 30 ms of latency!Since the initial version of the post ended up with broken video links, here are direct links for anyone who wants to see the experiment tasks.Videos fixed, thanks @jspjut!Powered by Discourse, best viewed with JavaScript enabled"
2959,ai-researchers-visualize-flooding-caused-by-global-warming,"Originally published at:			https://developer.nvidia.com/blog/ai-researchers-visualize-flooding-caused-by-global-warming/
The study uses generative adversarial networks to underscore the impacts of climate change and prompt collective action toward curbing emissions.Powered by Discourse, best viewed with JavaScript enabled"
2960,get-50-off-upcoming-hands-on-training-from-nvidia,"Originally published at:			Get 50% Off Upcoming Hands-on Training from NVIDIA for a Limited Time | NVIDIA Technical Blog
Register now for instructor-led workshops from the NVIDIA Deep Learning Institute.Powered by Discourse, best viewed with JavaScript enabled"
2961,insider-s-guide-to-gtc-computer-vision-nlp-recommenders-and-robotics,"Originally published at:			https://developer.nvidia.com/blog/insiders-guide-to-gtc-computer-vision-nlp-recommenders-and-robotics/
Great sessions on custom computer vision models, expressive TTS, localized NLP, scalable recommenders, and commercial and healthcare robotics apps.Powered by Discourse, best viewed with JavaScript enabled"
2962,nvidia-research-at-neurips-2019,"Originally published at:			NVIDIA Research at NeurIPS 2019 | NVIDIA Technical Blog
NVIDIA Researchers will present seven accepted papers and posted at the annual conference on Neural Information Processing Systems, Sun Dec 8th through Sat the 14th, 2019 at the Vancouver Convention Center. Papers & Posters Improved Precision and Recall Metric for Assessing Generative Models Tuomas Kynkäänniemi · Tero Karras · Samuli Laine · Jaakko Lehtinen ·…Powered by Discourse, best viewed with JavaScript enabled"
2963,getting-started-with-nvidia-networking,"Originally published at:			https://developer.nvidia.com/blog/getting-started-with-nvidia-networking/
Preview and test Cumulus Linux in your own environment, at your own pace, without organizational or economic barriers.Powered by Discourse, best viewed with JavaScript enabled"
2964,gtc-2020-how-cuda-math-libraries-can-help-you-unleash-the-power-of-the-new-nvidia-a100-gpu,"GTC 2020 S21681
Presenters: Azzam Haidar,NVIDIA; Harun Bayraktar, NVIDIA
Abstract
Part 1: Harun Bayraktar, Senior Manager, CUDA Math Libraries, NVIDIA 
Part 2: Azzam Haidar, Senior Math Libraries Engineer, NVIDIA
 
In the first part of this talk we will focus on how the new features of the NVIDIA A100 GPU can be accessed through the CUDA 11.0 Math libraries. These include 3rd generation tensor core functionality for double precision (FP64), TensorFloat-32 (TF32), half precision (FP16) and Bfloat16 (BF16); as well as increased memory bandwidth, multi-GPU performance improvements, and the hardware JPEG decoder.

In the second part of the talk, we will deep dive into the mixed-precision tensor core accelerated solvers and see how 3rd generation tensor cores can boost many HPC applications (workload) bringing exciting speedups up to 4x on the A100 GPU.Watch this session
Join in the conversation below.Powered by Discourse, best viewed with JavaScript enabled"
2965,isc-2020-visualizing-150-terabytes-of-data,"ISC 2020 disc02
Presenters: DemoTeam, NVIDIA
Abstract
A team of NASA scientists and engineers are using Summit, the world’s fastest supercomputer at Oak Ridge National Laboratory (ORNL), to simulate retropropulsion using NASA’s FUN3D, a computational fluid dynamics simulation library. The amount of data produced during this simulation, which took approximately one week to run on Summit, was a massive 128 TB—equivalent to approximately 25,000 4K movies. The existing solution was to create a frame-by-frame video rendering of the data, a process that took hours. This demonstration uses two NVIDIA technologies—IndeX and GPUDirect Storage—to allow researchers to fly through the massive dataset in real time, volumetrically, and even navigate through it while the simulation data continuously updates.Watch this session
Join in the conversation below.Powered by Discourse, best viewed with JavaScript enabled"
2966,nvidia-research-puts-smart-picking-within-grasp,"Originally published at:			NVIDIA Research Puts Smart Picking Within Grasp | NVIDIA Technical Blog
As another step toward enabling robots to work effectively in complex environments, robotics researchers from NVIDIA have developed a novel deep learning-based system that allows a robot to perceive household objects in its environment for the purpose of grasping the objects and interacting with them. With this technique, the robot is able to perform simple…Powered by Discourse, best viewed with JavaScript enabled"
2967,video-technology-helps-pokemon-interact-with-real-world,"Originally published at:			Video Technology Helps Pokémon Interact with Real World | NVIDIA Technical Blog
The augmented reality game Pokémon Go is by far the most popular mobile game ever created – but what if its virtual characters could interact with the real world? Researchers at MIT recently published a paper introducing a technique called “interactive dynamic video,” which lets people reach in and touch objects in videos. Using an…Powered by Discourse, best viewed with JavaScript enabled"
2968,upcoming-workshop-applications-of-ai-for-anomaly-detection,"Originally published at:			Deep Learning Instructor-led Remote Training | NVIDIA
Learn to detect data abnormalities before they impact your business by using XGBoost, autoencoders, and GANs.Powered by Discourse, best viewed with JavaScript enabled"
2969,real-time-ray-tracing-has-come-to-unreal-engine-with-the-release-of-ue4-22,"Originally published at:			Real-Time Ray Tracing Has Come to Unreal Engine with the Release of UE4.22 | NVIDIA Technical Blog
Epic Games announced the release of Unreal Engine 4.22. This update introduces early access support for a Real-Time Ray Tracer and a Path Tracer, optimized for DXR (DirectX Raytracing) and NVIDIA RTX series GPUs.Powered by Discourse, best viewed with JavaScript enabled"
2970,simulating-intelligent-robots-of-the-future-with-nvidia-isaac-sim-2022-2,"Originally published at:			https://developer.nvidia.com/blog/simulating-intelligent-robots-of-the-future-with-isaac-sim-2022-2/
NVIDIA announces the availability of the 2022.2 release of NVIDIA Isaac Sim. As a robotics simulation and synthetic data generation (SDG) tool, this NVIDIA Omniverse application accelerates the development, testing, training, and deployment of intelligent robots. With NVIDIA Isaac Sim, you can easily import the robot model of your choice. Use it to build realistic…Powered by Discourse, best viewed with JavaScript enabled"
2971,creating-smarter-spaces-with-nvidia-metropolis-and-edge-ai,"Originally published at:			https://developer.nvidia.com/blog/creating-smarter-spaces-with-nvidia-metropolis-and-edge-ai/
Learn how AI-enabled video analytics is helping companies and employees work smarter and safer.Powered by Discourse, best viewed with JavaScript enabled"
2972,trained-robot-autonomously-spray-paints-bicycle-frames,"Originally published at:			https://developer.nvidia.com/blog/trained-robot-autonomously-spray-paints-bicycle-frames/
A team from the Advanced Intelligent Robot Laboratory at the National Taiwan University of Science and Technology used CUDA, GeForce and Kinect Fusion to train a multi-axis robot to autonomously spray paint bicycle frames. Nowadays, autonomous spraying painting is an important process in industries for several reasons. However, programming an industrial robot by the ordinary…Powered by Discourse, best viewed with JavaScript enabled"
2973,using-nsight-compute-or-nvprof-to-show-mixed-precision-use-in-deep-learning-models,"Originally published at:			Using Nsight Compute or Nvprof to Show Mixed Precision Use in Deep Learning Models | NVIDIA Technical Blog
Mixed precision combines different numerical precisions in a computational method. The Volta and Turing generation of GPUs introduced Tensor Cores, which provide significant throughput speedups over single precision math pipelines. Deep learning networks can be trained with lower precision for high throughput, by halving storage requirements and memory traffic on gradient and activation tensors. The…Powered by Discourse, best viewed with JavaScript enabled"
2974,gtc-2020-fast-data-pre-processing-with-nvidia-data-loading-library-dali,"GTC 2020 S21139
Presenters: Albert Wolant,NVIDIA; Joaquin Anton Guirao,NVIDIA
Abstract
With every new generation of GPU, it gets harder to keep the data pipeline full so that the GPU can be fully utilized. NVIDIA Data Loading Library (DALI) solves that problem. It’s a portable, open-source library for decoding and augmenting images and videos to accelerate deep-learning applications. Our goal is to show what DALI is and how it addresses the problems it was designed to solve. We’ll also cover the challenges faced, and what we achieved in the past year.Watch this session
Join in the conversation below.Powered by Discourse, best viewed with JavaScript enabled"
2975,get3d-rights-protection,"Will Get3d be programmed with a certificate system or something to let it know whether it can use an image or not?GET3D is a research project, not for commercial use., hence we do not plan to provide any certificate systemThank you for answering. I had the wrong program in mind obviously.This topic was automatically closed 14 days after the last reply. New replies are no longer allowed.Powered by Discourse, best viewed with JavaScript enabled"
2976,zerolight-improves-automotive-product-visualisation-quality-and-performance-with-vrs,"Originally published at:			ZeroLight Improves Automotive Product Visualisation Quality and Performance with VRS | NVIDIA Technical Blog
ZeroLight’s proprietary visualisation platform is used across the automotive industry. Offering real-time product rendering, hyper-realistic visuals, our proprietary supersampling technology, and fully configurable models, our platform has enabled automotive OEMs to exceed their customers’ expectations with a range of dynamic, high-quality experiences for various outputs. Virtual reality represents a key output for us because it…Powered by Discourse, best viewed with JavaScript enabled"
2977,nvidia-announces-nsight-systems-2019-3,"Originally published at:			NVIDIA announces Nsight Systems 2019.3 | NVIDIA Technical Blog
NVIDIA Nsight Systems 2019.3 is now available for download. In this release, Nsight Systems offers improvements for use-cases ranging from AI compute servers to graphics applications. The many enhancements in the 2019.3 release include improved user experience, easier data collection on servers, and upgraded result validation using statistics produced with SQLite exports. Additionally, graphics support…Powered by Discourse, best viewed with JavaScript enabled"
2978,about-the-technical-blogs-events-category,"Find discussions about our technical blogs, our live connect with experts events, recorded presentations and webinars.
Discuss the topics with peers, post questions for the presenters and authors.Powered by Discourse, best viewed with JavaScript enabled"
2979,how-ai-enabled-functionality-is-transforming-5g-ran,"Originally published at:			https://developer.nvidia.com/blog/how-ai-enabled-functionality-is-transforming-5g-ran/
AI is transforming 5G RAN in four key ways: energy savings, mobility management and optimization, load balancing, and Cloud RAN.Powered by Discourse, best viewed with JavaScript enabled"
2980,nvidia-dgx-superpod-delivers-world-record-supercomputing-to-any-enterprise,"Originally published at:			NVIDIA DGX SuperPOD Delivers World Record Supercomputing to Any Enterprise | NVIDIA Technical Blog
The NVIDIA DGX SuperPOD™ simplifies how the world approaches supercomputing, delivering world-record setting performance that can now be acquired by every enterprise in weeks instead of years. NVIDIA sets the bar once again in supercomputing, building a well-balanced system with 96 NVIDIA® DGX-2H™ servers containing 1,536 NVIDIA Tesla® V100 SXM3 GPUs. The DGX SuperPOD has earned the 22nd spot on…Powered by Discourse, best viewed with JavaScript enabled"
2981,mixed-precision-training-of-deep-neural-networks,"Originally published at:			Mixed-Precision Training of Deep Neural Networks | NVIDIA Technical Blog
Deep Neural Networks (DNNs) have lead to breakthroughs in a number of areas, including image processing and understanding, language modeling, language translation, speech processing, game playing, and many others. DNN complexity has been increasing to achieve these results, which in turn has increased the computational resources required to train these networks. Mixed-precision training lowers the required…May be I missed it - but what is the real speedup achieved for these models?What is the accuracy drop if FP8 or FP10 are used ?Have not tried the mixed precision in training but we did so with simulations so can share few remarks about the overall speedup here https://medium.com/@marcroj...Hi, where I can find the pre-trained models for Faster R-CNN and Multibox SSD (FP16 and FP32)?What's the `VGG-D` model in Table.1?Powered by Discourse, best viewed with JavaScript enabled"
2982,demystifying-nvidia-doca,"Originally published at:			https://developer.nvidia.com/blog/demystifying-doca/
The early access version of the NVIDIA DOCA SDK was announced earlier this year at GTC. DOCA marks our focus on finding new ways to accelerate computing. The emergence of the DPU paradigm as the evolution of SmartNICs is finally here. We enable developers and application architects to squeeze more value out of general-purpose CPUs…Powered by Discourse, best viewed with JavaScript enabled"
2983,drop-in-acceleration-of-gnu-octave,"Originally published at:			https://developer.nvidia.com/blog/drop-in-acceleration-gnu-octave/
cuBLAS is an implementation of the BLAS library that leverages the teraflops of performance provided by NVIDIA GPUs.  However, cuBLAS can not be used as a direct BLAS replacement for applications originally intended to run on the CPU. In order to use the cuBLAS API: a CUDA context first needs to be created a cuBLAS handle needs…Typo: ""terraflops"" = ""teraflops""I didn't understand your first paragraph. What's the point of drop-in library if refactoring existing code and recompiling is required? Maybe I misunderstood what cuBLAS drop-in feature is.cuBLAS is NVIDIA's BLAS implementation for NVIDIA GPUs. It's not the drop-in library and if its interface is closed to BLAS, it's not exactly the same. nvBLAS, on the other hand, is the drop-in library. You can see it as a wrapper of cuBLAS which can be used in place of any other BLAS. Hi, Sorry to come late to this discussion, hope to get an answer...I was looking at the test on matrix-matrix multiplication.As far as I see you get a speed-up of 306 only using OpenBLAS (765 / 2.5).I'am also using OpenBLAS, and I have a similar architecture (Intel Xeon CPU E5-2620 v4 @ 2.10GHz, 16 cores). However I get a speed-up of ""only"" 90, measured exactly in the same way you did, and using your octave script above.I was wondering if this may be due to the slightly different CPU (I have used E5-2620, you have used E5-2690) or to particular switches that must be used when compiling OpenBLAS in order to get an fully optimized library.Did you compile OpenBLAS with the ""standard"" command line or with particular options ?Any other suggestion ?Any help would be more than appreciatedKind regardsMarcoThe library was built using gfortran from GCC 4.7.3 following the installation guide from OpenBLAS webpage. We also verified the  Makefile log mentions the library was tuned for sandybridge cores.I can see one discrepancy in the post – wrong link to the Intel website.  It should be E5-2690 v2 (25M Cache, 3.00 GHz). 2x Ivy Bridge sockets totaling 20 cores. We will fix this. Though it does not completely explain the difference you are seeing. Could you please check  absolute FLOPs you are measuring? More recent Octave might just do a better job with SGEMMs with built-in BLASThanks for the answer. In the meanwhile I took a look with the system administrator working in my lab, and I actually discovered that OpenBLAS was not compiled at all, it was installed directly as a Debian package, and that's the reason of the difference in the benchmark. We re-compiled OpenBLAS on our architecture and we get impressive performances, as you have: 2.65 Gflops with standard BLAS, 238 with OpenBLAS as Debian package, 874 Gflops with re-compiled OpenBLAS, single precision. With double precision speed-up is smaller but still impressive (112 Gflops vs. 355 Gflops). I'm using 16 cores every time, not 20, that's why I think this is impressive.So, is there a step-by-step installation instruction for dummies? you know, not every scientist/engineer in this world is a programmer and system administrator. Is there a way to use it with Octave for Win64 ?Nice. I'm getting 25 GFLOPs on my default install on a i7-5930K. 395 GFLOPs using 11 threads on OpenBlas, and at least 2.7 TFLOPs using two GTX-1070s.(""at least"" because I'm in the middle of training a wide-resnet on both GPUs at the time test was run, so I suspect my GPUs performance would otherwise be even higher than 2.7 TF.)I'll take a 100x speedup any day of the week.If one has a function which consumes 80% of calculation time, the gpu acceleration ( to simplify the function calculation time is zeroed ) would be only 5 times. If one is processing big data mainly in IO manner it would be better accelaraion via buying an external RAID controller to SSD's in RAID 0 manner.Cool results - consider shared memory technique in LINUX: https://stackoverflow.com/q...and RAMDISK. If you are looking for max performance you should keep data in GPU and use CPU RAM as ordinary programmable disk. Did anyone seen sth. comparable to Octave enviroment for CUDA C ( interactive console mode on gpu-on-air data )?Powered by Discourse, best viewed with JavaScript enabled"
2984,ai-helps-create-rock-song-honoring-queen,"Originally published at:			AI Helps Create Rock Song Honoring Queen | NVIDIA Technical Blog
Making quality music is tough – it requires talent, time, and in most cases technological expertise.  This can be a major roadblock for YouTubers, filmmakers, video game designers, coffee shop singers, music producers, and many others.  To help accelerate the process – AIVA – a Luxembourg-based startup developed a tool that can help musicians and…Powered by Discourse, best viewed with JavaScript enabled"
2985,build-mainstream-servers-for-ai-training-and-5g-with-the-nvidia-h100-cnx,"Originally published at:			https://developer.nvidia.com/blog/build-mainstream-servers-for-ai-training-and-5g-with-the-nvidia-h100-cnx/
Learn about the H100 CNX, an innovative new hardware accelerator for GPU-accelerated I/O intensive workloads.Powered by Discourse, best viewed with JavaScript enabled"
2986,building-the-smart-cloud-using-the-best-smartnics-and-dpus-part-2,"Originally published at:			https://developer.nvidia.com/blog/building-the-smart-cloud-using-the-best-smartnics-and-dpus-part-2/
This post was originally published on the Mellanox blog. In part one, I said that the smart devices around us are changing our lives in remarkable ways. However, the infrastructure to support these smart innovations hasn’t fully evolved in terms of flexibility, performance, and efficiency. A software-defined world offers flexibility but at the cost of…Powered by Discourse, best viewed with JavaScript enabled"
2987,ai-detects-a-new-class-of-mutations-surrounding-autism,"Originally published at:			https://developer.nvidia.com/blog/detecting-a-new-class-of-mutations-behind-autism-with-ai/
A new research paper published in the journal Nature Genomics shows how a Princeton University-led team developed a deep learning-based method for searching the human genome for mutations, including their associations with diseases such as autism. “Many mutations in DNA that contribute to disease are not in actual genes but instead lie in the 99% of the genome…Powered by Discourse, best viewed with JavaScript enabled"
2988,optimizing-dx12-resource-uploads-to-the-gpu-using-cpu-visible-vram,"Originally published at:			https://developer.nvidia.com/blog/optimizing-dx12-resource-uploads-to-the-gpu-using-cpu-visible-vram/
How to optimize DX12 resource uploads from the CPU to the GPU over the PCIe bus is an old problem with many possible solutions, each with their pros and cons. In this post, I show how moving cherry-picked DX12 UPLOAD heaps to CPU-Visible VRAM (CVV) using NVAPI can be a simple solution to speed up…Powered by Discourse, best viewed with JavaScript enabled"
2989,in-game-gpu-profiling-for-directx-12-using-setbackgroundprocessingmode,"Originally published at:			https://developer.nvidia.com/blog/in-game-gpu-profiling-for-dx12-using-setbackgroundprocessingmode/
Leanr best practices for performing in-game GPU profiling while monitoring the state of the background driver optimizations, using the DX12 SetBackgroundProcessingMode API on NVIDIA GPUs.Powered by Discourse, best viewed with JavaScript enabled"
2990,virtual-3d-teleportation-in-real-time-with-nvidia-gpus,"Originally published at:			https://developer.nvidia.com/blog/virtual-3d-teleportation-in-real-time-with-nvidia-gpus/
Imagine being able to virtually teleport from one space to another in real time. With 3D-capture cameras and a mixed reality display such as HoloLens, Microsoft Research’s new ‘haloportation’ innovation allows users to see, hear, and interact with remote participants in 3D as if they are actually present in the same physical space. Custom software…Powered by Discourse, best viewed with JavaScript enabled"
2991,vr-sli-accelerating-opengl-virtual-reality-with-multi-gpu-rendering,"Originally published at:			https://developer.nvidia.com/blog/vr-sli-accelerating-opengl-virtual-reality-multi-gpu-rendering/
High-performance stereo head-mounted display (HMD) rendering is a fundamental component of the virtual reality ecosystem. HMD rendering requires substantial graphics horsepower to deliver high-quality, high-resolution stereo rendering with a high frame rate. Today, NVIDIA is releasing VR SLI for OpenGL via a new OpenGL extension called “GL_NVX_linked_gpu_multicast” that can be used to greatly improve the…cant download the driver, just says ""unable to download,try later""Interesting how there is no VR and yet it seems to be talked about and hyped as much as something that actually exists at a consumer level... Dev kits in the hands of a small amount of people don't count.so???????????????????????????????????????????????????????????200.000 dev units sold total and Gear VR is sold to consumers right now. Besides, there's no point in selling VR if there's no graphic driver support and that's what's happening now :) Release of consumer HMDs are slated Q1 2015 so they're all in a hurry to get things ready.why Company of Heroes 2 Ardennes Assault not working with nvidia drivers ?but it's working with cpu ghraphics when unistall the driverfailed to find a supported hardware rendering device.ensure that your system meets the minimum requirements...............my card is gigabyte gtx 670Q1 2016*Can you submit feedback through the driver feedback form so that we may investigate?  http://surveys.nvidia.com/i...If you wish to chat with an NVIDIA Customer Care representative, you may reach the NVIDIA Customer Care team at www.nvidia.com/nvccim having trouble downloading a driver also. The version is the 361.43 which was released on 12/21/2015. Im downloading this the day of the release yet it is saying that i have no internet connection, yet i am at my house trying to download it. I have just gotten rainbow six siege and it is saying i have an outdated driver and to update for maximum speed. If you could please respond back to me that would be very helpful. Thank you.Thank you, the driver has download sucesfull :3wait for next year, everything is gonna explode with vr...people see it therefore people prepareI cant download it either. Last week i downloaded Windows 10, and since then my Nvidia Geforce Experience hasn't been able to download. It says; that this Nvidia-graficcard isn't compatible with this version of windows, but on your webside it says that it work with Windows 10.thanks for replyingcant download the driver ""GeForce Gaming Driver"" 21.12.2015 Computer MSI Leopard pro NVIDIA DTX 950MThis is quite interresting as it shows the progress the NVidia driver engineers are making for VR support.cant download the driver, just says ""unable to download,try later"" too. ı am attempting to download gt 730, help me pleaseWhich operating system are you trying to download for?Which operating system?  Are you downloading through GeForce Experience or our website?i cant play assassin's creed syndicate with gt610. too much lagging, but u said gt 610 supports that game. mine is core i5+8GB of RAM..I fix the problem via regedit. THANKS for responsiblePowered by Discourse, best viewed with JavaScript enabled"
2992,gtc-2020-nvidia-vgpu-virtualizing-nvidia-gpus,"GTC 2020 CWE21712
Presenters: Doug-Traill,NVIDIA; Peter-Bernard, ; Randall-Siggers, ; John-Li, ; Emily-Apsey, ; Jimmy-Rotella, ; Mitch-VanDyken,
Abstract
Meet NVIDIANs who have helped customers deploy NVIDIA vGPUs and learn from their experience. Ask questions, get help deploying an NVIDIA vGPU, give feedback, or just chat with us!Watch this session
Join in the conversation below.Useful session. Would have like to see a live demo or usecase demonstration.Powered by Discourse, best viewed with JavaScript enabled"
2993,advanced-api-performance-setstablepowerstate,"Originally published at:			https://developer.nvidia.com/blog/advanced-api-performance-setstablepowerstate/
This post covers best practices for using SetStablePowerState on NVIDIA GPUs. To get a high and consistent frame rate in your applications, see all Advanced API Performance tips.Thanks for the post!
After many tries, this use of nvidia-smi --lock functions gave us the persistent performance the project requiresIt is very useful APIs when using the GPU as a DSP that need to work without any frequency lowering / idle behaviorRelated to this post,
can you explain the different usage of the following Performance approaches:Use “Nvidia Control Panel” GUI settingsUse nvidia-smi command line callsUse nvml CUDA APII prefers to use NVML to get the GPU persistent behavior, is it possible to get such persistent behavior only using nvml on GeForce RTX 3050 ?Hi!That’s great news.Did you encounter differences between the article’s recommendations and your usage? I’d like to update it to match current uses. nvidia-smi isn’t guaranteed to be a fixed target, unfortunately and it’s been a while since I wrote this article!Thanks, RyanHello again-I’m not familiar with what you’re referring to with 1/control panel, can you elaborate?I’m also not certain about the differences between 2 and 3. I suspect on the backend they are poking the same driver components to achieve their goals, because there’s basically one place to set these things. Unfortunately, the best way in the near term to establish this is probably to do a bit of testing.I’ll try to find the maintainers of these routes and report back if/when I get definitive answers.Thanks, RyanHello,
This sequence from the post works perfectly to set GPU core & GPU memory clock frequenciesnvidia-smi --query-supported-clocks=timestamp,gpu_name,gpu_uuid,memory,graphics --format=csv`
nvidia-smi --lock-gpu-clocks=<core_clock_rate_from_csv>
nvidia-smi --lock-memory-clocks=<memory_clock_rate_from_csv>There is additional GPU performance option on:
Windows → “Nvidia Control Panel” → “3D setting” → “Manage 3D setting” → “Global Setting” / “Program Setting” → “Power management mode” = “Prefer maximum performance”Ah, I would not rely on that setting. That’s more of a behavioral suggestion, it doesn’t pin the frequencies.nvidia-smi --query-supported-clocks=timestamp,gpu_name,gpu_uuid,memory,graphics --format=csv`
nvidia-smi --lock-gpu-clocks=<core_clock_rate_from_csv>
nvidia-smi --lock-memory-clocks=<memory_clock_rate_from_csv>Excellent, so it looks like the commands are the same. Thanks!Powered by Discourse, best viewed with JavaScript enabled"
2994,nvidia-chief-scientist-highlights-new-ai-research-in-gtc-keynote,"Originally published at:			NVIDIA Chief Scientist Highlights New AI Research in GTC Keynote | NVIDIA Technical Blog
In a keynote released this week for a virtual GTC China event, NVIDIA’s chief scientist Bill Dally described how his team is driving an annual doubling of AI performance. Dally delves into NVIDIA’s domain-specific platforms for a variety of industries such as healthcare, self-driving cars and robotics. The keynote is just one of more than 220 sessions…Powered by Discourse, best viewed with JavaScript enabled"
2995,aws-launches-first-nvidia-gpu-accelerated-graviton-based-instance-with-amazon-ec2-g5g,"Originally published at:			AWS Launches First NVIDIA GPU-Accelerated Graviton-Based Instance with Amazon EC2 G5g | NVIDIA Technical Blog
The new Amazon EC2 G5g instances feature the AWS Graviton2 processors and NVIDIA T4G Tensor Core GPUs, to power rich android game streaming for mobile devices.Powered by Discourse, best viewed with JavaScript enabled"
2996,ai-generates-images-of-a-finished-meal-using-only-a-written-recipe,"Originally published at:			https://developer.nvidia.com/blog/ai-generates-images-of-a-finished-meal-using-only-a-written-recipe/
In computer vision, creating an image of a long list of text is complicated. To help accelerate research in this field, a team from Tel Aviv University in Israel developed a deep learning-based system that can automatically generate pictures of a finished meal from a simple text-based recipe. “We propose a novel task of synthesizing…Powered by Discourse, best viewed with JavaScript enabled"
2997,isc20-featured-demo-visualizing-150-terabytes-of-data,"Originally published at:			ISC20 Featured Demo: Visualizing 150 Terabytes of Data | NVIDIA Technical Blog
It can take up to 10 months for a spacecraft to get from Earth to Mars. But the entire journey can be in vain if something goes wrong in the last six minutes. To plan the landing for NASA’s first manned mission to Mars, a team of NASA scientists and engineers are relying on high-resolution,…Powered by Discourse, best viewed with JavaScript enabled"
2998,ray-tracing,"Hey all,What is the most exciting research you’ve seen recently in the area of ray tracing?There is lots!  It is tough for me to choose but I will pick 2.  The ReSTIR work (more broadly, intelligently weighting samples), and the differentiable rendering work which seems pretty critical for predictive engineering applications.Powered by Discourse, best viewed with JavaScript enabled"
2999,change-the-rules-of-the-game-nvidia-omniverse-brings-an-arsenal-of-rtx-and-ai-powered-apps-extensions-and-diy-toolkits-to-accelerate-game-development-pipelines,"Originally published at:			https://developer.nvidia.com/blog/change-the-rules-of-the-game-nvidia-omniverse-brings-an-arsenal-of-rtx-and-ai-powered-apps-extensions-and-diy-toolkits-to-accelerate-game-development-pipelines/
At GDC 2021, NVIDIA introduced a suite of Omniverse apps and tools to simplify and accelerate game development content creation pipelines.Powered by Discourse, best viewed with JavaScript enabled"
3000,transforming-ai-healthcare-with-federated-learning,"Originally published at:			https://developer.nvidia.com/blog/transforming-ai-healthcare-with-federated-learning/
NVIDIA researchers, in collaboration with Owkin scientists, a premier member of NVIDIA Inception, as well as other scientists, recently published a new research paper on Nature Partner Journals Digital Medicine about the future of digital health with federated learning.Powered by Discourse, best viewed with JavaScript enabled"
3001,deep-learning-for-image-understanding-in-planetary-science,"Originally published at:			https://developer.nvidia.com/blog/deep-learning-image-understanding-planetary-science/
Went from training 700 img/s in MNIST to 1500 img/s (using CUDA) to 4000 img/s (using cuDNN) that is just freaking amazing! @GPUComputing — Leon Palafox (@leonpalafox) March 27, 2015 I stumbled upon the above tweet by Leon Palafox, a Postdoctoral Fellow at the The University of Arizona Lunar and Planetary Laboratory, and reached out…Powered by Discourse, best viewed with JavaScript enabled"
3002,choosing-nvidia-spectrum-for-microsoft-azure-sonic,"Originally published at:			https://developer.nvidia.com/blog/choosing-spectrum-for-microsoft-azure-sonic/
NVIDIA supports open Ethernet and contributes innovations to the SONiC developer community project.Powered by Discourse, best viewed with JavaScript enabled"
3003,explainer-what-is-accelerated-computing,"Originally published at:			What Is Accelerated Computing? | NVIDIA Blog
Accelerated computing uses parallel processing to speed up work on demanding applications, from AI and data analytics to simulations and visualizations.Powered by Discourse, best viewed with JavaScript enabled"
3004,dlss-three-things-you-need-to-know,"Originally published at:			DLSS: Three Things you Need to Know | NVIDIA Technical Blog
In this video, Andrew Edelsten, a Deep Learning Engineering Director at NVIDIA, details the three most important things developers need to know about DLSS. You can learn more by attending Andrew’s talk at GDC: Truly Next-Gen: Adding Deep Learning to Games & GraphicsPresenters:Andrew Edelsten (Director, Developer Technologies (Deep Learning), NVIDIA)Paula Jukarainen (Developer Technology Engineer, NVIDIA)Anjul…Powered by Discourse, best viewed with JavaScript enabled"
3005,deep-learning-helps-robot-learn-to-walk-the-way-humans-do,"Originally published at:			https://developer.nvidia.com/blog/humanoid-robot-learns-to-walk-like-a-human/
University of California, Berkeley researchers are using deep learning and NVIDIA GPUs to create a new generation of robots that adapt to changing environments and new situations without a human reprogramming them. Their robot “Darwin” learned how to keep his balance on an uneven surface – and GPUs were essential for learning of this complexity.…Powered by Discourse, best viewed with JavaScript enabled"
3006,summit-gpu-supercomputer-enables-smarter-science,"Originally published at:			Summit GPU Supercomputer Enables Smarter Science | NVIDIA Technical Blog
Today the world of open science received its greatest asset in the form of the Summit supercomputer at Oak Ridge National Laboratory (ORNL). This represents an historic milestone because it is the world’s first supercomputer fusing high performance, data-intensive, and AI computing into one system. Summit is capable of delivering a peak 200 petaflops, ten…The best technology in the worldUntil the Chinese steal the design and make a faster one. Suspect they already have bribed the right folks.NVLink is very fast (300GB/s), but would the speed be bottlenecked by the inter-node connection?  What is the inter-node bandwidth (or data rate) of the networking within this supercomputer? Thanks a lot!Powered by Discourse, best viewed with JavaScript enabled"
3007,mesh-shading-three-things-you-need-to-know,"Originally published at:			Mesh Shading: Three Things You Need to Know | NVIDIA Technical Blog
In this video, Manuel Kraemer, Senior Software Engineer at NVIDIA, details the three most important things developers need to know about mesh shading. To learn more, you can attend their talk at GDC:Title: ASTEROIDS MESH SHADING WITH DX12Location: Room 205, South HallDate: Friday, March 22Time: 3:00pm – 4:00pmPass Type: All Access, GDC Conference + Summits,…Powered by Discourse, best viewed with JavaScript enabled"
3008,openai-s-jukebox-produces-music-with-lyrics-from-scratch,"Originally published at:			OpenAI’s Jukebox Produces Music with Lyrics from Scratch | NVIDIA Technical Blog
This week, OpenAI released Jukebox, a neural network that generates music with rudimentary singing, in a variety of genres and artist styles.  “Provided with genre, artist, and lyrics as input, Jukebox outputs a new music sample produced from scratch,” the company stated in their post, Jukebox. Generating CD-quality music is a challenging problem to solve,…Powered by Discourse, best viewed with JavaScript enabled"
3009,improving-machine-learning-security-skills-at-a-def-con-competition,"Originally published at:			https://developer.nvidia.com/blog/improving-machine-learning-security-skills-at-a-def-con-competition/
NVIDIA recently helped run an innovative competition at DEF CON 30, providing an opportunity for security and data professions to improve their machine learning security skills.Powered by Discourse, best viewed with JavaScript enabled"
3010,creating-an-apparel-detection-app-with-nvidia-gpu-accelerated-ai-technologies,"Originally published at:			https://developer.nvidia.com/blog/creating-an-apparel-detection-app-with-nvidia-gpu-accelerated-ai-technologies/
Vision AI solution provider Drishtic AI developed an apparel detection application using NVIDIA TAO toolkit and DeepStream SDK.Powered by Discourse, best viewed with JavaScript enabled"
3011,cudacasts-episode-1-installing-cuda-on-windows,"Originally published at:			https://developer.nvidia.com/blog/cudacasts-episode-1-installing-cuda-windows/
Today I’m excited to announce CUDACasts, a brand new series of short, useful screencast videos about parallel programming on the CUDA platform. I’ll be bringing you new CUDACasts every week covering topics including step-by-step beginner how-tos, programming techniques and CUDA Pro Tips, and overviews of new CUDA features and tools. CUDACasts Episode #1 gives an…Hello SIr,im really interested in doing my postgraduate research on cuda pogramming model. The challenge im facing is that my computer is intel, with HD Graphics 4000.How can i install cuda on my computer which does not have Nvidia GPU?Thanks alotabdullahi ibrahimYou can certainly install the CUDA Toolkit. And you will be able to compile CUDA programs. However you won't be able to run them on your local machine. If you have access to a remote Linux server with a GPU, you could develop on it over VNC, SSH, etc. You can even use NSight Eclipse edition on a local Linux machine to develop and run on a remote Linux machine. You can also run in the cloud on Amazon AWS nodes with GPUs. Finally, you could buy a new GPU if your computer is a desktop machine, or try out a Jetson TX1 for embedded / low power development with CUDA.Hello,I have a problem.When I use cuda4.2,the fftPlan1d or fftPlanMany is very fast which nearly only need 2ms;but when I use cuda6.5 and other high version ,it cost nearly 600ms. why???my email is DuLinRain@mail.hfut.edu.cnPLEASE HELP MEHello sir i am a photographer and i am using intel i5 vth gen. board-intel DH87RL plz suggest me the nvidia graphic card for photoshop and after effect alsoHas anyone figured out WHY the new After Effects 15.3 (on MAC) will not use the GPU for 3D rendering yet? The previous version worked JUST FINE... just trying to find some answers. :-/Powered by Discourse, best viewed with JavaScript enabled"
3012,nvlink-pascal-and-stacked-memory-feeding-the-appetite-for-big-data,"Originally published at:			https://developer.nvidia.com/blog/nvlink-pascal-stacked-memory-feeding-appetite-big-data/
For more recent info on NVLink, check out the post, “How NVLink Will Enable Faster, Easier Multi-GPU Computing”. NVIDIA GPU accelerators have emerged in High-Performance Computing as an energy-efficient way to provide significant compute capability. The Green500 supercomputer list makes this clear: the top 10 supercomputers on the list feature NVIDIA GPUs. Today at the 2014…This is pretty cool stuff. Looking at the future is always interesting. What fun problems to solve.Kepler survives on quite well. Have to unfortunately replace my GTX 680, going from Kepler to Kepler! Maybe an upgrade to GK110 though.Have any CPU vendors already committed to support NVLink? I could imagine that there was little incentive for AMD/Intel to foster their competition.IBM has announced planned support for NVLink in future Power CPUs.http://nvidianews.nvidia.co...Makes sense. Close integration of GPU and Power cores is more promising than pairing the GPU with an ARM CPU. Looks like there is no shortage of exciting new supercomputer architectures. :-)Can any estimates on latency and max. bus lengths be shared? I'm curious if NVLink could be used only as an in-rack network, or even beyond.In a multi-GPU setup, will the available bandwidth between GPUs (and the CPU) be configurable? If so, will it be a hardware or software setting?Unfortunately it's too early to provide detail at this level. Stay tuned.It's too early to provide detail at this level. Stay tuned.how can I get Specs for NVLINK?We have not provided any more details in terms of specs for NVLINK.  Stay tuned!I have a Xeon 1230V2 (Ivy Bridge), just got it a few months ago so I will not be upgrading for awhile, will these GPU's be backward compatible or will I have to get a whole new Motherboard/Processor to use these things? Maybe an adapter will be made available for us? My tech knowledge is not novice but limited so I hope someone can at least give me a good guess. I know I may not be able to benefit from the 80+GB bandwidth but I could benefit from all the other enhancements. Even though this is extreamly bad ass stuff. We the consumers dont really need this yet... We havent even touched the bandwith pcie 3.0 has...This is true with GDDR5 but with HBM 2.0 we will indeed, and that is why this is coming out too as Nvidia had to do something as PCIE 3.0 was holding back performance. This will allow 60+FPS 4K on 1 card.Will the NVLink specifications be available for external high-speed I/O devices (FPGAs) to send data directly to GPUs for processing?Powered by Discourse, best viewed with JavaScript enabled"
3013,ai-helps-detect-prostate-cancer,"Originally published at:			AI Helps Detect Prostate Cancer | NVIDIA Technical Blog
Prostate cancer is expected to be the leading source of new cancer for men and the second most frequent cause of death after lung cancer. It is also cancer that is very hard to detect, and small lesions can comprise just a fraction of 1% of the tissue surface. To help solve the problem, researchers…Powered by Discourse, best viewed with JavaScript enabled"
3014,google-cloud-lowers-the-price-of-nvidia-tesla-gpus,"Originally published at:			Google Cloud Lowers the Price of NVIDIA Tesla GPUs | NVIDIA Technical Blog
Google announced they are cutting the price of NVIDIA Tesla GPUs in the cloud by up to 36 percent. In US regions, each K80 GPU attached to a Google Compute Engine virtual machine is priced at $0.45 per hour while each P100 costs $1.46 per hour.  “Scientists, artists and engineers need access to massively parallel…Powered by Discourse, best viewed with JavaScript enabled"
3015,nvidia-rtx-top-3-week-of-december-7-2018,"Originally published at:			NVIDIA RTX Top 3: Week of December 7, 2018 | NVIDIA Technical Blog
Every week, we’ll be delivering 3 interesting stories coming from the world of RTX Game Development. NVIDIA Invents AI Interactive Graphics Using a conditional generative neural network as a starting point, a team at NVIDIA trained a neural network to render new 3D environments, after being trained on existing videos. This AI breakthrough will allow…Powered by Discourse, best viewed with JavaScript enabled"
3016,new-nvidia-maxine-microservices-enhance-real-time-audio-and-video-effects-for-conferences,"Originally published at:			https://developer.nvidia.com/blog/new-maxine-microservices-enhance-real-time-audio-and-video-effects-for-video-conferences-at-scale/
At CES 2023, NVIDIA Maxine announced SDK updates and new microservices, enabling clear communications in video conferences through private or public clouds. NVIDIA Maxine is a suite of GPU-accelerated AI SDKs and cloud-native microservices for deploying optimized and accelerated AI features that enhance audio, video, and augmented reality (AR) effects for real-time communications. Powered by…Powered by Discourse, best viewed with JavaScript enabled"
3017,nvidia-rtx-top-3-week-of-february-14-2019,"Originally published at:			NVIDIA RTX Top 3: Week of February 14, 2019 | NVIDIA Technical Blog
Every week, we’ll be delivering 3 interesting stories coming from the world of RTX Game Development. 1 – Unreal Engine 4.22 Preview 1 is now available with support for real-time ray tracing Preview 1 includes support for real-time ray tracing features, such as real-time global illumination, translucency, clearcoat and more. It also features a denoiser…Powered by Discourse, best viewed with JavaScript enabled"
3018,using-fortran-standard-parallel-programming-for-gpu-acceleration,"Originally published at:			https://developer.nvidia.com/blog/using-fortran-standard-parallel-programming-for-gpu-acceleration/
We present lessons learned from refactoring a Fortran application to use modern do concurrent loops in place of OpenACC for GPU acceleration.Powered by Discourse, best viewed with JavaScript enabled"
3019,scaling-vasp-with-nvidia-magnum-io,"Originally published at:			https://developer.nvidia.com/blog/scaling-vasp-with-nvidia-magnum-io/
You could make an argument that the history of civilization and technological advancement is the history of the search and discovery of materials. Ages are named not for leaders or civilizations but for the materials that defined them: Stone Age, Bronze Age, and so on. The current digital or information age could be renamed the…I had the pleasure of working with the other authors to learn a lot more about VASP, quantum chemistry, and how asynchronous parallel communications helps many GPUs to scale applications.  It’s great to illustrate the benefits of Magnum IO NCCL with hard data.  If you have any question or comments, please let us know.Powered by Discourse, best viewed with JavaScript enabled"
3020,gpu-accelerated-supercomputer-helps-automatically-detect-skin-cancer,"Originally published at:			GPU-accelerated Supercomputer Helps Automatically Detect Skin Cancer | NVIDIA Technical Blog
Researchers at the University of Queensland in Australia are using GPU-accelerated supercomputers and deep learning to diagnose skin cancer from histology slides with the same accuracy as a trained pathologist.   “Pathologists do an incredible job given the enormity of their task. They are in many cases looking for a needle in a very large haystack,…Powered by Discourse, best viewed with JavaScript enabled"
3021,free-dli-mini-self-paced-course-assemble-a-simple-robot-in-nvidia-isaac-sim,"Originally published at:			Courses – NVIDIA
This self-paced, free tutorial provides a basic understanding of the Isaac Sim interface and the documentation needed to begin robot simulation projects.Powered by Discourse, best viewed with JavaScript enabled"
3022,nvidia-morpheus-helps-defend-against-spear-phishing-with-generative-ai,"Originally published at:			https://developer.nvidia.com/blog/nvidia-morpheus-helps-defend-against-spear-phishing-with-generative-ai/
The NVIDIA Morpheus cybersecurity framework helps defend against spear phishing using generative AI.Where can I get more information? I want to know how to deploy my data processing and execution, thank you very muchHi there! Thanks for reaching out! You can get more information on the latest ways you can leverage NVIDIA Morpheus, including the spear phishing use case by watching Bartley Richardson’s GTC session on demand here Learn About New AI-Based Cybersecurity Use Cases and Capabilities | NVIDIA On-DemandPowered by Discourse, best viewed with JavaScript enabled"
3023,nvidia-hpc-sdk-20-11-now-available,"Originally published at:			NVIDIA HPC SDK 20.11 Now Available | NVIDIA Technical Blog
The latest update the NVIDIA HPC SDK is now available for download. The NVIDIA HPC SDK is a comprehensive suite of compilers, libraries, and tools enabling HPC developers to program the entire HPC platform from the GPU foundation to the CPU, and through the interconnect. It is the only comprehensive, integrated SDK for programming accelerated…Powered by Discourse, best viewed with JavaScript enabled"
3024,accelerating-relu-and-gelu-activation-functions-and-batched-sparse-gemm-in-cusparselt-v0-2-0,"Originally published at:			Accelerating ReLu and GeLu Activation Functions, and Batched Sparse GEMM in cuSPARSELt v0.2.0 | NVIDIA Technical Blog
NVIDIA cuSPARSELt v0.2 now supports ReLu and GeLu activation functions, bias vector, and batched Sparse GEMM.Powered by Discourse, best viewed with JavaScript enabled"
3025,make-sense-of-the-universe-with-rapids-ai,"Originally published at:			Make Sense of the Universe with Rapids.ai | NVIDIA Technical Blog
Image from Large Synoptic Survey Telescope (LSST) Classification of astronomical sources in the night sky is important for understanding the universe. It helps us understand the properties of what makes up celestial systems, from our solar system to the most distant galaxy and everything in between. The Photometric LSST Astronomical Time-Series Classification Challenge (PLAsTiCC) wants to…This is great work.  Congratulations !This is outstanding.  I, too, am working with RAPIDS, to formulate a solution to classify objects from NASA's WFIRST and TESS missions.  The link to the annotated source code seems to be broken.Powered by Discourse, best viewed with JavaScript enabled"
3026,deep-learning-and-ros-collide-to-bring-new-levels-of-autonomy-to-robots,"Originally published at:			https://developer.nvidia.com/blog/deep-learning-and-ros-collide-to-bring-new-levels-of-autonomy-to-robots/
The NVIDIA Jetson team was in Seoul, Korea last week at ROSCon. More than 450 attendees from across the globe trekked to the conference to learn and network with the ROS (Robot Operating System) community. As a Platinum sponsor, and backer of the Open Source Robotics Foundation for the past three years, NVIDIA is a…Powered by Discourse, best viewed with JavaScript enabled"
3027,new-cublas-12-0-features-and-matrix-multiplication-performance-on-nvidia-hopper-gpus,"Originally published at:			https://developer.nvidia.com/blog/new-cublas-12-0-features-and-matrix-multiplication-performance-on-nvidia-hopper-gpus/
Explore the NVIDIA cuBLAS library in CUDA 12.0, including the recently-introduced FP8 format, GEMM performance on NVIDIA Hopper GPUs, and user experience improvements.Powered by Discourse, best viewed with JavaScript enabled"
3028,reinforcing-the-value-of-simulation-by-teaching-dexterity-to-a-real-robot-hand,"Originally published at:			https://developer.nvidia.com/blog/reinforcing-the-value-of-simulation-by-teaching-dexterity-to-a-real-robot-hand/
The human hand is one of the most remarkable outcomes of millions of years of evolution. The ability to pick up all sorts of objects and use them as tools is a crucial differentiator enabling us to shape our world. For robots to work in the everyday human world, the ability to deftly interact with…What a wonderful work! (“What a time to be alive!” ;-)Now the proof is made that the training of humanoid robotic hand in simulation (and sim2real) can be carried out on a few GPUs and are no longer reserved for a few happy people who have access to unlimited funds …The gap between the training time, with and without randomization (for sim2real), seems to be in par (proportionally) with the previous work (cf. OpenAI).So, if I understood correctly, we should be able to train / try various tasks in fast simulations on a single GPU (as, say: 1-6h / A100 [1], and even 2h / RTX 3090 ! [2]) and then, when we are satisfied, we can train them with randomization with a more powerful workstation.The article talks about 1.5/2.5 days on AWS g5.48xlarge (8 x A10G GPU) so if things scale linearly it should be possible to train with randomization (sim2real) on a “simple” workstation with 4 x RTX 3080Ti/3090 within 3/5 days… Amazing!Also, maybe we will see the emergence of a kind of “transfer learning” for humanoid robotic hand…[1] https://arxiv.org/pdf/2108.10470.pdf , “Isaac Gym: High Performance GPU-Based Physics Simulation For Robot Learning”
[2] ttps://sites.google.com/view/isaacgymIndeed that’s right, and this is one of the reasons we are so pleased to be able to help democratize this capability.You can see a demo here of sim2real transfer of a simpler problem (quadruped locomotion) that’s fast enough to be run live on stage in a handful of minutes: Keynote speech Marco Hutter ICRA 2022 - YouTubeGlad you enjoyed our post!Take care,
-GavThanks for the feedback and the interesting link.Definitely, being able to see the progress of training directly on the robot in “almost” real time is very appreciable.
On this point, I suppose that it is also a question of computing power that you can put. But it should be feasible even for more complex tasks. At least it deserves to be tested.keep up the good work!-)
I’m looking forward to your code …
Best wishes for the holiday season!Powered by Discourse, best viewed with JavaScript enabled"
3029,announcing-nvidia-cuda-11-3-toolkit-availability-and-preview-release-of-cuda-python,"Originally published at:			Announcing NVIDIA CUDA 11.3 Toolkit Availability and Preview Release of CUDA Python | NVIDIA Technical Blog
The CUDA toolkit and development environment, consists of GPU-accelerated libraries, debugging and optimization tools, a C/C++ compiler, and a runtime library to build and deploy your application on major architectures.Why the download link points to 11.1?@easton.hou – Which link are you referring to?Powered by Discourse, best viewed with JavaScript enabled"
3030,q-amp-a-with-remedy-entertainment-adopting-usd-into-the-game-development-pipeline,"Originally published at:			https://developer.nvidia.com/blog/qa-with-remedy-entertainment-adopting-usd-into-the-game-development-pipeline/
Mika Vehkala has worked in the game development industry for nearly three decades and contributed to projects such as Horizon Zero Dawn and The Walking Dead: No Man’s Land. Now, he’s director of technology at Remedy Entertainment, the studio that created Alan Wake and Control. Vehkala spoke with NVIDIA about embracing USD into its game…Powered by Discourse, best viewed with JavaScript enabled"
3031,nvidia-looking-for-worlds-best-ai-startup,"Originally published at:			https://developer.nvidia.com/blog/nvidia-looking-for-worlds-best-ai-startup/
At the 2018 GPU Technology Conference in San Jose on March 26-29, the NVIDIA Inception virtual accelerator program will host a $1 million competition to find this year’s best AI startup across three categories: Healthcare, Autonomous Systems, and Enterprise. Twelve startups will compete live on stage for the award in front of an audience of…Powered by Discourse, best viewed with JavaScript enabled"
3032,using-neural-nano-optics-researchers-create-a-camera-the-size-of-a-salt-grain,"Originally published at:			https://developer.nvidia.com/blog/using-neural-nano-optics-researchers-create-a-camera-the-size-of-a-salt-grain/
The groundbreaking technology uses an optical metasurface and machine-learning algorithms to produce high-quality color images with a wide field of view.Powered by Discourse, best viewed with JavaScript enabled"
3033,solving-entry-level-edge-ai-challenges-with-nvidia-jetson-orin-nano,"Originally published at:			Solving Entry-Level Edge AI Challenges with NVIDIA Jetson Orin Nano | NVIDIA Technical Blog
NVIDIA Jetson Orin Nano series system-on-modules (SoMs) deliver up to 80x the AI performance of NVIDIA Jetson Nano and set the new standard for entry-level edge AI.Powered by Discourse, best viewed with JavaScript enabled"
3034,federated-learning-with-homomorphic-encryption,"Originally published at:			https://developer.nvidia.com/blog/federated-learning-with-homomorphic-encryption/
In NVIDIA Clara Train 4.0, we added homomorphic encryption (HE) tools for federated learning (FL). HE enables you to compute data while the data is still encrypted. In Clara Train 3.1, all clients used certified SSL channels to communicate their local model updates with the server. The SSL certificates are needed to establish trusted communication…Powered by Discourse, best viewed with JavaScript enabled"
3035,using-semaphore-and-memory-sharing-extensions-for-vulkan-interop-with-nvidia-opencl,"Originally published at:			https://developer.nvidia.com/blog/using-semaphore-and-memory-sharing-extensions-for-vulkan-interop-with-opencl/
New OpenCL support for Vulkan interoperability using semaphores and memory sharing extensions.Thank you for this information.  The sample code is not allowing download: http://developer.download.nvidia.com/compute/DevZone/OpenCL/Projects/vk_ocl_interop_samples.zipAlso, do you know of any good resources for installing OpenCL on RHEL 8 or do most people install from source?  Thanks for your time.Hi! Sorry to hear this. What error message are you getting? Are you on a Windows or Linux platform? What browser are you using? If Windows, have you checked your Download folder to see if the Zip file is there? We will continue to investigate why this is happening.The sample doesn’t work.OpenCL\src\vulkanImageOpenCL\vulkanImageOpenCL.cpp:auto vertShaderCode = readFile(“shader.vert”); // GLSL source code! :)))
…
VkShaderModule vertShaderModule = createShaderModule(vertShaderCode); // Crash :))))
Vulkan doesn’t know about GLSL :)))Thanks for trying out and reporting the issue. We are working on updating the samples to use SPIR-V shaders.༼ つ ◕__◕ ༽つ Finally! Was waiting for some kind of convergence with OpenCL interops, if seemed to have been dropped there in limbo for a while 😅Powered by Discourse, best viewed with JavaScript enabled"
3036,determined-ai-deep-learning-application-now-on-the-ngc-catalog,"Originally published at:			Determined AI Deep Learning Application now on the NGC Catalog | NVIDIA Technical Blog
As AI becomes universal, enterprise leaders are looking to empower their AI teams with fully integrated and automated development environments. Determined AI’s application available in the NVIDIA NGC catalog, a GPU-optimized hub for AI applications, provides an open-source platform that enables deep learning engineers to focus on building models and not managing infrastructure. Determined AI is…Powered by Discourse, best viewed with JavaScript enabled"
3037,open-beta-nvidia-cunumeric-and-nvidia-legate,"Originally published at:			cuNumeric Library Download | NVIDIA Developer
NVIDIA announces the cuNumeric and Legate beta release. The cuNumeric library provides an accelerated NumPy alternative, while Legate provides a parallel computing runtime abstraction layer.Powered by Discourse, best viewed with JavaScript enabled"
3038,close-knowledge-gaps-and-elevate-training-with-digital-twin-nvidia-air,"Originally published at:			Close Knowledge Gaps and Elevate Training with Digital Twin NVIDIA Air | NVIDIA Technical Blog
Learn about the NVIDIA Air platform,  a fully functional digital twin of a production environment.Powered by Discourse, best viewed with JavaScript enabled"
3039,nvidia-vrss-2-dynamic-foveated-rendering-no-assembly-required,"Originally published at:			NVIDIA VRSS 2: Dynamic Foveated Rendering, No Assembly Required | NVIDIA Technical Blog
NVIDIA is releasing the latest version of Variable Rate Supersampling (VRSS), which now includes gaze-tracked foveated rendering.I’m a Unity developer. Do I have to send every build to Nvidia to run VRSS mode? I don’t know how I’d handle that, not to mention the fact that it’s classified automotive development data.
Isn’t there some human way to run VRSS for Unity builds?
Thank you.Please email me at ppang@nvidia.com to discuss your VRSS questions in more detail. Typically yes, each build needs to be submitted and we can go over potential options.Powered by Discourse, best viewed with JavaScript enabled"
3040,gradient-boosting-decision-trees-and-xgboost-with-cuda,"Originally published at:			Gradient Boosting, Decision Trees and XGBoost with CUDA | NVIDIA Technical Blog
Gradient boosting is a powerful machine learning algorithm used to achieve state-of-the-art accuracy on a variety of tasks such as regression, classification and ranking. It has achieved notice in machine learning competitions in recent years by “winning practically every competition in the structured data category”. If you don’t use deep neural networks for your problem,…Powered by Discourse, best viewed with JavaScript enabled"
3041,ai-camera-might-one-day-detect-lies-better-than-a-polygraph,"Originally published at:			AI Camera Might One Day Detect Lies Better Than a Polygraph | NVIDIA Technical Blog
The Russian machine learning firm Tselina Data Lab developed a deep learning-based camera algorithm called Fraudoscope that detects lies on facial emotions. Trained with CUDA and TITAN X GPUs, the lie-detecting app uses a high-definition camera to observe an interrogation and decode the results. The camera focuses on the interviewee — the software maps changing…Powered by Discourse, best viewed with JavaScript enabled"
3042,tell-us-what-new-features-you-would-like-to-see,"You are invited to tell us what what kind of new features you would like to see.Powered by Discourse, best viewed with JavaScript enabled"
3043,cuda-spotlight-gpu-accelerated-quantum-chemistry,"Originally published at:			https://developer.nvidia.com/blog/cuda-spotlight-gpu-accelerated-quantum-chemistry/
This week’s Spotlight is on Professor Todd Martínez of Stanford. Professor Martínez’ research lies in the area of theoretical chemistry, emphasizing the development and application of new methods which accurately and efficiently capture quantum mechanical effects. Professor Martínez pioneered the use of GPU technology for computational chemistry, culminating in the TeraChem software package that uses…Powered by Discourse, best viewed with JavaScript enabled"
3044,develop-the-next-generation-of-hpc-applications-with-the-nvidia-arm-hpc-developer-kit,"Originally published at:			Develop the Next Generation of HPC Applications with the NVIDIA Arm HPC Developer Kit | NVIDIA Technical Blog
NVIDIA and partners have been working hard to get the NVIDIA Arm HPC Developer Kit units into the hands of developers and enhance the software stack.Powered by Discourse, best viewed with JavaScript enabled"
3045,gtc-2020-accelerating-recommender-system-training-and-inference-on-nvidia-gpus,"GTC 2020 CWE21747
Presenters: Even Oldridge,NVIDIA; Alec Gunny & Akshay Subramaniam, NVIDIA; Onur Yilmaz & Chirayu Garg, NVIDIA; Pawel Morkisz & Minseok Lee, NVIDIA; Lukasz Mazurek & Scott LeGrand, NVIDIA; Paulius Micikevicius & Levs Dolgovs, NVIDIA
Abstract
Come and learn about how you can use NVIDIA technologies to accelerate your recommender system training and inference pipelines. We’ve been doing some ground-breaking work on optimizing performance for many stages of recommender system, including ETL of tabular data, training with terabyte-size embeddings for CTR models on multiple nodes, low-latency inference for Wide & Deep, and more. Running on NVIDIA GPUs, many of these are more than an order of magnitude faster than conventional CPU implementations. We’d be thrilled to learn from you how these accelerated components may apply to your setup and, if not, what’s missing. We’d also like to hear the roles recommenders play in your products, the types of systems you’re building, and the challenges you face. This session is ideal for data scientists and engineers who are responsible for developing, deploying, and scaling their recommender pipelines. Please join us for what’s sure to be an interesting series of discussions.Watch this session
Join in the conversation below.Powered by Discourse, best viewed with JavaScript enabled"
3046,maximize-your-gtc-experience-what-to-know-before-you-log-in,"Originally published at:			Maximize Your GTC Experience: What to Know Before You Log In | NVIDIA Technical Blog
NVIDIA’s GTC always delivers the latest breakthroughs in AI, HPC, data science, graphics, healthcare, government, and this October’s GTC will be no exception. To help maximize your GTC online experience we’ve outlined four actionable steps you can take to prepare for next week’s show. TIP #1: MANAGING YOUR TIME AND EXPECTATIONS Organizing your calendar and…Powered by Discourse, best viewed with JavaScript enabled"
3047,cuda-spotlight-gpu-accelerated-cosmology,"Originally published at:			https://developer.nvidia.com/blog/cuda-spotlight-gpu-accelerated-cosmology/
This week’s Spotlight is on Dr. Debbie Bard, a cosmologist at the Kavli Institute for Particle Astrophysics and Cosmology (KIPAC). KIPAC members work in the Physics and Applied Physics Departments at Stanford University and at the SLAC National Accelerator Laboratory. To handle the massive amounts of data involved in cosmological measurements, Debbie and her colleagues…Powered by Discourse, best viewed with JavaScript enabled"
3048,designing-arithmetic-circuits-with-deep-reinforcement-learning,"Originally published at:			Designing Arithmetic Circuits with Deep Reinforcement Learning | NVIDIA Technical Blog
Learn how NVIDIA researchers use AI to design better arithmetic circuits that power our AI chips.Powered by Discourse, best viewed with JavaScript enabled"
3049,top-metaverse-developer-sessions-at-nvidia-gtc-2023,"Originally published at:			https://developer.nvidia.com/blog/top-metaverse-developer-sessions-at-nvidia-gtc-2023/
Learn how developers are building metaverse applications, extensions, and microservices.Powered by Discourse, best viewed with JavaScript enabled"
3050,how-to-build-a-winning-recommendation-system-part-2-deep-learning-for-recommender-systems,"Originally published at:			How to Build a Deep Learning Powered Recommender System, Part 2 | NVIDIA Technical Blog
Recommender systems (RecSys) have become a key component in many online services, such as e-commerce, social media, news service, or online video streaming.  However with the growth in importance,  the growth in scale of industry datasets, and more sophisticated models, the bar has been raised for computational resources required for recommendation systems.  To meet the…Powered by Discourse, best viewed with JavaScript enabled"
3051,mastering-string-transformations-in-rapids-libcudf,"Originally published at:			https://developer.nvidia.com/blog/mastering-string-transformations-in-rapids-libcudf/
Learn how RAPIDS libcudf accelerates data science with optimized string processing algorithms and explore techniques for writing your own custom string transformations.To run the examples, first build:and then execute:Check it out!I’m happy to share that we now have a cuDF-python implementation of the “redact” strings transformation. In this case, cuDF uses the libcudf strings API to deliver excellent acceleration.Please see our new notebook for more information: GitHub - shwina/cudf-string-examples: Examples of working with strings in cuDF
image1230×1038 43.2 KB
Powered by Discourse, best viewed with JavaScript enabled"
3052,making-robotics-easier-with-benchbot-and-nvidia-isaac-sim,"Originally published at:			https://developer.nvidia.com/blog/making-robotics-easier-with-benchbot-and-isaac-sim/
We built BenchBot to allow roboticists to spend more time researching the exciting and interesting problems in robotics. This post tells BenchBot’s story.Powered by Discourse, best viewed with JavaScript enabled"
3053,ai-system-automatically-examines-abdominal-ultrasounds,"Originally published at:			AI System Automatically Examines Abdominal Ultrasounds | NVIDIA Technical Blog
Sixty to seventy million people in the U.S. suffer from gastrointestinal diseases and the best way to clinically diagnose the exact problem is to perform an abdominal ultrasound. However, the process is labor intensive and sometimes inefficient. To help solve the issue, researchers from Siemens and Vanderbilt University developed a deep learning-based system that can…Powered by Discourse, best viewed with JavaScript enabled"
3054,webinar-unleash-the-power-of-vision-transformers,"Originally published at:			https://developer.nvidia.com/blog/webinar-unleash-the-power-of-vision-transformers/
Learn how Vision Transformers are revolutionizing AI applications, offering unprecedented image understanding and analysisPowered by Discourse, best viewed with JavaScript enabled"
3055,how-xsplit-delivers-rich-content-for-live-streaming-with-nvidia-broadcast,"Originally published at:			How XSplit Delivers Rich Content for Live Streaming with NVIDIA Broadcast | NVIDIA Technical Blog
In this interview, Miguel Molina, Director of Developer Relations at SplitmediaLabs, the makers of XSplit, discussed how they were able to easily integrate NVIDIA Broadcast into their vastly popular streaming service.  For those who may not know, tell us about yourself? My name is Miguel Molina, currently the Director of Developer Relations at SplitmediaLabs, the…Powered by Discourse, best viewed with JavaScript enabled"
3056,learning-to-defend-ai-deployments-using-an-exploit-simulation-environment,"Originally published at:			https://developer.nvidia.com/blog/learning-to-defend-ai-deployments-using-an-exploit-simulation-environment/
MintNV, an AI/ML educational exercise that showcases how an adversary can bypass defensive ML mechanisms to compromise a host, is now on the NVIDIA NGC catalog.Powered by Discourse, best viewed with JavaScript enabled"
3057,nvidia-announces-nsight-graphics-2019-3,"Originally published at:			https://developer.nvidia.com/blog/nvidia-announces-nsight-graphics-2019-3/
NVIDIA announces Nsight Graphics 2019.3! In this release, GPU Trace has been revamped with a new analysis mode, the Configurable Range Profiler is now the default view in the profiling activity, and the Acceleration Structure Viewer can now export acceleration structures for standalone viewing. For full details on the new capabilities in the 2019.3 release,…Powered by Discourse, best viewed with JavaScript enabled"
3058,coming-right-up-high-schoolers-build-indoor-delivery-robot-with-nvidia-jetson-tx2,"Originally published at:			https://developer.nvidia.com/blog/coming-right-up-high-schoolers-build-indoor-delivery-robot-with-nvidia-jetson-tx2/
By Grace Lam, Mokshith Voodarla, Nicholas Liu How long does it take to program an office delivery robot? Apparently, less than seven weeks. This summer, three NVIDIA high school interns, Team Electron, built a completely autonomous indoor delivery robot with a Turtlebot base and Jetson TX2. Simply message the robot to deliver anything from pens…Powered by Discourse, best viewed with JavaScript enabled"
3059,singapore-based-research-institution-uses-ai-to-improve-the-air-cargo-process,"Originally published at:			Singapore Based Research Institution Uses AI to Improve the Air Cargo Process | NVIDIA Technical Blog
Air cargo is complex, flight engineers and ground staff spend hours every day planning the many variables that go into shipping material from locale to another. The process is labor intensive and tedious, and one mistake could cause significant problems for the airline and the traveling public. To take the heavy lifting out of the…Powered by Discourse, best viewed with JavaScript enabled"
3060,ai-system-beats-pros-at-texas-hold-em,"Originally published at:			https://developer.nvidia.com/blog/ai-system-beats-pros-at-texas-holdem/
A team of researchers from University of Alberta, Charles University in Prague and Czech Technical University developed an AI system called DeepStack that defeated professional poker players, making it the first computer program to beat professional players in heads-up no-limit Texas hold’em poker. “Poker has been a longstanding challenge problem in artificial intelligence,” says Michael…Powered by Discourse, best viewed with JavaScript enabled"
3061,managing-memory-for-acceleration-structures-in-directx-raytracing,"Originally published at:			https://developer.nvidia.com/blog/managing-memory-for-acceleration-structures-in-dxr/
In Microsoft Direct3D, anything that uses memory is considered a resource: textures, vertex buffers, index buffers, render targets, constant buffers, structured buffers, and so on. It’s natural to think that each individual object, such as a texture, is always one resource. In this post, I discuss DXR’s Bottom Level Acceleration Structures (BLASes) and best practices…First!I hope this is a useful post.  Please post any questions, comments, suggestions or any other feedback here and I’ll do my best to get back to you.Powered by Discourse, best viewed with JavaScript enabled"
3062,nvidia-releases-nsight-systems-2020-2,"Originally published at:			NVIDIA Releases Nsight Systems 2020.2 | NVIDIA Technical Blog
NVIDIA Nsight Systems 2020.2 is now available for download. Nsight Systems is a system-wide performance analysis tool, designed to help developers tune and scale software across CPUs and GPUs. This release adds support for  Direct3D12 multi-GPU, Windows video memory usage graph, paging queue, page eviction events, hotkey to insert user annotation markers (NVTX), OpenMP 5,…Powered by Discourse, best viewed with JavaScript enabled"
3063,sparsity-in-int8-training-workflow-and-best-practices-for-nvidia-tensorrt-acceleration,"Originally published at:			https://developer.nvidia.com/blog/sparsity-in-int8-training-workflow-and-best-practices-for-tensorrt-acceleration/
The training stage of deep learning (DL) models consists of learning numerous dense floating-point weight matrices, which results in a massive amount of floating-point computations during inference. Research has shown that many of those computations can be skipped by forcing some weights to be zero, with little impact on the final accuracy. In parallel to…Powered by Discourse, best viewed with JavaScript enabled"
3064,numba-high-performance-python-with-cuda-acceleration,"Originally published at:			https://developer.nvidia.com/blog/numba-python-cuda-acceleration/
Looking for more? Check out the hands-on DLI training course: Fundamentals of Accelerated Computing with CUDA Python [Note, this post was originally published September 19, 2013. It was updated on September 19, 2017.] Python is a high-productivity dynamic programming language that is widely used in science, engineering, and data analytics applications. There are a number…Hi,could you please indicate what modules you include in the fractal example (Mendel kernel)? I tried withfrom numpy import uint32from numbapro import cudabut I am still missing somethingThank youGiampieroCheck the iPython notebook here: https://github.com/harrism/...But to answer your question, you need:import numpy as npfrom pylab import imshow, showfrom timeit import default_timer as timerfrom numbapro import cudafrom numba import *Hi Mark,thank you! Now it's working, provided that I change the ""ThreadIdx"" attributes to ""threadIdx"", that is with lower case ""T"".That's a typo, which I have now fixed -- thanks!guess it works only with python 2.7?looking at the tutorialand Python versions:    Python 2.7, 3.3-3.6    NumPy 1.8 and laterhttps://github.com/Continuu...Notebook is now updated also -- no longer import from numbapro, just numba. See the notebook.It works with Python 3.x, if you set parantheses at the print statements.amazing tutorial thank you! Numba is amazing.Is anyone writing a book on Python 3.6 CUDA programming?  I did an Amazon and Google search but I couldn't find anything.  If you want people to use Python 3.6 and CUDA, someone should write a book. Even a PDF would be OK.Please help me.https://stackoverflow.com/q...Is there a way to synchronize Numba.CUDA threads on incoming data in the GPU? #GPUdirectIs there any book and/ or tutorial for Python + CUDA?There is an NVIDIA DLI course: https://courses.nvidia.com/...One question: is it possible to access and use the RT cores on a Turing from Numba? I need to compute intersections for my science application and I prefer to use the hardware approach.One question: is it possible to access and use the RT cores on a Turing from Numba?It’s not possible to directly access them using Numba (or CUDA in general). They can be used via OptiX, the DirectX Ray Tracing (DXR) API, or the VKRay extensions to Vulkan. Each of these are referred to on the https://developer.nvidia.com/rtx/raytracing#rtcores site.The Numba CUDA target is extensible (see Extending the Numba CUDA Target, a part of the Numba for CUDA Programmers tutorial). I’m not aware of an extension that provides access to OptiX functions, but it would be possible to build a Numba extension that provides access to the OptiX APIs.Powered by Discourse, best viewed with JavaScript enabled"
3065,gtc-2020-autofaq-automation-of-customer-support-for-most-common-questions,"GTC 2020 S21252
Presenters: Vitaly Davydov,Poteha Labs
Abstract
In most support systems, up to 70% of user questions are very similar to each other. Instead of manually answering each of them, it makes sense to automate it. We’ll discuss the business value of such systems, and how to integrate them into processes; how to develop an architecture for automating question-answering; how to set up a training loop, and which algorithms to choose; which models need to be trained, and why; and state-of-the-art language modeling models. Knowledge of Python, NLP, language models, and cloud computing will be helpful, but it’s not essential.Watch this session
Join in the conversation below.Powered by Discourse, best viewed with JavaScript enabled"
3066,automatic-colorization-of-grayscale-images,"Originally published at:			Automatic Colorization of Grayscale Images | NVIDIA Technical Blog
Colorization of grayscale images is a simple task for the human imagination. Researchers from the Toyota Technological Institute at Chicago and University of Chicago developed a fully automatic image colorization system using deep learning and GPUs. Their paper mentions previous approaches required some level of user input. Using a TITAN X GPU, they trained their…Powered by Discourse, best viewed with JavaScript enabled"
3067,share-your-science-finding-interesting-statistics-from-massive-datasets,"Originally published at:			https://developer.nvidia.com/blog/share-your-science-finding-interesting-statistics-from-massive-datasets/
Adam McLaughlin, PhD student at Georgia Tech shares how he is using NVIDIA Tesla GPUs for his research on Betweenness Centrality – a graph analytics algorithm that tracks the most important vertices within a network. This can be applied to a broad range of applications, such as finding the head of a crime ring or…Powered by Discourse, best viewed with JavaScript enabled"
3068,top-3-pillars-of-ai-enabled-edge-computing-in-retail,"Originally published at:			https://developer.nvidia.com/blog/top-3-pillars-of-ai-enabled-edge-computing-in-retail/
Learn how AI is transforming the retail industry through enabling intelligent stores, omnichannel management, and automated supply chains.Powered by Discourse, best viewed with JavaScript enabled"
3069,360-degree-8-player-super-mario-bros,"Originally published at:			360-degree 8-player ‘Super Mario Bros’ | NVIDIA Technical Blog
Bringing supersized retro gaming to life, a team of researchers from ETH Zurich and Disney transformed the 8-bit Nintendo Entertainment System into an eight-player immersive game experience in a Swiss nightclub.   To drive the 360-degree projection system and compensate for the different aspect ratios of the walls in the large event space, the team…Powered by Discourse, best viewed with JavaScript enabled"
3070,fox-sports-teleports-fans-into-major-league-baseball-stadium-seats,"Originally published at:			FOX Sports Teleports Fans Into Major League Baseball Stadium Seats | NVIDIA Technical Blog
When the 2020 Major League Baseball season kicks off this week, fans will not be in the stands due to the ongoing COVID-19 pandemic. Instead, viewers watching on FOX Sports will see “virtual fans” filling up the empty seats.  “We had a vision for making our Major League Baseball broadcasts look as natural as they…Powered by Discourse, best viewed with JavaScript enabled"
3071,nvidia-announces-cloud-native-metropolis-microservices-and-retail-ai-workflows-for-theft-prevention,"Originally published at:			https://developer.nvidia.com/blog/nvidia-announces-cloud-native-metropolis-microservices-and-retail-ai-workflows-for-theft-prevention/
NVIDIA is releasing a suite of microservices, along with Retail AI Workflows, to help software developers accelerate the development of retail loss prevention solutions.When is the tentative release date of MTMC? I have questions specific to that.Powered by Discourse, best viewed with JavaScript enabled"
3072,cudacasts-episode-10-accelerate-python-on-gpus,"Originally published at:			https://developer.nvidia.com/blog/cudacasts-episode-10-accelerate-python-gpus/
This week’s CUDACast continues the Parallel Forall Python theme kicked off in last week’s post by Mark Harris, demonstrating exciting new support for CUDA acceleration in Python with NumbaPro. This video is the first in a 3-part series showing various ways to accelerate your Python code on NVIDIA GPUs. Tomorrow you won’t want to miss…Powered by Discourse, best viewed with JavaScript enabled"
3073,automatic-mixed-precision-helps-nvidia-gaugan-researchers-dramatically-speed-up-their-dl-training,"Originally published at:			Automatic Mixed Precision Helps NVIDIA GauGan Researchers Dramatically Speed Up their DL Training | NVIDIA Technical Blog
To help developers and AI researchers use mixed precision in their deep learning training workflows, NVIDIA researchers at the International Conference on Computer Vision will present a hands-on workshop focused on the use of single and half-precision in their workflows.  NVIDIA researcher Ming-Yu Liu, one of the developers behind NVIDIA GauGan, the viral AI tool…Powered by Discourse, best viewed with JavaScript enabled"
3074,developer-blog-learning-to-rank-with-xgboost-and-gpus,"Originally published at:			https://developer.nvidia.com/blog/developer-blog-learning-to-rank-with-xgboost-and-gpu/
This technical blog describes an approach taken to accelerate ranking algorithms with XGBoost and NVIDIA GPUs.Powered by Discourse, best viewed with JavaScript enabled"
3075,gpu-trained-system-understands-movies,"Originally published at:			GPU-Trained System Understands Movies | NVIDIA Technical Blog
Researchers from Karlsruhe Institute of Tech, MIT and University of Toronto published MovieQA, a dataset that contains 7702 reasoning questions and answers from 294 movies. Their innovative dataset and accuracy metrics provide a well-defined challenge for question/answer machine learning algorithms. The questions range from simpler ‘Who’ did ‘What’ to ‘Whom’ that can be solved by…Powered by Discourse, best viewed with JavaScript enabled"
3076,machine-learning-in-practice-deploy-an-ml-model-on-google-cloud-platform,"Originally published at:			https://developer.nvidia.com/blog/machine-learning-in-practice-deploy-an-ml-model-on-google-cloud-platform/
This series looks at the development and deployment of machine learning (ML) models. In this post, you deploy ML models on Google Cloud Platform. Part 1 gave an overview of the ML workflow, considering the stages involved in using machine learning and data science to deliver business value. In part 2, you trained an ML…Powered by Discourse, best viewed with JavaScript enabled"
3077,tracking-the-endangered-african-grevys-zebra-with-deep-learning,"Originally published at:			Tracking the Endangered African Grevy’s Zebra with Deep Learning | NVIDIA Technical Blog
In January, nearly 120 teams of scientists, schoolchildren and the U.S. ambassador to Kenya took part in the first ever Great Grevy’s Rally — driving over 25,000 square km to census the Grevy’s zebra, one of the most endangered mammals on the planet. In the 1970s there were 15,000 Grevy’s zebras and now there is…Powered by Discourse, best viewed with JavaScript enabled"
3078,scaling-recommendation-system-inference-with-merlin-hierarchical-parameter-server,"Originally published at:			Scaling Recommendation System Inference with Merlin Hierarchical Parameter Server | NVIDIA Technical Blog
NVIDIA Merlin introduces the Hierarchical Parameter Server (HPS), a scalable solution with multilevel adaptive storage to enable deployment of terabyte-size models under real-time latency constraints.why use rockdb in SSD rather than relational RDMS like sqlite?Powered by Discourse, best viewed with JavaScript enabled"
3079,gtc-2020-winning-kaggle-competitions-with-gpus-reflections-from-kaggle-grandmasters,"GTC 2020 CWE22495
Presenters: ,
Abstract
Meet Kaggle grandmasters and learn how to approach and succeed in different types of Kaggle competitions including tabular, image, natural language processing, and physics. Explore solutions and see how NVIDIA GPUs create top-performing models. Also learn how NVIDIA RAPIDS is allowing more possibilities with GPUs.Kaggle is an online platform that challenges participants to build models from real-world data to solve real-world problems while competing for highest model accuracy. NVIDIA RAPIDS is an open-source library that allows data scientists to build entire pipelines on GPU. RAPIDS accelerates feature search and engineering. And RAPIDS accelerates model training, validation, and inference.Watch this session
Join in the conversation below.I enjoyed this thanks!Powered by Discourse, best viewed with JavaScript enabled"
3080,gtc-2020-scaling-deep-learning-for-automatic-speech-recognition,"GTC 2020 S21838
Presenters: Jacob Kahn,Facebook; Vineel Pratap,Facebook; Vitaliy Liptchinsky,Facebook AI Research
Abstract
We’ll discuss challenges of scaling automatic speech recognition (ASR) workloads with wav2letter++, a fast C++ toolkit for ASR. We’ll introduce distributed training techniques used to achieve almost linear scalability and compare wav2letter to other popular ASR toolkits. Constant increase in model and dataset sizes, along with current trends toward unsupervised and semi-supervised learning, require squeezing out every bit of performance. In addition to distributed training, we’ll cover other approaches for faster training and for training large models.Watch this session
Join in the conversation below.Powered by Discourse, best viewed with JavaScript enabled"
3081,gtc-2020-condensa-a-programming-system-for-dnn-model-compression,"GTC 2020 S21599
Presenters: Saurav Muralidharan,NVIDIA
Abstract
Deep neural networks contain far more weights than they need for the specific task they’re trained to perform. They can be compressed using techniques such as weight pruning and quantization that reduce both model size and inference time without appreciable loss in accuracy, but finding the best compression strategy for a given neural network target platform and optimization objective often requires extensive experimentation. Also, finding optimal hyperparameters for a given compression strategy results in even more expensive, and frequently manual, trial-and-error exploration. We’ll introduce a programmable system for model compression called Condensa. Users of our framework can programmatically compose simple operators in Python to build complex compression strategies. Given a strategy and a user-provided objective, such as minimizing runtime, Condensa uses a novel sample efficient constrained Bayesian optimization-based algorithm to automatically infer optimal sparsity ratios.Watch this session
Join in the conversation below.Powered by Discourse, best viewed with JavaScript enabled"
3082,gaugan-wins-major-awards-at-siggraph-2019s-real-time-live-competition,"Originally published at:			https://developer.nvidia.com/blog/gaugan-wins-major-awards-at-siggraph-2019s-real-time-live-competition/
GauGAN, NVIDIA’s viral real-time AI art application just won two major SIGGRAPH 2019 awards, “Best of Show” and “Audience Choice,” at the “Real Time Live” competition at SIGGRAPH 2019Powered by Discourse, best viewed with JavaScript enabled"
3083,pedestrian-following-service-robots-made-possible-with-cuda-acceleration,"Originally published at:			https://developer.nvidia.com/blog/pedestrian-following-service-robots-made-possible-with-cuda-acceleration/
A team of researchers from Seoul National University built a pedestrian-following service robot to drive smart shopping carts and other autonomous helpers. The performance of their initial CPU-only implementation was “not acceptable for a real-time system” so they now use a GPU-accelerated CUDA implementation for a 13x performance boost. Mobile robots that track a person…Powered by Discourse, best viewed with JavaScript enabled"
3084,upcoming-webinar-detecting-cyber-threats-with-unsupervised-learning,"Originally published at:			https://info.nvidia.com/detecting-cyber-threats-with-unsupervised-learning.html?nvid=nv-int-txtad-297947-vt43#cid=dl24_nv-int-txtad_en-us
Discover how to detect cyber threats using machine learning and NVIDIA Morpheus, an open-source AI framework.Powered by Discourse, best viewed with JavaScript enabled"
3085,gtc-presentations-now-available-explore-the-latest-in-graphics-technologies,"Originally published at:			GTC Presentations Now Available: Explore the Latest in Graphics Technologies | NVIDIA Technical Blog
Our GTC Fall 2020 virtual event featured a record breaking number of sessions, podcasts, demos, research posters, and more. We are now opening access to all the great content shared at the conference through the new NVIDIA On-Demand catalog. Learn more about breakthrough NVIDIA technologies and dive into our expansive selection of graphics and simulation…Powered by Discourse, best viewed with JavaScript enabled"
3086,accelerate-genome-assembly-and-analysis-with-clara-genomics-sdk-0-2,"Originally published at:			https://developer.nvidia.com/blog/accelerate-genome-assembly-and-analysis-with-clara-genomics-sdk-0-2/
The Clara Genomics SDK has been upgraded with high performance analysis algorithms for long read sequencing and early access to deep learning-based processing of short read ATAC sequencing.Powered by Discourse, best viewed with JavaScript enabled"
3087,nvidia-brings-ai-to-icml-in-sydney,"Originally published at:			https://developer.nvidia.com/blog/nvidia-brings-ai-to-icml-in-sydney/
NVIDIA will be bringing together the best of minds and technologies in the field of AI to the International Conference on Machine Learning (ICML) in Sydney, Australia from August 6 to 11. Nearly 3,000 attendees are expected to attend, consisting of faculty, researchers and PhD students in machine learning, data science, data mining, AI, statistics,…Powered by Discourse, best viewed with JavaScript enabled"
3088,5-cool-automotive-sessions-at-gtc-2019,"Originally published at:			https://developer.nvidia.com/blog/top-5-automotive-sessions-at-gtc-2019/
NVIDIA’s GPU Technology Conference is the premier event for AI-driven automotive innovation. See the latest deep learning breakthroughs that are revolutionizing the transportation industry, from the DRIVE AutoPilot Level 2+ solution, to cutting edge simulation, to open and flexible self-driving software. In the video below, see a preview of some of the sessions you can…Powered by Discourse, best viewed with JavaScript enabled"
3089,looking-behind-the-curtain-of-evpn-traffic-flows,"Originally published at:			https://developer.nvidia.com/blog/looking-behind-the-curtain-of-evpn-traffic-flows/
Is EVPN magic? As Arthur C Clarke said, any sufficiently advanced technology is indistinguishable from magic. On that premise, moving from a traditional layer 2 environment to VXLAN driven by EVPN has much of that same hocus-pocus feeling. To help demystify the sorcery, I aim to help users new to EVPN understand how EVPN works…Powered by Discourse, best viewed with JavaScript enabled"
3090,gtc-2020-nvidia-egx-platform-for-edge-computing,"GTC 2020 S22039
Presenters: Erik Bohnhorst, NVIDIA
Abstract
We’ll introduce the EGX platform, NVIDIA’s solution for edge computing. Handling terabytes of data from the millions of internet-of-things sensors that are equipped at edge locations is a key challenge in real-time AI. Right now, most of the AI computation is happening at the data-center level, and data collection is at the edge level. As IoT sensor networks get more complicated and computationally challenging, we need better node management and orchestration tools. In addition to powerful computation processors like NVIDIA’s T4 GPU at the edge to process data, NVIDIA’s EGX provides a scalable, automated platform that can deliver AI to the edge platforms/servers and supports containerization and orchestration tools to manage edge-computing platforms.Watch this session
Join in the conversation below.Hi everyone, here are some of the popular questions asked during this webinar. Feel free to comment below for addition questions.Q: What is the difference between Helm and the GPU operator?
A: Helm is a Cloud Native Computing Foundation Kubernetes package manager. The NVIDIA GPU Operator uses the operator framework within Kubernetes to automate the management of all NVIDIA software components needed to provision GPUs.Q: Can it be used on a conventional robot where video, audio, and text processing models would be used in parallel? Or other hardware?
A: Yes, it can. Just make sure to use NGC-Ready for Edge Systems and have sufficient GPUs available for all pods.Q: Can the DU/CU be cohosted on a single platform similar to the HLR/VLR construct in current network ontologies?
A: The CU/DU functions can be run on the same systems. Our approach follows the ORAN 7.2 split.Q: You mentioned streaming data is run on GPU on the edge, and then propagated to the CPU if different computation is needed. What determines whether data should be run on the GPU or the CPU with live data?
A: In a RAN context, with our approach, the L1 upper phy is accelerated on the GPU, and the L2+ is handled by the CPU. Our approach follows the ORAN 7.2 split.Q: How does the GPU Operator relate to Kubernetes?
A: The GPU Operator allows administrators of Kubernetes clusters to manage GPU nodes just like CPU nodes in the cluster. The NVIDIA GPU Operator uses the operator framework within Kubernetes to automate the management of all NVIDIA software components needed to provision GPUs.Q: Are NGC certified systems limited to T4 or does the architecture also support RTX?
A: RTX is supported; see NGC-Ready Systems :: NVIDIA GPU Cloud DocumentationPowered by Discourse, best viewed with JavaScript enabled"
3091,nvidia-announces-nsight-systems-2020-1,"Originally published at:			https://developer.nvidia.com/blog/nvidia-announces-nsight-systems-2020-1/
NVIDIA Nsight Systems 2020.1 is now available for download. This release adds CLI support for Power9 architecture. The ability to run multiple recording sessions simultaneously in the Command Line Interface, and UX improvements and Stats export to the GUI and CLI. Nsight Systems is a system-wide performance analysis tool, designed to help developers tune and…Powered by Discourse, best viewed with JavaScript enabled"
3092,enhancing-zero-trust-security-with-data,"Originally published at:			https://developer.nvidia.com/blog/enhancing-zero-trust-security-with-data/
Doing zero trust well requires enterprises to monitor and analyze massive amounts of data, supported by artificial intelligence and accelerated computing.Powered by Discourse, best viewed with JavaScript enabled"
3093,lip-reading-ai-more-accurate-than-humans,"Originally published at:			https://developer.nvidia.com/blog/lip-reading-ai-more-accurate-than-humans/
Researchers from Google’s DeepMind and the University of Oxford developed a deep learning system that outperformed a professional lip reader. Using a TITAN X GPU, CUDA and the TensorFlow deep learning framework, the team trained their models on over 100,000 sentences from nearly 5,000 hours of BBC programs. By looking at each speaker’s lips, the…Powered by Discourse, best viewed with JavaScript enabled"
3094,nvidia-announces-nsight-graphics-2018-5,"Originally published at:			NVIDIA Announces Nsight Graphics 2018.5 | NVIDIA Technical Blog
NVIDIA is proud to announce Nsight Graphics 2018.5 with Turing GPU support! This release offers Turing profiling support for Nsight Graphics’ Range Profiler and GPU Trace activities. In addition, copy queue activity can now be visualized in GPU Trace (our new GPU Occupancy Profiler).   Nsight Graphics has day zero support for the new Turing…Powered by Discourse, best viewed with JavaScript enabled"
3095,amgx-v1-0-enabling-reservoir-simulation-with-classical-amg,"Originally published at:			https://developer.nvidia.com/blog/amgx-v1-0-enabling-reservoir-simulation-with-classical-amg/
Back in January I wrote a post about the public beta availability of AmgX, a linear solver library for large-scale industrial applications.  Since then, AmgX has grown up!  Now we can solve problems that were impossible for us before, due to the addition of “classical” Algebraic Multi-Grid (often called Ruge-Stueben AMG).  V1.0 comes complete with…Those are very impressive results! Did you use double or single precision when computing the speed up? And what smoother did you use in the plot when comparing to HYPRE? I just implemented a solver for the inhomogeneous poisson equation in my own simulation code using HYPRE. Reading this, it is very tempting to try out AmgX :)These are all double precision results.  SPE10 has such a wider range of coefficient values that single precision misses a lot of the action.  For HYPRE, we used Gauss-Seidel smoothing with 1 pre and post sweep. For AmgX we used a Jacobi variant called Jacobi-L1.  This takes the sum of the absolute value of each entry in each row, and uses the inverse of this as a damping factor. Both solvers used D2 interpolation and a truncated prolongation (4 elements are kept).Joe, could you please share with us the actual timings for build and solve stages for the 5M and 10M SPE10 benchmarks? Also, since these seem to be modified problems compared to the basic SPE10, would you be able to provide the matrix and RHS for our internal benchmarking purposes?Thanks in advance; adrinHello, Sir i am working on CUDA and i want to use AmgX with OpenFOAM but due to lack of resources i am not able to identify feasibility with OpenFOAM. can u suggest me some guidelines.Hello, Sir I’m facing same issue, I want to use AmgX with OpenFOAM , I am also working on Cuda. can u share me some guidelines.Powered by Discourse, best viewed with JavaScript enabled"
3096,build-speech-ai-in-multiple-languages-and-train-large-language-models-with-the-latest-from-riva-and-nemo-megatron,"Originally published at:			https://developer.nvidia.com/blog/build-speech-ai-in-multiple-languages-and-train-large-language-models-with-the-latest-from-riva-and-nemo-megatron/
Read a recap of conversational AI announcements from NVIDIA GTC.Powered by Discourse, best viewed with JavaScript enabled"
3097,discover-the-latest-in-machine-learning-graphics-hpc-and-iot-at-aws-re-invent,"Originally published at:			Discover the Latest in Machine Learning, Graphics, HPC, and IoT at AWS re:Invent | NVIDIA Technical Blog
NVIDIA created content for AWS re:Invent, helping developers learn more about applying the power of GPUs to reach their goals faster and more easily.Powered by Discourse, best viewed with JavaScript enabled"
3098,cudacasts-episode-6-cuda-on-arm-with-cuda-5-5,"Originally published at:			https://developer.nvidia.com/blog/cudacasts-episode-6-cuda-arm-cuda-55/
In CUDACast #5, we saw how to use the new NVIDIA RPM and Debian packages to install the CUDA toolkit, samples, and driver on a supported Linux OS with a standard package manager. With CUDA 5.5, it is now possible to compile and run CUDA applications on ARM-based systems such as the Kayla development platform. In…Powered by Discourse, best viewed with JavaScript enabled"
3099,register-for-gtc-2014-now-and-save-40,"Originally published at:			https://developer.nvidia.com/blog/register-gtc-2014-now-save/
It’s that time of year again!  Here at NVIDIA, we’re hard at work getting ready for the 2014 GPU Technology Conference, the world’s most important GPU developer conference. Taking place in the heart of Silicon Valley, GTC offers unmatched opportunities to learn how to harness the latest GPU technology including 500 sessions, hands-on labs and tutorials,…Powered by Discourse, best viewed with JavaScript enabled"
3100,gtc-2020-cuda-new-features-and-beyond,"GTC 2020 S21760
Presenters: Stephen Jones,NVIDIA
Abstract
Learn what’s new in the CUDA Toolkit, the foundation of NVIDIA’s GPU computing platform. We’ll cover the latest and greatest features in the CUDA language, compiler, libraries, and tools — and, as usual, we’ll give you a sneak peek at what’s coming up over the next year.Watch this session
Join in the conversation below.Powered by Discourse, best viewed with JavaScript enabled"
3101,supporting-server-redundancy-with-nvidia-evpn-multihoming,"Originally published at:			https://developer.nvidia.com/blog/supporting-server-redundancy-with-nvidia-evpn-multihoming/
This post was originally published on the Cumulus Networks site. EVPN multihoming (EVPN-MH) is the latest addition to the NVIDIA EVPN story. In this three-part video series, I walk you through the various design elements of EVPN-MH.  Part 1: EVPN multihoming vs. EVPN with MLAG EVPN-MH provides support for all-active server redundancy. In this video,…Powered by Discourse, best viewed with JavaScript enabled"
3102,the-peak-performance-percentage-analysis-method-for-optimizing-any-gpu-workload,"Originally published at:			The Peak-Performance-Percentage Analysis Method for Optimizing Any GPU Workload | NVIDIA Technical Blog
Figuring out how to reduce the GPU frame time of a rendering application on PC is challenging for even the most experienced PC game developers. In this blog post, we describe a performance triage method we’ve been using internally at NVIDIA to let us figure out the main performance limiters of any given GPU workload…NVIDIA to figure out the main performance limiters of any given GPU workload (also known as perf marker or call range), using NVIDIA-specific hardware metrics.Will it works with unity and unreal engine!??!Himy Top SOL look like this - VRAM 86% L2 48% SM 35% TEX 22% What does this mean? I tried to read through the article and there is not a lot information when the TOP SOL is VRAM :/Could you please help thanks!In this case, your current GPU workload is mainly limited by the throughput of the VRAM (GDDR5 or GDDR5X memory). As written in the blog post: ""If the top SOL unit is the VRAM, and its SOL% value is not poor (>60%), then this workload is VRAM-throughput limited and merging it with another pass should speedup the frame. A typical example is merging a gamma-correction pass with another post-processing pass."" => I guess your workload is not just a simple copy pass, but something more complex. what are you doing in this workload? SSR?To speedup VRAM-throughput-limited workloads, you should try and understand what is causing your current VRAM traffic.Looking at TEX & L2 hit rates can help:- To reduce the VRAM traffic, you can try to increase your L2 hit rate (which can be done by reducing the working set size of your GPU workload, for instance sampling a half-res texture instead of a full one).- To try and reduce your L2 hit rate, you can try to increase your TEX/L1 hitrate.See the second section of this talk for some strategies to attack both hit rates for screen-space sampling algorithms: https://developer.nvidia.co...If you can replace a texture/buffer format by another that takes less space in VRAM, that should reduce the number of bytes transferred from VRAM and should produce a perf increase in this case of high VRAM SOL%.Can you please post a screenshot of your Range Profiler output, along with the GPU product name you're running on?Can you explain the difference between (the warp stall reasons) short and long scoreboard? I could not find any information on what a MIO operation is.I am analyzing a game, and the Graphics/Compute Idle is always 100%. The top SOLs : L2: 1.6%, VRAM: 1.3%, SM: 1.3%, TEX: 1.2%, CROP: 0.8%SM Active is 4.3%However, when I use Nvidia-smi, the GPU utilization is 37%.Why it is like that? Is there something wrong with the tool? or because of my mis-operation?Powered by Discourse, best viewed with JavaScript enabled"
3103,nvidia-research-an-analytic-brdf-for-materials-with-spherical-lambertian-scatterers,"Originally published at:			https://developer.nvidia.com/blog/nvidia-research-an-analytic-brdf-for-materials-with-spherical-lambertian-scatterers/
Researchers at NVIDIA presented a new paper “An Analytic BRDF for Materials with Spherical Lambertian Scatterers” at Eurographics Symposium on Rendering 2021 (EGSR), June 29-July 2, introducing a new BRDF for dusty/diffuse surfaces.  Our new Lambert-sphere BRDF (right) accurately and efficiently models the reflectance of a porous microstructure consisting of Lambertian spherical particles. Most rough…See:
Short.pdf (46.3 KB)Best wishesPowered by Discourse, best viewed with JavaScript enabled"
3104,recreate-high-fidelity-digital-twins-with-neural-kernel-surface-reconstruction,"Originally published at:			https://developer.nvidia.com/blog/recreate-high-fidelity-digital-twins-with-neural-kernel-surface-reconstruction/
Neural Kernel Surface Reconstruction (NKSR) is the new NVIDIA algorithm for reconstructing high-fidelity surfaces from large point clouds.Hi, I can’t turn points into objects, can I get some advice from you?Powered by Discourse, best viewed with JavaScript enabled"
3105,3rd-installment-of-ray-tracing-gems-now-available-for-free,"Originally published at:			3rd Installment of “Ray Tracing Gems Now Available For Free | NVIDIA Technical Blog
Ray tracing provides developers with an organic, photoreal solution to crafting reflections, refractions, and shadows. These elements are the focus of the third FREE installment of Ray Tracing Gems, which can be downloaded at NVIDIA Developer Zone. Here is the forward for Part III, written by Peter Shirley, distinguished research scientist at NVIDIA: “Any ray…Powered by Discourse, best viewed with JavaScript enabled"
3106,using-tensor-cores-in-cuda-fortran,"Originally published at:			https://developer.nvidia.com/blog/using-tensor-cores-in-cuda-fortran/
Tensor Cores, which are programmable matrix multiply and accumulate units, were first introduced in the V100 GPUs where they operated on half-precision (16-bit) multiplicands. Tensor Core functionality has been expanded in the following architectures, and in the Ampere A100 GPUs (compute capability 8.0) support for other data types was added, including double precision. Access to…Powered by Discourse, best viewed with JavaScript enabled"
3107,google-releases-tensorflow-1-0,"Originally published at:			Google Releases TensorFlow 1.0 | NVIDIA Technical Blog
Google recently announced the release of version 1.0 of its TensorFlow deep learning framework at their inaugural TensorFlow Developer Summit. In just its first year, the popular framework has helped researchers make progress with everything from language translation to early detection of skin cancer and preventing blindness in diabetics. The first major version comes with…Powered by Discourse, best viewed with JavaScript enabled"
3108,announcing-cuda-on-windows-subsystem-for-linux-2,"Originally published at:			https://developer.nvidia.com/blog/announcing-cuda-on-windows-subsystem-for-linux-2/
Figure 1. Stack image showing layers involved while running Linux AI frameworks in WSL 2 containers. In response to popular demand, Microsoft announced a new feature of the Windows Subsystem for Linux 2 (WSL 2)—GPU acceleration—at the Build conference in May 2020. This feature opens the gate for many compute applications, professional tools, and workloads…Great article! Since it was published in mid June maybe it can be updated to accurately reflect what has changed since then?Would love to see information about redistribution. I was pointed towards WSL2 when asking why CUDA on mingw64 wasn’t supported while porting a GTK application to windows.But I see no information about distributing a compiled binary with an installer to customers such that they can use my Cuda application without first doing a bunch of work to install WSL2/Linux containers.Is this ready for customers who simply have windows 10/11 installed and buy my linux developed software? Is it point and click like the experience with MSVC compiled native EXEs with cuda?I ask because the whole point of porting to windows for us is the easy install process and large existing install base. We already were dealing with a linux-only product that required customers to be linux-savy and if using WSL2 CUDA doesn’t alleviate that pain and customers still have to go toy with installing new subsystems/drivers/docker images I don’t think it’s going to work for us.Please advise.Installing WSL is not arduous. In Windows 11 it can be installed using the Windows Store, using the new WSL installation method. This is expected to be ported to Windows 10.Docker is not required to run a deployed binary. It would be sufficient to simply have a WSL distribution installed. On a command line that can be as simple as wsl.exe --install to get Ubuntu up and running. That would be executed by an installation script, if you set one up.The installation and use of WSL is by far the lowest bar I’ve seen for any Linux distribution in the last 20 years. The only issue that comes up is when the virtualization isn’t enabled on the CPU in bios, which is less and less a problem, as W11 requires that for many security features, and I believe is the default setting going forward.If you would like to get some help, we maintain a support forum at www.reddit.com/r/bashonubuntuonwindowsAre there any plans to have the nvidia driver install do this step or at least prepare WSL2 for cuda use?Is there a chance the WSL.exe --install step can fail or not meet system requirements?How much disk space and network bandwidth will this install step take? How long should I tell the users to wait?Honestly this feels like a really poor way to ship cuda windows software so I’m still going to pursue Mingw and just compile a small static-stub with Visual Studio since nvidia doesn’t feel the need to support Mingw(gcc) yet.I’m also looking into rewriting my Cuda application to use the C-driver API which is lower level and can statically-link on windows into a nice and neat exe file.I don’t so much need help as I believe the direction itself is wrong and not one I wish to follow. I already can tell my customers to install a windows VM and give a dedicated nvidia graphics card to the VM to use cuda accelerated code within the VM. No customer of mine will do that. Nor will they accept any “containers” or various permutations of the concept despite assurance it’s supported by Microsoft.Step 1: Install Ubuntu on your windows machine with this command. (My customers are Manufacturing Plants and Universities that would immediately stop here)If you want Cuda into manufacturing or university labs, I need a better way to get my GTK linux software running on windows without the fuss.You support GCC on Arm with the Nanos, why can’t I use mingw(gcc) on X86-64 to do the same thing in Windows?Well I’ve been over there on Reddit and basically finding what I thought originally here to be the case.WSL is a piece of junk. It is NOT a valid replacement for the mingw toolchain. It’s difficult to develop in a VM, it requires trusting the end users machine can even run the “wsl --install” step which on my testing results in a VM that can’t even start, and when I switch to WSL version 1 it now has no $DISPLAY variable.Mingw tool chain DOESNT REQUIRE VIRTUALIZATION. WSL with a gui apparently requires wslG which is not supported when virtualization is not enabled (such as in a vm).
Mingw tool chain produces a basic .exe binary that can just run. No network downloads of other linux distributions, permissions for network, issues with linux wsl having a different ip address than windows host, etc. Too many pitfalls.I’m not accepting this Nvidia. You need to do better.Hey coming back again to mention WSL2 does not support USB cameras which is kind of our applications main data collection method…So anyone using CUDA with WSL2 cannot also use USB cameras. Bummer.Powered by Discourse, best viewed with JavaScript enabled"
3109,jetson-project-of-the-month-robotic-arm-masters-the-game-of-cornhole,"Originally published at:			https://developer.nvidia.com/blog/jetson-project-of-the-month-robotic-arm-masters-the-game-of-cornhole/
David Niewinski of Dave’s Armoury won the ‘Jetson Project of the Month’ for building a robot arm capable of playing a perfect game of cornhole.Powered by Discourse, best viewed with JavaScript enabled"
3110,deep-learning-classifies-largest-ever-catalog-of-distant-galaxies,"Originally published at:			Deep Learning Classifies Largest-Ever Catalog of Distant Galaxies | NVIDIA Technical Blog
University of Pennsylvania researchers have used convolutional neural networks to catalog the morphology of 27 million galaxies, giving astronomers a massive dataset for studying the evolution of the universe.Powered by Discourse, best viewed with JavaScript enabled"
3111,ai-algorithm-can-read-your-mind,"Originally published at:			AI Algorithm Can Read Your Mind | NVIDIA Technical Blog
Researchers from ATR Computational Neuroscience Laboratories and Kyoto University in Japan developed a deep learning-based algorithm that can generate images from brain activity.   “The reconstruction algorithm starts from a random image and iteratively optimize the pixel values so that the DNN (deep neural network) features of the input image become similar to those decoded…Powered by Discourse, best viewed with JavaScript enabled"
3112,sc20-demos-new-nsight-systems-and-nsight-compute-demos,"Originally published at:			https://developer.nvidia.com/blog/sc20-demos-new-nsight-systems-and-nsight-compute-demos/
NVIDIA Nsight Compute: Roofline and NVIDIA Ampere GPU Architecture Analysis This demo shows the latest CUDA kernel analysis capabilities in NVIDIA Nsight Compute, including the popular Roofline Analysis Method and a new feature for the NVIDIA Ampere GPU Architecture. Specifically, we’ll demonstrate profiling the hardware-supported asynchronous data copy feature, which can boost the performance of…Powered by Discourse, best viewed with JavaScript enabled"
3113,speeding-up-deep-learning-inference-using-tensorflow-onnx-and-tensorrt,"Originally published at:			https://developer.nvidia.com/blog/speeding-up-deep-learning-inference-using-tensorflow-onnx-and-tensorrt/
Starting with TensorRT 7.0,  the Universal Framework Format (UFF) is being deprecated. In this post, you learn how to deploy TensorFlow trained deep learning models using the new TensorFlow-ONNX-TensorRT workflow. Figure 1 shows the high-level workflow of TensorRT. Figure 1. TensorRT is an inference accelerator. First, a network is trained using any framework. After a…I am not able to do
import engine as engI am getting :
ModuleNotFoundError: No module named 'engine' What should I install for this?Can you check this ?Download the code examples in this post.In the section “Creating the TensorRT engine from ONNX”, please copy the code in a file engine.py then you can import that file.The code for doing inference using TensorRT cannot work with flask API?an error caused at stream = cuda.Stream()
with error msg:
pycuda._driver.LogicError: explicit_context_dependent failed: invalid device context - no currently active context?when I tried to add:
cfx = cuda.Device(0).make_context()cfx.pop()new errors will showany idea how to solve it?HiI already have the .onnx files for these models: InceptionV1, V3 and V4.How much will the scripts: engine.py and buildEngine.py and inference.py will have to be changed?Have you done any example with those models?Thank youHi
If you already have .onnx files, you need to modify the scripts accordingly under the same workflow in your own way.
Or, for latency benchmark, you can try with ‘trtexec’ tool referring to https://github.com/NVIDIA/TensorRT/blob/master/samples/opensource/trtexec/README.md#example-4-running-an-onnx-model-with-full-dimensions-and-dynamic-shapesWill your code work with TensorRT 7.1?I am checking this GIT onnx-tensorrt and that is where I found this image.Have you done the same in your code? Or is it better to install a version lower than 7.1 to reuse your code?

image999×222 10.9 KB
Adding info: On this post, we used onnx 1.6.0 (OPSET 11).Thanks, what about the Tensorflow version ?For reference, this is the Dockerfile I used:FROM nvcr.io/nvidia/tensorflow:20.03-tf1-py3WORKDIR /workspaceADD requirements.txt .RUN pip install -r requirements.txt# docker build -t tf:20.03-tf1-py3 .# docker run -it -u $(id -u):$(id -g) -v $(pwd):/workspace --rm tf:20.03-tf1-py3 bashrequirements.txt: keraskeras2onnxonnx==1.6.0pycudatf2onnxtensorrtKeras ==2.3.1keras2onnx==1.6.0onnx==1.6.0pycuda==2019.1.2tf2onnx==1.6.0tensorrt 7.0.0.11These code snippets do not work with TF2.0. Can you please rectify them? Especially loadResNet.pyHelloI am using the TX2 which has a shared memory.
Is your code using the Unified Memory? If not, can you give me some clues on how to implement it using PyCUDA?Thank youHi! We have added the TF2 code example in the post.Has anyone run this? The code has errors:engine_path does not exist. Is this supposed to be plan_path?@loophole64  – Good catch! I’ve made that fix.Unsupported ONNX data type: UINT8 (2)
[TensorRT] ERROR: [network.cpp::getInput::1589] Error Code 3: Internal Error (Parameter check failed at: optimizer/api/network.cpp::getInput::1589, condition: index < getNbInputs()
)getting this error, while building modelHi sumeshthkr1,
We don’t support UINT8 type natively in TRT. We only support INT8 for calibration but not UINT8.Hello,
I have .pb file and .onnx file and engine.py file saved with my onnx model shape.
But when I,m trying to create engine I am getting an error saying-
import engine as eng
#from engine import build_engine
import argparse
from onnx import ModelProto
import tensorrt as trtengine_name = “engine.plan”
onnx_path = “/home/hipe/Documents/code/training/model/model.onnx”
batch_size = 1model = ModelProto()
with open(onnx_path, “rb”) as f:
model.ParseFromString(f.read())d0 = model.graph.input[0].type.tensor_type.shape.dim[1].dim_value
d1 = model.graph.input[0].type.tensor_type.shape.dim[2].dim_value
#d2 = model.graph.input[0].type.tensor_type.shape.dim[3].dim_value
shape = [batch_size , d0, d1]# d2]
engine = eng.build_engine(onnx_path, shape= [110,16])
eng.save_engine(engine, engine_name)AttributeError: module ‘engine’ has no attribute ‘build_engine’I have tried import engine from engine import build_engine and also other ways as well. but not able proceed.specifically I am getting this error at this part----> engine = eng.build_engine(onnx_path, shape= [110,16])Would appreciate any help to resolve this error.
Thanks in advance!Powered by Discourse, best viewed with JavaScript enabled"
3114,inside-pascal-nvidias-newest-computing-platform,"Also it is a very expensive part and a waste in the consumer space, 1/3 of the die is just Double precision shaders that will never be used on the desktop. They will more than likely use specific parts for consumers that lack dedicated DP shaders.NVlink It is compatible with pci express 3.0? ._.Next year?????Where are the GPU'S for gaming? u.uHBM is invention of AMD btw...Aye, VoltaI have a doubt. Considering that each processing block withing a SM only has 8 LD/ST units, wouldn't a load take 4 cycles to be performed?Any plans for low/no FP64 server cards?Indeed, but in the industry it's common practice to make claims (sometimes borderline false) about having invented something or being first. Yeah, technically they're first with HBM2, but is is available? No. It won't be until next year.Nvidia is always surprising me . Thank You NvidiaGot a question regarding nvcuvid... Will P100 improve what is actually done??Sorry, but it will be Vega for meMy GTX 680 broke, using a spare 560Ti - can't wait any longer :(@Mark_Harris""Support for the GPU ISA means that programs running on NVLink-connected GPUs can execute directly on data in the memory of another GPU as well as on local memory. GPUs can also perform atomic memory operations on remote GPU memory addresses...""- Question1:  Is it possible to use multiple GPUs with just ""sigle kernel"" call ? That is, the thread blocks from single kernel call will be assigned to SMs belonging to multiple GPUs ?Question2: Assuming that threads blocks from a single kernel call MUST reside on ONE GPU. Say I'm using DGX-1 (8 GPUs connected with NVLINK). My Data is big enough not to fit in the memory of one GPU.  Is it possible for a Kernel running on  GPU-0  of  a DGX-1 to access and atomically update the memory residing on GPU-1  of the same DGX-1 ?When is going to come pascal?nvidia uvm doesn't work under linux 4.4.32 + rt patch + pascal cards(like P4)Can you provide details of what problems you see?I had issues with install on old system with updated kernel. Reinstalled os from scratch and it works.Does anyone know whether L2 cache has write-through  write policy or write-back policy?Using a higher number of cores powering the gpu's networkPowered by Discourse, best viewed with JavaScript enabled"
3115,gtc-2020-cuda-graphs,"GTC 2020 CWE21914
Presenters: Alan-Gray,NVIDIA; Steven-Gurfinkel, NVIDIA; Evan-Weinberg, NVIDIA; Jeff-Larkin, NVIDIA
Abstract
This is an opportunity to find out more about CUDA Graphs, discuss why they may be advantageous to your particular application, and get help on any problems you may have faced when trying to use them.Graphs present a new model for work submission in CUDA. A graph is a series of operations (such as kernel launches) connected by dependencies, which is defined separately from its execution. This allows a graph to be defined once and then launched repeatedly. Separating out the definition of a graph from its execution enables a number of optimizations. First, CPU launch costs are reduced compared to streams because much of the setup is done in advance. Second, presenting the whole workflow to CUDA enables optimizations that might not be possible with the piecewise work submission mechanism of streams.This session will be staffed by representatives from the NVIDIA CUDA team developing graphs, as well as NVIDIA application specialists with experience using graphs.Watch this session
Join in the conversation below.Is anyone else able to view this session? When trying to play it, I just see the spinning loading symbol forever.Thanks for reporting this - I just checked, looks like there was no video recording of this “Meet the experts” session but there is a link to the presentation used. That link is : https://developer.nvidia.com/gtc/2020/slides/cwe21914.pdfSorry about that.Powered by Discourse, best viewed with JavaScript enabled"
3116,nvidia-launches-magnum-io-software-suite,"Originally published at:			NVIDIA Launches Magnum IO Software Suite | NVIDIA Technical Blog
At Supercomputing 2019 NVIDIA introduced NVIDIA Magnum IO, a suite of software to help data scientists and AI and high performance computing researchers process massive amounts of data in minutes, rather than hours. Optimized to eliminate storage and input/output bottlenecks, Magnum IO delivers up to 20x faster data processing for multi-server, multi-GPU computing nodes when…Powered by Discourse, best viewed with JavaScript enabled"
3117,gpu-accelerated-robot-wins-amazon-warehouse-challenge,"Originally published at:			GPU-Accelerated Robot Wins Amazon Warehouse Challenge | NVIDIA Technical Blog
Teams worldwide competed in the Amazon Picking Challenge, held at RoboCup 2016 in Leipzig, Germany, to see who’s robot can autonomously recognize objects and pick, and stow, the desired targets from a range of unsorted items. Working in collaboration with Delft Robotics, the team from Delft University of Technology in the Netherlands won the competition…Powered by Discourse, best viewed with JavaScript enabled"
3118,deep-learning-helps-yelp-identify-cover-photos,"Originally published at:			https://developer.nvidia.com/blog/deep-learning-helps-yelp-identify-cover-photos/
Engineers at Yelp, the popular crowd-sourced business review site, developed an algorithm that understands and determines which uploaded image should be a restaurant’s cover photo. In the past, cover photos were chosen by calculating a function based on votes, likes, upload date and image caption – but the system was highly subject to selection bias.…Powered by Discourse, best viewed with JavaScript enabled"
3119,categorical-features-in-xgboost-without-manual-encoding,"Originally published at:			https://developer.nvidia.com/blog/categorical-features-in-xgboost-without-manual-encoding/
XGBoost is a decision-tree–based, ensemble machine learning algorithm based on gradient boosting. However, until recently, it didn’t natively support categorical data. Categorical features had to be manually encoded before they could be used for training or inference. In the case of ordinal categories, for example, school grades, this is often done using label encoding where…It was exciting to explore how XGBoost’s experimental categorical support can save time and improve performance when working with categorical data. If you have any questions or comments, let us know!I was a great post, and happy to find out XGBoost supports categorical data. Do you know if this also extends to arrays of categorical data and how we should deal with them?Powered by Discourse, best viewed with JavaScript enabled"
3120,deploying-real-time-object-detection-models-with-the-nvidia-isaac-sdk-and-nvidia-transfer-learning-toolkit,"Originally published at:			Deploying Real-time Object Detection Models with the NVIDIA Isaac SDK and NVIDIA Transfer Learning Toolkit | NVIDIA Technical Blog
This post is the first in a series that shows you how to use Docker for object detection with NVIDIA Transfer Learning Toolkit (TLT). For part 2, see Using the NVIDIA Isaac SDK Object Detection Pipeline with Docker and the NVIDIA Transfer Learning Toolkit. Figure 1. Inference bounding boxes from DetectNetv2 models fine-tuned on synthetic…Powered by Discourse, best viewed with JavaScript enabled"
3121,lower-end-hardware,"What ongoing work is NVIDIA looking into to make path tracing possible on lower spec hardware?Improvements to path tracing generally fall under ‘hardware’ or ‘software’ categories. Future hardware innovations won’t help improve the performance of existing hardware. But software (algorithmic) improvements generally make things work better across all hardware - including existing hardware and lower spec hardware.Powered by Discourse, best viewed with JavaScript enabled"
3122,cucim-rapid-n-dimensional-image-processing-and-i-o-on-gpus,"Originally published at:			https://developer.nvidia.com/blog/cucim-rapid-n-dimensional-image-processing-and-i-o-on-gpus/
Overview cuCIM is a new RAPIDS library for accelerated n-dimensional image processing and image I/O. The project is now publicly available under a permissive license (Apache 2.0) and welcomes community contributions. In this post, we will cover the overall motivation behind the library and some initial benchmark results. In a complementary post on the Quansight…Is there an implementation example existing for Jetson Nano?
Thx+regardsHi @amirtousa , I am sorry for the late reply.cuCIM’s image loading part (CuImage class and read_region() method) is implemented in C++ and we don’t support wheel library for Jetson Nano (though Conda package supports cuCIM for arm64 SBSA Files :: Anaconda.org (arm64 SBSA includes Jetson boards such as Jetson Xavior/Orin).If you are able to install CuPy package (Add arm64 binary for Jetson Nano · Issue #3196 · cupy/cupy · GitHub and CuPy: Arm Wheels ) in your Jetson Nano, you can install and use cuCIM’s image processing feature.
Please see [FEA] Windows compatibility · Issue #454 · rapidsai/cucim · GitHubThank you!Powered by Discourse, best viewed with JavaScript enabled"
3123,nvidia-announces-nsight-graphics-2021-3-now-available-for-download,"Originally published at:			https://developer.nvidia.com/blog/nvidia-announces-nsight-graphics-2021-3-now-available-for-download/
Nsight Graphics 2021.3 is an all-in-one graphics debugger and profiler to help game developers get the most out of NVIDIA hardware.Powered by Discourse, best viewed with JavaScript enabled"
3124,cuda-spotlight-gpu-accelerated-deep-learning,"Originally published at:			https://developer.nvidia.com/blog/cuda-spotlight-gpu-accelerated-deep-learning/
Our Spotlight is on Dr. Ren Wu, a distinguished scientist at Baidu’s Institute of Deep Learning (IDL). He is known for his pioneering research in using GPUs to accelerate big data analytics and his contribution to large-scale clustering algorithms via the GPU. Ren was a speaker at GTC14 and was originally featured as a CUDA Spotlight…Powered by Discourse, best viewed with JavaScript enabled"
3125,gtc-digital-autonomous-vehicles-presentations-demos-and-posters,"Originally published at:			GTC Digital: Autonomous Vehicles Presentations, Demos, and Posters | NVIDIA Technical Blog
GTC Digital is all the great training, research, insights, and direct access to the brilliant minds of NVIDIA’s GPU Technology Conference, now online. Join live webinars, training, and Connect with the Experts sessions, or choose from a library of talks, panels, research posters, and demos that you can view on your own schedule, at your own…Powered by Discourse, best viewed with JavaScript enabled"
3126,gtc-2020-fast-distributed-joins-with-rapids-and-ucx,"GTC 2020 S21482
Presenters: Nikolay Sakharnykh,NVIDIA; Hao Gao,University of Illinois Urbana-Champaign
Abstract
There are numerous optimized single-GPU join implementations (such as RAPIDS cuDF), but scaling out to multiple GPUs across multiple nodes is challenging. The repartitioned join approach is one of the most popular distributed join algorithms, featuring all-to-all exchange as the main communication pattern. We’ll show how to leverage UCX for efficient all-to-all implementation and demonstrate various optimization strategies, such as reusing communication buffers to speed up GPU-to-GPU transfers and overlapping compute with communications. The implementation is designed to reuse RAPIDS components for single-GPU, and scales to NVLINK and Infiniband systems. Our latest performance results demonstrate that a single DGX-2 can achieve 220 GB/s throughput for joining 8B/8B key-value pairs, while 18 DGX-1V nodes (144 GPUs) connected over IB achieve 503 GB/s, which is comparable with 244 CPU nodes (2K cores) in the best-known distributed CPU implementation.Watch this session
Join in the conversation below.Powered by Discourse, best viewed with JavaScript enabled"
3127,unlocking-new-opportunities-with-ai-cloud-infrastructure-for-5g-vran,"Originally published at:			Unlocking New Opportunities with AI Cloud Infrastructure for 5G vRAN | NVIDIA Technical Blog
NVIDIA is enabling a new approach to host 5G vRAN and AI on the same cloud infrastructure to improve telecom operators’ operational efficiency and unlock new revenue opportunities.Powered by Discourse, best viewed with JavaScript enabled"
3128,gtc-2020-make-decisions-with-real-data-how-to-properly-size-benchmark-and-configure-a-proof-of-concept-with-nvidia-vgpu,"GTC 2020 S21064
Presenters: Jimmy Rotella,NVIDIA
Abstract
Getting started with NVIDIA vGPU technology can appear intimidating, but it’s actually not complicated at all. It’s even easier if you start with some quantifiable data that can help inform your decision-making. Whether you are planning on moving from physical desktops to vGPU-enabled virtual infrastructure, are interested in adding GPUs to your virtual desktop infrastructure, or upgrading the GPUs in your environment, we’ll help you understand the resources required for your environment’s workloads. We’ll review the steps required to do a proof of concept, and we’ll discuss available benchmarking tools, how to test servers at load, and how to collect quantitative and qualitative data for use in your decision-making process.Watch this session
Join in the conversation below.Powered by Discourse, best viewed with JavaScript enabled"
3129,improving-computer-vision-with-nvidia-a100-gpus,"Originally published at:			Improving Computer Vision with NVIDIA A100 GPUs | NVIDIA Technical Blog
During the 2020 NVIDIA GPU Technology Conference keynote address, NVIDIA founder and CEO Jensen Huang introduced the new NVIDIA A100 GPU based on the NVIDIA Ampere GPU architecture. In this post, we detail the exciting new features of the A100 that make NVIDIA GPUs an ever-better powerhouse for computer vision workloads. We also showcase two…Powered by Discourse, best viewed with JavaScript enabled"
3130,thinking-parallel-part-ii-tree-traversal-on-the-gpu,"Originally published at:			https://developer.nvidia.com/blog/thinking-parallel-part-ii-tree-traversal-gpu/
In the first part of this series, we looked at collision detection on the GPU and discussed two commonly used algorithms that find potentially colliding pairs in a set of 3D objects using their axis-aligned bounding boxes (AABBs). Each of the two algorithms has its weaknesses: sort and sweep suffers from high execution divergence, while…Hi! Have anybody managed to get close to 0.25ms for BVH tree traversal timings stated above? My implementation of traverseIterative on GPU is extremely slow (>100ms) which is very surprising as implementation of algorithms from part iii gives very close timings to those declared in paper. I'm testing a 18K triangles scene, which has about 50K potential intersections. If anybody can help with advice, I can provide my source code.Why the if-else in ""traverseIterative"" does not result in divergent? I can't understand this.Are you referring to the piece I quote below?        if (!traverseL && !traverseR)            node = *--stackPtr; // pop        else        {            node = (traverseL) ? childL : childR;            if (traverseL && traverseR)                *stackPtr++ = childR; // push        }I think that this causes execution to diverge, but then probably it can converge again at the end of the else block.  That is only my speculation though...  I too would like to see somebody explain more fully when execution divergence does or does not happen.  I imagine it will vary from one piece of hardware to another, and/or from one compiler to another...I'm down to 2.5 ms or so.  Not what's claimed above (I need to optimize something it seems) but better than 100 ms.In the section where the stack is explicitly managed during traversal, a hard-coded stack size of 64 was used. Where did the number 64 come from? Is there a bound on the number of nodes that will end up in the stack?I think I may have determined the reason. In a complete binary tree, the maximum stack size required is equal to the depth of the tree. The max depth of the tree is limited to 64 since 2^64 is is the limit of the address space on 64 bit machines. Or at least this is the case if you use an array as the underlying implementation of a complete binary tree.Powered by Discourse, best viewed with JavaScript enabled"
3131,real-time-pedestrian-detection-using-cascades-of-deep-neural-networks,"Originally published at:			https://developer.nvidia.com/blog/real-time-pedestrian-detection-using-cascades-of-deep-neural-networks/
Google Research presents a new real-time approach to object detection that exploits the efficiency of cascade classifiers with the accuracy of deep neural networks. Pedestrian detectors is very important as it relates to a variety of applications including advanced driver assistance systems, or surveillance systems. The need for very high-accurate and real-time speed is crucial…Powered by Discourse, best viewed with JavaScript enabled"
3132,building-recommender-systems-faster-using-jupyter-notebooks-from-ngc,"Originally published at:			https://developer.nvidia.com/blog/building-recommender-systems-faster-using-jupyter-notebooks-from-ngc/
The NVIDIA NGC team is hosting a webinar with live Q&A to dive into this Jupyter notebook available from the NGC catalog. Learn how to use these resources to kickstart your AI journey. Register now: NVIDIA NGC Jupyter Notebook Day: Recommender System. Recommender systems deal with predicting user preferences for products based on historical behavior…Thanks for the blog post! I followed along until I couldn’t proceed further because I’m using a Pascal GPU at home (1080Ti).
A couple suggestions:Thanks, looking forward to the webinar!Update: I was able to get past the CUDA error in the training step and it’s training on my lowly 1080Ti! :)@daqieq - would you share how you resolved CUDA error on your 1080Ti? Thanks.@lz3awuo - to be honest … I just ran it again to check it before I gave up and shut down.Thanks for the article. I have a problem with the premise of your article. And that would be this statement: “The Variational Autoencoder (VAE) shown here is an optimized implementation of the architecture first described in Variational Autoencoders for Collaborative Filtering and can be used for recommendation tasks.”And to be clear, with the very last part of the sentence: “VAE can be used for recommendation tasks”. I’m not sure if that’s true. I mean there might have been attempts to use a VAE for such a purpose but I don’t think that works. A recommender system requires a space where distance represents similarity. And I don’t see how the latent space of a VAE satisfies that requirement.I appreciate it if you could help me understand how the latent space of a VAE is suitable for a recommender system.Thanks.@mehranziadloo. The demo and this blog use this nvidia/ngc resource: VAE for TensorFlow | NVIDIA NGC and in this resource we trained the model with movie rating dataset so the model can estimate the rate of a movie for the  new user. With a trained model, you can run inference to predict what items is a new user most likely to interact with so when we trained with rating dataset it can recommend a movie based on the estimated rate.Powered by Discourse, best viewed with JavaScript enabled"
3133,new-nvidia-jetson-framework-containers-now-available-on-ngc,"Originally published at:			https://developer.nvidia.com/blog/new-nvidia-jetson-framework-containers-now-available-on-ngc/
To help developers using NVIDIA Jetson Developer Kits, NVIDIA is releasing new containers on NGC that include the latest AI frameworks and dependencies. The new containers can dramatically reduce the installation time, helping robotics and autonomous machines developers get started with their projects right away.  These include: TensorFlow Container: Includes TensorFlow pre-installed in a Python…Powered by Discourse, best viewed with JavaScript enabled"
3134,sensing-new-frontiers-with-neural-lidar-fields-for-autonomous-vehicle-simulation,"Originally published at:			https://developer.nvidia.com/blog/sensing-new-frontiers-with-neural-lidar-fields-for-autonomous-vehicle-simulation/
Autonomous vehicle (AV) development requires massive amounts of sensor data for perception development. Developers typically get this data from two sources—replay streams of real-world drives or simulation. However, real-world datasets offer limited flexibility, as the data is fixed to only the objects, events, and view angles captured by the physical sensors. It is also difficult…Powered by Discourse, best viewed with JavaScript enabled"
3135,using-openacc-to-port-solar-storm-modeling-code-to-gpus,"Originally published at:			Using OpenACC to Port Solar Storm Modeling Code to GPUs | NVIDIA Technical Blog
Solar storms consist of massive explosions on the Sun that can release the energy of over 2 billion megatons of TNT in the form of solar flares and Coronal Mass Ejections (CMEs). CMEs eject billions of tons of magnetized plasma into space, and while most of them miss Earth entirely, there have been some in…Great job, Ron!This is a fantastic walkthrough. Thanks for writing this report with so many useful details and advice!Powered by Discourse, best viewed with JavaScript enabled"
3136,new-app-uses-deep-learning-to-suggest-emojis,"Originally published at:			New App Uses Deep Learning to Suggest Emojis | NVIDIA Technical Blog
Dango is a new mobile that automatically suggests the best emoji, stickers and GIF images to include in your text conversations. The app relies on a neural network to understand the context of your conversation and predict the best visuals to help you communicate. To do this, the developers used the Theano deep learning framework…Powered by Discourse, best viewed with JavaScript enabled"
3137,new-on-ngc-nvidia-nemo-hpc-sdk-doca-pytorch-lightning-and-more,"Originally published at:			https://developer.nvidia.com/blog/new-on-ngc-nvidia-nemo-hpc-sdk-doca-pytorch-lightning-and-more/
Learn about the latest additions and software updates to the NVIDIA NGC catalog, including NVIDIA NeMo, HPC SDK, DOCA, and PyTorch Lightning.Powered by Discourse, best viewed with JavaScript enabled"
3138,eni-unveils-52-petaflops-supercomputer-world-s-most-powerful-industrial-system,"Originally published at:			Eni Unveils 52 Petaflops Supercomputer, World’s Most Powerful Industrial System | NVIDIA Technical Blog
The world’s most powerful industrial supercomputer is now in operation. Eni, the Italian energy company headquartered in Milan, today announced the supercomputer named HPC5, a GPU-accelerated system capable of performing 52 million billion operations per second, is now in use.  HPC5 will help Eni accelerate research and development of clean energy sources, as well as…Powered by Discourse, best viewed with JavaScript enabled"
3139,artificial-intelligence-system-predicts-human-interactions,"Originally published at:			https://developer.nvidia.com/blog/artificial-intelligence-system-predicts-human-interactions/
Predicting what will happen in the future is challenging. Researchers from MIT’s Computer Science and Artificial Intelligence Laboratory developed an algorithm that can predict whether two individuals will hug, kiss, shake hands or slap five in the next scene. Using a Tesla K40 GPU with the cuDNN-accelerated Caffe deep learning framework, the researchers trained their…Powered by Discourse, best viewed with JavaScript enabled"
3140,deciphering-ancient-texts-with-ai,"Originally published at:			https://developer.nvidia.com/blog/deciphering-ancient-texts-with-ai/
Using machine learning and visual psychophysics, researchers are developing AI models capable of transcribing ancient manuscripts.Powered by Discourse, best viewed with JavaScript enabled"
3141,university-of-florida-launches-ai-partnership-with-nvidia,"Originally published at:			https://developer.nvidia.com/blog/uf-launches-ai-partnership-with-nvidia/
The University of Florida recently announced a $70 million public-private partnership with NVIDIA that will provide students, faculty, and researchers access to the most powerful AI training and tools. This partnership also aims to create a framework to ensure equitable access to bringing AI to students and faculty from across campus and across the state…Powered by Discourse, best viewed with JavaScript enabled"
3142,stanford-researchers-develop-ai-that-can-help-diagnose-alzheimers-disease,"Originally published at:			Stanford Researchers Develop AI that Can Help Diagnose Alzheimer’s Disease | NVIDIA Technical Blog
According to the Alzheimer’s Association, an estimated 5.7 million Americans of all ages are living with Alzheimer’s disease in the United States. This includes 5.5 million people age 65 and older and around 200,000 individuals under age 65 who have younger-onset Alzheimer’s. To help doctors diagnose the disease, researchers from Stanford University developed a deep…Powered by Discourse, best viewed with JavaScript enabled"
3143,video-tutorial-accelerating-inference-performance-of-recommendation-systems-with-tensorrt,"Originally published at:			Video Tutorial: Accelerating Inference Performance of Recommendation Systems with TensorRT | NVIDIA Technical Blog
NVIDIA TensorRT is a high-performance deep learning inference optimizer and runtime that delivers low latency and high-throughput for deep learning inference applications. You can import trained models from every deep learning framework into TensorRT, and easily create highly efficient inference engines that can be incorporated into larger applications and services. This video demonstrates the steps…Powered by Discourse, best viewed with JavaScript enabled"
3144,new-course-develop-customize-and-publish-in-nvidia-omniverse-with-extensions,"Originally published at:			Courses – NVIDIA
Learn how to create and customize the NVIDIA Omniverse experience with extensions using Python code in this free hands-on and self-paced course.Powered by Discourse, best viewed with JavaScript enabled"
3145,video-processing-and-streaming-top-resources-from-gtc-21,"Originally published at:			https://developer.nvidia.com/blog/video-processing-and-streaming-top-resources-from-gtc-21/
This year at GTC we announced the release of NVIDIA Maxine, a GPU-accelerated SDK for building innovative virtual collaboration and content creation applications such as video conferencing and live streaming.Powered by Discourse, best viewed with JavaScript enabled"
3146,ai-predicts-future-lane-changes-of-other-drivers,"Originally published at:			AI Predicts Future Lane Changes of Other Drivers | NVIDIA Technical Blog
In work sponsored by Toyota, University of Michigan researchers developed a deep learning framework trained on GPUs to anticipate maneuvers of other highway vehicles up to three seconds into the future, such as performing left or right lane changes or staying in the same lane. “In the event of sensor failure, it is necessary for…Powered by Discourse, best viewed with JavaScript enabled"
3147,accelerating-python-for-exotic-option-pricing,"Originally published at:			Accelerating Python for Exotic Option Pricing | NVIDIA Technical Blog
In finance, computation efficiency can be directly converted to trading profits sometimes. Quants are facing the challenges of trading off research efficiency with computation efficiency. Using Python can produce succinct research codes, which improves research efficiency. However, vanilla Python code is known to be slow and not suitable for production. In this post, I explore…Powered by Discourse, best viewed with JavaScript enabled"
3148,ace-r-d-testing-options,"Dear devs,I’m leading a start up that will benefit from the ACE solutions. At the moment we’re doing extensive R&D on other technologies, but we’re on time to chose a different path. Then I’m afraid it would be late to re-code: is there a way to have a talk for doing some testing with your solution?ThanksAre you a part of the Inception Program? If so, you should reach out to your inception account manager. They have resources to help with this kind of request. Join NVIDIA Inception for StartupsReading the requirements we are not fully compliant, and this one sounds weird - as a startupMinimum requirements
[…]
** Your startup needs to be incorporated and have been incorporated in the last 10 years.*Please contact me directly - and I can put you in touch with our Inception team who can adviseOk, done thanksPowered by Discourse, best viewed with JavaScript enabled"
3149,generating-new-insights-about-opioids-with-the-summit-supercomputer,"Originally published at:			https://developer.nvidia.com/blog/generating-new-insights-about-opioids-with-the-summit-supercomputer/
The opioid crisis has become one of the worst epidemics in U.S. history, claiming thousands of lives every year. In 2016, over 50,000 people died from opioid overdoses, and in 2017 it is predicted that over 72,000 people died from drug overdoses. To help get a better understanding of how opioids interact with human genes,…Powered by Discourse, best viewed with JavaScript enabled"
3150,developer-spotlight-automated-biomedical-image-annotation,"Originally published at:			Developer Spotlight: Automated Biomedical Image Annotation | NVIDIA Technical Blog
Ishtar Nyawira, a data science intern at the Pittsburgh Supercomputer Center shares how she is using deep learning and the GPU-accelerated Bridges supercomputer to automate the process of biological image annotation from high-resolution scanning electron microscope (SEM) imagery. The goal of the project is to ultimately understand how the neurons in our brains are wired.…Powered by Discourse, best viewed with JavaScript enabled"
3151,catapulting-enterprises-to-the-leading-edge-of-ai-with-nvidia-ai-enterprise-3-1,"Originally published at:			https://developer.nvidia.com/blog/catapulting-enterprises-to-the-leading-edge-of-ai-with-ai-enterprise-3-1/
Generative AI has marked an important milestone in the AI revolution journey. We are at a fundamental breaking point where enterprises are not only getting their feet wet but jumping into the deep end. With over 50 frameworks, pretrained models, and development tools, NVIDIA AI Enterprise, the software layer of the NVIDIA AI platform, is…Powered by Discourse, best viewed with JavaScript enabled"
3152,nvidia-research-tensors-are-the-future-of-deep-learning,"Originally published at:			https://developer.nvidia.com/blog/nvidia-research-tensors-are-the-future-of-deep-learning/
This post discusses tensor methods, how they are used in NVIDIA, and how they are central to the next generation of AI algorithms. Tensors in modern machine learning Tensors, which generalize matrices to more than two dimensions, are everywhere in modern machine learning. From deep neural networks features to videos or fMRI data, the structure…Modern Machine Learning is using tensors to power the next generation of AI algorithms - but have you every tried to describe tensor methods? Check out this post and let us know if this background information is helpful.Powered by Discourse, best viewed with JavaScript enabled"
3153,facial-recognition-software-helping-caterpillar-identify-sleepy-operators,"Originally published at:			Facial Recognition Software Helping Caterpillar Identify Sleepy Operators | NVIDIA Technical Blog
Operator fatigue can potentially be a fatal problem for Caterpillar employees driving the massive mine trucks on long, repetitive shifts throughout the night. Caterpillar recognized this and joined forces with Seeing Machines to install their fatigue detection software in thousands of mining trucks worldwide. Using NVIDIA TITAN X and GTX 1080 GPUs along with the…Powered by Discourse, best viewed with JavaScript enabled"
3154,new-features-in-cuda-7-5,"Originally published at:			New Features in CUDA 7.5 | NVIDIA Technical Blog
Today I’m happy to announce that the CUDA Toolkit 7.5 Release Candidate is now available. The CUDA Toolkit 7.5 adds support for FP16 storage for up to 2x larger data sets and reduced memory bandwidth, cuSPARSE GEMVI routines, instruction-level profiling and more. Read on for full details. 16-bit Floating Point (FP16) Data CUDA 7.5 expands…Thank you for adding Clang support on Linux!  I've been looking forward to that feature for a while.  Once Clang gains full MSVC ABI compatibility on Windows ( http://clang.llvm.org/docs/... ), is there a chance that Cuda will support compiling with Clang on Windows?Clang on Linux has reached a level of maturity that made it a) feasible for us to support it as a host compiler and b) created demand for it as a host compiler. If/when Clang achieves that level of maturity on Windows, naturally NVIDIA will consider supporting it as a host compiler on Windows too.Does the FP16 supported in cuFFT? for example, can we use __half2 complex?.I get this message on my MacBook Pronvcc fatal   : The version ('60100') of the host compiler ('Apple clang') is not supportedA colleague recently recommended this post that covers the use of recursive neural networks RNNs in Natural Language Processing (NLP). Good stuff: http://colah.github.io/post...Anyone tried CUDA 7.5 RC (64bit version for windows 8.1 64) yet? I have encountered some serious problems with this version. Some computationally-intensive codes works flawless with CUDA 6.0-7.0, now broken with this 7.5. For instance my program can hang there forever and never returns if it is built by CUDA 7.5 yet works flawlessly with eariler CUDA versions(6.0, 6.5 and 7.0 are the versions I tested so far), and Nsight/cuda-memchecker cannot find any problems with programs either (they simply hang there forever without reporting any issues).I tested it on two different systems, one is a laptop (Haswell with a 970M gtx) and the other is a 3-GPU 2-socket Haswell-EP workstation with one K6000 and two Titan-X installed, on both systems, the programs hang there forever with CUDA 7.5 and return normally with CUDA 7.0.I dont know if this is just me or anyone else have experienced simliar problem?In CUDA 7.5, have nvidia change any default behaviors for CUDA besides the null stream's behavior? or could that be just a driver issue.At the moment I dont have time to locate the problems so I simply go back to 7.0.I had the exact same problem with CUDA7.0 on my laptop machine in debug mode under Visual Studio. The program hang at the first line of code to allocate GPU memory. Actually I found out if I wait long enough, it will return eventually, but the wait is painfully long.  I reported the bug but that bug report is still open as of today. I just installed CUDA7.5 and was hoping that they fixed the problem in the new version. Apparently, the problem is still there in this new version.Hi wq, this is one of the reasons we do a public RC -- to find issues early! I'm sorry you are having a bad experience so far. Would you be able to create a repro example? If you are a registered developer (register if you have not already here: https://developer.nvidia.co..., you can log in and file a bug and attach the repro.  Alternatively you can email it to me at <first initial="""" last="""" name@nvidia.com="""">  If you file a bug, please post or send the bug ID#. Thanks!Hi sally, I'm sorry to hear this.  Can you post the bug ID#?  Thanks!Hi Mark,Thanks for the response. The bug ID# is 1644368. I also sent a repro project to your team and got confirmed that was reproducible ""on a Dell XPS platform that configured with a GT640M GPU with CUDA 7.0 Production release. Looks like it’s a specific issue on notebook model GPUs which using another graphic(i.e. intel) as display output."".   I can forward you the email if you want. Thanks again.Thanks for your reply, after a little investigation I can be 99% certain that  this problem is related to an open-source CUDA library function my program called, don't know if this is a CUDA 7.5 issue, but this library has not given me any problems when built by earlier CUDA versions, I will try to contact the author of the library first.Hi Mark,I found out more. The code generation setting for CUDA C/C++\DEVICE on my project was set to ""compute_20,sm_20;"". When I added compute_30,sm_30 to the setting, that problem is gone.Hi Sally, this sounds like JIT compilation overhead. When you compile with the SM version of the GPU you are running on, the runtime doesn't need to JIT from PTX. But if you compile only for an older version, it has to JIT at startup. Usually it stores the JITted code in a cache, but there are cases where it gets flushed, or doesn't fit, or is on a remote share (in a cluster situation), all of which can make this overhead take longer than normal.  I recommend you read this post: http://devblogs.nvidia.com/...Please see my reply to sally below regarding JIT compilation overhead. Let's make sure you are not having the same problem. Perhaps the library is compiled for the default or an older architecture, and it has a lot of kernels that have to be JITted at startup? What library is it?Mark,Just read the blog you recommended. From what I have seen, it does look like JIT overhead, and since my project is pretty big and uses several libraries, so it could have hit the cache size limit. In my testing when I removed all the files from my project and just have very few lines of testing code in the main function, that long start up time is gone. But as I added my files back, even I have no code calling them, I start to see this problem again.What I don't understand is that my code generation setting was ""compute_20,sm_20"", and I was running debug code on my local machine which has number of SMs = 2. So it should not need JIT.  I don't have this problem if I switch the build target back to CUDA 6.5. And I don't see this long wait with CUDA7 running in release mode either. At least now I have a workaround of building ""Fat Binaries"" to avoid this problem.Hi Sally: sm_20 refers to NVIDIA SM architecture version 2.0 (also known as Compute Capability 2.0), aka Fermi. It does not refer to the number of SMs on the GPU, but the capabilities (i.e. instruction set architecture) of the SM. So when you compile for a different SM version, you are targeting a different instruction set, not a different number of SMs. What GPU do you have on your local machine? I suspect it is a Kepler GPU, SM version 3.x.No, I always build my codes with the specific compiler options that match the targeting CUDA device (in my case: cc/sm 3.5 and 5.2),  the library involved is NVIDIA's CUB library, which is a template C++ library, so it has the same compiler options as my main programs,  with CUDA  7.5  certain CUB library functions behave like they encounter dead-locks: the GPU is busy but the actual load is very low, and it hangs there basically forever, but thats just my wild guess.I have already informed the author of CUB about this.Oh, now I got it. Yes my GPU is Kepler with COMPUTE_CAPABILITY_MAJOR = 3 and COMPUTE_CAPABILITY_MINOR = 0I will check with Duane (CUB author). But if you can provide a simple repro that would help.Powered by Discourse, best viewed with JavaScript enabled"
3155,turbocharging-multi-cloud-security-and-application-delivery-with-virtio-offloading,"Originally published at:			Turbocharging Multi-Cloud Security and Application Delivery with VirtIO Offloading | NVIDIA Technical Blog
By accelerating Virtio-net in hardware, poor network performance can be avoided while maintaining transparent software implementation, including full support for VM live migration.Powered by Discourse, best viewed with JavaScript enabled"
3156,nvidia-announces-nsight-graphics-2021-1,"Originally published at:			https://developer.nvidia.com/blog/nvidia-announces-nsight-graphics-2021-1/
Nsight Graphics 2021.1 is available to download. We now provide you with the ability to set any key to be the capture shortcut. This new keybinding is supported for all activities, including GPU Trace. F11 is the default binding for both capture and trace, but if you prefer the old behavior, the original capture keybinding…Powered by Discourse, best viewed with JavaScript enabled"
3157,introducing-isaac-sdk-and-sim-2020-1,"Originally published at:			https://developer.nvidia.com/blog/introducing-isaac-sdk-2020-1/
The NVIDIA Isaac Software Development Kit (SDK) enables accelerated AI robot development workflows. Stacked with new tools and application support, Isaac SDK 2020.1 is an end-to-end solution supporting each step of robot fleet deployment, from design collaboration and training to the ongoing maintenance of AI applications.  Isaac SDK 2020.1, the latest version of Isaac SDK…Powered by Discourse, best viewed with JavaScript enabled"
3158,telemetry-driven-network-quality-and-reliability-monitoring-with-nvidia-netq-4-0-0,"Originally published at:			https://developer.nvidia.com/blog/telemetry-driven-network-quality-and-reliability-monitoring-with-nvidia-netq-4-0-0/
NVIDIA NetQ is a highly-scalable modern network operations tool leveraging fabric-wide telemetry data for visibility and troubleshooting of the overlay and underlay network in real-time.Powered by Discourse, best viewed with JavaScript enabled"
3159,ai-composer-creates-new-rock-music,"Originally published at:			AI Composer Creates New Rock Music | NVIDIA Technical Blog
Aiva Technologies, the company behind the world’s first non-human AI music composition, just released a brand new AI-generated rock music track. The work builds on the company’s previous compositions that range from cinematic film scores to Chinese music. Using NVIDIA TITAN Xp GPUs, GeForce GTX 1080 Ti GPUs, the company re-trained their deep neural network…Powered by Discourse, best viewed with JavaScript enabled"
3160,6-can-t-miss-experiences-at-the-gpu-technology-conference,"Originally published at:			https://developer.nvidia.com/blog/6-cant-miss-experiences-at-the-gpu-technology-conference/
Explore the future of artificial intelligent and deep learning, experience virtual reality, and see what the future holds for self-driving cars at the GPU Technology Conference in Silicon Valley, April 4-7. In addition to keynotes by notable speakers that include NVIDIA CEO Jen-Hsun Hung, Toyota Research Institute CEO Gill Pratt, and IBM Watson CTO Rob…Powered by Discourse, best viewed with JavaScript enabled"
3161,input-and-output-configurability-in-rapids-cuml,"Originally published at:			https://developer.nvidia.com/blog/input-and-output-configurability-in-rapids-cuml/
The RAPIDS machine learning library, cuML, supports several types of input data formats while attempting to return results in the output format that fits best into users’ workflows. The RAPIDS team has added functionality to cuML to support diverse types of users: Figure 1: An example optimized cuML workflow. Maximize compatibility Users with existing NumPy,…Powered by Discourse, best viewed with JavaScript enabled"
3162,rendering-perfect-reflections-and-refractions-in-path-traced-games,"Originally published at:			Rendering Perfect Reflections and Refractions in Path-Traced Games | NVIDIA Technical Blog
Figure 1. Railgun trail reflects and refracts as a Quake II RTX player shoots the railgun from underwater. With the introduction of hardware-accelerated ray tracing in NVIDIA Turing GPUs, game developers have received an opportunity to significantly improve the realism of gameplay rendered in real time. In these early days of real-time ray tracing, most…Powered by Discourse, best viewed with JavaScript enabled"
3163,nvidia-collaborates-with-taiwan-government-to-supercharge-ai-efforts,"Originally published at:			NVIDIA Collaborates with Taiwan Government to Supercharge AI Efforts | NVIDIA Technical Blog
NVIDIA announced at Computex 2018 the extensive collaboration with Taiwan’s Ministry of Science and Technology that will advance the country’s artificial intelligence capabilities. “Taiwan was at the center of the PC revolution and now it is investing to play an important role in the next era of computing,” said Jensen Huang, founder and chief executive…Powered by Discourse, best viewed with JavaScript enabled"
3164,autonomous-robot-starts-work-as-office-manager,"Originally published at:			Autonomous Robot Starts Work as Office Manager | NVIDIA Technical Blog
Programmed with the latest artificial intelligence software, Betty will spend the next two months working as an office manager at Transport Systems Catapult monitoring staff and check environmental conditions. The robot, developed by engineers at the University of Birmingham, uses NVIDIA GPUs for various forms of computer vision — like feature extraction — and 3D…Powered by Discourse, best viewed with JavaScript enabled"
3165,dealing-with-outliers-using-three-robust-linear-regression-models,"Originally published at:			3 Robust Linear Regression Models to Handle Outliers | NVIDIA Technical Blog
Learn how different robust linear regression models handle outliers, which can significantly affect the results of a linear regression analysis.Powered by Discourse, best viewed with JavaScript enabled"
3166,new-method-helps-vr-users-walk-in-small-physical-spaces,"Originally published at:			https://developer.nvidia.com/blog/new-method-helps-vr-users-walk-in-small-physical-spaces/
Researchers from NVIDIA, Adobe, and Stony Brook University developed a system that allows VR users to explore large virtual worlds in small physical spaces, enabling them to avoid walls, furniture, or other players. The research, first presented at the GPU Technology Conference (GTC) in San Jose earlier this year and featured in an NVIDIA blog,…Powered by Discourse, best viewed with JavaScript enabled"
3167,cross-process-synchronization-improves-vr-performance,"Originally published at:			Cross-Process Synchronization Improves VR Performance | NVIDIA Technical Blog
Modern VR Rendering Pipeline Architecture The modern VR rendering pipeline involves multiple steps broadly categorized as rendering and post processing. Post rendering, the HMD compositor processes the rendered views to better suit the characteristics of the display and to compensate for pose latency before the textures are displayed. Figure 1 illustrates the process flow. This post discusses cross-process synchronization,…Powered by Discourse, best viewed with JavaScript enabled"
3168,easy-multi-gpu-deep-learning-with-digits-2,"Originally published at:			https://developer.nvidia.com/blog/easy-multi-gpu-deep-learning-digits-2/
DIGITS is an interactive deep learning development tool for data scientists and researchers, designed for rapid development and deployment of an optimized deep neural network. NVIDIA introduced DIGITS in March 2015, and today we are excited to announce the release of DIGITS 2, which includes automatic multi-GPU scaling. Whether you are developing an optimized neural…Thanks for the post. We just got the digits box this week and comes with digits v1 pre-installed.what is the best procedure to update to Digits V2?got it.  I follow the instructions from:https://github.com/NVIDIA/D...both digits are running at same time in different ports by using ./runme.sh -p 5001I am a researcher running many different models. Is there a way to script the model set-up (maybe through a command line interface, a la DIGITS 1)?How do you extend Digits (1 or 2) to support speech recognition tasks?In the standard setup only image datasets can be imported, no audio datasets.Also all models you can create are for image classification only.How could you add functionality to do something as described in ""Deep Speech: Accurate Speech Recognition with GPU-Accelerated Deep Learning"" in this web server based platform with ""Improved Visualization and Monitoring"" and ""classification during training"" for sound files etc.?If you can insert your data as a linear array such a [1xnumber], you can upload it into DIGITS. It will treat the input data like a image with a single line of pixels. Right now DIGITS supports classification, would you like to perform classification with your audio data?Are you interested in quickly visualizing each network's performance? It is easy to toggle between network results via the main console, and it is relatively easy to launch a training on each of your GPUs or multiple ones via the web interface.  If you don't care about the visualization part you could just write a shell script that will launch all of your trainings for you, assigning them to the GPUs.Thanks for your quick reply Allison! I am interested in the latter. Is there documentation that I can follow to set this up? The downside of a beautiful GUI is one loses contact with the commandsI just reviewed our API commands and I don't see anything for training with DIGITS this way, https://github.com/NVIDIA/D...If you just want to launch trainings you can use Caffe directly./path/to/caffe/build/tools/caffe train --gpus=0 --solver=solver.prototxtIf you downloaded DIGITS with the web installer (https://developer.nvidia.co..., caffe will likely be in path/to/digits-1.0/caffe.I just realized you mention ""model setup"" above, do you also want to create a variety of different NN configurations too via the command line?Hi again,Yes, basically I want to create many different kind of models using the command line. I suppose I can do that via Caffe, I just assumed there was something in DIGITS (performance-related) that was not part of the master branch of Caffe.If this is indeed not the case, is there still a way to load trained Caffe models into DIGITS and visualise them, or do I need to create them via the DIGITS GUI for that? I just find very time-consuming to have to navigate through the buttons and menus to set up multiple models.Thanks,I have not seen an easy way to do that with DIGITS.I have heard of folks using tools (or creating them themselves) for this but I don't recall their names now. I tried to do a quick search on github but didn't find anything.I stumbled across this tool, but it doesn't seem to be a random NN creator, https://github.com/Chasvort... which it sounds like you are looking for.If I were going to do this myself, I would create it in python (just because it is a language I am comfortable with),  allowing for patching layer components together and generating/saving the new train_val.prototxt files.Have you tried posting this question on one of the framework user groups?Yes, you can create a model outside of DIGITS and then use it for classification. There are some instructions here - https://github.com/NVIDIA/D.... This allows you to load a pretrained network, but it won't show you the change in accuracy and loss as a function of epoch.Any plans of having a Windows version of Digits in the near future?It looks like there has been some activity from users on this -https://github.com/NVIDIA/D...Nvidia,please release a windows version! You hava a large base of windows users (Gaming community, etc) so I think is the natural evolution of ML to take this market. I have my self a windows server 2012 with 2 Maxwell 980 GPU's waiting for this... and if you really launch it, I will swap those 2 for 3 Titan X.... your move Nvidia :)I am trying to run it on a virtual machine but I am receiving the error message: ""CUDA driver version is insufficient for CUDA runtime version"". Is it possible to run on VM?You should be able to run DIGITS on a VM. What OS and driver are you using? Are you able to run any of the CUDA samples?I'm getting a similar problem (Ubuntu 14.04 on a VM), when I run ./runme.sh I get ""cudaRuntimeGetVersion() failed with error #35"", the server loads and the interface is reachable but I'm assuming DIGITS doesn't get GPU assistance here. When I try to install the Linux drivers on my virtual machine (Ubuntu) I get an error saying it does not recognize a compatible card. I have a GeForce GTX 460. Any ideas? Thanks!I haven't tried using a GeForce card for pass-through on a VM. I assume you are able to run nvidia-smi and any CUDA sample program, is this correct?when I run nvidia-smi I get ""NVIDIA-SMI has failed because it couldn't communicate with the NVIDIA driver. Make sure that the latest NVIDIA driver is installed and running"". And if I try installing the drivers I downloaded from the NVidia website (Linux 64) I also get an error saying it couldn't find a CUDA compatible card.... My card is CUDA compliant according to the Nvidia web site, as far as I understand. Much appreciate your help!Do you know if your VM can see the GPU? If you run ""lspci | grep NVIDIA"" do you see any NVIDIA devices?How did you install your NVIDIA driver? Are you using the cuda db file from  -https://developer.nvidia.co...Powered by Discourse, best viewed with JavaScript enabled"
3169,nasa-study-uses-ai-and-supercomputers-to-reveal-billions-of-trees-in-the-sahara-desert,"Originally published at:			NASA Study Uses AI and Supercomputers to Reveal Billions of Trees in the Sahara Desert | NVIDIA Technical Blog
NASA scientists and collaborators have achieved a supercomputing and AI breakthrough, a deep learning model that has the potential, with some limitations, to map the location and size of every tree worldwide.  The new model, described in a new Nature paper, lays the foundation for a more accurate global measure of carbon storage on land.  In…Powered by Discourse, best viewed with JavaScript enabled"
3170,developing-robotics-applications-in-python-with-nvidia-isaac-sdk,"Originally published at:			https://developer.nvidia.com/blog/developing-robotics-applications-in-python-with-isaac-sdk/
Figure 1. Controlling a virtual robot in IsaacSim with a Jupyter notebook and the Isaac SDK Python API. The modular and easy-to-use perception stack of NVIDIA Isaac SDK continues to accelerate the development of various mobile robots. Isaac SDK 2020.1 introduces the Python API, making it easier to build robotic applications for those who are…The Audio GEMS have been removed from Isaac 2020.1NX is there somewhere I can download them?Hi, @jjhwadsworth,The audio gems was removed from 2020.1NX due to limited resource for migrating gems. Sorry we don’t have a running version on NX this moment.Thanks for the reply. Can you put a highlighted note at the top of the NX documentation / downloads to let developers know this please as it will avoid anyone else “upgrading” who needs the audio GEMS. Do you have a time frame for migrating them or should I just downgrade to 2020.1?Hi, @jjhwadsworth,At the moment I don’t have much info about audio package. If it is important then guess you could stick with 2020.1 release for now.@jjhwadsworth,We will be bringing Conversational AI capabilities to Isaac soon. Stay tuned.I’ve signed up for the Jarvis Beta, when can I expect someone to look at my application and approve access?i have not been able to find a instalation guide and or videos to help me install and launch,the instalation page firts step is a location i do not have on my drive. i never thought i was so terrible at this,sure is embarassing and energy consuming. i wish i could just have it running… ast resort is herei dont think it can be made harder to install and launch. 2 lines i cant find,is there a genie i need to ask.or do i need to become a space engeneer to launch it.when at the beginning, I run bazel run apps/mybot:mybot, I got the error
ERROR: Skipping ‘apps/mybot:mybot’: error loading package ‘apps/mybot’: Every .bzl file must have a corresponding package, but ‘//engine/build:isaac.bzl’ does not have one. Please create a BUILD file in the same or any parent directory. Note that this BUILD file does not need to do anything except exist.I have the same error. Have you solved this?Hi, @toanngosy ,The audio package was removed due to limited resource. Please stick with 2020.1 release if the package is important to you.Powered by Discourse, best viewed with JavaScript enabled"
3171,new-app-turns-your-selfie-into-a-personalized-emoji,"Originally published at:			New App Turns Your Selfie Into a Personalized Emoji | NVIDIA Technical Blog
San Francisco-based startup Mirror AI released the beta version of their deep learning-based mobile app that instantly turns your selfie into a collection of custom emojis that look like you. “[Mirror Emoji Keyboard] is the culmination of a year’s worth of work from our dedicated and experienced engineering team and is only the beginning,” Serge…Powered by Discourse, best viewed with JavaScript enabled"
3172,gtc-digital-autonomous-machines-presentations-demos-and-posters,"Originally published at:			GTC Digital: Autonomous Machines Presentations, Demos, and Posters | NVIDIA Technical Blog
GTC Digital is all the great training, research, insights, and direct access to the brilliant minds of NVIDIA’s GPU Technology Conference, now online. Join live webinars, training, and Connect with the Experts sessions, or choose from a library of talks, panels, research posters, and demos that you can view on your own schedule, at your own…Powered by Discourse, best viewed with JavaScript enabled"
3173,for-the-first-time-ai-helps-recreate-hidden-picasso-painting,"Originally published at:			For the First Time, AI Helps Recreate Hidden Picasso Painting | NVIDIA Technical Blog
To help recreate a lost Picasso painting, University College London researchers used deep learning to recreate parts of The Old Guitarist, one of Picasso’s most famous paintings from the Blue Period.  During this period, dating back to the early 1900s, Picasso used blue in his paintings to convey the pain and solitude he experienced during…Powered by Discourse, best viewed with JavaScript enabled"
3174,advanced-api-performance-async-compute-and-overlap,"Originally published at:			https://developer.nvidia.com/blog/advanced-api-performance-async-compute-and-overlap/
This post covers best practices for async compute and overlap on NVIDIA GPUs. To get a high and consistent frame rate in your applications, see all Advanced API Performance tips. The general principle behind async compute is to increase the overall unit throughput by reducing the number of unused warp slots and to use nonconflicting…Writing this blog has been very insightful. Finding overlap opportunities for different datapaths is my personal favorite. Perhaps because it’s more challenging to find.I would like to underscore the importance of GPU Trace for displaying a large amount of valuable performance data in a meaningful and organized way. Its an absolute enabler in visualizing performance gaps that can lead to improvement opportunities such as async compute.If you have any questions or you want to share your experience on this topic, please feel free to reply!Powered by Discourse, best viewed with JavaScript enabled"
3175,developing-cuda-kernels-to-push-tensor-cores-to-the-absolute-limit-on-nvidia-a100,"GTC 2020 S21745
Presenters: Andrew Kerr, NVIDIA
Abstract
NVIDIA Ampere GPU Architecture pushes the performance envelope by doubling the math throughput of Tensor Cores for mixed precision and also adds support for double precision, Tensor Float 32, and bfloat16 data types. We’ll describe how to implement high-performance CUDA kernels using Tensor Cores on A100, applying techniques such as register blocking, software pipelining, and carefully constructed memory layouts to avoid bank conflicts. Then we’ll describe abstractions for programming Tensor Cores available in CUTLASS, as well as other new features. This talk is intended for advanced CUDA C++ programmers who are eager to write kernels pushing Tensor Cores to peak performance. We recommend that you review previous presentations on this topic such as the introduction to CUTLASS (GTC 2018) and Programming Volta Tensor Cores in CUTLASS (GTC 2019).Watch this session
Join in the conversation below.Powered by Discourse, best viewed with JavaScript enabled"
3176,cutensor-v1-3-0-now-available-up-to-2x-performance,"Originally published at:			cuTENSOR v1.3.0 Now Available: Up to 2x Performance | NVIDIA Technical Blog
Today, NVIDIA is announcing the availability of cuTENSOR version 1.3.0. This software can be downloaded now free for members of the NVIDIA Developer Program.Powered by Discourse, best viewed with JavaScript enabled"
3177,custom-datasets,"Does the license allow us to train our own dataset?  Can we provide a bunch of 3D human characters and train it on those 3D models?
As mentioned in another thread… that would also be useful to come with a standard game engine skeleton (unreal’s skeleton) and yes eventually also trained with the text description data at the same time.Yes, you can use the released code to train the model on your own dataset. However, please make sure to first check the intended use is in agreement with Nvidia Source Code license (GET3D/LICENSE.txt at master · nv-tlabs/GET3D · GitHub)This topic was automatically closed 14 days after the last reply. New replies are no longer allowed.Powered by Discourse, best viewed with JavaScript enabled"
3178,inception-spotlight-deepset-collaborates-with-nvidia-and-aws-on-bert-optimization,"Originally published at:			Inception Spotlight: Deepset collaborates with NVIDIA and AWS on BERT Optimization | NVIDIA Technical Blog
Deepset bridges the gap between NLP research and industry - their core product, Haystack, is an open-source framework that enables developers to utilize the latest NLP models for semantic search and question answering at scale.Powered by Discourse, best viewed with JavaScript enabled"
3179,increasing-inference-acceleration-of-kogpt-with-nvidia-fastertransformer,"Originally published at:			https://developer.nvidia.com/blog/increasing-inference-acceleration-of-kogpt-with-fastertransformer/
Transformers are one of the most influential AI model architectures today and are shaping the direction of future AI R&D. First invented as a tool for natural language processing (NLP), transformers are now used in almost every AI task, including computer vision, automatic speech recognition, molecular structure classification, and financial data processing. In Korea, KakaoBrain…Powered by Discourse, best viewed with JavaScript enabled"
3180,nvidia-opens-robotics-research-lab-in-seattle,"Originally published at:			NVIDIA Opens Robotics Research Lab in Seattle | NVIDIA Technical Blog
NVIDIA is opening a new robotics research lab in Seattle near the University of Washington campus led by Dieter Fox, senior director of robotics research at NVIDIA and professor in the UW Paul G. Allen School of Computer Science and Engineering. The charter of the lab is to drive breakthrough robotics research to enable the…Powered by Discourse, best viewed with JavaScript enabled"
3181,is-there-a-sample-of-pt-sdk-with-blendshape-animation,"the sample code shows how to integrate PT sdk with skeleton animation,
is there a sample showing PT sdk with blendshape animation?Thank you for the question. There isn’t any support right now. But the main thing that the PT SDK is trying to demonstrate is that it does work with animated scene elements. The actual implementation of the animation isn’t important for demonstrating path tracing. There is nothing preventing blend shape animation being implemented in principle though.Powered by Discourse, best viewed with JavaScript enabled"
3182,join-the-geforce-now-cloud-gaming-open-platform,"Originally published at:			Join the GeForce NOW Cloud Gaming Open Platform | NVIDIA Technical Blog
NVIDIA has been optimizing PC game streaming for more than a decade. In April, we’re making our technology available to the game development community through a limited release of the GeForce NOW SDK.Powered by Discourse, best viewed with JavaScript enabled"
3183,download-the-second-installment-in-2019-s-defining-book-on-ray-tracing-for-free,"Originally published at:			Download the Second Installment in 2019’s Defining Book on Ray Tracing for Free | NVIDIA Technical Blog
We’ve kicked off 2019 with a free gift to the development community. Ray Tracing Gems – a cutting-edge book about ray tracing techniques  – can be downloaded at no cost as a seven-part series, hosted on NVIDIA Developer Zone. The book is releasing in hardcover soon, but we wanted to give developers access to the…Powered by Discourse, best viewed with JavaScript enabled"
3184,nvidia-and-mozilla-release-common-voice-dataset-surpassing-13-000-hours-for-the-first-time,"Originally published at:			https://developer.nvidia.com/blog/nvidia-and-mozilla-release-common-voice-dataset-surpassing-13000-hours-for-the-first-time/
As part of NVIDIA’s collaboration with Mozilla Common Voice, the models trained on this and other public datasets are made available for free via an open-source toolkit called NVIDIA NeMo.Powered by Discourse, best viewed with JavaScript enabled"
3185,artificial-intelligence-and-drones-help-combat-poaching-in-africa,"Originally published at:			Artificial Intelligence and Drones Help Combat Poaching in Africa | NVIDIA Technical Blog
Boston-based AI startup Neurala has partnered with the Lindbergh Foundation in an effort to combat poaching in Africa using intelligent drones and deep learning. “This is a terrific example of how AI technology can be a vital force for good,” said Neurala CEO Max Versace. “We’re thrilled to be working with the Lindbergh Foundation in…Powered by Discourse, best viewed with JavaScript enabled"
3186,meet-the-researcher-avantika-lal-discovering-genes-proteins-and-biological-processes-altered-by-covid-19,"Originally published at:			https://developer.nvidia.com/blog/meet-the-researcher-avantika-lal-discovering-genes-proteins-and-biological-processes-altered-by-covid-19/
Dr. Avantika Lal is a deep learning and genomics scientist at NVIDIA and was previously a researcher at Stanford University. She holds a PhD in genomics and is an expert in the genomics of infectious diseases and cancer. At NVIDIA, she develops artificial intelligence techniques to analyze genomic data, and applies these methods to understand…Working with Avantika, her deep appreciation for science and the desire to push the research envelope is self evident. This Q&A format of discovery has been a great way to learn more about her background along with the current AI research she is leading at NVIDIA. If you have any follow on questions or comments, please let us know.Powered by Discourse, best viewed with JavaScript enabled"
3187,free-self-paced-online-course-for-intelligent-video-analytics-now-available,"Originally published at:			Free Self-Paced Online Course for Intelligent Video Analytics Now Available | NVIDIA Technical Blog
Converting large amounts of video data to real-time actionable insights is crucial to success in every industry, from smart cities, retail, manufacturing, to healthcare. Companies and institutions are looking for professionals and enthusiasts who can understand the challenges in traditional video analytics and can bring their AI expertise to enable and build intelligent video analytics-based…Powered by Discourse, best viewed with JavaScript enabled"
3188,visualizing-interactive-simulations-with-omniverse-extension-for-nvidia-modulus,"Originally published at:			https://developer.nvidia.com/blog/visualizing-interactive-simulations-with-omniverse-extension-for-nvidia-modulus/
Learn how you can prepost process your NVIDIA Modulus simulations using the Modulus Omniverse extension.Powered by Discourse, best viewed with JavaScript enabled"
3189,accelerate-academic-research-and-curriculum-with-the-nvidia-hardware-grant-program,"Originally published at:			https://developer.nvidia.com/blog/accelerate-academic-research-and-curriculum-with-the-nvidia-hardware-grant-program/
The NVIDIA Hardware Grant Program helps advance AI and data science by partnering with academic institutions around the world to enable researchers and educators with industry-leading hardware and software.Powered by Discourse, best viewed with JavaScript enabled"
3190,question-about-ris-reservoir-sampling-pdf-calculations-for-mis,"Hi,I recently read through the free RTGII chapter about reservoir sampling. I’ve also noticed that resampled importance sampling has been gaining popularity (e.g. ReSTIR). These both seem like useful techniques when sampling direct lighting, but it seems difficult to re-compute PDFs for the sake of MIS when a light has been sampled through a BSDF sample.For example, in the case of resampled importance sampling, we don’t know what the other samples in the list would be. Could you provide any insights on this problem?Thanks!Interesting question indeed - had to think about that for a while, and
actually had to write two different answers :-).One possible answer is that sample importance resampling is only
generating samples relative to an existing PDF (that you choose to
take these samples), so it should be possible to evaluate that
existing PDF to compute PDF values for other samples (in an MIS context).I’d have to think a bit more about it, though, as it’s a tricky
technical question indeed. I’d be happy to chat more about that offlineAwesome, thanks for the response. What would be the best way to contact you?Either DM through the forum, or google my academic email address - should be easy to find :-)This topic was automatically closed 14 days after the last reply. New replies are no longer allowed.Powered by Discourse, best viewed with JavaScript enabled"
3191,new-ai-breast-cancer-model-is-the-first-to-show-diagnostic-process,"Originally published at:			https://developer.nvidia.com/blog/new-ai-breast-cancer-model-is-the-first-to-show-diagnostic-process/
Researchers create a new AI algorithm that can analyze mammography scans, identify whether a lesion is malignant, and show how it reached its conclusion.Powered by Discourse, best viewed with JavaScript enabled"
3192,nasa-using-deep-belief-networks-for-image-classification,"Originally published at:			NASA Using Deep Belief Networks for Image Classification | NVIDIA Technical Blog
Spurred by the need for neural networks capable of tackling vast wells of high-res satellite data, a team from the NASA Advanced Supercomputing Division at NASA Ames and Louisiana State University have sought a new blend of deep learning techniques that can build on existing neural nets to create something robust enough for satellite datasets.…Powered by Discourse, best viewed with JavaScript enabled"
3193,gtc-2020-unlocking-gpu-accelerated-data-science-at-scale-with-active-analytics,"GTC 2020 S21818
Presenters: Nima Negahban,Kinetica; Nick Alonso,Kinetica
Abstract
We’ll highlight how a GPU-accelerated active analytics platform streamlines traditional data-science workloads, in a democratized, pushbutton environment. We’ll explore a demo that shows how GPU-accelerated data science can significantly reduce time to insight, uncover new results, and improve model performance. Key topics will include simplified data access and engineering, integrated model training with RAPIDS, autonomous Kubernetes orchestration, comprehensive model auditing, and accelerated geospatial analytics at scale using the raw processing power of the GPU. You’ll see how GPUs are a critical component in accelerating and augmenting the modern data science life cycle.Watch this session
Join in the conversation below.Powered by Discourse, best viewed with JavaScript enabled"
3194,regarding-generating-3d-shapes-of-cars-bikes-and-chairs,"Hi Team. I ran the inference code on colab and was able to generate 25 shapes of Cars, Bikes and chairs. Is there a way to create more? and is there way to create 3d shapes of other categories?Yes, it’s possible to generate more objects, you can simply change a grid_size to a larger number in this line GET3D/inference_3d.py at master · nv-tlabs/GET3D · GitHub e.g. something like (10,10) can give you 100 shapes. It’s also possible create other categories, but it would require us to train the GET3D for it, right now due to license restrictions, we are only able to releaase, cars, bikes, chairs and tables.This topic was automatically closed 14 days after the last reply. New replies are no longer allowed.Powered by Discourse, best viewed with JavaScript enabled"
3195,nvidia-announces-nsight-graphics-2020-2,"Originally published at:			NVIDIA Announces Nsight Graphics 2020.2 | NVIDIA Technical Blog
Nsight Graphics 2020.2 is now available for download. We’ve added a number of features that dramatically expand the functionality of our tools when it comes to Vulkan, as well as making some great improvements to developer workflows for Microsoft Visual Studio users. By popular demand, GPU Trace can now profile Vulkan based applications! When profiling…Powered by Discourse, best viewed with JavaScript enabled"
3196,openai-releases-musenet-ai-algorithm-automatically-generates-music,"Originally published at:			OpenAI Releases MuseNet: AI Algorithm Automatically Generates Music | NVIDIA Technical Blog
Trying to generate music like Mozart, Beethoven, or perhaps Lady Gaga? AI research organization OpenAI just released a demo of a new deep learning algorithm that can automatically generate original music using many different instruments and styles. “We’ve created Musenet, a deep neural network that can generate 4-minute musical compositions with 10 different instruments, and…Powered by Discourse, best viewed with JavaScript enabled"
3197,nvidia-clara-parabricks-pipelines-v3-5-accelerates-google-s-deepvariant-v1-0,"Originally published at:			https://developer.nvidia.com/blog/nvidia-clara-parabricks-pipelines-v3-5-accelerates-googles-deepvariant-v1-0/
NVIDIA recently released NVIDIA Clara Parabricks Pipelines version 3.5, adding a set of new features to the software suite that accelerates end-to-end genome sequencing analysis. With the release of v3.5, Clara Parabricks Pipelines now provides acceleration to Google’s DeepVariant 1.0, in addition to a suite of existing DNA and RNA tools. The addition of DeepVariant…Powered by Discourse, best viewed with JavaScript enabled"
3198,from-federated-learning-to-embedded-ai-nvidia-clara-brings-ai-to-the-edge-for-developers,"Originally published at:			https://developer.nvidia.com/blog/nvidia-clara-brings-ai-to-the-edge-for-developers/
At RSNA 2019 NVIDIA announced updates to the Clara Application Framework that takes healthcare AI to the edge.Powered by Discourse, best viewed with JavaScript enabled"
3199,powering-video-conferencing-and-productivity-tools-with-nvidia-grid-vpc,"Originally published at:			https://developer.nvidia.com/blog/powering-video-conferencing-and-productivity-tools-with-nvidia-grid-vpc/
As 2020 progresses, remote work solutions have become the new normal for many. Organizations are looking to proven solutions like virtual desktop infrastructure (VDI) to enable their teams to securely work from anywhere. However, the latest productivity and video conferencing applications require more powerful desktops to ensure a good user experience. With the NVIDIA GRID…Powered by Discourse, best viewed with JavaScript enabled"
3200,ray-tracing-from-the-1980-s-to-today-an-interview-with-morgan-mcguire-nvidia,"Originally published at:			Ray Tracing From the 1980’s to Today An Interview with Morgan McGuire, NVIDIA | NVIDIA Technical Blog
Morgan McGuire, a member of the NVIDIA research team, walks us through the history of shaders, rasterization, and finally, ray tracing. Q: The games industry has been working on developing a ray tracing solution for decades. Can you give us an overview of the history? A:  It’s really interesting to look back at the late…Powered by Discourse, best viewed with JavaScript enabled"
3201,developer-spotlight-creating-photorealistic-cgi-environments,"Originally published at:			https://developer.nvidia.com/blog/artist-spotlight-creating-photorealistic-cgi-environments-in-real-time/
Get to know Rense de Boer, a technical art director from Sweden, who is not only pushing the envelope of photo-real CGI environments, but he’s doing it all in a real-time engine! Over the last few years de Bower has been focused on advancing real-time graphics by expanding beyond current workflows, and experimenting with cutting-edge…Powered by Discourse, best viewed with JavaScript enabled"
3202,preparing-models-for-object-detection-with-real-and-synthetic-data-and-the-nvidia-transfer-learning-toolkit,"Originally published at:			https://developer.nvidia.com/blog/preparing-models-for-object-detection-with-real-and-synthetic-data-and-tlt/
The long, cumbersome slog of data procurement has been slowing down innovation in AI, especially in computer vision, which relies on labeled images and video for training. But now you can jumpstart your machine learning process by quickly generating synthetic data using AI.Reverie. With the AI.Reverie synthetic data platform, you can create the exact training…Author here. I’m glad I had the opportunity to trial NVIDIA’s latest TLT package to see what it could do in combination with the synthetic data we create at AI.Reverie. I’m still amazed at how easy it was to optimize our model sizes and inference speeds to be hundreds of times better! TLT is definitely a handy tool to have at your disposal if you are deploying neural nets to production or on edge.If you have any questions or comments, please let us know.When I run this in jupyter notebookdownload(‘s3://rareplanes-public/real/tarballs/metadata_annotations.tar.gz’,
‘metadata_annotations’, 9)I get this errorBStarting download
fatal error: Unable to locate credentials
Extracting…
pv: metadata_annotations.tar.gz: No such file or directorygzip: stdin: unexpected end of file
tar: Child returned status 1
tar: Error is not recoverable: exiting now
Removing compressed file.
rm: cannot remove ‘data/real/tarballs/metadata_annotations.tar.gz’: No such file or directoryI had to get AWS credential setup to allow me to download imagesIm running the TLT3  jupyter notebook and I have got hung up because it cant find the path to “tlt_pretrained_detectnet_v2_vresnet18/resnet18.hdf5”here is a video that explains it better@adventuredaisy This is interesting, could you try runningThis let’s you shell into the docker container. At this point you can navigate the file system to see if everything is mounting properly. e.g.Let me know if the file is accessible through the docker container then we can figure out which direction to take our debugging.Here is a video of me running the suggestions from your post
Thank youOK looks like this is working now!tlt detectnet_v2 train --key tlt --gpu_index 0 
-e /workspace/tlt-experiments/specs/detectnet_v2_train_resnet18_kitti_real.txt 
-r /workspace/tlt-experiments/detectnet_v2_outputs/resnet18_real_amp16 
-n resnet18_real_amp16 
#–use_amp > out_resnet18_real_amp16.logEthier you guys got it of I didn’t fat finger something this time around on my end.Unable to use
“#–use_amp > out_resnet18_real_amp16.log”
option thoughI ran through the jupyter notebook.
Its quite on par with TLT3 jupyter notebooks from nvidia.
Nice work.Only thing I came across that my be issue is when I run this:convert_split(‘kitti_synthetic_train’)It seems like it cant find a pictureMatplotlib created a temporary config/cache directory at /tmp/matplotlib-afk40id4 because the default path (/.config/matplotlib) is not a writable directory; it is highly recommended to set the MPLCONFIGDIR environment variable to a writable directory, in particular to speed up the import of Matplotlib and to better support multiprocessing.
Using TensorFlow backend.
WARNING:tensorflow:Deprecation warnings have been disabled. Set TF_ENABLE_DEPRECATION_WARNINGS=1 to re-enable them.
Using TensorFlow backend.
2021-06-21 12:28:10,764 - iva.detectnet_v2.dataio.build_converter - INFO - Instantiating a kitti converter
2021-06-21 12:28:16,463 - iva.detectnet_v2.dataio.kitti_converter_lib - INFO - Num images in
Train: 44550	Val: 450
2021-06-21 12:28:16,463 - iva.detectnet_v2.dataio.kitti_converter_lib - INFO - Validation data in partition 0. Hence, while choosing the validationset during training choose validation_fold 0.
2021-06-21 12:28:16,500 - iva.detectnet_v2.dataio.dataset_converter_lib - INFO - Writing partition 0, shard 0
WARNING:tensorflow:From /home/vpraveen/.cache/dazel/_dazel_vpraveen/216c8b41e526c3295d3b802489ac2034/execroot/ai_infra/bazel-out/k8-fastbuild/bin/magnet/packages/iva/build_wheel.runfiles/ai_infra/iva/detectnet_v2/dataio/dataset_converter_lib.py:142: The name tf.python_io.TFRecordWriter is deprecated. Please use tf.io.TFRecordWriter instead.2021-06-21 12:28:16,500 - tensorflow - WARNING - From /home/vpraveen/.cache/dazel/_dazel_vpraveen/216c8b41e526c3295d3b802489ac2034/execroot/ai_infra/bazel-out/k8-fastbuild/bin/magnet/packages/iva/build_wheel.runfiles/ai_infra/iva/detectnet_v2/dataio/dataset_converter_lib.py:142: The name tf.python_io.TFRecordWriter is deprecated. Please use tf.io.TFRecordWriter instead./usr/local/lib/python3.6/dist-packages/iva/detectnet_v2/dataio/kitti_converter_lib.py:273: VisibleDeprecationWarning: Reading unicode strings without specifying the encoding argument is deprecated. Set the encoding, use None for the system default.
2021-06-21 12:28:19,905 - iva.detectnet_v2.dataio.dataset_converter_lib - INFO - Writing partition 0, shard 1
2021-06-21 12:28:22,604 - iva.detectnet_v2.dataio.dataset_converter_lib - INFO - Writing partition 0, shard 2
2021-06-21 12:28:25,107 - iva.detectnet_v2.dataio.dataset_converter_lib - INFO - Writing partition 0, shard 3
2021-06-21 12:28:27,343 - iva.detectnet_v2.dataio.dataset_converter_lib - INFO - Writing partition 0, shard 4
Traceback (most recent call last):
File “/home/vpraveen/.cache/dazel/_dazel_vpraveen/216c8b41e526c3295d3b802489ac2034/execroot/ai_infra/bazel-out/k8-fastbuild/bin/magnet/packages/iva/build_wheel.runfiles/ai_infra/iva/detectnet_v2/scripts/dataset_convert.py”, line 90, in 
File “/home/vpraveen/.cache/dazel/_dazel_vpraveen/216c8b41e526c3295d3b802489ac2034/execroot/ai_infra/bazel-out/k8-fastbuild/bin/magnet/packages/iva/build_wheel.runfiles/ai_infra/iva/detectnet_v2/scripts/dataset_convert.py”, line 86, in main
File “/home/vpraveen/.cache/dazel/_dazel_vpraveen/216c8b41e526c3295d3b802489ac2034/execroot/ai_infra/bazel-out/k8-fastbuild/bin/magnet/packages/iva/build_wheel.runfiles/ai_infra/iva/detectnet_v2/dataio/dataset_converter_lib.py”, line 71, in convert
File “/home/vpraveen/.cache/dazel/_dazel_vpraveen/216c8b41e526c3295d3b802489ac2034/execroot/ai_infra/bazel-out/k8-fastbuild/bin/magnet/packages/iva/build_wheel.runfiles/ai_infra/iva/detectnet_v2/dataio/dataset_converter_lib.py”, line 105, in _write_partitions
File “/home/vpraveen/.cache/dazel/_dazel_vpraveen/216c8b41e526c3295d3b802489ac2034/execroot/ai_infra/bazel-out/k8-fastbuild/bin/magnet/packages/iva/build_wheel.runfiles/ai_infra/iva/detectnet_v2/dataio/dataset_converter_lib.py”, line 146, in _write_shard
File “/home/vpraveen/.cache/dazel/_dazel_vpraveen/216c8b41e526c3295d3b802489ac2034/execroot/ai_infra/bazel-out/k8-fastbuild/bin/magnet/packages/iva/build_wheel.runfiles/ai_infra/iva/detectnet_v2/dataio/kitti_converter_lib.py”, line 173, in _create_example_proto
File “/home/vpraveen/.cache/dazel/_dazel_vpraveen/216c8b41e526c3295d3b802489ac2034/execroot/ai_infra/bazel-out/k8-fastbuild/bin/magnet/packages/iva/build_wheel.runfiles/ai_infra/iva/detectnet_v2/dataio/kitti_converter_lib.py”, line 212, in _example_proto
File “/home/vpraveen/.cache/dazel/_dazel_vpraveen/216c8b41e526c3295d3b802489ac2034/execroot/ai_infra/bazel-out/k8-fastbuild/bin/magnet/packages/iva/build_wheel.runfiles/ai_infra/iva/detectnet_v2/dataio/kitti_converter_lib.py”, line 206, in _get_image_size
File “/usr/local/lib/python3.6/dist-packages/PIL/Image.py”, line 2766, in open
fp = builtins.open(filename, “rb”)
FileNotFoundError: [Errno 2] No such file or directory: ‘/workspace/tlt-experiments/data/kitti/synthetic_train/images/Miami_Airport_0_0_200_25266.png’
Traceback (most recent call last):
File “/usr/local/bin/detectnet_v2”, line 8, in 
sys.exit(main())
File “/home/vpraveen/.cache/dazel/_dazel_vpraveen/216c8b41e526c3295d3b802489ac2034/execroot/ai_infra/bazel-out/k8-fastbuild/bin/magnet/packages/iva/build_wheel.runfiles/ai_infra/iva/detectnet_v2/entrypoint/detectnet_v2.py”, line 12, in main
File “/home/vpraveen/.cache/dazel/_dazel_vpraveen/216c8b41e526c3295d3b802489ac2034/execroot/ai_infra/bazel-out/k8-fastbuild/bin/magnet/packages/iva/build_wheel.runfiles/ai_infra/iva/common/entrypoint/entrypoint.py”, line 296, in launch_job
AssertionError: Process run failed.
2021-06-21 05:28:30,000 [INFO] tlt.components.docker_handler.docker_handler: Stopping container.@adventuredaisy-use_amp would only work on GPUs that have support for mixed precision training. Can I ask which GPU you are trying this on?As for the image file, can you confirm that it was downloaded to your local disk. e.g., I saved it here: ls -altr ~/Data/rareplanes-public/kitti/synthetic_train/images/Miami_Airport_0_0_200_25266.png If it’s not there, you might want to try redownloading from s3. If it is, we need to verify that the file is accessible in the docker container.Create a shell into the docker container like before: tlt detectnet_v2 run ""/bin/sh""Then we check it the file is available through the docker container: ls -altr tlt-experiments/data/kitti/synthetic_train/images/Miami_Airport_0_0_200_25266.pngIf it’s not there, try and cd into the folders under tlt-experiments and see if any of the images are making it through at all.For those interested here is a video walk through of the TLT3 jupyter notebook:
Preparing Models for Object Detection with Real and Synthetic Data and the NVIDIA Transfer Learning ToolkitPowered by Discourse, best viewed with JavaScript enabled"
3203,new-software-enhancements-for-intelligent-video-analytics-and-iot,"Originally published at:			https://developer.nvidia.com/blog/new-software-enhancements-for-iva-iot/
AI developers, data scientists and companies building intelligent video analytics apps face significant challenges in creating and deploying highly accurate AI. Some of the key issues include dataset collection and labeling, achieving high accuracy with available dataset, deploying on legacy infrastructure, and scalability of apps and services.   With billions of cameras and sensors deployed across…Powered by Discourse, best viewed with JavaScript enabled"
3204,new-app-automatically-identifies-your-best-photos,"Originally published at:			https://developer.nvidia.com/blog/new-app-automatically-identifies-your-best-photos/
Finding all the best pictures on your phone just became a bit easier. EyeEm, a photo sharing startup, released a new app called The Roll which relies on NVIDIA GPUs and deep learning to automatically organize your photos. Using the company’s AI-powered image analysis technology, each photo in your collection is assigned an aesthetic quality…Powered by Discourse, best viewed with JavaScript enabled"
3205,gtc-2020-creating-ai-workgroups-within-the-enterprise-new-best-practices-for-developers-and-sys-admins,"GTC 2020 S21695
Presenters: Michael Balint,NVIDIA; Markus Weber, NVIDIA
Abstract
Multi-GPU systems have proven to be excellent resources for deep learning and machine learning teams within small and large organizations. What are the best practices for extending AI compute power to these teams without needing to build and manage a data center? We’ll start with practical tips on how multiple users can share a single system (such as an NVIDIA DGX Station) and scale to more advanced concepts of multi-node, multi-user model training and deployment. Learn how teams building powerful AI applications might not need to own servers or depend on data center access; how to leverage best practices involving containers, orchestration, monitoring, and scheduling tools; see demos of how to set up your AI workgroup with ease; and learn best practices for AI developer productivity.Watch this session
Join in the conversation below.Powered by Discourse, best viewed with JavaScript enabled"
3206,building-an-nvidia-pure-sonic-image,"Originally published at:			https://developer.nvidia.com/blog/building-pure-sonic-image/
Pure SONiC is the version of SONiC that eliminates vendor dependence. Being community developed, publicly available, and 100% open source enables you to build a Pure SONiC image that is in sync with the desired community branch. It means that every line of code of SONiC and NVIDIA implementation of SAI (switch abstraction interface) is…When can support Debian 10?201911 is unstable for python 3.Sonic is try to use py3 totally.Hello Lanhsin, thanks for your query.Debian 10 is supported on the 2020-12 branch.you could build your own tested image based on 5bdbfcfb2647cf1db59401e82807c748838ffee4 from GitHub - sonic-net/sonic-buildimage at 202012Please reach out to me for any queries.Thanks for your quickly response.It’s nice to hear the news.Maybe I need to find the time to talk to my colleague when to switch to 202012.Thanks again.Hello Yuval,
I am trying to build SONIC on debian 9, arm64 machine for VS . The build(make terget/sonic-vs.img.gz) fails looking for some amd.deb files and when I resume the build it continues for some more time and fails again. I wanted to know if you have made any changes while building SONIC for arm64 in any of the Makefile or the source code. Any help or pointers are much appreciated.
Thanks in advance.Hello prashantkalikotay1995] , Let’s continue this discussion via email and I’ll update this thread when we get to a conclusionHello Yuval, I encountered a similar error to prashantkalikotay1995. Any resolutions ? I am trying to build an image with PINSFor anyone who’s curious. I was able to successfully build the image on 2nd attempt. The following features were enabled specifically by editing the rules/config fileZTP, RESTAPI and P4RT. Using the latest master branchThe only thing I did differently was that I run an apt-get update before my 2nd attempt. I did not use the scripts in the blog but definitely follow the steps.If you are trying to build it, make sure you specify your target similar to this target/sonic-mellanox.bin in your make commandPowered by Discourse, best viewed with JavaScript enabled"
3207,elevate-game-content-creation-and-collaboration-with-nvidia-omniverse,"Originally published at:			https://developer.nvidia.com/blog/elevate-game-content-creation-and-collaboration-with-nvidia-omniverse/
With everyone shifting to a remote work environment, game development and professional visualization teams around the world need a solution for real-time collaboration and more efficient workflows.Powered by Discourse, best viewed with JavaScript enabled"
3208,cuda-12-1-supports-large-kernel-parameters,"Originally published at:			https://developer.nvidia.com/blog/cuda-12-1-supports-large-kernel-parameters/
CUDA 12.1 offers you the option of passing up to 32,764 bytes using kernel parameters, which can be exploited to simplify applications as well as gain performance improvements.Powered by Discourse, best viewed with JavaScript enabled"
3209,nvidia-research-at-iccv-generating-new-city-road-layouts-with-ai,"Originally published at:			NVIDIA Research at ICCV: Generating New City Road Layouts with AI | NVIDIA Technical Blog
Generating road layout for different city styles is a time-consuming task. Artists need to manually create the road geometry and adjust its parameters like width and curvature by comparing it with real-world maps. But, what if there was a better way to do this? At the International Conference on Computer Vision in Seoul, Korea, NVIDIA…Powered by Discourse, best viewed with JavaScript enabled"
3210,share-your-science-fighting-ebola-with-supercomputer-simulations,"Originally published at:			Share Your Science: Fighting Ebola with Supercomputer Simulations | NVIDIA Technical Blog
Thomas Cheatham, professor of Medicinal Chemistry and the director of research computing at University of Utah shares how they’re using the GPU-accelerated Blue Waters supercomputer and NVLink to compute the interactions of atoms that can lead to drug design and materials design.    “The GPUs have been really helpful, because we’ve optimized our codes (AMBER, a…Powered by Discourse, best viewed with JavaScript enabled"
3211,gtc-2020-building-oran-based-high-performance-5g-ran-systems-with-nvidia-gpus-and-mellanox-nic,"GTC 2020 S22009
Presenters: Elena Agostini,NVIDIA; Joe Boccuzzi,NVIDIA
Abstract
NVIDIA Aerial is a set of SDKs that enables GPU-accelerated, software-defined 5G wireless RANs. Today, NVIDIA Aerial provides two critical SDKs: cuVNF (optimized Input/Output leveraged with GPUDirect RDMA) and cuBB (fully-offloaded 5G Signal Processing pipeline). These SDKs can be combined to implement a software-accelerated physical layer on the centralized radio controllers (base band unit) that is able to dialog, by means of a Fronthaul I/O interface, with a set of radio heads to send, receive, and process 5G packets using GPUs. We’ll show our implementation of the Fronthaul I/O interface to enable an ORAN-compliant (O-RAN Alliance) dialog with a radio unit, giving an overview of the most challenging issues we faced in differentiating between hardware- and software-accelerated features.Watch this session
Join in the conversation below.Hello, great content. However how can I find the specs of the vBBU that you have built? The graph with the performance does not tell me if thoses specs are the ones you used to crate the vBBU. Let´s say that I which to have a 20MHz spectrum with 5G NR, how many GPUs would it require? what about 100 MHz?Powered by Discourse, best viewed with JavaScript enabled"
3212,creating-virtual-reality-experiences-for-the-nfl-with-gpus,"Originally published at:			https://developer.nvidia.com/blog/creating-virtual-reality-experiences-for-the-nfl-with-gpus/
The Tampa Bay Buccaneers unveiled a state-of-the-art experience to provide fans with the ability to virtually sample the new gameday enhancements that will debut beginning with the team’s 2016 season. While the new Raymond James Stadium is under construction, current and prospective ticket holders can use a virtual reality headset to experience the new stadium…Powered by Discourse, best viewed with JavaScript enabled"
3213,gtc-2020-5g-meets-deep-learning-ray-tracing-and-gpus,"GTC 2020 S21693
Presenters: Adriana Flores,NVIDIA; Nima Mohammad Pour Nejatian, NVIDIA; Ahmed Alkhateeb, Arizona State University
Abstract
Applying deep learning in 5G can potentially enable new functionalities and overcome the existing system’s limitations. After a short review of DL-based use cases in wireless physical layer, we’ll present a novel neural network architecture called the auto-precoder, a GPU-accelerated DL model that jointly senses the millimeter wave (mmWave) MIMO 5G channel and designs the hybrid precoding matrices with only a few training pilots. The proposed DL model does that by leveraging prior observations of the channel. The lack of accurate training datasets is a key challenge for evaluating and using DL models in wireless systems. To overcome this, we’ll show how GPU-accelerated ray tracing algorithms (based on REMCOM technology) can be used to generate accurate training data. We’ll also demonstrate the accuracy of the auto-precoder model across different scenarios and benchmark its performance on CPUs and GPUs.Watch this session
Join in the conversation below.Powered by Discourse, best viewed with JavaScript enabled"
3214,diffusion-3d-model-on-consumer-level-hardware,"When might we see a path from stable diffusion into Get3d (or similar) possible to run on consumer grade hardware? Or are we going to be in ‘8 A100 class GPUs’ for the forseeable future? If the latter, will nvidia or a partner be hosting a cloud-based service to make this tech available to businesses not outfitted with that level of hardware?Hi, thanks for the questions! We can run inference for GET3D on consumer-grade GPUs if you’re following the instructions from here: GitHub - nv-tlabs/GET3D, which means you can run GET3D on any cloud service following our licence GET3D/LICENSE.txt at master · nv-tlabs/GET3D · GitHubThis topic was automatically closed 14 days after the last reply. New replies are no longer allowed.Powered by Discourse, best viewed with JavaScript enabled"
3215,optimizing-and-serving-models-with-nvidia-tensorrt-and-nvidia-triton,"Originally published at:			https://developer.nvidia.com/blog/optimizing-and-serving-models-with-nvidia-tensorrt-and-nvidia-triton/
Learn how to optimize models from TensorFlow, PyTorch, or any other framework and then deploy/serve them at scale with NVIDIA TensorRT and NVIDIA TritonI hope this blog helps readers get an intuition about how to use TensorRT and Triton in a pipeline. Feel free to reach out for any follow-up questions!Powered by Discourse, best viewed with JavaScript enabled"
3216,empowering-smart-hospitals-with-nvidia-clara-guardian-from-ngc-and-nvidia-fleet-command,"Originally published at:			https://developer.nvidia.com/blog/empowering-smart-hospitals-with-nvidia-clara-guardian-from-ngc-and-nvidia-fleet-command/
Hospitals today are seeking to overhaul their existing digital infrastructure to improve their internal processes, deliver better patient care, and reduce operational expenses. Such a transition is required if hospitals are to cope with the needs of a burgeoning human population, accumulation of medical patient data, and a pandemic. The goal is not only to…Powered by Discourse, best viewed with JavaScript enabled"
3217,jetson-project-of-the-month-spaghetti-detective-is-on-the-case,"Originally published at:			https://developer.nvidia.com/blog/jetson-project-of-the-month-spaghetti-detective/
Meet The Spaghetti Detective, an AI-based failure-detection tool for 3D printer remote management and monitoring.Powered by Discourse, best viewed with JavaScript enabled"
3218,how-we-achieved-record-finance-benchmark-performance-on-tesla-k80,"Originally published at:			https://developer.nvidia.com/blog/how-we-achieved-record-finance-benchmark-performance-tesla-k80/
STAC Research develops financial benchmarks in partnership with leading banks and software or hardware vendors. The STAC-A2 suite of benchmarks aims to represent the standard risk analysis workload that banks and insurance companies use to measure exposure on the financial markets. Earlier this year we published a Parallel Forall post on Monte Carlo simulation for the pricing of…Powered by Discourse, best viewed with JavaScript enabled"
3219,ai-robotics-system-can-recognize-objects-from-touch,"Originally published at:			https://developer.nvidia.com/blog/ai-robotics-system-can-recognize-objects-from-touch/
Drawing inspiration from how humans interact with objects through touch, University of California, Berkeley researchers developed a deep learning-based perception framework that can recognize over 98 different objects from touch. According to the team, this is the first project that addresses this type of robot-object interaction using only touch at a large-scale. “When we see a soft toy,…Powered by Discourse, best viewed with JavaScript enabled"
3220,facebook-self-supervised-ai-outperforms-state-of-the-art-computer-vision-models,"Originally published at:			Facebook Self-Supervised AI Outperforms State-of-the-Art Computer Vision Models | NVIDIA Technical Blog
Facebook AI researchers this week announced SEER, a self-supervised model that surpasses the best self-supervised systems, and also outperforms supervised models on tasks including image classification, object detection, and segmentation.  Combining RegNet architectures with the SwAV online clustering approach, SEER is a billion-parameter model pretrained on a billion random images. Instead of relying on labeled…Powered by Discourse, best viewed with JavaScript enabled"
3221,yelp-is-using-gpus-to-classify-business-photos,"Originally published at:			Yelp is Using GPUs to Classify Business Photos | NVIDIA Technical Blog
Yelp, a user-generated restaurant and business review website, has began to build an image classification system to help surface the best photos taken at different locations. Yelp hosts tens of millions of photos uploaded by their users worldwide. The wide variety of these photos provides a rich window into local businesses. One way they’re trying…Powered by Discourse, best viewed with JavaScript enabled"
3222,labellio-scalable-cloud-architecture-for-efficient-multi-gpu-deep-learning,"Originally published at:			https://developer.nvidia.com/blog/labellio-scalable-cloud-architecture-efficient-multi-gpu-deep-learning/
Labellio is the world’s easiest deep learning web service for computer vision. It aims to provide a deep learning environment for image data where non-experts in deep learning can experiment with their ideas for image classification applications. Watch our video embedded here to see how easy it is. The challenges in deep learning today are not…Awesome!!! Exactly what I was looking for :) Why buy my own GPU cores when I can use a cloud service which scales to my needs... You have plans for a more general solution/service to enable/leverage DIGITS sdk platform?Powered by Discourse, best viewed with JavaScript enabled"
3223,docker-compatibility-with-singularity-for-hpc,"Originally published at:			Docker Compatibility with Singularity for HPC | NVIDIA Technical Blog
Bare-metal installations of HPC applications on a shared system require system administrators to build environment modules for 100s of applications which is complicated, high maintenance, and time consuming. Furthermore, upgrading an application to the latest revision requires carefully updating the environment modules. Networks of dependencies often break during new installs while upgrades unintentionally crash other…Is it possible that the Nvidia driver of the host and the container be different?Hi Eshan,    Userspace NVIDIA driver components from the host are dynamically mounted in the container at runtime, which is to say none are present in the container image itself and so an exact match isn't required. Applications within each container are built against a particular CUDA release which does impose minimum driver version requirements. For example an application built against the CUDA/9.0 toolchain requires a host CUDA driver greater than or equal to R384. Host driver version requirements are detailed in the images NGC documentation.Powered by Discourse, best viewed with JavaScript enabled"
3224,maintaining-container-security-as-the-core-of-ngc-with-anchore-enterprise,"Originally published at:			https://developer.nvidia.com/blog/maintaining-container-security-as-the-core-of-ngc-with-anchore-enterprise/
Containers have quickly gained strong adoption in the software development and deployment process and has truly enabled us to manage software complexity. It is not surprising that, by a recent Gartner report, more than 70% of global organizations will be running containerized applications in production by 2023. That’s up from less than 20% in 2019.…Understanding the security posture of NGC containers, and what scanning goes into ensuring containers on NGC is a regular ask from numerous Enterprise customers.Transparent scanning policies and security reports of containers from NGC goes a long way in building trust and confidence in the security legwork already done for Enterprise users - so they can ultimately worry less about the logistics of the software that accelerates their workflows and focus instead on building and solving the great challenges they work on.Powered by Discourse, best viewed with JavaScript enabled"
3225,top-data-center-energy-efficiency-sessions-at-nvidia-gtc-2023,"Originally published at:			https://developer.nvidia.com/blog/top-data-center-energy-efficiency-sessions-at-nvidia-gtc-2023/
Learn how accelerated computing can reduce your total carbon footprint and support your organization’s energy efficiency efforts.Powered by Discourse, best viewed with JavaScript enabled"
3226,share-your-science-advanced-retina-assessment-and-diagnostic-services,"Originally published at:			Share Your Science: Advanced Retina Assessment and Diagnostic Services | NVIDIA Technical Blog
Nicholas Bedworth, founder of SocialEyes, shares how they are developing cost-effective mobile AI diagnostic tools that can provide critically-needed medical services to low-resource societies. “Being able to diagnose people near to where they live – at home, at work, in the community, pharmacies – we can catch these diseases very early before they develop into…Powered by Discourse, best viewed with JavaScript enabled"
3227,render-millions-of-direct-lights-in-real-time-with-rtx-direct-illumination-rtxdi,"Originally published at:			Render Millions of Direct Lights in Real-Time With RTX Direct Illumination (RTXDI) | NVIDIA Technical Blog
Until today, artists had performance constraints that artificially limited lighting complexity; real-time renderers simply could not support more than a handful of dynamic lights. For years, NVIDIA sought methods to remove this barrier and enable real-time rendering of arbitrarily complex lighting. With NVIDIA’s Marbles at Night demo shown at the launch of the Ampere GeForce…Powered by Discourse, best viewed with JavaScript enabled"
3228,gtc-2020-practical-guidelines-for-optimizing-and-accurate-sizing-of-medical-imaging-workflows,"GTC 2020 S21997
Presenters: Anas Abidin ,NVIDIA ; Christopher Bridge,Center for Clinical Data Science
Abstract
Deep dive into the various considerations that could be critical to substantially improve existing medical imaging workflows. We’ll motivate our discussions through a detailed analysis of two common medical imaging workflows: 2D classification and 3D segmentation. Our results suggest that controlling for aspects such as i/o format, selection of hyper-parameters, and mixed-precision training could all be key to maximizing hardware performance and reducing turnaround time for experiments. Furthermore, we aim to discuss other strategies, such as learning-rate warmup and scaling, effect of optimizers, scaling to multiple GPUs, and their potential effects on training throughput. We were able to show a 5x improvement for the 2D classification task through our ablation experiments. We’ll use these results suggest best practices learned, and also build a sizing calculator for providing quantitative insights for hardware investments.Watch this session
Join in the conversation below.Powered by Discourse, best viewed with JavaScript enabled"
3229,how-dsc-will-enable-higher-fidelity-in-vr-experiences,"Originally published at:			https://developer.nvidia.com/blog/dsc-higher-fidelity-vr/
By Arpit Agrawal HMD vendors now have access to Digital Stream Compression (DSC) through the VRWorks DirectMode API available in VRWorks Graphics SDK v3.4. Let’s explore this feature and how it enabled higher resolution VR experiences. What is DSC? As described in VESA’s white paper: Display Stream Compression (DSC) was developed as an industry-wide standard…Hello, I downloaded a VRworks_3.5.2_graphics_public SDK, but it did not mention the API that can modify VESA DSC configuration. So, where can I find the API to modify VESA DSC PPS?  For example, modify slice_width, Color_Format and so on.Powered by Discourse, best viewed with JavaScript enabled"
3230,insider-s-guide-to-gtc-ar-vr-rendering-simulation-and-video-streaming,"Originally published at:			https://developer.nvidia.com/blog/insiders-guide-to-gtc-ar-vr-rendering-simulation-and-video-streaming/
Notable sessions in the making of Green Planet AR, animating realistic digital humans, building a power industry digital twin, and making virtual production more accessible.Powered by Discourse, best viewed with JavaScript enabled"
3231,gpus-help-researchers-solve-mystery-of-gas-giant-formation,"Originally published at:			GPUs Help Researchers Solve Mystery of Gas Giant Formation | NVIDIA Technical Blog
Ever wondered how the planets like Jupiter formed? A  team of researchers used a cluster of Tesla GPUs to make a breakthrough discovery, finally explaining how gas giants are formed. United by a common expertise in planet-disk interactions, four researchers from three countries recently set out to close this gap between theory and observation. They…Powered by Discourse, best viewed with JavaScript enabled"
3232,gpu-accelerated-spark-xgboost-a-major-milestone-on-the-road-to-large-scale-ai,"Originally published at:			GPU-Accelerated Spark XGBoost - A Major Milestone on the Road to Large-Scale AI | NVIDIA Technical Blog
Machine learning at scale can deliver powerful, predictive capabilities to millions of users, but it hinges on overcoming two key challenges across on-prem or cloud infrastructure – speeding up pre-processing of massive volumes of data and accelerating compute intensive model training. To tackle these challenges, we started with the popular gradient boosting library XGBoost as…Powered by Discourse, best viewed with JavaScript enabled"
3233,how-to-query-device-properties-and-handle-errors-in-cuda-c-c,"Originally published at:			https://developer.nvidia.com/blog/how-query-device-properties-and-handle-errors-cuda-cc/
In this third post of the CUDA C/C++ series we discuss various characteristics of the wide range of CUDA-capable GPUs, how to query device properties from within a CUDA C/C++ program, and how to handle errors. Querying Device Properties In our last post, about performance metrics, we discussed how to compute the theoretical peak bandwidth of a…Nice explanation Mark! Thanks for share with us.I would like to explain how to deal with errors and free memory allocated device.A common practice in many CUDA examples is a macro that check error and call exit(-1); in that case. But... what about free allocated memory?There is some method to free all allocated memory from current application before exit?This will be a nice topic for a  Part-II of this explanation.Thanks!Thanks for reading! The CUDA examples are not meant to be professional applications.  A professional application would of course correctly manage memory and free allocations at exit, rather than just exiting directly. But there are many ways to manage memory and many opinions on best practices. Properly cleaning up your application is a standard programming practice and is not GPU- or parallel programming-specific, and therefore I consider it outside the scope of this article / blog.FYI: helpful.Powered by Discourse, best viewed with JavaScript enabled"
3234,nvidia-and-snowflake-collaboration-boosts-data-cloud-ai-capabilities,"Originally published at:			NVIDIA and Snowflake Collaboration Boosts Data Cloud AI Capabilities | NVIDIA Technical Blog
NVIDIA and Snowflake announced a new partnership bringing accelerated computing to the Data Cloud with the new Snowpark Container Services (private preview), a runtime for developers to manage and deploy containerized workloads. By integrating the capabilities of GPUs and AI into the Snowflake platform, customers can enhance ML performance and efficiently fine-tune LLMs. They achieve…As an enthusiast in LLM, I’d like to suggest a hands-on demo with the Data model for training while using Nvidia’s 90-day trial in this environment to help us understand how to build Data Cloud AI using the environment capabilities.Powered by Discourse, best viewed with JavaScript enabled"
3235,entropy-based-methods-for-word-level-asr-confidence-estimation,"Originally published at:			https://developer.nvidia.com/blog/entropy-based-methods-for-word-level-asr-confidence-estimation/
Learn how to achieve fast, simple word-level ASR confidence estimation using entropy-based methods.Powered by Discourse, best viewed with JavaScript enabled"
3236,top-cybersecurity-sessions-at-nvidia-gtc-2023,"Originally published at:			Cybersecurity Conference Sessions | GTC 2023 Spring | NVIDIA
Learn how AI is improving your cybersecurity to detect threats faster.Powered by Discourse, best viewed with JavaScript enabled"
3237,building-real-time-dermatology-classification-with-nvidia-clara-agx,"Originally published at:			https://developer.nvidia.com/blog/building-real-time-dermatology-classification-with-nvidia-clara-agx/
The most commonly diagnosed cancer in the US today is skin cancer. There are three main variants: melanoma, basal cell carcinoma (BCC), and squamous cell carcinoma (SCC). Though melanoma only accounts for roughly 1% of all skin cancers, it is the most fatal, metastasizing rapidly without early detection and treatment. This makes early detection critical,…For those wishing to run this container on the Clara AGX/Holoscan dGPU, here is a workaround:First - run the Dermatology container with the added option: -v /media/m2:/m2We’ll need to copy the following folders to the m2 drive:cp models/ /m2/ -rcp source/ /m2/ -rNow pull the AGX PyTorch Container:docker pull nvcr.io/nvidia/clara-agx/agx-pytorch:21.05-1.7-py3export DISPLAY=:0
xhost +
mkdir /home/nvidia/resultssudo docker run --gpus all -it --rm -e DISPLAY=$DISPLAY --device /dev/video0:/dev/video0 -v /media/m2:/m2 -v /tmp/.X11-unix:/tmp/.X11-unix nvcr.io/nvidia/clara-agx/agx-pytorch:21.05-1.7-py3apt-get update
apt-get install python3-setuptools ffmpeg libsm6 libxext6 libhdf5-dev opencv-python -y
pip3 install --upgrade pip
pip3 install efficientnet-pytorchcp /m2/models/ . -r
cp /m2/source/ . -r
cd source/
python3 demo.pyPowered by Discourse, best viewed with JavaScript enabled"
3238,gpu-accelerated-biomolecular-simulations-and-ai-powered-chemistry,"Originally published at:			GPU-Accelerated Biomolecular Simulations and AI-Powered Chemistry | NVIDIA Technical Blog
Molecular dynamics simulations and AI-powered computational chemistry are playing a key role in the fight against COVID-19, providing atomic-scale insights to viral mechanisms including virus-to-cell fusion, viral protein function, and ultimately possible therapeutics. Molecular dynamics simulations offer accurate approximations of real molecular behavior providing a tool to better understand how drugs might bind to a…Powered by Discourse, best viewed with JavaScript enabled"
3239,optimizing-recurrent-neural-networks-in-cudnn-5,"Originally published at:			https://developer.nvidia.com/blog/optimizing-recurrent-neural-networks-cudnn-5/
Figure 1: cuDNN 5 + Torch speedup vs. Torch-rnn implementation, M40, Intel® Xeon® Processor E5-2698 Network A: RNN size 2560, input size 2560, 1 layer, Seq length 200, batch size 64. Network B: RNN size 256, input size 64, 3 layers, batch size 64. Network C: RNN size 256, input size 256, 1 layer, batch…About optimization 2, shouldn't the hardware scheduler take care of that bit assuming that that the two sgemms have outputs in different locations?Also, just recently researchers managed to get batch normalization to work for recurrent nets, so I am wondering whether it would be possible to combine BN with the above optimized implementations?Probably not as is, but as a all of the optimizations above work on the linear parts, it might be more worth making specialized linear layer functions rather that recurrent ones. Then if those linear layer functions could be extended to use convolutions, it might enable the more efficient use of architectures such as neural GPUs.Also cuDNN currently lacks a broadcast matrix-vector multiplication operation such as the one that is surely used inside the BN functions. Another weakness of the above arrangement and in favor of having functions specifically for linear layers is that it would allow one to use activations such as steep sigmoid or PrRelu.> About optimization 2, shouldn't the hardware scheduler take care of that bit assuming that that the two sgemms have outputs in different locations?Without additional information it's very hard/impossible to prove that two calls can be executed in parallel. Different calls won't be executed concurrently unless the user allows them to be via streams.> Also, just recently researchers managed to get batch normalization to work for recurrent nets, so I am wondering whether it would be possible to combine BN with the above optimized implementations?Thanks for the link. This (and your other comments) are interesting directions. There are a lot of different ways you could modify RNNs, and covering them all while keeping memory usage low, performance high, and the API easy to use is tricky. The optimization strategies I've described in this blog post are all possible to implement in CUDA, and modifications to your favourite framework to allow some/all of them would hopefully lead to similar speedup for custom RNNs.Thanks Jeremy, great post. It never occurred to me to batch dot products together in this way.I'm confused. In this article's opener, it mentions the new native support for RNNs in cuDNN v5. Yet nowhere in the article and nowhere in the associated code samples in github do I find any use of the new tensor descriptors, nor RNN descriptors, nor any of the new RNN-related API calls. In fact, I don't see anything in your code that's demonstrating any of the new RNN features of cuDNN v5. Please correct me if I've missed something.Nice post Jeremy! I have a question about Step 1 Optimization 2 in which independent matrix multiplications within an LSTM are enqueued into different streams. This option is not available in cuDNN (v 5 as of now), correct?There are corner cases where only one stream is possible. Excluding them, cuDNN v5 should be using this optimization under-the-hood.But cuDNN documentation states that cudnnSetStream basically sets the one stream to execute all subsequent cuDNN calls with. Does cuDNN create and use implicitly-defined private streams for this purpose?That's correct. These streams will not start to execute before all prior work is completed on the stream set by cudnnSetStream, nor will work scheduled to that stream begin while the private streams are still executing. In other words, from the caller perspective, behaviour is the same as if there were no private streams.Very nice post. Thanks for sharing the optimizations.Is it possible to share the dimensions of each matrix in a single SGEMM call? I am assuming that out of 8 SGEMM calls, there are 2 groups of 4 SGEMMs, all SGEMMs in a group will have same input matrices dimensions. I am trying to understand the calculations and the how large matrices are. Thanks.Typically each of the eight matrices are square with side length equal to the hidden state size of the model. In the first layer the non-recurrent input may be a different dimension, in which case rectangular matrices need to be used for those four operations. The hidden state size and input size are user defined and typically lie in the 256-2048 range, though larger and smaller values are also sometimes used.Thanks. That makes sense to a large extent. I will look closely into more calculations to understand it completely.Also, is there any framework which takes advantage of these optimizations? I think the DNN frameworks need to be updated so that they can take advantage of the optimizations. Please let me know if there is a framework available with this implementation.It's ok, the article is about optimisation, not about the new tensor descriptors.Most major frameworks have ways to interact with the cuDNN RNN API (eg. Tensorflow, PyTorch, Caffe2 and more).Thanks for the great tutorial. I am trying to understand better what ""fusing element-wise operation"" means. Please let me know if the following is correct:Suppose I want to compute ReLU(A*B). Without fusing the pointwise operation, this means that I launch a GEMM kernel to compute A*B. Once the kernel is finished it will send the product C=A*B back to global memory. Then I will launch a kernel to compute ReLU(C). To do this I will need to go to fetch the matrix C in global memory, send it to the device, and then threshold all the entries of C. Obviously in this last step, all the time is spent in fetching the matrix C from global memory. The goal of fusing is to eliminate this unnecessary fetching time.In the ""fusing scenario"" we launch a single GEMM kernel, with simply an extra line at the end of the kernel code to threshold each entry of C once they become available available.Did I understand correctly what ""fusing element-wise operation"" means?ThanksThanks for the great tutorial. I am trying to understand better what ""fusing element-wise operation"" means. Please let me know if the following is correct:Suppose I want to compute ReLU(A*B). Without fusing the pointwise operation, this means that I launch a GEMM kernel to compute A*B. Once the kernel is finished it will send the product C=A*B back to global memory. Then I will launch a kernel to compute ReLU(C). To do this I will need to go to fetch the matrix C in global memory, send it to the shared memory, and then threshold all the entries of C. Obviously in this last step, all the time is spent in fetching the matrix C from global memory. The goal of fusing is to eliminate this unnecessary fetching time.In the ""fusing scenario"" we launch a single GEMM kernel, with simply an extra line at the end of the kernel code to threshold each entry of C once they become available available.Did I understand correctly what ""fusing element-wise operation"" means?Thanks> Did I understand correctly what ""fusing element-wise operation"" means?Yes. In this case the optimizations are not fusing operations with the GEMM kernel, but instead implementing all of the tanh/sigmoid/addition/multiplication operations as a single kernel rather than as individual kernels.Fusing these operations with GEMM kernels isn't out of the question. In this recent post on CUTLASS: https://devblogs.nvidia.com... there is an example showing how a GEMM can be fused with a bias and ReLU activation function.Thank you for this excellent blog. I have some question about Optimization 5. I think in every sub-iteration, when we finish the computation in stream B, we need to perform point-wise computation because the next iteration requires outputs from last iteration. I don't know if my idea is right. So any answer will be welcomed, thank you.Powered by Discourse, best viewed with JavaScript enabled"
3240,advanced-api-performance-clears,"Originally published at:			https://developer.nvidia.com/blog/advanced-api-performance-clears/
Surface clearing is a widely used accessory operation. This post covers best practices for clears on NVIDIA GPUs.Thanks for reading this post!
Hope you find these recommendations are useful.
If you have any questions or comments, please let us knowPowered by Discourse, best viewed with JavaScript enabled"
3241,new-breakthrough-in-coronavirus-research-uses-gpu-accelerated-software-to-support-treatment-development,"Originally published at:			New Breakthrough in Coronavirus Research Uses GPU-Accelerated Software to Support Treatment Development | NVIDIA Technical Blog
To help in the development of a treatment or vaccine for COVID-19, University of Texas at Austin and National Institute of Health (NIH) researchers recently achieved a critical breakthrough—creating the first 3D, atomic-scale map of the virus.  “2019-nCoV makes use of a densely glycosylated spike (S) protein to gain entry into host cells,” the researchers…Powered by Discourse, best viewed with JavaScript enabled"
3242,gtc-2020-accelerated-light-transport-simulation-using-neural-networks,"GTC 2020 S21852
Presenters: Thomas Muller,NVIDIA
Abstract
Neural network-based techniques have taken many fields by storm, but until recently have seen relatively little use in the field of physically-based rendering. This has begun to change. We’ll present techniques for accelerating Monte Carlo integration of light transport without introducing bias by utilizing functions learned by neural networks for variance reduction. Our techniques yield on-par or higher performance than competing machine learning-based techniques at equal sample counts and generalize beyond physically-based rendering, being applicable to other high-dimensional integration problems such as Bayesian inference and reinforcement learning.Watch this session
Join in the conversation below.Powered by Discourse, best viewed with JavaScript enabled"
3243,nvidia-and-nih-researchers-develop-an-ai-tool-with-clara-train-sdk-to-better-detect-prostate-cancer,"Originally published at:			https://developer.nvidia.com/blog/nvidia-and-nih-researchers-developed-an-ai-tool-with-clara-train-sdk-to-better-detect-prostate-cancer/
By Dong Yang, Thomas Sanford, and Daguang Xu. Last year, the National Institutes of Health (NIH) and NVIDIA began working together on the development of several clinical deep learning tools in diagnostic and interventional imaging. The research involved the analysis of prostate cancer using imaging and pathology data from clinical trials at the NIH. Prostate…Powered by Discourse, best viewed with JavaScript enabled"
3244,driving-data-center-innovation-through-ecosystem-partners,"Originally published at:			https://developer.nvidia.com/blog/driving-data-center-innovation-through-ecosystem-partners/
Leading security, storage, and networking vendors are joining the DOCA and DPU community.Powered by Discourse, best viewed with JavaScript enabled"
3245,gtc-2020-deep-learning-training-for-conversational-ai-workloads-audio-speech-recognition-natural-language-processing-and-text-to-speech,"GTC 2020 CWE22673
Presenters: Davide Onofrio,NVIDIA; Grzegorz Karch, ; Oleksii Kuchaiev, ; Evelina Bakhturina,
Abstract
We will discuss about advanced techniques on Deep Learning for Conversational AI, which includes Audio Speech Recognition, Natural Language Processing and Text-to-SpeechWatch this session
Join in the conversation below.I am crossposting this memo, hoping this is the right place to put my questions.
I hit a roadblock when trying to use KALDI for a corpus of english-spanish language data using this code which seems to be taylored to Chinese.
More details in this status report There is a paragraph ‘discontinuing the project’ explaining the data preparation issue. I would appreciate any help on thisPowered by Discourse, best viewed with JavaScript enabled"
3246,rtx-backstage-10-questions-for-nixxes-about-the-awesome-pc-tech-in-shadow-of-the-tomb-raider,"Originally published at:			RTX Backstage: 10 Questions for Nixxes About the Awesome PC Tech in Shadow of the Tomb Raider | NVIDIA Technical Blog
The Tomb Raider series has always pushed the envelope technologically… the original release in 1996 was the first 3D game many players ever tried. It makes sense that the latest installment, Shadow of the Tomb Raider, would employ a wide range of cutting-edge NVIDIA features on PC. We talked with Jurjen Katsman at Nixxes to…Powered by Discourse, best viewed with JavaScript enabled"
3247,developer-spotlight-visualizing-high-resolution-atomic-structures-to-simulate-molecular-dynamics,"Originally published at:			Developer Spotlight: Visualizing High-Resolution Atomic Structures to Simulate Molecular Dynamics | NVIDIA Technical Blog
Researchers at the Beckman Institute, part of The University of Illinois, Urbana-Champaign are developing research tools like NAMD and VMD for computational scientists all over the world to analyze large and cumbersome datasets to visualize and simulate molecular dynamics. NAMD is a molecular dynamics simulation package that runs on anything from desktops all the way…Powered by Discourse, best viewed with JavaScript enabled"
3248,gtc-21-top-5-data-center-networking-sessions,"Originally published at:			https://developer.nvidia.com/blog/gtc-21-top-5-data-center-networking-sessions/
Attend GTC to learn more about breakthroughs in data center and cloud networking, including optimized modern workloads and programmable data center infrastructure.Powered by Discourse, best viewed with JavaScript enabled"
3249,nvidia-jetson-agx-orin-charts-new-path-for-edge-ai-and-robotics,"Originally published at:			https://developer.nvidia.com/blog/nvidia-jetson-agx-orin-charts-new-path-for-edge-ai-and-robotics/
Learn about the newly announced  NVIDIA Jetson AGX Orin, the smallest, most powerful, and energy-efficient AI supercomputer.Cannot wait to see how the new Jetson AGX Orin dev kit is used to build the next small thing in autonomous machines!Powered by Discourse, best viewed with JavaScript enabled"
3250,nvidia-and-red-hat-announce-support-for-rhel-on-dgx-1-servers,"Originally published at:			NVIDIA and Red Hat Announce Support for RHEL on DGX-1 Servers | NVIDIA Technical Blog
Artificial Intelligence (AI) has become increasingly pervasive in driving internal and external innovation in enterprises — especially so with the proliferation of NVIDIA DGX-1 servers. IT teams in enterprise data centers often rely on Red Hat Enterprise Linux (RHEL) leveraging its feature richness, stability, and manageability. Taking advantage of that familiarity and helping to enable…Powered by Discourse, best viewed with JavaScript enabled"
3251,grandmaster-series-how-to-build-a-world-class-ml-model-for-melanoma-detection,"Originally published at:			https://developer.nvidia.com/blog/grandmaster-series-how-to-build-a-world-class-ml-model-for-melanoma-detection/
In episode one of the Grandmaster Series you’ll learn from three members of the Kaggle Grandmasters of NVIDIA (KGMON) team Chris Deotte, Bo Liu, and Gilberto Titericz. Watch the video below to learn how they built the winning ML model for the SIIM-ISIC Melanoma Classification Kaggle competition.  Add to your calendar In this competition, the…Powered by Discourse, best viewed with JavaScript enabled"
3252,driving-5g-era-innovation-with-ai-and-accelerated-computing,"Originally published at:			https://developer.nvidia.com/blog/driving-5g-era-innovation-with-ai-and-accelerated-computing/
Telcos are starting to use AI and accelerated computing to address key industry challenges in the 5G era. NVIDIA accelerated computing and AI platforms are making this possible.Powered by Discourse, best viewed with JavaScript enabled"
3253,nvidia-grace-cpu-superchip-architecture-in-depth,"Originally published at:			https://developer.nvidia.com/blog/nvidia-grace-cpu-superchip-architecture-in-depth/
The NVIDIA Grace CPU Superchip brings together two high-performance and power-efficient NVIDIA Grace CPUs with server-class LPDDR5X memory connected with NVIDIA NVLink-C2C.Powered by Discourse, best viewed with JavaScript enabled"
3254,gtc-2020-developing-real-time-neural-networks-for-jetson,"GTC 2020 S22676
Presenters: John Welsh,NVIDIA
Abstract
In this presentation we will explore techniques for developing real time neural network applications for NVIDIA Jetson. We’ll cover various workflows for profiling and optimizing neural networks designed using the frameworks PyTorch and TensorFlow. Additionally, we’ll discuss practical constraints to consider when designing neural networks with real time deployment in mind. If you’re familiar with deep learning, but unfamiliar with the tools for optimization that NVIDIA provides, this session is for you. We hope this session will help facilitate the deployment of real time applications on NVIDIA Jetson.Watch this session
Join in the conversation below.Powered by Discourse, best viewed with JavaScript enabled"
3255,deep-learning-for-object-detection-with-digits,"Hi,Do we have support for multiple object detection(pedestrians, cars in an image) in DIGITS 4? If not, is it planned for the future?You can and here is a PR with some information about it -https://github.com/NVIDIA/c...Hi!""Unfortunately, this dataset is not shareable"" - maybe the net weights are shareable? The data look the same as in my problem (cars viewed above), would love to try it out on my dataset :)try ""ALT"" from alpslabel.wordpress.com but no guarantee its the easiest way. Prefer considering it ""easier"".Im pretty sure that recall is Tp / (Tp + Fn), not Tp / (Tp + Tn)Good catch. Fixed it, thanks.Can we use torch model instead of caffe in custom network option? can we convert caffe prototxt to torch lua?I have very large images at about 5000x4000 with training bounding boxes that are mostly about 110x110. There are more than 500 of these images. Is there any advice with dealing with this much data or estimation of how long it could take to train? I am using a Tesla K40. Any idea what batch size I will likely have to use or even a reference as to how to determine the batch size?Hi Leonard, for input image size, you are restricted to gpu memory. I am training large images also and for detectnet, at 12 GB gpu RAM, 4-4.5 megapixel RGB images are maximum. This means something like 2000x2000 or 4000X1000,  etc.Have you checked the tools at alpslabel.wordpress.com ? You may find some useful stuff there.Thank you Baker and Prasanna for step by step guidanceI am new bee here and I followed all your steps to create DB using KITTI vision with 56 sets for training and 13 for validationI got error when created DB and I really dont know what the error msg means2017-05-11 09:39:43 [ERROR] ValueError: invalid literal for int() with base 10: '116.41870117188'Traceback (most recent call last):File ""/home/yasirac/digits/digits/tools/create_generic_db.py"", line 478, in <module>args['stage']File ""/home/yasirac/digits/digits/tools/create_generic_db.py"", line 443, in create_generic_dbforce_same_shape)File ""/home/yasirac/digits/digits/tools/create_generic_db.py"", line 296, in create_dbentry_ids = extension.itemize_entries(stage)File ""/home/yasirac/digits/digits/extensions/data/objectDetection/data.py"", line 183, in itemize_entriesself.load_ground_truth(self.train_label_folder)File ""/home/yasirac/digits/digits/extensions/data/objectDetection/data.py"", line 208, in load_ground_truthdatasrc.load_gt_obj()File ""/home/yasirac/digits/digits/extensions/data/objectDetection/utils.py"", line 193, in load_gt_objgt.occlusion = int(row[2])ValueError: invalid literal for int() with base 10: '116.41870117188'I want to know the performance impact in object detection if the image resolution is high.classification + localization = object detection. Am I right ?YesI am new to Digits and TX2. I am trying to create object detection modelI created dataset sucessfully. The issue is with the modelWhile creating a model, I am getting the following error.Memory required for data: 3268934784creating layer bbox_loss        Creating Layer bbox_lossbbox_loss <- bboxes-obj-masked-normbbox_loss <- bbox-obj-label-normbbox_loss -> loss_bboxSetting up bbox_lossTop shape: (1)with loss weight 2Memory required for data: 3268934788Creating layer coverage_lossCreating Layer coverage_losscoverage_loss <- coverage_coverage/sig_0_split_0coverage_loss <- coverage-label_slice-label_4_split_0coverage_loss -> loss_coverageSetting up coverage_lossTop shape: (1)with loss weight 1Memory required for data: 3268934792Creating layer clusterThe job directory information on the left is:Job Directory/home/nvidia/DIGITS/digits/jobs/20180816-161051-e67aDisk Size0 BNetwork (train/val)train_val.prototxtNetwork (deploy)deploy.prototxtNetwork (original)original.prototxtSolversolver.prototxtRaw caffe outputcaffe_output.logPretrained Model/home/nvidia/bvlc_googlenet.caffemodel.4VisualizationsTensorboardThe error on the server is2018-08-16 16:10:53 [20180816-161051-e67a] [INFO ] Task subprocess args: ""/home/nvidia/Caffe/caffe/build/tools/caffe train --solver=/home/nvidia/DIGITS/digits/jobs/20180816-161051-e67a/solver.prototxt --gpu=0 --weights=/home/nvidia/bvlc_googlenet.caffemodel.4""2018-08-16 16:11:00 [20180816-161051-e67a] [ERROR] Train Caffe Model task failed with error code 1I have no idea on how to free up memory as I have more than 2 gb available in the job directory. Please help me. Thanks in advance.I don't believe this is a memory error - those memory messages are just informational.  Perhaps check the complete caffe_output.log as there may be more information on the error in there.Thank you. I found this in caffe_output.log file'I0817 14:06:07.410957 24829 net.cpp:159] Memory required for data: 661169480I0817 14:06:07.410969 24829 layer_factory.hpp:77] Creating layer clusterImportError: No module named layers.detectnet.clustering'Please enlighten me.Hi,I was trying to implement research paper titled  "" Accurate Object Localization in Remote Sensing Images Based on Convolutional Neural Networks "" but i am stuck in it. I was wondering if any one can help me in its implementation Or could suggest me that either i should implement it or not?Hi,I ran into an issue with the BDD dataset using NVIDIA DIGITS. I converted the BDD dataset format to KITTI format. However, I am getting an error because of the object classes: traffic light and traffic sign. These are the errors:ERROR: ValueError: could not convert string to float: signERROR: ValueError: could not convert string to float: lightAny advice or recommendations to solve this issue? Please help. Thank you!does detectnet detects only one box per grid cell?Is DIGITS framework still active and supported?Powered by Discourse, best viewed with JavaScript enabled"
3256,new-ai-style-transfer-algorithm-allows-users-to-create-millions-of-artistic-combinations,"Originally published at:			https://developer.nvidia.com/blog/new-ai-style-transfer-algorithm-allows-users-to-create-millions-of-artistic-combinations/
Current style transfer models are large and require substantial computing resources to achieve the desired results. To accelerate the work and make style transfer a tool that is more widely adopted, researchers from NVIDIA and the University of California, Merced developed a new deep learning-based algorithm for style transfer that is effective and efficient.  The…Powered by Discourse, best viewed with JavaScript enabled"
3257,new-gpu-optimized-models-and-notebooks-available-from-tensorflow-hub-google-ai-hub-google-colab,"Originally published at:			New GPU Optimized Models and Notebooks Available from TensorFlow Hub, Google AI Hub, Google Colab | NVIDIA Technical Blog
This week at TensorFlow World, Google announced community contributions to TensorFlow hub, a machine learning model library. NVIDIA was a key participant, providing models and notebooks to TensorFlow Hub along with new contributions to Google AI Hub and Google Colab containing GPU optimizations from NVIDIA CUDA-X AI libraries. UNet Models and Notebooks for Industrial Quality…Powered by Discourse, best viewed with JavaScript enabled"
3258,nvidia-clara-imaging-brings-ai-assisted-annotation-and-model-training-to-xnat-to-enable-medical-imaging-ai,"Originally published at:			https://developer.nvidia.com/blog/nvidia-clara-imaging-powers-xnat-ml-release-to-enable-medical-imaging-ai/
Building on the announcement at RSNA 2019, XNAT, the most widely-used open-source informatics platform for imaging research, announced the beta release of XNAT Machine Learning (XNAT ML).Powered by Discourse, best viewed with JavaScript enabled"
3259,how-to-train-a-defect-detection-model-using-synthetic-data-with-nvidia-omniverse-replicator,"Originally published at:			https://developer.nvidia.com/blog/how-to-train-a-defect-detection-model-using-synthetic-data-with-nvidia-omniverse-replicator/
Learn how to train an object detection model entirely with synthetic data, improve its accuracy with limited ground truth real data, and validate it against images that model has never seen before.Thanks for the article, it’s a good reference design.I started playing with Replicator using a similar method, i.e. putting all my Replicator code in an extension so I could use VS Code to debug.  The problem I have though is Omniverse Code crashes after a few iterations through the debugger.  I have a VERY simple Replicator extension and I’ve tried everything I could to free memory and such to avoid memory issues, but it still crashes.  Headless mode works fine but there’s no debugger and I’m not the best Python programmer so I rely on the VS Code debugger / Github Copilot to hold my hand.  It’s extremely frustrating when I have to restart OV Code every few tries of my Replicator code, obviously.I’m on Windows 11 with 64 GB RAM and an RTX 4090.  What kind of machine are people using to successfully work with Replicator extensions?  What machine was this extension developed on.  Should I just max out my RAM and pray or what?  Should I switch to Linux?  What’s a known reference machine specs for using Replicator with no crashes, currently?Any pointers appreciated.Thanks,
DaveYour computer specs look great, I would start by making sure that OV code, your computer, and especially your GPU drivers are all up to date.Let me know how that goes and we’ll move forward from there!EricPowered by Discourse, best viewed with JavaScript enabled"
3260,deploying-ai-deep-learning-models-with-nvidia-triton-inference-server,"Originally published at:			https://developer.nvidia.com/blog/deploying-ai-deep-learning-models-with-triton-inference-server/
In the world of machine learning, models are trained using existing data sets and then deployed to do inference on new data. In a previous post, Simplifying and Scaling Inference Serving with NVIDIA Triton 2.3, we discussed inference workflow and the need for an efficient inference serving solution. In that post, we introduced Triton Inference…Powered by Discourse, best viewed with JavaScript enabled"
3261,gtc-2020-accelerating-quantum-chemistry-simulations-with-ai,"GTC 2020 S21273
Presenters: Abe Stern,NVIDIA
Abstract
We’ll discuss computational chemistry applications of machine learning covering three topics. First, we’ll examine the use of neural networks and other machined-learned methods for describing a quantum-accurate potential energy surface. Second, we’ll cover graph convolution neural networks and graph message-passing networks for predicting molecular properties at a fraction of the cost of traditional electronic structure calculations. Third, we’ll discuss variational autoencoders for molecule discovery and illustrate their application to drug discovery.Watch this session
Join in the conversation below.Powered by Discourse, best viewed with JavaScript enabled"
3262,nvidia-accelerates-conversational-ai-from-research-to-production-with-latest-updates-in-nemo-and-jarvis,"Originally published at:			https://developer.nvidia.com/blog/nvidia-accelerates-conversational-ai-from-research-to-production-with-latest-updates-in-nemo-and-jarvis/
Today, NVIDIA released world class speech recognition capability for enterprises to generate highly accurate transcriptions and NeMo 1.0 which includes new state-of-the-art speech and language models for democratizing and accelerating conversational AI research.Powered by Discourse, best viewed with JavaScript enabled"
3263,edge-ai-is-powering-a-safer-smarter-world,"Originally published at:			Edge AI is Powering a Safer, Smarter World | NVIDIA Technical Blog
NVIDIA is partnering with IronYun to leverage the capabilities of edge AI to help make the world a smarter, safer, more efficient place.Powered by Discourse, best viewed with JavaScript enabled"
3264,announcing-major-updates-to-designworks-and-vrworks-developer-tools,"Originally published at:			https://developer.nvidia.com/blog/announcing-major-updates-to-designworks-and-vrworks-developer-tools/
Today, in conjunction with the NVIDIA GPU Technology Conference (GTC) in San Jose, California, we are announcing major updates to our industry-leading DesignWorks and VRWorks SDKs and developer tools. Developers working in professional graphics, advanced rendering, video processing, 360-degree videos, material design, and 3D printing can immediately begin using the updated SDKs and tools. The…Powered by Discourse, best viewed with JavaScript enabled"
3265,surgical-robot-performs-first-solo-operation,"Originally published at:			https://developer.nvidia.com/blog/surgical-robot-performs-first-solo-operation/
Using machine learning and computer vision, a surgical robot successfully performs laparoscopic surgery, demonstrating a notable step toward automated surgery.Powered by Discourse, best viewed with JavaScript enabled"
3266,gtc-2020-deep-into-triton-inference-server-bert-practical-deployment-on-nvidia-gpu,"GTC 2020 S21736
Presenters: Tianhao Xu,NVIDIA
Abstract
We’ll give an overview of the TensorRT Hyperscale Inference Platform. We start with a deep dive into current features and internal architecture, then go into deployment possibilities in a generic deployment ecosystem. Next, we’ll give a hands-on overview of NVIDIA Bert, FasterTransformer and TRT-optimized BERT inference. Then we’ll get into how to deploy BERT TensorFlow model with custom op, how to deploy BERT TensorRT model with plugins, and benchmarking. We’ll finish with other optimization techniques and open discussion.Watch this session
Join in the conversation below.I saw this Document, and I am trying to run this Bert on Triton Infer Server. Eveything goes fine, even I can run bert_fastertransformer correctly, but only I got error in bert_trt model:
./install/bin/perf_client -m bert_trt -d -x32 -c8 -l200 -p2000 -b32 -i grpc -u 127.0.0.1:8001 -t1 --max-threads=1
I followed every step in the document, so why is this?
the error message from Server is like this:
I0416 10:01:21.706121 60 grpc_server.cc:643] GRPC allocation failed for type 1 for cls_squad_logits
I0416 10:01:21.706179 60 grpc_server.cc:685] GRPC allocation: cls_squad_logits, size 1024, addr 0x7f858e45b0b0
I0416 10:01:21.730079 60 trtserver.cc:1218] Infer failed: failed to use CUDA copy for tensor ‘cls_squad_logits’: an illegal memory access was encountered
I0416 10:01:21.730093 60 grpc_server.cc:954] InferHandler::InferComplete, 3 step ISSUEDPowered by Discourse, best viewed with JavaScript enabled"
3267,accelerating-nvshmem-2-0-team-based-collectives-using-nccl,"Originally published at:			https://developer.nvidia.com/blog/accelerating-nvshmem-2-0-team-based-collectives-using-nccl/
NVSHMEM 2.0 is introducing a new API for performing collective operations based on the Team Management feature of the OpenSHMEM 1.5 specification. A team is a subset of processing elements (PEs) in an OpenSHMEM job. The concept is analogous to communicators in MPI. The new Teams API is a replacement for the active-set-based API for…Powered by Discourse, best viewed with JavaScript enabled"
3268,gtc-2020-a-new-era-of-medical-imaging,"GTC 2020 S22554
Presenters: Nicola Rieke,NVIDIA
Abstract
Deep Learning has revolutionized medical imaging research and has become an essential element for every single step of the imaging pipeline. In this session, we will discuss some recent trends and share key learnings from related conferences. We will go through the general imaging pipeline - from signal, to image, to image understanding, to actionable insights - and provide examples how Deep Learning can accelerate, augment and improve various steps of the pipeline. And even enable the previously impossible.Watch this session
Join in the conversation below.Powered by Discourse, best viewed with JavaScript enabled"
3269,deploying-ai-models-at-scale-with-an-operating-system-for-smart-hospitals,"Originally published at:			https://developer.nvidia.com/blog/deploying-ai-models-at-scale-with-an-operating-system-for-smart-hospitals/
AIDE is a new operating system for the hospital that allows healthcare providers to deploy AI models safely, effectively, and efficiently.Powered by Discourse, best viewed with JavaScript enabled"
3270,creating-differentiable-graphics-and-physics-simulation-in-python-with-nvidia-warp,"Originally published at:			https://developer.nvidia.com/blog/creating-differentiable-graphics-and-physics-simulation-in-python-with-nvidia-warp/
Warp is a Python API framework for writing GPU graphics and simulation code, especially within Omniverse.Does Nvidia Warp Python API Framework include the new PhysX 5.0 or just the old PhysX 4.0? And Can I run it outside Omniverse?Powered by Discourse, best viewed with JavaScript enabled"
3271,introducing-parallel-forall,"Originally published at:			https://developer.nvidia.com/blog/introducing-parallel-forall/
Welcome to Parallel Forall, NVIDIA’s developer blog focused on providing detailed technical information on a variety of GPU computing programming topics, including, including CUDA C/C++, OpenACC, GPU-accelerated libraries, GPU programming techniques, and much more. At Parallel Forall we will provide useful information about productive, high-performance programming techniques for the latest GPU technology. Our readers will have the opportunity to engage in discussion…Powered by Discourse, best viewed with JavaScript enabled"
3272,accelerating-inference-with-nvidia-triton-inference-server-and-nvidia-dali,"Originally published at:			https://developer.nvidia.com/blog/accelerating-inference-with-triton-inference-server-and-dali/
When you are working on optimizing inference scenarios for the best performance, you may underestimate the effect of data preprocessing. These are the operations required before forwarding an input sample through the model. This post highlights the impact of the data preprocessing on inference performance and how you can easily speed it up on the…Though the article explains how to decode images on server side very well, and it worked like charm, but I am struggling to find a similar thing for videos.Inferencing on video datasets is even more network intensive, and with the likes of ffmpeg-python, I am able to encode a 32/64 frame sequences into an encoded h264 byte stream. If this can be decoded in dali on server side to make the required NTHWC tensor, it would be great.
After spending hours, I could only find Video readers that read from file, but nothing that can read videos from external_sourceHi @sufiyan,Thank you for checking DALI. That is true, it doesn’t support video decoding with TRITON.
What you can do is to check out DeepStream which aims to handle video streaming and provides integration with TRITON.Powered by Discourse, best viewed with JavaScript enabled"
3273,connect-with-nvidia-at-supercomputing-2017,"Originally published at:			Connect with NVIDIA at Supercomputing 2017 | NVIDIA Technical Blog
Join NVIDIA at SC17 in Denver, Colorado to learn how GPU-accelerated computing is changing the very definition of the word possible. Humanity’s moonshots, like understanding the most fundamental laws of physics, breakthroughs in drug development, and sustainable energy are being achieved right now. See how leaders in fields such as HPC, accelerated supercomputing, and deep…Powered by Discourse, best viewed with JavaScript enabled"
3274,overcoming-advanced-computing-challenges-with-million-x-performance,"Originally published at:			https://developer.nvidia.com/blog/overcoming-advanced-computing-challenges-with-million-x-performance/
Learn more about the many ways scientists are applying advancements in Million-X computing and solving global challenges.What’s your Million-X challenge?  We’d love to hear your thoughts.Powered by Discourse, best viewed with JavaScript enabled"
3275,tensorrt-integration-speeds-up-tensorflow-inference,"I have same problem, did you solved?If you're having technical issues, you might drop by the NVIDIA Developer Forum. TensorRT discussions between users and NVIDIA engineers are ongoing here:https://devtalk.nvidia.com/...@pharrellyhy:disqus please post on the TensorRT forum as others benefit from the discussion + solution as well:https://devtalk.nvidia.com/...As a member of the NVIDIA Developer Program, you can submit bugs at: https://developer.nvidia.co...This worked for me ""If you are using pip install tensorflow-gpu, could you please make sure that you have downloaded TensorRT3.0.4 for Ubuntu *14.04* and not 16.04. Due to compatibility requirements of TensorFlow we need to use 14.04 release of TensorRT when using TensorFlow pip packages. We hope to resolve this in the next TensorRT release. If you are building from sources, you can use your native version of TensorRT. If these doesn't help please provide us means to contact you. @Rohith B, TensorRT4RC should work if you compile from sources but is not officially supported so might encounter problems""  from Sami.  See the comments aboveHello, Thanks for sharing it, I have downloaded the code but I am not able to run given code ""run_all.sh, tftrt_sample.py"" completely, Error ""Core Dump"" after starting build engine message. But I am able to run another code in example directory, ""uff_mnist.py"". I am getting around 100 times less inference time using TensorRT optimized engine, Thanks to you all for this. Now I want to see the Optimized Graph of my optimized engine in TensorBoard, which is mentioned in Figure 2 (Right). Can anyone help me here? I am using TensorRT 4.0.0.3 version. Thanks in Advance.I meet the same problem, got ""munmap_chunk(): invalid pointer Aborted (core dumped)""System information:    Have I written custom code (as opposed to using a stock example script provided in TensorFlow): no    OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 18.04    TensorFlow installed from (source or binary): source    TensorFlow version (use command below): 1.8.0    TensorRT version: 4.0.0.3    Bazel version (if compiling from source): 0.10.1    CUDA/cuDNN version: 9.0/7.1.3    GPU model and memory: GTX 1080 8gb https://uploads.disquscdn.c...Yes, Most of people are getting the same error, reported https://devtalk.nvidia.com/... also. I saw people are waiting from more than 1 month, but I No One from Nvidia is bother about issue we are facing. Now, I think, its time to move other platforms?I see this problem .but the  the tensorrt graph output value is same as the original graph, so what is this mean?2018-05-23 16:30:20.755670: I tensorflow/core/grappler/devices.cc:51] Number of eligible GPUs (core count >= 8): 12018-05-23 16:30:20.795501: I tensorflow/contrib/tensorrt/convert/convert_graph.cc:383] MULTIPLE tensorrt candidate conversion: 32018-05-23 16:30:20.798464: W tensorflow/contrib/tensorrt/convert/convert_graph.cc:412] subgraph conversion error for subgraph_index:0 due to: ""Unimplemented: Require 4 dimensional input. Got 2 alexnet/fc2/MatMul"" SKIPPING......( 2 nodes)2018-05-23 16:30:20.805049: W tensorflow/contrib/tensorrt/convert/convert_nodes.cc:459] input: alexnet/conv1/biases not available for node at alexnet/conv1/BiasAdd2018-05-23 16:30:20.805135: W tensorflow/contrib/tensorrt/convert/convert_graph.cc:412] subgraph conversion error for subgraph_index:1 due to: ""Invalid argument: Node alexnet/conv1/BiasAdd should have an input named 'alexnet/conv1/biases' but it is not available"" SKIPPING......( 56 nodes)2018-05-23 16:30:20.807891: W tensorflow/contrib/tensorrt/convert/convert_graph.cc:412] subgraph conversion error for subgraph_index:2 due to: ""Unimplemented: Require 4 dimensional input. Got 2 alexnet/fc1/MatMul"" SKIPPING......( 2 nodes)tensorflow 1.7 tenosrrt4.0 installed and compiled from source.when use int8 int8_calib_gdef = trt.create_inference_graph(    input_graph_def=tf_model,    outputs=[""alexnet/logits/BiasAdd""],    max_batch_size=100,    max_workspace_size_bytes=1 << 25,    precision_mode=""INT8"",  # TRT Engine precision ""FP32"",""FP16"" or ""INT8""    minimum_segment_size=2  # minimum number of nodes in an engine)raise an error:InvalidArgumentError: Node alexnet/fc2/BiasAdd should have an input named 'alexnet/fc2/biases' but it is not availablebut fp16 and fp32 are both okI have the same problem, did you solved? I also meet the subgraph conversion error. I guess if the tensorrt 4.0 have solved unsupported layers conversion. https://uploads.disquscdn.c...I see this problem,Traceback (most recent call last):  File ""/root/anaconda3/lib/python3.4/site-packages/tensorflow/python/framework/importer.py"", line 489, in import_graph_def    graph._c_graph, serialized, options)  # pylint: disable=protected-accesstensorflow.python.framework.errors_impl.InvalidArgumentError: Shape must be rank 2 but is rank 4 for 'import/dense_p7/MatMul' (op: 'MatMul') with input shapes: [1,256,1,1], [256,1]we can successfully complete the tensorrt subgraph convension, but we meet the problem during the inference phases. My model is resnet-50 based tensorflow. who can help me solve this problem, thanks!AttributeError: module 'tensorrt' has no attribute 'create_inference_graph'tensorRT:3.0.4@mirakhtarali:disqus  Did you import python bindings for TensorRT (i.e. import tensorrt) or tensorflow.contrib.tensorrt?Yes I import tensorrtThen you imported TensorRT not TF-TRT. Please import tensorflow.contrib.tensorrt, which is the Tensorflow-TensorRT integration discussed in this articleThanks @Sami Kama USsing TensorFlow backend.2018-09-05 18:51:09.623615: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: FMA2018-09-05 18:51:09.759702: W tensorflow/stream_executor/cuda/cuda_driver.cc:513] A non-primary context 0x62df4c0 for device 0 exists before initializing the StreamExecutor. The primary context is now 0x62b1c60. We haven't verified StreamExecutor works with that.2018-09-05 18:51:09.760573: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1405] Found device 0 with properties: name: GeForce GTX 1080 major: 6 minor: 1 memoryClockRate(GHz): 1.7335pciBusID: 0000:01:00.0totalMemory: 7.93GiB freeMemory: 7.70GiB2018-09-05 18:51:09.760608: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1484] Adding visible gpu devices: 02018-09-05 18:51:10.249471: I tensorflow/core/common_runtime/gpu/gpu_device.cc:965] Device interconnect StreamExecutor with strength 1 edge matrix:2018-09-05 18:51:10.249526: I tensorflow/core/common_runtime/gpu/gpu_device.cc:971]      0 2018-09-05 18:51:10.249536: I tensorflow/core/common_runtime/gpu/gpu_device.cc:984] 0:   N 2018-09-05 18:51:10.249791: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1097] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 7408 MB memory) -> physical GPU (device: 0, name: GeForce GTX 1080, pci bus id: 0000:01:00.0, compute capability: 6.1)model_data/yolo.h5 model, anchors, and classes loaded.Using output node dense_2/SoftmaxConverting to UFF graphTraceback (most recent call last):  File ""demo.py"", line 193, in <module>    main(YOLO())  File ""demo.py"", line 43, in main    uff_model = uff.from_tensorflow_frozen_model(""frozen_inference_graph.pb"", [""dense_2/Softmax""])  File ""/usr/lib/python3.5/dist-packages/uff/converters/tensorflow/conversion_helpers.py"", line 149, in from_tensorflow_frozen_model    return from_tensorflow(graphdef, output_nodes, preprocessor, **kwargs)  File ""/usr/lib/python3.5/dist-packages/uff/converters/tensorflow/conversion_helpers.py"", line 120, in from_tensorflow    name=""main"")  File ""/usr/lib/python3.5/dist-packages/uff/converters/tensorflow/converter.py"", line 76, in convert_tf2uff_graph    uff_graph, input_replacements)  File ""/usr/lib/python3.5/dist-packages/uff/converters/tensorflow/converter.py"", line 53, in convert_tf2uff_node    raise UffException(str(name) + "" was not found in the graph. Please use the -l option to list nodes in the graph."")NameError: name 'UffException' is not definedDoes the throughput image/sec of 2657 in for just 1x V100 GPU?, and what was the FP used?,  I ran the test and I got only ~938 img/sec (for INT8), ~906 img/sec (for FP16), and ~607 img/sec (for FP32.).Also, does it work on P4 with FP16?. I ran the test but got the below error:For FP16: DefaultLogger Half2 support requested on hardware without native FP16 support, performance will be negatively affected.``` TensorRT uses the distribution of node data to quantize the weights for the nodes```I would nice if someone would give some light on this statement.wow TensorRT looks amazing, I will probe it in my Jetson Nano. I'm very excited, thanks NVidia!I'm starting now with Jetson Nano and would like to train a model with custom classes but already using the benefits of TensorRT, where should I start?Powered by Discourse, best viewed with JavaScript enabled"
3276,nvidia-introduces-bluefield-dpu-as-a-platform-for-zero-trust-security-with-doca-1-2,"Originally published at:			NVIDIA DOCA: a foundation for zero trust | NVIDIA Technical Blog
NVIDIA DOCA 1.2 offers new features that make the NVIDIA BlueField DPU the ideal foundation for a zero trust security platform.Powered by Discourse, best viewed with JavaScript enabled"
3277,japans-university-of-aizu-uses-nvidia-jetson-to-nurture-ai-and-robotics-talent,"Originally published at:			https://developer.nvidia.com/blog/japans-university-of-aizu-uses-nvidia-jetson-to-nurture-ai-and-robotics-talent/
University of Aizu, a premier computer science and engineering institution in Japan, conducted a two-week intensive learning program based on the NVIDIA Jetson Nano edge AI platform and Jetson AI Certification.Powered by Discourse, best viewed with JavaScript enabled"
3278,building-software-defined-smart-grid-technology,"Originally published at:			https://developer.nvidia.com/blog/building-software-defined-smart-grid-technology/
Edge AI and high-performance computing are modernizing the electric grid, from power generation to transmission and distribution to the grid-edge.This space is moving rapidly and we are always looking for new partners to join our efforts. If you have any questions, please don’t hesitate to reach out!
Reynaldog@nvidia.comPowered by Discourse, best viewed with JavaScript enabled"
3279,calling-all-hpc-developers-hpc-summit-digital,"Originally published at:			Calling all HPC Developers - HPC Summit Digital | NVIDIA Technical Blog
From energy and space exploration to weather and ocean modeling, HPC is one of the most fundamental tools fueling the advancement of science and technology for developers and researchers. This year, there are two online events for HPC developers to tune into.  ISC Digital From June 22-25, attendees can view NVIDIA featured sessions that are part…Powered by Discourse, best viewed with JavaScript enabled"
3280,nvidia-ai-red-team-an-introduction,"Originally published at:			https://developer.nvidia.com/blog/nvidia-ai-red-team-an-introduction/
Machine learning has the promise to improve our world, and in many ways it already has. However, research and lived experiences continue to show this technology has risks. Capabilities that used to be restricted to science fiction and academia are increasingly available to the public. The responsible use and development of AI requires categorizing, assessing,…Powered by Discourse, best viewed with JavaScript enabled"
3281,gtc-2020-graduate-fellowship-fast-forward-talks,"GTC 2020 S21115
Presenters: Bill Dally,NVIDIA; Bastian Hagedorn,University of Münster; Chen-Hsuan Lin,Carnegie Mellon University; Huaizu Jiang,University of Massachusetts, Amherst; Lifan Wu,University of California, San Diego; Jeremy Bernstein,California Institute of Technology; Daniel Gordon,University of Washington; Ching-An Cheng,Georgia Institute of Technology; De-An Huang,Stanford University; Mariya Popova,Carnegie Mellon University
Abstract
Join a special presentation from our 2019-20 Graduate Fellowship recipients to learn “what’s next” from the world of research and academia. Sponsored projects involve a variety of technical challenges, including efficiency of deep networks, robot learning, interactive AI, performance portability, deep-learning algorithms for computational chemistry, computer-vision techniques to support learning from large-scale unlabeled data, and physically-based rendering. We believe that these minds lead the future in our industry and we’re proud to support the 2019-20 NVIDIA Graduate Fellows. We’ll also announce the 2020-21 Graduate Fellows at this session. For more information on the NVIDIA Graduate Fellowship program, visit Graduate Fellows & Fellowship Opportunities | NVIDIA ResearchWatch this session
Join in the conversation below.Powered by Discourse, best viewed with JavaScript enabled"
3282,cuda-spotlight-gpu-accelerated-guidance-and-control-for-robotic-systems,"Originally published at:			https://developer.nvidia.com/blog/cuda-spotlight-gpu-accelerated-guidance-and-control-robotic-systems/
This week’s CUDA Spotlight is on Jon Rogers of Texas A&M University. Jon is director of the Helicopter and Unmanned Systems Lab, where he works on new technologies for autonomous systems. He is currently exploring new algorithms and sensing technologies to increase task complexity of robotic devices. His research encompasses the fields of non-linear dynamics, robust control,…Powered by Discourse, best viewed with JavaScript enabled"
3283,maxwell-the-most-advanced-cuda-gpu-ever-made,"Originally published at:			https://developer.nvidia.com/blog/maxwell-most-advanced-cuda-gpu-ever-made/
Today NVIDIA introduced the new GM204 GPU, based on the Maxwell architecture. GM204 is the first GPU based on second-generation Maxwell, the full realization of the Maxwell architecture. The GeForce GTX 980 and 970 GPUs introduced today are the most advanced gaming and graphics GPUs ever made. But of course they also make fantastic CUDA development GPUs, with full…am wondering if the warp scheduler is still 0 overhead scheduling ready instructions.and if the L1 cache is similar to Kepler that it is only reserved for local memory accesses such as register spilling, not for global data accesses.im guessing th Fp32 to Fp64 ratio for upcoming cards like GM200 for tesla/quadro/titan will be back to 1:2?A question on compiling the existing code for Maxwell: if I use CUDA libraries, like cuBLAS or cuFFT, should I switch to newer versions of these?Sorry, I can't comment on unannounced products.In general it is in your best interest to upgrade to the latest versions of CUDA libraries. Every release contains new improvements and performance optimizations, as well as new features.That's fine, I understand :-)Very impressed by the performance utilization we're getting out of these cards!Some users are reporting up to 98% utilization on SGEMM OR 6.35 TFLOPS on an overclocked GTX980, probably some kind of record!L1/Texture cache is now unified and the same and local memory spilling is handled by the L2 instead.""Local loads also are cached in L2 only, which could increase the cost of register spillingif L1 local load hit rates were high with Kepler. The balance of occupancy versusspilling should therefore be reevaluated to ensure best performance. Especially giventhe improvements to arithmetic latencies, code built for Maxwell may benefit fromsomewha""Look like the SM 5.2 also works well on the 750 Ti, but give similar performance on it, unlike the 970 (compared to SM 5.0)Any chance sample code with VXGI implementation will be available for all developers?I'm not sure what you mean.  I suspect you mean you are compiling with -arch sm_52 but running on a 750 Ti, which is sm_50. All that means is that your program will JIT the PTX stored in the binary to sm_50 before running, so naturally performance should be similar to compiling with sm_50 explicitly.See this post: http://devblogs.nvidia.com/...Im not sure, when i compile with both 50 and 52, the 750 Ti seems to use the 52 variant, which is a bit slower on it... I already seen the compile lines when the JIT is used, and its not the case.Correct me if I am wrong, but, considering that each quadrant only has 8 LD/ST units, I think a memory operation (warp) would take 4 cycles to be completed, wouldn't it? How does that fit the scheduling and dispatching?Hi, My interest is in massively parallel computing. Do you know if a Maxwell Tesla (compute only) card will appear any time  soon ?  Do you advise to buy current Tesla K40 or wait for a Maxwell Tesla card  and try to get around with one or several GTX980 ?Thanks in advance for your answer!!Hi Mark, I was just wondering that how can I enable the ""Local L1 cache"" of maxwell second generation(sm_52 arch)? I want to use L1 cache to improve the performance when register spill to local memory, is that possible? I search on the internet and only I can see is --Xptxas -dlcm= ca. But this is for global L1 cache, so what is the specifier for local l1 cache? Thank you very much!Hi YTROCK, on Maxwell locals are cached in L2 only. You can query whether a given GPU supports local caching in L1 using the localL1CacheSupported device attribute. Full details here: http://docs.nvidia.com/cuda...Got it, thank you very much!the cache hierarchy was changed,I want to know the size of L1cache in Maxwell.I appreciate it if you reply me . thank you very muchHi all, I have just bought Nvidia Jetson tx1. It has Maxwell GPU with 256 cores. Does anybode know where I can find the documentation for this specific Maxwell GPU with 256 cores. Thank you in advance.Hi Hong Quan, yes, these GPUs should support MPS, however keep in mind that MPS requires 64-bit Linux. Also, some of the GPUs you mention are laptop GPUs which only have a small number of SMs, so they might not be suited well to MPS. Also, you can't run MPS when the X Server is running, which may make it difficult to use it on a laptop.Powered by Discourse, best viewed with JavaScript enabled"
3284,speech-ai-spotlight-reimagine-customer-service-with-virtual-agents,"Originally published at:			https://developer.nvidia.com/blog/speech-ai-spotlight-reimagine-customer-service-with-virtual-agents/
Virtual agents or voice-enabled assistants have been around for quite some time. But in the last decade, their usefulness and popularity have exploded with the use of AI. According to Gartner, virtual assistants will automate up to 75% of tasks for call center agents by 2025–up from 30% in 2021. This translates to a better…Powered by Discourse, best viewed with JavaScript enabled"
3285,optimizing-system-latency-for-esports-with-nvidia-reflex-sdk,"Originally published at:			https://developer.nvidia.com/blog/optimizing-system-latency-for-esports-with-nvidia-reflex-sdk/
eSports is a competition measured in milliseconds that requires lightning fast reflexes and incredible timing. That’s why competitive gamers continuously look for ways to optimize their PCs and game performance. One way to do this is to optimize system latency, which is the time between a mouse click and refreshed pixels on a display.  In…Powered by Discourse, best viewed with JavaScript enabled"
3286,baidu-launches-augmented-reality-platform-for-smartphones,"Originally published at:			Baidu Launches Augmented Reality Platform for Smartphones | NVIDIA Technical Blog
Baidu’s new DuSee platform allows people to make use of augmented reality within the Chinese internet giant’s apps, such as Mobile Baidu search, and takes advertising to the next level. In a demo of the technology, when a user of the Mobile Baidu app points their smartphone at a map of Shanghai, a virtual 3D…Powered by Discourse, best viewed with JavaScript enabled"
3287,deep-learning-for-computer-vision-with-matlab-and-cudnn,"Hi Shashank Prasanna,first of all thanks for your great blog post, really interesting and useful.I have tried it and I've had a problem in this partfor ii = 1:numel(imset)  for jj = 1:imset(ii).Count      trainingImages(:,:,:,jj) = imresize(single(read(imset(ii),jj)),imageSize(1:2));  endendPROBLEM --> Assignment has fewer non-singleton rhs dimensions than non-singleton subscriptsIf I continue doing the following steps, in this one I get another problem:svmmdl = fitcsvm(cnnFeatures,trainingLabels);PROBLEM --> Error using classreg.learning.FullClassificationRegressionModel.prepareDataCR (line 138)X and Y do not have the same number of observations.Error in ClassificationSVM.prepareData (line 607)            [X,Y,W,dataSummary] = ...Error in classreg.learning.FitTemplate/fit (line 205)                [X,Y,dataPrepOut{1:this.NDataPrepOut}] = ...Error in ClassificationSVM.fit (line 237)            this = fit(temp,X,Y);Error in fitcsvm (line 279)obj = ClassificationSVM.fit(X,Y,varargin{:});What could I do to solve it? Thank you very much.I found where is the problem.I tracked back ""trainingImages"" (""predImage"" in cnnPredict).I found line 16 :n_obs = size(predImage,4);I changed to :n_obs = size(predImage,5);because size(trainingImages) returns :ans =   224   224     3     1   289And now it works.Hello, could anyone please send me matconvnet-1.0-beta 15 zip? Can't run beta 18 :(julia.fumbarev@gmail.comHello Rodrigues,could you please send me matconvnet-1.0-beta 15 zip folder? Can't run beta 18 :(julia.fumbarev@gmail.comHi bfos, Can you please tell me how did you solved this error:PROBLEM --> Assignment has fewer non-singleton rhs dimensions than non-singleton subscriptsI am also getting this error, and want to know the workaround.Thanks.Not solved yet, waiting for a solution!!Hi bfos,In my case, this error was due to few images which are gray images, thus have 2 dimensions only. What I did is to replicate them into a 3 channel image and then copied into trainingImages. It worked for me.But now I am getting that same ""X and Y do not have the same number of observations."" error. Looking into it now. If anyone knows the solution then please let me know. Thanks!Hello,could you please send me your code i need it so much floraza16@gmail.comHi, Shashank Prasanna! I want to construct a composite deep neural network, in which two sub-CNNs are combined through a fusion layer. Can MatConvNet do it? Thank you so much!Hi Shashank,I am getting this error.Attempt to execute SCRIPT vl_nnconv as a function:C:\Saurabh\CNN\matconvnet-1.0-beta15\matlab\vl_nnconv.mError in vl_simplenn (line 193)        res(i+1).x = vl_nnconv(res(i).x, l.weights{1}, l.weights{2}, ...Can you please help !Hello Prasanna,   Thank you so much for the series on CNN. I'd like to do it for Bio-Medical Image feature extraction.And later process with classifier apps. What are the steps to be needed to extract the features and to create the .mat files? Also, it could be used for predicting the test images? It would be helpful and appreciate your efforts. Thanks.Means if you have 100 img, than you must have 100 set of features(100 img *1000 dimension feature)In your case, you only have 1 of 100 dimension feature(total one img), not match you img number(should be more than one).check your matlab workspace out.Hi Shashank,Thanks for your post, and I've been worked on this experiment for a few days and I met a problem that I still can't solve.I found that after I type the command(I use this because I was working without GPU support, does this matter?): [~, cnnFeatures, timeCPU] = cnnPredict(cnnModel,trainingImages,'UseGPU',false)the output is like:Using GPU: falseNumber of images: 10Number of batches: 10Number of layers in the Network: 21-------------------------------------Batch:  1/10. Execution time: 0.0682Batch:  2/10. Execution time: 0.0543Batch:  3/10. Execution time: 0.0509Batch:  4/10. Execution time: 0.0511Batch:  5/10. Execution time: 0.0496Batch:  6/10. Execution time: 0.0514Batch:  7/10. Execution time: 0.0527Batch:  8/10. Execution time: 0.0522Batch:  9/10. Execution time: 0.0506Batch: 10/10. Execution time: 0.0534Avg. execution time/batch:   0.0534-------------------------------------Total execution time:        0.5343-------------------------------------cnnFeature =    'Walker hound, Walker foxhound'    'Lhasa, Lhasa apso'    'Labrador retriever'    'Walker hound, Walker foxhound'    'Brittany spaniel'    'golden retriever'    'German shepherd, German shepherd dog, German p…'    'black-and-tan coonhound'    'Saluki, gazelle hound'    'cradle'The output shows that the number of images is 10, but actually I have more than 10 images, 26 exactly, and I tried I lot to figure this out but I failed.And as a result when I tried this:svmmdl = fitcsvm(cnnFeatures,trainingLabels)and came out:classreg.learning.FullClassificationRegressionModel.prepareDataCR (line 138)X and Y do not have the same number of observations.I'm new to Deep Learning and I found difficult to solve this, could you please help me?I will also be very appreciate to anyone who can offer me any help.thanks again!Well done !An updated version of this demo is available along with source code at the following MathWorks page:http://www.mathworks.com/co...In order to run this demo, you will need a compatible GPU and CUDA driver installed. For more information on the system requirements, check out the following page:http://www.mathworks.com/pr...Hello Shashank Prasanna,Thanks for your valuable post.I'd like to do it in fingerprint liveness detection. How can i do feature extraction in fingerprint images?? what are the steps? I'd also like to classify it into real or fake fingerprint images. after it could used to test on other images. How can i do it sir? Can you please tell me the codes for it. It would be helpful and appreciate your efforts. Thanks.Hi Shashank, I tried to use trainFastRCNNObjectDetector with custom region proposals. so far i've got --> Extracting region proposals from 1064 training images...done.btw there are errors in vision.internal.cnn.fastrcnn.RegionReader (line 146) Unable to find any region proposals to use as positive or negative training samples.how can i solve this problem?It's likely that your custom region proposal method is producing ROIs that are too small to process. The minimum size that we can currently process is limited by the amount of downsampling the network does prior to the last max pooling layer. (Note that we are going to remove this limitation in a future release).By default, with alexnet the minimum size is 105x105. Knowing which network you're using for training would help determine if this is the cause of the error.If you find that this is the cause, then you can expand small ROIs returned by your region proposal method so that they are above the minimum. Alternatively, you can resize your training images so that your objects are larger. But you would have to be careful not to make the image too large because this can increase the amount of GPU memory required to process the data. If this happens you can try to resize the image and then crop around the object.Or you can change the network itself by reducing the output size of the final max pooling layer. For example, with alexnet you can change the last max pooling layer's pool size to 5 so that the output feature map is smaller. This changes the minimum size to 88-by-88:=== code below===net = alexnet;layers = net.Layers;% Reduce output size of final max pooling layer by increasing pool size to 5.% This changes the minimum size to 88-by-88.layers(16) = maxPooling2dLayer(5,'stride',2);  % reset fully connected layers because of the size change. % Note: This may not be the ideal set of layers and might require some experimentation% to figure out the best number of layers after making this change to the max pooling% layer. layers(17) = fullyConnectedLayer(4096);layers(20) = fullyConnectedLayer(4096);layers(23) = fullyConnectedLayer(2)layers(end) = classificationLayer()====Code ends===Figuring out which approach to use might require some experimentation with the layers that follow the final max pooling layer.The other cause of this error could be that the PositiveOverlapRange and NegativeOverlapRange are set to values that make it harder to find training samples that overlap with your custom region proposals. For example, large positive overlap values could make it impossible for any of your region proposals to overlap with the ground truth.Thank you Shashank for this valuable post.I'm new to this field, and you post makes it easy to understand some concepts.Just a question regarding the SVM classification accuracy, is it the Cross validation accuracy that got printed out?and did you compute confusion matrix for this?Thank you again.Powered by Discourse, best viewed with JavaScript enabled"
3288,gtc-2020-nvidia-isaac-sim-amazing-robot-models-and-tasks-simulated-in-isaac-sim-2020-1,"GTC 2020 D2S43
Presenters: Tech Demo Team,NVIDIA
Abstract
During the GTC 2020 virtual keynote, NVIDIA CEO and founder Jensen Huang demonstrated the industry’s first robotic AI development platform with simulation, navigation and manipulation. The first scene is a model of NVIDIA’s Kaya robot pushing around rigid bodies and also soft-body beach balls; second, a model manipulator picks up a bin with a suction cup, and the behavior of the suction cup is demonstrated by dropping heavy items into the bin – the viewer will notice the bin move in response to the impulses caused by the objects landing in it; third, the manipulator stacks bins, and the model manipulator is shown connected to the Isaac SDK application. Lastly, you will see a demonstration of a model logistics #robot, transporting bins that were stacked on the pallet.Learn more about Isaac SIM: NVIDIA Isaac SDK | NVIDIA Developer
Watch the entire GTC 2020 keynote: Keynote by NVIDIA CEO Jensen Huang | GTC 2022 | NVIDIAWatch this session
Join in the conversation below.Powered by Discourse, best viewed with JavaScript enabled"
3289,ai-helps-detect-covid-19-in-chest-ct-images,"Originally published at:			AI Helps Detect COVID-19 in Chest CT Images | NVIDIA Technical Blog
NVIDIA Inception partner TrainingData.io recently developed a segmentation AI model that enables instant analysis of the progression of COVID-19 in chest CT images. When the virus progresses through a patient’s body, there is a build-up of fluid in the tiny air sacs in the lungs called alveoli. The presence of this fluid causes inflammation of…Powered by Discourse, best viewed with JavaScript enabled"
3290,nvidia-jetson-tx2-delivers-twice-the-intelligence-to-the-edge,"Originally published at:			https://developer.nvidia.com/blog/jetson-tx2-delivers-twice-intelligence-edge/
Figure 1: NVIDIA Jetson TX2 embedded system-on-module with Thermal Transfer Plate (TTP). Today at an AI meetup in San Francisco, NVIDIA launched Jetson TX2 and the JetPack 3.0 AI SDK. Jetson is the world’s leading low-power embedded platform, enabling server-class AI compute performance for edge devices everywhere. Jetson TX2 features an integrated 256-core NVIDIA Pascal…Looks amazing Dustin! Great article, thanks!Wow, that is just great .... I have tried TensorRT already thanks to your other great post and it did outperform YOLO about 3 times on TegraX1. Looking forward to have TX2 n see how fast it does the same experiments 😍 So excited lolAmazing! 4kp60 Encoding. My dreams come true!Wow, Nice Article, Thanks!Please correct me if my calculations are wrong: it seems like nominal ALU efficiency of GoogLeNet execution is only about 50 to 70% of the hardware peak, depending on the batch size. Is there a rogue or missing multiplication factor in the following back-of the-envelope estimation logic somewhere :Single GoogLeNet inference == 3.16 GFLOPs of ""nominal"" multiplications or additions, assuming all convolutions are done by a variation of direct time-domain, approach not Winograd convolution or any other technique to reduce the amount of actual computation.Assuming 1 ""core"" = FMA32x1 = FMA16x2 = 4 FP16 FLOP, peak FP16 gigaflops for Jetson TX2 at maximum clock = 256 cores * 4 FP16 FLOPs/core * 1.302 GHz = 1333 FP16 GFLOP/s  (mentioned in the post as  ""more than a TFLOP/s of performance"") From this 1) GoogLeNet efficiency @ batch = 2: 201 FPS * 3.16 GFLOP / 1331 GFLOP/s = 48%, 2) GoogLeNet efficiency @ batch = 128: 290 FPS * 3.16 GFLOP / 1333 GFLOP = 69% 3) AlexNet efficiency @batch = 2, 250 FPS * 1.3 GFLOP / 1333 GFLOP/s = 25% 4) AlexNet efficiency @batch = 128: 692 FPS * 1.3 GFLOP / 1333 GFLOP/s = 68%?RegardsI have NVidia geforce 1080 on my windows 10 desktop..  could i run an Ubuntu VM on hyper-v that runs this software and can process stuff from a webcam or streamed from another video source like a locally networked android phone?  just for learning dont have the 599 right now.This is definitely a supercomputer among SoMs however this tiny thing is half a size and also packs a punch:http://linuxgizmos.com/tiny...Powered by Discourse, best viewed with JavaScript enabled"
3291,visualizing-star-polymers-in-record-time,"Originally published at:			Visualizing Star Polymers in Record Time | NVIDIA Technical Blog
In the last five minutes, you have probably come into contact with more polymers than you can count. In fact, they are everywhere; in grocery bags,  water bottles, phones, computers, food packaging, auto parts, tires, airplanes, and toys. To get a better sense of how polymers behave in different environments Airidas Korolkovas, a postdoctoral researcher…Powered by Discourse, best viewed with JavaScript enabled"
3292,expanding-the-nvidia-doca-community-for-developers-in-china,"Originally published at:			Expanding the NVIDIA DOCA Developer Community to China | NVIDIA Technical Blog
The developer community for NVIDIA DOCA continues to gain momentum around the world.Powered by Discourse, best viewed with JavaScript enabled"
3293,using-vxlan-routing-with-evpn-through-asymmetric-or-symmetric-models,"Originally published at:			https://developer.nvidia.com/blog/using-vxlan-routing-with-evpn-through-asymmetric-or-symmetric-models/
This posts compares asymmetric and symmetric EVPN routing models using EVPN as the control plane. It provides architecture differences and maps them to specific NOS CLI output for educational purposes.Powered by Discourse, best viewed with JavaScript enabled"
3294,from-ai-research-to-clinical-evaluation-nvidia-clara-for-clinical-deployment-at-the-ohio-state-university,"Originally published at:			From AI Research to Clinical Evaluation: NVIDIA Clara for Clinical Deployment at The Ohio State University | NVIDIA Technical Blog
Deploying AI algorithms in a clinical environment is challenging as the tools and workflows of a data scientist’s AI lab are, in general, significantly different from those of radiologists in a clinical environment. To develop clinically relevant AI algorithms, it is essential that researchers and engineers bring their expertise to integrate these algorithms to a radiologist’s…Powered by Discourse, best viewed with JavaScript enabled"
3295,what-s-new-in-nvidia-vkray,"Originally published at:			What’s New in NVIDIA VKRay | NVIDIA Technical Blog
NVIDIA VKRay is a set of three extensions that bring ray tracing functionality to the Vulkan open, royalty-free standard for GPU acceleration.Powered by Discourse, best viewed with JavaScript enabled"
3296,new-service-uses-artificial-intelligence-to-keep-your-kids-safe-online,"Originally published at:			https://developer.nvidia.com/blog/new-service-uses-artificial-intelligence-to-keep-your-kids-safe-online/
A new service called Bark will act as an online watchdog to protect your child from online predators and cyberbullying, as well as looking out for mental health concerns like depression or suicidal thoughts. Using deep learning and GPUs hosted in the Amazon Web Services cloud, Bark scans through social media messages and alerts parents…Powered by Discourse, best viewed with JavaScript enabled"
3297,pgi-community-edition-18-10-now-available,"Originally published at:			PGI Community Edition 18.10 Now Available | NVIDIA Technical Blog
PGI Compilers & Tools are used by scientists and engineers developing applications for high-performance computing (HPC). PGI products deliver world-class multicore CPU performance, an easy on-ramp to GPU computing with OpenACC directives, and performance portability across all major HPC platforms. Available for free download. PGI in the Cloud: Now available on Amazon Web Services (AWS)…Powered by Discourse, best viewed with JavaScript enabled"
3298,programming-distributed-multi-gpu-tensor-operations-with-cutensor-v1-4,"Originally published at:			Programming Distributed Multi-GPU Tensor Operations with cuTENSOR v1.4 | NVIDIA Technical Blog
NVIDIA cuTENSOR, version 1.4, library supports 64-dimensional tensors, distributed multi-GPU tensor operations, and improves tensor contraction performance models.Powered by Discourse, best viewed with JavaScript enabled"
3299,first-deep-learning-based-3d-simulation-of-the-universe,"Originally published at:			First Deep Learning-Based 3D Simulation of the Universe | NVIDIA Technical Blog
To attain a better understanding of the cosmos, researchers successfully developed the first deep learning-based 3D simulation of the universe.  “We can run these simulations in a few milliseconds, while other ‘fast’ simulations take a couple of minutes,” says study co-author Shirley Ho, a group leader at the Flatiron Institute’s Center for Computational Astrophysics in New York City…Powered by Discourse, best viewed with JavaScript enabled"
3300,introducing-the-redesigned-drive-developer-program,"Originally published at:			https://developer.nvidia.com/blog/introducing-the-redesigned-drive-developer-program/
This week at the GPU Technology Conference in San Jose, Calif., NVIDIA launched its revamped automotive developer program for the DRIVE Development Platform. Coinciding with the introduction of the NVIDIA DRIVE AP2X full-stack software solution, the NVIDIA DRIVE Developer program introduces autonomous vehicle (AV) developers to the end-to-end DRIVE Platform. These innovative autonomous solutions span…Powered by Discourse, best viewed with JavaScript enabled"
3301,orchestrating-accelerated-virtual-machines-with-kubernetes-using-nvidia-gpu-operator,"Originally published at:			https://developer.nvidia.com/blog/orchestrating-accelerated-virtual-machines-with-kubernetes-using-nvidia-gpu-operator/
The latest release of GPU Operator adds support for KubeVirt and OpenShift Virtualization, enabling the use of Kubernetes to orchestrate GPU-accelerated applications running as virtual machines.Powered by Discourse, best viewed with JavaScript enabled"
3302,gtc-2020-predictionnet-predicting-the-future-in-multi-agent-environments-for-autonomous-vehicle-applications,"GTC 2020 S21899
Presenters: Alexey Kamenev,NVIDIA; Nikolai Smolyanskiy, NVIDIA
Abstract
Predicting the future trajectories of road agents is an import part of the planning&control stack in autonomous vehicles. Deep-learning approaches can be superior to classical methods in this domain, because neural networks can learn to use context and environment as a prior to improve prediction. We’ll present PredictionNet — a deep neural network (DNN) that can be used for predicting future behavior/trajectories of road agents in autonomous-vehicle applications. Our DNN takes a rasterized top-down view of the world provided by the perception system and computes future predictions from past observations. We’ll present its architecture, training-data collection process, and our training procedures. We’ll also show video demos of live predictions on our self-driving car.Watch this session
Join in the conversation below.Powered by Discourse, best viewed with JavaScript enabled"
3303,robot-uses-deep-learning-to-grasp-awkward-and-unusual-objects,"Originally published at:			Robot Uses Deep Learning to Grasp Awkward and Unusual Objects | NVIDIA Technical Blog
Researchers from University of California, Berkeley and Siemens designed a robot that learns how to grip new objects just by studying a database of 3D shapes. Using a GTX 1080 GPU and cuDNN with the TensorFlow deep learning framework, the team generated 6.7 million synthetic point clouds from thousands of 3D models to train their…Powered by Discourse, best viewed with JavaScript enabled"
3304,detectnet-deep-neural-network-for-object-detection-in-digits,"Hi Alp, You can modify your image size in network. I usually do this in the network customization window.  The image size is inserted into the network both at the beginning and the end.Hi Allison, I managed to create a trainingset with 1248x384 images, but this time, I cannot test with images larger than this resolution, it fails to detect. Is it not possible test with larger images, in this system? I was hoping to detect objects in, like 5000x5000 images, but seems not possible?Are you getting an error or just no bounding boxes? Are you trying to perform detection with with a network that was trained with 1234x384 and then do detection on 5000x5000 pixel images? If so, you need to change the image dimensions in your network in the customization window. This needs to be done in the labels, transformation, cluster, and mAP layers. I hope this helps.Thank you for taking time, I have tried your advice with 5120x256 dimensions. If I dont get you wrong, train_transform, val_transform, cluster, cluster_gt and mAP sections were modified with 5120x256 dimensions but training with new values immediately stopped with out of memory error (dataset is 1248x384 custom images, normally converges well with default values).In this point, can we say, under limitation of 12gb GPU memory, it is simply not possible to test large size images ?I actually need to know, before spending several days, am I  trying the impossible, again, within my hardware limitations (Single Titan X maxwell).-----problem solved----------Problem solved now, thank you for the guidance. I set batch size & acc. to 3 & 4 ; along with above modifications to the network, this way, avoided """"out of memory"" error and succesfully trained 855 of 5120x256 res. images. https://uploads.disquscdn.c... https://uploads.disquscdn.c...When creating your own labels for detectnet, what is the easiest way to specify the bounding boxes for your objects in the images ?Successfully trained an object detection model using Detectnet. My system configuration is Core i5 processor, 16 GB RAM, and GTX 1080 GPU. Input image size is 1360 X 800. While testing it took almost 15 sec to detect object and plot the boxes on a single image.It is specified in the passage that testing an input image size 1536×1024 will take only  41ms on Titan X GPU.Network model I used is the detectnet_network.prototxt which is available along with caffe-0.15.14.What is the reason for high execution timeIs your timing based on using the DIGITS interface with the option selected to visualize the individual layer activations?  If so, most of that time is spent in generating all the additional information for the layer visualizations and activation histograms.  The 41ms timing quoted in the post is solely the feed-forward inference time (obtained using the ""caffe time"" command line utility), so it doesn't include those overheads.There are a number of open-source labelling tools available for this purpose, e.g. https://github.com/cvhciKIT...The statement is correct - an FCN only contains convolutional layers and has no fully-connected layers.Yes, the option for visualize individual layer activation was selected. Removed the selection and run the test. The execution time reduced to 3 sec. Can I get an execution time near to 41ms using my system configuration (Core i5 processor, 16 GB RAM, and GTX 1080 GPU) and Input image size is 1360 X 800?Yes, you should be able to use the ""caffe time"" function along with the deploy.prototxt for DetectNet to see similar inference times.  You will need to use batch size 1.Kitti dontcare labeling is not quite like the DetectNet one. Kitti's README says: ""'DontCare labels denote regions in which objects have not been labeled"" and ""You can use the don't care labels in the training set to avoid that your objectdetector is harvesting hard negatives from those areas, in case you considernon-object regions from the training images as negative examples.""The way I look at it, the dontcare label is supposed to denote a region where no labeling work has been done (there could be positive and negative examples there). The way you use it in DetectNet, is for negative regions, meaning places where there is no object of interest.Am I right?I guess there is no way I could use DetectNet with Kitti's semantic, given that any other labels are mapped to dontcare, so they are treated as negative examples. Any ideas?I wrote a small macro tool for faster labeling of custom detectnet datasets. It allows you to draw bounding boxes around objects, provide a label name and save its coordinates. Rest of the parameters (truncated, occluded etc.,) recorded as zero.Nowhere near a bulletproof coding, but it serves the purpose.https://alpslabel.wordpress...Dear all. What would be the correct way to refer DetectNet in a scientific publication?Can you please provide a detailed description of the configuration parameters mentioned in this article?Namely,detectnet_groundtruth_param {    stride: 16      ---This was explained    scale_cvg: 0.4    gridbox_type: GRIDBOX_MIN    min_cvg_len: 20    coverage_type: RECTANGULAR    image_size_x: 1024    -----This was explained    image_size_y: 512      -----This was explained    obj_norm: true    crop_bboxes: false  }I actually can guess what the following does from their names but your input is appreciated.detectnet_augmentation_param {    crop_prob: 1.0    shift_x: 32    shift_y: 32    scale_prob: 0.4    scale_min: 0.8    scale_max: 1.2    flip_prob: 0.5    rotation_prob: 0.0    max_rotate_degree: 5.0    hue_rotation_prob: 0.8    hue_rotation: 30.0    desaturation_prob: 0.8    desaturation_max: 0.8  }For creating training data, you could use https://dataturks.com/, It provides Image Annotation and Labeling tool, free for open source and research projects. Sample projects https://dataturks.com/proje...Detectnet works super well on the Jetson.  Has anyone converted it to Tensorflow and run it on a server?  Also what's the license for Detectnet?Hello, I'm working on digits for detecting vehicles as well, but I would like to change the stride.  I've changed it in the places above as shown in the picture, as well as the three python parameters listed on the bottom of the custom net, but I am still getting errors.  Was anyone here able to change the stride?Dear All,                    Im very new to Digits & Annotations.i create the  own datasets for prepare the AI model.share the output results as images,the problem is unable to detected the particular images [Boundary Box].plz give suggestion or solution for this issues.Thanks in advanceWith RegardsBabu Raju https://uploads.disquscdn.c... https://uploads.disquscdn.c...Powered by Discourse, best viewed with JavaScript enabled"
3305,deploying-ai-accelerated-medical-devices-with-nvidia-clara-holoscan,"Originally published at:			https://developer.nvidia.com/blog/deploying-ai-accelerated-medical-devices-with-nvidia-clara-holoscan/
NVIDIA Clara Holoscan accelerates deployment of production-quality medical applications by providing a set of OpenEmbedded build recipes and reference configurations.Powered by Discourse, best viewed with JavaScript enabled"
3306,webinar-performant-multiphase-flow-simulation-at-leadership-class-scale,"Originally published at:			Performant Multiphase Flow Simulation at Leadership-Class Scale via OpenACC
On June 6, learn how researchers use OpenACC for GPU acceleration of multiphase and compressible flow solvers that obtain speedups at scale.Powered by Discourse, best viewed with JavaScript enabled"
3307,how-jet-com-built-a-gpu-powered-fulfillment-engine-with-f-and-cuda,"Originally published at:			How Jet.com Built a GPU-Powered Fulfillment Engine with F# and CUDA | NVIDIA Technical Blog
Have you ever looked at your shopping list and tried to optimize your trip based on things like distance to store, price, and number of items you can buy at each store? The quest for a smarter shopping cart is never-ending, and the complexity of finding even a sub-optimal solution to this problem can quickly…Powered by Discourse, best viewed with JavaScript enabled"
3308,offline-to-online-feature-storage-for-real-time-recommendation-systems-with-nvidia-merlin,"Originally published at:			https://developer.nvidia.com/blog/offline-to-online-feature-storage-for-real-time-recommendation-systems-with-nvidia-merlin/
Recommendation models have progressed rapidly in recent years due to advances in deep learning and the use of vector embeddings. The growing complexity of these models demands robust systems to support them, which can be challenging to deploy and maintain in production. In the paper Monolith: Real Time Recommendation System With Collisionless Embedding Table, ByteDance…Powered by Discourse, best viewed with JavaScript enabled"
3309,unified-memory-in-cuda-6,"Originally published at:			https://developer.nvidia.com/blog/unified-memory-in-cuda-6/
With CUDA 6, NVIDIA introduced one of the most dramatic programming model improvements in the history of the CUDA platform, Unified Memory. In a typical PC or cluster node today, the memories of the CPU and GPU are physically distinct and separated by the PCI-Express bus. Before CUDA 6, that is exactly how the programmer…As someone who works with CUDA Fortran, I am hoping the day comes soon when NVIDIA/PGI Fortran also includes a similar functionality. I'd really like to get rid of all those freaking cudaMemcpy's in my code!We will be rolling out Unified Memory for additional languages and platforms in future releases of CUDA (and CUDA Fortran).great functionality, definitely a move in the right direction for allowing porting existing code rather than rewriting.  can we expect virtual function table rewiring for true C++ object copying to device?  any support for STL on device (start with vector, shared_ptr) - even just read-only?Very nice from CUDA 6. Really eager to get started with this. Just one question; does unified memory reads both PC and GPU as one combined memory or is still seen as seperate and CUDA automatically does all the copying instead of the programmer?Eager to get started with this version. And if I have a var like this ""int *raw_ptr"" with NxN size, can I have another var such as ""int **ptrs"" to point to the data of raw_ptr, ie ""*ptrs[0]=raw_ptr[0];*ptrs[1]=raw_ptr[N-1]; "" ? Thanks a lotI've written a system for abstracting memory copies into my API, so the user can just use his data on the CPU and GPU seamlessly, using a checksum internally to determine if anything has changed and only transferring as late as necessary. Every part of the API is made more complex because of this. I'm really looking forward to just deleting all of that logic.The problem with virtual function tables is that AFAIK the C++ standard does not specify the format/layout/implenmentation of vftables. This makes it nearly impossible to support calling virtual functions on shared objects across all C++ host compilers supported by CUDA / nvcc.  As for STL, that is something that we intend to look at, but nothing I can share here yet.On current hardware, the latter -- in a PC today the GPU and CPU memories are physically discrete. But the same programming model could be used for physically unified memory systems, such as Tegra SOCs.It's all just memory, so yes.  I didn't mention in the above post, but there is also a `__managed__` declaration specifier, which allows you to declare global managed device pointers.Will CUDA Unified Memory be supported on GTX cards in Windows 7 and 8 or will it be limited to Tesla cards (due to requirement for TCC driver)? I am really looking forward to using Unified Memory in my CUDA applications, but do not want to limit my clients to using Tesla cards only.Unified Memory will be supported on Compute Capability 3.0 and later (sm_30 - so K10 or GTX 680 or later), on 64-bit Linux, Windows 7 and Windows 8 in CUDA 6.0. Support for other operating systems will come at a later date.Does unified memory support to overlap execution with data transfers on default stream? Or still do I need to split the operations with cudaMemcpyAsync and put them in separate streams for ovelapping?You can always use cudaMemcpyAsync to explicitly copy data and overlap it with kernels in other stream.  Unified Memory does not take away your ability to optimize.In CUDA 6, pages from managed allocations that were touched on the CPU are migrated back to the GPU just before any kernel launch -- so there is no overlap with that kernel. However you can still get overlap between multiple kernels in separate streams.Also, not discussed in this post, is an API in CUDA 6 that allows you to attach a managed allocation to a specific stream, so that you can control which allocations are synchronized on specific kernel launches, and increase concurrency.Future CUDA releases will add more optimizations, such as prefetching.What about the constant memory. I would like to be able allocate it for example like this: int* pint = cudaConstMalloc();And free it like that: cudaConstFree(pint);Or by using: cudaConstMallocManaged();Its a very nice article a small note on C++ this is a has-a class and not is-a class...so there is no need for inheritance. ;-)Unfortunately due to the implementation of constant banks in the hardware this is not possible at this time.We want the class to satisfy ""is a Managed class"", so I believe inheritance of Managed is warranted in this case. If you disagree, can you provide an example of how this would work with a has-a implementation?Oh so deeply sorry about this, I was very absent minded now I see it. Yes of course it is has-a string and is-a managed. Apologies it was too late at night. :-)And I must say you have demonstrated how well you can instantiate classes to be CUDA 6.0 managed objects thanks for putting this example. :-)Powered by Discourse, best viewed with JavaScript enabled"
3310,ai-drones-help-inspect-industrial-equipment,"Originally published at:			AI Drones Help Inspect Industrial Equipment | NVIDIA Technical Blog
Avita Systems, a GE Venture, turned to NVIDIA DGX Systems to enable advanced robotic inspection and automated defect recognition services across the oil and gas, transportation, and energy industries. “Oil and gas companies spend hundreds of millions of dollars on maintenance and inspection each year,” said Alex Tepper, founder and head of corporate and business…Powered by Discourse, best viewed with JavaScript enabled"
3311,accelerating-connection-tracking-to-turbo-charge-stateful-security,"Originally published at:			https://developer.nvidia.com/blog/accelerating-connection-tracking-to-turbo-charge-stateful-security/
Public cloud and telecommunication service providers are moving in the direction of large hyperscale datacenters like Google and Amazon. They are looking for ways to increase efficiencies, flexibility, and agility, and so are turning towards disaggregation and server virtualization as critical tenets of their modernization efforts. However, in doing so, they are stumbling across several…Powered by Discourse, best viewed with JavaScript enabled"
3312,gtc-2020-rtx-accelerated-hair-brought-to-life-with-nvidia-iray,"GTC 2020 S22494
Presenters: Carsten Waechter,NVIDIA
Abstract
We’ll present our learnings from implementing hair (or all kinds of fibers in general) with support within the NVIDIA Iray rendering and light transport simulation engine. This also includes adding support for RTX-accelerated curve rendering, leveraging upcoming OptiX 7 support. We’ll also give a short overview of how we added RTX/OptiX 7 support to Iray and the resulting performance improvements we received from it.Watch this session
Join in the conversation below.Hi and welcome! Hope you enjoyed the presentation, now bring on your questions!Powered by Discourse, best viewed with JavaScript enabled"
3313,teaching-a-self-driving-car-to-follow-a-lane-in-under-20-minutes,"Originally published at:			https://developer.nvidia.com/blog/teaching-a-self-driving-car-to-follow-a-lane-in-under-20-minutes/
Developers at Wayve, a UK-based startup, recently taught a car to perform autonomous vehicle functions from scratch in under 20 minutes, resulting in the first application of reinforcement learning on a full-sized autonomous vehicle. “In order to make autonomous driving a truly ubiquitous technology, we advocate for robotic systems which address the ability to drive…Powered by Discourse, best viewed with JavaScript enabled"
3314,nvidia-releases-jetson-xavier-nx-developer-kit-with-cloud-native-support,"Originally published at:			https://developer.nvidia.com/blog/nvidia-releases-jetson-xavier-nx-developer-kit-with-cloud-native-support/
NVIDIA today announced availability of the NVIDIA Jetson Xavier NX developer kit with cloud-native support — and the extension of this support to the entire NVIDIA Jetson edge computing lineup for autonomous machines. The Jetson Xavier NX module is the ideal platform to accelerate AI applications, delivering greater than 10x higher performance compared to its widely adopted…Powered by Discourse, best viewed with JavaScript enabled"
3315,dji-launches-gpu-based-high-performance-embedded-computer-for-drones,"Originally published at:			https://developer.nvidia.com/blog/dji-launches-gpu-based-high-performance-embedded-computer-for-drones-2/
Leading camera drone manufacturer DJI unveiled their new NVIDIA Tegra TK1-powered “Manifold” embedded computer this week. Now, developers can transform aerial platforms into truly intelligent flying robots that perform complex computing tasks and advanced image processing in the sky. “Designed for developers, the Manifold’s built-in Ubuntu operating system supports CUDA, OpenCV, and ROS,” DJI said.…Powered by Discourse, best viewed with JavaScript enabled"
3316,decreasing-mri-scan-times-using-deep-learning-with-nvidia-clara-agx,"Originally published at:			https://developer.nvidia.com/blog/decreasing-mri-scan-times-using-deep-learning-with-nvidia-clara-agx/
An intern on the NVIDIA Clara AGX team gives an overview on a deep learning method to remove noise and the Gibbs phenomenon in magnetic resonance imaging (MRI). She discusses how this method could allow for reduced scan times in MRI.Read article about decreasing the effects of the Gibbs phenomenon in MRI.Before I came across an article that showed how to compensate for the data shift effect in the fft, I
was considering using ML to obtain a signed magnitude spectra from the complex fft data.
This for any arbitrary input vector, might you have
attempted similar.We haven’t tried that before, but sounds like an interesting approach.Powered by Discourse, best viewed with JavaScript enabled"
3317,developing-streaming-sensor-applications-with-holohub-from-nvidia-holoscan,"Originally published at:			https://developer.nvidia.com/blog/developing-streaming-sensor-applications-with-holohub-from-nvidia-holoscan/
The average car contains over 100 sensors to both monitor and respond to vital information. Ranging from an overheating engine to low tire pressure and erratic steering, sensors exist to provide automatic data, insight, and control. They make the act of driving safer for both the passengers and everyone else on the road. Of course,…Powered by Discourse, best viewed with JavaScript enabled"
3318,transforming-paintings-and-photos-into-animations-with-ai,"Originally published at:			Transforming Paintings and Photos Into Animations With AI | NVIDIA Technical Blog
Researchers from the University of Washington and Facebook recently released a paper that shows a deep learning-based system that can transform still images and paintings into animations. The algorithm called Photo Wake-Up uses a convolutional neural network to animate a person or character in 3D from a single still image. “Our method works with a large…Powered by Discourse, best viewed with JavaScript enabled"
3319,dask-tutorial-beginners-guide-to-distributed-computing-with-gpus-in-python,"Originally published at:			https://developer.nvidia.com/blog/dask-tutorial-beginners-guide-to-distributed-computing-with-gpus-in-python/
This is the fourth installment of the series of introductions to the RAPIDS ecosystem. The series explores and discusses various aspects of RAPIDS that allow its users solve ETL (Extract, Transform, Load) problems, build ML (Machine Learning) and DL (Deep Learning) models, explore expansive graphs, process geospatial, signal, and system log data, or use SQL…Powered by Discourse, best viewed with JavaScript enabled"
3320,reusable-computational-patterns-for-machine-learning-and-data-analytics-with-rapids-raft,"Originally published at:			https://developer.nvidia.com/blog/reusable-computational-patterns-for-machine-learning-and-data-analytics-with-rapids-raft/
In many data analytics and machine learning algorithms, computational bottlenecks tend to come from a small subset of steps that dominate the end-to-end performance. Reusable solutions for these steps often require low-level primitives that are non-trivial and time-consuming to write well. NVIDIA made RAPIDS RAFT to address these bottlenecks and maximize reuse when building algorithms…Powered by Discourse, best viewed with JavaScript enabled"
3321,bringing-scale-to-the-edge-with-multi-access-edge-computing,"Originally published at:			https://developer.nvidia.com/blog/bringing-scale-to-the-edge-with-multi-access-edge-computing/
Multi-access edge computing (MEC) is the telco-centric approach to delivering edge computing by integrating it with fixed and mobile access networks. MEC is often used interchangeably to mean edge computing. But is this appropriate? And how does MEC relate to edge computing?Powered by Discourse, best viewed with JavaScript enabled"
3322,gtc-2020-sharing-physically-based-materials-between-renderers-with-mdl,"GTC 2020 S21220
Presenters: Lutz Kettner,NVIDIA; Jan Jordan,NVIDIA
Abstract
We’ll discuss the basics of NVIDIA’s Material Definition Language, showing how a single material can be used to define matching appearances between different renderers and rendering techniques. Learn how physically based definitions can be defined, and what’s entailed in supporting MDL within products or renderers.Watch this session
Join in the conversation below.Powered by Discourse, best viewed with JavaScript enabled"
3323,building-a-simple-ai-assistant-with-deeppavlov-and-nvidia-nemo,"Originally published at:			https://developer.nvidia.com/blog/building-a-simple-ai-assistant-with-deeppavlov-and-nemo/
In the past few years, voice-based interaction has become a feature of many industrial products. Voice platforms like Amazon Alexa, Google Home, Xiaomi Xiaz, Yandex Alice, and other in-home voice assistants provide easy-to-install, smart home technologies to even the least technologically savvy consumers. The fast adoption and rising performance of voice platforms drive interest in…Powered by Discourse, best viewed with JavaScript enabled"
3324,join-the-virtual-monai-bootcamp-sept-22-24,"Originally published at:			https://developer.nvidia.com/blog/join-the-virtual-monai-bootcamp-sept-22-24/
Apply for the Sept. 22-24 MONAI virtual bootcamp featuring presentations, hands-on labs, and a mini-challenge day.Powered by Discourse, best viewed with JavaScript enabled"
3325,introducing-jetson-xavier-nx-the-world-s-smallest-ai-supercomputer,"Originally published at:			Introducing Jetson Xavier NX, the World’s Smallest AI Supercomputer | NVIDIA Technical Blog
Figure 1: Jetson Xavier NX delivers up to 21 TOPS of compute under 15W of power, or up to 14 TOPS of compute under 10W. Today NVIDIA announced Jetson Xavier NX, the world’s smallest, most advanced embedded AI supercomputer for autonomous robotics and edge computing devices. Capable of deploying server-class performance in a compact 70x45mm…""Supercomputer""You keep using that word, I do not think it means what you think it means.Any word on a DevKit version with a carrier board? Hopefully a compact, ready-to-prototype version like the AGX (only smaller)?Also noted that the vision accelerator was left out of the NX. Was this even ever enabled in JetPack for the AGX? If not, I guess this likely means it won't be, and thus it's removal isn't of consequence.Hi Paul, we don't have any information to share about the devkit at this time, however for now you can get started with the emulation package for the Jetson AGX Xavier Developer Kit and the Jetson Nano reference carrier hardware design (Xavier NX is pin-compatible with Nano).Ah, I totally missed that in the post. Cool. So theoretically just swap out the module on one of my nano carriers for ""cheap"" dev/prototype.serialized CudaEngine built by jetson nano can run on Xavier NX?Powered by Discourse, best viewed with JavaScript enabled"
3326,supporting-low-latency-streaming-video-for-ai-powered-medical-devices-with-clara-holoscan,"Originally published at:			Supporting Low-Latency Streaming Video for AI-Powered Medical Devices with Clara Holoscan | NVIDIA Technical Blog
NVIDIA Clara Holoscan and Clara AGX Developer Kit accelerates development of AI for endoscopy, laparoscopy, and other surgical procedures.Powered by Discourse, best viewed with JavaScript enabled"
3327,nvidia-dlss-natively-supported-in-unity-2021-2,"Originally published at:			NVIDIA DLSS Natively Supported in Unity 2021.2 | NVIDIA Technical Blog
Unity made real-time ray tracing available to all of their developers in 2019 with the release of 2019LTS. Before the end of 2021, NVIDIA DLSS  will be natively supported for HDRP in Unity 2021.2.Powered by Discourse, best viewed with JavaScript enabled"
3328,ask-me-anything-experts-answer-your-nvidia-cugraph-questions-live,"Originally published at:			AMA cuGraph: Graph analysis and GNN - NVIDIA Developer Forums
Join us April 12 and ask experts about NVIDIA cuGraph, which recently added support for GNN with accelerated aggregators, models, and extensions to DGL and PyG.Powered by Discourse, best viewed with JavaScript enabled"
3329,nvidia-isaac-ros-delivers-ai-perception-to-ros-developers,"Originally published at:			https://developer.nvidia.com/blog/nvidia-isaac-ros-delivers-ai-perception-to-ros-developers/
Perceiving and understanding the surrounding world is a critical challenge for autonomous robots. In conjunction with ROS World 2021, NVIDIA announced its latest efforts to deliver performant perception technologies to the ROS developer community. These initiatives will accelerate product development, improve product performance, and ultimately simplify the task of incorporating cutting-edge computer vision and AI/ML…Powered by Discourse, best viewed with JavaScript enabled"
3330,nvidia-research-auditing-ai-models-for-verified-deployment-under-semantic-specifications,"Originally published at:			https://developer.nvidia.com/blog/nvidia-research-auditing-ai-models-for-verified-deployment-under-semantic-specifications/
○	Quality assurance and audits are necessary for deep learning models. Current AI models require large data sets for training or a designed reward function that must be optimized. Algorithmically, AI is prone to optimizing behaviors that were not intended by the human designer. To help combat this, the AuditAI framework was developed to help audit these problems, which increases safety and ethical use of deep learning models during deployment.Do deep learning models need auditing? Find out about AuditAI and see how you can benefit from QA for your AI model.What is the link for additional information?
“For more information, see Auditing AI models for Verified Deployment under Semantic Specifications [LINK].”Good catch, @medgar! The paper is still under review, so I’ll add that link when the paper’s available online.The paper with technical details is now available:Auditing trained deep learning (DL) models prior to deployment is vital for
preventing unintended consequences. One of the biggest challenges in auditing
is the lack of human-interpretable specifications for the DL models that are
directly useful to...@medgar and @animeshg, the post is now updated with the link. Thanks!Powered by Discourse, best viewed with JavaScript enabled"
3331,scaling-inference-in-high-energy-particle-physics-at-fermilab-using-nvidia-triton-inference-server,"Originally published at:			Scaling Inference in High Energy Particle Physics at Fermilab Using NVIDIA Triton Inference Server | NVIDIA Technical Blog
High-energy physics research aims to understand the mysteries of the universe by describing the fundamental constituents of matter and the interactions between them. Diverse experiments exist on Earth to re-create the first instants of the universe. Two examples of the most complex experiments in the world are at the Large Hadron Collider (LHC) at CERN…Powered by Discourse, best viewed with JavaScript enabled"
3332,gtc-2020-inside-nvidias-multi-instance-gpu-feature,"GTC 2020 S21975
Presenters: Jay Duluk,NVIDIA; Piotr Jaroszynski, NVIDIA
Abstract
NVIDIA’s latest GPUs have an important new feature: Multi-Instance GPU (MIG). MIG allows large GPUs to be effectively divided into multiple instances of smaller GPUs. The primary benefit of the MIG feature is increasing GPU utilization by enabling the GPU to be efficiently shared by unrelated parallel compute workloads on bare metal, GPU pass-through, or on multiple vGPUs.Watch this session
Join in the conversation below.I am wondering why do we have to disable “L2 residency” feature when enable MIG?Powered by Discourse, best viewed with JavaScript enabled"
3333,new-drive-os-and-driveworks-updates-enable-streamlined-av-software-development,"Originally published at:			https://developer.nvidia.com/blog/drive-os-driveworks-updates-streamlined-av-software/
DRIVE OS and DriveWorks releases are now available on NVIDIA DRIVE Developer, providing DRIVE OS users access to DriveWorks middleware and even more updates.Powered by Discourse, best viewed with JavaScript enabled"
3334,finite-difference-methods-in-cuda-fortran-part-2,"Originally published at:			https://developer.nvidia.com/blog/finite-difference-methods-cuda-fortran-part-2/
CUDA Fortran for Scientists and Engineers shows how high-performance application developers can leverage the power of GPUs using Fortran. In the last CUDA Fortran post we dove in to 3D finite difference computations in CUDA Fortran, demonstrating how to implement the x derivative part of the computation. In this post, let’s continue by exploring how we…Hi, I coudn't understand why you chose that block dimensions. What does a perfect memory coalescing mean? Thanks.Suggest you read this. https://devblogs.nvidia.com...Thanks!Now, why is necessary the instruction below?do j = threadIdx%y, my, blockDim%y   f_s(i_l,j) = f(i,j,k)enddoWhat's the difference between this and:j = (blockIdx%y-1)*blockDim%y + threadIdx%yf_s(i_l,j) = f(i,j,k) ?I created a global memory version with:The results using double precision are:Using shared memory tile of x-y:64x4 RMS error:   0.3380241952306925      MAX error:    1.852001381073753      Average time (ms):    6.2668800E-02 Average Bandwidth (GB/s):     66.92810Using global memory tile of x-y:64x4 RMS error:   0.3380245069407786      MAX error:    1.852001381073753      Average time (ms):    5.1046401E-02 Average Bandwidth (GB/s):     82.16650Why are the RMS error different between these versions?attributes(global) subroutine deriv_x_global(f,df)      implicit nonereal(fp_kind), intent(in)  :: f(mx,my,mz)      real(fp_kind), intent(out) :: df(mx,my,mz)      integer  :: i,j,k      i  =  threadIdx%x      j  =  (blockIdx%x-1)*blockDim%y+threadIdx%y      k  =  blockIdx%yif (i>3 .and. i<mx-3) then="""" (...)="""" end="""" if="""">The do loop is required because the code uses a 32x8x1 thread block to load a 32x64x1 tile of data into shared memory, so each thread loads 8 elements of the tile.The statements:j = (blockIdx%y-1)*blockDim%y + threadIdx%yf_s(i_l,j) = f(i,j,k)are used when there is a one-to-one mapping of threads to elements in the tile.Powered by Discourse, best viewed with JavaScript enabled"
3335,maximizing-performance-with-massively-parallel-hash-maps-on-gpus,"Originally published at:			https://developer.nvidia.com/blog/maximizing-performance-with-massively-parallel-hash-maps-on-gpus/
Learn the fundamentals of hash maps and how their memory access patterns make them well suited for GPU acceleration.Powered by Discourse, best viewed with JavaScript enabled"
3336,structured-sparsity-in-the-nvidia-ampere-architecture-and-applications-in-search-engines,"Originally published at:			https://developer.nvidia.com/blog/structured-sparsity-in-the-nvidia-ampere-architecture-and-applications-in-search-engines/
Deep learning is achieving significant success in various fields and areas, as it has revolutionized the way we analyze, understand, and manipulate data. There are many success stories in computer vision, natural language processing (NLP), medical diagnosis and health care, autonomous vehicles, recommendation systems, and climate and weather modeling. In an era of ever-growing neural…Powered by Discourse, best viewed with JavaScript enabled"
3337,bmw-group-selects-nvidia-to-redefine-factory-logistics,"Originally published at:			https://developer.nvidia.com/blog/bmw-group-selects-nvidia-to-redefine-factory-logistics/
BMW Group has selected the new NVIDIA Isaac robotics platform to enhance its automotive factories — utilizing logistics robots built on advanced AI computing and visualization technologies, the companies announced today. The collaboration centers on implementing an end-to-end system based on NVIDIA technologies — from training and testing through to deployment — with robots developed using one software architecture,…Powered by Discourse, best viewed with JavaScript enabled"
3338,ai-models-recap-scalable-pretrained-models-across-industries,"Originally published at:			https://developer.nvidia.com/blog/ai-models-recap-scalable-pretrained-models-across-industries/
The year 2022 has thus far been a momentous, thrilling, and an overwhelming year for AI aficionados. Get3D is pushing the boundaries of generative 3D modeling, an AI model can now diagnose breast cancer from MRIs as accurately as board-certified radiologists, and state-of-the-art speech AI models have widened their horizons to extended reality. Pretrained models…Powered by Discourse, best viewed with JavaScript enabled"
3339,scaling-large-graphs,"I created an analytic on a small graph, how easy is it to scale my analytic to large graph?At the python layer, cuGraph leverages dask to distribute the work/data accross multiple GPUs. For that transition, one has to setup the dask cluster and the MG API almost matches the SG one. Checkout more here: Multi-GPU with cuGraph — cugraph 23.02.00 documentationThis topic was automatically closed 14 days after the last reply. New replies are no longer allowed.Powered by Discourse, best viewed with JavaScript enabled"
3340,power-your-big-data-analytics-with-the-latest-nvidia-gpus-in-the-cloud,"Originally published at:			Power Your Big Data Analytics with the Latest NVIDIA GPUs in the Cloud | NVIDIA Technical Blog
Dask is an accessible and powerful solution for natively scaling Python analytics. Using familiar interfaces, it allows data scientists familiar with PyData tools to scale big data workloads easily. Dask is such a powerful tool that we have adopted it throughout a variety of projects at NVIDIA. When paired with RAPIDS, data practitioners can distribute…Powered by Discourse, best viewed with JavaScript enabled"
3341,ray-tracing-essentials-part-6-the-rendering-equation,"Originally published at:			Ray Tracing Essentials Part 6: The Rendering Equation | NVIDIA Technical Blog
NVIDIA recently published Ray Tracing Gems, a deep-dive into best practices for real-time ray tracing. The book was made free-to-download, to help all developers embrace the bleeding edge of rendering technology. Ray Tracing Essentials is a seven-part video series hosted by the editor of Ray Tracing Gems, NVIDIA’s Eric Haines. The aim of this program…Powered by Discourse, best viewed with JavaScript enabled"
3342,how-language-neutralization-is-transforming-customer-service-contact-centers,"Originally published at:			https://developer.nvidia.com/blog/how-language-neutralization-is-transforming-customer-service-contact-centers/
The powerful language neutralization offered by Infosys Cortex and based on NVIDIA Riva speech and translation enables contact center agents to communicate effectively with customers.Powered by Discourse, best viewed with JavaScript enabled"
3343,realistic-lighting-in-justice-with-mesh-shading,"Originally published at:			https://developer.nvidia.com/blog/realistic-lighting-in-justice-with-mesh-shading/
NetEase Thunder Fire Games Uses Mesh Shading To Create Beautiful Game Environments for Justice In December, we interviewed Haiyong Qian, NetEase Game Engine Development Researcher and Manager of NetEase Thunder Fire Games Technical Center, to see what he’s learned as the Justice team added NVIDIA ray-tracing solutions to their development pipeline. Their results were nothing…Fantastic! :)Are these 1.8 Billion only the highest LOD or all LODS added together? :DHi! 1.8 billion is only the highest LOD.Fantastic!, thanks for the response! Do you think it’s possible with 2 billion?, where there 1.8 billion on screen at once or on the map in total? also do you know if it’s possible with all types of meshes or only static? :)It is definitely impossible to render 2 billion triangles on screen without any sort of LOD, but with LOD I think 2 billion on screen at once is possible. The complexity is only dependent on screen resolution approximately.
Currently we only support static meshes. It may be possible to render dynamic meshes with mesh shading efficiently, but culling will be a difficult part since dynamic meshes have no fixed bounding box or something similar.Powered by Discourse, best viewed with JavaScript enabled"
3344,antarctic-observatory-data-ray-tracing-advance-astrophysics-research,"Originally published at:			Antarctic Observatory Data, Ray Tracing Advance Astrophysics Research | NVIDIA Technical Blog
Scientists from the Wisconsin IceCube Particle Astrophysics Center are using ray tracing on NVIDIA GPUs to accelerate simulations of subatomic particles by hundreds of times.  Using NVIDIA RTX GPUs in a subsystem of the Texas Advanced Computing Center’s Frontera supercomputer, the researchers can calculate the path of light as it travels through a one cubic…Powered by Discourse, best viewed with JavaScript enabled"
3345,accelerating-dissipative-particle-dynamics-simulation-on-tesla-gpus,"Originally published at:			https://developer.nvidia.com/blog/accelerating-dissipative-particle-dynamics-simulation-tesla-gpus/
As you are probably aware, CUDA 7 was officially released during the 2015 GPU Technology Conference. For this Spotlight I took a few minutes to pick the brain of an early adopter of CUDA 7 to see how his work benefits from the new C++11 support. I interviewed Yu-Hang Tang, a Ph.D. candidate in the Division of Applied…Powered by Discourse, best viewed with JavaScript enabled"
3346,continuously-improving-recommender-systems-for-competitive-advantage-using-nvidia-merlin-and-mlops,"Originally published at:			https://developer.nvidia.com/blog/continuously-improving-recommender-systems-for-competitive-advantage-with-merlin-and-mlops/
Recommendation systems must constantly evolve through the digestion of new data or algorithmic improvements of the model for its recommendations to stay effective and relevant. In this post, we focus on how NVIDIA Merlin components fit into a complete MLOps pipeline to operationalize a recommendation system, and continuously deliver improvements in productionWith this post, we aim to address how developers can build a performant recommendation system with NVIDIA Merlin employing MLOps tools and practices. This solution and blog post is aligned with the GTC Spring '21 talk “Gain Competitive Advantage using ML Ops: Kubeflow and NVIDIA Merlin and Google Cloud”, please check that out for more context, as well as for the demo video!Hello, I’d like to check if there’s some published work on implementing the complete Merlin pipeline for a recommender system in production using AWS. I have found a blog that details that implementation on Google Cloud and I wonder if there’s a published implementation on AWS.ThanksPowered by Discourse, best viewed with JavaScript enabled"
3347,enabling-scalable-user-experiences-with-modern-workloads-on-windows-virtual-desktop,"Originally published at:			Enabling Scalable User Experiences with Modern Workloads on Windows Virtual Desktop | NVIDIA Technical Blog
Windows Virtual Desktop If you’re supporting the recent influx in remote work, you’ve probably noticed that business applications are more graphics-heavy than ever before. Applications such as Microsoft Office, Google Chrome, and PDF readers now offer graphics-rich features that require more power. In addition, 4K and multiple high-resolution monitors, as well as multimedia streaming, are…Powered by Discourse, best viewed with JavaScript enabled"
3348,announcing-nvidia-riva-combining-speech-vision-and-other-sensors-into-one-ai-sdk,"Originally published at:			https://developer.nvidia.com/blog/announcing-nvidia-riva/
NVIDIA Co-founder and CEO Jensen Huang announced NVIDIA Riva - an SDK for building and deploying AI applications that fuse vision, speech, and other sensors into one system.Powered by Discourse, best viewed with JavaScript enabled"
3349,deep-visualization-with-the-caffe-framework,"Originally published at:			https://developer.nvidia.com/blog/deep-visualization-with-the-caffe-framework/
University of Wyoming’s Evolving Artificial Intelligence Laboratory have been using the power of NVIDIA Tesla GPUs to accelerate their research since 2012. The Lab, which focuses on evolving artificial intelligence with a major focus on large-scale, structurally organized neural networks, has garnered press from some of the largest media outlets, including BBC, National Geographic, NBC…Powered by Discourse, best viewed with JavaScript enabled"
3350,minimizing-storage-usage-on-jetson,"Originally published at:			https://developer.nvidia.com/blog/minimizing-storage-usage-on-jetson/
Some NVIDIA Jetson modules have limited storage space, which imposes a challenge in packing applications and libraries. Here are ways to cut down on disk usage.This breaks everything. Tried to install python3.8 packages, got this:Impossible to deploy our applications here (currently using internal limited emmc memory as stated in your example).@jwitsoe how do we use minimized configuration for adding wifi network auto-connection to the filesystem of e.g. jetson sdcard without OS booted?Hi @Andrey1984,
Unfortunately, there is no specifically built scheme to do that like how it is on RPi.
If it is using Network Manager, one may be able to manually create the .nmconnection file under /etc/NetworkManager/system-connections/, but I’m not sure how default user creation through the OEM-config affects it.For our reference, if possible, would you explain what situation you think such wifi auto-connection configuration scheme is needed?Hi @cyato
Thank you for your reply.
I think I have tried just copying existing /etc/NetworkManager/system-connections/  to nvidia supplied sdcard image filesystem after writing it to sdcard. It did not seem that the robot appeared on the wifi network after the boot. Maybe more files need to be edited or copied?Our use case:Remote team has a robot that got jetson nx devkit deep inside.
Kind of no serial or usb or other cables or ports are accessible of the nx devkit.
The only somehow accessible  thing is  sdcard slot. The robot does need to boot from sdcard
The need is to prepare sdcard at Host PC in a way, so once it is added to the robot the robot on the first boot can be accessed via wifi already. As otherwise there is no way to access it at all.Moreover, people in remote factory [ robot deployment location] can not  do more complex action than to insert sdcard to HostPC or to nx devkit[robot] in case OS needs to be deployed. The rest is remote operation.Reference findings include but are not limited to listed below:Hi @Andrey1984,
Thank you for explaining your use-case.How about setting up another Xavier NX Developer Kit by flashing the NVIDIA-provided SD card image, setting up the WLAN connection, then making the whole image of the SD card?
You can flash another microSD card using the image, send the new microSD card to your remote team, and let them boot the robot with the new microSD card, and it should connect to the wireless AP on site.When you download the NVIDIA-provided SD card image for the Xavier NX Developer Kit, be sure to select the same version of JetPack as the one that is currently running on the robot.If you need guidance on cloning the SD card, please let us know.I think I have tried just copying existing /etc/NetworkManager/system-connections/ to nvidia supplied sdcard image filesystem after writing it to sdcard. It did not seem that the robot appeared on the wifi network after the boot. Maybe more files need to be edited or copied?Thank you also for providing the result of what you tested.
According to this post, the file actually contains the MAC address of the NIC of the unit, so that needs to be replaced.
A hack would be to perform this sed operation on the first boot.Hi @cyato
Thank you for pointing out the sdcard clone method. We used it. It is known.However, it is not easily remotely applicable due to existing constraints.We are trying to figure out how to add wifi entry using nvidia sdcard image filesystem edits, or rootfs edits / BSP edits specifically. Without a need to use a second nx devkit unit. So that all edits ca be done at Host PC already , then written to sdcard. So that sdcard  can be used for one-time-bringup of a robot once added to nx devkit without a need to interface anyhow with the system but for wireless connection.
Like as it is implemented in rpi wpa_supplicant or jetson sdcard for duckietown wpa_supplicant. So that just after editing files on the sdcard filesystem first boot reqults in connection already. Without need to use second reference hardware devkit somehow.Could you confirm that you were able to combine the sed + copying the system-connection so that sdcard does boot up with connection activated already if using the nvidia supplied sdcard image for nx devkit?Powered by Discourse, best viewed with JavaScript enabled"
3351,gtc-2020-medical-volume-raytracing-in-virtual-reality,"GTC 2020 S22030
Presenters: Jeroen Stinstra,NVIDIA
Abstract
Raytracing voxels in medical images is a relatively new technique that lets us see medical images in a new light. It allows for creating realistic-looking light effects, like soft shadows. Exploring medical images in virtual reality benefits because these advances make objects look more real, making it easier for a physician to interpret what they are seeing. We’ll discuss the challenges with doing volume raytracing in VR, and will demonstrate a solution in CUDA involving rendering to an eye-tracked foveated/warped space. We’ll discuss how we can stream this warped space from a server to a head-mounted display, and how to effectively de-noise the results for VR. We’ll also show how to use the extra GPU budget that we created by foveated/warped rendering to improve visuals by including better material choices based on DL-generated label maps and define less stair-stepped implicit surfaces based on tricubic interpolation.Watch this session
Join in the conversation below.StinstraThank you very much for this excellent presentation Dr. Stinstra. I really enjoyed learning so many technical details on you voxel ray tracer.
I have one question, how did you implement the voxel acceleration structure for maximum efficiency?
Have a nice day!Powered by Discourse, best viewed with JavaScript enabled"
3352,new-resource-for-developers-access-technical-content-through-nvidia-on-demand,"Originally published at:			New Resource for Developers: Access Technical Content through NVIDIA On-Demand | NVIDIA Technical Blog
Now available to all NVIDIA developers, NVIDIA On-Demand is a catalog of technical sessions, podcasts, past keynotes, demos, research posters, and more from NVIDIA GPU Technology Conferences across the global, as well as leading industry events. This will enable developers to learn at their own time, at their own pace, anywhere. Through NVIDIA On-Demand, developers…Powered by Discourse, best viewed with JavaScript enabled"
3353,predicting-how-images-influence-visual-reaction-speed,"Originally published at:			Predicting How Images Influence Visual Reaction Speed | NVIDIA Technical Blog
When it comes to pushing the boundaries of human performance in virtual environments, understanding visual reaction speed can unlock significant improvements.Powered by Discourse, best viewed with JavaScript enabled"
3354,leverage-3d-geospatial-data-for-immersive-environments-with-cesium,"Originally published at:			Leverage 3D Geospatial Data for Immersive Environments with Cesium | NVIDIA Technical Blog
With Cesium for Omniverse, you can jump-start 3D geospatial app development with tiling pipelines for streaming your own content.Powered by Discourse, best viewed with JavaScript enabled"
3355,artificial-intelligence-helps-grade-exams-90-faster,"Originally published at:			Artificial Intelligence Helps Grade Exams 90% Faster | NVIDIA Technical Blog
Four UC Berkeley researchers developed a program to help grade papers during their time working as teaching assistants – and now, they’ve added artificial intelligence to their app to help instructors speed up the grading process. The team launched the online grading app Gradescope two years ago and have accumulated 10 million answers to around…Powered by Discourse, best viewed with JavaScript enabled"
3356,building-and-deploying-a-face-mask-detection-application-using-ngc-collections,"Originally published at:			https://developer.nvidia.com/blog/building-and-deploying-a-face-mask-detection-application-using-ngc-collections/
AI workflows are complex. Building an AI application is no trivial task, as it takes various stakeholders with domain expertise to develop and deploy the application at scale. Data scientists and developers need easy access to software building blocks, such as models and containers, that are not only secure and highly performant, but which have…Hello and thanks for the information.I’m trying to recreate the code in my environment but I cannot find any clue to set the following parameters:–category-limit $CATEGORY_LIMIT 
–tlt-input-dims_width $TLT_INPUT_DIMS_WIDTH
–tlt-input-dims_height $TLT_INPUT_DIMS_HEIGHT \I will really appreciate some guidance about this or any point out to a reference.Thanks in advance!Great question! We actually go into more detail about the input-dims parameter in this blog which is definitely worth checking out if you’re interested.Determines the number of inference categories to display in DeepStream.Tells TLT what resolution to use. So while I can’t give you an absolute answer without knowing what camera/feed/input sensor/dataset you’re using, you should try updating them to match the height/width of your images/feed.The full docs are here.Dear jwitsoe,I followed the above blog to train Face Mask model  and it is working fine with deepstream. But I tried to make a study code to inference this model after using tlt_convert to convert to egine .
For processing output , I tried to follow jetson-inference/detectNet.cpp at master · dusty-nv/jetson-inference · GitHub to process as caffe model:
// clusterDetections (caffe)
int detectNet::clusterDetections( Detection* detections, uint32_t width, uint32_t height )
{
// cluster detection bboxes
float* net_cvg   = mOutputs[OUTPUT_CVG].CPU;
float* net_rects = mOutputs[OUTPUT_BBOX].CPU;#ifdef DEBUG_CLUSTERING	
LogDebug(LOG_TRT “input width %i height %i\n”, (int)DIMS_W(mInputDims), (int)DIMS_H(mInputDims));
LogDebug(LOG_TRT “cells x %i  y %i\n”, ow, oh);
LogDebug(LOG_TRT “cell width %f  height %f\n”, cell_width, cell_height);
LogDebug(LOG_TRT “scale x %f  y %f\n”, scale_x, scale_y);
#endif}But it doesn’t work. The outpout value is very small.
How can I treat with output_bbox/BiasAdd (8, 34, 60) output_cov/Sigmoid (2, 34, 60).
Thank for your support!
Thuy@thuy.hoang19 ,
Your question is actually talking about how to run inference with the tensorrt engine.
Officially, please see Integrating TAO CV Models with Triton Inference Server — TAO Toolkit 3.22.05 documentation and then leverage tao-toolkit-triton-apps/detectnet_processor.py at main · NVIDIA-AI-IOT/tao-toolkit-triton-apps · GitHub and tao-toolkit-triton-apps/utils.py at fc7e222c036354498e53a8ed11b5cf7c0a3e5239 · NVIDIA-AI-IOT/tao-toolkit-triton-apps · GitHubAlso, there are similar topics shared by other customers in TAO fourm.@Morganh : Thank you~Powered by Discourse, best viewed with JavaScript enabled"
3357,the-nvidia-optix-sdk-release-7-2-adds-ai-denoiser-layer-support-and-demand-loaded-sparse-textures,"Originally published at:			The NVIDIA OptiX SDK Release 7.2 Adds AI Denoiser Layer Support, and Demand-Loaded Sparse Textures | NVIDIA Technical Blog
NVIDIA OptiX is an API for optimal ray tracing performance on the GPU that is used for film and video production as well as many other professional graphics applications. OptiX SDK 7.2 is the latest update to the new OptiX 7 API. This version introduces API changes to the OptiX denoiser to support layered AOV…Powered by Discourse, best viewed with JavaScript enabled"
3358,cvpr-2020-the-robotics-factory-of-future,"CVPR 2020 dcv10
Presenters: Tech Demo Team, NVIDIA
Abstract
NVIDIA Isaac Sim is the industry’s first robotic AI development platform with simulation, navigation and manipulation.Watch this session
Join in the conversation below.Hi there please help me running the Isaac factory of the future simulation?
I have already opened a thread here:
please guide me if i am missing something?
regardsPowered by Discourse, best viewed with JavaScript enabled"
3359,controlled-adaption-of-speech-recognition-models-to-new-domains,"Originally published at:			https://developer.nvidia.com/blog/controlled-adaption-of-speech-recognition-models-to-new-domains/
Using adapters for parameter-efficient training reduces the effects of catastrophic forgetting of general speech recognition.Powered by Discourse, best viewed with JavaScript enabled"
3360,machine-learning-in-practice-ml-workflows,"Originally published at:			https://developer.nvidia.com/blog/machine-learning-in-practice-ml-workflows/
This series looks at the development and deployment of machine learning (ML) models. This post gives an overview of the ML workflow, considering the stages involved in using machine learning and data science to deliver business value. In part 2, you train an ML model and save that model so it can be deployed as…Powered by Discourse, best viewed with JavaScript enabled"
3361,oak-ridge-national-laboratory-scientists-identify-compounds-that-could-shed-more-light-on-the-coronavirus,"Originally published at:			Oak Ridge National Laboratory Scientists Identify Compounds That Could Shed More Light on the Coronavirus | NVIDIA Technical Blog
To help respond to the COVID-19 coronavirus outbreak, researchers at the Oak Ridge National Laboratory (ORNL) are using the world’s fastest supercomputer to identify compounds that may effectively combat the virus.  Using Summit, which is powered by 9,216 IBM Power9 CPUs and over 27,000 NVIDIA V100 Tensor Core GPUs, the researchers identified 77 small-molecule drug…Powered by Discourse, best viewed with JavaScript enabled"
3362,nvidia-at-tensorflow-world-2019,"Originally published at:			NVIDIA at TensorFlow World 2019 | NVIDIA Technical Blog
From TensorFlow 2.0 and TensorRT, to using automatic mixed precision for better training performance, to running the latest ASR models in production on NVIDIA GPUs, learn how NVIDIA GPUs and TensorFlow are helping developers dramatically accelerate their AI-based applications.  From October 28-31, Join NVIDIA at TensorFlow World 2019 at Santa Clara, California for insights and…Powered by Discourse, best viewed with JavaScript enabled"
3363,discover-new-cuda-11-4-features,"Originally published at:			https://developer.nvidia.com/blog/discover-new-cuda-11-4-features/
The new release consists of GPU-accelerated libraries, debugging and optimization tools, an updated C/C++ compiler, and a runtime library to build and deploy your application on major architectures.I want to read CUDA Driver enhancement in GPUDirect RDMA.But I cannot found description on Relase Notes.The Release Notes for the CUDA Toolkit.Would you point the changes on CUDA Driver enhancement about GPUDirect RDMA?With respect - what’s the deal with the silly post?Discover New CUDA 11.4 FeaturesYou go there, and it doesn’t tell you anything about those new features.It says there are “enhancements to this” and “enhancements to that”. Woopdee-doo… doesn’t tell me anything.Hi Sakaia, please let me know if this helps address your question:The API reference guide for enabling GPUDirect RDMA connections to NVIDIA GPUs.Thanks,
FredUPDATE: Hi epk, hope you’re able to get more from the new CUDA 11.4 blog we just published: https://developer.nvidia.com/blog/discovering-new-features-in-cuda-11-4/. There is also some great information on using Stream-ordered Memory Allocation APIs here: https://developer.nvidia.com/blog/using-the-nvidia-cuda-stream-ordered-memory-allocator-part-1/Hi EPK, yes, you’re right. Apologies that the CUDA 11.4 DevBlog with more details is delayed a little. I’ll be sure to provide you the link as soon as it goes live early next week.Thx!
FredThank you for your suggestion.I found the changes.The API reference guide for enabling GPUDirect RDMA connections to NVIDIA GPUs.That’s great to know. Thanks for your patience. :)what mean about "" The driver also enables new MIG configurations for the recently launched NVIDIA A30 GPU to double the amount of memory per MIG slice"",maybe one slice have 12G memory?but I don’t find this.When will cuDNN 8.2.2 be released to go alongside CUDA 11.4?I am also wondering if the accompanying libraries are released soon as well, like cuDNN and maybe TensorRT 8?We are setting up some infrastructure right now, and would like to avoid duplicate work if new releases are coming next week or so.I am also wondering if the accompanying libraries are released soon as well, like cuDNN and maybe TensorRT 8?Apologies for the delayed response, as I was trying to track down an answer to your question. Turns out, our policy is not to discuss future product releases. Please reach out to your sales representative or partner manager to see if you can get the information you need.Great news! cuDNN 8.2.2 has just been released: https://developer.nvidia.com/rdp/cudnn-downloadGood luck!Stay tuned for more information on TensorRT.Powered by Discourse, best viewed with JavaScript enabled"
3364,gtc-2020-adopting-ai-in-virtualized-it-deployments,"GTC 2020 S22087
Presenters: Seydou Djabate,NVIDIA; Sumit Kumar,NVIDIA
Abstract
While deep learning researchers and engineers require flexibility, reliability, and performance, IT organizations demand visibility and control over the environment to maximize data security and ensure compliance with industry regulations. Virtualization has unlocked the power of data center consolidation and workforce mobility, where every employee can work on their favorite device from anywhere in the world — while also providing IT organizations with a way to manage and secure resources. We’ll share our work on how organizations can leverage existing investments in virtualization to provide researchers and engineers with secure, mobile, and GPU-ready environments that are able to support deep learning workloads with NVIDIA Virtual GPU and GPU Cloud.Watch this session
Join in the conversation below.Powered by Discourse, best viewed with JavaScript enabled"
3365,beyond-gpu-memory-limits-with-unified-memory-on-pascal,"Originally published at:			https://developer.nvidia.com/blog/beyond-gpu-memory-limits-unified-memory-pascal/
Figure 1: Dimethyl ether jet simulations designed to study complex new fuels. Image courtesy of the Center for Exascale Simulation of Combustion in Turbulence (ExaCT). Modern computer architectures have a hierarchy of memories of varying size and performance. GPU architectures are approaching a terabyte per second memory bandwidth that, coupled with high-throughput computational cores, creates…Can I take advantage of the Page Migration Engine in Pascal GPUs when using OpenCL?  I think it would drastically simplify some parts of the code generator for my compiler, but it generates only OpenCL code.Hi Troels. Currently the Page Migration Engine functionality is not supported in our OpenCL implementation.That's a shame.  Is there any hope of support being added in the future?Sorry, at this time we do not have plans to add support for it.Hi Nikolay.  The unified memory techniques you applied to simulation is amazing thanks for your introduction. I have a question about the Pascal GPU, I'm a student and I cannot afford the expensive Pascal P100 GPU (even I know it has powerful double precision computing capabilities), so does Geforce series GPU support the unified memory features(such as GTX1080 GTX Titan X)? if yes that would be nice for me to some fast simulation prototyping even with single precision. Another question is currently I'm using Python to write simulation code, do you know if there's any possibilities or python packages which can utilize the unified memory features?  Thanks!Hi Liu. Thank you for your feedback. Any Pascal GPU supports the new Unified Memory features such as on-demand paging and GPU memory oversubscription, so you can definitely prototype your application on GTX 1080 or GTX Titan X. Regarding Python, there are two most popular packages out there: Numba http://numba.pydata.org/ and PyCuda https://mathema.tician.de/s.... Numba does not support Unified Memory allocations, but there is a way to use Unified Memory in PyCuda, see https://documen.tician.de/p....Just to note - you can get access to a P100 server with NVlink enabled in the cloud for $5/hour.  You could prototype on GTX1080 and then give a full-speed test on the cloud server.Cloud link: https://power.jarvice.com (choose the Ubuntu Linux for Power8 with NVLink support).Re Fig. 9 - hints and prefetching seem to harm P8+NVLink on small data sets on the left side of the graph...what's up?Thanks, good catch! This was mostly likely a measurement error or driver overhead. I reran the numbers with a more recent driver and updated the performance charts. The issue for small datasets had disappeared and now the prefetching approach is always better than the baseline. Moreover, the throughput of the largest test case increased significantly due to driver improvements related to page fault handling.Hello, Is there any limit on number for prefetch calls I can issue concurrently ? I am seeing some performance slow down after 64 calls. Am using cudaMemPrefetchAsync()Hi Vishal, I have never tested so many concurrent prefetches. Can you put your example test code on gist or email me details at ""first initial last name at nvidia dot com""? I'd like to look into this.Hi Nikolay, I am not sure which methodology to be used in my program i.e. UM or non-UM approach. In my application's pipeline most of the components are GPU kernel based / cudaAPI based. CPU does not come into picture for accessing this data in between this pipeline. So in such scenario, which type of memory i.e. UM or use cudaMalloc / device memory. I think device memory would be faster in between my pipeline, but I need to manage them using cudaSetDevice in case of multi-gpu environment. I hope for multi-GPU using cudaMallocManaged is simpler, but not sure about its performance impact as it will limited by PCIe bandwidth.Can you please share your opinion on this and suggest better way of architecting multi-GPU use case with very minimal buffer access by CPU.Does this mean that gTX1050 using PASCAL architecture also supports the same functionality, i.e. GPU memory oversubscription?Yes, any Pascal+ GPU supports Unified Memory-based GPU memory oversubscription on Linux platforms. See this paragraph in the CUDA programming guide for detailed requirements: Programming Guide :: CUDA Toolkit DocumentationThanks,  your reply saves me from spending money on a much more expensive GPUPowered by Discourse, best viewed with JavaScript enabled"
3366,new-video-composition-and-layering-with-universal-scene-description,"Originally published at:			https://developer.nvidia.com/blog/new-video-composition-and-layering-with-universal-scene-description/
Developers are using Universal Scene Description (OpenUSD) to push the boundaries of 3D workflows. As an ecosystem and interchange paradigm, OpenUSD models, labels, classifies, and combines a wide range of data sources into a composed ground truth. It is also highly extensible with four key features that help developers meet the demands of virtual worlds.…Powered by Discourse, best viewed with JavaScript enabled"
3367,build-an-ai-cat-chaser-with-jetson-tx1-and-caffe,"Originally published at:			https://developer.nvidia.com/blog/ai-cat-chaser-jetson-tx1-caffe/
Fun projects arise when you combine a problem that needs to be solved with a desire to learn. My problem was the neighbors’ cats: I wanted to encourage them to hang out somewhere other than my front yard. At the same time, I wanted to learn more about neural network software and deep learning and…This project is beautiful. I made good experiences to ""educate"" cats to use their own toilet at home.Hello, Robert. It's a great work! I have so many questions! Can I write you a private message?Powered by Discourse, best viewed with JavaScript enabled"
3368,ai-improves-mri-analysis-process,"Originally published at:			AI Improves MRI Analysis Process | NVIDIA Technical Blog
Researchers from IBM developed a deep learning system that can perform image registration, or the process of aligning two or more images of the same scene, with superb accuracy and speed. The method uses generative adversarial networks (GANs), and has the potential to radically improve the analysis of magnetic resonance images, allowing doctors and specialists…Powered by Discourse, best viewed with JavaScript enabled"
3369,testing-container-images-against-multiple-platforms-with-container-canary,"Originally published at:			https://developer.nvidia.com/blog/testing-container-images-against-multiple-platforms-with-container-canary/
This post details how to use Container Canary from installation and validation to writing custom manifests and container automation.Hey there folks! I’m the author of Container Canary. The project grew out of a need for unit testing and most importantly regression testing for the containers we manage. Specifically, I was trying to support multiple platforms with a single image and kept causing regressions with changes I was making. So Canary was born as a side project of mine to help me test our images and give me the confidence I wasn’t breaking things.We then open-sourced it so that the community can benefit from it too. If you have any questions please don’t hesitate to ask!Powered by Discourse, best viewed with JavaScript enabled"
3370,how-jet-built-a-gpu-powered-fulfillment-engine-with-f-and-cuda,"Originally published at:			How Jet Built a GPU-Powered Fulfillment Engine with F# and CUDA | NVIDIA Technical Blog
Have you ever looked at your shopping list and tried to optimize your trip based on things like distance to store, price, and number of items you can buy at each store? The quest for a smarter shopping cart is never-ending, and the complexity of finding even a sub-optimal solution to this problem can quickly…This reminds me of portfolio optimisation with transaction costs in finance.For that there is an efficient branch and bound solution I've built in the past based on this paper:https://faculty.washington....Not sure if there are equivalent obvious bounds in your case.This is wonderful! I am geeking out. Thank you for posting the code snippets.It seems this  approach is trying to re-invent convex optimization.Powered by Discourse, best viewed with JavaScript enabled"
3371,automatically-convert-your-photos-into-3d-images-with-ai,"Originally published at:			Automatically Convert Your Photos into 3D Images with AI | NVIDIA Technical Blog
To convert a single RGB-D input image into a 3D photo, a team of researchers from Virginia Tech and Facebook developed a deep learning-based image inpainting model that can synthesize color and depth structures in regions occluded in the original view.  “Classic image-based reconstruction and rendering techniques require elaborate capture setups involving many images with…Powered by Discourse, best viewed with JavaScript enabled"
3372,announcing-nvidia-merlin-an-application-framework-for-deep-recommender-systems,"Originally published at:			Announcing NVIDIA Merlin: An Application Framework for Deep Recommender Systems | NVIDIA Technical Blog
Recommender systems drive every action that you take online, from the selection of this web page that you’re reading now to more obvious examples like online shopping. They play a critical role in driving user engagement on online platforms, selecting a few relevant goods or services from the exponentially growing number of available options. On…Powered by Discourse, best viewed with JavaScript enabled"
3373,leveraging-ai-music-with-nvidia-dgx-2,"Originally published at:			Applying Language Model Techniques to Compose AI Music | NVIDIA Technical Blog
Learn about a series of experiments performed in the field of AI music using the NVIDIA DGX-2 platform.Powered by Discourse, best viewed with JavaScript enabled"
3374,modeling-cities-in-3d-using-only-image-data,"Originally published at:			Modeling Cities in 3D Using Only Image Data | NVIDIA Technical Blog
ETH Zurich scientists leveraged deep learning to automatically stich together millions of public images and video into a three-dimensional, living model of the city of Zurich. The platform called “VarCity” combines a variety of different image sources: aerial photographs, 360-degree panoramic images taken from vehicles, photos published by tourists on social networks and video material…Powered by Discourse, best viewed with JavaScript enabled"
3375,high-precision-image-editing-with-ai-editgan,"Originally published at:			https://developer.nvidia.com/blog/high-precision-image-editing-with-ai-editgan/
EditGAN takes AI-driven image editing to the next level by providing high levels of accuracy while not sacrificing image quality.Powered by Discourse, best viewed with JavaScript enabled"
3376,ai-helps-seismologists-identify-earthquakes,"Originally published at:			AI Helps Seismologists Identify Earthquakes | NVIDIA Technical Blog
Researchers at MIT and Harvard University recently published a study outlining how deep learning is helping seismologists detect earthquakes that might have otherwise been missed. Published in the Science Advances Journal last month, the study focused on Oklahoma, a state that before 2009 only experienced around two earthquakes of magnitude 3.0 or higher per year.…Powered by Discourse, best viewed with JavaScript enabled"
3377,deep-learning-detects-earthquakes-at-millimeter-scale,"Originally published at:			https://developer.nvidia.com/blog/deep-learning-detects-earthquakes-at-millimeter-scale/
Researchers create a neural network that automatically detects tectonic fault deformation, crucial to understanding and possibly predicting earthquake behavior.Powered by Discourse, best viewed with JavaScript enabled"
3378,exploring-unique-applications-of-automatic-speech-recognition-technology,"Originally published at:			Exploring Unique Applications of Automatic Speech Recognition Technology | NVIDIA Technical Blog
Learn how businesses are innovating user-facing applications with speech recognition, from interactive learning to digital human services.Powered by Discourse, best viewed with JavaScript enabled"
3379,updating-ai-product-performance-from-throughput-to-time-to-solution,"Originally published at:			https://developer.nvidia.com/blog/updating-ai-product-performance-from-throughput-to-time-to-solution/
Data scientists and researchers work toward solving the grand challenges of humanity with AI projects such as developing autonomous cars or nuclear fusion energy research. They depend on powerful, high-performance AI platforms as essential tools to conduct their work. Even enterprise-grade AI implementation efforts—adding intelligent video analytics to existing video camera streams or image classification…Thank you for taking the time to read our blog! Developing convergence time to solution data was a heavy lift for our engineering folks and making monthly updates is a significant time investment by our Performance Lab. I hope this blog gives you a good view into why convergence is so critical for your organization to be able to productize AI. Please let us know if you have any questions or comments!Powered by Discourse, best viewed with JavaScript enabled"
3380,rtx-global-illumination-sdk-now-available,"Originally published at:			https://developer.nvidia.com/blog/announcing-nvidia-rtxgi-sdk/
The RTX Global Illumination (RTXGI) SDK is now available. With the RTXGI SDK, game developers and artists can leverage the power of ray tracing to achieve multi-bounce global illumination without bake times, light leaks, or prohibitive run-time performance requirements. RTXGI is supported on any DXR-enabled GPU, and is an ideal starting point to bring the…Powered by Discourse, best viewed with JavaScript enabled"
3381,getting-started-with-nvidia-instant-nerfs,"Originally published at:			https://developer.nvidia.com/blog/getting-started-with-nvidia-instant-nerfs/
Johnathan Stephens provides a walkthrough of how he started using Instant NeRF.Hi
What/Where is the right place in this forum for technical assistance/exchange  regarding NeRFs ?Hi,
I was trying to build the NeRF ‘program’ but I got an error.
The build stops because it can’t find the file :
\instant-ngp\build\CMakeFiles\3.25.0\CompilerIdCUDA\CompilerIdCUDA.vcxproj
Does anyone have an idea how to solve this issue ?
Thank you all for your answers,Warmest regardsIf running without GUI, how to generate animation videos? When run .\build\testbed --scene data\nerf\fox --no-gui ( without GUI) it won’t stop the training…
Thanks,
JimPowered by Discourse, best viewed with JavaScript enabled"
3382,nvidia-bluefield-dpu-ecosystem-expands-as-partners-introduce-joint-solutions,"Originally published at:			NVIDIA BlueField DPU Ecosystem Expands as Partners Introduce Joint Solutions | NVIDIA Technical Blog
Learn how these Industry leaders have started to integrate their solutions using the DPU/DOCA architecture as key partners showcase these solutions at the recent NVIDIA GTC.Powered by Discourse, best viewed with JavaScript enabled"
3383,nvidia-releases-latest-jetpack-3-1-sdk-with-tensorrt-2-1-for-ai-at-the-edge,"Originally published at:			https://developer.nvidia.com/blog/nvidia-releases-latest-jetpack-3-1-sdk-with-tensorrt-2-1-for-ai-at-the-edge/
NVIDIA released JetPack 3.1, the production software release for the Jetson TX1/TX2 platforms for AI at the edge. JetPack 3.1 doubles the deep learning inference performance for low latency applications for batch size 1. For the first time, developers will have access to the same unified code base for both TX1 and TX2, including a…Powered by Discourse, best viewed with JavaScript enabled"
3384,remotely-operating-systems-and-applications-at-the-edge,"Originally published at:			Remotely Operating Systems and Applications at the Edge | NVIDIA Technical Blog
Remotely troubleshoot systems and applications with remote management features on Fleet Command.Powered by Discourse, best viewed with JavaScript enabled"
3385,speed-to-safety-autonomous-rc-car-aids-emergency-evacuation,"Originally published at:			https://developer.nvidia.com/blog/speed-to-safety-autonomous-rc-car-aids-emergency-evacuation/
By Abhinav Ayalur, Isaac Wilcove, Lynn Dang, Ricky Avina The alarm is ringing. You smell smoke and see people running for the exit, but you don’t do the same. Why? Because you’re the fire marshal. As the fire circles, you have the responsibility of making sure everyone gets out safely before you can save yourself.…Powered by Discourse, best viewed with JavaScript enabled"
3386,accelerating-io-in-the-modern-data-center-magnum-io-storage,"Originally published at:			https://developer.nvidia.com/blog/accelerating-io-in-the-modern-data-center-magnum-io-storage/
This is the fourth post in the Accelerating IO series. It addresses storage issues and shares recent results and directions with our partners. We cover the new GPUDirect Storage release, benefits, and implementation. Accelerated computing needs accelerated IO. Otherwise, computing resources get starved for data. Given that the fraction of all workflows for which data…Delighted to have this explanation of GPUDirect Storage broadly available, and looking forward to future installments with partner and application data.Powered by Discourse, best viewed with JavaScript enabled"
3387,smart-spaces-end-to-end-ai-application-boosts-factory-efficiency,"Originally published at:			https://developer.nvidia.com/blog/smart-spaces-end-to-end-ai-application-boosts-factory-efficiency/
Smart spaces are delivering unprecedented value, creating a continuous flow of information between physical and digital worlds.  By incorporating technologies such as the Internet of Things (IoT), cloud computing, machine learning, and AI at the edge, world-class businesses can capture digital data and turn them into actionable insights.  However, the process is complicated with edge…Powered by Discourse, best viewed with JavaScript enabled"
3388,nvidia-clocks-world-s-fastest-bert-training-time-and-largest-transformer-based-model-paving-path-for-advanced-conversational-ai,"Originally published at:			NVIDIA Clocks World’s Fastest BERT Training Time and Largest Transformer Based Model, Paving Path For Advanced Conversational AI | NVIDIA Technical Blog
NVIDIA DGX SuperPOD trains BERT-Large in just 47 minutes, and trains GPT-2 8B, the largest Transformer Network Ever with 8.3Bn parameters  Conversational AI is an essential building block of human interactions with intelligent machines and applications – from robots and cars, to home assistants and mobile apps. Getting computers to understand human languages, with all their…its very helpful and informative blogWhat can one say to the inference times?How big is the effort for an inference machinewith the trained GPT-2 8B model?A SuperGLUE entry would worth a thousand blog posts.Powered by Discourse, best viewed with JavaScript enabled"
3389,building-state-of-the-art-biomedical-and-clinical-nlp-models-with-bio-megatron,"Originally published at:			https://developer.nvidia.com/blog/building-state-of-the-art-biomedical-and-clinical-nlp-models-with-bio-megatron/
With the advent of new deep learning approaches based on transformer architecture, natural language processing (NLP) techniques have undergone a revolution in performance and capabilities. Cutting-edge NLP models are becoming the core of modern search engines, voice assistants, chatbots, and more. Modern NLP models can synthesize human-like text and answer questions posed in natural language.…Before BIOMegatron, there was Optimus Prime, M.D.:von Davier, M. (2019). Training Optimus Prime, M.D.: Generating Medical Certification Items by Fine-Tuning OpenAI’s GPT-2 Transformer Model. [1908.08594] Training Optimus Prime, M.D.: Generating Medical Certification Items by Fine-Tuning OpenAI's gpt2 Transformer ModelPowered by Discourse, best viewed with JavaScript enabled"
3390,ml-for-rt,"Morning friends! What’s the latest scoop on using ML training to help with ray-traced GI? Or, more generally, any neat advances in ray tracing that benefit from deep learning? Have you hooked together lower sampling and filtering using an ML upscaling 2D filter?Hi Wex,
welcome to the developer forums - and welcome to the AMA, your question has been queued up for the team !Thanks for the question - there’s been quite a lot of work in the machine learning space to assist with real-time (and not real-time) graphics. For ray traced global illumination, check out a paper recently published by Muller et al called Real-time Neural Radiance Caching for Path Tracing. Their approach trains a neural network to learn the light transport characteristics of a scene and then builds a light cache that can be queried at a lower cost than tracing the full paths.Powered by Discourse, best viewed with JavaScript enabled"
3391,nvidia-clara-train-annotation-will-be-integrated-into-mitk,"Originally published at:			NVIDIA Clara Train Annotation Will be integrated into MITK | NVIDIA Technical Blog
Powering Medical Viewers with AI assisted annotation capabilities. The Medical Imaging Interaction Toolkit (MITK) is a free open-source software toolkit for development of interactive medical image processing software by the German Cancer Research Center (DKFZ). NVIDIA and DKFZ closely collaborated to bring Clara Train SDK into MITK. Clara Train SDK annotation will be part of…Powered by Discourse, best viewed with JavaScript enabled"
3392,accelerating-single-cell-genomic-analysis-using-rapids,"Originally published at:			Accelerating Single Cell Genomic Analysis using RAPIDS | NVIDIA Technical Blog
The human body is made up of nearly 40 trillion cells, of many different types. Recent advances in experimental biology have made it possible to explore the genetic material of single cells. With the birth of this new field of single-cell genomics, scientists can now probe the DNA and RNA of individual cells in the…Very cool application!Our lab has extensively refactored our code to incorporate RAPIDS libraries to accelerate our single-cell analyses.I recently re-implemented the widely-used single-cell phenotyping algorithm PhenoGraph (GitHub - jacoblevine/PhenoGraph: Subpopulation detection in high-dimensional single-cell data) using a combination of cuML, cuGraph, cupy, and cupyx.sparse. The GPU-based implementation (Erik Burlingame / grapheno · GitLab) yields an orders-of-magnitude boost in speed over the CPU-based implementation, without sacrificing cell cluster quality.Keep up the great work, RAPIDS team!We’re excited to accelerate single-cell research with RAPIDS and enable better, faster science. If you have any questions or comments, or are interested in applying RAPIDS to your single-cell research, please let us know.Great to see that you’re seeing speedups using RAPIDS. I run the RAPIDS blog. Would you be interested in contributing a guest blog on your use case? I’d be happy to collaborate.Hi, @mbeaumont. Absolutely! I believe I have a Jupyter notebook that might serve as a good starting point for a blog post.Can you reach out to me over email? mbeaumont@nvidia.comHello @jwitsoe
When loading data to the GPU for your testing was that done with only one core of the CPU? During my walking though your guide (great guide by the way), I noticed that when I ran htop. I was thinking the performance would be better if more cores loaded the GPU with data. Is this being done by one core the expected behavior?Powered by Discourse, best viewed with JavaScript enabled"
3393,ai-painting-robot,"Originally published at:			AI Painting Robot | NVIDIA Technical Blog
Virginia-based artist Pindar Van Arman and his two kids built a deep learning painting robot that makes all artistic decisions without human intervention. The robot named CloudPainter has painted over 1,000 canvases in the last ten years with a combination of AI and human direction. “I started experimenting with various artificial intelligence algorithms and tried…Powered by Discourse, best viewed with JavaScript enabled"
3394,nvidia-dpu-hackathon-unveils-ai-cloud-and-accelerated-computing-breakthroughs,"Originally published at:			https://developer.nvidia.com/blog/nvidia-dpu-hackathon-unveils-ai-cloud-and-accelerated-computing-breakthroughs/
NVIDIA announces the winners from the second global DPU Hackathon.Powered by Discourse, best viewed with JavaScript enabled"
3395,uplifting-optimizations-debugging-and-performance-tuning-with-nvidia-nsight-developer-tools,"Originally published at:			https://developer.nvidia.com/blog/uplifting-optimizations-debugging-and-performance-tuning-with-nvidia-nsight-developer-tools/
When developing on NVIDIA platforms, the hardware should be transparent to you. GPUs can feel like magic, but in the interest of optimized and performant games, it’s best to have an understanding of low-level processes behind the curtain. NVIDIA Nsight Developer Tools are built for this very reason. Imagine a proud homeowner who lives in…Powered by Discourse, best viewed with JavaScript enabled"
3396,gtc-2020-high-performance-remote-scientific-visualization-in-jupyter-notebooks,"GTC 2020 S22111
Presenters: Nick Leaf,NVIDIA; Maximilian Rietmann,NVIDIA
Abstract
We’ll introduce iPyParaView, a Jupyter widget for interactive rendering in the notebook with ParaView. iPyParaView leverages the GPU in your local or remote Jupyter instance to render data without copying it back to the client. It uses ParaView’s Python bindings to expose its full capabilities — including the RTX path-tracing back end and IndeX volume rendering plugin. We’ll use live demos to show how to pre-process data with RAPIDS and render with ray-tracing, as well as how to use a Dask-MPI cluster for distributed-memory processing and visualization.Watch this session
Join in the conversation below.Really impressed with visualizations in Jupyter! Is it possible to select points?Hello, glad you like it!iPyParaView is using a custom camera model to translate mouse events into camera movement. This means that some things which work in ParaView’s desktop UI aren’t handled. Unfortunately this includes data selection, so selecting points through the viewing window in Jupyter isn’t currently supported.I’m really impressed with this!Powered by Discourse, best viewed with JavaScript enabled"
3397,explainer-what-is-an-ai-cockpit,"Originally published at:			What Is an AI Cockpit? | NVIDIA Blog
Intelligent interiors are transforming transportation.Powered by Discourse, best viewed with JavaScript enabled"
3398,introducing-the-ultimate-starter-ai-computer-the-nvidia-jetson-nano-2gb-developer-kit,"Originally published at:			https://developer.nvidia.com/blog/ultimate-starter-ai-computer-jetson-nano-2gb-developer-kit/
Today, NVIDIA announced the Jetson Nano 2GB Developer Kit, the ideal hands-on platform for teaching, learning, and developing AI and robotics applications. The NVIDIA Jetson platform introduced six years ago revolutionized embedded computing by delivering the power of artificial intelligence to edge computing devices. NVIDIA Jetson today is widely used in diverse fields such as…We will need a way of connecting cellular modems,  in particular a modem that supports the new 3.5GHz CBRS band to use this part.   What will be the best way to get that capability?There is no out of box suppor for cellular modems. You may need to select the modem you want ( i see some USB modems with CRBS support) and then get the appropriate driver and build it with our kernel.Looks good. Can someone say what the max USB bandwidth is? I want to connect four UVC cameras and want to know if this new Nano would support 2 cameras on a USB Type C hub and 2 on the built in USB 2.0 Type A ports.Powered by Discourse, best viewed with JavaScript enabled"
3399,how-to-accelerate-signal-processing-in-python,"Originally published at:			https://developer.nvidia.com/blog/how-to-accelerate-signal-processing-in-python/
This post is the eighth installment of the series of articles on the RAPIDS ecosystem. The series explores and discusses various aspects of RAPIDS that allow its users solve ETL (Extract, Transform, Load) problems, build ML (Machine Learning) and DL (Deep Learning) models, explore expansive graphs, process signal and system log, or use SQL language…Powered by Discourse, best viewed with JavaScript enabled"
3400,nvidia-continues-bluefield-dpu-doca-momentum-with-the-release-of-doca-1-1,"Originally published at:			https://developer.nvidia.com/blog/nvidia-continues-bluefield-dpu-doca-momentum-with-the-release-of-doca-1-1/
NVIDIA released the NVIDIA DOCA 1.1 software framework for NVIDIA BlueField DPUs, the world’s most advanced Data Processing Unit (DPU).Powered by Discourse, best viewed with JavaScript enabled"
3401,nvidia-gtc-top-sessions-for-optimizing-performance-and-securing-network-infrastructure,"Originally published at:			https://developer.nvidia.com/blog/nvidia-gtc-top-sessions-for-optimizing-performance-and-securing-network-infrastructure/
Learn about the top data center infrastructure sessions at GTC.Powered by Discourse, best viewed with JavaScript enabled"
3402,power-the-next-wave-of-applications-with-nvidia-bluefield-3-dpus,"Originally published at:			https://developer.nvidia.com/blog/power-the-next-wave-of-applications-with-nvidia-bluefield-3-dpus/
​NVIDIA BlueField-3 DPUs transform traditional computing environments into efficient, high-performance, secure, and sustainable data centers, enabling the delivery of the next wave of applications.Powered by Discourse, best viewed with JavaScript enabled"
3403,share-your-science-robots-thinking-more-like-humans-with-deep-reinforcement-learning,"Originally published at:			Share Your Science: Robots Thinking More Like Humans with Deep Reinforcement Learning | NVIDIA Technical Blog
Pieter Abbeel, Professor at UC Berkeley shares how his Artificial Intelligence lab is using NVIDIA GPUs and deep reinforcement learning to enable a robot to learn on its own. Abbeel’s robotics work was recently highlighted in Rolling Stone’s story “Inside the Artificial Intelligence Revolution: A Special Report, Pt. 1”. The video demonstrates how their robot…Powered by Discourse, best viewed with JavaScript enabled"
3404,spotlight-chaos-enscape-streamlines-vr-quality-and-performance-with-nvidia-vcr,"Originally published at:			https://developer.nvidia.com/blog/spotlight-chaos-enscape-streamlines-vr-quality-and-performance-with-vcr/
Precisely reproducing VR experiences is critical to many workflows, yet it is extremely challenging. But VR testing is critical for many teams, especially when they’re looking to troubleshoot a VR experience, or want to gain more insights into what customers see when they put on the headset. Chaos Enscape is using NVIDIA VR Capture and…Powered by Discourse, best viewed with JavaScript enabled"
3405,ai-helps-doctors-diagnose-lung-cancer-with-97-percent-accuracy,"Originally published at:			AI Helps Doctors Diagnose Lung Cancer with 97 Percent Accuracy | NVIDIA Technical Blog
According to the American Cancer Society, over 200,000 people in the United States are diagnosed with lung cancer every year. To help with early detection, researchers at the New York University school developed a deep learning algorithm that can identify two common types of lung cancer with human accuracy. “The purpose of this study was…Powered by Discourse, best viewed with JavaScript enabled"
3406,generating-ray-traced-caustic-effects-in-unreal-engine-4-part-1,"Originally published at:			https://developer.nvidia.com/blog/generating-ray-traced-caustic-effects-in-unreal-engine-4-part-1/
Caustics are common optical phenomenon in the real world. From the sloshing sparkles by water surfaces to the curved highlights in the backlight of clear glass, they are everywhere. However, simulating accurate caustic in 3D graphics is not an easy job. For those who have been involved in creating ray tracers, you might know that…Find out how to access Unreal Engine source code on GitHubIt will grant you the access to Epic’s official UE4 repos and NVIDIA’s folks.The NVRTX_Caustics branch of UE4 is based on the UE4.25.3 NVRTX branch. The upgrade for 4.25.4 and 4.26 will be available soon.The sample scenes used in this post can be found at the following places:Executable demos: Tech_Demos - Google DriveAssets and projects: Demo_Projects - Google DriveIf you have any question on the algorithms, source, performance and usages of mesh caustics, please ask here.Do you have any plans to fix the bug that translucent objects are displayed incorrectly in reflections?
And what about ray tracing subsurface scattering?
Looking forward to your reply. Thx!To show translucent objects in reflections, you can use the following command lines to enable full binary tree of reflection+refraction bounces, which allow the refractions to be seen in reflections:Ray tracing subsurface scattering is still under researching stage. We are working behind it now.Has anyone seen this make error before with the NvRTX_Caustics-4.26 branch?What could be the fix?[2/613] Compile Module.Engine.35_of_48.cpp
In file included from /mnt/2.26/Engine/Intermediate/Build/Linux/B4D820EA/UE4Editor/Development/Engine/Module.Engine.11_of_48.cpp:11:
/mnt/2.26/Engine/Source/Runtime/Engine/Private/Components/LightComponent.cpp:321:4: error: field ‘bTiledDeferredLightingSupported’ will be initialized after field ‘bAffectWaterCaustics’ [-Werror,-Wreorder-ctor]
, bTiledDeferredLightingSupported(false)
^Ubunbtu Linux Kernel 5.4.0-66-generic
NVIDIA Driver Version: 460.73.01
CUDA Version: 11.2, and an RTX 3090Thanks in  advance,
GregoryReporting back that building on Windows with different hardware produces the exact same errors.  I wonder how people are successfully building this?This error message is indicating that the field ‘bTiledDeferredLightingSupported’ is being initialized after the field ‘bAffectWaterCaustics’, which is causing a reordering of the constructor initialization list.A possible fix for this issue would be to reorder the fields in the constructor initialization list so that ‘bAffectWaterCaustics’ is initialized before ‘bTiledDeferredLightingSupported’.Here’s an example of how the constructor could look after making this change:I downloaded NvRTX_Caustics-5.1 source code and Ran Setup.bat.
Then It showed below error.Checking dependencies…
Updating dependencies:   0% (0/97588)…
Unhandled exception. System.PlatformNotSupportedException: Thread abort is not supported on this platform.
at System.Threading.Thread.Abort()
at GitDependencies.Program.DownloadDependencies(String RootPath, IEnumerable1 RequiredFiles, IEnumerable1 Blobs, IEnumerable1 Packs, Int32 NumThreads, Int32 MaxRetries, Uri Proxy, String CachePath)    at GitDependencies.Program.UpdateWorkingTree(Boolean bDryRun, String RootPath, HashSet1 ExcludeFolders, Int32 NumThreads, Int32 MaxRetries, Uri Proxy, OverwriteMode Overwrite, String CachePath, Single CacheSizeMultiplier, Int32 CacheDays)
at GitDependencies.Program.Main(String Args)I tried this,but it didn’t work.What should i do?by the way,
In the NvRTX_Caustics-4.27, It showed below error.Failed to download ': The remote server returned an error: (403) Forbidden. (WebException)Getting exact same error. Is there a solution?@yaobino @jwitsoe :
Are there any plans to integrate this functionality in more recent Unreal Engine versions? It would be amazing if it was put in a plugin, like DLSS. :):):)Or, I guess another question is: Did any of this functionality make it into the new NvRTX 5.2.0-1?Powered by Discourse, best viewed with JavaScript enabled"
3407,university-of-michigan-to-install-a-new-gpu-accelerated-cluster,"Originally published at:			University of Michigan to Install a New GPU-Accelerated Cluster | NVIDIA Technical Blog
The University of Michigan has just announced a brand new GPU-accelerated cluster that will include NVIDIA’s Tesla V100 Tensor Core GPUs, the most advanced data center GPU ever built. The new cluster, which will be named Great Lakes, will support applications in aerospace engineering, deep learning, molecular dynamics, genomics, and cell biology. “High-performance research computing…Powered by Discourse, best viewed with JavaScript enabled"
3408,vxrail-boosts-application-performance-using-vsan-rdma,"Originally published at:			https://developer.nvidia.com/blog/vxrail-boosts-application-performance-by-using-vsan-rdma/
Hybrid cloud refers to a mix of computing and storage services of on-premises infrastructure, like Dell EMC VxRail hyperconverged infrastructure (HCI) and multipublic cloud services such as Amazon Web Services or Microsoft Azure. Hybrid cloud architecture gives you the flexibility to maintain traditional IT on-premises deployments for running business-critical applications or to protect sensitive data…Powered by Discourse, best viewed with JavaScript enabled"
3409,build-generative-ai-pipelines-for-drug-discovery-with-nvidia-bionemo-service,"Originally published at:			https://developer.nvidia.com/blog/build-generative-ai-pipelines-for-drug-discovery-with-bionemo-service/
Creating new drug candidates is a heroic endeavor, often taking over 10 years to bring a drug to market. New supercomputing-scale large language models (LLMs) that understand biology and chemistry text are helping scientists understand proteins, small molecules, DNA, and biomedical text. These state-of-the-art AI models help generate de novo proteins and molecules and predict…Powered by Discourse, best viewed with JavaScript enabled"
3410,upcoming-workshop-computer-vision-for-industrial-inspection,"Originally published at:			Computer Vision for Industrial Inspection Workshop | NVIDIA
Learn how to create an end-to-end hardware-accelerated industrial inspection pipeline to automate defect detection in this workshop on January 18 (CET).Powered by Discourse, best viewed with JavaScript enabled"
3411,deep-learning-vs-machine-learning-challenger-models-for-default-risk-with-explainability,"Originally published at:			https://developer.nvidia.com/blog/deep-learning-vs-machine-learning-challenger-models-for-default-risk-with-explainability/
This post details the credit default risk prediction with deep learning and Machine learning models.Powered by Discourse, best viewed with JavaScript enabled"
3412,tips-and-tricks-ray-tracing-best-practices,"Originally published at:			Tips and Tricks: Ray Tracing Best Practices | NVIDIA Technical Blog
This post presents best practices for implementing ray tracing in games and other real-time graphics applications. We present these as briefly as possible to help you quickly find key ideas. This is based on a presentation made at the 2019 GDC by NVIDIA engineers. Main Points Optimize your acceleration structure (BLAS/TLAS) build/update to take at…Hi, I recently purchased GeForce 1050 Ti. Unfortunately After Effect does not support this graphic card. Even there are some other applications such as Magix Vegas 16.0, Element 3D are also not supporting. Big disappointment for me, do you have any solution to utilize GeForce 1050's strength for After Effects and other related applications?I suggest you take this to the NVIDIA Devtalk forums, https://devtalk.nvidia.com/. You're more likely to find someone to address your issues there.Apology for extremely late reply. Thanks for reply and guidance. That truly helped. Allah bless youPowered by Discourse, best viewed with JavaScript enabled"
3413,geometry-reinvented-with-mesh-shading,"Originally published at:			Geometry Reinvented with Mesh Shading | NVIDIA Technical Blog
During SIGGRAPH 2019, NVIDIA presented a talk entitled “Applications of Mesh Shading with DX12,” which explained how developers could take complete control over geometry processing.  The full presentation can be found here.  The 12 minute video below is an excerpt from the full 60 minute talk. The mesh shading technology built into NVIDIA’s Turing GPU…Powered by Discourse, best viewed with JavaScript enabled"
3414,breakthrough-ai-confirms-50-new-planets-using-nasa-data,"Originally published at:			Breakthrough AI Confirms 50 New Planets Using NASA Data | NVIDIA Technical Blog
To help confirm new planets from NASA telescope data, a team of scientists from the University of Warwick, working in collaboration with Alan Turing Institute researchers, developed a deep learning model that identified 50 new planets.  According to the scientists, this is the first time AI has been used to sample potential planets and determine…Powered by Discourse, best viewed with JavaScript enabled"
3415,gtc-2020-toward-an-exascale-earth-system-model-with-machine-learning-components-an-update,"GTC 2020 S21834
Presenters: Richard Loft,Computational and Information Systems Laboratory, National Center for Atmospheric Research
Abstract
Many have speculated that combining exascale GPU computational power with machine-learning algorithms could radically improve weather and climate modeling. We’ll discuss the status of an ambitious project at the U.S. National Center for Atmospheric Research that’s moving in that direction. Having achieved performance portability for a standalone version of the Model for Prediction Across Scales-Atmosphere (MPAS-A) on heterogeneous CPU/GPU architectures across thousands of GPUs using OpenACC, our project has begun looking at two new directions. First, we’ve launched an effort to port the MOM-6 Ocean Model. Second, machine-learning scientists at NCAR and elsewhere have begun evaluating replacing atmospheric parameterizations with machine-learned emulators, including the atmospheric surface layer, cloud microphysics, and aerosol parameterizations. We’ll also discuss related efforts to apply machine-learning emulation to model physics.Watch this session
Join in the conversation below.Powered by Discourse, best viewed with JavaScript enabled"
3416,using-fully-redesigned-batch-api-and-performance-optimizations-in-nvcomp-v2-1-0,"Originally published at:			Using Fully Redesigned Batch API and Performance Optimizations in nvCOMP v2.1.0 | NVIDIA Technical Blog
New nvCOMP v2.1.0 Library with Redesigned Batch API and Performance OptimizationsPowered by Discourse, best viewed with JavaScript enabled"
3417,how-to-build-an-edge-solution-common-questions-and-resources-for-success,"Originally published at:			How to Build an Edge Solution: Common Questions and Resources for Success | NVIDIA Technical Blog
Discover key takeaways from the Edge Computing 201 webinar, with resources for success and continued research.Powered by Discourse, best viewed with JavaScript enabled"
3418,introductry-research-efforts-in-generative-ai,"Hello all,
I have just started my research in the Generative AI field so I’m a complete beginner; I wanted to know if there are any open-source Nvidia software or models that I can use in my upcoming projects for text-to-3D object generation.
Also since I have just started, I would appreciate any advice on how to proceed in order to learn more about 3D object generative AI. In addition, I want to know if any team member in the Omniverse team is open to future collaborations or discussions about interesting ideas about Gen AI.
Thank you for checking my questions.Hello! Yes, we’d love to learn more about your use cases. We have a few generative AI offering. Text-to-3D is part of the Picasso service - NVIDIA Picasso In general, you will be building an OV Kit extension with any gen AI APIs.This topic was automatically closed 14 days after the last reply. New replies are no longer allowed.Powered by Discourse, best viewed with JavaScript enabled"
3419,vfio-iommu-nvidia-jetson-xavier-nx,"I have the following questions for Jetson NXThanks in advance!Powered by Discourse, best viewed with JavaScript enabled"
3420,develop-on-your-notebook-with-geforce-deploy-on-tesla,"Originally published at:			https://developer.nvidia.com/blog/develop-your-notebook-geforce-deploy-tesla/
There’s a new post over on the NVIDIA Corporate Blog by my colleague Mark Ebersole about the latest line of laptops powered by new GeForce 700-series GPUs. As Mark explains, the GeForce 700 series (GT 730M, GT 735M, and GT 740M), powered by the low-power GK208 GPU has the latest compute features of the Tesla K20…This sounds fantastic!  Though, until I started trying to find out when cc3.5 notebook cards were coming out, I was completely unaware of this, as the compute capabilities for this family of cards is listed at 3.0: https://developer.nvidia.co....I'm guessing this is just a typo?No its not a typo on the compute capability page. This article is wrong unfortunately. There are no 3.5 devices for notebooks. I have a GTX 780M and it is definitely only a 3.0 device.I checked on this. I'm told that any non-ASUS GeForce GT 730M is a GK208, which is SM 3.5. So the article is not wrong.Powered by Discourse, best viewed with JavaScript enabled"
3421,coffee-break-series-nvidia-ansel,"Originally published at:			Coffee Break Series: NVIDIA Ansel | NVIDIA Technical Blog
Coffee Break: NVIDIA Ansel Credit: From The Witcher III: Wild Hunt. Taken with Ansel by community member herocrash12 for shotwithgeforce.com NVIDIA Ansel is a powerful in-game camera that lets players take professional-grade photographs in their games. Screenshot art has allowed players to tap into their artistic side; Ansel gives them virtual photographic tools to make…Powered by Discourse, best viewed with JavaScript enabled"
3422,nvidia-reveals-the-titan-of-turing-titan-rtx,"Originally published at:			NVIDIA Reveals the TITAN of Turing: TITAN RTX | NVIDIA Technical Blog
At the Conference on Neural Information Processing Systems in Montreal, Canada (NeurIPS), NVIDIA announced the TITAN RTX GPU.  The NVIDIA TITAN RTX is the world’s most powerful desktop GPU, providing performance for AI research, data science, and creative applications. “Turing is NVIDIA’s biggest advance in a decade – fusing shaders, ray tracing, and deep learning…Powered by Discourse, best viewed with JavaScript enabled"
3423,optix-7-delivers-new-levels-of-flexibility-to-application-developers,"Originally published at:			OptiX 7 Delivers New Levels of Flexibility to Application Developers | NVIDIA Technical Blog
The NVIDIA Studio stack including OptiX, MDL and vMaterials provides an unrivaled foundation for RTX rendering applications This year at Siggraph, NVIDIA is showcasing the latest advances in NVIDIA RTX technology, including AI-enhanced workflows and real-time ray tracing. At the heart of NVIDIA’s rendering solutions lies the OptiX ray tracing engine, providing developers the full…Powered by Discourse, best viewed with JavaScript enabled"
3424,text-normalization-and-inverse-text-normalization-with-nvidia-nemo,"Originally published at:			https://developer.nvidia.com/blog/text-normalization-and-inverse-text-normalization-with-nvidia-nemo/
Learn about text normalization and inverse text normalization to improve the quality of TTS and the readability of ASR output.Powered by Discourse, best viewed with JavaScript enabled"
3425,ray-tracing-essentials-part-4-the-ray-tracing-pipeline,"Originally published at:			Ray Tracing Essentials Part 4: The Ray Tracing Pipeline | NVIDIA Technical Blog
NVIDIA recently published Ray Tracing Gems, a deep-dive into best practices for real-time ray tracing. The book was made free-to-download, to help all developers embrace the bleeding edge of rendering technology. Ray Tracing Essentials is a seven-part video series hosted by the editor of Ray Tracing Gems, NVIDIA’s Eric Haines. The aim of this program is to make developers…Powered by Discourse, best viewed with JavaScript enabled"
3426,tensorrt-5-rc-now-available,"Originally published at:			TensorRT 5 RC Now Available | NVIDIA Technical Blog
AT GTC Japan, NVIDIA announced the latest version of the TensorRT’s high-performance deep learning inference optimizer and runtime. Today we are releasing the TensorRT 5 Release Candidate.  TensorRT 5 supports the new Turing architecture, provides new optimizations, and INT8 APIs achieving up to 40x faster inference over CPU-only platforms. This latest version also dramatically speeds up…Powered by Discourse, best viewed with JavaScript enabled"
3427,nvidia-simnet-ai-accelerated-multi-physics-simulation-toolkit,"Originally published at:			https://developer.nvidia.com/blog/nvidia-simnet-ai-accelerated-multi-physics-simulation-toolkit/
Today NVIDIA announced the availability of NVIDIA SimNet, a simulation toolkit intended to address the challenges of using AI and physics. Simulations are pervasive in every domain of science and engineering, but they’re constrained by long computational times, limited compute resources, tedious manual setup efforts, and the need for technical expertise.   SimNet not only accelerates simulations compared to…Powered by Discourse, best viewed with JavaScript enabled"
3428,gaugan-debuts-at-the-ars-electronica-center-in-europe,"Originally published at:			GauGAN Debuts at the Ars Electronica Center in Europe | NVIDIA Technical Blog
GauGAN, NVIDIA’s generative adversarial network that can convert segmentation maps into lifelike images, is being shown for the first time at the brand new Ars Electronica Center in Linz, Austria at the “Understanding AI” exhibition. Developed by NVIDIA researchers earlier this year, GauGAN is the first semantic image synthesis model that can produce complex and…Powered by Discourse, best viewed with JavaScript enabled"
3429,speeding-up-deep-learning-inference-using-tensorrt,"Originally published at:			https://developer.nvidia.com/blog/speeding-up-deep-learning-inference-using-tensorrt/
Looking for more? Check out the hands-on DLI training course: Optimization and Deployment of TensorFlow Models with TensorRT This is an updated version of How to Speed Up Deep Learning Inference Using TensorRT. This version starts from a PyTorch model instead of the ONNX model, upgrades the sample application to use TensorRT 7, and replaces…This line:>> tar xvf speeding-up-unet.7z # Unpack the model data into the unet folderis out of date. It’ll throw an error “this does not look like a tar archive”.This fix is:The article or the file should be updated, if possibleI think this blog page needs to be updated. For example, you make mention of pulling down a Docker container for pytorch, but then make no mention of it.When I try to build code_samples/posts/TensorRT-introduction with the default Ubuntu 20.04 protobuf I get an error about post.inc. I then grabbed the latest protobuf repo and tried that. That version gave me different errors. Now I’m on protobuf v3.17.3 and I receive the following errors:The errors go on and on and on. Very difficult to follow an “introductory example.”Hi @Shadowmind, sorry about the inconvenience.This tutorial has been tested on Ubuntu 18.04, so can you please try using the following container?error occurs as belowl don’t know where to import the normalize_volume fucntion, so i  guess it just normalize the input image, using code below replace the original functionthe model does works, but it does’t work as amazing as the examples from the tourial.So my question is how to fix the bug and using the normalize_volume function, thanks!When I try to build code_samples/posts/TensorRT-introduction, there is an error:How to fix it?Powered by Discourse, best viewed with JavaScript enabled"
3430,nvidia-research-at-iccv-meta-sim-learning-to-generate-synthetic-datasets,"Originally published at:			NVIDIA Research at ICCV: Meta-Sim: Learning to Generate Synthetic Datasets | NVIDIA Technical Blog
At the International Conference on Computer Vision in Seoul, Korea, NVIDIA researchers, in collaboration with University of Toronto, the Vector Institute and MIT presented Meta-Sim, a deep learning model that can generate synthetic datasets with unlabeled real data (i.e. camera footage), bridging the gap between real and synthetic training data. Meta-Sim is a method to…Very InterestingThanks! Have you seen the Meta-Sim 2 paper yet?Powered by Discourse, best viewed with JavaScript enabled"
3431,a-metaverse-for-engineers-introducing-nvidia-omniverse-developer-day-at-nvidia-gtc,"Originally published at:			https://developer.nvidia.com/blog/a-metaverse-for-engineers-introducing-nvidia-omniverse-developer-day-at-nvidia-gtc/
Discover the top 3D design, simulation, and collaboration sessions for developers at GTC, November 8-11.Powered by Discourse, best viewed with JavaScript enabled"
3432,metropolis-spotlight-bluecity-combines-vision-ai-and-lidar-for-real-time-road-safety-and-traffic-congestion,"Originally published at:			https://developer.nvidia.com/blog/metropolis-spotlight-bluecity-combines-vision-ai-and-lidar-for-real-time-road-safety-and-traffic-congestion/
Cities now have access to real-time, multimodal traffic data that improves road safety and reduces traffic congestion.Powered by Discourse, best viewed with JavaScript enabled"
3433,drive-labs-covering-every-angle-with-surround-camera-radar-fusion,"Originally published at:			https://developer.nvidia.com/blog/drive-labs-covering-every-angle-with-surround-camera-radar-fusion/
By: Bala Siva Sashank Jujjavarapu Editor’s note: This is the latest post in our NVIDIA DRIVE Labs series, which takes an engineering-focused look at individual autonomous vehicle challenges and how NVIDIA DRIVE addresses them. Catch up on all of our automotive posts, here. For autonomous driving technology to advance beyond automated assisted driving, it must have reliable,…Powered by Discourse, best viewed with JavaScript enabled"
3434,developer-blog-series-beginner-cuda-refresher-tutorials,"Originally published at:			Developer Blog Series: Beginner ‘CUDA Refresher’ Tutorials | NVIDIA Technical Blog
The CUDA Refresher blog posts are authored by NVIDIA’s Pradeep Gupta, Director of the Solutions Architecture and Engineering team, with the goal of refreshing key concepts in CUDA, tools, and optimization for beginning or intermediate developers. Part 1: Reviewing the Origins of GPU Computing Scientific discovery and business analytics drive an insatiable demand for more computing resources.…Powered by Discourse, best viewed with JavaScript enabled"
3435,optimizing-data-movement-in-gpu-applications-with-the-nvidia-magnum-io-developer-environment,"Originally published at:			Optimizing Data Movement in GPU Applications with the NVIDIA Magnum IO Developer Environment | NVIDIA Technical Blog
Magnum IO is the collection of IO technologies from NVIDIA and Mellanox that make up the IO subsystem of the modern data center and enable applications at scale. If you are trying to scale up your application to multiple GPUs, or scaling it out across multiple nodes, you are probably using some of the libraries…Powered by Discourse, best viewed with JavaScript enabled"
3436,pinterest-sharpens-its-visual-search-skills,"Originally published at:			Pinterest Sharpens its Visual-Search Skills | NVIDIA Technical Blog
The photo-sharing website Pinterest just rolled out a visual search tool that lets you zoom in on a specific object in a pinned image (or “Pin”) and discover visually similar objects, colors, patterns and more. For example, see a lamp that you like in a Pin of a living room? Tap the search tool in…Pinterest, the popular social media platform known for its visual discovery and bookmarking features, has recently improved its visual-search skills through the introduction of several new features. These include a revamped search engine that uses machine learning to better understand search queries and offer more personalized results, as well as a “Complete the Look” feature that suggests similar items to the ones in a user’s search query. Additionally, Pinterest has implemented a “Lens Your Look” feature that allows users to take a photo of an outfit and receive recommendations for similar items. These updates demonstrate Pinterest’s commitment to enhancing its visual-search capabilities and providing users with a more seamless and enjoyable experience.Powered by Discourse, best viewed with JavaScript enabled"
3437,unlocking-the-promise-of-5g,"Originally published at:			https://developer.nvidia.com/blog/unlocking-the-promise-of-5g/
Wireless carriers have been hyping the next generation cellular technology, 5G, for years but the reality of it is certain to start rolling out this year. Wireless networks are always evolving, but this is more than a cellular upgrade. 5G not only increases speeds but offers enhancements in latency, vastly improving responsiveness that opens new…Powered by Discourse, best viewed with JavaScript enabled"
3438,turing-optimized-sdks-for-creators-and-deep-learning,"Originally published at:			Turing Optimized SDKs for Creators and Deep Learning | NVIDIA Technical Blog
At NeurIPS in Montreal, Canada, NVIDIA introduced the TITAN RTX, the world’s most powerful GPU for the PC, providing massive performance for AI research, data science and content creation. Below is a summary of the newly announced developer software which will take advantage of the NVIDIA Turing architecture behind the TITAN RTX. NGX SDK NVIDIA…Powered by Discourse, best viewed with JavaScript enabled"
3439,nsight-compute-2020-3-simplifies-cuda-kernel-profiling-and-optimization,"Originally published at:			Nsight Compute 2020.3 Simplifies CUDA Kernel Profiling and Optimization | NVIDIA Technical Blog
The 2020.3 release of NVIDIA Nsight Compute included in CUDA Toolkit 11.2 introduces several new features that simplify the process of CUDA kernel profiling and optimization.Powered by Discourse, best viewed with JavaScript enabled"
3440,gtc-2020-autonomous-vehicles-learn-ai-training-and-simulation-for-validation-using-nvidia-drive-platform,"GTC 2020 CWE21104
Presenters: Manish-Harsh,NVIDIA; Matt Cragun  , NVIDIA; Siddha Ganju , NVIDIA; Ananth S  , NVIDIA; Mikhail Yurasov , NVIDIA; Venkatesh Rao , NVIDIA; Luke Harvey , NVIDIA
Abstract
Learn about the end-to-end platform, resources, and how to connect with experts supporting the developer community. We’ll discuss applications, benchmarking, and architecture fundamentals required for the AI infrastructure of self driving.Watch this session
Join in the conversation below.Powered by Discourse, best viewed with JavaScript enabled"
3441,optimizing-t5-and-gpt-2-for-real-time-inference-with-nvidia-tensorrt,"Originally published at:			https://developer.nvidia.com/blog/optimizing-t5-and-gpt-2-for-real-time-inference-with-tensorrt/
TensorRT 8.2 optimizes HuggingFace T5 and GPT-2 models. With TensorRT-accelerated GPT-2 and T5, you can generate excellent human-like texts and build real-time translation, summarization, and other online NLP applications within strict latency requirements.I am facing issue while converting T5 base model using the steps in the above blog. I was able to convert the T5 -small model to TRT using the above blog and the associated notebook.Below is the issue I am facing when converting T5-base to TRT:
PolygraphyException                       Traceback (most recent call last)
 in ()
1 t5_trt_encoder_engine = T5EncoderONNXFile(
2                 os.path.join(onnx_model_path, encoder_onnx_model_fpath), metadata
----> 3             ).as_trt_engine(os.path.join(tensorrt_model_path, encoder_onnx_model_fpath) + “.engine”)
4
5 t5_trt_decoder_engine = T5DecoderONNXFile(8 frames
 in func_impl(network, config, save_timing_cache) in func_impl(serialized_engine)/usr/local/lib/python3.7/dist-packages/polygraphy/logger/logger.py in critical(self, message)
347         from polygraphy.exception import PolygraphyException
348
 → 349         raise PolygraphyException(message) from None
350
351     def internal_error(self, message):PolygraphyException: Invalid Engine. Please ensure the engine was built correctlyI have also tried increasing precision to FP 32. But still getting the same issue.Could you please fill an issue at Issues · NVIDIA/TensorRT (github.com) together with some information of your system, e.g. OS, GPU type, Memory…You can also try the script version, e.g. python3 run.py run T5 trt --variant T5-basehave you solve the problem?Powered by Discourse, best viewed with JavaScript enabled"
3442,discover-industry-breakthroughs-using-ai-technology-at-microsoft-build-2022,"Originally published at:			https://developer.nvidia.com/blog/discover-industry-breakthroughs-using-ai-technology-at-microsoft-build-2022/
Join Microsoft Build 2022 to learn how NVIDIA AI technology solutions are transforming industries such as retail, manufacturing, automotive, and healthcare.Powered by Discourse, best viewed with JavaScript enabled"
3443,nvidia-wins-mlperf-inference-benchmarks,"Originally published at:			NVIDIA Wins MLPerf Inference Benchmarks | NVIDIA Technical Blog
Today, NVIDIA posted the fastest results on new MLPerf benchmarks measuring the performance of AI inference workloads in data centers and at the edge. The new results come on the heels of the company’s equally strong results in the MLPerf benchmarks posted earlier this year.  MLPerf’s five inference benchmarks — applied across a range of…Powered by Discourse, best viewed with JavaScript enabled"
3444,fast-3d-reconstruction-using-get3d,"Hey there. I was wondering if GET3D can be readily used for 3D reconstruction using slices or projections from medical imaging (DICOM, tiff, or NIFTI files)? If so, would it be available to students for free? Thanks.Thanks for the question! Currently we do not support shape generation conditioned on such information due to time constraint.This topic was automatically closed 14 days after the last reply. New replies are no longer allowed.Powered by Discourse, best viewed with JavaScript enabled"
3445,all-of-nvidia-s-gdc-19-slides-in-one-place,"Originally published at:			https://developer.nvidia.com/blog/all-of-nvidias-gdc-19-slides-in-one-place/
We recognize it’s impossible to take in all of the GDC talks you want to see when you’re at the event. We’re here to help! We’ve just made all of NVIDIA’s GDC 2019 slides available to download in one handy place. It’s a one-stop-shop for learning more about real-time ray tracing, machine learning, mesh shading,…Powered by Discourse, best viewed with JavaScript enabled"
3446,let-it-flow-ai-researchers-create-looping-videos-from-still-images,"Originally published at:			https://developer.nvidia.com/blog/ai-looping-videos-from-still-images/
Researchers from University of Washington and Facebook used deep learning to convert still images into realistic animated looping videos.  Their approach, which will be presented at the upcoming Conference on Computer Vision and Pattern Recognition (CVPR), imitates continuous fluid motion — such as flowing water, smoke and clouds — to turn still images into short…You do not need AI to loop still images. There is a website that ha been doing this for 5 about years.hasYour talking about multiple images. This article explains the use of a single image - get your head around that.Hence…AISorry you did not understand my post. I was (and am) talking about a single image. Single image = one photo. Take one photo and animate it. By the way, it’s not “your”, it’s “you’re”.RE:English grammar
I am replying to michaelvandeelen’s comment, but also expanding on it for completeness, fairness, and entertainment value.I agree that squeakycoder, in order to comply with grammatical convention of today’s English, needs to either use the contracted form of you are, or edit to follow “noun phrase” syntax. This is where complexity resides, since a noun phrase allows the possessive pronoun’s use as the headword of a noun phrase, as long as it is understood as a noun phrase (catch-22) or is unambiguous as such.Here are some examples:
Your strategizing nonstop is difficult to watch, with all that pacing and paroxysms of profanity, but the results speak for themselves.
You’re at it again, your talking about god follows the same pattern as always.
Your talking about her betrays your misogyny, so your attempts to portray yourself as noble and her as evil only makes people take her side.Squeakycoder omitted the 2nd part, which likely excludes it from the small percentage of grammatically correct copy, but I found it easy to understand the meaning he was communicating, and I did not detect any underlying motive other than helping you understand what his research is about.If I apply the same analysis to your comments, right away I detected a thinly disguised snarkyness, or maybe professional jealousy if applicable. In any case, an undeniable negative sentiment drives your comments. Now let’s discuss grammar. I would first ask you to clarify what an about year is, as Google search engine was unable to help me understand this unit of metrology. Correct use of grammar would strongly suggest combining your two sentences into a compound sentence, resulting in something like this: Since myimaginarywebsite.bs has successfully harnessed the power of mechanical turks to output so-so animated still images since circa 2016, deep learning AI is obviously superfluous use of Nvidia’s amazing technology.You did not specify which website, contrast their results with the images presented here by the UW team (who made me feel a hint of pride for the UW alumnin association card in my wallet).And finally, in the ad hominem tradition, you neglected the standard politeness of any forum contributor, which frankly, when scientists and engineers present their work, you ought to treat them with the utmost respect. The work of scientists, inventors, researchers, and engineers is the business of creating knowledge, which humanity benefits from en masse. This is also a moment when many researchers feel shy, since they are opening the door to original material they identify with personally and emotionally, and the same door allows criticism of all variety to enter and attack.Regardless of all the rest, if there is really an algorithm that can create animated images with as convincing visuals as this algorithm, I would love to see it. I have seen quite a few previous methods use multiple images, and find nice ways to blend them at the in/out points to create a nice looping feel, but never have I seen starting with a single image, and create a realistic animation. I have seen some cheesy ones that are adding fake ripples in the image, etc.  Really just putting a perspective sin wave displacement, not anything you would ever have a real use for…This I could see possible uses for, but nothing else I have seen before I could same the same. That didn’t stop people from adding really cheesy animation to their online social media post, etc, but overall nothing that I would say directly compares.   But I totally have not seen everything online so perhaps I am oblivious to this other technique… Can you post a webpage, or a paper?  Again I am honestly interested if such an algorithm exists?Is the model and code for this open source??Researchers from University of Washington and Facebook used deep learning to convert still images into realistic animated looping videos. Their approach, which will be presented at the upcoming Conference on Computer Vision and Pattern Recognition (CVPR), imitates continuous fluid motion — such as flowing water, smoke and clouds — to turn still images into shortAny update or links to this ?Powered by Discourse, best viewed with JavaScript enabled"
3447,upcoming-event-level-up-with-nvidia-rtx-in-unity,"Originally published at:			Level Up with NVIDIA
Learn how to leverage the latest NVIDIA RTX technology in Unity Engine and connect with experts during a live Q&A at this webinar on November 16.Powered by Discourse, best viewed with JavaScript enabled"
3448,titan-rtx-slashes-time-for-detecting-osteoporosis,"Originally published at:			TITAN RTX Slashes Time for Detecting Osteoporosis | NVIDIA Technical Blog
Sometimes simply upgrading your GPU can lead to massive performance improvements. The most recent example comes from a team of researchers at Dartmouth College who upgraded from a TITAN Xp GPU to the newly released NVIDIA TITAN RTX. Running their existing code on the new GPU, the team achieved an 80% performance increase when training…Powered by Discourse, best viewed with JavaScript enabled"
3449,ai-can-now-reconstruct-blurry-images,"Originally published at:			AI Can Now Reconstruct Blurry Images | NVIDIA Technical Blog
Researchers from the Lawrence Livermore National Laboratory in California developed a deep learning-based method that can reconstruct blurry or pixelated images without seeing the original. The work is the first that delivers a single unsupervised solution to solving inverse problems. Using a generative adversarial network (GANs), the researchers were able to reconstruct the images in…Powered by Discourse, best viewed with JavaScript enabled"
3450,nvidia-gtc-taking-it-to-the-edge,"Originally published at:			https://developer.nvidia.com/blog/nvidia-gtc-taking-it-to-the-edge/
Register for NVIDIA GTC to learn about 5G, IoT, and edge AIPowered by Discourse, best viewed with JavaScript enabled"
3451,nvidia-on-demand-top-data-science-sessions-from-gtc-2023,"Originally published at:			Playlist | Top Data Science Sessions | NVIDIA On-Demand
Learn from experts about how to optimize a data pipeline or use machine learning for anomaly detection with these 15 educational sessions.Powered by Discourse, best viewed with JavaScript enabled"
3452,tokyo-tech-building-fastest-ai-supercomputer-with-nvidia-technology,"Originally published at:			Tokyo Tech Building Fastest AI Supercomputer With NVIDIA Technology | NVIDIA Technical Blog
The Tokyo Institute of Technology announced they will be using NVIDIA’s accelerated computing platform to build Japan’s fastest AI supercomputer. TSUBAME3.0, is expected to deliver more than two times the performance of its predecessor, TSUBAME2.5, and will be equipped with Pascal-based Tesla P100 GPUs. The supercomputer will excel in AI computation, expected to deliver more…Powered by Discourse, best viewed with JavaScript enabled"
3453,nvidia-researchers-and-collaborators-receive-outstanding-paper-award-at-icml-2020,"Originally published at:			NVIDIA Researchers and Collaborators Receive Outstanding Paper Award at ICML 2020 | NVIDIA Technical Blog
Today the International Conference on Machine Learning (ICML) presented the ‘Outstanding Paper Award’ to researchers from NVIDIA, Stanford University, and Bar Ilan University for their paper, On Learning Sets of Symmetric Elements. The work introduces a principled approach to learning sets of general symmetric elements that can be used in a variety of applications including…Powered by Discourse, best viewed with JavaScript enabled"
3454,inception-spotlight-deepzen-uses-ai-to-generate-speech-for-audiobooks,"Originally published at:			https://developer.nvidia.com/blog/inception-spotlight-deepzen-uses-ai-to-generate-speech-for-audiobooks/
Almost 1,000,000 books are published every year in the United States, however, only around 40,000 of them are converted into audiobooks, primarily due to costs and production time. To help with the process, DeepZen, a London-based company, and a member of the Inception program, NVIDIA’s start-up incubator, developed a deep learning-based system that can generate…Powered by Discourse, best viewed with JavaScript enabled"
3455,tensorrt-7-accelerate-end-to-end-conversational-ai-with-new-compiler,"Originally published at:			https://developer.nvidia.com/blog/tensorrt-7-conversational-ai/
NVIDIA announces new inference speedups for automatic speech recognition (ASR), natural language processing (NLP) and text to speech (TTS) with TensorRT 7.Powered by Discourse, best viewed with JavaScript enabled"
3456,generating-ai-based-accident-scenarios-for-autonomous-vehicles,"Originally published at:			https://developer.nvidia.com/blog/generating-ai-based-accident-scenarios-for-autonomous-vehicles/
Testing autonomous vehicles in potential-accident scenarios is critical for safety. Learn about recent research that explores automatically generating accident scenarios in simulation using AI.Powered by Discourse, best viewed with JavaScript enabled"
3457,nvidia-announces-nsight-graphics-2020-1,"Originally published at:			NVIDIA Announces Nsight Graphics 2020.1 | NVIDIA Technical Blog
Nsight Graphics 2020.1 is now available for download. We have added a number of feature enhancements that improve developer workflow.Powered by Discourse, best viewed with JavaScript enabled"
3458,microsoft-researchers-use-dialogue-to-generate-images,"Originally published at:			https://developer.nvidia.com/blog/microsoft-researchers-use-dialogue-to-generate-images/
Researchers at Microsoft and the University of Montreal recently developed a deep learning system that can generate realistic images from written dialogue. The research, according to Microsoft, is the first known project that uses dialogue, instead of captions, to generate pictures. The researchers used a method called Generative Adversarial Networks (GANs). Previous work in this…Powered by Discourse, best viewed with JavaScript enabled"
3459,ai-ups-accuracy-of-brain-cancer-prognosis,"Originally published at:			AI Ups Accuracy of Brain Cancer Prognosis | NVIDIA Technical Blog
Researchers at Northwestern and Emory Universities recently developed a deep learning algorithm that improves the accuracy of brain cancer prognosis. The study, recently published in Proceedings of the National Academy of Sciences, says the neural network’s predictions were more accurate than those made by highly specialized doctors who undergo years of training for the same…Powered by Discourse, best viewed with JavaScript enabled"
3460,using-the-nvidia-isaac-sdk-object-detection-pipeline-with-docker-and-the-nvidia-transfer-learning-toolkit,"Originally published at:			Using the NVIDIA Isaac SDK Object Detection Pipeline with Docker and the NVIDIA Transfer Learning Toolkit | NVIDIA Technical Blog
This post is the second in a series that shows you how to use Docker for object detection with NVIDIA Transfer Learning Toolkit (TLT). For part 1, see Deploying Real-time Object Detection Models with the NVIDIA Isaac SDK and NVIDIA Transfer Learning Toolkit. Figure 1. Inference bounding boxes from the default tennis ball model trained…Powered by Discourse, best viewed with JavaScript enabled"
3461,implementing-high-precision-decimal-arithmetic-with-cuda-int128,"Originally published at:			https://developer.nvidia.com/blog/implementing-high-precision-decimal-arithmetic-with-cuda-int128/
This post details CUDA’s new int128 support and how to implement decimal fixed-point arithmetic on top of it.Powered by Discourse, best viewed with JavaScript enabled"
3462,microsoft-advancing-ai-powered-cloud-speech-using-gpu-inference,"Originally published at:			Microsoft Advancing AI-Powered Cloud Speech Using GPU Inference | NVIDIA Technical Blog
Delivering speech-driven recommendations in real time with NVIDIA GPU Inference When you ask your phone a question, you don’t just want the right answer. You want the right answer, right now. The answer to this seemingly simple question requires an AI-powered service that involves multiple neural networks that have to perform a variety of predictions…Powered by Discourse, best viewed with JavaScript enabled"
3463,nvidia-merlin-powers-fastest-commercially-available-solution-for-recommender-systems-training,"Originally published at:			https://developer.nvidia.com/blog/nvidia-merlin-powers-fastest-commercially-available-solution-for-recommender-systems-training/
NVIDIA Merlin is the powerhouse behind NVIDIA’s MLPerf .07 submission that establishes NVIDIA as the fastest commercially available solution for recommender system training.Powered by Discourse, best viewed with JavaScript enabled"
3464,cugraph-pain-points,"Where are cuGraph users having issues and/or experiencing difficulties? For example: “doing X is slow” or “I really want to do Y, but cannot”. What are those Xs and Ys?We have a variety of users with different goals and objectives.  Their pain points vary just as widely.  Some of the pain points we have observed:
•        Running out of GPU memory.  Graph algorithms often require auxiliary memory, and sometimes it’s not obvious a priori whether there is sufficient GPU memory
•        Running in multi-node-multi-GPU can be complex.  At the C++ layer we use mpirun to launch multiple processes.  Running in multi-node multi GPU in python is accomplished via dask.  Creating the proper environment to run these tools can be a challenge.Access to working fast inerconnect across multi nodes like IB/NVLINK is a challenge for customers running especially in cloud environments is non trivialThis topic was automatically closed 14 days after the last reply. New replies are no longer allowed.Powered by Discourse, best viewed with JavaScript enabled"
3465,enhancing-memory-allocation-with-new-nvidia-cuda-11-2-features,"Originally published at:			https://developer.nvidia.com/blog/enhancing-memory-allocation-with-new-cuda-11-2-features/
CUDA is the software development platform for building GPU-accelerated applications, providing all the components needed to develop applications targeting every NVIDIA GPU platform for general purpose compute acceleration. The latest CUDA release, CUDA 11.2, is focused on improving the user experience and application performance for CUDA developers. CUDA 11.2 has several important features including programming…Powered by Discourse, best viewed with JavaScript enabled"
3466,nvidia-driveworks-4-0-now-available,"Originally published at:			https://developer.nvidia.com/blog/nvidia-driveworks-4-0-now-available/
NVIDIA DriveWorks 4.0 SDK is now available on the NVIDIA DRIVE Developer site, providing developers with the latest middleware and development environment purpose-built for autonomous vehicles.Powered by Discourse, best viewed with JavaScript enabled"
3467,nvidia-research-at-neurips-2020,"Originally published at:			NVIDIA Research at NeurIPS 2020 | NVIDIA Technical Blog
Researchers, developers, and engineers from all over the world are gathering virtually this year for the 2020 Neural Information Processing Systems (NeurlPS). NVIDIA Research will present its research through spotlight and posters. NVIDIA’s accepted papers at this year’s online NeurIPS feature a range of groundbreaking research in the ﬁeld of neural information processing systems. From…Powered by Discourse, best viewed with JavaScript enabled"
3468,developing-a-pallet-detection-model-using-openusd-and-synthetic-data,"Originally published at:			https://developer.nvidia.com/blog/developing-a-pallet-detection-model-using-openusd-and-synthetic-data/
By iteratively developing with synthetic data, our team developed a pallet detection model that works on real-world images.Hi, could you share the code based on USD Scene Construction Utilities that was used to generate the pallet scene for Replicator?Powered by Discourse, best viewed with JavaScript enabled"
3469,gtc-2020-high-performance-inferencing-at-scale-using-the-tensorrt-inference-server,"GTC 2020 S22418
Presenters: David Goodwin,NVIDIA
Abstract
A critical task when deploying an inferencing solution at scale is to optimize latency and throughput to meet the solution’s service level objectives. We’ll discuss some of the capabilities provided by the NVIDIA TensorRT Inference Server that you can leverage to reach these performance objectives. These capabilities include:
• Dynamic TensorFlow and ONNX model optimization using TensorRT
• Inference compute optimization using advanced scheduling and batching techniques
• Model pipeline optimization that communicates intermediate results via GPU memory
• End-to-end solution optimization using system or CUDA shared memory to reduce network I/O.
For all these techniques, we’ll quantify the improvements by providing performance results using the latest NVIDIA GPUs.Watch this session
Join in the conversation below.Powered by Discourse, best viewed with JavaScript enabled"
3470,how-driveworks-makes-it-easy-to-record-and-replay-data-for-av-development,"Originally published at:			https://developer.nvidia.com/blog/how-driveworks-makes-it-easy-to-record-and-replay-data-for-av-development/
The NVIDIA DriveWorks SDK middleware framework contains everything needed to work on autonomous vehicle software. It also includes everything needed to check that work. The SDK provides a complete set of tools and APIs to quickly get started with recording and replaying data, freeing up valuable development time. Developers can learn how to use these…Powered by Discourse, best viewed with JavaScript enabled"
3471,fujitsu-breaks-imagenet-record-with-v100-tensor-core-gpus,"Originally published at:			https://developer.nvidia.com/blog/fujitsu-breaks-imagenet-record-with-v100-tensor-core-gpus/
Researchers from Fujitsu just announced a new speed record for training ImageNet to 75% accuracy in 74.7 seconds. The new record is faster than the previous test by more than 47 seconds achieved by Sony in November of last year. The team achieved the record by using 2,048 NVIDIA Tesla V100 GPUs, and the MXNet…Powered by Discourse, best viewed with JavaScript enabled"
3472,advanced-kernel-profiling-with-the-latest-nsight-compute,"Originally published at:			https://developer.nvidia.com/blog/advanced-kernel-profiling-with-the-latest-nsight-compute/
Nsight Compute kernel profiler now includes Range Replay, Memory Analysis, and Guided Analysis enhancements.Powered by Discourse, best viewed with JavaScript enabled"
3473,neural-modules-for-fast-development-of-speech-and-language-models,"Originally published at:			Neural Modules for Fast Development of Speech and Language Models | NVIDIA Technical Blog
This post has been updated with Announcing NVIDIA NeMo: Fast Development of Speech and Language Models. The new version has information about pretrained models in NGC and fine-tuning models on custom dataset sections, upgrades the NeMo diagram with the text-to-speech collection, and replaces the AN4 dataset in the example with the LibriSpeech dataset. As a…Powered by Discourse, best viewed with JavaScript enabled"
3474,nvidia-grant-alert-up-to-400k-available-for-cancer-research,"Originally published at:			NVIDIA Grant Alert: Up to $400K Available for Cancer Research | NVIDIA Technical Blog
The NVIDIA Foundation is now accepting proposals for its annual Compute the Cure Cancer Research grant program, which supports researchers using innovative computing methods to advance the fight against cancer. Up to two research grants worth $200,000 each will be awarded to projects that use computational omics to dramatically impact the battle against cancer and…Powered by Discourse, best viewed with JavaScript enabled"
3475,teenager-develops-ai-visual-search-app-that-recognizes-17-000-objects,"Originally published at:			https://developer.nvidia.com/blog/teenager-develops-ai-visual-search-app-that-recognizes-17000-objects/
This week, a high schooler from Seattle, Washington released an iPhone app that uses deep learning to identify what you’re looking at and then displays matching or similar products. The SmartLens mobile app can identify over 17,000 objects such as fruits, plants, household items, office products, animals, tools, and foods. When the app detects an…Powered by Discourse, best viewed with JavaScript enabled"
3476,accelerating-io-in-the-modern-data-center-computing-and-io-management,"Originally published at:			https://developer.nvidia.com/blog/accelerating-io-in-the-modern-data-center-computing-and-io-management/
This is the third post in the Explaining Magnum IO series, which has the goal of describing the architecture, components, and benefits of Magnum IO, the IO subsystem of the modern data center. The first post in this series introduced the Magnum IO architecture; positioned it in the broader context of CUDA, CUDA-X, and vertical…Powered by Discourse, best viewed with JavaScript enabled"
3477,optimizing-production-ai-performance-and-efficiency-with-nvidia-ai-enterprise-3-0,"Originally published at:			https://developer.nvidia.com/blog/optimizing-production-ai-performance-and-efficiency-with-nvidia-ai-enterprise-3-0/
Organizations can reduce development time of production AI with the performance and efficiency optimizations in NVIDIA AI Enterprise 3.0.Powered by Discourse, best viewed with JavaScript enabled"
3478,ai-helps-map-every-building-in-the-u-s,"Originally published at:			AI Helps Map Every Building in the U.S. | NVIDIA Technical Blog
Researchers at the Oak Ridge National Laboratory in Tennessee developed a deep learning-based system that can map every building in the contiguous United States from satellite imagery. The system, which is the first to map the country and the spatial extent between buildings, has the potential to help emergency planning before and after a disaster…Powered by Discourse, best viewed with JavaScript enabled"
3479,managing-data-centers-securely-and-intelligently-with-nvidia-ufm-cyber-ai,"Originally published at:			https://developer.nvidia.com/blog/managing-data-centers-securely-intelligently-ufm-cyber-ai/
The NVIDIA UFM Cyber-AI platform helps to minimize downtime in InfiniBand data centers by harnessing AI-powered analytics to detect security threats and operational issues, as well as predict network failures. This post outlines the advanced features that system administrators can use to quickly detect and respond to potential security threats and upcoming failures, saving costs and ensuring consistent customer service.Powered by Discourse, best viewed with JavaScript enabled"
3480,major-upgrade-to-national-institutes-of-health-supercomputer,"Originally published at:			Major Upgrade to National Institutes of Health Supercomputer | NVIDIA Technical Blog
The Biowulf supercomputer at the National Institutes of Health (NIH) received an upgrade with the addition of 144 NVIDIA GPUs to help researchers discover new cures and save lives. According to CSRA, the system integrator and service company responsible for the installation: “The second stage of computing power announced today will enable NIH researchers to make…Powered by Discourse, best viewed with JavaScript enabled"
3481,cudacasts-episode-17-unstructured-data-lifetimes-in-openacc-2-0,"Originally published at:			https://developer.nvidia.com/blog/cudacasts-episode-17-unstructured-data-lifetimes-openacc-2-0/
The OpenACC 2.0 specification focuses on increasing programmer productivity by addressing limitations of OpenACC 1.0. Previously, programmers were required to use structured code blocks to control when to transfer data to or from the device, which limited the applications that could quickly be accelerated without major code restructuring. It also prevented adding OpenACC directives to…Powered by Discourse, best viewed with JavaScript enabled"
3482,accelerated-data-analytics-faster-time-series-analysis-with-rapids-cudf,"Originally published at:			https://developer.nvidia.com/blog/accelerated-data-analytics-faster-time-series-analysis-with-rapids-cudf/
This post walks you through the common steps of time series data processing with RAPIDS cuDF.Powered by Discourse, best viewed with JavaScript enabled"
3483,drive-labs-how-multi-view-lidarnet-presents-rich-perspective-for-self-driving-cars,"Originally published at:			https://developer.nvidia.com/blog/drive-labs-how-multi-view-lidarnet-presents-rich-perspective-for-self-driving-cars/
This is the latest post in our NVIDIA DRIVE Labs series, which takes an engineering-focused look at individual autonomous vehicle challenges and how NVIDIA DRIVE addresses them.Powered by Discourse, best viewed with JavaScript enabled"
3484,accelerated-data-analytics-machine-learning-with-gpu-accelerated-pandas-and-scikit-learn,"Originally published at:			https://developer.nvidia.com/blog/accelerated-data-analytics-machine-learning-with-gpu-accelerated-pandas-and-scikit-learn/
Learn how GPU-accelerated machine learning with cuDF and cuML can drastically speed up your data science pipelines.Powered by Discourse, best viewed with JavaScript enabled"
3485,jetson-xavier-nx-brings-cloud-native-agility-to-edge-ai-devices,"GTC 2020 S22707
Presenters: Suhas Sheshadri, NVIDIA
Abstract
Cloud-native technologies on AI edge devices are the way forward. Join us to learn how NVIDIA Jetson is bringing the cloud-native transformation to AI edge devices. We’ll be covering an in-depth demo showcasing Jetson’s ability to run multiple containerized applications and AI models simultaneously.  In this webinar, we’ll cover: How to build a container and deploy on Jetson; Insights into how microservice architecture, containerization, and orchestration have enabled cloud applications to escape the constraints of monolithic software workflows; A detailed overview of the latest capabilities the Jetson Family has to offer, including Cloud Native integration at-the-edge.Watch this session
Join in the conversation below.Powered by Discourse, best viewed with JavaScript enabled"
3486,cuda-heterogeneous-memory-management-hmm-support,"How will the CUDA 12.2 HMM impact my A100-based system?A good way to think about HMM is that it’s like UVM, but better. So where UVM gives you the ability to do things like using managed memory to make data migration easier, oversubscribe memory, etc. when you allocate memory through specific UVM APIs like cudaMallocManaged(), HMM gives you all of that with standard malloc() / free(). It makes it so much easier to work with large applications and libraries that might have their own internal memory management, but still use your GPU for acceleration.Powered by Discourse, best viewed with JavaScript enabled"
3487,finite-difference-methods-in-cuda-c-c-part-1,"Originally published at:			https://developer.nvidia.com/blog/finite-difference-methods-cuda-cc-part-1/
In the previous CUDA C/C++ post we investigated how we can use shared memory to optimize a matrix transpose, achieving roughly an order of magnitude improvement in effective bandwidth by using shared memory to coalesce global memory access. The topic of today’s post is to show how to use shared memory to enhance data reuse…My experiments show that if sPencils=1 the result will be better.Besides, for mesh larger than 64^3, for example 1024^3, shared memory is not enough to support the tile method and become very difficult to design block size and indexing.  Can you provide more details on the hardware you're using?My program ran on a Quadro  K4000 card.Would it be more or less efficient, if we calculated all the derivatives in the same kernel with smaller (say 16 x 16) block. How does data reuse compare with coalesced memory access, which to be fair may not be prefect in practical cases?Arbitrarily (large) sized grids - naively, I changed mx=my=mz for the grid size (originally 64^3) to 92^3 (i.e. mx=my=mz=92) and anything above 92, I obtain a Segmentation fault (core dumped).  I was simply curious what was happening; is it a limitation on the GPU hardware?  If so, which parameter? I'm on a NVIDIA GeForce GTX 980Ti.The program won't let you set mx/my/mz to 92 because it is not a whole multiple of 32. It will let you set it to 96. When I do this on a Tesla M40 (same compute capability as your 980Ti) I get no errors. When I try it on my older 750 M (in my laptop), I get a segfault and I'm having trouble finding the root cause. What version of CUDA are you using?I think I figured it out. It looks like it was a stack overflow caused by the arrays f, sol, etc in the function runTest. I didn't get any error, but when they were too large, it would crash as soon as the function is called. I checked in a fix -- just changed to allocate these three arrays on the heap with new instead of making them stack arrays.Let me know if this lets you use larger sizes.@Mark_Harris:disqus Thanks for looking into this.With your latest reply, I'm assuming it's on the github repository?  I'll go there right now.Just for completeness, for the second to last reply, once I was obtaining Segmentation faults for multiples of 32, e.g. 128^3, I made a copy of the finitedifference.cu code and commented out the ""long pencils"" lpencils and tested out the ""small pencils"" derivatives and tried to get to where it fails (small pencils, unless I'm wrong, I think only need multiples of 4 for the tile sizes), and ""got up to"" 92.Here's the device name print out and compute capability; I can print out more info if it may help in the next reply (i.e. cudaGetDeviceProperties)```Device name: GeForce GTX 980 Ti  Compute Capability: 5.2Segmentation fault (core dumped)  ```I'm on CUDA ToolKit 7.5P.S. I became very interested in your post because I wanted to implement 3-dim. Navier-Stokes Eq. with finite-difference methods on CUDA C/C++ and obviously derivatives are needed for the gradient and divergence.  Has this already been done? (and is it on github?) I Google and github searched as much as I could already.I made a copy from https://github.com/parallel... and I changed the grid size to 352^3 - any choice above 352, e.g. say for mx=384, but my=mz=352, or all 3, mx=my=mz=352, I obtain a compilation error (only doing nvcc finite-difference.cu) in the direction with too large of a grid dimension:ptxas error   : Entry function '_Z21derivative_x_lPencilsPfS_' uses too much shared data (0xc400 bytes, 0xc000 max)Is this because of the inherent limitation of the GTX 980Ti itself? (I can cudaGetDeviceProperties and print out the max. threads per dimension) Or something inherently limited about shared data per block and so this method can't scale beyond, on the GPU's shared data per block?  For instance, I can do a naive implementation of derivatives on total threads in 1 dim. of 1920 threads (1920x1920x32) on the __global__ memory of the device GPU, 16x16x4 threads per block; any much ""bigger"" kernel launch I do results in segmentation fault.   Using that pencil method, as an improvement of the ""naive"" shared memory method (i.e. tiling per block, with 2 ""dark orange"" region blocks, the 4x16 in the very first example in the article above), I believe you'd need a 1920x4 tile, which may not work, if I can only compute the derivative on a 352x352x352 grid? (note 352^3 < 1920*1920*32)You need to modify the code to decouple your total domain size from the tile size. The tile size is limited by available shared memory (see the post on shared memory). But there's no reason you can't do domains that are much larger (limited by device memory, or system memory if you use Unified Memory).I don't understand what it means to ""decouple"" the total domain size from the tile size.  Let's take the concrete example of finite_difference.cu.  The grid dimensions of the desired grid is (mx,my,mz).  The size of a tile, as above in this article, is mx*sPencils (for the case of the x-direction).  This seems to be the heart of the ""pencils"" scheme, choosing the size of the tile in the direction of the derivative to be the entire mx (for the x-direction).  This scheme is unlikely to be to scale at all, as look at the grid dimensions and the block dimensions that you're seeking to launch for each of the kernel function (calls) derivative_x (and derivative_y, derivative_z):(my/spencils, mz,1) for the grid dimensions (number of blocks in each dimension) and (mx,spencils,1) for the block dimensions (number of threads in each (thread) block)Then you'd have mx*spencils*1 threads to launch on each block, which is severely limited by the maximum number of threads per block.I do cudaGetDeviceProperties (or I run the code queryb.cu off my github repository https://github.com/ernestya... and on the GTX 980Ti, I get 1024 Max threads per block.However, the number of blocks that can be launched on the grid is much higher (Max grid dimensions:   (2147483647, 65535, 65535) ) and so to possibly ""save"" the pencil method would be to try to launch more blocks.Also note that we're, in this code, finite_difference.cu, demanding __shared__ float s_f[sPencils][mx+8] an array of floats of size sPencils*(mx+8) and if we want our grid to scale to larger values than 64, say mx = 1920, then we'd have severe demands on the shared memory.  I'm not sure if the domain size, mx in this case, can be decoupled from the tile size, which I thinking is, correct me if I'm wrong, this __shared__ 2-dim. array in this case, which depends directly on mx, without throwing out this idea of pencils entirely.I don't know how to specifically modify the example here (finite_difference.cu) to scale beyond a grid of 354^3: I'm suspecting this is the maximum because of the Max threads per block (1024) for the GTX 980 Ti.  I've tried changing the size of I also don't understand how to decouple this domain size from the tile size, without throwing out the entire idea of pencils.I put a write up of this post, because, at least to me, writing in LaTeX is much clearer, here: https://github.com/ernestya...pp. 30 or the section ""Note on finite-difference methods on the shared memory of the device GPU, in particular, the pencil Then the total number of threads launched in each direction, x and y, is method, that attempts to improve upon the double loading of boundary “halo” cells (of the grid).""Threads can execute loops, so you can loop and process multiple tiles of the entire domain using a single thread block. don't try to fit the entire domain in shared memory at once. That's what I mean by decouple.I'm sorry Ernest, but I have to leave this as an exercise for you, I don't have time to solve it for you. I didn't develop this approach, I only translated the Fortran posts that Greg Ruetsch wrote to C++ (https://devblogs.nvidia.com...Thanks a lot for these clear tutorials!When I run the FDM code on my GTX970 with CUDA 8, there is almost no difference between 4x64 and 32x64 for Y and Z derivatives. Does it mean that coalescence is not so important any more ? :)Powered by Discourse, best viewed with JavaScript enabled"
3488,running-docker-containers-directly-on-nvidia-drive-agx-orin,"Originally published at:			https://developer.nvidia.com/blog/running-docker-containers-directly-on-nvidia-drive-agx-orin/
Learn how to run a few sample applications inside a Docker container running on the target NVIDIA DRIVE AGX hardware.https://developer.nvidia.com/blog/running-docker-containers-directly-on-nvidia-drive-agx-orin/
Not Found@weijia.li  – Sorry about that! There was a delay in the release announcement but we’ll get this post back out early next year.Hi, @jwitsoeFriendly ping, but is this feature of running docker containers directly on drive agx orin available for now 2023-03-10 for the sdk version of 6.0.4?Thanks and looking forward to your reply!Hi @lizhensheng,
The feature is available from SDK version 6.0.5 onwards.The feature is available from SDK version 6.0.5 onwards.Thank you for your quick reply!I know about the runtime-container for jetson agx which is l4t-jetpack NVIDIA L4T JetPack | NVIDIA NGC that containing all sdk provided by nvidia in the jetson container.Is there a runtime-container for drive agx products like l4t-jetpack?
Is there any tutorial about building the runtime-container in the drive agx orin that can make use of drive os sdk? (perhaps the name would be v5l-drivesdk)Thanks.Only host side DRIVE OS SDK docker containers with are available as of now. There is no tutorial as of now, as we recommend keeping the development only on the host machine. With that, currently the blog post will help with running sample applications within docker containers on the target.Hey @kchemudupati  , from the blog you sharing, seems we can only run docker container on orin target with runtime mode, is they any possible we can create personal docker image base on the running runtime container, so we can exec it and do some debug? ThanksYou should be able to run the containers without --runtime flag as well. It is used when access to the GPU is required.You should also be able to build your own docker images and run them just like on a host system.build your own docker images and run them just like on a host system.Thanks. What confused us is that we need to run program on aarch64 architecture ,but we cannot find an aarch64 architecture already with DRIVE OS base docker image in https://catalog.ngc.nvidia.com/, and we cannot get the critical deb package since we are develper so we cannot build our personal image?So do you have any good idea about that? Thanks for your favor~Yes, there are currently no aarch64 architecture DRIVE OS base docker images, as we currently recommend the building and the development to be done on the host machine itself.You should be able to mount the targetfs to an aarch64 QEMU environment on host machine to build your image there, or you can flash the target and build your image directly on the target itself. Additionally, as mentioned in the blog post you should be able to edit the two csv files to make any libraries and drivers available inside the container.Hi @kchemudupati, I saw you built the sample code directly on target, how can I do that? I don’t have nvcc on my target.I saw you built the sample code directly on target, how can I do that? I don’t have nvcc on my target.As there is no any compile-debug toolchain in target-orin-kit, the sample code is cross-compiled in the host machine with deb/docker of DriveSDK development environment.@kchemudupati  am I right?Thanks.Yes, there are currently no aarch64 architecture DRIVE OS base docker images, as we currently recommend the building and the development to be done on the host machine itself.@kchemudupatiDo you know any roadmap about this?Thanks.The blogpost showed compiling a small CUDA sample directly on the target for simplicity purposes. The ideal recommended method is to cross-compile on the host.You can run the sample compilation command on the target after flashing the DRIVE AGX Orin with DRIVE OS SDK. You should then be able to find the CUDA toolchain and samples directly on the target at /usr/local/cuda-11.4/ as shown in the blogpost.As there is no any compile-debug toolchain in target-orin-kitI wonder if you agree with the priciples above? @kchemudupatiIf you agree I would have the qst to ensure that any cross-compiled program from host-driveos-docker would running well in the target-orin-docker.Thanks!@kchemudupatiWhat’s the internal mechnisim of /etc/nvidia-container-runtime/host-files-for-container.d/? Is it something like docker run -v  filesystem mapping?Thanks.What’s the internal mechnisim of /etc/nvidia-container-runtime/host-files-for-container.d/? Is it something like docker run -v  filesystem mapping?After searching and reading, I know this is the csv mode of nvidia-container-runtime.@kchemudupatiCould you help me with this topic when I use the target-docker-container with non-root-user?[BUG] target-docker-container running cuda-samples require unintended extra permission - DRIVE AGX Orin / DRIVE AGX Orin General - NVIDIA Developer ForumsThanks!Powered by Discourse, best viewed with JavaScript enabled"
3489,gpu-pro-tip-cuda-7-streams-simplify-concurrency,"My CUDA version is 7.5, I am using Visual Studio 2013, device is GTX 850M but when I run the program, nvvp shows no timeline. What can be the problem ?And when I run nvprof, it says :==8128== NVPROF is profiling process 8128, command: CudaTest.exe==8128== Profiling application: CudaTest.exe==8128== Profiling result:No kernels were profiled.==8128== API calls:No API activities were profiled.I test the first one code on my PC, in the linux, it works fine. But in the windows, with VS2013 the stream doesn't run concurrently, with the ""default-stream per-thread"" flag. Also, I compile the code in the command-line ""nvcc –default-stream per-thread ./stream_test.cu -o stream_per-thread"". It doesn't work concurrently too.dear Mr. Harris,I have try out your instructions above and then I come an idea like this.void threadExecute(void *input_data, int nx){cufftComplex *data = (cufftComplex*)input_data;cufftHandle plan;cufftPlan1d(&plan, nx, CUFFT_C2C, 1);cufftSetStream(plan, this_stream);cufftExecC2C(plan, data, data, CUFFT_FORWARD);cufftDestroy(plan);cudaStreamSynchronize(0);}As I understand, each CPU thread will be given it own stream on GPU. Does it correct ?If it is correct how can I get the stream that is assigned to each CPU thread so that I can pass it to the cufftSetStream().If it is not correct so how can I use cufft API with multiple CPU thread and multiple stream?Could you please help me with this?I will be very appreciate.See my answer above regarding the NPP library, which is similar. If you follow the instructions in the post and compile your code to use ""-default-stream per-thread"", you should be able to pass cudaStreamPerThread to cufftSetStream() so that it uses the default stream in each thread. Does this work?Hello Mark,I understand that as you mentioned, Enabling PTDS for your compilation units doesn't enable it for libraries that are separately compiled.But I wish to enable PTDS for thrust libraryHow to I call thrustSetStream() Thank you.To set a stream for a Thrust algorithm you need to use the .on() method on the cuda::par execution policy, like so:    thrust::sort(thrust:cuda::par.on(stream), begin, end, comparator)Hi Mark,The example above works perfectly in my ubuntu system as well.I could even obtained Figure 2 shown above using the commandnvcc --default-stream per-threadHowever, when I try to do the same thing in the Nsight editor, I do not see the effect of  --default-stream per-thread command.I have written the command "" --default-stream per-thread"" on the Command box  as ""nvcc --default-stream per-thread"" on the project properties -> settings -> Tool Settings in the NVCC Compiler.I suspect if this is the correct place to put this flag. I have even tried putting it on the Build Stages -> Preprocessor options (-Xcompiler) but that too did not work.Could you please guide me where should I put this command on the Nsight Editor.Thanks and Warm RegardsAmit GurungHi Amit,Try adding the flag in Project Properties -> Settings -> Tool Settings -> NVCC Compiler -> Expert Setting:1.  ${COMMAND} --default-stream per-thread ${FLAGS} ${OUTPUT_FLAG} ${OUTPUT_PREFIX} ${OUTPUT} ${INPUTS}Thanks a lot Mark, it is indeed an Expert advice. It worked fine.Thanks again I hope this answer will be very beneficial to all.Hi, Mark,Could you please have a look at this profile output generated using the nvprof command and comment why I am not able to see the concurrency in the kernel executions (just like the above Figure 2 or Figure 4, kernels one below the others) note that I do not even have any kernel in the Default stream, i.e., I only have kernels using 10 streams, I have also used the --default-stream per-thread flag as per your instruction (which works fine for the example that you have posted above).The profile output link ( https://drive.google.com/op... )Thanks a lot in advance.I can't access that link.I have made it public now, anyone can download it.Thanks once again.That file doesn't have an extension. I have no idea what format it is. I can't risk opening it. Suggest you post a screenshot.Sorry for the file with no extension.I have the file now with the extension .nvprof as shown in one of your post for output of nvprof. Kindly find the new link (  https://drive.google.com/op...   )I would have posted the screenshot but my profile output does not fit in a single screen and if I zoom out then it may not make any sense.I hope you can access the output file now.Thank you once again.I have edited the typo with the above link, I hope you can access it now.Hi Mark, I have a question about the multi-threading example.Each of 8 threads calls the kernel but have all the 8 threads access to the same global memory or each thread works to its own global memory (of its kernel)?In the multi-threaded example, each of the 8 threads calls launch_kernel, which allocates memory with cudaMalloc. So in this case all threads access different regions of memory.Thanks for the answer! But it is poissible to allocate once (in the global memory) and then all the other threads access this region of memory every time they call the kernel? So they can read the same data?Yes i think that should work.We can see your code every kernel just occupy one grid block: kernel<<<1, 64>>>(data, N);, so every kernel execution should fully occupy GPU resouce, however, we can see the timeline ,when concurrency comes. every kernel execution time would be much longer than serialization condition. that is why？Powered by Discourse, best viewed with JavaScript enabled"
3490,register-today-for-jetson-developer-meetup-at-nvidia-headquarters,"Originally published at:			Register Today for Jetson Developer Meetup at NVIDIA Headquarters | NVIDIA Technical Blog
Developers working on next-generation autonomous machines, including AI-powered delivery robots, drones and more, should head to NVIDIA Endeavor on Wednesday, December 12 from 6:00 pm to 9:00 pm. NVIDIA will be hosting a developer meetup tailored for users of the NVIDIA Jetson platform, including our latest Jetson AGX Xavier. The event will be a great…Powered by Discourse, best viewed with JavaScript enabled"
3491,ai-helps-farmers-predict-crop-production,"Originally published at:			https://developer.nvidia.com/blog/ai-helps-farmers-predict-crop-production/
The startup, Earth Observing Systems (EOS), uses NVIDIA GTX 1080 GPUs, and NVIDIA Tesla V100 GPUs on the Amazon Cloud, with the cuDNN-accelerated TensorFlow deep learning framework to train their algorithm on both historical and current observations, including satellite imagery and historical data. Once trained, EOS implements the deep learning system to calculate crop conditions…Powered by Discourse, best viewed with JavaScript enabled"
3492,rendering-millions-of-dynamic-lights-in-real-time,"Originally published at:			Rendering Millions of Dynamic Lights in Real-Time | NVIDIA Technical Blog
Today NVIDIA is releasing the ACM SIGGRAPH 2020 research paper showing how to render dynamic direct lighting and shadows from millions of area lights in real-time. This was previously impossible.Powered by Discourse, best viewed with JavaScript enabled"
3493,mark-cuban-and-dick-parsons-invest-in-computer-vision-startup,"Originally published at:			https://developer.nvidia.com/blog/mark-cuban-and-dick-parsons-invest-in-computer-vision-startup/
Packing an NVIDIA Tegra K1 GPU and a camera, Percepto is developing hardware and software that drones can use to handle computer vision processes such as obstacle avoidance. Percepto recently closed a $1 million seed round led by top investors to help launch their open source computer vision solution to drone manufacturers, developers and drone…Powered by Discourse, best viewed with JavaScript enabled"
3494,how-to-deploy-an-ai-model-in-python-with-pytriton,"Originally published at:			https://developer.nvidia.com/blog/how-to-deploy-an-ai-model-in-python-with-pytriton/
Learn how to use NVIDIA Triton Inference Server to serve models within your Python code and environment using the new PyTriton interface.Powered by Discourse, best viewed with JavaScript enabled"
3495,what-is-a-smart-hospital,"Originally published at:			What Is a Smart Hospital? | NVIDIA Blog
A smart hospital relies on data-driven insights, including machine learning models and AI-powered medical devices, to facilitate decision-making.Powered by Discourse, best viewed with JavaScript enabled"
3496,understanding-the-need-for-time-sensitive-networking-for-critical-applications,"Originally published at:			Understanding the Need for Time-Sensitive Networking for Critical Applications | NVIDIA Technical Blog
With increasing speeds in network infrastructures, timing constraints become ever tighter. Carefully managing time within modern networked applications is now a requirement.Powered by Discourse, best viewed with JavaScript enabled"
3497,rtx-coffee-break-ray-traced-ambient-occlusion-4-17-minutes,"Originally published at:			https://developer.nvidia.com/blog/rtx-coffee-break-ray-traced-ambient-occlusion-417-minutes/
SSAO (Screen Space Ambient Occlusion) is popular, but a limited process being used in contemporary games. Ray tracing provides better results. We explain why, and prove it with a set of side-by-side examples. Five Things to Remember: SSAO is a less-than-ideal process: it darkens the corners and edges in a scene, and leaves a dark…Powered by Discourse, best viewed with JavaScript enabled"
3498,nvidia-hpc-sdk-now-available-for-free-download,"Originally published at:			NVIDIA HPC SDK Now Available For Free Download | NVIDIA Technical Blog
The NVIDIA HPC SDK is a comprehensive suite of compilers, libraries, and tools enabling HPC developers to program the entire HPC platform from the GPU foundation to the CPU, and through the interconnect. It is the only comprehensive, integrated SDK for programming accelerated computing systems. Today, NVIDIA announces General Availability (GA) of HPC SDK for…Powered by Discourse, best viewed with JavaScript enabled"
3499,nvidia-releases-new-asr-model-and-speech-toolkit-at-interspeech-2019,"Originally published at:			https://developer.nvidia.com/blog/new-asr-model-speech-toolkit-interspeech2019/
As speech recognition applications become mainstream and get deployed through devices in the home, car, and office, research from academia and industry for this space has exploded. To present their latest work, global AI leaders and developers, including NVIDIA researchers, will come together in Austria next week to discuss the latest  automatic speech recognition (ASR)…Powered by Discourse, best viewed with JavaScript enabled"
3500,startup-goat-uses-ai-to-verify-shoe-authenticity,"Originally published at:			Startup GOAT uses AI to Verify Shoe Authenticity | NVIDIA Technical Blog
Shoes are the most counterfeited products in the world so if you’re looking to get a pair of Nike Air Force 1 on a second-hand online marketplace such as Craigslist or eBay, how can you trust they are real? Sneaker-selling startup GOAT is fighting the problem by using AI to verify the shoe’s authenticity. “When…Powered by Discourse, best viewed with JavaScript enabled"
3501,nvidia-rolls-out-new-drivers-for-vulkan-ray-tracing-upgrades-quake-ii-rtx,"Originally published at:			NVIDIA Rolls Out New Drivers for Vulkan Ray Tracing, Upgrades Quake II RTX | NVIDIA Technical Blog
Vulkan is the industry’s first open, cross-vendor standard ray tracing API, enabling portable ray tracing acceleration across diverse platforms. In November 2020, The Khronos Group released the final versions of the Vulkan Ray Tracing extension specifications that seamlessly integrate ray tracing into the existing Vulkan framework so that developers can reach more platforms and customers…Powered by Discourse, best viewed with JavaScript enabled"
3502,boosting-inline-packet-processing-using-dpdk-and-gpudev-with-gpus,"Originally published at:			https://developer.nvidia.com/blog/optimizing-inline-packet-processing-using-dpdk-and-gpudev-with-gpus/
Inline processing of network packets using GPUs is a packet analysis technique useful to a number of different applications.This was a really interesting article. Looking at the DPDK links I noticed the only supported cards the driver supports are  V100/A100 Tesla class GPU’s. Would there be any intention to change this in the future to lower spec Quadro cards? Also is this technique specific to infiniband or can it be used for vanilla Ethernet too?I recently extended the support for more GPUs dpdk/devices.h at main · DPDK/dpdk · GitHub if your Tesla or Quadro GPU is not there please let me know and I will add it.
You can use whatever card supports GPUDirect RDMA to receive packets in GPU memory but so far this solution has been tested with ConnectX cards only.Great - my card (A4000) is there now - thanks! I’m intending to test this with a ConnectX-5 NIC to process ethernet packets. One more question - is a multi GPU setup required for this - I am assuming for persistent kernels this is a requirement, but how about for method 4 ?  I would like to try a dev setup with the A4000 card as both my display device and a packet processor.Thank you for an interesting article. How is the data consistency issue (as described in [1]) resolved for the persistent kernel (method 3)? As I understand the GDRCopy translates to RDMA operations, but afaik ordering between RDMA ops aren’t ensured from the perspective of a concurrently running GPU kernel.[1] GPUDirect RDMA :: CUDA Toolkit DocumentationNo you don’t need a multi-GPU setup with any of the methods described in the post. As an example, in case of persistent kernel you need to tune the number of CUDA blocks (i.e. persistent kernel occupancy) to not occupy the entire GPU and have SMs available for other processing kernelsTo address the data consistency issue you can use the rte_gpu_wmb() function before notifying to the running CUDA kernel that a new set of packets is ready. You can find an example hereI have the same question about the A4000.  I modified the cuda.c driver to include the A4000
device id, but I’m getting a crash in the mlx5 driver (I’m using a ConnectX-4):#0  0x00005555563b35f7 in mlx5_tx_burst_mti ()
#1  0x0000555556233c09 in rte_eth_tx_burst (nb_pkts=, tx_pkts=, queue_id=2, port_id=0)
at /home/rick/NVIDIA/l2fwd-nv/src/main.cpp:570
#2  tx_core (arg=0x2) at /home/rick/NVIDIA/l2fwd-nv/src/main.cpp:584
#3  0x0000555555763f04 in eal_thread_loop.cold () at /home/rick/NVIDIA/l2fwd-nv/src/main.cpp:757
#4  0x00007ffff7f16609 in start_thread (arg=) at pthread_create.c:477
#5  0x00007ffff7a85133 in clone () at …/sysdeps/unix/sysv/linux/x86_64/clone.S:95I have the same question about the A4000.  I modified the cuda.c driver to include the A4000
device id, but I’m getting a crash in the mlx5 driver:#0  0x00005555563b35f7 in mlx5_tx_burst_mti ()
#1  0x0000555556233c09 in rte_eth_tx_burst (nb_pkts=, tx_pkts=, queue_id=2, port_id=0)
at /home/rick/NVIDIA/l2fwd-nv/src/main.cpp:570
#2  tx_core (arg=0x2) at /home/rick/NVIDIA/l2fwd-nv/src/main.cpp:584
#3  0x0000555555763f04 in eal_thread_loop.cold () at /home/rick/NVIDIA/l2fwd-nv/src/main.cpp:757
#4  0x00007ffff7f16609 in start_thread (arg=) at pthread_create.c:477
#5  0x00007ffff7a85133 in clone () at …/sysdeps/unix/sysv/linux/x86_64/clone.S:95ConnectX4 is an old card and it’s been a while since I tested this solution on it. Can you build DPDK in debug mode and provide more info about the problematic line in mlx5_tx_burst_mti() function?Re-ran with debug enabled in dpdk lib (below).I ordered a ConnectX-5, which should arrive next week or so, i.e. hopefully newer board.hread 9 “lcore-worker-5” received signal SIGSEGV, Segmentation fault.
[Switching to Thread 0x7fffd7fff000 (LWP 527851)]
0x00005555567a5be4 in mlx5_tx_eseg_data (olx=83, tso=0, inlen=18, vlan=0, wqe=0x7ff7ff9a3000,
loc=0x7fffd7ff95d0, txq=0x7ff7ff9e4380) at …/drivers/net/mlx5/mlx5_tx.h:1016
1016		es->inline_data = *(unaligned_uint16_t *)psrc;
(gdb) where
#0  0x00005555567a5be4 in mlx5_tx_eseg_data (olx=83, tso=0, inlen=18, vlan=0,
wqe=0x7ff7ff9a3000, loc=0x7fffd7ff95d0, txq=0x7ff7ff9e4380)
at …/drivers/net/mlx5/mlx5_tx.h:1016
#1  mlx5_tx_burst_single_send (olx=83, loc=0x7fffd7ff95d0, pkts_n=64, pkts=0x7ff7f34a8b48,
txq=0x7ff7ff9e4380) at …/drivers/net/mlx5/mlx5_tx.h:3222
#2  mlx5_tx_burst_single (olx=83, loc=0x7fffd7ff95d0, pkts_n=64, pkts=0x7ff7f34a8b40,
txq=0x7ff7ff9e4380) at …/drivers/net/mlx5/mlx5_tx.h:3366
#3  mlx5_tx_burst_tmpl (olx=83, pkts_n=64, pkts=0x7ff7f34a8b40, txq=0x7ff7ff9e4380)
at …/drivers/net/mlx5/mlx5_tx.h:3564
#4  mlx5_tx_burst_mti (txq=0x7ff7ff9e4380, pkts=0x7ff7f34a8b40, pkts_n=64)
at …/drivers/net/mlx5/mlx5_tx_nompw.c:27
#5  0x00005555556f99e9 in rte_eth_tx_burst (nb_pkts=, tx_pkts=0x7ff7f34a8b40,
queue_id=2, port_id=0) at /home/rick/NVIDIA/l2fwd-nv/src/main.cpp:570
#6  tx_core (arg=0x2) at /home/rick/NVIDIA/l2fwd-nv/src/main.cpp:588
#7  0x000055555aa8926f in eal_thread_loop (arg=0x0) at …/lib/eal/linux/eal_thread.c:140
#8  0x00007ffff7f16609 in start_thread (arg=) at pthread_create.c:477
#9  0x00007ffff7a78133 in clone () at …/sysdeps/unix/sysv/linux/x86_64/clone.S:95Did you disable the tx inlining when launching l2fwd-nv? As an example, -a b5:00.1,txq_inline_max=0Yes:$L2FWD/build/l2fwdnv -l 0-9 -n 1 -a 06:00.0,txq_inline_max=0 -a 01:00.0 – -m 1 -w 0 -b 64 -p 4 -v 0 -z 0Interestingly, if I reduce the number of cores and the number of pipelines, the app
doesn’t segv, and seems to behave.   This might be a learning curve issue, sorry for the false alarm.Thanks for the reply. I was rather wondering how I could best ensure the consistency with pure RDMA (and not DPDK). Can I simply issue a local RDMA write from the CPU to GPU after the CPU has detected the new RDMA message on the GPU (e.g., through an RDMA completion event). When the GPU detects the flag, is it then ensured that the original RDMA message is completely consistent on the GPU? even for concurrently running GPU kernels?Great blog post and very good sample code in l2fwd-nv with GPU. Do you have it working with DPDK/DOCA on convergent DPU with GPU?  My version of DPU uses different version of DPDK so I was not sure if all dependencies will be satisfied or updated version of DPDK will soon enough make to converged DPU.The DPDK you find in the DPU already has this gpudev library installed so you can just use it. Anyway if you want your own DPDK version with gpudev you can just download the upstream DPDK from github and build it for arm64 on your DPU.Thank you for this great article. btw I have a question on you posting. Form figure 13, peak I/O performance throughput for the CPU and GPU are the same. In my understanding, using CPU memory means vanilla DPDK right? If so, afaik ConnectX6-Dx with DPDK can achieve the line rate at 64-byte packets also. But, from your posting the throughput for 64-bytes packets are just below 20 Gbps… why is that?Below is the DPDK performance report for ConnectX6-Dx.
https://fast.dpdk.org/doc/perf/DPDK_20_11_Mellanox_NIC_performance_report.pdfHello, may I ask why the l2fw-nv project uses a 60B digital packet as a segmentation data? Is there any basis for this?Powered by Discourse, best viewed with JavaScript enabled"
3503,fast-and-scalable-ai-model-deployment-with-nvidia-triton-inference-server,"Originally published at:			Fast and Scalable AI Model Deployment with NVIDIA Triton Inference Server | NVIDIA Technical Blog
Deploying fast and scalable AI models with NVIDIA Triton Inference Server supports high-performance.Powered by Discourse, best viewed with JavaScript enabled"
3504,france-to-install-a-new-gpu-accelerated-supercomputer,"Originally published at:			France To Install a New GPU-Accelerated Supercomputer | NVIDIA Technical Blog
France’s Institute for Development and Resources in Intensive Scientific Computing has just announced plans to build a new GPU-accelerated supercomputer designed for AI workloads. “Artificial intelligence (AI) is rapidly influencing the next wave of digital experiences, and in France, we see it as a major opportunity for scientific and economic growth,” said Gilles Thiebaut, Vice…Powered by Discourse, best viewed with JavaScript enabled"
3505,learn-how-to-set-up-hardware-ray-tracing-in-unreal-engine-5,"Did you ever wonder how you can get started with hardware ray tracing in Unreal Engine 5? This video will give you a short overview of how to do just that.This is part of a series of Unreal 5 related introductory videos first listed in our technical blog on the developer pages. https://developer.nvidia.com/blog/shaping-the-future-of-graphics-with-nvidia-technologies-in-unreal-engine-5/Powered by Discourse, best viewed with JavaScript enabled"
3506,the-full-stack-optimization-powering-nvidia-mlperf-training-v2-0-performance,"Originally published at:			https://developer.nvidia.com/blog/boosting-mlperf-training-performance-with-full-stack-optimization/
Learn about the full-stack optimizations that enabled the NVIDIA platform to deliver even more performance in MLPerf Training v2.0.Powered by Discourse, best viewed with JavaScript enabled"
3507,accelerating-etl-on-kubeflow-with-rapids,"Originally published at:			Accelerating ETL on KubeFlow with RAPIDS | NVIDIA Technical Blog
Using RAPIDS on your KubeFlow cluster empowers you to GPU-accelerate your ETL work in both your interactive sessions and ETL pipelines.Hey, post author here! Thanks for reading, I really hope it was useful.This post was particularly exciting for me and my team because it marks the culmination of months of engineering work. At the start of 2022 we set out to ensure that RAPIDS integrates seamlessly with KubeFlow and as a result ended up completely overhauling how Dask deployments work on Kubernetes.It has taken many hours of engineering effort to even make this post possible, so a huge thank you to everyone both within RAPIDS and also in the Dask community who came together to make this happen.Powered by Discourse, best viewed with JavaScript enabled"
3508,accelerated-motion-processing-brought-to-vulkan-with-the-nvidia-optical-flow-sdk,"Originally published at:			https://developer.nvidia.com/blog/accelerated-motion-processing-brought-to-vulkan-with-optical-flow-sdk/
The NVIDIA Optical Flow Accelerator (NVOFA) is a dedicated hardware unit on newer NVIDIA GPUs for computing optical flow between a pair of images at high performance. The NVIDIA Optical Flow SDK exposes developer APIs that enable you to leverage the power of NVOFA hardware in your applications.  We are excited to announce the availability…Powered by Discourse, best viewed with JavaScript enabled"
3509,enabling-gpus-in-the-container-runtime-ecosystem,"Originally published at:			Enabling GPUs in the Container Runtime Ecosystem | NVIDIA Technical Blog
NVIDIA uses containers to develop, test, benchmark, and deploy deep learning (DL) frameworks and HPC applications. We wrote about building and deploying GPU containers at scale using NVIDIA-Docker roughly two years ago. Since then,  NVIDIA-Docker has been downloaded close to 2 million times. A variety of customers used NVIDIA-Docker to containerize and run GPU accelerated workloads. NVIDIA…I wish you'd talk more about Singularity.Gostei muito.this is nice! but, the gpu's needed as documented elsewhere on the Nvidia site  areNVIDIA TITAN V (Volta)NVIDIA TITAN X (Pascal)NVIDIA TITAN Xp (Pascal)NVIDIA Quadro GV100 (Volta)NVIDIA Quadro GP100 (Pascal)NVIDIA Quadro P6000 (Pascal)the web site https://docs.nvidia.com/ngc... has no information about the RTX cards as yet. Your documentation is all over the place, not up to date, and seems to confuse issues for the sake of clearing inventory...I want to run my cloud hybrid or on premises with commodity (ish) hardware, so my old titan, or new gtx1080ti do not get a look in irrespective of their capabilities... I can try and flash the bios so that my 1080ti looks like something acceptable, but reliable sources tell me that Nvidia has precluded this with watchdogs that will brick my new card... Nice. So it looks as if I should try for second hand titan x (pascal) or titan Xp cards.Trouble with this is that I expect the next round of iterations to jerk the rug out from under my feet, again, forcing me to submit to the scrutiny and rental costs of the cloud, or go for the extremely expensive route. Or go with intel/amd... It may be I just freeze in time with a titan x (pascal) or two, after the RTX2080 and 2080Ti (and god knows what other jack in the boxes are about to be foist on me) depress the market for lesser cards. This does make it very difficult for sole operators like me, who are being frozen out of developing their ideas inexpensively. And I know that if I go cloud/hybrid with some of the juicier ones, then it won't be too long before some bright eyed ivy leaguer launches another billion dollar company... Thanks Nvidia!This works fine for me using a GTX 1080Ti?wow! really? you can spin up on premises (aka on your home box) GPU cloud instances on a GTX1080Ti? Can you detail what you did, please? This is very good news, will save me a LOT of stuffing around. CHEERS :))I'm not sure if this is exactly the same as GPU cloud. But you can definitely spin up nvidia-docker containers on a home/in premisise machine with GTX 1080Ti's. If you're not familiar with docker, a docker contain is basically a small headless virtual machine with low overhead and a scripted build process so repeatable.I installed Ubuntu 18.04 - I tried Debian and Fedora but it's far less painful with Ubuntu.You then need to install cuda 10.0 with the driver which ships with it 410.48, if you try and download the driver by itself you get 396.xx which doesn't support cuda 10.0 (you may also need to disable the open source nouveau driver). I first disabled the nouveau driver  by following https://linuxconfig.org/how... (I'm not sure if this is required or not)Then installed cuda 10.0$ sudo apt-get install build-essential dkms$ sudo apt-get install freeglut3 freeglut3-dev libxi-dev libxmu-devNow download the deb file installer and following the instructions here:https://developer.nvidia.co...$ sudo dpkg -i cuda-repo-ubuntu1804_10.0.130-1_amd64.deb$ sudo apt-key adv --fetch-keys https://developer.download....$ sudo apt-get update$ sudo apt-get install cuda$ rebootThen install docker-ce by following the instructions here:https://docs.docker.com/ins...Then the nvidia docker runtime following the instructions here:https://github.com/nvidia/n...Now you should be able to bring up a nvidia-docker container# Test nvidia-smi with the cuda 9.0$ docker run --runtime=nvidia --rm nvidia/cuda:9.0-base nvidia-smi# Test nvidia-smi with the cuda 9.0$ docker run --runtime=nvidia --rm nvidia/cuda:10.0-base nvidia-smiThere's a ton of different base images you can use i.e. https://hub.docker.com/r/nv...I've been using nvidia/cuda:9.0-cudnn7-devel-ubuntu16.04 as it matches the requirements for tensorflow-gpu for examplehey thanks for this, I am about to embark on another build quest, so it will be interesting to compare your instructions with what I have already done. I am still not holding my breath on the NVIDIA GPU cloud images working though...What about windows?Actually windows is much needed.I need it to run directx 12.I have a question, if I have a server with 4 tesla M10,  can I only launch 4 containers?Docker can run on both Windows and macOS operating systems. This is enabled by the Docker architecturesAs described in this blog:cuda-drivers package may not work on Ubuntu 18.04 LTS systemsI want to know what’s and why exactly, since we’re using Ubuntu 18.04.6This blog was wrote in Jun 01, 2018, and no update history shown. So it’s still proper today?
Is there any update needed?Thanks!Hi @KeelungPlease refer to the official documentation on installing drivers on supported Linux distributions:NVIDIA Driver Installation Quickstart Guide :: NVIDIA Tesla DocumentationHope that answers your question. ThanksPowered by Discourse, best viewed with JavaScript enabled"
3510,insider-s-guide-to-gtc-cybersecurity-data-center-data-science-and-networking,"Originally published at:			https://developer.nvidia.com/blog/insiders-guide-to-gtc-cybersecurity-data-center-data-science-and-networking/
Informative sessions on automated cyberattack prevention, power grid management from the edge, optimizing network storage, edge AI-on-5G, and addressing malware with data science.Powered by Discourse, best viewed with JavaScript enabled"
3511,gpu-accelerated-hierarchical-dbscan-with-rapids-cuml-let-s-get-back-to-the-future,"Originally published at:			https://developer.nvidia.com/blog/gpu-accelerated-hierarchical-dbscan-with-rapids-cuml-lets-get-back-to-the-future/
Hierarchical Density-Based Spatial Clustering of Applications w/ Noise (HDBSCAN) though a relatively new density-based clustering algorithm, has found much practical use in industry and scientific computing applications.Powered by Discourse, best viewed with JavaScript enabled"
3512,five-unique-real-time-rendering-tips-from-nvidia-experts,"Originally published at:			Five Unique Real-Time Rendering Tips from NVIDIA Experts | NVIDIA Technical Blog
Check out the top five questions and answers from our recent Ask Me Anything with the editors of Ray Tracing Gems.Powered by Discourse, best viewed with JavaScript enabled"
3513,exploring-explosive-star-scenarios-with-3d-simulations,"Originally published at:			Exploring Explosive Star Scenarios with 3D Simulations | NVIDIA Technical Blog
Stony Brook University researchers are exploring the physics of Type Ia supernovas using the Tesla-accelerated Titan Supercomputer at Oak Ridge National Laboratory. “Outflows” (red), regions where plumes of hot gas escape the intense nuclear burning at a star’s surface, form at the onset of convection in the helium shell of some white dwarf stars. This…Powered by Discourse, best viewed with JavaScript enabled"
3514,get-started-on-nvidia-triton-with-an-introductory-course-from-nvidia-dli,"Originally published at:			Get Started on NVIDIA Triton with an Introductory Course from NVIDIA DLI | NVIDIA Technical Blog
Practice machine learning operations and learn how to deploy your own machine learning models on a NVIDIA Triton GPU server.Powered by Discourse, best viewed with JavaScript enabled"
3515,using-shared-memory-in-cuda-c-c,"Hi mark,I am trying to use two shared memory arrays in one kernel but with same data type. I am confused how to use it. I tried above approch you have mentioned but it is not working. can you help me?Hey I have figured it out. Any way thank you for the informationHi Mark, Thank you as usual fro the great post, I am mainly interested in the cache/shared memory configuration, I wonder is it expensive to set that configuration on a per kernel basis? If I have an application running 20-30 cuda kernels or more per frame, with target real-time (24-30~ fps), am I going to pay a heavy overhead to set cache memory configuration per kernel? For example if I am not using shared memory at all, is it good practice to minimize the shared memory size and maximize L1 cache to try get better performance?Best RegardsM.Hi Marco,As the API docs explain, currently running code may be forced to finish before the setting can be changed on the hardware. So changing frequently between kernels (flipping back and forth between max L1 and SMEM) could introduce significant pipeline bubbles depending on the running time of each kernel.My advice is to not prematurely optimize for the size of L1 or SMEM: use the optimization when you know it is beneficial (i.e. if you know you aren't bottlenecked by memory it will have much lower benefit). Test the setting for performance benefit on each kernel on each GPU architecture.Yes, thank you for the reply, I found it later on the doc that changing this settings might trigger a device sync. Currently the kind of work load we have does not leverage shared memory due to the kind of alg it deals with which would allow to easily set the option card wide to get an initial bench-marking. Thank you for getting back to me, as usual, thank you for the great article.Hi Mark. A very interesting article. Very useful for learning. But I'd like to know how you would reorder an bigger array. For example, if my device has a maximum threads per block size of 1024, does it mean that I can only reorder an 1024-element array at most?Many thanks.Do not be misunderstood with LINUX shared memory ( quite useful way for managing data transfers to device ) in real-word already-made code accelerations. Theoretically it is joinable with java programming language, but I am not sure. Some trivial C++ example:https://github.com/PiotrLen...Post Scriptum:. quite useful in distributed manner computations in client-server application.Hey did you figure out the answer?Hi Mark. The weirdest thing is happening. I declared a shared array in a global kernel, set some values into it, and whenever I try to access it, it returns a value of zero. The only time it returns a value is if I access the shared array with the thread index. Is this common? My head's seriously spinning over this.It's hard to debug code I can't see. If you are writing to the location with one thread and reading the same location from another, then you must synchronize between the accesses (__syncthreads()), or else you have a race condition which results in undefined behavior.Thanks for answering. I did __synthreads() before and after, and I also did it in a ""if(id ==0)"" condition, to no avail. I suspected a bad installation on my end. But before reinstalling Visual Studio and CUDA, I changed the __shared__ array to a normal one stored in DRAM since it will only be accessed sqrt(n) times in total in an execution. Thank you for your time with me.thank you Mr Harris, These Discussions Are Very Helpful...but my question is what if I want to use of static reverse function in different streams?how should I specify the size of shared memory?I think it would be something like this:<<<k,t,64*sizeof(int),s1>>>(...)after specifying the size of shared memory It seems I'm using of dynamic reverse version! is it true?Hi Mark, I tried your dynamic allocation approach for multiple arrays. But the complier says nC and nF are undefined. Should I define them before calling the kernel?Are bank conflicts still something to look out for in the newest architectures (Turing, Pascal etc.)?Yes, although in the grand scheme of things they are a micro-optimization in most kernels.No, you can launch a LOT of blocks. And loops also work just fine in CUDA C/C++. So your problem size is not limited.Hello MarkThis is quite informative.Could you please specify which metrics I can use from the profiler tools which can hint at shared memory bank conflicts?Also, I have been trying to figure out the metrics which could signify cache misses in a CUDA application. It would be really helpful if you could tell which ones would help me!In NSight Compute, you can collect e.g. the `Memory Workload Analysis Tables` section, which includes detailed information on shared memory usage. https://uploads.disquscdn.c...The Raw page will show you which exact metrics are collected as part of this `group:memory__shared_table`. The exact metrics can change depending on which GPU is targeted. e.g.```l1tex__data_bank_conflicts_pipe_lsu_mem_shared_op_ld.suml1tex__data_bank_conflicts_pipe_lsu_mem_shared_op_st.suml1tex__data_pipe_lsu_wavefronts_mem_shared_cmd_read.suml1tex__data_pipe_lsu_wavefronts_mem_shared_cmd_read.sum.pct_of_peak_sustained_activel1tex__data_pipe_lsu_wavefronts_mem_shared_cmd_write.suml1tex__data_pipe_lsu_wavefronts_mem_shared_cmd_write.sum.pct_of_peak_sustained_activesass__inst_executed_shared_loadssass__inst_executed_shared_storessmsp__inst_executed_op_shared_atom.sum```From my understanding, there are 4 warp schedulers per SM and means 4 warps can execute concurrently in a single SM, if possible. If you use 32-bit mode as in [1] on a device that supports 64-bit transactions, it says that no bank conflict is created when two 32-bit addresses are accessed in the same 64-bit word as it maps to one memory bank and can be multicasted to the two threads in the same warp. This means in total only 16 banks need to be accessed by one warp.My question is thus: is it possible for another warp to access the latter 16 banks concurrently? I.e. will using 32-bit floats double my throughput from shared memory when compared to using 64-bit floats? (in case it makes a difference I’m using a C.C. 7.5 device)[1] Programming Guide :: CUDA Toolkit DocumentationUpon further reading, I discovered that 64-bit mode is only supported for C.C. 3.0 and was changed in C.C. 5.0 and newer to only support 32-bit mode. So in my case (C.C. 7.5), using doubles will result in bank conflicts and 2 transactions from shared memory will be required.[1] Best Practices Guide :: CUDA Toolkit DocumentationPowered by Discourse, best viewed with JavaScript enabled"
3516,optimizing-video-memory-usage-with-the-nvdecode-api-and-nvidia-video-codec-sdk,"Originally published at:			Optimizing Video Memory Usage with the NVDECODE API and NVIDIA Video Codec SDK | NVIDIA Technical Blog
The NVIDIA Video Codec SDK consists of GPU hardware-accelerated APIs for the following tasks: Video encoding, with the NVENCODE APIVideo decoding, with NVDECODE API (formerly known as nvcuvid) While writing an application using the NVDECODE or NVENCODE APIs, it is crucial to use video memory in an efficient way. If an application uses multiple decoders…Hi there.Figure 2 (A typical decoding pipeline) shows video parser as optional component.  The article also says: “These components are not dependent on each other and hence can be used independently.”
But to calculate the ulNumDecodeSurfaces the article suggests to use CUVIDEOFORMAT::min_num_decode_surfaces provided by the sequence callback from the parser.
Does it mean that usage of the video parser is actually required, not optional?
If parser is not required, how do I determine a value of the ulNumDecodeSurfaces without video parser call back and CUVIDEOFORMAT::min_num_decode_surfaces ?
Thank you!Does it mean that usage of the video parser is actually required, not optional?Hi @petr.mpp,Let me clarify it. Developers has freedom to implement their own parser and use. Parser is MUST for decoder pipeline but NVIDIA video parser can be replaced by their own implementation.ulNumDecodeSurfaces can be derived from bitstream DPB info from either their own parser or Nvidia video parser. Or can be set to MAX DPB size defined in codec spec.Thank you.
VikasPowered by Discourse, best viewed with JavaScript enabled"
3517,sparse-forests-with-fil,"Originally published at:			https://developer.nvidia.com/blog/sparse-forests-with-fil/
Introduction The RAPIDS Forest Inference Library, affectionately known as FIL, dramatically accelerates inference (prediction) for tree-based models, including gradient-boosted decision tree models (like those from XGBoost and LightGBM) and random forests. (For a deeper dive into the library overall, check out the original FIL blog.) Models in the original FIL are stored as dense binary…Powered by Discourse, best viewed with JavaScript enabled"
3518,thank-you-for-joining-us-for-our-path-tracing-ama-we-appreciate-all-the-questions,"Thanks for joining us for this AMAThanks from both Filip and I enjoyed answering the questions
Here are some additional resources for Path Tracing:
Path Tracing SDK overview page: RTX Path Tracing SDK | NVIDIA Developer
Overview Session from GTC Spring: How to Build a Real-time Path Tracer | NVIDIA On-DemandPowered by Discourse, best viewed with JavaScript enabled"
3519,deep-learning-for-automated-driving-with-matlab,"Originally published at:			Deep Learning for Automated Driving with MATLAB | NVIDIA Technical Blog
Figure 1. Car approaching a vehicle in an intersection. You’ve probably seen headlines about innovation in automated driving now that there are several cars available on the market that have some level of self-driving capability. I often get questions from colleagues on how automated driving systems perceive their environment and make “human-like” decisions. The autonomous car…Hi Avinash and Arvind. Can I get both of your email? Or can you email me? I'm eager to ask and discuss something related to autonomous vehicles. Here is my email : izzah.amani@ymail.com or izzahamani13@gmail.comHi Avinash and Arvind.I am conducting an experiment using R-CNN and Fast R-CNN on several datasets in MATLAB. I wish to automate my Ground Truth labeling process using an algorithm that is based on the R-CNN detector or the Fast R-CNN detector. Please kindly assist. My e-mail address is: masitakatleho7@gmail.com, thank you.I would recommend looking at the help which has a few examples of how to use a custom automation algorithm. To do so type ""doc driving"" or ""driving"" demos in MATLAB.How do we get access to the ""vehicleDataset"" dataset. The code provided works but impossible to go further. Or how do we build our own dataset?Powered by Discourse, best viewed with JavaScript enabled"
3520,nvidia-docker-gpu-server-application-deployment-made-easy,"Thanks Itai.  Glad you are up and running!Would linux docker containers running on windows support GPU as well?Can you direct me to any tutorials if so?It's not supported right now, you would need to do GPU passthrough with HyperV.How can I set up a GPU pass through? Is it straightforward?Hello when I run this command sudo nvidia-docker run -it -p 8888:8888 gcr.io/tensorflow/tensorflo... then I open localhost:8888 in my browser, I am asked to enter the login password in my browser. could you please help me to fix this issue?  I am newbie in this field. I would be grateful to have detailed solution. https://uploads.disquscdn.c...See my answer here: https://github.com/NVIDIA/n...VMWare VMs have had the ability to use nvidia (well) on a Windows Host. What's the roadblock to making it possible in docker? I know that's an over simplified question, but having used nvidia in docker ( https://hub.docker.com/r/ra... ) for a Linux host and having used nvidia in VM's for a couple years, it seems odd this isn't in place. Doubly so when WIndows 10 is getting a fair bit of love from Docker in every other way.See this issue: https://github.com/NVIDIA/n...If you spawn a VM and setup GPU passthrough, you can then use Docker and nvidia-docker inside this VM without any issue. I've tested this successfully with KVM.But, if you want to use docker for Windows (Hyper-V), you would need Discrete Device Assignment support from Docker and Hyper-V (only on Windows Server 2016). That's why we don't support it.Very good. Thank you. Might be worth mentioning that the default being promoted by Docker (Hyper-V) is where things break down, but it is possible with other hypervisors.I'm a bit fuzzy about how I use KVM (or other hypervisors?) to get around this though since I didn't setup Hyper-V explicitly. Are you suggesting I use any other hypervisor that can see the GPU, then setup from there as if that was my host; such as installing CentOS in a VMWare (or KVM) VM, then installing docker and doing my development inside that VM (losing my Windows environment), or is there a way to get Docker for WIndows to use that VM so I can continue to work in Windows, use Powershell, etc..?Is there an article that explains this you can link me to?If you want to use nvidia-docker inside a VM, you indeed need to treat this VM as a regular machine and install the distro, the NVIDIA drivers, docker and nvidia-docker.There is currently no way (as far as I know), to use your Docker client from the host and spawn a GPU VM with Docker. But note that I don't have much experience with Docker on Windows.I was able to get things working using latest-gpu. However, when I run tensorflow I get the CPU warnings saying that I could have faster execution if built with SSE instructions. How do I get this speed up? Evidently the latest-gpu does not have this. Hi Zak, I think that's a question for the TensorFlow team. Thanks!Hi guys,We have ubuntu terminal setup, our goal is to try running 3D simulation on server(with nvidia gpu) and expecting nvidia docker to stream 3D content to client(without gpu). Please note that 3D simulation is unreal executable that is runnable in linux OS. Can you please confirm if this is possible using nvidia GPU?Tried one of the above example with following command:""nvidia-docker run --name digits --rm -ti -p 8000:34448 nvidia/digits"" but getting following exception when building:curl localhost:8000 -vv* Rebuilt URL to: localhost:8000/*   Trying 127.0.0.1...* Connected to localhost (127.0.0.1) port 8000 (#0)> GET / HTTP/1.1> Host: localhost:8000> User-Agent: curl/7.47.0> Accept: */*> * Recv failure: Connection reset by peer* Closing connection 0curl: (56) Recv failure: Connection reset by peerAny help will be greatly appreciated. Thank you!`nvidia/digits` has changed the port that it listens on.For `nvidia/digits:4.0` and early, your command is correct.  For `nvidia/digits:5.0` and later, you'll need to map to port 5000 inside the container.Change your port mapping to `-p 8000:5000`.Thank you very much for clarification!Hi guys,When trying to run nvidia docker nbody sample from ubuntu terminal on azure VM, it looks to be giving following result rather than graphical output. Wondering if it is possible to see nbody graphical simulation? Any help would be appreciated! Thank you.nvidia-docker run --rm sample:nbodyRun ""nbody -benchmark [-numbodies=<numbodies>]"" to measure performance.-fullscreen       (run n-body simulation in fullscreen mode)-fp64             (use double precision floating point values for simulation)-hostmem          (stores simulation data in host memory)-benchmark        (run benchmark to measure performance) -numbodies=<n>    (number of bodies (>= 1) to run in simulation) -device=<d>       (where d=0,1,2.... for the CUDA device to use)-numdevices=   (where i=(number of CUDA devices > 0) to use for simulation)-compare          (compares simulation results running once on the default GPU and once on the CPU)-cpu              (run n-body simulation on the CPU)-tipsy=<file.bin> (load a tipsy model file for simulation)NOTE: The CUDA Samples are not meant for performance measurements. Results may vary when GPU Boost is enabled.> Windowed mode> Simulation data stored in video memory> Single precision floating point simulation> 1 Devices used for simulationGPU Device 0: ""Tesla K80"" with compute capability 3.7> Compute 3.7 CUDA device: [Tesla K80]13312 bodies, total time for 10 iterations: 32.774 ms= 54.071 billion interactions per second= 1081.413 single-precision GFLOP/s at 20 flops per interactionI cant seem to listen to the right port. I am new to most of this but I doI ssh into the aws server with:ssh -i ""l***.pem"" -L 8000:***:8000 ubuntu@***and then nvidia-docker run --rm -ti -p 8000:5000 nvidia/digitthe application seems to run since DIGITS appearsbut I get channel 4: open failed: connect failed: Connection timed outWhere do you think things are going wrong? Any ideasThanks in advancei am using nvidia tesla v100 machine running linux but want to change to windows 10is nvidia docker available for windows 10?Hi.  It would be helpful if you included an example on how to use docker-compose as well.seems these threads are old and didnt get answer, whats the point, expected much much more from a company like NvidiaPowered by Discourse, best viewed with JavaScript enabled"
3521,cuda-dynamic-parallelism-api-and-principles,"Originally published at:			https://developer.nvidia.com/blog/cuda-dynamic-parallelism-api-principles/
This post is the second in a series on CUDA Dynamic Parallelism. In my first post, I introduced Dynamic Parallelism by using it to compute images of the Mandelbrot set using recursive subdivision, resulting in large increases in performance and efficiency. This post is an in-depth tutorial on the ins and outs of programming with…Hello, Andrew.thanks for your post, it is hard to find good information about CDP.I have some questions about error checking in CDP. I have such a complex CDP code, and I faced some problems when I launch a big number of kernels.Sometimes I can get the error number 11, but, sometimes, I just receive wrong answer and no error. If in CUDA 7.5 the pool is a virtualized pool that can handle a big number of kernels, how can I get a error when I receive a wrong answer? Because, sometimes, I receive good answer, but not all blocks were processed.All the best,Tiago Carneiro.Hello Tiago,>> thanks for your post, it is hard to find good information about CDP.Agree with you on this! However, as a first step, we gave a talk at this GTC regarding dynamic parallelism. Even though the major focus was on its perf characteristics, at the end of the talk, it also contains some information on error handling. Hopefully, that information will be useful to you. The talk is here: https://registration.gputec.... The recordings and slides will be up sooner.>> Sometimes I can get the error number 11, but, sometimes, I just receive wrong answer and no error.Yes. As of today, the error handling is difficult on device-side with dyn-par. As such, the debug strategy depends upon the reason for the error in such scenarios.1.If the error is because of an OOB access, one can use the combination of ‘nvcc -lineinfo' compilation and ‘cuda-memcheck’ to pin-point the source of the error.2.If the error is suspected to be due to cuda calls on device side, AFAIK currently, there’s no easy way to pass this info to the host side. Meaning, cudaGetLastError on the host side after completing parent kernel will not catch this! Hence, I’ve been using ‘cudaGetLastError’ on the device side + device-side-printfs to debug such errors. I believe this warrants an nvbug, but I’ve not been able to find time + a real use-case to do so.Finally, if you can give us a mini-app regarding these issues, that’ll be of tremendous help while making a case to fix and improve error-handling on device side!Regards,ThejaswiHello, Thejaswi.Thanks for sending me the GTC Talking, it will be very useful, and thank you for your explanation, it will help me a lot.Please, tell me, how can I send you the app with the different error situations?Regards,Tiago Carneiro.Hi Tiago,You can send us the app in either of the 2 ways:1. Recommended approach is to become a registered developer [https://developer.nvidia.co...] and then file a bug from the portal, while attaching the app. So that way you can follow-up on the progress.2. You can contact me through s n a n dit ale AT nvidia.com.Kindly also provide a README describing steps to build and run your app, in order to reproduce the issues.Regards,ThejaswiHello Thejaswi,Thank you for your help!No problem, I'm making a version of the program with a Makefile and a Readme to reproduce the issue, and I'll send you soon.I'm registered in NVidia Developer since 2012, but I didn't know about this help with bugs, that's great, I'll do it too.All the best,Tiago Carneiro.Ps: The correct contact is  s n a n dit ale or  s n a n dot ale?Hi Tiago, it's ""dit"".Dear Thejaswi Rao,Thank you for your previous support.Now I'm facing new CDP challenges. I'm trying to make a CDP code that allocates memory dynamically and I'm facing the following problem.Even if I set cudaLimitMallocHeapSize big enough, as the problem size grows the application just returns ""CUDA error: an illegal memory access was encountered"".Doing some debugging, I've found that problem with the allocations. Looks like if I perform a huge number of allocations on the device side, the GPU returns me this error.Looks like that even if I set cudaLimitMallocHeapSize big enough, CUDA may be unable to perform a huge amount of allocations.Have you ever faced this situation?Best regards,Tiago Carneiro.Hello Andrew. Thanks for your post.I'm facing new CDP challenges. I'm trying to make a CDP code that allocates memory dynamically and I'm facing the following problem.Even if I set cudaLimitMallocHeapSize big enough, as the problem size grows the application just returns ""CUDA error: an illegal memory access was encountered"".Doing some debugging, I've found that problem with the allocations. Looks like if I perform a huge number of allocations on the device side, the GPU returns me this error.Looks like that even if I set cudaLimitMallocHeapSize big enough, CUDA may be unable to perform a huge amount of allocations.Have you ever faced this situation?Best regards,Tiago Carneiro.Hi Tiago. Can you provide an as-simple-as-possible program that reproduces the problem somewhere (e.g. on Github, or as a Gist)?Dear Mark,thank you for your answer!Yes, I can. I'm going to prepare a simple code.All the best,Tiago Carneiro.Hello Andrew,I tried to implement simple program for Dynamic Parallelism from latest CUDA Programming Guide.I am facing ERROR -Error1error : calling a __global__ function(""childKernel"") from a __global__ function(""parentKernel"") is only allowed on the compute_35 architecture or aboveBut my GPU is Tesla K40c with compute capability 3.5.I used CUDA 8.0 toolkitRest of CUDA programs run fineIs there any initial conditions I should set for Dynamic parallelism ?Thank you.Make sure you compile with at least the ""-arch=sm_35"" flag to NVCC. The default is sm_20, which doesn't support dynamic parallelism.Hello Mark,I use Visual Studio 2013, on a Windows 10 system.I changed the configuration property by changing Code Generation option in Device section of Cuda C/C++ from ""compute_20,sm_20"" to ""compute_35,sm_35"", (I did this for all configurations and all Platforms).I am getting the following error -""kernel launch from __device__ or __global__ functions requires separate compilation mode""Thank you for your valuable help,AmeyaI was able to resolve that error by adding -""nvcc --gpu-architecture=sm_35 --device-c""to Command line.But unfortunstely it lead to following error -Error1error MSB3721: The command """"C:\Program Files\NVIDIA GPU Computing Toolkit\CUDA\v8.0\bin\nvcc.exe"" -gencode=arch=compute_35,code=\""sm_35,compute_35\"" --use-local-env --cl-version 2013 -ccbin ""C:\Program Files (x86)\Microsoft Visual Studio 12.0\VC\bin""  -I""C:\Program Files\NVIDIA GPU Computing Toolkit\CUDA\v8.0\include"" -I""C:\Program Files\NVIDIA GPU Computing Toolkit\CUDA\v8.0\include""  -G   --keep-dir Debug -maxrregcount=0  --machine 32 --compile -cudart static nvcc --gpu-architecture=sm_35 --device-c kernel.cu -g   -DWIN32 -D_DEBUG -D_CONSOLE -D_MBCS -Xcompiler ""/EHsc /W3 /nologo /Od /FS /Zi /RTC1 /MDd "" -o Debug\kernel.cu.obj ""c:\Users\awadekar\documents\visual studio 2013\Projects\Dynamic Parallelism 3\Dynamic Parallelism 3\kernel.cu"""" exited with code 1.Hello Mark,I am still unable to implement Dynamic parallelism on my system I use (Visual Studio 2015)I am getting following error(Uploaded a screenshot):- https://uploads.disquscdn.c... I am unable to understand the reason for this error.I would really appreciate if you could clarify.Thank you,Ameya.Hi Ameya, can you post the code you are trying to compile and the full command line somewhere so we can try to reproduce? A GitHub Gist is one way to do this. https://gist.github.comDear Ameya Wadekar. According to your picture, I think the following flags are missing: ""-lcudadevrt  -rdc=true"".Best regards,Tiago Carneiro.Dear Mark Harris,I have a series of questions concerning the issues a huge number of kernels launched may cause.I performed some experiments in this direction. These experiments launch a huge number of kernels (more than 2048), and I observed that the following issues sometimes happen.EI)The program halts, getLastError() on the host returns “an illegal memory access was encountered” and NVPROF returns the following message: “==18406== Warning: This can happen if device ran out of memory or if a device kernel was stopped due to an assertion.”EII) The program does not halt, takes much longer, from 10x to 100x longer, and returns a wrong answer. Neither host or NVPROF returns an error message. In this situation, Nvprof returns no information concerning kernels launched by CDP, although it returns information concerning the kernel launched by the host. According to a global variable, CDP launches all kernels it is supposed to launch.We have noticed that  EI) and EII) are more likely to happen when CDP kernels dynamically allocate memory.Some remarks:If we set the heap (cudaLimitMallocHeapSize) to a big size, e.g., 65% of the global memory, problems E1) and E2) happen much more often.2) Another interesting information is that error 1) is the same error we get when the heap is not enough.3) The device never gets an error by using getLastError().Now, I have the following questions:q1) Does the device can get any error by using getLastError()?q2) Do the errors getLastError(), Nvprof and cuda-memcheck return in CDP applications make sense? Are they just random errors?q3) As said before, for bigger heaps, errors happen more often. Does the device need other memory than cudaLimitDevRuntimePendingLaunchCount and cudaLimitDevRuntimeSyncDepth to manage kernels that CDP launches?If the answer for q3) is yes, is that the reason errors EI) and EII) happen more often when we set a big heap?All the best,Tiago Carneiro.Dear Tiago,I was working on Visual studio, so couldnt figure it out.But now I got the code working.Thank you,AmeyaPowered by Discourse, best viewed with JavaScript enabled"
3522,satellite-images-help-track-a-vehicle,"Originally published at:			https://developer.nvidia.com/blog/satellite-images-help-track-a-vehicle/
Researchers from the Toyota Technological Institute at Chicago (TTIC) and Carnegie Mellon University developed a deep learning-based method that locates a ground vehicle by using satellite imagery as the only prior knowledge of the environment. Knowing the exact location of a vehicle is critical for autonomous cars, and currently GPS systems are being used which…Powered by Discourse, best viewed with JavaScript enabled"
3523,reach-new-frontiers-of-immersive-production-with-nvidia-cloudxr-and-htc-vive-focus-3,"Originally published at:			Reach New Frontiers of Immersive Production with NVIDIA CloudXR and HTC VIVE Focus 3 | NVIDIA Technical Blog
HTC released a CloudXR client to support their VIVE Focus 3, which  provides a ""best of both worlds” solution to the difficult tradeoffs VR developers face.Can’t say enough about the great working relationship we have with the Vive team - from VRWorks to CloudXR Vive is taking advantage of every piece of NV tech. to bring their customers an excellent VR experience!Powered by Discourse, best viewed with JavaScript enabled"
3524,virtual-character-animation-system-uses-ai-to-generate-more-human-like-movements,"Originally published at:			Virtual Character Animation System Uses AI to Generate More Human-Like Movements | NVIDIA Technical Blog
To help virtual characters move more naturally, researchers from the University of Edinburgh and Adobe developed a character control system that uses deep learning to assist characters to run, jump, avoid obstacles, open and enter through doors, pick up and carry objects from just a single model in real-time.   “Our neural architecture internally acquires a…Powered by Discourse, best viewed with JavaScript enabled"
3525,accelerating-model-development-and-ai-training-with-synthetic-data-sky-engine-ai-platform-and-nvidia-transfer-learning-toolkit,"Originally published at:			https://developer.nvidia.com/blog/accelerating-model-development-and-ai-training-with-synthetic-data-sky-engine-ai-platform-and-tlt/
In AI and computer vision, data acquisition is costly and time-consuming and human-based labeling can be error-prone. The accuracy of the models is also affected by insufficient and poorly balanced data and the prolonged time required to improve the deep learning models. It always requires the reacquisition of data in the real world. The collection,…We are very excited bringing our outstanding technology to the market to replicate all rare and complicated deep learning problems in Virtual Reality and train models on generated data in a single, full stack platform. If you have any questions or comments, let us know.
Jakub Pietrzak, CTO of Sky EnginePowered by Discourse, best viewed with JavaScript enabled"
3526,create-real-time-simulations-with-nvidia-omniverse-and-bentley-lumenrt,"Originally published at:			https://developer.nvidia.com/blog/create-real-time-simulations-with-nvidia-omniverse-and-bentley-lumenrt/
Organizations across industries are using LumenRT for NVIDIA Omniverse, powered by Bentley iTwin Platform, to create compelling, high-quality visualizations and project deliverables.Powered by Discourse, best viewed with JavaScript enabled"
3527,inception-spotlight-darwinai-achieves-96-screening-accuracy-for-covid-19-with-diverse-ct-dataset,"Originally published at:			https://developer.nvidia.com/blog/darwinai-achieves-96-screening-accuracy-for-covid-19-with-diverse-ct-dataset/
The model, COVID-Net CT-2, was built using a number of large and diverse datasets created over several months with the University of Waterloo and is publicly available on GitHub.Powered by Discourse, best viewed with JavaScript enabled"
3528,sim2sg-generating-sim-to-real-scene-graphs-for-transfer-learning,"Originally published at:			https://developer.nvidia.com/blog/sim2sg-generating-sim-to-real-scene-graphs-for-transfer-learning/
Scene graphs (SGs) in both computer vision and computer graphics are an interpretable and structural representation of scenes. A scene graph summarizes entities in the scene and plausible relationships among them. SGs have applications in the fields of computer vision, robotics, autonomous vehicles, and so on. Current SG-generation techniques rely on the limited availability of…SIM2SG, scene graph generation for transfer learning, closes the Domain Gap between synthetic and real data - check it out and tell us what you think.Recently, scene graph (SG) generation has gained a lot of traction. Current SG generation techniques, on the other hand, depend on the costly and restricted availability of labelled datasets. Since labels are basically free, synthetic data is a viable alternative. However, due to the domain distance, neural network models trained on synthetic data do not perform well on real data.Powered by Discourse, best viewed with JavaScript enabled"
3529,share-your-science-magnetic-nanotechnology-in-biomedicine,"Originally published at:			Share Your Science: Magnetic Nanotechnology in Biomedicine | NVIDIA Technical Blog
Oliver Laslett, Post-Graduate Researcher at University of Southampton discusses how magnetic nanotechnology can be used to improve bio-medicine.   Share your GPU-accelerated science with us: http://nvda.ly/Vpjxr Watch more scientists and researchers share how accelerated computing is #thepathforward: Watch Now  Powered by Discourse, best viewed with JavaScript enabled"
3530,sc20-demo-simplifying-application-deployments-on-hpc-clusters,"Originally published at:			https://developer.nvidia.com/blog/sc20-demo-simplifying-application-deployments-on-hpc-clusters/
Many system administrators use environment modules to manage software deployments. The advantages of environment modules are that they allow you to load and unload software configurations dynamically in a clean fashion, while providing end users with the best experience when it comes deploying applications. However, robustly supporting HPC and deep learning applications with complex dependencies…Powered by Discourse, best viewed with JavaScript enabled"
3531,professional-quality-3d-asset-library-for-research-launched-by-nvidia-amazon-lumberyard-and-speedtree,"Originally published at:			https://developer.nvidia.com/blog/professional-quality-3d-asset-library-for-research-launched-by-nvidia-amazon-lumberyard-and-speedtree/
By Aaron Lefohn, Director of NVIDIA Research Falcor Rendering R&D Framework Updated for DirectX 12 and Vulkan This morning at SIGGRAPH 2017, Amazon Lumberyard, SpeedTree, and NVIDIA announced ORCA (Open Research Content Archive) a free, open library of 3D assets for computer graphics researchers. This collaboration is a first for the industry as researchers struggle…Powered by Discourse, best viewed with JavaScript enabled"
3532,learn-about-cuda-5-at-gtc,"Originally published at:			https://developer.nvidia.com/blog/learn-about-cuda-5-gtc/
I still remember my excitement nearly nine years ago, when I first came to work at NVIDIA and I learned that NVIDIA had a super-secret project to build a GPU architecture with dedicated support for general-purpose parallel computing. I had been doing GPGPU research for a few years and I wasn’t sure of its future,…Powered by Discourse, best viewed with JavaScript enabled"
3533,teaching-an-ai-to-detect-key-actors-in-multi-person-videos,"Originally published at:			Teaching an AI to Detect Key Actors in Multi-person Videos | NVIDIA Technical Blog
Researchers from Google and Stanford have taught their computer vision model to detect the most important person in a multi-person video scene – for example, who the shooter is in a basketball game which typically contains dozens or hundreds of people in a scene. Using 20 Tesla K40 GPUs and the cuDNN-accelerated Tensorflow deep learning…Powered by Discourse, best viewed with JavaScript enabled"
3534,simplifying-home-decor-shopping,"Originally published at:			Simplifying Home Décor Shopping | NVIDIA Technical Blog
A UK startup developed a deep learning-based platform that allows users to visualize new decorations and furniture in their own homes. Digital Bridge’s technology integrates into a home décor retailer’s website such as IKEA, Home Depot or John Lewis, and lets you take a photo of your room and remove furniture, wallpaper and other home…Powered by Discourse, best viewed with JavaScript enabled"
3535,kubernetes-for-network-engineers,"Originally published at:			https://developer.nvidia.com/blog/kubernetes-for-network-engineers/
Using the same orchestration on-premise and on the public cloud allows a high level of agility and ease of operations. You can use the same API across bare metal and public clouds. Kubernetes is an open-source, container-orchestration system for automating the deployment, scaling, and management of containerized applications. It was originally designed by Google and…Continuing the discussion from Kubernetes for Network Engineers:在本地和公共雲上使用相同的編排可以實現高度的敏捷性和易操作性。您可以跨裸機和公共雲使用相同的 API。Kubernetes 是一個開源的容器編排系統，用於自動化容器化應用程序的部署、擴展和管理。它最初是由谷歌設計的……Powered by Discourse, best viewed with JavaScript enabled"
3536,next-generation-under-vehicle-inspection-system,"Originally published at:			Next-Generation Under Vehicle Inspection System | NVIDIA Technical Blog
Israeli startup UVeye developed an AI-based recognition system that scans the underside of moving vehicles to identify potential hidden threats. “UVeye is changing the way people approach security when traveling by vehicle with a fast, accurate and automatic machine learning inspection system that can detect threatening objects or unlawful substances, for example, bombs, unexposed weapons…Powered by Discourse, best viewed with JavaScript enabled"
3537,ai-nails-it-viral-manicure-robot-powered-by-gpu-accelerated-computer-vision,"Originally published at:			https://developer.nvidia.com/blog/ai-nails-it-viral-manicure-robot-powered-by-gpu-accelerated-computer-vision/
San Francisco startup Clockwork recently launched a pop-up location offering the first robot nail painting service in the form of 10-minute “minicures.”Powered by Discourse, best viewed with JavaScript enabled"
3538,training-your-own-voice-font-using-flowtron,"Originally published at:			https://developer.nvidia.com/blog/training-your-own-voice-font-using-flowtron/
Recent conversational AI research has demonstrated automatically generating high quality, human-like audio from text. For example, you can use Tacotron 2 and WaveGlow to convert text into high quality, natural-sounding speech in real time. You can also use FastPitch to generate mel spectrograms in parallel, achieving good speedup compared to Tacotron 2. However, current text-to-speech…Powered by Discourse, best viewed with JavaScript enabled"
3539,cuda-pro-tip-increase-performance-with-vectorized-memory-access,"Originally published at:			https://developer.nvidia.com/blog/cuda-pro-tip-increase-performance-with-vectorized-memory-access/
Many CUDA kernels are bandwidth bound, and the increasing ratio of flops to bandwidth in new hardware results in more bandwidth bound kernels. This makes it very important to take steps to mitigate bandwidth bottlenecks in your code. In this post, I will show you how to use vector loads and stores in CUDA C/C++ to…Thank you for interesting material. But it's seems to be a mistake in the end of device_copy_vector2_kernel code.Thanks Anton, I have fixed the formatting error.The section above that (process remaining elements) dangles in mid air...""Dereferencing those pointers will cause the compiler to generate the vectorized instructions.""I wonder where I can look for more on the conditions under which the compiler will take the initiative in this way, both for memory access and for local instructions? It seems a bit magical to a newbie.Another way of putting it: I want to understand *why* casting to a pointer to type int2 triggers automatic vectorization. (Happy to do my own reading, if someone would be kind enough to give helpful indirection.)This is fixed now too. Thanks!The compiler cannot guarantee that the pointer is int2 or int4 aligned and thus cannot generate vectorized loads in the absence of additional information.  By casting to a vector type the user is giving the compiler more information by explicitly saying it is aligned.  Of course if the user is wrong about the alignment the code is likely to produce a memory access fault. The Maximum vectorized load/store is 16 bytes.   Depending on the size of the struct the compiler can do some interesting things, for example on 24-byte structs it could load 8,8,8 bytes but not 16,8 (since the 16 would only be aligned every second struct element).Dr Harris,Thank you for your helpful simplified explanation. [...][I've deleted the rest of this post because my remaining questions (1) would be better asked in a different forum and (2) have shown up a blockage in my understanding that I need to address. The penny will eventually drop but in the meantime I don't want to bother others.][The penny has dropped. Thanks.]quick question:  how can I load four integers into registers (an array of four) which can be addressed by an index and not .x, .y, .z, .w?>if(i < N)this statement in the second vectorized variant should be replaced with for(;i < N;i++) since you may need to copy up to 3 remaining itemsIs there a compute capability requirement?  I'm on a Tesla C2070 with compute capability 2.0 and I don't see any LD.64 when I look at the SASS for device_copy_vector2_kernel, I just see regular LD'sThe examples here use Kepler, which has a different SASS ISA than Fermi (CC 2.x). However, the approach of vectorizing loads and stores should be valuable on Fermi GPUs as well.This is a pedantic question since I understand Luitjens intention/meaning but when he says ""However, it is important to note that there will be half as many instructions executed because the loop only executes N/2 times""  after he does the SASS/assembly of `device_copy_vector2`, does he mean that that only half of the *total* number of threads to be launched, N/2, is now needed?  The for loop skips by += blockDim.x * gridDim.x so to process elements in the int array that's >= blockDim.x * gridDim.x (in its index).  It's not += 1 (so for loop isn't looped over N/2 times)...If the for-loop that uses offsets were to be unrolled 2 times (for comparing with vec2) or 4 times (for comparing with vec4), would we still see a difference in performance?  This is puzzling because I would have thought that although the offset version requires more instructions the additional unrolled instructions would be pipelined while waiting for the first load (or store) to complete -- and so the additional unrolled instructions would not present a performance penalty.  (https://stackoverflow.com/q...My guess:  In the offset case, contiguous cache lines are ""loaded/stored"" to/from different warps and the cache lines a warp ""loads/stores"" are far apart in memory.  In the vector case, a warp ""loads/stores"" contiguous cache lines.  Stores involve read-write.  These three attributes would tend to make the vector version perform better.The blog post utilizes “type punning” which is Undefined Behavior as per the C/C++ Standard.Do you know if there is any section in the CUDA documentation that explicitly states this particular type of aliasing is allowed and well-defined?Powered by Discourse, best viewed with JavaScript enabled"
3540,learn-how-to-build-applications-of-ai-for-anomaly-detection,"Originally published at:			https://developer.nvidia.com/blog/learn-how-to-build-applications-of-ai-for-anomaly-detection/
The NVIDIA Deep Learning Institute (DLI) is offering instructor-led, hands-on training on how to implement multiple AI-based approaches to solve a specific use case of identifying network intrusions for telecommunications.Powered by Discourse, best viewed with JavaScript enabled"
3541,gtc-2020-nvidia-math-libraries,"GTC 2020 CWE21216
Presenters: Harun-Bayraktar,NVIDIA; Samuel-Rodriguez-Bernabeu, ; Markus-Hoehnerbach, ; Azzam-Haidar, ; Piotr-Majcher, ; Mahesh-Khadatare, ; Zoheb-Khan, ; Lukasz-Ligowski,
Abstract
Meet the engineers that create the NVIDIA Math Libraries to get answers to your questions or simply to give your feedback on existing functionality such as how can I leverage Tensor Cores? Or simply join us to request new functionality you need and is missing in our libraries. We will have engineers from linear algebra libraries: cuBLAS, cuSOLVER, cuSPARSE, cuTENSOR; and signal and image processing libraries: cuFFT, NPP and nvJPEG.Connect directly with NVIDIA Experts to get answers to all of your questions on GPU programming and code optimization, share your experience, and get guidance on how to achieve maximum performance on NVIDIA’s platform.Watch this session
Join in the conversation below.Powered by Discourse, best viewed with JavaScript enabled"
3542,gtc-2020-fast-evaluation-of-eigenvalues-of-the-overlap-dirac-operator-in-lattice-gauge-theory,"GTC 2020 S21219
Presenters: Naga Vijayalakshmi Vydyanathan,NVIDIA; Nilmani Mathur,Tata Institute of Fundamental Research
Abstract
We’ll describe how to efficiently perform numerical calculations of lattice gauge theory with the Overlap-Dirac action on a 4-dimensional Euclidean space-time lattice. Lattice gauge theory computations on large lattices are crucial for the quantitative study of the physics of strong interactions. This computationally expensive problem requires large supercomputing facilities. We’ll show how to accelerate the eigenvalue computations of the Overlap-Dirac operator for large lattices on multiple GPUs using MPI, OpenACC and CUDA, and discuss performance results on large lattices using hundreds of GPUs. This application is a property of the Indian Lattice Gauge Theory Initiative and its computations will be used in predicting results of experiments in high-energy laboratories such as at CERN (Switzerland), KEK (Japan), BES (China), Fair (Germany), and BNL (U.S.).Watch this session
Join in the conversation below.Powered by Discourse, best viewed with JavaScript enabled"
3543,optimizing-the-high-performance-conjugate-gradient-benchmark-on-gpus,"Originally published at:			https://developer.nvidia.com/blog/optimizing-high-performance-conjugate-gradient-benchmark-gpus/
[This post was co-written by Everett Phillips and Massimiliano Fatica.] The High Performance Conjugate Gradient Benchmark (HPCG) is a new benchmark intended to complement the High-Performance Linpack (HPL) benchmark currently used to rank supercomputers in the TOP500 list. This new benchmark solves a large sparse linear system using a multigrid preconditioned conjugate gradient (PCG) algorithm.…I have been looking for a good STREAM (Triad) implementation in CUDA. I hacked my own version which is however somewhat slower (247/217 GB/s) on the K40 (using GPU boost). Which implementation did you use? Is it available somewhere or can I just cite this post ? In that case do you have numbers for a K20?Are you going  to post this hpcg cuda optimized implementation either in binary or source code form to cuda registered developers?I noticed that the ""processor"" count in the section ""Analysis of the First HPCG List"" does not match the values in the HPCG results for June 2014 (even after correcting for just counting the accelerators).  Are there more detailed results available than the PDF table of results on the HPCG web site?The numbers used in the table are the number of MPI processes. Since all the systems used a single MPI process per accelerator (Xeon Phi or Tesla K20x) or node ( in the case of K),  it was the right metric to use to compute the available bandwidth of the systems. It also give a better idea of the size of the system. The official HPCG list shows the processor count, similar to HPL.  You can request the full YAML files from the HPCG authors. During the HPCG BoF at SC14, the authors indicated that YAML files should be downloadable in the near future.My very simple version of STREAM in CUDA delivers 150 GB/s on the K20 GPGPUs in the TACC Stampede system (ECC enabled).  This is almost exactly what one would expect from linear scaling of the K20X result above by the peak memory bandwidths of the two models: 182 * 208/250 = 151 GB/s.On the other hand, my version delivers unimpressive results on the K40 -- no more than 192 GB/s on the K40 GPGPUs in the TACC Maverick system (ECC enabled) -- about 12% lower than the value quoted above.Which frequency did you use on the K40?With ECC enabled I get 192GB/s with the base clock (745 Mhz) and 217 GB/s running at 875 MHz. This is a well known issue for the K40.Given the agreement with my result I assume you just left the K40 at its base clock ?Thanks!  My K40 was running at the base clock (745 MHz on this system), and I don't currently have permission to change the operating frequency.  I will go try to fix that problem.  ;-)745 MHz is the base clock of the K40. I mixed that up with the K20. I corrected my above comment.Any plan to release the source code? HPCG website released some binaries by NVIDA but only for CUDA 8 and 6.5, couldn't run it :( why not the source?Powered by Discourse, best viewed with JavaScript enabled"
3544,accelerating-ai-training-with-mlperf-containers-and-models-from-nvidia-ngc,"Originally published at:			https://developer.nvidia.com/blog/accelerating-ai-training-with-mlperf-containers-and-models-from-ngc/
The MLPerf consortium mission is to “build fair and useful benchmarks” to provide an unbiased training and inference performance reference for ML hardware, software, and services. MLPerf Training v0.7 is the third instantiation for training and continues to evolve to stay on the cutting edge. This round consists of eight different workloads that cover a…Powered by Discourse, best viewed with JavaScript enabled"
3545,scaling-xgboost-performance-with-rapids-cuml-dask-and-google-kubernetes-engine-gke,"Originally published at:			https://developer.nvidia.com/blog/scaling-xgboost-performance-with-rapids-cuml-dask-and-google-kubernetes-engine-gke/
RAPIDS cuML provides scalable, GPU-accelerated machine learning models with a Python interface based on the scikit-learn API. This guide will walk through how to easily train cuML models on multi-node, multi-GPU (MNMG) clusters managed by Google’s Kubernetes Engine (GKE) platform. We will examine a subset of the available MNMG algorithms, illustrate their use of leveraging…Powered by Discourse, best viewed with JavaScript enabled"
3546,new-physics-research-aims-to-answer-the-mysteries-of-the-universe,"Originally published at:			New Physics Research Aims to Answer the Mysteries of the Universe | NVIDIA Technical Blog
Physicists from more than a dozen institutions used the power of the GPU-accelerated Titan Supercomputer at Oak Ridge National Laboratory to calculate a subatomic-scale physics problem of measuring the lifetime of neutrons. The study, published in Nature this week, achieves groundbreaking precision and provides the research community with new data that could aid in the…Powered by Discourse, best viewed with JavaScript enabled"
3547,introduction-to-real-time-ray-tracing-with-vulkan,"Originally published at:			Introduction to Real-Time Ray Tracing with Vulkan | NVIDIA Technical Blog
NVIDIA’s new Turing GPU unleashed real-time ray-tracing in a consumer GPU for the first time. Since then, much virtual ink has been spilled discussing ray tracing in DirectX 12. However, many developers want to embrace a more open approach using Vulkan, the low-level API supported by the Khronos Group. Vulkan enables developers to target many…Hey Nuno, I have a question (or rather a suggestion) about building the acceleration structure. I've been trying to find more information about this but I think the documentation skips it. As far as I know there is no way of defining the instancing structure so it should be defined in the API but it is nowhere to be found on the documentation. I'm talking about the transform for each bottom level acceleration structure. In the NVIDIA Vulkan examples they use the given struct:struct VkGeometryInstance{    float transform[12];    uint32_t instanceId : 24;    uint32_t mask : 8;    uint32_t instanceOffset : 24;    uint32_t flags : 8;    uint64_t accelerationStructureHandle;};I found out the hard way that if you do not use this exact struct building the top level acceleration struct will kill the device handle. If this is correct, you might want to mention it in the documentation on vkCmdBuildAccelerationStructureNVX or even in this blog. If not, what do you think is going on with my program ? (Or am I not looking at the wrong or outdated documentation ?)Hi. Do you have a simple example intersection with one triangle?Powered by Discourse, best viewed with JavaScript enabled"
3548,gtc-2020-overcoming-latency-barriers-strong-scaling-hpc-applications-with-nvshmem,"GTC 2020 S21673
Presenters: Mathias Wagner,NVIDIA
Abstract
For scientific advancement through HPC, ever-increasing simulation capabilities are not the only key to success. Obtaining timely results is often even more important. Reducing the time-to-solution generally requires the application to be strong-scalable. However, scaling up improved single-GPU performance faces many obstacles. We’ll show you how to improve the strong-scaling on systems equipped with NVIDIA GPUs. Avoid or hide latencies by exploiting GPU-centric communication with NVSHMEM, an implementation of OpenSHMEM for GPUs. After introducing NVSHMEM, we’ll share best practices gathered from using NVSHMEM for QUDA, a library for Lattice QCD on GPUs used by codes as MILC and Chroma. We show results obtained on fat-GPU nodes like DGX-1/2, as well as scaling them to 1,000 GPUs in InfiniBand-connected systems, including Summit.Watch this session
Join in the conversation below.Powered by Discourse, best viewed with JavaScript enabled"
3549,improving-gpu-performance-by-reducing-instruction-cache-misses,"Originally published at:			https://developer.nvidia.com/blog/improving-gpu-performance-by-reducing-instruction-cache-misses/
Instruction cache misses can cause performance degradation for kernels that have a large instruction footprint, which is often caused by substantial loop unrolling.Powered by Discourse, best viewed with JavaScript enabled"
3550,nvidia-ai-perception-coming-to-ros-developers,"Originally published at:			https://developer.nvidia.com/blog/nvidia-ai-perception-coming-to-ros-developers/
NVIDIA announces new initiatives to deliver a suite of perception technologies to the ROS developer community.Powered by Discourse, best viewed with JavaScript enabled"
3551,nvidia-and-tech-leaders-team-to-build-gpu-accelerated-arm-servers,"Originally published at:			NVIDIA and Tech Leaders Team to Build GPU-Accelerated Arm Servers | NVIDIA Technical Blog
At Supercomputing 2019 in Denver, Colorado, NVIDIA introduced a reference design platform that enables companies to quickly build GPU-accelerated Arm-based servers, driving a new era of high performance computing for a growing range of applications in science and industry. The reference design platform — consisting of hardware and software building blocks — responds to growing…Powered by Discourse, best viewed with JavaScript enabled"
3552,gtc-2020-accelerating-adobe-s-intelligent-service-via-gpu-based-spark-3-0-and-xgboost,"GTC 2020 S22661
Presenters: Lei Zhang,Adobe
Abstract
In Adobe’s Intelligent Services, Journey AI can optimize the delivery of marketing messages. It includes several complex features and advanced machine learning models. With the debut of GPU-based Spark 3.0, the processing speed of these features can be greatly increased. Furthermore, GPU-based XGBoost offers a dramatic acceleration in model training for machine learning features. This talk will briefly introduce Adobe Intelligent Services and provide the evaluation results of GPU-based Spark 3.0 and XGBoost on the acceleration of Adobe’s Journey AI.Watch this session
Join in the conversation below.Powered by Discourse, best viewed with JavaScript enabled"
3553,improve-shader-performance-and-in-game-frame-rates-with-shader-execution-reordering,"Originally published at:			https://developer.nvidia.com/blog/improve-shader-performance-and-in-game-frame-rates-with-shader-execution-reordering/
Learn about Shader Execution Reordering (SER), a performance optimization that unlocks the potential for better ray and memory coherency in ray tracing shaders.Powered by Discourse, best viewed with JavaScript enabled"
3554,nvidia-optix-sdk-tackles-hair-and-fur-with-release-7-1,"Originally published at:			https://developer.nvidia.com/blog/optix-sdk-7-1/
NVIDIA OptiX is an application framework for optimal ray tracing performance on the GPU that is used for film and video production as well as many other professional graphics applications. OptiX SDK 7.1 is the latest update to the new OptiX 7 API. Version 7.1 introduces a new curve primitive for hair and fur, tiled…Powered by Discourse, best viewed with JavaScript enabled"
3555,cineca-to-build-world-s-fastest-ai-supercomputer,"Originally published at:			https://developer.nvidia.com/blog/cineca-to-build-worlds-fastest-ai-supercomputer/
NVIDIA this week announced that the Italian inter-university consortium CINECA — one of the world’s most important supercomputing centers — will use the company’s accelerated computing platform to build the world’s fastest AI supercomputer. The new “Leonardo” system, built with Atos, is expected to deliver 10 exaflops of FP16 AI performance to enable advanced AI and HPC converged…Powered by Discourse, best viewed with JavaScript enabled"
3556,gtc-2020-fast-ai-data-pre-processing-with-nvidia-data-loading-library-dali,"GTC 2020 CWE21758
Presenters: Janusz-Lisiecki,NVIDIA; Joaquin-Anton-Guirao, ; Krzysztof-Lecki, ; Serge-Panev, ; Rafal-Banas, ; Michal-S, ; Albert, ; Michal-Z,
Abstract
Come ask your AI/DL data loading, augmentation and pre-processing questions to experts in the field. Learn about NVIDIA’s Data Loading Library (DALI) and the strategies you can employ to avoid IO and memory limitation.With every generation of GPU it becomes increasingly more difficult to keep the data pipeline full so that the GPU can be fully utilized. NVIDIA Data Loading Library (DALI) is our response to that problem. It is a portable, open source library for decoding and augmenting images and videos to accelerate deep learning applications. In this session, you will learn more about DALI and how it can address your needs.Watch this session
Join in the conversation below.Powered by Discourse, best viewed with JavaScript enabled"
3557,capture-6x-better-temporal-resolution-cardiac-imaging-at-any-heart-rate-with-fujifilm-healthcare-cardio-stillshot,"Originally published at:			https://developer.nvidia.com/blog/capture-6x-better-temporal-resolution-cardiac-imaging-at-any-heart-rate-with-fujifilm-healthcare-cardio-stillshot/
Using NVIDIA GPUs, Fujifilm Healthcare developed Cardio StillShot to capture cardiac imaging at any heart rate, with 6x better temporal resolution of cardiac CT images.Powered by Discourse, best viewed with JavaScript enabled"
3558,folding-home-crowdsources-gpu-accelerated-exaflop-supercomputer-for-covid-19-research,"Originally published at:			Folding@Home Crowdsources GPU-accelerated exaFLOP Supercomputer for COVID-19 Research | NVIDIA Technical Blog
To help tackle COVID-19, the long-running Folding@Home program, a distributed computing project for simulating protein dynamics, hit a breakthrough by achieving more than an exaflop of processing power. That’s more than 1,000,000,000,000,000,000 operations per second, all through crowdsourcing efforts.  For comparison, Summit, the world’s fastest supercomputer, which is powered by more than 27,000 NVIDIA V100…Powered by Discourse, best viewed with JavaScript enabled"
3559,boston-university-researchers-use-ai-to-detect-kidney-disease,"Originally published at:			Boston University Researchers Use AI to Detect Kidney Disease | NVIDIA Technical Blog
Researchers at Boston University developed a deep learning algorithm that can assess kidney disease with better accuracy than trained pathologists. Detecting kidney damage is of great importance, and unlike many other diseases, symptoms often don’t appear until the disease is very advanced. Getting this diagnosis wrong can lead to a series of life-threatening conditions. “This…Powered by Discourse, best viewed with JavaScript enabled"
3560,refreshing-a-live-service-game,"Originally published at:			https://developer.nvidia.com/blog/refreshing-a-live-service-game/
We talked to Haiyong Qian, NetEase Game Engine Development Researcher and Manager of NetEase Thunder Fire Games Technical Center, to see what he’s learned as the Justice team added NVIDIA ray-tracing solutions to their development pipeline.Powered by Discourse, best viewed with JavaScript enabled"
3561,using-multi-scale-attention-for-semantic-segmentation,"Originally published at:			Using Multi-Scale Attention for Semantic Segmentation | NVIDIA Technical Blog
There’s an important technology that is commonly used in autonomous driving, medical imaging, and even Zoom virtual backgrounds: semantic segmentation. That’s the process of labelling pixels in an image as belonging to one of N classes (N being any number of classes), where the classes can be things like cars, roads, people, or trees. In…The code is now public here: GitHub - NVIDIA/semantic-segmentation: Nvidia Semantic Segmentation monorepoPowered by Discourse, best viewed with JavaScript enabled"
3562,nvidia-a100-gpus-available-on-google-cloud-s-compute-engine,"Originally published at:			https://developer.nvidia.com/blog/nvidia-a100-gpus-available-on-google-clouds-compute-engine/
By Dave Salvator, Senior Manager, Product Marketing at NVIDIA NVIDIA and Google Cloud are making it possible for applications to push the boundaries of accelerated AI across a wide array of applications. With its new A2 VM, announced today, Google Cloud provides customers the largest configuration of 16 NVIDIA A100 GPUs in a single VM. Also…Powered by Discourse, best viewed with JavaScript enabled"
3563,accelerate-r-applications-with-cuda,"Originally published at:			https://developer.nvidia.com/blog/accelerate-r-applications-cuda/
R is a free software environment for statistical computing and graphics that provides a programming language and built-in libraries of mathematics operations for statistics, data analysis, machine learning and much more. Many domain experts and researchers use the R platform and contribute R software, resulting in a large ecosystem of free software packages available through…Thanks Patric, great overview!We work in a Windows environment and have found very little compiled R application that can take advantage of CUDA. The matrix operations we do are highly divisible (perfect for CUDA) but most of what we do multi-threads over CPU cores.Can you recommend an R solution that will allow us to parallel process with CUDA in a windows environment.Thanks PatHi Patrick,Thanks for your comments.In your case for matrix operations, I suggest you employ cublasxt ( https://developer.nvidia.co... ) and/or magma (https://developer.nvidia.co... library from R. To apply these libraries, you need to write up your own interface functions as this post described.On windows, you can build your interface functions to DLL file and then still load by dyn.load() in R. One simple way is to use Visual Studio (IDE) to create/build a DLL project including your CUDA libraries call or CDUA C/C++/FORTRAN code. You can refer this document (http://docs.nvidia.com/cuda... to setup the development environment.Hope this can help you!-PatricHi Patric,Thank you very much for this nice tutorial!I have a question especially regarding the FFT.I was using your example:num <- 4set.seed(5)z <-  complex(real = stats::rnorm(num), imaginary = stats::rnorm(num))cpu <- fft(z)gpu <- cufft1D(z)However, if I compute the sums, I get (very slightly) different results in the imaginary part:> sum(cpu)[1] -3.363422+6.845763i> sum(gpu)[1] -3.363422+6.845764iIt does not look severe, but since I am using FFT on a very precise level, I found that this numerical error is multiplying itself rather severely. I am using CUDA 6.5.12 for my analysis.Do you have any suggestion what I could do?Hi Christian,It's great that you can run R by CUDA and leverage GPU's power :)And I'm sorry it is a little vague of data type in my code resulting a slight different results, and I will update the code later. Thanks for catching this point. Actually, in my example, I used single precision cuFFT flag and API ( cufft_C2C / cufftExecC2C). For high precision calculations, you can use double precision API of cuFFT by following changes in the code:type,singe precision, double precison data define,          cufftComplex,                    cufftDoubleComplexflag,                 CUFFT_C2C,                    CUFFT_Z2ZAPI,                 cufftExecC2C,                    cufftExecZ2ZRefer: http://docs.nvidia.com/cuda...So the results will be accurate :> sum <- 4> set.seed(5)> z <- complex(real = stats::rnorm(num), imaginary = stats::rnorm(num))> cpu <- fft(z)> gpu <- cufft1D(z)> sum(cpu)[1] -3.363422+6.845763i> sum(gpu)[1] -3.363422+6.845763i> cpu[1] -0.6418452+0.0009952i  0.4470997+0.8693907i -3.5508495+2.4775538i[4]  0.3821731+3.4978238i> gpu[1] -0.6418452+0.0009952i  0.4470997+0.8693907i -3.5508495+2.4775538i[4]  0.3821731+3.4978238iEnjoy CUDA and GPU programming!Thanks,-PatricHi Patric,thanks a lot, that worked for me!I am using FFT to convolute distribution functions, to receive something like a yearly loss distribution from several single losses that happened over the year. As can be shown, this convolution is quite tedious when done in the space of distributions, but rather easy when done by using the Fourier transformation.What is interesting, that the GPU does not always beat the CPU (at least in my case) but only when I calculate the FFT on a very huge grid. I will play around with it to find out where the ""break-even"" lies, to further define rules for me when to use the CPU and when to use the GPU. However, CUDA adds a great deal to my performance, especially with the R-interface.Kind regards,ChristianHi Christian,Thanks for your explanations about the background and performance improvements of your application.I suggest you to try BATCH mode of cuFFT for small size of computation if the data is independent.""Execution of multiple 1D, 2D and 3D transforms simultaneously. These batched transforms have higher performance than single transforms.""Check out the document of cuFFT including an example for batch mode computation.http://docs.nvidia.com/cuda...Hope this can help for you.BR,-PatricHi Patric, I am getting this error ""LIBCMTD.lib(crt0.obj) : error LNK2019: unresolved external symbol _main referenced in function ___tmainCRTStartup"" on VS2013 using CUDA 6.5. What could it be?Thanks Patric. I am getting Error in .C(""cufft"", as.integer(n), as.integer(inverse), as.double(Re(z)),  :   C symbol name ""cufft"" not in load tableIt is strange since I am using ""extern"" as you indicatedThanks for your interesting for our post :)Actually, I was compiling and testing my code in Linux, so I am not very sure this problem in VS IDE. But I think you're correct to add 'extern ""C""' keyword :)Several high level suggestions to debug the problem:0) save the source code with .c prefix (not .cpp)1) ensure the compiled function (32bit/64bit) matched your R-installation2) try to compile a pure C/C++ code (without CUDA code) and load in R3) build pure CUDA code in VS (with main function to run and test)4)  finally, combine 2) and 3)Hope this can help you. Feel free to let's know if you have further problems and questions in compiling or using CUDA in R.Best Regards,--PatricThank you very much Patric, really appreciate your help. I double checked my code in VS2013 in 64bits. Now, it works like a charm. I wasn't declaring properly the host function , as it should be for example 'extern ""C"" __declspec(dllexport) void vectorAdd(double* param){ '. My code is a .cu file, so there are not problems caused by the file type. By the way, the R function getLoadedDLLs() might be showing TRUE for our dll, but if we forgot to properly add ""__declspec(dllexport)"", we still wont be able to fire the functions as was happening to me (which naturally makes sense, since we are not flagging those functions to be exported)Cool! Thanks for these tips of VS2013 in Windows.hi patric. I need similar steps for windows. could you suggest me some good links for the same.Hi Randy,Thanks for your interest in our blog.I have updated the blog for how to build R applications with CUDA by VS2013 on Windows. Hope it can help for you.Thanks.Hi Patric,Can we expect the same at some point for Stata, either running under Windows or Unix?Best regards,EricDoes Stata have a programming language with a foreign function interface? Does it rely on any standard math library interfaces (e.g. BLAS) where CUDA libraries could be used as a replacement?Hi Patric, although this is 2 years ago posting, I really happy to see this article. Thank you! :DThank you for the interesting article and practical examples.I would like to point out what I suspect are two small errors:1) In Figure 2, the Cuda code for the FFT, the first line is an incomplete include statement. My guess would be that the line 1 should read:""""""#include <r.h>""""""(the R should be capital, Disqus is not rendering it as such on my end for some reason).And the file compiles when this change is made.2) In Figure 4, the Vector Add function, line 30, where the vector add kernel is being called, references variables ""gridsize"" and ""blocksize"", whereas the variables were defined with a capital S: ""gridSize"" and ""blockSize"".Fixed, thanks!Thanks for this detailed instruction. I have a performance Issues with CUDA and R. I achieved a good performance when I use cublas for a matrix multiplication. It is about 6 seconds for MatrixA(10000,10000), MatrixB(10000,10000).However,  the performance is not good when I used the kernel function developed in NVIDIA SDK for a matrix multiplication. It is about 40 seconds for MatrixA(10000,10000), MatrixB(10000,10000).I have no idea why it occurs. I really appreciate if you have any suggestions.Powered by Discourse, best viewed with JavaScript enabled"
3564,gpudirect-storage-a-direct-path-between-storage-and-gpu-memory,"Originally published at:			https://developer.nvidia.com/blog/gpudirect-storage/
Keeping GPUs Busy As AI and HPC datasets continue to increase in size, the time spent loading data for a given application begins to place a strain on the total application’s performance. When considering end-to-end application performance, fast GPUs are increasingly starved by slow I/O. I/O, the process of loading data from storage to GPUs…An application of PostgreSQL database. SSD-to-GPU Direct SQL exactly accelerate I/O intensive workloads using GPU!The slides below is at PGconf.EU 2018. Please check it out.https://www.slideshare.net/...Hello,This article does a good job of pointing out the bottleneck of PCIe connections between CPU and GPU that causes access of CPU memory to/from GPU to be at the speed of an I/O on servers using x86 CPU processor architectures.  Thankfully, NVIDIA has partnered with IBM to create servers (IBM AC922 with V100 GPUs) that overcome this bottleneck by having NVLink 2.0 between CPUs and GPUs, thereby allowing the CPU memory to be used by GPUs via NVLink.  This allows GPU models/data to grow up to about 2TB of memory on a single server and using GPUDirect RDMA over inifiniband to scale to many servers. It would be interesting to put that throughput on figure 6. If I am interpreting that figure correctly as bidirectional bandwidth, the line for the NVIDIA/IBM server with 4 GPUs would be 150GB/s (about three times as much as GPUDirect IO).  Did I interpret that figure correctly?  For inclusion on Figure 5, I believe that the latency for NVLink 2.0 access to/from GPU to IBM Power9 CPU is somewhere under 10 microseconds. Is the Y axis in figure 5 seconds, milliseconds, or microseconds?    For I/O, thankfully, the NVIDIA/IBM server also provides Gen4 PCIe to double that throughput as well as provide a faster interconnect for multi-node workloads using GPUDirect RDMA over Infiniband.  And it's important to not forget workload/data management provided by Spectrum LSF that can preload data to where it needs to be before a job starts, allowing data to be stored on low-cost storage until need, then moved to NVMe devices. Is there another article that includes the benefits of CPU to GPU NVLink 2.0 and PCIe Gen4 when combined with GPUDIrect I/O?While I understand the direct DMA to and from GPU memory part, it is not clear to me how this can achieve the advantages currently provided by file system cache. Also it is not clear how a programmer avoid typical problems like having a copy of same file data in each GPU's memory and keeping them in sync etc. What kind of tools are provided for debugging by Nvidia? May be I think old fashioned way but one has to always assume programmers will make mistakes and if you let hundreds of GPU cores do I/O operations that can get real tough to debug.This is a very impressive technical work!It is a bit unfortunate however that this blog does not cite or compare with prior art from my group which not only shows how to enable peer to peer DMA from GPU to/from NVMe including RAID, but also integrated it all with CPU page cache, combined the two in the best way and made it transparent for users via POSIX FS calls. The project called SPIN has been open source since 2017.https://www.usenix.org/conf...https://github.com/acsl-tec...This is really impressive, thanks for this post and for including such detailed level of performance measurements. I had the pleasure of talking with CJ at GTC this year. I find it interesting that mmap() and faulting in to GPU memory using UVA does not perform better than the mmap() + cudaMemcpy(), but I guess it may have something to do with the depth of the PCIe tree? Is there a plan to distribute GPUDirect Storage as an additional API, or adding it to CUDA, or is this conceived as a DGX feature only?I guess this may be the right place to post it, but I've implemented a NVMe-driver library for CUDA applications (that can run both on CPU and GPU) previously, and did a talk about it on GTC 2019. That work was heavily inspired by @kaigaikohei:disqus's poster on GTC a couple of years back. The software is open source, and works on a single machine, but can also work across multiple PCIe root complexes by using Dolphin's NTB technology.https://developer.nvidia.co...https://github.com/enfiskut...I have GTX 1650 GPU. DOES GDS WORK ON THAT?Bullet 1 here describes supported GPUsRelease information for NVIDIA® Magnum IO GPUDirect® Storage.Powered by Discourse, best viewed with JavaScript enabled"
3565,developing-a-question-answering-application-quickly-using-nvidia-jarvis,"Originally published at:			https://developer.nvidia.com/blog/developing-a-question-answering-application-quickly-using-jarvis/
There is a high chance that you have asked your smart speaker a question like, “How tall is Mount Everest?” If you did, it probably said, “Mount Everest is 29,032 feet above sea level.” Have you ever wondered how it found an answer for you? Question answering (QA) is loosely defined as a system consisting…Have you tried Jarvis QA yet? If not, you should.
Did you notice Jarvis QA fail to answer your question? If so, please mind that Jarvis QA has token size limit of 384 (w/ BERT Large). If you tried with the passage longer than that, the answer may be incorrect or missing. Nevertheless, you should not be discouraged with such limitation. See if you can come up with a way to cover the context in its entirety to find the answer.has anyone ran into an issue where running jarvis_nlp.NaturalQuery crashes the tritonserver? Looking at the start-jarvis script inside of the container, you cant tell which process actually failed.while sleep 5; do
ps aux | grep tritonserver | grep -q -v grep
PROCESS_1_STATUS=$?
ps aux | grep jarvis_server | grep -q -v grep
PROCESS_2_STATUS=$?
if [ $PROCESS_1_STATUS -ne 0 -o $PROCESS_2_STATUS -ne 0 ]; then
echo “One of the processes has exited unexpectedly. Stopping container.”
exit 1
fi
done
Screen Shot 2021-05-05 at 11.28.54 AM1914×1036 507 KB
Just tried using a different GPU and now things are working.@mika.ayenson – Good to hear! Let us know if you have any other problems going through this post’s solution.Anyone who would have answered IR is QA in my AI class would have gotten a 0 on the essay question. QA requires NLU. For example, You are sailing on a small boat with your two friends. You hop off the boat at the dock and look back at the boat. How many people do you see?You can’t answer such questions with IR, you have to understand implicit context, situational understanding, etc.Using Wikipedia as a source of knowledge is risky as while it may be good for IR demo’s it is will also contain incorrect knowledge.Powered by Discourse, best viewed with JavaScript enabled"
3566,full-stack-innovation-fuels-highest-mlperf-inference-2-1-results-for-nvidia,"Originally published at:			Full-Stack Innovation Fuels Highest MLPerf Inference 2.1 Results for NVIDIA | NVIDIA Technical Blog
NVIDIA delivered record-setting inference performance with the debut submission of H100 and the energy efficiency improvements delivered with the latest NVIDIA AGX Jetson Orin submission.Powered by Discourse, best viewed with JavaScript enabled"
3567,nvidia-hopper-architecture-in-depth,"Originally published at:			https://developer.nvidia.com/blog/nvidia-hopper-architecture-in-depth/
Everything you want to know about the new H100 GPU.A question, arose with reading the news, is related to L2 (L1) bandwidth for sustained 3TB/s memory bandwidth on that latency levels of Grace and Hopper architectures.
Are there suggestions or real numbers (from cpu/gpu analysis) for to sort in these high numbers on cache and memory transfer bw (thx)?Any news on this GPU accelerator?Where (in the cloud?) can I test it? More specifically, I would like to test its TEE capabilities. Is there any SDK/documentation available for this GPU?Powered by Discourse, best viewed with JavaScript enabled"
3568,nvidia-and-ibm-cloud-support-imagenet-large-scale-visual-recognition-challenge,"Originally published at:			https://developer.nvidia.com/blog/nvidia-ibm-cloud-support-imagenet-large-scale-visual-recognition-challenge/
This year’s ImageNet Large Scale Visual Recognition Challenge (ILSVRC) is about to begin. Every year, organizers from the University of North Carolina at Chapel Hill, Stanford University, and the University of Michigan host the ILSVRC, an object detection and image classification competition, to advance the fields of machine learning and pattern recognition. Competitors are given…Powered by Discourse, best viewed with JavaScript enabled"
3569,new-on-ngc-catalog-samsung-sds-brightics-an-ai-accelerator-for-automating-and-accelerating-deep-learning-training,"Originally published at:			https://developer.nvidia.com/blog/new-on-ngc-catalog-samsung-sdss-brightrics-an-ai-accelerator-for-automating-and-accelerating-deep-learning-training/
The Kubernetes-based, containerized application, is now available on the NVIDIA NGC Catalog - a GPU-optimized hub for AI and HPC containers.Powered by Discourse, best viewed with JavaScript enabled"
3570,using-machine-learning-for-green-screen-matting,"Originally published at:			Using Machine Learning for Green Screen Matting | NVIDIA Technical Blog
A Danish startup has trained neural networks to recognize the pattern of a perfect cut-out. CloudCutout is applying machine learning techniques to the quotidian yet pernickety process of isolating an image from a background. Their initial target market is school photography, with the aim of undercutting the market by charging half the “standard knockout” bulk cost…Powered by Discourse, best viewed with JavaScript enabled"
3571,announcing-the-jetson-xavier-nx-world-s-smallest-ai-supercomputer-for-embedded-systems,"Originally published at:			Announcing the Jetson Xavier NX, World’s Smallest AI Supercomputer for Embedded Systems | NVIDIA Technical Blog
To support critical embedded AI systems such as drones, portable medical devices, smart cameras, high-resolution sensors, and other IoT embedded systems, NVIDIA today introduced Jetson Xavier NX, the world’s smallest, most powerful AI supercomputer for embedded and edge systems.  The energy-efficient Jetson Xavier NX delivers server-class performance with up to 21 TOPS, perfect for running…Powered by Discourse, best viewed with JavaScript enabled"
3572,gtc-2020-how-to-create-generalizable-ai,"GTC 2020 S22553
Presenters: Anima Anandkumar,NVIDIA
Abstract
Current deep-learning benchmarks focus on generalization on the same distribution as the training data. However, real-world applications require generalization to new, unseen scenarios, domains, and tasks. I’ll present key ingredients that I believe are critical towards achieving this, including (1) compositional systems that have modular and interpretable components; (2) unsupervised learning to discover new concepts; (3) feedback mechanisms for robust inference; and (4) causal discovery and inference that capture underlying relationships and invariances. Domain knowledge and structure can help enable learning in these challenging settings. My talk is beginner-friendly and will give a high-level overview of these challenges.Watch this session
Join in the conversation below.Hi everyone, I am the speaker of this session. I hope you enjoyed listening to the webinar. Feel free to post questions/comments.Powered by Discourse, best viewed with JavaScript enabled"
3573,moving-the-data-pipeline-into-the-fast-lane,"Originally published at:			Moving the Data Pipeline into the Fast Lane | NVIDIA Technical Blog
How GPUDirect Storage helps data scientists bypass the CPU I/O bottleneck In the world of data scientists, there’s usually a lot of waiting. That’s because they’re often working with extraordinary amounts of data — petabyte-sized datasets are not uncommon — and it can take a lot of time to move it.  Data scientists have recently…Powered by Discourse, best viewed with JavaScript enabled"
3574,enhancing-digital-twin-models-and-simulations-with-nvidia-modulus-v22-09,"Originally published at:			https://developer.nvidia.com/blog/enhancing-digital-twin-models-and-simulations-with-nvidia-modulus-v22-09/
NVIDIA Modulus v22.09 is now available with greater composition flexibility for neural operator architectures, improved training convergence and performance, and improved UX and documentation.Powered by Discourse, best viewed with JavaScript enabled"
3575,microsoft-sets-new-speech-recognition-record,"Originally published at:			Microsoft Sets New Speech Recognition Record | NVIDIA Technical Blog
Researchers at Microsoft announced they reached a 5.1% error rate which is a new milestone in reaching human parity for recognizing words in a conversation as well as professional human transcribers. They improved the accuracy of their system from last year on the Switchboard conversational speech recognition task. The benchmarking task is a corpus of…Powered by Discourse, best viewed with JavaScript enabled"
3576,create-high-quality-computer-vision-applications-with-superb-ai-suite-and-nvidia-tao-toolkit,"Originally published at:			https://developer.nvidia.com/blog/create-high-quality-computer-vision-applications-with-superb-ai-suite-and-nvidia-tao-toolkit/
Superb AI has introduced a revolutionary way for computer vision teams to drastically decrease the time it takes to deliver high-quality training datasets.Powered by Discourse, best viewed with JavaScript enabled"
3577,learn-to-build-in-the-omniverse-from-experts-in-3d-design-and-simulation-at-nvidia-gtc,"Originally published at:			https://developer.nvidia.com/blog/learn-to-build-in-the-omniverse-from-experts-in-3d-design-and-simulation-at-nvidia-gtc/
At Omniverse Developer Day, deep-dive into panels and expert-led breakout sessions to learn how to build, sell, and distribute your own 3D tools. Happening at GTC March 21-24.Powered by Discourse, best viewed with JavaScript enabled"
3578,leveling-up-graphics-and-performance-with-rtx-dlss-and-reflex-at-nvidia-gtc,"Originally published at:			Leveling Up Graphics and Performance with RTX, DLSS and Reflex at NVIDIA GTC | NVIDIA Technical Blog
We are excited to share over a dozen new and updated developer tools released today at GTC for game developers, including NVIDIA Reflex, RTXDI, and our new RTX Technology Showcase.Powered by Discourse, best viewed with JavaScript enabled"
3579,accelerating-nvidia-hpc-software-with-sve-on-aws-graviton3,"Originally published at:			Accelerating NVIDIA HPC Software with SVE on AWS Graviton3 | NVIDIA Technical Blog
The NVIDIA HPC SDK 22.7 now supports the AWS Gravition3 with auto-vectorization for the Scalable Vector Extension to the Arm architecture.I am trying to reproduce the SPEC cpu2017 fpspeed results in this blog. Can you please share the build flags used for these runs? Thanks.All results we measured on a single c7g-16xlarge AWS Graviton3 instance which has 64 cores.For the GNU results, I started with the “Example-gcc-linux-aarch64.cfg” config file included with CPU2017 updating the optimization flags to:OPTIMIZE         = -Ofast -fallow-argument-mismatch -fopenmp -march=armv8.4-a+crypto+rcpc+sha3+sm4+sve+nodotprod -lm -fpermissiveFor the NVHPC runs, I used the following config:Let me know if you have issues or questions.-MatPowered by Discourse, best viewed with JavaScript enabled"
3580,setting-up-gpu-telemetry-with-nvidia-data-center-gpu-manager,"Originally published at:			Setting Up GPU Telemetry with NVIDIA Data Center GPU Manager | NVIDIA Technical Blog
Understanding GPU usage provides important insights for IT administrators managing a data center. Trends in GPU metrics correlate with workload behavior and make it possible to optimize resource allocation, diagnose anomalies, and increase overall data center efficiency. NVIDIA Data Center GPU Manager (DCGM) offers a comprehensive tool suite to simplify administration and monitoring of NVIDIA Tesla-accelerated data…Is it possible to install it with Cloud GPU instance?Yeslooking to run the poc python file  in the blog have all the relevant pre-requisite running on a vm such as dcgm and nv host engine along with Prometheus as wellThis document describes how to use the NVIDIA Data Center GPU Management (DCGM) software.unable to run the python file complains of a module and there is no way it can be installed via pip as well i dont find this modulewhat are the ways i can get dcgm metrics so i can get gpu metrics to display it via prometheous to grafanapython inheritance_reader_example.py
Traceback (most recent call last):
File “inheritance_reader_example.py”, line 2, in 
from DcgmReader import DcgmReader
ImportError: No module named ‘DcgmReader’$ python dictionary_reader_example.py
Traceback (most recent call last):
File “dictionary_reader_example.py”, line 2, in 
from DcgmReader import DcgmReader
ImportError: No module named ‘DcgmReader’You likely need to add the directory containing DcgmReader.py to your PYTHONPATH environment.For example, on my system, DcgmReader.py is in /usr/src/dcgm/bindings, so I would run
PYTHONPATH=/usr/src/dcgm/bindings python dictionary_reader_example.pyPowered by Discourse, best viewed with JavaScript enabled"
3581,gtc-21-top-5-high-performance-computing-technical-sessions,"Originally published at:			https://developer.nvidia.com/blog/gtc-21-top-5-high-performance-computing-technical-sessions/
From weather forecasting and energy exploration, to computational chemistry and molecular dynamics, NVIDIA compute and networking technologies are optimizing nearly 2,000 applications across a broad-range of scientific domains and industries. By leveraging GPU-powered parallel processing, users can accelerate advanced, large-scale applications efficiently and reliably, paving the way to scientific discovery. Explore sessions across a variety…Powered by Discourse, best viewed with JavaScript enabled"
3582,top-5-ai-stories-of-the-week-4-1,"Originally published at:			Top 5 AI Stories of the Week: 4/1 | NVIDIA Technical Blog
In this week’s Top 5 #AI stories: See a new ImageNet speed record, an AI dance app, and a robot that can pick and toss over 500 objects per hour. Watch below: 5 – Learning Semantic Embedding Spaces for Slicing Vegetables Researchers from the Intelligent-Autonomous-Manipulation Lab at Carnegie Mellon University developed an AI-based robot that…Powered by Discourse, best viewed with JavaScript enabled"
3583,how-to-deploy-real-time-text-to-speech-applications-on-gpus-using-tensorrt,"Originally published at:			https://developer.nvidia.com/blog/how-to-deploy-real-time-text-to-speech-applications-on-gpus-using-tensorrt/
Conversational AI is the technology that allows us to communicate with machines like with other people. With the advent of sophisticated deep learning models, the human-machine communication has risen to unprecedented levels. However, these models are compute intensive, and hence require optimized code for flawless interaction. In this post, we’ll walk through how to convert…Powered by Discourse, best viewed with JavaScript enabled"
3584,new-workshop-data-parallelism-how-to-train-deep-learning-models-on-multiple-gpus,"Originally published at:			Data Parallelism - Train Deep Learning Models on Multiple GPUs | NVIDIA
Learn how to decrease model training time by distributing data to multiple GPUs, while retaining the accuracy of training on a single GPU.Powered by Discourse, best viewed with JavaScript enabled"
3585,cuda-spotlight-gpu-accelerated-genomics,"Originally published at:			CUDA Spotlight: GPU-Accelerated Genomics | NVIDIA Technical Blog
This week’s Spotlight is on Dr. Knut Reinert. Knut is a professor at Freie Universität in Berlin, Germany, and chair of the Algorithms in Bioinformatics group in the Institute of Computer Science. Knut and his team focus on the development of novel algorithms and data structures for problems in the analysis of biomedical mass data. In particular,…Powered by Discourse, best viewed with JavaScript enabled"
3586,egl-eye-opengl-visualization-without-an-x-server,"Originally published at:			https://developer.nvidia.com/blog/egl-eye-opengl-visualization-without-x-server/
If you’re like me, you have a GPU-accelerated in-situ visualization toolkit that you need to run on the latest-generation supercomputer. Or maybe you have a fantastic OpenGL application that you want to deploy on a server farm for offline rendering. Even though you have access to all that amazing GPU power, you’re often out of…Hi PeterIf my understanding is correct, this should work on a EC2 ubuntu server which doesn't have any monitor attached. However, when I run your code I am getting the below errorlibEGL warning: DRI2: xcb_connect failedlibEGL warning: DRI2: xcb_connect failedlibEGL warning: GLX: XOpenDisplay failedCould you let me know how to solve it?Thanks,JasonI'm getting the same error as reported by jasjuang too on ec2. Some information / documentation / trivial working application sample would be really useful.I got a crash during context creation. When I backtracked, I got very interesting list of libs:* /usr/lib/nvidia-361/libnvidia-eglcore.so.361.42* /usr/lib/nvidia-361/libGLESv1_CM_nvidia.so.1* /usr/lib/nvidia-361/libEGL_nvidia.so.0The app were linked with libs (nvidia-361.42-0ubuntu2): -L/usr/lib/nvidia-361 -lGL -L/usr/lib/nvidia-361 -lEGLOr even thought EGL_OPENGL_API were requested, OpenGL ES 1 code were executed.if you could tarball what you have and send it my way (or post it), much appreciated. I'll be able to compare with what I am doing and why it doesnt work for me!Hi Peter. I just tried out EGL for Desktop on Linux (OpenGL 4.3) .Are you aware that NVIDIA Video Codec SDK doesn't work with this setup? The NVENC encoder crashes with NV_ENC_ERR_UNSUPPORTED_DEVICE when OpenGL context is created via EGL.Hi Michael. NVENC should indeed work together with EGL. What GPU and driver are you using? Does your code work on the same system with an OpenGL context managed by X? And can you say a few more words about your application e.g. how does it create the context, how do you set up the CUDA interop to get the OpenGL buffer to NVENC etc? Thanks, PeterWell, your guys at NvPipe said it shouldn't work.  I am on 384 driver, Ubuntu 16.04. I can't really put here the whole details of my app.  But it works fine when I use glx to create gl context. But when I try EGL ,NVENC throws unsupported device error upon encoder init. I do use GL to interop with nvenc. I also setup CUDA context because on Linux I am trying to use ABGR format directly. Thanks.What my colleagues in the other thread mentioned is not that NVENC doesn't work with EGL, but rather that the convenience routines to compress OpenGL buffers directly are only supported for GLX created contexts. So if you're using an EGL managed context, you will need to use the legacy path: Map your OpenGL buffer via OpenGL/CUDA interop into CUDA and then compress the CUDA buffer with NVENC. Hope this clarifies the situation.This is wonderfull thing. I have been waiting ages for such support.I've been able to create EGL context and render stuff using nVidia drivers 384 (and 367 before) on debian/stretch.However, I have to glFlush() the rendering pipeline each ~100,000 triangles to be able to read proper pixels from the output. Not to mention the need to glFinish() before calling glReadPixels(). Otherwise I'm getting garbage in the output image (or nothing at all). GL layer reports no error.Is this some inherent problem with the driver/libraries or I am doing something wrong? Any pointers?Nice post. But where can I find the EGL headers ?The khronos ""eglplatform.h"" still needs X11:  https://www.khronos.org/reg...Line 116I still can't compile my program without X11Were you able to resolve the issue? We are facing a similar issue after creating EGL context. My email is mohit.juneja@gmail.comI managed to hack function simple function that waits till everything is finished, see waitForGL in the following file:https://github.com/Melown/l...It basically creates sync object (fence) and waits when it is signalled (I do it in a loop and wait for half a second because I want to see that the program is alive in the log). Then, calls glFinish() twice.This is the only way how I managed to make sure that everything is processed and I can safely grab framebuffer content.Does this work with recent drivers on Ubuntu 16.04? If not, what driver version and OS do you use?I've tried to use this, and it mostly works, wrapping around an existing OpenGL application, except for one thing: glGenFrameBuffers() fails. It always returns 0. In fact, I've had it segfault. Any idea what could cause this?(Titan XP, driver version 418.56)Powered by Discourse, best viewed with JavaScript enabled"
3587,mit-course-enables-students-to-build-robotic-racecars-from-jetson-tk1-kits,"Originally published at:			MIT Course Enables Students to Build Robotic Racecars from Jetson TK1 Kits | NVIDIA Technical Blog
The students’ challenge: Build a self-driving mini-robot car that can zip around a tunnel maze track while navigating its twists and turns. During the three-week class, students prototyped and tested autonomy algorithms leading to a timed race around the basement hallways of MIT. To give the cars their autonomous abilities, students design and program algorithms using…Powered by Discourse, best viewed with JavaScript enabled"
3588,an-end-to-end-blueprint-for-customer-churn-modeling-and-prediction-part-2,"Originally published at:			https://developer.nvidia.com/blog/an-end-to-end-blueprint-for-customer-churn-modeling-and-prediction-part-2/
Machine learning techniques attract a great deal of popular attention: it isn’t difficult to find articles explaining how to extract features or train models. But the end-to-end discovery process that produces a usable machine learning system consists of more than mere techniques. Furthermore, solving business problems with data extends beyond machine learning to encompass exploratory…Powered by Discourse, best viewed with JavaScript enabled"
3589,upcoming-dl-recsys-summit-develop-and-optimize-deep-learning-recommender-systems,"Originally published at:			https://developer.nvidia.com/blog/upcoming-dl-recsys-summit-develop-and-optimize-deep-learning-recommender-systems/
The NVIDIA, Facebook, and TensorFlow recommender teams will be hosting a summit with live Q&A to dive into best practices and insights on how to develop and optimize deep learning recommender systems.Powered by Discourse, best viewed with JavaScript enabled"
3590,nvidia-rtx-top-3-week-of-january-9-2018,"Originally published at:			NVIDIA RTX Top 3: Week of January 9, 2018 | NVIDIA Technical Blog
Every week, we’ll be delivering 3 interesting stories coming from the world of RTX Game Development. 1 – NVIDIA Announces GeForce RTX 2060 NVIDIA today announced the NVIDIA® GeForce® RTX 2060, putting revolutionary Turing architecture GPUs within the reach of tens of millions PC gamers worldwide. 2 -Project Sol Part 2: A Real-Time Ray-Tracing Cinematic…Powered by Discourse, best viewed with JavaScript enabled"
3591,share-your-science-earthquake-simulations-at-extreme-scale,"Originally published at:			Share Your Science: Earthquake Simulations at Extreme Scale | NVIDIA Technical Blog
Phil Maechling, a computer scientist at USC’s Southern California Earthquake Center shares how they are using the Tesla GPU-accelerated Titan and Blue Waters supercomputers with CUDA to analyze the impact of earthquakes and why and when they occur. “Instead of waiting for earthquakes to happen, we do physics-based simulations of “scenario” earthquakes,” said Maechling. “We…Powered by Discourse, best viewed with JavaScript enabled"
3592,decentralizing-ai-with-a-liquid-cooled-development-platform-by-supermicro-and-nvidia,"Originally published at:			https://developer.nvidia.com/blog/decentralizing-ai-with-a-liquid-cooled-development-platform-by-supermicro-and-nvidia/
AI is the topic of conversation around the world in 2023. It is rapidly being adopted by all industries including media, entertainment, and broadcasting. To be successful in 2023 and beyond, companies and agencies must embrace and deploy AI more rapidly than ever before. The capabilities of new AI programs like video analytics, ChatGPT, recommenders,…Powered by Discourse, best viewed with JavaScript enabled"
3593,an-efficient-matrix-transpose-in-cuda-c-c,"Originally published at:			An Efficient Matrix Transpose in CUDA C/C++ | NVIDIA Technical Blog
My last CUDA C++ post covered the mechanics of using shared memory, including static and dynamic allocation. In this post I will show some of the performance gains achievable using shared memory. Specifically, I will optimize a matrix transpose to show how to use shared memory to reorder strided global memory accesses into coalesced accesses.…I have been trying to understand thread and block indexing pattern in simple matrix copy example. Why do we use TILE_DIM as a stride while calculating y since we know that our Block size is (TILE_DIM * BLOCK_ROWS). Besides we are amortizing the calculation by forcing each thread to do TILE_DIM / BLOCK_ROWS copies. I tried considering Threads Per Block as (4,1) and Blocks Per Grid as (2,2) with square matrix width as 8. I find that the offset values created also go beyond 15 which is above the matrix linear (1D) dimensions. Kindly help using an example. I would like to see some tutorial on Matrix tiling with amortization explained in detail.__global__ void copy(float *odata, const float *idata){  int x = blockIdx.x * TILE_DIM + threadIdx.x;  int y = blockIdx.y * TILE_DIM + threadIdx.y;  int width = gridDim.x * TILE_DIM;  for (int j = 0; j < TILE_DIM; j+= BLOCK_ROWS)    odata[(y+j)*width + x] = idata[(y+j)*width + x];}A square matrix of size 8x8 has linear size of 64, which is much greater than  64. Even though each block has a number of rows smaller than TILE_DIM, each block is responsible for a whole tile.  The key here is that tiles (and the whole matrix) are square, while the thread blocks are not.Many thanks Mark for clarifying. If I am not wrong 4 thread grids will run through the kernel to cover the entire square matrix of size 8x8. And each thread will handle the copy 4 elements row-wise of the tile in a matrix. I'm runnig this example on Tesla K20mMy output isDevice : Tesla K20mMatrix size: 1024 1024, Block size: 32 8, Tile size: 32 32dimGrid: 32 32 1. dimBlock: 32 8 1                  Routine         Bandwidth (GB/s)                     copy              126.53       shared memory copy               83.17          naive transpose               53.42      coalesced transpose               62.11  conflict-free transpose               62.27As you can see Bandwith is much smaller in comparison to Tesla K20c. Bandwith for kernels that are useing shared memory are very slow. Could you give me a clue what is happening? My computer 2xTesla K20M 1xQuadro 410, LinuxThere are many things that could be wrong, but basically this indicates to me your K20m is not running at full speed.  K20m is a passively cooled K20c (meaning it has no fan -- the server fans must cool it).  Are you sure it is sufficiently cooled?  Is it getting sufficient power? Is it power capping? What version of the driver/toolkit are you using?  Is it ashared/multi-user node? Is Persistence mode enabled?  Have you checked the output of nvidia-smi?Some nvidia-smi outputs are:Driver version 319.37Performance state P0Power drawn 117.83 (during computation's)Power limit 225PCIE generation current 2 ; max 2All clock's at max. My toolkit version is 5.5. Yes this is a multi user node, but during this computation only one user is logged in (me). The operating system is Scientific Linux 64 bit runlevel 3. I have enabled persistence mode, nothing changed. My power suply is 1500W (2x750W). Operating temperature is ~30 deg C, during long computations ~50. I've found that newest driver for tesla is 319.82 but Quadro 410 card is not suported. Newest driver for Quadro 410 is 331.38 not supporting Tesla. Should I reinstall the driver and get rid of Quadro card?Some nvidia-smi outputs are:Driver version 319.37Performance state P0Power drawn 117.83 (during computation's)Power limit 225PCIE generation current 2 ; max 2All clock's at max. My toolkit version is 5.5. Yes this is a multi user node, but during this computation only one user is logged in (me). The operating system is Scientific Linux 64 bit runlevel 3. I have enabled persistence mode, nothing changed. My power suply is 1500W (2x750W). Operating temperature is ~30 deg C, during long computations ~50. I've found that newest driver for tesla is 319.82 but Quadro 410 card is not suported in US version (in UK it is). Newest driver for Quadro 410 is 331.38 not supporting Tesla. I'm a bit lost in that... Could that drop in performance be caused by the driver itself? Should I reinstall the driver?Have you tried running other tests to see if you get expected performance?  I recommend you log in as a registered developer and file a bug. (register if you are not already here: https://developer.nvidia.co... )  Much easier than trying to debug in these comments. :)  Alternatively, try the forums at http://devtalk.nvidia.comNice Article Mark, thank youI'm trying to figure out how to change the indexesto adapt this kernel__global__ void transposeNoBankConflicts(float *odata, const float *idata)to non-square MxN arbitrary sizecould you give me some tips please?,Regards!I did some benchmarking on my 780 Ti for big matrices and different block configurations. I transposed a 11431 by 4651 matrix. I found that having 32x32 blocks performs the fastest. Also it seems like nvcc does not perform any loop unrolling, because by manual unrolling I gained a notable performance boost. Here are my results:-32x32, naive: 4.75 ms-16x16, naive: 3.71 ms-32x8: 3,64 ms-32x8, unrolled: 3,27 ms-32x16: 2,90 ms-32x16, unrolled: 2,69 ms-32x32, ""unrolled"": 2,62 msI had to extend the above code a little to work with non-square matrices. If anybody feels too lazy, here is my final version for 32x32 blocks (compute capability 2.0 and higher):__global__ void transpose_sharedMem(float *odata, const float *idata,      const uint32 width, const uint32 height){__shared__ float tile[32][33];int x = blockIdx.x * 32 + threadIdx.x;int y = blockIdx.y * 32 + threadIdx.y;if (x < width && y < height){ tile[threadIdx.y][threadIdx.x] = idata[y*width + x]; }__syncthreads();x = blockIdx.y * 32 + threadIdx.x;  // transpose block offsety = blockIdx.x * 32 + threadIdx.y;if (y < width && x < height){ odata[y*height + x] = tile[threadIdx.x][threadIdx.y]; }}Thougt that it could not work for non-square matrices, but maybe I was wrongI am getting following output on my machine which is not as expected. Can u please tell me what difference in architecture of my machine didn't give expected resultDevice : Tesla K20Xm Matrix size: 1024 1024, Block size: 32 8, Tile size: 32 32dimGrid: 32 32 1. dimBlock: 32 8 1                  Routine         Bandwidth (GB/s)                     copy              145.23       shared memory copy               92.51          naive transpose               61.84      coalesced transpose               69.81  conflict-free transpose               69.34why shared memory copy is giving u more bandwidth as without using shared memory, all loads of threads in the warp will be coalesced and then all store will be coalesced. in fact I think register will be used for loading and storing element. Therefore copy without shared memory should perform well.I noticed similar results, so I profiled the application and noted that there is considerable variation in the duration of the kernels. So I increased the number of runs from 100 to about 10000 and now my results are closer to what is expected:Device : GM20BMatrix size: 1024 1024, Block size: 32 8, Tile size: 32 32dimGrid: 32 32 1. dimBlock: 32 8 1                  Routine         Bandwidth (GB/s)                     copy               18.54       shared memory copy               18.32          naive transpose                7.47      coalesced transpose               14.26  conflict-free transpose               16.61I just found an nvidia-document from 2009 suggesting that diagonal block reordering is indispensable due to partition camping (http://www.cs.colostate.edu....Your benchmarks seem to disconfirm that. Is it indeed no longer a concern with today's (post-1.x) cards?Correct, partition camping is not really an issue on today's GPUs. Even at the time of this post it was not a big issue, so we omitted the block reordering guidance from the post to simplify the task.Just for curiosity: Why is it no longer an issue on today's GPUs and are there some exceptional cases where partition campaign does become relevant?We built more sophisticated memory address translation hardware to avoid camping in the common cases. The incidence of partition camping is greatly reduced so it is rarely an issue. There are of course exceptional cases. One example is if many thread blocks access all in coalesced fashion but they all access the same small amount of data (maybe 128 bytes) simultaneously, so they all map to the same partition.In my personal opinion CUDA is more user-friendly and easy to integrate than FPGA's.Powered by Discourse, best viewed with JavaScript enabled"
3594,gtc-2020-what-the-profiler-is-telling-you-how-to-get-the-most-performance-out-of-your-hardware,"GTC 2020 S22141
Presenters: Markus Hrywniak,NVIDIA; Milos Maric,NVIDIA
Abstract
We’ll explore how to analyze and optimize the performance of GPU-accelerated applications. Working with a real-world example, we’ll start by identifying high-level bottlenecks, then walk through an analysis-driven process leading to a series of kernel-level optimizations. Using NVIDIA’s Nsight Systems and Nsight Compute profiling tools as an example, you’ll learn about the fundamental performance limiters: instruction throughput, memory throughput, and latency. We’ll present strategies to identify and tackle each type of limiter.Watch this session
Join in the conversation below.Hi Markus, thanks for the talk, I learned a lot and the new tools look very impressive.I have a more general question regarding profiling and optimization.  My question is to what extent can the developer control the actual execution of the overlapping of different streams?  In other words, how much of the kernel overlapping is determined by the developer versus the scheduler?I have a multi-GPU code and I use streams to overlap computation with communication where possible.  I have found that if I write equivalent functions with different ordering (although the inter-stream dependencies are the same) I get slightly different overlap and different performance (as indicated by NVVP).  For example, one version of the code completely overlaps the communication while others have it go slightly beyond the kernel it was overlapping.  The timings are slightly different between each other although within 1-5% of each other.  For multi-hour runs, though, 1-5% can be a lot of compute time.Since there are many possible variations of the same code (e.g. I can use 2 streams to indicate dependence or more streams combined with events) what is the best approach to guiding the scheduler to produce the best timeline possible?  If possible, are there any literature sources I can read on this topic?Thanks.Hi Todd, glad you liked the talk. I think you already listed the main options you have. One other thing you could try if it makes sense for your application is setting stream priority: Programming Guide :: CUDA Toolkit DocumentationIn any case, using multiple streams and events to signal dependencies is the correct way to model things. The scheduling at runtime depends on the work that has been submitted to the GPU. The block scheduler is free to execute any active block on any SM and there is no programmatic way to influence this further.Your case might be more subtle though. If you’re seeing timing differences that don’t make sense, you could post another thread in the CUDA subforum (CUDA Programming and Performance - NVIDIA Developer Forums) with some profiling data attached.Thanks Markus.  I will look into priorities first to see if that helps with my scheduling.  If I am still having a good amount of variability I’ll post the issue on the forums.Thanks again.Powered by Discourse, best viewed with JavaScript enabled"
3595,gpu-pro-tip-track-mpi-calls-in-the-nvidia-visual-profiler,"Originally published at:			https://developer.nvidia.com/blog/gpu-pro-tip-track-mpi-calls-nvidia-visual-profiler/
Often when profiling GPU-accelerated applications that run on clusters, one needs to visualize MPI (Message Passing Interface) calls on the GPU timeline in the profiler. While tools like Vampir and Tau will allow programmers to see a big picture view of how a parallel application performs, sometimes all you need is a look at how MPI…In the example code, the include statement is missing the include file name.  Can you tell us what the actual filename is?I've fixed this.  Thanks!Powered by Discourse, best viewed with JavaScript enabled"
3596,ai-enables-markerless-animal-tracking,"Originally published at:			https://developer.nvidia.com/blog/ai-enables-markerless-animal-tracking/
Researchers from Harvard University along with other collaborators in academia developed a deep learning-based method called DeepLabCut to automatically track and label body parts of moving species with human-like accuracy. “Videography provides easy methods for the observation and recording of animal behavior in diverse settings, yet extracting particular aspects of a behavior for further analysis…Powered by Discourse, best viewed with JavaScript enabled"
3597,announcing-nsight-deep-learning-designer-2021-1-sdk-for-efficient-deep-learning-model-design-and-development,"Originally published at:			https://developer.nvidia.com/blog/announcing-nsight-deep-learning-designer-2021-1-sdk-for-efficient-deep-learning-model-design-and-development/
NVIDIA announces Nsight DL Designer - the first in-class integrated development environment to support efficient design of deep neural networks for in-app inference.Powered by Discourse, best viewed with JavaScript enabled"
3598,ai-app-suggests-recipes-based-on-food-pictures,"Originally published at:			AI App Suggests Recipes Based on Food Pictures | NVIDIA Technical Blog
MIT researchers developed a deep learning system that can compile a list of ingredients and suggest similar recipes by looking at a photo of food. “In computer vision, food is mostly neglected because we don’t have the large-scale datasets needed to make predictions,” says Yusuf Aytar, an MIT postdoc who co-wrote the paper about the…Powered by Discourse, best viewed with JavaScript enabled"
3599,gtc-2020-using-cuda-tensorrt-and-driveworks-on-nvidia-drive-agx,"GTC 2020 CWE21185
Presenters: Aaraadhya Narra,NVIDIA; Josh Park, ; Anurag Dixit, ; Yu-Te Cheng,
Abstract
Autonomous vehicles need fast, accurate perception to perform safely. NVIDIA CUDA software and TensorRT on DRIVE AGX can accelerate massive computation workloads in parallel and optimize DNN inference. Hear from our Developer Zone Forum experts on how to leverage these blocks, along with DriveWorks, and understand how to deal with safety when integrating CUDA into mission critical software.Watch this session
Join in the conversation below.Powered by Discourse, best viewed with JavaScript enabled"
3600,optimizing-applications-for-nvidia-ampere-gpu-architecture,"GTC 2020 S21819
Presenters: Guillaume Thomas-Collignon, NVIDIA; Vishal Mehta, NVIDIA
Abstract
We’ll take a deep dive into NVIDIA Ampere GPU Architecture, with practical code examples on how to optimize your application for NVIDIA A100 GPU.Watch this session
Join in the conversation below.Powered by Discourse, best viewed with JavaScript enabled"
3601,cusolvermp-v0-0-1-now-available-through-early-access,"Originally published at:			https://developer.nvidia.com/blog/cusolvermp-v0-0-1-now-available-through-early-access/
cuSOLVERMp provides a distributed-memory multi-node and multi-GPU solution for solving systems of linear equations at scale! In the future, it will also solve eigenvalue and singular value problems.Powered by Discourse, best viewed with JavaScript enabled"
3602,benchmarking-deep-neural-networks-for-low-latency-trading-and-rapid-backtesting-on-nvidia-gpus,"Originally published at:			https://developer.nvidia.com/blog/benchmarking-deep-neural-networks-for-low-latency-trading-and-rapid-backtesting-on-nvidia-gpus/
NVIDIA GPUs enable electronic trading applications to run inference in real time on very large LSTM models serving some of today’s fastest-moving markets.Thanks for the post; just a question. Would you mind refrencing the result for the same task on FPGA/ASIC?Other submissions for the STAC ML Inference Benchmarks can be found on the STAC website. For instance,Powered by Discourse, best viewed with JavaScript enabled"
3603,gtc-digital-learn-how-bmw-uses-nvidia-technology-to-develop-the-ultimate-industrial-autonomous-robot,"Originally published at:			https://developer.nvidia.com/blog/bmw-gtc-digital-2020/
At GTC Digital 2020, BMW Group researchers shared behind-the-scenes insights about how the company’s autonomous guided vehicle, Smart Transport Robot (STR), navigates indoors in industrial environments, using the NVIDIA ISAAC framework. “BMW Group strives for the highest levels of innovation extending beyond car production,” the company said. “With NVIDIA, we’re developing our own robotics pipeline…Powered by Discourse, best viewed with JavaScript enabled"
3604,upcoming-webinars-learn-how-to-use-nvidia-ngc-jupyter-notebook,"Originally published at:			https://developer.nvidia.com/blog/upcoming-webinars-learn-how-to-use-nvidia-ngc-jupyter-notebook/
Image segmentation and recommender system Jupyter notebooks are now available in the NGC catalog. These Jupyter notebooks come with complete instructions on how to train these models using the resources from the NGC catalog.  Upcoming Webinars The NVIDIA NGC team is hosting two webinars with live Q&A to dive into two new Jupyter notebooks available…Powered by Discourse, best viewed with JavaScript enabled"
3605,clara-train-3-1-brings-secure-enterprise-grade-federated-learning-to-developers,"Originally published at:			https://developer.nvidia.com/blog/clara-train-3-1-brings-secure-enterprise-grade-federated-learning-to-developers/
NVIDIA recently released Clara Train 3.1 for healthcare developers to collaborate on secure, enterprise-grade AI models.Powered by Discourse, best viewed with JavaScript enabled"
3606,how-to-scale-up-your-deep-learning-at-gtc,"Originally published at:			How to Scale Up Your Deep Learning at GTC | NVIDIA Technical Blog
Whether you are using GPU clusters in the cloud or using your own data center to train deep neural networks, leading deep learning frameworks rely on NVIDIA’s Deep Learning SDK libraries to accelerate the massive amount of computation required to achieve high accuracy with deep learning. NVIDIA NCCL uses automatic topology detection and intelligent communication…Powered by Discourse, best viewed with JavaScript enabled"
3607,getting-a-real-time-factor-over-60-for-text-to-speech-services-using-nvidia-jarvis,"Originally published at:			https://developer.nvidia.com/blog/getting-real-time-factor-over-60-for-text-to-speech-using-jarvis/
Figure 1. The Jarvis Server and the TTS pipeline. NVIDIA Jarvis is an application framework that provides several pipelines for accomplishing conversational AI tasks. Generating high-quality, natural-sounding speech from text with low latency, also known as text-to-speech (TTS), can be one of the most computationally challenging of those tasks. In this post, we focus on…Powered by Discourse, best viewed with JavaScript enabled"
3608,increasing-throughput-and-reducing-costs-for-ai-based-computer-vision-with-cv-cuda,"Originally published at:			https://developer.nvidia.com/blog/increasing-throughput-and-reducing-costs-for-computer-vision-with-cv-cuda-2/
Real-time cloud-scale applications that involve AI-based computer vision are growing rapidly. The use cases include image understanding, content creation, content moderation, mapping, recommender systems, and video conferencing. However, the compute cost of these workloads is growing too, driven by demand for increased sophistication in the processing. The shift from still images to video is also…How faster is it compared to equivalent OpenCV operations compiled with GPU support?Sorry for the slow response.  We are working on our performance benchmarking and it will be available for a future release.  Currently, we have some performance published: CV-CUDA | NVIDIA Developer.All I see is GPU comparison which is not enough for making a switch from OpenCV. Please update this topic when you get some comparison benchmarks with other frameworks.Powered by Discourse, best viewed with JavaScript enabled"
3609,ai-helps-autonomous-vehicles-locate-themselves,"Originally published at:			https://developer.nvidia.com/blog/ai-helps-autonomous-vehicles-locate-themselves/
Researchers at the NYU Tandon School of Engineering are developing an artificial intelligence system for autonomous vehicles that links them to HERE Live Map cloud-based mapping system. “Essentially, we want to be able to precisely match what the car sees with what’s in the cloud database. An incredibly precise ruler isn’t of much use if…Powered by Discourse, best viewed with JavaScript enabled"
3610,nvidia-founder-and-ceo-jensen-huang-to-speak-at-supercomputing-2018,"Originally published at:			NVIDIA Founder and CEO Jensen Huang to Speak at Supercomputing 2018 | NVIDIA Technical Blog
At Supercomputing 2018 (SC18) in Dallas, Texas, NVIDIA Founder and CEO Jensen Huang will deliver a special address about the latest innovations in GPU-accelerated supercomputing. The special address will be held at the Hyatt Regency Dallas, Landmark Ballroom C on November 12, 2018 from 3:00 PM – 5:00 PM CT. /> Space is limited, so…Powered by Discourse, best viewed with JavaScript enabled"
3611,in-the-trenches-at-gtc-cuda-5-and-beyond,"Originally published at:			https://developer.nvidia.com/blog/trenches-gtc-cuda-5-and-beyond/
By Michael Wang, The University Of Melbourne, Australia (GTC 2012 Guest Blogger) Following up the opening keynote by NVIDIA CEO and co-founder Jen Hsun-Huang, Mark Harris took the very same stage (albeit with a more intimate crowd) for his afternoon session entitled CUDA 5 And Beyond. Mark walked us through the major features of the upcoming…Powered by Discourse, best viewed with JavaScript enabled"
3612,predicting-protein-structures-with-deep-learning,"Originally published at:			https://developer.nvidia.com/blog/predicting-protein-structures-with-deep-learning/
Solving a mystery that stumped scientists for decades, last November a group of computational biologists from Alphabet’s DeepMind used AI to predict a protein’s structure from its amino acid sequence.  Not even a year later, a new study offers a more powerful model, capable of computing protein structures in as little as 10 minutes, on…Powered by Discourse, best viewed with JavaScript enabled"
3613,six-new-releases-to-advance-game-development,"Originally published at:			https://developer.nvidia.com/blog/six-new-releases-to-advance-game-development/
Over the past two weeks, NVIDIA has released software to enable game development teams of all sizes. Below is a list of the releases, and how you can get started today. NVIDIA Reflex SDK Available to apply for early access, the NVIDIA Reflex SDK allows game developers to implement a low latency mode that aligns game…Powered by Discourse, best viewed with JavaScript enabled"
3614,training-and-optimizing-a-2d-pose-estimation-model-with-the-nvidia-transfer-learning-toolkit-part-2,"Originally published at:			https://developer.nvidia.com/blog/training-optimizing-2d-pose-estimation-model-with-tlt-part-2/
The first post in this series covered how to train a 2D pose estimation model using an open-source COCO dataset with the BodyPoseNet app in the NVIDIA Transfer Learning Toolkit. In this post, you learn how to optimize the pose estimation model in the NVIDIA Transfer Learning Toolkit. It walks you through the steps of…Hi,I have a hard time following the document… sorry.
I just bought a AGX Orin
And I am using a Linux Desktop with RTX 2080 running the pbnet in jupyter notebook. (Already 2 days, still in progress)
I wonder how to use the model and run in my Orin?
Is there any step-by-step tutorial or video link?
BTW, I have been struggling for week already…really tired.ThxPowered by Discourse, best viewed with JavaScript enabled"
3615,gtc-2020-nvidia-clara-deploy-application-framework-powers-smart-hospitals,"GTC 2020 D2S11
Presenters: Tech Demo Team,NVIDIA
Abstract
Hospitals are moving to edge solutions for real-time processing and to maintain patient data privacy. This comes at a time when hundreds of AI algorithms are being deployed in smart hospitals for imaging, genomics, and video batch processing.
In this demo, to help address some of these critical shifts, we’ll highlight the latest features of the NVIDIA® Clara™ Deploy Application Framework. Clara Deploy enables best practices for cloud-native software development and helps significantly reduce technical debt and development time. We’ll showcase how developers can use Clara Deploy to build their applications and solutions on a common foundation—making production seamless for IT and DevOps. We’ll also highlight a key feature for multi-AI processing—now shipping with NVIDIA Clara Deploy—that allows users to develop more complicated workflows involving multiple AI models for deployment in a single pipeline. Additionally, we’ll introduce priority queuing and scheduling capabilities that prioritize the most critical jobs, guaranteeing quality of service.Watch this session
Join in the conversation below.Powered by Discourse, best viewed with JavaScript enabled"
3616,cuda-pro-tip-use-cufft-callbacks-for-custom-data-processing,"Originally published at:			https://developer.nvidia.com/blog/cuda-pro-tip-use-cufft-callbacks-custom-data-processing/
Digital signal processing (DSP) applications commonly transform input data before performing an FFT, or transform output data afterwards. For example, if the input data is supplied as low-resolution samples from an 8-bit analog-to-digital (A/D) converter, the samples may first have to be expanded into 32-bit floating point numbers before the FFT and the rest of…Thanks for posting this. My only question is what kind of operation you're doing with the data by just multiplying the samples? Assuming this is a continuous stream of input data, wouldn't you need to either do overlap-add or overlap-save with adjacent blocks of samples to produce a meaningful output?In this somewhat simplified example I use the multiplication as a general convolution operation for illustrative purposes. You are right that if we are dealing with a continuous input stream we probably want to do overlap-add or overlap-save between the segments--both of which have the multiplication at its core, however, and mostly differ by the way you split and recombine the signal. For other kinds of input data, such as correlating samples from multiple sources or processing image data, the operations you want to perform may look different.Hello Christoph,I want to ask you if the CUFFT callbacks will become part of the CUDA FFT shared library. Linking with the static library is a little problematic, for some of us using CMake.  Also, I heard that in CUDA 7.0 the license is not longer necessary. Can you confirm?Thanks for the great tutorial.The license is not longer required in CUDA 7.0. Adding callbacks support in the shared library is a lot of work, which we may do in the future, but we would need to justify the effort with user benefits. So can you explain specifically why using the static library is problematic with CMake?Hello Mark,I'm not very good at CMake. Still I found it more convenient than writing Makefiles. Using CMake project files to build CUDA code is usually straight forward, but the static library requires separable compilation and that I haven't succeed using CMake.My point is that if you already have a project that uses the CUFFT shared library, it would be easier to start using CUFFT callbacks if you can avoid trying to figure out why the CMake project file is no longer working.I wrote a toy example, of what I'm trying to attempt, and I have share the code in github [1]. I can compile and run such example using the command line.nvcc -arch=sm_35 -rdc=true -c src/thrust_fft_example.cunvcc -arch=sm_35 -dlink -o thrust_fft_example_link.o thrust_fft_example.o -lcudart -lcufft_staticg++ thrust_fft_example.o thrust_fft_example_link.o -L/usr/local/cuda-6.5/lib64 -lcudart -lcufft_static -lculibosI tried to compile the same example using CMake and I got stuck at the linking step. CMake translate the above three lines into:/usr/local/cuda-6.5/bin/nvcc /home/workspace/thrust_fft/src/thrust_fft_example.cu -dc -o /home/workspace/thrust_fft/build/CMakeFiles/cuda_compile.dir/src/./cuda_compile_generated_thrust_fft_example.cu.o -ccbin /usr/bin/gcc-4.6 -m64 -Xcompiler ,\""-g\"",\""-L/usr/local/cuda-6.5/lib64\"",\""-g\"" -gencode=arch=compute_35,code=sm_35 -DNVCC -I/usr/local/cuda-6.5/include/usr/local/cuda-6.5/bin/nvcc -gencode=arch=compute_35,code=sm_35 -m64 -ccbin ""/usr/bin/gcc-4.6"" -dlink /home/workspace/thrust_fft/build/CMakeFiles/cuda_compile.dir/src/./cuda_compile_generated_thrust_fft_example.cu.o -o /home/workspace/thrust_fft/build/CMakeFiles/thrust_fft.dir/./thrust_fft_intermediate_link.o/usr/bin/c++   -g -L/usr/local/cuda-6.5/lib64   CMakeFiles/cuda_compile.dir/src/cuda_compile_generated_thrust_fft_example.cu.o CMakeFiles/thrust_fft.dir/thrust_fft_intermediate_link.o  -o thrust_fft -rdynamic /usr/local/cuda-6.5/lib64/libcudart.so /usr/local/cuda-6.5/lib64/libcufft_static.a /usr/local/cuda-6.5/lib64/libculibos.a -Wl,-rpath,/usr/local/cuda-6.5/lib64:Those extra compiler flags ""-g -L/usr/local/cuda-6.5/lib64"" are there because the FinCUDA.cmake needs the variable CMAKE_CXX_FLAGS to be defined in order to work. That's also where I got stuck. Hopefully you guys can spot what I'm doing wrong.What is culibos? What happens when a CUFFT plan is employed to compute DIRECT and INVERSE FFT? Can you selectively tell when you want the callbacks to be used?The last question: What is the advantage of using CUFFT callbacks vs. Thrust callbacks?[1] Thrust and CUFFT callback example github repository.https://github.com/ovalerio...EDIT 1: After posting this message I found that the second line which is automatically generated by CMake is missing the -lcudart and -lcufft_static flags. Adding those manually will make it succeed. The reason why they got removed is still not clear. I will ask in the CMake list. Still I hope you could answer my other questions. Thanks.I'm curious-  is something similar possible with cuBLAS functions?  I often need to compute a matrix-matrix multiplication followed by an elementwise operation.  The trips to global memory are my main bottleneck.I tried to implement this example on a GTX 750. I was successful until I added callbacks. I am using Nsight Eclipse to develop as supplied with CUDA 7.0. I tried modifying the generated makefiles to include the compile/link flags above. Comparing the output (I wrote the output to disk and plotted it) to the non-callback version and the callback version leads me to believe that the callback is not being applied.I also tried compiling with the flags listed in the article. I get errors about SM Arch not found. (Nsight automatically compiles the code for me for my target architecture.)Is there a way to get this working with Nsight by default?The trick is to go Project->Properties->Build->Settings>Device linker mode: Separate compilation.Follow-up question:  is --relocatable-device-code=true is equivalent to -dc ?Almost: ""-dc"" is equivalent to ""--relocatable-device-code true -c"" or ""-rdc=true -c"". Note the ""-c"" to generate an object file. See the nvcc manual:http://docs.nvidia.com/cuda...I've tried out the source from github on a Jetson TX1. Without changing anything in the source, I executed the two applications a lot of times one after the other and I've observed the following: the no_callback one finishes at ~260-280ms each run but the callback one performs very differently each time (finishing at random values between ~170-260ms, also occasionally at higher values, max I've seen is ~600ms).I've also tried running only the callback version one after another, produces the same result. Do you have an idea what might cause this sort of behavior?For small workload sizes it's not uncommon to see runtime variance. I suggest you run multiple times and average.Hi Mark,Thanks for the answer. As I've said in the post I've used exactly the same code from github which does a 1000x1024 operation with 100 iterations. I've also ran *this* code (1000x1024, 100 iter) multiple times (~50 times) and took averages.No_callback -> in the 260-280ms range (avg ~= 270ms)Callback -> in the 170-600ms range (avg ~= 350ms)Do you have an idea what might cause this sort of behavior?Thanks,BurakI know this is an old post, but I think this is still an important topic.I think this example undersells the potential throughput improvements from using callbacks. I believe that the transpose step in the store callback prevents the store operations from being coalesced, greatly reducing potential throughput, especially with larger transform sizes. In my testing, removing the transpose stage (which isn't necessary for many frequency domain filtering applications anyway) resulted in halving the run-time vs the non-callback case. In other words, the callback case is twice as fast.There's also a mistake in your store callback. You are computing the product of ""filter"" and ""element"" and storing that in ""value"", but then you write out ""element"" to dataOut instead of the product stored in ""value"". I haven't looked at the assembly, but I would expect the compiler to optimize out the complex multiply and the read of filter because of that.Thanks for your comment, Brett. You are right, if you don't require the transpose you can get better coalescing by not transposing, resulting in higher performance. The application that was underlying our experiments expected the data to be transposed, which is why we included it in our code. The exact performance of cuFFT callbacks depends on the CUDA version and GPU you are using and both have changed significantly over the last years. On my V100 I am measuring a ~30%-35% performance increase by not transposing, which is a bit less than the 2x you experienced but still very significant. Thank you for this suggestion!Regarding the mistake in the store callback: Thanks for the catch. Luckily this is only a bug in the blog post. The code on github does the store in one statement and doesn't use a temporary value variable. We will fix the text.I know this post is old but I just implemented a small example which turns out to be a lot slower. As a test I am trying to replace the following Kernel:extern ""C""__global__ void mul_Real(int size, float val, float* outVol){const unsigned int x = blockIdx.x * blockDim.x + threadIdx.x;const unsigned int y = blockIdx.y * blockDim.y + threadIdx.y;const unsigned int z = blockIdx.z * blockDim.z + threadIdx.z;outVol[z * size * size + y * size + x] = outVol[z * size * size + y * size + x] * val;}with the following Callback:__device__ void CB_TEST(void *dataOut, size_t offset, cufftReal element, void *callerInfo, void *sharedPtr) {    float *filter = (float*)callerInfo;    ((cufftReal*)dataOut)[offset] = element*filter[0];}__managed__ cufftCallbackStoreR d_storeCallbackPtr = CB_TEST;The execution of the FFT is called like this:cufftPlanMany(&ffthandle, 3, {64,64,64}, NULL, 0, 0, NULL, 0, 0, CUFFT_C2R, 1);I would expect the that (since I don't have to touch the Real Result twice) I reduce the run time by a small factor but the duration actually gets worse. FFT+Kernel -> 0.015936msFFT+CB -> 0.020512msI am thankful for any suggestions.Thanks for your question, Alexander. cuFFT callbacks use device-side function calls. If the compiler has control over both the device function and the call site, it can often inline __device__ functions, resulting in zero overhead. However, for cuFFT callbacks the compiler does not have control over the call site, which lives inside the cuFFT library. Instead, as described in the blog post, you need to compile your callback as relocatable device code. The lack of inlining can incur some small, but non-zero overhead for each call (see Separate Compilation and Linking of CUDA C++ Device Code). Your callback function is doing relatively little work, only multiplying each element by a constant value, which may just not be enough to compensate for the call overhead. In such cases, running a separate kernel can sometimes result in better overall performance.For your code, you can try the following two things and check if you see an improvement:o Pass the filter through a __constant__ or __device__ variable declared outside of the callback function - this would be more efficient than passing through callerInfo. o __managed__ memory for storing callback functions may introduce some one-time setup cost. When measuring your execution time make sure you run the kernel multiple times and discard the first run. If there is measurable setup cost for __managed__ and that is a problem for your application, you can get rid of __managed__ and do it the traditional way.If this doesn't work I suggest you keep the separate kernel (and maybe try callbacks again in future cuFFT versions as there are regular improvements to the performance).After compiling the code, I got verification failed for both of them. I just started to try to understand why. If someone have any tips on that I would really appreciate to hear them.Powered by Discourse, best viewed with JavaScript enabled"
3617,share-your-science-accelerating-aircraft-design-on-the-cloud,"Originally published at:			https://developer.nvidia.com/blog/share-your-science-accelerating-aircraft-design-on-the-cloud/
Cyril Chamenois, co-founder and COO of Elixir Aircraft, shares how they are using NVIDIA technologies to develop the industry’s first aircraft designed using cloud-based applications. Chameonis mentions Elixir is a small start-up with limited resources, and they are taking advantage of using the best-in-class tools on the cloud instead of investing on a large IT…Powered by Discourse, best viewed with JavaScript enabled"
3618,gtc-2020-paddlepaddle-with-distributed-training-api-automatic-mixed-precision-and-tensorrt-integration,"GTC 2020 S21436
Presenters: Bai-Cheng Jeng,NVIDIA; Jie Fang,NVIDIA; Daming Lu,Baidu USA
Abstract
We’ll introduce PaddlePaddle (PArallel Distributed Deep LEarning), an easy-to-use, efficient, flexible, and scalable deep-learning platform, which has already deployed to real business scenarios. In training phase, PaddlePaddle provides a high-level API for distributed training named Fleet. It can distribute a training task to GPU cluster. To further increase performance, PaddlePaddle can use mixed precision training through adding few lines of code, which achieves significant speedup by Tensor Core. In inference phase, PaddlePaddle integrates TensorRT to fuse operations, and can infer models in lower precision mode to fully utilize GPU resources. With all the three technologies mentioned above, developers can significantly cut the needed time to train large-scale tasks and deploy models for operation.Watch this session
Join in the conversation below.Powered by Discourse, best viewed with JavaScript enabled"
3619,tensorflow-2-0-with-tighter-tensorrt-integration-now-available,"Originally published at:			TensorFlow 2.0 with Tighter TensorRT Integration Now Available | NVIDIA Technical Blog
To help developers build scalable ML-powered applications, Google released TensorFlow 2.0, one of the core open source libraries for training deep learning models. In this release, developers will see up to 3x faster training performance using mixed precision on NVIDIA Volta and Turing GPUs.  TensorFlow 2.0 features tighter integration with TensorRT, NVIDIA’s high-performance deep learning…Powered by Discourse, best viewed with JavaScript enabled"
3620,drive-labs-helping-cameras-see-clearly-with-ai,"Originally published at:			https://developer.nvidia.com/blog/drive-labs-helping-cameras-see-clearly-with-ai/
We developed ClearSightNet, a deep neural network (DNN) trained to evaluate cameras’ ability to see clearly and help determine root causes of occlusions, blockages, and reductions in visibility.Powered by Discourse, best viewed with JavaScript enabled"
3621,a-nvidia-driver-installing-error-with-amd-nvidia-tu116m-geforce-gtx-1660-ti-mobile,"Hi friends!
l would like to install driver for my Ubuntu20.04/win10 system.[win10 is work and failed in Ubuntu]
my kernel=5.9.1
l tried a lot of solution and all failed.
now my status is.
l decide the version of driver byubuntu-drivers devices
driver   : nvidia-driver-455 - third-party free recommendedso l install bysudo apt-get install nvidia-kernel-source-455
sudo apt-get install nvidia-driver-455i noticed something maybe wrong with during installNot creating home directory `/nonexistent’. No such file or directorythen l want to test the driver bynvidia-smi

nvidia_smi845×384 31.5 KB
nvidia-setting

nvidia-setting1234×387 58.5 KB

X-nvidia-setting676×531 9.11 KB
then l follow the topic of nvidia-xconfig doesnt do what i want it to, nor does nvidia-settings - NVIDIA Developer Forums 6110-amdgpu.conf
Section “OutputClass”
Identifier “AMDgpu”
MatchDriver “amdgpu”
Driver “modesetting”
EndSection10-nvidia.conf
Section “OutputClass”
Identifier “nvidia”
MatchDriver “nvidia-drm”
Driver “nvidia”
Option “AllowEmptyInitialConfiguration”
Option “PrimaryGPU” “yes”
ModulePath “/usr/lib/x86_64-linux-gnu/nvidia/xorg”
EndSectioncreate two files optimus.desktop in /etc/xdg/autostart/ and /usr/share/gdm/greeter/autostart/
l never create the /etc/X11/xorg.confthen l code__NV_PRIME_RENDER_OFFLOAD=1 __GLX_VENDOR_LIBRARY_NAME=nvidia glxinfo | grep vendor
return
X Error of failed request:  BadValue (integer parameter out of range for operation)
Major opcode of failed request:  152 (GLX)
Minor opcode of failed request:  24 (X_GLXCreateNewContext)
Value in failed request:  0x0
Serial number of failed request:  39
Current serial number in output stream:  40l have no idea how to continue
l generate the report
nvidia-bug-report.log.log (321.1 KB)please help me
thank U very much
if anyone has any solution please help me
@generixPowered by Discourse, best viewed with JavaScript enabled"
3622,share-your-science-the-impact-of-deep-learning-on-radiology,"Originally published at:			Share Your Science: The Impact of Deep Learning on Radiology | NVIDIA Technical Blog
Ronald Summers, Senior Investigator at the National Institutes of Health (NIH) shares how they are trying to improve patient care by increasing the accuracy of radiologic diagnosis with advanced computer techniques. His group is using deep learning and NVIDIA GPUs to assist physicians make a more accurate diagnosis by developing software that improves diagnosis, reduce…Powered by Discourse, best viewed with JavaScript enabled"
3623,photo-editing-with-generative-adversarial-networks-part-1,"Originally published at:			https://developer.nvidia.com/blog/photo-editing-generative-adversarial-networks-1/
Adversarial training (also called GAN for Generative Adversarial Networks), and the variations that are now being proposed, is the most interesting idea in the last 10 years in ML, in my opinion. – Yann LeCun, 2016 [1]. You heard it from the Deep Learning guru: Generative Adversarial Networks [2] are a very hot topic in…A nicely written article, thank you for the efforts.I agree with you!awesome work! Thanks for writing the auto-encoder to train images to generate the corresponding z values. I am excited to dig into this. Can this can be run through nvidia-docker yet?The dropbox link used to download the celebA data set has been temporarily disabled due to high traffic. Is there an alternative place to download the data from?Hello, the main page for the CelebA dataset (http://mmlab.ie.cuhk.edu.hk... mentions an alternative Baidu drive but that also seems to be down, unfortunately. You might want to get in touch with the authors? Sorry about that!I see you took the initiative of creating an issue on Github, thanks! For the record: https://github.com/gheinric...I made a basic walk through of how to get a gcp instance running, installing nvidia-docker, and simple setup to run the same example above with nvidia-docker. They are just personal notes, but I figured I would share if anyone was a total newb like me.Thanks to @enthusiasto:disqus for the Dockerfile!https://ljstrnadiii.github.io/Great illustration. It occurred to me that the sample space from which the forger obtains objects is finite. This implies that in all scenario the eventual outcome is that the Expert will beat the forger, will get to a point of perfection (a convergence to 100% accuracy). Is that right?Greg, would you mind sharing how you created your network architecture pictures?Hello, I used https://github.com/gwding/d...Hello, the z-space has a finite cardinality but the possibilities are infinite (ignoring floating-point quantization). Besides, the Expert usually does not have enough capacity (enough neurons) to memorize every single image.candicegerstner yesPowered by Discourse, best viewed with JavaScript enabled"
3624,cuda-spotlight-michela-taufer-on-gpu-accelerated-scientific-computing,"Originally published at:			https://developer.nvidia.com/blog/cuda-spotlight-michela-taufer-gpu-accelerated-scientific-computing/
Our Spotlight is on Dr. Michela Taufer, Associate Professor at the University of Delaware. Michela heads the Global Computing Lab (GCLab), which focuses on high performance computing (HPC) and its application to the sciences. Her research interests include software applications and their advanced programmability in heterogeneous computing (i.e., multi-core platforms and GPUs); cloud computing and…Powered by Discourse, best viewed with JavaScript enabled"
3625,accelerating-cloud-networking-the-right-way,"Originally published at:			https://developer.nvidia.com/blog/accelerating-cloud-networking-the-right-way/
NVIDIA delivers industry-leading SDN performance benchmark resultsPowered by Discourse, best viewed with JavaScript enabled"
3626,new-speech-synthesis-system-can-imitate-thousands-of-accents,"Originally published at:			New Speech Synthesis System Can Imitate Thousands of Accents | NVIDIA Technical Blog
Baidu announced their latest production-quality speech synthesis system that can imitate thousands of human voices from people across the globe. Deep Voice 1 focused on being the first real-time text-to-speech system and Deep Voice 2, with substantial improvements on Deep Voice 1, had the ability to reproduce several hundred voices using the same system. “Deep…Powered by Discourse, best viewed with JavaScript enabled"
3627,fighting-covid-19-with-deep-learning,"Originally published at:			https://developer.nvidia.com/blog/fighting-covid-19-with-deep-learning/
Researchers develop an AI model that predicts synergistic drug combinations for optimal COVID-19 treatment.Powered by Discourse, best viewed with JavaScript enabled"
3628,long-read-sequencing-workflows-and-higher-throughputs-in-nvidia-parabricks-4-1,"Originally published at:			https://developer.nvidia.com/blog/long-read-sequencing-workflows-and-higher-throughputs-in-nvidia-parabricks-4-1/
The upcoming 4.1 release of NVIDIA Parabricks, a suite of accelerated genomic analysis applications, goes further than ever before in accelerating sequencing alignment and increasing the accuracy of deep learning variant calling. The release includes a new workflow for PacBio long-read data, featuring an accelerated Minimap2 tool and Google’s DeepVariant for full GPU-enabled, end-to-end analysis…Powered by Discourse, best viewed with JavaScript enabled"
3629,deterring-ants-with-gpus-and-deep-learning,"Originally published at:			Deterring Ants with GPUs and Deep Learning | NVIDIA Technical Blog
Robert Bond, an NVIDIA engineer, created an “ant annoyer” using deep learning and a Jetson TK1 developer kit to pinpoint ants scuttling across his kitchen floor and target them with a 5 milliwatt laser beam. Robert mentioned the project was not even practical before he got his hands on the  Jetson board, which puts unprecedented…Powered by Discourse, best viewed with JavaScript enabled"
3630,in-the-trenches-at-gtc-swift-a-gpu-based-smith-waterman-sequence-alignment-program,"Originally published at:			https://developer.nvidia.com/blog/trenches-gtc-swift-gpu-based-smith-waterman-sequence-alignment-program/
By Jike Chong, Parasians (GTC 2012 Guest Blogger) This week at GTC, Pankaj Gupta, a bioinformatics application developer at St. Jude Children’s Research Hospital, presented his work in the area of sequence alignment. Sequence alignment is an important component of bioinformatics that is crucial for the vision of personalized medicine. Sequence alignment matches new sequences…Powered by Discourse, best viewed with JavaScript enabled"
3631,long-term-support-for-nvidia-doca-1-5,"Originally published at:			https://developer.nvidia.com/blog/long-term-support-for-nvidia-doca-1-5/
Today, NVIDIA announced the long-term support (LTS) release of NVIDIA DOCA 1.5. NVIDIA DOCA is the open cloud SDK and acceleration framework for NVIDIA BlueField DPUs. It unlocks data center innovation by enabling you to rapidly create applications and services for BlueField DPUs by using industry-standard APIs. The new NVIDIA DOCA 1.5 release includes several…LTS shows NVIDIAs commitment to build the BlueFied DPU and DOCA ecosystem!Powered by Discourse, best viewed with JavaScript enabled"
3632,calibrating-stitched-videos-with-vrworks-360-video-sdk,"Originally published at:			Calibrating Stitched Videos with VRWorks 360 Video SDK | NVIDIA Technical Blog
There are over one million VR headsets in use this year, and the popularity of 360 video is growing fast. From YouTube to Facebook, most social media platforms support 360 video and there are many cameras on the market that simplify capturing these videos. You can see an example still image in Figure 1 and…Is there any libraries to stitch videos on the Jetson TX2 ? The NVIDIA VRWorks 360 video only works on Windows.I also would like to know if there is a VRWorks 360 SDK version planned for TX2.Would there be any way to use this stitching SDK to make 180 degree video rather than 360?Hi, i'm interested in the 180 degree version, is there an update for this?Hi can you please post a tutorial as how to install the SDK and start using it for video stitching , any help will be appriciated.yes you are right video will be more helpful.Hello, VRWorks 360 Video SDK 2.1 for windows version can not download now.
Would you mind to send me download link address?
Thanks.@spongelinyi  – Sorry about the delay! I tried to track down a better link for you but the VRWorks 360 Video SDK is no longer available. I’ve added a note to that effect at the top of the post.Powered by Discourse, best viewed with JavaScript enabled"
3633,nvidia-ceo-jensen-huang-to-host-ai-pioneers-yoshua-bengio-geoffrey-hinton-and-yann-lecun-and-others-at-gtc,"Originally published at:			NVIDIA CEO Jensen Huang to Host AI Pioneers Yoshua Bengio, Geoffrey Hinton and Yann LeCun, and Others, at GTC21 | NVIDIA Newsroom
Powered by Discourse, best viewed with JavaScript enabled"
3634,automotive-top-5-resources-from-gtc-21,"Originally published at:			https://developer.nvidia.com/blog/automotive-top-5-resources-from-gtc-21/
The annual DRIVE Developer Days was held during GTC 2021, featuring a series of specialized sessions on AV development led by NVIDIA experts. Learn about perception, mapping, simulation and more anytime with NVIDIA On-Demand.Powered by Discourse, best viewed with JavaScript enabled"
3635,stanford-s-social-robot-jackrabbot-seeks-to-understand-pedestrian-behavior,"Originally published at:			Stanford’s Social Robot ‘Jackrabbot’ Seeks to Understand Pedestrian Behavior | NVIDIA Technical Blog
Stanford researchers in the Computational Vision and Geometry Lab developed a robot that could soon autonomously move among us with normal human social etiquettes — such as deciding rights of way on the sidewalk. Using a Tesla K40 GPU and CUDA to train the machine learning models, the robot is able to understand its surroundings…Powered by Discourse, best viewed with JavaScript enabled"
3636,nvidia-makes-bert-fly,"Originally published at:			NVIDIA Makes BERT Fly | NVIDIA Technical Blog
Training Script now Available on GitHub and NGC Script Section Late last year, we described how teams at NVIDIA had achieved a 4X speedup on the Bidirectional Encoder Representations from Transformers (BERT) network, a cutting-edge natural language processing (NLP) network. Since then, we’ve further refined this accelerated implementation, and will be releasing a script to…Powered by Discourse, best viewed with JavaScript enabled"
3637,amazon-trains-alexa-on-gpus-to-better-handle-complex-queries,"Originally published at:			https://developer.nvidia.com/blog/amazon-trains-alexa-on-gpus-to-better-handle-complex-queries/
To improve how natural language processing (NLP) systems such as Alexa handle complex requests, Amazon researchers, in collaboration with the University of Massachusetts Amherst, developed a deep learning-based, sequence-to-sequence model that can better handle simple and complex queries.  “Virtual assistants such as Amazon Alexa, Apple Siri, and Google Assistant often rely on a semantic parsing…Powered by Discourse, best viewed with JavaScript enabled"
3638,gtc-2020-how-ai-is-reinventing-retail,"GTC 2020 S22200
Presenters: Azita Martin,NVIDIA; Paul Hendricks,NVIDIA
Abstract
Innovative retailers and disruptive startups are using AI to streamline logistics and store operations, prevent shrinkage and deliver better shopping experiences both in stores and online. Deep learning and machine learning algorithms can help cut operational expenses, increase revenue and improve decision making. We’ll cover use cases in asset protection, store analytics, autonomous shopping, demand forecasting and warehouse optimization. Learn how NVIDIA’s application frameworks for Data Science, Video Analytics and Conversational AI can help your data science team quickly build AI applications.Watch this session
Join in the conversation below.The last slide embedded in the video is not clickable to redirect to more related retail sessions.
It would be helpful to add a link somewhere on the web page.That’s a great suggestion.
The links will be in the pdf of the session, but at the time of writing it was not available.
You can also use your GTC Scheduler to filter on “Industry”, select RetailPowered by Discourse, best viewed with JavaScript enabled"
3639,machine-learning-is-helping-change-the-solar-industry,"Originally published at:			Machine Learning is Helping Change the Solar Industry | NVIDIA Technical Blog
A startup from California is using GPUs and big data to predict what homes are likely to buy solar panels. PowerScout is using GPUs on the Amazon cloud and cuDNN to train their deep learning models on a mix of data from commercial databases and LIDAR to detect solar panels from satellite images, and to…Powered by Discourse, best viewed with JavaScript enabled"
3640,ai-model-can-generate-images-from-natural-language-descriptions,"Originally published at:			AI Model Can Generate Images from Natural Language Descriptions | NVIDIA Technical Blog
To potentially improve natural language queries, including the retrieval of images from speech, Researchers from IBM and the University of Virginia developed a deep learning model that can generate objects and their attributes from natural language descriptions. Unlike other recent methods, this approach does not use GANs.  “We show that under minor modifications, the proposed…Powered by Discourse, best viewed with JavaScript enabled"
3641,is-iot-defining-edge-computing-or-is-it-the-other-way-around,"Originally published at:			https://developer.nvidia.com/blog/is-iot-defining-edge-computing-or-is-it-the-other-way-around/
Edge computing is quickly becoming standard technology for organizations heavily invested in IoT, allowing organizations to process more data and generate better insights.Powered by Discourse, best viewed with JavaScript enabled"
3642,ai-technology-helps-drones-sense-and-avoid-obstacles,"Originally published at:			AI Technology Helps Drones Sense and Avoid Obstacles | NVIDIA Technical Blog
Iris Automation, a Vancouver-based startup, raised $1.5M to bring their technology to industrial-type drones. Using NVIDIA GPUs on the Amazon cloud to train their deep learning models and a Jetson TX1 onboard the UAV, Iris’ technology analyzes and draws insights from videos captured by the drone’s cameras in real-time. Its system has a detection range…Powered by Discourse, best viewed with JavaScript enabled"
3643,global-illumination-part-vii-of-ray-tracing-gems,"Originally published at:			Global Illumination: Part VII of Ray Tracing Gems | NVIDIA Technical Blog
In the final installment of Ray Tracing Gems, we examine the dramatic impact ray tracing can have on global illumination. Lighting feels warmer, richer, and more realistic using this technique. “Ray Tracing Gems Part VII” can be downloaded at NVIDIA Developer Zone. Here is the forward for Part VII, written by Matt Pharr, distinguished research…Powered by Discourse, best viewed with JavaScript enabled"
3644,get-started-on-doca-for-dpus-with-a-free-introductory-course,"Originally published at:			Get Started on DOCA for DPUs with a Free Introductory Course | NVIDIA Technical Blog
Following the announcement of Early Access to the NVIDIA DOCA Software Framework at this year’s GTC, held in November, we launched a self-paced DOCA course to help you start working with this new framework. The NVIDIA Deep Learning Institute (DLI) is offering a free self-paced course titled “Introduction to DOCA for DPUs.” In this 2-hour introductory course, you will learn how DOCA and…Powered by Discourse, best viewed with JavaScript enabled"
3645,animated-models,"Hey there!
It is probably a difficult feat, but given tools s.a. OpenPose for humanoid body poses - are you envisioning this featuring auto-rigging or even AI-made animations coming along with the models themselves? Even auto-rigging would probably be immensely valuable, given that animations for i.e. dogs could be applied to any AI-generated type of dog!Cheers,
FlorianYes, enabling the rigging is a great feature to support in the future, as it can enabling the animation. It’s possible to combine the human/animal rigging model (e.g. SMPL model) with GET3D, such that the generated 3D human body can have skinning weights for animation, we wold be interested to add in the future!This topic was automatically closed 14 days after the last reply. New replies are no longer allowed.Powered by Discourse, best viewed with JavaScript enabled"
3646,learn-how-industry-leaders-are-architecting-solutions-for-automotive-design-and-development-at-scale-at-gtc-2021,"Originally published at:			https://developer.nvidia.com/blog/develop-train-test-avs-gtc-2021/
Autonomous vehicles are born in the data center, and at GTC, attendees can learn exactly how high-performance compute is vital to developing, training, testing and validating the next generation of transportation.Powered by Discourse, best viewed with JavaScript enabled"
3647,monitoring-high-performance-machine-learning-models-with-rapids-and-whylogs,"Originally published at:			https://developer.nvidia.com/blog/monitoring-high-performance-machine-learning-models-with-rapids-and-whylogs/
Machine learning (ML) data is big and messy. Organizations have increasingly adopted RAPIDS and cuML to help their teams run experiments faster and achieve better model performance on larger datasets. That, in turn, accelerates the training of ML models using GPUs. With RAPIDS, data scientists can now train models 100X faster and more frequently. Like…Powered by Discourse, best viewed with JavaScript enabled"
3648,building-an-accelerated-5g-cloudran-at-the-edge,"Originally published at:			Building an Accelerated 5G CloudRAN at the Edge | NVIDIA Technical Blog
Fifth-generation networks (5G) are ushering in a new era in wireless communications that delivers 1000X the bandwidth and 100X the speed at 1/10th the latency of 4G. 5G also allows for millions of connected devices per square km and is being deployed as an alternative to WiFi at edge locations like factories and retail stores.…Powered by Discourse, best viewed with JavaScript enabled"
3649,unlocking-speech-ai-technology-for-global-language-users-top-q-amp-as,"Originally published at:			https://developer.nvidia.com/blog/unlocking-speech-ai-technology-for-global-language-users-top-qas/
This post summarizes the top questions asked during Unlocking Speech AI Technology for Global Language Users, a recorded talk from the Speech AI Summit 2022.Powered by Discourse, best viewed with JavaScript enabled"
3650,dxr-training-a-conversation-with-chris-wyman-principal-research-scientist-at-nvidia,"Originally published at:			DXR Training: A Conversation with Chris Wyman, Principal Research Scientist at NVIDIA | NVIDIA Technical Blog
On Tuesday, March 21st at 2:40pm, NVIDIA will host “Introduction to DirectX Raytracing,” a three hour tutorial session. Chris Wyman, Adam Mars, and Juha Sjoholm will provide GDC attendees with an overview on how to integrate ray tracing into existing raster applications. We asked Chris about his history with ray tracing, and what he hopes…Powered by Discourse, best viewed with JavaScript enabled"
3651,the-next-wave-of-enterprise-performance-with-java-power-systems-and-nvidia-gpus,"Originally published at:			https://developer.nvidia.com/blog/next-wave-enterprise-performance-java-power-systems-nvidia-gpus/
The Java ecosystem is the leading enterprise software development platform, with widespread industry support and deployment on platforms like the IBM WebSphere Application Server product family. Java provides a powerful object-oriented programming language with a large developer ecosystem and developer-friendly features like automated memory management, program safety, security and runtime portability, and high performance features…I'm just wondering how CUDA4J differs from available and open source JNI wrappers (such as JCuda)It seems that the programmer is again responsible for everything.Hi Mehran,You are correct that CUDA4J is a toolkit where the programmer is working with concepts from the CUDA programming model.  However, it is designed to provide an API that is consistent with Java best practices rather than providing a direct 1:1 mapping to the CUDA runtime and driver APIs.We believe the result is an API that is more concise, more natural to Java programmers, more secure, and less error-prone than existing offerings.  The CUDA4J API is available to application developers, but it also forms the basis for the higher-level GPU usage from existing Java APIs, as described in the article.--TimHello Tim,Thank you for your reply. Will CUDA4J be publicly available anytime soon?Mehran, you'll appreciate that I cannot pre-announce availability, but you will have seen the recent press releases around IBM Power 8 support for Nvidia GPUs, and you can keep an eye on IBM developerWorks for our latest Java SDK releases for 64-bit Power LE hardware here:http://www.ibm.com/develope...Tim, I understand that and thank you for your response.Hi Tim,I'm a graduate student doing research involving some simple string processing using MapReduce and the GPU.  Currently, I'm having to take my strings out of their collection and translate them into arrays of pointers.  Will CUDA4J be able to use complex types or will it be limited to C primitives?CUDA4J supports transferring data found in NIO buffers or in arrays of primitive types. The best performance is obtained using a ByteBuffer allocated via Cuda.allocatePinnedHostBuffer().If your 'complex' type is a POD C structure, you can create simple API to represent an array of your objects stored in a ByteBuffer. JIT compilers found in modern JVMs do a remarkable job eliminating the apparent overhead of doing so, leaving you with excellent performance on both the host and on devices.In the (hopefully not too distant) future, you should be able to express POD types directly in Java, removing some of the programming burden. There's a lot in common between IBM's 'Packed Objects' approach [1] and Oracle's 'Value Types' [2], so it seems safe to assume that the situation will improve.[1] https://www.ibm.com/develop...[2] http://cr.openjdk.java.net/...It looks like details about the API are starting to show upat  http://www-01.ibm.com/suppo...However, it's not clear where to download the SDK...Hi Jim,I think the CUDA4J API and the GPU-optimized Java SE sorting implementation are included in the IBM SDK, Java Technology Edition 7.0:http://www.ibm.com/develope...GPU Docs:http://www-01.ibm.com/suppo...Mehran,  The CUDA4J APIs have been released in IBM Java 7 Release 1 for Linux PowerPC LE, available from the IBM developerWorks site. You can read the release documentation here bit.ly/ibmgpu7Thanks Mark!  Since I'm a Windows-based Java developer, it looks like the Eclipse-based download will make the most sense.Hi all,Which version of the IBM JDK with CUDA should I download for Jetson Tegra TK1?Many thanksIs there a getting started guide for cuda4J.If it was 1:1 with the low level API then I guess I could look at the low level getting started.As it is not I think I would need to get a little more help.Hello Keith,The concepts are close to the low level API, and if you grab the latest IBM Java SDK from http://www.ibm.com/develope... there are a number of examples showing the use of CUDA4J in the package.  The on-line documentation for GPU programming with CUDA4J is at http://ibm.co/1F8S9VHIn addition, there is a tutorial being written at the moment, to be delivered at the GTC conference (see http://bit.ly/1FYvdqP).  If you are able to attend you can get some first-hand help with getting started!We'll look to distilling this tutorial material into some additional on-line articles to help you get started.Going to test today latest 8 version on Gentoo AMD64 system, as IBM SDK has been abandoned on Gentoo for reasons related to IBM license policy, etc. I will try to recover it and put into main tree as CUDA4J is really promising.I would love to see comparison of performance against pure C on more common parallel algorithms (compressions, AES, radix sort, hash generation, etc.), but I will eventually create my own comparison :-)Powered by Discourse, best viewed with JavaScript enabled"
3652,announcing-nsight-compute-2020-2-and-nsight-visual-studio-edition-2020-2,"Originally published at:			Announcing Nsight Compute 2020.2 and Nsight Visual Studio Edition 2020.2 | NVIDIA Technical Blog
Nsight Compute 2020.2, now available for download, puts more control in the hands of developers enabling new usage models and opportunities to get the best performance. For many use-cases this also means getting results faster so you can spend less time profiling and more time optimizing. The latest release adds the highly-requested “Application Replay” feature…Powered by Discourse, best viewed with JavaScript enabled"
3653,ray-tracing-essentials-part-5-ray-tracing-effects,"Originally published at:			Ray Tracing Essentials Part 5: Ray Tracing Effects | NVIDIA Technical Blog
NVIDIA recently published Ray Tracing Gems, a deep-dive into best practices for real-time ray tracing. The book was made free-to-download, to help all developers embrace the bleeding edge of rendering technology. Ray Tracing Essentials is a seven-part video series hosted by the editor of Ray Tracing Gems, NVIDIA’s Eric Haines. The aim of this program…Powered by Discourse, best viewed with JavaScript enabled"
3654,jetson-project-of-the-month-driver-assistance-system-using-jetson-nano,"Originally published at:			https://developer.nvidia.com/blog/jetson-project-of-the-month-driver-assistance-system-using-jetson-nano/
Viet Anh Nguyen was awarded the Jetson Project of the Month for his Advanced Driver Assistance System (ADAS). This prototype, which runs on a NVIDIA Jetson Nano, aids a driver with collision, lane departure and speeding warnings.  Viet Anh’s goal is to serve the low-end and older car models with his solution. He chose Jetson…Powered by Discourse, best viewed with JavaScript enabled"
3655,cuda-toolkit-11-and-nsight-developer-tools-released-for-general-availability,"Originally published at:			CUDA Toolkit 11 and Nsight Developer Tools Released for General Availability | NVIDIA Technical Blog
NVIDIA announced the NVIDIA Ampere architecture in May and disclosed the new NVIDIA A100 GPU was supported in an RC posting of CUDA Toolkit and Nsight Developer Tools.  Today, NVIDIA announces the General Availability (GA) of CUDA 11, Nsight Systems 2020.3 and Nsight Compute 2020.1 for members of the NVIDIA Developer Program. The new releases…Powered by Discourse, best viewed with JavaScript enabled"
3656,share-your-science-next-generation-computational-fluid-dynamics-technology,"Originally published at:			https://developer.nvidia.com/blog/share-your-science-next-generation-computational-fluid-dynamics-technology/
Peter Vincent of the Department of Aeronautics at Imperial College London shares how they are using NVIDIA Tesla GPUs to accelerate Computational Fluid Dynamics simulations that will improve the design processes used by companies that design aircrafts or Formula One race cars. Learn more about PyFR, the open-source CFD package developed by Vincent’s lab, that employs…Powered by Discourse, best viewed with JavaScript enabled"
3657,explainer-what-is-edge-computing,"Originally published at:			What Is Edge Computing? | NVIDIA Blog
Edge computing is the practice of processing data physically closer to its source.Powered by Discourse, best viewed with JavaScript enabled"
3658,advanced-api-performance-synchronization,"Originally published at:			https://developer.nvidia.com/blog/advanced-api-performance-synchronization/
Synchronization in graphics programming refers to the coordination and control of concurrent operations to ensure the correct and predictable execution of rendering tasks. Improper synchronization across the CPU and GPU can lead to slow performance, race conditions, and visual artifacts. Recommended If running workloads asynchronously, make sure that they stress different GPU units. For example,…Powered by Discourse, best viewed with JavaScript enabled"
3659,gtc-2020-neuroevolution-based-automated-model-building-how-to-create-better-models,"GTC 2020 S21550
Presenters: Keith Moore,SparkCognition
Abstract
Common neural network architectures may work well for known, established data problems; however, they can fall short when modern machine learning applications demand more performance and higher levels of sophistication. In this presentation, Keith Moore, director of product management at SparkCognition, covers how neural architecture search works, some of the challenges faced in the space, and why an evolutionary approach is capable of discovering sophisticated and elegant designs that fit your data. He’ll discuss how multiple models can be trained in parallel using MPS technology, how batch size loading can change performance curves, and how the use of PyTorch and automatic mixed precision on a Tesla V100 GPU can decrease training times by over 5x. In conclusion, Keith will take you through the journey their team faced on building out better models, provide some real-world examples and implementations, and also share some of the problems that are yet to be solved.Watch this session
Join in the conversation below.Powered by Discourse, best viewed with JavaScript enabled"
3660,nv-wavenet-better-speech-synthesis-using-gpu-enabled-wavenet-inference,"Originally published at:			Nv-Wavenet: Better Speech Synthesis Using GPU-Enabled WaveNet Inference | NVIDIA Technical Blog
WaveNets represent an exciting new neural network architecture used to generate raw audio waveforms, including the ability to synthesize very high quality speech. These networks have proven challenging to deploy on CPUs, as generating speech in real-time or better requires substantial computation in tight timeframes. Fortunately, GPUs offer the tremendous parallel compute capability needed to make…Nice workWhere can i hear a sample?Powered by Discourse, best viewed with JavaScript enabled"
3661,supercharging-ai-video-and-ai-inference-performance-with-nvidia-l4-gpus,"Originally published at:			https://developer.nvidia.com/blog/supercharging-ai-video-and-ai-inference-performance-with-nvidia-l4-gpus/
NVIDIA T4 was introduced 4 years ago as a universal GPU for use in mainstream servers. T4 GPUs achieved widespread adoption and are now the highest-volume NVIDIA data center GPU. T4 GPUs were deployed into use cases for AI inference, cloud gaming, video, and visual computing. At the NVIDIA GTC 2023 keynote, NVIDIA introduced several…Thank you to all the customers who allowed us to share their experiences with the NVIDIA L4 GPU.  If you have any questions or comments, let us know.Powered by Discourse, best viewed with JavaScript enabled"
3662,gtc-2020-accelerating-m-e-workflows-with-nvidia-rtx-server-and-virtual-gpu,"GTC 2020 S21554
Presenters: Konstantin Cvetanov,NVIDIA; Richard Grandy,NVIDIA
Abstract
Learn how NVIDIA RTX Server and Virtual GPU are revolutionizing media and entertainment workflows in the data center. We’ll present industry-specific use cases, real-world application examples, and implementation best practices for RTX Server in virtualized environments.Watch this session
Join in the conversation below.Powered by Discourse, best viewed with JavaScript enabled"
3663,using-the-nvidia-cuda-stream-ordered-memory-allocator-part-2,"Originally published at:			https://developer.nvidia.com/blog/using-the-nvidia-cuda-stream-ordered-memory-allocator-part-2/
In part 1 of this series, we introduced new API functions, cudaMallocAsync and cudaFreeAsync, that enable memory allocation and deallocation to be stream-ordered operations. In this post, we highlight the benefits of this new capability by sharing some big data benchmark results and provide a code migration guide for modifying your existing applications. We also…This does improve some performance, but there is a question。
Is it possible to pre-allocate a large chunk of video memory and then assign values directly to this chunk of video memory，Will this performance be better? The following part of the code：float* in_d;
float* in_d_sin[10];
cudaMalloc((void**)&in_d, 4 * 10);// sizeof(float)=4
for (int i = 0; i < 10; i++) {
in_d_sin[i] = in_d + i;
}Hey @bjhd_qcj could you elaborate a bit more on your question?I’m not sure how it relates to cudaMallocAsync or performance.Your code as written wouldn’t work because you’re allocating device memory with cudaMalloc and then attempting to write to it from host code in the for loop. For that to work you would need to use cudaMallocManaged.When the model is hot loaded or unloaded, the service experiences performance jitters for several minutes.
The initial analysis of the phenomenon is caused by the slow allocation of video memory, because the video memory grows slowly during the jitter.
Further analysis, our model will call cudaMalloc 3000 times when loading, guess it may be caused by this. So, we want to reduce cudaMalloc calls.Sounds like you should try out cudaMallocAsync and see if it makes a difference!Powered by Discourse, best viewed with JavaScript enabled"
3664,new-nvidia-healthcare-webinar-series-compute4covid,"Originally published at:			New NVIDIA Healthcare Webinar Series: COMPUTE4COVID | NVIDIA Technical Blog
The novel coronavirus has changed the landscape of healthcare and the need for GPU-accelerated tools has been placed front and center. HPC and AI tools are helping battle the novel coronavirus at every stage, from predicting and preventing outbreaks, detection and diagnosis, to drug and vaccine development, to treatment and patient care. This series of…Powered by Discourse, best viewed with JavaScript enabled"
3665,nvidia-is-now-opencl-3-0-conformant,"Originally published at:			NVIDIA is Now OpenCL 3.0 Conformant | NVIDIA Technical Blog
OpenCL 3.0’s focus on defining a baseline to enable developer-critical functionality to be widely adopted in future versions of the specification.Great!!! Now when can we see 465 drivers and OpenCL on the Jetson Xavier NX/AGX integrated into Jetpack?Is the list of OpenCL 3.0 features from the announcement exhaustive or would there be a more complete list (ideally a compliance matrix to the OpenCL 3.0.x specs) available somewhere?I’m particularly interested in support for generic address space and dynamic parallelism, so any information about support for these features (or a roadmap) would be very much appreciated.Thanks in advance!@setec_astronomy  – The release notes give the complete list of supported features, including experimental features from 2.0. Hope that helps!Where is cl_khr_spir?Also, you can run conformance_computeinfo which should report all optional features and extensions.NVIDIA OpenCL doesn’t support SPIR or SPIR-V (cl_khr_spir is a language extension and not part of core feature set for OpenCL 3.0)OpenCL is currently supported only on x86 Windows and LinuxPowered by Discourse, best viewed with JavaScript enabled"
3666,bootstrapping-object-detection-model-training-with-3d-synthetic-data,"Originally published at:			https://developer.nvidia.com/blog/bootstrapping-object-detection-model-training-with-3d-synthetic-data/
Learn step by step how to use NVIDIA Omniverse to generate your own synthetic dataset. Then fine-tune your computer vision model deployed in NVIDIA Triton for inference.I’m a bit confused about which direction to take with Replicator, the way described in this tutorial or using Replicator Composer.  Composer appears to be a higher level tool but I haven’t been able to find how to set the semantic labels of the items drawn.  I don’t see anything in the Replicator Composer manual related to semantics.Any pointers anyone?Thanks,
DaveHi Dave, thanks for this question! Composer as it is now will be deprecated soon so utilizing a Python script with the Replicator API is how we would suggest you can get started today with generating your semantic labels on your dataset. However, there will be a Composer update coming soon that you might be interested in checking out! Hope that helps.Thanks for the answer.  In the meantime however I got the Replicator Composer (RC) warehouse demo working and was able to swap in my own assets, get annotations, etc.So now that I have two ways of running Replicator, do you still recommend I stick with the Python more manual mode?I am happy to hear that Replicator Composer is working for you. I think it depends on the timeline you are looking at in regard to your projects. Python API is well developed and can be used now which is a big benefit. Composer will see changes in the future which might affect your workflow as it stands now. Up to you which works best for your specific case though!The problem is, and you can see in my other very long thread to tech support, the VS Code debugger is great but it crashes Code after a few iterations.  So I’m stuck with no python debugger.I REALLY REALLY wish you guys would prioritize fixing the stability of Code when using your own suggested extension debugging method.  Not sure where it is on the queue but it’s incredibly painful to have to reboot Code after each try of my Replicator debug session.Something I’ve noticed is the time to render grows exponentially with image number.
image3352×1318 335 KB
The only changes to the code I made were output image size (224x224), output directory and number of frames generated.I doubt exponential growth in render times is NVIDIA’s intention. For example I generated 1000 224x224 images and it takes about 3 minutes for 100 frames.  But took 17 hours for 1000 frames.I am building the synthetic data following this article. However, I was stuck when I was coding the following code:
bbox2d_loose_file_name = “bounding_box_2d_tight_0.npy”
data = np.load(os.path.join(out_dir, bbox2d_tight_file_name))bbox2d_tight_labels_file_name = “bounding_box_2d_tight_labels_0.json”
with open(os.path.join(out_dir, bbox2d_tight_labels_file_name), “r”) as json_data:
bbox2d_loose_id_to_labels = json.load(json_data)colorize_bbox_2d(rgb_path, data, bbox2d_loose_id_to_labels, os.path.join(vis_out_dir, “bbox2d_tight.png”))
The output noticed me that there is no bounding_box_2d_tight_0.npy despite the fact that I already made BasicWritter code before. Can you show me how can I get that .npy file ?Thanks for this question, it looks like you are trying to visualize the data. Do you have any files ending in .npy, or are you only seeing the .json and .png files?Linking to workaround posted on the other blog: Randomizer based Replicator code gets slower every frame - #5 by jlaflecheThat workaround doesn’t work as far as I can tell.  See the end of that other thread.I did find .npy, .json and .png in my created folder. Are they generated by the above codes, right? So .npy and .json shall be link via path to my folder and the .png are, too ?Yes all three types of files should be in the same folder! The BasicWriter code is where the files are generated. The code snippet you included above is actually us labelling and example image with the generated bounding boxes and labels.However, there will be a Composer update coming soonHas this been released?Powered by Discourse, best viewed with JavaScript enabled"
3667,cuda-context-independent-module-loading,"Originally published at:			https://developer.nvidia.com/blog/cuda-context-independent-module-loading/
Most CUDA developers are familiar with the cuModuleLoad API and its counterparts for loading a module containing device code into a CUDA context. In most cases, you want to load identical device code on all devices. This requires loading device code into each CUDA context explicitly. Moreover, libraries and frameworks that do not control context creation and…Powered by Discourse, best viewed with JavaScript enabled"
3668,supercomputers-reveal-how-the-hiv-virus-moves,"Originally published at:			Supercomputers Reveal How the HIV Virus Moves | NVIDIA Technical Blog
University of Illinois scientists used two GPU-accelerated supercomputers to simulate the behavior of 64 million atoms to capture 1.2 microseconds of the life of an HIV capsid. The simulation offers new insights into how the virus senses its environment and completes its infective cycle. “We are learning the details of the HIV capsid system, not…Powered by Discourse, best viewed with JavaScript enabled"
3669,enhancing-customer-experience-in-telecom-with-nvidia-customized-speech-ai,"Originally published at:			https://developer.nvidia.com/blog/enhancing-customer-experience-in-telecom-with-nvidia-customized-speech-ai/
Learn why conversational AI systems are essential and why it is important to have a high level of transcription accuracy for optimal performance in downstream tasks.Powered by Discourse, best viewed with JavaScript enabled"
3670,integrating-with-data-generation-and-labeling-tools-for-accurate-ai-training,"Originally published at:			Integrating with Data Generation and Labeling Tools for Accurate AI Training | NVIDIA Technical Blog
Data plays a crucial role in creating intelligent applications. To create an efficient AI/ ML app, you must train machine learning models with high-quality, labeled datasets. Generating and labeling such data from scratch has been a critical bottleneck for enterprises. Many companies prefer a one-stop solution to support their AI/ML workflow from data generation, data…Powered by Discourse, best viewed with JavaScript enabled"
3671,fast-tracking-hand-gesture-recognition-ai-applications-with-pretrained-models-from-ngc,"Originally published at:			https://developer.nvidia.com/blog/fast-tracking-hand-gesture-recognition-ai-applications-with-pretrained-models-from-ngc/
One of the main challenges and goals when creating an AI application is producing a robust model that is performant with high accuracy. Building such a deep learning model is time consuming. It can take weeks or months of retraining, fine-tuning, and optimizing until the model satisfies the necessary requirements. For many developers, building a…Powered by Discourse, best viewed with JavaScript enabled"
3672,ai-helps-preserve-the-endangered-seneca-language,"Originally published at:			AI Helps Preserve the Endangered Seneca Language | NVIDIA Technical Blog
There are nearly 7,000 languages worldwide and around half are considered endangered. That means many of them are no longer taught in schools, not used in commerce or government, and often incompatible with computer keyboards. To help preserve audio and textual evidence of one of these languages, researchers from the Rochester Institute of Technology developed…Powered by Discourse, best viewed with JavaScript enabled"
3673,modernize-your-network-using-netdevops,"Originally published at:			https://developer.nvidia.com/blog/modernize-your-network-using-netdevops/
In part 2 of this series, we focus on solutions that optimize and modernize data center network operations.Powered by Discourse, best viewed with JavaScript enabled"
3674,cuda-spotlight-gpu-accelerated-neuroscience,"Originally published at:			https://developer.nvidia.com/blog/cuda-spotlight-gpu-accelerated-neuroscience/
This week’s Spotlight is on Dr. Adam Gazzaley of UC San Francisco, where he is the founding director of the Neuroscience Imaging Center and an Associate Professor in Neurology, Physiology and Psychiatry. His work was featured in Nature in September 2013. NVIDIA: Adam, how are you using GPU computing in your research? Adam: We are working…Powered by Discourse, best viewed with JavaScript enabled"
3675,amazon-releases-open-source-deep-learning-software,"Originally published at:			Amazon Releases Open-Source Deep Learning Software | NVIDIA Technical Blog
With hundreds of millions of customers shopping daily on Amazon, it’s critical they help them discover the right product from their massive catalog of products. Amazon’s Deep Scalable Sparse Tensor Network Engine (DSSTNE or “Destiny”) is a deep learning framework built from the ground up to help researchers develop search and recommendation systems. With multi-GPU…Powered by Discourse, best viewed with JavaScript enabled"
3676,accelerating-volkswagen-connected-car-data-pipelines-100x-faster-with-nvidia-rapids,"Originally published at:			https://developer.nvidia.com/blog/accelerating-volkswagen-connected-car-data-pipelines-100x-faster-with-nvidia-rapids/
Connected cars are vehicles that communicate with other vehicles using backend systems to enhance usability, enable convenient services, and keep distributed software maintained and up to date. At Volkswagen, we are working on connected car with NVIDIA to solve the challenges which have computational inefficiencies like Geospatial Indexing and K-Nearest Neighbors when implemented in native…Powered by Discourse, best viewed with JavaScript enabled"
3677,adaptive-parallel-computation-with-cuda-dynamic-parallelism,"Originally published at:			https://developer.nvidia.com/blog/introduction-cuda-dynamic-parallelism/
Early CUDA programs had to conform to a flat, bulk parallel programming model. Programs had to perform a sequence of kernel launches, and for best performance each kernel had to expose enough parallelism to efficiently use the GPU. For applications consisting of “parallel for” loops the bulk parallel model is not too limiting, but some parallel…Pre-DP, this would require multiple kernel launches from the device side.  Do you have the timing of such a thing, for those with pre-3.5 devices?I’ve done a comparison like this for the Triplet Finder algorithm for the PANDA experiment. The the plot is available from GTC On-Demand (http://on-demand.gputechcon..., slide 33), and will also be presented in the third part of this blog post series.Great. Thanks!I went to high school with Rico Mariani :)Hi whoever you are :)DB!In the code of mandelbrot_block_k functionelse if (depth + 1 > MAX_DEPTH && d / SUBDIV < MIN_SIZE)should beelse if (depth + 1 < MAX_DEPTH && d / SUBDIV > MIN_SIZE)and in the main function mandelbrot_block_k function is called with complex(-1.5, 1) for cmin parameter, but it should be complex(-1.5, -1) according to the code in the GitHub.Powered by Discourse, best viewed with JavaScript enabled"
3678,ai-helps-predict-and-sketch-computer-aided-design-models,"Originally published at:			https://developer.nvidia.com/blog/ai-helps-predict-and-sketch-computer-aided-design-models/
Recently at the International Conference on Machine Learning (ICML), researchers from Princeton University and Columbia University introduced SketchGraphs, a large-scale dataset of computer-aided design (CAD) sketches that can be used to train models for AI-aided design. Machine learning CAD models trained using SketchGraphs have the potential to enable more efficient design processes for architects, engineers,…Powered by Discourse, best viewed with JavaScript enabled"
3679,share-your-science-creating-volumetric-video-of-real-people-in-virtual-reality,"Originally published at:			Share Your Science: Creating Volumetric Video of Real People in Virtual Reality | NVIDIA Technical Blog
Experience real people on any virtual reality headset. Joel Pitt, a machine learning researcher at 8i, shares how the startup is using NVIDIA GPUs and CUDA to put real volumetric video of humans in virtual reality environments. 8i’s technology allows creators to bring real people into volumetric virtual experiences by transforming HD video from multiple…It is amazing. Does it use neural network? Is TensorRT involved?Powered by Discourse, best viewed with JavaScript enabled"
3680,securing-and-accelerating-modern-data-center-workloads-with-nvidia-asap-technology,"Originally published at:			https://developer.nvidia.com/blog/securing-and-accelerating-modern-data-center-workloads-with-nvidia-asap²-technology/
At GTC 2021, global technology partners together with NVIDIA showcased the ways in which they leverage the ASAP2 technology to secure and accelerate modern data center workloads.Powered by Discourse, best viewed with JavaScript enabled"
3681,giving-virtual-dressing-rooms-a-makeover-with-computer-vision,"Originally published at:			https://developer.nvidia.com/blog/giving-virtual-dressing-rooms-a-makeover-with-computer-vision/
With the help of AI, a new fashion startup offers online retailers a scalable virtual dressing room, capable of cataloging over a million garment images weekly.Powered by Discourse, best viewed with JavaScript enabled"
3682,nano-pwm0-no-output,"Hi,
now i had config dts using lcd_bl_pwm0, but can’t see pwm output on header pin 32.
l4t 32.3.1/B01 module.
Anyone could give some advice?Duplicated with Nano pwm0 cant workPowered by Discourse, best viewed with JavaScript enabled"
3683,automating-data-center-networks-with-nvidia-cumulus-linux,"Originally published at:			https://developer.nvidia.com/blog/automating-data-center-networks-with-nvidia-cumulus-linux/
With evolving and ever-growing data centers, the days of simple networks that remained mostly unchanged are gone. Back then, when a configuration change was needed, it was simple for the network administrator to make the changes device per device, line-by-line. As data centers evolve from physical on-premises to digitized cloud infrastructures, the traditional networks have…Powered by Discourse, best viewed with JavaScript enabled"
3684,cuda-helps-see-people-through-walls,"Originally published at:			https://developer.nvidia.com/blog/cuda-helping-to-recognize-people-through-walls/
Researchers at MIT’s Computer Science and Artificial Intelligence Lab have developed software that uses variations in Wi-Fi signals to recognize human silhouettes through walls. The researchers presented RF-Capture, which tracks the 3D positions of a person’s limbs and body parts even when the person is fully occluded from its sensor, and does so without placing…Powered by Discourse, best viewed with JavaScript enabled"
3685,autonomous-search-and-rescue-drones-outperform-humans-at-navigating-forest-trails,"Originally published at:			Autonomous Search-and-Rescue Drones Outperform Humans at Navigating Forest Trails | NVIDIA Technical Blog
In what could one day help find missing people in forests, a team of researchers used deep learning to train an autonomous drone to navigate a previously-unseen trail in a densely wooded forest completely on its own. The researchers from Dalle Molle Institute for Artificial Intelligence, the University of Zurich, and NCCR Robotics, mounted three GoPro…Powered by Discourse, best viewed with JavaScript enabled"
3686,annotate-build-and-adapt-models-for-medical-imaging-with-the-clara-train-sdk,"Originally published at:			Annotate, Build, and Adapt Models for Medical Imaging with the Clara Train SDK | NVIDIA Technical Blog
Deep Learning in medical imaging has shown great potential for disease detection, localization, and classification within radiology. Deep Learning holds the potential to create solutions that can detect conditions that might have been overlooked and can improve the efficiency and effectiveness of the radiology team. However, for this to happen data scientists and radiologists need to…Powered by Discourse, best viewed with JavaScript enabled"
3687,share-your-work-with-thousands-of-developers-worldwide-at-gtc-2021,"Originally published at:			https://developer.nvidia.com/blog/share-your-work-with-thousands-of-developers-worldwide-at-gtc-2021/
Have you done any work with our software, GPU, or DPU technology and are passionate about the many ways accelerated computing can change the world? Then, we invite you to submit a talk or poster and join us as a presenter at our next digital GTC in March.Powered by Discourse, best viewed with JavaScript enabled"
3688,new-app-uses-ai-to-enable-users-to-explore-sneakers-in-ar,"Originally published at:			New App Uses AI to Enable Users to Explore Sneakers In AR | NVIDIA Technical Blog
Wannaby, a Belarus startup founded by ex-googler Sergey Arkhangelsky, just launched Wanna Kicks a new app that can let you virtually try on a pair of shoes. The iOS app uses augmented reality to let users try on on various pairs of shoes. In the backend, the app was developed using convolutional neural networks, NVIDIA…Powered by Discourse, best viewed with JavaScript enabled"
3689,gtc-2020-how-to-build-a-multi-camera-media-server-for-ai-processing-on-jetson,"GTC 2020 S22396
Presenters: Carlos Rodriguez,RidgeRun; Jennifer Caballero,RidgeRun
Abstract
We’ll build a simple multi-camera media server for AI processing on a Jetson Board and demonstrate how, by using GStreamer Daemon, Interpipes, and DeepStream, you can develop a scalable and robust prototype to capture from multiple cameras using GMSL2 Virtual Channels. Besides achieving real-time deep learning inference, the server is completely dynamic. We’ll show its flexibility by triggering actions such as taking a snapshot, recording a video, or starting a network streaming when a specific prediction is made. By the end of this session you’ll have acquired the framework basics that will allow you to scale to your specific multimedia and AI needs.Watch this session
Join in the conversation below.Powered by Discourse, best viewed with JavaScript enabled"
3690,object-detection-on-gpus-in-10-minutes,"Originally published at:			Object Detection on GPUs in 10 Minutes | NVIDIA Technical Blog
Object detection remains the primary driver for applications such as autonomous driving and intelligent video analytics. Object detection applications require substantial training using vast datasets to achieve high levels of accuracy. NVIDIA GPUs excel at the parallel compute performance required to train large networks in order to generate datasets for object detection inference. This post…Thank you for the wonderful read, did you test this using the Nvidia Jetson Nano?in my case (running it with zed stereolabs camera), the output is as follows: root@d86527a4fbaa:/mnt# python SSD_Model/detect_objects_webcam.py WARNING: Logging before flag parsing goes to stderr.W0721 12:00:52.702791 140176104285952 deprecation_wrapper.py:119] From /usr/lib/python3.5/dist-packages/graphsurgeon/_utils.py:2: The name tf.NodeDef is deprecated. Please use tf.compat.v1.NodeDef instead.W0721 12:00:52.703287 140176104285952 deprecation_wrapper.py:119] From /usr/lib/python3.5/dist-packages/graphsurgeon/DynamicGraph.py:4: The name tf.GraphDef is deprecated. Please use tf.compat.v1.GraphDef instead.TensorRT inference engine settings:  * Inference precision - DataType.FLOAT  * Max batch size - 1Loading cached TensorRT engine from /mnt/SSD_Model/utils/../workspace/engines/FLOAT/engine_bs_1.bufTRT ENGINE PATH /mnt/SSD_Model/utils/../workspace/engines/FLOAT/engine_bs_1.bufRunning webcam: TrueSegmentation fault (core dumped)root@d86527a4fbaa:/mnt# environment: Ubuntu 18.04second attempt:./setup_environment.sh Setting envivonment variables for the webcamnon-network local connections being added to access control listDownloading VOC dataset--2019-07-21 19:04:05--  http://host.robots.ox.ac.uk/pascal/VOC/voc2007/VOCtest_06-Nov-2007.tarResolving host.robots.ox.ac.uk (host.robots.ox.ac.uk)... 129.67.94.152Connecting to host.robots.ox.ac.uk (host.robots.ox.ac.uk)|129.67.94.152|:80... connected.HTTP request sent, awaiting response... 200 OKLength: 451020800 (430M) [application/x-tar]Saving to: ‘VOCtest_06-Nov-2007.tar’VOCtest_06-Nov-2007 100%[===================>] 430.13M  12.6MB/s    in 37s     2019-07-21 19:04:42 (11.6 MB/s) - ‘VOCtest_06-Nov-2007.tar’ saved [451020800/451020800]Dockerfile has already been builtStarting docker container======================= NVIDIA TensorRT =======================NVIDIA Release 19.05 (build 6392482)NVIDIA TensorRT 5.1.5 (c) 2016-2019, NVIDIA CORPORATION.  All rights reserved.Container image (c) 2019, NVIDIA CORPORATION.  All rights reserved.https://developer.nvidia.com/tensorrtTo install Python sample dependencies, run /opt/tensorrt/python/python_setup.shroot@efe2ba640ec8:/mnt# python SSD_Model/detect_objects_webcam.py WARNING: Logging before flag parsing goes to stderr.W0721 12:05:20.935808 140649279698688 deprecation_wrapper.py:119] From /usr/lib/python3.5/dist-packages/graphsurgeon/_utils.py:2: The name tf.NodeDef is deprecated. Please use tf.compat.v1.NodeDef instead.W0721 12:05:20.937713 140649279698688 deprecation_wrapper.py:119] From /usr/lib/python3.5/dist-packages/graphsurgeon/DynamicGraph.py:4: The name tf.GraphDef is deprecated. Please use tf.compat.v1.GraphDef instead.Preparing pretrained modelDownloading /mnt/SSD_Model/utils/../workspace/models/ssd_inception_v2_coco_2017_11_17.tar.gzDownload progress [==================================================] 100%Download completeUnpacking /mnt/SSD_Model/utils/../workspace/models/ssd_inception_v2_coco_2017_11_17.tar.gzExtracting completeRemoving /mnt/SSD_Model/utils/../workspace/models/ssd_inception_v2_coco_2017_11_17.tar.gzModel readyW0721 12:05:35.382501 140649279698688 deprecation_wrapper.py:119] From /usr/lib/python3.5/dist-packages/graphsurgeon/StaticGraph.py:125: The name tf.gfile.GFile is deprecated. Please use tf.io.gfile.GFile instead.WARNING: To create TensorRT plugin nodes, please use the `create_plugin_node` function instead.NOTE: UFF has been tested with TensorFlow 1.12.0. Other versions are not guaranteed to workWARNING: The version of TensorFlow installed on this system is not guaranteed to work with UFF.UFF Version 0.6.3=== Automatically deduced input nodes ===[name: ""Input""op: ""Placeholder""attr {  key: ""dtype""  value {    type: DT_FLOAT  }}attr {  key: ""shape""  value {    shape {      dim {        size: 1      }      dim {        size: 3      }      dim {        size: 300      }      dim {        size: 300      }    }  }}]=========================================Using output node NMSConverting to UFF graphWarning: No conversion function registered for layer: NMS_TRT yet.Converting NMS as custom op: NMS_TRTW0721 12:05:35.967093 140649279698688 deprecation_wrapper.py:119] From /usr/lib/python3.5/dist-packages/uff/converters/tensorflow/converter.py:179: The name tf.AttrValue is deprecated. Please use tf.compat.v1.AttrValue instead.Warning: No conversion function registered for layer: FlattenConcat_TRT yet.Converting concat_box_conf as custom op: FlattenConcat_TRTWarning: No conversion function registered for layer: GridAnchor_TRT yet.Converting GridAnchor as custom op: GridAnchor_TRTWarning: No conversion function registered for layer: FlattenConcat_TRT yet.Converting concat_box_loc as custom op: FlattenConcat_TRTNo. nodes: 563UFF Output written to /mnt/SSD_Model/utils/../workspace/models/ssd_inception_v2_coco_2017_11_17/frozen_inference_graph.uffUFF Text Output written to /mnt/SSD_Model/utils/../workspace/models/ssd_inception_v2_coco_2017_11_17/frozen_inference_graph.pbtxtTensorRT inference engine settings:  * Inference precision - DataType.FLOAT  * Max batch size - 1Building TensorRT engine. This may take few minutes.and it works!it was cache probably that needed to be removedRESOLVED.However, is ther ea way to run it on Nvidia Jetson Xavier device? e.g. without the docker part?Anytime @disqus_z5z4JTinTV:disqus ,I haven't tried running it on the Jetson Nano, but I assume that once you get Docker up and running, that the setup should be very similar.Hey @disqus_n4MSuSh2Pt:disqus ,Glad to hear you were able to resolve this by clearing the cache.I haven't tried running on Jetson Xavier without Docker, but to start, I would open the Dockerfile I provided in GitHub and try to install those packages manually. If you can do that successfully, then you will have the same environment and should be able to run the code.I pulled and ran the docker successful using ./setup_environment.sh, but when I called 'python SSD_Model/detect_objects_webcam.py' as instructed, I got the following error message:root@b54ce065632c:/mnt# python SSD_Model/detect_objects_webcam.pyTraceback (most recent call last):  File ""SSD_Model/detect_objects_webcam.py"", line 12, in <module>    import utils.inference as inference_utils # TRT/TF inference wrappers  File ""/mnt/SSD_Model/utils/inference.py"", line 57, in <module>    import pycuda.autoinit  File ""/usr/local/lib/python3.5/dist-packages/pycuda/autoinit.py"", line 9, in <module>    context = make_default_context()  File ""/usr/local/lib/python3.5/dist-packages/pycuda/tools.py"", line 204, in make_default_context    ""on any of the %d detected devices"" % ndevices)RuntimeError: make_default_context() wasn't able to create a context on any of the 1 detected devicesAny clue about this error?Hey @disqus_7m7lUq2eB8:disqus,Looks like it's having a hard time finding a device where it can create a CUDA context. Perhaps it's not detecting the GPU in your machine? I would try to run nvidia-smi from within the container to make sure that Docker is seeing your GPU. It should have automatically detected a GPU in your machine, but perhaps it did not and you may have to pass it to docker manually using the --gpus flag. Let me know how that goes.Better to work with CPU's. Look for OpenVino framework from Intel. It is more robust and better support. Nvidia forums nobody replies and nobody provides support. After 6 months of waiting i finally decide to move away from Nvidia to Intel.Hate you Nvidia. You wasted my entire year with your pathetic drivers and pathetic frameworks.Will this work on my own custom object detection model?Powered by Discourse, best viewed with JavaScript enabled"
3691,autonomous-drone-hunts-down-rogue-drones-from-the-sky,"Originally published at:			Autonomous Drone Hunts Down Rogue Drones From the Sky | NVIDIA Technical Blog
Once it classifies the object, the Jetson-powered Airspace drone fires a tethered net to capture the other craft from the sky and safely returns it to its landing pad. Airspace is the only drone security solution capable of identifying, tracking, and autonomously removing rogue drones from the sky. The start-up is using GeForce GTX 1080…Powered by Discourse, best viewed with JavaScript enabled"
3692,ray-tracing-essentials-part-3-ray-tracing-hardware,"Originally published at:			Ray Tracing Essentials Part 3: Ray Tracing Hardware | NVIDIA Technical Blog
NVIDIA recently published Ray Tracing Gems, a deep-dive into best practices for real-time ray tracing. The book was made free-to-download, to help all developers embrace the bleeding edge of rendering technology. Ray Tracing Essentials is a seven-part video series hosted by the editor of Ray Tracing Gems, NVIDIA’s Eric Haines. The aim of this program is to make developers…Powered by Discourse, best viewed with JavaScript enabled"
3693,boost-edge-ai-performance-with-the-new-nvidia-jetson-orin-nx-16gb,"Originally published at:			https://developer.nvidia.com/blog/boost-edge-ai-performance-with-the-new-nvidia-jetson-orin-nx-16gb/
Building on the momentum from last year’s expansion of NVIDIA Jetson edge AI devices, the NVIDIA Jetson Orin NX 16 GB module is now available for purchase worldwide.  The Jetson Orin NX 16 GB module is unmatched in performance and efficiency for small form-factor, low-power robots, and autonomous machines. This makes it ideal for use…Powered by Discourse, best viewed with JavaScript enabled"
3694,ai-ups-the-resolution-of-images,"Originally published at:			https://developer.nvidia.com/blog/ai-ups-the-resolution-of-images/
Entropix is an Oakland, California-based startup that uses deep learning to reconstruct video and still images to 9x its captured pixel density. Entropix calls this, “resolution on demand.” The startup demonstrated its patented technology at the GPU Technology Conference in San Jose, California this week. The technology has the potential to help law enforcement locate…Powered by Discourse, best viewed with JavaScript enabled"
3695,ford-using-deep-learning-for-lane-detection,"Originally published at:			Ford Using Deep Learning for Lane Detection | NVIDIA Technical Blog
Researchers from the recently expanded Ford Research and Innovation Center in Palo Alto, California developed a new sub-centimeter accurate approach to estimate a moving vehicle’s position within a lane in real-time. To achieve this level of precision the researchers trained a deep neural network, aptly named DeepLanes, to process input images from two laterally-mounted down-facing…Powered by Discourse, best viewed with JavaScript enabled"
3696,image-segmentation-using-digits-5,"any update on Digits 5 on AWS?I'm a little confused, how it is possible? https://uploads.disquscdn.c...Typos have been corrected on pool2/pool5/conv6 rows.Okay thanks, I'll await the release then.For custom dataset preparation, you may want to look to the following, a tool to prepare segmented images & save output in Digits compatible format;https://alpslabel.wordpress...Hello Greg,I think the paragraph ""Enabling finer segmentation"" refers to this paper: ""Improving Fully Convolution Network for Semantic Segmentation"" , where authors introduce modified FCN-8s as IFCN-8s.Then my question is, have you implemented their suggestion? if Yes, where are the prototxt files?Hello, ""Enabling finer segmentation"" in my article refers to section 4.2 (""Combining what and where"") of the FCN paper. In practice I found FCN-8s to work  well. I have not tried IFCN-8s.I'm using DIGITS 5, but I can't find any pre-trained models. Could you make the .prototxt files that you used in this article available on some web page instead?Hello, there is a walk-through on https://github.com/NVIDIA/D... with links to a pre-trained model and prototxt for FCN-Alexnet.Why is the kernel size of the deconvolution layer set at 63, when the stride is 32? Is it always 2*Stride-1 ? When the deconvolution overlaps (which it does here) what is the final class of the output pixel?Hi Greg,Thanks for the blog. My question is, In model page, DO I have to set subtract mean to None in Data Transformations for training fcn-alexnet?. ThanksSK06In fcn-alexnet in DIGITS there is a layer named ""shift"" which subtracts 127 from pixels, which results in an approximate centering of the data around zero. Therefore if you use this example, you should set mean subtraction to None on the model page because the network already does it.This is the recommended way to set-up an upsampling layer through bilinear interpolation: http://caffe.berkeleyvision...Hi Greg,May I ask you what code have you used to create those inferences? (Outside DIGITS)Why is the Zebra crossing of road labeled as Vegetation even in Ground truthas well as in inferred Pictures.Hello, this is an arbitrary choice. You might want to ask the authors of the KITTI dataset for more details.I am a bit confused about the cumulative offset. how is it calculated backwards, what's the formula of substitution?Why don't you like the idea of convolutionalizing VGG-16 manually? I feel this great blog is presenting references for everything.we just need the following:1. Convolutionalize the FCs, for that we will need to figure the kernel size used. made easy by your explanation2. Calculate the Stride and the Kernel Size of the Deconvolution layer made easy by Table 2.3. Find the Cumulative offset for cropping before the Softmax layer. (am not fully understanding this part)4. Perform a net surgery to reshape the original pre-trained weights of VGG-16 from their FC shape to their Convolutional counterpart.Am I missing something?Hi Greg,Thanks a lot! Could you help me with two questions:Is there a minimum version of caffe required for this example to work?I am using the nvidia-digits/latest image, which has preinstalled caffe 0.15.14. I received an error for which the solutions found online point to the version of caffe -- that we should have at least caffe 0.16. I am now struggling with how to update the caffe version within the digits container.Another question is whether we can anyhow access all the pretrained models that you are showing here?I only can see these models in the store: https://uploads.disquscdn.c...Dear Greg,thank you very much for your insights in segmentation with DIGITS and synthia!I can hardly find anything to skip connections/residual learning in case of implementation. Is it really just a ""fuse"" layer ?Would it be possible for you to please provide the prototxt or an example?AlexNet also consists of pool3 and pool5, your example misses those two layers. What kind of a difference do these two make?Did you train the Alexnet with the Synthia dataset ?How long did it take and how did you define the hyperparameters? When I run DIGITS without a pretrained model, no accuracy at all is achieved.Thank you!This is very misleading. ""For your convenience, a pre-trained FCN-8s can be downloaded from the public DIGITS model store."" Really? Where? You should edit the article or create a new one instead of having a link for how to obtain the pre-trained model buried in the comments.Powered by Discourse, best viewed with JavaScript enabled"
3697,nvidia-research-at-cvpr-2019,"Originally published at:			NVIDIA Research at CVPR 2019 | NVIDIA Technical Blog
NVIDIA Researchers will present 20 accepted papers and posters, eleven of them orals, at the annual Computer Vision and Pattern Recognition (CVPR) conference, June 16 – 20, in Long Beach, California. Orals Semantic Image Synthesis With Spatially-Adaptive Normalization Taesung Park, Ming-Yu Liu, Ting-Chun Wang, Jun-Yan Zhu Tuesday, June 18, 2019 , 1330 – 1520   Oral 1.2B We propose…Powered by Discourse, best viewed with JavaScript enabled"
3698,build-high-performance-robotic-applications-with-nvidia-isaac-ros-developer-preview-3,"Originally published at:			https://developer.nvidia.com/blog/build-high-performance-robotic-applications-with-nvidia-isaac-ros-developer-preview-3/
NVIDIA Isaac ROS DP3 includes major updates and enhancements, enabling the ROS community to benefit from hardware acceleration.Powered by Discourse, best viewed with JavaScript enabled"
3699,take-ai-learning-to-the-edge-with-nvidia-jetson,"Originally published at:			https://developer.nvidia.com/blog/take-ai-learning-to-the-edge-with-jetson/
The NVIDIA Jetson Orin Nano and Jetson AGX Orin Developer Kits are now available at a discount for qualified students, educators, and researchers.Since its initial release almost 10 years ago, the NVIDIA Jetson platform has set the global standard for embedded computing and edge AI. These high-performance, low-power modules and developer kits for deep learning…Is the education discount available in Canada?You should be able to get the discount in Canada from this link:NVIDIA offers discounts on Jetson Orin Nano Developer Kit and Jetson AGX Orin Developer Kit to the students. If you are educator, then contact us for multiple Jetson Developer Kits.This will be charged in USD, but should work for you. Thanks!Powered by Discourse, best viewed with JavaScript enabled"
3700,near-range-obstacle-perception-with-early-grid-fusion,"Originally published at:			https://developer.nvidia.com/blog/near-range-obstacle-perception-with-early-grid-fusion/
Automatic parking assist must overcome some unique challenges when perceiving obstacles. An ego vehicle contains sensors that perceive the environment around the vehicle. During parking, the ego vehicle must be close to dynamic obstacles like pedestrians and other vehicles, as well as static obstacles such as pillars and poles. To fit into the parking spot,…Powered by Discourse, best viewed with JavaScript enabled"
3701,nvidia-ampere-architecture-in-depth,"Originally published at:			https://developer.nvidia.com/blog/nvidia-ampere-architecture-in-depth/
Today, during the 2020 NVIDIA GTC keynote address, NVIDIA founder and CEO Jensen Huang introduced the new NVIDIA A100 GPU based on the new NVIDIA Ampere GPU architecture. This post gives you a look inside the new A100 GPU, and describes important new features of NVIDIA Ampere architecture GPUs.  The diversity of compute-intensive applications running…Powered by Discourse, best viewed with JavaScript enabled"
3702,nvidia-turing-architecture-in-depth,"Originally published at:			https://developer.nvidia.com/blog/nvidia-turing-architecture-in-depth/
Fueled by the ongoing growth of the gaming market and its insatiable demand for better 3D graphics, NVIDIA® has evolved the GPU into the world’s leading parallel processing engine for many computationally-intensive applications. In addition to rendering highly realistic and immersive 3D games, NVIDIA GPUs also accelerate content creation workflows, high performance computing (HPC) and…When you count ""CUDA cores"" in new GPUs, it's funny that you include only FP32 cores there. I think that you are better say about count of FP32 cores (even if previous gpus had fp/int cores)Regarding DLSS: in the Turing Whitepaper DLSS is described as:""... allows faster rendering at a lower input sample count...""But in the text in this blog entry, DLSS is described as:""... allows faster rendering at a lower input resolution...""The two aren't equivalent, so which one is correct? The whitepaper or this blog entry?Thanks for spotting the discrepency. The text in the white paper is the correct one; I've updated the blog post to reflect that.Here is what I want to know quite badly.  The RT core is really interesting to me, I think it I could access it at a lower level than the high level RT functions but use the underlying ISA like was done to create the pipeline functions, I could do some amazing things with it.  Given what I think the functionality might be based on discussions and documents.  So the question is, what level of underlying functionality of the ASIC will we have access to?  I am seeing high level API's but nothing PTX or Assembly, are there low level possibilities to use that silicon outside of ray tracing but more computationally like possibly spacial and such?Thanks for correcting the blog post so it's consistent with the whitepaper.There seems to be a lot of conflation between DLSS and AI-UpRes/SuperRes on social media, a lot of people seem to consider DLSS as rendering at a lower resolution than the game settings and then upscaling this to the target resolution set by the game. Is this how DLSS works, ie. rendering at a lower resolution?The links to Facebook, Twitter, email, etc. form a vertical bar on the left bottom of the screen which obscures the text that is under it leaving only a small window at the top left where the whole lines of the article appear.  This inhibited me from reading it.Hi, Don:What's your screen resolution? Also, if you shrink the browser window (drag the right side of the window to the left), the icons disappear completely. What browser are you running?If you're on mobile, the icons should only be at the very bottom.Waterfox on Win 7 laptop. Screen is 1366 x 768 and the browser window is about 90% in each direction.  Will try shrinking it.Yeah, shrinking the window from the right side made the icons disappear. Thanks.Can someone tell me what is Turing's Peak FP64 performance? I can't find this info in the whitepaper as usual (the comparison-with-previous-gen table to be specific).It's mentioned in a footnote under the full Turing block diagram as 1/32 the FP32 rate.Hi , would be great if i can get a clear answer regarding the memory and resource pooling in RTX 2080ti/2080 with NVLINK. Are they implemented to be functioning the same as their Quadro counterpart or are they non existent (essentially just an SLI bridge with x times the amount of speed) ?Powered by Discourse, best viewed with JavaScript enabled"
3703,scaling-out-the-deep-learning-cloud-efficiently,"Originally published at:			https://developer.nvidia.com/blog/scaling-out-the-deep-learning-cloud-efficiently/
The Duchess of Windsor famously said that you can never be too rich or too thin. A similar observation is true when trying to match deep learning applications and compute resources: You can never have too much horsepower. Intractable problems in fields as diverse as finance, security, medical research, resource exploration, self-driving vehicles, and defense…Powered by Discourse, best viewed with JavaScript enabled"
3704,smarter-retail-data-analytics-with-gpu-accelerated-apache-spark-workloads-on-google-cloud-dataproc,"Originally published at:			https://developer.nvidia.com/blog/smarter-retail-data-analytics-with-gpu-accelerated-apache-spark-workloads-on-google-cloud-dataproc/
A retailer’s supply chain includes the sourcing of raw materials or finished goods from suppliers; storing them in warehouses or distribution centers; and transporting them to stores or customers; managing sales. They also collect, store, and analyze data to optimize supply chain performance. Retailers have teams responsible for managing each stage of the supply chain,…Powered by Discourse, best viewed with JavaScript enabled"
3705,for-the-first-time-ai-probes-dark-matter-in-the-universe,"Originally published at:			For the First Time, AI Probes Dark Matter in the Universe | NVIDIA Technical Blog
How much dark matter is there in the universe? This AI model might have the answer. A team of physicists and computer scientists at ETH Zurich developed a deep learning-based model to estimate the amount of dark matter in the universe. This is the first time AI researchers have used this type of algorithm to analyze…Powered by Discourse, best viewed with JavaScript enabled"
3706,nvidia-research-transferring-dexterous-manipulation-from-gpu-simulation-to-a-remote-real-world-trifinger-task,"Originally published at:			https://developer.nvidia.com/blog/nvidia-research-transferring-dexterous-manipulation-from-gpu-simulation-to-a-remote-real-world-trifinger-task/
●	Large-scale GPU-based simulation can enable robot learning in simulation, and such solutions can be transferred to real robots without the need for physical access to the robots. This work combines the benefits of IsaacGym-based reinforcement learning for a three-finger in-hand manipulation task and transfers the learned policy to a remote real-world counterpart.The future just arrived early - this latest breakthrough shows that large-scale GPU-based simulation enabling robots to learn can then be transferred to real robots without the need for physical access to the robots. Tell us how this helps with your research.Powered by Discourse, best viewed with JavaScript enabled"
3707,inception-spotlight-ai-startup-neurala-sees-7x-speedup-with-ngc,"Originally published at:			Inception Spotlight: AI Startup Neurala Sees 7X Speedup with NGC | NVIDIA Technical Blog
To help businesses develop custom computer vision solutions quickly, Neurala, a member of NVIDIA’s start-up incubator Inception, has developed Brain Builder, a cloud platform that provides data scientists and developers that are new to deep learning with the ability to quickly and easily train neural networks.  “Neural is an an end-to-end SaaS platform to streamline…Powered by Discourse, best viewed with JavaScript enabled"
3708,deploying-diverse-ai-model-categories-from-public-model-zoo-using-nvidia-triton-inference-server,"Originally published at:			https://developer.nvidia.com/blog/deploying-diverse-ai-model-categories-from-public-model-zoo-using-nvidia-triton-inference-server/
Nowadays, a huge number of implementations of state-of-the-art (SOTA) models and modeling solutions are present for different frameworks like TensorFlow, ONNX, PyTorch, Keras, MXNet, and so on. These models can be used for out-of-the-box inference if you are interested in categories already in the datasets, or they can be embedded to custom business scenarios with…We hope you find this post a helpful starting point for AI model inference. If you have any questions or comments, let us knowGreat blog post!  I wanted to get the example code, but the link in the post appears to be broken.  How can I get the code?Thank you for the feedback. The code should be accessible now; please check it again.Powered by Discourse, best viewed with JavaScript enabled"
3709,federated-learning-powered-by-nvidia-clara,"Originally published at:			Federated Learning powered by NVIDIA Clara | NVIDIA Technical Blog
AI requires massive amounts of data. This is particularly true for industries such as healthcare. For example, training an automatic tumor diagnostic system often requires a large database in order to capture the full spectrum of possible anatomies and pathological patterns.   In order to build robust AI algorithms, hospitals and medical institutions often need…Powered by Discourse, best viewed with JavaScript enabled"
3710,making-a-plotly-dash-census-viz-powered-by-rapids,"Originally published at:			https://developer.nvidia.com/blog/making-a-plotly-dash-census-viz-powered-by-rapids/
TL; DR The use of Plotly’s Dash, RAPIDS, and Data shader allows users to build viz dashboards that both render datasets of 300 million+ rows and remain highly interactive without the need for precomputed aggregations. Using RAPIDS cuDF and Plotly Dash for real-time, interactive visual analytics on GPUs Dash is an open-source framework from Plotly…Powered by Discourse, best viewed with JavaScript enabled"
3711,end-to-end-ai-for-nvidia-based-pcs-nvidia-tensorrt-deployment,"Originally published at:			https://developer.nvidia.com/blog/end-to-end-ai-for-nvidia-based-pcs-nvidia-tensorrt-deployment/
This post is the fifth in a series about optimizing end-to-end AI. NVIDIA TensorRT is a solution for speed-of-light inference deployment on NVIDIA hardware. Provided with an AI model architecture, TensorRT can be used pre-deployment to run an excessive search for the most efficient execution strategy. TensorRT optimizations include reordering operations in a graph, optimizing…Powered by Discourse, best viewed with JavaScript enabled"
3712,nvidia-announces-nsight-graphics-2019-5,"Originally published at:			NVIDIA Announces Nsight Graphics 2019.5 | NVIDIA Technical Blog
NVIDIA launched Nsight Graphics 2019.5 with improved performance and resolved bug fixes to enhance the quality of the developer’s profiling and debugging experience.Powered by Discourse, best viewed with JavaScript enabled"
3713,win-up-to-10-000-in-the-nvidia-jetson-developer-challenge,"Originally published at:			Win up to $10,000 in the NVIDIA Jetson Developer Challenge | NVIDIA Technical Blog
Calling all great developers, engineers, scientists, startups, and students! NVIDIA is challenging you to show us how you can transform robotics, industrial IoT, healthcare, security, or any other industry with a powerful AI solution built on the NVIDIA Jetson platform. You’ll not only get the chance to win amazing prizes, but also a trip to present your…Powered by Discourse, best viewed with JavaScript enabled"
3714,startup-uses-ai-to-design-an-ultimate-aerodynamic-bike,"Originally published at:			https://developer.nvidia.com/blog/startup-uses-ai-to-design-an-ultimate-aerodynamic-bike/
At Supercomputing 2018 in Dallas, Texas, Swiss-based startup Neural Concept displayed their ultra-aerodynamic bike. The bike was developed using the company’s cloud-based machine learning software that leverages the power of NVIDIA GPUs. “Our program results in designs that are sometimes 5–20% more aerodynamic than conventional methods. But even more importantly, it can be used in…Powered by Discourse, best viewed with JavaScript enabled"
3715,getting-started-with-openacc,"Originally published at:			https://developer.nvidia.com/blog/getting-started-openacc/
This week NVIDIA has released the NVIDIA OpenACC Toolkit, a starting point for anyone interested in using OpenACC. OpenACC gives scientists and researchers a simple and powerful way to accelerate scientific computing without significant programming effort. The toolkit includes the PGI OpenACC Compiler, the NVIDIA Visual Profiler with CPU and GPU profiling, and the new OpenACC Programming…Obvious question, but will the toolkit be coming to Windows?Hi Mark. We don't have any immediate plans to release the toolkit for Windows, but we will definitely look into what it'd take to do so. Thanks for asking.thanks for OpenACC lessons! i find them a great way to learn new toolthere a few mistakes in published code:1. #pragma acc data copyin(A) create(Anew)- it should be copy(A) in both examples as stated in the description2. first example uses correct ""Anew[i,j] = f(A[i,j])"" code in the first loop while remaining examples use incorrect ""A[i,j] = f(Anew[i,j])""3. can you try to further optimize the code by using two loops: first one computes ""Anew[i,j] = f(A[i,j])"" and second one computes ""A[i,j] = f(Anew[i,j])"" - this way you can eliminate copying?Thanks for catching those and commenting @bulatziganshin:disqus. I've fixed the first two in the post. You're correct that there are other ways to do this operation that avoid the need to do the copy in the second loop nest. I use this code because it's simple to understand and show, but I may pursue changing the code in the future to avoid making the copy, as this is what most applications would do in production.I am wondering if I can run the OpenACC Toolkit on all NVIDIA GPUs or only on those that have a Kepler or Fermi architecture? For instance, can I use the toolkit with my NVIDIA GeForce GTX 750 Ti? This card comes with a Maxwell architecture and PGI's Cuda Fortran and their OpenACC did not run on it.There is anyway to use OpenACC with VM under a Windows host?Hello, has anything changed with regards to openacc toolkit for windows? I have read that the PGI compiler is compatible with Windows, however the toolkit is not?I just reached out to the team and there's still no Windows version of the toolkit. The Windows version of PGI does support OpenACC, however, and all of the bonus examples and documentation from the Linux version could be downloaded separately. You should definitely try downloading PGI for Windows with a trial license and if you need an extended trial, please reach out to the PGI support about this.Powered by Discourse, best viewed with JavaScript enabled"
3716,startup-builds-ai-system-to-transcribe-meetings,"Originally published at:			https://developer.nvidia.com/blog/startup-builds-ai-system-to-transcribe-meetings/
Voicea, a San Francisco Bay Area startup, recently announced $20 million funding for their GPU-based deep learning system that can now fully transcribe meetings and put together highlights. The system was designed to help teams better collaborate in an enterprise environment. Eva, the start-ups AI assistant, joins meetings and conference calls through a combination of…Powered by Discourse, best viewed with JavaScript enabled"
3717,ai-helps-improve-the-dexterity-of-robots,"Originally published at:			AI Helps Improve the Dexterity of Robots | NVIDIA Technical Blog
Developers from the California-based non-profit OpenAI announced today they trained a deep learning system that can grasp and manipulate real-world objects with remarkable dexterity. “While dexterous manipulation of objects is a fundamental everyday task for humans, it is still challenging for autonomous robots,” the developers stated in their research paper. In the work, the team…Powered by Discourse, best viewed with JavaScript enabled"
3718,pro-tip-pinpointing-runtime-errors-in-cuda-fortran,"Originally published at:			Pro Tip: Pinpointing Runtime Errors in CUDA Fortran | NVIDIA Technical Blog
CUDA Fortran for Scientists and Engineers shows how high-performance application developers can leverage the power of GPUs using Fortran. We’ve all been there. Your CUDA Fortran code is humming along and suddenly you get a runtime error: copyin, copyout, usually accompanied by FAILED in all caps.  In many cases, the error message gives you enough information…I found that if you turn on optimization with -fast flag, those detailed error outputs are gone. What I got are:0: copyin Memcpy (dev=0x(nil), host=0x0x7f80c2e8b230, size=18446744073709550692) FAILED: 11(invalid argument)Error: segmentation violation, address not mapped to objectHi Xinsheng:Thanks for catching this.  You are correct, in fact when using any optimization of -O2 or higher you also need to compile with -g to get the traceback.  I've modified the text (including the example) to reflect this.Thanks again,GregPowered by Discourse, best viewed with JavaScript enabled"
3719,nvidia-merlin-extends-open-source-interoperability-for-recommender-workflows-with-latest-update,"Originally published at:			NVIDIA Merlin Extends Open Source Interoperability for Recommender Workflows with Latest Update | NVIDIA Technical Blog
Check out NVIDIA Merlin’s latest updates including Transformers4Rec and SparseOperations Kit.Powered by Discourse, best viewed with JavaScript enabled"
3720,overcoming-data-collection-and-augmentation-roadblocks-with-nvidia-tao-toolkit-and-appen-data-annotation-platform,"Originally published at:			https://developer.nvidia.com/blog/overcoming-data-collection-and-augmentation-roadblocks-with-tao-toolkit-and-appen-data-annotation-platform/
Generating and labeling data to train AI models is time-consuming. Appen, helps label and annotate your data, which can then be used as inputs in the TAO Toolkit.Powered by Discourse, best viewed with JavaScript enabled"
3721,developing-and-deploying-your-custom-action-recognition-application-without-any-ai-expertise-using-nvidia-tao-and-nvidia-deepstream,"Originally published at:			https://developer.nvidia.com/blog/developing-and-deploying-your-custom-action-recognition-application-without-any-ai-expertise-using-tao-and-deepstream/
Build an action recognition app with pretrained models, the TAO Toolkit, and DeepStream without large training data sets or deep AI expertise.Hello,
I was trying to recreate the pre-trained model. I trained the net with the same 5 classes  (ride bike, walk, fall floor, run and push) and the default hyperparameters. Then I use this model as a pre-trained model and train again with different classes. However, the accuracy and F1 I get with my recreated model don’t even come close to the ones I get with the downloaded pre-trained model. I thought this could be cause the hyperparameters used to train the model were different from the ones I used, but I couldn’t find any information on how the pre-trained model was trained.
I would highly appreciate if anyone could provide me with this information. The hyperparameters I am referring to are the ones in the train_rgb_3d_finetune.yaml file, such as the learning rate,  epochs, momentum,  weight_decay, dropout ratio, steps, batch size etc…Thank you!Thanks for a fine guide on how to use TAO.I am using my own data to train a model with 2 categories, but the pictures a full HD - and a 12 picture sequence from each video.When I try to train the model I get an error :“ERROR: Unexpected bus error encountered in worker. This might be caused by insufficient shared memory (shm).”It looks like the docker image needs extra memory. Can you please advice me on how to do that?Thanks,
AndersHi, I am trying run deepstream-3d-action-recognition-app on jetson NX and it running out of memory. May this app can not run on jetson nx right?Hi @samvdh - Can you share mode details about your setup, please?ThanksHi, my issue is temporary solved at this topic Deepstream 3d action recognition app leak memory on jetson nx - #19 by samvdh. Thank you for your support.@samvdh  – Great to hear! Let us know if you have any other questions…How to collect sequence of tracked detected humans (after human detection (pgie) followed by tracker) to pass it as an input to an action classifier (sgie) ? The task is action detection of individual human beings appearing in the frame.Powered by Discourse, best viewed with JavaScript enabled"
3722,nvidia-gtc-top-conversational-ai-recommender-systems-and-video-conferencing-sessions,"Originally published at:			https://developer.nvidia.com/blog/nvidia-gtc-top-conversational-ai-recommender-systems-and-video-conferencing-sessions/
Join NVIDIA GTC for sessions covering the latest breakthroughs in conversational AI, recommender systems, and video conferencing.Powered by Discourse, best viewed with JavaScript enabled"
3723,top-5-higher-education-sessions-at-gtc-2019,"Originally published at:			Top 5 Higher Education Sessions at GTC 2019 | NVIDIA Technical Blog
NVIDIA’s GPU Technology Conference (GTC) is the premier AI and deep learning conference, providing training, insights, and direct access to experts from leading research institutions and national labs. Explore hundreds of sessions on cutting edge AI research and applications across industries. In this video below, see a preview of some of the higher education sessions…Powered by Discourse, best viewed with JavaScript enabled"
3724,monitoring-gpus-in-kubernetes-with-dcgm,"Originally published at:			https://developer.nvidia.com/blog/monitoring-gpus-in-kubernetes-with-dcgm/
Monitoring GPUs is critical for infrastructure or site reliability engineering (SRE) teams who manage large-scale GPU clusters for AI or HPC workloads. GPU metrics allow teams to understand workload behavior and thus optimize resource allocation and utilization, diagnose anomalies, and increase overall data center efficiency. Apart from infrastructure teams, you might also be interested in…Hi everyone,We look forward to hearing about your current monitoring solutions for GPUs in Kubernetes and look forward to your feedback on using DCGM!Thanks to nice post  :)
I tried to use dcgm exporter in kubernetes cluster (with version 1.15) to know ‘total gpu resource requests from pods and which pod is using GPU’.
But, our cluster use GTX and RTX series, so dcgm exporter said ’ Profiling is not supported for this group of GPUs or GPU’ .
Do you have any idea to use dcgm for GTX or RTX series? or plan to support these GPUs?Thanks for readingHi ydh0924,Thanks for reading the blog. Unfortunately, only the profiling metrics are limited to datacenter (previously “Tesla”) branded GPUs such as A100, V100 and T4. However, you can still use dcgm-exporter to get access to other GPU telemetry on GTX and RTX series. For this, you will have to override the ‘arguments’ variable and set it to nil in the Helm chart during installation. For e.g or you can modify values.yaml directly in the Helm chart.Doing so, will allow dcgm-exporter to give you all the GPU telemetry (just not the profiling metrics) and not result in an error during startup.
Hope that helps!Thanks for reply !!
it works in my prometheus  now 😁Hi,Just installed DCGM on linux server for two GPU servers but I do not see instance drop down in Grafana to select GPU server1 or GPU server2. How can I do that? I do see data for both servers being scraped.Thanks,AnilHello. This column explains how to use dcgm to show the usage of gpus allocated to each node.
But is there a way to know how much each gpu is used by the pod it is assigned to?
For example, Node A is using 4 GPUs.
Pod A and Pod B each use Node A’s 2 GPUs.
At this time, I want to know the GPU usage of Pod A. If PodA has 100% GPU usage and PodA has 0 usage, then the total GPU usage on NodeA will be 50%.
Can I know the gpu usage of each pod to which the gpu is allocated?Hi @user109130dcgm-exporter takes advantage of the pod resources API provided by the kubelet to associate pods to specific devices  (or resources) assigned to the pods. You can read more about this API (which was graduated to GA in v1.20:
https://kubernetes.io/blog/2020/12/16/third-party-device-metrics-reaches-ga/If you use the default daemonset provided for dcgm-exporter, then you will observe that we volume-mount pod-resources to access this API capability:Once you deploy dcgm-exporter, you should then be able to see exactly which GPU is assigned to the pod and thus metrics for that specific GPU (even if a node has multiple GPUs). We also emit the node name as part of the dcgm-exporter output to make it easy for scrapers to gather information on exactly which GPUs within that node were assigned to pods and metrics for those. An example dcgm-exporter output is shown below where we print the GPU UUID, the hostname, and power usage of the GPUs assigned to the pod:DCGM_FI_DEV_POWER_USAGE{gpu=""0"",UUID=""GPU-f2767511-aa82-fbeb-7f4b-9a1bec879e78"",device=""nvidia0"",modelName=""Tesla V100-SXM2-16GB"",Hostname=""dcgm-exporter-1641419650-jxqn2"",container=""dcgmproftester11"",namespace=""default"",pod=""dcgmproftester""} 24.523000Please use the latest release of the dcgm-exporter: 2.3.1-2.6.1 as it contains the most recent new features and bug fixes.Thanks for reading our blog post!Powered by Discourse, best viewed with JavaScript enabled"
3725,time-magazine-calls-nvidia-omniverse-one-of-year-s-100-best-inventions,"Originally published at:			TIME Magazine Calls NVIDIA Omniverse One of Year’s 100 Best Inventions | NVIDIA Technical Blog
NVIDIA Omniverse, a simulation and design collaboration platform for 3D virtual worlds, is already being evaluated by 700+ companies and 70,000 individuals.Powered by Discourse, best viewed with JavaScript enabled"
3726,deploying-gpt-j-and-t5-with-fastertransformer-and-triton-inference-server,"Originally published at:			Deploying GPT-J and T5 with NVIDIA Triton Inference Server | NVIDIA Technical Blog
Learn step by step how to use the FasterTransformer library and Triton Inference Server to serve T5-3B and GPT-J 6B models in an optimal manner with tensor parallelism.hi @jwitsoe ,
I am from the Chinese developer community.There seems to be a picture mismatch in the results section of the article. Figure 5 should be T5-3B model inference speed-up comparison, but it shows GPT-J 6B.
微信截图_20220812082056729×567 54.1 KB
@kun.he.love.u Thanks for letting us know. We’ve updated the image.Can you please provide similar step by step guide for multi node inference example with Triton server?wow, I was waiting for such a guide for a long. Waiting for smth like that with Trition server as was asked above by aneesinaec.I tried similar exercise with a bloom model.
I have 2 GPU’s with ~10 GB Memory on each.  While trying to load a 14GB model in 2 GPU config, I keep getting out  of memory error.Fasttransformer backend supposed to split the 14 GB model in to 2 CPU’s and load…rite? What am i possibly missing here?Thank you for the feedback. We will consider it. You can also refer the document in fastertransformer_backend/t5_guide.md at main · triton-inference-server/fastertransformer_backend · GitHub first.Can you share more details and your steps?Powered by Discourse, best viewed with JavaScript enabled"
3727,new-app-automatically-classifies-your-instagram-photos,"Originally published at:			New App Automatically Classifies Your Instagram Photos | NVIDIA Technical Blog
Have you ever wondered what your photos say about how you look at the world and who you are? Pictograph, developed by a machine intelligence research company, uses deep learning on GPUs in the Amazon cloud to analyze your Instagram photos and creates a visualization of what you like to photograph. The app is ready…It’s a genius idea! I’ve wanted to start a blog about photography and videography for a long time. I’ve created Instagram and TikTok accounts several times, but I haven’t been able to get many followers.Powered by Discourse, best viewed with JavaScript enabled"
3728,getting-kubernetes-ready-for-the-nvidia-a100-gpu-with-multi-instance-gpu,"Originally published at:			https://developer.nvidia.com/blog/getting-kubernetes-ready-for-the-a100-gpu-with-multi-instance-gpu/
Multi-Instance GPU (MIG) is a new feature of the latest generation of NVIDIA GPUs, such as A100. It enables users to maximize the utilization of a single GPU by running multiple GPU workloads concurrently as if there were multiple smaller GPUs. MIG supports running multiple workloads in parallel on a single A100 GPU or allowing…Hello,
I tried the instructions in the blog, it failed at the command:
‘sudo helm install --version=0.13.0-rc.2 --generate-name --set migStrategy=none nvdp/k8s-device-plugin’
I have 0.13.0.-rc.2 devic-plugin, but it reported the following error:
‘INSTALLATION FAILED: chart “k8s-device-plugin” matching 0.13.0-rc.2 not found in nvdp index’how to continue from here?thanks,Hello,
NVIDIA has changed the version names recently - please follow this guideNVIDIA device plugin for Kubernetes. Contribute to NVIDIA/k8s-device-plugin development by creating an account on GitHub.to install the current latest version (v0.12.3)
Thanks!The 0.13.0-rc.2 version is a release candidate. The full version of 0.13.0 will be released in the next couple of weeks.If you really want to run with 0.13.0-rc.2, then you can just add --devel to your helm command line so it can find release candidate versions like this.I got it run with GPU-operator. It works to remotely config MIG. I only applied ‘single’ policy for now.
thanks,Powered by Discourse, best viewed with JavaScript enabled"
3729,insertion-sort-explained-a-data-scientists-algorithm-guide,"Originally published at:			https://developer.nvidia.com/blog/insertion-sort-explained-a-data-scientists-algorithm-guide/
Learn a step-by-step breakdown of sorting algorithms. A fundamental tools used in data science.Powered by Discourse, best viewed with JavaScript enabled"
3730,nvidia-ai-platform-delivers-big-gains-for-large-language-models,"Originally published at:			https://developer.nvidia.com/blog/nvidia-ai-platform-delivers-big-gains-for-large-language-models/
NVIDIA AI platform makes LLMs accessible. Announcing new parallelism techniques and a hyperparameter tool to speed-up training by 30% on any number of GPUs.Powered by Discourse, best viewed with JavaScript enabled"
3731,build-and-deploy-ai-models-using-nvidia-deepstream-on-jetson-and-aws-iot-core,"Originally published at:			Build and Deploy AI Models Using NVIDIA DeepStream on Jetson and AWS IoT Core | NVIDIA Technical Blog
Edge computing is essential for processing and generating real-time insights from billions of sensors around the world. Pixels from cameras are converted to actionable insights using AI on the edge. Intelligent video analytics technology is used to aid traffic management for cities, automated checkout in retail stores, non-contact automated visual inspection in manufacturing facilities, or…Powered by Discourse, best viewed with JavaScript enabled"
3732,see-the-future-of-ai-in-public-sector-at-gtc,"Originally published at:			See the Future of AI in Public Sector at GTC | NVIDIA Technical Blog
Connect online with our developer community, partner ecosystem, and the brightest minds in the industry at GTC this fall to learn about the latest innovations and techniques for AI, computer vision, deep learning, data science, simulation, training, HPC, and more.   Make sure to catch the many Deep Learning Institute trainings being offered as GTC this…Powered by Discourse, best viewed with JavaScript enabled"
3733,creating-realistic-materials-for-the-hyundai-genesis,"Originally published at:			https://developer.nvidia.com/blog/creating-realistic-materials-for-the-hyundai-genesis/
At the 2016 GPU Technology Conference (GTC), Hyundai presented how they used GPU-accelerated design software to create a full set of virtual materials for the exterior and interior of the Hyundai Genesis G380. David Nikel, Digital Model Manager at Hyundai California Design Studio, explained the challenges involved in creating and refining new realistic materials, and…Powered by Discourse, best viewed with JavaScript enabled"
3734,low-power-sensing-and-autonomy-with-nvidia-jetson-tk1,"Originally published at:			https://developer.nvidia.com/blog/low-power-sensing-autonomy-nvidia-jetson-tk1/
Figure 1: simple TK1 block diagram NVIDIA’s Tegra K1 (TK1) is the first ARM system-on-chip (SoC) with integrated CUDA.  With 192 Kepler GPU cores and four ARM Cortex-A15 cores delivering a total of 327 GFLOPS of compute performance, TK1 has the capacity to process lots of data with CUDA while typically drawing less than 6W…Great article Dustin.  Great tips on setting up the hardware through the OS on the device.""the video stream is depacketized by the CPU into a buffer shared with the GPU,""could you please provide a link to an introduction to implementation?ie how does one actually get this done?Here's a link to an example from the CUDA C Programming Guide v6 on using unified managed memory:  http://docs.nvidia.com/cuda...Anyways in the case of networking sockets, one would use cudaMallocManaged() to allocate the buffer passed to recv()/read() before calling CUDA kernel.   Simplified psuedocode:void* my_image = NULL;cudaMallocManaged(&my_image, size_of_image);read(my_socket, my_image, size_of_image);my_cuda_kernel<<<...>>>(my_image);Hi Dustin,Great article, i am using TK1, its a wonderful board.I was trying to get VisionWorks kit, but i cant find its download link any where, can you tell where can i get it?Hi,thanks for the nice article... I'm interested mainly in the second case study: ""Tiled Tegra""; could you give some more information about the hardware you used to connect the boards? Switches and cables to do that are already available on the market or you had to implement custom hardware? Could you provide a kind of ""shopping list"" to reproduce that configuration?Thanks in advance and Best RegardsHi Dustin,As one of guys have mentioned before, I also have interests in Tiled Tegra and try to find necessary pcie switch and mini-pcie to pcie adapters. However, I cannot sure what sorts of those components I have to purchase in order to implement the Tiled Tegra.Is it possible for you to let us know what PCIE switch or adapters you used for your Tiled Tegra ?Best,Powered by Discourse, best viewed with JavaScript enabled"
3735,building-transcription-and-entity-recognition-apps-using-nvidia-jarvis,"Originally published at:			https://developer.nvidia.com/blog/building-transcription-and-entity-recognition-apps-using-jarvis/
In the past several months, many of us have grown accustomed to seeing our doctors over a video call. It’s certainly convenient, but after the call ends, those important pieces of advice from your doctor start to slip away. What was that new medication I needed to take? Were there any side effects to watch…Can sample rate for Jarvis ASR be 8000hz?Thanks for your question! Yes, Jarvis supports 8 kHz input for ASR. The server will automatically upsample to 16 kHz. Just make sure you specify the sample rate as 8000 in the recognitionConfig being sent in the request.@jwitsoe
Hello
We were able to run the rive docker client service
However how exactly to replicate the demo in the article https://developer.nvidia.com/blog/building-transcription-and-entity-recognition-apps-using-riva/#entry-content-comments ?Thanks for your interest in the demo. Glad to hear you’ve been able to run the docker client. We have included instructions on running that demo from the docker here: Riva Contact — NVIDIA RivaCheers,
Chris@cparisien
Hi, thank you for your reply
Could you extend on how to start the callcenter with Medical NER German model, please?
AVHi @Andrey1984,We don’t have a medical NER model in German, and unfortunately I’m not immediately aware of a good dataset you could use. A quick search turned up this one, which might be worth a try – I can’t comment on how well it might suit your application, and you’d likely have to do some data preprocessing.The current state of adoption of well-structured electronic health records
and integration of digital methods for storing medical patient data in
structured formats can often considered as inferior compared to the use of
traditional, unstructured...If you do find a suitable dataset, you could use TAO to train or adapt an appropriate model:
https://docs.nvidia.com/tao/tao-toolkit/text/nlp/token_classification.htmlThen once you have that model, you can deploy it in Riva:
https://docs.nvidia.com/deeplearning/riva/user-guide/docs/custom-model-deployment.htmlI hope that helps!ChrisThis topic was automatically closed 14 days after the last reply. New replies are no longer allowed.@cparisien
Hi, so what are the existing options for  german ASR?
Is there more support by now for the Medical NER ?
How do we integrate GitHub - frankkramer-lab/GERNERMED: GERNERMED is the first open neural NER model for medical entities designed for German data. ?
Could you extend on steps to get it as a proof of concept implemented by using “TAO to train or adapt an appropriate model”, please?
Thanks
AVWe do have some updates here:I hope that helps!
ChrisI was looking to replicate this demo (https://developer.nvidia.com/blog/building-transcription-and-entity-recognition-apps-using-riva/) which led me to this forum entry.  The solution link in this chat is trying to point to a “callcenter.html” demo.  After some searching I realized the demo has been renamed to “Riva Contact” (Riva Contact — NVIDIA Riva).  Hope this helps someone.@svaha  – Link fixed, thank you!Powered by Discourse, best viewed with JavaScript enabled"
3736,nvidia-parabricks-accelerating-genomic-analysis-from-days-to-an-hour,"Originally published at:			NVIDIA Parabricks: Accelerating Genomic Analysis from Days to an Hour | NVIDIA Technical Blog
Sequencing costs have plummeted with the help of new medical instruments:  almost every 6 months we see new instrument with higher throughput like BGI’s DNBSEQ-T7 at 60 genomes per day. At the world’s rate of sequencing, we’ll generate 20 ExaBytes of data by 2025 – more than Twitter, Youtube and astronomy combined. In fact, it…Powered by Discourse, best viewed with JavaScript enabled"
3737,speech-recognition-customizing-models-to-your-domain-using-transfer-learning,"Originally published at:			https://developer.nvidia.com/blog/speech-recognition-customizing-models-to-your-domain-using-transfer-learning/
Creating a new AI/DL model is a resource-intensive process. The NVIDIA TAO Toolkit can cut that time from 80 weeks to 8, using transfer learning.Powered by Discourse, best viewed with JavaScript enabled"
3738,improving-gpu-memory-oversubscription-performance,"Originally published at:			https://developer.nvidia.com/blog/improving-gpu-memory-oversubscription-performance/
Since its introduction more than 7 years ago, the CUDA Unified Memory programming model has kept gaining popularity among developers. Unified Memory provides a simple interface for prototyping GPU applications without manually migrating memory between host and device. Starting from the NVIDIA Pascal GPU architecture, Unified Memory enabled applications to use all available CPU and…where is the code example?You can find the code example here . Thank you for your interest.yw, I downloaded and updated to the latst Commit: 0754981b37b343474c45222ea487c9667551e854 [0754981] of master branch.
But could not find any project files, how to debug .cu file using window 10& VS2019?
Commit: 0754981b37b343474c45222ea487c9667551e854 [0754981]
Parents: 9ad4c010fd, 5a003551a1
Author: Mark Harris mharris@nvidia.com
Date: Tuesday, August 3, 2021 2:44:36 PM
Committer: GitHub
Merge pull request #36 from chirayuG-nvidia/unified_memoryAdd Unified Memory oversubscription benchmarkApologies for the delay in the response.
We don’t have visual studio project files for this sample, I think it should be easy to convert to a VS project from the provided Makefile. Worth mentioning that many of the Unified Memory features such as on-demand paging, oversubscription discussed here are not available on Windows, these limitation are documented here.Powered by Discourse, best viewed with JavaScript enabled"
3739,predicting-metastatic-cancer-risk-with-ai,"Originally published at:			https://developer.nvidia.com/blog/predicting-metastatic-cancer-risk-with-ai/
Using movies of living cancer cells, scientists create a convolutional neural network that can identify and predict aggressive metastatic melanomas.Powered by Discourse, best viewed with JavaScript enabled"
3740,ai-app-predicts-the-popularity-of-social-media-posts,"Originally published at:			AI App Predicts the Popularity of Social Media Posts | NVIDIA Technical Blog
Cornea AI takes the guesswork out of photo sharing by using deep learning to predict how well your photo will do on social platforms. Users can either take a photo, or import one from their gallery and the AI will score the photo — it then suggests popular filters and hashtags to increase the popularity…While I believe quality content and genuine engagement are more important than simply amassing a large following, building a social media presence can be challenging, and any little help can be appreciated.Powered by Discourse, best viewed with JavaScript enabled"
3741,inception-spotlight-datavisiooh-uses-ai-to-measure-outdoor-advertising-in-real-time,"Originally published at:			Inception Spotlight: DataVisiooh Uses AI to Measure Outdoor Advertising in Real-Time | NVIDIA Technical Blog
To measure the impact of outdoor advertising, Brazil-based DataVisiooh, a member of NVIDIA Inception, developed an AI solution to capture performance analytics in real-time.  “Our platform goes beyond simply counting the flow of people and vehicles and shows advanced data such as demographic information (gender, age group), screen viewing time, exposure time, and even how…Powered by Discourse, best viewed with JavaScript enabled"
3742,enabling-dynamic-control-flow-in-cuda-graphs-with-device-graph-launch,"Originally published at:			https://developer.nvidia.com/blog/enabling-dynamic-control-flow-in-cuda-graphs-with-device-graph-launch/
CUDA device graph launch offers a performant way to enable dynamic control flow within CUDA kernels.Hi, from the article it’s not clear but I suppose we cannot update the graph on device, am I right?
I mean, changing parameters like the number of threads with cudaGraphExecKernelNodeSetParams() or enable/device kernels with cudaGraphNodeSetEnabled() for exampleFrom the CUDA toolkit documentation the “cudaGraphLaunch” is still tagged as ""__host__​ "", is it a mistake? It should be possible to use that function on device nowLast question is about cudaGraphLaunch performances, with “graph length=100” do you mean you’re testing a graph with 100kernels or a graph with 100 sequential kernels?
I mean, if I have 10 straight lines with 10 sequential kernels… it’s a “graph length” equal to 100 or 10?
ThanksYou cannot update the graph from the device, that is correct. Updates can only be performed from the host (this applies both to parameter updates as well as node enable/disable), and the graph must also be re-uploaded for the changes to take effect in subsequent device launches. We are thinking of adding device-side update functionality in a future release, though.Yes, cudaGraphLaunch can be called from both the host and device. That does indeed appear to be a documentation error, thanks for bringing it to our attention! We’ll work on getting that fixed.The length is the sequential length, not the general size. For the single-entry parallel straight-line and straight-line graphs, that means that the straight-line sections are 100 sequential kernels each; and for the fork join case, it is 100 sequential iterations of forking out into 2 nodes and then joining back into a single node.I hope that answers all your questions.Powered by Discourse, best viewed with JavaScript enabled"
3743,explore-deploying-and-optimizing-industrial-scale-ai-at-gtc,"Originally published at:			https://developer.nvidia.com/blog/explore-deploying-and-optimizing-industrial-scale-ai-at-gtc/
Industrial-Scale AI content is at GTC. From April 12-16, 1,400 live and on-demand sessions will be at your fingertips. Many topics will be covered including solutions in computational fluid dynamics, predictive maintenance, inspection, and factory logistics across Industrial Manufacturing, Aerospace, Oil and Gas, Electronic Design Automation (EDA), Engineering Simulation (CAE), and more.  Free registration provides…Powered by Discourse, best viewed with JavaScript enabled"
3744,upcoming-webinar-accelerate-ai-model-development-with-pytorch-lightning,"Originally published at:			https://developer.nvidia.com/blog/upcoming-webinar-accelerate-ai-model-development-with-pytorch-lightning/
The NGC team is hosting a webinar with live Q&A to dive into how to build AI models using PyTorch Lightening, an AI framework built on top of PyTorch, from the NGC catalog.Powered by Discourse, best viewed with JavaScript enabled"
3745,what-a-deep-neural-network-thinks-about-your-selfie,"Originally published at:			What a Deep Neural Network Thinks About Your Selfie | NVIDIA Technical Blog
Stanford PhD student Andrej Karpathy trained a model overnight on a Tesla K40 to tell you how to take a better selfie photo. Convolutional Neural Networks are great at recognizing things, places and people in your personal photos, crops, traffic, various anomalies in medical images and all kinds of useful things. But once in a…Powered by Discourse, best viewed with JavaScript enabled"
3746,fortune-nvidia-must-be-stoked-this-startup-is-taking-graphics-chips-corporate,"Originally published at:			Fortune: “NVIDIA must be stoked: This startup is taking graphics chips corporate” | NVIDIA Technical Blog
MapD, an NVIDIA-funded startup, has created a new type of database to handle terabytes of data and impressive visualizations that relies on GPUs instead of the traditional AMD or Intel chips. Todd Mostak, Founder and CEO of MapD, decided to build a SQL database that relies on graphics processors when he was completing his thesis…Powered by Discourse, best viewed with JavaScript enabled"
3747,cudnn-v2-higher-performance-for-deep-learning-on-gpus,"Originally published at:			https://developer.nvidia.com/blog/cudnn-v2-higher-performance-deep-learning-gpus/
The cuDNN library team is excited to announce the second version of cuDNN, NVIDIA’s library of GPU-accelerated primitives for deep neural networks (DNNs). We are proud that the cuDNN library has seen broad adoption by the deep learning research community and is now integrated into major deep learning toolkits such as CAFFE, Theano and Torch.…Is there any comparison between Theano with cuDNN and Caffe with cuDNN, say for simple CNNs or multilayer perceptrons? Caffe is supposedly built for performance but I would like to see how much edge it has over Theano with both packages interfacing CUDNN.Have you found any benchmark since then?Powered by Discourse, best viewed with JavaScript enabled"
3748,gpu-accelerated-single-cell-rna-analysis-with-rapids-singlecell,"Originally published at:			https://developer.nvidia.com/blog/gpu-accelerated-single-cell-rna-analysis-with-rapids-singlecell/
RAPIDS-singlecell is a GPU-accelerated tool for scRNA analysis that offers seamless scverse compatibility for efficient single-cell data processing and analysis.Powered by Discourse, best viewed with JavaScript enabled"
3749,what-options-do-i-have-to-get-path-tracing-in-my-application-in-ue5,"What options do I have to get path tracing in my application in UE5?Currently integration of path tracing requires fairly intensive engine changes, you should be familiar with the rendering subsystem of whatever engine you are using. We currently do not have a custom implementation of path tracing in Unreal Engine 5. However, NVIDIA’s NVRTX branch does feature many elements from the PT SDK used in non path tracing modes, including RTXDI based sampled lighting, SER, NRD and DLSS 3.Powered by Discourse, best viewed with JavaScript enabled"
3750,bringing-far-field-objects-into-focus-with-synthetic-data-for-camera-based-av-perception,"Originally published at:			https://developer.nvidia.com/blog/bringing-far-field-objects-into-focus-with-synthetic-data-for-camera-based-av-perception/
The NVIDIA DRIVE AV team improved detection accuracy of far-field objects using synthetic camera data generated in NVIDIA DRIVE Sim, leveraging NVIDIA Omniverse Replicator.Powered by Discourse, best viewed with JavaScript enabled"
3751,artificial-intelligence-software-easily-generates-digital-art,"Originally published at:			Artificial Intelligence Software Easily Generates Digital Art | NVIDIA Technical Blog
Researchers from Adobe and University of California, Berkeley developed software that automatically generates images inspired by the color and shape of the digital brushstroke. The software uses deep neural networks to learn the features of landscapes and architecture, like the appearance of grass or blue skies. Drawing a dark-colored, upside-down V triggers the AI to…Powered by Discourse, best viewed with JavaScript enabled"
3752,red-hat-releases-infrastructure-for-nvidia-morpheus-ai-security-framework,"Originally published at:			https://developer.nvidia.com/blog/red-hat-releases-infrastructure-for-nvidia-morpheus-ai-security-framework/
The joint solution delivers the benefits of a proven, trusted Linux distribution and an advanced, comprehensive container management system when deploying solutions built on top of NVIDIA’s new AI security framework.Powered by Discourse, best viewed with JavaScript enabled"
3753,optimizing-nvidia-tensorrt-conversion-for-real-time-inference-on-autonomous-vehicles,"Originally published at:			Optimizing NVIDIA TensorRT Conversion for Real-time Inference on Autonomous Vehicles | NVIDIA Technical Blog
Autonomous driving systems use various neural network models that require extremely accurate and efficient computation on GPUs. Zoox is a startup developing robotaxis from the ground up, leveraging the high-performance, energy-efficient compute of NVIDIA DRIVE. Recently, Zoox released a one-hour fully autonomous ride in San Francisco, which showcases their AI stack in detail. Compared with…Powered by Discourse, best viewed with JavaScript enabled"
3754,step-into-the-future-of-industrial-grade-edge-ai-with-nvidia-jetson-agx-orin-industrial,"Originally published at:			https://developer.nvidia.com/blog/step-into-the-future-of-industrial-grade-edge-ai-with-nvidia-jetson-agx-orin-industrial/
Embedded edge AI is transforming industrial environments by introducing intelligence and real-time processing to even the most challenging settings. Edge AI is increasingly being used in agriculture, construction, energy, aerospace, satellites, the public sector, and more. With the NVIDIA Jetson edge AI and robotics platform, you can deploy AI and compute for sensor fusion in…Powered by Discourse, best viewed with JavaScript enabled"
3755,nvidia-jetson-project-of-the-month-an-ai-powered-autonomous-miniature-race-car-gets-on-track,"Originally published at:			https://developer.nvidia.com/blog/nvidia-jetson-project-of-the-month-an-ai-powered-autonomous-miniature-race-car-gets-on-track/
The 65th annual Daytona 500 will take place on February 19, 2023 and for many this elite NASCAR event is the pinnacle of the car racing world. For now, there are no plans to see an autonomous vehicle racing against cars with drivers, but it’s not too hard to imagine that scenario at a future…Powered by Discourse, best viewed with JavaScript enabled"
3756,accelerating-cloud-ready-infrastructure-and-kubernetes-with-red-hat-openshift-and-the-nvidia-bluefield-dpu,"Originally published at:			https://developer.nvidia.com/blog/accelerating-cloud-ready-infrastructure-and-kubernetes-with-red-hat-openshift-and-bluefield-dpu/
Take a deep dive into the integrated cloud-ready infrastructure solution from Red Hat and NVIDIAIn a reasonably complex data center network, there is a data plane, a control plane, and a management plane. The first step of this integration (circa 2018) was to offload the dataplane switching from the server’s X86 CPUs to the networking ASIC on the BlueField DPU.  The second step (circa 2021) was moving the networking control plane from the X86 cores to the DPU.  And now, by moving part of the Red Hat OpenShift Container Platform software to the DPU, more of that networking management plane has also been offloading from the X86 CPU cores to the DPU.And then on top of that, OpenShift helps deliver upgrades, automation, cluster management, remote monitoring, etc.Powered by Discourse, best viewed with JavaScript enabled"
3757,powering-nvidia-certified-enterprise-systems-with-arm-cpus,"Originally published at:			https://developer.nvidia.com/blog/powering-nvidia-certified-enterprise-systems-with-arm-cpus/
NVIDIA has approved the first NVIDIA-Certified Systems with Arm CPUs, giving enterprises an easy way to select systems that are optimally configured to run accelerated computing workloads.Powered by Discourse, best viewed with JavaScript enabled"
3758,cheeky-ai-chatbot-wants-to-be-your-bff,"Originally published at:			Cheeky AI Chatbot Wants to be your BFF | NVIDIA Technical Blog
Hugging Face, the deep learning-based chatbot acts primarily as a digital friend allowing people to text back and forth, trade selfies and jokes as if it were a real friend. “There are many people working on artificial intelligence for productivity or utility applications,” co-founder and CEO Clement Delangue told TechCrunch. “We’re building an AI so…Powered by Discourse, best viewed with JavaScript enabled"
3759,applying-natural-language-processing-across-the-world-s-languages,"Originally published at:			https://developer.nvidia.com/blog/applying-natural-language-processing-across-the-worlds-languages/
Despite unprecedented progress in NLP,  many state-of-the-art models are available in English only. NVIDIA has developed tools to enable the development of even the largest language models. This post describes the challenges associated with building and deploying large scale models and the solutions to enable global organizations to achieve NLP leadership in their regions.Great article! You might want to change “Hindu” to “Hindi”@deeperlearning  – Good catch! Fix made.Powered by Discourse, best viewed with JavaScript enabled"
3760,developer-blog-building-an-intelligent-robot-dog-with-the-nvidia-isaac-sdk,"Originally published at:			https://developer.nvidia.com/blog/developer-blog-building-an-intelligent-robot-dog-with-the-nvidia-isaac-sdk/
The modular and easy-to-use navigation stack of the NVIDIA Isaac SDK continues to accelerate the development of various mobile robots. Isaac SDK 2020.1 includes support for quadruped robots and adds to the other robot platforms supported in the SDK, such as differential wheel bases and holonomic wheel bases. Laikago, a quadruped robot by Unitree Robotics, is the newest member of…Powered by Discourse, best viewed with JavaScript enabled"
3761,gtc-2020-scaling-the-transformer-model-implementation-in-pytorch-across-multiple-nodes,"GTC 2020 S21351
Presenters: Mohammad Zulfiqar,NVIDIA; Robert Knight,NVIDIA
Abstract
We’ll dive deep behind the scenes into the Transformer model implementation in PyTorch to understand its performance weaknesses and work to make it scale across multiple nodes. We’ll describe an analysis of system-level profiling data of an example Transformer workload, spanning multiple DGX-2 systems. We’ll present the tools, collection methods, and data-analytics recipes, used to evaluate massive amounts of data and pinpoint the GPU/step of the algorithm causing issues. The described methodology can, in general, be applied to iterative DL and HPC workloads to achieve significant scaling gains.Watch this session
Join in the conversation below.Powered by Discourse, best viewed with JavaScript enabled"
3762,leveraging-openacc-to-compute-high-accuracy-molecular-energies,"Originally published at:			https://developer.nvidia.com/blog/leveraging-openacc-compute-high-accuracy-molecular-energies/
For this interview, I reached out to Janus Juul Eriksen, a Ph.D. fellow at Aarhus University in Denmark. Janus is a chemist by trade without any formal education in computer science; but he is getting up to 12x speed-up compared to his CPU-only code after modifying less than 100 lines of code with one week…CCSD(T) is dominated by DGEMM-like operations.  Did the OpenACC implementation of this code call CUBLAS DGEMM or were the tensor contractions written _natively_ in OpenACC?Hi Jeff.We only use CUBLAS (or libsci_acc on Cray) for the tensor contractions. However, all of the data transport over the bus is handled explicitly.hey Janus,I need help in CUDA Programmming...can u help me ?I'm afraid not, sorry.Powered by Discourse, best viewed with JavaScript enabled"
3763,gordon-bell-finalist-displays-earthquake-simulator-at-sc-18,"Originally published at:			Gordon Bell Finalist Displays Earthquake Simulator at SC 18 | NVIDIA Technical Blog
During a large earthquake, energy rips through the ground in the form of seismic waves that can cause serious harm on densely populated areas. The effects of earthquakes can be difficult to predict, and even the best modeling and simulation techniques to date have been unable to capture some of these earthquakes’ more complex characteristics. To…Powered by Discourse, best viewed with JavaScript enabled"
3764,nvidia-turing-sdks-now-available,"Originally published at:			NVIDIA Turing SDKs Now Available | NVIDIA Technical Blog
NVIDIA’s Turing architecture is one of the biggest leaps in computer graphics in 20 years. Here’s a look at the latest developer software releases to take advantage of this cutting-edge GPU. Deep Learning SDK CUDA 10 CUDA 10 includes support for Turing GPUs, performance optimized libraries, a new asynchronous task-graph programming model, enhanced CUDA &…Powered by Discourse, best viewed with JavaScript enabled"
3765,build-manage-and-deploy-ai-enhanced-clinical-workflows-with-clara-deploy-sdk,"Originally published at:			Build, Manage, and Deploy AI-enhanced Clinical Workflows with Clara Deploy SDK | NVIDIA Technical Blog
The medical imaging industry is undergoing a dramatic transformation driven by two technology trends. Artificial Intelligence and software-defined solutions are redefining the medical imaging workflow. Deep learning research in medical imaging is booming. However, most of this research today is performed in isolation and with limited datasets. This leads to overly simplified models which only…Powered by Discourse, best viewed with JavaScript enabled"
3766,virtual-agent-understands-your-social-cues,"Originally published at:			Virtual Agent Understands Your Social Cues | NVIDIA Technical Blog
A researcher from Carnegie Mellon University developed S.A.R.A. (Socially Aware Robot Assistant) that not only comprehends what you say, but also understands facial expressions and head movements. Using CUDA, GTX 1080 GPUs and cuDNN with TensorFlow to train the deep learning models, S.A.R.A. will reply differently if she detects a smile than someone frowning and…Powered by Discourse, best viewed with JavaScript enabled"
3767,ai-detects-gravitational-waves-faster-than-real-time,"Originally published at:			https://developer.nvidia.com/blog/ai-detects-gravitational-waves-faster-than-real-time/
New research creates a deployable AI framework for detecting gravitational waves within massive amounts of data at several magnitudes faster than real time.Powered by Discourse, best viewed with JavaScript enabled"
3768,catch-up-on-top-gtc-sessions-for-game-developers,"Originally published at:			https://developer.nvidia.com/blog/catch-up-on-top-gtc-sessions-for-game-developers/
Game developers around the world attended GTC to experience how the latest NVIDIA technologies are creating realistic graphics and interactive experiences in gaming.  Catch up on the top sessions available on NVIDIA On-Demand now.Powered by Discourse, best viewed with JavaScript enabled"
3769,accelerated-edge-ai-with-metropolis-and-fleet-command,"Originally published at:			https://developer.nvidia.com/blog/accelerated-edge-ai-with-metropolis-and-fleet-command/
Securely manage and scale AI apps with Fleet Command.Powered by Discourse, best viewed with JavaScript enabled"
3770,streamlining-nvidia-driver-deployment-on-rhel-8-with-modularity-streams,"@kmittmanThanks for the quick reply!It appears the driver didn’t get installed?@kmittmanDoing some further testing on another fresh install of RHEL8, I found that the .ko files are at the following path:
ls -alh /usr/lib/modules/4.18.0-348.20.1.el8_5.x86_64/extra/drivers/video/nvidia/
total 48M
drwxr-xr-x  2 root root  115 Mar 23 11:34 .
drwxr-xr-x. 3 root root   20 Mar 23 11:34 …
-rw-r–r--  1 root root 239K Mar 23 11:34 nvidia-drm.ko
-rw-r–r--  1 root root  44M Mar 23 11:34 nvidia.ko
-rw-r–r--  1 root root 1.6M Mar 23 11:34 nvidia-modeset.ko
-rw-r–r--  1 root root 115K Mar 23 11:34 nvidia-peermem.ko
-rw-r–r--  1 root root 2.2M Mar 23 11:34 nvidia-uvm.koAnd there is no nvidia/ under /proc/driver/.Okay, @routenull I finally got access to a RHEL8 machine with a Quadro P2000 GPU. The installation was successful on bare-metal. Is it possible that there is a configuration issue with the GPU pass-through to VM?Anyway, I am providing step-by-step with output so you can follow along and let me know where there is divergence on your machine. Note: for precompiled, the following are optional: gcc, EPEL repo (dkms), kernel-devel and kernel-headers packages.Pre-installation actionsVerify matching versionsCUDA Download Page instructionsInstall the latest precompiled kernel module streamAfter rebooting@kmittmanThank you for the detailed response and my apologies for the lengthy delay in responding, I hadn’t set the notifications correctly for this post.Is it possible that there is a configuration issue with the GPU pass-through to VM?It is entirely possible, right now I have the VMWare passthru configured as: DirectPath IO
https://i.imgur.com/QFNjGty.pngAll my steps match your steps line by line, until after the reboot. There is no driver loaded, but the hardware is seen by the OS install.[chronos01 ~] # lspci | grep -i nvidia
0b:00.0 VGA compatible controller: NVIDIA Corporation GP106GL [Quadro P2000] (rev a1)
0b:00.1 Audio device: NVIDIA Corporation GP106 High Definition Audio Controller (rev a1)[chronos01 ~] # rpm -qa | grep nvidia | sort
dnf-plugin-nvidia-2.0-1.el8.noarch
kmod-nvidia-510.47.03-4.18.0-348.20.1-510.47.03-3.el8_5.x86_64
nvidia-driver-510.47.03-1.el8.x86_64
nvidia-driver-cuda-510.47.03-1.el8.x86_64
nvidia-driver-cuda-libs-510.47.03-1.el8.x86_64
nvidia-driver-devel-510.47.03-1.el8.x86_64
nvidia-driver-libs-510.47.03-1.el8.x86_64
nvidia-driver-NvFBCOpenGL-510.47.03-1.el8.x86_64
nvidia-driver-NVML-510.47.03-1.el8.x86_64
nvidia-kmod-common-510.47.03-1.el8.noarch
nvidia-libXNVCtrl-510.47.03-1.el8.x86_64
nvidia-libXNVCtrl-devel-510.47.03-1.el8.x86_64
nvidia-modprobe-510.47.03-1.el8.x86_64
nvidia-persistenced-510.47.03-1.el8.x86_64
nvidia-settings-510.47.03-1.el8.x86_64
nvidia-xconfig-510.47.03-1.el8.x86_64[chronos01 ~] # find /lib/modules -name “nvidiako” | sort
/lib/modules/4.18.0-348.20.1.el8_5.x86_64/extra/drivers/video/nvidia/nvidia-drm.ko
/lib/modules/4.18.0-348.20.1.el8_5.x86_64/extra/drivers/video/nvidia/nvidia.ko
/lib/modules/4.18.0-348.20.1.el8_5.x86_64/extra/drivers/video/nvidia/nvidia-modeset.ko
/lib/modules/4.18.0-348.20.1.el8_5.x86_64/extra/drivers/video/nvidia/nvidia-peermem.ko
/lib/modules/4.18.0-348.20.1.el8_5.x86_64/extra/drivers/video/nvidia/nvidia-uvm.koI will try to see if changing any pass-through options makes a difference and report back.Does this work yet on RHEL 9, if not what is the best way to install nvidia on the new RHEL 9 release?Hi @rafjaimes
Official NVIDIA driver RPM packages for RHEL9 (both DKMS and precompiled streams) are not yet available but will be coming soon. The latest CUDA 11.7.0 / 515 driver was released on May 11th; Red Hat released RHEL 9.0 on May 18th.Currently does RHEL 8 support using dual NVIDIA® RTX™ A4000, 16 GB GDDR6 with the drivers above?Thanks!Hi @rafjaimes
The first precompiled kmod package for RHEL9 is now available (NVIDIA driver 515.48.07 @ 5.14.0-70.13.1 kernel):
https://developer.download.nvidia.com/compute/cuda/repos/rhel9/x86_64/precompiled/I’m currently experiencing issues with the module. Computer doesn’t boot after installing module, but I don’t have this issue when installing from driver downloaded from site. It doesn’t seem to pick up the RAID card after rebooting. I’m currently using RHEL8, Broadcom RAID card, and a RTX 4000.Hi @stso
Can you please provide more information to help reproduce the issueSorry for the late response.Commands:Modularity stream/Profile: latest/default
Kernel: 4.18.0-240.el8.x86_64
NVIDIA Driver: Latest(at the time) and 470.42.01 Same result for both
SKU: NVIDIA Corporation TU104GL [Quadro RTX 4000]Last week (Oct 2022), Fedora announced that EPEL 8 Modularity will be discontinued. How will this affect this process to install NVIDIA drivers? Will it become obsolete?EPEL 8 Modularity is going way. Archives will be maintained.
Est. reading time: 1 minute
Hi @rafjaimes
That deprecation should not affect NVIDIA driver installation on those distrosThe NVIDIA driver modularity YAML is hosted on the CUDA repository.More information in the docs: CUDA Installation Guide for Linuxnvidia rpms have been a gray area in the past for non-CUDA installations however is there any reason you can’t or shouldn’t use the CUDA repos for just the GPU display driver?Hi @dereksybau8
What do you mean by “gray area” ? Installation via the package manager is the recommended method.
Or do you mean the “CUDA repo” versus 3rd party repositories (RPM Fusion, negativo17, ELRepo, etc.) ?This blog post and forum thread is specifically about precompiled kmod RPM streams, which are aligned to official “stable” RHEL kernels, excluding: RHEL EUS kernels, RHCOS/OpenShift kernels. That also excludes RockyLinux and other RHEL-like distros, in which case DKMS streams are the supported installation method.Yes, using the cuda repo vs third-party repos that are also prebuilt kmods is what I mean.I remember asking this a while ago (2018?) when the cudo repos were relatively new and it was recommended to not use the cudo repo as a display driver and rather use the .run (or third party rpms).  I believe the claim was cuda repos at the time were not advertised to be display drivers (even though it worked), version x.y.z of cuda repo vs x.y.z of the .run were not “the same”.What’s the exact incompatibility when excluding non-RHEL EL distros (Rocky, Oracle, Alma)?  If these are built the same as RHEL they should just work.  I think the only issue is when RHEL releases are a little ahead of non-RHEL due to the updated releases haven’t been fully released due to build times/QA.I’ve installed the nvidia-driver:latest-dkms for RHEL 9.1, but when I enter the command: ‘nvidia-smi’
I get the error:
NVIDIA-SMI has failed because it couldn’t communicate with the NVIDIA driver.  Make sure the latest NVIDIA driver is installed and running.
If I perform ‘rpm -qa | grep nvidia-driver’ I see that nvidia-driver-530.30.02-1.el9.x86_64 is installed.
There are ko.xz files in the /usr/lib/modules/$(uname -r)/extra/ directory.
But there is no /proc/driver/nvidia directory.
How do I get to the next step and have the Nvidia driver ‘installed and running’?What are the kernel package signing key’s dates?
NVIDIA2019-public_key.derHas a similar key been created for RHEL 9?(replying to thread)@dereksybau8What’s the exact incompatibility when excluding non-RHEL EL distros (Rocky, Oracle, Alma)?I did some investigation:
https://github.com/NVIDIA/yum-packaging-precompiled-kmod/issues/43
Please direct any follow-up questions on Github.@gerald.trummer.ctr
As per the blog post at the beginning of this thread, the precompiled streams are recommended to avoid such issues.When using DKMS streams, it is important to verify that the kernel-devel and kernel-headers matching your running kernel are installed. In the case when the kernel is upgraded before or during the driver installation, without an explicit reboot then DKMS may build against the previous kernel, in which the modules do not match and cannot load.Another thing to check  is dmesg and that both the kernel string and driver string are the expected versions:@gsnead
It is the same X.509 certificate for RHEL8 and RHEL9 precompiled kmod packages, 2019 to present.
More info in this guide: https://github.com/NVIDIA/yum-packaging-precompiled-kmod/blob/main/UEFI.mdThanks!Powered by Discourse, best viewed with JavaScript enabled"
3771,nvidia-nsight-graphics-2022-1-supports-latest-vulkan-ray-tracing-extension,"Originally published at:			https://developer.nvidia.com/blog/nvidia-nsight-graphics-2022-1-supports-latest-vulkan-ray-tracing-extension/
The latest Nsight Graphics 2022.1 release supports Direct3D (11, 12, DXR), Vulkan 1.3, ray tracing extension, OpenGL, OpenVR, and the Oculus SDK.Powered by Discourse, best viewed with JavaScript enabled"
3772,bringing-data-center-management-features-to-the-edge,"Originally published at:			Bringing Data Center Management Features to the Edge | NVIDIA Technical Blog
NVIDIA Fleet Command announced new features giving IT administrators more advanced controls and protection for edge environments. Unlike traditional data centers with hundreds of servers in a single location, edge deployments have one or two servers in thousands of locations. Traditional IT management tools struggle to meet the needs of these distributed environments, especially when…Powered by Discourse, best viewed with JavaScript enabled"
3773,gtc-2020-effective-use-of-mixed-precision-for-hpc,"GTC 2020 S21725
Presenters: Kate Clark,NVIDIA
Abstract
We’ll discuss how using mixed-precision techniques effectively can significantly speed up current-generation supercomputers, as well as the practical and numerical challenges in adopting such techniques. We’ll focus specifically on lattice quantum chromodynamics, which is a regular top-cycle consumer of public supercomputers. Such techniques are an important and sometimes necessary optimization direction for HPC applications.Watch this session
Join in the conversation below.Powered by Discourse, best viewed with JavaScript enabled"
3774,estimating-depth-with-onnx-models-and-custom-layers-using-nvidia-tensorrt,"Originally published at:			Estimating Depth with ONNX Models and Custom Layers Using NVIDIA TensorRT | NVIDIA Technical Blog
TensorRT is an SDK for high performance, deep learning inference. It includes a deep learning inference optimizer and a runtime that delivers low latency and high throughput for deep learning applications. TensorRT uses the ONNX format as an intermediate representation for converting models from major frameworks such as TensorFlow and PyTorch. In this post, you…We have released a sample which demonstrates converting a Pytorch model into ONNX layers, transforming ONNX graphs using new ONNX-graphsurgeon API, implement plugins and execute using TensorRT. We hope this will be useful to accelerate your models with TensorRT. If you have any questions, let us know in comments.hello, when I try to customize the plug-in and use Python to convert onnx to trt, I ran into some problems. I can’t find a reference example. Onnx_packnet is introduced in the developer guide, but I can’t find the complete content from this example.For the packnet model on TensorRT,
Did you evaluate it over jetson nano devices?
What was the FPS acheived?@ehrichwen  You might have already found the sample but here is the link anyways TensorRT/samples/python/onnx_packnet at main · NVIDIA/TensorRT · GitHub@shanbhagdhiraj  We didn’t evaluate it on jetson devices (so no FPS numbers), but feel free to try it out. We don’t anticipate any issues building it on Jetson.@peri.dheeraj  Thanks for the reply,
Do you expect it to run at real time over jetson nano.Powered by Discourse, best viewed with JavaScript enabled"
3775,gtc-2020-multi-gpu-real-time-rendering-techniques,"GTC 2020 S21741
Presenters: Tim Woodard,NVIDIA
Abstract
Learn about state-of-the-art multi-GPU rendering methods using OpenGL, Vulkan, and DirectX for VR and large-scale display systems. These methods have many applications, including the development of command-and-control video walls, CAVEs, simulator visual systems, and location-based entertainment.Watch this session
Join in the conversation below.Powered by Discourse, best viewed with JavaScript enabled"
3776,nsight-the-most-important-ampere-tools-in-your-utility-belt,"Originally published at:			Nsight: The Most Important Ampere Tools In Your Utility Belt | NVIDIA Technical Blog
With the new NVIDIA Ampere GPU microarchitecture, developers have access to the most powerful consumer GPU NVIDIA has ever created. With such a massive improvement in performance, developers need glassbox access to understand the state of the GPU and what steps you can take to achieve peak performance. To make this possible, NVIDIA Nsight Developer…Powered by Discourse, best viewed with JavaScript enabled"
3777,nvvl-accelerates-machine-learning-on-video-datasets,"Originally published at:			NVVL Accelerates Machine Learning on Video Datasets | NVIDIA Technical Blog
Loading data onto GPUs for training has historically been a minor issue for most deep learning practitioners. Data read from a local spinning hard drive or NAS device would be preprocessed on the CPU, then shipped to the GPU for training. The data input pipeline rarely proved to be the bottleneck given the long number-crunching…Powered by Discourse, best viewed with JavaScript enabled"
3778,migrating-from-onyx-to-nvidia-cumulus-linux,"Originally published at:			https://developer.nvidia.com/blog/migrating-from-onyx-to-nvidia-cumulus-linux/
Migrating from Onyx to NVIDIA Cumulus Linux optimizes operational efficiency, enabling a DevOps approach to data center operations.Powered by Discourse, best viewed with JavaScript enabled"
3779,intelligent-monitoring-system-to-detect-asphalt-cracks,"Originally published at:			Intelligent Monitoring System to Detect Asphalt Cracks | NVIDIA Technical Blog
Training their algorithms with CUDA, researchers from Poland published a paper that presents a new image processing solution that will monitor pavement surfaces by using downward facing cameras placed on the rear of a vehicle. Cracks are the most requiring type of pavement distresses to detect and classify automatically. Due to its nature are easily…Powered by Discourse, best viewed with JavaScript enabled"
3780,coffee-crop-recognition-using-deep-learning,"Originally published at:			https://developer.nvidia.com/blog/coffee-crop-recognition-using-deep-learning/
A research team from Brazil are using Caffe and GeForce to identify crops from remote sensing images which is essential to know and monitor the land-use, or to estimate the feasible production amount. Identifying crops from remote sensing images is fundamental to know and monitor land-use. However, manual identification is expensive and maybe impracticable given…Powered by Discourse, best viewed with JavaScript enabled"
3781,accelerate-enterprise-apps-with-microsoft-azure-stack-hci-and-nvidia-bluefield-dpus,"Originally published at:			https://developer.nvidia.com/blog/accelerate-enterprise-apps-with-microsoft-azure-stack-hci-and-nvidia-bluefield-dpus/
Learn how the Microsoft Azure Stack HCI on NVIDIA BlueField DPUs prototype delivers network throughput gains and CPU core savings.Powered by Discourse, best viewed with JavaScript enabled"
3782,isaac-sim-2020-1-preview,"Originally published at:			https://developer.nvidia.com/blog/isaac-sim-2020-1-preview/
Today we release a preview of our next-gen Isaac Sim based on NVIDIA Omniverse. What’s new with Isaac Sim? About a year ago NVIDIA released the first iteration of an application called Isaac Sim, a 3D simulation environment designed for use in robot development with the NVIDIA Isaac SDK.  The year 2019 was a busy…Powered by Discourse, best viewed with JavaScript enabled"
3783,tensor-ops-made-easier-in-cudnn,"Originally published at:			Tensor Ops Made Easier in cuDNN | NVIDIA Technical Blog
Neural network models have quickly taken advantage of NVIDIA Tensor Cores for deep learning since their introduction in the Tesla V100 GPU last year. For example, new performance records for ResNet50 training were announced recently with Tensor Core-based solutions. (See the NVIDIA developer post on new performance milestones for additional details). NVIDIA’s cuDNN library enables CUDA programmers…Does the constraint on input and output channel being divisible by 8 still valid for every other configuration except for packed NCHW? So for NHWC, it will still reverse to cuda cores when either in or out channel is not divisible by 8?If so, then I assume this is because the cost of adding padding to the channel when channel is the inner-most dimension is so large that offsets the benefit of using tensorcore. If C is the inner most, then you are essentially copying the entire matrix with tons of uncoalesced gobal memory read & write. If my guess is wrong, please fill me in with the details. Thanks!Powered by Discourse, best viewed with JavaScript enabled"
3784,using-neural-networks-for-your-recommender-system,"Originally published at:			https://developer.nvidia.com/blog/using-neural-networks-for-your-recommender-system/
This post is an introduction to deep learning-based recommender systems. It highlights the benefits of using neural networks and explains the different components. Neural network architectures are covered from the basic matrix factorization with deep learning up to session-based models.Thank you for your help.
But I have some questions.
What features of users interactions dataset and products dataset should be used to make Embedding layers? Can you explain please?It depends on your dataset. The categorical feature are often fed through an embedding layer. You can use all categorical item feature for item embeddings and all categorical user features for the user embedding - a good start is item id and user it. If you have multiple categorical features, you can concatenate them.Powered by Discourse, best viewed with JavaScript enabled"
3785,gtc-2020-isaac-sim-2020-deep-dive,"GTC 2020 S21945
Presenters: Hai Loc Lu,NVIDIA; Hammad Mazhar,NVIDIA
Abstract
Join us for an in-depth exploration of Isaac Sim 2020, the latest version of NVIDIA’s simulator for robotics. Isaac Sim’s first release in 2019 was based on the Unreal Engine, and since then the development team has been hard at work building a brand-new robotics simulation solution with NVIDIA’s Omniverse platform. Topics will include:• How to obtain and set up the Isaac Sim 2020.1 Preview release: Docker container at NVIDIA NGC, deployment on AWS or workstation.
• Editor, simulation features and systems: Python scripting, RTX Graphics, Universal Scene Description, PhysX, MDL materials, cameras, and lights.
• Leonardo manipulation example. Tasks, motion policies and state machine.
• What’s next: road map includes sensor system, segmentation and labeling, domain randomization, mobile robots and more.
• Q&AWatch this session
Join in the conversation below.Powered by Discourse, best viewed with JavaScript enabled"
3786,explaining-how-end-to-end-deep-learning-steers-a-self-driving-car,"Originally published at:			Explaining How End-to-End Deep Learning Steers a Self-Driving Car | NVIDIA Technical Blog
As part of our autonomous driving research, NVIDIA has created a deep-learning based system, known as PilotNet, which learns to emulate the behavior of human drivers and can be deployed as a self-driving car controller. PilotNet is trained using road images paired with the steering angles generated by a human driving a data-collection car. It derives the…Great post! Thanks for sharing.I have a question though. How does a self driving car handles multi-path situations such as a T-point or a road intersection?What is the mechanism to turn left or right in case of these situations?Powered by Discourse, best viewed with JavaScript enabled"
3787,hands-on-lab-digital-fingerprinting-to-detect-cyber-threats,"Originally published at:			Digital Fingerprinting to Detect Cyber Threats | NVIDIA
Learn how to use an NVIDIA AI workflow to uniquely fingerprint users and machines across your network in a new, free NVIDIA LaunchPad hands-on lab.Powered by Discourse, best viewed with JavaScript enabled"
3788,taking-gpu-based-ngs-data-analysis-to-another-level-with-nvidia-clara-parabricks-pipelines-3-0,"Originally published at:			Taking GPU-based NGS Data Analysis to Another Level with NVIDIA Clara Parabricks Pipelines 3.0 | NVIDIA Technical Blog
The world of healthcare is under a giant spotlight these days. In these unprecedented times, everyone is focusing on keeping loved ones safe and contributing to the community as much as possible. It is amazing to see how the community is uniting in the past two months, which has led to a global, rapid emergence…Powered by Discourse, best viewed with JavaScript enabled"
3789,gtc-2020-transfer-learning-toolkit-pre-trained-models,"GTC 2020 CWE21102
Presenters: Farzin-Aghdasi-,NVIDIA; Yu Wang and Anil Ubale, NVIDIA; Subhashree-Radhakrishnan and Anil, NVIDIA; Jeff, NVIDIA; Arihant-Jain, NVIDIA
Abstract
The NVIDIA Transfer Learning Toolkit is ideal for deep learning application developers and data scientists seeking a faster, more efficient deep learning training workflow for Intelligent Video Analytics (IVA). Transfer Learning Toolkit abstracts and accelerates deep learning training by allowing developers to fine-tune NVIDIA-provided, pre-trained models that are domain specific instead of going through the time-consuming process of building Deep Neural Networks (DNNs) from scratch. The pre-trained models accelerate the developer’s deep learning training process and eliminate the higher costs associated with large-scale data collection, labeling, and training models from scratch. We’ll show how to train, prune, and optimize popular models and create TRT engines.Watch this session
Join in the conversation below.Powered by Discourse, best viewed with JavaScript enabled"
3790,nvidia-jetson-community-project-spotlight-point-voxel-cnn-for-efficient-3d-deep-learning,"Originally published at:			NVIDIA Jetson Community Project Spotlight: Point-Voxel CNN for Efficient 3D Deep Learning | NVIDIA Technical Blog
3D deep learning is used in a variety of applications including robotics, AR/VR systems, and autonomous machines.  In this month’s Jetson Community Project spotlight, researchers from MIT’s Han Lab developed an efficient, 3D, deep learning method for 3D object segmentation, designed to run on edge devices.  “We present Point-Voxel CNN (PVCNN) for efficient, fast 3D…good workPowered by Discourse, best viewed with JavaScript enabled"
3791,exelon-uses-synthetic-data-generation-of-grid-infrastructure-to-automate-drone-inspection,"Originally published at:			https://developer.nvidia.com/blog/exelon-uses-synthetic-data-generation-of-grid-infrastructure-to-automate-drone-inspection/
Most drone inspections still require a human to manually inspect the video for defects. Training a computer vision model to automate inspection is difficult without a large pool of labeled data for every possible defect. In a recent session at NVIDIA GTC, we shared how Exelon is using synthetic data generation in NVIDIA Omniverse to…Powered by Discourse, best viewed with JavaScript enabled"
3792,gtc-2020-nvidia-multi-instance-gpu-mig-enables-elastic-computing,"GTC 2020 D2S40
Presenters: Tech Demo Team,NVIDIA
Abstract
Learn how MIG enables admins to partition a single NVIDIA A100 into up to seven independent GPU instances, delivering 7X higher utilization compared to prior-generation GPUs in this demo on audio classification and BERT Q&A from the GTC2020 Keynote. 
  Learn more about MIG
Watch the entire GTC 2020 keynoteWatch this session
Join in the conversation below.Powered by Discourse, best viewed with JavaScript enabled"
3793,cudacasts-episode-21-porting-a-simple-opencv-sample-to-the-jetson-tk1-gpu,"Originally published at:			https://developer.nvidia.com/blog/cudacasts-episode-21-porting-simple-opencv-sample-jetson-tk1-gpu/
In the previous CUDACasts episode, we saw how to flash your Jetson TK1 to the latest release of Linux4Tegra, and install both the CUDA toolkit and OpenCV SDK.  We’ll continue exploring the power efficiency the Jetson TK1 Kepler-based GPU brings to computer vision by porting a simple OpenCV sample to run on the GPU.  We’ll explore computer…The last demo is optical flow? Is it running in real time?Hi,Can you have a future cudacast in which you demonstrate how to perform cross compilation with a Jetson?  Many of the online resources I found describe how to install a cross-compilation platform on the host system but don't walk through the entire process of installing the cross-compilation platform, compiling CUDA programs on the host, and placing those programs onto Jetson.Thanks!Powered by Discourse, best viewed with JavaScript enabled"
3794,best-practices-for-using-nvidia-rtx-ray-tracing-updated,"Originally published at:			Best Practices for Using NVIDIA RTX Ray Tracing (Updated) | NVIDIA Technical Blog
Optimize your use of NVIDIA RTX with these in-depth ray tracing tips.Powered by Discourse, best viewed with JavaScript enabled"
3795,ai-and-drug-discovery-nvidia-genomics-with-jonny-israeli,"Originally published at:			https://developer.nvidia.com/blog/ai-and-drug-discovery-nvidia-genomics-with-jonny-israeli/
NVIDIA is accelerating the field of genomics and drug discovery with the help of GPUs. We sit down with the lab lead to learn more about their work.Powered by Discourse, best viewed with JavaScript enabled"
3796,nvidia-announces-more-ngc-ready-systems,"Originally published at:			https://developer.nvidia.com/blog/nvidia-announces-availability-of-ngc-ready-systems/
NVIDIA is committed to making it easier for developers to deploy software from our NGC container registry.   As part of that commitment, last week we announced our NGC-Ready program, which expands the places users of powerful systems with NVIDIA GPUs can deploy GPU-accelerated software with confidence. Today, we’re announcing several new NGC-Ready systems from…Powered by Discourse, best viewed with JavaScript enabled"
3797,cutting-edge-startups-gather-in-washington-for-gtc-dc,"Originally published at:			https://developer.nvidia.com/blog/cutting-edge-startups-gather-in-washington-for-gtc-dc/
This year at GTC DC 2019, startups within NVIDIA’s startup incubator, Inception, will showcase their technology in data science, computer vision, natural language processing, and more at Washington, D.C. Attending? Don’t forget to add these sessions to your calendar. From Theory to Practice: Computer Vision on Edge Devices for Real-Time Traffic Optimization Yoav Valinsky, Computer…Powered by Discourse, best viewed with JavaScript enabled"
3798,running-large-scale-graph-analytics-with-memgraph-and-nvidia-cugraph-algorithms,"Originally published at:			https://developer.nvidia.com/blog/running-large-scale-graph-analytics-with-memgraph-and-nvidia-cugraph-algorithms/
Learn how to use PageRank graph analysis and Louvain community detection to analyze a Facebook dataset containing 1.3 million relationships.What is the location of the docker image?The docker image is available at the official docker repository and you can get it by running the command listed at Step3 in the post.This is the command:If you get stuck just reach Memgraph at one of their support channels.Powered by Discourse, best viewed with JavaScript enabled"
3799,end-to-end-ai-for-workstation-transitioning-ai-models-with-onnx,"Originally published at:			https://developer.nvidia.com/blog/end-to-end-ai-for-workstation-transitioning-ai-models-with-onnx/
This post is the second in a series about optimizing end-to-end AI for workstations. For more information, see part 1, End-to-End AI for Workstation: An Introduction, and part 3, End-to-End AI for Workstation: ONNX Runtime and Optimization. In this post, I discuss how to use ONNX to transition your AI models from research to production…Powered by Discourse, best viewed with JavaScript enabled"
3800,is-there-a-way-to-control-the-number-of-triangles-in-the-output-mesh,"Content creators are very often on a tight budget regarding triangle count. So is it possible to control that up-front with Get3D to avoid too much post-processing work on the generated meshes?The number of triangles in the resulting mesh can be controlled by changing the grid resolution of DMTet. However, this may come at the cost of compromising quality. We’re actively exploring ways to reduce the triangle count while maintaining high-quality meshes.Thanks!Looking forward to further developments!This topic was automatically closed 14 days after the last reply. New replies are no longer allowed.Powered by Discourse, best viewed with JavaScript enabled"
3801,detecting-financial-fraud-using-gans-at-swedbank-with-hopsworks-and-nvidia-gpus,"Originally published at:			https://developer.nvidia.com/blog/detecting-financial-fraud-using-gans-at-swedbank-with-hopsworks-and-gpus/
Recently, one of Sweden’s largest banks trained generative adversarial neural networks (GANs) using NVIDIA GPUs as part of its fraud and money-laundering prevention strategy. Financial fraud and money laundering pose immense challenges to financial institutions and society. Financial institutions invest huge amounts of resources in both identifying and preventing suspicious and illicit activities. There are…Powered by Discourse, best viewed with JavaScript enabled"
3802,announcing-onnx-runtime-availability-in-the-nvidia-jetson-zoo-for-high-performance-inferencing,"Originally published at:			Announcing ONNX Runtime Availability in the NVIDIA Jetson Zoo for High Performance Inferencing | NVIDIA Technical Blog
  Microsoft and NVIDIA have collaborated to build, validate and publish the ONNX Runtime Python package and Docker container for the NVIDIA Jetson platform, now available on the Jetson Zoo. Today’s release of ONNX Runtime for Jetson extends the performance and portability benefits of ONNX Runtime to Jetson edge AI systems, allowing models from many…I believe that the sample you are providing is a CPU-based sample… right?I got the following WARNINGS, is this the expected behaviorNo update for 1.8.1 or 1.8.2?
I have problem with v 1.8.0 on Nano. With agx xavier runs well.Hi @mapin.ai.project,Can you please provide some details of your issue at Issues · microsoft/onnxruntime · GitHub?NataliePowered by Discourse, best viewed with JavaScript enabled"
3803,upgrading-multi-gpu-interconnectivity-with-the-third-generation-nvidia-nvswitch,"Originally published at:			https://developer.nvidia.com/blog/upgrading-multi-gpu-interconnectivity-with-the-third-generation-nvidia-nvswitch/
NVIDIA NVSwitch is designed to provide connectivity within a node or to GPUs external to the node for the NVLink Switch System.Is the Custom-FW OSFP optical or electrical?Powered by Discourse, best viewed with JavaScript enabled"
3804,meet-the-researcher-sam-raymond-combining-ai-and-hpc-simulations-for-biomedical-research,"Originally published at:			Meet the Researcher, Sam Raymond: Combining AI and HPC Simulations for Biomedical Research | NVIDIA Technical Blog
‘Meet the Researcher’ is a series in which we spotlight different researchers in academia who are using GPUs to accelerate their work. This week, we spotlight Sam Raymond, a postdoctoral researcher at Stanford University. Dr. Raymond received his PhD from the Massachusetts Institute of Technology in 2020 in the field of Computational Science and Engineering.…Powered by Discourse, best viewed with JavaScript enabled"
3805,gtc-2020-panoptic-segmentation-dnn-for-autonomous-vehicles,"GTC 2020 S21879
Presenters: Ke Chen ,NVIDIA; Ryan Oldja,NVIDIA
Abstract
We’ll present our NVIDIA DriveAV’s Panoptic Segmentation Deep Neural Network (DNN), which can be used for semantic and instance segmentation of complex scenes for self-driving car scenarios, such as complex urban areas, congested traffic, construction zones with unusual activities, and so on. With Panoptic Segmentation DNN, input images can be accurately parsed for both semantic segmentation (which pixels represent which object class), as well as instance content (which pixels represent which object instance). Planning and control modules can use panoptic segmentation results to better inform autonomous driving decisions. We’ll cover our highly accurate GT dataset, DNN architecture, our multi-task training process, and our real-time inference (that includes post-processing steps) on vehicles’ AGX compute. Our network achieves state-of-the-art accuracy and runs at 7ms end-to-end on NVIDIA AGX GPUs. We’ll show videos of our experiments on a real vehicle in various challenging conditions.Watch this session
Join in the conversation below.Powered by Discourse, best viewed with JavaScript enabled"
3806,nvidia-announces-nsight-systems-2021-3,"Originally published at:			https://developer.nvidia.com/blog/nvidia-announces-nsight-systems-2021-3/
Nsight Systems is a system-wide performance analysis tool, designed to help developers tune and scale software across CPUs and GPUs.Would you show us any document about changes?
For example, I want to know “sample GPU PCIe BAR1 request activity”.Hi Sakaia,I would suggest checking out this blog that highlights the usage of “sample GPU PCIe BAR1 request activity” scenario in the typical developer scenario. Furthermore additional details on the release are captured on the NVIDIA Nsight Systems product page. NVIDIA Nsight Systems | NVIDIA DeveloperThanks.Powered by Discourse, best viewed with JavaScript enabled"
3807,announcing-nvidia-dali-and-nvidia-nvjpeg,"Originally published at:			Announcing NVIDIA DALI and NVIDIA nvJPEG | NVIDIA Technical Blog
Today at Computer Vision and Pattern Recognition (CVPR) conference, we’re making available new libraries for data augmentation and image decoding. NVIDIA DALI: A GPU-accelerated data augmentation and image loading library for optimizing data pipelines of deep learning frameworks NVIDIA nvJPEG: A high-performance GPU-accelerated library for JPEG decoding Computer vision applications powered by deep learning include…Powered by Discourse, best viewed with JavaScript enabled"
3808,top-avatar-sessions-at-nvidia-gtc-2023,"Originally published at:			https://nvda.ws/3Xrxg3Y#new_tab
Learn about the future of interactive avatars for gaming, the metaverse, and beyond at NVIDIA GTC 2023.Powered by Discourse, best viewed with JavaScript enabled"
3809,gpu-pro-tip-lerp-faster-in-c,"I think C.C. only happens with subtraction, right? That would require either v0 or v1 to be negative, since t usually isn't. In which case sure, that can happen. :) But I guess the FMA approach could be more accurate even ignoring catastrophic cancellation cases.Mark,I talked about this exact thing at GTC 2015. You can see it in my slides from the presentation (page 33);http://on-demand.gputechcon...And yes Norbert was the one who suggested this to me as well.What I am saying is that this is an optimization that a compiler should be smart enough to do, especially since a lot of compilers allow some option (e.g., fast-math in gcc https://gcc.gnu.org/wiki/Fl... ) that enables such unsafe optimizations.Yes, compilers cannot do all the optimizations possible, especially when there are layers and layers of code, but it looks that this is something that can (and should) be supported.Well, in fma(-t, v0, v0) v0 and v0 are the same sign, and -t is typically negative, so C.C. will certainly happen when t ~= 1, i.e. -t*v0 is *almost* as large as v0, but opposite sign.Good point!  But I thought you were comparing the accuracy of the FMA version to the non-FMA version, which is (1-t)*v0 + t*v1. In that version, the compiler will generate an FMUL, FADD, and FMA. And I don't think C.C will happen in that instruction sequence unless one of the three parameters is negative.Cool! Norbert knows his stuff.nvcc has fast math: --use_fast_math. But this optimization still doesn't happen with that enabled, I just checked. Have you checked that this does happen with `gcc -funsafe-math-optimizations`? I wouldn't be surprised if it does not.Fair enough :) Thanks for clarifying!'such unsafe optimizations"" ? Really? Unsafe in what regard?The best demonstration of the performance improvement of using the 'unsafe' fast math flag in CUDA is to compile the following with and without that flag:https://github.com/OlegKoni...After much testing I found very little difference between values generated using that 'unsafe' flag when compared to equivalent 64 bit MATLAB computation.Also the accuracy of those CUDA math functions used by that flag is detailed here:http://docs.nvidia.com/cuda...I usually use the form v0+t*(v1-v0) which can be more accurate for small differences - you said add followed by multiply, but fma seems in the reverse order?Can this form be as fast?Stuart, the form you suggest is precisely the form that Wikipedia points out is an ""Imprecise method which does not guarantee v = v1 when t = 1, due to floating-point arithmetic error.""  (https://en.wikipedia.org/?t...  I would also think that it would be prone to catastrophic cancellation when v1 is orders of magnitude larger than v0.I checked with Norbert Juffa, who says the accuracy of this alternate form is superior only when v0 and v1 are within a factor of two of each other. That is due to the Sterbenz lemma which states that the subtraction of two positive floating-point numbers a and b, that is, a-b, is exact for 0.5*a <= b <= 2*a. Therefore the maximum error in the final result is simply the error of the FMA, thus 0.5 ulps. However, the moment you allow v0,v1 to be apart by more than a factor of 2, the accuracy of v0+t*(v1-v0) suffers: if they can be within a factor of four of each other, the maximum error increases to 1.5 ulps. If they are within a factor of eight, 2.5 ulps. Etc.The form with two FMAs from the post has error <= 1 ulp across a very wide range of arguments, so for a general purpose lerp code, we think this is still the most robust form.  But clearly there is a trade-off, and if it is known in advance that the interpolants are always within a factor of two of each other (e.g. for interpolating in a known table, rather than sensor data), the proposed alternative form would be the way to go.Regarding performance: this form would generate an FADD and FMA, so it should be as fast as the 2-FMA version.(Thanks Norbert!)If it can't optimize the original eexpression, may be it will be happy with (v0-t*v0) + t*v1 ?Great post, thanks. I would understand this should guarantee:lerp(t, a, b) == a when a==blerp(0, a, b) == alerp(1, a, b) == bIs that correct?What is the meaning of FMA?Fused Multiply-Add. It's a single instruction that does A*B+C with full precision maintained between the multiply and add.isn't fma(t, v1 -v0, v0) better?fma only takes 3 arguments.I meant fma(t, (v1-v0), v0). But njuffa shows that your method is better:https://devtalk.nvidia.com/...I would call it Norbert Juffa's method! He provided the pro tip for the blog and did all the work. I just wrote it up for him. :)Powered by Discourse, best viewed with JavaScript enabled"
3810,mercedes-benz-nvidia-partner-to-build-the-world-s-most-advanced-software-defined-vehicles,"Originally published at:			Mercedes-Benz, NVIDIA Partner to Build the World’s Most Advanced, Software-Defined Vehicles | NVIDIA Technical Blog
The automaker responsible for the world’s first car is now the first to deliver software-defined vehicles across its entire fleet, powered by NVIDIA. In a live-streamed media event from Stuttgart and Silicon Valley, Mercedes-Benz CEO Ola Källenius and NVIDIA founder and CEO Jensen Huang announced that the automaker will launch software-defined, intelligent vehicles using end-to-end NVIDIA technology. “This is…Powered by Discourse, best viewed with JavaScript enabled"
3811,dlss-what-does-it-mean-for-game-developers,"Originally published at:			DLSS: What Does It Mean for Game Developers? | NVIDIA Technical Blog
We have all heard a lot about the importance of two emerging technologies for games: real-time ray tracing and AI. The former is easy to grasp immediately… all one needs to do is watch a short demo like Project Sol, and they can see the benefits of the technology straight away. DLSS (Deep Learning Super…Powered by Discourse, best viewed with JavaScript enabled"
3812,googles-tossingbot-can-toss-over-500-objects-per-hour-into-target-locations,"Originally published at:			https://developer.nvidia.com/blog/googles-tossingbot-can-toss-over-500-objects-per-hour-into-target-locations/
Researchers from Google, Princeton, Columbia and MIT developed a picking robot using physics and deep learning that can accurately toss random objects into bins two times faster than previous systems. “This robot, like many others, is designed to tolerate the dynamics of the unstructured world,” mentioned Andy Zeng, Student Researcher of Robotics at Google. “But…Powered by Discourse, best viewed with JavaScript enabled"
3813,cooperative-groups-flexible-cuda-thread-programming,"Excellent blog, thank you so much. As a minor observation, in reduce_sum_tile_shfl 'lane' seems unused.Good article--but looking for the follow up on multiblock synchronization. The user guide only talks about synchronizing the entire grid_group, but how do I synchronize among a subset of blocks in a grid_group? For example, I want to synchronize threads in the ""Z"" dimension but not all X,Y,Z blocks.Hi David, synchronizing a subset of blocks is not currently supported. Currently there's no partitioning capability for `grid_group`.Good catch! Fixed.It is part of the GPU instruction set. https://docs.nvidia.com/cud...Hello,I don't understand the purpose of second g.sync() in        temp[lane] = val;        g.sync(); // wait for all threads to store        if (lane < i) val += temp[lane + i];        g.sync(); // wait for all threads to loadLoads are done from second half of temp, while stores are essentially done to first half of temp (second half of vals is not updated because of ""if (lane < i)""). Isn't second g.sync() unnecessary?Hi Igor, while technically your suggestion may work for this specific code, in general it's incorrect to remove one of the syncs. You would probably have to mark temp as volatile, which is a hack. The g.sync()s prevent the compiler from performing code motion optimizations across the synchronization points. Without them you have a race condition, even if the data involved in the race is beyond what is used by the algorithm. As an example, if you changed from this downward reduction to a so-called ""butterfly"" reduction (using xor rather than + for the indexing), both syncs are absolutely required.hi,is the follow up post on the grid_group's already available?Questions (1 and 2) below might be showing a misunderstanding on my part.1) Should the comment ""Each thread adds its partial sum[i] to sum[lane+i]"" be something like ""Each thread adds the partial sum[lane+i] to its accumulator sum[lane] (only lane 0 will have the full accumulated value)""?2) For sum_kernel_block(), doesn't thread_sum() assumes n is divisible by 4, and doesn't the formula for nBlocks require n/4?3) Is the optimization of loop unrolling a -O2 feature or a -O3 feature?  (My project can only use -O2.)4) Is the optimization of removing the synchronization statement for warps a -O2 feature or a -O3 feature?5) After a vector-4 load, if a device function foo() is called as foo(v.x); foo(v.y); foo(v.z); foo(v.w);, will the compiler optimize across the four invocations of foo() or will each invocation be treated as a ""basic block of optimization""?  Please consider both optimization flags of -O2 and -O3.6) Please consider updating the grid-size loop blog with a section on threads working on vectors.     .To make this reduction compatible with input that is not divisible by 4, thread_sum() needs to be modified:To fix 2), see my comment on the main article.Are the .match_any() and .match_all() methods available on all generations? I know the definition of the intrinsics came by in Volta and newer generation, but can I make a function that does a similar job on previous architectures?Hello, any follow up regarding grid sync or device sync available?I am also looking for variant of grid synchronisation like the following:I would like to achieve synchronisation among the active thread blocks scheduled on a GPU.
Is this possible to do with the current co-operative thread grouping and grid synchronisation concept ?My requirement is that a current scheduled thread blocks co-operatively load a memory segment into shared memory and then compute and then synchronize until both are complete…I found two new functions in cooperative_groups. Primarily, sync_grids and sync_warp (inside include/cooperative_groups/sync.h). I wanted to know if there are any opensource or public projects that use these primitives. Can someone point to those?the last part of atomicAggInc, shouldn’t the returnbe a int* prev ?reference code:No, it returns the value, not the pointer.  The pointer should not change.  “prev” probably isn’t a good name for the variable, though.Oh I see, It’s my fault. I misunderstood the function of atomicAdd: it’s first parameter should be a pointer not a value. Thank you for the reply.By the way, do we need to add a cg::sync() after the if statement ?  Because in theory, as far as I can see,  we should add a sync here. And I wonder if the compiler will add a sync for us and we don’t have to write it down explicitly?Hi Is it available on cuda-11 ? and can anyone tell me what is the use of vec3_to_linear call ?This looks like it was fixed but seems to have reverted; the article says cg::partition again.Powered by Discourse, best viewed with JavaScript enabled"
3814,video-analytics-top-resources-from-gtc-21,"Originally published at:			https://developer.nvidia.com/blog/video-analytics-top-resources-from-gtc-21/
Here is the latest developer resources and news of AI-powered video analytics, including top featured talks, tutorials and success stories of NVIDIA pre-trained models, Transfer Learning Toolkit, and DeepStream SDK.Powered by Discourse, best viewed with JavaScript enabled"
3815,gtc-2020-gibson-environment,"GTC 2020 S21836
Presenters: Roberto Martin-Martin,Stanford University; Silvio Savarese, Stanford University
Abstract
Gibson Environment is a simulation environment that supports rendering of high-fidelity images from 3D reconstructed buildings. This environment is fundamental to train visuo-motor navigation skills for robotic agents. These models learn to navigate within the simulated environment based on the visual information. We further developed our simulator to enable not only navigation, but also interactions with the simulated environment. This opens new avenues in robotics, enabling agents to be trained for new tasks in simulation while maintaining a simple sim2real transfer, thanks to the visual realism. We’ll present our techniques to improve photo realism (computer vision and computer graphics) and examples of navigation and interactive agents trained in Gibson (robotics, machine learning, and reinforcement learning based on visual information).Watch this session
Join in the conversation below.Powered by Discourse, best viewed with JavaScript enabled"
3816,cudacasts-episode-14-racecheck-analysis-with-cuda-5-5,"Originally published at:			https://developer.nvidia.com/blog/cudacasts-episode-14-racecheck-analysis-cuda-5-5/
The key to the power of GPUs is their 1000’s of parallel processors that execute threads. Anyone who has worked with even a handful of threads know how easy it can be to introduce race conditions, and how difficult it  can be to debug and fix these errors. Because a modern GPU can have thousands…Mark,great post, would like to walk through...where is ""the simple version of Conway's Game of Life""?Powered by Discourse, best viewed with JavaScript enabled"
3817,nvidia-t4-gpus-now-available-on-google-cloud,"Originally published at:			NVIDIA T4 GPUs Now Available on Google Cloud | NVIDIA Technical Blog
Google Cloud today announced the general availability of the NVIDIA T4 GPU, making Google Cloud the first provider to offer the GPUs globally. The NVIDIA T4 GPUs are ideal for machine learning training and inference, high performance computing, data analytics, and graphics applications.   “NVIDIA’s Turing architecture brings the second generation of Tensor Cores to…Powered by Discourse, best viewed with JavaScript enabled"
3818,gtc-21-top-5-graphics-simulation-technical-sessions,"Originally published at:			https://developer.nvidia.com/blog/gtc-21-top-5-graphics-simulation-technical-sessions/
NVIDIA technology is powering some of the most stunning graphics and visuals you’ve ever seen, and now is your chance to learn exactly how developers and other professionals are driving innovation in graphics at GTC starting April 12.Powered by Discourse, best viewed with JavaScript enabled"
3819,australia-s-newest-and-fastest-supercomputer-to-be-powered-by-nvidia-v100-gpus,"Originally published at:			Australia’s Newest and Fastest Supercomputer to be Powered by NVIDIA V100 GPUs | NVIDIA Technical Blog
Fujitsu announced today they have been awarded the contract to build Australia’s newest and fastest supercomputer, powered by NVIDIA V100 Tensor Core GPUs. The new system will be housed at the Australian National University in Canberra and will be operated by the National Computational infrastructure of Australia (NCI).  Named Gadi, which means “to search for” in the language…Powered by Discourse, best viewed with JavaScript enabled"
3820,free-ray-tracing-gems-ii-chapter-covers-ray-tracing-in-remedy-s-control,"Originally published at:			https://developer.nvidia.com/blog/free-ray-tracing-gems-ii-chapter-covers-ray-tracing-in-remedys-control/
This chapter, written by Juha Sjöholm, Paula Jukarainen, and Tatu Aalto, presents how all ray tracing based effects were implemented in Remedy Entertainment’s Control.Powered by Discourse, best viewed with JavaScript enabled"
3821,accelerating-xgboost-on-gpu-clusters-with-dask,"Originally published at:			https://developer.nvidia.com/blog/accelerating-xgboost-on-gpu-clusters-with-dask/
In XGBoost 1.0, we introduced a new, official Dask interface to support efficient distributed training.  Fast-forwarding to XGBoost 1.4, the interface is now feature-complete. If you are new to the XGBoost Dask interface, look at the first post for a gentle introduction. In this post, we look at simple code examples, showing how to maximize…Powered by Discourse, best viewed with JavaScript enabled"
3822,simplifying-model-development-and-building-models-at-scale-with-pytorch-lightning-and-ngc,"Originally published at:			https://developer.nvidia.com/blog/simplifying-model-development-and-building-models-at-scale-with-pytorch-lightning-and-ngc/
PyTorch Lightning is a lightweight PyTorch wrapper for high-performance AI research. PyTorch code with Lightning enables seamless training on multiple-GPUs and uses best practices such as checkpointing, logging, sharding, and mixed precision. In this post, we walk you through building speech models with PyTorch Lightning on NVIDIA GPU-powered AWS instances managed by the Grid.ai platform.Powered by Discourse, best viewed with JavaScript enabled"
3823,restful-inference-with-the-tensorrt-container-and-nvidia-gpu-cloud,"Originally published at:			RESTful Inference with the TensorRT Container and NVIDIA GPU Cloud | NVIDIA Technical Blog
Once you have built, trained, tweaked and tuned your deep learning model, you need an inference solution that you need to deploy to a datacenter or to the cloud, and you need to get the maximum possible performance. You may have heard that NVIDIA TensorRT can maximize inference performance on NVIDIA GPUs, but how do…Powered by Discourse, best viewed with JavaScript enabled"
3824,identify-rare-diseases-with-a-selfie,"Originally published at:			Identify Rare Diseases with a Selfie | NVIDIA Technical Blog
A new deep learning app called Face2Gene lets doctors snap an image of a patient and receive a suggested diagnosis from thousands of genetic disorders. “One in 10 people suffer from a rare disease,” said Dekel Gelbman, FDNA CEO, the company who developed the app. “Many of these are life threatening, and often extremely difficult…Powered by Discourse, best viewed with JavaScript enabled"
3825,a-comprehensive-overview-of-regression-evaluation-metrics,"Originally published at:			https://developer.nvidia.com/blog/a-comprehensive-overview-of-regression-evaluation-metrics/
As a data scientist, evaluating machine learning model performance is a crucial aspect of your work. To do so effectively, you have a wide range of statistical metrics at your disposal, each with its own unique strengths and weaknesses. By developing a solid understanding of these metrics, you are not only better equipped to choose…I’m curious to know if you’re using any other regression evaluation metrics. If you are, which ones do you use?Powered by Discourse, best viewed with JavaScript enabled"
3826,nvidia-nsight-perf-sdk-v2021-1-now-available,"Originally published at:			https://developer.nvidia.com/blog/nvidia-nsight-perf-sdk-v2021-1-now-available/
The NVIDIA Nsight Perf SDK is a graphics profiling toolbox for DirectX, Vulkan, and OpenGL, enabling you to collect GPU performance metrics directly from your application.Powered by Discourse, best viewed with JavaScript enabled"
3827,advice-on-building-a-data-science-career-q-a-with-ken-jee,"Originally published at:			https://developer.nvidia.com/blog/advice-on-building-a-data-science-career-qa-with-ken-jee/

Ken Jee, a data science professional, shares insights on leveraging university resources, benefits of content creation, and useful learning methods for AI topics.Powered by Discourse, best viewed with JavaScript enabled"
3828,google-s-new-ai-model-improves-3d-image-synthesis-of-outdoor-scenes,"Originally published at:			Google’s New AI Model Improves 3D Image Synthesis of Outdoor Scenes | NVIDIA Technical Blog
Synthesizing 3D views of a scene using multiple camera angles, cameras, and lighting conditions is a challenging task for computer vision models and an important prerequisite for AR and VR applications. A group of researchers from Google Research and Google Brain are working to solve this problem by developing deep learning models that can synthesize…Powered by Discourse, best viewed with JavaScript enabled"
3829,gtc-2020-ai-as-a-service-aiaas-zero-to-kubeflow-supporting-your-data-science-teams-with-the-most-common-uses-cases,"GTC 2020 S22040
Presenters: Andrew Bull,NVIDIA; Adam Tetelman, NVIDIA
Abstract
We will provide an overview of how we built the underlying software stack using DeepOps, but we are going to walk you though deploying the most common use cases that are asked for with Kubeflow — interactive Jupyter notebooks, deploying DL pipelines/hyper-parameter search, and deploying models in production using the TensorRT Inference Server (TRTIS).Watch this session
Join in the conversation below.Powered by Discourse, best viewed with JavaScript enabled"
3830,gpu-support-for-ai-workloads-in-red-hat-openshift-4,"Originally published at:			GPU Support for AI Workloads in Red Hat OpenShift 4 | NVIDIA Technical Blog
Red Hat OpenShift is an enterprise-grade Kubernetes platform for managing Kubernetes clusters at scale, developed and supported by Red Hat. It offers a path to transform how organizations manage complex infrastructures on-premises as well as across the hybrid cloud. AI computing brings far-reaching transformations to modern business, including fraud detection in financial services and recommendations engines for entertainment and…Powered by Discourse, best viewed with JavaScript enabled"
3831,ray-tracing-essentials-part-7-denoising-for-ray-tracing,"Originally published at:			Ray Tracing Essentials Part 7: Denoising for Ray Tracing | NVIDIA Technical Blog
NVIDIA recently published Ray Tracing Gems, a deep-dive into best practices for real-time ray tracing. The book was made free-to-download, to help all developers embrace the bleeding edge of rendering technology. Ray Tracing Essentials is a seven-part video series hosted by the editor of Ray Tracing Gems, NVIDIA’s Eric Haines. The aim of this program…Powered by Discourse, best viewed with JavaScript enabled"
3832,deepsearch-license-question,"You mention in the blog that this feature requires DeepSearch with Nucleus. But it seems this only comes with the Enterprise license. Is it somehow possible for a single developer with the standard free license to use this extension?Yes, if you don’t have access to an Omniverse Enterprise Nucleus instance with DeepSearch enabled, you can uncheck the DeepSearch option in the extension and it will place boxes for the results.This topic was automatically closed 14 days after the last reply. New replies are no longer allowed.Powered by Discourse, best viewed with JavaScript enabled"
3833,gpu-accelerated-pc-solves-complex-problems-hundreds-of-times-faster-than-massive-cpu-only-supercomputers,"Originally published at:			GPU-Accelerated PC Solves Complex Problems Hundreds of Times Faster Than Massive CPU-only Supercomputers | NVIDIA Technical Blog
Russian scientists from Lomonosov Moscow State University used an ordinary GPU-accelerated desktop computer to solve complex quantum mechanics equations in just 15 minutes that would typically take two to three days on a large CPU-only supercomputer. Senior researchers Vladimir Pomerantcev and Olga Rubtsova and professor Vladimir Kukulin used a GeForce GTX 670 with CUDA and…Powered by Discourse, best viewed with JavaScript enabled"
3834,nvidia-announces-nsight-graphics-2019-2,"Originally published at:			https://developer.nvidia.com/blog/nvidia-announces-nsight-graphics-2019-2/
NVIDIA announces Nsight Graphics 2019.2! In this release, we greatly enhanced the DirectX Raytracing capabilities of the tool – adding DXR support for Nsight Aftermath, Acceleration Structure viewer improvements, and the ability to profile Acceleration Structure updates in the range profiler. In addition to DXR improvements, we have also added a D3D12fence visualization into GPU…Powered by Discourse, best viewed with JavaScript enabled"
3835,nvidia-deep-learning-institute-releases-accelerated-data-science-teaching-kit,"Originally published at:			https://developer.nvidia.com/blog/nvidia-dli-releases-accelerated-data-science-teaching-kit/
The NVIDIA Deep Learning Institute (DLI) released the Accelerated Data Science Teaching Kit, co-developed with Professor Polo Chau from Georgia Institute of Technology and Professor Xishuang Dong from Prairie View A&M University.Powered by Discourse, best viewed with JavaScript enabled"
3836,minimizing-deep-learning-inference-latency-with-nvidia-multi-instance-gpu,"Originally published at:			https://developer.nvidia.com/blog/minimizing-dl-inference-latency-with-mig/
Recently, NVIDIA unveiled the A100 GPU model, based on the NVIDIA Ampere architecture. Ampere introduced many features, including Multi-Instance GPU (MIG), that play a special role for deep learning-based (DL) applications. MIG makes it possible to use a single A100 GPU as if it were multiple smaller GPUs, maximizing utilization for DL workloads and providing…Hi, I have been trying to reproduce the Flower Demo on our system from this article: https://developer.nvidia.com/blog/minimizing-dl-inference-latency-with-mig. However, I got a little problem upon running the client GUI where it could not open the image directory.Could someone help to point out where I should put the images?Powered by Discourse, best viewed with JavaScript enabled"
3837,spotting-keywords-in-audio-and-video-files,"Originally published at:			Spotting Keywords in Audio and Video Files | NVIDIA Technical Blog
United States call centers generate more than a billion hours of audio recordings every year and less than 25% of the audio is made searchable or analyzed. Launching out of Y Combinator’s Winter 2016 class, DeepGram uses deep learning and GPUs hosted in the Amazon Web Services cloud to quickly index audio and make it searchable.…Powered by Discourse, best viewed with JavaScript enabled"
3838,swinburne-launches-new-gpu-accelerated-supercomputer,"Originally published at:			Swinburne Launches New GPU-Accelerated Supercomputer | NVIDIA Technical Blog
Swinburne University of Technology officially launched one of the most powerful supercomputers in Australia to help unlock the secrets of the Universe. Powered by Dell EMC, the $4 million OzSTAR supercomputer is equipped with 230 NVIDIA Tesla P100 GPUs and will reach a performance peak of 1.2 petaflops. The Swinburne-based Australian Research Council Centre of Excellence…Powered by Discourse, best viewed with JavaScript enabled"
3839,strategies-for-maximizing-data-center-energy-efficiency,"Originally published at:			https://developer.nvidia.com/blog/strategies-for-maximizing-data-center-energy-efficiency/
Data centers are an essential part of a modern enterprise, but they come with a hefty energy cost. To complicate matters, energy costs are rising and the need for data centers continues to expand, with a market size projected to grow 25% from 2023 to 2030. Globally, energy costs are already negatively affecting data centers…Powered by Discourse, best viewed with JavaScript enabled"
3840,top-5-public-sector-sessions-at-gtc-2019,"Originally published at:			Top 5 Public Sector Sessions at GTC 2019 | NVIDIA Technical Blog
Watch the video in this post to get a preview of some of the public sector sessions at GTC.Powered by Discourse, best viewed with JavaScript enabled"
3841,nvidia-accelerates-real-time-speech-to-text-transcription-3500x-with-kaldi,"Originally published at:			NVIDIA Accelerates Real Time Speech to Text Transcription 3500x with Kaldi | NVIDIA Technical Blog
Think of a sentence and repeat it aloud three times. If someone recorded this speech and performed a point-by-point comparison, they would find that no single utterance exactly matched the others. Similar to different resolutions, angles, and lighting conditions in imagery, human speech varies with respect to timing, pitch, amplitude, and even how base units…Hi,Will you stream the speech? Is 1pm local time, right?BR,EmilioI had a relatively easy time running the docker setup on a Tesla P4 as is. Can post my benchmark results and hardware configuration if it's interesting.Thanks,EmilianThat would be great. Please do!How many concurrent interference processes can the Tesla v100 handle? And To my understanding kaldi only processed interference on cpu...Hi @jwitsoe , thanks for the informative post, the performance benefits here are great :)I was particularly interested in the part ... Future container releases will focus on developer productivity, including scripts to help users quickly run their own ASR models and native support for additional pre-trained ones.Any updates on that? It would be great to read some docs about how to run an arbitrary pretrained Kaldi model here. Sorry if this exists already and I missed it.Powered by Discourse, best viewed with JavaScript enabled"
3842,gtc-2020-de-novo-protein-design-of-epitope-directed-inhibitors,"GTC 2020 S21348
Presenters: Mohammad ElGamacy,Friedrich Miescher Laboratory of the Max Planck Society
Abstract
Protein drugs have revolutionized modern cancer therapeutics, as protein-based binders can effectively target molecules that are otherwise undruggable by small molecules. Presently, such binders are based on monoclonal antibodies, which are developed and engineered through lengthy and resource-intensive processes of iterative optimization that are empirically guided. Conversely, de novo protein design can offer a rational means for generating new binders with bespoke scaffolds, guided by physics-based computations. Here, we’ll present our work on developing a purpose-built, GPU-accelerated computational pipeline for designing protein-based binders de novo. As a proof of principle, we designed proteins that target a key modulator of cancer metastasis. Our experimental characterization of only a few design candidates resulted in binders with strong binding affinities. Solving the structure of one design showed atomic-level agreement between the design model and the determined structure.Watch this session
Join in the conversation below.Powered by Discourse, best viewed with JavaScript enabled"
3843,gtc-2020-the-future-of-gpu-raytracing,"GTC 2020 S22263
Presenters: John Ison,NVIDIA Inc.; Kevin Margo,NVIDIA; Adrien Herubel,Autodesk; Jules Urbach,OTOY Inc; Vladimir Koylazov,Chaos Software, Ltd.; Max Liani,Pixar; Panagiotis Zompolas,Redshift Rendering Technologies; Luca Fascione,Weta Digital, Ltd.
Abstract
This panel brings together several of the world’s leading GPU raytracing technologists, plus a pioneering user, to discuss and debate how GPU raytracing will transform the creative process in design, animation, games, and visual effects.Watch this session
Join in the conversation below.Powered by Discourse, best viewed with JavaScript enabled"
3844,register-for-the-nvidia-metropolis-developer-webinars-on-sept-22,"Originally published at:			https://developer.nvidia.com/blog/register-for-the-nvidia-metropolis-developer-webinars-on-sept-22/
Sign up for webinars with NVIDIA experts and Metropolis partners on Sept. 22, featuring developer SDKs, GPUs, go-to-market opportunities, and more.Powered by Discourse, best viewed with JavaScript enabled"
3845,make-the-metaverse-with-us-go-behind-the-scenes-of-nvidia-omniverse-now-streaming-on-twitch,"Originally published at:			https://developer.nvidia.com/blog/make-the-metaverse-with-us-go-behind-the-scenes-of-nvidia-omniverse-now-streaming-on-twitch/
Hear directly from the engineers, designers, and creators that are at the edge of developing NVIDIA Omniverse, the virtual collaboration and physically accurate simulation platform, now live on Twitch.Powered by Discourse, best viewed with JavaScript enabled"
3846,gtc-2020-designing-with-jetson-nano,"GTC 2020 S21389
Presenters: Gabriele Gorla,NVIDIA; John Hsu,NVIDIA
Abstract
Learn how to integrate the Jetson Nano System on Module into your product effectively. We’ll explain how the engineers at NVIDIA design with the Jetson Nano platform. Topics include everything from feature selection to design trade-offs to electrical, mechanical, and thermal considerations, and more. We’ll also deep-dive into the creation of Jetson Nano Developer Kit and how you can leverage our design resources.Watch this session
Join in the conversation below.Powered by Discourse, best viewed with JavaScript enabled"
3847,jetson-project-of-the-month-opendog-a-gesture-controlled-robot,"Originally published at:			Jetson Project of the Month: OpenDog, a Gesture Controlled Robot | NVIDIA Technical Blog
James Bruton of XRobots was awarded the ‘Jetson Project of the Month’ for OpenDog V2. This project uses the NVIDIA Jetson Nano Developer Kit to recognize hand gestures and control a robot dog without a controller.  James, a robot inventor, thought it’d be nice if his OpenDog robot responded to hand gestures. To make this…Powered by Discourse, best viewed with JavaScript enabled"
3848,benefits-of-using-pull-requests-for-collaboration-and-code-review,"Originally published at:			https://developer.nvidia.com/blog/benefits-of-using-pull-requests-for-collaboration-and-code-review/
Learn about the benefits of using pull requests for code review and collaboration.Powered by Discourse, best viewed with JavaScript enabled"
3849,boost-ai-development-with-pretrained-models-and-the-nvidia-tao-toolkit,"Originally published at:			https://developer.nvidia.com/blog/boost-ai-development-with-pretrained-models-and-the-nvidia-tao-toolkit/
The latest version of the NVIDIA TAO Toolkit 4.0 boosts developer productivity with all-new AutoML capability, integration with third-party MLOPs services, and new pretrained vision AI models. The enterprise version now includes access to the full source code and model weights for pretrained models.  The toolkit enables efficient model training for vision and conversational AI.…will the output of these models work on the Jetson class devices ? If so what versions of Jetpack are supported ?TAO toolkit models work on the NVIDIA Jetson edge AI platform and are supported all the way up to the latest version of Jetpack 5.0.2.Powered by Discourse, best viewed with JavaScript enabled"
3850,ai-research-could-help-improve-alexa-s-speech-recognition-model-by-15,"Originally published at:			AI Research Could Help Improve Alexa’s Speech Recognition Model by 15% | NVIDIA Technical Blog
Researchers from Johns Hopkins University and Amazon published a new paper describing how they trained a deep learning system that can help Alexa ignore speech not intended for her, improving the speech recognition model by 15%. “Voice-controlled house-hold devices, like Amazon Echo or Google Home, face the problem of performing speech recognition of device directed…Powered by Discourse, best viewed with JavaScript enabled"
3851,optimizing-nvidia-ai-performance-for-mlperf-v0-7-training,"Originally published at:			Optimizing NVIDIA AI Performance for MLPerf v0.7 Training | NVIDIA Technical Blog
MLPerf is an industry-wide AI consortium that has developed a suite of performance benchmarks covering a range of leading AI workloads that are widely in use today. The latest MLPerf v0.7 training submission includes vision, language, recommenders, and reinforcement learning. NVIDIA submitted MLPerf v0.7 training results for all eight tests and the NVIDIA platform set…Powered by Discourse, best viewed with JavaScript enabled"
3852,data-structures,"What data structures do your customers ask for? Or do they care? Like, do you get requests that say “I’d really like to use X data structure, can you support that”?We support CSR and COO input formats within cuGraph.  Internally we reformat the data to an optimized DCSR hybrid format  which is highly optimized for our graph primitives and algorithm use.Do customers ask for anything else? (Like, is anything else on your radar where you say “we should think about supporting that”?)Vertex/edge masking/insertion/deletion. We’re planning to update our graph data structures to support these features.This topic was automatically closed 14 days after the last reply. New replies are no longer allowed.Powered by Discourse, best viewed with JavaScript enabled"
3853,asynchronous-error-reporting-when-printf-just-won-t-do,"Originally published at:			Asynchronous Error Reporting: When printf Just Won’t Do | NVIDIA Technical Blog
Some programming situations call for reporting “soft” errors asynchronously. While printf can be a useful tool, it can increase register use and impact performance. In this post, we present an alternative, including a header library for generating custom error and warning messages on the GPU without a hard stop to your kernel. Often error reporting…Powered by Discourse, best viewed with JavaScript enabled"
3854,ai-helps-classify-lung-cancer-at-the-pathologist-level,"Originally published at:			https://developer.nvidia.com/blog/ai-helps-classify-lung-cancer-at-the-pathologist-level/
According to the American Cancer Society, more than 229,000 people will be diagnosed with lung cancer in the United States this year, with adenocarcinoma being the most common type. To help with diagnosis, researchers from Dartmouth’s Norris Cotton Cancer Center and the Hassanpour Lab at Dartmouth University developed a deep learning-based system for automated classification…Powered by Discourse, best viewed with JavaScript enabled"
3855,building-ai-infrastructure-with-nvidia-dgx-a100-for-autonomous-vehicles,"Originally published at:			Building AI Infrastructure with NVIDIA DGX A100 for Autonomous Vehicles | NVIDIA Technical Blog
Autonomous vehicles (AV) are transforming the way we live, work, and play—creating safer and more efficient roads. In the future, every vehicle may be autonomous: cars, trucks, taxis, buses, and shuttles. AI and AV are transforming mobility and logistics, creating new business models and enormous efficiencies for the $10T transportation industry. These revolutionary benefits require…Powered by Discourse, best viewed with JavaScript enabled"
3856,the-future-of-computer-vision,"Originally published at:			https://developer.nvidia.com/blog/the-future-of-computer-vision/
Demonstrate your computer vision expertise by mastering cloud services, AutoML, and Transformer architectures.Thanks for reading and looking forward to your thoughtsThanks for sharingPowered by Discourse, best viewed with JavaScript enabled"
3857,dxr-tier-1-1-mesh-shading-and-sampler-feedback-now-available-in-windows-10-insider-preview-builds,"Originally published at:			DXR tier 1.1, Mesh Shading and Sampler Feedback Now Available in Windows 10 Insider Preview Builds | NVIDIA Technical Blog
To help developers boost the quality of their rendering and performance, Microsoft announced a suite of new graphics features, now available in Windows 10 Insider Preview Builds (20H1) through the Windows Insider Program.  “Back in October 2018, we released Windows 10 OS and SDK to support DirectX Raytracing (aka. DXR tier 1.0). Within one year…Powered by Discourse, best viewed with JavaScript enabled"
3858,hmm-linux-kernel-support,"Can a full list of the Linux Kernels that support HMM be provided as NVIDIA CUDA Toolkit 12.2 Unleashes Powerful Features for Boosting Applications | NVIDIA Technical Blog only states 6.1.24+ or 6.2.11+ (what about 6.3, 6.4, etc.)? Which executables in the extras/demo suite can be used to test HMM? Also, can the executables that are distributed with the demo suite linked against libglut be linked against libglut.so and not libglut.so.3 so a symbolic link does not have to be added for it such as in /usr/lib/x86_64-linux-gnu? Additionally, why is the “options nvidia NVreg_OpenRmEnableUnsupportedGpus=1” not automatically added by the current driver .run installer to a config file in /etc/modprobe.d/ since it seems to be necessary to get the open kernel driver to work currently per  XFree86/Linux-x86_64/535.86.05/README/kernel_open.html and given that a blacklist file is already being added there for the noveau driver? (The last url should have  us. download nvidia com in front of it, but is being blocked because of some stupid one URL per post as this account is deemed a new user.In general we don’t have a specific list of kernels, but anything after our upstream HMM patch set was applied should work: 6.1.24+, 6.2.11+, or 6.3+. For samples there’s no specific HMM sample, but you can use for example general vector add with malloc() and free() instead of CUDA-specific memory APIs.Regardling libglut.so that shouldn’t be the case, is that in a specific sample? We will investigate and get it fixed.nbody, oceanFFT & randomFogThanks for sharing thatPowered by Discourse, best viewed with JavaScript enabled"
3859,upcoming-event-openacc-and-hackathons-summit-2022,"Originally published at:			OpenACC and Hackathons Summit 2022
Join this digital conference from August 2-4 to learn how science is being advanced through the work done at Open Hackathons or accelerated using OpenACC.Powered by Discourse, best viewed with JavaScript enabled"
3860,furthering-nvidia-performance-leadership-with-mlperf-inference-1-1-results,"Originally published at:			https://developer.nvidia.com/blog/furthering-nvidia-performance-leadership-with-mlperf-inference-1-1-results/
A look at NVIDIA inference performance as measured by the MLPerf Inference 1.1 benchmark. NVIDIA delivers leadership performance across all workloads and scenarios, and in an industry-first, has delivered results on an Arm-based server in the data center category. MLPerf Inference 1.1 tests performance and power efficiency across usages including computer vision, natural language processing and recommender systems.Powered by Discourse, best viewed with JavaScript enabled"
3861,modeling-earth-s-atmosphere-with-spherical-fourier-neural-operators,"Originally published at:			https://developer.nvidia.com/blog/modeling-earths-atmosphere-with-spherical-fourier-neural-operators/
Machine learning-based weather prediction has emerged as a promising complement to traditional numerical weather prediction (NWP) models. Models such as NVIDIA FourCastNet have demonstrated that the computational time for generating weather forecasts can be reduced from hours to mere seconds, a significant improvement to current NWP-based workflows. Traditional methods are formulated from first principles and…Powered by Discourse, best viewed with JavaScript enabled"
3862,supercomputer-helps-provide-insight-into-mysterious-black-hole-relativistic-jets,"Originally published at:			Supercomputer Helps Provide Insight Into Mysterious Black Hole Relativistic Jets | NVIDIA Technical Blog
Researchers from University of Amsterdam and Northwestern University used the GPU-accelerated Blue Waters supercomputer to produce the first ever simulation to demonstrate that relativistic jets follow along with the precession of the tilted accretion disk around the black hole. “Understanding how rotating black holes drag the space-time around them and how this process affects what…Powered by Discourse, best viewed with JavaScript enabled"
3863,fast-and-fun-my-first-real-time-ray-tracing-demo,"Originally published at:			Fast and Fun: My First Real-Time Ray Tracing Demo | NVIDIA Technical Blog
Editor’s note: What happens when a veteran graphics programmer with substantial experience in old-school ray tracing (in other words, offline rendering), gets hold of hardware capable of real-time ray tracing? I’m finally convinced. I joined NVIDIA around SIGGRAPH, just as the RTX hardware for ray tracing was announced. I saw the demos, heard the stats, but…Builds ok but issues running with Windows 10 October Update (v 1809), which is the first formal release with DXR supported.It does not work on RS5 yet. No ETA on when it’ll be ready. (I'm quoting Chris Wyman here)No demo code?Available here: https://github.com/NVIDIAGa...Looks awesome!I would love to see a recorded video of the demo :oObviously won't be able to do realtime but can I do rendering without an RTX card? If not, can someone dig deep into their pockets and get me one lol.Also, the download for the MDL SDK appears to be broken. Seemingly no extension is provided or no mime and I am to paranoid to rename the extension in an attempt to extract it.Oh, I see, you're talking about this other blog post, https://devblogs.nvidia.com... - the demo code here doesn't use the MDL SDK, you don't need it. I think these demos *could* run on other cards, but haven't tried myself - let me know what you find. I suspect we'll need to update them to the Windows 1809 update (currently pulled by Microsoft due to a file deletion problem).For the MDL SDK, you could directly download it from https://github.com/NVIDIA/M.... I'll ask others at NVIDIA to fix the direct download link on the https://developer.nvidia.co... giving the file ""mdl-sdk-307800.2890.solitairetheme8"" - thanks for letting us know!we need real time generative cfd for unreal! Incredible study! thanks for sharing.I am having a hard time getting started using Visual Studio 2019. Is there an issue here with the incompatibility of VS2017 build tools and those of VS2019?How should I proceed? Thanks in advance for any help.Powered by Discourse, best viewed with JavaScript enabled"
3864,nsight-visual-studio-edition-5-5-introduces-graphics-pixel-history-next-gen-cuda-gpu-cpu-debugging-next-gen-cuda-profiling-and-now-supports-volta-gpus-win10-rs3-and-cuda-9-1,"Originally published at:			Nsight Visual Studio Edition 5.5 Introduces Graphics Pixel History, Next-Gen CUDA GPU+CPU debugging, Next-Gen CUDA Profiling, and now supports Volta GPUs, Win10 RS3, and CUDA 9.1 | NVIDIA Technical Blog
NVIDIA Nsight Visual Studio Edition 5.5 is now available for download in the NVIDIA Registered Developer Program. This release extends support to the latest Volta GPUs and Win10 RS3. The Graphics Debugger adds Pixel History (DirectX 11, OpenGL) and OpenVR 1.0.10 support as well as Vulkan and Range Profiler improvements. Nsight Visual Studio Edition version…Powered by Discourse, best viewed with JavaScript enabled"
3865,powering-automl-enabled-ai-model-training-with-clara-train,"Originally published at:			Powering AutoML-enabled AI Model Training with Clara Train | NVIDIA Technical Blog
Deep neural networks (DNNs) have been successfully applied to volume segmentation and other medical imaging tasks. They are capable of achieving state-of-the-art accuracy and can augment the medical imaging workflow with AI-powered insights.  However, training robust AI models for medical imaging analysis is time-consuming and tedious and requires iterative experimentation with parameter tuning. On the…Powered by Discourse, best viewed with JavaScript enabled"
3866,high-performance-geometric-multi-grid-with-gpu-acceleration,"Originally published at:			https://developer.nvidia.com/blog/high-performance-geometric-multi-grid-gpu-acceleration/
Linear solvers are probably the most common tool in scientific computing applications. There are two basic classes of methods that can be used to solve an equation: direct and iterative. Direct methods are usually robust, but have additional computational complexity and memory capacity requirements. Unlike direct solvers, iterative solvers require minimal memory overhead and feature better computational…Hi Nikolay, First of all very interesting text, and I am interested if you have some data about  runtimes (in seconds) on a single CPU/GPU for solving  problems with approx O(10^6) DOFs?Hi Stefan,Thanks for the feedback. I can give you numbers for ~2M DOFs using 128^3 grid size. The fourth-order F-cycle takes 53ms on a single NVIDIA Tesla K40 with Intel Xeon E5-2690 v2. The max norm reported by HPGMG in this case is 7.5e-06.Hi Nikolay,Thank you for the post, this is a great code.I am very interested on running a 2D multigrid solver on the GPU.Do you have such a version by now?Hi, Thank you for the feedback. Unfortunately, there is no support for 2D multigrid as the original CPU code and even the GPU implementation were designed for 3D grids from the ground up.Interesting article!Powered by Discourse, best viewed with JavaScript enabled"
3867,accelerating-random-forests-up-to-45x-using-cuml,"Originally published at:			https://developer.nvidia.com/blog/accelerating-random-forests-up-to-45x-using-cuml/
This post was originally published on RAPIDsai. Random forests are a popular machine learning technique for classification and regression problems. By building multiple independent decision trees, they reduce the problems of overfitting seen with individual trees. In this post, I review the basic random forest algorithms, show how their training can be parallelized on NVIDIA…Hi,
Thanks for this helpful post. I have issues using dask_util.  I am getting the error:NameError: name 'dask_utils' is not defined.Could you please explain, where I can get the dask_util file?Best,
Mandar KulkarniPowered by Discourse, best viewed with JavaScript enabled"
3868,real-time-object-detection-in-10-lines-of-python-on-jetson-nano,"Originally published at:			https://developer.nvidia.com/blog/realtime-object-detection-in-10-lines-of-python-on-jetson-nano/
To help you get up-and-running with deep learning and inference on NVIDIA’s Jetson platform, today we are releasing a new video series named Hello AI World to help you get started.  In the first episode Dustin Franklin, Developer Evangelist on the Jetson team at NVIDIA, shows you how to perform real-time object detection on the…I have ran this on my Jetson Nano!  Awesome!!!  Now I need to run in on a dGPU. What are the libraries that need to be changed?  What else?  Is there an example of doing this project on a dGPU?Powered by Discourse, best viewed with JavaScript enabled"
3869,discovering-gpu-friendly-deep-neural-networks-with-unified-neural-architecture-search,"Originally published at:			https://developer.nvidia.com/blog/discovering-gpu-friendly-deep-neural-networks-with-unified-neural-architecture-search/
After the first successes of deep learning, designing neural network architectures with desirable performance criteria for a given task (for example, high accuracy or low latency) has been a challenging problem. Some call it alchemy and some intuition, but the task of discovering a novel architecture often involves a tedious and costly trial-and-error process of…Powered by Discourse, best viewed with JavaScript enabled"
3870,in-the-trenches-at-gtc-programming-gpus-with-openacc,"Originally published at:			https://developer.nvidia.com/blog/trenches-gtc-programming-gpus-openacc/
By Adnan Boz (GTC 2012 Guest Blogger) It’s my first day at the GPU Technology Conference and I’ve already had the opportunity to meet gurus like Mark Harris (Chief Technologist, GPU Computing, NVIDIA, and founder of GPGPU.org) and learn about the latest advancements in the GPU and HPC arena from people like NVIDIA’s Will Ramey…Powered by Discourse, best viewed with JavaScript enabled"
3871,getting-started-with-the-deep-learning-accelerator-on-nvidia-jetson-orin,"Originally published at:			https://developer.nvidia.com/blog/getting-started-with-the-deep-learning-accelerator-on-nvidia-jetson-orin/
Learn how to free your Jetson GPU for additional tasks by deploying neural network models on the NVIDIA Jetson Orin Deep Learning Accelerator (DLA).Powered by Discourse, best viewed with JavaScript enabled"
3872,new-nvidia-kaolin-library-release-streamlines-3d-deep-learning-research-workflows,"Originally published at:			https://developer.nvidia.com/blog/new-nvidia-kaolin-library-release-streamlines-3d-deep-learning-research-workflows/
3D deep learning researchers can build on more cutting edge algorithms and simplify their workflows with the latest version of the Kaolin PyTorch Library.Powered by Discourse, best viewed with JavaScript enabled"
3873,end-to-end-blueprint-for-customer-churn-modeling-and-prediction-part-1,"Originally published at:			https://developer.nvidia.com/blog/end-to-end-blueprint-for-customer-churn-modeling-and-prediction-part-1/
If you want to solve a particular kind of business problem with machine learning, you’ll likely have no trouble finding a tutorial showing you how to extract features and train a model. However, building machine learning systems isn’t just about training models or even about finding the best features; if you want a blueprint for…Thnx for this article! The series sounds promising It addresses exactly our needs at a medium insurance company. Unfortunately I’m always miss the exact preconditions for installing rapids on my own workstations. At least one environment part have the wrong version number. In most cases I have installed a newer CUDA version (often to test new features).(Your link to the DataFrame.explain function is not working. It should be https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.sql.DataFrame.explain.html#pyspark.sql.DataFrame.explain )Powered by Discourse, best viewed with JavaScript enabled"
3874,applying-inference-over-specific-frame-regions-with-nvidia-deepstream,"Originally published at:			Applying Inference over Specific Frame Regions with NVIDIA DeepStream | NVIDIA Technical Blog
This tutorial shares how to apply inference over a predefined area of the incoming video frames.Great article! I find it very insightful and shows the hidden power of DeepStream SDK.A small question: After Figure 1 there is a paragraph that starts with "" Here are the performance metrics when yolov4 is applied over the entire frame compared to over the tile."", How can we see this comparison?Thanks.I have question is it possible to update ROI at runtime using NvDsPreprocessing or any other way?Powered by Discourse, best viewed with JavaScript enabled"
3875,researchers-demo-almost-unlimited-size-brain-simulations-using-gpus,"Originally published at:			https://developer.nvidia.com/blog/researchers-demo-almost-unlimited-size-brain-simulations-using-gpus/
To improve brain simulation technology, a team of researchers from the University of Sussex developed a GPU-accelerated approach that can generate brain simulation models of almost-unlimited size.  Researchers Dr. James Knight and Thomas Nowotny from the University of Sussex’s School of Engineering and Informatics detailed the work in a paper published in Nature Computational Science…Powered by Discourse, best viewed with JavaScript enabled"
3876,announcing-nvidia-nemo-fast-development-of-speech-and-language-models,"Originally published at:			Speeding Up Development of Speech and Language Models with NVIDIA NeMo | NVIDIA Technical Blog
This is an updated version of Neural Modules for Fast Development of Speech and Language Models. This post is updated with information about pretrained models in NGC and fine-tuning models on custom dataset sections, upgrades the NeMo diagram with the text-to-speech collection, and replaces the AN4 dataset in the example with the LibriSpeech dataset. As…Powered by Discourse, best viewed with JavaScript enabled"
3877,how-does-cugraph-support-gnns,"Thanks for hosting this - sorry if this is a dumb question but I am interested to know how exactly does cuGraph support GNN’sThanks again for your attention.Thanks !This topic was automatically closed 14 days after the last reply. New replies are no longer allowed.Powered by Discourse, best viewed with JavaScript enabled"
3878,creating-custom-production-ready-ai-models-faster-with-nvidia-tao,"Originally published at:			Creating Custom, Production-Ready AI Models Faster with NVIDIA TAO | NVIDIA Technical Blog
Learn about the latest updates to NVIDIA TAO, an AI-model-adaptation framework, and NVIDIA TAO toolkit, a CLI and Jupyter notebook-based version of TAO.Powered by Discourse, best viewed with JavaScript enabled"
3879,optimizing-data-transfer-using-lossless-compression-with-nvidia-nvcomp,"Originally published at:			https://developer.nvidia.com/blog/optimizing-data-transfer-using-lossless-compression-with-nvcomp/
One of the most interesting applications of compression is optimizing communications in GPU applications. GPUs are getting faster every year. For some apps, transfer rates for getting data in or out of the GPU can’t keep up with the increase in GPU memory bandwidth or computational power. Often, the bottleneck is the interconnect between GPUs…Hello CUDA developers!
Hope you enjoyed our blog about compression, and can find time to play with the library. Let us know if you have any questions or comments. Also feel free to submit issues directly to the GitHub page!@nsakharnykh, very interesting new feature and blog entry !from the introOften, the bottleneck is the interconnect between GPUs or between CPU and GPU.does it mean we can use nvcomp to compress data on the CPU and decompress it on the GPU (or vice versa) ?Currently, nvcomp only provides GPU implementations for compressors and decompressors, and one can implement CPU variants outside of nvcomp, since the compression format is fully open and explained in the docs. The main use case highlighted in the blog is for compressing GPU-to-GPU communications, and in this case we only need GPU-side compressors/decompressors. In near future we are planning to enable better compatibility with standard LZ4, so one can use existing CPU LZ4 libraries to compress on the CPU and nvcomp to decompress on the GPU - this is tracked here liblz4 compability · Issue #20 · NVIDIA/nvcomp · GitHub. Also see the following issue for tracking general CPU implementations  CPU compression/decompression implementations? · Issue #12 · NVIDIA/nvcomp · GitHub, but it’s not on our roadmap at the moment.Impressive work! Thank you for the blog post.Is there an overlap between the compression (or decompression) and the computation happening on the GPUs?@Alturkestani Thanks for the great question. In this example, we are not overlapping compression/decompression with other computations/operations.Overlapping CPU computations with compression/decompression is relatively easy, as both are implemented asynchronously in our current API, so you could initiate compression/decompression, and then perform computations on the CPU while the GPU is busy.Overlapping data transfer with compression/decompression, requires splitting the data into smaller chunks, so that while one chunk is being transferred, another can be compressed/decompressed.Powered by Discourse, best viewed with JavaScript enabled"
3880,building-a-real-time-redaction-app-using-nvidia-deepstream-part-1-training,"Originally published at:			Building a Real-time Redaction App Using NVIDIA DeepStream, Part 1: Training | NVIDIA Technical Blog
Some of the biggest challenges in deploying an AI-based application are the accuracy of the model and being able to extract insights in real time. There’s a trade-off between accuracy and inference throughput. Making the model more accurate makes the model larger which reduces the inference throughput.  This post series addresses both challenges. In part…There is a typo mistake:docker run -it --gpus all --rm --ipc=host -v $DATA_DIR:/data -v $WORKING_DIR:/src -w /src nvcr.io/nvidian/pytorch:19....The right container is (nvidia / not nvidian)docker run -it --gpus all --rm --ipc=host -v $DATA_DIR:/data -v $WORKING_DIR:/src -w /src nvcr.io/nvidia/pytorch:19.0...The typo has been fixed on the blog. Thank you Gary for pointing that out.Hello,I am trying to train as documented in this blog but I am getting this error, due to optimizer modifications you suggested in train.py ""optimizer = Adam(model.parameters(), lr=lr, weight_decay=0.0004, amsgrad=True)""Here is the error:    optimizer = Adam(model.parameters(), lr=lr, weight_decay=0.0004, amsgrad=True)NameError: name 'Adam' is not definedTo fix this, you need to document in the blog to replace:from torch.optim import SGDwithfrom torch.optim import AdamRgds,FlorinThank You Florin. Will get that fixed.Can the model be open sourced ?Many of us don't have compute to train on Open Images.We unfortunately can’t open source the model due to licensing restrictions with the images, however you could consider train your own model using a GPU instance from any major cloud service provider.Hi,I ´m trying to download the training open images using “download_open_images.sh” file. The problem is that the page from which the sh file tries to download images is not available. Is there another way to download them?Powered by Discourse, best viewed with JavaScript enabled"
3881,nvidia-index-now-available-on-google-cloud,"Originally published at:			NVIDIA IndeX Now Available on Google Cloud | NVIDIA Technical Blog
NVIDIA IndeX is now available on the Google Cloud Marketplace. With the IndeX SDK, scientists and researchers can visualize, interact with, and modify massive data sets, and also navigate to the most pertinent parts of the data — all in real time.Powered by Discourse, best viewed with JavaScript enabled"
3882,efficiently-scale-llm-training-across-a-large-gpu-cluster-with-alpa-and-ray,"Originally published at:			https://developer.nvidia.com/blog/efficiently-scale-llm-training-across-a-large-gpu-cluster-with-alpa-and-ray/
When used together, Alpa and Ray offer a scalable and efficient solution to train LLMs across large GPU clusters.Powered by Discourse, best viewed with JavaScript enabled"
3883,gtc-2020-nvtabular-gpu-accelerated-etl-for-recommender-systems,"GTC 2020 S21651
Presenters: Julio Perez,NVIDIA; Even Oldridge, NVIDIA
Abstract
Recommender systems require massive datasets to train, particularly for deep learning based solutions. The transformation of these datasets in order to prepare them for model training is particularly challenging. Often the time taken to do steps such as feature engineering, categorical encoding and normalization of continuous variables exceeds the time it takes to train a model. NVTabular is an open source feature engineering and preprocessing library for tabular data designed to quickly and easily manipulate terabyte scale datasets used to train deep learning based recommender systems. It provides a high level abstraction to simplify code, making development faster, and accelerates computation on the GPU using the RAPIDS cuDF library. It is available for download and contributions at GitHub - NVIDIA-Merlin/NVTabular: NVTabular is a feature engineering and preprocessing library for tabular data designed to quickly and easily manipulate terabyte scale datasets used to train deep learning based recommender systems.. A part of the Merlin Recommenders Framework, it pairs perfectly with HugeCTR to provide a straightforward method to train huge deep learning based recommender systems on GPU.Watch this session
Join in the conversation below.Powered by Discourse, best viewed with JavaScript enabled"
3884,evolving-record-fast-optoelectronic-chips-for-data-center-networks,"Originally published at:			https://developer.nvidia.com/blog/evolving-record-fast-optoelectronic-chips-for-data-center-networks/
The innovative technologies developed in the plaCMOS project provide the foundation for the evolution of optical interconnects in data center networks.Powered by Discourse, best viewed with JavaScript enabled"
3885,ai-identifies-legal-risks-in-ndas,"Originally published at:			https://developer.nvidia.com/blog/ai-identifies-legal-risks-in-ndas/
LawGeex, an Israel-based startup focused on automating contract reviews, released a study showing its deep learning software outperforms lawyers at identifying legal risks in nondisclosure agreement contracts. “Don’t expect machines to kill lawyers’ careers though,” said Shmuli Goldberg, vice president of marketing at LawGeex. He likened the use of LawGeex AI to people using spellcheckers…Powered by Discourse, best viewed with JavaScript enabled"
3886,algorithm-achieves-better-accuracy-than-humans-at-reading-lips,"Originally published at:			https://developer.nvidia.com/blog/algorithm-achieves-better-accuracy-than-humans-at-reading-lips/
Researchers at the University of East Anglia in the UK developed an algorithm that is able to interpret mouthed words with a greater degree of accuracy than human lip readers. Using Tesla K80 GPUs, the researchers trained a deep learning model to recognize mouth shapes corresponding to certain sounds as they are spoken, without any audio input…Powered by Discourse, best viewed with JavaScript enabled"
3887,nvidia-jetson-project-of-the-month-recognizing-birds-by-sound,"Originally published at:			https://developer.nvidia.com/blog/nvidia-jetson-project-of-the-month-recognizing-birds-by-sound/
Learn how researchers used portable devices connected to the NVIDIA Jetson Nano Developer Kit to capture audio recordings for bird identification.Powered by Discourse, best viewed with JavaScript enabled"
3888,zillow-launches-ai-powered-app-that-can-create-a-3d-home-tour,"Originally published at:			https://developer.nvidia.com/blog/zillow-launches-ai-powered-app-that-can-create-a-3d-home-tour/
To help potential homebuyers get a 360-degree tour of a home, Zillow, the online real estate database company, recently launched a new app and service across North America that relies on machine learning to generate 3D walkthroughs of a home.  “Previously, 3D tours were only found on high-end or expensive homes, due to the high…Powered by Discourse, best viewed with JavaScript enabled"
3889,gtc-2020-gpu-accelerated-data-pipeline-and-machine-learning-on-drive-agx-using-rapids,"GTC 2020 S21665
Presenters: Andy Park,NVIDIA; Anurag Dixit,NVIDIA
Abstract
We’ll present the extended capability of RAPIDS on the DRIVE AGX platform by demonstrating how it enhances the in-car user experience, with examples. ML algorithms are used extensively to address various challenges in autonomous cars. They include complex, multi-stage data science pipeline, sensor data processing, modeling, and analytics to accomplish new ML-based applications. Potential applications include recommender systems through driver or vehicle personalization, visual analytics of driving data, classification of driver condition, driving scenario, and more. In many cases, application workload runs on CPUs or ECUs, leading to performance bottlenecks as data size and their computes increase. Application workload can be parallelized, causing significant speedup, using GPUs. NVIDIA developed RAPIDS to accelerate entire end-to-end data science and analytics pipelines on GPUs. In this talk, we present extended capability of RAPIDS on NVIDA DRIVE AGX platform by demonstrating how RAPIDS works.Watch this session
Join in the conversation below.Powered by Discourse, best viewed with JavaScript enabled"
3890,deploying-a-scalable-object-detection-inference-pipeline,"Originally published at:			Deploying a Scalable Object Detection Inference Pipeline, Part 1 | NVIDIA Technical Blog
To learn more about architecting an object detection inference pipeline at scale, join the Autonomous Driving at Scale: Architect and Deploy Object Detection Inference Pipelines webinar on Sept. 2, led by NVIDIA and TCS experts. This post is the first in a series on Autonomous Driving at Scale, developed with Tata Consulting Services (TCS). In…Powered by Discourse, best viewed with JavaScript enabled"
3891,nvidia-arm-and-intel-publish-fp8-specification-for-standardization-as-an-interchange-format-for-ai,"Originally published at:			NVIDIA, Arm, and Intel Publish FP8 Specification for Standardization as an Interchange Format for AI | NVIDIA Technical Blog
NVIDIA, Arm, and Intel jointly propose a FP8 format for standardization as an interchange format for AI and improve computational efficiency.An unintended (or intended?) method of feature selection at each layer? Features (at each layer) < magnitude for representation are effectively zero, removing them from the “feature set” input to the next layer. Lowest magnitude features are typically noisy (analogy to recursive feature elimination (RFE) for linear SVM’s). So maybe the results indicate a bit of tradeoff between accuracy of hyperplane orientation at each node, and the practical effectiveness of feature selection. (I used to hate the idea of feature selection, but now use it heavily, including in my latest classifier, currently in use in a manufacturing quality control application for a high end sensor.)Powered by Discourse, best viewed with JavaScript enabled"
3892,protect-your-trademark-with-artificial-intelligence,"Originally published at:			Protect Your Trademark with Artificial Intelligence | NVIDIA Technical Blog
Australian-based TrademarkVision developed a deep learning-based reverse visual search platform that protects your brand by identifying similar trademarks from around the world. Simply upload your image to the platform, and their image recognition technology will compare it against other trademarked logos – making it much easier to identify IP infringements than the previous time-consuming and…Powered by Discourse, best viewed with JavaScript enabled"
3893,in-the-trenches-at-gtc-inside-kepler,"Originally published at:			https://developer.nvidia.com/blog/trenches-gtc-inside-kepler/
By Tomasz Bednarz (GTC 2012 Guest Blogger) I had been eagerly anticipating the “Inside Kepler” session since GTC 2012 opened. On Wednesday arvo, May 16th, two NVIDIA blokes, Stephen Jones (CUDA Model Lead) and Lars Nyland (Senior Architect), warmly introduced the new Kepler GK110 GPU, proudly announcing that it is all about “performance, efficiency and programmability.” Lars and Stephen…Powered by Discourse, best viewed with JavaScript enabled"
3894,google-researchers-use-ai-to-bring-still-photos-to-life,"Originally published at:			Google Researchers Use AI to Bring Still Photos to Life | NVIDIA Technical Blog
Researchers from Google developed a deep learning-based system that can create short video clips from still images shot on stereo cameras, VR cameras, and dual lens cameras, such as an iPhone 7 or X. “Given two images with known camera parameters, our goal is to learn a deep neural net to infer a global scene…Powered by Discourse, best viewed with JavaScript enabled"
3895,on-demand-webinar-jetpack-sdk-overview-installation-and-key-features,"Originally published at:			https://developer.nvidia.com/blog/on-demand-webinar-jetpack-sdk-overview-installation-and-key-features/
JetPack SDK is the most comprehensive solution for building Jetson-based AI applications for autonomous machines. It includes the latest OS image, along with libraries and APIs, samples, developer tools, and documentation — all that is needed to accelerate your AI application development. This webinar will provide you with a deep understanding of JetPack including live…Powered by Discourse, best viewed with JavaScript enabled"
3896,accelerating-the-pony-ai-av-sensor-data-processing-pipeline,"Originally published at:			https://developer.nvidia.com/blog/accelerating-the-pony-ai-av-sensor-data-processing-pipeline/
Here’s how Pony.ai, which develops autonomous driving systems for robotaxis and trucks, uses GPU technology to develop a highly efficient data processing pipeline.Powered by Discourse, best viewed with JavaScript enabled"
3897,deadline-approaching-for-february-2016-gpu-eurohack,"Originally published at:			Deadline Approaching for February 2016 GPU EuroHack | NVIDIA Technical Blog
In partnership with Jülich Supercomputing Center and Oak Ridge National Labs, TU Dresden  is hosting a “EuroHack” GPU Hackathon February 29 to March 4, 2016 at their Germany campus. Paired with two GPU mentors each, teams of scientific application developers will set forth on a five-day project to accelerate their code with GPUs. The mentors provide…Powered by Discourse, best viewed with JavaScript enabled"
3898,nvidia-research-achieves-ai-training-breakthrough-using-limited-datasets,"Originally published at:			NVIDIA Research Achieves AI Training Breakthrough | NVIDIA Blog
Powered by Discourse, best viewed with JavaScript enabled"
3899,implementing-robotics-applications-with-ros-2-and-ai-on-the-nvidia-jetson-platform,"Originally published at:			https://developer.nvidia.com/blog/implementing-robotics-applications-with-ros-2-and-ai-on-jetson-platform-2/
Deep learning is being adopted in robotics to accurately navigate indoor environments, detect and follow objects of interest, and maneuver without collisions. However, the increasing complexity of deep learning makes it challenging to accommodate these workloads on embedded systems. While you can make trade-offs between accuracy and deep learning model size, compromising accuracy to meet…Hi @jwitsoe
Could you extend how to run it on jp44 AGX Jetson within docker container please?
The instruction seems reffering to missed steps:
> sh docker_run.shThis will initialize docker. Clone this repository using following command and follow build and run instructions for ros2 package from here.What are the exact steps to follow after executing run docker_sh?
I wasn’t abler to find them in the file ros2_trt_pose/README.md at main · NVIDIA-AI-IOT/ros2_trt_pose · GitHub
Moreover, when trying to sort out the containers iinternals with steps from a separate non dockerized setup it would run into error afterMay I know how to proceed with the docker implementation, please?
ref: the instruction for Docker implementation is abrupt · Issue #2 · NVIDIA-AI-IOT/ros2_trt_pose · GitHubHi @jwitsoe
I have run into some errors and would appreciate your inputs in this topic.
I have been working on a Jetbot with 64GB SD Card and while trying to install ROS 2 crystal on the platform I have exhausted the space allocated in  Partition 1 (26GB). Can I extend the partition in the free space or try some other method for installing ROS 2?
I have referred the following link to install ROS 2 crystal - Building ROS 2 on Linux — ROS 2 Documentation: Crystal documentationPartition Description in my system:

Screenshot from 2021-07-06 22-37-02800×722 60.6 KB
Error log:Any input would be much appreciated! Thank You.it saysmaybe you need to address partitioning@prajwal You might want to increase partition space as @Andrey1984 is suggesting.Thank You for the reply, I extended the partition to the full unallocated space.

single_partition800×728 59.8 KB

The installation works now.Powered by Discourse, best viewed with JavaScript enabled"
3900,training-a-recommender-system-on-dgx-a100-with-100b-parameters-in-tensorflow-2,"Originally published at:			https://developer.nvidia.com/blog/training-a-recommender-system-on-dgx-a100-with-100b-parameters-in-tensorflow-2/
Learn how using the combination of model parallel and data parallel
enables practitioners to train large-scale recommender systems in minutes instead of days.Powered by Discourse, best viewed with JavaScript enabled"
3901,leading-mlperf-training-2-1-with-full-stack-optimizations-for-ai,"Originally published at:			https://developer.nvidia.com/blog/leading-mlperf-training-2-1-with-full-stack-optimizations-for-ai/
As AI becomes increasingly capable and pervasive, MLPerf benchmarks, developed by MLCommons, have emerged as an invaluable tool for organizations to evaluate the performance of AI infrastructure across a wide range of popular AI-based workloads. MLPerf Training v2.1—the seventh iteration of this AI training-focused benchmark suite—tested performance across a breadth of popular AI use cases,…Powered by Discourse, best viewed with JavaScript enabled"
3902,develop-smaller-speech-recognition-models-with-nvidia-s-nemo-framework,"Originally published at:			Develop Smaller Speech Recognition Models with NVIDIA’s NeMo Framework | NVIDIA Technical Blog
As computers and other personal devices have become increasingly prevalent, interest in conversational AI has grown due to its multitude of potential applications in a variety of situations. Each conversational AI framework is comprised of several more basic modules such as automatic speech recognition (ASR), and the models for these need to be lightweight in…For all the talk about the edge, the article fails to describe inference times or edge hardware requirements. Does it run on jetson nano? Rasberri pi? How much ram? EtcYes, QuartzNet inference in NeMo does run on Jetson Nano. We never tried Rasberri pi though. Note that QuartzNet is an architecture - e.g. QuartzNet15x5 has B=15 blocks with R=5 sub-blocks within each block. See https://nvidia.github.io/Ne... . To lessen memory footprint you can chose to have less blocks and/or subblocks, but then you will have to re-train yourself. Another (very effective) way to reduce memory footprint is to give it audio in shorter segments.Can I make an inference using NeMo on wav2letter? Does the library have methods to do it?We don't have wav2letter model in NeMo, but Jasper model is  similar to itSorry I did not explain myself correctly. I was referring to if the NeMo library has methods to make inference with a model?yes. we also provide high-quality pre-trained checkpoints for QuartzNetTake a look at https://github.com/NVIDIA/N...I have a question about retraining the NeMo QuartzNet 15x5. I would like to re-train on my own data set, but I don’t know any information, for example, what is the minimum and maximum size of the audio that QuartzNet 15x5 supports? What are the formats that QuartzNet 15x5 supports apart from wav?There’s no set minimum or maximum audio length for training (other than what’s limited by your GPU memory), but we tend to use a rule of thumb of 0.1s to around 17s.As for other formats, this PR introduced support for additional audio formats (including MP3, Ogg, etc.) via Pydub. Wav is the most well-tested audio format, so please let us know if you run into bugs/problems with any other formats by opening an issue.Hello everyone,Per “QuartzNet replaces Jasper’s 1D convolutions with 1D time-channel separable convolutions, which use many fewer parameters.”, what is the exact difference between the two? I might’ve gotten this wrong but 1D time-channel separable convolutions is basically two 1D convolutions with respect to time and channel, right? However, what about the regular 1D convolutions? What does it consist of?Also,…with five blocks that repeat fifteen times plus four additional convolutional layersshould be “…with five blocks that repeat three times with five subblock plus four additional convolutional layers”, right?Thanks in advance!Hi! 1D convolutions and 1D time-channel separable convolutions perform a roughly comparable operation (across time and channel of the input), but the latter splits it into two steps to save on parameters.In a normal 1D convolution, you’ll have K*c_in*c_out params since you need c_out kernels for every input channel c_in, multiplied by the number of params in the kernel.In a 1D time-channel separable convolution, we do the 1D convolution across each channel separately (c_in*K params), then a pointwise convolution for each time frame across all channels (c_in*c_out params), for a total of c_in*K + c_in*c_out parameters. It’s a little messy to try to explain without any images, so here’s a diagram I made a while ago that might help visualize this:

separable_conv1434×1014 79.1 KB
Re the 15, yep, that’s a typo… That should be five blocks that repeat three times each.It’s been a few years and things are fuzzier than I’d like, but hopefully this helps!Powered by Discourse, best viewed with JavaScript enabled"
3903,latest-update-to-isaac-sdk-is-available-for-download,"Originally published at:			https://developer.nvidia.com/blog/the-latest-update-to-isaac-sdk-is-available-for-download/
NVIDIA announces the release of Isaac SDK 2020.2, an update that packs an essential set of robot tools and added capabilities for intralogistics applications. Developers will experience greater support from both our deeper software integration with Isaac Sim built on the Omniverse platform and hardware support from our partner ecosystem.  This customer feedback-focused update to…Powered by Discourse, best viewed with JavaScript enabled"
3904,upcoming-event-nvidia-at-the-game-developers-conference-gdc,"Originally published at:			Game Developer Conference (GDC) 2023 | NVIDIA
Join us March 20-24 to discover the latest NVIDIA RTX and neural rendering technologies accelerating game development.Powered by Discourse, best viewed with JavaScript enabled"
3905,meet-the-researcher-jean-philip-piquemal-accelerating-hpc-for-molecular-dynamics-simulations,"Originally published at:			Meet the Researcher, Jean-Philip Piquemal, Accelerating HPC for Molecular Dynamics Simulations | NVIDIA Technical Blog
“Meet the Researcher” is a series in which we spotlight different researchers in academia who are using GPUs to accelerate their work. This week, we spotlight Jean-Philip Piquemal, Professor at Sorbonne University in Paris, France. Jean-Philip Piquemal is Professor of Theoretical Chemistry at Sorbonne University and Director of the Theoretical Chemistry Laboratory (LCT), a joint…Powered by Discourse, best viewed with JavaScript enabled"
3906,updates-to-nvidia-s-unreal-engine-4-branch-dlss-and-rtxgi-available-now,"Originally published at:			https://developer.nvidia.com/blog/updates-to-nvidias-unreal-engine-4-branch-dlss-and-rtxgi-available-now/
NVIDIA has released updates to DLSS, NVIDIA’s Unreal Engine 4 Branch, and RTXGl.Powered by Discourse, best viewed with JavaScript enabled"
3907,font-identification-from-real-world-images,"Originally published at:			https://developer.nvidia.com/blog/font-identification-from-real-world-images/
DeepFont is a new CUDA-based system by researchers from Adobe, Google, and Snapchat that achieves higher than 80% top-5 accuracy. Graphic designers have the desire to identify the fonts they encounter in daily life for later use. While they might take a photo of the text of a particularly interesting font and seek out an…Powered by Discourse, best viewed with JavaScript enabled"
3908,accelerate-whole-exome-analysis-with-deep-learning-at-70-cost-reduction-using-nvidia-parabricks,"Originally published at:			https://developer.nvidia.com/blog/accelerate-whole-exome-analysis-with-deep-learning-at-70-cost-reduction-using-nvidia-parabricks/
NVIDIA Parabricks is a suite of accelerated genomic analysis applications for high-throughput data that can be used for exome analysis.Powered by Discourse, best viewed with JavaScript enabled"
3909,programming-efficiently-with-the-nvidia-cuda-11-3-compiler-toolchain,"Originally published at:			Programming Efficiently with the NVIDIA CUDA 11.3 Compiler Toolchain | NVIDIA Technical Blog
The CUDA 11.3 release of the CUDA C++ compiler toolchain incorporates new features aimed at improving developer productivity and code performance. NVIDIA is introducing cu++flt, a standalone demangler tool that allows you to decode mangled function names to aid source code correlation. Starting with this release, the NVRTC shared library versioning scheme is relaxed to…CUDA 11.3 significantly improves the performance of Ampere/Turing/Volta Tensor Core kernels.298TFLOPS was recorded on A100 when benchmarking FP16 GEMM from CUTLASS, an open source CUDA DL/HPC library (GitHub - NVIDIA/cutlass: CUDA Templates for Linear Algebra Subroutines).  This is 14% higher than CUDA 11.2. FP32(via TF32) GEMM is improved by 39% and can reach 143TFLOPS. The same speedup applies to the CONV kernels.Also, see the discussion here: CUDA 11.3 significantly improved the performance of CUTLASS · Discussion #241 · NVIDIA/cutlass · GitHubHow do I use the toolkit to build CUDA for my custom x86-64 OS (Yocto Built) with the support of an Nvidia GPU card using my Ubuntu x86-64 host system?
Thanks.Powered by Discourse, best viewed with JavaScript enabled"
3910,training-and-fine-tuning-bert-using-nvidia-ngc,"Originally published at:			Training and Fine-tuning BERT Using NVIDIA NGC | NVIDIA Technical Blog
Imagine an AI program that can understand language better than humans can. Imagine building your own personal Siri or Google Search for a customized domain or application. Google BERT (Bidirectional Encoder Representations from Transformers) provides a game-changing twist to the field of natural language processing (NLP). BERT runs on supercomputers powered by NVIDIA GPUs to…Powered by Discourse, best viewed with JavaScript enabled"
3911,gpu-accelerated-docker-containers,"Originally published at:			GPU-Accelerated Docker Containers | NVIDIA Technical Blog
Containers wrap applications into an isolated virtual environment to simplify data center deployment. By including all application dependencies (binaries and libraries), application containers can run seamlessly in any data center environment. Docker, the leading container platform, can now be used to containerize GPU-accelerated applications. To make it easier to deploy GPU-accelerated applications in software containers,…Powered by Discourse, best viewed with JavaScript enabled"
3912,time-to-generate-3d-models,"Via Get3D I understand it can take around 30 minutes or so to generate 3D models from text description at this point.  What are the expectations on the ability to speed up that time?  Is it in the realm of possibility to watch that time decrease as quickly as the image generators did?Text-guided shape generation takes around 30min as we use a relatively simple approach of finetuning the 3D generator with a directional CLIP loss. The time to generate shapes based on the text description could be largely reduced by training a text-conditioned 3D generative model in the first place.This topic was automatically closed 14 days after the last reply. New replies are no longer allowed.Powered by Discourse, best viewed with JavaScript enabled"
3913,nvidia-cloudxr-now-available-on-aws-marketplace,"Originally published at:			NVIDIA CloudXR Now Available on AWS Marketplace | NVIDIA Technical Blog
NVIDIA CloudXR is now publicly available through the AWS Marketplace as an Amazon Machine Image (AMI).Powered by Discourse, best viewed with JavaScript enabled"
3914,using-virtual-reality-at-the-ibm-watson-image-recognition-hackathon,"Originally published at:			https://developer.nvidia.com/blog/using-virtual-reality-at-the-ibm-watson-image-recognition-hackathon/
Five teams of developers gathered at the Silicon Valley Virtual Reality (SVVR) headquarters in California last month to learn about the new features of IBM Watson’s Visual Recognition service, like the ability to train and retrain custom classes on top of the stock API, that allow the service to have new and interesting use cases…Powered by Discourse, best viewed with JavaScript enabled"
3915,shutterstocks-new-ai-tool-lets-you-search-by-image-composition,"Originally published at:			Shutterstock’s New AI Tool Lets You Search by Image Composition | NVIDIA Technical Blog
Leading stock photo company Shutterstock unveiled a new deep learning-based tool that lets users search photos by their composition. “Built on our next generation visual similarity model, this tool helps you find the exact image you need by placing keywords on a canvas and moving them around where you want subject matter to appear in…Powered by Discourse, best viewed with JavaScript enabled"
3916,uncovering-treatments-for-addiction-and-depression-with-gpu-accelerated-supercomputing,"Originally published at:			Uncovering Treatments for Addiction and Depression with GPU-Accelerated Supercomputing | NVIDIA Technical Blog
A team lead by Cornell University researchers are using the Titan Supercomputer at Oak Ridge National Laboratory to study mechanisms of sodium-powered transporters in cell-to-cell communications. Harel Weinstein’s lab at the Weill Cornell Medical College of Cornell University have constructed complex 3D molecular models of a specific family of neurotransmitter transporters called neurotransmitter sodium symporters…Wow, this sounds like an incredible research project! The use of GPU-accelerated supercomputing to uncover treatments for addiction and depression is genuinely fascinating. It’s incredible how technology enables researchers to delve deeper into the mechanisms of cell-to-cell communications. I’m excited to see how their complex 3D molecular models of neurotransmitter sodium symporters contribute to our understanding of these conditions. I also think that once this technology is proven safe and beneficial, leading rehabs (like rehab Essex) will implement it. Thanks for sharing, and keep posting exciting news!Powered by Discourse, best viewed with JavaScript enabled"
3917,is-there-any-inversion-method-instead-of-pti-optimization,"PTI based method is used to invert the image into the latent code but it requires time. So is there any specific encoder based on your method?This would be a great feature to have! We do not have it implemented at the moment, and would love to see other developers/researchers working on this in the future!Will Nvidia provide funding for the project of Encoder ?You can register for academic grant program using this link: Join the NVIDIA Applied Research Accelerator ProgramThank you sirPowered by Discourse, best viewed with JavaScript enabled"
3918,nvidia-announces-nsight-graphics-2018-7,"Originally published at:			NVIDIA Announces Nsight Graphics 2018.7 | NVIDIA Technical Blog
Today we’re announcing Nsight Graphics 2018.7. In this release, we enhanced our Ray Tracing support by adding the ability to view the scene Bounding Volume Hierarchy: the standardized acceleration structure in realtime ray tracing. We also greatly improved the ability to serialize C++ Captures of DirectX Raytracing (DXR) applications, increased Vulkan debugging capabilities with the…Powered by Discourse, best viewed with JavaScript enabled"
3919,3-versatile-openacc-interoperability-techniques,"Originally published at:			https://developer.nvidia.com/blog/3-versatile-openacc-interoperability-techniques/
OpenACC is a high-level programming model for accelerating applications with GPUs and other devices using compiler directives compiler directives to specify loops and regions of code in standard C, C++ and Fortran to offload from a host CPU to an attached accelerator. OpenACC simplifies accelerating applications with GPUs. OpenACC tutorial: Three Steps to More Science An often-overlooked feature…FWIW: ""... using compiler directives compiler directives to specify ...""It happens.Hey Jeff,When I try to build the sources that you provided in the Github, the first few of them are built without any errors. But, for ""openacc_cuda_device"", I get following error:pgc++ -o openacc_cuda_device -fast -acc -ta=nvidia:rdc -Minfo=accel saxpy_cuda_device.o openacc_cuda_device.o -Mcuda nvlink fatal   : Input file 'saxpy_cuda_device.o' newer than toolkitpgacclnk: child process exit status 2: /opt/pgi/linux86-64/16.5/bin/pgnvdWhat do you think? What could be wrong?nvcc version: Cuda compilation tools, release 7.5, V7.5.17pgi version: pgcc 16.5-0 64-bit target on x86-64 Linux -tp nehalemRegards,I'm seeing the same thing. In my case, I was able to build by changing -Mcuda to -Mcuda=7.5.Thanks Jeff. Setting Mcuda to 7.5 makes the error go away. I didn't know you could assign Mcuda to 7.5. PGI compiler doesn't say anything about this in there HELP output.Let me add here a method to call CUDA device functions from OpenACC kernels:https://parallel-computing....Nice technique! Thanks for sharing it Dmitry.Powered by Discourse, best viewed with JavaScript enabled"
3920,clara-train-4-0-upgrades-to-monai-and-supports-fl-with-homomorphic-encryption,"Originally published at:			https://developer.nvidia.com/blog/clara-train-4-0-upgrades-to-monai-and-supports-fl-with-homomorphic-encryption/
NVIDIA recently released Clara Train 4.0, an application framework for medical imaging that includes pre-trained models, AI-Assisted Annotation, AutoML, and Federated Learning. In this 4.0 release, there are three new features to help get you started training quicker.Powered by Discourse, best viewed with JavaScript enabled"
3921,introducing-profiling-enhancements-with-the-latest-nvidia-nsight-systems,"Originally published at:			https://developer.nvidia.com/blog/introducing-profiling-enhancements-with-the-latest-nvidia-nsight-systems/
The latest update to NVIDIA Nsight Systems is now available and introduces several improvements aimed to enhance the profiling experience.where is the web address for the latest NVIDIA Nsight Systems?Nsight Systems product page: NVIDIA Nsight Systems | NVIDIA DeveloperthanksPowered by Discourse, best viewed with JavaScript enabled"
3922,one-click-deployment-of-triton-inference-server-to-simplify-ai-inference-on-google-kubernetes-engine-gke,"Originally published at:			https://developer.nvidia.com/blog/one-click-deployment-of-triton-inference-server-to-simplify-ai-inference-on-google-kubernetes-engine-gke/
NVIDIA and Google Cloud have collaborated to make it easier for enterprises to take AI to production by combining the power of NVIDIA Triton Inference Server with Google Kubernetes Engine(GKE).Powered by Discourse, best viewed with JavaScript enabled"
3923,nvidia-rtx-top-3-week-of-february-7-2019,"Originally published at:			NVIDIA RTX Top 3: Week of February 7, 2019 | NVIDIA Technical Blog
Every week, we’ll be delivering 3 interesting stories coming from the world of RTX Game Development. 1 – Project SOL Textured with Substance In this article, Allegorithmic talks with the artists from NVIDIA who worked on Project Sol. They explain how they used Substance in the process, including a complete video breakdown on texturing one of…Powered by Discourse, best viewed with JavaScript enabled"
3924,metropolis-spotlight-nota-is-transforming-traffic-management-systems-with-ai,"Originally published at:			https://developer.nvidia.com/blog/metropolis-spotlight-nota-is-transforming-traffic-management-systems-with-ai/
Nota, an NVIDIA Metropolis partner, is using AI to make roadways safer and more efficient with NVIDIA’s edge GPUs and deep learning SDKs.TRAFFIC JUMP Problem: Omnidirectional hardware technology 360BALL DRIVE (Ballbot 2050) ProjectPowered by Discourse, best viewed with JavaScript enabled"
3925,cuda-pro-tip-profiling-mpi-applications,"Originally published at:			CUDA Pro Tip: Profiling MPI Applications | Parallel Forall | NVIDIA Technical Blog
When I profile MPI+CUDA applications, sometimes performance issues only occur for certain MPI ranks. To fix these, it’s necessary to identify the MPI rank where the performance issue occurs. Before CUDA 6.5 it was hard to do this because the CUDA profiler only shows the PID of the processes and leaves the developer to figure…Is this code compatible with the cuda cores on a cluster of Jetson TK1s?Powered by Discourse, best viewed with JavaScript enabled"
3926,2021-marked-the-year-of-virtual-worlds-with-innovative-tools-from-nvidia-omniverse,"Originally published at:			https://developer.nvidia.com/blog/2021-marked-the-year-of-virtual-worlds-with-innovative-tools-from-nvidia-omniverse/
NVIDIA Omniverse for virtual world building brought design collaboration and digital twins to center stage in 2021.Powered by Discourse, best viewed with JavaScript enabled"
3927,my-favorites-from-gtc-2012,"Originally published at:			https://developer.nvidia.com/blog/my-favorites-gtc-2012/
I had a great time at GTC 2012; there was incredible energy and excitement from everyone I talked to. I think the energy level had a lot to do with all of the exciting technology announcements from NVIDIA (Kepler, GK110 architecture, CUDA 5, NSight Eclipse Edition, VGX, and GeForce Grid, to name a few!), but I think that having heaps…Powered by Discourse, best viewed with JavaScript enabled"
3928,gtc-2020-dlss-image-reconstruction-for-real-time-rendering-with-deep-learning,"GTC 2020 S22698
Presenters: Edward Liu,NVIDIA
Abstract
In this talk, Edward Liu from NVIDIA Applied Deep Learning Research delves into the latest research progress on Deep Learning Super Sampling (DLSS), which uses deep learning and the NVIDIA Tensor Cores to reconstruct super sampled frames in real-time. He discusses and demonstrates why scaling and image reconstruction for real-time rendering is an extremely challenging problem, and examines the cause of common artifacts produced by traditional up-sampling techniques. With this background, Edward then shows how the DLSS, deep learning based reconstruction algorithm, achieves better image quality with less input samples than traditional techniques. The talk concludes with a walkthrough of DLSS integration in Unreal Engine 4 and a look at DLSS in the acclaimed sci-fi adventure “Deliver Us The Moon” from KeokeN Interactive.Watch this session
Join in the conversation below.Powered by Discourse, best viewed with JavaScript enabled"
3929,ai-sonnet-writing-poet-resembles-shakespeare,"Originally published at:			https://developer.nvidia.com/blog/ai-sonnet-writing-poet-resembles-shakespeare/
Researchers from IBM, Thomson Reuters, The University of Melbourne, and University of Toronto trained a deep learning model called “Deep-speare” to capture the language, rhyme, and meter for sonnets, generating poems that resemble ones most famously written by William Shakespeare. “With the recent surge of interest in deep learning, one question that is being asked…Powered by Discourse, best viewed with JavaScript enabled"
3930,developing-an-end-to-end-auto-labeling-pipeline-for-autonomous-vehicle-perception,"Originally published at:			https://developer.nvidia.com/blog/developing-an-end-to-end-auto-labeling-pipeline-for-autonomous-vehicle-perception/
Learn about a GPU-powered automated labeling pipeline developed as a part of Tata’s AI-based autonomous vehicle platform.Powered by Discourse, best viewed with JavaScript enabled"
3931,share-your-science-accelerating-graph-analytics-with-gpus,"Originally published at:			https://developer.nvidia.com/blog/share-your-science-accelerating-graph-analytics-with-gpus/
Brad Bebee, CEO of Blazegraph, shares how their Blazegraph GPU solution is using Tesla K40/K80 GPUs to accelerate graph queries and very large scale graph analytics. Drug discovery researchers are employing their solution and using graph analytics to identify new disease treatment possibilities for existing approved drugs. Watch Brad’s talk “Overcoming the Barriers of Graphs on GPUs:…Powered by Discourse, best viewed with JavaScript enabled"
3932,aiming-faster-in-games-with-low-computer-system-latency,"Originally published at:			Aiming Faster in Games with Low Computer System Latency | NVIDIA Technical Blog
Figure 1. A screenshot from our experimental FPS game, called First Person Science. Players must aim at and click on the green targets to eliminate them. Competitive gamers play games to win and get most of their enjoyment from doing well. In fact, the entire field of esports is based on this style of play,…Hey everyone, I’m really glad we’ve been able to share this post with you. For anyone looking to do similar work, I want to point out that we have published the software and source code for First Person Science on github. Let me know if you have any questions!How do I set up my computer to know I have around 12 ms & 20 ms latency?. And what scenario in the Kovaak do I need to play. From my experience durring actual multiplayer FPS, its about situation awareness & team communication. And long time player experience, and thinking outside the box of the computer and game. Some may refer to it as pree aim and ghost kill. For most like actual eSports team in any game titel how they achive this high aiming skills/awareness is most likely the their secrete sauce. I guess hardware specifications have a good part to play in that.The Kovaak scenarios are still in progress. We’ll have more to say about that in the future.It is true that situational awareness and team communication are very important parts of competitive games. However, your individual performance still matters, and we’ve found evidence here that your system latency gives a larger impact on your response time than just the latency alone.In terms of how to measure the latency of your computer, the Reflex Latancy Analyzer that will be available in some upcoming 360 Hz G-SYNC monitors. If you’re a game developer, the Reflex SDK will give you the ability to measure and control some parts of your system latency. If you’re a bit of a maker, you could also build your own latency analyzer like we’ve done.Ah ok thanks I thought the device you made was something you could buy seperate from the reflex suported screens. Thank for the answer, Im still waiting for screen to come to shop in my place. But thanks for the good answer.Powered by Discourse, best viewed with JavaScript enabled"
3933,automatically-detect-nuclear-power-plant-cracks-with-deep-learning,"Originally published at:			https://developer.nvidia.com/blog/automatically-detect-nuclear-power-plant-cracks-with-deep-learning/
Researchers from Purdue University developed a deep learning-based system to automatically detect cracks in the steel components of nuclear power plants and has been shown to be more accurate than other automated systems. “Periodic inspection of the components of nuclear power plants is important to avoid accidents and ensure safe operation,” said Mohammad R. Jahanshahi,…Powered by Discourse, best viewed with JavaScript enabled"
3934,implementing-path-tracing-in-justice-an-interview-with-dinggen-zhan-of-netease,"Originally published at:			Implementing Path Tracing in ‘Justice’: An Interview with Dinggen Zhan of NetEase | NVIDIA Technical Blog
We sat down with NetEase Lead Programmer Dinggen Zhan to find out how his team integrated path tracing into the open world of Justice.Powered by Discourse, best viewed with JavaScript enabled"
3935,gpu-accelerated-black-hole-simulations,"Originally published at:			https://developer.nvidia.com/blog/gpu-accelerated-black-hole-simulations/
In February 2016, scientists from the Laser Interferometry Gravitational-Wave Observatory (LIGO) announced the first ever observation of gravitational waves—ripples in the fabric of spacetime. This detection directly confirms for the first time the existence of gravitational radiation, a cornerstone prediction of Albert Einstein’s general theory of relativity. Furthermore, it suggests that black holes may occur…So, it seems that optimizing a given code by hand is a no-go, as is starting from scratch. Still, you have to wonder if that is where the real speed-up is.Powered by Discourse, best viewed with JavaScript enabled"
3936,ai-helps-transform-audio-into-music-playing-avatars,"Originally published at:			https://developer.nvidia.com/blog/ai-helps-transform-audio-into-music-playing-avatars/
Researchers from Facebook, Stanford, and the University of Washington developed a deep learning based method that can transform audio of musical instruments into skeleton predictions, which can be used to animate an avatar. “The key idea is to create an animation of an avatar that moves their hands similarly to how a pianist or violinist…Powered by Discourse, best viewed with JavaScript enabled"
3937,a-data-scientist-s-guide-to-gradient-descent-and-backpropagation-algorithms,"Originally published at:			https://developer.nvidia.com/blog/a-data-scientists-guide-to-gradient-descent-and-backpropagation-algorithms/
Read on how gradient descent and backpropagation algorithms relate to machine learning algorithms.Powered by Discourse, best viewed with JavaScript enabled"
3938,cuda-spotlight-gpu-accelerated-shape-sensing,"Originally published at:			https://developer.nvidia.com/blog/cuda-spotlight-gpu-accelerated-shape-sensing/
This week’s Spotlight is on Patrick Roye of Luna, Inc. Patrick works on accelerating Luna’s processing algorithms using GPUs. He and a team of engineers and scientists are developing a prototype system that uses CUDA to calculate the shape of a fiber-optic sensor in real-time. Luna’s shape-sensing systems, which are currently in development, will be…Powered by Discourse, best viewed with JavaScript enabled"
3939,accelerating-bioinformatics-with-nvbio,"Originally published at:			https://developer.nvidia.com/blog/accelerating-bioinformatics-nvbio/
NVBIO is an open-source C++ template library of high performance parallel algorithms and containers designed by NVIDIA to accelerate sequence analysis and bioinformatics applications. NVBIO has a threefold focus: Performance, providing a suite of state-of-the-art parallel algorithms that offer a significant leap in performance; Reusability, providing a suite of highly expressive and flexible template algorithms…Powered by Discourse, best viewed with JavaScript enabled"
3940,from-neuroscience-to-data-science-my-road-into-cybersecurity,"Originally published at:			https://developer.nvidia.com/blog/from-neuroscience-to-data-science-my-road-into-cybersecurity/
If you asked a group of cybersecurity professionals how they got into the field, you might be surprised by the answers that you receive. With military officers, program managers, technical writers, and IT practitioners, their backgrounds are varied. There is no single path into a cybersecurity career, let alone one that incorporates both cybersecurity and…Powered by Discourse, best viewed with JavaScript enabled"
3941,optimize-your-ray-tracing-graphics-with-the-new-nvidia-rtx-branch-of-unreal-engine-5,"Originally published at:			https://developer.nvidia.com/blog/optimize-your-ray-tracing-graphics-with-the-new-nvidia-rtx-branch-of-unreal-engine-5/
This feature-rich branch is fully compatible with Unreal Engine 5, and contains all of the latest developments from NVIDIA in the world of ray tracing.Powered by Discourse, best viewed with JavaScript enabled"
3942,top-5-ai-stories-of-the-week-3-15,"Originally published at:			Top 5 AI Stories of the Week: 3/15 | NVIDIA Technical Blog
In this week’s video, see a robot that can recognize objects from touch, an AI tool that can automatically detect open parking spots, a new technology to generate lifelike avatars, and much more. Plus, see how GIPHY used AI and NVIDIA GPUs to develop an open source celebrity detection model. Watch below: 5 – AI…Powered by Discourse, best viewed with JavaScript enabled"
3943,top-professional-visualization-sessions-at-nvidia-gtc-2023,"Originally published at:			Developer Conference Session Catalog | NVIDIA GTC
Explore professional visualization developer tools including NVIDIA NeuralVDB, NVIDIA OptiX, and NVIDIA Video Codec.Powered by Discourse, best viewed with JavaScript enabled"
3944,investing-in-developer-communities-across-africa-nvidia-ai-emerging-chapters-and-python-ghana,"Originally published at:			https://developer.nvidia.com/blog/investing-in-developer-communities-across-africa-nvidia-ai-emerging-chapters-and-python-ghana/
Developers across Africa honed their skills in recent online trainings made possible by the NVIDIA AI Emerging Chapters and Python Ghana collaboration.Powered by Discourse, best viewed with JavaScript enabled"
3945,photo-editing-with-generative-adversarial-networks-part-2,"Originally published at:			https://developer.nvidia.com/blog/photo-editing-generative-adversarial-networks-2/
In part 1 of this series I introduced Generative Adversarial Networks (GANs) and showed how to generate images of handwritten digits using a GAN. In this post I will do something much more exciting: use Generative Adversarial Networks to generate images of celebrity faces. I am going to use CelebA [1], a dataset of 200,000…How much general admission ticket for GTC 2017 ?gender, young or old, smiling or not, pointy nose, etc. See Figure 1 for a preview of the first 10 samples in the dataset, and Table 1 for some example attributes.Powered by Discourse, best viewed with JavaScript enabled"
3946,bringing-tensor-cores-to-standard-fortran,"Originally published at:			https://developer.nvidia.com/blog/bringing-tensor-cores-to-standard-fortran/
Tuned math libraries are an easy and dependable way to extract the ultimate performance from your HPC system. However, for long-lived applications or those that need to run on a variety of platforms, adapting library calls for each vendor or library version can be a maintenance nightmare.  A compiler that can automatically generate calls to…great post, thanks for sharing!Powered by Discourse, best viewed with JavaScript enabled"
3947,plaster-bringing-deep-learning-inferencing-to-millions-of-servers,"Originally published at:			PLASTER: Bringing Deep Learning Inferencing to Millions of Servers | NVIDIA Technical Blog
At the GPU Technology Conference in Silicon Valley earlier this year, NVIDIA CEO Jensen Huang introduced a new acronym named PLASTER to address seven major challenges for delivering AI-based services: Programability, Latency, Accuracy, Size, Throughput, Energy efficiency and Rate of learning. Meeting these challenges will require more than just sticking an ASIC or an FPGA…Powered by Discourse, best viewed with JavaScript enabled"
3948,nvidia-omniverse-transforming-how-aec-firms-collaborate,"GTC 2020 S22705
Presenters: Andrew Rink, NVIDIA; George Matos, NVIDIA
Abstract
With a long history of delivering advanced visual computing technologies to the AEC industry, NVIDIA recently announced an easy-to-use universal digital collaboration platform to enhance building design workflows. NVIDIA Omniverse is an exciting breakthrough that will help AEC design teams and their clients overcome many of the challenges that complicate the conceptual design process and result in lengthy design reviews.Watch this session
Join in the conversation below.Powered by Discourse, best viewed with JavaScript enabled"
3949,augmented-reality-sandbox,"Originally published at:			Augmented Reality Sandbox | NVIDIA Technical Blog
Powered by GeForce and OpenGL, the Augmented Reality Sandbox lets users sculpt mountains, canyons and rivers, then fill them with water or even create erupting volcanoes. The Modeling and Educational Demonstrations Laboratory Curriculum Materials Center in the Department of Earth, Planetary, and Space Sciences at UCLA have created the sandbox to help students learn about…Powered by Discourse, best viewed with JavaScript enabled"
3950,sculpture-symmetry,"I am working with Get3D to try to generate novel sculpture from my existing sculptures, they look like this. I altered the generator to help it generate the complex topology, and it works: Get3D can replicate sculptures faithfully and provide small neighborhoods around them.
However, interpolated forms, and all generated forms that do not replicate dataset items, are asymmetrical even if all dataset items have the same symmetry group. I would like to add a loss term to lock in the symmetry. Any ideas how to do this?PS my version runs on one GTX1070 with 8GB, the generator has 6M parameters. That seems like enough, except for the symmetry issue it’s working, it can make motorbikes if needed. :-)Enforcing symmetry in the shape is possible by mirroring the signed distance function (SDF) values with respect to a plane of symmetry. For instance, if you’d like the shape to be symmetric with respect to the x-y plane, you can add a line after GET3D/networks_get3d.py at 2a09f5836e2a3ed49a37a6825a7139d11328446d · nv-tlabs/GET3D · GitHub to set sdf[ …, sdf.shape[-1]//2 + k] = sdf[ …,sdf.shape[-1]//2 - k], for k in range(sdf.shape[-1]//2). Additionally, you can also inference on only half of the grid to save memory.Thank you, I work with groups which include rotations and inversion transforms, in addition to reflection, but what you say can generalize.Enforcing rigid symmetry in this way, or inferencing only on the fundamental unit of the grid, will clearly work.  However it would create rigidly exact symmetric output, and I am looking for fully generated “fuzzy” symmetry which would (for example) allow interpolation between data items with different symmetry groups.I tried comparing the generated mesh vertices to themselves under symmetry transforms using sliced optimal transport, and using that distance as a loss. I also tried doing a similarity loss on images from symmetrically related cameras. In both cases the generator could not learn to reduce asymmetry below random chance. I feel like it “should” be able to learn this, but I haven’t thought of the right way to train it.Powered by Discourse, best viewed with JavaScript enabled"
3951,edge-computing-is-the-next-big-cybersecurity-challenge,"Originally published at:			Edge Computing Is the Next Big Cybersecurity Challenge | NVIDIA Technical Blog
F5 discusses their use of NVIDIA Morpheus and BlueField DPUs to accelerate access to analytics while providing security across the cloud and emerging edge.Powered by Discourse, best viewed with JavaScript enabled"
3952,using-carbon-capture-and-storage-digital-twins-for-net-zero-strategies,"Originally published at:			https://developer.nvidia.com/blog/using-carbon-capture-and-storage-digital-twins-for-net-zero-strategies/
CO2 capture and storage technologies (CCS) catch CO2 from its production source, compress it, transport it through pipelines or by ships, and store it underground. CCS enables industries to massively reduce their CO2 emissions and is a powerful tool to help industrial manufacturers achieve net-zero goals. In many heavy industrial processes, greenhouse gas (GHG) emissions…An admirable concept, but the problem is creating a digital twin of an environment that is unobservable as well as being subject to numerous temporal variables that interact dynamically based on chemical, physical and geological processes.  It is an impossible task.Powered by Discourse, best viewed with JavaScript enabled"
3953,on-demand-technical-sessions-develop-and-deploy-ai-solutions-in-the-cloud-using-nvidia-ngc,"Originally published at:			https://developer.nvidia.com/blog/on-demand-technical-sessions-develop-and-deploy-ai-solutions-in-the-cloud-using-nvidia-ngc/
At GTC '21, experts presented a variety of technical talks to help people new to AI, or those just looking for tools to speed-up their AI development using the various components of NGC.Powered by Discourse, best viewed with JavaScript enabled"
3954,using-windows-ml-onnx-and-nvidia-tensor-cores,"Originally published at:			Using Windows ML, ONNX, and NVIDIA Tensor Cores | NVIDIA Technical Blog
As more and more deep learning models are being deployed into production environments, there is a growing need for a separation between the work on the model itself, and the work of integrating it into a production pipeline. Windows ML caters to this demand by addressing efficient deployment of pretrained deep learning models into Windows…Powered by Discourse, best viewed with JavaScript enabled"
3955,building-a-multi-camera-media-server-for-ai-processing-on-the-nvidia-jetson-platform,"Originally published at:			Building a Multi-Camera Media Server for AI Processing on the NVIDIA Jetson Platform | NVIDIA Technical Blog
A media server provides multimedia all-in-one features, such as video capture, processing, streaming, recording, and, in some cases, the ability to trigger actions under certain events, for example, automatically taking a snapshot. For you to make the best out of a media server, it must be scalable, modular, and easy to integrate with other processes.…Hello , Can I use jetson nano carrier board with raspberry pi2 camera for this type of application ?Need some sample application for the Multi camera media server.@sachin.gaikwad yes you can use the Raspberry Pi Camera Module v2 (IMX219) with the Jetson Nano carrier board.   You can test it with nvgstcapture-1.0 test program or a simple script like this that uses the nvarguscamerasrc GStreamer element:   GitHub - JetsonHacksNano/CSI-Camera: Simple example of using a CSI-Camera (like the Raspberry Pi Version 2 camera) with the NVIDIA Jetson Developer KitDeepStream also supports it - I would review the latest DeepStream documentation, as this article you commented on is from 2020.Powered by Discourse, best viewed with JavaScript enabled"
3956,rapids-accelerator-for-apache-spark-v21-06-release,"Originally published at:			https://developer.nvidia.com/blog/rapids-accelerator-for-apache-spark-v21-06-release/
Introduction RAPIDS Accelerator for Apache Spark v21.06 is here! You may notice right away that we’ve had a huge leap in version number since we announced our last release. Don’t worry, you haven’t missed anything. RAPIDS Accelerator is built on cuDF, part of the RAPIDS ecosystem. RAPIDS transitioned to calendar versioning (CalVer) in the last…Powered by Discourse, best viewed with JavaScript enabled"
3957,building-intelligent-video-analytics-apps-using-nvidia-deepstream-5-0-developer-preview-edition,"Originally published at:			Building Intelligent Video Analytics Apps Using NVIDIA DeepStream 5.0 (Updated for GA) | NVIDIA Technical Blog
Get notified about production releases:DeepStream SDK 5.0 Transfer Learning Toolkit 2.0 Whether it’s a warehouse looking to balance product distribution and optimize traffic, a factory assembly line inspection, or hospital management making sure that employees and caregivers use personal protection equipment (PPE) while attending to patients, advanced intelligent video analytics (IVA) turn out to be…Powered by Discourse, best viewed with JavaScript enabled"
3958,choosing-a-development-environment-for-nvidia-bluefield-dpu-applications,"Originally published at:			https://developer.nvidia.com/blog/choosing-a-development-environment-for-bluefield-dpu-applications/
This post describes different ways to compile an application using various development environments for the BlueField DPU.Powered by Discourse, best viewed with JavaScript enabled"
3959,white-paper-nvidia-dgx-1-with-tesla-v100,"Originally published at:			White Paper: NVIDIA DGX-1 with Tesla V100 | NVIDIA Technical Blog
A new technical white paper provides an in-depth look at the hardware and software technologies inside the NVIDIA DGX-1 — the fastest platform for deep learning training. The demand for deep learning performance is rapidly growing. Facebook CTO Mike Schroepfer noted that: Facebook’s deployed neural networks process more than 6 million predictions per second; 25%…Powered by Discourse, best viewed with JavaScript enabled"
3960,time-series-forecasting-with-the-nvidia-time-series-prediction-platform-and-triton-inference-server,"Originally published at:			https://developer.nvidia.com/blog/time-series-forecasting-with-the-nvidia-time-series-prediction-platform-and-triton-inference-server/
Learn how the Time Series Prediction Platform provides an end-to-end framework that enables users to train, tune, and deploy time series models.how to use TSF with C++? And how do I customize components? Such as the loss function.Hey, thank you for you interest!  At the moment, the NVIDIA TSPP is just for Python.  In order to customize the loss function (or any component), there are 2 main things needed.Let us know if you have further issues/questions and don’t hesitate to open an issue on the github: Issues · NVIDIA/DeepLearningExamples · GitHubIs there a way to do Classification only with the TFT? I have a time series dataset that I need to do classification on and I thought I could potentially do classification on it with the TFT. Thoughts?Powered by Discourse, best viewed with JavaScript enabled"
3961,controlling-data-movement-to-boost-performance-on-the-nvidia-ampere-architecture,"Originally published at:			https://developer.nvidia.com/blog/controlling-data-movement-to-boost-performance-on-ampere-architecture/
The NVIDIA Ampere architecture provides new mechanisms to control data movement within the GPU and CUDA 11.1 puts those controls into your hands. These mechanisms include asynchronously copying data into shared memory and influencing residency of data in L2 cache. This post walks through how to use the asynchronous copy feature, and how to set…Powered by Discourse, best viewed with JavaScript enabled"
3962,gpu-based-3d-painting-simulation,"Originally published at:			GPU-based 3D Painting Simulation | NVIDIA Technical Blog
This could quite possibly be the future of painting! Researchers from Adobe Research and Ohio State University recently demonstrated Wetbrush, a real-time painting system that simulates the interactions among brush, paint, and canvas at the bristle level. The resulting system can realistically simulate not only the motions of brush bristles and paint liquid, but also…Powered by Discourse, best viewed with JavaScript enabled"
3963,isc-2020-accelerating-covid-19-research-with-gpus,"ISC 2020 disc04
Presenters: DemoTeam, NVIDIA
Abstract
The stakes are high as scientists and researchers are racing against the clock to find a cure for COVID-19. The search for an antiviral depends on finding the right chemical structure that binds to the virus and interferences with its viral replication. Finding the right drug requires screening billions of drug candidates and identifying which drugs will most favorably bind with the virus. However, calculating the binding potential is computationally intensive and will take years to screen a billion compounds on a CPU system. The GPU accelerates this process by 33x and GPU-powered Summit system can now scan over 1B compounds in 12 hours.Watch this session
Join in the conversation below.Powered by Discourse, best viewed with JavaScript enabled"
3964,cuda-spotlight-dr-cris-cecka-on-gpu-accelerated-computational-mathematics,"Originally published at:			https://developer.nvidia.com/blog/cuda-spotlight-dr-cris-cecka-on-gpu-accelerated-computational-mathematics/
Our Spotlight is on Dr. Cris Cecka, a research scientist and lecturer in the new Institute for Applied Computational Science (IACS) at Harvard University. Harvard has been a CUDA Center of Excellence since 2009, led by Dr. Hanspeter Pfister, IACS Director. Cris is currently also performing research with the Mathematics Department at the Massachusetts Institute of Technology.…Powered by Discourse, best viewed with JavaScript enabled"
3965,essential-guide-to-automatic-speech-recognition-technology,"Originally published at:			What is Automatic Speech Recognition? | NVIDIA Technical Blog
Discover what automatic speech recognition (ASR) means for practitioners. Learn about ARS advancements, challenges, industry impact, and more.Powered by Discourse, best viewed with JavaScript enabled"
3966,ai-assists-doctors-in-analyzing-heart-scans,"Originally published at:			https://developer.nvidia.com/blog/ai-assists-doctors-in-analyzing-heart-scans/
Researchers from the University of California, San Francisco developed a deep learning algorithm to classify images from heart ultrasound tests with better accuracy than skilled human technicians. Published in the Nature Partner Journal, Digital Medicine, the researchers created a convolutional neural network that automatically classifies echocardiograms, resulting in 98% overall accuracy. “These results suggest our…Powered by Discourse, best viewed with JavaScript enabled"
3967,detecting-objects-in-point-clouds-with-nvidia-cuda-pointpillars,"Originally published at:			https://developer.nvidia.com/blog/detecting-objects-in-point-clouds-with-cuda-pointpillars/
Use long-range and high-precision data sets to achieve 3D object detection for perception, mapping, and localization algorithms.Can this run on a live point cloud?
If so do you have recommendations on hardware to generate a live point cloud for this application?The model uses points cloud  as input which is {x,y,z,i} same with the point type from PCL.Hi Lei Fan, thanks for your work!
By the export to onnx part, I have a question. How can you ensure you only export the middel part of the network(after voxelization and encode to 10 feature per pillar), not the whole part which include voxelization, pillar feature extraction, scatter to bev, backbone, postprocess?thanks!We remove pre-process and post-process form pointpillar in OpenPCDet and just keep the middle part, so that we can get the onnx which has no the two parts.
You can check the dir:“tool” which has source code about how to convert trained model into onnx file.How can I input (x,y,z,i) to model?
float * points = (float*)malloc(cloud.size() * 4 * sizeof(float));
for(size_t i=0;i<cloud.size() ;i++)
{
*(points + 0 + i * 4) = cloud.at(i).x;
*(points + 1 + i * 4) = cloud.at(i).y;
*(points + 2 + i * 4) = cloud.at(i).z;
*(points + 3 + i * 4) = cloud.at(i).intensity;
}
is right?Powered by Discourse, best viewed with JavaScript enabled"
3968,trained-on-nvidia-gpus-facebook-unveils-an-ai-fashion-assistant,"Originally published at:			Trained on NVIDIA GPUs, Facebook Unveils an AI Fashion Assistant | NVIDIA Technical Blog
Using GANs, this deep learning-based system can act as a personal fashion designer by recommending changes that can make a person’s outfit more fashionable.  Using large portions of NVIDIA’s Pix2PixHD code, Facebook AI researchers in collaboration with UT Austin, Cornell University, and Georgia Tech developedFashion++, a deep learning-based model that uses GANs to offer suggestions on…Powered by Discourse, best viewed with JavaScript enabled"
3969,an-it-manager-s-guide-to-deploying-an-edge-ai-solution,"Originally published at:			https://developer.nvidia.com/blog/an-it-managers-guide-to-deploying-an-edge-ai-solution/
Check out these recommendations for how to successfully deploy an edge AI solution for your organization.Powered by Discourse, best viewed with JavaScript enabled"
3970,gan-for-all-seasons-ai-generated-art-accompanies-pandemic-poetry-in-the-washington-post,"Originally published at:			https://developer.nvidia.com/blog/ai-generated-art-accompanies-pandemic-poetry-washington-post/
A recent National Poetry Month feature in The Washington Post presented AI-generated artwork alongside five original poems reflecting on seasons of the past year.Powered by Discourse, best viewed with JavaScript enabled"
