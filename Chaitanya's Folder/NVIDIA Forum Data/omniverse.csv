,query,data
0,is-it-possible-to-perform-cfd-computational-fluid-dynamics-simulations-using-omniverse-tools,"As the Modulus OV extension is presently paused for development according to MODULUS with 2022.3.3 versionI even bought the course on the NVIDIA platform: “Introduction to Physics-informed Machine Learning with Modulus”Then, I ask:Is it currently possible to perform CFD simulations with Omniverse? Could you enlighten me if it is possible to use Omniverse for CFD simulations and how can I do this?Thank you in advance
Best regardsPowered by Discourse, best viewed with JavaScript enabled"
1,how-to-edit-a-pivot-point-on-an-object,"Hi, I’m trying to figure out how to edit a pivot point on a object.
Example, like adjusting a pivot point so I can rotate a door on it’s correct axis.I can’t seem to find the pivot tool on Machinima. Does anyone know?Hello @Wintoons!  I’ve been told that we will be adding the ability to edit pivot points in Create version 2022.2.  As an extension, it should be available for use in Machinima, but I am asking the dev team to confirm.Just got confirmation!  The Pivot Adjustment Tool will work in Machinima as well!That’s great news to hear! I actually figured how to, if you select the prim object and go to Add and then TransformOp and select pivot, you can then edit the pivot that by changing the XYZ coordinates.But if they adding an extension to adjust the pivot points that will be very handy.
Thanks for your help, Wendy! :)MachinimaWhat’s the name of the extension?   “Metrics assembler UI”?  But can not find the “Metrics assembler UI” in extension list now?Powered by Discourse, best viewed with JavaScript enabled"
2,robot-reset-failing-if-in-contact-or-simultaneously-resetting-object-since-v2022-2-0,"Hi,I am using RL to train on a robotic task. For this I set the robot to the center of a small room and spawn obstacles in random positions around it. At reset, the object positions are changed withwhere self._obstacles is an XFormPrimView.
The robot is reset withwhere dof_pos are default joint values and 0s for the base joints. Both these calls are issued, then a physics step is taken. The new robot position is guaranteed to be free of any obstacle.This worked well in 2022.1.1, but sometimes fails in 2022.2.0 and 2022.2.1. It seems to fail in two cases:
a) the robot is in contact with an obstacle when resetting - see image 1 before reset, 2 after reset. The robot position is set to the center of the room, but instead the robot seems to fly off somewhere (outside the images)
b) an obstacle is being reset to the last position of the robot before the reset - see image 3 before reset, 4 after reset
My guess is that something changed in the physics, such that simultaneously resetting these objects no longer works. Could this be the case? As I mentioned, the exact same code worked well before.This reset logic follows the OmniIsaacGymEnvs repository. Do you have any tipps how to remedy this? Are there any settings or object properties I could change to make this work? Or how / in what order should I do a reset in this scenario?
Screenshot from 2023-04-02 13-12-041342×1183 75.4 KB


Screenshot from 2023-04-02 13-11-461350×1206 76.3 KB


Screenshot from 2023-04-02 13-11-121324×1107 61.8 KB


Screenshot from 2023-04-02 13-10-261326×1093 56.9 KB
Hi there, this sounds like the correct approach to reset the objects and robot. Are you also setting the root transforms of the robot at reset?No, I’m not touching the root transform, i.e. never calling self._robots.set_world_poses(), this should remain unchanged at the original position (set by the gridcloner)Can you try explicitly resetting the world poses to its original values at reset time to see if that helps? We’ve recently discovered that not resetting root transforms may cause large errors in the joints pre and post reset in the physics solver, it’s possible that this may be related.Thanks for the suggestion. unfortunately resetting the robot root (self._robots.set_world_poses(*self._robots.get_world_poses())) leads the simulator to fail. I think for some reason this moves the robot into some invalid position. So I can’t quite test this hypothesisWhat I did find is that the initial problem disappears if I add the obstacles as a RigidPrimView instead of an XFormPrimView. The downside of this is that now the obstacles also collide with each other, meaning I either have to put a lot of work into ensuring that they do not collide or write my own custom collision filter. I’m also not sure if this comes with additional computational overhead. My aim was to just have some static obstacles with minimal simulation dynamics, which the XFormPrimView seemed to fulfill (until the update to the latest version)Lastly, trying the version with RigidPrimView and custom collision filter, most of the time it works fine, except for some rare cases in which the robot is suddenly able to drive through some of the obstaclesIf you are using rigid bodies, you can enable kinematic on the rigid bodies so that they do not behave physically.rb = UsdPhysics.RigidBodyAPI.Get(stage, prim)
rb.CreateKinematicEnabledAttr().Set(True)Unfortunately this for some reason also leads the simulation to crash. Though also from the description it sounds like that would disable all physical collisions? We would still like to have it collide with the robotBut otherwise I can also live with using the RigidPrimView with my custom collision filter. Seems to work alright for our current purposes.Hi, there seems to be a larger issue with collisions when setting multiple object poses in the same stepI am now running a different task setup with a robot and a single articulated object. During reset I (i) set the robot position and robot joint values and (ii) set the articulated object positions and joint values with articulation.set_joint_positions() and articulation.set_joint_velocities(). Both the old and the new positions of robot and articulated object are far apart and cannot be in collision. I have also checked the collision boxes for both and they match the visuals of the objects pretty much perfectly.Sometimes (not everytime) this leads to the following:This does not happen with e.g. a DynamicCuboid instead of the articulated object. But it happens as soon as I add a second Articulation & ArticulationView such as the articulated object above or the franka robot (omniisaacgymenvs.robots.articulations.franka). I have also tested it with a different robot than the PR2, the result is the same, making me confident that it is not related to the collision meshes of any of these objects.Furthermore, it also still happens if I do not set any state of the articulated object in the reset, but only callIn particular it seems to be related to self._robots.set_joint_positions(dof_pos.clone(), indices=indices) (tested by commenting out each of the commands above)Lastly, this only happens on the GPU. If I set use_gpu_pipeline: False and use_flatcache: False (as defined by OmniIsaacGymEnvs) this does not occur.@kellyg Are you aware of this happening in other cases? Is there something in the order of the calls that I can change to circumvent this?Hi there, the set_joint_positions API will also call set_joint_position_targets as it’s required to set both when setting the positions of joints. Therefore, you do not need an additional call to set_joint_position_targets.Thank you for letting me know @kellyg , I have removed the unnessary function call.Unfortunately, that does not solve the issue with the contacts. I was previously told that Nvidia is planning to release a new version in June. Do you know what the current status on that is? Would be super helpful for our planning.Thanks for you help!The next release is currently planned for August. Sounds like the issue may be a bug with the GPU pipeline. In order for us to repro this, do we require 2 articulations in the scene, then setting the root and joint states for both simultaneously?Hi @kellyg , thank you for the update.Regarding the issue, I find it hard to produce a simple setting to reproduce it. In general it seems to happen with:But it does not always happen and not with all robots / articulations. It may indeed be the GPU pipeline, though I cannot confirm for certain.Trying to find a reproducible, simple case, things start to behave weirdly before I even get to this point. In the attached file I use assets from nvidia. If device = ""cpu"", everything behaves fine (robot standing still, correct joint values being set). If you change it to device = ""cuda"", the robot starts moving without getting any commands - this also happens without a second articulated object. If at the same time we spawn a cabinet, the robot starts to fly around - no idea why,While this is not the exact same issue, it seems very related. If I spawn a different articulated object, that I did not include as it’s based on much more files, the robot does not start flying, but the object doors always open after resets etc. - while on CPU the joints get set to the correct state and then do not move any further.
All these issues seem somewhat related to me. The python files are attached in the zip file below.
testcase.tar.xz (5.2 KB)Your help would be greatly appreciated! I am really, really struggling to use the simulator at this point and have run out of ideas on how to make the physics behave in a stable manner.Thanks for providing the repro scripts. I tested the scripts on our latest internal version of Isaac Sim and I did not observe any issues. The behaviour was the the same with device set to “cpu” and “cuda”, and I didn’t get any undesired movement of the robot with the GPU pipeline. It’s likely that the issue has already been fixed and will be made available in the next Isaac Sim release.Thanks for the quick reply. Fingers crossed then!Powered by Discourse, best viewed with JavaScript enabled"
3,the-issue-pertains-to-a-memory-leak-that-arises-when-employing-the-delete-prim-and-create-prim-functions,"Hi,When I employ the “delete_prim” function, GPU memory doesn’t reduce before deleting assets.
I think this problem caused by add ArticulationPrimView or RigidbodyPrimView…
How I can remove the memory leak after delete_prim?Hi @gwangin -  The delete_prim function in Isaac Sim deletes the primitive from the scene graph, but it does not immediately free up the GPU memory. The GPU memory is managed by the graphics driver and it is not directly controllable by the application. The driver decides when to free up the memory based on its own memory management strategy.Use the unload_asset function to unload the asset from the asset manager. This will free up the memory used by the asset.Use the destroy function to destroy the ArticulationPrimView or RigidbodyPrimView. This will free up the memory used by the view.Use the gc.collect() function in Python to force a garbage collection. This can help to free up the memory used by the Python objects.If you are using a lot of textures or materials, try to reduce their resolution or complexity. This can significantly reduce the GPU memory usage.Powered by Discourse, best viewed with JavaScript enabled"
4,memory-leak-with-omniisaacgymenv,"Hello,
I have been doing some trainings with OmniIsaacGym and there’s always quite a big RAM memory leak during training, which persists even after I cut the python process I launched.  I have to restart my computer after around 20 / 30 launches of the training script i’d sayIt’s the first time I’m seeing a leak persisting AFTER killing the process.Where does it could come from and what can I do to prevent it ?Regards.Powered by Discourse, best viewed with JavaScript enabled"
5,sementic-type-on-face-in-a-mesh-prim,"Hi
Is there an approch to set semantic types of different faces in a mesh prim, instead of the whole prim?
I’d like to generate segmentation map like facial parsing via Replicator.Thanks for reading this :)Hello @AlanHuang! Great question and a great candidate for a new tutorial. Unfortunately, we’ve discovered that GeomSubset segmentation is broken in current releases and are working to provide a fix ASAP for the upcoming release updates.Thanks for reply!
Looking forword to it :)Powered by Discourse, best viewed with JavaScript enabled"
6,omniverse-kit-unit-tests-pass-but-set-fails,"Something strange is going on with the unit tests in my extension. The tests all pass but then the run fails. I stripped it down to 1 basic test that passes. And the set still fails . I have attached the log.
kit_20230803_165236.log (152.5 KB)
Any help is appreciated.Powered by Discourse, best viewed with JavaScript enabled"
7,do-you-have-a-plan-to-also-convert-quilts-to-om-instead-of-only-solid-parts,"Do you have a plan to also convert “Quilts” to OM instead of only “solid parts”?Powered by Discourse, best viewed with JavaScript enabled"
8,cant-install-maya-legacy-connector-200-4,"Hi,
I when I update the connector from versions 1.xxx to 2.xxx it dissapears from Maya 2023.I tried reinstalling it from the scratch multiple times with and without the bifrost and usd plugins installed. I also did multiple clean reinstallation of the whole Omniverse package. Still no luckKarol@karol.osinski
Are you trying to have two versions of the Maya Connector installed or only Maya Legacy Connector v200.4?We have added a feature to the installation process that only one version of the Maya Connector is installed so previous versions are removed/uninstalled when the new version is installed. Is it possible this is the issue your are running into?Hi @kecollins  thanks for your response!I always uninstall the previous version before updating it to the newer one.This is pretty annoying. All other connectors install seamlessly, Maya is the only one I have had an issue with. I’ve un/reinstalled 3 times and even tried installing on a new computer and it still gives the same error message: Autodesk Maya is currently not installed. How fix? I need to get to work =(The Maya Connector needs registry information to find out Maya 2023/2024 installation path. Maybe your Registry is different. Then you can create this registry key and make the Maya Connector find it.
And you do not have to uninstall Maya Connector via Launcher. You can go to the Maya Connector installed path and run MayaSetup.exe after you add the registry key.
As a reference, this reg file is generated based on my Registry key. You can modify it in a plain text editor to fit your Maya and write it to registry.
Maya2023-2024Path.reg (626 Bytes)If Maya Connector fails to install via Launcher because of Maya missing issue, you can create the above Registry first and then install Maya Connector via Launcher.I hope your friends tell you how dope you are everyday.  Easiest fix ever, Thanks!Glad that it fixes your problem. If you have time, can you tell me what is the Registry path of Maya installation path on your PC? I do not know that Autodesk Maya makes it different on some PCs.Powered by Discourse, best viewed with JavaScript enabled"
9,unreal-engine-speech-audio-to-audio2face,"Hi,Is there a way to send audio data to Audio2Face?Hello and welcome to the forums @mark243Here are a couple of way to send audio to Audio2Face:Overview of Streaming Audio Player in Omniverse Audio2Face - YouTubeAudio2Face Headless and RestAPI Overview - YouTubePowered by Discourse, best viewed with JavaScript enabled"
10,reinforcement-learning-example-jetracer-jetbot,"I ask me, where do I find the two reinforcement Learning examples provide by NVIDA Issaac Sim??https://docs.omniverse.nvidia.com/app_isaacsim/app_isaacsim/rl_samples.htmlThey arent in the new Documentation… I neeed the Documentation to work on my project.We are planning on simplifying and adding back the jetbot RL sample for the december bugfix releaseMay I ask if these updates were done, or if there are such RL examples for the JetRacer? I am going to get IsaacSim set up locally this week and would like to explore some example code for help in my senior design project. Thanks!@vminin , can you  try this link?19. Reinforcement Learning using Stable Baselines — Omniverse Robotics documentationIs there any new link for reinforcement learning example/tutorial in Isaac Sim?? Especially for jetbot or jetracer.Hi @ming_johnson - You can find more information for Isaac Gym and tutorials here: 1. Overview & Getting Started — isaacsim 2022.2.1 documentationI am trying to find documents for jetbot/jetracer road following examples in isaac sim. All of them posted in the past were deleted.Powered by Discourse, best viewed with JavaScript enabled"
11,usd-export-use-settings-for-render-crashes-create-22-3-3,"Hello! I found out that Use settings for “Render” crashes Create 22.3.3 - at least if there is animation of the scene…First I exported with  Use settings for “Viewport”:

settings for viewport891×628 43.3 KB
and then another version with Use settings for “Render”:

settings for render757×697 45.6 KB
See here the video, its visible that the fur is much more dense and realistic. That´s cool but Create crashes when I move the timeline :PekkaHello @pekka.varis.  Would you be able to provide the USD which crashes Create/Composer?  Please feel free to message me directly to provide the file, if you prefer.  Thanks.Powered by Discourse, best viewed with JavaScript enabled"
12,unable-to-create-fire-flow,"Hello Omniverse community,Can you provide guidance on how to generate synthetic data for creating a fire flow? Is it possible to utilize the usda format, as shown below?When I run replicator, fire flow does not come up. omni.flowusd extension is enabled.Thanks!Best regards,
ShakhizatHi @shahizat Currently there’s no way to segment flow effects. However this isn’t the first time its been requested, and we do have this on the roadmap to implement. Unfortunately I can’t give a date for this yet. I am however looking into whether there’s a workaround for this, but nothing to report yet.Hi @pcallender, thank you for your reply. If someone is interested,[here] I implemented an  automated image labeling using Grounding Dino zero shot approach for flow effects(Fire Detector Using Omniverse Replicator And Arduino Nicla - Hackster.io).Powered by Discourse, best viewed with JavaScript enabled"
13,is-there-a-way-to-manage-prim-like-unitys-prefab-or-sketchups-component,"I want to manage prim like Unity’s prefab or sketchup’s component.If it’s a prim with the same properties, all of them change when you change it.In USD you can do this by putting the model in a separate file then creating a “reference” to the file. If you change the settings in the referenced file (the equivalent of a Unity prefab), all locations referencing it get updated. So “references” are kinda like prefab references, and “layers” and kinda like “overrides” of prefabsThank you for answerPowered by Discourse, best viewed with JavaScript enabled"
14,weird-joint-effort-target-pos-and-vel-behavior,"Hello,Si I’ve been so far working with a setting where I send joint position and velocity targets:But I now want to get the joint efforts to minimise them. Unfortunately the get_applied_joint_effort doesn’t seem to wwork in hte current release, so I was trying to do it myself, using the formula with stiffness and damping and trying this:he problem is that it just doesn’t work and I’m not sure why. Simulation results are very different (and also with +forces instead of - forces, and with serveral tweaks too). This is supposed to be a direct replacement of the above method righ t? what am i doing wrong ?The method you’re trying to use to calculate joint efforts is based on a simple model of a damped spring system, where the force is proportional to the displacement (difference between target and current position) and the velocity (difference between target and current velocity). This is a common model used in control systems, but it may not accurately represent the behavior of a real robot, especially if the robot has complex dynamics or non-linearities.Here are a few things you could check:Powered by Discourse, best viewed with JavaScript enabled"
15,feature-request-for-omniclientliveprocess,"Hello,based on the c+±api-documentation omniClientLiveProcess() cannot be called while no other usd-apis are called. That makes sence because thise calls would create more changes to the usd scenes that are synced by calling omniClientLiveProcess().But what if a process maintains more than one usd scene in parallel (like in our scenario)? Than calling omniClientLiveProcess() created a bottleneck because no usd scene can be changed while the changes (that are made to one of those scenes) are synced.Would it be possible to extend omniClientLiveProcess() to pass the usd-file one would like to sync? In that way it would be possible to change and sync more than one usd scene in parallel without creating a bottleneck.That would realy help us.ThanksCarlPowered by Discourse, best viewed with JavaScript enabled"
16,not-able-to-sign-in-revit-omniverse-connector-file-transferring-failed,"Hi team ,
I am exporting the file from Revit 2023 using Omniverse connector 201.1.2338 to Omniverse Create.
I am facing two problems in connector .
1.Sometimes I am not able to sign in in Revit connector. I have attached the error for your reference.
2. While exporting the sample 3d file from Revit to Omniverse create using connector , In omniverse no files are visible. The stage is empty. I have added the screenshot. I am transfer the file using Nucleus server with connector .
Screenshot 2023-05-19 1235581908×954 124 KB

Screenshot 2023-05-19 1230591076×178 35.6 KB
in the settings in revit can you enable logging and download the log after you export.
also regarding the login issue you said you attached the error but I don’t see anything in postThanks @rcervellione . I will Check this and get back to you.Powered by Discourse, best viewed with JavaScript enabled"
17,failed-to-execute-script-main,"When i launch Omniverse, an error says “Failed to execute script__main__”.
If i launch Create oy audio2face, the screen stays blank with a ""
do not answer"" message.Could you please help?System :
Windows 10 : build 19044.1889,
AMD Ryzen 9 5950X 16-Core Processor 3.40 GHz,
NVIDIA Geoforce RTX 3080, driver version 516.94.Any help? Please 🙏Has this problem been solved? I also have this situation, can you help me solve itPowered by Discourse, best viewed with JavaScript enabled"
18,nvidia-folder-lock-in-the-local-host,"Hi,I am following a github page to establish ROS2 with VDA5050 Isaac mission dispatch.
Screenshot from 2023-07-20 16-57-141246×827 80.5 KB
According to the tutorial, I must be able to access the files in Nvidia. However, it looks ‘read-only’ permission.Could you let me know on how to change the permission to edit?These are not editable files. These are accessible for all customers to read. You can copy them into your own working directory and edit them from there.Powered by Discourse, best viewed with JavaScript enabled"
19,omniverse-revit-connector-v202-0-2400,"We are pleased to announce the release of Omniverse Revit Connector v202.0.2400!This version adds the Omniverse Kit UI file and folder pickers as well as big improvements to batch exports and customizable 1-click exports.More information on this release can be found here:
release ntoesReinstaling windows 10, Revit 2024 and omniverse connector  202.0.2400. Export the sample project and no idea the UV  still bad. I dont understand why  and “0” support since months???

Img1515×1057 60.9 KB


image1144×831 396 KB
Powered by Discourse, best viewed with JavaScript enabled"
20,how-to-randomize-only-one-material-for-multiple-objects,"Hey there,I want to randomize the materials of the floor tiles from the warehouse (warehouse_multiple_shelves.usd) example. Therefore I added some Materials from the materials tab to the scene which are then located at  /Root/Looks/* . I want to choose just one material for all the floor Elements but I can’t find the right method to do so.I am able to choose random Materials for every floor Element individually with the following method:But I would like to be able to do something like this:But I don’t know how to get one random material from the  rep.get.material  method as it returns a  rep.replicator.core.get.material  and i can’t find the documentation on those objects. Is there any documentation on the  rep.replicator.core.get.material  Object and/or on the  ReplicatorItem  object to look things up?
Also the  rep.modify.attribute(name=""material"", ...)  method does not change the material of the Tiles. Maybe I did something wrong there.Thanks in Advance.First I would move all of the floor prims into a newly created Xform Prim:Then I would try running this script:What this will do is change the material for SM_floor and apply it to all of it’s children. It is import to have rep.create.material_omnipbr inside of the trigger functionality to see the material change.Also note that loading material textures can be a bit expensive, so if you have only a limited number of textures (as in you know you’ll be re-using them) and you’re going to be shuffling your training data before training, you can place that block under a separate trigger with a higher interval. That way your characters and camera randomize each frame but you minimize the number of time Kit has to load in a new texture.Thank you very much for your fast reply.I try to do this with materials from the  Window/Browsers/Materials which I add to the stage. As they are better suited for what I need them and I want to randomly choose one material from a large number of materials which would be a lot of work to put them all in a list manually and as they may vary from stage to stage it would not be very efficient.Isn’t there a possibility to do this with those materials? So to get a list of all the materials in the  /Root/Looks/ folder from the stage. And then use theThanks for the hint. I also considered doing the material randomization not every frame but instead like every 100 Frames. I hope I understood it right how to do this (in pseudocode below). So that in the End it would look  something like this:Are there any updates on this topic? Because I am trying to use the Materials from Window/Browsers/Materials but i am not able to assign them via a Python Script? What is the reccomended way to do this?If I assign the material like this:

image749×191 11.1 KB
I get the following error message and the rendering process stops ==> I have to stop the script manually:

image1326×130 38.6 KB
Maybe there is a possibility to load a material to the stage via a script?My solution was to first load the material into the stage and then assigning it to the prim:

image857×475 25.7 KB
We will have some material workflow updates coming to a future version of Replicator that will help with adding materials to a scene, applying them and examples of editing their properties.For others who might be coming here in the future, we have and will be adding material example scripts here.Thank you, looking forward to it.Powered by Discourse, best viewed with JavaScript enabled"
21,problem-of-using-the-replicator-with-a-fully-developed-scene,"I did use the codes below to label a bear in existing loaded Issac sim scene but it seem not work.import omni.replicator.core as rep
with rep.new_layer():
render_product = rep.create.render_product(‘/Root/Camera’, (2048, 1024))I do not know if that is the complete code, but your script is missing rep.orchestrator.run() in the end. Please post the complete code if possible.I just want to find the prim and label it but the full code requires triggering randomization to both randomize and label.Yes I understand that. I had a similar problem when I tried to retrieve the camera position.  What you’re actually doing is preparing a graph - but that graph hasn’t run yet. I asume that’s why when you try to apply the labels that it is not working.Hi @dangkhoaI tried your script with a slight modification in Isaac.To simplify the scene added a cube instead of a bear called “BearCube”, but the script is functionally the same, aside from adding the replicator import at the top.This script works correctly, but as julian mentioned above, when you enter this script into the script editor, you must click the “run selected” button at the bottom of the script editor to create the replicator graph. Then you can preview results by selecting Replicator>Preview a few times to verify that the bear is being randomly hidden or made visible.Powered by Discourse, best viewed with JavaScript enabled"
22,isaac-sim-rendering-is-getting-slow-after-some-time,"I am using isaac sim for the following purposeHi @nv_rtx - The gradual slowdown in rendering speed could be due to a few reasons:Here are a few suggestions to improve the rendering speed:after setting the world I am calling set_pose first to change the position of the object. Then using render function I am getting the image. This happens in loop in an another program. Do you think I am missing something herePowered by Discourse, best viewed with JavaScript enabled"
23,telecentric-lens-in-isaac-sim,"Is it possible to model a telecentric lens in ISAAC sim?Would the camera parameters be enough to setup such a camera?Powered by Discourse, best viewed with JavaScript enabled"
24,issac-sim-people-simulation,"Hello, I’m working on characters simulation with omni.anim.people. I have an issue when I try to increase the scale of the character.
If I apply a transformation on the scale of the character on the xform root, my character is scaled but when I start the simulate, the character no longer moves to the target position and he moves weird.
I think it’s because the skeleton returns to its original scale when the simulation is launched. How can I also apply the scale transformation while the simulation is running?Powered by Discourse, best viewed with JavaScript enabled"
25,bug-some-writer-rgb-images-are-blacked-out-when-generating-with-randomizer-instantiate,"Hi,I’m working in headless mode to generate RGB images and Semantic Masks for several models (usd files all under the same directory), for each frame, I want only a single object with its mask.To do that, I’m using randomizer.instantiate with size=1, after calling utils.get_usd_files on the models directory (this is similar to the SGD tutorial).For most results, this is working correctly, however some images (about 5%) are completely blacked out
The segmentation mask is still fine.
Note the source USD file is ok - If I’m only running the script on a single model at a time (using the from_usd command) this works well with all models in the folder.Here’s the code:I’m not able to reproduce the issue on my end. Which version of Replicator are you using? Also, what are the values you are using for IMAGE_SIZE and FRAMES?Would you also be able to share the .usd file for further testing? You can DM the asset if possible.Powered by Discourse, best viewed with JavaScript enabled"
26,maya-arnold-error,"For unknown reasons, the arnold shortcut toolbar will be lost when the connector is loaded, and the rendering settings will lose arnold。
maya2023.2
arnold core 7.1.3.0Sorry for the trouble. Arnold needs to load later then Maya Connector to make it render MDLs right.
Maybe you can run a script to make sure Arnold loaded after Maya starts.Powered by Discourse, best viewed with JavaScript enabled"
27,compute-the-corners-for-an-array-of-prims-not-a-prim-in-omniverse-code-script-editor,"I wrote the following code which computes the corners for a single prim.How can I pass an array of prims to it and compute the corners? I need to do this in Omniverse Code Script Editor not Isaac Sim.Also could you please verify if the way I have setup bbcash is correct? I am new to USD API.Hi @mona.jalalDid you find any solution for your query?Hello, here’s an example of how to combine bounding boxes for multiple primsIt is also possible to put the multiple prims under the same xform parent and retrieve the bounding box from that object.Powered by Discourse, best viewed with JavaScript enabled"
28,cat-model-is-craching-my-blender-3-6,"See, when I try to export this cat model, blender crashes:As you see from video, I did not even tried to use “child particles” or animation.
It just crashs when I export with default settings.
Blender Log files are here for you:
UMMLibrary.zip (147.3 KB)The other cat model with fur is exporting just fine!I cannot fidn out why its doing this, this is supposed to be AAA model.Royalty free 3D model Cat Grey Tabby for download as blend, obj, and fbx on TurboSquid: 3D models for games, architecture, videos. (1935214)I hope someone from Nvidia could check this cat model and why it crashes.
I managed to export it few times, but then crashing started and Blender re-intallation did not helped me…Hi @pekka.varis.  Thanks for reporting this!  By any chance, would you be able to attach the Blender crash log when this happens?  The crash log is usually written to the temporary directory:https://docs.blender.org/manual/en/latest/troubleshooting/crash.htmlThanks!
I made it crash again with the same export try.
Then I searched my PC for the file “GreyTabbyCat.crash.txt” but nothing was found…I just sent the original blender file for you as PM.
Hope you have some time to check it out :)Ha! Windows found the crash log after a while…GreyTabbyCat.crash.txt (39.5 KB)Hi @pekka.varis.  Please try the export with the latest Blender 3.6.0 Alpha v200.1 that was released today to the Omniverse launcher.  From my tests, this version addresses UI issues that were causing the crash, but please let me know if this scene still crashes for you.  Thanks.Hello!
Yes it does not crash anymore, but the fur animation is not exported to usd anymore:See in the previous version it worked:What might this be?Hi @pekka.varis.  I’m sorry for the delayed reply.   We’ve been investigating this issue and will report back once we have a resolution.   Thanks for reporting this.@makowalski Hi there!
Is this issue fixed on the latest 3.6 Alpha ?
just asking, i am on a holyday without a RTX gpuHi @pekka.varis.  It appears that the lack of animation is actually an issue in Composer.  The Kit/Composer team is investigating this.  I will update this thread when this is resolved.@pekka.varis As an update, we were just informed that this issue has bee fixed in Composer.  We will let you know when this fix has been released.Great news, Thanks @makowalski !Powered by Discourse, best viewed with JavaScript enabled"
29,isaac-sim-lane-following-from-jetbot,"How to train the JetBot in Isaac Sim with reinforcement learning and test this trained RL model on NVIDIA Jetson Nano.this link shows about training jetbot using Isaac sim had plenty of tutorials on the NVIDIA website which are no longer there.
i am trying to train a jetbot to do lane following using the camera module.master/isaacsim_patchAn educational AI robot based on NVIDIA Jetson Nano.
this link sadly has plenty of deprecate code
for instance “omnikithelper” is no longer used. there is “from omni.isaac.kit import SimulationApp”i use the camera function from the from “omni.isaac.sensor import Camera”camera = Camera(
prim_path=“/World/Jetbot/chassis/rgb_camera/jetbot_camera”,
resolution=(256, 256),
)my_world.scene.add_default_ground_plane()
my_controller = DifferentialController(name=“simple_control”, wheel_radius=0.03, wheel_base=0.1125)
my_world.reset()
camera.initialize()i = 0
camera.add_motion_vectors_to_frame()while simulation_app.is_running():
my_world.step(render=True)
print(camera.get_current_frame())
if my_world.is_playing():
if my_world.current_time_step_index == 0:
my_world.reset()
my_controller.reset()i followed some of the tutorials which just shows standalone to train the bot to reach an object.
but sadly nothing on using camera from the jetbot to do real time decision using ML model.
any suggestions are welcome here.There is a bug in the current camera API that will be addressed in the next release.  I have a work around for this though!  You will need to use the replicator API directly thoughFirst, we need to define the render product and attach it to an annotator.then, when we want to capture we need to step the orchestrator, and retrieve the data from our annotatorThis is a much more elegant solution than the hack I came up with for my live stream :/ such is life!Powered by Discourse, best viewed with JavaScript enabled"
30,abs-hard-leather-grey-turns-into-red-after-being-added-to-mesh,"
image1435×811 60.1 KB
After opening my project this morning all grey meshes with the material ABS_Hard_Leather_Grey turned into the red, which can be seen in above picture. This picture also shows a newly created stage, where I added ABS_Hard_Leather_Grey to a cube, which turned red instead of some grey. Are other people experiencing this bug as well?I’ve run into this too recently. Let me check with the dev team to see what it means when a material goes red like that.Hi @riekeles. Red material is usually an error material. Something failed. Either compilation or Material resolution.  What is ABS_Hard_Leather_Grey and is it from NVIDIA? If so, can you provide:Hey Sorry for not replying - must have missed it :/
The ABS material was from NVIDIA and probably deleted. I just changed the colour to a different grey, provided by NVIDIA.Powered by Discourse, best viewed with JavaScript enabled"
31,omniverse-connect-sample-updates,"100.2100.1101.1.5Hi，I found a bug in connect sample
If I run run_py_sample.bat, ERROR comes：
Python, USD, and Omniverse Client libraries are missing.  Run prebuild.bat to retrieve them.I solve it by modifying the bat file below：Just change %PYTHON% to “%PYTHON%”
Because sometimes usrers like me will install omniverse in a directory with blank space，for example
“E:\ProgramData\NVIDIA Corporation\Omniverse\lib\connectsample-101.1.6”Thank you for finding this and fixing it.  I’ll make sure this makes it into the next update!102.1.5In this release we’ve fixed a few things that needed cleaning up, but the biggest change was allowing the user to use their own Visual Studio compiler on Windows using a build tool configuration utility if they choose.  We added a dome light with an HDR texture and release notes for the Omniverse Client Library.
image957×363 43.8 KB
SamplesOmniverse Client Library103.1.0In this release we added a couple helpful samples referenced in the forums, docs, and the November 2021 GTC “Making a Connector for Omniverse” session.
A31415+MakingAnOmniverseConnector+LouRohan (Time 0_45_43;22)1920×1080 164 KB
SamplesOmniverse Client Library104.1.0SamplesOmniverse Client Library200.0.0The 200.0 release signifies big changes with the distributed Omniverse libraries. The Omniverse team has separated the USD resolver plugin from the Client library to make it easier to support more versions of USD. Live synchronization is improved for the purposes of performance, simultaneous editors, and supporting the entire USD layer editing feature set. Omniverse applications and Connectors are also now subscribing to the notion of live sessions to support non-destructive workflows. This version of the Connect Samples includes new Python and C++ examples to demonstrate this live session workflow.SamplesOmniverse USD ResolverOmniverse Client Library200.1.0
This small update to the Connect Samples fixes some simple bugs in the live session classes and updates the Omniverse Client Library and USD Resolver with the latest versions.202.0In this update we added some sample code for adding a reference, payload, and modifying a material parameter from code.

image1246×433 98.5 KB
203.0Release Date: June 2023With this release the shipped USD version is updated to 22.11 and Python 3.10 to align with the rest of the Omniverse platform.  Also a skinned skeletal mesh example was added to demonstrate how to author, skin, and animate skeletons in a live layer.Powered by Discourse, best viewed with JavaScript enabled"
32,cant-assemble-uploading-1-gif,"How to troubleshoot assembly issues？
cannot be inserted？it can not be inserted,how to solve?Hi @ahaobuptIt seems that collisions are not defined in an proper way.
Please, check the documentation for details:https://docs.omniverse.nvidia.com/app_machinima/prod_extensions/ext_physics/rigid-bodies.html#collision-settingsAlso, visit Physics Core — Omniverse USD Composer documentation for visualizing collision-shapesthanks，I solved this problem by changing the convex decompositionThis topic was automatically closed 14 days after the last reply. New replies are no longer allowed.Powered by Discourse, best viewed with JavaScript enabled"
33,omnigraph-node-value-readout-using-python-code,"I am trying to articulate ur5 robot by subscribing to joint_states topic. I don’t want to use articulation controller to move the robot.  Is there anyway to readout the values from ROS2 joint state subscriber node using python code?Hi @yadun.murali.kurikalveed  -  Yes, you can subscribe to the joint_states topic in ROS2 using Python. Here is a basic example of how you can do this:In this code, callback is a function that will be called every time a new JointState message is published on the joint_states topic. The JointState message contains information about the positions, velocities, and efforts of the joints.Please note that you’ll need to have the sensor_msgs package installed to use the JointState message. You can install it with sudo apt install ros-<distro>-sensor-msgs, replacing <distro> with your ROS2 distribution name (e.g., foxy).Also, remember to source your ROS2 environment before running the script, like so: source /opt/ros/<distro>/setup.bash.Powered by Discourse, best viewed with JavaScript enabled"
34,issue-with-objects-with-link-from-iclone8-project,"Hi, I imported in Omniverse a project from Iclone 8. In this project, a character picks some objects and places them into a box. Each object is first linked to a character’s hand and then linked to the box when placed in it (the character holds the box with a hand and by the other hand picks the objects, so they are moving in the scene). The issue is that in Omniverse these objects are not in the place they are supposed to be, instead they “fly” around the space.
Is there any way to solve this problem?Are you exporting USD or transfer over live session?
could you private message me the project to check?Hi, thanks for your reply. Unfortunately I cannot send this project. It was exported in USD. I “solved” the problem by making a frame by frame rendering. Believe it or not, if I render the sequence these objects go around the space, but if I render each frame separately they stay where they are supposed to be. Maybe there is some issue with these objects in Iclone but I’ve not understood what kind of issue yet.
Thank you very muchPowered by Discourse, best viewed with JavaScript enabled"
35,how-to-report-an-issue-with-omniverse,"Please note: if you are an Enterprise Customer, please contact your Omniverse representative for further assistance.The best place for getting help with any technical issues is through our Omniverse development forums (you are here!)If you are experiencing an issue, please let us know so that we can improve your experience.The more information you give us about your issue, the faster we will be able to help.
Please include the following:You can found the location of your Logs Directory via the Omniverse Launcher > User Icon > Settings > Logs LocationLogs Location1249×697 188 KBCache Output Logs: C:\Users\<USERNAME>\AppData\Local\ov\pkg\<CACHE_VERSION>Console Logs: The location where logs are stored on your machine can be accessed by clicking on the folder icon in the Console tab:For crashes, please zip and attach your full logs found here: C:\Users\ [YOUR NAME] \ .nvidia-omniverse\logsTwitter | Twitch | YouTubePowered by Discourse, best viewed with JavaScript enabled"
36,audio2face-lip-synch-not-working,"Hello,I’ve gone through all of the steps to bring in the face of a DAZ 3D Genesis 8 character into Audio2Face.  This character has previously been setup and rigged in Blender for use there.I got everything to work up to and including the generation of Blendshapes, exporting those and then importing them into Blender as shape keys, as well as transferring them to the original mesh.That all works fine.However, when setting up the A2F Pipeline there’s an issue.  When I attempt to set things up according to an NVIDIA Omniverse livestream from 2 months ago, the dialog box does not appear the same as the one from the livestream.For Select A2F Core Type I select Full-Face Core and all of the additional meshes: Upper Teeth, Lower Teeth, Tongue, Left Eye and Right Eye should appear in the dialog box, but I don’t see them.  Additionally even when I click the “YES, ATTACH” button, the player for the voice appears, but neither the Mark mesh, nor my fitted mesh, move with the audio.I’ve attached a screen image of the A2F Pipeline dialog box.I’ve followed tutorials several times, but I can’t figure out what’s going wrong.I can provide files which might be helpful.Thanks so much for any help you can provide.  :)
Audio2Face (1)1920×1020 205 KB
Here’s a screen image of the Character Transfer tab as well as to show I’m including the additional meshes with the skin.
Audio2Face (2)1920×1017 229 KB
I figured out what was going on.  The dialog box has apparently been updated since the tutorial videos and livestream that I watched.So, I’ve been able to set things up correctly to get the lip synch working with my imported mesh!  :)This topic was automatically closed 14 days after the last reply. New replies are no longer allowed.Powered by Discourse, best viewed with JavaScript enabled"
37,quadruped-controller-in-the-example-cannot-move-backwards,"Hi I’m planning to use the Quadruped Controller in Isaac to control Unitree A1 Robot movement using Python. I tried the example from [here] (Quadruped — Omniverse Robotics documentation) in Isaac Sim, but for some reason the A1 cannot move backwards when I pressed the “NUMPAD_2” or “DOWN” key.
Is there anything I should look into or anything I can do to fix this? Thank youHi @sanundsha - Do you mind sharing the python script here to debug the issue further?Powered by Discourse, best viewed with JavaScript enabled"
38,removing-prims-between-frames,"Hi,
Is it possible to remove prims and generate new ones between frames?
For example, generating a random number of spheres and cubes, scattering them on a plane for one frame.
Then removing those and generating a random number of spheres and cubes, scattering them on the same plane the next frame, but with different numbers of spheres and cubes.
I am unable to find any methods to remove previously generated prims in the API.
Help on the matter would be appreciated.ThanksHere’s the code i used. The cubes & spheres are generated on first spawn, and do not change between frames. I would like to have it so every frame has a random combination of cube and spheres.If I understand what you are trying to randomize correctly, something like this should work:Basically, you’ll want to create all the shapes you want to randomize first in addition to the planes where you want them to be scattered. Then you can group them all and randomly toggle their visibility, and then scatter the “left” and “right” shapes on the appropriate plane.Hopefully this helps!Would it be possible to implement a better way to do this? For my purposes I am planning to have >10 different types of USD imported in place of the sphere/cube as well as >5 planes for them to be scattered across, and using this method would result in a large mass of code and unused prims.Take a look at this exampleRandomizing appearance, placement and orientation of an existing 3D assets with a built-in writer — Omniverse Extensions documentation (nvidia.com)This instanciates a number of plant props in a scene.  You can modify it to use scatter_2d to place them on planesIs there a way to ensure annotation labels are saved with that method? I have tried saving usd files with annotations before but apparently the annotations aren’t saved? Is there a way to modify the usd files such that they are annotated?Which annotations do you mean?Semantic annotations should be able to be saved on the objects- though depending on how you save it could be in an override layer of USD instead of the original USD file.For standard Replicator Annotators - they would be output to disk via a writerPowered by Discourse, best viewed with JavaScript enabled"
39,beginner-questions-of-audio2face-tool,"Hey guys, I’m new in Audio2Face tool, and I gotta, say I dont have a gpu with RTX to test most of the things of the omniverse on my own laptop yet. So I don’t understand much of it so far and I’d like to get to know more of it before buying a hardware to use it on my work pipeline. Can you help me with a few questions?
_In order to use the Audio2Face to generate facial expressions do I have to create the Blendshapes previously from scratch? Eg: Creates Anger blenshap  on Blender export and then A2F will read the audio and interprete it as angler and use my pre made blendshape? Or do AUdio2Face can sorta “rigify” the face and use its new or previous own blendshapes over the character? Can we use a blendshape preset?Welcome to A2F! I would encourage you to take a peek at the tutorials by Charles:It may not answer ALL of the questions you have listed, but it should give you a general overview on what you can expect following the workflow in Blender. I will let more seasoned users chime in on your specific questions (i am still learning A2F myself)I am sure you may have already seen this, but if you do plan to upgrade or procure a machine with the intent to use A2F, here are the recommended spec for both laptop and workstation for professional use. In addition, this page will indicate the OS and mesh requirements before you commit to anything.Some of the demo scenes would use around 6GB of VRAM, so having a beefy GPU is definitely preferred especially if you plan to do livesync with another app.Powered by Discourse, best viewed with JavaScript enabled"
40,reproduce-mouse-behaviour-from-scripts,"Hello,How it’s possible to replicate the SHIFT+LEFT_CLICK behavior from the scripts to interact with the scene?I searched the source code and the documentation but still haven’t’s found any solutionHi @user126140 - Can you clarify what “SHIFT+LEFT_CLICK”  does in your scene?Powered by Discourse, best viewed with JavaScript enabled"
41,how-to-unistall-nividia-omniverse-launcher,"hello sir how can i unistall it completly because my control panel does not showing this application in his list and whenever i turn on or restart my laptop  the omniverse launcher application popup show on my screen ?? can you please help to remove this issue  because i alredy tried nivida clean up tool but they did not work they remove all driver from nivida but not omniverse launcher !!.Hi @kapilkataria420. You can use this cleanup tool to fully uninstall omniverse software and data: Cleanup Tool — Omniverse Launcher documentationPowered by Discourse, best viewed with JavaScript enabled"
42,isaac-sim-omnigraph-python-api-make-array,"Hello,We are trying to programmatically use a make array (ConstructArray), but we are unable to set anything which is not input0.Error:Reproduction script:In the GUI, there is the plus button for adding inputs, but how can it be done via the python api?Hi @omers  - It seems like the script is not able to find the attribute for the input1 of the velocity_command_duplicator node. This error could be caused by the node not being created properly before trying to set its input values.To fix this issue, you can try splitting the creation and setting of values into two separate calls to the og.Controller.edit() method. Here’s the modified script:import omni.graph.core as ogkeys = og.Controller.Keysog.Controller.edit(
{“graph_path”: “/nakai_container/MovementGraph”, “evaluator_name”: “execution”},
{
keys.CREATE_NODES: [
(“velocity_command_duplicator”, “omni.graph.nodes.ConstructArray”),
],
},
)og.Controller.edit(
{“graph_path”: “/nakai_container/MovementGraph”, “evaluator_name”: “execution”},
{
keys.SET_VALUES: [
(“velocity_command_duplicator.inputs:arraySize”, 4),
(“velocity_command_duplicator.inputs:arrayType”, “double”),
(“velocity_command_duplicator.inputs:input0”, 1),
(“velocity_command_duplicator.inputs:input1”, 2),
(“velocity_command_duplicator.inputs:input2”, 3),
(“velocity_command_duplicator.inputs:input3”, 4),
],
},
)Let us know if it works or not.@rthaker Thanks for the detailed response.First part creating the node working, second part trying to edit it throws the following exception:Hi @omers  - Can you try this other script? if it doesn’t work then I will bring this issue to the Omnigraph experts.Powered by Discourse, best viewed with JavaScript enabled"
43,no-correspondence-markers-bug,"I’m trying to add markers, but they are not showing up. Even when I close a2f and then open + start a new project, I’m not seeing them.image1920×1063 133 KBhrm looks like they are behind the camera. My mesh was export with all transforms reset.image946×846 29.8 KBLooks like a bug to me. The bug seems to be trying to set markers in front view. In the video you can see that I set them correctly in perspective but then lose them when I switch to front view.
i can confirm the issue occurs whenever the mouse hovers over anything in the camera control dropdown. this will result in all the correspondence points reset to the world origin (at least visually speaking) and not limited to just the front view (even if you stay in perspective).Thank you for reporting this issue. We’ll get this sorted asap.Powered by Discourse, best viewed with JavaScript enabled"
44,hi-how-can-i-create-a-new-user-in-nucleus-with-api-thanks-a-lot,"hi,how can I create a new user in nucleus with api.thanks a lotYou cannot create user. You need your Omniverse server manager to create account for you.Powered by Discourse, best viewed with JavaScript enabled"
45,how-to-listen-to-audio2face-livelink-server-in-python,"Hello there,I’m trying to use audio2face to generate blendshapes that I can read in my app. What I need is -I understand that I would need to make some code changes in the audio2face build to get that working so that the API returns blendshapes as well. Wondering how do i go about setting up the listener that sends requests to livelink server and receives the response in Python.Powered by Discourse, best viewed with JavaScript enabled"
46,is-it-possible-to-load-medical-imagiing-data-in-omniverse,"Hello I am working with some medical imaging data - I know that for this purpose Nvidia has Nvidia Clara, but I was unable to find a way to work with GPU accelerated mesh algorithms there.Mesh algorithms seem to be very well supported in omniverse - for example in warp API. As far as I know there is a way to use most of the python packages - but is it possible to load medical images in the omniverse for example using ITK ?Also is it possible to view the images not only as meshes but also looking through 3D object slice by slice?I know that all API’s can be used outside of the omniverse in traditional scripts - but It seems to be interesting idea to start developing medical imaging algorithms in omniverse - is it a bad idea?Thanks for helpThis is exactly what we want Omniverse to be used for!  It sounds like you would need to to make a Connector to ITK which would import your medical images over to Omniverse.  Check out Lou Rohan’s GTC talk about doing this here: Making a ConnectorAnd yes, your medical images can be “sliced” (Check out the Section Tool).Thanks for response @WendyGram  One more question becouse i was not able to find it is there some supported cuda accelerated stencil library?Stencil operations particularly for simulations are critical so i suppose there is a chance for a library in omniverse ecosystem covering this need@WendyGram hi! i was googling for similar use case of ingesting DICOM assets into Omniverse and stumbled upon this thread. Could you please let me know if there was any progress with Medical Imaging in Omniverse? Any URLs to read through?
Thank you!Just to clarify, I did not managed to find sufficient time to make it workPowered by Discourse, best viewed with JavaScript enabled"
47,can-omniverse-build-4d-progress-without-synchro-or-other-3rd-party-software,"Just coding in Omniverse, change color of the elements by time.Powered by Discourse, best viewed with JavaScript enabled"
48,erro-while-runing-python-script-to-setup-action-graph,"I am following the tutorial (8. ROS2 Joint Control: Extension Python Scripting — Omniverse Robotics documentation) to setup ROS2 bridge from Isaac Sim.In step 2, 8.3 Add Joint States in Extension. When I execute the python script it gives the following error.OmniGraphError: Failed to wrap graph in node given {‘graph_path’: ‘/ActionGraph’, ‘evaluator_name’: ‘execution’}I’m using Isaac Sim 2022.2.1The error occurs for me when I already have an Action Graph with the same name and the same path. Try renaming the name from “/ActionGraph” to something else (lines 5 and 36 in the doc).Powered by Discourse, best viewed with JavaScript enabled"
49,mcc-file-problems-for-maya,"Greetings,
I am trying to export an audio2face animation to Unreal Engine. The model is not a metahuman, only have a 3D mesh model. The process I’m following is to export it to Maya and then from there to Unreal Engine.After several attempts, I managed to make it work, but now I’m unable to repeat the process. The working geometry cache was generated with the ‘mcx’ extension, but now it only generates ‘mcc’.When bringing the “mcc” file into Maya, I’m encountering an error:“// Error: …_003_001_resultCache1 (Cache File): ..._003_001_result may have an invalid cache description or invalid, missing or empty cache file.
Cached Playback: The presence of unsupported Cache File node ‘…_003_001_resultCache1’ has triggered Safe Mode, and caching is disabled.
You can continue work, but caching has stopped.
Remove animation from ‘…_003_001_resultCache1’, or delete the node entirely, to resume caching.”When opening the associated XML file for the “mcc” file, I noticed that it has a strange format as it doesn’t identify the model, whereas in the “mcx” file, the model is referenced. And “mcc” file have a tag for maya 2018.mcc file:
“?xml version=“1.0”?
Autodesk_Cache_File
cacheType Type=“OneFile” Format=“mcc”
time Range=“0-102500”
cacheTimePerFrame TimePerFrame=“100”
cacheVersion Version=“2.0”
extra maya 2018 x64 extra
extra username extra
Channels
channel0 ChannelName=“head_meshShape” ChannelType=“DoubleVectorArray” ChannelInterpretation=“positions” SamplingType=“Regular” SamplingRate=“100” StartTime=“0” EndTime=“102500”
Channels
Autodesk_Cache_File”mcx file:
“?xml version=“1.0”?
Autodesk_Cache_File
cacheType Type=“OneFile” Format=“mcx”
time Range=“0-38200”
cacheTimePerFrame TimePerFrame=“200”
cacheVersion Version=“2.0”
Channels
channel0 ChannelName=”…_003_001_result"" ChannelType=“FloatVectorArray” ChannelInterpretation=“Points” SamplingType=“Regular” SamplingRate=“200” StartTime=“0” EndTime=“38200”
Channels
Autodesk_Cache_File""using audio2Face 2022.2.1 and Maya 2024Does anyone have any idea why this is happening and how to solve it?Can you try exporting using .usd file format to see if it works?
I see you have a very big time range (100500 frames) which might be the reason Maya cache stops working (just a guess though).Just make sure to install the latest Maya Connector (Legacy) from Launcher and use that one to import your usd file into Maya.This topic was automatically closed 14 days after the last reply. New replies are no longer allowed.Powered by Discourse, best viewed with JavaScript enabled"
50,graphical-artifacts-and-errors-in-launcher-ui,"image1317×705 53.7 KB
Host: OMEN by HP Laptop 17-ck1xxx
OS: Ubuntu 22.04.2 LTS x86_64
GPU: NVIDIA 01:00.0 NVIDIA Corporation Device 2460
Driver Version: 515.86.01os info hp omen laptop
d445ded5f6d7a8b343336875cf3f73a858×257 12.4 KBPowered by Discourse, best viewed with JavaScript enabled"
51,saved-usd-after-adding-script-with-script-editor,"Hi,I followed this tutorial to add a 3D-LiDAR to my scene using the script editor and it all worked fine:
https://docs.omniverse.nvidia.com/isaacsim/latest/isaac_sim_sensors_rtx_based_lidar.htmlWhen I run the simulation by pressing the “Play” button it creates an action graph that is not visible in the tree structure on the right hand side of the GUI, but I can can see that it exists when I press the “Edit Action Graph” button. Now, I don’t want the script to be executed anymore but unfortunately I already saved my USD.How can I delete the script that it is not executed anymore when I press play?Best regards,
LukasHi @lukas-bergs  - To remove the script from your USD, you can follow these steps:Please note that this will permanently remove the script from your USD file. If you think you might need the script again in the future, you might want to save a copy of your USD file before deleting the script.Thanks for your answer!
Unfortunately the node does not appear in the tree structure. I can only see the created action graph when I press the “Edit Action Graph” button.Screenshot from 2023-07-25 15-04-141802×926 81.9 KBIf I navigate to the layer tab I can see the Render Layer but I cannot delete it.
Do you have any suggestions what to do?I just found out that the SDGPipeline ActionGraph was not generated from the script I added but from the sim_camera ActionGraph. So everything works as it should.Powered by Discourse, best viewed with JavaScript enabled"
52,isaac-sim-jupyter-notebook-instance-in-windows,"how do i run issac sim on windows.
i am using python.bat in the terminak to run it.
but i want to use VScode and use jupyter notebook in it.
i was assigning the python kernal to the path
C:\Users\AI_Admin\AppData\Local\ov\pkg\isaac_sim-2022.2.1\kit\python\python.exe.
but this doesnt recognise the omni packages and so on.
the python.bat file does.
is there a work around it to use .exe file of any sort to assign to vscode kernelHi @sneha.verma -  Currently, Jupyter notebook is supported on Linux only.https://docs.omniverse.nvidia.com/app_isaacsim/app_isaacsim/install_python.htmlPowered by Discourse, best viewed with JavaScript enabled"
53,integrate-husky-franka-robot-error,"hello. I want to integrate husky 4-differential robot and franka manipulator.I tried merge two of them, but it have some problem.When run the script below error is occured.

Screenshot from 2023-01-16 17-35-091305×176 42.9 KB
And i want when the sim is started, franka goes to initial position. The result of these process is successfully finished. but franka robot movement is effect to the husky robot.How can i fix of this problem?I did fixed joint between husky & franka, and tried exclude from articulation. after the chosen above option robot is goes to sky. I don’t know what is the problem these progress.
Just want to clarify a few things first:thanks,Hi @leejisue , were you able to resolve the issue? if not, then can you please answer the questions asked in the thread in order to help you resolve the issue.Sorry for the late reply. It has not been resolved at this time, and it seems that mobile manipulator is not working in isaac sim. Can you solve the problem in detail?? I’d really appreciate your help.Hi @leejisue - Can you help answer these questions first?Just want to clarify a few things first:Hello. I’m sorry for the late.As a result, we have solved the problem at the momentarily. It has been confirmed that the two robots are working normally for the combined model as shown below.
Powered by Discourse, best viewed with JavaScript enabled"
54,randomizer-gui,"Hey, i installed omniverse last week and i have already one question. I tried to follow this tutorialCollecting a variety of data is important for AI model generalization. A good dataset consists of objects with different perspectives, backgrounds, colors, and sometimes obstructed views.and noticed, that my GUI has some deviations compared to the GUI in that video.I failed to create a randomnizer scene via  windows->issac-> Movecomponent and co.I found some Randomizer Examples docs:Randomizer Examples — Omniverse Extensions documentation (nvidia.com)My question is, is it possible to create randomnizer scenes only with python or is there still the GUI option?I prefer for the beginning to use the GUI but if it is not possible i can switch to python.Hi @jenkings23 - It is possible to use Randomizer tool with GUI. Please refer the attached document: Randomizer Tool — Omniverse Extensions documentationI found this docs also this week, but i think i can only radomize my objects after i pressed seed. My goal is to run the randomization automatically all few minutes without setting anything.Sorry, do you want to automatically load this extension every time you start the app?

image1253×1062 191 KB
If you want to automate the randomization then you will have to write the code. More information can be found here: Programming GuideHey, yes i want to automate the randomization and i notice i have to write my own code. Thanks for responding.This topic was automatically closed 14 days after the last reply. New replies are no longer allowed.Powered by Discourse, best viewed with JavaScript enabled"
55,how-to-programmatically-select-a-prim-in-create,"Hi All,I’m trying to build an extension and need to select a prim programmatically from within the extension. I am not able to find any code snippet to do so.Any help in this regard would be much appreciated.You can check an example here Select a Prim by Prim Path — Omniverse Kit documentation.This topic was automatically closed 14 days after the last reply. New replies are no longer allowed.Powered by Discourse, best viewed with JavaScript enabled"
56,accesviolation-exception-while-saving-usd-stage-with-c-api,"Hello,we have writte a c++ wrapper around the omniverse api for calling it from our .net application. We have an external method “Save” that has the following code:void Session::Save(bool usdLiveProcess)
{
//stage->SendFastUpdateNotice() ???
stage->Save();}When using this we get sporadic AccessViolation-Exceptions when calling this method. In most cases it works as it should, but in some cases it throws an ave and the process terminates.Are there any known scenarios when this could happen (for example using the desktop nucleus with more that 2 parallel users / calls) or something like that?Can anyone provide any help how to figure our what causes this problem?Thank you very much for the helpCarlI’ve stripped down the Save-method to just a single call to omniClientLiveProcess(). But still getting an AccessViolation (but less often).That leads me to a general question about the client api… is it thread safe? Our connector is a plugin for our server application that syncs many different sessions to a corresponding usd scene… so it is possible that the calls against the nucleus client api could be made in parallel… is thid a problem? If yes what is thread safe and what not?Powered by Discourse, best viewed with JavaScript enabled"
57,have-problem-getting-started,"Hello,I am new to this software and would like to use this on my AGV project.
When I try to follow the tutorials on the website some sections are unavailable (404 not found).
Currently, I am following the tutorial for the Omni graph. After I add the controllers it shows “prim path not valid”. Can you please help and take a look at where it is wrong?
image1885×1160 290 KBThanksMilesHi @c7ma  - The documentation platform was going through upgradation. I don’t think there should be any 404 issues but if you do then please share and I can help resolve that.Can you share at what step from the tutorial you are having issues?Hello,Thanks for replying! I was browsing through other posts these days and found the updated links for the documentation. And after I tried another several times it finally worked. I think originally I made a mistake when specifying the token name.CMThis topic was automatically closed 14 days after the last reply. New replies are no longer allowed.Powered by Discourse, best viewed with JavaScript enabled"
58,crash-during-train,"I am training using RL with omniverse isaac, during the training with Headless=True, I got this errorwhat is the problem?Hi there,This error generally occurs when the observation buffer in your task contains NaNs. It looks like physx is throwing some warnings of Invalid PhysX transform detected, which may have caused incorrect behaviours in the physics simulation. Could you double check to see if your scene setup is correct by running with the viewer first?@kellyg , I have the same issue with viewer too, it happen randomly during training…is there anyway to restart the environment?Were you able to observe anything off in the simulation with the viewer? You can try using the random_policy.py script instead of training so that the NaNs don’t cause crashes when propagated to the training code. Generally, this is a result of bad simulation behaviour potentially caused by incorrect physics parameters, which will cause NaNs in simulation. When you retrieve the physics states as part of your observations, the NaNs are picked up and then propagated into the training code, leading to the error. Please also verify if you do indeed have NaN values in your observation buffer.hello,i have faced the same problems, When the training process is up to 10%, the nan value came out. The truth is that the training process could be started successfully, but the nan value came out randomly, which will terminate the overall training process. Is there any solution to avoid this? Of course, I still need to modify my scene, but I dont know the nan value is related to my closed chain in my robot,.Powered by Discourse, best viewed with JavaScript enabled"
59,cinema-4d-support,"Are you going to release a livesync for cinema 4d?Hi @slckyldr27!  Yes, Cinema 4d is on the development schedule! I don’t know what the expected release date will be.I would love to hear if there is any updates on this.Hi @nickjito!  We do have it on our development schedule, but I don’t have dates.  I reached out to our Connector Lead for more up-to-date information and I will post back here when I hear back.To my Community Friends, if this is an application that you want to see on Omniverse, let us know!@nickjito Good news!  The latest update from the Development team is that we are actively working on a Cinema 4D connector.  So keep your eyes peeled for this very soon!@WendyGram Thanks for the great update! I can’t imagine an other application having higher priority than C4D ;)Cheers!I joined the community today because if my interest in the C4D connecter.  Just downloaded Omniverse today, and couldn’t find anything about c4D…  look forward to update soon ;)Very necessary! Cinema4D has a large user base for making motiongraphics, and Omniverse’s real-time Path tracer can solve a lot of rendering pain. Looking forward to seamlessly integrating Cinema4D’s powerful motion graphics module into the Omniverse.
p.s. Also, do you know about X-particles? A bit like the role of Tyflow in 3DMAX, it can create complex particle, smoke, volume, and cloth interactions, but it is not real-time. If OMNIVERSE’s FLOW can have the ease of use of X-particles in the future, I believe there will be a large number of 3D artists come to use!Hope to achieve these effects in this video in Omniverse in the future, it will be popular on social media.

Would thoroughly enjoy C4D support. Although Maya is a standard, it’s sort of like the Avid Protools (audio mixing) of 3D graphics. Old, clunky, and has a way of doing things that can’t be changed because too many big studios use it.Omniverse, and for that matter, Unreal, are all about making things easier and more fluid. I think C4D really fits in well to omniverse. Hopefully it is added soon.@WendyGram Does omniverse now support C4D?
Thank you.Hi, i would love to know how is the development of the Cinema 4D connector going. Any advance? Thanks.Hi @WendyGram
Is there any update about C4D?
I can’t find the beta download link.
Thank you.Hey all!  I do not have any new information on the C4D connector.  It is on our development plan, but I do not see any dates posted for it yet.  Several people on our team use C4D, so this definitely an something we want to make available as soon as possible.Thanks for the update. Would love to see it happenhoping for c4D connector soon! will really complete the toolbox and make sense if omniverse has blender, maya, 3ds max, substance painter and unreal engine 😁Just wanted to pipe in and say C4D support would be greatly appreciated.Hi guys, is there any ETA for C4D connection plugin ? Cheers, LevI really REALLY hope this is not D.O.A. ?!Hi all - here’s the official update:We are working closely with Maxon in the development of an enhanced Maxon/Omniverse USD workflow including live connectivity with Cinema 4D and Redshift. At this time we are not able to provide a firm timeframe, but we are excited that the community is enthusiastic in seeing this development come to life. We’ll share more as soon as we can.P.S. Have you registered for GTC, which is free and virtual?  GTC 2022: #1 AI ConferenceAlso, If you have work in Omniverse that you’d like to share, please submit it to the Omniverse Gallery Page! Omniverse Gallery Submissions | NVIDIAhe / him | LinkedIn | Discord ID: Prof E#2041
Community Engagement for Omniverse: "" A New Era of Digital Twins & Virtual Worlds with NVIDIA Omniverse"" Video
What is NVIDIA Omniverse? Video | Check out the “Meet the Omnivore” blog series hereOmniverse Resources:
Omniverse | Documentation | Tech Requirements | Videos | Twitter | Discord | TwitchThe Omniverse documentation site has more info and links to all tutorial videos (under the “Learning and Feedback” category on left nav bar): Omniverse Platform Overview — Omniverse Developer documentationPLEASE NOTE: If you are reporting a bug/issue, please provide OS, GPU, GPU Driver, the version of the app, and full log file (if applicable). For crashes, please zip and provide a link to your logs → C:\Users\ [YOUR NAME] \ .nvidia-omniverse\logsPowered by Discourse, best viewed with JavaScript enabled"
60,omni-machinima-warmup-bat,"I have been trying to install Machinima from exchange and it keeps getting stuck on OMNI.MACHINIMA.WARMUP.BAT. I’ve tried uninstalling and reinstalling omniverse, tried to log out and log back in, etc.Powered by Discourse, best viewed with JavaScript enabled"
61,why-the-ros2-joint-space-subscriber-cannot-recieve-efforts,"Hello, I am working on Isaac Sim to adapt it to legged robotics.So far as I know, it seems that the ROS2 Subscribe Joint State node cannot receive JointState.efforts msg.
(As described in 8. ROS2 Joint Control: Extension Python Scripting — Omniverse Robotics documentation)
When I sent JointSpace.effort type message in the above example, it showed an error like the below picture.

image1918×1044 329 KB
It seems a bit strange as it works well with the position or velocity commands, and even the effort command already exists in the node options.I wonder whether I am doing something wrong or if the feature will be added in the next release.
Thanks in advance.Hi - Sorry for the delay in the response. Let us know if you still having this issue/question with the latest Isaac Sim 2022.2.1 release.Hi there,
I’m still facing this issue with the latest Isaac Sim release.
Do you have any update regarding this?GreetingsPowered by Discourse, best viewed with JavaScript enabled"
62,bug-report-articulations-with-cylinder-based-collisions-fall-through-default-ground-plane-in-isaac-sim-2022-2-1-gpu-physx,"Some robots fall through the default ground plane when running physx on the GPU.The asset we used for testing was generated by:the resulting humanoid_robot_cylinder.usda is attached
humanoid_robot_cylinder.usda (71.9 KB)Next, run the following script that adds the humanoid robot to a scene, and drops it above a ground plane:
fall_detector.py (2 KB)call the script via python fall_detector.py humanoid_robot_cylinder.usda humanoid_instanceable/torso --physics-device cudaWe print the world poses of the articulation view after 300 simulation steps – we expect this to be above ground. But when simulating physics on the GPU, this is below ground.If we re-enable collisions on the ground plane after world reset, the articulation collides with the ground as expected.Here’s the relevant code sample as text:Powered by Discourse, best viewed with JavaScript enabled"
63,trouble-with-starting-reinforcement-learning-internal-index-error-suspected-due-to-an-articulationview,"Hi, I’m attempting to set up a Reinforcement Learning Agent (think: box with four wheels stuck on the side), having cloned the OmniIsaacGymEnvs repo and followed their format. I think I’m having an issue with setting up my ArticulationView. I know that my joints are accessed correctly through the regex path given; I also tried following the stable-baselines3 demo as well. However, in either case, after creating and adding the ArticulationView to the stage, I’ve been receiving this index error (not in my code directly):I’m starting to think it could be a problem with the usd file I’m using but it’s very barebones.
Any help is appreciated, I’ve been stumped for days.Hi @falquez  - The error message you’re seeing is indicating that there’s an attempt to access an index that is out of bounds for the given dimension. This is a common error in Python when dealing with arrays or tensors.In your case, it seems like the error is coming from the get_gains method in the articulation_view.py file. The line kps[self._backend_utils.expand_dims(indices, 1), joint_indices] is trying to access an index that doesn’t exist in the kps tensor.Here are a few things you could check:Powered by Discourse, best viewed with JavaScript enabled"
64,there-are-no-debug-kit-exe-in-the-extension-template-with-kit-105,"Hi, I’m currently working with the cpp Extension Template. I used kit 104, and now kit 105.
But with kit 105, when I build the project with build.bat, the _build/windows-x86_64/debug files are not build :_build/windows-x86_64/debug folder after a build in kit 104 :
_build/windows-x86_64/debug folder after a build in kit 105 :
We can still use the template (with omni.app.kit.bev.bat from the release folder), but we can’t use the debugger (like from Visual studio 2019).Powered by Discourse, best viewed with JavaScript enabled"
65,error-while-using-rep-distribution-choice,"Getting the following error while using rep.distribution.choice:You can reproduce the error by running following code:Hi @ShyamPatel. Welcome to the forums! This is more of a Replicator-specific question so I’ve moved it over to Synthetic Data Generation forum.Hi @ShyamPatel we have a bug tracked for this. The issue is that a uniform float can’t be passed, it need to be a vector. Try something like this:@pcallender Thanks for your reply.I have one more doubt here. So finally rep.distribution.choice() will return output vector with shape of 3, right? But, here I need only single value.scale takes 3 parameters explicitly (x, y, z).There are some convenience methods elsewhere that interpret a single value as a uniform scale, but currently in this case the scale parameter expects 3 values.Using choices=[(0.8,0.8,0.8), (1.0,1.0,1.0)] is currently the correct way to select a choice of uniform scaleThis topic was automatically closed 14 days after the last reply. New replies are no longer allowed.Powered by Discourse, best viewed with JavaScript enabled"
66,loading-lights-into-isaac-sim,"https://docs.omniverse.nvidia.com/kit/docs/kit-manual/latest/api/pxr.html?highlight=lux#usdlux-moduleWhere is the documentation for UsdLux? I’d like to add a sphere light and set it’s location, but can’t figure out how to do so from the docs.All I can do is scraped from examples, like:Hi,I’ve implemented it as follow:The class is then used in the following function:Thank you!This topic was automatically closed 14 days after the last reply. New replies are no longer allowed.Powered by Discourse, best viewed with JavaScript enabled"
67,unable-to-install-blender-3-6-200-2-via-launcher-preexisting-blender-3-6-install,"The installation seems to get all the way to the end and then stops and removes all the files it installed in my content folder.I get a lot of errors that reference various addons I have installed in a regular installation of Blender 3.6 I have installed. It seems like the issue is due to a preexisting installation of Blender. How can I install the Omniverse version alongside it? If this is caused by certain addons from the appdata folder can I isolate omniverse from all addons that aren’t included in its package?Here’s a small snippet of the error.Error occurred during installation of Blender 3.6 alpha USD branch: Command failed: “Y:\Omniverse\Library\blender-3.6.0-usd.200.2\omni.blender.install.bat”
Traceback (most recent call last):
File “Y:\Omniverse\Library\blender-3.6.0-usd.200.2\Release\3.6\scripts\modules\addon_utils.py”, line 369, in enable
mod.register()
File “C:\Users\Veiss\AppData\Roaming\Blender Foundation\Blender\3.6\scripts\addons\curves_to_mesh_init_.py”, line 115, in register
add_to_key_map()
File “C:\Users\Veiss\AppData\Roaming\Blender Foundation\Blender\3.6\scripts\addons\curves_to_mesh_init_.py”, line 48, in add_to_key_map
km = kc.keymaps.new(name=‘3D View’, space_type=‘VIEW_3D’)
AttributeError: ‘NoneType’ object has no attribute ‘keymaps’
Traceback (most recent call last):
File “Y:\Omniverse\Library\blender-3.6.0-usd.200.2\Release\3.6\scripts\modules\addon_utils.py”, line 369, in enable
mod.register()
File “C:\Users\Veiss\AppData\Roaming\Blender Foundation\Blender\3.6\scripts\addons\blender_calculator.py”, line 534, in register
register_keymap_a8a0d47113()
File “C:\Users\Veiss\AppData\Roaming\Blender Foundation\Blender\3.6\scripts\addons\blender_calculator.py”, line 383, in register_keymap_a8a0d47113
km = kc.keymaps.new(name=“3D View”, space_type=“VIEW_3D”)
AttributeError: ‘NoneType’ object has no attribute ‘keymaps’
Traceback (most recent call last):
File “Y:\Omniverse\Library\blender-3.6.0-usd.200.2\Release\3.6\scripts\modules\addon_utils.py”, line 369, in enable
mod.register()
File “C:\Users\Veiss\AppData\Roaming\Blender Foundation\Blender\3.6\scripts\addons\QuickCurve_init_.py”, line 239, in register
keymaps = register_keymaps(keys)
File “C:\Users\Veiss\AppData\Roaming\Blender Foundation\Blender\3.6\scripts\addons\QuickCurve_init_.py”, line 197, in register_keymaps
km = kc.keymaps.new(name=keymap, space_type=space_type)Powered by Discourse, best viewed with JavaScript enabled"
68,material-opacity,"Hello,
is there a way to gradually adjust the opacity of a material? At the moment its either full or no opacity,
which ever Opacity Amount I am setting.Thanks, Matthias
Screenshot 2023-01-25 1158511942×1032 243 KB


Screenshot 2023-01-25 1159221968×1032 284 KB
Hi @helmreichThis post may help:Powered by Discourse, best viewed with JavaScript enabled"
69,blender-3-6-0-alpha-v200-0-now-on-launcher,"We are excited to announce that the latest release of Blender is on the Omniverse Launcher!  The biggest feature in this release is the first release of the Nucleus Connector Add-on, allowing Blender users in Omniverse to read and write directly to Nucleus servers.New Features:Bug Fixes:thanks, just wanted to confirm, if i am not mistaken, this is windows only.
is it possible to download this for linux ?? some graphicall builds area? thanks in adv.I have same question with @lalamax3d . I cannot find it on the Linux Omniverse Launcher. Is it available only on Nucleus ? Or Windows only?Powered by Discourse, best viewed with JavaScript enabled"
70,command-line-farm-submit,"Anyone know how to submit a create render from the command line?Powered by Discourse, best viewed with JavaScript enabled"
71,where-can-i-download-plugin-nvidia-omniverse-audio2face-livelink-could-not-find-it-in-unreal-plugin,"where can I download plugin plugin “NVIDIA Omniverse Audio2Face LiveLink” ? could not find it in Unreal pluginsame here, 2023.1 release page shows the plugin, but nowhere can find it…The UE LiveLink plugin is currently exclusive to ACE users.However, due to numerous similar requests, we are actively developing a hotfix to make it available for all users.Stay tuned and monitor the forums for updates.Hi,
We plan to hold a small visualization in the middle of August, for which this plugin would be perfect. Is it possible to know roughly how they are progressing with the development, or whether it will be public after Siggraph?
Thanks,
DanielWe’re planning to release a hotfix in a couple of days. Please keep an eye on the forums.This plugin is now available on 2023.1.1.
See for instructions: Audio2Face 2023.1.1 (Open Beta) Released - Apps / Audio2Face - NVIDIA Developer ForumsPowered by Discourse, best viewed with JavaScript enabled"
72,ptx-loading,"
image1920×761 56.6 KB
I only see in Pixar Storm renderer.
How can I fix this issue?The Omniverse viewport requires an RTX-capable GPU to function as it leverages the RT cores and Tensor Cores found on RTX hardware. The 1080ti is a GTX card that does not have this hardware capability. You would need a GPU branded as RTX which would be a 20 series or up but we recommend an RTX 3070 and up. Technical Requirements — Omniverse USD Composer documentation (nvidia.com)Powered by Discourse, best viewed with JavaScript enabled"
73,articulation-force-sensor-cant-get-contact-value,"Hello, I’m using articulation force sensor mentioned here. In my view, this sensor can read external force acted on itself by joint and gravity.I change end effort of franka like this: franka(remove gripper)->fix joint->small box as force sensor->fix joint->cylinder(to be contacted). However, when I change franka joint angle to make cylinder contact, the reading of force sensor is strange.The initial scene like this, the reading of free state is explainable. (almost, z is good, but y is positive, also strange)

image1432×818 201 KB
Then I change a joint target position while playing, but the reading of force sensor doesn’t change much. Even change joint’s target position to make stronger contact (stronger force should be appear because pd controller is used)

image1432×815 200 KB
It seems that articulation force sensor can’t get valid contact value? I use this sensor because it could easy read 6-axis force like ATI sensor, but maybe it still has some problems…In second picture, because contact, this z force will be positive at least, but it change very small.In addition, we use real experiment scene like this, get force 20~30N usually.By the way, the mass of peg is 0.0739kg. So the force value of free state maybe strage… (should be 0.7N)
image898×147 10.2 KB
I made more test to reveal what does force sensor do, the result is confused.

image1432×825 204 KB
I build this manipulator with upper mass is 1, corresponding gravity is 9.8N.
at the zero position, force sensor read -9.8N, good.Then I change a joint position to 90 deg, like this:

image1432×823 200 KB
The reading of force-sensor change to y force -2.7N, doesn’t correspond the mass of upper arm.Dose force sensor really read the external force acted on the body?Same question here.
I can not find detailed documentation for the force sensor.
Which forces are actually measured?I don’t know until now…We’ve been working with the physics team to understand this issue better and filed a bug, the physics team has committed to fixing this and providing better documentation for a future release.HI why not use the rigid body net forces? This will tell you the forces on the gripper and you can get both forces by getting info from both the rigid bodies on either side of the gripper.Here is an example of how to get the force tnsor for mutiple rl actors. The API documentation on this is not that clearhere is the gist…using the OmbiIsaacGymEnvs task framework e.g. anymal.py but no need anymore for the anymalview import which just serves to confuse and is nor helpful if you are using your own modelfrom omni.isaac.core.articulations import ArticulationView #if you need an articulation view (likely)
from omni.isaac.core.prims import RigidPrimViewdef set_up_scene(self, scene) → None:
…self._robots = ArticulationView(prim_paths_expr=“/World/envs/./biped"", name=“biped_view”)
…self._lfoot = RigidPrimView(prim_paths_expr=""/World/envs/./biped/LeftFoot”, name=“lfoot_view”, reset_xform_properties=False, track_contact_forces=True, prepare_contact_sensors=False)
…scene.add(self._robots)
…scene.add(self._lfoot)def update_buffers(self):
…self.contacts_left = (self._lfoot.get_net_contact_forces(clone=False)[:,2] > 0.00)
…print(self.contacts_left.shape)
… print(self.contacts_left)so this example takes the left foot prim from my biped.usd file for my 512 robot instances during training and detects if there is any contact force in the z plain (>0.00) that gives you a bool tensor to see if it is on the floor or not… in your case you probably want to know the actual force so you dont break the object the gripper is holding.Powered by Discourse, best viewed with JavaScript enabled"
74,please-help-viewport-are-no-demo-model-show-up-after-open-audio2face,"
audio2face-problem11810×764 180 KB

i was facing a big problem where there is no demo model show up when i open audio2face…it’s totally blank on viewportWelcome to the forum @symphonyplay888This looks like a graphics issue. A2F needs an RTX card.Would you be able to please send your latest Audio2Face log file which can be found here: C:\Users\<USERNAME>\.nvidia-omniverse\logs\Kit\Audio2Face.In the meantime, you can try updating your graphics card.my existing graphix card is Nvidia GeForce RTX 3050kit_20230527_180451.log (667 KB)Found this in your log file. It looks like your version of Windows is a little older than Audio2Face expects. So updating your Windows and reinstalling A2F should hopefully fix this.Powered by Discourse, best viewed with JavaScript enabled"
75,streaming-xr,"is it necessary to use wifi 6 for streaming to a vr headset or ipad (AR)Powered by Discourse, best viewed with JavaScript enabled"
76,maya-2024-mayaomniverse-connector,"Will the Maya connector be updated and support Maya 2024 anytime soon?Is there a way to get the current version installed with Maya 2024?Maya (Native) connector v207.0 will be released next and it will include Maya 2024 support.Hi @kecollins  what are the expected updates and improvements to the Native connector? Will MDL export be available again?Thanks!Powered by Discourse, best viewed with JavaScript enabled"
77,exporting-marbles-sample-to-fbx-has-missing-textures,"Exporting Marbles sample to FBX has missing textures. Most of them are missing, a few are exported.

image1684×925 116 KB
To reproduce:Also weird: when “Embed Textures” is selected, textures are BOTH put on disk in a “textures” folder AND embedded in the FBX. It should be either/or.Related to Faces are flipped in exported glTF file when exporting Marbles sample sceneGot to say it really feels like NVidia is trying to trap you in Omniverse once you have reached that part in the pipeline.The fact you can’t easily export assets like the Marbles ones is pretty poor given we’re all spending thousands on graphics cards every few years.It’s put me off working in the omniverse until more basic features such as this are fixed.For now I figured a workaround but it needs some basic dev skills:Still, it’ll always be better than Suckaturds metaverse. Now enjoy, fellow creators!Hello @herbst!  I’m not sure what could be happening here.  You should be able to export the materials out of Omniverse as well, but I will double check with the dev team to confirm and get further assistance!Let me test this and get back to you. Are you just testing our generate FBX export systems or is there a reason you are trying to get our demo scene out of Create ?@Richard3D I’m generally looking at interop to/from Omniverse. My main focus is import/export of glTF but given that this has lots of issues (see my other threads here) I had tested other formats as well. Not sure if that’s all Assimp under the hood and thus suffering from the same issues, but it looks more like Omniverse doesn’t properly convert textures in specific formats (e.g. precompressed ones like dds) to formats available in glTF/FBX.I usually test exporting/importing with the sample scenes that ship with any given software, to get a gut feel of whether it’s worth exploring more or interop simply hasn’t been a priority so far.I have begun looking into the exporting issues. Some of the demo files are a little older and may look great in Create, but do not meet our up to date export standards. That is not to say that export to FBX in general should not work. I will give you more information when it is available. And just from a workflow point of view, you are importing data in, and then exporting it out… out to what platform ? When you import data, is it just a naked model, and you are adding materials and lights and animation in Create ? We would love to hear more about your workflow.Happy to explain more - I also started an email thread with Jennifer, maybe she can add you to that? And I’m available in Discord in case that works for you for faster communication (herbst🌵#5425)Hi all, I am also facing the same issue and let me know if there is any solution for that.The Marbles scene is a really great example test scene for Create, but since it is an older scene, it does contain some mdl materials that do not export correctly through the fbx export engine. It is just unfortunately an issue with the age of the scene. The fbx exporter should work well for new modern scenes.Are you aware of a scene of comparable complexity available already in Omniverse that exports cleanly?Powered by Discourse, best viewed with JavaScript enabled"
78,script-for-convex-decomposition-collisions,"Hello everyone, I have a small problem and would like to ask if anyone knows something about how to solve this. I wrote a small script to add collisionAPI to an object. As I do not yet specify which approximation for the collision mesh I want to use (triangle mesh, convex hull, convex decomposition, …) it automatically applies the convex hull approximation. I would like to use convex decomposition, though.
Unfortunately, I cannot just set the attribute “pyhsics:approximation” to “convexDecomposition” because a Token is expected (I am unsure of how this mechanic works…). Does anyone know how to set these kinds of specific attributes correctly?Here is a cleaned and minimal working version of the script that loads a usd file, sets the CollisionAPI and then saves it:def add_collision(usd_path_load, usd_path_save):
usd_context = get_context()
stage = usd_context.open_stage(usd_path_load)
stage = get_context().get_stage()
rigidPrim = stage.GetPrimAtPath(“/World/model_normalized/mesh”)
UsdPhysics.CollisionAPI.Apply(rigidPrim)
# rigidPrim.GetAttribute(“physics:approximation”).Set(“convexDecomposition”)
stage.Export(usd_path_save)GetAttribute(): Universal Scene Description: UsdPrim Class ReferenceHi @christopher117  - The physics:approximation attribute expects a token, which is a specific type of string used in USD to represent a limited set of predefined values. In this case, the valid tokens for physics:approximation are none, convexHull, box, capsule, sphere, plane, and convexDecomposition.To set the physics:approximation attribute to convexDecomposition, you can use the UsdPhysics.Tokens class, which provides predefined tokens for all the valid values. Here’s how you can modify your script:In this script, UsdPhysics.Tokens.convexDecomposition is a token representing the string ""convexDecomposition"". The GetPhysicsApproximationAttr().Set() method is used to set the physics:approximation attribute of the CollisionAPI.Please note that the actual path to your mesh and the context setup might need to be adjusted according to your specific setup.Powered by Discourse, best viewed with JavaScript enabled"
79,object-falling-during-manipulation,"Hi, everyone,I created a demonstration where a robot manipulates objects and carries them to a neighboring table. However, I am experiencing an issue where the grasped objects fall as shown in the following video.When I parallelize the environment, only a specific environment seems to cause this problem. Could you please help me identify the possible causes?While most environments allow for successful grasping and placing of objects onto the neighboring table, in certain environments, the object consistently falls even after resetting the environment. This leads me to believe that there might be some configuration issue. Or is there an incorrect friction or other setting on the gripper side of the robot?The properties assigned to the objects are set as follows:And set PhysicsMaterial as follows:Thank you in advance for your help.Best regards,
smakolon385Have you tried simply increasing the friction in the material? I dont have too much of an intuition yet on what a good friction number should be but maybe you can find a suitable value fairly easily by just playing with it abit?Thank you for your response!
Yes, I tried adjusting various settings.Currently, it seems that having solver_velocity_iteration_count set to 1 is not ideal. Following your suggestion, I increased static_friction and dynamic_friction to 1.0, and set solver_position_iteration_count to 16 and solver_velocity_iteration_count to 16. With these changes, I was able to pick up the object without dropping it and place it on the adjacent table. However, when I set solver_position_iteration_count to 32 and solver_velocity_iteration_count to 16, it did not work as expected.
I’m not exactly sure why there is a difference, but for now, it is working correctly.Thank you for your help!Powered by Discourse, best viewed with JavaScript enabled"
80,audio2face-2023-1-open-beta-released,"Audio2Face 2023.1 comes packed with lots of new features, functionality, and workflow tutorials. We’ve expanded on our AI models further improving A2F multi-language support with the addition of a new Asian female AI model called Claire - providing users with more options for generating realistic facial animations.A2F_2023_1_ReleaseImage_041920×1080 93 KBIn addition, 2023.1 comes with Livelink and Avatar Streaming, providing the ability to stream blendshape animation from Audio2Face to other Applications like “Unreal Engines, Meta Human”. With blendshape support for tongues on both AI Models. Also, the Streaming audio player now engages Audio2Emotion providing developers and users the full performance capabilities of Audio2Face. In addition, we have improved the Rest API with more comprehensive controls. Plus, the ability to export “emotion keyframes” from Audio2Emotion for use in other applications.New AI model ClaireMeet Claire, our new multi-language Asian Female AI model.Introducing the new Ai Models PanelNew to this release is the AI models panel - providing more choice and a simplified workflow for getting started within the application. Choose which Ai Model you would like to use with which Audio Player. You can also quickly change between available trained Networks for each respective AI model where multiple networks are present.Ai Model Panel Tutorial.https://www.nvidia.com/en-us/on-demand/session/omniverse2020-om1745Live Link and Avatar StreamingA2F can now output a Live stream of blendshape animation data to connect to external applications that drive a characters facial animation performance. See the tutorial here for setting up the Streaming Workflow.Avatar Stream Tutorialhttps://www.nvidia.com/en-us/on-demand/session/omniverse2020-om1744/BlendShape Streaming to Metahuman – Part 1https://www.nvidia.com/en-us/on-demand/session/omniverse2020-om1747BlendShape Streaming to Metahuman – Part 2https://www.nvidia.com/en-us/on-demand/session/omniverse2020-om1748Improved Rest APIImproved BlendShapes with added Tongue supportPerformance improvements to Blendshape Solvers and We’ve added a tongue blendshape solution for both Mark and Claire models. You can find these assets in the A2F Samples Tab.Export Emotion keyframesProvided in the export tab is the ability to export emotion keyframes as .jsonA2F 2023.1 Sample filesAll Sample files are available in the examples Tab within the A2F application or via the content browser in your localhost at NVIDIA/Assets/Audio2Face/Samples_2023.1/Additional Tutorials.Updated Tutorial for Setting up Custom Character.https://www.nvidia.com/en-us/on-demand/session/omniverse2020-om1746Arkit Blendshape – part 1 - Ferret Model Overview.https://www.nvidia.com/en-us/on-demand/session/omniverse2020-om1749Arkit Blendshape – part 2 - Ferret Character Setup.https://www.nvidia.com/en-us/on-demand/session/omniverse2020-om1750/OM-96646 - AI model Template UI.OM- 94489 - Separate Jaw Transform retarget out of main A2F node into its own omnigraph node.OM-94221 - Add Mandarin Audio Samples to A2F.OM-93156 - Claire Template for Blendshape Solve.OM- 92260 - [A2F] Add NumPy dependency to mesh separate.OM-92178 - [A2F] Send and receive BlendShape through burst mode.OM-91784 - [A2F] REST API - Expose new emotion key export.OM-89282 - Expose Blendshape transfer from custom source to charTransfer UI.OM-88437 - Enable Samples browser in A2F.OM-86846 - [A2F] Add USDSKel Mesh BindTransform fix python script to toolbox.OM-86868 - [A2F] add support of receiving streaming audio into AvatarStream or Kit.OM-86264 - [A2F] Support Burst mode protocol on Receive Livelink node or related workflow.OM-79096 - Integrate Training for Claires tongue into A2F.OM-77176 - [A2F] Add livestream option for blendshapes.OM-67428 -Support for Long audio files. Improved handling when file is too long.OM-61494 - Streaming Audio2Emotion.OM-64642 - Ability to Export emotion keyframes from Audio2Face.OM-63926 - Integrate Mandarin data training and test into product.OM-97202 - Delete setup and delete correspondence makes error in fullface generic template.OM-94931 - [A2E] RestAPI errors with streaming player - GetEmotions.OM-94911 - Turning on/off A2E with streaming player breaks the keyframe functions.OM-94286 - [A2F] load preset on Float Array Tuner error - no such file or directory.OM-94051 - Fix jaw open poses that have issues.OM-92740 - Jaw Broken on target character when switching A2F model.OM-91680 - Cuda memory access error when working with keyframes from the old example files.OM-90670 - Target mesh deformation breaks after previewing Blendshape Transfer.OM-87886 - Old scene file with keyframe cannot be loaded properly (keyframe lost).OM-84827 - [A2F] Default scene not loading after creating key in post processing.OM-84055 - A2F Start up Freeze. The demo scene gets stuck at 90% after the jaw/tongue verification. It doesn’t lead to a crash.OM-83997 - A2E Better handling of model update in older scenes.OM-83825 - [A2F] Audio2Emotion with 105-SDK crashes in both Regular and Streaming Player.OM-97222 - Exporting USD Cache animation with Batch enabled stops at saving time on the first export.OM-79210 - User prompt of Auto Generate On Track Change keeps showing up on every scene save.OM-78655 - Unable to export geometry cache for a charTransferred tongue.OM-75641 - Some menu item buttons are behaving as toggles (menu → Audio2Face).OM-59989 - Blendshape solve toggle use button is not working correctly.OM-56982 - Auto Generate on Track ON stops the scene from saving.OM-56276 - A2E .TRT build taking very long time.OM-96966 - Remove “reset Layout” on demo Scene when opened.OM-95980 - Disable Shadow Denoiser in A2F.OM-93726 - Remove use of GetPoints if they can be bypassed.OM-93045 - Improve demo Scene Lighting.OM-93054 - Pickup lighting changes for Demo Scene.OM-87683 - Update to kit 105.OM-86273 - Replace mark teeth/gum with the new mesh.OM-85539 - Improve ARkit To Metahuman mapping.OM-78699 - Update A2E network to remove neutral slider.Thanks for this realese more than hoped for, solve many problems, cartoon faces and animals to ue fantastics.
just downloaded, christmas in july. thanks.Where can we locate the unreal audio2face plugin?Can you find it? I have the same problem.audio2face  plugin?
About Epic Games Unreal Engine 5.2 Omniverse Connector is from 12 july maybe its not in there maybe needs uppdate , its in the videos i looked at first. also looking for the ferret examples? it relased on a friday so maby some will adjust this when it opens on monday.The current availability of the Unreal plugin is limited to ACE users. However, we are collaborating with other teams to develop a beta version of the plugin that will be accessible to the public via the Launcher.The Ferret model is scheduled to be published shortly after Siggraph, approximately within a month. However, it’s important to note that this is just one of the available content options, and for testing purposes, users are free to utilize any model they prefer.We will keep the forum updated with any notifications regarding these.我的系统是centos stream8，用root权限能运行omniverse-launcher-linux.AppImage，但是打不开audio2face, 点启动之后没反应。
My system is CentOS Stream 8, and I can run omniverse-launcher-linux.AppImage with root privileges, but I cannot open Audio2Face. There’s no response after I click ‘start’.Can you please send your latest Audio2Face log file which is located inside C:\Users\<USERNAME>\.nvidia-omniverse\logs\Kit\Audio2Face on Windows and ~/.nvidia-omniverse/logs/Kit/Audio2Face on Linux?logs.zip (548.2 KB)
[root@centos8Stream logs]# ls
Cache   cache-setup  ‘cache-setup\cache-setup.log’  ‘Discovery Service’   launcher.log   omni.processlifetime.log  ‘System Monitor’
[root@centos8Stream logs]# pwd
/root/.nvidia-omniverse/logsI can’t find the Audio2FaceUnfortunately Audio2Face doesn’t support CentOS.
Here’s the requirements page: Audio2Face Overview — audio2face latest documentation (nvidia.com)Powered by Discourse, best viewed with JavaScript enabled"
81,could-not-find-ros-package-isaac-tutorials,"Hello all.
I’ve been working on the tutorials from the website for a while. But I cant find the ROS package named
isaac_tutorials. After several reviews, there is still no more founds.
image778×543 31 KB
Did I missed it in some place before? Or where should I have installed it ?
thk you.I just could not go further without it.
image940×72 7.98 KBI’ve found it in omniverse’s github : https://github.com/NVIDIA-Omniverse/IsaacSim-ros_workspaces.its solved!This topic was automatically closed 14 days after the last reply. New replies are no longer allowed.Powered by Discourse, best viewed with JavaScript enabled"
82,how-to-use-stage-recorder-extension,"I am learning stage recorder extension, following this only available tutorial about it:
https://docs.omniverse.nvidia.com/prod_extensions/prod_extensions/ext_animation_stage-recorder.htmlIn the above doc it is told that I can the access the Stage Recorder window by:Well, I do have not such a icon in my timeline in Composer:
image1730×150 13.1 KBEh, I do not have that either:
My extension is ON and set on autoload.
image1356×1508 249 KBSo I do have that menu item, but I cannot follow the tutorial on usage of this tool if I cannot open that Stage Recorder window…
I installed Machinima to see if I could access this Stage Recorder window from there, but no its not even listed in the extensions.image981×999 125 KBAm I the only one having this exiting new tool missing?Powered by Discourse, best viewed with JavaScript enabled"
83,using-audio2face-in-offline-mod,"Hello I would like to use using Audio2Face in offline mod, it worked in early versions, using windows 10 latest.
Its works if i have internet connection and shut internet connection down i can use Audio2Face if I have logged in while i was connected. would be great to start up Audio2Face on laptop without having a connection.Thanks  Audio2Face comming along great, suggestion have memory shortcuts in viewer when jumping between meshes when doing mesh fitting, specially when inside mouth, cant find a wireframe mode that works, have rtx 4090.Thanks for sharing this @ttpcWhile Launcher needs internet connection, you can run Audio2Face directly from the installation folder instead of using Launcher. It’s usually installed in a folder like this on Windows: C:\Users\{USER}\AppData\Local\ov\pkg\prod-audio2face-2022.2.1\audio2face.kit.batYou can turn on wireframe mode from RTX-Realtime -> Wireframe.
By default, the wireframe is too thick, so you might not see any change in the viewport. Reduce wireframe thickness from Render Settings -> Common -> Wireframe Settings -> Thickness
Screenshot_71920×747 89 KB
Thanksworks fine, cant change thickness value lower than 0.1, but i can change render meter per unit from default to big, medium or small then i see wireframe.  Can probably insert 0.01 thickness when I find the settings file responsible for  the render settings.running a2f offline works great on both win 10 and win 11(laptop).To make the thickness lower than 0.1, you can Ctrl-Click on the number and enter any value.This topic was automatically closed 14 days after the last reply. New replies are no longer allowed.Powered by Discourse, best viewed with JavaScript enabled"
84,the-script-of-randomizing-appearance-can-not-be-run,"Hi there,
I follow the tutorial below but get the crash error as attached
https://docs.omniverse.nvidia.com/extensions/latest/ext_replicator/shrubs_and_worker_example.htmlIs any idea to solve it?2023-07-18_16-17_11424×918 167 KB
2023-07-18_16-171920×645 233 KBThanks and regards,
DarienPowered by Discourse, best viewed with JavaScript enabled"
85,how-to-add-kinematics-and-dynamics-to-the-existing-robot-body,"I have learned how to import and build a robot model to add physical properties, joint connections and drives through official documents. How should kinematics and dynamics be added further。Are there any relevant tutorials or cases for reference?Kinematics: You can define the kinematics of your robot by setting up the joint connections in your robot model. This includes defining the type of joint (revolute, prismatic, etc.), the axis of rotation or translation, and the parent-child link relationships. This is typically done in the URDF or SDF file of your robot model.Dynamics: You can define the dynamics of your robot by setting up the physical properties of the links in your robot model. This includes defining the mass, inertia, and center of mass of each link. This is also typically done in the URDF or SDF file of your robot model.Drives: You can define the drives of your robot by setting up the joint controllers in your robot model. This includes defining the type of controller (position, velocity, etc.), the control gains, and the control limits. This is typically done in the configuration file of your robot model.You can find some basic information here:
https://docs.omniverse.nvidia.com/prod_extensions/prod_extensions/ext_physics.html#overviewYou can also refer to the URDF and SDF specifications for more information on how to define the kinematics and dynamics of your robot model:Powered by Discourse, best viewed with JavaScript enabled"
86,compiling-omniverse-compliant-ros2-custom-messages,"Hi there,In the project I’m working on, we need to work with custom ros2 messages in Omniverse.We are using ros2 humble, however compiling our messages using a docker container with a vanilla humble image and importing those binaries into Omniverse did not work.After some research, we did something similar to what is described here, where we created a container with ros2 humble using Python 3.7.This seems to do the trick, but it looks a bit hacky to me and not very future proof.Is there an easier way to achieve this and streamline the process of working with custom messages in Omniverse? If not, are there any plans to provide this support in the future?Hi @jominga  - We are aware of it and planning to provide the support in the future Isaac Sim releases.Powered by Discourse, best viewed with JavaScript enabled"
87,is-the-documentation-for-isaac-sim-extensions-api-complete,"Hello.I am currently working on creating objects via python scripting on Isaac Sim and I feel like the documentation for Isaac Sim Extensions API (python) is incomplete.
A lot of the documentation for classes, functions, and parameters are left black or omitted.https://docs.omniverse.nvidia.com/py/isaacsim/https://docs.omniverse.nvidia.com/py/isaacsim/source/extensions/omni.isaac.core/docs/index.htmlfor example
omni.isaac.core719×738 131 KB
the parameters “radius”, “height” are described as arrays but they should be integers/floats.
The parameter for “orientation” does not specify if they are quaternion or eulers.If the documentation was more specific, it would make it much easier for everyone to search/google the documents and make use of these APIs.Can you share when we can expect an update on these documents?Thanks!Thank you for the feedback @jaeyeun . I will run by this feedback to our developer.Powered by Discourse, best viewed with JavaScript enabled"
88,pip-not-able-to-download-the-azureopenai-for-langchain-llms,"Hi,I am trying to integrate the langchain llms hosted on Azure in a custom extension for USD Composer. But somehow the AzureOpenAI in langchain is not getting downloaded with the PIP.I tried all the possible ways to download it but somehow its not downloading the full/correct version of LangChain[python.pipapi]
requirements = [“openai”, “langchain[llms]”]
#requirements = [“openai”, “langchain[all]”]
#requirements = [“openai”, “langchain”]
#requirements = [“openai”, “langchain==0.0.228”]
use_online_index = true2023-07-10 07:52:43  [Error] [omni.ext.python] ImportError: cannot import name 'AzureOpenAI' from 'langchain.llms' (C:\Users\pranav.buradkar\AppData\Local\ov\data\Kit\Create.Next\2022.3\pip3-envs\default\langchain\llms\__init__.py)Hi @Pranav.Buradkar. Let me check with the dev team on this.Hi @Pranav.Buradkar. While we’re waiting to hear back from the dev team, could you try on the latest version of USD Composer 2023? That version has Python 3.10 instead of 3.7. Isaac Sim would be upgrading to that in the near future too.Sure, I will test that.What I would recommend is installing pip packages you need outside of the app process, using the python we include or system python of the same version. When doing pip install you can pass --target=[path] to install into a specific folder. This is what omni.kit.pipapi extension is doing if you look inside.Then you can add that path either explicitly to sys.path or just make it one of the extensions. This is how extensions like omni.kit.pip_archive are made, we install at build time and prepackage all packages in the extensions.Another thing to know is that pip installation errors go to stdout and if you run from the Launcher, you won’t see them (to be fixed in next versions). You can run an app from terminal, using sh/bat file (like omnu.create.sh) then you would probably have more information on what failed. Run with --clear-data to make sure it tries to install again.@anovoselovI checked with 2022 and 2023 versions of Create and still facing the same issue. I tried with installing the LangChain in external as well as in internal directory and provided those as explicit import paths  but facing the same issue.I am able to import langchain on command line with the provided versions of python in omniverse, but its not working with script editor or as an extension.LOG -EDIT-I have changed the sys.append path to Windows style, I think its able to find the module but still cant locate the langchain.llmsFull Log -That probably happens because typing_extensions that we include is of an older version and it is imported first.
You probably can work it around if you del sys.modules[""typing_extensions""] before importing langchain. That will triggers re-import and If the right version is higher on a sys.path it should be picked up.Still the same issue persists.Powered by Discourse, best viewed with JavaScript enabled"
89,can-not-modify-angular-damping-parameter-by-python-command,"Hi!I would like to change the “angular damping” rigid body parameter from python using kit.commands. However the code doesn’t work and there is no error output.Here is the command I am running:I was able to modify the rigid body properties using the PhysxRigidBodyAPI.This worked for me.Yes, just changing the attribute does not help, the API must be applied. These attributes belong to that API, those are not standalone attributes.This topic was automatically closed 14 days after the last reply. New replies are no longer allowed.Powered by Discourse, best viewed with JavaScript enabled"
90,simulation-only-running-once,"Hey there,I am new to running a simulation with Isaac Sim, so I got the following question.I have created a script to randomize an object in Isaac Sim. Images of this object are stored using 2 cameras to generate synthetic data.
For this I have found a minimal example, which is explained in the following link:Contribute to gigwegbe/synthetic_data_with_nvidia_replicator_and_edge_impulse development by creating an account on GitHub.When I run the script the first time everything happens as defined in the script.When I run the script a second time, the simulation and the saved data do not show the loaded object anymore.
My current workaround is to close and restart Isaac Sim after each run of the script.Adjusting the parameters is therefore only possible in a cumbersome way.Is there a solution for this problem? How can I run the script several times and adjust the parameters in the script without closing and restarting Isaac Sim?Thanks in advance,
HugoI am able to reproduce the issue by running the objects_position_random.py script and have logged the issue.In the meantime, I’d suggest trying to open a new Stage (File > New) instead of closing and restarting Isaac Sim. I was able to do that and run the script getting the objects output each time.Powered by Discourse, best viewed with JavaScript enabled"
91,linux-vs-windows-load-order-issue-kit104-no-module-named-omni-git-extension,"I have been interested in developing a cpu driven cicd pipeline for custom extensions. I have seen significant progress and am able to run pipelines both locally (Windows), and in CICD (linux). The linux pipeline is a modification of the kit-104 container ( test scripts, kit file, rootless folder structure, etc.) . This sufficed until I needed to add other extensions as a dependency.The odd part is it windows works… for the most part.
By adding the following lines into my .kit file[settings.app.exts]
folders.“++” = [
“git://github.com/NVIDIA-Omniverse/kit-extension-sample-ui-scene?branch=main&dir=exts”,
]I am able to, 90% of the time pull down any git dependency (slightly modified).…
2023-06-21 15:46:50 [280ms] [Info] [omni.ext.plugin] [ext: omni.kit.extpath.git-0.0.0] applying settings
2023-06-21 15:46:50 [417ms] [Info] [carb] Plugin carb.scripting-python.plugin is already a dependency of omni.kit.app.plugin; not changing unload order
2023-06-21 15:46:50 [417ms] [Info] [omni.ext._impl._internal] Adding c:.…\MY_AWESOME_EXTENSION\app\kit\extscore\omni.kit.extpath.git to sys.path
2023-06-21 15:46:50 [417ms] [Info] [omni.ext.plugin] About to startup: [ext: omni.kit.extpath.git-0.0.0] (order: -1000) Triggered by API/CLI/Config. (path: c:/…/MY_AWESOME_EXTENSION/app/kit/extscore/omni.kit.extpath.git)
2023-06-21 15:46:50 [417ms] [Info] [omni.kit.app._impl] python GC: gc.disable()
2023-06-21 15:46:50 [417ms] [Info] [omni.kit.app.plugin] [0.517s] [ext: omni.kit.extpath.git-0.0.0] startup
2023-06-21 15:46:50 [425ms] [Info] [omni.ext._impl._internal] Searching for classes derived from ‘omni.ext.IExt’ in ‘omni.kit.extpath.git’ (‘c:\users.…\MY_AWESOME_EXTENSION\app\kit\extscore\omni.kit.extpath.git’)
2023-06-21 15:46:50 [426ms] [Info] [omni.ext._impl._internal] Found class ExtPathGitExt derived from omni.ext.IExt in module: ‘omni.kit.extpath.git’ in ‘c:…\MY_AWESOME_EXTENSION\app\kit\extscore\omni.kit.extpath.git’. Calling on_startup.
2023-06-21 15:46:50 [426ms] [Info] [omni.kit.extpath.git.utils] [omni.kit.extpath.git]: run process: [‘git’, ‘–version’]
2023-06-21 15:46:50 [498ms] [Info] [omni.ext.plugin] Registered protocol provider for scheme: git
2023-06-21 15:46:50 [498ms] [Info] [omni.ext.plugin] Registered protocol provider for scheme: git+https
2023-06-21 15:46:50 [498ms] [Info] [omni.ext.plugin] Registered protocol provider for scheme: git+http
2023-06-21 15:46:50 [498ms] [Info] [omni.ext.plugin] Registered protocol provider for scheme: git+ssh
2023-06-21 15:46:50 [498ms] [Info] [omni.ext.plugin] [ext: omni.kit.extpath.git-0.0.0] started, startup time: 80 (ms)
2023-06-21 15:46:50 [498ms] [Info] [omni.kit.extpath.git.utils] [omni.kit.extpath.git]: ExtPathGitExt::_on_add_path: git:/github.com/NVIDIA-Omniverse/kit-extension-sample-ui-scene?branch=main&dir=exts
2023-06-21 15:46:50 [499ms] [Info] [omni.kit.app._impl] [py stdout]: hello git:/github.com/NVIDIA-Omniverse/kit-extension-sample-ui-scene?branch=main&dir=exts, C:/Users/…/AppData/Local/ov/cache/gitexts
…However, when making a new modification to the …/app/kit/extscore/omni.kit.extpath.git file, I am able to manually trigger an error similarly to what I see every run on the linux side.…
2023-06-22 18:36:34 [91ms] [Info] [omni.ext.plugin] [ext: omni.usd.libs-1.0.0] registered (path: /opt/nvidia/omniverse/kit-sdk-launcher/extscore/omni.usd.libs)
2023-06-22 18:36:34 [91ms] [Info] [omni.ext.plugin] Extensions dependency solver solution:
[ext: omni.kit.extpath.git-0.0.0] no deps2023-06-22 18:36:34 [91ms] [Info] [omni.ext.plugin] [ext: omni.kit.extpath.git-0.0.0] applying settings
2023-06-22 18:36:34 [91ms] [Info] [carb] Initializing plugin: carb.scripting-python.plugin (interfaces: [carb::scripting::IScripting v1.0]) (impl: carb.scripting-python.plugin)
2023-06-22 18:36:34 [92ms] [Info] [carb.scripting-python.plugin] Attempting to initialize Python interpreter. Failure of doing it will lead to the termination of the application.
2023-06-22 18:36:34 [128ms] [Info] [carb.scripting-python.plugin] Python interpreter was successfully initialized.
2023-06-22 18:36:34 [128ms] [Error] [carb.scripting-python.plugin] ModuleNotFoundError: No module named ‘omni’At:
PythonExtension.cpp::prestartup()(1): 2023-06-22 18:36:34 [128ms] [Error] [omni.ext.plugin] [ext: omni.kit.extpath.git-0.0.0] Failed to process python module extension in ‘/opt/nvidia/omniverse/kit-sdk-launcher/extscore/omni.kit.extpath.git/.’.
2023-06-22 18:36:34 [128ms] [Info] [omni.ext.plugin] About to startup: [ext: omni.kit.extpath.git-0.0.0] (order: -1000) Triggered by API/CLI/Config. (path: /opt/nvidia/omniverse/kit-sdk-launcher/extscore/omni.kit.extpath.git)
2023-06-22 18:36:34 [128ms] [Info] [omni.kit.app.plugin] [0.154s] [ext: omni.kit.extpath.git-0.0.0] startup
2023-06-22 18:36:34 [128ms] [Error] [carb.scripting-python.plugin] ModuleNotFoundError: No module named ‘omni’At:
PythonExtension.cpp::shutdown()(1): 2023-06-22 18:36:34 [128ms] [Error] [omni.ext.plugin] Failed while calling shutdown_extensions function: /opt/nvidia/omniverse/kit-sdk-launcher/extscore/omni.kit.extpath.git
2023-06-22 18:36:34 [128ms] [Info] [omni.ext.plugin] [ext: omni.kit.extpath.git-0.0.0] started, startup time: 0 (ms)
2023-06-22 18:36:34 [128ms] [Error] [omni.ext.plugin] Failed to add extension path: ‘git:/github.com/NVIDIA-Omniverse/kit-extension-sample-ui-scene?branch=main&dir=exts’. Local path is empty or scheme is not supported.
2023-06-22 18:36:34 [363ms] [Info] [omni.kit.app._impl] python GC: gc.disable()
2023-06-22 18:36:34 [363ms] [Info] [carb] Plugin carb.scripting-python.plugin is already a dependency of omni.kit.app.plugin; not changing unload order
…I added some logging to the extension itself, but I believe the issue to be a unix specific race condition. I have tried playing with the load order and adding dependencies to no avail.Additionally, I have been looking at task manager for performance clues and on this fail it seemed to touch 100% CPU usage, memory looks fine at about 50%)Does anyone have insights? Perhaps the git code has an undocumented dependency? Let me know if i can provide additional information.Hi @michael.s1.ellis. Thanks for bringing this to our attention. Checking with the dev team.Yes please let me know if there is any development. It may be worth noting that I have only tried the kit 104 app as a back end (for linux, it was the only docker container other than isaac-sim available) whereas the local tests on windows are linked to code. Seeing as the underline kit version is the same i do not know why this would matter.Is there any update on this? Perhaps it is a known issue being worked into kit 105?Powered by Discourse, best viewed with JavaScript enabled"
92,export-usd-model-from-usd-composer-isaac-sim,"Dear community,
I have a question how can I export a model (robot) to a USD file without additional hierarchy levels.
What I do:And in step 4 I get a problem, if I try to reopen the model it gets an extra hierarchy level.
Let’s say, before exporting it was like this:
After exporting (right click on “my_robot”, save) to the file husky_v2_1.usd the model opens as:
And now if I open this model with python API - no methods and classes will work with it, because they can’t “look” deeper and look for joints and other primitives only at first hierarchy level and are lost if I need to do 2 steps.Maybe my question seems obvious, but I have not found any mention how I can export the model the same way I imported it (without husky_v2 as intermediary).Hi @BezuglyjVitaly. What’s your process for exporting the model? Why don’t you just do a “Save” on the Stage instead?What’s your process for exporting the model? Why don’t you just do a “Save” on the Stage instead?I just choose a model, right click and choose “Save Selected”.
As I mentioned, I want to import this model later via the Python API. And now I’m importing into the simulator just to edit the model and therefore it seems strange to save the whole scene - as it will save it together with the lighting, World primitive, etc. Whereas I only need the robot. So I do as I described.Sorry, I just checked the scene save mechanism and it actually does not save World and lighting as I wrote above, but it still imports my model with the intermediate XForm primitiveGot it. I don’t have a good reason for why it should add an extra prim in that case. I moved this over to the Composer forum where someone will hopefully be able to speak more to that workflow.Ok, I’ll wait for :)Thank you!I figured it out. (By opening the source code.)The point is that the function “Save Selected” allows you to save the selected set of primitives in a separate file, and, accordingly, creates an intermediate primitive in which saves the selected ones. (So that when you import them, they are added as a single asset, not a disjointed “something”)And it does not check how many primitives are selected.  I adjusted the code, added such a check and made an alternative version of the save instructions in case only one primitive is saved.For those who want to repeat this trick - the handler function is in the file isaac_sim-2022.2.1/kit/exts/omni.kit.widget.stage/omni/kit/widget/stage/export_utils.py line 42. My version of the file is attached.Note: At the moment my patch can’t handle “external” dependencies relative to exported primitive - I just skipped this part of code (for example, if you use texture which is in your project, but it’s not part of exported primitive - it will just ignore it), but for my purposes it’s enough.
export_utils.py (12.2 KB)This topic was automatically closed 14 days after the last reply. New replies are no longer allowed.Powered by Discourse, best viewed with JavaScript enabled"
93,view-is-failing-to-launch-on-a-linux,"I just installed the latest AppImage on my linux machine. I have a 3060 Card and the proprietary drivers. I use plenty of other nVidia tech on this machine, so I know the card/drivers are setup well.Trying ton install Omniverse here ( I use it on other machiens) and I primarily need View. When I updated the launcher, and installed View, I receive no errors.When I attempt to launch USD View from the Launcher it just hangs until the spinning goes away, and the ‘Launch’ comes back.If I attempt to directly launch View from the command line I get the following error[Error] [omni.kit.app.plugin] Failed to load core plugin at: /home/loki/.local/share/ov/pkg/deps/51302263f5b89a4c77847c8d4498d701/kernel/plugins/libcarb.scripting-python.plugin.so 
[Error] [carb] [Plugin: omni.ext.plugin] Dependency: [carb::scripting::IScripting v1.0] failed to be resolved. Are there some pre-reqs I may not have installed? Is there a way to dig in deeper on this issue and see what the root cause is?Powered by Discourse, best viewed with JavaScript enabled"
94,how-to-make-the-object-static-i-mean-it-shouldn-t-move-at-all-even-if-it-gets-hit,"Hi,
I’m using isaac sim to build an environment for AMR to simulate driving where there are many obstacles (the  obstacles  immovable)，but i  didn’t find a way to make the object static, do anyone  has any  suggestions?thank you!!Hi, maybe you could try adding to the object only a collider components without a rigid body component.
Let me know if it helpsthanks for your reply .After I “lock” the object I want to fix, I can achieve the desired result.Powered by Discourse, best viewed with JavaScript enabled"
95,cant-export-blendshapes,"hi guys.trying to export blendshapes with new beta release, as older releases can´t start at all anymore, for some reason.geting this errors

image1088×175 26.5 KB
what do i need to do?thanks!Hello @sinisa!   Let me know if this is correct.  It sounds like you installed the new version of Audio2Face and now the older versions are no longer working.  You are also not able to export any blendshapes.So I have a couple of suggestions for when I run into this issue.Let me know if this cleans up the problem!  If not, send me a copy of your logs from here: C:\Users\<USERNAME>\.nvidia-omniverse\logs\Kit\Audio2Face and include any new details that may help us troubleshoot the issue!Good morning,
I too have a very similar problem with the latest beta version of Audio2Face (2022.2.1).
I attach the errors:
Error1313×352 84.3 KB
Can anyone help me?
ThanksWelcome to the forum @design31
Can you try Wendy’s post  and let us know if it doesn’t work?Yes,
i have already tried but doesn’t work.
could it be a bug?Are you able to send your scene file? If not, are you able to send your A2F log file which can be found here: C:\Users\<USERNAME>\.nvidia-omniverse\logs\Kit\Audio2Face?kit_20230614_173022.log (733.1 KB)From the log, it seems the blendShape solver node doesn’t have the right inputs.
Can you redo the setup from scratch and see if it keeps erroring?
Or are you able to share your scene file? You’d need to save it as Flattened to make sure everything is saved in one file.VISO_LEGNO_SCULPT_03_test.usd (22.2 MB)The tool is supposed to figures out the blendShape weights on copy of the mesh character which has already blendShapes on it. But I couldn’t find a mesh with blendShapes in your scene.As a test, I exported one of your meshes into Maya, added a few blendShapes and exported the mesh with blendShapes back into A2F using Maya Legacy Connector (which can be installed using Launcher). Then did the blendShape conversion and it worked as expected.Watching these videos might help have a better understanding of the workflow:This topic was automatically closed 14 days after the last reply. New replies are no longer allowed.Powered by Discourse, best viewed with JavaScript enabled"
96,get-camera-image-from-gym-in-headless-mode,"I’m attempting to obtain the camera image from a robot-mounted camera that I have imported into my project in USD format. When I run the program in normal mode, I am able to successfully retrieve the camera feed. However, when running in headless mode, I encounter the error message below.It’s worth noting that this issue pertains to the Omni Isaac Gym environments, which are a new gym integrated into Isaac Sim. To provide context, I have placed my robot inside the cartpole example. @toni.sm do you have any idea what could be causing the issue? Or what could be a way around it.The code in set_up_sceneI feel this is the important part of the error, but all error is bellow to.Hi @firaThe SimulationApp instance, in headless, use a simplified experience configuration (apps/omni.isaac.sim.python.gym.headless.kit) that doesn’t include several viewport/rendering-related extensions.Although I don’t get the same error as you, I had to perform the following steps to be able to launch a headless simulation with a camera (like your code snippet) and without errorscomment the lines 33-34 in exts/omni.isaac.gym/omni/isaac/gym/vec_env/vec_env_base.py to avoid loading omni.isaac.sim.python.gym.headless.kit experience in headless moderun the script with the following arguments:Note: Even with that, I am not sure everything with works. I think we need to wait for the integration of camera tensor API in futures releases to do massive parallel RL with visual information successfullyThanks @toni.sm ! Yes, the headless app that the gym environments use does not currently support headless rendering. We will be modifying the logic for this slightly in the upcoming release and adding support for headless rendering by specifying the enable_cameras parameter in the task config.@kellyg has this been resolved in 2022.2.1 version?Yes. You can try it in the Isaac Sim 2022.2.1 and let us know if you have any other questions.Hi, @rthakerI tried but I cannot get any images with headless mode.
I wrote two topics but cannot get any answers.Should I had to try with another method for getting images with headless mode?Powered by Discourse, best viewed with JavaScript enabled"
97,could-not-load-the-qt-platform-plugin-xcb-in-cv2-qt-plugins-even-though-it-was-found,"I got this error when I was debugging my application:qt.qpa.plugin: Could not load the Qt platform plugin “xcb” in “/home/user/.local/share/ov/pkg/isaac_sim-2022.2.1/kit/python/lib/python3.7/site-packages/cv2/qt/plugins” even though it was found. This application failed to start because no Qt platform plugin could be initialized. Reinstalling the application may fix this problem.There are no error when I run the application normally.  I am not using Jetson.
Please find attached the error log. Let me know the solution; I do not want to re-install the application as mentioned in the error log.Qt platform plugin .log (20.7 KB)Hi @kartik.sachdev1 - Can you follow these steps and confirm if the issue resolves or not?Check if the “libxcb” library installed on your system, if not then you can install it.Set the QT_QPA_PLATFORM_PLUGIN_PATH environment variable to point to the correct location of the Qt platform plugins.If the issue still persists, then you can reinstall the Qt libraries and plugins.Powered by Discourse, best viewed with JavaScript enabled"
98,isaac-sim-2022-2-1-doesnt-start,"Here is my hardware information:
Intel(R) Xeon(R) Gold 6242R CPU
NVIDIA Quadro RTX 8000 x5
Mem 755 G
Swp 7.63G
Ubuntu 22.04.1 LTS
Linux version 5.15.0-43-genericHere is my log.
kit_20230704_231121.log (649.0 KB)When i try to run Isaac sim from terminal by using isaac-sim.sh -v, some errors happen
image1622×668 118 KBHi @zhili0818  - Based on your hardware information, your system should be able to support Vulkan 1.1 as the NVIDIA Quadro RTX 8000 supports Vulkan up to version 1.2.Here are some steps you can take to resolve this issue:Powered by Discourse, best viewed with JavaScript enabled"
99,isaac-sim-bookmarks-do-not-persist-after-container-restart,"The command I use to start the container is the following:I would assume that somehow the information regarding bookmarks is stored in these folders for persistence, but the bookmarks are deleted after the container restart. How can I make them persistent?Hi @ozhanozenThe --rm flag automatically removes the container and its associated file system after it exits.
If you want to keep the changes you made, you must remove the --rm flag.Visit Clean up (–rm) for more detailsThanks for the answer @toni.sm.Is there a way to keep the changes over while keeping the --rm flag? Similar to how the cache is stored.Hi @ozhanozenNot that I know of
It would be necessary to explore the docker run command options to identify that case.Can you provide more detail on what you want to achieve with your setup?Ok, @toni.sm, thank you for the answer.Nothing specific; we restart the PC, which contains the containers, regularly, so I was losing the bookmarks. I thought this wouldn’t be the expected behaviour, but it is not a big problem.Hi @ozhanozen. The Bookmarks list is located in this file: /root/.nvidia-omniverse/omniverse.tomlTry running the container with that folder mounted. The second run will have it cached.docker run --name isaac-sim --entrypoint bash -it --gpus all -e “ACCEPT_EULA=Y” --rm --network=host 
-v ~/docker/isaac-sim/cache/kit:/isaac-sim/kit/cache/Kit:rw 
-v ~/docker/isaac-sim/cache/ov:/root/.cache/ov:rw 
-v ~/docker/isaac-sim/cache/pip:/root/.cache/pip:rw 
-v ~/docker/isaac-sim/cache/glcache:/root/.cache/nvidia/GLCache:rw 
-v ~/docker/isaac-sim/cache/computecache:/root/.nv/ComputeCache:rw 
-v ~/docker/isaac-sim/logs:/root/.nvidia-omniverse/logs:rw 
-v ~/docker/isaac-sim/nv/config:/root/.nvidia-omniverse/config:rw 
-v ~/docker/isaac-sim/data:/root/.local/share/ov/data:rw 
-v ~/docker/isaac-sim/documents:/root/Documents:rw 
nvcr.io/nvidia/isaac-sim:2022.2.1The Bookmarks list is located in this file: /root/.nvidia-omniverse/omniverse.tomlTry running the container with that folder mounted. The second run will have it cached.Thanks. This solved the problem!This topic was automatically closed 14 days after the last reply. New replies are no longer allowed.Powered by Discourse, best viewed with JavaScript enabled"
100,does-rtxgi-cutted-from-unreal-engine-5-2-nvidia-rtx-branch,"I cannot find RTXGI plugin in built RTX branch of UE 5.2. Is it have been cutted from this release?(Powered by Discourse, best viewed with JavaScript enabled"
101,the-option-import-face-animation-is-not-present-in-ue5-ue-connectors-are-installed,"I hope someone can help me figure out why this is happening. I have been watching tutorials on YouTube about how to import Audio2Face facial animations into Unreal Engine 5. In those tutorials, they show the option “Import Facial Animation”, when they right click on the content browser. I don’t have that option in my UE5 project, even though I have installed all the UE connectors from Nvidia Omniverse. Please, look at the following screenshots. This one was taken from the YouTube tutorial:Import Face Animation option1268×714 82.9 KBThis one is what I see from my own Unreal Engine project:Unreal Engine no Import Face Animation option1909×1030 131 KBAnd here you can see the connectors page:Nvidia Omniverse Audio2Face - Unreal Engine connectors1901×647 57.8 KBAny idea how to fix this issue? Any help would be really appreciated. Thanks a lot in advance!!!Hi rominachirre,
We combined A2F animation and USD animation into Import USD animation, you can select the animation type in the importer dialog of Connect 202.1.
For your case, you should select Audio2Face MetaHuman CurvesThanksThank you so much for clarifying this for me, JulianCao! I was able to animate the head of my metahuman thanks to your reply to my post! You are amazing! Thanks a million for your precious help!Just a note, I love to answer questions with a documentation link.  Information about importing Audio2Face facial animation is pretty well documented here: Importing Audio2Face Facial Animation.Thanks @rominachirre for using the connector!  I hope you’re able to make beautiful things.Oh, thank YOU so much for sharing this tutorial with me, LouRohan!!! I will definitely read it. I’m eager to learn!!! Very appreciated!!Powered by Discourse, best viewed with JavaScript enabled"
102,modulenotfound-importing-python-kit-modules-into-connectorsample,"I would like to write some standalone utilities that use omniverse functionality from things like kit and issac, things that are not explicitly mentioned in the connect sample documentation.
(Connect Sample — connect latest documentation)It doesn’t seem like it should be an issue as there is a facility for installing most any other module using pip - which I have used successfuly on things like zeromq.
(Overview — omni.kit.pipapi 105.2 documentation)However when I add a reference to something like “omni.kit.commands” to use functionality that I developed in Code, I get a python “ModuleNotFound” import error. I have tried augmented the syspath with everthing that Code was using, but that didn’t fix the problem, there were DLLS that could not get found. Doing something similar with the Windows path didn’t fix that.So is it simply not possible? And why exactly? Seems like it would be a very useful thing.Repo:image1458×443 32 KBPowered by Discourse, best viewed with JavaScript enabled"
103,is-machinina-the-right-tool-to-mock-up-a-stage-play,"Hello.  I am a playwright with a few scripts that have not seen significant production.  I’ve searched for quite some time for a tool that would allow me to take a play script and create a 3D rendered production of my scripts either as a promotional/marketing tool or as a full on production for release to a streaming platform such as YouTube.This search started well before the current state of evolution in desktop 3D rendering tools infused with AI technology.  It feels like omniverse machinina might be approaching the sweet spot in terms of its power to simplicity ratio to allow me to do what I envision.However, before downloading this tool and undertaking an arduous learning curve only to discover many hours into it that I’m not using the right tool, I thought I would run the question by this community.The particular script that I have in mind is mostly drama with little action.  It has a ballroom dancing scene between two characters, a brief fight scene, and a couple of kissing scenes.I have fundamental video editing skills, solid coding skills but very little experience in 3D editing/rendering.  My hope is that this tool has enough support in terms of assets, that between the tool itself and whatever motion captured animations I can find in supported 3rd party assets (that don’t break the bank) I could leverage machinina to bring my vision to life through self education on the tool using existing tutorials.I know machinina is mostly geared toward cut scenes for video games and such, but might it be used for more traditional story telling?Thoughts?And if you’ve made it this far, thank you very much for reading!TL/DR: You can use Machinima, but it’s likely not the only program you will need to use to make it happen.The Machinima app can be used for a variety of things. It’s more or less a sandbox where you can set up 3D scenes with environments, characters, simulation, camera shots, and animate every piece of content to your hearts desire if you have the hardware to make it all happen. Despite the term ‘Machinima’ stemming from its frequent usage in the gaming industry, it’s not necessarily a limiting factor should you choose to leverage it for making a short.That said, you may need other tools in addition to OV to accomplish such endeavor. For example, if you need to make a digital set for your scene and find yourself needing custom props, you may find there are other programs that are more capable and better suited to create digital contents in. But that’s where OpenUSD in OV comes in to help bridge the gap between different DCC (digital content creation) packages. You have the luxury of using AI to assist with a specific needs, such as facial animation using Audio2Face. Such apps would allow you to enrich your character animation and dialog dynamic in your play.Sounds like your goal is going to be heavily driven by character interaction and dynamic, so you can look into programs such as Reallusion. There are hardware that will enable you to act out your own play using a premade character with motion capture technology as well; but, it’ll be one extra toolset you’ll need to learn in order to achieve your goal. There are community users on Discord that have had experiences doing shorts and are keen on sharing their experiences, so perhaps you can also pick their brains to get a better idea how to approach this effort before committing to anything.my 2cPowered by Discourse, best viewed with JavaScript enabled"
104,omniverse-launcher-news-doesnt-open-to-a-browser,"All NVidia employees must have great vision. I am used to Code and Composer not having a font selector, but even the news stories open to an internal app of some kind.These pages look like HTML pages, but I don’t see the URL that I can open the same page with a browser, so I can use 150% zoom I need to see.Is there a way to open these news stories in a browser, or is there a way to change the zoom  level?image1920×1032 187 KBI didn’t see a ‘Launcher’ forum, so I am posting here.Thanks@DataJuggler Launcher forum can be found here for future reference - Launcher - NVIDIA Developer Forumsand in those “browser” popups, supposedly you can still use combination of Ctrl and + or - to change zoom level. That said, i noticed “Ctrl + -” (zoom out) seems to be the one working (at least on my end); and for zoom in, i had to go to View > Zoom In instead, which can be a bit cumbersome.The font for ‘View’ is so small I didn’t notice any of the menu options until you told me it was there. I had tried Control Mouse Wheel which works in a browser, I didn’t think to try Control +.Thank you.To add to that, most of the news entry can also be found on the blog or news section, should you choose to view in browser instead 🙂I usually just read the news when I start Launcher, but the web version is good to know if I ever need to.Powered by Discourse, best viewed with JavaScript enabled"
105,cad-importer-and-the-importance-of-hierarchy,"Several times when importing large STEP files now, the only thing that has happened is that a folder is created and an USD file is placed in it for every single body in the original STEP file.I cannot see why this has any use.From a single STEP file, a single USD file should be generated because the hierarchy present in the STEP file is very important! When the CAD importer ruins that hierarchy, it becomes a great burden to manage the result.STEP file importing will not have this behavior in the next release and instead it will create a single USD file.Powered by Discourse, best viewed with JavaScript enabled"
106,get-list-of-objects-in-a-scene,"Based on the Hello World Core API Tutorial I would like to print the position of every object in the scene over the Python API. The catch is, I do not generate all the objects programmatically in the python script, so I don’t know how many objects there are and what their names are.Is there a possibility to get a list of objects in a scene? The information exists in the SceneRegistry, e.g. for rigid objects. I just don’t know how I can access it from the scene class, since there is only a function to get information about a specific object when you know the name of the object.I figured it out. I use get_prim_at_path to get a high-level prim, I know and then I use get_prim_children to iterate down.Powered by Discourse, best viewed with JavaScript enabled"
107,importing-rigged-character,"I have a issue when importing an animation as a fbx it imports in. Check out the video and let me know what I’m doing wrong??HI @ajpanimations    It’s hard to know what’s going on without seeing how your rig hierarchy is constructed.  But it appears that your joints may not be under a single hierarchy?Hi Ronan,I managed to solve the problem with the discord group, mainly its the way I export the characterthanks for getting back to meHere’s the outcome: https://twitter.com/AJPAnimations/status/1679442103204474881?s=20Cheers.Powered by Discourse, best viewed with JavaScript enabled"
108,isaac-sim-launch-stuck-after-rtx-ready,"Isaac sim launch stuck after “RTX ready”. The shader is built successfully. But it keeps “progress…” forever.Same hereAnyone??I 'm still facing the same problem. I have to remove omniverse entirelly to get the Isaac Sim to work… After a few success launch, the problem came back again. RTX ready then the ‘progress’ indicator keep running and Isaac Sim is not responding.I got the same one. No solutions yet??Please share full logs.Here’s the logThere’s no errors in the log file. Can you run Isaac Sim from terminal using the -v flag or add it as an extra argument in the Isaac Sim Selector? Please share the logs.Also wondering if you can successfully run Create?Hi, I also experienced the same problem. The window is forzen after RTX ready shown in the terminal. I have added -v flag to isaac sim and generated the log. The log also didn’t reveal the reason behind the freeze.
kit_20230724_161627.log (1.3 MB)@chris.tung Yes, there are no errors in the logs. Seems to be ok. Does Isaac Sim starts after about 5-10mins?
Do you see the same behavior with other Omniverse apps?
If you are seeing the same with other apps, try updating you drivers to the “Latest Production Branch Version” here: Unix Drivers | NVIDIAHi, I ran omni.isaac.sim.warmup.sh and now it’s ready to roll. Thank you for your help.Not work for me. After running this script RTX recompiled. After that, when launching isaac sim still stuck at RTX ready.did you reinstall the driver as mentioned by Sheikh in previous message?Powered by Discourse, best viewed with JavaScript enabled"
109,how-to-input-camera-feed-into-pose-tracker,"I do not understand the logic behind camera input in Pose Tracker section. It displays “Camera 0” to “Camera 5”. None of these is working. There is no video input section in preferences also. I have many camera devices including Blackmagic Video Capture Card however cannot figure out how it should be set. The documentation on this topic is also very poor - limited to only 1 sentence. Is it even possible? Thanks.Powered by Discourse, best viewed with JavaScript enabled"
110,instancing,"I am about to make 1000s of instances of this alembic file import.I started by making it “instaceable”, but it disappears:
Can you tell me why?Also if someone knows a good extension or native tool to spread duplicates of these all over the sky in all directions with randomization features also, that would be great!I know there is the surface instancer
https://docs.omniverse.nvidia.com/prod_extensions/prod_extensions/ext_surface-instancer.htmlbut I need to spread all over the empty sky!PekkaHi @pekka.varis. Ideally, your material assignments would be inside of the payload. What looks to be happening is that that material binding is breaking when you make the payload instanceable which is by design. So, you should either:Also, note that setting instanceable (native instancing) on that mesh doesn’t do anything. Native instancing is really meant to be used on the top-level prim of a referenced asset.Powered by Discourse, best viewed with JavaScript enabled"
111,maya-v-ray-workflow-connector,"Hello,What is the workflow to get Maya Vray scene into omniverse with lights, materials?Is only 3DMax connector able to transfer Vray material?One of the Vray developers mentioned on chaos group forum that there might be a nightly connector build for Maya that could do this? How to access it?Thank youMaya Connector does not support VRay yet. Max Connector supports it.This topic was automatically closed 14 days after the last reply. New replies are no longer allowed.Powered by Discourse, best viewed with JavaScript enabled"
112,scaling-problem-for-imported-usd-objects,"Hello!In my python script I want to import (provided USD like pallets) objects, but facing the issue that the units of meters are not always matching my current scene. Is there a way to check that dynamically in a script and change the units per meters for the new USD object?How is it supposed to work in python to match such differences in Object sizes?Best regards,
ChristofHi @christof.schuetzenhoefer  - In USD, the unit of measurement is defined at the stage level and is typically set to meters. When you import a USD file into another stage, the imported objects will be scaled according to the unit of the destination stage. If the units of the source and destination stages are different, the objects may appear too large or too small.okay, and how can I know the units per meter in advance and change it respectively for the imported usd?
That’s what I want to know so that I can use the offered usd’s in my python simulation.Powered by Discourse, best viewed with JavaScript enabled"
113,import-omniverse-create-extension-but-got-permissionerror,"Hi,  I can not import Move.AI extension, I got this error
image2814×723 307 KB
It seems like a permission error. … And I launched my omniverse app by administrator again. Still got same error.What did I miss? Any suggestions?My environment:
windows 11
ominverse 2022.3.1This is move.ai extension zip file if you need.
MoveAi_Omniverse_Extension.zip (428.3 KB)I am having this problem too. Seems like there should be a simple answer.Powered by Discourse, best viewed with JavaScript enabled"
114,api-integration,"Hi! I am part of a startup office based in Canada. We are currently working on an exciting project to create a digital twin of our office space. Our project involves the deployment of various sensors (e.g., temperature, humidity) and cameras (e.g., people counting, zone crowd estimation) in our office, from which we collect daily data and integrate it into the digital twin.We kindly request your expertise and guidance in identifying API integration options that are more suitable for our project. Specifically, we are interested in solutions that support Python for seamless customization. Python is our preferred language, and leveraging it would enable us to tailor the integration to our specific requirements using appropriate libraries or tools.
We greatly value the insights and experiences of the community, and we would truly appreciate any recommendations, suggestions, or resources you can provide.Hi @User0110. Welcome to the forums! It sounds like creating an Extension or Python Connector may be a good fit for you. This is a good place to learn more and get started: Omniverse Developer Guide — Omniverse Kit documentationPowered by Discourse, best viewed with JavaScript enabled"
115,issac-sim-crusch-after-play,"In the process of work, after building the scene, physics and objects, I run (play button) a simulation and the application freezes, and after that it crashes. What could be the matter? ThanksHi @darombeer - Can you share the logfile or error screenshots. That will help us debug further.Hi @rthaker tell me please, where i looking this journal?You can find it in the “Console” section at the bottom
image1920×1520 200 KBPowered by Discourse, best viewed with JavaScript enabled"
116,gas-flow-in-a-tube,"I’m trying to simulate the effect of the gas flow in a tube on a robot of mine. But I’m not be able to find how to add gas flow in to simulation environment in Isaac Sim. Is there anyone who tried it or done it before? The gas flow could be a air flow as long as I can change its properties to make it the gas I need.Hi @semir.sunkun  - Can you look at this documentation and see if that answers your question!
https://docs.omniverse.nvidia.com/prod_extensions/prod_extensions/ext_fluid-dynamics.htmlPowered by Discourse, best viewed with JavaScript enabled"
117,semantics-schema-editor-visualization-using-point-instancers,"Hey,I am having problems using point instancers along with semantic data, and I wanted to know what was the correct way of going about this. To give context, I have some rocks scattered inside a scene, and I am using a UsdGeom.PointInstancer for that. All of the rocks are being assigned a class and then added to the cache of the instancer. When I visualize the instance segmentation, the rocks that have the same model are considered as the same instance:
Screenshot from 2023-08-02 16-53-07861×641 209 KB
If I randomize their position, the scene looks like that:
Screenshot from 2023-08-02 17-49-402333×1138 195 KB
This is weird seeing that the instance id segmentation does show them as individual objects.
Screenshot from 2023-08-02 17-11-351034×666 76.5 KBSmall detail there are 4 assets in the cache, so the times when I have more than 4 out off all the rocks are really puzzling me.I am a bit confused as to what can cause the issue. Surely I am doing something wrong when I am creating/randomizing the instances. Could someone point me in the right direction?Cheers,AntoineI’m also joining a script for reproduction purposes:
instancer_code.py (4.4 KB)The only thing needed is some assets on which semantic data has been applied beforehand.The behavior is as follow, loading the scene the instancer will work properly. Though, if one of its properties changes it will start to behave weirdly.Thanks,AntoinePowered by Discourse, best viewed with JavaScript enabled"
118,reducing-dependencies-for-extensions-in-headless-kit-container,"We have implemented many Omniverse extensions that will be used in two different contexts:In the first setup, there would be some functionality we would access using a UI implemented in the extension’s own window. In the second setup, we would use the functionality programmatically.We would like to reduce the number of dependencies (on other extensions) when we run the headless container to a minimum. Especially we would like to have no omni.ui dependencies when we run the container, but include them when we run the extension interactively.Is there any elegant way to achieve this?Thanks for any help
BrunoHi @bruno.vetter. You’ll find many examples in NVIDIA-authored extensions where we split a “feature” into UI, core, and sometimes bundle extensions. This way you can decouple your business logic from your UI. It also should help with what you’re after in that your UI extension is the one that declares theUI dependencies and when you run your headless Kit, you can just not include your UI extension.Thanks @mati-nvidia, this is a good suggestion!This topic was automatically closed 14 days after the last reply. New replies are no longer allowed.Powered by Discourse, best viewed with JavaScript enabled"
119,unitiliazed-robot-articulation-error-when-simulating-robot-and-clothing-in-the-same-scene,"Continuing the discussion from How can I make deformable body in python?:I am also experiencing the same issue, and I have the same fix in GUI. I am using the franka pickandPlaceController for manipulating a Franka robot, which is 99% based on example 4.5 in 4. Adding a Manipulator Robot — Omniverse Robotics documentation (nvidia.com)) .
I have modified my scene to try to experiment with cloth manipulation, but I am experiencing some errors when the robot interacts with the cloth.
image762×549 75.3 KBBefore fixing the GPU not enabled error as described in the linked issues the robot simply moves through the cloth and, completing all 9 steps of the pickandplace routine on the small cube.
image946×554 116 KBIf I fix it however, the cloth is properly simulated but the robot fails to pick up the cube. In fact it fails to start the picking sequence at all. It simply returns to this idle position as soon as the simulation starts.
image957×539 93.6 KBIt seems to be some form of problem when the cloth interacts with the robot path planning, as it seems that the Articulation fails and gets unitiliazed.Healthy log:Unhealthy log with cloth simulated under robotic arm:The log message in the unhealthy log gets produces continously as long as the simulation is stepping using physics_step.Interrestingly I am able to fix all my issues by moving the cloth out of the way of the robot, then the robot is able to execute the pickandplace routine while the cloth is simulated to the side.
Source code:
hello_world.py (11.1 KB)Fixed! Sortof… I was able to make the robot move as expected by using the reset button in the world control widget that comes with the Hello World sample series.Powered by Discourse, best viewed with JavaScript enabled"
120,how-to-can-i-add-children-to-xform-while-simulating-using-python-standalone-workflow,"I wanted to create a Xform prim containing an existing 4 wheeled robot and keep adding different children to it throughout the simulation, using python standalone application or python extension workflow. How can I achieve this?Powered by Discourse, best viewed with JavaScript enabled"
121,how-to-display-timeline-on-isaac-sim-interface,"I’m learning the tutorials of Isaac Sim. It seems like the timeline will display at the bottom of isaac sim interface  when I press the play button. But after I pressed the play button, there is nothing happen. How to display timeline on isaac sim interface?This is my isaac sim interface :
Screenshot from 2023-06-28 16-36-361693×1052 201 KBHi @DesnyXu  - You can go to Window → Extension and turn on the animation timeline window.
Thank you, I find the timeline now~This topic was automatically closed 14 days after the last reply. New replies are no longer allowed.Powered by Discourse, best viewed with JavaScript enabled"
122,reading-primvars-in-material,"Hi,
Is it possible to read primvars in a mdl material? Say I have a a varying color/float primvar I want to use to vary the shading across the surface.
It looked like the “Data Lookup” nodes are there to read primvars but I can not get them to read any varying primvars.Hi @aslak1!  I’ve let the dev team know you have a question.  Thanks for reaching out!Hi @aslak1
Primvar reading is currently in development but not yet available in our Material Graph. The nodes are there but the renderer does not yet the data from USD.We anticipate this will be available in the near future.Hi there,Just wondering if there has been any update on implementing this?Thanks!Hello @dbr!  I will ask the dev team if there is any update on this.  Thanks for asking!I would also love to know if there has been any update to this yet :)Hi Everyone!  Know how much all you watching this thread want this supported.  I contacted the development team to get an update.  Support for PrimVars is still in development, but they are just cleaning up a few bugs before it is released.  It’s almost ready!Hi. This was a while back, but I’m assuming Primvars in materials still can’t be read? I’ve tried in the latest beta version with a basis curve and it’s not reading the verified groom_root_uv data at an individual strand level.Primvar reading is available through the data_lookup nodes in the material editor. However primvar reading is not yet available for curves.Powered by Discourse, best viewed with JavaScript enabled"
123,failed-to-upload-minidump-error-message-while-running-isaac-sim-from-docker,"I have encountered an error message in my log while running WebSocket headless Isaac Sim from docker. I manage to connect to the streaming client, and Isaac runs, however in my log I see this error message:I would like to know what this error means and, if possible, how to solve it. I attached my full log file below.Note:
I am running on Ubuntu 20.04 and using Chrome to connect to the streaming client.Hi @francesco.sarno - The error you are encountering indicates the libcurl.so shared library file is missing in your system.For Ubuntu-based containers:After installing the libcurl package, restart your Docker container and try running the WebSocket headless Isaac Sim again. The error message should no longer appear in your log.Powered by Discourse, best viewed with JavaScript enabled"
124,blendshape-solver-doesnt-apply-on-bs-mesh,"Hi guys, I’m new with A2F and followed a guidance for preparation of an export for metahumans.I tried the default setup with “mark” and the “mark_bs_46.usd” which I dragged in, then created a Blendshape Solver  as you can see in my screenshot. But unfortunately it doesn’t seem to work at all.image1920×1047 123 KBCan someone guide me into the right direction? I’m not sure which steps I forgot.Thank you very much!Do you get get error? Please note that before Exporting Weights, you need to select the blendShape solver node inside the BlendShape Conversion section of A2F Data Conversion tab.Thanks for your quick reply. Oh yes, I forgot to look into the console, and yes my bad. I forgot that I first tried to get pythonosc working and I had some “bad code” in the facsSolver.py. I removed this code and now it works perfectly. Thanks.This topic was automatically closed 14 days after the last reply. New replies are no longer allowed.Powered by Discourse, best viewed with JavaScript enabled"
125,is-there-a-blendshape-test-tool-for-usd-composer-feature-request-if-not,"You can use audio2face to generate blendshapes, and it will also generate a sample mesh of every blendshape, but I would like to be able to test the blendshapes by being able to adjust the weights.For example, on Skeleton you can adjust bone rotations to move legs etc, but for blendshapes it looks like you can have the Skeletal Animation panel reference a SkelAnimation prim, but I could not see how to look at and adjust the blenshape weights for SkelAnimation prims. So I had to edit the USD file and look at the array of blendshapes by hand to work out which number to adjust. Rather tedious and error prone. Is there a better way?Step 1: You can select the Skeleton prim and it will identify a SkelAnimaton prim that it previews.
image1026×443 42.8 KBStep 2: You can click on the referenced SkelAnimation prim. Looking at the raw properties, you can see there is blendshape data, but its not visible and you cannot edit it.
image1034×519 28.8 KBStep 3: You can edit the USD file and look for the SkelAnimation node. If you change the weights and save the file, then you can see the impact of the weight changes.
image1278×261 24.2 KB
The left eye at 50%.
image556×636 32.2 KBPowered by Discourse, best viewed with JavaScript enabled"
126,can-audio2face-headless-rest-api-be-used-for-commercial-use,"Hi,I am confused wether, I can host and use A2F Rest API on a cloud provider (like AWS) to generate blendshapes to animate an avatar (in external 3d engine) and stream it to users.
That may requiers spinning off many instances (gpus) in the cloud to accommodate the demand.@WendyGram would you mind helping on thisThanks in advance.Hi @bahaa. Welcome to the forums! Please consider join our community on Discord too: NVIDIA OmniverseI’ve moved this question to the A2F forum where someone will be able to jump in an help answer this one.To answer the the question in the title, yes, Audio2Face is Free to use for any kind of project.Here’s a good tutorial on Rest API: Audio2Face Headless and RestAPI Overview - YouTubeI’m not quite sure what you mean by it requiring many GPUs in the cloud. @RogerBR might be able to help@mati-nvidia Thank you for your support - I did joined the Discord.
@Ehsan.HM Great to know that.In NVIDIA OMNIVERSE LICENSE AGREEMENT, 2.3, it is written: “(b) use of Batch by an individual is limited to two GPUs.” I am confused about this. As I may end creating a A2F instance for each active end user session.ThanksHi @bahaaRight now the Audio2face app does not support batch processing ( simultaneously) different files. Howeverm you can launch several instances of A2F on a single machine sharing a same GPU as long as there is enough memory left.It is possible to stream a single animation to multiple users.However, A2F is mostly an GUI app and the restAPI is mostly to control the app remotely ( ie. you want to render several files on a server and automatize the process by script)Eventually for controlling multiple avatars or providing an online service the Avatar Cloud Engine will be the right solution when it is released, you can learn more here:Generative AI Sparks Life into Virtual Characters with NVIDIA ACE for Games | NVIDIA Technical BlogHi @RogerBR - Thank you for your support and guidance.Although, we applied to ACE, we could not get an invite for beta access and we do not know the release time. Seeing the nice results of A2F, I guess I have to create an instance for each user session on strong servers and use headless api (That is why I asked about Licenses).Another option would be to create the animations for all corpus (chatbot responses) by A2F and cache them somewhere on the sever. Streaming same animation to multiple users is not a fit for our use case where each user will get a personliazed experience.We need to ship the product as soon as possible. I would appreciate your advices if there is a better way of doing this or if we can test ACE for our use case.Many thanks@wtelford1 might be able to help you with your question about licenses and testing ACEin the meantime, @bahaa, using multiple instances A2F or caching the animations as you mentioned will work.There is a waiting list for ACE at the moment as we got an overwhelming amount of interest.  Stay tuned for updates and contact from the ACE team.Powered by Discourse, best viewed with JavaScript enabled"
127,can-i-now-import-alembic-data,"Hi!What is the current state, can I import abc files to Composer?I found this but I cannot understand how that should setup in Composer:
https://openusd.org/release/plugins_alembic.htmlOk, I can import abc to blender USD release. I got that way for now, and try to bring them into Composer.Hi Pekka!
You can try dragging an alembic .abc from your file browser into the stage or layer areas of your kit-based app.ooh :) what was that emoji of the office guy in amazement while camera zooms??Yeah, it’s really not very obvious but pretty cool that it works!Wow it totally works!!But as you can see from the video - all abc files are not visible.
How could I solve this?This topic was automatically closed 14 days after the last reply. New replies are no longer allowed.Powered by Discourse, best viewed with JavaScript enabled"
128,support-for-3d-spacemouse-3dconnexion,"It would be nice to have support for the industry standard (CAD but not only) way of moving around and navigating the 3D view.
3DConnexion Spacemouse.Hello @user27228!  We have an internal ticket to include support for the 3DConnexion (Internal Ticket OM-35839).  I will inform the development team about this post.In the meantime , take a look at this post link for a possible workaround until the dev team  Python SpaceNavagatorOh man thanks for this reply I would also like to add my whole hearted support for this feature! Is there any way to follow  the ticket? Either way, thank you and I look forward to it.Yes, a 3d mouse increase the productivity a lot. Left hand to 3d navigate and right hand for the mouse.+1 for spacemouse support in OmniverseI notice that in recent versions of Omniverse, Carb reports the following:2022-11-08 08:59:17 [2,900ms] [Warning] [carb.windowing-glfw.gamepad] Joystick with unknown remapping detected (will be ignored):  SpaceMouse Wireless [030000006f2500002ec6000000000000]
2022-11-08 08:59:17 [2,933ms] [Warning] [carb.windowing-glfw.gamepad] Joystick with unknown remapping detected (will be ignored):  3Dconnexion KMJ Emulator [03000000efbe00006d04000000000000]
2022-11-08 08:59:17 [2,934ms] [Warning] [carb.windowing-glfw.gamepad] Joystick with unknown remapping detected (will be ignored):  SpaceMouse Wireless Receiver [030000006f2500002fc6000000000000]Is there a Python interface to carb.windowing-glfw.gamepad that allows us to map (inject) mouse and keyboard events into the stack?  If that is possible, it would be possible for the community to write a shim between the spacemouse events coming from GitHub - johnhw/pyspacenavigator: 3Dconnexion Space Navigator in Python using raw HID (windows only) and carb, that remaps to combinations of keypresses and mouse events.I’ve looked at carb.windowing — kit-sdk 103.1 documentation but that doesn’t appear to cover windowing.gamepadSame here, could you provide support for Spacemouse ?Hello @WendyGram , was there any progress on this ticket, having support would be incredibly helpful.
ThanksNo update ? No reply ?Unfortunately, this ticket and this issue is still being evaluated. We do not get a lot of requests for this type of device.I know, there are less power users than regular users but all other software on the market have a dedicated plugin (Unreal, Unity, Enscape, Revit, Rhino, Lumion).
We might be a small community but we are mighty !!Yes I understand and appreciate the request. I will bring up the ticket to the devs again.Adding my full support for this. Not being able to use the spacemouse seems like a major drawback, when you can use it in pretty much any other professional 3d tool.It is so common in the engineering and pro 3d world, which is one the target markets for professional omniverse users.Is there no way to do this with C#?You may not get a lot of requests for this type of device because you possibly don’t have enough users in this particular space yet?Sort of like Wacom has an effective monopoly on tablets in the professional 2D design world, 3DConnexion has an effective monopoly in the professional 3D design world (note, I’m talking design for physical products here, not gaming).I would go so far to say that not supporting 3DConnexion is pretty close to being a showstopper for adoption where I work.“3D connexion mouse” it’s in the first place with more replies in the Omniverse/Developer forum and in the 4º place with more views  (3.2k)… these numbers speak for themselves.I acknowledge this :-) I will pass it alongPowered by Discourse, best viewed with JavaScript enabled"
129,viewport-not-responding-to-input-or-loading-meshes,"Hi! I’ve been using Audio2Face for the past few months without any issues, but opened it today to discover that the viewport seems to be not responding. I’m getting no mesh visibility, and I’m unable to tumble, pan or manipulate the camera at all. The rest of the app (Stage view, other tool tabs,) is responsive, it’s just the the viewport.This happens regardless of file opened, even on first opening the app, where I would normally see a mark head in the default scene, I’m only seeing black. and I’m working in Audio2Face version 2022.2.1
I can try and grab a printout from the console, or any other information, if it would help.This is what the viewport looks like when I open the file NVIDIA>Assets>Audio2Face>Samples>char_transfer>allison_tagged.usd. This file has worked many times before, but I can no longer see meshes, only the correspondence tags, and I can’t tumble or move the camera at all. :(
a2fIssue1568×1235 125 KBI’m really hoping it’s just some kind of visibility or navigation setting I’ve managed to bump, but I’m a bit at a loss. This is a major blocker, so any help at all would be greatly appreciated!!EDIT: I’ve tried downloading an earlier version, (2022.1.2,) and the viewport is working as normal when using it. I do need some functionality in the later version though, and I’m not sure it explains why vers 2022.2.1 was working for me previously, but now suddenly is not :(EDIT: I’ve tried downloading an earlier version, (2022.1.2,) and the viewport is working as normal when using it. I do need some functionality in the laterHello and welcome to the forums @amybuc6Is the viewport black even when you start a new template scene? Or it’s only black when you open an old scene?Does adding a light help?Can you please send your latest Audio2Face log file which can be found here: C:\Users\<USERNAME>\.nvidia-omniverse\logs\Kit\Audio2FaceHi Ehsan, thanks for your help!
Yes, on first startup the default template scene viewport is still black, and it stays black regardless of the scene I load. The default scene, and other scenes, do have lights in them, so I’m not sure if that’s the cause. For clarity, when I load in the ‘alllison_tagged’ char_transfer sample scene, I can see the correspondence tags in the viewport, just not any meshes. And the viewport doesn’t respond when I try and tumble or move the camera.
I have attached my most recent log file, thanks again for looking into this!!It seems you have 2 graphics cards and Audio2Face gets confused. While this is a bug and we’ll work on it, hopefully disabling the 2nd graphics card, should fix this. You can disable GPUs from Device Manager in Windows.Disabling the second graphics card and restarting Audio2Face was the solution, everything is working as expected now.
Thanks so much for your help, it’s much appreciated!!This topic was automatically closed 14 days after the last reply. New replies are no longer allowed.Powered by Discourse, best viewed with JavaScript enabled"
130,audio2face-emotions-in-streaming-livelink-error,"Version:2023.1.1
Unreal Version:5.2When I run Audio2Face in either GUI or Headless Server mode, when trying to stream via LiveLink to Unreal Engine, everything works but the Emotion Streaming and the following Error appears:I run on a 4090 but with all the stuff thats loaded and running im close to max memory.Important to note, it did work before this just happened and stayed.image771×1037 54.2 KBPowered by Discourse, best viewed with JavaScript enabled"
131,ros-navigation,"When conducting ROS navigation, how to set the direction of odometry, whether it is set by the urdf model or other settings, I hope someone can help me answer it, thank you very much! !Are you looking to setup the initial location of the robot ?Hi @fmy553 - I think this document will be useful to you for ROS Navigation. Let us know if you have any particular question from it.https://docs.omniverse.nvidia.com/app_isaacsim/app_isaacsim/tutorial_ros_navigation.htmlyes,Because the direction of the car in Rviz is wrong when I am navigating. @eds17m007~/.local/share/ov/pkg/isaac_sim-2022.2.0  This is the path of Isaac Sim , if you do ""ls""in this path , you can see the ros_workspace and ros2_workspace.if you are using ros2_workspace then in carter_navigation package inside ros2_workspace , then carter_navigation_params.yaml file is there where params
set_initial_pose: True
initial_pose: {x: -6.0, y: -1.0, z: 0.0, yaw: 3.14159}
you can change this to setup the initial location and orientation of the robot.for ros_workspace the things are also same ,
carter_2d nav package under navigation , in carter_navigation.launch file you can set the initial pose for amcl to set the initial location.Powered by Discourse, best viewed with JavaScript enabled"
132,calling-own-functions-between-2-frames-with-replicator,"Hello,I am currently working on a project involving the generation of synthetic data for a pick and place robot ([2208.03963] MetaGraspNet: A Large-Scale Benchmark Dataset for Scene-Aware Ambidextrous Bin Picking via Physics-based Metaverse Synthesis). In this context, I drop various objects into a bin and aim to capture RGB, depth, and instance segmentation data from multiple camera positions for different scenes. So far, the process has been successful using the replicator.However, I encountered challenges while trying to implement an additional feature. I wish to create scenarios where, for each camera position, only one of all the objects from the original scene, along with the bin, is visible. I also need to capture the corresponding instance segmentation data for these scenarios. Currently, I am using rep.trigger.on_frame() to trigger changes in the camera position. While this approach works for changing the camera position, I am uncertain about how to execute additional functions, as described above, for each camera position in this context.forumquestion.py (375 Bytes)Is there a way to interrupt the replicator between two triggers, adjust the visibility of objects, and obtain the corresponding instance segmentation data for the desired scenarios? The goal is to get the visible and invisible mask of each object in that bin. The code snippet above shows how I imagined it could work, but it didn’t.Thank you very much.Best regards,
ValentinPowered by Discourse, best viewed with JavaScript enabled"
133,avoid-collisions-with-replicator,"Hey there,For my master thesis I’m trying to load an environment and randomize the scene for synthetic data generation. Specifically, I want to randomize some production modules in a production environment. However, I struggle to implement the avoidance of collisions. In particular, collisions with walls and other production modules of the environment.I’m trying to use rep.modify.pose() to randomize the position and rotation of the modules in the environment. Even though I’m using rep.physics.collider() the randomized modules appear to collide with the surrounding objects and the environment (as seen in the attached image).
rgb_00081024×1024 93.4 KB
The flat white module shouldn’t touch either the wall or the other module. The green “box” is supposed to be the approximation_shape of the collider, but is always shown a frame too late.After I read that similar problems were solved with the rep.randomizer.scatter_2d function, I tried to solve it the same way. I made sure that check_for_collision=True. It still results in the same output, that production modules collide with the surrounding environment (e.g. walls and other production modules). Furthermore, the modules are placed in a random position, but the rotation doesn’t change.This is the Code I’m using so far, but might be hard to recreate as the data are generated with usd files of the production site:Substituting rep.orchestrator.run() with asyncio.ensure_future(rep.orchestrator.step_async()) slowed down the data generation and only generated one image of the origin. However, it didn’t solve the collisions.For my setup, I’m using:
Isaac Sim: 2022.2.0
Replicator: 1.6.4@hclever are you the right person to talk to?I’m happy if you can help me out or push me in the right direction. Thanks a lot!I worked on an example, which shows the same issues that I have with the simulation and you can also try on your own.I found this post and the code example you gave and adapted it a little bit for my example.So this is my example code:with Isaac Sim 2022.2.1 and Replicator 1.7.8 The result is shown in the picture below:

scatter_2d Isaac Sim 2022.2.01850×1055 354 KB
Is there any option to avoid the collision with the bigger sphere?Hi Hugo,Unfortunately, this version of replicator (1.7.8) only supports collision checking between the sampled input prims – not any other prims in the scene. You have three options -
(1) wait for the next version of Code / Isaac to be released that has replicator 1.9 (about a month I think)
(2) step physics for 1 step, which should move things a bit out of collision
(3) use my workaround that is detailed in a previous post – here is a link – Scatter_2d collisions with static objects - #18 by jorge29 . You’ll have to replace a number of the scripts in replicator with ones I provided:
omni.replicator/source/extensions/omni.replicator.core/python/_impl/nodes/OgnScatter2D.py:
OgnScatter2D.py (16.8 KB)omni.replicator/source/extensions/omni.replicator.core/python/scripts/randomizer.py:
randomizer.py (17.8 KB)/home/hclever/git1/omni.replicator/source/extensions/omni.replicator.core/python/scripts/utils/utils.py:
utils.py (44.1 KB)/home/hclever/git1/omni.replicator/source/extensions/omni.replicator.core.scripts.utils.viewport_manager.py:
viewport_manager.py (12.2 KB)You can run this script to test it:The first item in the surface_prims list is the item sampled on (the plane) and the second and every one thereafter are checked for collisions.Let me know if this works-
HenryI am facing an issue when trying to rotate the scattered prims randomly around the Global Z-Axis. I am using the following Script:If I am using rep.modify.pose() the objects collide with each other. If i use rep.randomizer.rotation(), the collision detection works, however I need to set all the max_angles larger than the min_angles, otherwise i will get an error message. Maybe someone knows a way to only modify the rotation around the Z-Axis randomly.
image1524×105 8.8 KB
Can you just make the difference from min and max to be very small? I.e. try min=(0,0,-180) and max=(0.001, 0.001, 180)? Or is this small change of x and y significant?If I make the difference very small (e.g. 0.1), Omniverse stops at “RTX ready” and it does not seem to start. I waited for 45min and then it rendered the first frame. After that the rendering continuous, but is super slow (like 500 seconds) per frame.image1329×288 145 KBAt a difference of 1 degree, the effect is not very huge if you look straight from the top. But if you look from the side, the rotation is very noticeable, as the parts start to intersect with the floorplane.Powered by Discourse, best viewed with JavaScript enabled"
134,composer-not-booting-up-in-aws,"I have this instance in AWS amazon:

image1299×193 30.6 KB
It is made by following this tut:But now lately Omniverse Composer did not booted, it stuck at this for hours:

omniverse not booting1821×1141 141 KB
So I tried to update the driver. Official latest one for A10 did not worked.
I tried this one:Download the English (US) Data Center Driver for Windows for  Windows Server 2016, Windows Server 2019, Windows Server 2022 systems. Released 2022.6.6Now it boots only for few sec to show the composer launch screen but then it crashes without any warning messages.What driver should I use here to make this work again?Ok, this was solved by deleting the Instance and simply creating a new one by following this excellent new tutorial.https://docs.omniverse.nvidia.com/prod_nucleus/prod_nucleus/enterprise/cloud_aws_vdi.htmlAnyways, what is the driver to install if I decide to update the driver?
Tesla series A10 for win server driver messed it totally.This topic was automatically closed 14 days after the last reply. New replies are no longer allowed.Powered by Discourse, best viewed with JavaScript enabled"
135,isaac-sim-omnigraph-read-imu-node-index-out-of-bounds,"Hello,We have an IMU sensor, and an OmniGraph node to read from the IMU, and later publish it to ROS2 using another node.
The read IMU node throws the following exception:Though, the ROS2 topic does publish IMU messages fine afterwards.Hello, thank you for reporting this issue. I think this might be one of the edge case where there’s no data populated in the very first physical step for the sensor. we will investigate into this issue further our end. Thanks again.Powered by Discourse, best viewed with JavaScript enabled"
136,lost-all-material-when-open-usd-files,"
image1920×1030 44.2 KB
OH NO!  I am reporting this to the development team now!Can you please provide us with the following information:Also, I would greatly appreciate it if you could give me more details on the steps you took that resulted in this problem.When you exported from 3dsMax, Can you confirm that you had the option “Include MDL” turned on?Also what types of materials are you exporting? (std, physical, Arnold, VRAY)?materials is VrayThe connector logs are here, can you send it after an export
C:\Users\  \Documents\Omniverse\Max\MaxPlug_xxx.log@seanliangbally Would it be possible to get your Source File (USD)?  I can PM you an email address if you need to keep it confidential.sure.Google Drive file.Can’t sing in to the sever when use 2022 3d max, I already reinstall my windows.Hello @seanliangbally!  The development team is looking into your file.  Thank you for your patience!An internal Bug was created for this post OM-36502I don’t know how to fix it, still need help for sign in Error. like have more Omniverse  post more tutorials.of counrse the 3D max to Omniverse tutorials need update more.It seems there is no texture in the output. It is most likely a bug in exporter. Do you have simplified Max or Maya file for us to debug?@seanliangbally There was no max file with vray in the package so I tried to reproduce with the supplied fbx file.  However, I imported the fbx into a few apps and none of them showed texture connections. My suspicion is the fbx doesn’t have textures connected up to it, at least in the package posted here.  Can you confirm  textures are hooked up in the Max scene prior to export? If so can you send your max file with texture connections?I just to login to Omniverse cann’t work,I will just use USD export plug in from 2021 Max export that.Just came across this thread. Did it work finally?Powered by Discourse, best viewed with JavaScript enabled"
137,missing-nodes-in-action-graph-when-i-reopen-the-usd-file-which-has-been-saved,"As the title shows,i notice that,some nodes seem to be missing when i reopen the usd file which has been saved .However in the stage ,the nodes are still there.When i want to edit them again, i can not find them.Look at the pics below,where are the ros2_camera_helper,ros2_contex…
Does anybody knows why？ THANK YOU!!!
Screenshot from 2023-07-26 18-14-241017×377 24 KB
Hi @gaozhao22  - This issue might be related to the way Omniverse Kit handles the saving of nodes in the USD file. When you save a USD file in Omniverse Kit, it saves the state of the stage, including all the prims, properties, and relationships that exist on the stage. However, certain nodes, especially those related to Omnigraph (the visual scripting system in Omniverse), might not be saved directly in the USD file. Instead, they are saved in a separate Omnigraph file (.omnigraph), which is then referenced in the USD file.If you’re not seeing certain nodes when you reopen the USD file, it’s possible that these nodes are part of an Omnigraph and you need to open the corresponding Omnigraph file to see them. You can do this by going to Window > Visual Scripting > Action Graph in the Omniverse Kit interface.If you’re still having trouble finding the nodes, it might be a good idea to check if there are any errors or warnings in the console when you open the USD file. These messages might give you a clue about what’s going wrong.Powered by Discourse, best viewed with JavaScript enabled"
138,character-transfer-issues,"yanyan a2f.7z (18.6 MB)
Hello friends,
This is the file I’m working on with A2F2023.1 the newest version. Can anyone help with it? What process did I miss or something. The head/teeth mesh is not moving with Mark or Claire. It’s driving me crazy for two days.
There’s no issues in version2022 but my personal project is in asian language so really want to work with claire.
Thanks a lot!For teeth to work, the upper and lower teeth must be separate meshes. Additionally, you need to click Jaw Motion Prep in the Character Transfer tab.Screenshot_4677×974 47.1 KBJust tried your scene file and the girl’s face is moving. Not sure why it didn’t move in your recording. Maybe saving and reloading the scene would help?
If not, can you please send your latest Audio2Face log file which can be found here: C:\Users\<USERNAME>\.nvidia-omniverse\logs\Kit\Audio2FaceScreenshot_31829×716 124 KBHi Ehsan,
Thank you so much for replying. wow so its actually moving on your side. well yeah i need to seperate my upper and lower teeth. maybe tongue too?
This is the files in my kit please help check it out.
yanyan kit files.zip (8.6 MB)It looks like your GPU driver is outdated. Can you please update your driver and try again?I see… thank you. I will give a try.This topic was automatically closed 14 days after the last reply. New replies are no longer allowed.Powered by Discourse, best viewed with JavaScript enabled"
139,pip-libraries-in-usd-composer-not-installing,"I’m having trouble with pip libraries and the new USD Composer that replaced Create. In my extension, I’m using some third-party libraries from pip, which I’ve been importing through the extension.toml configuration e.g.This has been working fine in Code, View, and Create, but fails in the USD Composer app that recently replaced Create with an error like ModuleNotFoundError: No module named 'pandas' .I’m having the same issue with “pandas” and “opencv-python”; once again only on the new USD Composer app as it used to work just fine on 2022.3.* versions.Same issue in Code. I tried opencv, opencv-python.
I tried the “script editor” in omniverse Code as wellI found an issue that on the first installation our pipapi extension tries to update pip to an older version, which we probably didn’t update since the python update.There is a custom hack you can do, in that file:
omni.kit.pipapi\omni\kit\pipapi\pipapi.pychange line _attempted_to_upgrade_pip = False to _attempted_to_upgrade_pip = Truethen you need to clean app data (pip installation dir specifically), either manually remove:
%localappdata%\ov\data\Kit\USD.Composer\2023.1\pip3-envs
or restart an app with --clear-data CLI flagAfter that I was able to install pandas.Thanks @anovoselov !
Is this the omni.kit.pipapi\omni\kit\pipapi\pipapi.py file that is in the USDComposer extscache? (i.e. C:\Users\SFOEBB\AppData\Local\ov\pkg\create-2023.1.0\extscache\omni.kit.pipapi-0.0.0+ed961c5c\omni\kit\pipapi\pipapi.py)Powered by Discourse, best viewed with JavaScript enabled"
140,how-does-eye-compute-automatically-work-in-a2f,"So whenever i try this option it shows this,
But the mesh is selected where i want the pivot while choosing the option am I doing something wrong?Can you try selecting them in the Character Transfer Tab?
Screenshot_8620×973 43 KB
Powered by Discourse, best viewed with JavaScript enabled"
141,imu-topic-publish-code-error,"I wrote the python code to publish the imu topic in Isaac Simulator like below code. And I set the sensor_period to 1/400, which is the 400 hz. But I got the UI FPS(like below image 89), not 400.I run below code using script editor in Isaac Simulator.
What’s the matter in my code? And why am I getting from UI FPS, not sensor_period?My Isaac version is “2022.2.0” and ros version is Noetic.
Thank you so much for reading this problem!Hi @phr0201 - Someone from our team will review the question and respond back to you.hi, I have this question, too. Did you solved it?Hello,thanks for reaching out to the forum. I think the issue is that the IMU is called by the render callback (by default), not the  physics callback.  To do so, you need to first instantiate the world with physics rate based on your desired IMU frequency (1/400). For example, the snippet below will create a world with physics rate being 400hz, rendering at 50fpsmy_world = World(stage_units_in_meters=1.0, physics_dt=1 / 400, rendering_dt=1 / 50)then create a physics call back in your constructor to get the IMU data and publish it each physics step
my_world.add_physics_callback(""sensor_measurement"", onPhysicsStep)Let us know if this workes Thanks!Powered by Discourse, best viewed with JavaScript enabled"
142,problem-with-generated-bounding-boxes-in-omniverse-replicator,"Hi, I’m having an issue with some of the bounding boxes generated using Omniverse Replicator (via Omniverse Code).As you can see from the uploaded image, certain bounding boxes are too large and they cover more than one object, which is wrong.
output1024×1024 58.6 KB
Here’s the code I’ve used. I’ve fixed the seed so you should be able to replicate the problem.
The problematic bounding box can be found in ‘rgb_0003.png’ and ‘bounding_box_2d_tight_0003.npy’ in the output folder.Hello @user156715!  I’ve shared your post with the dev team for further assistance.Hello,I have used the code you provided and am unable to reproduce the issue:

Did you let the code run for more than one iteration? I got the same error with other objects after letting the code run for a few iterations. It seems that the bounding boxes used some value from a buffer. See my error here:
Bounding Boxes are wrong. Too many Instances in Bounding Box - Omniverse / Synthetic Data Generation (SDG) - NVIDIA Developer ForumsRunning your script for 100+ iterations still produces correct results for mePowered by Discourse, best viewed with JavaScript enabled"
143,super-slow-execution-rtx-flow-plugin-failed,"I am not sure why after uninstalling an installing back the isaac sim now the execution is super slow for headless and raytracing. Please note that it produces the results but it is super slow. Any tip is really appreciated.sys info:$ nvtop out:
Screenshot from 2023-06-14 10-25-06778×846 34.6 KBScreenshot from 2023-06-14 10-25-39745×372 28.5 KB$ btop  outScreenshot from 2023-06-14 10-26-171933×600 115 KBMy Isaac Sim version. I installed it today:
Screenshot from 2023-06-14 10-27-201250×569 198 KBPowered by Discourse, best viewed with JavaScript enabled"
144,cannot-use-requests-py-module-in-my-extension-for-create,"I created a simple extension and loaded it in CREATE 2023.3.3-rc.15 (with the help of link_app.bat and startup.bat).
I want to use requests py module in the extension.
For this I modified extension.toml and added lines:Also, I added ‘import requests’ to the extension .py file and after saving it I saw a message in console:I have also installed requests py module with the command ‘pip install requests’.
Please advise on how to resolve this.
Thank you.I found an answer. CREATE uses it’s own python which is located at ‘app\kit\python\python.exe’.
‘app’ is a folder link created in your extension’s folder after running link_app.bat.
So, I started terminal at folder ‘app\kit\python’ and there are files like:I run a command
python.exe -m pip install requests
and this allowed me to get requests module into CREATE’s python.
After that all works as expected.This topic was automatically closed 14 days after the last reply. New replies are no longer allowed.Powered by Discourse, best viewed with JavaScript enabled"
145,trouble-in-moveit2,"I’m using ubuntu22.04,ros2humble, and when I follow this tutorial(9. MoveIt 2 — isaacsim latest documentation)、(How To Command Simulated Isaac Robot — MoveIt Documentation: Rolling documentation), I have a couple problems:
First of all: “controller manager not available”,All I can ensure is that I have the controller manager installed.  And the ‘[ERROR] [move_group-5]: process has died [pid 59, exit code -6, cmd ‘/opt/ros/humble/lib/moveit_ros_move_group/move_group --ros-args --log-level info --ros-args --params-file /tmp/launch_params_89dev4yd’].’ appears.Besides,I can not see ’
You can start planning now!’ in the terminal.
Secondly: in the rviz2 interface, there is a warning like 'requesting initial scene failed '.
Lastly, when I move the trackball and give control commands, the arm does not move when I click ‘plan & execute’ and I get this error in the terminal 'Movegroup action client/server not ready '.
Help ！！！！！！Screenshot from 2023-08-05 17-21-491920×1078 505 KB
Screenshot from 2023-08-05 17-26-211920×1078 216 KB
Screenshot from 2023-08-05 17-26-251920×1078 422 KBPowered by Discourse, best viewed with JavaScript enabled"
146,cad-importer-missing-objects-other-issues,"Here is an imported STEP file in Rhino:
bild789×765 82 KB
Here is the same imported STEP file in Omniverse:
bild580×573 57.5 KB
Notice how the deck and inner roof is missing.Also, a lot of the EVA protection on the deck is doubled up in the Omniverse import, while that is not the case in the source STEP file or the Rhino import.Finally, note how the materials have the wrong color.Also, materials in Omniverse are apparently traditionally UV mapped like in games, rather than triplanar mapped, so one would expect the CAD importer to solve that because unless a user is technically minded, they will be very confused as to why the textured rubber looks like this:
bild1031×1096 59.7 KB
Powered by Discourse, best viewed with JavaScript enabled"
147,bookmarks,"When I try to open a file in 105 beta (2023.1 beta), the “bookmarks” does not show the bookmarks. They are there in the Content window, just not the “Open File” dialog.image1114×529 115 KBThank you for reporting your finding. We will be looking into this asap. Your ticket number is OM-100275. Can you try creating fresh bookmarks and deleting the old ones and seeing if that helps ?Ah, new bookmarks work. It is only old bookmarks. I added “Backups” as a new bookmark.image1192×379 48.4 KBSo your call whether to fix. I can always delete and add the bookmarks back in.Oh, and I noticed the old bookmarks look like documents? They don’t have the folder icon. (They are folders, just like “Backups”). Maybe the bookmarks are there, but being filtered out by the Open File dialog box because they are the “wrong type”.(And yes, deleteing and re-adding the bookmark now works. The icon changes for “My Projects” to a folder, but “Omniverse” (not deleted) is a “file” icon.image747×369 38.8 KBAfter asking the Dev team, this is the response. It seems that older bookmarks may not come through correctly, since we have modifed the code since. It may be best to delete them and recreate them. I am sorry I can not offer an automatic solution.“this issue would happen if the user has bookmarks created from older versions (since we didn’t use to support bookmarking files until 2023.1.0), so previous created bookmarks are treated as files”Thanks for the feedback. It’s not one I am going to lose sleep over (they are quick to recreate), but since file bookmarks were not previously supported, it would have made more sense that previously created bookmarks are treated as directories and not files since they could not have possibly been files before.Powered by Discourse, best viewed with JavaScript enabled"
148,isaac-sim-steamvr-error,"Hi all,
I’m working on a isaac sim simulation where i have created my extension on the latest version 2022.2.1.
Actually i’m facing the problem of enabling the xr profile extension, because the streaming perfectly works but the joypad cannot allow me to nsvigate into the world.
The checkbox of the navigstion is enabled but both thejoypad cannot make me any movement.
Rolling back to the previous version only the 2022.1.1 seems to work.
Somebody have already faced this issue?Hi @vincenzodepaola94  - Someone from our team will review and answer your question.In addition, will it be possible for you to provide video of screen recording of the problem?Hi, I am also experiencing this issue, could you please share some thoughts about this? Thanks a lotHi @sochongl  - It is a known issue in the Isaac Sim 2022.2.1 release. This should be fixed in our upcoming Isaac Sim release.Powered by Discourse, best viewed with JavaScript enabled"
149,is-there-a-way-to-get-friction-forces,"I am obtaining contact forces using RigidPrimView.get_net_contact_forces(), but this appears to not include friction forces. Is there a way to get the friction force between a body that is in contact with the ground?Hi @jake.levy  - In the Isaac Sim, the RigidPrimView.get_net_contact_forces() function provides the net contact forces on a rigid body, which includes both normal and friction forces. However, it does not provide a direct way to isolate the friction forces.Friction forces are typically calculated internally within the physics engine based on the contact normal force, the friction coefficient of the materials in contact, and the relative velocity at the contact point. This information is not directly exposed through the Isaac Sim API.If you need to estimate the friction force, you could potentially do so by making some assumptions and calculations. For example, if you know the total force, the normal force (which can be calculated from the weight of the object), and the angle of the surface, you could calculate the friction force using the equation:Friction Force = Total Force - Normal Force * cos(Angle)Please note that this is a simplification and may not accurately represent the actual friction force, especially in complex simulations with multiple contacts and dynamic conditions.Hello @rthaker - thank you very much for the reply. However, I do not believe RigidPrimView.get_net_contact_forces() is giving friction forces. Here is a little experiment I did to check (but please let me know if I am doing something wrong).I modified the Hello World isaac sim tutorial as follows:Here is the code:Now, when I load the cube in, start the sim, and let it fall to the ground, the reported contact forces are close to [0.0, 0.0, 9.81] N, which is expected for the cube at rest on the ground:
Next, with the simulation paused, I will set the y-axis linear velocity of the random_cube to 5 m/s:
Now, basic physics calculations assuming m=1.0 kg; mu=0.5; g=9.81 ms^-2; v0 = 5.0 m/s result in a change in y of 2.55 m. I let the simulation play out until the cube comes to rest and look at the y-position of the cube:
The cube stops at 2.507 m; close to our guess of 2.55 m. This should validate that friction forces are happening to the cube with a coefficient of dynamic friction of 0.5.However, when I examine the output from get_net_contact_forces() while the cube was moving during the experiment, the contact force in the y-direction is still zero, while the expected force should be (mu)(m)(g) = 4.9 N:
This is what leads me to believe that friction is not being reported as part of the contact forces.@jake.levy
You are correct. The current behavior of the get_net_contact_forces and other contact APIs do not include any frictional effects. We are in the process of including frictional components in the API for the upcoming Isaac sim release.Powered by Discourse, best viewed with JavaScript enabled"
150,create-uneven-terrain-on-isaac-sim,"Hi,Is there a way to create uneven terrain on Isaac Sim that looks natural (not just by placing random shapes on the ground). I’d like to try and create an outdoor environment.Thank you!Hi  @j.t - Can you elaborate on your requirement? maybe an example would be good.Powered by Discourse, best viewed with JavaScript enabled"
151,can-t-open-usd-file,"when I save my file to the .Usd ;then reopen the file ,it can’t be open .  And I try to open the file that demo’s .usd  ,and it encounter  same problem.when i open the .usd file ,then terminal display that “/addr2line : DWARF error : section .debug is larger than its filesize” ,and then the isaac sim shutdown.software specifications :
ubuntu :20.04If there are  any methods to deal with it,please let me know,thanks you.Hi @benson.ee12  - The error message you’re seeing suggests that there might be an issue with the .usd file itself or with the way Isaac Sim is trying to open it. Here are a few troubleshooting steps you can try:This topic was automatically closed 14 days after the last reply. New replies are no longer allowed.Powered by Discourse, best viewed with JavaScript enabled"
152,camera-shake,"Is there any easy way to add camera shake animation in the Omniverse composer itself?This would be a real easy python script / extension to write. Can you tell me a little about what you are wanting to use this for and what you expect to happen before I see if I can do it?Is this to lo look like a human is carrying it and just not holding it still, or do you want it to look more like an earth quake or bombs are shaking the ground?For me its the first option, but procedural and controllable amount of animation noise is kind of more preferable…OK, I will play around with this afternoon and see if I can come up with anything.Hi @Aqualonix  and @DataJuggler !
I would be also very interested in the procedural noise of the camera. Looking forward to hear news from ya!I got busy yesterday afternoon. I will try today.Any hope?I have some time this weekend. I tried to open my project Camera Keys, and I guess I am getting old because I forgot how to open my own project in VS Code.I have a bunch of time this weekend, so let me see if I can get it updated.Sorry, I have too many projects and things I want to learn.Powered by Discourse, best viewed with JavaScript enabled"
153,unable-to-register-pose-annotator-with-replicator-and-omniverse-code,"Hi,I’ve been following multiple tutorials and have a version of the “omniverse code extension” tutorial. I’ve modified it to use my own custom Writer, where the only modification is to also write out pose. I’m getting the following error that the pose node could not be found, but I literally copied the code from an isaac sim offline data generation tutorial and parts of the YCBVideoWriter.Here’s the error:Here’s my custom writer, which works fine until I call the line AnnotatorRegistry.get_annotator(""pose"", ...)Here’s the code for registering a pose annotator:And finally, here’s the part where the writer is created:I believe there must be some issue with running code as an extension vs calling the isaac sim python bash script. I’ve encountered other issues where I tried importing core libraries like “from omni.isaac.core import World” and get an error that this couldn’t be found. Maybe I just need to include the python path?Hi there,could you provide a short script with the steps in to reproduce the error?  If you think you this is caused by import errors, it might be due to omni libraries imported before from omni.isaac.kit import SimulationApp. See 1. Hello World — isaacsim latest documentationPowered by Discourse, best viewed with JavaScript enabled"
154,script-usdgeomxformable-placement-and-visibility-on-timeline,"Hi,I have a few questions and capabilities I’m curious about when it comes to scripting prim placement and visibility in Omniverse.My goal is to toggle visibility (probably with omni.kit.commands as I do not know the direct Omniverse API that handles visibility toggles - please let me know what that is if you’re familiar) of certain objects in a scene at a certain frame number, and to have the visibility reset once the timeline resets. I would like all children to appear/disappear as well.My other goal is to set the position of certain USD references based on the current frame number. This I believe I can do with raw USD via their timeline and xform.The main point of concern for me is that the visibility toggle appears to be an Omniverse function and not something that I can find in base USD.My questions are:Is omni.timeline interacting with the same timeline inherently part of a USD file? I looked at the docs and don’t quite understand how to achieve my goals with what’s documented there.What’s the best way to programmatically toggle the visibility of a prim and its children via python API?What’s the best way to create events or run scripts at a certain frame of play in Omniverse? Or just in pxr USD?I appreciate any advice, thank you!Hi @LMTraina99. This is actually all things that you can do with the USD API and encode directly into your USD data. To understand the frame-based attribute values, I recommend checking out this tutorial: Transformations, Time-sampled Animation, and Layer Offsets — Universal Scene Description 23.05 documentationVisibility is actually a property and not Omniverse-specific. You can see it listed in the Property window. Once you’ve gone through that tutorial, visibility is a little tricky in that you need to use your Usd.Prim object to create a UsdGeom.Imageable object which has the API for setting visibility: Universal Scene Description: UsdGeomImageable Class ReferenceHi Mati,This is just what I was looking for. I will give it a look!Thank you!-MatthewFor anyone with the same question on visibility, you may find that the MakeVisible() function on the UsdGeomImageable class does not behave as desired. It’s probably me screwing something up but here is a useful code snippet as a workaround:currentprim = current_stage.GetPrimAtPath(SOME_SDF_PATH)imageable = UsdGeom.Imageable(currentprim)imageable.GetVisibilityAttr().Set(‘inherited’, Usd.TimeCode(SOME_TIME))
imageable.GetVisibilityAttr().Set(‘invisible’, Usd.TimeCode(SOME_TIME))Usd.TimeCode.EarliestTime()Cheers everyone and thank you again Mati!This topic was automatically closed 14 days after the last reply. New replies are no longer allowed.Powered by Discourse, best viewed with JavaScript enabled"
155,where-to-find-the-linux-version,"Hello,
Where do I find the Linux version of the Omniverse version of Blender?  The Exchange site seems to only have a Windows version.
Thanks.Hi, We don’t publish a linux version but you can download one here: Blender Builds - blender.orgWe recommend you download the 3.6 Linux version and please keep in mind the Connector only works on Windows.You can download the Omniverse Blender add-on for linux here GitHub - NVIDIA-Omniverse/blender_omniverse_addons: Add-ons for Blender for working with NVIDIA OmniversePowered by Discourse, best viewed with JavaScript enabled"
156,how-to-query-sdf-for-cumtomized-shapes,"The use case is that I want to generate SDF samples of an object for training implicit representations. I know that we can ask Isaac Sim to compute SDF, as discussed in Rigid-Body Simulation — extensions latest documentation. But is there any way to efficiently query the SDF values in-operation?Hi btx0424Yes we are planning to have an isaac.core API for the upcoming isaac sim release to efficiently query the SDF values and gradients.Powered by Discourse, best viewed with JavaScript enabled"
157,i-cant-upload-my-usda-file-with-crawl-into-other-usda-file-or-python,"After building my own bot I can’t build it in another usda file via pull or Python, so I’m wondering if anyone has the same problem.thanks.Hi @benson.ee12  - If you’re having trouble building your bot in another USDA file, there could be several reasons for this. Here are a few things you can check:This topic was automatically closed 14 days after the last reply. New replies are no longer allowed.Powered by Discourse, best viewed with JavaScript enabled"
158,wheel-robot-with-4-joints-cant-move-by-differential-controller-using-action-graph-and-ros,"Hi everyone,I followed Isaac Sim tutorials to recreate Turtlebot3 and control their 2 joints it using ROS Noetic by teleop_keyboard node. Until this everything is OK. Then I wanted to follow the same steps with another robot of 4 wheel then 4 joints. But occurred these warnings referring the 2 extra joints.Then I added another Differential Controller to vinculate the extra 2 joints but the same warnings appeared twice and the robot is stopped.
Screenshot from 2022-09-22 19-08-151148×568 89 KB
2022-09-22 23:35:56 [364,737ms] [Warning] [omni.graph.core.plugin] OmniGraph Warning: shape mismatch: value array of shape (1,2) could not be broadcast to indexing result of shape (1,4)
(from compute() at line 110 in /home/catolica/.local/share/ov/pkg/isaac_sim-2022.1.1/exts/omni.isaac.core_nodes/omni/isaac/core_nodes/ogn/nodes/OgnIsaacArticulationController.py)
2022-09-22 23:35:56 [364,740ms] [Warning] [omni.graph.core.plugin] OmniGraph Warning: shape mismatch: value array of shape (1,2) could not be broadcast to indexing result of shape (1,4)
(from compute() at line 110 in /home/catolica/.local/share/ov/pkg/isaac_sim-2022.1.1/exts/omni.isaac.core_nodes/omni/isaac/core_nodes/ogn/nodes/OgnIsaacArticulationController.py)And this is the joint tree of the robot
Screenshot from 2022-09-22 19-09-291852×785 191 KB
Has something similar happened to someone?Hello. You don’t need a second Articulation controller. You have to form a vector of 4 values and feed it to the controller input.I have same problem. anyone this solve it?Thanks for the advise @pme1976 but I’ve tried with a vector of 4 values feeding “Make array” but the problem and warning remains.OmniGraph Warning: shape mismatch: value array of shape (1,2) could not be broadcast to indexing result of shape (1,4)Or I have to feed Articulation controller “Joint names” with another element?I think that warning appears because articulation controller really have only 2 outputs: left joint and right joint. There are 2 extra joints that “cause the problem” and the purpose here is to replicate actions of left_joint to left_joint_2, same thing with the right_joint.Does anyone could control 4 joints with this method (Action graph) or antoher using ROS?In theory you should be able to either put in an array of 4 for velocity command and leave the joint names empty, or you also need to put in an array of 4 for the joint names with all four joints. If that doesn’t work, then can you please attach your usd file and we can take a look at it.Thanks to everyone for your advise! it can be solved creating an array of shape (1,4) from the velocity command output of the Differential Controller. Using 2 “Array Index” to get the values of the velocity command and “Make Array” to create the new array [right_joint, left_joint, right_joint2, left_joint2] or the order that you want. Finally feeding the Articulation Controller.
Screenshot from 2022-09-28 14-56-131028×744 79.4 KB
scout_v2_09_29.usb.usd (8.0 KB)I designed the same as you, but it didn’t work properly.Could you tell me how you set the setting values?
Screenshot from 2022-09-29 15-56-15903×338 38.6 KB
Yep  @donghoon.sejong . The parameters are the followings:And some screens of the configurations:
Screenshot from 2022-09-29 11-06-161060×334 36.2 KB

Screenshot from 2022-09-29 11-05-12983×727 130 KB
I hope that helpsHello,I am facing a similar issue. At first, I was indeed using 2 Articulation Controllers (and I already had in mind this was not the right way to do this), and then I found this thread. I implemented it the same way, and it works exactly the same as with the 2 Articulation Controllers. My problem is that the wheels spin in the correct direction, but the robot does not move accordingly. Instead of doing a spot turn, it creates a very large circle.Any idea what could cause this behaviour?Here is the original post I did before I found this threat here. It is not a duplicate, as I face another issue with the movement o the rover, not only the connection.Any suggestions are welcome.is your robot a 4-wheel drive or a 2-wheel drive with 2 extra wheels that are not actuated? If it’s a 2-wheel drive, then I would recommend to exclude the non-actuating wheels from the articulation and let them be free joints that just rotates. If it’s a 4-wheel drive and it’s not working as expected, then I would start with tuning some wheel friction properties and chassis mass and see if that helps.https://docs.omniverse.nvidia.com/app_isaacsim/app_isaacsim/tutorial_ros2_turtlebot.html#tune-the-robotHi @qwan,The robot is 4-wheeled, and all four wheels are driven. The four wheels are spinning in the correct direction. Just the entirety of the robot is not moving as it should when rotating.I have created a physics material that I attribute to the wheels with a friction coefficient of 1 (static and dynamic) as in the 2-wheeled examples from Isaac Sim.Related to the mass properties, I use the masses of the parts as they were specified in the URDF that I imported. The masses of the parts and the total mass are within reasonable tolerances with respect to the real robot. The robot weighs about 5 kg (in real and in simulation).Would you suggest increasing or decreasing the mass and/or friction coefficients?I would play with the mass and not worry about matching real life first and just to see if it makes any difference. If you want to attach your usd file here, I can take a look at it as well.Here is my USD file of my leorover model. I removed the camera sensors and only added the Action Graph for the differential drive.I use it with ROS2 Foxy on ROS_DOMAIN_ID 55 in case you need this info.I uploaded the USD file to my drive: https://drive.google.com/file/d/1SPNs1v7cnWTvS228nx0o02Du3mY5uFpq/view?usp=share_linkThank you, this solved my problem!Powered by Discourse, best viewed with JavaScript enabled"
159,inconsistent-simulation-speeds-in-movie-capture-extension,"Hello,I am seeking assistance regarding the movie capture extension for my robotics application. My goal is to capture the real-time robot movements, but I have encountered significant variations in the results. Specifically, I have noticed that the simulation appears to run faster in the RTX-Real-Time video compared to the Path Tracing version.To provide more context, both videos were rendered using the same scene and physics parameters for 50 frames, with a frame rate of 24fps. :RTX-Interactive Path Tracing:
RTX-Real-Time:
Those are the Movie Capture settings I used:
settings_path_tracing1341×1470 99.5 KBWhat caught my attention is that the path-tracing version takes approximately 1 minute and 30 seconds to render, while the real-time version takes 7 minutes, both with 10,000 tsps. Could the path-tracing version be skipping physics steps?Furthermore, I noticed that the Frame rate attribute only affects the encoding. As an example, I have captured the same scene as above using path-tracing for 50 frames, but with a frame rate of 60fps:In this case, instead of observing half of the movements as expected, the simulation appears to be sped up (since only the encoding, not the simulation itself, is affected by the change in frame rate). Is this intentional?I am uncertain how to obtain accurate timing with a render. Based on these results, it is challenging for me to determine the robot’s real-time movement speed.Here is the usd file of the scene:
example_gripper_collisions_3.usd (14.8 MB)Kind regardsAxelHi @axel.goedrich  - The difference in simulation speed between the RTX-Real-Time and Path Tracing versions is likely due to the difference in computational complexity between the two rendering methods. Path Tracing is a more computationally intensive method that simulates the physical behavior of light, which can result in more realistic images but at the cost of longer rendering times. On the other hand, RTX-Real-Time rendering is optimized for speed and can produce high-quality images much faster, but it may not be as accurate in terms of light simulation.As for the Frame rate attribute, you’re correct that it only affects the encoding of the video, not the simulation itself. The simulation runs at its own pace, independent of the frame rate of the video. When you change the frame rate, you’re changing how many frames are captured and encoded per second of video, but you’re not changing how many simulation steps are performed per second. This is why the simulation appears to be sped up when you increase the frame rate: you’re seeing more frames of the simulation in the same amount of video time.If you want to capture the real-time movements of the robot, you might need to synchronize the simulation time with the video time. This could involve adjusting the simulation step size or the simulation speed to match the frame rate of the video. However, this can be a complex task that requires a good understanding of both the simulation and the video encoding process.Hi @rthaker,The difference in simulation speed between the RTX-Real-Time and Path Tracing versions is likely due to the difference in computational complexity between the two rendering methods. Path Tracing is a more computationally intensive method that simulates the physical behavior of light, which can result in more realistic images but at the cost of longer rendering times. On the other hand, RTX-Real-Time rendering is optimized for speed and can produce high-quality images much faster, but it may not be as accurate in terms of light simulation.I get that for the viewport, but I was hoping that the Movie Capture-extension would step the simulation so that all physics steps are executed and the frame rate is the same as defined.
For example, if I set the simulation time steps per second (tsps) attribute to 1000 and the frame rate to 25, I would expect that the simulation would execute 40 physics steps, then one render update, another 40 physics steps, one render update, and so on. 25 frames should then equal one second of simulation time (just for clarification, the time that is passed in the simulation, not the time needed to calculate it).
If this were the case, the choice between path-tracing or rtx-realtime would only affect the look and rendering/calculation time to generate the video, not the simulation speed.Interestingly, contrary to your description and my expectations, the video rendering was actually faster with path-tracing than with rtx-realtime. Maybe this is just a glitch caused by the high number of tsps (10000) and some skipping of physics steps:What caught my attention is that the path-tracing version takes approximately 1 minute and 30 seconds to render, while the real-time version takes 7 minutes, both with 10,000 tsps. Could the path-tracing version be skipping physics steps?We mainly want to use the simulation for reinforcement learning, with some camera-guided tasks as well.
So if I want that all physics steps are executed and that the camera feed records the correct simulation speed, I would probably have to step the simulation manually and then trigger a frame/rendering update after x physics steps?Hi @axel.goedrich  - Do you mind sharing the RT (Ray-Tracing) movie capture settings which were used in your case?Hi,
those are the RT (Ray-Tracing) movie capture settings I used:settings_path_tracing1341×1470 99.5 KBsettings_path_tracing1341×1470 99.5 KBI left all other render settings on default.Regarding the physics simulation, I set the tsps (Time Steps per Second) attribute in the physics scene to 10,000.If you want to reproduce this, you can use the the USD-file I uploaded:Here is the usd file of the scene:
example_gripper_collisions_3.usd (14.8 MB)Hi @axel.goedrich  - The setting you shared is for path tracing. Do you mind update that to ray tracing?
image993×215 32.5 KBHi @rthaker,
do you mean RTX-Real-Time? Here are the settings for that:
settings_real_time1334×1276 76.4 KBPath-Tracing and RTX-Realtime are the only two options I can choose in Isaac Sim.Powered by Discourse, best viewed with JavaScript enabled"
160,lula-rrt-support-for-orientation-targets-and-mesh-collision-avoidance,"Hello Team,We wish to use the Isaac Motion Planning interface, specifically the Lula RRT motion planning feature in Isaac 2022.2
However, there are two key features currently missing:When do you think these functionalities will be added? Will they be available in the next release?Hi Snehal,While we’re planning to add both features in the future, they are not currently planned for the next release.  The earliest they might be available is late this year.Hello team,In that case, is it currently possible to use the LulaIK solver in a collision-aware manner?We need some fast, collision-free behaviour from the robot. Just an IK solution is also enough for us but we need it to be collision-awareRegards,
Snehal JauhriHi Snehal,We don’t provide collision-aware IK out of the box, unfortunately.  One option would be to use RMPflow to roll out a trajectory to your desired end-effector pose.If your robot happens to be 6-DOF, another might be to implement your own collision-aware IK by:A similar approach might work even if your robot is 7-DOF, but since the space of solutions is continuous, I fear that step (1) wouldn’t do a good job of sampling the space.My suggestion would be to try RMPflow first, since that should be easiest.  Note that after rolling out a trajectory with RMPflow, you could discard the time information and just use the resulting path in the same way you might use a (smoothed, densely-sampled) path from RRT.Alright, thanks @BuckBabich. I will try this. Is it possible to parallelize the provided RMPflow controller across multiple environments? (I will try to run it with Omniverse Isaac Gym)Regards,
Snehal JauhriWe don’t currently provide a parallel (batch-style) interface to RMPflow or the other Lula algorithms, but I’d suggest trying a separate instance per environment to see whether it’s a performance bottleneck.Powered by Discourse, best viewed with JavaScript enabled"
161,many-holes-in-point-cloud-reconstruction-with-dmtet,"Hello.
I’ve been trying to mesh my own point cloud data using the jupyter notebook code provided in kaolin’s examples material (dmtet_tutorial.ipynb).The point cloud I’m using comes from a .obj file. I’m just importing the vertices as the reference points to send to the regularizer.
I can see the point cloud using kaolin-dash3d and it seems ok.The only part of the code I’ve changed was the lines to import the vertices:Any other part of the code is exactly the same as the original.I’m attaching an image with the reconstructed mesh (a candle) after 5000 iterations. I’m also showing the bear example using the same code after the same 5000 iterations.Any tips why the tessellation is full of holes?
Many thanks!
Screen Shot 2023-06-19 at 19.04.271858×1020 103 KBHi @marcostrinca. Welcome to the forums! I’ve moved this over to the Kaolin forum where someone should be able to help you.Hi @marcostrinca , Thank you for your interest in Kaolin. Do you mind transfering your request to Issues · NVIDIAGameWorks/kaolin · GitHub? This forum is more dedicated to the Omniverse part of Kaolin.From the look of it on intuition would be that your point cloud is too sparse w.r.t to the tetrahedral grid. could you have a more dense point cloud? Otherwise I can add some coarse grid as initialization.Powered by Discourse, best viewed with JavaScript enabled"
162,livestream-3d-scene-composition-with-usd-composer-wed-may-31-11am-pst,"
new_colors400.17831920×1080 51.9 KB
Join us as we invite members of the community and NVIDIA team members to discuss the latest Community Challenge, #SetTheScene, and their submissions. Guests include Omniverse Ambassador from the Community, Pekka Varis, Tanja Langgner, Stephen Tong, and Frank Sheng!For more details on the Community Challenge, see: Show Us How You #SetTheSceneWe’ll be broadcasting to:
YouTube: 3D Scene Composition with USD Composer - YouTube
Twitch: TwitchAdd to your own calendar:Join us as we invite members of the community and NVIDIA team members to discuss the latest Community Challenge, #SetTheScene, and their submissions. For more info: https://forums.developer.nvidia.com/t/show-us-how-you-setthescene/252883We...Powered by Discourse, best viewed with JavaScript enabled"
163,how-to-get-distance-to-the-deformable-body,"I want to set distance senor to robot. I used raycast_closest(origin, rayDir, distance) and get distance to the rigid body, but I can’t get distance to the deformable body.Frequently Used Python Snippets — isaacsim 2022.2.1 documentation (nvidia.com)On the other hand, I found another API which called ray_cast().
Core [omni.isaac.core] — isaac_sim 2022.2.1-beta.29 documentation (nvidia.com)
My code is here.But, I have the following error.What should I do to get distance to the deformable body?
Please tell me the solution.Hi @Yuya_t - The error message you’re seeing is due to a mismatch in the argument types for the Gf.Quatf function. This function is used to create a quaternion, which represents a rotation in 3D space. The error message indicates that the function expects either a single float (for a quaternion representing no rotation), four floats (representing the real part and the i, j, k imaginary parts of the quaternion), or a GfVec3f object (representing the imaginary part of the quaternion).In your code, you’re passing in three floats to Gf.Quatf, which is not a valid set of arguments for this function. If you’re trying to create a quaternion from Euler angles, you can use the Gf.Rotation class instead:Here, angle is the angle of rotation in degrees.As for getting the distance to a deformable body, you can use the ray_cast function in the same way as you would for a rigid body. The function should return the distance to the closest point on the deformable body that the ray intersects. If the ray does not intersect the deformable body, the function will return a large value (e.g., inf).If you’re not getting the expected results, make sure that the deformable body is correctly set up and that the ray is being cast in the right direction. You might also want to check if the deformable body is included in the collision group that the ray_cast function is checking against.Thank you for reply @rthaker !I changed the program, and new error has occurred.
My new code is this.This is new error message.Any idea why I got this error?And also I want to check if the deformable body is included in the collision group that the ray_cast function is checking against, please tell me how to do.Hi @Yuya_t  -  The error message you’re seeing is due to the fact that the ‘Quatd’ object does not have a ‘tolist’ method. The ‘Quatd’ object is a quaternion, and it seems like the ‘ray_cast’ function is expecting a list or array-like object, not a quaternion.To fix this, you can convert the quaternion to a list or array. Here’s how you can do it:As for checking if the deformable body is included in the collision group that the ray_cast function is checking against, you would need to check the documentation or the implementation of the ‘ray_cast’ function. If the function does not provide a way to specify the collision group, you might need to modify the function or create a new one that allows you to specify the collision group.If the ‘ray_cast’ function returns information about the object that was hit, you can check if the hit object is the deformable body. If the function does not return this information, you might need to modify the function or create a new one that does.I add your opinion to the program, but get the error that seems to be the same.Why I got the same error?
However, if you prepare quaternions directly instead of Euler angles, the error did not occur.Also, is it possible to visualize ray_cast() like Lidar?Hi @Yuya_t  -  The error message you’re seeing is due to the fact that the ray_cast function is expecting a numpy array, not a list. When you convert the quaternion to a list and pass it to the ray_cast function, it’s still a list, not a numpy array. You can fix this by converting the list to a numpy array:As for visualizing ray_cast like Lidar, it depends on the capabilities of the ray_cast function and the rendering system you’re using. If the ray_cast function returns the points that the ray intersects, you can use those points to create a visualization. For example, you could create a line or a series of points at the intersection points. If the ray_cast function doesn’t return the intersection points, you would need to modify the function or create a new one that does.If you’re using a rendering system that supports it, you could also create a visualization of the ray itself, not just the intersection points. This would give you a visualization similar to Lidar. However, this would likely require a more complex implementation and a deeper understanding of the rendering system.Thanks to you the error is resolved, but it doesn’t seem to be able to detect deformable objects.I confirmed that a rigid body can be detected with ray_cast, but it could not be detected even if a deformable object was placed at the same position. The sensor’s ray is being cast in the right direction.Is there something wrong with my flexible settings? Below is my flexible settings.If there is no problem with flexible settings, I think the ray_cast collision group is the cause, but there was no information about that in the document below.Core [omni.isaac.core] — isaac_sim 2022.2.1-beta.29 documentation (nvidia.com)What should I do to solve the probrem?Powered by Discourse, best viewed with JavaScript enabled"
164,streaming-a2f-in-machinama,"Asked in the Machinama forum with no response so trying the A2F forum. I have a CC4 character transferred and working with the streaming player in A2F. I’d like to use that setup within Machinima to add additional animations etc. I see tutorials on how to export USD cache of the default player and bring those into Machinima, but I don’t see any documentation on how to use the streaming A2F player in Machinima. Is that possible? Happy to script this if needed if you can point me in the right direction. Thanks.Currently this is not available out of the box in Machinima. The plan is to include this functionality inUSD ComposerThis topic was automatically closed 14 days after the last reply. New replies are no longer allowed.Powered by Discourse, best viewed with JavaScript enabled"
165,importing-extensions-into-nvidia-omniverse,"I am having issues importing an extension into Nvidia omniverse code 2022.3.3. Won’t let me import, varying error messages from ‘combobox constructor: argument is not a str’,  ‘extension.toml’ does not existHi @tedmellow. Welcome to the forums! What is the extension you are trying to import and what steps are you taking.Powered by Discourse, best viewed with JavaScript enabled"
166,audio2face-working-on-quadro-p5000,"Hello! Anyone knows if it,s possible to run audio2face on a Quadro P5000? I tried but character doesn,t  appear on viewport. Thanks.Hi @mariosantanamusic  and welcome to the forums!Unfortunately, Quadro P5000 doesn’t support RTX and Audio2Face requires and RTX graphics card to work properly.Hello. Thanks so much for answering.This topic was automatically closed 14 days after the last reply. New replies are no longer allowed.Powered by Discourse, best viewed with JavaScript enabled"
167,maya-connector-wont-install-for-thin-client-installations,"Hello nvidia,we run maya as a “thin client” meaning maya does not get installed on every workstation, just copied over. OV launcher wont detect these types of installs, so the installation of the connector fails. Is there a way to manually download and install the connector?thanks,
KoenCurrently there is no manual way to install it. But if you want to control it, you can copy the binaries by finding it at C:\Users\user name\AppData\Local\ov\pkg. Then you can manually copy it to other PCs and run MayaSetup.exe in the root folder on the other PCs.
MayaSetup.exe can use argument “/INSTALL” in command line mode. Or “/UNINSTALL” to uninstall.Powered by Discourse, best viewed with JavaScript enabled"
168,turtlebot-joint-velocities-dont-match-the-desire-target-velocity,"Hi everyone,
I have some issues on using turtlebot robot in isaac sim and it would be appreciated if you can provide any comments. I noticed that the turtlebot joint velocities don’t match the desire target velocities in velocity control mode. To reproduce I followed:
1-  used this tutorial to import the turtlebot urdf and create a usd file.
2- used this tutorial and used the turtlebot usd instead of Jetbot for reinforcement learning example (robot is imported as wheelrobot class).
3- set the velocity using “apply_wheel_actions(ArticulationAction(joint_velocities))” function.
4- check the velocity using “get_joint_velocitie” function.ThanksI had the same paroblem, when I applied velocity 10.0 to a joint in multiple consecutive time step, but I 9.xxx vel after the first step with get_joint_velocitie(), then got 5.xxx vel in subsequent time steps.
Did you solve the problem?Hi - Apologies for the late response. Are you still having any issues with the latest Isaac Sim 2022.2.1 release?Yes, I still encountered this problem. I gave the car a speed of 0.2m/s, but I used Inspect Physics and saw that the speed was around 0.14m/s. I tried to change some physical parameters, such as damping and mass, but nothing changedHi- The discrepancy between the applied joint velocity and the actual joint velocity you’re observing could be due to several factors:If all of these factors are checked and no issues found then let me know and I will connect you with the right dev to look into it.Powered by Discourse, best viewed with JavaScript enabled"
169,add-noise-to-the-camera-animation,"Hi all,
is there any way to add noise to the animation curve of the camera?
I would like to add some very subtle walking effect to the camera movement to add more realism.I know it can be done in Houdini for example but then I am not sure how well the camera export would work from there, ideally I would like to keep as much of the workflow inside the USD Composer.Have a great day !
KarolPowered by Discourse, best viewed with JavaScript enabled"
170,advanced-skeleton-is-not-working-with-audio2face,"Hello I am using Advanced Skeleton version 6.043 and I am trying to use the Audio2Face Option I have all the files and folders set up as mentioned in the tutorial but when I hit apply it gives me an error message.“C:/Users/User/Downloads/Compressed/AdvancedSkeleton/AdvancedSkeletonFiles/Selector/biped.mel line 5296: No object matches name: Lip_R.zip”I tried searching online but I couldn’t find a solution, please advise!Best RegardsI am using Maya 2023.3This is the tutorial link Audio2Face - YouTubeHi @dannytareen21I’m not familiar with AdvancedSkeleton, but Audio2Face has been updated since a year ago and that AdvancedSkeleton video might be outdated.I’d suggest asking AdvancedSkeleton team to see if they need to update their scripts.Powered by Discourse, best viewed with JavaScript enabled"
171,not-able-to-open-any-nvidia-assets-on-the-omniverse-create-or-isaac-sim-apps,"Hi everyone,
I am setting up the Ubuntu 20.04 Workstation for running Omniverse workloads. So, for that after installation of NVIDIA Driver, Omniverse Launcher, and Client Apps, I am facing an issue with opening NVIDIA Assets on the Create and Isaac Sim applications. I am getting the following Warnings and Errors on the Console:And please find below the full logs file:
create-logs.txt (907.5 KB)Also, on running Isaac Sim in Terminal, I tried running the command ./python.sh standalone_examples/replicator/offline_generation.py. I am getting the following error:System specifications:@WendyGram Can you help us in directing this to the concerned person or team for the support on this issue.Any solution for this? I’m facing the same problem on Ubuntu. Also I noticed that cache doesn’t seem to be working either:[omni.client.plugin]  Tick: provider_nucleus: CC-493: Request through cache failedI am also having the same issue where none of the omniverse provided assets are loading (also on Ubuntu 20.04 with similar PC specs). Whenever a USD is attempted to be added to the scene, the isaac sim terminal shows errors similar to these:[Error] [carb.imaging.plugin] STB Failed to load image info: Image not of any known type, or corrupt
[Warning] [gpu.foundation.plugin] Couldn’t process the file, it might not have written completely.
[Error] [gpu.foundation.plugin] Failed to load texture omniverse://localhost/NVIDIA/Environments/2023_1/Templates/.thumbs/256x256/desert_road.usd.png (reason: 3)
[Warning] [omni.ui] Failed to upload UI Image ‘omniverse://localhost/NVIDIA/Environments/2023_1/Templates/.thumbs/256x256/baybridge.usd.png’Powered by Discourse, best viewed with JavaScript enabled"
172,a-way-to-secure-the-source-of-the-extension,"Hi,
I found a similar question from last year, but maybe something has changed in that time.
I am trying to find the correct way to copy-protect the source of my extension. The extension will be installed on a pre-configured computer/VPS server that will be shared with our end client.Powered by Discourse, best viewed with JavaScript enabled"
173,how-to-get-object-geometry-properties-in-a-scene,"I have loaded randomized primitive objects in a scene using replicator API. What I want to do is to be able to get the SDF information of the scene created.For this, my thought process is to be able to somehow convert the primitive object of type [omni.replicator.core.create.cube or something similar] to a mesh object which is readable by open3D for me to compute SDF.I have the following questions:Any help would be greatly appreciated.
Thanks!Hi @abalram1  - The Omniverse Kit and Isaac Sim do not currently provide a direct way to convert a primitive object created with the Replicator API to an Open3D mesh object. However, you can use the USD API to export the scene to a .usd or .usda file, which can then be converted to a format that Open3D can read, such as .ply or .obj.Here’s a basic example of how you might do this:You would then use Open3D to read the .ply or .obj file.As for obtaining SDF information within Isaac Sim itself, the PhysX extension provides a way to compute SDFs for triangle meshes, which can be used for collision detection. However, this is not directly accessible through the Python API and is used internally for physics simulation.If you need to compute SDFs for use in your own algorithms, you would likely need to use a separate library or tool that provides this functionality, such as Open3D or PyTorch3D. You would export your scene or mesh as described above, then load it into the other tool to compute the SDF.Powered by Discourse, best viewed with JavaScript enabled"
174,for-loop-with-orchestrator-in-replicator,"How to wait for orchestrator to finish before making another cycle of the for loop in a script like this:To run this script I use the headless method
.\omnicode.code.sh --no-windows --omni/replicator/script=/home/user/test.pyThis renders only the cube because it is the last cycle and no new orchestrator.run() is called.I updated the script with a working one, so you can try by yourself.
I realy hope to find a solution to this problem because I think it’s essential for this software to be able to run multiple scenario with one script.Hello @glonor!  I’ve shared your post with the dev team for further assistance.Hello @WendyGram, is there any update? We are facing the same issue. Basically, the replicator populates all layers and renders them at the same time. What we are hoping to achieve is to render each layer separately without restarting replicator.
Thanks,
MateuszI encountered into the same issue, too. Here’s my workaround though I don’t know if it’s correct to do it.
To wait for the replicator, the function / script should leverage the async mechanic in python.Since my modification of nodes are hard to achieve in rep.modifiy, I implement the modification and render 1 frame in a round.Hope this help :)Hi, could you please share a working code snippet that implements the workaround you mentioned. It would be really helpful if you could provide a code example similar to the one I shared in my first post that successfully renders the three shapes.Thank you in advance for your support and contributions.There you go :)
I run the script in OV UI’s script windowAt first, I tried the headless method, but it didn’t work out. Then, I noticed that you used the UI and decided to try that approach, and finally, it worked.  Thank you for your help. I really appreciate it.I am trying to do a similar thing, but i would like to run omniverse headless. The problem is when i run it headless, that i starts the next run of the for loop, before the rtx renderer starts.Are there any updates on how to accomplish this when running omniverse in headless mode?I was able to solve it by removing num_frames and using step_async() + stop() instead of run_until_complete_async()If I run the script like this, it works for me:This topic was automatically closed 14 days after the last reply. New replies are no longer allowed.Powered by Discourse, best viewed with JavaScript enabled"
175,luxurious-penthouse-archviz-animation,"Hi all,
just wanted to share with you a work-in-progress animation we are working at the moment. It will be the the 3rd archviz animation with Omniverse I do.
It is a highend luxiorious penthouse on in NYC.What I am planning to add to it is.It is still missing a lot of material work / objects.
Looking forward to your feedback!
Powered by Discourse, best viewed with JavaScript enabled"
176,instance-segmentation-not-stable,"Hello all,I am using a standalone application to record some synthetic data. In order receive the groundtruth, I use the “get_groundtruth” function from SyntheticDataHelper class.
For example:Everytime, when the simulation is restarted, I realized that the instance_segmentation_array has different amount of pixels that are labeled 0. The label zero is not included in the output array given by get_groundtruth function. So I plotted the zero points whenever the simulation is restarted using this array and matplotlib.pyplot:The result is as follows:
Figure_41920×1003 39.6 KB


As you can see, these zero points change after restart, even when there is no change done either to the scene or to the code.What could be the reason for that behavior? How do you suggest that we solve the issue? We would actually need stable data between “takes”, so to say.Thanks in advance!Sincerely, upyzmP.S.: I am using the version 2022.2.0.Hi @upyzm - Is there a reason you are not on the latest Isaac Sim 2022.2.1 release?Can you try that and let us know if the issue still persists.Can you provide a short snippet on how you setup and restart your scenario? I would also recommend using Replicator API for getting synthetic data:Thank you for answering. I attached a code snippet with the issue.
The environment is a reference to: omniverse://localhost/NVIDIA/Assets/Isaac/2022.1/Isaac/Environments/Simple_Warehouse/warehouse_multiple_shelves.usd . A Carter robot is added to scene with an additional custom sensors setup: 3 Cameras and a Lidar at the same position, orientations vary.
I use a standalone application so that I can fuse the groundtruth provided by cameras with the point cloud provided by the Lidar. I wasn’t able to get the point cloud via replicator. Besides, I’d like to do the fusion online, if possible.@rthaker - The issue persists with the newest release as well. I had not migrated my workspace to the newest version, that was the reason why.simulation_lidar_snippet.py (6.5 KB)For getting the point cloud you can use the pointcloud annotator. In the examples you would for example need to change:
rep.AnnotatorRegistry.get_annotator(""rgb"") to rep.AnnotatorRegistry.get_annotator(""pointcloud"")I’ll try that to see if it works for our use case, thanks. Were you able to replicate the problem from the snippet I send you?Powered by Discourse, best viewed with JavaScript enabled"
177,replicator-interactive-sdg-fruit-box-tutorial-on-local-machine,"I am trying to run the fruitbox demo in the Script Editor of Omniverse Code. However, the fruits are not shown in the Nucleus. Further, I see this message regarding collision.Do you know why fruit props are not visible?Screenshot from 2023-07-04 20-14-051243×697 64 KBThe full code is:this is really strange, search doesn’t work (previously it worked for other things like forklift)Screenshot from 2023-07-04 20-18-341920×1065 179 KBHowever, it is somehow stuck in running phase and no image is produced.
I wonder if it may have anything to do with the collision warning?Here’s output of $ tiptop:
Screenshot from 2023-07-04 20-20-451913×862 209 KBAre there any errors or warnings in the log?I tried running and saw the warning: 'function' object has no attribute 'camera'
So I had to change camera = rep.randomizer.register.camera() to camera = rep.create.camera() and the script ran, but generation had other errors.Ok, after a bit more debugging, I found that there was an issue with the way visibility was being set.I changedtoAnother issue on my end at least was running on Windows with ‘:’ characters in the output path is not allowed, so I removed now from the output_dir and was able to run and get output.eg.
rgb_00001024×1024 40.1 KB@ecameracci thanks a lot for trying this. With using both your changes, it works now.
However, the collisions are not realistic. Do you know how it can get fixed? For example the pomegranate and avocado are spawn in middle of the fruitbox which is not realistic. Thanks for any feedback.
Screenshot from 2023-07-12 12-25-541920×1080 68.8 KBHi, Mona :)Which version of replicator are you using?Unfortunately, I do not believe that these collider commands on the crate (or even adding colliders to the fruit in the box) will have any affect on how they are spawned. I believe the root position of each is just sampled randomly within the x,y,z coordinates you specified in rep.modify.pose and it is placed there. So it’s possible the fruits could initially overlap each other too.One way you could get the fruits to not initially overlap each other is to use the scatter3d function, which has a flag where you can set collision checking to True. For this, you would want to create a cuboid representing the inside of the box, and use that as an arg to scatter3d (as well as the fruit). However, replicator 1.6.3 (which you are likely using) does not have a way of checking for collisions with the hull of the box. So as a workaround you could specify the cuboid for the inside of the box to be slightly smaller, so it is a distance “r” from the inside of the box, where “r” is the radius of the largest piece of fruit.Let me know if this makes sense and is a feasible solution.Replicator 1.9 makes all of this easier, and does collision checking with as many other shapes as you want – and it also can prune the sampling space to make sampling more efficient. I think I made some hacked code for doing this with 1.6.3 but it’s kind of messy and it currently is just for scatter2d.Sneek peek at 1.9 scatter3d function attached … where cones are prevented from initially colliding with the inner blue cylinder.-Henry
Screenshot from 2023-04-28 09-59-46885×709 167 KB
Screenshot from 2023-04-28 10-08-18843×801 371 KBThis topic was automatically closed 14 days after the last reply. New replies are no longer allowed.Powered by Discourse, best viewed with JavaScript enabled"
178,what-music-interferes-with-instructors-audio-adam-moravansky-tutorial-physics-ext-kit-104,"Dear Nvidian content creators,
This is a nice tutorial series. Well laid out and seems like a must follow for people trying to get familiar with the basics of physics in Omniverse. Particularly useful for folks wanting to participate in the Jetracer project.

Part 1 of the series is spot on, good pace, clear instruction, ideal beginner example w/o no real setup required. All around great -  so I was looking forward to dedicating the time to work through the next 23  tutorials in the series.
However in Part 2, music is introduced in the background.
In Part 3, the music is almost as loud as Adam.
As a video person, I totally understand the temptation to have the music in, but I believe it takes away from the clarity of the instruction here, which in instructive vs. cinematic. In this case, less, can be more.
Music is totally appropriate for the opening and close and I think that is a great place to express that desire to enhance the user experience and express one’s artistic self.
Also I know a lot of people work with their own music in the background, so leaving the music out might be helpful for them.
Following tutorials in near real time requires concentration, since every single step is new. Imagine a baby taking steps trying not to fall over. As beginners, we walk on uneven ground.It’s an ask, but for the sake of all the good work that has already been done on this series, and the good these videos can do for the community, I’d request that these video’s be re-exported sans the music track under the instruction.
Not to be confused with disabling Adam’s audio track. He’s great.
Otherwise it is shaping up to be a great series.Thanks,
-ZiaThanks Zia! I’ll share this feedback with Adam and the team.Hi Zia,you are of course completely right about this, the music has been added on top is unhelpful.  If you really have trouble understanding any part of the tutorial I can share with you the raw video recordings before any professional grade value add was performed, just let me know.You are absolutely right Zia, there are a few episodes that have some audio issues. We are fixing them now. Thanks for reporting it back and we are moving to not adding any audio at all in our upcoming template update.Thanks so much. I really look fwd to the new release. In going through them I also noticed that there was one misnamed file and also maybe one or two segments that didn’t have a distinct ending… At least one where the narration seems to trail off.
If it is helpful, I can probably locate the segments in question.
Thanks again for taking the time and putting forth the effort to get these details right so that everyone can easily benefit from these excellent lessons.
Much obliged!
Cheers,
-ZiaHi Adam,
Thanks so much for taking the time to reply. 'Really enjoy the content and your excellent narration!
Thanks to the magic of AI nowadays, I was able to isolate the voice track and drop the music on my end - so that was a local solution.
I love cinema and scores… so I was a bit hesitant to leave a note but for my approach to learning, I find I most benefit from either silence or sometimes baroque music in the background. So I appreciate keeping this aspect ‘modular’ and customizable!
'Very exciting to have such a responsive team at Nvidia whose willing to address this. Thank you one and all!This topic was automatically closed 14 days after the last reply. New replies are no longer allowed.Powered by Discourse, best viewed with JavaScript enabled"
179,omni-anim-people,"I currently use Isaac sim 2022.2.0 and the latest documentation of Omni.Anim.People shows a version 1.6 whereas my version shows only 1.4. I could not work with the extension as the versions are mismatching and I am not able to update the latest extension version.Hi @ashin.anandakrishnanTwo possible solutions are:Thanks for the clarification @toni.sm
But Even when i enabled the extension and set to autoload, the extension seems to be toggled off each time when i start a new simulation. Any suggestions would be helpfulHi @ashin.anandakrishnan  - You can find more information in "" 3.1. Enable Omni.Anim.People""3.1. Enable Omni.Anim.PeoplThanks for the info.Powered by Discourse, best viewed with JavaScript enabled"
180,wrong-credentials-or-the-user-does-not-exist,"Hi.
I tried 10 times to launch Omniverse NUCLEUS at localhost,  but i failed login by admin account.
I double checked my id and password when i writ it at the start building admin account.
I attach my log file, my Omniverse NUCLEUS version is 2022.4.2
Nucleus.log (433.5 KB)I found a solution
https://docs.omniverse.nvidia.com/prod_nucleus/prod_nucleus/usage/auth_user_mgmt.html#use-a-system-account
login this and change admin’s password(using reset password by link copied @clipboard) what i wantPowered by Discourse, best viewed with JavaScript enabled"
181,omniverse-create-when-using-movie-capture-resource-usage-is-very-low,"We are currently busy with a project where we require a fly through of a particular model with a system that sports 128GB of RAM and a 4090RTX GPU and a Ryzen 9CPU with the typical bells and whistles one would expect with such a system, when capturing a movie after defining camera paths we notice that regardless of the render settings we use, the CPU and GPU usage is very low, we have bumped up the GPU cache to 10GB, but when we hit render the CPU and GPU never cross the following values:CPU - 16%
GPU - 16%Is this normal? and if not how do i get omniverse to use all the available capacity we have, currently it feels like im driving a ferrai in first gear trying to get this video out the doorHi,
We have found that the Windows task manager often does not convey the proper resource utilization. In any kit-based app, you can go to Window>Utilities>Statistics and switch the panel to “GPU utilization” in the top right drop-down menu.In some cases, if the scene is relatively simple and the frame renders are very fast the GPU might be waiting for the images to get saved to disk.Can you try an included scene like EuclidVR_Stage to see if it’s a system issue or if it may be related to your file? You can find it in the “Samples” tab in the lower half of Create. If Euclid also renders using minimal resources, can you provide the render settings used?Powered by Discourse, best viewed with JavaScript enabled"
182,lots-of-isaac-sim-links-broken,"Lots of links related to Isaac sim are broken, including top Google results for things like “isaac install” and “isaac sim requirements”The broken links look like:https://docs.omniverse.nvidia.com/app_isaacsim/app_isaacsim/requirements.htmlI reported this to the atlassian email address, but thought I’d put it here too so others could track it.Please fix, thanks!Hi @dan.sandbergThis post may help:Still broken – example:https://docs.omniverse.nvidia.com/isaacsim/latest/install_workstation.htmlClick on “Install Omniverse Launcher”You get a 404.  This kind of thing makes it very difficult to get started with Isaac Sim.Hi @dan.sandbergWhile the broken URLs are being fixed, you can manually modify them according to the following pattern:FromtoExample:with MAIN=launcher and PAGE=installing_launcher.html, then the fixed URL is:https://docs.omniverse.nvidia.com/launcher/latest/installing_launcher.htmlOk, that’s great, what about URLS like this one?https://docs.omniverse.nvidia.com/prod_digital-twins/app_isaacsim/advanced_tutorials/tutorial_configure_rmpflow_denso.htmlThanks!-DanHi @dan.sandbergIn this case, just replace prod_digital-twins/app_isaacsim with isaacsim/latest as followhttps://docs.omniverse.nvidia.com/isaacsim/latest/advanced_tutorials/tutorial_configure_rmpflow_denso.htmlThank you @toni.sm for your help.@dan.sandberg  - Apologies for any inconvenience  with the docs. As I have mentioned in other thread, the docs platform was transitioning to new one caused the broken link issue.The issue is now fixed but if you find any broken link then feel free to reach out us and we will try to fix them ASAP.Powered by Discourse, best viewed with JavaScript enabled"
183,how-to-assign-texture-map-to-material-in-python,"Through Python, I’m trying to assign a texture to a currently empty normal map input in an mdl that was generated from converting an obj to usd. I’ve pretty much copied the code from the ‘Commands’ window after assigning it manually, but I can’t get it to assign through code, even if I leave the default path. I think the problem lies with ‘.inputs:normalmap_texture’
An example of printing the path I’m using would look like this (material and shader are both named Material_000001):I’ve been looking everywhere (for days) to find some documentation for Python texture assignments in Omniverse specifically, but I’ve found nothing.I’m fairly confident the error isn’t because of some logic outside this function, because immediately after I call it, I also call a function to enable opacity on the mdl which works just fine.Edit: I’ve opened up the AlderLarge_01_autumn.usda file and it looks like there are a list of inputs that can be controlled (like inputs:enable_opacity), however, there’s no sign of normal map inputs, which I’m guessing is why the code doesn’t work.
I’m guessing Omniverse creates the normalmap_texture input automatically when manually assigning the texture file in the editor, however, that code is not traced in the ‘Commands’ window. Anyone know how to do this? I’m trying to do this for 800+ files.Edit 2: I’ve created an attribute just before I assign the texture:The ‘Normal’ dropdown window in the properties panel has now moved from around the bottom to basically the top after adding that code, so I’m guessing I successfully assigned the attribute, however, I still can’t get the texture to assign.Hi @drkuzmick. Arter you create the attribute can you try .Set() on the attribute object that CreateAttribute() returns. Maybe it’s an issue with ChangeProperty command.Powered by Discourse, best viewed with JavaScript enabled"
184,blender-3-6-exchange-version-to-omniverse-to-unreal-engine-5-2-with-nucleus-localhost-does-not-seem-efficient,"Could someone please take a look at my workflow and tell me if there is a way to use omniverse that is better than a traditional FBX/GLTF/USD export please?I should also add I know that I can right click on the USD in Unreals content browser and then Import USD to convert it over to a static mesh. But this removes all the benefit of using Omniverse to me. As I was hoping I could use Omniverse to build a level and place meshes in unreal to say for example block out a level for a game or for virtual production but then later down the line replace those USD files with more polished assets and just with a few clicks of an omniverse button see those changes propagate into the Unreal engine viewport. If I have to manually convert the USD to a static mesh and then place the actors again it seems pointless to use omniverse. I should just stick to using Blenders default USD export or FBX right?Please let me know if I am missing any info as when using Omniverse and a localhost nucleus server the .USD that I get in unreal is not really usable at the moment and differs in the experience to just exporting a .USD using blenders built in USD exporter and then importing that .USD into unreal manually without Omniverse and Nucleus and that workflow does work more or less as expected as shown towards the end of the attached video capture I recorded. I can see the materials displayed as I would expect and see the material and parent material that has been generated. I would like a better way to automate the master material and the way the material instance is applied but that is another topic.I do also realize that complex blender shaders are not supported but I can always bake textures from more complicated shader setups down to PBR if I can get this workflow going somehow.Hi @dirkteucher. Let me check with the team on this.@dirkteucher I’m happy to report that this is merely a workflow issue!When you export the mesh + material from Blender you need to specify that the Material Prim Path be a child of the Default Prim Path — this way when a USD file is referenced by the Unreal Connector (when you drag and drop a USD asset it makes a reference) it will be find the Material prim that is bound to the mesh.  When referencing USD files, the Unreal Connector currently only uses the defaultRoot as the reference prim path.
image452×530 33.1 KB
After modifying my Material Prim Path I was able to see a correctly “textured” cube from my test Blender export:
image841×367 46.2 KB
Note, I reported this issue to the Blender Connector team and they are considering a change to put the Materials prim under the defaultPrim by default.Thank you for the detailed video and I hope you can continue to enjoy creating with Omniverse!  If there are any other issues that I didn’t address please point them out to me.First of all. Wow it is working and that is a game changer in terms of speed to transfer files from Blender to Unreal. Amazing.I would have never guessed to change the default stage settings to “/root/Looks” , so thank you that is now doing what I was hoping it could.I also hope you change the default settings for future updates to the Blender Omniverse exporter plugin, I am sure anyone else who tests this will have the same problem and will be scratching their heads in puzzlement.I will also just add that if I then make changes to the mesh or material in blender and send to Omniverse and overwrite the existing USD with the new changes then in Unreal I have to go to the outliner and select the OmniverseStageActor and then in the details panel I have to click the “Use selected Asset from the content browser” next to the USD property and I have to make sure that I have the USD selected in the content browser to see my changes update in the viewport. It would be great if the changes just updated with a right click and refresh of the USD instead as it does currently with .FBXEven so, this is a real hidden gem as it is for sending lots of files at the same time. I am sure a lot of people will want to use this workflow when they figure it out. Fantastic.This topic was automatically closed 14 days after the last reply. New replies are no longer allowed.Powered by Discourse, best viewed with JavaScript enabled"
185,cannot-delete-the-folders-from-nucleus,"Hi,I was trying to delete the folders and files from Nucleus website.
Somehow I met the message bellow and the folder can not be deleted.

image1351×583 72.4 KB


image1124×586 47.4 KB

Is there any other way to delete the folders?
I am using omniverse enterprise version, so Nucleus is installed in an individual PC.
Thank you very much.Frank LiuHello @frank_liu!  I’ll get someone from the dev team to help you with this!The account you are logged in with - is it an Admin account?Not proposing that you have to be an administrator to delete items - but it looks like there’s been a snag with the permissions on the thumbnails. If you have an admin account you should be able to override those permissions and just delete.Hi @WendyGram  and @mkarlsson,Thank you for the reply.
The account name “omniverse” I use to delete the folders and files is an Admin account, which is the default Master superuser of Nucleus server.

image888×732 59.7 KB

By the way, I login with the account who created the folder, still cannot delete the folder.
The error message shows “FOLDER_NOT_EMPTY”, but I didn’t see anything in this folder.

image1129×619 57.2 KB


image1241×558 33.1 KB
got it - discussing with teamcan you show me the permissions on that Linked folder please?
also, try to edit the path you are viewing by manually entering /Linked/.thumbs after Assets - what permissions do you see on items there?Hi @mkarlsson ,The screen shot shows as bellow.

image1127×330 37.5 KB
After editing the path to enter /Linked/.thumbs after the path of Assets shows the error message at the bottom right.

image1181×616 39.9 KB
By the way, I login as both “omniverse”(account as super user) and “user2”(account who created the folder). The error message show “Path does not exist.” after editing the path of Assets. .Hi @frank_liuIf you are following the default docker-compose setting, please check the “content.1.1” folder under “/var/lib/omni/nucleus-data/data/”.If the folder is empty then there should be no file uploaded.In addition, which OS you are hosting the nucleus server?@frank_liu what exact version of Nucleus are you running please?Hi @frankt ,The folder content.1.1 is not empty and with several omni.content.xxx files in the path.

image1840×400 70.6 KB
I installed Ubuntu 18.04.6 LTS as the OS of nucleus server.Hi @mkarlsson ,I used the version 2022.1.0 of nucleus server, the file name is “nucleus-stack-2022.1.0+tag-2022.1.0.gitlab.3983146.613004ac.tar.gz”.
By the way, I didn’t install the cache for which file name is “nucleus-cache-2022.1.0+tag-2022.1.0.gitlab.3983147.c11aebcf.tar.gz”.Hi @frank_liuHave you ever upload files from navigator? or web client? looks like there are already files uploaded to nucleus serverHi @frank_liuPlease use “NVIDIA Omniverse Nucleus Tools”, to check the folder from terminal.If you are encountering trouble with above steps, we can set a session via Microsoft Teams.Hi @frankt ,I was stuck at the docker pull command, I got the message bellow when I trying to pull the nucleus-tools:1.0.0 images.$ sudo docker pull nvcr.io/omniverse/prerel/nucleus-tools:1.0.0
Error response from daemon: unauthorized: authentication requiredI also tried to login NGC with my omniverse account, generated the key with Username $oauthtoken, and login nvcr.io with the given password, finally I’ve got the message bellow.$ sudo docker login nvcr.io
Username: $oauthtoken
Password:
Login Succeeded$ sudo docker pull nvcr.io/omniverse/prerel/nucleus-tools:1.0.0
Error response from daemon: pull access denied for nvcr.io/omniverse/prerel/nucleus-tools, repository does not exist or may require ‘docker login’: denied: requested access to the resource is deniedCould you help me with the permission issue, please?Hi @frank_liuShall we setup a session, kindly check the message box1 have met the same problem,have you figure it out ?+1 same problem hereHi! Any update on the issue with the pulling nucleus docker tool image? I have the same issue as well.@jun.tang7 , @rodolphe.assere @bastian.dose
Can you please tell me if you each have purchased Nucleus Enterprise or are within a company that purchased it?Powered by Discourse, best viewed with JavaScript enabled"
186,i-can-not-found-the-unreal-plugin,"Audio2Face to Metahuman Blendshape Streaming PART 1 | NVIDIA On-Demand   In this video , I know a new version a2f is published.  I saw an unreal plugin in named nvidia arkit livelink in that video . where is it ?audio2face plugin?
About Epic Games Unreal Engine 5.2 Omniverse Connector is from 12 july maybe its not in there maybe needs uppdate , its in the videos i looked at first. also looking for the ferret examples? it relased on a friday so maby some will adjust this when it opens on monday.The current availability of the Unreal plugin is limited to ACE users. However, we are collaborating with other teams to develop a beta version of the plugin that will be accessible to the public via the Launcher.We will keep the forum updated with any notifications regarding these.This plugin is now available on 2023.1.1.
See for instructions: Audio2Face 2023.1.1 (Open Beta) Released - Apps / Audio2Face - NVIDIA Developer ForumsPowered by Discourse, best viewed with JavaScript enabled"
187,isaac-sim-documentation-404,"Hello, before the document can be opened, and then half of the use today suddenly 4042023-07-14 20-59-49 的屏幕截图1920×1080 188 KBHi @1365351984It seems that the URL changed to https://docs.omniverse.nvidia.com/isaacsim/latest/index.html.
You can always access all Omniverse applications documentation from the Omniverse Documentation main pageç.Still problems – if you go to the link you suggested:https://docs.omniverse.nvidia.com/isaacsim/latest/requirements.htmlAnd click on the “Technical Requirements” link on the bottom, you get a 404.  But you can manually fix the URL and then it’ll work.Thank you very much for your reply, it is already accessibleThank you very much for your replyI am also not able to find the nucleus documentation! the links are not also working!!This site can be accessed. @ahn.paf
https://docs.omniverse.nvidia.com/isaacsim/latest/index.htmlIt’s still broke – try clicking on Workstation installation and then “Install Omniverse Launcher”I have similar “404 Not Found” error when accessing some documents of Isaac Sim and I check the details of the redirected link.
I found that some links are invalidly redirected.When I access the ""Learn More About ROS2 Support "" in the top page of Isaac Sim,
my browser is trying to access the address docs.omniverse.nvidia.com/app_isaacsim/app_isaacsim/tutorial_ros2_turtlebot.html.But the address is redirected to the other address omniverse-docs-production.s3-website-us-west-1.amazonaws.com/isaacsim/latesttutorial_ros2_turtlebot.html.
And I can not access the redirected address.This address has one slash missing after the word “latest”.
And if the address is added one slash after the word “latest”, I can access the redirected address omniverse-docs-production.s3-website-us-west-1.amazonaws.com/isaacsim/latest/tutorial_ros2_turtlebot.html.The following other links in the top page of Isaac Sim are similarly invalidly redirected.Access link : docs.omniverse.nvidia.com/app_isaacsim/app_isaacsim/tutorial_ros2_rtx_lidar.html?highlight=lidar
Invalid redirection : omniverse-docs-production.s3-website-us-west-1.amazonaws.com/isaacsim/latesttutorial_ros2_rtx_lidar.htmlAccess link : docs.omniverse.nvidia.com/app_isaacsim/app_isaacsim/logistics_tutorial_cuopt.html
Invalid redirection : omniverse-docs-production.s3-website-us-west-1.amazonaws.com/isaacsim/latestlogistics_tutorial_cuopt.htmlAccess link : docs.omniverse.nvidia.com/app_isaacsim/app_isaacsim/ext_omni_anim_people.html
Invalid redirection : omniverse-docs-production.s3-website-us-west-1.amazonaws.com/isaacsim/latestext_omni_anim_people.htmlI think that the above invalid redirections have one slash missing after the word “latest”.
I can access the links which are added one slash after the word “latest”.
And I have reported these invalid redirections from “Contact Us | NVIDIA Developer” to NVIDIA.I don’t know that my error is just same error which you mentioned.
But I hope this helps.Hello All,Omniverse document platform was upgrading to a newer platform which caused 404 error. I believe all of these issues should be resolved but if you still see any 404 error issues then please let me know.https://docs.omniverse.nvidia.com/isaacsim/latest/index.htmlThere’s still lots of broken links.  For example, go to here:https://docs.omniverse.nvidia.com/isaacsim/latest/install_workstation.htmlAnd then click “Cache”Hi @dan.sandberg - Those should be fixed now. Please let me know if you see any other 404. Thank you for your patience.Powered by Discourse, best viewed with JavaScript enabled"
188,running-replicator-headless-never-runs,"I have a script that generates random scenes and saves out data, and this works when run in the Omniverse Code Script Editor. I then tried running it headless, following this tutorial: Running Replicator Headlessly   but the command just hangs after the output “RTX Ready”. I am getting the same output as referenced in this other post:  Running Replicator Headlessly not working (no output)  I also tried just running the example code in the tutorial and end up with the same issue.I saw the solution was to run with absolute paths, and I am doing that, moving directly into my omniverse code directory and specifying the absolute python filepath. I noticed that if I intentionally misspell the filepath, I get the same output with no errors, only warnings. I tried running without the --no-window flag, and the code window opens up but shows no issues or errors. Here’s my full output:
output (28.3 KB)Hello @alvinshek99, thank you for reaching out. Looking at the log, am I right in understanding the command run was ./omni.code.sh /home/alvin/isaac_sim/rep_tutorial.py? If so, please try specifying it like this:./omni.code.sh --/omni/replicator/script=/home/alvin/isaac_sim/rep_tutorial.pyPowered by Discourse, best viewed with JavaScript enabled"
189,nucleus-server-doesnot-have-any-services,"I am new to Omniverse and I install Local Nucleus Service 2023.1.0 on my PC but on localhost:3080, there is no list of Services as shown in some tutorial. Also there is no way i can connect to Localhost in Audio2Face. Please helpAttached: Image and System Monitor Log
image1916×1005 29.3 KB
System Monitor (1).log (112.9 KB)You installed Nucleus through the Launcher on the Nucleus Tab? Did you select the Launch item in the hamburger menu?Powered by Discourse, best viewed with JavaScript enabled"
190,usd-asset-for-deformable-cable-rope,"Hello,I would like to create a simulation setup of grasping a deformable cable using any articulated robot. Could you please refer me to any documentation or tutorial where cable simulation has been showed or used ?Regards,
SanjeevHi @ksu1rng  - You can find the info here in other forum post: What is the correct way to build a simulated rope? - #8 by AlesBorovickaPowered by Discourse, best viewed with JavaScript enabled"
191,openning-anonymous-layers,"Hello everyone.
Is there a way to show   anonymous layers (in memory layers)  in simulation with isaacsim. ?Is there a way to show anonymous layers (in memory layers) in simulation with isaacsim. ?Hi @a.adjanohoun - As of now, Isaac Sim does not provide a direct way to visualize anonymous or in-memory layers. The layers in Isaac Sim are generally associated with a USD file, and the visualization tools in Isaac Sim are designed to work with these file-based layers.However, you could potentially create a workaround by writing your in-memory layers to a temporary USD file and then loading that file into Isaac Sim.Powered by Discourse, best viewed with JavaScript enabled"
192,omniverse-code-crashes-while-running-script,"Hi,I am running following script in omniverse code script editor, but it’s crashing the Code. I am using Omniverse Code 2022.3.3. I could not find any specific error in logs.You can find complete log file here:
kit_20230529_155209.log (1004.6 KB)Hi @ShyamPatel. Judging from the log it looks like you’re running out of VRAM. I’m not sure how heavy your assets and textures are, but that’s probably the area that I’d try to optimize first. It looks like raw Kit takes about 5+ GB of VRAM on an empty scene, so take that into consideration.@pcallender Thanks for your reply.One thing I analysed here is that, the same code is working when I am using point_instance mode in rep.randomizer.instantiate function. But the Code crashes while using scene_instance or reference.Can you please elaborate, how each mode can change the memory usage and why?Hi @ShyamPatel,I can’t say why specifically one would crash from memory while the other would not, however, this link below has a detailed breakdown on the two and may lead you to a better understanding of where the memory differences are in your situation.https://openusd.org/dev/api/_usd__page__scenegraph_instancing.html#:~:text=USD's%20instancing%20functionality%20allows%20prims,to%20a%20scene%20via%20compositionPowered by Discourse, best viewed with JavaScript enabled"
193,our-guide-to-nvidia-omniverse-at-siggraph-2023,"siggraph23-paid-media-888-keynote-970x250970×250 62.8 KBPioneering Graphics for the AI GenerationAugust 6-10, 2023SIGGRAPH is right around the corner, and this year NVIDIA will be onsite in Los Angeles (and virtually) to celebrate the latest breakthrough research in graphics, research, Universal Scene Description (OpenUSD), and AI that are driving next-generation discoveries.Whether you are an Omniverse user or just curious, book some one-on-one time to ask all of your burning questions about the Omniverse platform, or have a relaxed chat with our experts. Reserve your spot now.Here’s a complete list of Omniverse activities at SIGGRAPH:SUNDAY, AUG 6MONDAY, AUG 7 - Hands-on LabsTUESDAY, AUG 8 - KeynoteWEDNESDAY, AUG 9 - OpenUSD DayHave questions, want to discuss a project, or get help? Finish out OpenUSD Day at SIGGRAPH with informal office hours at 4:45 - 6:00 p.m. PDT in Room 404 B to meet our engineering, research, and community teams.THURSDAY, AUG 10In addition to the events listed above, if you’re looking for some light reading, check out the 20 papers that showcase NVIDIA research advancements in rendering and gen AI across the community.We’ll be recording all the sessions listed above and will post them to Discord once they’re available.We hope to see you there and make sure to stop by our session room and labs to say hi!Learn More: NVIDIA at SIGGRAPH 2023Stay up-to-date with the latest Omniverse announcements by following us on Instagram, Twitter, YouTube, Twitch, Medium and Discord.Powered by Discourse, best viewed with JavaScript enabled"
194,omniverse-houdini-connector-docker-errors,"HiI’m attempting to add the Omniverse Connector plugin to our Houdini docker container but we’re getting some issues.
On my new Linux machine (not docker) using sudo on my user account for this process, I’ve been able to install the latest version of Houdini, copy the “houdini-connector-102.1.0” folder to the packages folder in the Houdini install directory (/opt/hfs19.5/packages) and run the HoudiniConnectorInstall.sh. I’ve been able to load up Hython in my terminal and import the homni and omni libraries and query the nucleus server. There are no other Omniverse files or programs on the machine except for the connector plugin folder.When i try to repeat this process on Docker i get an error saying no such file or directory for a file called libHoudiniAPPS3.so.
image1135×307 13.3 KB
I can see that the file does exist on the system and despite the plugin installed the exact same way as it was on my Linux machine, i’m getting different outcomes. The only difference being the user in the docker container being the root user.
I added print statements in the shiboken py file that’s throwing the error to pin point the error being from the homni module. The client.so file being binary, i can’t tell why it can’t see the file and there are no readable files in that module that would be able to help me figure this out.For the install of Houdini it’s the standard Linux install with the only additional option/arg. This is the install line from the Houdini docker file which is available on the sidefx website.
./houdiniInstaller/houdini*/houdini.install --auto-install --install-bin-symlink --accept-EULA ${EULA_DATE} \We’re using the docker file from the sidefx website with the only changes being adding the plugin file and running the “HoudiniConnectorInstall.sh” file.Thanks!Hello @jacob73
Could you please give v.102.4 a try? We fixed some compatible issues between Houdini patch versions. Although you mentioned that it works fine for you in a non-docker environment, I don’t think it is Houdini versions related issue, but let’s give it a try.Another thing I would check is, in your docker environment, does your Houdini lib paths are included in PATH and/or LD_LIBRARY_PATH?Ah yes! setting LD_LIBRARY_PATH fixes that issue. We just tested this solution not long before your post.
But having conquered that when we try to connect to the nucleus server with the reconnect function we get thrown an error from xdg-open requesting a TUI web browser for www-browser and listing some packages such as elink, w3m, lynx, links etc. These packages are OLD with most not receiving an update since 2009/2013 and required ubuntu packages having been versioned up since ubuntu 12-18.We’ve installed links but we get thrown an error saying that it doesn’t support javascript. But none of these TUI web browsers really support javascript and all of them has issues when compiling them to enable javascript. Is there a way to connect to nucleus without the need for javascript?Powered by Discourse, best viewed with JavaScript enabled"
195,omniverse-create-material-loading-issue,"Hi all,So I’m getting the following message in the console:2023-06-19 12:42:36  [Error] [omni.hydra] Failed to create MDL shade node for prim ‘/World/Looks/Ceramic_Tiles_Subway_Antique_White_Dirty_02/Shader’. Unsupported identifiers.This happened all of a sudden, out of the blue. Materials were loading fine, and the next second, not. I also tried to export the MDL and re-apply the exported MDL but it seems that there are issues with exporting the MDL too.Is there a fix for this issue please?Thanks in advance!Powered by Discourse, best viewed with JavaScript enabled"
196,more-powerful-ease-in-controls-for-keyframes-please,"Hey, I would love to have an “ease in” option on keyframes.
This camera movement would be much professional with a really slow ending…I found the weighted option but that does not give much more of soft slowing towards the end of the camera sweep from sun.Powered by Discourse, best viewed with JavaScript enabled"
197,how-to-get-the-camera-data-for-centerpose-model-training,"Hello,First of all, What I want is to get camera data which is needed for CenterPose Model Training.I’ve been trying to make some custom synthetic dataset for CenterPose model by NV labs.According to the GitHub page(https://github.com/NVlabs/CenterPose/blob/main/data/README.md), CenterPose data format is compatible with DOPE format.
Hence, I made the dataset using DOPE writer from the Offline Pose Estimation Synthetic Data Generation.
But the output file doesn’t have the camera data.When I look into the dope_writer.py  in line 152, adding camera data seems to be on the To-Do list. I hope this means that I can get the camera data some how and if so, please kindly provide me how.
image985×105 6.65 KBIt also says that it’s not used for training script but CenterPose needs them unlike DOPE.I’v found some data such as camera intrinsics from dope_config.yaml. But that’s not enough.I need camera projection matrix, camera view matrix and etc. (You might know what data CenterPose needs) and also AR data(plane center, plance normal).Please help me get the camera data and AR data for CenterPose.
If there is completed version of CenterPose writer instead of dope_writer.py, it would be better.I hope I can hear from you soon.
Thanks!Hello,Try using the CameraParams annotatorThis topic was automatically closed 14 days after the last reply. New replies are no longer allowed.Powered by Discourse, best viewed with JavaScript enabled"
198,mark-solved-arkit-audio2face-2023-1-0-is-not-working,"I’m experiencing an issue with Audio2Face 2023.1.0. When using the “mark_solved_arkit” example, the facial simulation doesn’t work while the audio is running. The console displays the following errors:2023-07-26 18:05:27 [Error] [omni.graph.core.plugin] /World/audio2face/BlendshapeSolve: Assertion raised in compute - local variable ‘cout’ referenced before assignment2023-07-26 18:05:27 [Error] [omni.graph.core.plugin] File “d:\program files\omniverse\lib\audio2face-2023.1.0\exts\omni.audio2face.exporter\omni\audio2face\exporter\ogn\python\nodes\OgnBsSolveNode.py”, line 183, in compute2023-07-26 18:05:27 [Error] [omni.graph.core.plugin] out_weight = db.internal_state.solver.computeFacsWeights(points.reshape(-1, 1))2023-07-26 18:05:27 [Error] [omni.graph.core.plugin] File “d:/program files/omniverse/lib/audio2face-2023.1.0/exts/omni.audio2face.exporter/omni/audio2face/exporter/scripts/facsSolver.py”, line 215, in computeFacsWeights2023-07-26 18:05:27 [Error] [omni.graph.core.plugin] client.send_message(‘/’ + str(blend[cout]), i)2023-07-26 18:05:27 [Error] [omni.graph.core.plugin]I need help resolving this as it’s crucial for my work. Please advise on how to fix this.Thank you
kit_20230726_212629.log (2.8 MB)Hello and welcome to the forums @ahmedzrashadThe errors you’re showing in your post don’t show up in the log file. Is it possible that this log is an old one?Can you please send your latest Audio2Face log file which is located inside C:\Users\<USERNAME>\.nvidia-omniverse\logs\Kit\Audio2Face?Also it would help us troubleshoot this much easier if you can show the steps you took that resulted in this error.that was the last log I created, maybe I did something wrong, so I recorded a video of what I was doing and sending a new log
kit_20230726_231318.log (4.2 MB)
I apologize for the inconvenience caused. The issue was on my side. I had written some code in facsSolver.py to export facial data to Unreal Engine, which caused the crash. After removing that code, Audio2Face 2023.1.0 is now working perfectly.
probably it’s a Python library installation issue or something related.
thank you <3
Powered by Discourse, best viewed with JavaScript enabled"
199,isaac-sim-2022-2-1-stuck-opening-hello-world-or-any-usd-saved-file,"Hello,Using fresh install, default configurations, trying to open ANY .usd file, including any of the examples (e.g Hello World) Isaac Sim GUI gets stuck indefinitely in the loading phase. Logs show nothing, no errors, no warnings.Launching the Isaac Sim works fine, it’s just trying to load any .usd file. Even an empty one that only has one cube with no textures.Shaders are already compiled.Downgrading to Isaac Sim 2022.2.0, everything works exactly like expected.Using propriety NVIDIA drivers 535Hi @fayezsalka - Would it be possible to share the log file for the issue you are facing on Isaac Sim 2022.2.1?Have you tried reinstalling the Isaac Sim 2022.2.1 and updating the GPU drivers as mentioned in the document? https://docs.omniverse.nvidia.com/isaacsim/latest/requirements.htmlI met the same problem， I can’t open any .USD saved files, including any examples.Launching the Isaac Sim 2022.2.1 in remote server on docker with client
here is the log:system:ubuntu20.04
driver:535.54.03Then, I try your method that Downgrading to Isaac Sim 2022.2.0, and It works. Thanks for your advice!I provide the same problem log, would it help you?I fixed the problem by downgrading Nvidia drivers from 535 to 525. Using Ubuntu and RTX 4090.Powered by Discourse, best viewed with JavaScript enabled"
200,please-someone-help-its-the-first-time-im-using-usd-and-it-is-already-not-loading-in-my-maya-2022,"USD plug-in not loading in my Maya 2022.
Screenshot (3)1920×1630 161 KBHi @alek.lazarov , You’ll need to search for “mayaUsdPlugin” in that Plug-in Manager window, and Unload it.image632×814 8.86 KBScreenshot (12)3840×2160 437 KB
Hi Devin
I have done that but double checked again and the result is the same error messagethank you for trying to help@alek.lazarov Can you confirm that you completely restarted Maya after unchecking “Loaded”, and after relaunching maya, confirming that mayaUsdPlugin was unloaded after launch.Hi DevinYes,absolutely…I noticed I was using and old version of usd and  so I just finished installing the latest version 0.24
but all that in vainthx for your patience with me@alek.lazarov - Looks like the Omniverse plugin is loaded? If you look at the top toolbar, there is a menu item called “Omniverse”. The error you are seeing down below is related to the MayaUSD Plugin and would be good to restart Maya to see if that clears out.You can check to see that the omniverse plugin is loaded by checking once more in the plugin manager, there should be a checkbox for the “Loaded” optionimage1920×1081 114 KBPowered by Discourse, best viewed with JavaScript enabled"
201,extension-processing-currently-opened-stage,"Hi everyone,
Trying to implement an extension for omniverse I have stumbled upon a problem, which I can’t seem to figure out.
My extension writes, the active scene in omniverse’s viewport, to a usd file using the following method: omni.usd.get_context().save_as_stage(usd_path, None)
Then, it passes this usd_path to an external software, which processes it and returns an image to display in my extension.
When I execute the extension, it displays the current scene without problems, but when I modify the scene in omniverse and execute the extension again (without closing and opening the extension) it shows the previous image without the modification.
I have checked the usd_path and it is saving it with the modification correctly, but somehow with the extension active when I try to process it with the external software, it always displays the first instance. Anyone knows what may be happening?Thanks in advancePowered by Discourse, best viewed with JavaScript enabled"
202,how-to-insert-camera-intrinsic-matrix-and-distortion-coefficients-for-the-camera-other-than-fisheyepolynomial,"I am looking at the following link:
https://docs.omniverse.nvidia.com/app_isaacsim/prod_materials-and-rendering/103/cameras.htmlAre the properties nominal width/height, offset center X/Y referring to the camera intrinsic values fx,fy,cx,cy?Is there a way to insert camera intrinsic values and distortion coefficients for popular opencv camera models such as plumb_bob?Thanks.Hi @naruto.uzumaki  - Someone from our team will review and respond to your question.Hi Naruto Uzumaki,We are working on adding support for popular camera models, like the OpenCV plump_bob.  In the current release of Isaac Sim, this model is not supported directly by the RTX renderer.  But, the following workaround could be used,  to allow fitting an existing OpenCV distortion model into the supported by Isaac Sim fTheta model.Please, refer to the following example. Please, don’t hesitate to contact us, if you need clarification or have further questions.Thank you for this example. To get Isaac Sim to do the lens distortion while rendering instead of postprocessing, what are the fTheta coefficients k0-k5 and Projection Type to put into the camera parameters in Isaac Sim, given the OpenCV camera_matrix and distortion_coefficients? Thanks!Hi Eric. Welcome!The code sample above explains onto how to calculate the  the fTheta coefficients k0-k4 of the fisheye polynomial model from the OpenCV rational polynomial model.  And the comment at the bottom of that code provides a reference onto how to set these coefficients for the Isaac Sim camera sensor.I’d recommend first, follow the tutorial in the this code sample that explains how to set the camera parameters.And running it either in a notebook or as a script.  And then adding the code above, to calculate the distortion parameters based on your OpenCV model.  We’ll provide simplified support for the OpenCV rational polynomial model [extrinsics/intrinsics], but for now the tutorial and the sample above are likely the best starting point.Thanks for the clarification, Dmitry! The crucial thing you pointed out is to calibrate using the fisheye polynomial model from the OpenCV rational polynomial model, rather than the standard radial-tangential distortion model (AKA Plumb Bob or Brown-Conrady or Original OpenCV model). Although the lens is not actually fisheye (i.e., FOV < 120°), this fTheta conversion approximates the radial distortion without tangential distortion, so it’s pretty close!Looking forward to using the OpenCV plump_bob model in Isaac Sim when that’s ready, but this gets us by for now.Thanks!
EricThank you very much @dchichkov .This example helps.One quick question: Will I also be able to use the camera model above and publish it to ROS? Would that need to happen only using the ActionGraph?could you please help how to set the camera and its intrinsics to be D435 from Intel RealSense?Powered by Discourse, best viewed with JavaScript enabled"
203,rendered-image-not-visible-camera-sensor-standalone-python-example,"In order to use the camera in a project, I was following this tutorial. According to the instructions, the image should be rendered after a few simulation steps, but it doesn’t appear.Can you please update on this thread?Hi @ksu1rng - Are you seeing any errors or warnings when you run the standalone python script?Hello,I am getting the following text message as a kind of warning in Isaac Sim GUI while running the camera sensor standalone python example.“SdRenderVarToRawArray missing valid input renderVar TargetMotionSD”Are you able to render the images as shown in tutorial ?Hello,Can you please update on this ?
I just want to know, according to the instructions, the image should be rendered [pop up window will appear] when this script  runs. But it doesn’t happen.Hi there,if you comment out all the print statements (just to avoid filling up the terminal) do you get the following warning?Best,
AndreiHi,Yes I am getting the following warnings that you’ve mentionedDo you know, work around this ?Best,
SanjeevHi,I am also facing another issue. In order to move forward with my current project, I decided to save images using the OpenCV library instead of showing via  matplotlib.pyplot. To install the OpenCV library, I used the following command: ./python.sh -m pip install <library_name> within the existing Python environment of isaac-sim.I attempted to save the image from the camera using the get_rgba() method on the camera instance. I tried to store only the RGB image using cv2.imwrite(""file_name"", camera.get_rgba()[:, :, :3]). However, I am not getting the exact colors in the image.Expected color of the ground-plane is “Blue”
But in saved image, I am getting “somewhat orange shade”Do you know the reason for this ?Regards,
SanjeevYou can try installing a GUI backend such as qt using ./python.sh -m pip install pyqt5 or see if the save_rgb example snippet helps:This topic was automatically closed 14 days after the last reply. New replies are no longer allowed.Powered by Discourse, best viewed with JavaScript enabled"
204,code-crashes-after-running-replicator,"Hi,
When I try to run the replicator after making a code change, omniverse Code crashes.I am trying to run the code shown here, except changing the props to be pallets and empty shelves instead of shrubs and puddles.https://docs.omniverse.nvidia.com/prod_extensions/prod_extensions/ext_replicator/shrubs_and_worker_example.htmlI am working on
processor:11th Gen Intel(R) Core™ i9-11980HK @ 2.60GHz   3.30 GHz
RAM: 32.0 GB
GPU: RTX 3080Hi @niShah. Welcome to the forums! Please consider joining out Discord community too: NVIDIA OmniverseI’ve moved your topic to the Synthetic Data Generation forum. Someone should be able to help you there. Please also share your log files so we can see what may have produced the crash: How to Report an issue with OmniverseI am currently running on A40 and have exactly the same issue, I have to create a new stage every time I make a change to code or settings. Also had the issue on an 3060 the other day.Hi @niShah While I try and repro this on my side, could you provide the log?Powered by Discourse, best viewed with JavaScript enabled"
205,question-on-using-surface-gripper-by-ros-in-isaac-sim,"Hello, recently I’m working on a project, which will use a suction cup.I have read through the document of surface gripper. However, I still don’t know how to controll it by Ros.In the Create/Isaac/Ros, there is an option of surface gripper. And I’m confused about how to set it up correctly. So that I can toggle the gripper by Ros.Please help me out, thanks.have read through the document of surface gripper. However, I still don’t know how to controll it by Ros.In the Create/Isaac/Ros, there is an option of surface gripper. And I’m confused about how to set it up correctly. So that I can toggle the gripper by Ros.After some attempts, I still can’t use the surface gripper in Isaac sim by ros. I found out that if I check the “/gripper_state” topic in Rviz, its error says that “Recieveing different joint name and effort”. So I believe the more specific problem might be ""how to write a proper joint in urdf to apply the ROS_surface_gripper on it. ""Hi ableho01,do you solve this problem now? I have the same questYes, we have successfully used the suction cup function in Isaac Sim with ROS.
However, it was an old project, so I forgot most of the details.Here are some of the settings in Isaac Sim and the Python code for ROS.
Hope these things can help you.
Screenshot from 2023-05-10 16-46-051920×1080 241 KB

Screenshot from 2023-05-10 16-46-261920×1080 235 KB
thank you so much, it was felpfulPowered by Discourse, best viewed with JavaScript enabled"
206,cant-load-omni-physx-when-running-create,"Hey!I’m trying to enable Omniverse Visual Debugger to debug my Physx application. But for some reason, when running Create app, the omni.physx plugin fails to load.(Note that I have tried with 2022.3.3 and 2022.3.1 versions and both gave me the same error)You can see the log output in the creator.log attached file. And the console log I can’t attach since I’m a new user and it only allows to upload one file for new users. But more or less had the same info.I believe this is the relevant first error:[4.535s] [ext: omni.physx-104.1.6-5.1] startup 2023-05-29 09:04:08 [4,481ms] [Warning] [carb] [Plugin: libomni.physx.plugin.so] Could not locate the function: carbGetFrameworkVersion
2023-05-29 09:04:08 [4,481ms] [Warning] [carb] Potential plugin preload failed: /home/lpares/.local/share/ov/pkg/deps/82b461274b877b7d5e1a405066cfd47a/extsPhysics/omni.physx-104.1.6-5.1/bin/libomni.physx.plugin.so
2023-05-29 09:04:08 [4,481ms] [Error] [omni.ext.plugin] [ext: omni.physx-104.1.6-5.1] failed to load native plugin:
home/lpares/.local/share/ov/pkg/deps/82b461274b877b7d5e1a405066cfd47a/extsPhysics/omni.physx-104.1.6-5.1/bin/libomni.physx.plugin.soI’m running it in Ubuntu 20.04, with integrated Intel graphics.Any help will be appreciated!console.log (1.1 MB)Hi,
there is a bug that prevents omni.physx to start when no Nvidia GPU is found, sorry about that. We have a fix for the upcoming release, where omni.physx should be able to start without Nvidia GPU.
You should be still able to use the old PhysX Debugger:The PhysX Visual Debugger (PVD) allows you to visualize, debug, and interact with your PhysX application's physical scene representation. The PhysX Visual Debugger (Version 3.0) supports the current PhysX SDK release and all previous versions.
Regards,
AlesI see, any ideas when this next release will be? Is it possible to use it as a beta version somehow?As for the PhysX Visual Debugger, I can’t find any download link for Linux. Is it only available on Windows?Thank you!Yes, sorry thats Windows only correct. If you have some windows machine you can run it there and setup the PVD connection to that machine.Next release should be out within a few weeks.Regards,
AlesAny ideas when the newer version will be available?Still a couple of weeks sorryThis topic was automatically closed 14 days after the last reply. New replies are no longer allowed.Powered by Discourse, best viewed with JavaScript enabled"
207,ferret-example-where,"I would like to folow the feeret example, where can i acces it. not in the regular nuculeus nvida folder.
in example tutorial  “Audio2Face Drive Character Using ARKit Blendshape Solve - Part 1”
its accesed in omniverse://ov-sandbox.nvidia.com i cant acces that?The Ferret model is scheduled to be published shortly after Siggraph, approximately within a month. However, it’s important to note that this is just one of the available content options, and for testing purposes, users are free to utilize any model they prefer.ok thanks, i will run my own model, just nice to begin with a functional model from tutorial as proof of concept.Powered by Discourse, best viewed with JavaScript enabled"
208,how-to-switch-from-one-environment-stage-to-another-using-action-graph,"I have multiple environments/stage (in terms of unity called Scenes) in my project I’ve room 1, room 2, and room 3. So I want to switch between those rooms on user input which I can reference in action graph.Hi @roshan200302kumar. I don’t think I have specific nodes for that at the moment, but you could use a Script Node to open a new Stage using Python: Omni.USD — omni.kit.usd_docs 1.1.1 documentationPowered by Discourse, best viewed with JavaScript enabled"
209,what-is-the-correct-way-to-build-a-simulated-rope,"Hi there. I am trying to build up a robot transportation task where we want to programmatically build ropes and attach a payload to my robot with them. However, so far I’ve come across these issues:Hi,
currently the only way to create the ropes is to use capsules and connect them with joints, see for example the physics ropes demo (Windows->Simulation->Demos->Rope).
One thing that you should make sure is that the joints are excluded from the articulation, there is a bool exclude from articulation on a joint:Otherwise if you connect the rope to the articulation, the parsing will try to create a graph including those joints and the whole system would be solved as one articulation, which would not be desired.Note that in next release we plan to release joint instancer, that would allow to create the ropes more easily.In the long term, we are trying to add more tech to support better rope simulation, but currently using capsules and spherical joints is the best way to do, try the ropes demo to see what the sim can look like.Regards,
AlesThanks! I managed to make a rope similar to that in the example. However, the simulation seems to become very unstable (objects blowing themselves away and Cuda errors) when I attached four ropes from four robots (simulated drones as Articulations) to a payload (objects.DynamicCube), with all collisions disabled.It keeps giving error messages like:2023-03-07 15:34:34 [24,902ms] [Error] [omni.physx.plugin] PhysX error: PxGeometryQuery::getWorldBounds(): pose is not valid., FILE /buildAgent/work/f25a4639a4b1bdc1/source/geomutils/src/GuGeometryQuery.cpp, LINE 319I suspect it is related to the capacity of the GPU buffers, i.e.,
gpu_max_rigid_contact_count: 5242880
gpu_max_rigid_patch_count: 33554432
gpu_found_lost_pairs_capacity: 4194304
gpu_found_lost_aggregate_pairs_capacity: 335544320
gpu_total_aggregate_pairs_capacity: 41943040
gpu_max_soft_body_contacts: 1048576
gpu_max_particle_contacts: 1048576
gpu_heap_capacity: 134217728
gpu_temp_buffer_capacity: 33554432
gpu_max_num_partitions: 8
and the simulation parameters. But the Doc has no clear description of these parameters.Is it currently a limitation of Isaac Sim or I have missed something? What do those gpu buffer parameters mean?Yes, those are preallocated buffers for the GPU simulation, if you try to simulate really long rope you will have to increase those.
For a start you can try to simulate this on the CPU first:
a) select PhysicsScene (or create one if you dont have it) (here you actually have those buffers configurable in the GPU section at the bottom if you want to run on GPU)
b) set GPU dynamics to false, set broadphase to MBPThanks! I also found doing the simulation on CPU is more stable and with masses of near magnitudes things look fine.I have ropes of 24 links (much shorter than the example) but a large number of them (>1000), created by a GridCloner. How can I decide what are the reasonable values for the GPU buffers?You should see in the log what was the required size, the first error should tell you what the expected size is, then you get a lot of errors because something failed. But the first error should tell you want and how to increase the buffer.I looked into the logs but the first error is not telling me what is wrong.It looks like the following. This is even before the articulation creation messages.2023-03-08 16:53:46 [20,049ms] [Info] [omni.kit.menu.utils.scripts.utils] omni.kit.menu.utils.refresh_menu_items Create2023-03-08 16:53:52 [26,353ms] [Info] [omni.kit.manipulator.prim.model] Tf.Notice.Register in PrimTransformModel2023-03-08 16:53:52 [26,353ms] [Info] [omni.kit.menu.utils.scripts.utils] omni.kit.menu.utils.refresh_menu_items Create2023-03-08 16:53:52 [26,360ms] [Info] [omni.appwindow.plugin] Created window: width=210,height=114.2023-03-08 16:53:52 [26,360ms] [Info] [omni.kit.renderer.plugin] Attaching default app window “omni.ui.scene.Widget capture”2023-03-08 16:54:26 [60,766ms] [Error] [omni.physx.plugin] PhysX error: PxGeometryQuery::getWorldBounds(): pose is not valid., FILE /buildAgent/work/f25a4639a4b1bdc1/source/geomutils/src/GuGeometryQuery.cpp, LINE 3192023-03-08 16:54:26 [60,766ms] [Error] [omni.physx.plugin] PhysX error: PxGeometryQuery::getWorldBounds(): pose is not valid., FILE /buildAgent/work/f25a4639a4b1bdc1/source/geomutils/src/GuGeometryQuery.cpp, LINE 3192023-03-08 16:54:26 [60,766ms] [Error] [omni.physx.plugin] PhysX error: PxGeometryQuery::getWorldBounds(): pose is not valid., FILE /buildAgent/work/f25a4639a4b1bdc1/source/geomutils/src/GuGeometryQuery.cpp, LINE 3192023-03-08 16:54:26 [60,766ms] [Error] [omni.physx.plugin] PhysX error: PxGeometryQuery::getWorldBounds(): pose is not valid., FILE /buildAgent/work/f25a4639a4b1bdc1/source/geomutils/src/GuGeometryQuery.cpp, LINE 3192023-03-08 16:54:26 [60,766ms] [Error] [omni.physx.plugin] PhysX error: PxGeometryQuery::getWorldBounds(): pose is not valid., FILE /buildAgent/work/f25a4639a4b1bdc1/source/geomutils/src/GuGeometryQuery.cpp, LINE 3192023-03-08 16:54:26 [60,767ms] [Error] [omni.physx.plugin] PhysX error: PxGeometryQuery::getWorldBounds(): pose is not valid., FILE /buildAgent/work/f25a4639a4b1bdc1/source/geomutils/src/GuGeometryQuery.cpp, LINE 3192023-03-08 16:54:26 [60,767ms] [Error] [omni.physx.plugin] PhysX error: PxGeometryQuery::getWorldBounds(): pose is not valid., FILE /buildAgent/work/f25a4639a4b1bdc1/source/geomutils/src/GuGeometryQuery.cpp, LINE 3192023-03-08 16:54:26 [60,767ms] [Error] [omni.physx.plugin] PhysX error: PxGeometryQuery::getWorldBounds(): pose is not valid., FILE /buildAgent/work/f25a4639a4b1bdc1/source/geomutils/src/GuGeometryQuery.cpp, LINE 3192023-03-08 16:54:26 [60,767ms] [Error] [omni.physx.plugin] PhysX error: PxGeometryQuery::getWorldBounds(): pose is not valid., FILE /buildAgent/work/f25a4639a4b1bdc1/source/geomutils/src/GuGeometryQuery.cpp, LINE 319I see, so there is something trying to add NaN into the system, please is there a way for me to check the assets?Some asset is bringing NaN into physics, that will cause issues to the simulation, not sure if you can identify the asset, but there seems to be some mesh with an invalid transformation.Hi,I am looking for a rope asset, and I don’t find it, someone knows about it something?Hi,
we have a sample for rope simulation using regular maximum coordinate joints, if you open physics demos, run rope demo.
Window->Simulation->demosRegards,
AlesPowered by Discourse, best viewed with JavaScript enabled"
210,update-displacement-maps,"Hello,
Any news on the development of displacement maps? The last forum threads about it are over a year old.Thanks.Hello @uarizonasselMDL-based displacement is planned to be supported by the RTX Renderer later this year in the fall.This topic was automatically closed 14 days after the last reply. New replies are no longer allowed.Powered by Discourse, best viewed with JavaScript enabled"
211,how-to-get-torque-values-of-robot-joints-with-rmpflow-motion-policy-in-frankas-pick-and-place-task,"I have followed the official documentation to create a pick-and-place task with the Franka manipulator arm: pick and place.As I understand it, the pick-and-place controller class utilizes RMPFlow under the hood. RMPFlow outputs the desired joint positions and joint velocities for the next time frame.To gain a better understanding of the joint forces and torques, I utilized the get_applied_joint_efforts() method from the Franka class and printed out the Joint Efforts. However, the output I received was an array with all values set to zero.Now, my question is: since the robot is simulating and picking up an object while moving from the initial position to the picking location, how is it possible to have zero joint efforts? Could you please assist me with this?For reference, I am attaching the code that I have written.Could anyone please update on this thread ?Hi @ksu1rng - Thank you for your patience. Someone from our team will review and respond to your question.Hi @ksu1rng - Each physics step you can set joint position targets, joint velocity targets or joint efforts. The get_applied_joint_efforts  is meant to return efforts that you applied but not the computed values. We will release extra apis to get the computed joint efforts in the upcoming release. These are not exposed for now.This topic was automatically closed 14 days after the last reply. New replies are no longer allowed.Powered by Discourse, best viewed with JavaScript enabled"
212,couple-issues-ive-not-found-a-solution-to-gizmo-local-fwd,"When I press setup character I get the errorAlso, my gizmo is no longer visible and I have not been able to get it back.I can work around this by starting a new project and starting over, but I’d like to understand what is happening.@kewkdb just out of curiosity, is the gizmo disappearing only in A2F or in other apps as well?I only use a2f so not sure. It happened after I clicked one of the pivot selections. Was looking for a way to reset it but did not find it. So just restarted.I just reproduced it by clicking authored pivot. When I zoom out I can see it again. Tho, pretty sure last time I zoomed way out and couldn’t see it as well.isn’t authored pivot ticked on by default for you? or did you mean when you switch out and back to it is when the gizmo went away because i can’t seem to reproduce it on my end.This should help.i believe that’s intended behavior for it to change based on your Selection Pivot Placement set in the dropdown. Authored pivot, by the definition in the doc, is to “reflect the actual Prim pivot attribute’s position.” when you said it disappeared, i was under the impression that it went away completely, which is something you’d get if you switch out of the move mode (W) and into selection mode (Q).Powered by Discourse, best viewed with JavaScript enabled"
213,crashed-when-saving-in-omniverse-setting-in-revit,"OS: Windows 11
Revit version: 2021
Omniverse version: Enterprise
Connector: Revitmaking any changes in the Omniverse setting will crash the Revit application.
dumpdata.zip (1.2 MB)In the omniverse.toml file , the following would be created.extension_root = “C:\Users\jkim7\Documents\kit\shared\exts”But in the version with Microsoft Passport ID, it was creating this in OneDrive. And the “Documents” folder wasn’t in ASCII for my case.
This was causing the crash.Powered by Discourse, best viewed with JavaScript enabled"
214,composer-2023-1-1-error-omni-usd-has-no-attribute-utils,"In this short video, I show something that used to work in composer (Create), and still works in Code, but does not work in Composer.I am asking this here because this affects my extension DataJuggler.CameraKeys, and I am curious was this depreciated or renamed or something?The error apears to occur on this linepose = omni.usd.utils.get_world_transform_matrix(camera, current_frame)Is there a newer way of getting the world transform for an object?Thank youJust posting the difference between the way the Left, Forward, Back and Right buttons all use this:It looks like xform.GetLocalTransformation() does something similar to the way this code gets the transform values.I will try switching the other buttons to work with the new way and see if everything works.Powered by Discourse, best viewed with JavaScript enabled"
215,the-image-of-the-self-made-camera-becomes-the-image-of-the-perspective-camera,"Isaac 2022.1.1I am doing reinforcement learning using images using Isaac sim. There is a problem that the camera image of the agent, which is the observed value, becomes the image of the perspective camera.Below is the camera class.Below is an instance.I checked the image like this as a test.The result was a perspective camera image.The agent has a camera like this.
camera1890×1135 81 KBHow can I get the agent’s camera image?The following warning is displayed. Is it related?[Warning] [omni.kit.window.viewport.plugin] Specified bound camera in usd does not exist: /Fork/Fork_mini_1_twin_camera/Fork_mini_1/Frame/CameraHi there,would using the Camera sensor class or Replicator API to access the data solve your issues? If not can you provide a short working script to test? Thanks!I simply mistyped the camera prim pass. Also, I didn’t know that perspective camera images would be acquired instead.Powered by Discourse, best viewed with JavaScript enabled"
216,isaac-sim-crashes-when-loading-a-demo-scene-windows-11,"Hello.I am able to successfully launch the Isaac Sim app on my Windows desktop (OS: Windows 11).
However, whenever I try to load a demo scene on Isaac Sim, I get the following error messages and the app just abruptly crashes.As you can see from the error messages below, I am able to load the App.
But whenever I try to do something (load a demo scene etc.) it just crashes with an error message that begins with "" [Error] [carb.crashreporter-breakpad.plugin] crash detected""What would be the cause of this crash?Isaac Sim seems to work fine on my Ubuntu 22.04 setup but for some reason it crashes on my Windows11 machine whenever I try to load a scene or a USD file.Thanks in advance!Hi @jaeyeun  - Can you try to reinstall the drivers as mentioned here in the official documentation? : 1. Isaac Sim Requirements — isaacsim latest documentationAre you able to install other applications such as USD Composer, Code?Powered by Discourse, best viewed with JavaScript enabled"
217,where-is-the-domain-randomizer-dr,"I am following the tutorial on training JetBot to Avoid Collisions Using NVIDIA Isaac Sim (Training Your NVIDIA JetBot to Avoid Collisions Using NVIDIA Isaac Sim | NVIDIA Technical Blog). The Tutorial said to use Isaac DR to randomize the light and movement of the objects. However I cannot find any DR in my Isaac Sim 2022.2.1. I was assuming it may be some version issue but I cannot find anyone discussing it was disabled or something like that.Powered by Discourse, best viewed with JavaScript enabled"
218,about-issacgym,"issacgym을 윈도우 환경에서 활용 할 수 있는 방법이 있을까요?영어로 번역해줘Hi @kqu0585 - You can refer to this document. 1. Overview & Getting Started — isaacsim latest documentationPowered by Discourse, best viewed with JavaScript enabled"
219,movie-capture-extension-not-available-in-audio2face-in-version-2022-2-1,"Hi Team,
I am not able to find movie capture extension inside Audio2face (2022.2.1)
As per the above link, it should have been availableSince it was not available in a2f, I was trying to load the extension and its dependency from its path which I fetched from CREATE, but was getting the below error.Thanks,
Priyanka
Hi @priyanka.manda. Any reason why you wouldn’t just save the stage and then render in USD Composer?Powered by Discourse, best viewed with JavaScript enabled"
220,stack-trace-trying-to-add-cloth-mesh-to-hair,"I have my character in a file. I tried to add a physics particle mesh for the hair but got the error and stack trace below. I then created a stage and referenced the character from the stage and then added the mesh and it worked.So it looks like it is expecting a certain depth of the character? I notice it says Cannot append  child 'ParticleSystem' to path ''. - that is, the path was empty (hit the root of the stage?). Adding the character into a stage on the other hand worked.My situation is I have a character in its own file, so I am trying to add all the meshes etc to that character. So I probably want the particle system added directly under “/” (or under “/Environment”?) because it should not be a part of the character model that someone else references. There should only be one for the stage? But the exception means I don’t know how to add mesh to my character in its own file.Powered by Discourse, best viewed with JavaScript enabled"
221,how-to-keyframe-the-properties-of-multiple-lights,"Hello,
As in the title, I am trying to keyframe the temperature / intensity value of multiple light selected at once. Is it possible?Thanks,
KarolThanks for the post. It seems like keyframing multiple objects at once is broken. We are investigating this as a bug. Thank you for your post and alerting us.Thank you Richard, I got about 50 lights in multiple groups to keyframe and it is a real pain to do it one by one. Please keep me updated on this one! Thank you!Well there is a much quicker method, which will at least help. Whilst it looks like multi object keyframing has a bug right now (which has been reported), there is a really quick method for this situation. Keyframe ONE light and then right click on the value you are keyframing and select “Copy Animation”. Then you can simply “paste” the animation back at the same place for each new light. It is still not quite as good, but it saves you from recreating the keyframes.Powered by Discourse, best viewed with JavaScript enabled"
222,composer-installation-error,"Hi, I’m getting the following error when installing 2023.1Error occurred during installation of USD Composer: Command failed: “C:\Users\harry\AppData\Local\ov\pkg\create-2023.1.0\pull_kit_sdk.bat” -q
& was unexpected at this time.
The system cannot find the path specified.and logs are:[2023-07-03 10:18:11.191] [info]  Omniverse Launcher 1.8.3 (production)
[2023-07-03 10:18:11.194] [info]  Argv: C:\Users\harry\AppData\Local\Programs\omniverse-launcher\NVIDIA Omniverse Launcher.exe --hidden
[2023-07-03 10:18:11.194] [info]  Crash dumps directory: C:\Users\harry\AppData\Roaming\omniverse-launcher\Crashpad
[2023-07-03 10:18:11.256] [debug] Running “C:\Users\harry\AppData\Local\ov\pkg\cache-2023.1.0\System Monitor\omni-system-monitor.exe”
[2023-07-03 10:18:11.262] [debug] Running “C:\Users\harry\AppData\Local\ov\pkg\nucleus-workstation-2023.1.0\System Monitor\omni-system-monitor.exe”
[2023-07-03 10:18:11.266] [debug] Running “C:\Users\harry\AppData\Local\ov\pkg\drive-200.0.0\Omniverse Drive 2\odrive2-app.exe”
[2023-07-03 10:18:11.273] [debug] Reset current installer.
[2023-07-03 10:18:11.299] [info]  Running production web server.
[2023-07-03 10:18:11.307] [info]  HTTP endpoints listening at http://localhost:33480
[2023-07-03 10:18:11.321] [debug] Sharing: true
[2023-07-03 10:18:11.336] [info]  Started the Navigator web server on 0.0.0.0:34080.
[2023-07-03 10:18:11.365] [info]  Downloading Launcher 1.8.7…
[2023-07-03 10:18:11.539] [info]  Launcher 1.8.7 is downloaded and ready to be installed after restart.
[2023-07-03 10:18:11.829] [info]  Registered omniverse-launcher for Windows.
[2023-07-03 10:18:11.829] [info]  Registered omniverse for Windows.
[2023-07-03 10:18:11.889] [info]  Logged in.
[2023-07-03 10:18:30.332] [debug] Waiting for authentication…
[2023-07-03 10:18:30.332] [debug] Initializing create@2023.1.0 installer…
[2023-07-03 10:18:30.332] [info]  Fetching create information…
[2023-07-03 10:18:30.867] [debug] Enqueue [947c4911-04f7-4c78-a671-2420bd2379c2] create.
[2023-07-03 10:18:30.867] [debug] Set current installer [947c4911-04f7-4c78-a671-2420bd2379c2] create.
[2023-07-03 10:18:30.883] [debug] Enqueued [947c4911-04f7-4c78-a671-2420bd2379c2] installer.
[2023-07-03 10:18:30.884] [debug] [947c4911-04f7-4c78-a671-2420bd2379c2] Start downloading USD Composer for create@2023.1.0…
[2023-07-03 10:18:30.884] [debug] [947c4911-04f7-4c78-a671-2420bd2379c2] Preparing downloader for C:\Users\harry\AppData\Local\ov\pkg\deps\77ca49d7041682cbd680fae8d0c94ea7.zip
[2023-07-03 10:18:31.270] [info]  Signed url: https://asset.launcher.omniverse.nvidia.com/create/2023.1.0/windows-x86_64/77ca49d7041682cbd680fae8d0c94ea7.zip?Expires=1688419108&Policy=eyJTdGF0ZW1lbnQiOlt7IlJlc291cmNlIjoiaHR0cHM6Ly9hc3NldC5sYXVuY2hlci5vbW5pdmVyc2UubnZpZGlhLmNvbS9jcmVhdGUvMjAyMy4xLjAvd2luZG93cy14ODZfNjQvNzdjYTQ5ZDcwNDE2ODJjYmQ2ODBmYWU4ZDBjOTRlYTcuemlwIiwiQ29uZGl0aW9uIjp7IkRhdGVMZXNzVGhhbiI6eyJBV1M6RXBvY2hUaW1lIjoxNjg4NDE5MTA4fX19XX0_&Signature=GIMovAwOuI2cnoSO5XcJcD0uCuU56Oj7h6qSYC-x3Jxwms8nLU9LKdasei1dULqM6RD04LJ~qrrXdjwcalyefJfHUT88TwZ9NL1VfVkhdU1R06woJHj7YFoL76iYZOin2UIjRezq09tQ13jsjV8THAuTherkPNvXszawGOVG~XHsJ2bDX5dVkgxbNQiKvMbXm6isRJSbaTFoYlpUt8LNInH0GpdpMDhwrMp6EjLBLVRiRHCMGIVB1Z7M87UxJjyTEvM9on5vVM7GoSiX-W~~34hcCEiyk4IGVnhIqUTRbRiuM6HRzd1wZ3oubljP94127ksWceS9AvZgjkWS0SxxxQ__&Key-Pair-Id=K13PD0MHC2KFRP
[2023-07-03 10:18:31.271] [info]  [947c4911-04f7-4c78-a671-2420bd2379c2] Calculating package size for https://asset.launcher.omniverse.nvidia.com/create/2023.1.0/windows-x86_64/77ca49d7041682cbd680fae8d0c94ea7.zip?Expires=1688419108&Policy=eyJTdGF0ZW1lbnQiOlt7IlJlc291cmNlIjoiaHR0cHM6Ly9hc3NldC5sYXVuY2hlci5vbW5pdmVyc2UubnZpZGlhLmNvbS9jcmVhdGUvMjAyMy4xLjAvd2luZG93cy14ODZfNjQvNzdjYTQ5ZDcwNDE2ODJjYmQ2ODBmYWU4ZDBjOTRlYTcuemlwIiwiQ29uZGl0aW9uIjp7IkRhdGVMZXNzVGhhbiI6eyJBV1M6RXBvY2hUaW1lIjoxNjg4NDE5MTA4fX19XX0_&Signature=GIMovAwOuI2cnoSO5XcJcD0uCuU56Oj7h6qSYC-x3Jxwms8nLU9LKdasei1dULqM6RD04LJ~qrrXdjwcalyefJfHUT88TwZ9NL1VfVkhdU1R06woJHj7YFoL76iYZOin2UIjRezq09tQ13jsjV8THAuTherkPNvXszawGOVG~XHsJ2bDX5dVkgxbNQiKvMbXm6isRJSbaTFoYlpUt8LNInH0GpdpMDhwrMp6EjLBLVRiRHCMGIVB1Z7M87UxJjyTEvM9on5vVM7GoSiX-W~~34hcCEiyk4IGVnhIqUTRbRiuM6HRzd1wZ3oubljP94127ksWceS9AvZgjkWS0SxxxQ__&Key-Pair-Id=K13PD0MHC2KFRP…
[2023-07-03 10:18:31.401] [debug] [947c4911-04f7-4c78-a671-2420bd2379c2] Downloading https://asset.launcher.omniverse.nvidia.com/create/2023.1.0/windows-x86_64/77ca49d7041682cbd680fae8d0c94ea7.zip?Expires=1688419108&Policy=eyJTdGF0ZW1lbnQiOlt7IlJlc291cmNlIjoiaHR0cHM6Ly9hc3NldC5sYXVuY2hlci5vbW5pdmVyc2UubnZpZGlhLmNvbS9jcmVhdGUvMjAyMy4xLjAvd2luZG93cy14ODZfNjQvNzdjYTQ5ZDcwNDE2ODJjYmQ2ODBmYWU4ZDBjOTRlYTcuemlwIiwiQ29uZGl0aW9uIjp7IkRhdGVMZXNzVGhhbiI6eyJBV1M6RXBvY2hUaW1lIjoxNjg4NDE5MTA4fX19XX0_&Signature=GIMovAwOuI2cnoSO5XcJcD0uCuU56Oj7h6qSYC-x3Jxwms8nLU9LKdasei1dULqM6RD04LJ~qrrXdjwcalyefJfHUT88TwZ9NL1VfVkhdU1R06woJHj7YFoL76iYZOin2UIjRezq09tQ13jsjV8THAuTherkPNvXszawGOVG~XHsJ2bDX5dVkgxbNQiKvMbXm6isRJSbaTFoYlpUt8LNInH0GpdpMDhwrMp6EjLBLVRiRHCMGIVB1Z7M87UxJjyTEvM9on5vVM7GoSiX-W~~34hcCEiyk4IGVnhIqUTRbRiuM6HRzd1wZ3oubljP94127ksWceS9AvZgjkWS0SxxxQ__&Key-Pair-Id=K13PD0MHC2KFRP…
[2023-07-03 10:18:31.462] [debug] [947c4911-04f7-4c78-a671-2420bd2379c2] Save https://asset.launcher.omniverse.nvidia.com/create/2023.1.0/windows-x86_64/77ca49d7041682cbd680fae8d0c94ea7.zip?Expires=1688419108&Policy=eyJTdGF0ZW1lbnQiOlt7IlJlc291cmNlIjoiaHR0cHM6Ly9hc3NldC5sYXVuY2hlci5vbW5pdmVyc2UubnZpZGlhLmNvbS9jcmVhdGUvMjAyMy4xLjAvd2luZG93cy14ODZfNjQvNzdjYTQ5ZDcwNDE2ODJjYmQ2ODBmYWU4ZDBjOTRlYTcuemlwIiwiQ29uZGl0aW9uIjp7IkRhdGVMZXNzVGhhbiI6eyJBV1M6RXBvY2hUaW1lIjoxNjg4NDE5MTA4fX19XX0_&Signature=GIMovAwOuI2cnoSO5XcJcD0uCuU56Oj7h6qSYC-x3Jxwms8nLU9LKdasei1dULqM6RD04LJ~qrrXdjwcalyefJfHUT88TwZ9NL1VfVkhdU1R06woJHj7YFoL76iYZOin2UIjRezq09tQ13jsjV8THAuTherkPNvXszawGOVG~XHsJ2bDX5dVkgxbNQiKvMbXm6isRJSbaTFoYlpUt8LNInH0GpdpMDhwrMp6EjLBLVRiRHCMGIVB1Z7M87UxJjyTEvM9on5vVM7GoSiX-W~~34hcCEiyk4IGVnhIqUTRbRiuM6HRzd1wZ3oubljP94127ksWceS9AvZgjkWS0SxxxQ__&Key-Pair-Id=K13PD0MHC2KFRP to C:\Users\harry\AppData\Local\ov\pkg\deps\77ca49d7041682cbd680fae8d0c94ea7.zip
[2023-07-03 10:19:55.187] [debug] [947c4911-04f7-4c78-a671-2420bd2379c2] Extracting USD Composer for create@2023.1.0 to C:\Users\harry\AppData\Local\ov\pkg\deps\77ca49d7041682cbd680fae8d0c94ea7.zip…
[2023-07-03 10:19:55.198] [error] (node:20908) [DEP0005] DeprecationWarning: Buffer() is deprecated due to security and usability issues. Please use the Buffer.alloc(), Buffer.allocUnsafe(), or Buffer.from() methods instead.
(Use NVIDIA Omniverse Launcher --trace-deprecation ... to show where the warning was created)
[2023-07-03 10:21:58.028] [debug] [947c4911-04f7-4c78-a671-2420bd2379c2] Start downloading kit-kernel for create@2023.1.0…
[2023-07-03 10:21:58.028] [debug] [947c4911-04f7-4c78-a671-2420bd2379c2] Preparing downloader for C:\Users\harry\AppData\Local\ov\pkg\deps\b7a0b9df5a1918450373458b74d781d8.zip
[2023-07-03 10:21:58.416] [info]  Signed url: https://asset.launcher.omniverse.nvidia.com/create/2023.1.0/windows-x86_64/kit/b7a0b9df5a1918450373458b74d781d8.zip?Expires=1688419315&Policy=eyJTdGF0ZW1lbnQiOlt7IlJlc291cmNlIjoiaHR0cHM6Ly9hc3NldC5sYXVuY2hlci5vbW5pdmVyc2UubnZpZGlhLmNvbS9jcmVhdGUvMjAyMy4xLjAvd2luZG93cy14ODZfNjQva2l0L2I3YTBiOWRmNWExOTE4NDUwMzczNDU4Yjc0ZDc4MWQ4LnppcCIsIkNvbmRpdGlvbiI6eyJEYXRlTGVzc1RoYW4iOnsiQVdTOkVwb2NoVGltZSI6MTY4ODQxOTMxNX19fV19&Signature=goSG-ZM-BOkTOHrPC~HZCqu53-MKGl0594afbFYQSqhP7HT18FJjo9vGHxLk84Kiy~YYdxIVvF412C8tFId9RvX3hAai~PSYPPg3k2y1vxrcKPDkDGR90avEBPFv2aE5tqE6TN4bFDXM1~H6WLxbdy811ooC0lVieoSjoPg~ysVxqGv1UCyDRrd3Z~Mz2AgLxl4WYDbGmo0wT~jXKuVUZzsSdtEEmpKE~Pae6fK7DSilbOqZPWz7zkiGcp~i378CyYh0~VPMrfW~YeTxBRW5bWr37Et3jqqwyNZjmwzLqP8RpW8ZLPsoIUL~CV-B0z8Xoc7reGkOSJzUuUKzE8f1VA__&Key-Pair-Id=K13PD0MHC2KFRP
[2023-07-03 10:21:58.416] [info]  [947c4911-04f7-4c78-a671-2420bd2379c2] Calculating package size for https://asset.launcher.omniverse.nvidia.com/create/2023.1.0/windows-x86_64/kit/b7a0b9df5a1918450373458b74d781d8.zip?Expires=1688419315&Policy=eyJTdGF0ZW1lbnQiOlt7IlJlc291cmNlIjoiaHR0cHM6Ly9hc3NldC5sYXVuY2hlci5vbW5pdmVyc2UubnZpZGlhLmNvbS9jcmVhdGUvMjAyMy4xLjAvd2luZG93cy14ODZfNjQva2l0L2I3YTBiOWRmNWExOTE4NDUwMzczNDU4Yjc0ZDc4MWQ4LnppcCIsIkNvbmRpdGlvbiI6eyJEYXRlTGVzc1RoYW4iOnsiQVdTOkVwb2NoVGltZSI6MTY4ODQxOTMxNX19fV19&Signature=goSG-ZM-BOkTOHrPC~HZCqu53-MKGl0594afbFYQSqhP7HT18FJjo9vGHxLk84Kiy~YYdxIVvF412C8tFId9RvX3hAai~PSYPPg3k2y1vxrcKPDkDGR90avEBPFv2aE5tqE6TN4bFDXM1~H6WLxbdy811ooC0lVieoSjoPg~ysVxqGv1UCyDRrd3Z~Mz2AgLxl4WYDbGmo0wT~jXKuVUZzsSdtEEmpKE~Pae6fK7DSilbOqZPWz7zkiGcp~i378CyYh0~VPMrfW~YeTxBRW5bWr37Et3jqqwyNZjmwzLqP8RpW8ZLPsoIUL~CV-B0z8Xoc7reGkOSJzUuUKzE8f1VA__&Key-Pair-Id=K13PD0MHC2KFRP…
[2023-07-03 10:21:58.506] [debug] [947c4911-04f7-4c78-a671-2420bd2379c2] Downloading https://asset.launcher.omniverse.nvidia.com/create/2023.1.0/windows-x86_64/kit/b7a0b9df5a1918450373458b74d781d8.zip?Expires=1688419315&Policy=eyJTdGF0ZW1lbnQiOlt7IlJlc291cmNlIjoiaHR0cHM6Ly9hc3NldC5sYXVuY2hlci5vbW5pdmVyc2UubnZpZGlhLmNvbS9jcmVhdGUvMjAyMy4xLjAvd2luZG93cy14ODZfNjQva2l0L2I3YTBiOWRmNWExOTE4NDUwMzczNDU4Yjc0ZDc4MWQ4LnppcCIsIkNvbmRpdGlvbiI6eyJEYXRlTGVzc1RoYW4iOnsiQVdTOkVwb2NoVGltZSI6MTY4ODQxOTMxNX19fV19&Signature=goSG-ZM-BOkTOHrPC~HZCqu53-MKGl0594afbFYQSqhP7HT18FJjo9vGHxLk84Kiy~YYdxIVvF412C8tFId9RvX3hAai~PSYPPg3k2y1vxrcKPDkDGR90avEBPFv2aE5tqE6TN4bFDXM1~H6WLxbdy811ooC0lVieoSjoPg~ysVxqGv1UCyDRrd3Z~Mz2AgLxl4WYDbGmo0wT~jXKuVUZzsSdtEEmpKE~Pae6fK7DSilbOqZPWz7zkiGcp~i378CyYh0~VPMrfW~YeTxBRW5bWr37Et3jqqwyNZjmwzLqP8RpW8ZLPsoIUL~CV-B0z8Xoc7reGkOSJzUuUKzE8f1VA__&Key-Pair-Id=K13PD0MHC2KFRP…
[2023-07-03 10:21:58.562] [debug] [947c4911-04f7-4c78-a671-2420bd2379c2] Save https://asset.launcher.omniverse.nvidia.com/create/2023.1.0/windows-x86_64/kit/b7a0b9df5a1918450373458b74d781d8.zip?Expires=1688419315&Policy=eyJTdGF0ZW1lbnQiOlt7IlJlc291cmNlIjoiaHR0cHM6Ly9hc3NldC5sYXVuY2hlci5vbW5pdmVyc2UubnZpZGlhLmNvbS9jcmVhdGUvMjAyMy4xLjAvd2luZG93cy14ODZfNjQva2l0L2I3YTBiOWRmNWExOTE4NDUwMzczNDU4Yjc0ZDc4MWQ4LnppcCIsIkNvbmRpdGlvbiI6eyJEYXRlTGVzc1RoYW4iOnsiQVdTOkVwb2NoVGltZSI6MTY4ODQxOTMxNX19fV19&Signature=goSG-ZM-BOkTOHrPC~HZCqu53-MKGl0594afbFYQSqhP7HT18FJjo9vGHxLk84Kiy~YYdxIVvF412C8tFId9RvX3hAai~PSYPPg3k2y1vxrcKPDkDGR90avEBPFv2aE5tqE6TN4bFDXM1~H6WLxbdy811ooC0lVieoSjoPg~ysVxqGv1UCyDRrd3Z~Mz2AgLxl4WYDbGmo0wT~jXKuVUZzsSdtEEmpKE~Pae6fK7DSilbOqZPWz7zkiGcp~i378CyYh0~VPMrfW~YeTxBRW5bWr37Et3jqqwyNZjmwzLqP8RpW8ZLPsoIUL~CV-B0z8Xoc7reGkOSJzUuUKzE8f1VA__&Key-Pair-Id=K13PD0MHC2KFRP to C:\Users\harry\AppData\Local\ov\pkg\deps\b7a0b9df5a1918450373458b74d781d8.zip
[2023-07-03 10:22:01.156] [debug] [947c4911-04f7-4c78-a671-2420bd2379c2] Extracting kit-kernel for create@2023.1.0 to C:\Users\harry\AppData\Local\ov\pkg\deps\b7a0b9df5a1918450373458b74d781d8.zip…
[2023-07-03 10:22:12.263] [info]  [947c4911-04f7-4c78-a671-2420bd2379c2] Linking USD Composer to C:\Users\harry\AppData\Local\ov\pkg\create-2023.1.0…
[2023-07-03 10:22:12.265] [info]  [947c4911-04f7-4c78-a671-2420bd2379c2] Linking kit-kernel to C:\Users\harry\AppData\Local\ov\pkg\create-2023.1.0\kit…
[2023-07-03 10:22:12.265] [debug] [947c4911-04f7-4c78-a671-2420bd2379c2] Installing create@2023.1.0 from C:\Users\harry\AppData\Local\ov\pkg\deps\77ca49d7041682cbd680fae8d0c94ea7…
[2023-07-03 10:22:12.266] [debug] [947c4911-04f7-4c78-a671-2420bd2379c2] Load installation scripts from C:\Users\harry\AppData\Local\ov\pkg\create-2023.1.0
[2023-07-03 10:22:12.915] [debug] [947c4911-04f7-4c78-a671-2420bd2379c2] Install C:\Users\harry\AppData\Local\ov\pkg\create-2023.1.0\pull_kit_sdk.bat
[2023-07-03 10:22:12.915] [debug] Running “C:\Users\harry\AppData\Local\ov\pkg\create-2023.1.0\pull_kit_sdk.bat” -q
[2023-07-03 10:22:35.201] [debug] Dequeue [947c4911-04f7-4c78-a671-2420bd2379c2] create.
[2023-07-03 10:22:35.201] [debug] Reset current installer.
[2023-07-03 10:22:35.202] [debug] [947c4911-04f7-4c78-a671-2420bd2379c2] Removing the installation folder: C:\Users\harry\AppData\Local\ov\pkg\create-2023.1.0
[2023-07-03 10:22:35.206] [debug] [947c4911-04f7-4c78-a671-2420bd2379c2] Remove unused packages from the library.
[2023-07-03 10:22:39.367] [error] Error: Command failed: “C:\Users\harry\AppData\Local\ov\pkg\create-2023.1.0\pull_kit_sdk.bat” -q
& was unexpected at this time.
The system cannot find the path specified.Other apps install with no issues, any idea of a fix?
Cheers, HarryThank you Harry, for your post. I have made a ticket and we will look into it. Can you try again ?Can you provide all logs found in this location ? C:\Users[user].nvidia-omniverse\logs\Kit\USD.Composer\2023.1Hi Richard, still getting the same error and there is no USD.Composer folder in that log folder.The …\ov\pkg\create-2023.1.0 folder gets created and populated during the install but then gets deleted after the error happens.Yes that means it never got installed at all. Try this one…
“C:\Users\USER.nvidia-omniverse\logs\launcher.log”Here you go:[2023-07-04 08:44:18.897] [debug] Waiting for authentication…
[2023-07-04 08:44:18.898] [debug] Initializing create@2023.1.0 installer…
[2023-07-04 08:44:18.898] [info]  Fetching create information…
[2023-07-04 08:44:19.492] [debug] Enqueue [44cdeb04-6fd1-479c-ae87-d637929ed33c] create.
[2023-07-04 08:44:19.493] [debug] Set current installer [44cdeb04-6fd1-479c-ae87-d637929ed33c] create.
[2023-07-04 08:44:19.506] [debug] Enqueued [44cdeb04-6fd1-479c-ae87-d637929ed33c] installer.
[2023-07-04 08:44:19.507] [debug] [44cdeb04-6fd1-479c-ae87-d637929ed33c] Start downloading USD Composer for create@2023.1.0…
[2023-07-04 08:44:19.507] [debug] [44cdeb04-6fd1-479c-ae87-d637929ed33c] Preparing downloader for C:\Users\harry\AppData\Local\ov\pkg\deps\77ca49d7041682cbd680fae8d0c94ea7.zip
[2023-07-04 08:44:20.036] [info]  Signed url: https://asset.launcher.omniverse.nvidia.com/create/2023.1.0/windows-x86_64/77ca49d7041682cbd680fae8d0c94ea7.zip?Expires=1688499856&Policy=eyJTdGF0ZW1lbnQiOlt7IlJlc291cmNlIjoiaHR0cHM6Ly9hc3NldC5sYXVuY2hlci5vbW5pdmVyc2UubnZpZGlhLmNvbS9jcmVhdGUvMjAyMy4xLjAvd2luZG93cy14ODZfNjQvNzdjYTQ5ZDcwNDE2ODJjYmQ2ODBmYWU4ZDBjOTRlYTcuemlwIiwiQ29uZGl0aW9uIjp7IkRhdGVMZXNzVGhhbiI6eyJBV1M6RXBvY2hUaW1lIjoxNjg4NDk5ODU2fX19XX0_&Signature=F7dFX2X84wIn5HVX8816Xq9bPKQEe6bKbazmK~qAJXWQpGjmFppJE0IQEnrZJN1R7uLEJuCPi0lhRubYLzicN1plyPx0MYUhUoIVN4Hhh36NjIqpbej~rxHQVx67ibPpaAbUm2BveEDBcbJxrK9Uqzj6UlDqbVlfH4crnCoVPbhQcXzulR2NUrqvI9eJeUi4TTjMJ8cjKXVl7zlRmoWs0aabBQ5JulHXHGGlirCxNAGLsfidCTsKwWG6y2W~ZYoBWwkpP1re9fpweA4Iq9NHn1hTzGS5LxmTSyui3MfBF-rPAIxmRe~o0EAFoa-Oi8NLHAnsIwc3KgJUpMbS1l5PHg__&Key-Pair-Id=K13PD0MHC2KFRP
[2023-07-04 08:44:20.037] [info]  [44cdeb04-6fd1-479c-ae87-d637929ed33c] Calculating package size for https://asset.launcher.omniverse.nvidia.com/create/2023.1.0/windows-x86_64/77ca49d7041682cbd680fae8d0c94ea7.zip?Expires=1688499856&Policy=eyJTdGF0ZW1lbnQiOlt7IlJlc291cmNlIjoiaHR0cHM6Ly9hc3NldC5sYXVuY2hlci5vbW5pdmVyc2UubnZpZGlhLmNvbS9jcmVhdGUvMjAyMy4xLjAvd2luZG93cy14ODZfNjQvNzdjYTQ5ZDcwNDE2ODJjYmQ2ODBmYWU4ZDBjOTRlYTcuemlwIiwiQ29uZGl0aW9uIjp7IkRhdGVMZXNzVGhhbiI6eyJBV1M6RXBvY2hUaW1lIjoxNjg4NDk5ODU2fX19XX0_&Signature=F7dFX2X84wIn5HVX8816Xq9bPKQEe6bKbazmK~qAJXWQpGjmFppJE0IQEnrZJN1R7uLEJuCPi0lhRubYLzicN1plyPx0MYUhUoIVN4Hhh36NjIqpbej~rxHQVx67ibPpaAbUm2BveEDBcbJxrK9Uqzj6UlDqbVlfH4crnCoVPbhQcXzulR2NUrqvI9eJeUi4TTjMJ8cjKXVl7zlRmoWs0aabBQ5JulHXHGGlirCxNAGLsfidCTsKwWG6y2W~ZYoBWwkpP1re9fpweA4Iq9NHn1hTzGS5LxmTSyui3MfBF-rPAIxmRe~o0EAFoa-Oi8NLHAnsIwc3KgJUpMbS1l5PHg__&Key-Pair-Id=K13PD0MHC2KFRP…
[2023-07-04 08:44:20.132] [debug] [44cdeb04-6fd1-479c-ae87-d637929ed33c] Downloading https://asset.launcher.omniverse.nvidia.com/create/2023.1.0/windows-x86_64/77ca49d7041682cbd680fae8d0c94ea7.zip?Expires=1688499856&Policy=eyJTdGF0ZW1lbnQiOlt7IlJlc291cmNlIjoiaHR0cHM6Ly9hc3NldC5sYXVuY2hlci5vbW5pdmVyc2UubnZpZGlhLmNvbS9jcmVhdGUvMjAyMy4xLjAvd2luZG93cy14ODZfNjQvNzdjYTQ5ZDcwNDE2ODJjYmQ2ODBmYWU4ZDBjOTRlYTcuemlwIiwiQ29uZGl0aW9uIjp7IkRhdGVMZXNzVGhhbiI6eyJBV1M6RXBvY2hUaW1lIjoxNjg4NDk5ODU2fX19XX0_&Signature=F7dFX2X84wIn5HVX8816Xq9bPKQEe6bKbazmK~qAJXWQpGjmFppJE0IQEnrZJN1R7uLEJuCPi0lhRubYLzicN1plyPx0MYUhUoIVN4Hhh36NjIqpbej~rxHQVx67ibPpaAbUm2BveEDBcbJxrK9Uqzj6UlDqbVlfH4crnCoVPbhQcXzulR2NUrqvI9eJeUi4TTjMJ8cjKXVl7zlRmoWs0aabBQ5JulHXHGGlirCxNAGLsfidCTsKwWG6y2W~ZYoBWwkpP1re9fpweA4Iq9NHn1hTzGS5LxmTSyui3MfBF-rPAIxmRe~o0EAFoa-Oi8NLHAnsIwc3KgJUpMbS1l5PHg__&Key-Pair-Id=K13PD0MHC2KFRP…
[2023-07-04 08:44:20.185] [debug] [44cdeb04-6fd1-479c-ae87-d637929ed33c] Save https://asset.launcher.omniverse.nvidia.com/create/2023.1.0/windows-x86_64/77ca49d7041682cbd680fae8d0c94ea7.zip?Expires=1688499856&Policy=eyJTdGF0ZW1lbnQiOlt7IlJlc291cmNlIjoiaHR0cHM6Ly9hc3NldC5sYXVuY2hlci5vbW5pdmVyc2UubnZpZGlhLmNvbS9jcmVhdGUvMjAyMy4xLjAvd2luZG93cy14ODZfNjQvNzdjYTQ5ZDcwNDE2ODJjYmQ2ODBmYWU4ZDBjOTRlYTcuemlwIiwiQ29uZGl0aW9uIjp7IkRhdGVMZXNzVGhhbiI6eyJBV1M6RXBvY2hUaW1lIjoxNjg4NDk5ODU2fX19XX0_&Signature=F7dFX2X84wIn5HVX8816Xq9bPKQEe6bKbazmK~qAJXWQpGjmFppJE0IQEnrZJN1R7uLEJuCPi0lhRubYLzicN1plyPx0MYUhUoIVN4Hhh36NjIqpbej~rxHQVx67ibPpaAbUm2BveEDBcbJxrK9Uqzj6UlDqbVlfH4crnCoVPbhQcXzulR2NUrqvI9eJeUi4TTjMJ8cjKXVl7zlRmoWs0aabBQ5JulHXHGGlirCxNAGLsfidCTsKwWG6y2W~ZYoBWwkpP1re9fpweA4Iq9NHn1hTzGS5LxmTSyui3MfBF-rPAIxmRe~o0EAFoa-Oi8NLHAnsIwc3KgJUpMbS1l5PHg__&Key-Pair-Id=K13PD0MHC2KFRP to C:\Users\harry\AppData\Local\ov\pkg\deps\77ca49d7041682cbd680fae8d0c94ea7.zip
[2023-07-04 08:45:46.479] [debug] [44cdeb04-6fd1-479c-ae87-d637929ed33c] Extracting USD Composer for create@2023.1.0 to C:\Users\harry\AppData\Local\ov\pkg\deps\77ca49d7041682cbd680fae8d0c94ea7.zip…
[2023-07-04 08:45:46.492] [error] (node:28104) [DEP0005] DeprecationWarning: Buffer() is deprecated due to security and usability issues. Please use the Buffer.alloc(), Buffer.allocUnsafe(), or Buffer.from() methods instead.
(Use NVIDIA Omniverse Launcher --trace-deprecation ... to show where the warning was created)
[2023-07-04 08:47:40.526] [debug] [44cdeb04-6fd1-479c-ae87-d637929ed33c] Start downloading kit-kernel for create@2023.1.0…
[2023-07-04 08:47:40.527] [debug] [44cdeb04-6fd1-479c-ae87-d637929ed33c] Preparing downloader for C:\Users\harry\AppData\Local\ov\pkg\deps\b7a0b9df5a1918450373458b74d781d8.zip
[2023-07-04 08:47:41.030] [info]  Signed url: https://asset.launcher.omniverse.nvidia.com/create/2023.1.0/windows-x86_64/kit/b7a0b9df5a1918450373458b74d781d8.zip?Expires=1688500057&Policy=eyJTdGF0ZW1lbnQiOlt7IlJlc291cmNlIjoiaHR0cHM6Ly9hc3NldC5sYXVuY2hlci5vbW5pdmVyc2UubnZpZGlhLmNvbS9jcmVhdGUvMjAyMy4xLjAvd2luZG93cy14ODZfNjQva2l0L2I3YTBiOWRmNWExOTE4NDUwMzczNDU4Yjc0ZDc4MWQ4LnppcCIsIkNvbmRpdGlvbiI6eyJEYXRlTGVzc1RoYW4iOnsiQVdTOkVwb2NoVGltZSI6MTY4ODUwMDA1N319fV19&Signature=KTav0JHql5rL4buanURS04A~E2EmHBPyRt-eQTYFM-RguwnJVb84y2iN3KD1I8JiuQGXlbY0e79VE2ww4hlmdlOEBq8aYczcpxY4k~OnjVXsbCzFAGFXr9Nr-rokWpi-zn8x9Iupqg8mHoC2Vw-xSvQXYiPp9vqYTQcpUGdO81LUhbpwXGaJW8rpqL-zds4aSCmCtmOFN6BmPSsNiNTPUwboOuoRH1~d1-PeDlgt9tcH2rufXvq7nzTlT2RzpV6nZpqcWRipLrHdUMKLwNvEOFT4eOFpj9jy1kFE8YmqedKC9bhi2tNkSljRhgHn00DZo4T6QY0S6IXTjmrkFXZ8RQ__&Key-Pair-Id=K13PD0MHC2KFRP
[2023-07-04 08:47:41.031] [info]  [44cdeb04-6fd1-479c-ae87-d637929ed33c] Calculating package size for https://asset.launcher.omniverse.nvidia.com/create/2023.1.0/windows-x86_64/kit/b7a0b9df5a1918450373458b74d781d8.zip?Expires=1688500057&Policy=eyJTdGF0ZW1lbnQiOlt7IlJlc291cmNlIjoiaHR0cHM6Ly9hc3NldC5sYXVuY2hlci5vbW5pdmVyc2UubnZpZGlhLmNvbS9jcmVhdGUvMjAyMy4xLjAvd2luZG93cy14ODZfNjQva2l0L2I3YTBiOWRmNWExOTE4NDUwMzczNDU4Yjc0ZDc4MWQ4LnppcCIsIkNvbmRpdGlvbiI6eyJEYXRlTGVzc1RoYW4iOnsiQVdTOkVwb2NoVGltZSI6MTY4ODUwMDA1N319fV19&Signature=KTav0JHql5rL4buanURS04A~E2EmHBPyRt-eQTYFM-RguwnJVb84y2iN3KD1I8JiuQGXlbY0e79VE2ww4hlmdlOEBq8aYczcpxY4k~OnjVXsbCzFAGFXr9Nr-rokWpi-zn8x9Iupqg8mHoC2Vw-xSvQXYiPp9vqYTQcpUGdO81LUhbpwXGaJW8rpqL-zds4aSCmCtmOFN6BmPSsNiNTPUwboOuoRH1~d1-PeDlgt9tcH2rufXvq7nzTlT2RzpV6nZpqcWRipLrHdUMKLwNvEOFT4eOFpj9jy1kFE8YmqedKC9bhi2tNkSljRhgHn00DZo4T6QY0S6IXTjmrkFXZ8RQ__&Key-Pair-Id=K13PD0MHC2KFRP…
[2023-07-04 08:47:41.122] [debug] [44cdeb04-6fd1-479c-ae87-d637929ed33c] Downloading https://asset.launcher.omniverse.nvidia.com/create/2023.1.0/windows-x86_64/kit/b7a0b9df5a1918450373458b74d781d8.zip?Expires=1688500057&Policy=eyJTdGF0ZW1lbnQiOlt7IlJlc291cmNlIjoiaHR0cHM6Ly9hc3NldC5sYXVuY2hlci5vbW5pdmVyc2UubnZpZGlhLmNvbS9jcmVhdGUvMjAyMy4xLjAvd2luZG93cy14ODZfNjQva2l0L2I3YTBiOWRmNWExOTE4NDUwMzczNDU4Yjc0ZDc4MWQ4LnppcCIsIkNvbmRpdGlvbiI6eyJEYXRlTGVzc1RoYW4iOnsiQVdTOkVwb2NoVGltZSI6MTY4ODUwMDA1N319fV19&Signature=KTav0JHql5rL4buanURS04A~E2EmHBPyRt-eQTYFM-RguwnJVb84y2iN3KD1I8JiuQGXlbY0e79VE2ww4hlmdlOEBq8aYczcpxY4k~OnjVXsbCzFAGFXr9Nr-rokWpi-zn8x9Iupqg8mHoC2Vw-xSvQXYiPp9vqYTQcpUGdO81LUhbpwXGaJW8rpqL-zds4aSCmCtmOFN6BmPSsNiNTPUwboOuoRH1~d1-PeDlgt9tcH2rufXvq7nzTlT2RzpV6nZpqcWRipLrHdUMKLwNvEOFT4eOFpj9jy1kFE8YmqedKC9bhi2tNkSljRhgHn00DZo4T6QY0S6IXTjmrkFXZ8RQ__&Key-Pair-Id=K13PD0MHC2KFRP…
[2023-07-04 08:47:41.182] [debug] [44cdeb04-6fd1-479c-ae87-d637929ed33c] Save https://asset.launcher.omniverse.nvidia.com/create/2023.1.0/windows-x86_64/kit/b7a0b9df5a1918450373458b74d781d8.zip?Expires=1688500057&Policy=eyJTdGF0ZW1lbnQiOlt7IlJlc291cmNlIjoiaHR0cHM6Ly9hc3NldC5sYXVuY2hlci5vbW5pdmVyc2UubnZpZGlhLmNvbS9jcmVhdGUvMjAyMy4xLjAvd2luZG93cy14ODZfNjQva2l0L2I3YTBiOWRmNWExOTE4NDUwMzczNDU4Yjc0ZDc4MWQ4LnppcCIsIkNvbmRpdGlvbiI6eyJEYXRlTGVzc1RoYW4iOnsiQVdTOkVwb2NoVGltZSI6MTY4ODUwMDA1N319fV19&Signature=KTav0JHql5rL4buanURS04A~E2EmHBPyRt-eQTYFM-RguwnJVb84y2iN3KD1I8JiuQGXlbY0e79VE2ww4hlmdlOEBq8aYczcpxY4k~OnjVXsbCzFAGFXr9Nr-rokWpi-zn8x9Iupqg8mHoC2Vw-xSvQXYiPp9vqYTQcpUGdO81LUhbpwXGaJW8rpqL-zds4aSCmCtmOFN6BmPSsNiNTPUwboOuoRH1~d1-PeDlgt9tcH2rufXvq7nzTlT2RzpV6nZpqcWRipLrHdUMKLwNvEOFT4eOFpj9jy1kFE8YmqedKC9bhi2tNkSljRhgHn00DZo4T6QY0S6IXTjmrkFXZ8RQ__&Key-Pair-Id=K13PD0MHC2KFRP to C:\Users\harry\AppData\Local\ov\pkg\deps\b7a0b9df5a1918450373458b74d781d8.zip
[2023-07-04 08:47:43.750] [debug] [44cdeb04-6fd1-479c-ae87-d637929ed33c] Extracting kit-kernel for create@2023.1.0 to C:\Users\harry\AppData\Local\ov\pkg\deps\b7a0b9df5a1918450373458b74d781d8.zip…
[2023-07-04 08:47:54.789] [info]  [44cdeb04-6fd1-479c-ae87-d637929ed33c] Linking USD Composer to C:\Users\harry\AppData\Local\ov\pkg\create-2023.1.0…
[2023-07-04 08:47:54.790] [info]  [44cdeb04-6fd1-479c-ae87-d637929ed33c] Linking kit-kernel to C:\Users\harry\AppData\Local\ov\pkg\create-2023.1.0\kit…
[2023-07-04 08:47:54.791] [debug] [44cdeb04-6fd1-479c-ae87-d637929ed33c] Installing create@2023.1.0 from C:\Users\harry\AppData\Local\ov\pkg\deps\77ca49d7041682cbd680fae8d0c94ea7…
[2023-07-04 08:47:54.791] [debug] [44cdeb04-6fd1-479c-ae87-d637929ed33c] Load installation scripts from C:\Users\harry\AppData\Local\ov\pkg\create-2023.1.0
[2023-07-04 08:47:55.411] [debug] [44cdeb04-6fd1-479c-ae87-d637929ed33c] Install C:\Users\harry\AppData\Local\ov\pkg\create-2023.1.0\pull_kit_sdk.bat
[2023-07-04 08:47:55.411] [debug] Running “C:\Users\harry\AppData\Local\ov\pkg\create-2023.1.0\pull_kit_sdk.bat” -q
[2023-07-04 08:48:17.755] [debug] Dequeue [44cdeb04-6fd1-479c-ae87-d637929ed33c] create.
[2023-07-04 08:48:17.756] [debug] Reset current installer.
[2023-07-04 08:48:17.756] [debug] [44cdeb04-6fd1-479c-ae87-d637929ed33c] Removing the installation folder: C:\Users\harry\AppData\Local\ov\pkg\create-2023.1.0
[2023-07-04 08:48:17.761] [debug] [44cdeb04-6fd1-479c-ae87-d637929ed33c] Remove unused packages from the library.
[2023-07-04 08:48:21.778] [error] Error: Command failed: “C:\Users\harry\AppData\Local\ov\pkg\create-2023.1.0\pull_kit_sdk.bat” -q
& was unexpected at this time.
The system cannot find the path specified.Thanks. We will take a lookI’ve managed to fix it, my cmd.exe autorun was broken, fixing that let it install correctly, thanks for your help!Hi Harry, thanks for the update. Can you elaborate with a lot more detail so we can understand and file it ?I had the same issue as here with another program: powershell - cmd.exe throws error ""& was unexpected at this time."" - Stack OverflowFollowing the instructions in the answer fixed both issues (amongst others)Great thanks !Powered by Discourse, best viewed with JavaScript enabled"
223,contact-reporter-between-deformable-and-rigid-bodies,"Is it possible to get contact reports between the rigid and deformable bodies?
I tried the demo “Contact Report Immediate API” for testing it, but it didn’t report the collisions between the rigid and deformable bodies.Powered by Discourse, best viewed with JavaScript enabled"
224,isaac-sim-could-not-be-launched,"Hardware: cpuR9 5950X GPU:4090 ubuntu2004  RAM39GB
I just install one pure ubuntu, and my gpu driver is 525, my cuda version is 11.7, The question is that after i download the ISaac SIm , the ""launch""button dosnt work at all. And another problem is that the NUCLEUS Service could not be started all: Auth | Discovery Service | Nucleus could not be started, what should i do?By the way, the Isaac sim version is 2022.2.1, and 2022.2.0 could not be launched eitherThat means Isaac Sim and Nucleus are not properly installed. Can you please try to reinstall them?
https://docs.omniverse.nvidia.com/app_isaacsim/app_isaacsim/install_workstation.htmlPowered by Discourse, best viewed with JavaScript enabled"
225,randomizer-based-replicator-code-gets-slower-every-frame,"The following code which is a minor tweak of this demo from the docs gets slower to render with each frame.  It’s much slower when run as an extension vs. headless, but the phenomenon is the same.  Per frame times go from sub second to something like 4 minutes per frame after just 1000 frames.Anyone else seeing this or have any bright ideas on how to fix, other than restarting every 100 frames or so?
image1220×498 65.1 KB
import omni.replicator.core as repdef on_click():
# print(“Made it into the script”)
print(“Made it into the script”)
with rep.new_layer():
# Define paths for the character, the props, the environment, and the surface where the assets will be scattered in.
# WORKER = ‘omniverse://localhost/NVIDIA/Assets/Characters/Reallusion/Worker/Worker.usd’
WORKER = ‘C:/Users/Public/cgiAssets/Drill2.obj’
# PROPS = ‘omniverse://localhost/NVIDIA/Assets/Vegetation/Shrub’
PROPS =   ‘C:/Users/Public/cgiAssets/’
ENVS = ‘omniverse://localhost/NVIDIA/Assets/Scenes/Templates/Outdoor/Puddles.usd’
SURFACE = ‘omniverse://localhost/NVIDIA/Assets/Scenes/Templates/Basic/display_riser.usd’
# SURFACE = ‘omniverse://localhost/NVIDIA/Assets/Scenes/Templates/Basic/abandoned_parking_lot.usd’on_click()This is essentially the demo from the code but pointing to local assets so it’s fast.
image803×660 68.2 KB
Only changes (except camera image is smaller too)
render_product = rep.create.render_product(camera, resolution=(224, 224))BTW, I timed this on Ubuntu and Windows 11 and I see the same pattern.===
More fun things to know, even if you make an extension that generates 10 frames and keep running it from the Replicator menu, it STILL takes more time each frame if you run it several times in a row.  I thought it would garbage collect or something in between, but nope.Help me Obi Wan…Also
with rep.trigger.on_time(num=10):
has same growth pattern as
with rep.trigger.on_frame(num_frames=10):And just for good measure, here’s NVIDIA’s recent fruit demo timing.  I haven’t changed anything in the code except the output directory.  Otherwise it’s verbatim from github.
image1482×1216 93.2 KB
Here is perhaps a better way to get the point across, the time to render grows exponentially with the number of images, where everyone expects it to be linear.
image3352×1318 335 KB
The only changes I made to the fruit demo was output image size, output directory and number of images.Hello @holdendp and thank you so much for bringing this to our attention. We’ve identified the cause and will have a fix implemented in the next release. Meanwhile, a workaround is to use mode=""reference"" instead of mode=""point_instance"" within the instantiate() call. This comes at a performance penalty vs. the fixed point_instance version but will avoid the bug that you’ve uncovered. In testing this version, I discovered that images are now rendering before all materials load. Use rt_subframes=3 in the trigger to avoid this issue (effectively renders 3 frames per iteration). See code snippet below:Cool, thanks.  I ran your code with 200 frames and it is consistent time per frame now.  Thanks for fixing this and I’m glad my efforts could help you.Also, thanks for fixing the materials not rendering issue.-Dave
image1442×826 62.7 KB
While the exact code you provided does have consistent render times, the proposed fix does not fix the issue in general.  e.g. when applied to the fruit crate example I still see the issue.
image1920×1001 98.9 KB
Meanwhile, a workaround is to use mode=""reference"" instead of mode=""point_instance"" within the instantiate() call.
image1920×1012 96.7 KB
fyiThanks for the update @holdendp . Could you please provide some details regardign the Fruit Example script you’re using?It’s NVIDIA’s code I believe:I’m just running this, as is on GitHub and then with the suggested workaroundMeanwhile, a workaround is to use mode=""reference"" instead of mode=""point_instance"" within the instantiate() call.They both get slower with each frame as of a day or two ago at least.Let me know if questions.I am also having the same experience as mentioned above. Both mode=“reference” and mode=""mode=“point_instance” are causing a slowing of the rendering. I have to say, that I was able to reduce the effect, making the instantiate() call before calling rep.trigger.on_frame(), but it still is very noticeable.After doing some further testing I can say that calling instantiate outside of the for loop fixes the issue for me:Figure_11400×600 48 KBThank you @julian.grimm and @holdendp. With your help, we were able to locate the root cause of this slowdown and implement a fix. Please look for it in the next upcoming IsaacSim release! If this remains a blocker, please let us know and we can provide steps to patch the problematic files in existing builds.Thank you for taking a look at this issue. I am currently using Replicator in Omniverse Code. Will this fix be already implemented in the upcoming release?Unfortunately the fix will not make it for the new Code release. However if this is blocking progress in your work, please let us know and we can walk through steps to address the issue based on your use case.Thank you for providing help on this topic. The issue where the rendering slows down is currently not a problem for me because I do not have to render that many images at once. However I am curious if this bug is related to the problem I mentioned in here:Maybe there is a better way of achieving the following:Currently my implementation looks like this:Thank you for your efforts!
JulianI worked around the slowdown by restarting my headless rendering util after n images.  Ugly hack but good enough until a real fix arrives.Powered by Discourse, best viewed with JavaScript enabled"
226,webrtc-on-omniverse-apps-does-not-stream-audio-output-to-browser,"Hi,I am having trouble getting the audio to play on the browser when I am playing the audio source on the Audio2Face App. If I am using the localhost setup, I would hear the audio but that is because it is playing through the Audio2Face App and not the front end browser. When I disable the audio output from the Audio2Face app on my windows settings, there is completely no audio playing on the browser, which leads me to believe the audio is not streamed through the WebRTC extension.I could see on the html file of the WebRTC client extension that it has declared the remote-audio but yet nothing is actually sent there to be played.Thank you,
BoonHello @user149936, I informed the WebRTC developer to help us out with your question.  I will post back here as soon as I hear back!Hello Wendy,Have you heard back from the WebRTC developer on this issue yet? I’d love to solve it as soon as I can so if there are any hints as to where to fix this issue that would be great.I have already searched through all the code that relates to the WebRTC extensions and have had no luck, so any help would be greatly appreciated!Best regard,
BoonHello Wendy @WendyGram,I as wondering if there is any solution to the issue? We cannot get audio output in any client: native, xr nor WebRTC.Would really appreciate any feedback.
Thank you!Kind regards,
AlexWe have created a ticket (OM-98209) for this issue and will update you as soon as it’s addressed.Powered by Discourse, best viewed with JavaScript enabled"
227,read-value-from-omnigraph-node,"controller = og.Controller()
keys = og.Controller.Keys
(graph, nodes_constructed, prims_constructed, path_to_object_map) = controller.edit(
{“graph_path”: “/ActionGraph”, “evaluator_name”: “execution”},
{
keys.CREATE_NODES: [
(“ROS2Context”, “omni.isaac.ros2_bridge.ROS2Context”),
(“OnPlaybackTick”, “omni.graph.action.OnPlaybackTick”),
(“PublishJointState”, “omni.isaac.ros2_bridge.ROS2PublishJointState”),
(“SubscribeJointState”, “omni.isaac.ros2_bridge.ROS2SubscribeJointState”),
],
keys.CONNECT: [
(“ROS2Context.outputs:context”, “PublishJointState.inputs:context”),
(“ROS2Context.outputs:context”, “SubscribeJointState.inputs:context”),
(“OnPlaybackTick.outputs:tick”, “PublishJointState.inputs:execIn”),
(“OnPlaybackTick.outputs:tick”, “SubscribeJointState.inputs:execIn”),
],
keys.SET_VALUES: [
(“ROS2Context.inputs:useDomainIDEnvVar”, True),
#(“PublishJointState.inputs:targetPrim”, “/World/UR5”)
],
},
)I have the above snippet to create and connect action graph. How do I read the values in a specific node output now?
For eg, I would like to read out the value subscribed by SubscribeJointState in outputs:positionCommandThank youHi @yadun.murali.kurikalveed - To read the output values of a node in action graph, you can use get_values method of the controller object.In this code, keys.GET_VALUES is used to specify the outputs you want to read. You provide a list of strings, where each string is the path to an output in the action graph.The get_values method returns a dictionary where the keys are the paths you provided and the values are the corresponding output values.Please note that you need to replace ""/ActionGraph/SubscribeJointState.outputs:positionCommand"" with the actual path to the positionCommand output in your action graph. The exact path might be different depending on how you constructed your action graph.2023-07-07 11:12:02 [412,280ms] [Error] [omni.ui.python] AttributeError: ‘Controller’ object has no attribute ‘get_values’
[omni.ui.python] AttributeError: type object ‘GraphSetupKeys’ has no attribute ‘GET_VALUES’I get these errorsHi @yadun.murali.kurikalveed  - Can you check if the get_values method is available in your version?you can use Python’s built-in dir() function. This function returns a list of all the attributes and methods of an object.Here’s how you can use it:In this code, dir(controller) returns a list of all the attributes and methods of the controller object. The print statement then prints this list to the console.If get_values is in the list, then the get_values method is available. If it’s not in the list, then the get_values method is not available.Powered by Discourse, best viewed with JavaScript enabled"
228,replicator-composer-rtx-lidar-sensor,"Hello all,Is the replicator composer limited to a simple camera/stereo?I am asking, because I want to generate pointclouds from a lidar sensor that are further used for training my model. Is there any opportunity to do so?Please give me support for my project!Thanks in advance,Best Regards,
ChristofHi Christof, you can create multiple cameras that look at different place of the scene, and create each render product for each of the camera, and attach the writer to these render products. Currently Composer does not support this, but you can use replicator code to achieve this:This topic was automatically closed 14 days after the last reply. New replies are no longer allowed.Powered by Discourse, best viewed with JavaScript enabled"
229,extensions-are-not-loading,"I don’t know why the extensions are not loading when I start isaac sim, I have the following error I do not know if it is related to this issue:2023-07-09 14:51:45 [3,061ms] [Error] [carb.scripting-python.plugin] TypeError: unsupported operand type(s) for +: ‘NoneType’ and ‘str’At:
/home/amirhossein/.local/share/ov/pkg/isaac_sim-2022.2.1/exts/omni.isaac.examples/omni/isaac/examples/franka_nut_and_bolt/franka_nut_and_bolt.py(84): init
/home/amirhossein/.local/share/ov/pkg/isaac_sim-2022.2.1/exts/omni.isaac.examples/omni/isaac/examples/franka_nut_and_bolt/franka_nut_and_bolt_extension.py(25): on_startup
/home/amirhossein/.local/share/ov/pkg/isaac_sim-2022.2.1/kit/kernel/py/omni/ext/_impl/_internal.py(147): _startup_ext
/home/amirhossein/.local/share/ov/pkg/isaac_sim-2022.2.1/kit/kernel/py/carb/profiler/init.py(81): wrapper
/home/amirhossein/.local/share/ov/pkg/isaac_sim-2022.2.1/kit/kernel/py/omni/ext/_impl/_internal.py(198): startup
/home/amirhossein/.local/share/ov/pkg/isaac_sim-2022.2.1/kit/kernel/py/omni/ext/_impl/_internal.py(285): startup_extension
PythonExtension.cpp::startup()(2): 2023-07-09 14:51:45 [3,061ms] [Error] [omni.ext.plugin] [ext: omni.isaac.examples-1.5.6] Failed to startup python extension.I have reinstalled Isaac Sim once but this error still persists! any suggestion on what is happening here?I am also not able to use Python using ./python.sh
I would again encounter the following error:
TypeError: unsupported operand type(s) for +: ‘NoneType’ and ‘str’
2023-07-09 15:28:04 [5,780ms] [Warning] [carb.audio.context] 1 contexts were leaked
./python.sh: line 41: 379376 Segmentation fault      (core dumped) $python_exe “$@” $args
There was an error running pythonHi @ahn.pafAccording to exts/omni.isaac.examples/omni/isaac/examples/franka_nut_and_bolt/franka_nut_and_bolt.py file (line 84), there is a problem trying to find the root path to the Isaac Sim assets on a Nucleus server.Make sure Nucleus is installed and running… and additionally, share the logs to identify specific the specific problemgreat that worked! Thank you for your help!
I still do not understand how the nucleus server works and what its usage is! Is there any video or guidance on that?
I made an account at first but later I had to make another one to be able to log in to Isaac Sim! How does it all work?
I appreciate your time and responseThis topic was automatically closed 14 days after the last reply. New replies are no longer allowed.Powered by Discourse, best viewed with JavaScript enabled"
230,adding-2d-image-as-texture-to-a-height-field,"I’ve been generating a mesh terrain using a 2D height map by converting them to vertices and triangles, and then adding them to stage as follows:Now, I want to use a 2D RGB image of the same size as the height map, as texture for the mesh. How can I do that?Hi @ahallak  - To apply a texture to a mesh in USD, you need to create a material, bind it to the mesh, and then set the texture file path to the diffuse color input of the material’s shader. Here’s how you can do it:This code creates a material with a PBR shader and a texture for the diffuse color. The texture file path is set to the ‘file’ input of the texture shader. The ‘rgb’ output of the texture shader is then connected to the ‘diffuseColor’ input of the PBR shader. Finally, the material is bound to the mesh using the UsdShade.MaterialBindingAPI.Please replace ‘path/to/your/texture.png’ with the actual path to your texture file.Thank you very much for your reply.
I pasted the code you sent, while it doesn’t crash, it also doesn’t seem to work.
I’ve tried replacing the png with jpeg, and also instead of using my desired image using a simple blue screen from online. None of them shows.
I do see in the IsaacSim screen opening up the material in the right path, and seemed to be binded.Any idea what am I doing wrong?
The plot thickens…
Even though your code didn’t pull through, it led me to this online source Create a UsdPreviewSurface Material — dev-guide latest documentationThe last part was very similar to what you proposed, so I added everything without understanding anything  and got the following code:Now this code, worked on an online png image I found, but I couldn’t get it to work on a HxWx3 numpy array  have:Do you understand what was missing from your original snippet, maybe I’m merging the sources wrong?
Any idea how to save a HxWx3 numpy array as an image in a way which is compatible with the texture?OK, new hint - by playing with additional PNGs, I’ve realized that although some don’t work, these that do result in a monochromatic rendering - either all are shades of blue, or shades of red, or shades of grayscale. I couldn’t find any clue to why this may happen in the snippet, maybe you have an idea.Powered by Discourse, best viewed with JavaScript enabled"
231,is-there-a-built-in-torque-sensor,"Hi,  we plan to simulate peg-in-hole insertion in Isaac Sim. Isaac Sim 2021 open beta has a contact force sensor, and we are wondering if there is a torque sensor as well?We searched the documentation but did not find it. Is the release of the torque sensor on the roadmap?Hi ycsuphy,As you have seen in the documentation at the moment we only have force sensor, but torque sensor is on the roadmap.
In some applications you can use the force sensor and just add some post processing to it to make is toques sensor.Kindly,
Liilacan you make that contact sensor to a multi dof force sensor?PhysX provides support for adding force/torque sensors directly to an articulation
https://docs.omniverse.nvidia.com/app_create/prod_extensions/ext_physics.html#force-sensorsIs there an update on if/when we can see a force-torque sensor with isaac-sim?Hi @msakthivel1  - This is something we are going to do in future Isaac Sim releases.Powered by Discourse, best viewed with JavaScript enabled"
232,usd-plug-in-not-loading-up-in-maya-2022-23,"Hi all. I recently connected to omniverse but it seems all in vain as the backbone of omniverse i.e USD files not working in my Maya…the plug-in is unable to load. I would greatly appreciate any help with fixing this issue!
Screenshot (11)3840×2160 469 KBHi @alek.lazarov . Can you clarify on  what you’re trying to do? We don’t distribute the mayaUsd plugin. That is from Autodesk. Are you trying to use the Maya Connector? If so, are you using the Maya Native or Maya Legacy connector?Hi MatiI am an animation student and wanted to create a beautiful set dressing for my shot in omniverse but soon realised that that is near impossible and the registering on the platform is pointless without USD.
I am quite new to this type of files never used them before and never have I been aware of this issue I had on my hands.
It was only now that I realised it is not loading up in my Maya for some mysterious reason.
Even after looking for resolutions on the Internet I am still baffled and I cannot figure out how to overcome this on my own.
I came across a similar issue someone had experienced and they said it was something they installed that was in the path of USD,they had Nuke installed,and after getting rid of Nuke USD loaded up successfully.
I do not have Nuke installed on my system but I am using Substance painter and sometimes designer along with Gimp and Davinci resolve.Could any of these softwares be interfering and thus creating conflict with USD?
Your help would be greatly appreciated.I am really looking forward to find a solution and being able to use the power of omniverse in my future projects.Kind regards,AleksandarPowered by Discourse, best viewed with JavaScript enabled"
233,object-is-not-rendered-when-dragged-in-through-the-content-browser-but-is-ok-when-added-as-a-layer,"USD Composer 2023.1.0An object is not rendered when dragged in through the content browser but is ok when added as a layer.
image1687×896 147 KBimage1691×891 122 KB2023-07-04 09:46:53  [Warning] [omni.usd] Warning (secondary thread): in _ReportErrors at line 2890 of W:\ac88d7d902b57417\USD\pxr\usd\usd\stage.cpp – In </World/Foil2023_02/Circle_009/Circle_002.material:binding>: The relationship target </Materials/BP4> from </root/Circle_009/Circle_002.material:binding> in layer @D:/Nvidia Omniverse/Bicycle/Foil2023.usd@ refers to a path outside the scope of the payload from </World/Foil2023_02>.  Ignoring. (getting targets for relationship </World/Foil2023_02/Circle_009/Circle_002.material:binding> on stage @anon:00000229A4395DF0:World1.usd@ <000002292F8B7230>)I would like to add.
The models are created using Blender
I get various different results when exporting in two different ways
A.
image238×505 24.2 KBI have now tried using
Export USD from the File Menu
I am having a hard time replicating the same results also…Ok thanks. We will take a look and get back to you. Can you send us any logs you have for Composer at the time of the error and also the scene files you are using, both the raw Blender file and the USD.Having talked to the engineers, the reason the drop and drag is failing is that you have not set a default Prim to attach to. You must first set this for drop and drag to work. In the future release we will be adding a warning when you attempt this action.Here is a great video on usd and default prim.Hi Richard,Thank you very much this is a great video and shows a couple of different ways to handle your models.I just added root to the material path for export in Blender
Much appreciated. Have great dayRynoGreat !!Powered by Discourse, best viewed with JavaScript enabled"
234,trigger-randomization,"Hello all!I am still busy with generating synthetic data (lidar sensor) and now I am facing some issues.Is it possible to do so? Are there any examples of the registration of trigger events?My overall plan for the data generation is the following:Are there any suggestions from you? I did not find any documentation to go ahead with this.
Does my plan make sense?Best regard,
ChristofJust for my understanding:What is the order of execution in action graph? Is it that the randomization is first executed and when it is finished that the sensor measurement is scheduled?Hi @christof.schuetzenhoefer! You’re correct, randomization is executed, then measurement (ie. annotation) is scheduled, but may not be complete by the time the scene is modified for the next frame. Custom triggers for randomization is possible but not yet easily done. Triggers for writers is a feature we’re actively developing and which is not currently possible. Could you give more details on your use case? We may be able to provide you with a workaround until these features are available, and I’d love to incorporate your feedback into the development and ensure we’re offering the best experience.Thanks for the response @jlafleche !Workflow should look like this (preferable that the functions blocks the execution until the desired processing is done):For my use case, I want to use the replicator for the randomization of a scene. When this is done, then the measurement of the sensors can be initiated (or a way that I can get the correct results for the finished scene). Finally, the writer can be directly triggered from a function (or after one correct measurement?).In my opinion, it should be guaranteed that the measurement is captured when the randomization is done (this is essentially for synthetic data). Or another idea is to get the first measurement of the randomized scene after the randomization step (additionally the writers are also triggered).I hope it is clear for my use case and I am looking forward to figuring out a workaround until this becomes a feature!
: -)Best Regards,
ChristofHi @christof.schuetzenhoefer ! If I understood your need correctly, you effectively want to be able to run the simulation for X frames before capturing. We are working on a better solution for this scenario, but for now, please try the following workaround, and do let us know if you have any questions!You should see in the console:Thank you for the workaround!I have a question:Otherwise this workaround looks very good to me! :-)Hi @christof.schuetzenhoefer,Apologies for the delay. If you’re referring to per-frame randomizations, then they will always be performed before writing. (ie. step_async() will trigger randomizations, then schedule the frame for capture)Powered by Discourse, best viewed with JavaScript enabled"
235,composer-crashes-with-renderfarm-4x-a4000-24c-32g,"Hello!
I have a VM with 4x A4000 +24c +32G.
I rendered stuff there few weeks ago ok, but now it kinda “corrupted”.First it stucked at “loading material 97%” forever.
I tried to fix this by running the official omniverse cleanup tool, that went through without any erroresAfter that complete uninstallation, I tried to install Composer again, but I got this error:

cannto install create1249×753 118 KB
This was fixed by renderfarm support:
"" just rebooted, waited for OV to launch.   Quit it.  Deleted the folder showing in your screenshot.   Rebooted.   Then installed Create""After this successful installation of Composer, it crashed at the first test run:

crashes1920×984 93.8 KB
I used task-manager to quit the Composer and then when I launched Composer again, I was stucked again at the same old 97%:

loading material forever 97 percent2592×1297 336 KB
Here are the logs for you, please can you help us to use omniverse in this renderfarm?Pekka Varislogs.zip (597.4 KB)Powered by Discourse, best viewed with JavaScript enabled"
236,grid-geometry-keeps-reappearing,"Hi all,
every time I re-open the scene or even update some .usd file inside the USD Composer scene, the original scene grid geometry reappears and it must be deleted  by hand everytime. I tried to hide it but it reappearing anyways.It already caused me so many issues by overlapping with the my pavement geometry.Ok I will investigate. Can you record a video ?Hi Richard, I think it actually happens only when I update some .usd file inside the scene, not when I simply reopen it. Anyway I will record whenever it happens again.
Thanks!I see it all the time steps to reproduce for me are1.drag a .usd onto the stage
2. hide ground plane
3. save with options(boom ground plane appears)Powered by Discourse, best viewed with JavaScript enabled"
237,does-orbit-support-domain-randomization,"As topicHi @berternatsYes, Orbit supports all the features coming from Isaac Sim and upstream (as those are the underlying dependencies). You can use the replicator API to perform DR for various materials/textures/lighting based randomization. For resetting the state of the environment (object pose), we recommend directly working with the physics APIs since you have more control over the randomization done.We will be including an example of this in June with many other upgrades.This topic was automatically closed 14 days after the last reply. New replies are no longer allowed.Powered by Discourse, best viewed with JavaScript enabled"
238,general-omniverse-apps-launching-problem,"Hello,I installed Windows 11 on Proxmox virtualization software.I directed the video card to the client system as GPU Passthrough.In general, the video card works fine, but all “omniverse” applications crash at startup.When you delete the following 2 files from the relevant application, the application opens, but it gives the following error, what do these files do?
carb.profiler-cpu.plugin.dll
carb.profiler-gpu.plugin.dllThen some Omniverse application closes itself after a certain period of time.Powered by Discourse, best viewed with JavaScript enabled"
239,instance-segmentation-with-pointcloud-generation,"Hello,
I am trying to generate pointclouds using the replicator with a custom writer. The data generated by the annotator includes the normals, positions, rgb, and segmentation. However, I would like to have also the instance segmentation of the points so I can keep track of the points of each instance pertaining to the same class.Is there a way to change the configuration of the annotator to get also the instance segmentation of points?Thank you for your time,Best
AnthonyHi @toninsemaan , currently the pointcloud only outputs semantic segmentation info per point. But we are undergoing a refracting and optimization for this annotator. We will support both instance and semantic segmentation for each point.Hello @jiehanw ,
Thank you for getting back to me.Great! Have you set the release date of the new feature / version?
Thank you again,
Anthony@toninsemaan It is TBD. But we plan to release it in the next version. Hopefully it will be next month.Powered by Discourse, best viewed with JavaScript enabled"
240,any-policy-on-loading-py-modules-with-dll-in-dependencies,"I wonder if there are any policies added to USD Composer 2023.1.1 Beta comparing to latest Create release that are controlling (actually blocking) py code from calling code in dll?
I have code that uses python_vlc
Code:import vlc
inst = vlc.Instance()does not return any value, inst is None when this code is run in the USD Composer 2023.1.1 Beta.
When I run this code in separate play.py file on the same PC, value of the inst is non empty.
The python_vlc uses dll that is located at C:\Program Files\VideoLAN\VLC\libvlc.dll on my PC.
I debugged python_vlc module’s code down to thedef libvlc_new(argc, argv)which does this:This code is a direct call of the dll’s function and it returns None in USD Composer.Powered by Discourse, best viewed with JavaScript enabled"
241,isaac-sim-crashing-while-launching-ros-navigation-example,"Hi Team ,I am trying to open completed USD files and basic ROS2 Navigation examples but isaac sim is crashing.The system configuration is :RTX 4080 , Ubuntu 22 , ROS Humble installed. Attaching a screencast to get better clarity on the issue.Regards,
Rafi
The log report2023-07-31 09:17:25 [68,848ms] [Fatal] [carb.crashreporter-breakpad.plugin] Thread 12 backtrace follows:
2023-07-31 09:17:25 [68,918ms] [Fatal] [carb.crashreporter-breakpad.plugin] 000: libpthread.so.0!funlockfile+0x60
2023-07-31 09:17:25 [68,989ms] [Fatal] [carb.crashreporter-breakpad.plugin] 001: libc.so.6!gsignal+0xcb
2023-07-31 09:17:25 [69,060ms] [Fatal] [carb.crashreporter-breakpad.plugin] 002: libc.so.6!abort+0x12b
2023-07-31 09:17:25 [69,060ms] [Fatal] [carb.crashreporter-breakpad.plugin] 003: 171!+0x102905
2023-07-31 09:17:25 [69,061ms] [Fatal] [carb.crashreporter-breakpad.plugin] 004: 171!+0x100e66
2023-07-31 09:17:25 [69,061ms] [Fatal] [carb.crashreporter-breakpad.plugin] 005: 171!+0x100ea1
2023-07-31 09:17:25 [69,061ms] [Fatal] [carb.crashreporter-breakpad.plugin] 006: 171!+0x100d03
2023-07-31 09:17:25 [69,061ms] [Fatal] [carb.crashreporter-breakpad.plugin] 007: 171!+0xf9be2
2023-07-31 09:17:25 [69,061ms] [Fatal] [carb.crashreporter-breakpad.plugin] 008: 171!+0xf593f
2023-07-31 09:17:25 [69,061ms] [Fatal] [carb.crashreporter-breakpad.plugin] 009: 171!+0xfab47
2023-07-31 09:17:25 [69,061ms] [Fatal] [carb.crashreporter-breakpad.plugin] 010: 171!+0xd1690
2023-07-31 09:17:25 [69,061ms] [Fatal] [carb.crashreporter-breakpad.plugin] 011: 171!+0xd4110
2023-07-31 09:17:25 [69,062ms] [Fatal] [carb.crashreporter-breakpad.plugin] 012: 171!+0xb55c8
2023-07-31 09:17:25 [69,062ms] [Fatal] [carb.crashreporter-breakpad.plugin] 013: 171!+0x92cdd
2023-07-31 09:17:25 [69,062ms] [Fatal] [carb.crashreporter-breakpad.plugin] 014: 171!+0x6e9cd
2023-07-31 09:17:25 [69,132ms] [Fatal] [carb.crashreporter-breakpad.plugin] 015: libcarb.graphics-vulkan.plugin.so!std::_Hashtable<void*, void*, std::allocator<void*>, std::__detail::_Identity, std::equal_to<void*>, std::hash<void*>, std::__detail::_Mod_range_hashing, std::__detail::_Default_ranged_hash, std::__detail::_Prime_rehash_policy, std::__detail::_Hashtable_traits<false, true, true> >::_M_insert_unique_node(unsigned long, unsigned long, std::__detail::_Hash_node<void*, false>)+0x7187
2023-07-31 09:17:25 [69,203ms] [Fatal] [carb.crashreporter-breakpad.plugin] 016: libcarb.graphics-vulkan.plugin.so!std::_Hashtable<void, void*, std::allocator<void*>, std::__detail::_Identity, std::equal_to<void*>, std::hash<void*>, std::__detail::_Mod_range_hashing, std::__detail::_Default_ranged_hash, std::__detail::_Prime_rehash_policy, std::__detail::_Hashtable_traits<false, true, true> >::_M_insert_unique_node(unsigned long, unsigned long, std::__detail::_Hash_node<void*, false>)+0x640e
2023-07-31 09:17:25 [69,275ms] [Fatal] [carb.crashreporter-breakpad.plugin] 017: librtx.postprocessing.plugin.so!_init+0x1de80
2023-07-31 09:17:25 [69,345ms] [Fatal] [carb.crashreporter-breakpad.plugin] 018: librtx.postprocessing.plugin.so!_init+0x2027a
2023-07-31 09:17:25 [69,416ms] [Fatal] [carb.crashreporter-breakpad.plugin] 019: librtx.postprocessing.plugin.so!_init+0x207c3
2023-07-31 09:17:25 [69,487ms] [Fatal] [carb.crashreporter-breakpad.plugin] 020: libgpu.foundation.plugin.so!std::vector<unsigned char, std::allocator >::_M_default_append(unsigned long)+0xdb39
2023-07-31 09:17:25 [69,557ms] [Fatal] [carb.crashreporter-breakpad.plugin] 021: libcarb.scenerenderer-rtx.plugin.so!void std::vector<int, std::allocator >::emplace_back(int&&)+0x142be
2023-07-31 09:17:25 [69,627ms] [Fatal] [carb.crashreporter-breakpad.plugin] 022: libcarb.scenerenderer-rtx.plugin.so!void std::vector<int, std::allocator >::emplace_back(int&&)+0x1b4c9
2023-07-31 09:17:25 [69,698ms] [Fatal] [carb.crashreporter-breakpad.plugin] 023: libcarb.scenerenderer-rtx.plugin.so!void std::vector<int, std::allocator >::emplace_back(int&&)+0x1bb63
2023-07-31 09:17:25 [69,768ms] [Fatal] [carb.crashreporter-breakpad.plugin] 024: libcarb.scenerenderer-rtx.plugin.so!void std::vector<int, std::allocator >::emplace_back(int&&)+0x4a7db
2023-07-31 09:17:26 [69,838ms] [Fatal] [carb.crashreporter-breakpad.plugin] 025: libcarb.scenerenderer-rtx.plugin.so!void std::vector<int, std::allocator >::emplace_back(int&&)+0x4e21b
2023-07-31 09:17:26 [69,909ms] [Fatal] [carb.crashreporter-breakpad.plugin] 026: libcarb.scenerenderer-rtx.plugin.so!std::_Hashtable<unsigned int, unsigned int, std::allocator, std::__detail::_Identity, std::equal_to, std::hash, std::__detail::_Mod_range_hashing, std::__detail::_Default_ranged_hash, std::__detail::_Prime_rehash_policy, std::__detail::_Hashtable_traits<false, true, true> >::_M_insert_unique_node(unsigned long, unsigned long, std::__detail::_Hash_node<unsigned int, false>)+0x1a917
2023-07-31 09:17:26 [69,980ms] [Fatal] [carb.crashreporter-breakpad.plugin] 027: librtx.hydra.so!carb::progress::ScopedLoadingEvent* std::__uninitialized_copy::__uninit_copy<std::move_iteratorcarb::progress::ScopedLoadingEvent*, carb::progress::ScopedLoadingEvent*>(std::move_iteratorcarb::progress::ScopedLoadingEvent*, std::move_iteratorcarb::progress::ScopedLoadingEvent*, carb::progress::ScopedLoadingEvent*)+0x78e4
2023-07-31 09:17:26 [70,051ms] [Fatal] [carb.crashreporter-breakpad.plugin] 028: librtx.hydra.so!carb::progress::ScopedLoadingEvent* std::__uninitialized_copy::__uninit_copy<std::move_iteratorcarb::progress::ScopedLoadingEvent*, carb::progress::ScopedLoadingEvent*>(std::move_iteratorcarb::progress::ScopedLoadingEvent*, std::move_iteratorcarb::progress::ScopedLoadingEvent*, carb::progress::ScopedLoadingEvent*)+0xae01
2023-07-31 09:17:26 [70,123ms] [Fatal] [carb.crashreporter-breakpad.plugin] 029: libomni.usd.so!std::_Function_handler<std::unique_ptr<std::__future_base::_Result_base, std::__future_base::_Result_base::_Deleter> (), std::__future_base::_Task_setter<std::unique_ptr<std::__future_base::_Result, std::__future_base::_Result_base::_Deleter>, std::thread::_Invoker<std::tuple<omni::usd::UsdContext::Impl::saveLayers(std::string const&, std::vector<std::string, std::allocatorstd::string > const&, bool)::{lambda()#1}> >, void> >::_M_invoke(std::_Any_data const&)+0x5e7a
2023-07-31 09:17:26 [70,193ms] [Fatal] [carb.crashreporter-breakpad.plugin] 030: libomni.usd.so!std::_Function_handler<std::unique_ptr<std::__future_base::_Result_base, std::__future_base::_Result_base::_Deleter> (), std::__future_base::_Task_setter<std::unique_ptr<std::__future_base::_Result, std::__future_base::_Result_base::_Deleter>, std::thread::_Invoker<std::tuple<omni::usd::UsdContext::Impl::saveLayers(std::string const&, std::vector<std::string, std::allocatorstd::string > const&, bool)::{lambda()#1}> >, void> >::_M_invoke(std::_Any_data const&)+0xbfe1
2023-07-31 09:17:26 [70,264ms] [Fatal] [carb.crashreporter-breakpad.plugin] 031: libomni.usd.so!std::_Function_handler<std::unique_ptr<std::__future_base::_Result_base, std::__future_base::_Result_base::_Deleter> (), std::__future_base::_Task_setter<std::unique_ptr<std::__future_base::_Result, std::__future_base::_Result_base::_Deleter>, std::thread::_Invoker<std::tuple<omni::usd::UsdContext::Impl::saveLayers(std::string const&, std::vector<std::string, std::allocatorstd::string > const&, bool)::{lambda()#1}> >, void> >::_M_invoke(std::_Any_data const&)+0xc9c6
2023-07-31 09:17:26 [70,336ms] [Fatal] [carb.crashreporter-breakpad.plugin] 032: libomni.usd.so!std::_Function_handler<std::unique_ptr<std::__future_base::_Result_base, std::__future_base::_Result_base::_Deleter> (), std::__future_base::_Task_setter<std::unique_ptr<std::__future_base::_Result, std::__future_base::_Result_base::_Deleter>, std::thread::_Invoker<std::tuple<omni::usd::UsdContext::Impl::saveLayers(std::string const&, std::vector<std::string, std::allocatorstd::string > const&, bool)::{lambda()#1}> >, void> >::_M_invoke(std::_Any_data const&)+0xcf41
2023-07-31 09:17:26 [70,408ms] [Fatal] [carb.crashreporter-breakpad.plugin] 033: libomni.usd.so!std::_Rb_tree<std::string, std::string, std::_Identitystd::string, std::lessstd::string, std::allocatorstd::string >::_M_erase(std::_Rb_tree_nodestd::string)+0x4f9f
2023-07-31 09:17:26 [70,479ms] [Fatal] [carb.crashreporter-breakpad.plugin] 034: libcarb.events.plugin.so!std::_Hashtable<unsigned long, unsigned long, std::allocator, std::__detail::_Identity, std::equal_to, std::hash, std::__detail::_Mod_range_hashing, std::__detail::_Default_ranged_hash, std::__detail::_Prime_rehash_policy, std::__detail::_Hashtable_traits<false, true, true> >::_M_insert_unique_node(unsigned long, unsigned long, std::__detail::_Hash_node<unsigned long, false>)+0x188c
2023-07-31 09:17:26 [70,551ms] [Fatal] [carb.crashreporter-breakpad.plugin] 035: libcarb.events.plugin.so!std::_Hashtable<unsigned long, unsigned long, std::allocator, std::__detail::_Identity, std::equal_to, std::hash, std::__detail::_Mod_range_hashing, std::__detail::_Default_ranged_hash, std::__detail::_Prime_rehash_policy, std::__detail::_Hashtable_traits<false, true, true> >::_M_insert_unique_node(unsigned long, unsigned long, std::__detail::_Hash_node<unsigned long, false>*)+0x2026
2023-07-31 09:17:26 [70,622ms] [Fatal] [carb.crashreporter-breakpad.plugin] 036: libomni.kit.loop-default.plugin.so!std::thread::_State_impl<std::thread::_Invoker<std::tupleomni::kit::RunLoopThread::run():{lambda()#1} > >::~_State_impl()+0x8ac
2023-07-31 09:17:26 [70,693ms] [Fatal] [carb.crashreporter-breakpad.plugin] 037: libomni.kit.loop-default.plugin.so!std::thread::_State_impl<std::thread::_Invoker<std::tupleomni::kit::RunLoopThread::run():{lambda()#1} > >::~_State_impl()+0xe8e3
2023-07-31 09:17:26 [70,765ms] [Fatal] [carb.crashreporter-breakpad.plugin] 038: libomni.kit.app.plugin.so!_init+0x28c7
2023-07-31 09:17:27 [70,836ms] [Fatal] [carb.crashreporter-breakpad.plugin] 039: libomni.kit.app.plugin.so!carbOnPluginPostShutdown+0x907
2023-07-31 09:17:27 [70,906ms] [Fatal] [carb.crashreporter-breakpad.plugin] 040: kit!_init+0x635
2023-07-31 09:17:27 [70,978ms] [Fatal] [carb.crashreporter-breakpad.plugin] 041: libc.so.6!__libc_start_main+0xf3
2023-07-31 09:17:27 [71,047ms] [Fatal] [carb.crashreporter-breakpad.plugin] 042: kit!_init+0x9cbHi @rafism1997  - Are you using ROS Navigation or ROS2 Navigation?
Please see the ROS/ROS2 compatibility in the official document: 6. ROS & ROS 2 Installation — isaacsim latest documentationPowered by Discourse, best viewed with JavaScript enabled"
242,custom-omnigraph-ros-publisher-node,"My understanding is Nvidia recommends OmniGraph for publishing ROS messages. There are some publisher nodes available, but not for the type of message I want to publish. I would like to implement a new publisher node type, preferably in Python. I know how to implement new OmniGraph nodes in general, but need some instructions/examples on the specific task of implementing a publisher node.Any advice welcome
BrunoLooking at some Omniverse code, it seems that there is a concept of “node writers” related to this. See omni.replicator.core.scripts.writers.register_node_writer()I would also be interested in understanding how to implement such a thing.Hi Bruno,
We have he ability to publish ROS Custom Messages but not yet a direct way of doing that for ROS2. This is being planned for the next release though.
However there is a workaround you can do by compiling the custom message packages for python 3.7 and include the binaries in the Isaac Sim repo . Check here for more details Custom Ros 2 messagesThanks @TeresaC,that is a good suggestion.
But this would work outside of OmniGraph, right?I was thinking about implementing a custom OmniGraph publisher node, but do not know how.Powered by Discourse, best viewed with JavaScript enabled"
243,siggraph-sizzle-reel-opportunity,"We have an exciting opportunity to contribute to our NVIDIA RTX highlight reel that will are planning to release for SIGGRAPH! The video will showcase the incredible capabilities of our NVIDIA RTX platform solutions for professionals.While this video will be created for SIGGRAPH, the goal is to continue sharing it at industry and customer events. Please let me know if there are any guidelines on how and where we can show your contribution.The deadline for submitting your video is Friday, June 2nd. Thanks for your consideration to be part of this exciting showcase for SIGGRAPH.If you are interested, please send the video link along with below details to me via email: edmar@nvidia.comFormat to submit video clips:Kindly include the following details with your submission:What is the rendering performance increase with NVIDIA RTX compared to previous gen?Describe how RTX accelerates ray tracking in your pipeline?What is the GPU rendering advantage over CPU rendering from pre-vis through final frames?How did/will the use of RTX reduce your time and cost on a production?Do you use AI accelerated software? If yes, answer questions below:What part of your workflow is accelerated with AI?Describe how AI accelerates your workflow as compared to before AI-augmented tools were available.How did/will the use of RTX and AI-augmented tools and applications reduce time and cost for your projects & programs?Do you use virtual/augmented reality? If yes, answer questions below:Describe how you use virtual/augmented reality in your workflow.How did/will the use of NVIDIA RTX and virtual/augmented reality reduce time and cost or provide other benefits for your projects & programsDo you use multiple displays beyond just multiple displays for your desktop (i.e., beyond just using multiple displays to give you more screen real estate on your desktop)? If yes, answer questions below:Do you use ultra-high-resolution displays or projectors?Describe how multi-display technology is used as part of your workflowDo you use or require multi-GPU synchronization for your multi-display solutions?How did/will the use of multi-display benefit your work?Content Submissions Due: Friday, June 2ndWe appreciate your time and contribution to this project. Should you have any further questions or require clarification, please do not hesitate to reach out via email or Discord (edmendi).Best,
EdmarPowered by Discourse, best viewed with JavaScript enabled"
244,kit-105-how-to-get-early-access,"Hi we would really like early access to the 105 Kit. How can we do so ?
Sincerely,
Suresh.Hi. Kit 105 was released today. You can download it from the Omniverse Launcher.This topic was automatically closed 14 days after the last reply. New replies are no longer allowed.Powered by Discourse, best viewed with JavaScript enabled"
245,error-installing-composer-2023-1-0,"Hey,
While trying to install new USD Composer I get following error in my launcher.Thank you for reporting this installation error. We will look into this. Could you first go to ""C:\Users\USER\AppData\Local\ov"" and delete the entire “packman-repo” folder and try again.Can you provide all logs found in this location ? C:\Users[user].nvidia-omniverse\logs\Kit\USD.Composer\2023.1 pleaseAlso, are you the user who originally installed Omniverse Launcher?
Did the settings for where to install apps change recently?Powered by Discourse, best viewed with JavaScript enabled"
246,can-not-turn-on-live-in-tutorial-using-jupyter,"Hi everyone.
I am following this tutorial but the cloud button is not activated can you walk me through the solution?
this is the tutorial page I am followingScreenshot from 2023-06-11 19-04-471880×983 181 KB
Screenshot from 2023-06-11 19-07-191880×983 210 KBhttps://docs.omniverse.nvidia.com/app_isaacsim/app_isaacsim/tutorial_core_hello_world.html#converting-the-example-to-a-standalone-application-using-jupyter-notebookHi @ahn.paf - You can use this documentation link to get more information about omni live: OmniLive — Omniverse Extensions documentationPowered by Discourse, best viewed with JavaScript enabled"
247,wrong-mtethod-signature-in-physicsutils,"The method add_rigid_cylinder in omni.physix.script.physicsUtils.py has in the signature the parameter axis set with default value Gf.Vec3f. Instead, it should be a string as in the method add_cylinder.Hi,
thanks for the find, will fill in a Jira and we fix this making it consistent.
Thanks!
Regards,
AlesPowered by Discourse, best viewed with JavaScript enabled"
248,how-to-display-data-from-a-csv-file-in-a-usd-file-on-an-object,"Hello everyone,
i am currently struggling with showing/displaying data in a usd file. I want to extract data (in my case temperature data which i collected) from a csv file an display it in a scene/in a usd file on an object, e.g. on a thermometer. Is there a way in Omniverse to do it ?Hi @m.herman. Yes, that is possible to do in Omniverse. It depends on what sort of result you’re going for. Is it a digital thermometer or old school mercury thermometer? Omniverse Kit has a rich SDK to be able to build something like this.Powered by Discourse, best viewed with JavaScript enabled"
249,what-is-this-little-blue-marker,"Did I click the wrong thing and turn this on? How do I turn it off.Thanksimage1920×1040 104 KB@DataJuggler I am pretty sure it’s a waypoint, more about them (and how to get rid of them) in the doc - Waypoints — extensions latest documentation (they’ll also be in your stage)I restarted Composer and it went away. I will read the docs.ThanksPowered by Discourse, best viewed with JavaScript enabled"
250,possible-memory-leak-in-kit,"We have a rendering server with on own kit agent application. this application starts a kit process, loads a stage and renders in image. after that this kit process is terminated (by calling the required python methods to clean up and shut down).every time a kit process is started and killed the overall memory load of the machine is slightly higher than bevor. we are using the latest stable linux divers.On a production system this is a big problem, because after rendering round about 1000 images this way, the overall memory consumption is about 400gb and at the end the machine stops responding.We will try to change the process in a way that the kit processes are not started fresh every time and are reused, but this takes some time. At the moment we have to restart the machine every day…Powered by Discourse, best viewed with JavaScript enabled"
251,in-z-up-usd-save-select-prim-to-new-usd-the-geometry-gets-flipped-like-y-up,"attach video using 2022, but we tried 2023.1.1 also the same
is this an expected behavior?Thank you :)スクリーンショット 2023-07-14 1001011036×638 113 KB
Isn’t it because the x and z in the data before exporting have 90 degrees in them? ?Hi @k-kitta
do you mean UI save selected won’t export the selected xform rotation is the expected behavior?Hi,  @WendyGram  , can you help to ask USD Composer RD about  this UI behavior,
saving selecting prim to new usd, (in this case swamp_location)
won’t save its rotation to the new  usd’s  default primPowered by Discourse, best viewed with JavaScript enabled"
252,usd-composer-2023-11-beta-now-available-hot-fix-for-people-with-installation-issues,"USD Composer 2023.11 Beta Now Available ! - Hot Fix for people with installation issues !Some people are reporting installation issues with 2023.1.0. They are getting various messages in Launcher relating to “packman-repo” or “pull_kit_sdk.bat”. If you have these issues, please try to update to 2023.1.1 and let us know on this thread if there are any other issues.I willl try now and report if I have any issues ;-)I cannot install it. My installation stucks with no error. It says above “INSTALLİNG EXTSCACHE/OMNI.KIT.WIDGET.MATERIAL PREVIEW-1…” in the green bar and it stays like that forever.Sorry to hear that. Can you try again ? Maybe your lost internet connection.I can install it, but cannot launch it. 2022.3.3 or 2023.2.2. RTX 4080 536.67 and win10. I see the splash screen and then blue window with “this app cannot be executed on this pc (in german)”, i need a compatible version for my pc?@usurpine it may be best to make a new post in the Composer section and supply the Launcher log from C:\Users\<user name>\.nvidia-omniverse\logs\launcher.log so the dev and others can help troubleshoot for/with you🙂(just wanted to avoid elongating this pinned thread)Powered by Discourse, best viewed with JavaScript enabled"
253,from-omniverse-to-sketchfab,"Anyone knows how to export USD scenes from Create to Sketchfab with materials?Please help if you can, this is a work assignment and I can pay you money.Hi @pekka.varis!  I know the development team is working on support for glTF file imports.   You might be able to export your SketchFab file as an FBX and import that into Create?  Let me know it that works.  I’ll reach out to the dev team to see if I can find out some more information for you!Thanks Wendy! I am looking for a way to export from Create to Sketchfab.
If I export fbx, I most commonly end up with a blank model without textures in Sketchfab.Hi Wendy,  I think Pekka meant exporting from Omniverse in order to import the scene into sketchfab.  So that his scene could be shared with others via the cloud.  I’m wondering the same thing.  When I take my USD file that was created in Omniverse, it loses all materials when imported into Sketchfab.Hi! What is the status of the Omniverse to Sketchfab import?Is there a method yet? Thanks.Powered by Discourse, best viewed with JavaScript enabled"
254,nvidia-assets-samples-environments-etc-are-not-loading,"Log.txt (21.5 KB)
please find the attatched log where i am getting error and the contents in the application are not loading.Hello @krishna.sriharsha!  I took a look at your logs and I am seeing issues with your cache.  Could you please try updating your cache to the latest version and seeing if that solves the problem?   You can do this by opening the Omnverse Launcher > Exchange tab and typing in “cache” into the search bar.  I see that the latest version as of now should be 2022.2.0.Also, please update your GPU drivers to the latest version.  You can look for your specific GPU here: Official Drivers | NVIDIAHi Wendy, I followed the same process . I have used clean up tool and reinstalled the cache and extensions and also updated my drivers. Still the same issue persists. Can you please help.Thank you for doing that.  Can you provide me with your your new logs generated from the application you are using so that I can share it with the dev team?Here is a link on how to get those logs if you need help with that: How to Report an issue with Omniversekit_20230608_230438.log (1.1 MB)
kit_20230608_231849.log (1.1 MB)
kit_20230608_235224.log (1.1 MB)
kit_20230608_235934.log (1.1 MB)
please find the log fileslog.txt (21.6 KB)
this is the newly generated console log fileThank you @krishna.sriharsha!  I am sending these right over to the dev team!Thank you @WendyGramHi @krishna.sriharsha , are you by any chance in an air-gapped environment (i.e machine without internet access / or with firewall restrictions)? It might be that you just can’t access the assets on the aws s3 bucket. Check if this helps User Guide — Omniverse Installation Guide documentationYes as mentioned please make sure you are on a good, stable internet connection and not behind a firewall. It requires an open connection to the Amazon servers.Hi Richard, I have good internet connection and there are no firewall issues. I have even checked with my IT team and they said the same, that there are no blockers either from network connectivity or firewall.Hi @Richard3D  I am also having a lot of errors trying to download assets from the aws servers. Are there any particular ports that we should open ?Looking at the logs, all of the errors are listed as the same issue… “Please check your network and connection to the url”. I am not sure why there is a connection error from you to the Amazon S3 servers, but it seems that is the most likely explanation. It could have been a temporary issue. It could be something specific blocking the traffic. As far as we know, everything is up and running and should work perfectly.I will look into the specific error messages and get back to you.Powered by Discourse, best viewed with JavaScript enabled"
255,lagging-in-ue5-2-when-connector-plugin-is-active,"hello!
In UE5.2 with connector version: 201.1.158 Every time when I try to move rotate or scale a static mesh, my computer’s vents go loud and the operations go lagging. Yesterday it was fine so it’s not my pc config. Can anyone help me please?
Only does it with static meshes, skeletal mesh is fine. I tried to delete intermediate and save folders, still not working.
Please let me know if you need further details!
I turned omniverse plugin off and all get back to normal. Turned back on, and now lagging again.
Thanks!Not sure if this is practical for you but since running UE5 and Composer side by side creates a scenario where both systems are pulling resources from the same gpu, I was thinking about having a second pc in a side by side setup, one for each program.It may not be set by default and I can’t recall the name of it, but I think there is a setting in Unreal where it will go into sort of a background mode if its minimized or if you jump over to another program, so its still on and running but not pulling maximum possible resources. This can be toggled if this has anything to do with your scenario.@muszti86, in 202.0 we made a change to “Improve the perf of moving a transform and changing the hierarchy in a large stage”.  I would encourage you to try the latest Connector version (now 202.1) and see if there’s any improvement.Oh, now that I see the version you listed, 201.1.158, you probably mistyped and meant to type 202.1.158, which is indeed the latest version.When you say that you “move rotate or scale a static mesh and things slow down”, is this just in any plain Unreal level, or specifically in a USD stage that you’ve opened?Dear Lou Rohan!
Now I can use it just fine.It was a mistake made by myself.
I was using a scene from my nucleus, then uninstalled the nucleus server and reinstalled it to another SSD drive. Ue5 had my omniverse connector actor in the hierarchy. With empty version of the models came out of usdz.
I deleted those empty game objects. And started to import usds as a model, not as a scene.
All is good now. Thanks for trying to help!LouRohan via NVIDIA Developer Forums <notifications@nvidia.discoursemail.com> ezt írta (időpont: 2023. júl. 25., K 23:48):Powered by Discourse, best viewed with JavaScript enabled"
256,audio2face-unreal-export-custom-metahuman-head-from-unreal-with-blendshapes-for-blendshape-solve,"I am trying to export metahuman head from Unreal Engine 5.1. I export as FBX and select export morph targets, but when I import into Audio2Face I cannot see the blendshapes anywhere and so I cannot do blendshape solve. How do i properly export metahuman head from Unreal?I believe Metahuman characters don’t have blendShapes. There’s this other post similar to this post which might give you more information: Can you import a Metahuman Mesh into Audio2Face? - #2 by yseolTo driver a Metahuman face using A2F, you might not need to bring your own character from Unreal to A2F. You can just export the blendShape weight animation from A2F to Unreal. Take a look at this video for example: Omniverse Audio2face Metahuman Tutorial - YouTubePowered by Discourse, best viewed with JavaScript enabled"
257,question-about-orbit-rl-environment,"I am trying to understand how the lift_env.py is working.lift_env.py:I have two questions:How action, which is from “def _step_impl(self, actions: torch.Tensor)” function, is generated?
Does it random;y generated from “self.action_space = gym.spaces.Box(low=-1.0, high=1.0, shape=(self.num_actions,))” ?In this script, they are two control types, which are joint control and inverse kinematics.
In the “def _step_impl(self, actions: torch.Tensor)” function, the argument, action, is the target position of the joint for joint control, which is normal, but somehow the action becomes the end effector desired position and orientation for inverse kinematics? Which does not make sense because the dimension for those two data is completely different. What is the mechanism behind it?Thank you.Hi,The actions are provided by the user as inputs to the environment. The suggested box range from (-1, 1) is the typical expected range of actions but the tensor shape should always be (num_envs, num_actions). You can look at the random_agent.py file in standalone/environments to see how the mechanism works.Switching between different control mechanisms (IK or joint control) is being handled by the configuration instance. If it says the control mode is “inverse_kinematics” then the input actions are expected like that and they get resolved inside the class to compute joint level commands from the IK control. If the control actions are default (i.e. joint), then they are passed to the robot as it is.Powered by Discourse, best viewed with JavaScript enabled"
258,alembic-file-imports-look-different-that-native-mesh-with-same-materal,"Check this out:
Why do they look so different even the have the same material assigned on them?
The animated water drops are Alembic file exports from Houdini.PekkaI found it!
It was the IOR value of the material.
This topic was automatically closed 14 days after the last reply. New replies are no longer allowed.Powered by Discourse, best viewed with JavaScript enabled"
259,where-do-the-content-packs-install-to,"I noticed on the bottom of Exchange tab in Create, there are some Content Packs, so I installed them (all 6). Some of these files are huge, but I don’t know where they installed to.I open up Create after installing, and I don’t see any difference vs what I had seen prior to downloading about 40 gigs of content.Does Create know I installed these things?Thanks.As a feedback tip, I would have preferred to use another drive for these as my C drive is getting full.Correction: I did notice a difference when I change tabs of the Materials. This used to take a little time to “connect” and now is much faster.Still not sure how to see the Residential Assets installed.Hello @DataJuggler!When you download a Content Pack from the Exchange, if you click on the Save As button you should bring up the path where the content pack will be saved.
content11241×690 49.4 KB


content21246×696 176 KB
You can change where these packs are saved and set it to remember that path.Let me know if this helped.  I may need more information to help answer your question!I think I clicked Download without clicking the box to the right.Thanks for the reply.I have since found the folder. My ov folder is in my [User] folder:[User]\appdata\local\ov\cache is over 160 gig.I also found the pkg folder is over 67 gig[User]\appdata\local\ov\pkgThis is mostly because old versions of kit apps are not removed.I am writing a Kit App removal tool today since I am off for the long weekend. I will post it on Git Hub if anyone wants it once finished.When you install new versions of Kit apps, asking the user to remove old ones would be helpful. I will create a separate post for this once the removal tool is done.I have 6 gigs just in old copies of Create.
image884×720 248 KB
By default my content packs were downloading to the windows “download” folder.  I started to copy from there onto the omniverse local host, but then realized some of the content looks familiar.Question:  are most or some of these content packs already installed with the base omniverse install?  I don’t want to download them twice if they’re all there already.Powered by Discourse, best viewed with JavaScript enabled"
260,how-to-edit-the-example-scenarios-in-issac-sim,"Hi, I 'm trying to replace the bins in the UR10 example scenerio. can someone help me out here please.Hi, I 'm trying to replace the bins in the UR10 example scenerio.Hi @user122836 - Sure, here are the steps to replace the bins in the UR10 example scenario in Isaac Sim:Powered by Discourse, best viewed with JavaScript enabled"
261,with-4-vdb-s-composer-is-still-crashing-quite-a-lot,"However, it is better that Create, thanks!
If you want to check out my win 10 / A6000  logs, they are here…Few crashes just happened so you can find those from the logs.logs.zip (92.4 MB)Powered by Discourse, best viewed with JavaScript enabled"
262,isaac-sim-crashed-on-launch-isaac-sim-2022-2-1-and-isaac-sim-2022-2-0,"I am developing an application for a robot and want to create a simulation in Isaac Sim.
I had exactly the same problem Isaac_sim crashed when I clicke ""load world"" in my extension when running isaac_sim-2022.2.1 and isaac_sim-2022.2.0
The driver has been installed.
I am using ubuntu 20.04When I use command “isaac_sim-2022.2.1/isaac-sim.sh -v”, then I get an error:Logs
logs-isaac.zip (1.2 MB)Hi. There  is an error in your logs.What GPU are and drivers are you using?Please reinstall the “Latest Production Branch Version” drivers.Thank you. Indeed, there were problems with the video card. The ticket can be closed.Powered by Discourse, best viewed with JavaScript enabled"
263,my-computer-is-in-the-shop-will-omniverse-run-on-a-1080ti,"This is Data Juggler, I am currently locked out of my email thanks to Microsoft forcing me to use a 2FA authentication app called Authy.No one told me that if my hard drive ever crashed, I would have to wait up to 30 days for the Microsoft Data Protection Team to contact me.Not only does this lock me out of Microsoft, Nvidia wants to email me a code, and I have other services I use that operate this way.(End Microsoft Rant).I have an old computer up and running, and until the computer hospital tells me if my computer is savable or if I have to take my RTX3090 out of and put it in this one (when I get it back).I read you have to have a 2080 or something, but I don’t care if it is slow if i can use it. Is this possible?Thanks, 2FA is meant to keep out bad guys, but at the moment I would have been back up in running in 1 hour without it, so no one can convince me it is worth it.Corby / Data Jugglerimage1024×1024 114 KBUnfortunately trying to use omniverse on a 1080 is well below the minimum specs.Powered by Discourse, best viewed with JavaScript enabled"
264,issac-sim-launch-issue-using-omniverse,"I installed Isaac SIM, Nucleous and Cache using this instructions. Once I am trying to Launch Isaac SIM, without any error on the Omniverse client it just never launches Isaac SIM GUI.Server: Amazon EC2 with 4 NVIDIA A10G Tensor Core GPUs
OS: Ubuntu 20.04
NVIDIA-SMI: 535.54.03Hello,I am moving this topic to the Isaac Sim category for better visibility.Hi @asriaws  - Possible for you to share the log file?Powered by Discourse, best viewed with JavaScript enabled"
265,omniverse-vscode-dubug-configuration,"Using launch.json configuration to show a connection failure, is there something wrong with my configuration? What are the functions of pathMapping and runtimeArgs in your JSON file?Should I set my own working path address behind the WorksapceFolder?
“remoteRoot”: “${workspaceFolder}”,
“localRoot”: “${workspaceRoot}”,
I don’t know what to put in the path here? Like the kit.exe path to createHi. Those paths should be correct for most cases. They’re used to inform debugpy. Does your VSCode $workspaceFolder have an “app” symlink? This should be created automatically or by running the link_app.bat in the project. Also, have you selected the Kit python interpreter in VSCode?I think only host and port really important, for it just works. Maybe port is not available? What the error says?
image1917×972 163 KB

11920×952 55.8 KB

According to your configuration, I cannot import PXR and access its source codeSorry if my reply confused you. You shouldn’t need to edit the launch.json unless to try a different port number like @anovoselov suggested. Here’s what my launch.json looks like:

image1022×666 37.5 KB
Let us know if you’re getting any error messages when you try to start the debugger that might help us narrow down the cause.VS Code may generate a wrong version of the launch.json which looks like (in my case):This json has smth wrong in it, as it is possible to connect to the CREATE from VS Code, but debugger catches no breakpoints. I use VS Code 1.79.1. Probably VS Code, or some plugin that knows about python debugging, prepares json in a wrong way.
A json that mati-nvidia has posted is the one that brought back my debugger to the proper work.Powered by Discourse, best viewed with JavaScript enabled"
266,exporting-skinned-objects,"Will the Omniverse USD exporter support exported skinned model like the built in 3DS Max USD plugin does?Yes - latest version (v201.0) exports skinned meshes.This topic was automatically closed 14 days after the last reply. New replies are no longer allowed.Powered by Discourse, best viewed with JavaScript enabled"
267,omniverse-xr-blurry-vr-scene,"Hello,
the VR Scene appears to be very blurry in the RTX - Interactive (Path Tracing Mode). See image below. Is there any way to increase the quality or is that Rendering mode simply not meant to be used in VR?I tried with the following set-ups. Non of which would achieve a rendering quality near to what you demonstrate online.Thanks. MatthiasGoggles (Tethered)
Quest 2 & Quest 2 Pro, HTC Vive Pro & HTC Vive Pro 2pc 1
Processor	11th Gen Intel(R) Core™ i9-11900KF @ 3.50GHz   3.50 GHz
Installed RAM	64.0 GB (63.8 GB usable)
NVIDIA GeForce RTX 3080 Ti 12GBpc 2
Processor	Intel(R) Xeon(R) CPU E3-1505M v5 @ 2.80GHz   2.81 GHz
Installed RAM	16.0 GB (15.9 GB usable)
NVIDIA GeForce RTX 3070 Eagle OC 8GBpc 3
Processor	Intel(R) Xeon(R) Gold 6248R CPU @ 3.00GHz   3.00 GHz  (2 processors)
Installed RAM	512 GB (511 GB usable)
NVIDIA Quadro RTX 8000 48GB  (2 graphic cards)pc 4
Processor	Intel(R) Xeon(R) Gold 6248R CPU @ 3.00GHz   3.00 GHz  (2 processors)
Installed RAM	512 GB (511 GB usable)
NVIDIA Quadro RTX 6000 24 GB  (4 graphic cards)
image1398×521 45.1 KB
Hello @helmreich!  I’ve shared your post with the dev team for further assistance.In the meantime, take a look at our documentation if you haven’t already.  I may have some helpful information related to your questions.  Omniverse Create XR — Omniverse Create XR documentation(Just to note: Omniverse VR User start point is a continuation of this forum post)Hi,Here are my settings. I hope this helps.I’m very pleased (and impressed) with the rendering quality, especially in a simple environment like this.With any of your setup described you shouldn’t be getting that result.One thing that helps is to enable Foveation. That technique renders low quality outside of your focal view, enhancing the performance. Explore a little bit on that and you may have a better quality on VR.My setup is:Intel i9 - 9th Gen
64 GB RAM DDR4
RTX2070 Super 8GB
Liquid cooled
RiftS and Quest 2
LG C1 OLED 48in monitorI bought both RTX4080 (still have to install) and Quest Pro (waiting for delivery).
Screenshot 2022-11-20 1001331920×1036 112 KB
More advanced settings
Screenshot 2022-11-20 102903951×1080 67.9 KB


Screenshot 2022-11-20 102834967×1989 119 KB
Hey @barizon,Path Tracing mode isn’t currently supported in VR. We made it possible to preview this mode, but with effort (you need to manually set it). While we’ve been having fun with Path Tracing in VR, we don’t have a timeline for resolving all the issues with it yet, but we’re optimistic.I can’t get it to work clearly with RTX mode, let alone path tracing.  It’s a blurry mess.  Anyone have any suggestions on what I could try?  I’ve attempted adjusting every setting I could find.I have a RTX4090 and a Oculus Pro.Powered by Discourse, best viewed with JavaScript enabled"
268,error-on-nucleus-stack-ssl-yml,"Inside the file nucleus-stack-ssl.yml PUBLIC_URL is duplicated for the nucleus Authentication Service.In line 280 we have the first key PUBLIC_URL: https://${SSL_INGRESS_HOST}:${SSL_INGRESS_PORT}/omni/auth/login/However in the line 321 the same key is repeated
PUBLIC_URL: https://${SSL_INGRESS_HOST}:${SSL_INGRESS_PORT}/omni/auth/login/YML cannot have 2 keys named the same within the same indentation.
This was messing with nucleus authentication.Hi @Pappachuck_renan - This is a known issue and is fixed in the forthcoming release of the Nucleus Enterprise artifacts.  (Release should be 2023.1.0 and be out very soon.)Thanks for reporting!Powered by Discourse, best viewed with JavaScript enabled"
269,stream-audio2face-blendshapes-between-two-computers,"Hello everyone,
I’ve been using Audio2face for all my projects recently, and with the latest update, it’s more than great after adding the Livelink capabilities. My only problem is that my computer couldn’t handle it while streaming to another program, which is UE5 in this case (I think it’s time to upgrade for me). I tried to export USD facial animations into UE5, so I work in one program at a time. It worked great but with no head movement, and I don’t like it this way because I’ll add extra work to combine facial animation and other animations. I want that simple head movement. So, what came to my mind is my other computer (Both have RTX cards). I want to know if I can stream the Audio2face data from one computer to another because I won’t upgrade my setup now. I want a quick solution for now!I’m sorry for this long story; here are my questions:
1. Is it possible to include all the data from Audio2face in the USD file as it is seen when connected in Livelink, and how?
2. Is it possible to stream data from one computer to another?
3. Is it possible to make it faster by reducing quality or disabling features?This is my first topic, and I hope I followed the rules. Thanks!Is it possible to include all the data from Audio2face in the USD file as it is seen when connected in Livelink, and how?Is it possible to stream data from one computer to another?Screenshot_101127×610 72.8 KBIs it possible to make it faster by reducing quality or disabling features? Thank you so much for your answers, but regarding the first answer, exporting the USD file only includes the facial animation without head movement, but when connected through Livelink the head moves. I’m not sure. Maybe I’m missing something!Powered by Discourse, best viewed with JavaScript enabled"
270,build-an-avatar-with-asr-chatgpt-tts-and-omniverse-audio2face,"Below, I present the results of my work using NVIDIA Audio2Face and ChatGPT to create a basic interactive virtual human. Users can engage with it through voice input and have interactive conversations.This is an update to my previously published article on a simple interactive conversational virtual human technology. It has been a year since I last wrote about it, and I finally have the time to release new content. Over this past year, there have been significant developments, including improvements to Audio2Face and the launch of ChatGPT. With these convenient AI tools, creating a more convincing and lifelike virtual human experience has become easier than ever before.I have published the source code for this micro-project on my GitHub repository. Feel free to download it from: https://github.com/metaiintw/build-an-avatar-with-ASR-TTS-Transformer-Omniverse-Audio2Face/tree/main/2.Avatar_With_ChatGPTUsing this Github repo to build the avatar is straightforward. Just use Anaconda to create a Python virtual environment with avatar_requirements.yml.Open claire_audio_streaming.usd in the USD_files folder using NVIDIA Audio2Face (Version 2023.1.0).Finally, activate the Python virtual environment and run build-an-avatar-with-ASR-TTS-ChatGPTOmniverse-Audio2Face.ipynb.Please note that you need to have a ChatGPT account and token to use the ChatGPT API in the “build-an-avatar-with-ASR-TTS-ChatGPTOmniverse-Audio2Face.ipynb” notebook. You should input your ChatGPT token to access the API. Instructions on how to obtain the token can be found within the notebook.Once you have completed the above steps, you can start experiencing this simple virtual human application.I will update the documentation on the GitHub repo and this article to provide more details about the development process. I hope this content is helpful to you.Powered by Discourse, best viewed with JavaScript enabled"
271,audio2face-saas,"Hello Everyone,We are trying to build a application similar to MISTY. Is there a way to programmatically use the Audio2Face plugin without the Omniverse kit?
I want to use the Audio2Face in our python codebase; The text to speech input will be provided to the Audio2Face plugin which in return should output us the sample 3D model with lip sync.Also is there any repo which contains the end to end setup for MISTY application.Thanks.Hi, thank you for the interest.Yes, with TTS you can stream the output to A2F audio player and with the right setup you can drive A2F this way.  For misty we had a custom Jarvis client API to do this for us.  That part is not hard to build.  With more interest in this, we will consider including this in future release so is easier for anyone with any audio stream or input to drive A2F directly.Right now there are no exact repo for Misty.  We do have a standalone app for Misty that we build using these technologies.  But that is not available for download at the moment.ThanksHey @siyuen, we’re also interested in this! We’re looking at generating lip sync for a character in Unreal Engine, with the TTS coming from an external source. We’d like to take that TTS and generate (and play) the resulting animation in realtime on the character as soon after the TTS is received as possible.What do you think the flow for this would look like in Unreal Engine? Would it be something like this:Is this the right kind of flow? Can Audio2Face somehow run natively in Unreal or outside of the Omniverse application?You also mentioned about a custom Jarvis client to stream the output to Audio2Face. I can’t really see where the hooks would lie for this, is this something I can currently do by tinkering with Omniverse Kit? Or is there an SDK somewhere I’m missing?Thanks! :)Hi @charisma-ben ,I think it could be something like this:TTS = (ie: Jarvis or some audio signal)I think the flow would be like this:
TTS > Audio2Face > UEWe currently don’t have an API to take TTS audio stream to Audio2Face as input (what we did on Misty) but it is not hard to do actually with existing Python audio libraries.  The more people asking for this, the more we will consider releasing something like this so is easier for people to connect other audio input to Audio2Face.The A2F > UE part can first try the Omniverse UE connector.  It should be able to get the results out to UE live.Hi Team,We are planning to stream the TTS audio stream to Audio2Face using the existing Python audio libraries as mentioned in the above comment.
Please can anyone guide us with the steps or the python library to create the application like MISTY?Hi @BCSAudio2Face, we are looking at this internally right now and it most likely will need some updates on Audio2Face to make this easier for users to integrate TTS / streaming audio to directly drive Audio2Face.  The more inquiry we get about this the more it can help us prioritize this feature.We are looking into adding this support officially like the Jarvis Client Library API I mention above so stay tuned for updates.@dkorobchenko for visHi Team,Is there any way for us to build a standalone app similar to the Misty application with the existing resource?
If there is a way can you suggest us the steps to proceed with the development?We are trying to build application similar to Misty, please suggest us with some solution to start the development.Hi @BCSAudio2FaceHi,
is there any update available on releasing the API that would allow connecting external audio so that a video bot like Misty could be developed?No update yet.  It is something we are working on.  We will definitely let people know when we get closer to release dates.  But we are working on it and we like to make it easy to empower power to do exactly just that with Audio2Face.  Stay Tuned.@BCSAudio2FaceConnecting TTS is not so difficult.Here’s a sample snippet, where audio_buffer is your audio buffer (here assumed to be int16 and 48khz sample rate), and player_instance is AudioPlayer node instance and a2f_instance is Audio2Face node instace:Blockquote
audio_data = np.frombuffer(result.audio_data, dtype=np.int16)
audio_data = (1.0 / 32768.0) * audio_data.astype(np.float32)
track = omni.audio2face.core.a2f.audio.AudioTrack(audio_data, 48000)
player_instance.set_track2(track)
a2f_instance.set_a2f_track(player_instance.get_player().track)
player_instance.play_audio_player()When using mesh transfer, the graph is more complicated, but the same mesh is being driven, so the core concept does not change.Note that 48khz will be resampled, Audio2Face expects 16Khz as far as I can tell.I changed stream from microphone to any URL. Can you instruct me, how to animate the face in web as Misty was made?Audio2FaceWe have been looking for the most realistic lip-sync solution for quite some time.
Audio2Face is perfect. However it does not work along side are current stack, so please let us know once you guys have any type of beta SDK or RESTful API for Audio2Face also Unity is a huge part of our development stack. Are there any plans to support Unity with Audio2Face. Currently we are using speech blend however the results are far from perfect. I can see Audio2Face would solve this problem for us.Once you have some sort of SDK or API we would be happy to contribute towards the development.
Please let us know once you guys have something we can test, happy to collaborate with you on this.
Thanks.Definitely looking forward to this. Being able to connect Audio2face with Epic Games’ Unreal Engine to generate lip sync on the fly like it happens in the live mode would be beyond amazing.Hi thanks to the new audio player streaming I was able to send sounds to audio2face using GRPC protocol (client.py) and generate animations. But now I’m stuck receiving the output from audio2face. Can you guide me how to recieve back the  animation output from audi2face using GRPC ??Hello,I was wondering if there’s any update on an Audio2Face API? I’m using audio recordings from voice actors and converting them to animations so that I can import them into UE and apply them to Metahumans. I was wondering if there was an API available for use so that I could automate the process of creating the animations using a python script so that I could take in name.wav and export anim_name.wav files.Is there another process that you can suggest I use to automate this?Thank you.Hey Aliannea,
We are working on implementing a Rest API and headless mode to assist in batch processing audio. We are targeting this feature for our next release, which we are hoping to have out before the end of the year.Sounds great. Thank you, Will!Does “batch processing” mean the new release won’t be able to do “live streaming”?Our goal (and it seems like many on this thread) is:Or would this not be online processing and instead 3D data generation after the fact? (Regarding your next planned release)Would you be willing to share your code for how to stream to Audio2face?Powered by Discourse, best viewed with JavaScript enabled"
272,color-each-side-of-the-cubeg,"hello, i want to color each side of the cube in isaac sim like link below.however,  I don’t know how to import the cube_multicolor.urdf in isaac sim.
I think the link support only isaac gym.In isaac sim, how to import cube_multicolor.urdf?
I want to not only load the file, but transform it and use it.
or
how can i color each surface of the cube in isaac sim?if you know, please tell me howHi @hoe8279  - In Isaac Sim, you can import URDF files using the URDF Importer extension. Here are the steps to do so:Once the URDF file is loaded, you should see your multicolored cube in the viewport.To transform the cube, you can use the Transform Manipulator tool in the viewport. Here are the steps:As for coloring each surface of the cube, you can do this by modifying the material properties of each face. Here are the steps:Repeat these steps for each face of the cube to give them different colors.Thank you for your reply.
I’m not sure where the cube_multicolor is.
Can you tell me the route?I believe you need to know where you saved your cube_multicolor.urdf file.Powered by Discourse, best viewed with JavaScript enabled"
273,please-help-create-is-crashing-my-whole-computer,"Create seems to be crashing my whole computer. Even with nothing loaded into the scene I see my memory and virtual memory climbing to a point and then my whole computer shuts down.Its happened every time I’ve opened create, within about 5 minutes and no amount of population or depopulation in the scene seems to make a difference.Running a Threadripper3970X, 3090RTX, 128GB Ram, on Windows 11’- all 980evopro NVME hard disksSYSLOG shows an error with a VM switch but doesnt give me much more.
Its not CPU overheating
Its not GPU over heating and cant see anyother issues except the VM switch error on the logs.Any help would be appreciated.
I am running GPU and CPU through the program.Hi there,
Thanks for posting. I am sorry to hear you are having such serious issues. Let’s run through some stuff.Hey Richard,
Thanks for the reply.Yes it’s shutting down my whole computer. No warning, no static, no blue screen, just a hard shutdown and no restart… it just hangs on the restart so I have to clear the system - switch off and hold down the power button for ten secsyes I selected CPU and GPU. I’ll try change back to GOU but haven’t actually rendered anything - not getting anywhere near that far before shutdown.Fixed - only when running blender simultaneously otherwise stable - that does mean that I cant use models accross platforms thoughYes that sounds like you are running some other heavy GPU apps at the same time and its causing some kind of hardware or driver failure. The other possibility is that your gpu or power supply is close to failure and the heavy gpu load is causing it to hard fail.Powered by Discourse, best viewed with JavaScript enabled"
274,a-quick-question-about-the-omnigraph-draw-screen-space-text,"Hello,I’d like to use the “Draw Screen Space Text” Graph to show some text in the viewport. But the problem is that I have 3 viewports, how can I configure the “Draw Screen Space Text” node, so the text can be shown in the desired Viewport, or at least the default one.I currently have the viewports with the names “Viewport”, “Viewport 2” and “Viewport 3”, and the text popped up in “ViewPort 2”, but it want it to be shown in “Viewport”.RegardsXiaozhuHi @juxiaozhu  - The “Draw Screen Space Text” node in Isaac Sim allows you to display text in a specific viewport. However, the node might not provide an option to select a specific viewport by name.Here are a few workarounds you can try:Hello, rthakerThanks for your reply. The first option seems more proper for us, and we will try it out.RegardsThis topic was automatically closed 14 days after the last reply. New replies are no longer allowed.Powered by Discourse, best viewed with JavaScript enabled"
275,how-to-build-a-python-app-from-connect-sample,"Is it possible to do similar things like Creating an Omniverse USD app from the Connect Sample in python?This is a great idea.  I have created an internal task [OM-101034] to track your suggestion.On Windows, a good start would be to look at the run_py_hello_world.bat script.  It is setting up the PATH and PYTHONPATH environment variables (though this is possibly no longer effective).  Now that we’re using Python 3.10 it’s important to use os.add_dll_directory() since Python no longer loads DLLs from PATH or CWD.  At the top of source/pyHelloWorld/helloWorld.py there is this code block:If you were making your own Connector you’d need to copy all of the dependencies into a particular place (see the copy directive in prebuild.toml), then you’d need to add that place to your Python DLL directories list.Powered by Discourse, best viewed with JavaScript enabled"
276,problem-for-contact-sensor,"Hi, I am trying to implement a contact sensor into my robot.https://docs.omniverse.nvidia.com/py/isaacsim/source/extensions/omni.isaac.sensor/docs/index.htmlThe problem is, the get_sensor_readings only functions when we set the defaultPrim in our stage.And, for some reason, when I set the defaultPrim, the error will pop pot due to the urdf file problem.May I know if we can listen to the contact sensor without setting the defaultPrim? Thank you in advance!Hi @berternats  - The get_sensor_readings function is designed to work with the default prim in the stage. This is because the function needs to know which prim (or object) in the scene it should get the sensor readings from.If you’re having trouble with setting the default prim due to a URDF file problem, it might be worth trying to resolve that issue first. The URDF file defines the structure and properties of the robot, and if there’s an error in the file, it could cause problems with the simulation.However, if you want to listen to the contact sensor without setting the default prim, you might need to modify the get_sensor_readings function or create a new function that can get the sensor readings from a specified prim. This would involve accessing the sensor data directly from the physics engine, which might require a deeper understanding of the Omniverse physics API.This topic was automatically closed 14 days after the last reply. New replies are no longer allowed.Powered by Discourse, best viewed with JavaScript enabled"
277,how-to-use-randomizer-tool-on-surface-instancer,"Hello Omniverse devs!Please see this video, its alembic file imported to Composer ( saved as new USD ) and duplicated to plane with Surface Instancer, then I try to randomize the position and most important - the rotation.But as you can see, I do not manage to get any variation or randomization on rotation, but this tool is supposed to do it. What am I doing wrong.Also, how can I make randomization on time value, so they would not play back 100% synchronized but each at different time?PekkaThe only way I found out was by manually duplicating my usd assets and lay them out in seq timeline and then use Randomizer to move & rotate them:Powered by Discourse, best viewed with JavaScript enabled"
278,cannot-find-modulus-extensions-in-create-2022-3-3,"Hello Omniverse,I am trying to how Modulus extensions works with Omniverse. But from this page: Modulus Extension — Omniverse Extensions documentation (nvidia.com).It only provide tools in Ubuntu. Does Omniverse provide libnvtoolsext1 for CentOS 7 or 8? How to install Modulus connector on CentOS?ThanksThanks!It seems to me that it will only work with Omniverse USD Composer 2022.2.0 or older according to Modulus Extension — Omniverse Extensions documentationI’m very anxious trying to use the MODULUS.Powered by Discourse, best viewed with JavaScript enabled"
279,exception-failed-to-get-dof-stiffnesses-from-backend-fixedjoints-only-robots-not-allowed-to-create-articulation-view,"Hello,I was trying to run my custom RL task that I created following the guidelines under the OmniIsaacGymEnvs library (OmniIsaacGymEnvs/framework.md at main · NVIDIA-Omniverse/OmniIsaacGymEnvs · GitHub), when I got the following exception that crashed the simulation:Exception: Failed to get DOF stiffnesses from backendFull error trace is : “Traceback (most recent call last):
File “scripts/random_policy.py”, line 55, in parse_hydra_configs
task = initialize_task(cfg_dict, env)
File “/home/matteo/Projects/OmniIsaacGymEnvs/omniisaacgymenvs/utils/task_util.py”, line 72, in initialize_task
env.set_task(task=task, sim_params=sim_config.get_physics_params(), backend=“torch”, init_sim=init_sim)
File “/home/matteo/Projects/OmniIsaacGymEnvs/omniisaacgymenvs/envs/vec_env_rlgames.py”, line 51, in set_task
super().set_task(task, backend, sim_params, init_sim)
File “/home/matteo/.local/share/ov/pkg/isaac_sim-2022.2.0/exts/omni.isaac.gym/omni/isaac/gym/vec_env/vec_env_base.py”, line 80, in set_task
self._world.reset()
File “/home/matteo/.local/share/ov/pkg/isaac_sim-2022.2.0/exts/omni.isaac.core/omni/isaac/core/world/world.py”, line 282, in reset
self._scene._finalize(self.physics_sim_view)
File “/home/matteo/.local/share/ov/pkg/isaac_sim-2022.2.0/exts/omni.isaac.core/omni/isaac/core/scenes/scene.py”, line 290, in _finalize
articulated_view.initialize(physics_sim_view)
File “/home/matteo/.local/share/ov/pkg/isaac_sim-2022.2.0/exts/omni.isaac.core/omni/isaac/core/articulations/articulation_view.py”, line 218, in initialize
self._default_kps, self._default_kds = self.get_gains(clone=True)
File “/home/matteo/.local/share/ov/pkg/isaac_sim-2022.2.0/exts/omni.isaac.core/omni/isaac/core/articulations/articulation_view.py”, line 1664, in get_gains
kps = self._physics_view.get_dof_stiffnesses()
File “/home/matteo/.local/share/ov/pkg/isaac_sim-2022.2.0/kit/extsPhysics/omni.physics.tensors-104.1.6-5.1/omni/physics/tensors/impl/api.py”, line 225, in get_dof_stiffnesses
raise Exception(“Failed to get DOF stiffnesses from backend”)
Exception: Failed to get DOF stiffnesses from backend”The problem resides in the “/omni/isaac/core/arti culations/articulation_view.py” file while trying to use the “self.get_gains(clone=True)” function: if trying to get the joints stiffness it doesn’t receive any value, the program crashes. (a working alternative for objects with fixed joints without stiffness seems to not exist).My workaround for now was to create a fake joint that satisfies the “join stiffness” retrieval attempt allowing the simulation to not crash.Maybe there could be an update to accommodate the use of  FixedJoints?Many thanks,
MatteoHi Matteo,Thanks for reporting this issue. Would it be possible for you to share the asset you are using? If the articulation only consists of a single fixed joint, calling get_dof_stiffnesses() should return an empty list. Alternatively, you can also use the RigidPrimView and treat the bodies as individual rigid bodies if they are only connected by a fixed joint.Hello Kellyg,Below is the screenshot of the asset I’m using (the last Xform is the “fake” body I added to create the RevoluteJoint as a workaround for the aforementioned issue. 
Screenshot from 2023-01-31 15-54-57.png576×549 30.8 KB
I am already using RigidPrimView to treat the bodies as individual rigid bodies connected through a fixed joint.Hope this helps,
MatteoHi @matteohariry , in order to help you further team will need the access to your asset. Is it possible for you to share it with @kellyg?Hi rrhaker,Here is the asset Matteo is using:
fp3.usd (47.2 KB)It includes the dummy joint that allows us to use the robot with OmniIsaacGymEnv.For context, without the dummy it means that self.num_dof is equal to 0,Doing something like that seems to solve this issue:But then it breaks in get_dof_position_targets with the following error:Cheers,AntoineAny updates on this? I have the same problem @ARi_31Hey, we are still using dummy jointsPowered by Discourse, best viewed with JavaScript enabled"
280,feature-request-thumbnails-for-various-texture-img-files-in-content-browser,"Hello team! I’d love to request working thumbnails for various texture and image file extensions in the Content Browser of Create, this would make it much easier to do texture work on different projects, instead of having to rely on file names or clicking on each file to view them in a bigger window.What puzzles me is that according to one of the video tutorials by Nvidia on youtube, the thumbnails are seemingly working, displaying his folder with .png image files just fine.As can be seen on the 7:50 minute mark:Thanks in advance!Thanks for your comment. Actually you should already been able to browse various types of texture files with Create. I am not sure why it is not working for you. Please send a video of what you are seeing.@Richard3D I suspect they are seeing no image at all and simply the default thumbnail as seen in attached screenshot:and, not to steal the post from OP, but does Composer support .dds thumbnail at all?Thumbnails are not working for me neitherPowered by Discourse, best viewed with JavaScript enabled"
281,have-anyone-had-success-with-mecanum-wheels-on-isaac,"Have anyone had success with Mecanum wheels on isaac?our Kaya robot uses mecanum wheels. you can find it here:@qwan
Out team  pointed out that Kaya robot doesn’t use mecanum wheels
Kaya robot uses omniwheels-BR
AV

Yes, I’m working on a model with four Mecanum wheels.  I don’t have data yet to know if it’s moving precisely like the real deal, but so far the movement looks correct.Hi @daniel.romaniuk1  , can you please share your process of development here… It would be great if you can give some insights on the navigation of the robot in the environment also.Hi @rafism1997 .  There have been a few challenges in getting a stable model.   The robot we’re modelling has an incomplete CAD file, so we had to put a lot of time on the wheels.  We did the 3D model work in Blender then imported to Isaac Sim and added joints and physics.  Tricky bit is getting the joints for those little rollers at the right positions and orientations.The robot makes lots of little jumps and sometimes goes airborne unless you do some additional tweaks:The robot moves slower if you run physics on the GPU instead of the CPU.Hello @daniel.romaniuk1
Thank you very much for your input.
I have tried to follow your advise, but my robot still shows signs of self collision
Could you explain a bit more how did you do the:I will leave a small video of my robot trying to walk in the y direction.Any help would be well appreciated.Hi Bezerra,From what I can tell, there are two ways to disable self-collision.  I am using the second one:5. Rigging Robots — Omniverse Robotics documentation
Add → Physics → Articulation Root. Under properties, disable the ‘Self collision’ check box.Rigid-Body Simulation — Omniverse Create documentation
Create - Physics - Collision group, and name it
Add all the elements you wish to it (or just a parent of all those elements)
In the Includes, add the collision group that you are defining.  This prevents self-collision.Hello @daniel.romaniuk1 ,
Thank you very much for your reply!
I tried to use both of the methods that you used, but it still shows almost the same behavior as before.
Do you have any additional thing that you have done?
Such as adjusting the position of the collision, and what type of collision approximation are you using?

Screenshot from 2023-05-10 11-38-461260×655 80.9 KB
Hi guys, I was having the same issues that @daniel.romaniuk1 mentioned earlier, including the same jumping behavior, I was trying to import a robot in URDF format created with Fusion 360 and the Fusion2PyBullet plugin, everything works fine until I pressed play in Isaac Sim. For me the problem was that the plugin exported the robot with a silver material, so in the simulation the chassis of a tiny robot weights like 16kg when in reality its about 3kg, that gravity force pushed the tiny wheels of my omni wheels too much, causing them to much into the grid causing overlap between them (this is the cause of the random mini jumpings). I just change the mass value of the chassis (in the property tab) to 3kg and all the problems were gone.Please take a look at the O3dyn model in the Isaac Folder - It has mecanum wheels modeled.The biggest challenge is that the rollers have variable radius so that the whole wheel has a perfect circular shape.
So you can’t approximate rollers with a cylinder or a single sphere, otherwise it will become jumpy as the wheel as a whole will have variable radius.In O3dyn we approximate the rollers with 6 spheres spreading out from the center. Given that the simulation as a whole is in time-steps, this approximation doesn’t hurt the simulalton smoothnes, as long as the wheel maintains contact with the ground.

image685×657 111 KB

image619×697 44.8 KB
Another key point you need to change is the simulation time steps per second - The reason for that is that rollers are tiny bodies running at a large velocity, this gives them a very short window of contact with the ground, and if that window is smaller than the simulation time step, that contact is not registered in the simulation and your robot will sink into the ground. By the next time-step, there will be a large error in the collision with the ground, and it will cause a big jerk up, making the robot jump.It seems that rescaling the o3dyn wheels works best for us while attempting to implement holonomic movement to our robot. However we would like to reproduce our own wheels which have more rollers in model. In order to do so several questions remain:Powered by Discourse, best viewed with JavaScript enabled"
282,a-problem-when-using-audio2face-rest-api,"Hi I got a problem when using the REST API.I put a.wav and b.wav in the track file root. Then I use the SetTrack API to set the current track as b.wav. But When I try to export the BS I found it is the result of a.wav.I check the current track name by GetCurrentTrack interface. It shows that the current track is still a.wav even though I just set the b.wav.It’s wierd. It looks like that it only takes the first wav file to generate animation.I check the current track name by GetCurrentTrack interface. It shows that the current track is still a.wav even though I just set the b.wav.Hi @ChairMan_Meow, it seems that SetTrack call might have failed, if the response fails then the current track won’t be changed.Do you mind checking the response from that call?if Set track was successful you should get a response like this :A2F_SetTrack.wmv (16.3 MB)Here is my screen record. The track root has already been set to F:\Audio.But the response shows that the current audio file is a.wav.The setTrack just takes the filename of the audio tracks inside the set root path.You can specify the file name directly when setting the track, in this case  ""file_name"" : ""b.wav"" will workBut it should have raised an error
We will make the docs more clear as well.Thanks for the feedback!Oh you saved me~ Thanks a lot !
Everything works well now.This topic was automatically closed 14 days after the last reply. New replies are no longer allowed.Powered by Discourse, best viewed with JavaScript enabled"
283,setting-different-colors-to-each-side-of-a-plane,"Hello,
For a while, I tried unsuccessfully to place different colors on opposite sides of a plane. I’m trying to do that because I want to make a cloth with different colors on different sides.
I tried to do it in a Blender also, and import it into Isaac Sim, but failed.
Here’s a video of what I want to achieve on the cloth: link
Is such a thing possible? Any help would be appreciated.
Thank youPowered by Discourse, best viewed with JavaScript enabled"
284,kit-app-play-physics,"Hello,
I have a simple Kit 105.0.1 custom app, with a ViewportWidget omni.kit.widget.viewport that can visualize a USD that contains a box with RigidBody and Collider.When I click Play using the omni.kit.window.toolbar, the box does not fall, although the omni.timeline is advancing.If I open that USD in Composer and click Play, it falls as expected.What might I be missing? Any extension? Any configuration? The use case is so simple I didn’t provide any code.Extra information, I’m using omni.hydra.pxr to render.Thanks.Hi,
you are missing the dependency on physics extensions. Add to your application kit file dependency on either:Regards,
AlesThanks for the answer @AlesBorovicka but the box is still not falling when I click Play (which internally calls Play() method of omni.timeline through omni.kit.commands)I have these dependencies in my simple kit app:Do I need to setup anything else?In my omni.ext I just open a USD stage with a box that has rigid body, I render it through a ViewportWidget and nothing else. Should setup or enable something from PhysX through python?boxRigidBody.usda (6.3 KB)That sounds about right, can you please attach the log file so that I can check if there were some errors?I have tried your dependencies, but I am not getting even the viewport visible.Once I added more dependencies though base dev app dependency:
“omni.app.dev” = {}
Things work fine for me.You do see viewport with the bare minimal dependencies you have?Regards,
Ales@AlesBorovicka I should send you that log through a different channel to not comprise my company, but I see all messages in the log have [Info], not warnings or errors I believe.When I add omni.app.dev I see that it has a different USD Context, than the one I’m rendering through omni.kit.widget.viewport in my extension, should I specify a USD Context to physX too?Ah I see, USD Context that might be the problem that physx is connected to the “default” context that is created with the app.
If you have your own stage/context you could connect omni.physx to it through a python code, but then you need to step it yourself.Thanks @AlesBorovicka ! Do you know how to connect omni.physx to my self._usd_ctx?Last question, what do you mean to step it yourself? Do you mean I cannot play the simulation by calling this?Hmm this is interesting problem, omni.physx does not really work with UsdContext, it does work with a given stage that it attaches to and then given delta time it should simulate.
Its tied to UsdContext through an inbuild mechanism called StageUpdate, which is a part of UsdContext.
However I would have to debug what happens if you create your own another context how this information is passed down to the StageUpdate that should give this information to omni.physx. I never tried that so far. Looking at the StageUpdate it should work somehow but I would need to debug what calls I get in omni.physx.You could register to timeline events and write the physics update yourself as a temp workaround…Anyway I can take a look, if you could send me through a private msg simplified version of the extension that you need to work that would help me significantly. Otherwise I will try tomorrow to repro the issue and figure out what is wrong.AlesLooking at the sample snippet, we cant simulate yet another context, the solution here is to use the existing “global” context and pass it to the viewport.
Something like this:Regards,
AlesPowered by Discourse, best viewed with JavaScript enabled"
285,issues-with-using-omniverse-with-unreal-engine-5,"I’m attempting bring a USD file I created in 3ds Max using Omniverse to export it, however every time I’ve tried bringing it into an Unreal scene the engine crashes.Thinking it might have been the model I create I attempted to bring in some of the models that were included in the Nvidia folder. However every time I do this I get the same result, I pull an USD into the scene and Unreal crashes.I have also tested this in 3ds Max where I have been able to import USD models with the Omniverse importer with little issue.Hi @stephenhw.art. Welcome to the forums! Any chance you can provide a sample scene that produces the crash or log files so we can try to diagnose the issue?Powered by Discourse, best viewed with JavaScript enabled"
286,isnt-there-an-extension-called-flatcache,"I remember using this extension, but I can’t find it in the latest Composer.ThanksHi,
do you mean omni.physx.flatcache extension that was improving simulation output performance? It got renamed, flatcache got rebranded into Fabric, so the extension to look at now is omni.physx.fabric.
Sorry about the trouble.
Regards,
AlesYes, that is the one I meant. Thank you.Powered by Discourse, best viewed with JavaScript enabled"
287,focus-on-a-node-in-mdl-graph-editor,"When I reopen my graph (which is working) in the editor I get a blank window.  I assume that’s because my actual graph is somewhere outside of the window bounds.How do I quickly move to my nodes in the editor, without knowing where they are relative to the current position if the window?Usually, in other software, this is some form of the F key (F, ctrl-f, alt-f) but none of those seem to work?thanksHi @user123724. F key should work, but it looks like there’s a bug in Code. I tested on the next Code release and it looks like its fixed there. You can try in USD Composer (Create) to verify that the F key is working for you.This topic was automatically closed 14 days after the last reply. New replies are no longer allowed.Powered by Discourse, best viewed with JavaScript enabled"
288,fisheye-lens-parameters-modification,"Hello everyone. I’m trying to modify camera parameters programmatically (with python) to try diferents options for data générations and computer vision algorithms testing. Sadly, i can’t figure out how to modify fisheye lens properties, as they are not part of the standard pixar’s  usd implementation.

fisheye_lens_params581×699 30.4 KB
Anyone has the same issue or a solution to this problem  ?Hi @a.adjanohoun - Please go through these documentations and forum posts and let us know if you have more follow up questions.https://docs.opencv.org/4.x/db/d58/group__calib3d__fisheye.htmlhttps://docs.omniverse.nvidia.com/app_isaacsim/prod_materials-and-rendering/cameras.htmlHello. So what you’re saying is that i should distord the images myself after getting them according to my own parameters. Thank you for the reminder. I just thought nvidia made some kind of commands or extendind the usd library for the camera.This topic was automatically closed 14 days after the last reply. New replies are no longer allowed.Powered by Discourse, best viewed with JavaScript enabled"
289,incorrect-forces-are-returned-by-rigidprimview-get-net-contact-forces,"I modified the “Hello World” Isaac Example to give the cube a mass of 1.0 kg and to add a RigidPrimView that I can call get_net_contact_forces() on, following advice from here. Gravity is set to 9.81 m/s^2, so the contact force should be 9.81 N. However 1.635e-01 N is reported by Isaac Sim. Code and output below.I figured this out. get_net_contact_forces() returns a contact impulse and you have to set the parameter dt to convert it to a force. For example:This topic was automatically closed 14 days after the last reply. New replies are no longer allowed.Powered by Discourse, best viewed with JavaScript enabled"
290,houdini-omniverse-connector-v102-3-release-error,"Get this error after installing new release:(0) Couldn’t load C:/Users/***/AppData/Local/ov/pkg/houdini-connector-102.3.0/houdini19.5/houdini/dso/OP_Omni.dll.The specified procedure could not be found.Hello @lorsange.dominoCould you please try uninstalling the Houdini Connector 102.3.0 through the Launcher (Settings → Uninstall), and reinstall the version again? Make sure all Houdini sessions are closed while uninstalling and reinstalling.

Screenshot 2023-02-22 2155571239×685 56.9 KB
If not still working, please try delete this folder - C:/Users/***/AppData/Local/ov/pkg/houdini-connector-102.3.0 and reinstall Houdini Connector from Launcher again.My guess is - there might be a permission error while Launcher trying to install the new version Houdini Connector.Please let me know if the solutions work for you, and thank you for reporting this issue!WayneHi @walai
Yes, I tried lot of times with deleting and uninstalling, with 102.1and 102.3.
Still got the problem .
It worked well before updating@lorsange.domino Could you please share the screen shot of the directories -
C:/Users/***/AppData/Local/ov/pkg/houdini-connector-102.3.0/houdini19.5/houdini/dso and
C:/Users/***/AppData/Local/ov/pkg/houdini-connector-102.3.0/houdini19.5
C:/Users/***/AppData/Local/ov/pkg/houdini-connector-102.3.0/houdini19.5/libAre there any error messages while installing Houdini Connector in Launcher ?

image1257×703 27.7 KB
I having the same issue, and it even stops my karma renders, so I have to change the package file to another location so it won’t stop the render each frame


image755×689 24.5 KB
Hello @maxmax003 ,
Does your issue start at102.3.0 or even earlier versions - in your screenshot, it is 102.1.0What is your OS - Windows 10, Windows 11To be honest I just updated all my 3d apps since I’ve not being doing much the pass 4monts, so I just update houdini to 19.5.534, the connector I have is 102.1.0 but also tried the beta 102.3.019.5.493 works fine. Between 19.5.493 and 19.5.534, there are lots of compatibility change.Actually it’s started working well after lot of tries, but I got this warning after last installing
It’s not ruin Houdini, so just FYILet me try 19.5.534 here. Thanks for the info.Good found @lorsange.dominoThis might not be related directly, but it gives us a clue. I am wondering if one of the $HOUDINI_PATH contains corrupted or conflicted plugin files.If you are interested in dig into this more, you can try -In Houdini’s Python interpreter (alt + shift + p) type followingif you exam all directories it outputs, and if there is more than one directory contain packages/omniverse_h19x.json or contain dso/fs/FS_Omni.dll but not dso/OP_Omni.dll - that might be an issue. That also could be where the warning opalias:  The alias 'labs::retime'..... coming from.Hello @jiejiaio
I tested 19.5.534 today and it worked fine for me. I am wondering if there is something else. For the next Houdini Connector release, we can try to build against Houdini 19.5.534. Thank you again for providing the information and what you found!WayneSorry, haven’t checked forum for a while:
Was mistaken, plugin works with 19.5.493 but still got the error with 19.5.534 mentioned aboveC:/Users/***/AppData/Local/ov/pkg/houdini-connector-102.3.0/houdini19.5/houdini/dso

C:/Users/***/AppData/Local/ov/pkg/houdini-connector-102.3.0/houdini19.5

C:/Users/***/AppData/Local/ov/pkg/houdini-connector-102.3.0/houdini19.5/lib

image706×573 24.1 KB
About $HOUDINI_PATH:
image1492×731 146 KB
C:/Users//Documents/houdini19.5/otls/qLib;
C:/Users//Documents/houdini19.5/packages/MOPS-1.7.1;
C:/ProgramData/redshift/Plugins/Houdini/19.5.534;
C:/Program Files/Side Effects Software/sidefx_packages/SideFXLabs19.5;
C:/PROGRA~1/SIDEEF~1/HOUDIN~1.534/packages/kinefx;
C:/Program Files/Pixar/RenderManForHoudini-24.4-py3/&;
&:/Users//Documents/houdini19.5/otls/qLib;
H:/Houdini_otls/ox_tools_package_0.1;
C:/Users//AppData/Local/ov/pkg/houdini-connector-102.3.0/houdini19.5/houdini;&;
C:/Users//Documents/houdini19.5/packages/hpaste-master;
C:/Users//Documents/houdini19.5/packages/nLib-master;
C:/Prism/Plugins/Apps/Houdini/Integration;
C:/Users//Documents/houdini19.5/packages/MOPS-1.7.1;
C:/Users//Documents/houdini19.5/LookDev_Assets;
H:/_OCIO_Color_profiles/aces_1.0.3/config.ocio;
C:/Program Files/3Delight/houdini/19.5;
C:/Users//Documents/houdini19.5/packages/axiom3.0.120/houdini19.5.534;
C:/Users//Documents/houdini19.5/packages/axiom3.0.120/houdini;
K:/ALL/ASSETS/MEGASCAN/support/plugins/houdini/4.6/MSLiveLink;
C:/ProgramData/redshift/Plugins/Solaris/19.5.534;
C:/Users/*****/AppData/Local/ov/pkg/houdini-connector-102.3.0/houdini19.5/lib/usd/omniverse/resources;&Hello @lorsange.domino
Sorry for the delay - we finally identified where the issue is and working with SideFx to get this solved. The fix will be included in the next Houdini Connector release.The only affected functionality is the experimental Live Sync node . In the meantime, if you are not using the node, it is safe to delete this file C:/Users/***/AppData/Local/ov/pkg/houdini-connector-102.3.0/houdini19.5/houdini/dso/OP_Omni.dll to suppress the warning.Thank you again for reporting this issue - it really helps us improve our product! Also thanks to @jiejiaio , your comment helped us identified the problem.WayneHi @walai .
Thank You for informingThis topic was automatically closed 14 days after the last reply. New replies are no longer allowed.Powered by Discourse, best viewed with JavaScript enabled"
291,is-there-any-progress-or-plans-to-enable-font-sizes-in-code-or-composer,"I am trying to work in Code today for the first time in months, and I forgot everything.You have this one option, and it shows a tool tip, and even with Windows Magnifier and my glasses on, I couldn’t read this my life depended on it.image1312×521 59.4 KBI understand most people can see better than me, but I couldn’t read font size 8, 30 years ago.It seems accessibility is not on your priority list.Powered by Discourse, best viewed with JavaScript enabled"
292,problem-when-import-deformable-object-collision-mesh,"Hi,
When I import deformable objects, It generates poor-quality of deformable object mesh.
I want to increase the simulation mesh resolution, but the mesh is crashed.
How can I fix this problem?
I want to fix this problem by simulation some parameters, not using re-meshing approaches(ex. blender).
Hi @gwangin ,
Not quite sure from your description what you want to achieve.
Is it crashing when you adjust the simulation mesh resolution to high values?
How does the simulation mesh look like? Not that the visualization you shared above is not the simulation mesh, but the collision mesh.
Could you share your asset?Powered by Discourse, best viewed with JavaScript enabled"
293,trouble-with-adding-contact-sensors-to-robot,"Hello!I’m to figure out why I cannot get any reading from a contact sensor I have attached to my robot.When I use the physics debugger, I can see there are contact points and forces, but my sensor has no readings.Screenshot from 2023-07-27 18-03-141231×270 200 KBHere is the .usd file I’m using:Arm_with_Two_Fingers_Manual_SDF_Clean_Manual_Just_Robot_Edit_To_Match_Ant_ToF_Contact_Sensor.usda (51.3 KB)Hi @jess7654  - I have assigned the question to an expert. Meanwhile, I can provide some general advice on how to troubleshoot this issue.Powered by Discourse, best viewed with JavaScript enabled"
294,cant-retrieve-skeleton-data-with-synthetic-data-recorder,"Hello Community,I’ve encountered an issue while trying to access the skeleton_data of built-in characters in Isaac Sim. Upon execution, I receive the following error message:2023-06-09 04:37:33 [1,265,659ms] [Error] [omni.graph.core.plugin] /Render/PostProcess/SDGPipeline/RenderProduct_Replicator_skeleton_data: Assertion raised in compute - Unable to write from unknown dtype, kind=O, size=8The error seems to be rooted at line 403 in the compute function of the OgnGetSkeletonData.py script. Additionally, it also appears while executing the setattr function in the OgnGetSkeletonDataDatabase.py and database.py scripts.In an attempt to solve the issue, I referenced this forum post: Synthetic Data Recorder skeleton_data. Following the suggestions provided, I tried upgrading my replicator version from 1.7.8 to at least 1.9.6, but the Isaac Sim extension manager shows my current version as the latest available one. I couldn’t find an alternative way to upgrade it.I also made an effort to annotate the skeleton and its joints using the Semantics Schema Editor, without any noticeable improvement. Interestingly, I am able to successfully extract skeleton_data for one character, “worker.usd”, which seems to have additional property attributes related to its skeleton.I’m reaching out to seek advice on how to effectively extract skeleton data from other characters as well. Any assistance would be greatly appreciated!Thank you in advance for your time and support.Hi there,indeed replicator v1.9.6+ should have a fix for this. If using Isaac Sim the next release will ship with a newer replicator version. If not bound on using Isaac Sim you can try if  Code / USD Composer have a newer extension version with the fix included.Best,
AndreiPowered by Discourse, best viewed with JavaScript enabled"
295,waiting-for-compilation-of-ray-tracing-shaders-by-gpu-driver-on-ubuntu-22-04,"Hi,
when I am trying to launch an Omniverse App, I get the message “Waiting for compilation of ray tracing shaders by GPU driver” indefinitely. I am using a freshly installed Ubuntu 22.04 with an RTX 3060. I already installed the newest driver from the website (NVIDIA-Linux-x86_64-535.54.03) and deleted the shader cache as recommended in topic 1 and topic 2. However, the issue persists. I would appreciate any help.
Thanks a lot!Powered by Discourse, best viewed with JavaScript enabled"
296,omniverse-code-close-itself-and-crash-many-times,"Hi there,Not sure why is that the Code on my machine is not stable
When running, sometimes closes itself, and sometimes crashes as executed scriptCan you help check my log, Are any clues that can cause this?Code_Logs.zip (11.9 MB)
Crash screen1
Crash screen2Last error for referenceThanks and regards,
DarienPowered by Discourse, best viewed with JavaScript enabled"
297,audio2face-and-animation-graph-integration,"Animation Graph has nodes that can accept “Pose” data, including the PoseProvider node that can take as input joint translations, joint rotations, skeleton delta translation, skeleton delta rotation. Audio2face seems to be a combination of blendshapes and at least eye rotations, so was not sure how to use Animation Graph with Audio2Face to mix things together (either live or recorded audio2face output).This will be possible in the future. But for now if you have the body animation, you can bring it in Audio2Face and apply face animations to it.Here’s a tutorial from the Camila tutorial series:
Camila Asset Pt 6: Connect Character Setup Meshes to Drive the Full Body in Omniverse Audio2Face - YouTubeThanks for the video. Yes, I have watched it a number of times. I am trying to work out how to use audio2face in a more realistic project. Imagine a scene in a movie where multiple characters talk backwards and forwards, with changing emotions and other facial animations (like winking) beyond what audio2face can do. BlendShapes make the most sense because you can merge in other facial expressions that audio2face cannot generate.So my question is how to mix a combination of various animation sources (with blends) and blendshape animations (including from audio2face). Animation Graph is one approach - it has blends and various capabilities built in. I was trying to see if audio2face had any integration with that. Sounds like “not yet”.Otherwise I will write my own Sequencer and blend all the sources together myself for a final “baked” animation clip (one per character, with all blendshapes and body motions). I need to work through the a2f APIs to work out how to achieve this (including the REST APIs to work out exactly what it supports).Powered by Discourse, best viewed with JavaScript enabled"
298,why-cant-the-lidar-detect-deformable-body,"I tried to detect deformed object using lidar in GUI, but Lidar didn’t react to deformable objects.The image below shows Rigid Body with Collidars Preset, Collidars Preset, and Deformabl body in order from the left cube. As you can see, it responds to the cube given the collidar, but it cannot detect the deformable body.
We have confirmed that this deformable body has a contact judgment with a rigid body. Nevertheless, why can’t lidar detect deformable bodies?Thanks in advance.Hi @Yuya_t  - Are you using RTX lidar or physX lidar?Thanks to reply @rthaker .I use physX lidar in GUI.
I selected Crate >> Isaac >> Sensors >> Lidar >> Rotating.Powered by Discourse, best viewed with JavaScript enabled"
299,mysterious-nucleus-installation-problem-no-localhost-cloud-under-omniverse-after-installation,"I’ve been trying to install the Omniverse Nucleus. I get no errors or anything, but nothing appears after the installation on the sidebar, and when I try to connect to localhost, I can’t. Here is a step-by-step image guide of me doing the process and the problem I run into.I talked to a chat agent, and they sent me here. I’ve been trying to troubleshoot this for days; however, I am no tech genius and really am walking into this blind. I have a business that needs this, and will be purchasing omniverse enterprise if I can get the very basic nucleus to function properly.couldntuploadmorethanonefile1444×4000 332 KB(Open image in new tap and zoom in/wouldn’t let me post more than one image)Highly appreciate whatever help you throw my way. Thank you all!-JamesYou and I were able to find a solution on Discord. I believe the culprit was OneDrive interfering with the install of the crucial contents and packages that lead to issues when installing/setting up Nucleus.Here’s what we had to do to get it to work:Powered by Discourse, best viewed with JavaScript enabled"
300,creating-randomized-perlin-texture,"Hi
I would like to create randomized Perlin textures for randomizing the appearance of my objects of interest in my dataset.Is there an easy way of creating randomized Perlin textures from within python/replicator code?
All I could find in the documentation was this page: Perlin Noise Texture — Omniverse Materials and Rendering documentation
Which I am not sure how to incorporate into a python scripting workflow.Any help would be appreciated :)
Thanks
ThomasYou can do this with the standard Material Graph capabilities, some custom primvars and replicator.The relevant code to alter the primvars in the repo above is:Let me know if you run into any issues and I can help out further.image1814×977 204 KBPowered by Discourse, best viewed with JavaScript enabled"
301,how-to-fix-retargeting-issues-in-audio2gesture-machinima,"
Screenshot_2023-05-25_1343291920×1575 221 KB

Hi, I’m trying to get the Audio2Gesture feature in Machinima to work. I’ve tried both a custom avatar and a Ready Player Me avatar and they both have armatures that work and import perfectly fine as a humanoid rig in Unity, but in Omniverse, they’re a horrific mess. I’ve tried to get around this by using the Unity connector, and when I do, all the bones and parents are in the right locations, but it doesn’t recognize it as an armature so that doesn’t work either. The Ready Player Me avatar works fine except for the hip bone, I think. I’ve also tried messing with the retargeting settings and manually selecting tags. What could I be doing wrong? ThanksPowered by Discourse, best viewed with JavaScript enabled"
302,omniverse-launcher-stuck-at-login-in-ubuntu,"Since last friday I encounter a problem with the Omniverse Launcher in ubuntu Ubuntu 22.04.1 LTS.
Usually I connect to my account via firefox and then directly into omniverse it continue, but this time It keep loading, even after 1 hour nothing changed.What I tried :Is there any reason for that ?
Thank you very much.V.H.Screenshot from 2023-07-28 07-34-471920×880 79.3 KBI am having the same problem. Also tried on Chrome but no luck…this worked for me (Could NOT login the omniverse launcher), but feels like the wrong way to solve this problem.After putting the missing file in autostart, I have a new error showing :Did a research (grep) but couldn’t find the file.
Where can I get back this file or how can I uninstall Omniverse, I have difficulty about all these thing in ubuntu …Powered by Discourse, best viewed with JavaScript enabled"
303,how-to-handle-material-assignment-on-geomsubsets,"Hello,I have a mesh prim that contains multiple GeomSubset prims. Each GeomSubset prim has a different optical material assigned to it, resulting in the coloration you can see in the screenshot below.nvidia_forum_geom_subsets.PNG1919×1088 142 KBNow, I want to apply physics to the mesh prim by defining it as an SDF collider and applying a physics material to it.Everything seems fine until I start the simulation, at which point I encounter the following error message:One way to resolve this error would be to delete all the GeomSubsets, but then the body would only have one color.Is there a way to retain the GeomSubsets (each with its own optical material) and only use the physics material of the mesh prim for the physics calculations?Please note that the materials on the GeomSubsets are already set to “weaker than descendants,” while the physics material on the mesh prim is set to “stronger than descendants.”Kind regardsAxelHi,
Hmm I think I know what the problem is, I will have to play with this and try to resolve this. Its certainly a bug and we need to fix it.
However as a workaround the only way I see is to create an xform and put two same meshes below it.
One for rendering with the subgeometries and one without them. Then the one without them set its purpose to guide and collide against this one (guide will make sure its not rendered).
So this way you can have different graphics and collision representation while if the rigid body is on the xform all move together.Sorry about the trouble, will try to fix the issue for next release.Regards,
AlesHi Ales,Thank you for looking into it and for your suggestion.I am looking forward to the next release.Kind regards,AxelTrying to repro this issue, just to be clear this issue is there only if the materials applied to the UsdGeomSubset does have physics APIs right? So those materials do have physics properties. If I create a subset with just rendering material, it seems to work right.For my case, the materials on the USDGeomSubsets don’t seem to have a physics API.
The materials were automatically created when I converted a STEP file from a CAD program to USD.Those are the properties of the material:
Material_settings1188×1852 137 KBAnd those are the properties of the shader:
Material_settings_shader1196×2203 170 KBI converted three of those materials to MDL:
Example_material_2.mdl (1.6 KB)
Example_material_3.mdl (1.7 KB)
Example_material.mdl (1.7 KB)I encountered the error mentioned above with those materials applied to the USDGeomSubsets. There was no error after deleting the GeomSubsets.Thanks, I was able to repro now, will try to fix it for next release.This topic was automatically closed 14 days after the last reply. New replies are no longer allowed.Powered by Discourse, best viewed with JavaScript enabled"
304,kit-105-and-particle-cloth-for-hair-physics-various-strange-behavior,"I was trying out USD Composer (Kit 105) with particle cloth for getting some hair bounce. Someone was asking what I did, so I did a rough recording of what I did. A number of things went wrong (hair jumps to wrong position, OV crashes, etc). Some success, but sharing the video in case useful to look into the various problems that occurred.Examples of problems:Note: I reported some of these in the past - this is just an update for 105.Powered by Discourse, best viewed with JavaScript enabled"
305,composer-2023-1-errors-on-startup,"Hello Nvidia,
I just installed the new composer, and on startup, I get a the same error printed over and over, I attached a screengrab. I believe I have no other usd installation in the path at this time. Is this a known issue?
thanks,
Koen
composer_errors1842×576 132 KBThanks for the reply. We will look into this and get back to you. Does Composer work well otherwise ?Hello Richard,I dropped back a version, as it looks like just reporting this error is taking quite a lot or resources. Safer for me to stay in the release version for now.Thanks,
KoenIf you have it still installed please go here and send us the log
image1347×359 9.79 KBIf you have installed it, then go here and zip up this folder… “C:\Users\user\nvidia-omniverse\logs\Kit\USD.Composer\2023.1”Powered by Discourse, best viewed with JavaScript enabled"
306,error-when-importing-a-robot-from-usd-in-python-scripting,"Hi, I’m trying to import a robot from usd file (jackal_basic.usd) which is same as the example over the server, but I saved a local copy without payload. When I try to load the robot from url it works fine, other wise I get this error-2023-06-15 02:27:00 [10,608ms] [Warning] [omni.isaac.dynamic_control.plugin] Failed to find articulation at ‘/basic’
2023-06-15 02:27:00 [10,608ms] [Error] [omni.isaac.dynamic_control.plugin] DcGetArticulationRootBody: Invalid or expired articulation handle
2023-06-15 02:27:00 [10,608ms] [Warning] [omni.physx.tensors.plugin] Failed to find articulation at ‘/basic’
2023-06-15 02:27:00 [10,608ms] [Error] [omni.physx.tensors.plugin] Pattern ‘/basic’ did not match any articulationsAfter trying too long, it seems that this issue is related with payload. Whenever usd file contains a payload or reference, the program refuses to work. I don’t know if it is a limitation of Issac now or I’m doing something incorrectly. I was trying to port DRL library for my system posted over Github here – GitHub - Lauqz/Easy_DRL_Isaac_Sim: Fix (almost) framework for Isaac Sim 2022.2.1Powered by Discourse, best viewed with JavaScript enabled"
307,basic-tutorials-on-coding-content-creation,"I’m trying to dive, head first, into Omniverse Code. However I find it tricky as there doesn’t seem to be many tutorials out there.I thought I should make a room generator where you can type in your measures and have a standard empty room generated with windows, doors floorboards etc. in the right places.As far as I understand I should head to Omniverse Code, and I have found the  template that creates a window with two buttons. What I miss is a tutorial showing the basics in content creating code. I managed to create a cube, but I have a problem to translate is more than once. Which libraries are involved.? Can anyone point me in the right direction?Hi @laban. I created a tutorial series about programmatically creating content with USD. I think you might find it helpful for what you’re wanting to do: Hit the USD Books in this Metaverse Coding Series | by NVIDIA Omniverse | MediumThanks. I started to watch and create at the same time, but it’s a bit overwhelming to begin with. I’m struggling with the simplest tasks. It seems like the way to go is to copy the commands from the commands window, but then everything begins with execute(‘some command name’).Just renaming an object would beIt’s all very cumbersome. Isn’t there another API with shorter commands, something like SetPath, SetName, Rename or something? Maybe it would become more clear if I were to watch the entire series first. As of now I feel that I am heading off the wrong way. Is there an even more basic tutorial to begin with?Hi @laban. You can use the USD API directly if you prefer. I do that too in the video series. The advantage of the Kit Commands is that it gives you the ability to undo. Here are some examples of using the USD API directly: USD — Omniverse Kit documentationOk. Will give it a try. Do you know right away how to rename an object? The search function in the doc doesn’t give much support.Hi @laban. We have a Kit Command for that called MovePrim. You can see an example here: developer-office-hours/exts/maticodes.doh_2022_08_12/scripts/create_group_anywhere.py at df3da24da3fc86ffcb1de30cdfc279d3d5de3bbe · mati-nvidia/developer-office-hours · GitHubPowered by Discourse, best viewed with JavaScript enabled"
308,urgent-help-please-doexecute-error-always-appears,"Dear everyone,I am facing an error that makes me very frustrated and upset.I am a very new user to Omniverse & Isaac Sim. I am trying to run the Offline Dataset Generation script from the Documentation webpage: Offline Dataset Generation. However, when executing the command: ./python.sh standalone_examples/replicator/offline_generation.py, the weird error occurs:The Isaac Sim I am using is the latest 2022.2.1. My machine is ROG Strix G634JZ laptop. It has an Intel iGPU and a NVIDIA RTX4080 Laptop GPU. I have already disabled nouveau driver and installed the latest NVIDIA GPU driver 535.54.03 manually.I will attach the full error log from terminal below, and I will also attach my system hardware configuration. Thanks a lot for your time and kind help on my issue. I will really appreciate your help and kindness. This is a very urgent asking for help post because I am starting to use Omniverse Isaac Sim for high-quality research projects with my professor. Thanks again!error_log.txt (21.6 KB)
system_info1920×1341 214 KBI also found a very related post in the forum that might be very similar to my issue. I will attach it here for you to refer: a very similar post.However, as I am using the latest NVIDIA GPU driver that has support to my RTX4080 Laptop GPU, which should have installed the latest Vulkan by default during the driver installation process. Therefore, I don’t understand the reasoning behind that post, and I also think my case is a bit different from that.when executing the command: ./python.sh standalone_examples/replicator/offline_generation.py, the weird error occurs:Hi @lizhuoran2000111 - Can you update the driver as per the Isaac Sim 2022.2.1 documentation? 1. Isaac Sim Requirements — Omniverse Robotics documentationimage1177×437 28.1 KBI have the same issue, and I’m not able to downgrade to 525.60.11 bc my system won’t boot. Here’s my setup:Also, I tried downgrading to IsaacSim 2022.2.0, but I’m not even able to launch it, crashes with Vulkan version mistmatch.@rthaker any ideas how to go around it?Powered by Discourse, best viewed with JavaScript enabled"
309,launcher-cannot-log-in,"Hey, I am having trouble installing Omniverse. When I run launcher, I get to log in step and after putting my credentials in the browser I get “You can continue to work in the application.” However the launcher is still stuck on log in page and does not move forward. I attach the launcher logs.Thanks for all the help.
launcher.log (9.8 KB)hello @michal.nazarczuk1
can you verify that this file exists
/home/michaln/.nvidia-omniverse/config/auth.tomland do an “ls -l” on all the files in
/home/michaln/.nvidia-omniverse/configHey, it’s just ‘Nucleus Navigator’, omniverse.toml, and privacy.toml in /home/michaln/.nvidia-omniverse/confighello @michal.nazarczuk1
what are the permissions and owners on all of those files?can you send “ls -l” on that directory.Also, can you run the cleanup tool (below) and then re-install omniversehttp://omniverse-docs-production.s3-website-us-west-1.amazonaws.com/utilities/latest/cleanup-tool.html#cleanup-toolThat would be:
Powered by Discourse, best viewed with JavaScript enabled"
310,audio2face-stuck-on-loading-tensorrt-engine,"Hello I have an RTX graphics card but the Audio2Face is Stuck on Loading TensorRT Engine for the past 1 hour, I have  updated GPU drivers and everything but it just wont budge.Help PleaseWelcome to the forums @dannytareen21Can you please send your latest Audio2Face log file which can be found here: C:\Users\<USERNAME>\.nvidia-omniverse\logs\Kit\Audio2FaceC:\Users<USERNAME>.nvidia-omniverse\logs\Kit\Audio2Facekit_20230606_230857.log (628.1 KB)Have you found a solution do I need to do a fresh install?
It is also giving me errors that drive H not found I dont have a drive HI see the same H:/ drive error. Not sure why this has happened, but reinstalling will hopefully fix it.How do I completely uninstall the entire omniverse please guideTo uninstall Audio2Face, go to Launcher -> Library -> Audio2Face -> Settings -> Uninstall
Screenshot_81868×984 223 KB

Screenshot_91862×1047 115 KB
Powered by Discourse, best viewed with JavaScript enabled"
311,skeleton-bones-in-viewport-not-showing-in-105,"I have Machinma 104 still loaded and I turn on skeleton bone view and can see the bones. This version uses SkelJoints under the Skeleton etc.image1165×624 163 KBI load the same file into USD Compose 105 and turn on the same skeleton view but can see nothing. 105 however uses Omnijoints now, not SkelJoints. So I was wondering if the bone viewer needs updating as well to display the new bones approach in 105.image1135×636 161 KBBUT! I just tried the Omniverse provided example for walking following path etc, and the bones showed up fine there. So its not completely broken (they are using Omnijoint too).Powered by Discourse, best viewed with JavaScript enabled"
312,ros2-humble-requesting-update-on-source-code-for-7-multiple-robot-ros2-navigation-tutorial-that-is-compatible-with-ros2-humble,"Hello.Our team is currently coordinating an experimental project on testing the feasibility of multiple robot navigation simulation for warehouse automation.I would love to test Isaac Sim’s “7. Multiple Robot ROS2 Navigation” tutorial (https://docs.omniverse.nvidia.com/app_isaacsim/app_isaacsim/tutorial_ros2_multi_navigation.html) but it seems like the source code provided is not compatible with ROS2 Humble and the recent version of the Navigation2 stack.I have posted multiple times on this forum regarding this tutorial requesting for aid but to no avail so far.Can we please get an update on the ROS2 source code for this tutorial that is compatible with ROS2 Humble?Thanks!Hi @jaeyeun - We are aware of this issue and currently working on to fix it. You should be able to see it fixed in our upcoming Isaac Sim release (around Aug timeframe)Thank you.
But it would have been nice if you could have mentioned that in the documentation!Hey @jaeyeun! I don’t know if you’re still struggling with this but I actually found a fix that allowed me to run this example (more or less). Apparently there is a problem with namespacing within launch files that was preventing any multi-robot navigation from working.I found a fix for this here: ROS2 Nav2 Not Starting with namespace - ROS Answers: Open Source Q&A ForumI hope this helps! Sorry if this wasn’t the issue you were facing.thanks for the feedback.
ill definitely look into it!Hello @juan17Your proposed solution worked!I was told that the ROS2 Humble Bridge was broken and that it would be fixed in the next Isaac Sim update which is scheduled on August but I no longer have to wait another month to test this out.You saved me so much time.Again, thank you so much for sharing this!sincerely, jaeyeun.No worries! I’m glad it fixed your problem!This topic was automatically closed 14 days after the last reply. New replies are no longer allowed.Powered by Discourse, best viewed with JavaScript enabled"
313,rigidprimview-reg-exp-path-error,"The error occurs after RigidPrimView initializes with multi-regular expressions in prim_paths_expr of RigidPrimView.Codes:
rigids = RigidPrimView(prim_paths_expr=f""{base_env_path}/.*/Humanoid/(?!joints)."")
world.scene.add(rigids)
world.reset()Error:
2023-05-27 10:11:34 [13,934ms] [Warning] [omni.usd] Warning: in SdfPath at line 97 of /buildAgent/work/ca6c508eae419cf8/USD/pxr/usd/sdf/path.cpp – Ill-formed SdfPath </World/envs/env_66/Humanoid/(?!joints).+>: syntax errorHi @hardy3663 - It seems like a syntex error in your code. Can you try to update your code based on this core API document?
https://docs.omniverse.nvidia.com/py/isaacsim/source/extensions/omni.isaac.core/docs/index.html#module-omni.isaac.core.primsThe path works well in omni.isaac.core.utils.prims.find_matching_prim_paths, the error occurs at articulation_view.py

image1054×112 11.9 KB
The error occurs after RigidPrimView initializes with multi-regular expressions in prim_paths_expr of RigidPrimView.Codes:
rigids = RigidPrimView(prim_paths_expr=f""{base_env_path}/.*/Humanoid/(?!joints)."")
world.scene.add(rigids)
world.reset()Error:
2023-05-27 10:11:34 [13,934ms] [Warning] [omni.usd] Warning: in SdfPath at line 97 of /buildAgent/work/ca6c508eae419cf8/USD/pxr/usd/sdf/path.cpp – Ill-formed SdfPath </World/envs/env_66/Humanoid/(?!joints).+>: syntax errorThe error message indicates that the regular expression you’re using in prim_paths_expr is not valid for the SdfPath class in the USD library. The SdfPath class expects a string that represents a path in the scene graph, and it does not support regular expressions.The RigidPrimView class in Isaac Sim is designed to create a view of all rigid bodies that match a certain pattern in the scene graph. The prim_paths_expr parameter is a string that specifies this pattern. It can contain wildcard characters (* and ?), but it does not support full regular expressions.If you want to create a view of all rigid bodies in the Humanoid group except those in the joints group, you might need to do it in two steps:Please note that this code is a general example and might need to be adjusted based on your specific setup and requirements. Also, make sure that the scene is properly set up and the paths are correct before trying to create the views.This topic was automatically closed 14 days after the last reply. New replies are no longer allowed.Powered by Discourse, best viewed with JavaScript enabled"
314,memory-corruption-when-trying-to-create-blend-shapes-and-skeleton-animation-prims,"The attached extension code works once, but when trying to run it a second time it starts talking about “invalid prims”. Exiting and restarting USD Composer allows me to run it again, once. The file created seems to be all working if I save it. It feels like something being done in this extension script is causing a memory corruption or similar. Restarting USD Composer cleans things up.To run it, create an extension and drop this in its place. Then create a new empty stage, click the button on the left to create a new SkeRoot/Skeleton with children of eastWindAnimation, westWindAnimation, southWindAnimation, northWindAnimation, noWindAnimation, and testWindAnimation the default turned on, so you can hit play and see the animation).
image1019×314 27.5 KBThen drag a model (e.g. from NVIDIA Assets, for a tree or similar to under the SkelRoot node, then click the button on the right. It should add blendshapes to the Mesh layers. Hitting play should make the object sway backwards and forwards. I normally have to save the stage and reopen it before the animation starts working.extension.py (10.4 KB)If I try to repeat, I see errors on the console likePowered by Discourse, best viewed with JavaScript enabled"
315,isaac-sim-moveit-motion-planning-framework-franka,"We are getting below error as soon as we launch following command -command -ErrorRLException: unused args [fake_execution] for include of [/home/sysadmin/Desktop/Reactorx150-Controls/app/ros_workspace/src/panda_moveit_config-noetic-devel/launch/move_group.launch]
The traceback for the exception was written to the log filePlease suggest a solution for this -
I have attached the terminal error screenshot, log terminal and also the file for which I’m getting error i.e., move_group.launch
ErrorLog1920×1080 320 KB

move_group.launch (4.7 KB)

Terminal_ROS_FrankaExecution1920×1080 72.5 KB
I got the same bug. Do you have any update?Powered by Discourse, best viewed with JavaScript enabled"
316,audio2face-headless-mode-error-when-export-blendshape,"Hi,
I’ve tried audio2face headless mode from http://localhost:8011/docs# on my previous computer and was working fine. However when I switch to another PC and try to do the same thing. I could successfully do /status, /A2F/USD/Load,  /A2F/Player/SetRootPath and /A2F/Player/GetCurrentTrack.
However when I tried A2F/Exporter/ExportBlendshapes. I got Error: Internal Server Error. When I looked at the prompt it showsI could finish the exact same thing on the previous computer.
I would like to narrow this problem down a little. My current guess is, on this computer I’m having problem enable the nucleus local host, that could be a potential issue. But I’m not sure if this is the correct reason since I could do other curl call as mentioned before.Thanks in advance for any suggestion on what I need to look into!Hi, @yujing.liuDo you mind telling us which A2F version are you using ?The error seems to indicate that it is not possible to access the male template model.can you verify that there are A2F core nodes as well using
A2F/GetInstances?Can you try your script/API calls on the GUI version of A2F?Powered by Discourse, best viewed with JavaScript enabled"
317,where-is-nvidia-omniverse-audio2face-livelink-plugin,"Hello, in the tutorial here is a plugin used called “Nvidia Omniverse Audio2Face LiveLink”.In this series, we will cover how to use BlendShapes from Audio2Fact to Unreal MetahumanCan not find it at all. Where is it? Thanksa2flivelink1008×697 111 KBThe current availability of the Unreal plugin is limited to ACE users. However, we are collaborating with other teams to develop a beta version of the plugin that will be accessible to the public via the Launcher.
We will keep the forum updated with any notifications regarding these.This plugin is now available on 2023.1.1
See for instructions: Audio2Face 2023.1.1 (Open Beta) Released - Apps / Audio2Face - NVIDIA Developer ForumsPowered by Discourse, best viewed with JavaScript enabled"
318,omni-anim-people-issue-with-frame-rate-when-capturing-movie,"I am trying to capture a video of a scene with animated people. I am using this example from Isaac Sim: 3. Omni.Anim.People — Omniverse Robotics documentationWhen I capture the movie, it plays much too fast:
Frame rate is set to 24 in Movie Capture and in the timeline.I would also like to capture such a scene with 60FPS. So I am setting FPS to 60 in Movie capture and in the timeline. This is the result:
Any advice appreciated
BrunoInterestingly, the capture looks more natural, if I set 24FPS in Movie Capture and 120FPS in the timeline!!
The problem is still there in Isaac Sim 2022.2.1Hi @bruno.vetter  - Someone from our team will respond to your question.Hi, I have the same issue, is there any news regarding this topic? Thank you.Powered by Discourse, best viewed with JavaScript enabled"
319,visualize-a-simple-voxelgrid,"Hi there! I want to create a simple voxelgrid of 8 voxels (2x2x2) and save it to a file so Kaolin Dataset Visualizer can properly display it. Help me with the code, please. I read the documentation but there is not full explanation on voxelsPowered by Discourse, best viewed with JavaScript enabled"
320,robotiq-problem,"image718×447 63.7 KB1.There seems to be a problem with the robotiq hand grip design. It seems that some joints are missing between left_inner_knuckle and left_inner_finger, and the right side is also the same
robotiq
手抓设计好像有问题，好像缺一些关节在left_inner_knuckle和left_inner_finger之间，右边同样也是3.I hope I can give some models or examples of the combination of UR robotic arm and robotiq hand.
我希望可以给一些UR机械臂和robotiq手抓的结合在一起的模型或者示例。
If there is class RobotiqGripperController(BaseGripperController):
that would be great
如果有class RobotiqGripperController(BaseGripperController):
那将太好了Thanks！！！！！！！！！！！！！！Moving to the Isaac Sim forums for a more experienced audience.Powered by Discourse, best viewed with JavaScript enabled"
321,the-protocal-of-streaming,"Hi, I would like to create a socket and receive the streaming data on Unity. I first had a try creating a socket in Python but the receiving result looks like this:
image2298×137 24.9 KB
There seems to be a string of unknown characters at the beginning of JSON data.
When I tested with TCPListener it was also not displaying properly when I tried to debug and log the received message.
Is the method I’m using wrong or do I need some extra steps?The extension omni.avatar.livelink is used for streaming to Unreal Engine.
It can be used as an example to create one for Unity. You can find the extension on disc using extension manager.Screenshot_222400×1266 362 KBThanks!Powered by Discourse, best viewed with JavaScript enabled"
322,using-waveworks-2-0-with-omniverse,"Is it possible to display WaveWorks 2.0 (which so far as I can tell uses hlsl) in Omniverse Create?Also, is there any documentation, tutorial, or example that uses WaveWorks in Omniverse The only application I could find using WaveWorks was this, only it’s using Unreal Engine: One Thousand Ships with Real-time Buoyancy and NVIDIA WaveWorks | NVIDIA DeveloperHello @jayliu50!  WaveWorks was created through our Geforce Experience team for game development.  I do not know if they are moving that technology over to Omniverse, so I will ask!Have you looked into the isosurface component in Omniverse yet? Particle Simulation — Omniverse Create documentationThank you so much for your reply and for looking into that.After some research, I discovered maybe I could stuff all the HLSL code into a SlangNode on Connect. Maybe the dev team could also help me know whether that’s a bad idea, or what.I’ll look into the Particle link you suggested, so thank you for sharing that!I’m new to all this, if that wasn’t already apparent!I guess I just need 1-2 square km of realistic dynamic ocean. Would the particle sim method you mentioned computationally suitable at that scale?Hi @jayliu50,We are planning to publish an omni.ocean extension with the next Create that will enable large-scale ocean simulations using Warp.In the meantime you can also check out the Warp examples (e.g.: ripple deformer) for an example of how to integrate GPU-based deformers into OmniGraph.Cheers,
MilesBtw the method we will publish in omni.ocean will be based on this paper: Visual Computing @ IST AustriaCheers,
MilesIs Omni.ocean out yet? Where can I find information about it? It’s very difficult to find information about things in Omniverse!EDIT: Ok… so the extension is called “Ocean Deformer” apparently? And the only documentation is a very bare bones step by step in the extension description? There’s a video link sure, but you can’t even click it (or copy it) and it seems you have to sign in to some “Slack” before allowed to watch it? What if I don’t have an account.This doesn’t feel ready for general use, I must admit.Hi @EDRobert ,Yes documentation is limited - but there is a sample in the Sample Browser under Simulation->Ocean, that may be good starting point for creating your own scenes.Cheers,
MilesHmm… well we’ve just begun investigating Omniverse and that sample gets only 30fps in the viewport, while other files get 60fps.Attempting to drop one of our smaller boats into the sample yields a completely unusable application:
Screenshot 2023-06-19 2355311144×730 148 KB
We were hoping to be able to show off boats in VR but that seems like a showstopper.Also, why have you tied the viewport framerate to the responsiveness of the UI in the entire application?Hi @EDRobert ,Can I ask what version you are using?Also curious about the boat asset - is it this slow when you view it separately? Is the asset available online anywhere we could try?Cheers,
MilesI just downloaded it, so it should be the latest. It says 2022.3.3 in the launcher for Create.The boat has 60fps when viewed separately. It’s our latest production boat so unfortunately I can’t share it.Powered by Discourse, best viewed with JavaScript enabled"
323,face-indices-returned-by-getfacevertexindicesattr-incomplete-warp-usdgeom,"I am using the raytracing example from Warp that ships with IsaacSim and for some reason the array describing the indices of triangular faces is incomplete. My mesh is a simple cube created in Isaac Sim as a mesh. The figure shows the matplotlib print out of the raytracing on the mesh. As you can see not all triangles are present and it draws internal triangles as well.Hi @paolo.a.torrado,The Warp Mesh object expects triangular indices for faces. I think you would first need to triangulate the mesh which can be done with a simple snippet below:You call this with the USD faceVertexCounts and faceVertexIndices from the UsdGeomMesh before passing the triangulated indices to the Warp Mesh.We’ll look into providing a helper to do this for you.Cheers,
MilesThank you.This topic was automatically closed 14 days after the last reply. New replies are no longer allowed.Powered by Discourse, best viewed with JavaScript enabled"
324,cant-receive-image-from-isaac-sim-docker-image-using-ros2-humble,"Hi, I am using isaac sim using the provided Docker container with Ubuntu 20.04, on which I installed ROS2 Humble.
I did the setup for a ROS2 camera inside Isaac sim following the tutorial on the documentation, the problem is that when I subscribe to the topic by another machine the messages can’t be received and the images can’t be visualized inside Rviz2.By subscribing to the image topic in another bash inside docker I can correctly receive the images and by using the command ros2 topic bw /image_topic the output says that the data weigh 200Mb/s for 100 messages. Could this excessive size be the problem?The ROS2 communication works well with other sensor data like the laserscan from a Lidar sensor.
Do you have any solutions?Hi @parmeggiani.alessio1 - Have you reviewed this ROS/ROS2 Isaac Sim document?
https://docs.omniverse.nvidia.com/isaacsim/latest/install_ros.htmlYes, I reviewed the document and didn’t find anything useful.
The problem seems to be related to the connection quality, the images consume too much bandwidth.  There is a way to compress the images from inside an action graph?Hi @parmeggiani.alessio1  - Yes, the size of the data could indeed be the problem. Transmitting large amounts of data over a network can be challenging, especially if the network is not very fast or reliable.In ROS2, you can use image_transport package to compress and decompress the images. This package provides “plugins” for various forms of image transport. It allows interchangeability between different algorithms for image compression and decompression.Powered by Discourse, best viewed with JavaScript enabled"
325,sclect-xform-case-robot-move-when-importing-urdf,"After importing URDF, when I click on the XForm on the right, the robot will also move with it? I’m not sure what the problem is, but it seems to affect the reading of the force sensor.urdf.zip (6.4 MB)It can see that when I click , the position was clean and auto set to zero, As I continued to click, the robotic arm moved itself. Is this a bug or is it related to my URDF file?Hi @Robo_qq  - When you import a URDF file into Isaac Sim, the robot’s structure and joint information are preserved. If you select the robot and modify the XForm (transform), it will move the entire robot as a single entity, including all its child components and joints. This is because the XForm represents the global transform of the robot in the scene.If you have a force sensor attached to the robot, moving the robot using the XForm could indeed affect the readings of the force sensor. This is because the force sensor measures forces relative to its own frame of reference, which is attached to the robot. If the robot moves, the force sensor’s frame of reference moves with it, which could change the sensor readings.If you want to move parts of the robot independently without affecting the force sensor readings, you should manipulate the joints of the robot instead of the global XForm. You can do this by selecting individual joints and modifying their properties.want to move parts of the robot independently without affecting the force sensor readings, you should manipulate the joints of the robot instead of the global XForm. You can do this by selecting individual joints and modifying their properties.Hi，@ rthaker. I just clicked on XForm on the right side and didn’t move or modify them, but the robot did move(see the video)
And when I drag the position of one joint in the articulation inspector, other joints will also follow the movement. I think this is abnormal, but what is the reason for this?Powered by Discourse, best viewed with JavaScript enabled"
326,camera-info-in-ros-topic-after-changing-the-height-and-width-have-large-difference-in-fx-and-fy-why-does-this-happen,"The fx and fy values represent the focal length of the camera in the x and y directions and expressed in pixel units. These values are related to the camera’s field of view and the size of the image sensor.When you change the image size, the relationship between the field of view and no. of pixels in the image changes, which affects the focal length values. If the aspect ratio of the new image size is different from the original aspect ratio, the difference between fx and fy will be more significant.To maintain the correct camera calibration, you should recalibrate your camera after changing the image size. This will ensure that the fx and fy values are accurate for the new image dimensions.You can use the ROS camera calibration tools, such as the camera_calibration package, to perform the calibration.Powered by Discourse, best viewed with JavaScript enabled"
327,scaled-objects-do-not-change-physical-properties,"The forces/torques required to move a scaled object does not seem to change. I have attached a video with a standard cube and a scaled cubed. It can be seen that it requires same force to move both cubes. How do I scale the physical properties with the same amount as the visual? Changing e.g. the mass does not seem to have an effect.
ThanksThe forces/torques required to move a scaled object does not seem to change. I have attached a video with a standard cube and a scaled cubed. It can be seen that it requires same force to move both cubes. How do I scale the physical properties with the same amount as the visual? Changing e.g. the mass does not seem to have an effect.Hi @thokr19 - In physics-based simulations, scaling an object’s visual representation does not automatically scale its physical properties. This is because the physical properties of an object, such as its mass, inertia, and friction, are often defined separately from its visual properties.If you want to scale an object’s physical properties in Isaac Sim, you’ll need to manually adjust these properties. Here’s how you can do it:Remember that these adjustments might not result in a perfectly scaled behavior, due to the complexities of physics simulations. You might need to experiment with different values and observe the results to get the behavior you want.It makes perfect sense. I’ll try it.
ThanksPowered by Discourse, best viewed with JavaScript enabled"
328,issac-sim-installation-error-related-to-pip-versionu,"Error occurred during installation of Isaac Sim: Command failed: “/home/dijinli/.local/share/ov/pkg/isaac_sim-2022.2.1/omni.isaac.sim.post.install.sh” >“/home/dijinli/.local/share/ov/pkg/isaac_sim-2022.2.1”/omni.isaac.sim.post.install.log
ERROR: Could not find a version that satisfies the requirement werkzeug>=1.0.1 (from tensorboard) (from versions: none)
ERROR: No matching distribution found for werkzeug>=1.0.1
WARNING: You are using pip version 21.2.1+nv1; however, version 23.1.2 is available.
You should consider upgrading via the ‘/home/dijinli/.local/share/ov/pkg/isaac_sim-2022.2.1/kit/python/bin/python3 -m pip install --upgrade pip’ command.Hi @dijinli574 - The error message indicates that the installation script is unable to find a suitable version of the werkzeug package, which is a dependency for tensorboard.Here are a few steps you can take to resolve this issue:If the installation is successful, try installing Isaac Sim again.
3. Check your Python version: Make sure you are using a compatible version of Python. Isaac Sim 2022.2.1 requires Python 3.6 or later. You can check your Python version by running python --version in your terminal.
4. Check your network connection: The error might be due to a temporary issue with the Python Package Index (PyPI) or your network connection. Make sure you have a stable internet connection and try the installation again later.Powered by Discourse, best viewed with JavaScript enabled"
329,usd-composer-xr-vr-experiencing-really-poor-quality-in-vr,"I have a Oculus Pro headset and a Razer laptop with a RTX4090.  When I use the VR plugin in USD Composer, the quality is awful.  There’s a ton of visual noise, it’s glitchy in performance, everything seems slightly blurry unless you are very close to an object, and it almost gives me the appearance as if I were underwater-- because the way the noise in the light continues to bounce of objects.I’ve tried all the settings in the VR tab including foviation and resolution settings.  I can’t figure out why the quality is so poor.If I use the same headset, cable, and laptop in Unreal Engine, it looks great.Has anyone ran into this issue before?  Has anyone tried an Oculus with USD Composer on a 4090 GPU?  If so, what sort of results are you getting?Powered by Discourse, best viewed with JavaScript enabled"
330,sim2real-of-rmpflowcontrol-on-my-robot,"Hi.I am using IsaacSim to customize my own robot manipulation (Doosan M1013).Ive been trying to use ‘RMPflowcontorl’ solving IK and moving my robot end-effector.After that, how can I control my ‘real’ robot using RMPflowcontrol in the virtual world(Omniveres) ?Is there any easy method to control the real robot using RMP, otherwise should I make a specific interface to communicate with my robot?Could you tell me about the Sim2Real method of robots that IsaacSIm developers mainly use?Thank you !Hi Swimpark,WE will have some new tools for this in our next release, in April.Please stay tuned.kindly,
LiilaHi @ltorabi ,I have a similar problem. I have built an Isaac Sim simulation with a Universal robot and the RMPflow motion policy. This works quite well so far. Now I am not sure what approaches there are to transfer the RMPflow motion policy to a real universal robot, for example in Python or as ROS block…Can you help me further?Many thanks for your supportHi @robin-vetsch , that is great to hear.
Let me assign someone to help.We don’t have anything out-of-the-box like that for the UR line at the moment like we do for Franka. What you’d need to do is implement a controller which accepts the RMPflow command stream and translates that to desired velocities for the UR velocity control interface. See this discussion for a discussion of how to use the velocity controller:These questions are related to ros_controllers represented in the ur3_controllers.yaml file. Specifically the pos_joint_traj_controller, vel_joint_traj_controller and joint_group_vel_controller.  In the first part I ask questions about the 3...
Reading time: 2 mins 🕑
Likes: 4 ❤

This would likely involve translating the desired positions to velocity commands using a law similar toalpha and beta are both between 0 and 1. The first line nudges the velocity command in the direction of the current position error, while the second blends the desired command toward the desired velocity as a regularizer. You might also need to blend toward zero (regularize back toward zero).In the end, you’ll likely have to play with the parameters, or the form of the controller. These are well studied problems, so there are some good referenced out there on how to do this. It’d help to find a controls expert to consult with.We’re working to add more low-level controllers to enable easier connection to physical robots, but currently we only have the Franka as an example. Take a look at that to see what it’s doing:
https://docs.omniverse.nvidia.com/app_isaacsim/app_isaacsim/tutorial_cortex_6_ros_synchronization.html#controlling-physical-robots
You’d need to implement a version doing something similar for the UR using the UR’s control interface:Universal Robots ROS driver supporting CB3 and e-Series - GitHub - UniversalRobots/Universal_Robots_ROS_Driver: Universal Robots ROS driver supporting CB3 and e-SeriesPowered by Discourse, best viewed with JavaScript enabled"
331,failed-to-login-stuck-after-logging-in-web-browser,"I’m using the linux version of omniverse launcher(appimage), everything goes well before today, I have a problem of logging in, after I login in my web browser(the web page notice me that I can continue in launcher), the launcher stuck in login page like in the following screenshot:
failed_to_login1233×686 251 KBthe log here:I used to use the isaac sim and composer with this launcher, it works fine------------------------- EDIT -----------------------
found this post Omniverse launcher can not login - #2 by WendyGram
don’t know if it is the reason why I can’t login? but how to explain the login process works fine before?I am having the same problem, I am writing here to avoid having a duplicate topic.
Until two days ago everything was running fine, but today it gets stuck at the login page. @Te77iiiii_Ho if you managed to solve the issue, can you please tell me how?I have the same problem since July 18 2023Hi, @michele.angelini2 @charbel-boumaroun . sry I still can not login the omniverse-launcher, but if you want to use the specific app, you can directly launch it from command line. e.g. for composer(creator) just run <home_dir>/.local/share/ov/pkg/create-2022.3.3/omni.create.sh.but for isaac sim, when I ran isaac-sim.sh I can’t find ros examples from it, which is differ from launching isaac sim from omniverse launcherI am also having the same issue since the 19th of July. After I confirm the login in the browser, the launcher loads forever.
Would maybe deleting the local cache for omniverse  solve this?@Te77iiiii_Ho
I solved the issue by looking for the “nvidia-omniverse-launcher.desktop” file and copying it into the directory mentioned by the error (“/home/user/.config/autostart/”).
I hope this helps.Hi, did anyone solve the problem? I don’t get an error message but I can’t log in on Omniverse Launcher either. Unfortunately, @michele.angelini2 's hint didn’t help for me.This worked for me too, thank you!I used find / -name '*.desktop' | grep -i ""omniverse-launcher.desktop"" to find the .desktop file (In my case it was in the /tmp directory: “/tmp/.mount_omniveHeXU3G”) and copied it to “/home/USER/.config/autostart/”works for me~!
so weird that if missing the .desktop file is the issue, why I can login before…could you share some logs? @michele.angelini2 's method works for me. I launch the omniverse launcher through the AppImage file in command line, and I can check the log in terminal, if your log isn’t the same with mine, I think it might be a another issue.apparently, another file is missing, but I don’t know where to take it from. Do you have a privacy.toml / auth.toml at /home/user/.nvidia-omniverse/config?
the logs
Screenshot from 2023-07-24 09-54-481828×503 130 KB@sina.heidemann  Yes, I have those two files, but the auth.toml is empty, and privacy.toml is like this:This topic was automatically closed 14 days after the last reply. New replies are no longer allowed.Powered by Discourse, best viewed with JavaScript enabled"
332,public-audio2face-python-api-for-audio-animation,"I know there is a public REST API for Audio2Face, but is there a public (i.e., relatively stable) Python API? This thread Python API/SDK to inject custom parameters. in omniverse audio2face - #11 by ankitgargit123 is similar, but no actual Python API was listed (and some of the linked pages are now broken links).I wanted to automate as much as possible - another thread I posted recently talked about APIs to support the workflow to set up a character, but this time I am looking at how to convert lots of audio clips into animation clips. I was planning to do this inside a Kit extension, so using the REST API felt kinda weird for that.(After the above, still trying to decide if need to build a more functional sequencer so can blend smoothly between clips, merge in emotions along with lip sync, etc., or should I be using animation graphs somehow which already has a blend feature. This is per camera shot in an animated film sort of thing.)This one seems like a good start:
Developer Quick Start — Omniverse Kit documentation (nvidia.com)One other great way of learning is by looking at the original code for each extension. You can use the Extension Manager window to find, open and edit the python files used to generate each extension.Screenshot_51920×1032 152 KBScreenshot_6703×1207 83.3 KBThanks. The problem with looking at code is (1) it’s often not commented (!), and (2) it’s not a definition of a “public” API which is recommended for programmers to use.Powered by Discourse, best viewed with JavaScript enabled"
333,force-units-for-angular-and-linear-drive,"Hello,I am trying to figure out the units of force applied in an angular and linear drive for a joint.I have made a 1m lever arm with a 100g weight at the end to simulate a 1Nm force, I am looking for the minimum force capable of lifting the system.
I did this in 2 of my files and I get different values for each of themAll my files have the same following parameters:
-units in cm (0.01 meter per unit in the root layer)
-magnitude = 981In my first file which was an empty file, it takes an angular force of 160 to lift the lever arm.
In the second file, which is my project file, I need an angular force of 10000For the linear drive i got 1N = 16 (force set to 16 to lift 100g) in both filesWhat could be the cause and what parameters does the force depend on?
ThanksHow are you applying the force? Through an offset in drive target?Note that you should disable any damping/friction to get accurate forces for such a measurement. In particular, joint friction for articulations, and rigid-body linear and angular damping.Then the forces should be torque or force in the units system that you setup, e.g. 0.01 meter per unit and should match.Are you accounting for the weight of the link that holds the 100g weight?Thank’s for the reply,
I’m applying the force with both an offset in drive target and an effort with the function articulation.set_joints_efforts() in the python api.
I’m using isaacsim 2022.2.1My setup looks like this :

Joint friction and damping are set to 0.Root layer property :
My world Xform scale is set to 0.1 to get units in mm (cylinder lenth = 1000 for 1m):

worldproperty422×527 32.5 KB
physics scene property:
When I control it throught the usd project I have these settings :
To lift the cube with linear drive :
To lift the sphere with angular drive :
When I control it throught python api I delete the joint drive and use this function :
articulation.set_joint_efforts(joint_effort , joint_indice)I get same value for both :Now if I try in a new empty file, with exactly the same parameters listed above, I need an effort of 160 to lift the sphere (angular drive), and 1.6 to lift the cube (linear drive)In my previous post i said my linear effort ratio was 1N= 16 in both files:The main fact remains that the values are not the same between the 2 files, and I don’t know where these ratios come from or whether I can rely on them.Now if I try in a new empty file, with exactly the same parameters listed above, I need an effort of 160 to lift the sphere (angular drive), and 1.6 to lift the cube (linear drive)Hi Adrien,I would recommend comparing parameters between the scene where it works and the new setup. Probably easiest is to save the stage as usda and do a diff?This topic was automatically closed 14 days after the last reply. New replies are no longer allowed.Powered by Discourse, best viewed with JavaScript enabled"
334,add-search-path-for-lidar-configs,"Hello!Is there any possibility to add a search path at the beginning of a python script to find a custom lidar config file in my project directory?Thanks in advance!Best regards,
ChristofHi @christof.schuetzenhoefer - You can try the following script and let us know if you still face any issues:You can replace /path/to/your_project_directory with the actual path to your project directory, and replace custom_lidar_config with the name of your LiDAR config file (without the .py extension).Is this working with json files like the config file Example_Rotary? I assume this will only work with python scripts, but the question for me is, how can I use python scripts as lidar configuration files?In my case, I would like to have multiple json config files for multiple sensors and when I create a new sensor I would like to use just the name of the config file like in RTX Lidar Sensor — Omniverse Robotics documentation.You can add folders to the app.sensors.nv.lidar.profileBaseFolder setting in the exts/omni.isaac.sensor/config/extension.tomlfile.The search will use the first config if finds with the name you put in, so don’t have two configs named the same!See Lidar Config FilesThere is no way to set this in python, it must be set in the toml file before isaac sim starts up.This topic was automatically closed 14 days after the last reply. New replies are no longer allowed.Powered by Discourse, best viewed with JavaScript enabled"
335,set-exact-intrinsic-matrix-of-camera-fails,"Hi community,I want to mimic a real world camera of a robot. Thus, I want to precisely set the intrinsic matrix. I am using this code to set the parameters:Especially, the horizontal and vertical aperture offset do not seem to be set in the simulator. Would be a good solution to have a function that receives an intrinsic matrix and the focal length and sets all the corresponding parameters correctly.To reproduce, I am using a camera with focal_length=1.0, width=1440 and height=1080 with intrinsic matrix= (575.60504, 0.0, 745.73121, 0.0, 578.56484, 519.52070, 0.0, 0.0, 1.0).System Info:
Isaac Sim Version: 2022.2.1-rc.14
OS: Ubuntu 20.04Hi @roth.pascal  - The intrinsic matrix you provided seems to be in the format of:The horizontal_aperture, vertical_aperture, horizontal_aperture_offset, and vertical_aperture_offset parameters in USD are used to define the camera’s field of view and the lens shift. These parameters are not directly equivalent to the intrinsic parameters fx, fy, cx, and cy.The horizontal_aperture and vertical_aperture parameters in USD are defined in millimeters, and they represent the size of the camera’s film back. The horizontal_aperture_offset and vertical_aperture_offset parameters represent the lens shift, which is the offset of the lens from the center of the film back.To convert from the intrinsic parameters to the USD parameters, you can use the following formulas:Here, pixel_size_in_mm is the size of a pixel in millimeters. This value depends on the specific camera sensor you are using, and you would need to look up this value in the camera’s specifications.Please note that these formulas assume that the camera’s coordinate system is right-handed with the x-axis pointing to the right, the y-axis pointing up, and the z-axis pointing towards the viewer. If your camera’s coordinate system is different, you might need to adjust the formulas accordingly.Also, please note that the horizontal_aperture_offset and vertical_aperture_offset parameters in USD represent the lens shift, which is a relatively small effect that is usually only noticeable in wide-angle lenses or when using a tilt-shift lens. If you are not seeing a noticeable effect when changing these parameters, it might be because your camera’s field of view is not wide enough, or because the lens shift is too small to be noticeable.Powered by Discourse, best viewed with JavaScript enabled"
336,fail-to-load-built-in-material,"Hi, I added some built-in materials into an object.
I can’t load these materials nowHow to fix this, thanks!Hi - Sorry for the delay in the response. Let us know if you still having this issue/question with the latest Isaac Sim 2022.2.1 release.Failed to create MDL shade node for prim ‘/World/Apriltags/Looks/AprilTag_ID_02/Shader’. Unsupported identifiers.Hey all I have a similar problem but with the apriltagI got the similar error as doing this example2023-07-19 01:36:03  [Error] [omni.usd] [Watcher] Failed to resolve USD Asset Identifier ‘./materials/Pine_bark.mdl’ from prim ‘/Replicator/SampledAssets/Population_ed33029f/Cached31_Prim_pi/Looks/Pine_bark/Pine_bark’Hi @hannah.schieber - The error message you’re seeing suggests that there’s an issue with the Material Definition Language (MDL) shader node for the AprilTag. This could be due to a few reasons:Hi @darien.wei - The error message you’re seeing indicates that the system is unable to find the Material Definition Language (MDL) file Pine_bark.mdl in the ./materials/ directory relative to the location of your USD file.Here are a few things you can check:Powered by Discourse, best viewed with JavaScript enabled"
337,rtx-loading-message-on-the-viewport,"I’m new to Isaac sim. I followed the guideline to install Isaac Sim. I’m running it off a PC with a RTX 4090 GPU. when I launch Isaac Sim , it opens but I get a banner message that says "" RTX loading""image1920×1600 157 KBThis is the error message. How can I fix this?
image1908×1258 175 KBHi @user113648  - There two GPUs detected in the system. Can you please disable one and then try to run the Isaac Sim?Powered by Discourse, best viewed with JavaScript enabled"
338,fail-to-launch-nvblox-with-isaac-sim,"Dear NV team,
I tried to run NVBlox with Isaac SIM by following the instructions below:
https://github.com/NVIDIA-ISAAC-ROS/isaac_ros_nvblox/blob/main/docs/tutorial-isaac-sim.mdWhen going to step 5, it gives the errors as follows and exit finally:Tried different versions of NVidia drivers, it didn’t help. For RTX 4090, what driver version can be used? Thanks.System configuration:
OS: Ubuntu 20.04.06 LTS
CPU: Intel i9 13900K
GPU: NVidia RTX 4090
NVidia Driver version: 525.116.04
CUDA version: 12.0
docker version: 24.0.2
Python3 version: 3.8.10vulkan issue log (23.3 KB)
Log file is attached.Hi @andrew.li20181108 - You can find the Isaac Sim requirements here: 1. Isaac Sim Requirements — Omniverse Robotics documentationAlso, you can find the “Error_Device_Lost” related troubleshooting information here: Linux Troubleshooting — Omniverse Launcher documentationHi rthaker,
After loading driver 525.60.11 and selecting proper versions for other tools, NVBlox works now. Thanks a lot.
–andrewThis topic was automatically closed 14 days after the last reply. New replies are no longer allowed.Powered by Discourse, best viewed with JavaScript enabled"
339,omni-anim-people-lidar,"Currently using a wheeled robot with lidar to do SLAM. I added the omni.anim.people extension following the guide, to generate more rich tests with the navigation stack, just to see that actors are not detected by the lidars (Even if I add collisions to their mesh, as collisions do not move in conjuction with the meshes being animated). I saw in Omni.Anim.People People can not be detected by Lidars that you could use rtx lidar, but found no way to construct the omni action graph to publish the laser scan using it.Tried https://docs.omniverse.nvidia.com/app_isaacsim/_images/isaac_sim_sensors_rtx_based_lidar_node_overview.png and connecting it to the ROS2 Publish Laser Scan graph node but it does not workHi @tomas.lobo.it , please take a look at this . You should be able to setup and use an RTX Lidar sensor which will publish the lidar scan over to the navigation stack. People detections should come through on the occupancy map/laser scan with it.Update: Lidar RTX does work properly with the actor’s visuals, tho it has an issue. I have been checking the rtx  lidar documentation but it does not matter what values I set to ’ [start,end]AzimuthDeg’ or ’ valid[Start,End]AzimuthDeg’, I cant get the result to not be 380 deg.The other issue (tho not that it changes much) is if I change the numberOfEmitters to 1 and leave only a emitter with “azimuthDeg”:0.0, “elevationDeg”:0.0 and “fireTimeNs”:0, nothing works and it just crashes. Any help would be appreciated@rchadha friendly pingSame here for the following issue:
if I change the numberOfEmitters to 1 and leave only a emitter with “azimuthDeg”:0.0, “elevationDeg”:0.0 and “fireTimeNs”:0, nothing works and it just crashes.@mcarlson1 can you please help with the RTX Lidar issues mentioned above?Your link does not work, so I can only guess what is going on… however, let me just mention a few common problems.See this page:
https://docs.omniverse.nvidia.com/isaacsim/latest/isaac_sim_sensors_rtx_based_lidar/node_overview.htmlPowered by Discourse, best viewed with JavaScript enabled"
340,cant-load-physx-on-integrated-gpu-with-version-2023-1-0,"So few weeks ago I opened this issue: Can't load omni.physx when running CreateIt looked like there was a problem with physx extension not working on non Nvidia GPUs. And was told the newest version would fix it.But after installing the newest version, I see the error:2023-06-30 09:59:31  [Error] [omni.physx.plugin] PhysX error: Could not load libcuda.so: libcuda.so: cannot open shared object file: No such file or directory
2023-06-30 09:59:31  [Error] [omni.physx.plugin] , FILE /buildAgent/work/bd15983b3a9aba56/source/physx/src/gpu/PxPhysXGpuModuleLoader.cpp, LINE 200And can’t run any Physx related thing.I have no idea what CUDA is, but after checking online I found a link that I can’t post (but it’s NVIDIA CUDA Installation Guide for Linux from the official cuda page). And it looks like only Nvidia GPUs support CUDA.So there hasn’t been a fix for non-Nvidia GPUs yet? I currently have an Intel Integrated GPU, and have no problem running PhysX as standalone and outside of USD Composer. That makes sense because physx runs on CPU by default, so I don’t see why this shouldn’t work.Anyways, hopefully this can be resolved.Hi Ipares12,
We appreciate your comment and feedback. I do see your previous ticket regarding using physics on a non nvidia gpu. I will check into this and report back. Thank you.Hi, no news yet?I will check with the particular engineer again. Sorry for the delay.Hi @lpares12, sorry for the delay! Can you post the log for me please? I don’t have a non-nvidia gpu machine with Ubuntu handy so will need to figure out a way how to debug this locally.
I would think that omni.physx loads, but the error is caused by another extension that through a dependency executes some CUDA GPU work during startup (we do run a base test that starts up omni.physx on a machine with no GPU and that seems to be passing from what I am seeing).This topic was automatically closed 14 days after the last reply. New replies are no longer allowed.Powered by Discourse, best viewed with JavaScript enabled"
341,isaac-sim-ros2-dependencies,"Hi, I am writing a custom omnigraph node with ROS2 in Isaac Sim. I want to use the code below but I couldn’t find the package.xml file or where to use “rosdep” like in other ROS2 workspaces. How can I install a package like “cv_bridge” in Isaac Sim?import from cv_bridge import CvBridgeHi @MuratKOSE - Did you follow this ROS/ROS2 installation document?https://docs.omniverse.nvidia.com/app_isaacsim/app_isaacsim/install_ros.html#isaac-sim-app-install-rosPowered by Discourse, best viewed with JavaScript enabled"
342,omniverse-create-and-isaac-sim-fail-to-launch-in-linux,"Hi,I am using Omniverse for Linux in Ubuntu, but Create and Isaac Sim are unable to launch from the Omniverse Launcher as it will stay idle on the splash screen and nothing will come out.Some similar posts in the forum suggest it might be caused by a conflicting non-RTX GPU; however, my RTX GPU is 3070 Ti, which should fit.Is this issue prossibly resulted from any other factors?Really appreciate your help and advice.Lu YishengHi. Please share a copy of your full logs, the output of nvidia-smi and the version of OS your are running on.Powered by Discourse, best viewed with JavaScript enabled"
343,cannot-load-hello-world,"This is the error I get when trying to load Hello World
Using Isaac Sim 2022.2.1Hi @candy.lin  - It seems that code is trying to call the GetPath method on object that is None . This usually happens when property of an object is accessed before it has been properly initialized.In your case, prim object is None when code is trying to call prim.Gethpath().pathString.  This could be because the get_first_matching_child_prim function didn’t find a matching child prim and returned None .Please check following things:Powered by Discourse, best viewed with JavaScript enabled"
344,ghosting-previous-objects-shapes-on-new-render,"Hello folks,
Does anybody know how to get rid of the “ghost” objects that were present in previous renders but their shape is somehow still present in next render iterations?
I am trying to get rid of those in order to get clean images for offline data generation.Running the offline dataset generator script (3. Offline Dataset Generation — Omniverse Robotics documentation) or running the render directly from Isaac gives the same result.That’s the result after 9 iterations of viewport generation:

9788×616 53.7 KB
Disabling AO: Aggressive denoising makes the ghosting last for even longer. So I would assume that the denoising somehow has to be increased even more.
I followed the suggestion from Ghosting in the viewport rendering - #4 by cornibert , but that didn’t help.RegardsHello,It does look like what you can currently expect when Sampled Lighting is enabled in the Ray Tracing/Direct Lighting settings, if Shadows are enabled. This will be improved in the future. In a scene with few lights, keeping Sampled Lighting disabled is usually preferable.I disabled sampled lighting but that did not help.
The only way I can think of is changing the rendering mode, but that’s not what I am searching for.The only way to remove ghosting is to increase settle latency in the rendering settings in the movie recorder. The more frame “settles” the less ghosting you can expect.settle latencyHi where abouts is this? I don’t see it in the settings.Did you ever get this resolved? I have insane amounts of ghosting using replicator.
rgb_00241024×1024 124 KB
rgb_00251024×1024 125 KB
rgb_00201024×1024 118 KB
rgb_00221024×1024 119 KB
rgb_00231024×1024 119 KBPowered by Discourse, best viewed with JavaScript enabled"
345,isaac-sim-imu-not-yielding-correct-measurements-when-changing-its-orientation,"Hi there,I found that whenever we change the initial orientation of the IMUSensor on ISAAC, the values it yields are not correct.
These are the steps to follow to reproduce the error:Here a little video showing this. When it starts it yields the correct value of acceleration given the local frame. However, when it gets rotated, it yields incorrect values.It’s hard to tell from your example what orientation the IMU output should be at.  However, the IMU gives results in it’s own frame of reference.Try this.   Start the scene as you normally would, where the coordinate system (frame of reference) for the cube is the same as the world.  Hit play and see that the IMU reads 0,0,9.8 once the cube is at rest.Now reset the scene with the stop button, select the cube and enter 180 in the Y component of the cubes orientation.   The play again.   My result is 0,0,-9.8 as I would expect.  Since I rotated the cube 180 around Y, so it’s frame of reference is pointing up instead of down.Hi, thanks for the answer.I may have not well explained the problem:In the video, there are two simulations:@mcarlson1 I also noticed that if the unit of the the simulation is not 1 the linear measurements are scaled twice when using _is.get_sensor_sim_reading(handle + “/imu_sensor”) and also on the GUI.Hello, thank you for raising this issue. We are looking at this issue now and will get back to you when we have a fix or a solution. Thanks!Powered by Discourse, best viewed with JavaScript enabled"
346,is-there-a-simpler-example-on-how-rl-environment-works-in-isaac-sim-and-gym,"Hi, I have been trying to intergrate my own reinforcement learning algorithm into the Ant task and test it. However, I found it super challenging, because in your example from Github, you are using a half-completed package called rl_games. The package is poorly organized and has no comments or documents in it, which makes it almost impossible to understand. I understand you chose the package because it runs really fast. But can you  (dev team) publish an easy-to-use example for AI developers who value the algorithm itself and correctness over efficiency? Not every user is working for NVIDIA afterall.My current biggest question is, why do we need to register a RLGPUEnvs (from rl_games) when the environment related to my target task is defined elsewhere? And what does an observer do? And additionally, I can’t find “features” in your configurations, so why do you reference “features” in your AMPAgent?The codes can be found at:Hi @xuanyaoming. Since this is more focused on Isaac Sim, I’ve moved this topic to that forum for you.Hi @xuanyaomingI encourage you to try the skrl library.skrl is an open-source modular library for Reinforcement Learning written in Python (using PyTorch) and designed with a focus on readability, simplicity, and transparency of algorithm implementation. In addition to supporting the OpenAI Gym / Farama Gymnasium and DeepMind and other environment interfaces, it allows loading and configuring NVIDIA Isaac Gym, NVIDIA Isaac Orbit and NVIDIA Omniverse Isaac Gym environments, enabling agents’ simultaneous training by scopes (subsets of environments among all available environments), which may or may not share resources, in the same run.Visit is comprehensive documentation to get started :)https://skrl.readthedocs.io/en/latest/Many thanks! Just had a quick glance at the codes and documents. SKRL is a lot easier to read and fits my coding style perfectly. And supporting Isaac Orbit is a pleasant surprise. There shouldn’t be too much work to convert my RL code into their style.  Thanks again for your suggestion!Hi @xuanyaomingGlad to help :)Please don’t be afraid to create a new discussion or issue in the skrl repository if you have any questions or problems.This topic was automatically closed 14 days after the last reply. New replies are no longer allowed.Powered by Discourse, best viewed with JavaScript enabled"
347,need-viewports-in-standalone-windows-but-built-in-operation-crashes-isaac-sim,"Hi,I’m using Isaac Sim 2022.2.1 on Ubuntu 20.04.I need viewports that pop out into their own windows for the purpose of aligning them in a pixel-perfect manner on external displays for stereo vision capabilities. The built in function of right clicking a viewport tab and dislocating it from the kit app window seems to just crash isaac sim 2022.2.1. Is there a fix or workaround for this?Additionally, how would I go about generating more than 2 viewports? I recall being able to in a previous(or maybe future) version of kit where there was an ‘add viewport’ option that was very extensible.I’ve had quite a bit of confusion trying to parse the viewport documentation, as there is the ‘old viewport’ and the ‘new viewport’ APIs.Could I get some direction/suggestions on how best to do this?Thank you,-MatthewEDIT: I should add that I prefer doing all of this in the form of an Omniverse Kit extension so that I can automate all of the setup.Hi @LMTraina99 - Does this document and previous post help answering some of your questions?https://docs.omniverse.nvidia.com/prod_kit/prod_kit/programmer_ref/viewport.htmlHello!Not quite, though it is nice to know that I’m not the only one who has accidentally used the old viewport API.I’m specifically looking for the following capabilities:1- Popping out viewports into standalone windows that can be maximized
2- Generating new viewportsBoth of these elements have existed at one point in Kit, but neither of them are accessible via the Isaac 2022.2.1 GUI, so I’m looking for the API calls related to these functions.Thank youHi @LMTraina99 - This youtube video shows how to create the viewport. NVIDIA Omniverse Developer Office Hour - 10/07/2022 | NVIDIA On-Demand.This extension has some wrapper API for creating viewport windows: omni.kit.viewport.utility — omni.kit.viewport.utility 1.0.14 documentationHi,That’s helpful for creating more viewports, but what about creating viewports in new windows or popping ui elements out into their own windows?you can find out that information in this document: Window — omni.ui 2.15.6 documentationPowered by Discourse, best viewed with JavaScript enabled"
348,how-to-generate-objects-binary-masks-for-synthetic-data-offline-pose-generation,"Hi,I have a question regarding synthetic data offline pose generation. I have a specific requirement for my model where I need masks of objects in each frame. Currently, I have generated visibility masks, which are binary images representing the visible parts of each object. However, I also need the complete masks of objects, including the parts that are not visible. I was able to retrieve the visibility masks using the semantic segmentation annotator. Can anyone provide advice or suggestions on how to solve this problem?Hi @francesco.sarno,Are you using your own custom writer or one of the existing writers in the example?If you have the original object mesh file as an .obj or .ply, one hacky solution I can think of is reprojecting the mesh back onto the image using the ground truth annotations. That way, you can get a complete object mask.If you think this could work for you, let me know and I can find some scripts that do the reprojection.In the meantime, I will see if it is possible to directly get the complete object masks when you generate data.Hello,How precise are you looking to be?You can use the Bounding Box Loose annotator if working with bounding boxes for annotations.If you want an exact mask for a small amount of items, you could do it with multiple triggers and modifying visibility for a group of objects NOT the item(s) you are trying to detectExample:If you are looking to do exact pixel masks for every object in your scene it is not something that is done easily, but you could do something like the above with every object in the scene.  It would not be performant, and would need a lot of data-managing afterwardsHi @andrew_g,Thank you for your answer! Yes, I am running my own writer scripts, and I have a .obj file for the mesh of the objects. I think that your solution can work. If you can help me with the scripts you mentioned, it would be great!Best,
FrancescoHi @francesco.sarno,Here is the script and a simple example:
Forum_Question_Reprojection_Script.zip (60.0 MB)Note that I am not sure what annotation format you are using. The script expects annotations in the BOP format. But if you are using a different format, it should be pretty straightforward to change the way the script reads in the annotations.For this particular example, you can run the script using the command:python bop_reproject_script.py  --path_json 001001/scene_gt.json  --mesh_path obj_000003.plyLet me know if you run into any trouble.Best,AndrewPowered by Discourse, best viewed with JavaScript enabled"
349,cloth-simulation-parameters,"I am trying to perform cloth simulation, but I am struggling with the parameters. Specifically, since it is uses the stiffness, two hanging cloths with the same mass and same stiffness have different deformations based on the number of particles. How can we have consistent deformations independently of the number of particles, like using the Young module? (Obviously, I expect that more particles equal a more accurate simulation)Hi @giorgio.nicola,
Unfortunately the PBD particle solver we use for the cloth is not accurate in that respect. More particles lead to worse convergence. You can try to fight that to a certain degree using higher iteration counts and smaller time step sizes.
We will introduce a new deformable surface type soon that will be more realistic, featuring Young module as you mention, but even there it will be crucial to tweak iteration counts and time step sizes across different mesh resolutions.Cheers,
SimonHi @SimonPhysX  thanks for the quick update. What it concern me most is that in complex meshes the particles are likely to be not equally distributed. In particular density will be defined only based on the geometry complexity. Thus there will be accurate patches and inaccurate patches depending on the local density of particles, inaccurate patches might be where more stresses are applied to the cloth.
I suppose it would be better to manually generate the meshes to tweak the density of particles where the more strain is expected, similarly to FEM analysis.Have you already planned the release date for the new deformable surface type?This topic was automatically closed 14 days after the last reply. New replies are no longer allowed.Powered by Discourse, best viewed with JavaScript enabled"
350,im-looking-for-photo-realistic-articulated-objects-assets,"Hi, I’m looking for jointed / articulated objects like drawer, closet, door, etc… Does Omniverse provide any official articulated, photo-realistic objects?I’m trying to create this object with primitive meshes, but I would like to use more photo-realistic objects.
For example, please see this scene, which use IsaacGym.Thanks!Hi @PARKBONG  - You can always go to Isaac Assets tab and go through different categories to see what assets are available.In case if you don’t find what you are looking for then you can import them from any 3D model available online and then add that articulation.Unfortunately, I cannot find any articulated asset on the NVIDIA/Isaac Assets Tab.
But I did download some 3D mesh files online, and after adding a joint, It looks quite awesome.Thanks for your reply!This topic was automatically closed 14 days after the last reply. New replies are no longer allowed.Powered by Discourse, best viewed with JavaScript enabled"
351,isaac-sim-install-issue-buildpipeline-and-shader-loading-failed,"For background info, I’m running ubuntu 20.04 on a RTX 2070, with 8gb of ram, on driver version 50.86.01. Those all seem to be in line with the minimum specs, and compatible drivers. Nvidia-smi output pasted at the end.When running isaac sim, it seems to mostly load, but freezes, and this error shows up on the console. Any clue how to solve?For nvidia-smi,and for nvidia-smi -LHi @user10385 - Can you update your drivers according to instructions provided in the doc: 1. Isaac Sim Requirements — Omniverse Robotics documentationAlso, this thread might helpful to you:@rthaker I updated the driver to 525.105.17 following the instructions in the installation guide. No change in error. Then, cleared shader cache with  rm -r ~/.cache/ov/Kit/*. No change in error. Haven’t tried downgrading to 2022.2.0 Sim version. I have 32 G ram, and 8G VRAM so I seem to meet minimum spec. What else could be wrong?Hi. Try removing the folders below:Then resintall the latest 525 drivers.@Sheikh_Dawood Did that, with sudo rm -rf /etc/vulkan/ and sudo rm -rf /usr/share/vulkan/ and reinstalled the driver, with no effect. Same error. Only change is my driver version is now 525.116.03.Also tried deleting the shader cache and the vulkan folders, and then reinstalling, to no effect.Hi. Can you please share your Isaac Sim log file.@Sheikh_DawoodHere’s a log file from a run without clearing anything:
kit_20230505_101151.log (1.4 MB)And I think this is one where I cleared the vulkan and cache folders:
kit_20230501_103142.log (1.4 MB)Thanks. Are you able to install and run Create and Code?Another thing to try is uninstall Isaac Sim, make sure this folder is empty or remove it.
~/.local/share/ov/pkg/isaac_sim-2022.2.1
Then re-install Isaac Sim and run.It seems like I can run create and code, but they showRTX LoadingIn the viewport, and exit themselves after a bit. So I can’t actually use them. I’ll try re-installing.@Sheikh_DawoodOkay looks like I got a different error. Here’s the log from it. I couldn’t copy the error message because it closed the terminal.kit_20230505_131219.log (1.7 MB)Hi. Are you able to run and older version of Isaac Sim or Create successfully?After the last change, it started to work, so I’m able to run sim now.This topic was automatically closed 14 days after the last reply. New replies are no longer allowed.Powered by Discourse, best viewed with JavaScript enabled"
352,unable-to-install-omniverse-launcher-failed-to-get-documents-path,"As the title says, I am having trouble installing Omniverse. When downloading and opening the launcher it gives me the error “Failed to get documents path”.I have tried deleting every instance of Omniverse from my computer, and I have used the Nvidia cleanup tool to no avail. Here is the log generated:[2023-02-22 18:24:01.098] [info]  Omniverse Launcher 1.8.2 (production)
[2023-02-22 18:24:01.101] [info]  Argv: C:\Users\kaede\AppData\Local\Programs\omniverse-launcher\NVIDIA Omniverse Launcher.exe
[2023-02-22 18:24:01.101] [info]  Crash dumps directory: C:\Users\kaede\AppData\Roaming\omniverse-launcher\Crashpad
[2023-02-22 18:24:01.103] [error] Error: Failed to get ‘documents’ path
at get extensionPath [as extensionPath] (C:\Users\kaede\AppData\Local\Programs\omniverse-launcher\resources\app.asar\dist\main.js:409:249880)
at new ie (C:\Users\kaede\AppData\Local\Programs\omniverse-launcher\resources\app.asar\dist\main.js:409:250992)
at Object.reset (C:\Users\kaede\AppData\Local\Programs\omniverse-launcher\resources\app.asar\dist\main.js:409:429501)
at Object.initializeApps (C:\Users\kaede\AppData\Local\Programs\omniverse-launcher\resources\app.asar\dist\main.js:409:429240)
at C:\Users\kaede\AppData\Local\Programs\omniverse-launcher\resources\app.asar\dist\main.js:409:493201
at App. (C:\Users\kaede\AppData\Local\Programs\omniverse-launcher\resources\app.asar\dist\main.js:409:498298)
at App.emit (node:events:527:28)Thanks in advance!every instance of Omniverse from my computer, and I have used the Nvidia cleanup tool to no avail. Here is the log generated:does the issue persist on your end or were you able to figure out workaround?To circle back as I was trying to help another user with the same issue on the OV Discord, he was able to figure out the issue. Turns out it’s related to OneDrive. Here’s his solution -It was really the Documents path is not exist due to “OneDrive”. The Document folder’s location was set to “C:\<user_name>\OneDrive\Documents” instead of “C:\<user_name>\Documents”. Also I disable OneDrive and changed Desktop location from OneDrive but not all thing.First I tried to change it through property but it refused. So, I changed it using Regedit.exe. Go to “HKEY_CURRENT_USER\Software\Microsoft\Windows\CurrentVersion\Explorer\User SHeel Folders” and remove \OnDrive\ and front abosolutepath and re-matching it like “%USERPROFILE%+\Docunemts”. After this change, I could change the location of documents and Pictures. Then I re-installed Omniverse Launcher to confirm it works.My Guess is that installing Window with Microsoft Account  makes onedrive active and it changes path of some under directory to OneDrive.There may be some typos in there, but hope this helps.Powered by Discourse, best viewed with JavaScript enabled"
353,generating-synthetic-data-with-preset-animation,"Hello,I’ve worked through the Replicator Composer and Synthetic Data Generator tutorials and they work fine for recording synthetic data annotations of assets without animations. However, I would like to record annotations of synthetic humans doing everyday tasks (like the animations/actions captured by omni.anim.people or even just the static animations in NVIDIA/Assets/Isaac/2022.2.1/NVIDIA/Assets/Characters/Reallusion/Worker/Props).Here’s an example of what I tried. I opened the full_warehouse_worker_and_anim_cameras.usd stage (from NVIDIA/Assets/Isaac/2022.2.1/Isaac/Samples/Replicator/Stage). The worker animation (from NVIDIA/Assets/Isaac/2022.2.1/NVIDIA/Assets/Characters/Reallusion/Worker/Props) plays as expected when I press the play button, but once I press the start button in Synthetic Data Recorder, the worker animation is clearly not recorded in the recorded frames (the worker is stationary). The exact same thing happens when I run the tutorial for omni.anim.people - the human assets are still doing T-poses (see images below).semantic_segmentation_0000512×512 1.8 KB
rgb_0000512×512 211 KBIs it possible to get the Synthetic Data Recorder and/or omni.anim.people working with such a preset animation? If not, how about Replicator Composer?Thank you,
LyndonHi @lyndon.chan I’ve moved this post to the isaacsim forum where I think you’ll have people who have more expertise in your question.Powered by Discourse, best viewed with JavaScript enabled"
354,3dsmax-2023-exporting-models-with-instances-and-broken-shaders,"I am using the Latest of everything, fully updated to the latest of 3dsmax 2023,
vray 6,
omnierse latest 2022.3.0, and
3dsmax connectors latest 200.1.03dsmax 2023 exporting models with instances…This is becoming a Major BUG for me, as I am trying to develop a Library system for my client, and many of my files assets have instanced meshes with in them. they are starting to get annoyed at me, for failing…If I export a file with instances option turned on, MDL options on etc, and with the shared textures option.
force file upload ticked also.It will export my model into the usd fine, In CREATE you can see your model fine.
The Instanced object, say the first one has the shader on working, but any instanced objects of that one, do not have any shaders on…so to clarify, if you have Teapot_000, then instances of it, 002, 003, 004, 005 etc
only the Teapot_000 has the intended shader on, the rest come in Blank, they are However referencing the Teapot_000.usd mesh which is great, Just not the shaders at all.now imagine this on a large scene, well it’s un-usable.the only way I know how to fix this is not to use instances at all and manually in CREATE make each object reference back to the _000.this is quite time-consuming when you know the other file with instances does do this, just not with the shaders.PLEASE FIX THIS, asap, so shaders are taken across instances also, it seems it’s simply not been thought about in the export.I’m happy to fix this in CREATE if someone can tell me how to also.Actually this problem have been around from previous version too and the worst is that even trying to do a quick over write of the material doesn’t work and causes other issues with the models(some models just disappear in some instances). Been wanting to report this but been busy to create a test scene/case, probably should so that omniverse team can fix this issue.Update , created and attached two simple scene and weirdly I have problem exporting 3dsmax primitive box instance or not in 3dsmax 2023(200.1.0 connector and latest nvidia studio driver). The other geosphere one export fine but instance export do not have any textures, while non instance export works as expected, just wish Arnold/physical material can be totally supported. Thanks for the support omniverse team !3dsmax2023 file for instance testing.zip (506.7 KB)Hello @chris.wade & @DavidDPD!  I’ve sent this issue over to the dev team to fix.  Thanks for bringing this to our attention!A development ticket was generated from this post. OM-72576: 3dsmax 2023 exporting models with instances and Broken ShadersHi David, The next version of the connector that will be coming very soon I believe has this issue fixed, I have tested your sample scene and appears to work properly.  However , as a note. there is a condition you should still be mindful of when going from 3dsMax to USD. 3dsmax allows you to override material assignments on instances while USD does not.  So if you have your 3dsMax scene authoring with instances and have material overrides on them, you will not get the material overrides into USD.Hey @chris.wade, Could you verify in the export settings if “Include Dependencies” was selected in the drop down next to MDL Context?This is necessary to get all dependency files for the shared textures option.Thanks for bringing this to our attention.Guys, for some reason, I did not see or missed I had any responses to my question, sorry for the delay in replying to this…lol, I forgot to check also, kinda gave up on it also.Yes of course I had Dependencies ticked.
It turned as I have spoken directly with the Programmers now, that this is a BUG in the export, and I have been told in the JUNE 2023 update to the 3dsmax connector, this bug has FINALLY been fixed.I am holding my breath on this one, if it is, then thank you.
what is slightly annoying though, is the button to do this is in the exporter window as if they have thought about it originally, but obviously no one tested it and fixed it at the time, and no one tells you this feature does not work yet, so you spend HOURS and HOURS trying to suss it out.I did find a way to Create a Fixed Database system myself, using a methodical naming system, and then not using instances at all, this gives you a fully working scene, not referenced back to the Database, but if you are happy to spend many hours manually going through each asset, and relinking the filename in the usd, back to its relevant _000 version of itself then that works with materials, might not have explained that very well.
the drawback is it’s time-consuming, and I work on quite large files, so turned out to be not practical, so I gave up and have been waiting for a fix ever since.OM-72576@chris.wade : Please try out v201.0 - we have addressed this issue. Let us know your results.Powered by Discourse, best viewed with JavaScript enabled"
355,trajectory-generator-for-franka-panda-arm-not-working,"Hello,I have tried to generate the trajectory of the Franka Panda robot using the LulaTaskSpaceTrajectoryGenerator. To do this, I followed a similar code style provided in the example: using UR10. However, I am encountering the following error:I have written the following script, which is causing the aforementioned error:I have attempted to debug the code, but I am unable to identify the cause of this issue. Any assistance would be greatly appreciated.Thanks!Hi @ksu1rng  - The error message suggests that the trajectory object is None. This means that the compute_task_space_trajectory_from_points method failed to generate a trajectory.This could be due to several reasons:To debug this issue, you can add some print statements or use a debugger to inspect the values of the variables and the state of the program at various points. For example, you can print the values of task_space_position_targets, task_space_orientation_targets, and trajectory to see if they are what you expect.Powered by Discourse, best viewed with JavaScript enabled"
356,can-we-create-game-using-omniverse-apps-and-publish-them-to-playstation,"Hello,
my company intended to create games (VR and non-VR) for PlayStation using NVidia Omniverse.
Is it possible, I searched the internet and did not get direct answer.Hi @kislay.sharma and welcome to the NVIDIA developer forums!I think I let the Omniverse experts answer that, let me move your post to the correct category.I can also recommend joining the Omniverse Discord!Thanks!Hi, Omniverse is an excellent companion to any development pipeline but it is not a runtime engine that can be deployed on consoles. Where Omniverse would fit in the game development pipeline would be to facilitate level building, collaborate on large scenes with multiple users and bring in assets from any digital content creation tool.  With connectors for Unreal Engine and Unity you could leverage Omniverse to accelerate your game asset development.Here is an example of Omniverse connecting different content creation software to accelerate scene development. Unreal Engine City of Brass Multi-Application Omniverse Workflow Tutorial with Sir Wade Neistadt - YouTubeHere are game development specific sessions:New career opportunities are emerging in the development of shared virtual 3D worlds that are interactive, immersive, and collaborativeNVIDIA will cover how to use the various collaboration tools available in Omniverse for game development pipelinesPowered by Discourse, best viewed with JavaScript enabled"
357,orientation-of-camera,"Hello!I want to attach a camera to custom objects and there are some problems regarding the orientation of the camera. The parent is a XForm prim that is there for proper orientation of the camera, but whatever I try, the orientation does not really change as I would expect. The XForm is also not changing the orientation as I am trying to do using set_local_pose. So, now the problem is that I cannot fix the orientation of the camera such that I can capture synthetic data.It does not matter, if I try to change the orientation of the XForm or directly the camera, the result is somehow weird and not logical to me.How is it supposed to work?Best Regards,
ChristofHey Christof,This is a great question! There are three different frames that the Isaac Sim/Omniverse supports wrt to the camera:If you take the pose of the camera from the Property tab of the scene and put it into omni.isaac.sensor Camera object via get/set, then it would result in weird behaviors because the pose does not match.To work around this, you can do one of two things:If you know the pose of the camera that you like in the world frame, directly use omni.isaac.sensor.Camera . Example:If you would like to set the pose of the camera using the USD pose, directly use the XForm and set pose using the data from Property panel.We are actively looking to improve the camera interface for Isaac Sim and we have improvements coming in the next Isaac Sim release. The features that will help you with this issue are as follows:Powered by Discourse, best viewed with JavaScript enabled"
358,grasshopper-mdl-table,"I  am unable to add a material in the Grasshopper MDL table.
I duplicated an existing Blue Glass material and simply changed the color to Red…
Moved the MDL to the “omniverse://localhost/Projects/TestSquare/” location.
Added the following to the omniverseMDLtable.csv… “Red Glass,NONE,Red_GlassC,/Projects/TestSquare,”Unfortunately is not loading…
What am I missing?@rdavoliog
Here are the instructions for Rhino
https://docs.omniverse.nvidia.com/con_connect/con_connect/rhino/materials.htmlYou can use these same instructions for Grasshopper materials handling.As there is an equivalent Documents\Omniverse\Grasshopper\omniverseMDLtable.csv file that can be edited in the same way.Powered by Discourse, best viewed with JavaScript enabled"
359,docker-issue,"I have this docker issue with Isaac sim 2022.1.1 docker:I was able to get it to work on certain 515 driver before, but linux dirvier store do not have this driver anymore, and some how automatically bump me to a new driver version.root@ubuntu:/isaac-sim# ./python.sh standalone_examples/api/omni.isaac.franka/pick_place.py
libGLX_nvidia.so.0 (libc6,x86-64) => /usr/lib/x86_64-linux-gnu/libGLX_nvidia.so.0
Writing disposable ICD file (/tmp/tmp_icd_POsgK2.json)…
GPU0
apiVersion     = 1.3.224
driverVersion  = 525.60.11
vendorID       = 0x10de
deviceID       = 0x2231
deviceName     = NVIDIA RTX A5000Writing ICD file to (/etc/vulkan/icd.d/nvidia_icd.json)
Could not open (/etc/vulkan/icd.d/nvidia_icd.json) for writeroot@ubuntu:/isaac-sim# cat /etc/vulkan/icd.d/nvidia_icd.json
{
“file_format_version” : “1.0.0”,
“ICD”: {
“library_path”: “libGLX_nvidia.so.0”,
“api_version” : “1.3.224”
}
}root@ubuntu:/isaac-sim# nvidia-smi
Thu Jul 13 23:24:35 2023
±----------------------------------------------------------------------------+
| NVIDIA-SMI 525.60.11    Driver Version: 525.60.11    CUDA Version: 12.0     |
|-------------------------------±---------------------±---------------------+
| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |
|                               |                      |               MIG M. |
|===============================+======================+======================|
|   0  NVIDIA RTX A5000    Off  | 00000000:2D:00.0  On |                  Off |
| 30%   31C    P8    24W / 230W |   1646MiB / 24564MiB |      0%      Default |
|                               |                      |                  N/A |
±------------------------------±---------------------±---------------------+±----------------------------------------------------------------------------+
| Processes:                                                                  |
|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |
|        ID   ID                                                   Usage      |
|=============================================================================|
±----------------------------------------------------------------------------+Can some one provide some help?some update:it seems this line of code in vulkan_check.sh caused the issue:LD_LIBRARY_PATH=/isaac-sim/kit/plugins/carb_gfx     /opt/nvidia/omniverse/vkapiversion/bin/vkapiversion     /etc/vulkan/icd.d/nvidia_icd.jsonisaac sim 2022.1.1 is not supported on docker. even though it works locally.
In addition, there are some issues with the base image, I need to install a bunch of other stuff to get it to work.Hi. We recommend to use the latest Isaac Sim version and latest production stable drivers.
May I know why you would prefer to stick to version 2022.1.1?Here’s is a dockerfile for 2022.1.1 that may help. You can modify it to add any custom add ons.Hi, Thanks for the reply. This is because we are preparing for the code release for our research paper which is based on 2022.1.1. Some of our code relies on the internal implementation of Isaac sim, which might break if switching to other versions. I think I figured out the solution eventually. I have to use the recommended driver. 525.60.11. Other driver versions, even the newer drivers, will cause issues. In addition, it seems ubuntu 22.04 is not supported.Powered by Discourse, best viewed with JavaScript enabled"
360,replicator-sometimes-seems-to-break-the-existing-camera,"Hi!I’m sometimes struggling with Replicator (in Isaac Sim 2022.2.1) as it randomly starts to produce these errors when I’m programmatically capturing synthetic data (in this example rgb and depth):This happens with the exact same code that first did work but then breaks without me changing anything. Once it is broken, I cannot attach the render products to a BasicWriter anymore, nor can I capture the synthetic data.The problem seems to be in the Camera USD, because when I replace the camera again by creating a new one, it does work for a while. So what I’m doing is the following:I think that the question comes down to the following:How do I programmatically use Replicator with an already-existing camera in the scene without the code breaking the camera? This includes fixing that the viewport should switch to the actual camera instead of temporarily changing the Perspective’s POV.Thanks!Hello @rosalie_va, thank you for reaching out! Creating a render_product with an existing camera works exactly as you’ve shown (assuming you have a  camera existing at ""/World/Camera""). This new render product is not connected to the viewport in any way, so if the viewport is currently set to the “Perspective” camera, the viewport should not be affected. To see from the capture camera’s perspective, you need to manually change the viewport’s camera. So the “the Perspective is temporarily put to the POV of the Camera” behaviour is unexpected. Please provide a more detailed example of the code you are running, I’d love to help you get to the bottom of this.HelloDid you find the solution or what caused it?Powered by Discourse, best viewed with JavaScript enabled"
361,water-materials-black-spots-how-to-eliminate,"Hey, I managed to bring in Alembic data to Composer!Now I play with a rain drop material, but I cannot eliminate that black ring… Real Time RTX is preferred. Any ideas?same thing with native sphere mesh

image780×473 47.9 KB
This looks like a case of not enough reflection bounces, so the spheres are showing up as black. I am assuming that in path tracing mode, this does not happen and it looks correct. This is easy to solve, you just need to increase the number of reflection bounces from “1”, which may be set by default, to a larger number like 4. Increase by +1 until desired result.Hi!
Sorry this was not a solution:and those black reflections are also in path traced more as you can see from the videos I have posted here.Hey, on brass material this setting thing works!
So it turned out that the water material I had tweaked myself was somehow
strange…I had problems with all water materials from library gave me just black surface.
I write a new topic about that.This topic was automatically closed 14 days after the last reply. New replies are no longer allowed.Powered by Discourse, best viewed with JavaScript enabled"
362,script-editor-not-loading-the-code-from-jupyter-lab,"I bought the course “Introductions to Robotics” from NVIDIA and got access to the Jupyter Lab of the course. Unfortunately, for a reason that neither the NVIDIA itself can explain, I cant access the course anymore. Luckly, I have saved the text content of the Jupyter Lab so I could try later by myself. So today, I opened the Isaac Sim in my machine, and tried to do the content of the Jupyter Lab by hand.
The thing is, on the Jupyter Lab, we can just press “run” to a code in it to run by itself and show up the results on the Isaac Sim of the Streaming Client. But, by myself, I cant make the code works. I opened Script Editor, that seems to work when I print random strings, but using the code from the Jupyter Lab, it doesnt return any errors but doesnt do anything either. The first and only code I tried is the following:from omni.isaac.examples.base_sample import Base Sampleclass HelloGround(BaseSample):How can I make this and all of the code from JupyterLab work when doing by myself?Hi @spi.admic  - Can you share the error message you are receiving when you run the code?Powered by Discourse, best viewed with JavaScript enabled"
363,can-intel-i5-12400f-install-isaac-sim,"The install requirements require i7 7th Generation, but my computer is i5 12th Generation, it is enough to install isaac sim?A Core i5-12xxx should be more than sufficient, assuming your system satisfies the GPU and RAM requirements.You can find more information here for Isaac Sim installation requirements: 1. Isaac Sim Requirements — Omniverse Robotics documentationthanksThis topic was automatically closed 14 days after the last reply. New replies are no longer allowed.Powered by Discourse, best viewed with JavaScript enabled"
364,objects-on-the-cloned-environments-copy-the-transformations-of-the-objects-in-the-base-environment,"The context is, for example, when using GridCloner for RL purposes, such as in this orbit example. I start the simulation app from a standalone Python script.When you move an object in the base environment, the corresponding objects in the cloned environments also move (but not the other way around). This transformation association between the objects of the base and the specific cloned environment breaks when you move the object in the specific cloned environment at least one time manually (e.g., with the gizmos from the GUI), while the association stays the same for the other clones environment objects.The attached video below demonstrates the issue well.Is this a bug or a known working principle? I would appreciate some info regarding why this happens and how to prevent it from happening.Hi @ozhanozenThis is a known issue with how the cloner in Isaac Sim behaves. In 2022.2.1 release, it currently “inherits” all the USD attributes/properties from the template/source environment to the clones ones. Due to this, when you are doing the GUI transformation in env_0, the changes are getting propogated to env_N with N=1,…There has been a fixed made for this for the upcoming 2023.1 release, where the environments “copy” from the source prim and not inherit.As a quick fix, you might be able to get it working by modifying the omni.isaac.cloner:cloner.py file. Instead of doing the following in L-215 in the class Cloner:you can change it to do a copy from source:I hope it helps solve the issue!@mmittal Thanks a lot for the information.The same issue appears (while running Isaac Sim from a Python script) when I create an XFormPrim for the source prim and apply set set_world_pose on it. Is this due to the API using USD as the middle layer?Is there actually a way to bypass the USD (e.g., when using flatcache) for the set_world_pose and get_world_pose operations and directly read/write on the cache? This would also be beneficial for performance reasons considering an RL training pipeline.Yes. Any USD-level changes affect all the clones that are inheriting from each other. Currently (in Isaac Sim 2022.2), it isn’t possible to by-pass the USD API for some things.For your usecase, if the prim you are manipulating has physics enabled on it, I recommend using the RigidPrimView or ArticulationView classes from Isaac Sim. There you can directly set the poses through Omni-Physics (i.e. via flatcache mechanism).@mmittal, thanks a lot. As far as I understand, RigidPrimView or ArticulationView should be preferred over XformPrimView as they use flatcache mechanism, which is faster for RL reasons. However, I am a bit confused about how to do this.In our situation, we have a “module” xform, that represents all the parts inside, including some “bolts” (with meshes). We want, that when we move the module, all the bolts move accordingly, and then we access the positions of these bolts at each step of the training. How would you set the module and bolt, in this case, using RigidPrimView or ArticulationView?I have tried making the module have an articulation root (and accessing it through ArticulationView), and making its “bolt” (component) have rigid body properties (and accessing it through RigidPrimView). However, I am not sure if it worked as intended, as the training is very slow. It would be nice to have a guideline regarding how to set up access with complex hierarchies using RigidPrim and Articulation.Hi @ozhanozenFor this case, it probably makes sense that you have a rigid prim view for each asset (module as well as the bolts) and offset all of them together using the physics API (i.e. apply the same offset transformation to each and set them in the world frame using the set_world_poses method).This doesn’t exploit the scene hierarchy that nicely though. But I am not sure if the PhysX tensor APIs can allow such operations.  It might be possible to handle a more generalized version of the tensor APIs called Fabric and USDRT that will be available from Kit 105.In your case, is it that you don’t want to create a rigid body on the module (i.e. it is a static collider and not really movable)?Hi @mmittalThe module is not an individual part but rather a container that has maybe 150 small subcomponents inside, including the screws. If I put a rigid body on the module, I cannot put one to the screw as the screw is the child of a module (actually, there are even more layers in between), and PhysicsX complains.As all these 150 subcomponents are normally fixed to each other, when you move a module, you expect that all the subcomponents move accordingly. This would be very cumbersome to do without using a common parent element such as the “module”, as I would need to iterate for all subcomponents. That is why I thought putting an Articulation on the “module” (and a rigid body on the subcomponents) could be an option. Does this make sense? If so, how should I structure the module/subcomponent assets for optimum performance? (e.g., do I need to construct PhysicsFixedJoints between the module and screws and similar for performance reasons)Thanks in advance for your support.Powered by Discourse, best viewed with JavaScript enabled"
365,cant-load-custom-assets-into-isaac-sim,"I am trying to load a custom asset from a python script using SimulationApp. I get the following error on trying to load the custom asset from the localhost:-The same asset can be dragged and dropped onto a blank scene in Isaac SIM. I would really appreciate some suggestions.I also have this problem!@jay.anjankar can you share the usd asset and a minimal python script that reproduces the issue?What API are you using to load the custom asset?@jay.anjankar / @leila , as requested by Hammad in previous comment, can you please share the usd asset and minimal python script?cone.zip (15.7 KB)
custom_assets.py (2.3 KB)
Hi Hammad and rthaker,
This is the script to recreate the issue related to loading of custom assets. The custom asset can be loaded locally (from the Desktop) but it cannot be loaded from inside the local server inside nucleus.Hi - Sorry for the delay in the response. Let us know if you still having this issue with the latest Isaac Sim 2022.2.1 release.Hi rthaker, I am still facing the issue in 2022.2.1. I can load the assets from my local system (from the Desktop for instance) but am unable to do so from my local Nucleus server.Hi. Can you try copying the file to local disk and edit this line to the point to a local path?
CONE = 'omniverse://localhost/path_to/traffic_cone.usd'Hi Sheikh,
I am able to load the assets from my local disk (Desktop). I was wondering if there was a way to add a path to a local host using something similar to find_nucleus_server().Do you mean to use a local disk path as Isaac Sim default root asset path?
This is feature is not available yet by default. We will work on this for the next version.No, I would like to load custom assets from my local Nucleus server. I can drag and drop the assets onto Isaac Sim but I am unable to add them via the Python interface.@oahmed - Can you help with this?omniverse://localhost/path_to/traffic_cone.usdHello, I have the same probleme running Isaac sim as container. I want to test the offline dataset generation (forklift) by changing the cone in the simulation with a custom usd asset.I think I understood that all the assets are remote for the container version: “The Isaac Sim docker container does not include Nucleus and will access assets directly from the Cloud by default.”I changed this part of the code as you mentionned:
def place_cones(): #prefix_with_isaac_asset_server(CONE_URL)
cones = rep.create.from_usd(""omniverse://localhost/isaac-sim/standalone_examples/replicator/drone.usd "", semantics=[(“class”, “TrafficCone”)])But I get the following error:
[Warning] [omni.usd] Warning: in _ReportErrors at line 2830 of /buildAgent/work/ca6c508eae419cf8/USD/pxr/usd/usd/stage.cpp – Could not open asset @omniverse://localhost/isaac-sim/standalone_examples/replicator/drone.usd%20@Is it doable on the container version?Hi @jay.anjankar - The error message you are encountering might be an issue with the connection to the Omniverse server. I would suggest few steps for you to follow:Make sure the Omniverse server is running and accessible from your python script. You can test connection in web browser navigating to the omniverse server’s address (http://localhost:3009)Ensure that you’re using the correct path to the custom asset in your Python script. The path should be in the format omniverse://localhost/Projects/YourProject/YourAsset.usd. Double-check the project name and asset file name for any typos or inconsistencies.Verify that you have the necessary permissions to access the custom asset on the Omniverse server. You might need to log in with your Omniverse credentials in your Python script using the omni.client API.Make sure you’re using the correct API calls to load the custom asset in your Python script. Here’s an example of how to load a custom asset using the SimulationApp API:Replace YourProject and YourAsset.usd with the appropriate project name and asset file name.@thealduin64 - It seems like there is an issue with the asset path you are using. The error message indicates that there is an extra space character %20 at the end of the asset path, which might be causing the problem.Please make sure there are no extra spaces or special characters in the asset path. Update the asset path in your code as follows:OK, I don’t think there was a space when i tried it but will double check, so you’re saying It’s not mandatory to create a nucleus with my assets and that I can use the local path, even using the container deployment (as it does not contain any nucleus by default).Yes, you can use local paths to load assets in Isaac Sim, even when running in a Docker container. However, you need to make sure that the Docker container has access to the local file system where the assets are stored.When you start the Docker container, you can use the -v option to mount a local directory to a directory in the Docker container. For example:This command mounts the local directory /path/to/local/assets to the directory /assets_in_docker in the Docker container. You can then load assets from the /assets_in_docker directory in your Isaac Sim scripts.Please note that the local directory should be an absolute path. Also, the Docker container needs to have the necessary permissions to read the files in the local directory.If you have a lot of assets or if you want to share assets between different machines, using a Nucleus server could be more convenient. But for a small number of assets or for testing purposes, using local paths can be a simpler solution.Powered by Discourse, best viewed with JavaScript enabled"
366,how-can-i-handle-images-at-headless-mode,"Hello,I am trying to handle RGBD images.
Before using images via features, I tried to save images on my disk.However, I cannot save images in headless mode though I can save images in non-headless mode.In my function def capture_and_save_images(self):, there is a creating image function from simulation instance to image that I used by cam_rgba = self._rgb_cameras[idx].rgb_cam.get_rgba().
However cam_rgba is empty.
Is there another way to get images?Here is my code.
reaching_ur5e.7z (15.7 MB)ThanksActually, I asked before but, there is no more reply. I think the cause of the problem is not come from SKRL so I rewrite the question and change the title.Dear, @kellygCan’t we still get images at headless mode?
I read a comment at March 13.Hi there, rendering in headless mode should work if the enable_cameras flag in the task config file is set to True. Without this flag, OmniIsaacGymEnvs will by default disable all rendering extensions for performance purposes.Thanks, @kellygI tired ""enable_cameras"": True but, the same error messages still exist.I changed the config in env file such asThe error messages is same as above.Could you please try with the Isaac Sim 2022.2.1 release? The fix should be in that release.Thanks, @kellyg !
It works.However, when I increase the number of environments. The error appeared.Is it due to the limitation of GPU computing power?Hi @psh9002 ,Yes, this error means that the machine has ran out of memory. We are continuing to work on memory and speed performance issues related to rendering. We are aware that currently, only a few cameras can be defined in the scene.This topic was automatically closed 14 days after the last reply. New replies are no longer allowed.Powered by Discourse, best viewed with JavaScript enabled"
367,point-cloud-transformation-error-from-depth-map,"Dear community,I am using an NN to detect objects in a scene and I transform the pixels inside the bounding box into a point cloud. Unfortunately, the resulting points have a deviation of -2cm with respect to the ground. Is there any way to fix this?
The function I use is,from the camera sensor.The resulting pointcloud looks as follows:objpcloud33210×1630 173 KBI am using an NN to detect objects in a scene and I transform the pixels inside the bounding box into a point cloud. Unfortunately, the resulting points have a deviation of -2cm with respect to the ground. Is there any way to fix this?
The function I use is,Hi @dgut  - The deviation you’re seeing could be due to a variety of factors, including the accuracy of your neural network, the precision of your depth sensor, or the transformation you’re using to convert image coordinates to world coordinates. Here are a few suggestions:Powered by Discourse, best viewed with JavaScript enabled"
368,ros2-camera-infos-distortion-model-is-invalid,"Hi,The published camera_info using the ROS2 Camera Helper node, has pinhole under its distortion_model:
This is an unsupported distortion model, which causes exceptions such as: Cannot call rectifyPoint when distortion is unknown.Under CameraInfos documentation it references the supported distortion models, which are:

image733×186 20.8 KB
Hi - Sorry for the delay in the response. Let us know if you still having this issue with the latest Isaac Sim 2022.2.1 release.@rthaker still having this issue in 2022.2.1:Camera info’s topic echo:Hi I am facing the same issue :( @omers were you able to solve it?Hi @omers and  @jasonjabbour .  I understand that this error could be a result of a call to the rectifyPoint, that allows to get rectified pixel coordinates from unrectified ones.The issue is, image is already undistorted one, so this call shouldn’t happen in the first place (rectifyPoint should return the same point in that case).  If possible, please try to comment out that call for the case, when the distortion coefficient parameter array have zero length (pinhole camera case), in the ROS node.On the ROS interface, yes, it is likely that logic of the Isaac Sim to supply the ‘pinhole’ in the distortion type for undistorted images is wrong. And the parameter should be omitted altogether, instead of specifying ‘pinhole’.The workaround for this is to check, if the distortion coefficient array length is zero (many ROS nodes use that check to determine that the image comes from the pinhole camera). Or to check explicitly that ‘pinhole’ is specified.We have a bit more flexible support for distortions upcoming, that allows to specify distortion model communicated to ROS separately from the underlying RTX rendering model.Powered by Discourse, best viewed with JavaScript enabled"
369,orthographic-camera-still-bugged,"So according to the post https://forums.developer.nvidia.com/t/orthographic-cameras-flickering/232447the orthographic camera “flickering” was planned to be fixed in Isaac Sim version 2022.2.1.But for me it seems like its not fixed yet.
Zooming in/out the scene and changing the camera height leads to flickering.Is it really not fixed yet?E.g. a simple scene with a cube on a plane and a orthographic camera:
Hi there,sorry about that, the bug fix got postponed and it is now planned for Kit 105.1 (late summer). It is not clear yet if this Which will ship with the next Isaac Sim release.Best,
AndreiThis topic was automatically closed 14 days after the last reply. New replies are no longer allowed.Powered by Discourse, best viewed with JavaScript enabled"
370,closed-chain-robot-in-isaac-sim,"Hi, I am very excited to see Isaac Gym is integrated. Thanks for your effort!To my knowledge, currently Isaac Gym does not support creating closed chain mechanisms (like Delta Robot below) since the body relationship does not form a tree structure.
I am not familar with Isaac Sim, but may I know whether Isaac Sim supports simulating this kind of robot?Thanks in advance!Hi Alan,You should be able to simulate closed chain mechanism,  have you tried to import the model into Isaac Sim?The articulation API doesn’t support close loop chains, and articulation API is what you are probably using to solve for actuation. To make it work for, you’ll have to “break” the chain. Or in another word, leave some joints just as plain simple joints, and not articulation joints, so they are not in the articulation tree.To delete articulation api from a joint, select the joint on the stage tree, and scroll down in its property panel, and simply x out the menu item that has to do with articulations.Thanks a lot! I will try thatThanks very much for your information!Have you succeedded?  Could you give some solutions?Hi, I can confirm that close-chain mechanism can be simulated in Isaac Sim, but you have to modify the USD file such as excluding the un-actuated joints from articulation. I recently test the sim-to-real transfer, and the result is acceptable, so there should be no problem simulating close-chain (but you should increase the position/velocity solver iteration or decrease physics_dt to ensure a good sim-to-real performance).You can also check this post: Support Parallel Robots?Powered by Discourse, best viewed with JavaScript enabled"
371,isaac-sim-crashes-when-try-load,"When I try to load my extension, load usd files, or load the examples that come predefined as “Hello word”, the application just crashes, can you help me to solve this problem please.
Next I will leave one of the many logs that the application generates for me.
kit_20230611_231030.log (1.9 MB)Hi @jhonsmith981 - Can you let me know in what step of the tutorial are you seeing the issue?
https://docs.omniverse.nvidia.com/app_isaacsim/app_isaacsim/tutorial_core_hello_world.htmlAlso, I see one of the error is “Error: Not Found. URI=‘http://localhost:8891/.cloudfront.toml’: File not found!”.Can you make sure that the .cloudfront.toml file actually exists in the location where your system is trying to access it.Powered by Discourse, best viewed with JavaScript enabled"
372,omniverse-create-installation-failed,"
image754×222 11.1 KB

I download the create installation always failed, do not know why?(Use NVIDIA Omniverse Launcher --trace-warnings ... to show where the warning was created)
[2022-07-17 18:26:05.325] [info]  Omniverse Launcher 1.5.7 (production)
[2022-07-17 18:26:05.329] [info]  Argv: C:\Users\Orinter\AppData\Local\Programs\omniverse-launcher\NVIDIA Omniverse Launcher.exe
[2022-07-17 18:26:05.329] [info]  Crash dumps directory: C:\Users\Orinter\AppData\Roaming\omniverse-launcher\Crashpad
[2022-07-17 18:26:05.374] [debug] Running “E:\Omniverse\nucleus-workstation-2022.1.2\System Monitor\omni-system-monitor.exe”
[2022-07-17 18:26:05.397] [debug] Reset current installer.
[2022-07-17 18:26:05.682] [info]  Running production web server.
[2022-07-17 18:26:05.689] [info]  HTTP endpoints listening at http:///localhost:33480
[2022-07-17 18:26:05.876] [error] Got an error during the token refresh: Error: Unexpected error happened during the authentication. Please try to log out from the launcher and authenticate again. ([invalid_token] – Token not found)
[2022-07-17 18:26:34.090] [info]  Login URL opened.
[2022-07-17 18:26:34.149] [info]  Waiting for login result…
[2022-07-17 18:29:06.206] [info]  WEB /login-result
[2022-07-17 18:29:06.503] [info]  Received login result.
[2022-07-17 18:29:06.616] [info]  WEB /favicon.ico
[2022-07-17 18:29:08.478] [info]  Logged in.
[2022-07-17 18:30:00.503] [debug] Waiting for authentication…
[2022-07-17 18:30:00.504] [debug] Initializing create@2022.1.4 installer…
[2022-07-17 18:30:00.504] [info]  Fetching create information…
[2022-07-17 18:30:01.401] [debug] Enqueue [f4d3830d-65d9-4289-972a-dac8df37856f] create.
[2022-07-17 18:30:01.401] [debug] Set current installer [f4d3830d-65d9-4289-972a-dac8df37856f] create.
[2022-07-17 18:30:01.410] [debug] Enqueued [f4d3830d-65d9-4289-972a-dac8df37856f] installer.
[2022-07-17 18:30:01.411] [debug] [f4d3830d-65d9-4289-972a-dac8df37856f] Start downloading Create for create@2022.1.4…
[2022-07-17 18:30:01.411] [debug] [f4d3830d-65d9-4289-972a-dac8df37856f] Preparing downloader for E:\Omniverse\deps\655be6ef6c4912cca7d166003fb5b900.zip
[2022-07-17 18:30:02.589] [info]  Signed url: https:///asset.launcher.omniverse.nvidia.com/create/2022.1.4/windows-x86_64/655be6ef6c4912cca7d166003fb5b900.zip?Expires=1658097002&Policy=eyJTdGF0ZW1lbnQiOlt7IlJlc291cmNlIjoiaHR0cHM6Ly9hc3NldC5sYXVuY2hlci5vbW5pdmVyc2UubnZpZGlhLmNvbS9jcmVhdGUvMjAyMi4xLjQvd2luZG93cy14ODZfNjQvNjU1YmU2ZWY2YzQ5MTJjY2E3ZDE2NjAwM2ZiNWI5MDAuemlwIiwiQ29uZGl0aW9uIjp7IkRhdGVMZXNzVGhhbiI6eyJBV1M6RXBvY2hUaW1lIjoxNjU4MDk3MDAyfX19XX0_&Signature=LkLNCyBO3jao~1lIxgMPYSyyPM8gPowf4EptvlBXCEyQe3WQR7RG0BBGi5ygDAKBWRbw83vQWV7N7f-p2Fj-I-otorv~WxwCyv1v9NRiOW6T1W3DeaYrna8VO0j5YybRZmAw-7aIXNFwklaMZKgEdvBT3-Q965zeGLdFnPlQMnYS37n~ZSEqBUpC7RxO~F4FBQ8jhAWmtF8I70cUzb4vK2Klmrj11trwFxvkh3r95g4iO9epSIqHdgtIlJmhB1LlXuyaUqlYV5S04otB42R74~J-nV0sjJhSgIuIQWjc~P188ab8f9KfvBavsRn8XInM-Px~MVmBaYIaP5KS6G3s-Q__&Key-Pair-Id=K13PD0MHC2KFRP
[2022-07-17 18:30:02.590] [info]  [f4d3830d-65d9-4289-972a-dac8df37856f] Calculating package size for https:///asset.launcher.omniverse.nvidia.com/create/2022.1.4/windows-x86_64/655be6ef6c4912cca7d166003fb5b900.zip?Expires=1658097002&Policy=eyJTdGF0ZW1lbnQiOlt7IlJlc291cmNlIjoiaHR0cHM6Ly9hc3NldC5sYXVuY2hlci5vbW5pdmVyc2UubnZpZGlhLmNvbS9jcmVhdGUvMjAyMi4xLjQvd2luZG93cy14ODZfNjQvNjU1YmU2ZWY2YzQ5MTJjY2E3ZDE2NjAwM2ZiNWI5MDAuemlwIiwiQ29uZGl0aW9uIjp7IkRhdGVMZXNzVGhhbiI6eyJBV1M6RXBvY2hUaW1lIjoxNjU4MDk3MDAyfX19XX0_&Signature=LkLNCyBO3jao~1lIxgMPYSyyPM8gPowf4EptvlBXCEyQe3WQR7RG0BBGi5ygDAKBWRbw83vQWV7N7f-p2Fj-I-otorv~WxwCyv1v9NRiOW6T1W3DeaYrna8VO0j5YybRZmAw-7aIXNFwklaMZKgEdvBT3-Q965zeGLdFnPlQMnYS37n~ZSEqBUpC7RxO~F4FBQ8jhAWmtF8I70cUzb4vK2Klmrj11trwFxvkh3r95g4iO9epSIqHdgtIlJmhB1LlXuyaUqlYV5S04otB42R74~J-nV0sjJhSgIuIQWjc~P188ab8f9KfvBavsRn8XInM-Px~MVmBaYIaP5KS6G3s-Q__&Key-Pair-Id=K13PD0MHC2KFRP…
[2022-07-17 18:30:04.183] [debug] [f4d3830d-65d9-4289-972a-dac8df37856f] Downloading https:///asset.launcher.omniverse.nvidia.com/create/2022.1.4/windows-x86_64/655be6ef6c4912cca7d166003fb5b900.zip?Expires=1658097002&Policy=eyJTdGF0ZW1lbnQiOlt7IlJlc291cmNlIjoiaHR0cHM6Ly9hc3NldC5sYXVuY2hlci5vbW5pdmVyc2UubnZpZGlhLmNvbS9jcmVhdGUvMjAyMi4xLjQvd2luZG93cy14ODZfNjQvNjU1YmU2ZWY2YzQ5MTJjY2E3ZDE2NjAwM2ZiNWI5MDAuemlwIiwiQ29uZGl0aW9uIjp7IkRhdGVMZXNzVGhhbiI6eyJBV1M6RXBvY2hUaW1lIjoxNjU4MDk3MDAyfX19XX0_&Signature=LkLNCyBO3jao~1lIxgMPYSyyPM8gPowf4EptvlBXCEyQe3WQR7RG0BBGi5ygDAKBWRbw83vQWV7N7f-p2Fj-I-otorv~WxwCyv1v9NRiOW6T1W3DeaYrna8VO0j5YybRZmAw-7aIXNFwklaMZKgEdvBT3-Q965zeGLdFnPlQMnYS37n~ZSEqBUpC7RxO~F4FBQ8jhAWmtF8I70cUzb4vK2Klmrj11trwFxvkh3r95g4iO9epSIqHdgtIlJmhB1LlXuyaUqlYV5S04otB42R74~J-nV0sjJhSgIuIQWjc~P188ab8f9KfvBavsRn8XInM-Px~MVmBaYIaP5KS6G3s-Q__&Key-Pair-Id=K13PD0MHC2KFRP…
[2022-07-17 18:30:04.664] [debug] [f4d3830d-65d9-4289-972a-dac8df37856f] Save https:///asset.launcher.omniverse.nvidia.com/create/2022.1.4/windows-x86_64/655be6ef6c4912cca7d166003fb5b900.zip?Expires=1658097002&Policy=eyJTdGF0ZW1lbnQiOlt7IlJlc291cmNlIjoiaHR0cHM6Ly9hc3NldC5sYXVuY2hlci5vbW5pdmVyc2UubnZpZGlhLmNvbS9jcmVhdGUvMjAyMi4xLjQvd2luZG93cy14ODZfNjQvNjU1YmU2ZWY2YzQ5MTJjY2E3ZDE2NjAwM2ZiNWI5MDAuemlwIiwiQ29uZGl0aW9uIjp7IkRhdGVMZXNzVGhhbiI6eyJBV1M6RXBvY2hUaW1lIjoxNjU4MDk3MDAyfX19XX0_&Signature=LkLNCyBO3jao~1lIxgMPYSyyPM8gPowf4EptvlBXCEyQe3WQR7RG0BBGi5ygDAKBWRbw83vQWV7N7f-p2Fj-I-otorv~WxwCyv1v9NRiOW6T1W3DeaYrna8VO0j5YybRZmAw-7aIXNFwklaMZKgEdvBT3-Q965zeGLdFnPlQMnYS37n~ZSEqBUpC7RxO~F4FBQ8jhAWmtF8I70cUzb4vK2Klmrj11trwFxvkh3r95g4iO9epSIqHdgtIlJmhB1LlXuyaUqlYV5S04otB42R74~J-nV0sjJhSgIuIQWjc~P188ab8f9KfvBavsRn8XInM-Px~MVmBaYIaP5KS6G3s-Q__&Key-Pair-Id=K13PD0MHC2KFRP to E:\Omniverse\deps\655be6ef6c4912cca7d166003fb5b900.zip
[2022-07-17 18:31:11.409] [debug] [f4d3830d-65d9-4289-972a-dac8df37856f] Extracting Create for create@2022.1.4 to E:\Omniverse\deps\655be6ef6c4912cca7d166003fb5b900.zip…
[2022-07-17 18:31:11.429] [error] (node:16104) [DEP0005] DeprecationWarning: Buffer() is deprecated due to security and usability issues. Please use the Buffer.alloc(), Buffer.allocUnsafe(), or Buffer.from() methods instead.
(Use NVIDIA Omniverse Launcher --trace-deprecation ... to show where the warning was created)
[2022-07-17 18:33:16.294] [debug] [f4d3830d-65d9-4289-972a-dac8df37856f] Start downloading kit-sdk-launcher for create@2022.1.4…
[2022-07-17 18:33:16.295] [debug] [f4d3830d-65d9-4289-972a-dac8df37856f] Preparing downloader for E:\Omniverse\deps\dfee4680079044fbc534c9c84aa86d19.zip
[2022-07-17 18:33:17.347] [info]  Signed url: https:///asset.launcher.omniverse.nvidia.com/create/2022.1.4/windows-x86_64/kit/dfee4680079044fbc534c9c84aa86d19.zip?Expires=1658097197&Policy=eyJTdGF0ZW1lbnQiOlt7IlJlc291cmNlIjoiaHR0cHM6Ly9hc3NldC5sYXVuY2hlci5vbW5pdmVyc2UubnZpZGlhLmNvbS9jcmVhdGUvMjAyMi4xLjQvd2luZG93cy14ODZfNjQva2l0L2RmZWU0NjgwMDc5MDQ0ZmJjNTM0YzljODRhYTg2ZDE5LnppcCIsIkNvbmRpdGlvbiI6eyJEYXRlTGVzc1RoYW4iOnsiQVdTOkVwb2NoVGltZSI6MTY1ODA5NzE5N319fV19&Signature=Ta~FFuW8MXS4glqf6lz33UmJEgFA0Gc12Mt8ILAds9HddnLvvJMa3fXI2Wd6e2YO9O~3bxPrCVpRyZlPo11-yksOEpj91v2bO7fqH5iO~df8L2g9u-IHi08OY-47B03N5m3n8UW8SXu69k8mJwthAuzWWZSsZnlaL5a0mzrHcbfhjMAuTJ9VN0KKgYSxbA~5HrjLP2knhbGrmEFATVfNeOmOqvrxBeWb-xspGwmetptLoSB2m8Ee0eKODqPcFmo-m-jUh-vr80RiLOeQ-CrQIzKgPdKK6j3gtatVbRbmv84-GIbl-aA-kpmXV9q4NrP89J4GxP21qBOGatZvwnew__&Key-Pair-Id=K13PD0MHC2KFRP
[2022-07-17 18:33:17.348] [info]  [f4d3830d-65d9-4289-972a-dac8df37856f] Calculating package size for https://asset.launcher.omniverse.nvidia.com/create/2022.1.4/windows-x86_64/kit/dfee4680079044fbc534c9c84aa86d19.zip?Expires=1658097197&Policy=eyJTdGF0ZW1lbnQiOlt7IlJlc291cmNlIjoiaHR0cHM6Ly9hc3NldC5sYXVuY2hlci5vbW5pdmVyc2UubnZpZGlhLmNvbS9jcmVhdGUvMjAyMi4xLjQvd2luZG93cy14ODZfNjQva2l0L2RmZWU0NjgwMDc5MDQ0ZmJjNTM0YzljODRhYTg2ZDE5LnppcCIsIkNvbmRpdGlvbiI6eyJEYXRlTGVzc1RoYW4iOnsiQVdTOkVwb2NoVGltZSI6MTY1ODA5NzE5N319fV19&Signature=Ta~FFuW8MXS4glqf6lz33UmJEgFA0Gc12Mt8ILAds9HddnLvvJMa3fXI2Wd6e2YO9O~3bxPrCVpRyZlPo11-y~~ksOEpj91v2bO7fqH5iO~df8L2g9u-IHi08OY-47B03N5m3n8UW8SXu69k8mJwthAuzWWZSsZnlaL5a0mzrHcbfhjMAuTJ9VN0KKgYSxbA~5HrjLP2knhbGrmEFATVfNeOmOqvrxBeWb-xspGwmetptLoSB2m8Ee0eKODqPcFmo-m-jUh-vr80RiLOeQ-CrQIzKgPdKK6j3gtatVbRbmv84-GIbl-aA-kpmXV9q4NrP89J4GxP21qBOGatZvwnew__&Key-Pair-Id=K13PD0MHC2KFRP…
[2022-07-17 18:33:18.280] [debug] [f4d3830d-65d9-4289-972a-dac8df37856f] Downloading https:///asset.launcher.omniverse.nvidia.com/create/2022.1.4/windows-x86_64/kit/dfee4680079044fbc534c9c84aa86d19.zip?Expires=1658097197&Policy=eyJTdGF0ZW1lbnQiOlt7IlJlc291cmNlIjoiaHR0cHM6Ly9hc3NldC5sYXVuY2hlci5vbW5pdmVyc2UubnZpZGlhLmNvbS9jcmVhdGUvMjAyMi4xLjQvd2luZG93cy14ODZfNjQva2l0L2RmZWU0NjgwMDc5MDQ0ZmJjNTM0YzljODRhYTg2ZDE5LnppcCIsIkNvbmRpdGlvbiI6eyJEYXRlTGVzc1RoYW4iOnsiQVdTOkVwb2NoVGltZSI6MTY1ODA5NzE5N319fV19&Signature=Ta~FFuW8MXS4glqf6lz33UmJEgFA0Gc12Mt8ILAds9HddnLvvJMa3fXI2Wd6e2YO9O~3bxPrCVpRyZlPo11-yksOEpj91v2bO7fqH5iO~df8L2g9u-IHi08OY-47B03N5m3n8UW8SXu69k8mJwthAuzWWZSsZnlaL5a0mzrHcbfhjMAuTJ9VN0KKgYSxbA~5HrjLP2knhbGrmEFATVfNeOmOqvrxBeWb-xspGwmetptLoSB2m8Ee0eKODqPcFmo-m-jUh-vr80RiLOeQ-CrQIzKgPdKK6j3gtatVbRbmv84-GIbl-aA-kpmXV9q4NrP89J4GxP21qBOGatZvwnew__&Key-Pair-Id=K13PD0MHC2KFRP…
[2022-07-17 18:33:18.905] [debug] [f4d3830d-65d9-4289-972a-dac8df37856f] Save https:///asset.launcher.omniverse.nvidia.com/create/2022.1.4/windows-x86_64/kit/dfee4680079044fbc534c9c84aa86d19.zip?Expires=1658097197&Policy=eyJTdGF0ZW1lbnQiOlt7IlJlc291cmNlIjoiaHR0cHM6Ly9hc3NldC5sYXVuY2hlci5vbW5pdmVyc2UubnZpZGlhLmNvbS9jcmVhdGUvMjAyMi4xLjQvd2luZG93cy14ODZfNjQva2l0L2RmZWU0NjgwMDc5MDQ0ZmJjNTM0YzljODRhYTg2ZDE5LnppcCIsIkNvbmRpdGlvbiI6eyJEYXRlTGVzc1RoYW4iOnsiQVdTOkVwb2NoVGltZSI6MTY1ODA5NzE5N319fV19&Signature=Ta~FFuW8MXS4glqf6lz33UmJEgFA0Gc12Mt8ILAds9HddnLvvJMa3fXI2Wd6e2YO9O~3bxPrCVpRyZlPo11-y~~ksOEpj91v2bO7fqH5iO~df8L2g9u-IHi08OY-47B03N5m3n8UW8SXu69k8mJwthAuzWWZSsZnlaL5a0mzrHcbfhjMAuTJ9VN0KKgYSxbA~5HrjLP2knhbGrmEFATVfNeOmOqvrxBeWb-xspGwmetptLoSB2m8Ee0eKODqPcFmo-m-jUh-vr80RiLOeQ-CrQIzKgPdKK6j3gtatVbRbmv84-GIbl-aA-kpmXV9q4NrP89J4GxP21qBOGatZvwnew__&Key-Pair-Id=K13PD0MHC2KFRP to E:\Omniverse\deps\dfee4680079044fbc534c9c84aa86d19.zip
[2022-07-17 18:39:49.402] [debug] [f4d3830d-65d9-4289-972a-dac8df37856f] Extracting kit-sdk-launcher for create@2022.1.4 to E:\Omniverse\deps\dfee4680079044fbc534c9c84aa86d19.zip…
[2022-07-17 18:42:37.752] [info]  [f4d3830d-65d9-4289-972a-dac8df37856f] Linking Create to E:\Omniverse\create-2022.1.4…
[2022-07-17 18:42:37.762] [debug] Dequeue [f4d3830d-65d9-4289-972a-dac8df37856f] create.
[2022-07-17 18:42:37.763] [debug] Reset current installer.
[2022-07-17 18:42:37.763] [debug] [f4d3830d-65d9-4289-972a-dac8df37856f] Removing the installation folder: E:\Omniverse\create-2022.1.4
[2022-07-17 18:42:37.763] [debug] [f4d3830d-65d9-4289-972a-dac8df37856f] Could not remove the installation folder E:\Omniverse\create-2022.1.4 Error: ENOENT: no such file or directory, stat ‘E:\Omniverse\create-2022.1.4’
[2022-07-17 18:42:37.764] [debug] [f4d3830d-65d9-4289-972a-dac8df37856f] Remove unused packages from the library.
[2022-07-17 18:43:22.887] [error] Error: EISDIR: illegal operation on a directory, symlink ‘E:\Omniverse\deps\655be6ef6c4912cca7d166003fb5b900’ → ‘E:\Omniverse\create-2022.1.4’Without the full logs, it looks like a directory issue.  Can you install Omniverse Create on your C drive instead of your E drive and let me know if this error is still happening?  If it is, please attach a copy of your logs found here: C:\Users\<USERNAME>\.nvidia-omniverse\logs\launcherAlso, What is your Operating System?  and What GPU / GPU driver are you using?I’m getting a similar issue trying to install to a separate hard drive (D:) with USD Composer. Not enough room for all of the Omniverse on my laptop!This is for an older version “create-2022.1.4”. I would recommend deleting this app altogether and then doing a fresh install of the latest Composer 2023.1.0Powered by Discourse, best viewed with JavaScript enabled"
373,omniverse-launcher-can-not-open,"Hi, it starts maybe yesterday.
i wake up my computer and it start omiverse launcher at boot.
and i’ve got

I did clean up tool but there is no effect.
I tried uninstall and reinstall omniverse launcher but it gives me the same error above one.uploads logs
launcher.log (8.0 KB)I found the Problem & Solution.It was really the Documents path is not exist due to “OneDrive”.The Document folder’s location was set by
“C:<user_name>\OneDrive\Documents” not “C:<user_name>\Documents”.Also i disable Onedrive after i install Window11 and i changed Desktop location from OneDrive but not all thing.First I tried change it by property but it refused.So i change it using Regedit.exeGo to “HKEY_CURRENT_USER\Software\Microsoft\Windows\CurrentVersion\Explorer\User SHeel Folders”
and find & remove \OnDrive\ and front absolute path and re-matching it like “%USERPROFILE%+\Docunemts”.After change i could change the location of Documents and Pictures.Success to launch re-installed omniverse-launcher.This topic was automatically closed 14 days after the last reply. New replies are no longer allowed.Powered by Discourse, best viewed with JavaScript enabled"
374,omniverse-audio2face-doesnt-load-the-face-mesh-please-help,"Omniverse Audio2Face doesn’t load the face mesh Please HELP
[Warning] [omni.gpu_foundation_factory.plugin] RT-capable GPU not found, switching to compatibility mode
Couldn’t acquire tokens interface and initialize default tokens.
[Error] [omni.kit.window.viewport.plugin] Failed to initialize resource manager
[omni.kit.window.viewport.plugin] Failed to prepare GPU Foundation
[Warning] [omni.usd] Failed to create Hydra Engine for viewport!
[omni.usd] No hydra engine was found with name  running at tickrate -1
[Error] [gpu.foundation.plugin] No device could be created. Some known system issues:Hello @user81339!  I see that you have multiple posts.  I am going to close your other post here (Omniverse Audio2Face doesn't load the face mesh Please HELP)Mentioned in Previous post:i get these lines in RED letters
PLEASE HELP ME
the face MESH doesn’t load
[Error] [gpu.foundation.plugin] No device could be created. Some known system issues:Hello @user81339.
I let the Audio2Face team know about your frustrations.  We should hear a reply shortly.In the meantime, take a look at our Audio2Face documentation which may help answer your questions: Audio2Face Overview — Omniverse Audio2Face documentationHi @user81339
Sorry for your trouble in starting Audio2Face.
From the error  message it looks like your graphic card is not supported. RTX card is required.
Can you share your system information and your current driver version?
Thank you.pr intel core i7 5820k
os win 10 1903 64x
gpu game ready driver version 497.09 geforce gtx 970Hi @user81339 , the minimum requirement to run Omniverse Audio2Face is an RTX gpu.  The issue you have is due to incompatible gpu.Hope this helpsHello, the documentation link says 404-error. Do you have a working link?This topic was automatically closed 14 days after the last reply. New replies are no longer allowed.Powered by Discourse, best viewed with JavaScript enabled"
375,audio2face-stand-alone-application-deployment,"Dear all,Greetings, I am creating a ChatGPT application with python recently and I am interested exploring the A2F from Omniverse. I am completely new on this. I would like to create an talking chatbot with my streaming wav. file. However, I cannot find any document or tutorial about the deployment of A2F to a stand alone application. Although I have heard about ACE, I would be appreciated if anyone can show me guidance on exporting the A2F file to an application? Does it means we need to export the files to editor engines like Unreal or Unity? Thanks for your time.Hi there,Can you elaborate on this a little? Is this for a phone app, a desktop app or a web app for example?You should be able to use Rest API and WebRTC kit extension to stream videos in any web browser.
WebRTC Browser Client — Omniverse Extensions documentation (nvidia.com)Creating a custom app can be achieved using ACE Omniverse Avatar Cloud Engine (ACE) | NVIDIA Developer.While it’s still not available to public, you may register for an early access here: NVIDIA Omniverse ACE Early Access Program | NVIDIA DeveloperYou will probably need to convert text to audio which can be achieved using Riva: Speech AI SDK - Riva | NVIDIASure! I am working on a caring robot for elderly, to help them pick stuffs from point to point. Which I use ROS and python code for the robot and chatGPT api. Therefore, I want a stand alone interface for demonstrating the face of chatGPT (I use Flet library to create a python UI). Is it the way for me to use Rest API and WebRTC kit extension in order to stream A2F interface on my python UI? If so, I may need to consider adding a web client on my UI.To be clear, I have done a ROS2 robot which can be controlled by ChatGPT response. So I want to give a A2F interface to the voice create by GPT’s text2speech. Thank you so much for reading.For now, I am generating a wav. file from python scripts to create a voice for GPT’s response. I then stream the wav. file into A2F using test.client.py functions. Which is far from putting it into the robot.
Screenshot from 2023-05-31 11-08-391920×1001 80.8 KB
what I want to do is like Violet.Powered by Discourse, best viewed with JavaScript enabled"
376,i-created-a-server-i-think-but-i-dont-know-my-server-name,"I went through the setup process, I believe it created it, but how I would check that and find the name?I tried again thinking I did something wrong, and tells me workstation2023 is already installed (or something like that).Thanks,Powered by Discourse, best viewed with JavaScript enabled"
377,omniverse-code-extension-spawn-prim-with-non-standard-height-width-and-depth,"Hi interwebs,I’m writing my first Omni Code extension and am struggling with the function arguments. I have an extension working with a button that spawns a cube prim. I’d like to modify it to spawn a cube that doesn’t have a scale: [1, 1, 1]. This is what I’m currently using.Hi @austencabret. I know we connected earlier on Discord, have you had any luck since?Hi again Mati!I’m able to scale the cubes! Thank you! I’m actually now trying to make a function that deletes all prims in a scene but it seems like wildcards (the * symbol) don’t work. When i do something, like select the prims by path ./World/Cube_* to choose all. I can delete a single prim. Do you know of a way to delete multiple?Powered by Discourse, best viewed with JavaScript enabled"
378,moveit2-cannot-build-ros2-humble-ubuntu-22-04,"HelloI have followed the instructions on Moveit Tutorial webpage (How To Command Simulated Isaac Robot — MoveIt Documentation: Humble documentation)
but it gives the following error.
image1048×346 63.5 KBimage1298×346 65.3 KBI think its the “colcon build” command thats causing the error so I tried to “colcon build” in the following directory and it throws this errorimage1298×228 34.8 KBAny suggestion on how to fix this error so i can continue with the Moveit2 tutorial on Isaac Sim?Hi @jaeyeun - Can you please confirm you have followed all the steps as mentioned in this document?Also, I would suggest to update to the latest Isaac Sim 2022.2.1 release.https://docs.omniverse.nvidia.com/app_isaacsim/app_isaacsim/tutorial_ros2_moveit.htmlyes. I have followed every step meticulously.the following error occurs during the 4th step of “computer setup” stage in the following link provided in the tutorial.
(moveit2_tutorials/isaac_panda_tutorial.rst at humble · ros-planning/moveit2_tutorials · GitHub)whenever I “docker compose build” I get the following error.Any advice on how to fix this?@jaeyeun sorry you have been running into issues! We have been preparing for World MoveIt Day by upgrading this Tutorial/Docker image. It now includes MoveIt’s new Python bindings and a valid installation of Pytorch for those that want to integrate ML with motion planning.Could you please try to build the image again but follow this tutorial? The difference being that you need to be on the main branch of the tutorials before building the Docker image. I will make an update to the humble branch to point to the new tutorial.thanks for the prompt response!
I will try the new instructions and follow up on this thread if I encounter new problems.
Thanks!Ive tried the revised command that you shared in your reply and was able to successfully build the source code.however when I execute the command to run the “./python.sh isaac_moveit.py” command (high-lighted in orange in the image below)

image1139×519 69.7 KB
I get the following error message.

image1842×940 213 KB
to be more specific the the error message that appears in the terminal in red text in the image file above is:it seems like when Isaac Sim is launched via the “./python.sh isaac_moveit.py” command, it tries starts the “ROS2 Bridge” extension instead of the “ROS2 Humble Bridge” extension.The updated instructions for Moveit2 still doesnt work for ROS2 Humble on Isaac Sim.That’s weird, have you modified any the the files in the isaac_sim-2022.2.1/apps directory? I just tested and my setup works with the ROS 2 and ROS 2 Humble bridge.Can you try and modify this line to extensions.enable_extension(""omni.isaac.ros2_bridge-humble"") to see if the humble bridge works for you?hello!
thanks for the prompt reply.However, when I try to load the environment via the provided python script, I still get an Error even after modifying the “isaac_moveit.py” per your suggestion.I haven’t modified any files in the “isaac_sim-2022.2.1/apps” directory.
I have modified the following line in “isaac_moveit.py” as you suggestedand now I get a different error message.(the video below is what happens when I run the command “./python.sh isaac_moveit.py”Error message:I think there are some errors in the python scriptThanks!I am able to load the environment by going to the menu “Isaac Examples → ROS → MoveIt” and then also launch RViz with the “docker compose up demo_isaac” command.If you are running 2022.2.1 then starting the world from the Isaac Examples menu works (I just tried it and it looks like Nvidia updated the example to have the correct topic names). You just need to make sure that your ROS_DOMAIN_ID is set to 0 (export ROS_DOMAIN_ID=0) on the host before starting the Docker container because the Isaac Examples it is not setup to use environment variables.

image428×678 25.3 KB
2023-06-07 09:29:55 [5,346ms] [Warning] [carb] [Plugin: libomni.isaac.ros2_humble_bridge.plugin.so] Could not locate the function: carbGetFrameworkVersionThis is the reason I think the script fails (looks like you are missing libraries). It sounds like there could be something wrong with your installation of Isaac. Can you run any other examples included with Isaac?FYI this example did not work with the MoveIt2 Tutorial on my machine, it has the wrong topic names.Are you running Isaac on 22.04 or 20.04?This is the reason I think the script fails (looks like you are missing libraries). It sounds like there could be something wrong with your installation of Isaac. Can you run any other examples included with Isaac?I don’t think there is a problem with my installation.
I have uninstalled and re-installed Isaac Sim and ROS2 multiple times thinking that it could have been the installation that I messed up but I don’t think thats the case here. (since I am able to run other examples without any problem)
And yes I am able to run the standalone example for “moveit.py”.FYI this example did not work with the MoveIt2 Tutorial on my machine, it has the wrong topic names.
Are you running Isaac on 22.04 or 20.04?I am running Isaac Sim with ROS2 Humble on Ubuntu 22.04.Thanks!I am running Isaac Sim with ROS2 Humble on Ubuntu 22.04.The tutorial is currently only supporting Isaac installed on 20.04 (sounds like Nvida may have changed some stuff between Ubuntu distros). If I have time to get a setup on the same environment I will work on debugging it.Sorry it took me so long to get back to this! Here is a PR that should fix the issue.Powered by Discourse, best viewed with JavaScript enabled"
379,omniverse-audio2face,"Hi all
We are planning to use NVIDIA Omniverse Audio2face to one of our product.
We are skeptical about the license issue over it whether its open source or do we have to purchase any license for using it
Can you guys help me out on the clarity over it.
Thank youPowered by Discourse, best viewed with JavaScript enabled"
380,root-motion-for-characters-in-sequencer,"How to make a walking loop to walk forwads? - #14 by RonanDB describes a problem with an animation clip being played in a loop. (Not sure whether this belongs in Sequencer or Mechanim channel, but the other post was here.)This problem is called Root Motion in Unity and Unreal Engine. At the end of a walk loop, the character should not return to the start position if you loop an animation clip in the Sequencer… You want the ability to chain a few clips with root motion together - e.g. walk a few cycles, then have a walk-to-run clip once, then run a few cycles.Is there any support for root motion in Omniverse? I could not find it. If I had a list of related requests for the Sequencer (such as blending between clips) where should I ask them?Hey Alan. Sorry. This one slipped through the cracks. I moved this one to Machinima since that’s the main app that uses the Sequencer extension.Hi @alan.james.kent. I heard back from the dev team and you can achieve root motion with Animation Graph, but it is not supported in Sequencer.Thanks! I was afraid that was the case. Thanks for confirming!This topic was automatically closed 14 days after the last reply. New replies are no longer allowed.Powered by Discourse, best viewed with JavaScript enabled"
381,gpu-load-from-new-kit-process-even-without-stage-and-active-eco-mode,"Hello,we’ve written our own kit agent to create and monitor kit processes on a server. Those kit processes are for rendering and optional for streaming purpose.This kit agent should pool those kit processes to minimize the startup duration when rendering an image… that works very nice. But when we create a new kit process (very small, basically just a viewport) this process has 12% gpu load even without a loaded scene and without stream AND active eco mode. Is that correct?The system is running under ubuntu with latests drivers.It there a way to completly pause a kit process render loop (or whatever is producing this idle load)?Hi @c.bickmeier. Let me check with the dev team.Hi @c.bickmeier. We opened two internal issues to address what we think may be the cause: OM-96623 and OM-96625.Thank you very much… what does those OM-Numbers mean to me? Is there any way to track the status oh those tickets?This topic was automatically closed 14 days after the last reply. New replies are no longer allowed.Hi Carl. You can look out for those OM numbers in release notes for when they’re fixed.Powered by Discourse, best viewed with JavaScript enabled"
382,issac-sim-python-api-camera-orientation-ignored,"Creating a Camera sensor using the Python API, sets a default orientation, that cannot be changed. Trying to set orientation, is just being ignored. Here is a reproduction:
image1582×298 29.4 KB
hello, have you solve it ,? I also found that the orientation can not setUnfortunately not, I decided to return back to using Gazebo after fighting with Isaac Sim for too longHi omers,What you are seeing is the expected behaviour using the Camera class api since the orientation is being set using the right-handed coordinate conventions (+Z UP and +X forward). However the usd camera properties shown in the property panel are shown with the coordinate convention of (+Y up and -Z Forward).Setting the orientation as [1, 0, 0, 0] should point the camera forwards towards the +X. Are you expecting the USD camera to be set in the ROS convention?  Or you are confused with the difference between the api and USD property panel values?Let us know if you are encountering more problems.Take a look here for an explanation of the conventions: Conventions Reference — Omniverse Robotics documentation@oahmed Thanks for the reply. It has nothing to do with ROS. We set a value in the python api, and expect to see the same value in the properties panel. The python api convention is WXYZ, and even if it wasn’t [1,0,0,0], wouldn’t be converted to [-0.5, -0.5, 0.5, 0.5]. You can see in the image.Camera prims in USD are Y up -Z forwardThe Camera() class that wraps around the usd camera prim hides this difference and presents a Z up X forwards interface for consistency. This is why setting the orientation of the Camera() class does not match whats in the property window.One solution is to place the camera prim as a child of an XformPrim, changing the orientation of the XformPrim would match as you would expect.@Hammad_M I think the issue here might be misunderstood. The camera has a default orientation of [-0.5, -0.5, 0.5, 0.5], which is in euler is x: 90, y: -90. No matter what this value can’t be changed. We can work around it by having a prim that will rotate it back to 0, but this doesn’t solve the bug, it is just a workaround. There is a bug here that needs to be solved regardless of the parent prim workaround.To clarify, do you mean that in the property window, when you change the euler x,y,z angles, the object doesn’t rotate?That specific bug has been fixed and will be part of 2022.2.1 coming out soon.I mean that is you change it in the Python API, it doesn’t change it in the property window, nor rotate the object. Might be that it is related to the fixed bug, but not sure.Can you run this in the script editor? This should work for changing the orientation of the camera. Make sure to enable camera visualisation to see the orientation more clearly.Try also following the docs at Camera — Omniverse Robotics documentation for better understanding of cameras.Ignore the USD euler orientation property shown in the USD property panel for now since it had a bug that will be fixed in the next release.Let us know if you have more questions.This topic was automatically closed 14 days after the last reply. New replies are no longer allowed.Hi @oahmedI tried the script given by you, and it’s working perfectly fine. However, when I write the similar code in the following way, it doesn’t work. Could you please let me know the reason why the orientation doesn’t change with my code?I used the script editor for this given snippet. Since the default camera object has a focus distance equal to zero, I set the focus distance to 40 so that I can see the camera’s field of view.My actual requirement is:I want to add the camera to the end-effector of the Franka Panda robot. To achieve this, I am creating a camera primitive object as a child of the “panda_hand” link, and later I will adjust the orientation of the camera.Powered by Discourse, best viewed with JavaScript enabled"
383,omniverse-connector-for-siemens-plant-simulation-and-process-simulate,"I am currently looking at ways to showcase the use of Discrete event simulation and process simulation in VR and for this purpose it would be very interesting for me if I could directly connect omniverse to Siemens plant simulation or process simulate.I found out that this connector already exists and was presented in one of the nvidia webinars therefore  I was wondering if someone could tell me more about it? And if I could have access to it?Please find bellow the link to the webinar ( Starting at 18min29 for the part on the omniverse connector for Plant Simulation and Process Simulate)A major challenge in the digital transformation of industrial manufacturing plants is to abstract the complexities of the modeling and simulation world froThank you in advance for your assistance with thisGwenoleHello @gwenole.henry!  I’ve asked the team for more information about this.  I will post back when I have more information to share!Hi @WendyGram , that’s great thank you for your help with this !t ways to showcase the use of Discrete event simulation and process simulation in VR and for this purpose it would be very interesting for me if I could directly connect omniverse to Siemens plant simulation or process simulate.I found out that this connector already exists and was presented in one of the nvidia webinars therefore I was wondering if someone could tell me more about it? And if I could have access to it?Please find bellow the link to the webinar ( Startiintrested to follow this, i use seimens plcs and i want to research implementing ai in industrial processes.best of luck with your projectsHi, is there any update here? I am interested in this topic too.Hi @WendyGram
Is there any update on this?
I’m often working in Process Simulate for my job and I’m really interested in this topic. You can see Process Simulate 16.1 connected to Omniverse software in this demo below starting at 0:47It would be great to hear more about how to do this. Thanks in advance!Hi, @WendyGram
Has there been no update yet? I appreciate your help in advance.Hi @WendyGram
My team would like to showcase our plant simulation from process simulate in VR and would like to know if there is a connector for Nvidia Omniverse which can be used to do the same.Powered by Discourse, best viewed with JavaScript enabled"
384,problem-mesh-fitting-audio2face-points-attachment-to-the-mesh,"I remember having the same problem. My problem was that the normals on my Mesh Head were reversed, so I reversed them again in Blender and that fixed the problem.This topic was automatically closed 14 days after the last reply. New replies are no longer allowed.Powered by Discourse, best viewed with JavaScript enabled"
385,realsense2-description-no-definition-of-xacro-for-os-version-focal,"After I type rosdep install -i -r --from-paths src --rosdistro humble -y --skip-keys “libopencv-dev libopencv-contrib-dev libopencv-imgproc-dev python-opencv python3-opencv nvblox” in terminal, The error occured and realsense-viewer no rgb frames
ERROR: the following packages/stacks could not have their rosdep keys resolved
to system dependencies:
realsense2_description: No definition of [xacro] for OS version [focal]
realsense2_camera: No definition of [librealsense2] for OS version [focal]
Continuing to install resolvable dependencies…Hello,Welcome to the NVIDIA Developer forums. You posted in the Community Feedback category, this section does not offer technical support. Please provide more information about your environment and I will move this topic to the appropriate forum.Thanks,
Tomwhat I used is Ubuntu 18.04 with ros2 dashing。When I use ISAAC SIM nvblox with realsense, the error occurs and has depth frames but no rgb framesThanks, I am moving this topic to the Isaac Sim category for visibility.hello,
I have meet the same error. in tutorial-realsense
when I run
“cd /workspaces/isaac_ros-dev/ && 
rosdep install -i -r --from-paths src --rosdistro humble -y --skip-keys “libopencv-dev libopencv-contrib-dev libopencv-imgproc-dev python-opencv python3-opencv nvblox””I get error:
ERROR: the following packages/stacks could not have their rosdep keys resolved
to system dependencies:
realsense2_description: No definition of [xacro] for OS version [focal]
realsense2_camera: No definition of [librealsense2] for OS version [focal]
Continuing to install resolvable dependencies…I use AGX xavier, ubuntu20.04, jetpack5.1.1, realsense D455,“isaac_ros_nvblox quickstart demo” alone work well ,
“isacc ROS realsense setup” alone  work well.combine this two project together, meet this error.how to solve this ?Thanks, TomGot the same error, have you solved this?Getting the same error. Also, tried the following:and then:Got the message:Powered by Discourse, best viewed with JavaScript enabled"
386,creating-and-destroying-prims-during-runtime-in-isaac-sim,"Hi,Is there a way to instantiate and destroy prims during runtime in Isaac Sim?I want to create boxes (initialized with random rotations) and have them fall into a conveyor.Similarly, is there a way to destroy them (for example, upon exiting the FOV of the camera)?Thank youHi @felipe.cunha1  - Yes, you can create and destroy prims during runtime in Isaac Sim.To create a prim, you can use the create_prim function from the omni.usd module. Here is an example of how to create a cube:In this example, a new stage is created, a new Xform prim is defined at the path ‘/Cube’, and then a cube is defined as a child of that Xform.To delete a prim, you can use the RemovePrim function. Here is an example:In this example, the prim at the path ‘/Cube’ is removed from the stage.To create boxes with random rotations, you can use the UsdGeom.XformCommonAPI to set the rotation of the Xform prim. Here is an example:In this example, a UsdGeom.XformCommonAPI is created for the prim, and then its rotation is set to a random value.To destroy prims based on whether they are in the field of view of the camera, you would need to implement a check that determines whether the prim is in the camera’s field of view. If it is not, you can then call RemovePrim to remove it. The specifics of this check would depend on the details of your camera and scene setup.@rthaker thank you so much for your answer. This is exactly what I needed to get going.I have used stage = self.stage to create the prim into my current tree and it worked nicely.Powered by Discourse, best viewed with JavaScript enabled"
387,how-to-write-raw-usd-property-each-frame,"I have a prim with a Raw USD Proprty like show in figure. This property force_value is updated by an ActionGraph.I can get its value in python:But how I save this property in the dataset? I’m using BasicWriter and I would like that my property force_value is saved for each frame inside the dataset.Hi @federico.domeniconi. This sounds like a Replicator questions so I’ve moved it over to the SDG forum for you.I did this:Is this the correct way to do this? Or is there a better method?Hello @federico.domeniconi. I want to first apologize in the extensive delay in getting back to you on this.The issue with this approach is that the writer does not necessarily execute at the same time as the simulation. In fact, it’s at best going to be 1 frame behind and can be up to 3 frames out of step with the simulation. Because of this, it’s best to avoid querying the stage within the writer unless you know the property being queried is not changing.If you are using samplers (eg. rep.distribution.uniform) to set force_value, then a convenient solution is to simply name the distribution. The sampled value(s) from that distribution will then be automatically avalailable in the data payload under the provided name under distribution_outputs.Example:Powered by Discourse, best viewed with JavaScript enabled"
388,warning-while-calling-function-with-self-world-step-render-self-render-in-the-control-frequency-inv-loop,"I am trying to realize a long-term decision task A in OmniIsaacGymEnvs, so I set dt to 1/60 s and control_frequency_inv to 180(execute every 3s). Besides I also want to execute another function B every world frame, so I add it to the control_frequency_inv loop. However I got the warning below.2023-06-21 14:01:55 [69,210ms] [Warning] [omni.physx.tensors.plugin] GPU tensor function setRootTransforms: calling a setter after flush may result in incorrect values!according to my test, the reason is the calling of set_joint_velocity_targets in function B. How can I avoid this warning?my code where the functionB is added:The warning message you’re seeing is indicating that you’re trying to set the joint velocity targets after the physics simulation has been updated (flushed). This can result in incorrect values because the joint velocity targets should be set before the physics simulation is updated.Here’s how you can avoid this warning:Here’s an example of how you can modify your code:In this modified code, set_joint_velocity_targets() is called in functionB(), and then the physics simulation is immediately updated by calling step(). This ensures that the joint velocity targets are correctly applied before the physics simulation is updated.This topic was automatically closed 14 days after the last reply. New replies are no longer allowed.Powered by Discourse, best viewed with JavaScript enabled"
389,how-can-i-create-the-expanded-collapsed-ui-with-action-graph,"Hi Team,
I am using Create 2022.3.3
I have created UI with action graph. I want to know how to create the expanded or collapsed UI in the viewport with action graph.
I have attached the reference Image and short clip of video.

UI1251×982 87.1 KB
(In attached screenshot Marked red box I need in collapsed Menu)
Menu.mkv (4.1 MB)(In attached video If I am clicking the PAINTS menu, the below items needs to be collapsed)Also I want to know from action graph can I change the Text color in the Viewport Buttons or Instead of buttons Should I add the Custom Icons ?Please take a look and share some reference videos or documents which is more helpful.
Thanks in Advance.Powered by Discourse, best viewed with JavaScript enabled"
390,substance-painter-9-support,"HiI’m trying to use Substance Painter 9 with USD Composer 2022.3.1 but I get the message:The Omniverse Substance 3D Painter Link Link Extension is out of date.Are there plans to update it or is there a way to make the current one work ?Thanks,
GuillermoHi Guillermo,Which version of the Connector and Live Link extension do you currently have installed? The most recent Substance Painter Connector (version 203.0.204) is only compatible with the most recent version of the Live Link Extension (203.0.9). For compatibility, the major and minor versions between the two need to match (i.e. if you have a 203.0 connector, then you need a 203.0 extension). The 203.0.9 version of the extension was included with the 2023.1 Omniverse USD Composer release. Your options would be to either:If you want to use the latest versions, install the Omniverse USD Composer 2023.1 beta, and then install the latest Substance Painter Link Link extension via USD Composer’s extension manager (203.0.9) as well as the latest Substance Painter connector (version 203.0.204) via the Omniverse Launcher.If you need to remain on USD Composer 2022, you’d need to install an older version of the Substance Painter Connector via the Omniverse Launcher that matches the extension version that’s available with your USD Composer installation (if you’re using a 202 version of the extension, then you’d need a 202 version of the connector).If you have any questions, please let me know!Thanks for the replayI got it working with some of your comments. I had to remove both USD Composer and the painter connector. I reinstalled the beta version and this one is working.Thanks
GuillermoPowered by Discourse, best viewed with JavaScript enabled"
391,get-object-center-pose-in-camera-frame-viewport,"Hello,Is there some sort of function or code to retrieve the center position of an object not in world coordinates, but in my 2D image? I know how to retrieve the world pose, but am yet to find a solution to get the center position of the object for my neural network training. Is there anything existing that either just gives the object center pose of my asset directly or some kind of factor I can use to calculate from world position into camera pixels?Thank you!Hi there,using the cameraViewTransform and the cameraProjection from the  camera parameters annotator you can transform the world coordinates of the object to camera coordinates by multiplying the objects pose with the inverse of the view transform matrix.You can then apply the camera projection by multiplying the resulting camera coordinates with the projection matrix. Projecting the 3D coordinate onto the 2D image plane. Resulting in a 4-d coordinate vector [x, y, z, w] (where w is the scaling factor).To get the position of the object in the camera’s view frustum, with respect to the camera’s coord. system you should divide [x, y, z, w] with w resulting in [x’, y’, z’] (normalised coordinates).For the pixel coordinate conversion [u, v], normalize [x’, y’, z’] by dividing by z’:u = x’ / z’
v = y’ / z’and scaling the normalized coord. with the img. resolution:u *= image_width
v *= image_heightPowered by Discourse, best viewed with JavaScript enabled"
392,how-to-increase-rmp-lula-performance-on-a-custom-6d-robot,"Hi all,
i’m working on the creation of gym environment where a custom 6d robot arm is imported following the isaac sim workflow ( from urdf to the rmp lula motion policy).
Actually since the robot trajectory impacts on the simulation of an gym environment where the robot is awaited to try some picking movement, i realize that the actual parameter of the rmp are not the perfect for increasing the cycle time of my robot.
Starting from the franka file, changing parameter in the target_rmp section i reached out to increase the robot speed, but the behaviour evolves completly unstable.
There are some expert that can help me in understanding the problem?
Thank you so much in advance for the help.Hi @vincenzodepaola94Detailed information about each RMPflow parameter is documented here: Motion Generation — isaacsim 2022.2.1 documentationThis page also includes a tuning guide for creating stable performance:
https://docs.omniverse.nvidia.com/isaacsim/2022.2.1/ext_omni_isaac_motion_generation.html#rmpflow-tuning-guideIn general, RMPflow is not designed for creating time-optimal trajectories, so there will exist a tension between speed and stability. For a truly deep dive into the inner workings of RMPflow, this paper details the motivation and implementation quite fully: [1811.07049] RMPflow: A Computational Graph for Automatic Motion Policy GenerationThis topic was automatically closed 14 days after the last reply. New replies are no longer allowed.Powered by Discourse, best viewed with JavaScript enabled"
393,how-to-implement-the-functionality-automation-process-of-audio2gesture,"Hey, I want use the python implement the automaton with audio2Gesture in console of Windows System.
But how can I do it?
I try to exec ""kit.exe -exec  “my python script” int console, but it can’t work…the log info is that2023-07-20 07:49:43 [140ms] [Info] [omni.kit.app._impl] python GC: gc.disable()
2023-07-20 07:49:43 [140ms] [Info] [carb] Plugin carb.scripting-python.plugin is already a dependency of omni.kit.app.plugin; not changing unload order
2023-07-20 07:49:43 [140ms] [Info] [omni.kit.app.plugin] No run loop was found, quiting…
2023-07-20 07:49:43 [140ms] [Info] [omni.kit.app.plugin] Application auto-quits as it worked for the specified number of frames: 0
2023-07-20 07:49:43 [141ms] [Info] [omni.kit.app.plugin] app started
2023-07-20 07:49:43 [141ms] [Info] [carb] Initializing plugin: carb.threadtime-tracker.plugin (interfaces: [carb::threadtimetracker::IThreadTimeTracker v1.0]) (impl: carb.threadtime-tracker.plugin)==================
My purpose is that I have a audio file and I can execute my script to produce usda from audio2Gesture, rather than use the Machinima App.I need your help,plz.Powered by Discourse, best viewed with JavaScript enabled"
394,isaac-sim-ros2-humble-using-cyclonedds,"Hello,When trying to run the ROS2 Humble bridge with export RMW_IMPLEMENTATION=rmw_cyclonedds_cpp, we get the following error:Manually running the following fixed it for us:This topic was automatically closed 14 days after the last reply. New replies are no longer allowed.Powered by Discourse, best viewed with JavaScript enabled"
395,moveit2-and-omniverse-isaac-sim-connection-is-not-working-demo-isaac,"Hi. I want to link MoveIt2 and Isaac Sim in ROS2. I’m following the description on the site (9. MoveIt 2 — isaacsim latest documentation) ). I am writing an inquiry because the connection is not working well.1. EnvironmentUbuntu 22.04, ROS2 Humble
Omniverse Isaac Sim 2022.2.12. contextAs explained in the Omniverse guide, I installed ROS2 Humble and created a fastdds.xml file and set environmental variable. And I proceeded as explained in the MoveIt guide linked to the link. Omniverse runs normally, but when I run Rviz, it fails even if I move the robot, and the following log appears on the console.3. Log (Rviz)4. Screenshot(1) Rviz
스크린샷 2023-08-04 1126591284×816 118 KB(2) rqt graph
스크린샷 2023-08-04 1128251288×819 113 KBPowered by Discourse, best viewed with JavaScript enabled"
396,unexpected-robot-movement-in-environment-with-sdf-based-collision-and-fixed-joints,"Hello everyone,I wanted to report a problem that I encountered while working with a setup involving a robot with fixed joints and approximated collision using SDF (Signed Distance Fields). In my environment, I added objects with fixed joints, similar to the robot, using SDF-based collision approximation. However, in some environments in parallel, as shown in the following pictures, the robot unexpectedly teleports to strange positions. I am using the Docker image of IsaacSim2022.2.1.sdf_simulation1272×810 112 KBInterestingly, when I exclude the fixed joints from the SDF-based objects, the robot moves correctly, leading me to suspect that adding multiple fixed joints within a single environment triggers this bug. However, I noticed that in the Factory demo, bolt is able to have fixed joints without issues.I’m seeking guidance on how to address this problem. Is there a recommended approach for adding multiple fixed joints within an environment that uses SDF-based collision? Any insights or suggestions would be greatly appreciated.Thank you in advance for your help.Best regards,
smakolon385Here are the results when I executed the same implementation in IsaacSim 2022.2.0. The robot clearly spawned in an incorrect position.IsaacSim2022.2.01337×820 48.7 KBCould this possibly be a bug or some issue related to the IsaacSim 2022 version?In the above figures, I had set the pipeline to GPU for SDF simulation. As I mentioned before, when I switched the pipeline to CPU, the robot moved correctly, although SDF simulation was not possible. Is this an unresolved bug in the current version of IsaacSim? In the Factory demo, Franka operates properly even with the GPU pipeline, so there should be some solution available, but I’m not sure what it is. Could someone please help me with this issue?cpu_pipeline1668×859 177 KBPowered by Discourse, best viewed with JavaScript enabled"
397,how-to-use-code-to-move-anim-people-one-step,"Hi,
For example, if I have already ‘Spawn Tom’ and set ‘Tom GoTo 10 10 0 _’, Tom will go to (10,10). It consist of many walking steps, is there any way to seperate this whole action and render this ‘walking’ behavior step by step?
Because I was tring to add anim.people into Omnigibson, but when I add a robot(in Omnigibson) and 2 people(in anim.people) together in to a empty scene and set 2 person start navigation, they stood in their initiate place and didn’t move, and I was wondering if move one step in Omnigibson forbid 2 persons to move. So maybe if I could use code to move 2 persons might helpLooking forward to your reply!Screenshot from 2023-08-03 17-39-341920×1080 97.4 KBScreenshot from 2023-08-03 17-40-221920×1080 70.1 KBPowered by Discourse, best viewed with JavaScript enabled"
398,deepsearch-not-displaying-in-navigator-after-extension-enabled,"I’ve followed the video enabling the Deepsearch extension …and the documentation says:To use DeepSearch in Nucleus Navigator, follow these steps:Deep search is an enterprise service that is deployed against a specific nucleus instance currently.Your images show localhost is what you’re connecting to. Do you have deep search deployed here?Powered by Discourse, best viewed with JavaScript enabled"
399,reporting-position-of-objects-on-conveyor,"Is there any built in functionality within Isaac Sim that reports a list of all objects on a conveyor and reports their coordinates?  This would be helpful when creating robotic pick and place simulations.If there’s no current built in functionality, does anyone have any suggestions on how to approach this problem?Hi @tsosnoski  - Isaac Sim does not have a built-in functionality specifically for reporting a list of all objects on a conveyor and their coordinates. However, you can achieve this by using the Python API provided by Isaac Sim.Here’s a general approach:Identify the conveyor: You can use the stage API to get a reference to the conveyor object in the scene.Identify objects on the conveyor: You can use the stage API to iterate over all objects in the scene. For each object, you can check if it’s on the conveyor. This could be done by checking if the object’s bounding box intersects with the conveyor’s bounding box.Get object coordinates: For each object on the conveyor, you can use the stage API to get its world transform, which includes its position in world coordinates.Thanks rthaker for the response.  I found a way to accomplish it which was based of one of AshleyG’s learn with me live sessions.  She wrote a behavior script to count boxes in contact with a pallet.  I began with her code and reported back the coordinates of the objects instead of counting them.  Then I added a function to monitor when the objects fall off so they could be removed from the array.I like your suggestion of using bounding boxes.  It would likely be a cleaner approach.  I will need to investigate that.Powered by Discourse, best viewed with JavaScript enabled"
400,enable-ros-brdige-via-pyhton-script,"Hi Everyone,
My demo is setup to use a python script to load my environment robot etc. Within that script I would also like to enable the ros-bridge automatically.
Is it possible to load the ros-bridge extension via the script?I tried things like this: from omni.isaac.ros_bridge import RosBridge but this runs dependency errors…Is it possible to load the ros-bridge extension via the script?Hi @elias.treis  - Yes, it is possible to load the ros-bridge extension via a script in Isaac Sim. You can use the set_extension_enabled_immediate method of the extension manager to enable the ros-bridge extension.Here’s an example:This script will enable the ros-bridge extension immediately. Please note that the extension must be installed and available in your Isaac Sim installation for this to work.Powered by Discourse, best viewed with JavaScript enabled"
401,isaac-sim-full-screen-mode,"Is there any plan to add Full Screen Mode functionality via F11 key to Isaac Sim, similar to how Omniverse works where it maximizes the viewport only?If there’s another way to do it already built into Isaac Sim, please advise.Hi @tsosnoski  - There was a bug in recent Isaac Sim release which prevented it from happening. In the upcoming release (Aug/Sep) this should be fixed.Great to hear!  Looking forward to the next release.This topic was automatically closed 14 days after the last reply. New replies are no longer allowed.Powered by Discourse, best viewed with JavaScript enabled"
402,physics-simulation-in-replicator-docker-ovx-strange-results,"Hi,I am trying to run a simple simulation with physics in Replicator, using the Docker image  v1.6.3The scene has a torus falling down on a plane. I am writing out the RGB sequence. Looking at the results, I have the following issues:Any help welcome!
Brunosimulate.log (37.7 KB)
simulate.py (1.1 KB)To illustrate, let me attach a video where I run the sequence of images as a clip.
2023-04-27 11-45-04.mkv (3.8 MB)Regarding 2. and 3., I was able to create a physics scene like this:
rep.physics._setup_physics_scene()The error does not occur anymore. The function _setup_physics_scene() is not part of the official API though. It also seems that it is used nowhere from within Replicator code, but I am not sure.Problem 1. persists, though.Hi @bruno.vetter ,I just used your code to test on Replicator 1.6.3 and I get the same physics error ( [Error] [omni.physx.plugin] Physics scenes stepping is not the same, step subscription will be send with later step, per scene step is not yet supported.) but the torus drops onto the plane as it should - I am not seeing this strange behavior.Are you using replicator through Code or Usd Composer? If so what other extensions do you have turned on that are not on by default? Maybe there is some other imported package conflicting with something and causing the issue?-HenryHi Henry,I am using the Docker image for Replicator 1.6.3 provided by Nvidia. I run the Docker image on an OVX running Ubuntu.Best,
BrunoHi Bruno,I was able to repro this using docker with 1.6.3. It is units bug that was fixed some time ago, but didn’t show up on my local build of replicator because it didn’t have physx /isaac in it. Can you use the latest version of Isaac production app on the launcher? It should have replicator 1.7.8, and I just tested it and it does not have the issue. Alternatively, if you can get the docker image it is fixed on replicator 1.7.7 too.-HenryI agree this problem does not show up in Isaac Sim, but there is no version 1.7.7 Docker image available here: Omniverse Replicator | NVIDIA NGCThe newest version is 1.6.3 unfortunately.Hi Bruno,I’m sorry about the delay.This was a pretty nasty bug - took us some time to work through it. However, we have a workaround that should fix this specific case. I attached the code here.Meanwhile, I’ve tried to expedite the public release of Replicator 1.7.7, which is a much better solution to this. I had thought it was released publicly already but was wrong.Let me know if the workaround works for you.-Henry
om93987_fixed.py (1.8 KB)Powered by Discourse, best viewed with JavaScript enabled"
403,carter-v2-urdf-mesh,"Hi,
I’m trying to follow the tutorial ( Multiple Robot ROS Navigation) using carter_v2(Below figure).
image693×548 63.6 KBBut for using that tutorial, i have to need the urdf and mesh of carter_v2.
But Only carter_v1 urdf and mesh are provided in Isaac Simulator.So,
Where and How can i get the urdf and mesh of carter_v2?Thank you :)Powered by Discourse, best viewed with JavaScript enabled"
404,how-to-fuse-basicwriter-semantic-standard-classes-unlabelled-and-background-to-one-class,"Hey there,I noticed that my semantic labels contain two semantic classes for the “Default” class, which both get generated automatically. The UNLABELLED class is used, when Props simply have no label assigned and the BACKGROUND class represents the space where there is realy nothing.  Is it possible to fuse those two classes together, as I don’t want to separate them and I want both together as my “Default” class. Do I need to write a custom Writer for this?I initialize my BasicWriter like this:Thanks in advance.Hi @Braveblacklion , you can create your own writer, following the example of BasicWriter
At basicwriter line 413 semantic_seg_data = data[annotator][""data""], you can change toAnd change the id_to_labelsPowered by Discourse, best viewed with JavaScript enabled"
405,how-to-set-defaultprim-in-api,"I’m creating some synthetic prims to use in Replicator.    Works fine if I manually go in and set the root prim as defaultPrim.    Otherwise the prim doesn’t show if not defaultPrim.Is there a way using the API to mark the root as ‘defaultPrim’ ?Thanks!    (Sorry if this is a num-num)i.e., if marked as defaultPrim - my replicator works nicely.

image938×450 72.6 KB
work around
start with a template stage where defaultPrim is set.    then modify that to create a new object and save that.Hello @peter.gaston, I apologize for the extensive delay in getting back to you. A proposed workaround is to create a replicator function that will identify whether the referenced USD has a missing default prim and make a reasonable choice (here, choosing the first prim as the default)Powered by Discourse, best viewed with JavaScript enabled"
406,omniverse-launcher-login-problem-on-linux-kubuntu-20-04,"I had a serious trouble logging in the Omniverse Launcher on Kubuntu 20.04 LTS. I have all browsers (Firefox, Chromium) provided from snap so that they are sandboxed and contain latest security fixes. The login opens a default browser by xdg and of course, not any one of them could log in because they are making HTTP request to localhost to notify omniverse launcher after successfully entering user credentials. As a workaround I had to install Microsoft Edge for Linux and make it temporarily default browser and also clean all Omniverse Launcher configuration. After several attempts without success it finally logged in. Seems that Edge has less strict security, or there might have been a glitch with Omniverse.Your developers should fix omniverse launcher on Linux so that it uses some dumbest looking login form that actually works, as Linux people don’t care about how it looks but about it working. Alternatively make it so the security token can be copy pasted to Omniverse Launcher directly, as sending JSON request on localhost from nvidia page is not usually possible as most browsers like Firefox or Chrome block it, and it is only a matter of time before Edge does the same…So temporary workaround is to use Microsoft Edge for Linux, but remember that workaround is not solution!I have the exact same problem on Linux Ubuntu 20.04
Neither Firefox, Microsoft Edge nor Chrome are working for me however.Console output after attempted login:11:38:19.697 › Login URL opened.
11:38:19.699 › Waiting for login result…
11:39:28.393 › WEB /login-result?ui_locales=en&code=YE21MRuoHcoWc2kl4b25dE6JF49R6tDdzHAQHZ_imKtRnRUig_RDVAahXUhaQRvCYybF96bDj4yG0hFaWm5IQQ
11:39:31.767 › (node:25743) UnhandledPromiseRejectionWarning: Error: ENOENT: no such file or directory, unlink ‘/home/user/.config/autostart/nvidia-omniverse-launcher.desktop’
11:39:31.767 › (node:25743) UnhandledPromiseRejectionWarning: Unhandled promise rejection. This error originated either by throwing inside of an async function without a catch block, or by rejecting a promise which was not handled with .catch(). To terminate the node process on unhandled promise rejection, use the CLI flag --unhandled-rejections=strict (see Command-line API | Node.js v20.5.0 Documentation). (rejection id: 2)EDIT: I further investigated the issue. There was a new Omniverse launcher release in July 2023
I transferred an old appImage from another workstation and that one worked.
Once logged in the current appImage was also able to start the omniverse launcher, even though the errors are still occuring in the console.I also had the same issue after installing the new launcher. I was able to solve it by toggling “Launch on Login” option. This options adds or removes the “/home/user/.config/autostart/nvidia-omniverse-launcher.desktop” file.
We can find it when you right click on Omniverse logo on the top bar.Powered by Discourse, best viewed with JavaScript enabled"
407,nucleus-hell,"Screenshot 2023-07-21 1520231844×993 107 KBHello, I just want to use Audio2face, simple enough but I just cant. All fine on other machine. Now i installed Omniverse on another machine, I made an admin account for Nucleus, it says credentials wrong. 10 time reinstalled make new user and password, credentials wrong every time. Why is there no local host folder on Nucleus tab? Why does this refuse to work all day long? thanks.If you were to go to the hamburger icon next to the folder with an arrow, then choose “Settings”, what do you see?Secondly, do you have OneDrive set up on this machine that is giving you problems?Powered by Discourse, best viewed with JavaScript enabled"
408,may-i-use-the-audio2face-with-command-line,"I know that the app already provides http interfaces. I wonder if I can call the app just with command lines?Yes You can use Audio2Face Headless: Audio2Face Headless and RestAPI Overview - YouTubeGet it. Thank you for the reply.But I found a problem… If I put it on my own computer and share it with my classmate, I need to write a request queue manager myself…Or I may get an animation generated with others’ wav file.It is possible to configure the audio path through a REST API. The following REST API calls are available for this purpose:To retrieve or set the root folder containing the audio files:To retrieve or set the tracks in the player:This topic was automatically closed 14 days after the last reply. New replies are no longer allowed.Powered by Discourse, best viewed with JavaScript enabled"
409,unable-to-log-to-nucleus-server-through-code,"Trying to run the offline_generation.py (forklift) and change the cone asset to a 3d asset from a nucleus server.
I’m running my workstation as a container, I tried to load the usd as follows:
rep.create.from_usd(""omniverse://myserver/my_usd_file.usd)
Of course I needed logging input so I added those as env variables:
OMNI_USER
OMNI_PASS
as mentionned in Access Nucleus on Host from Replicator inside DockerBut still, I get the following error:
timed out waiting for results for Nucleus Auth/CredentialsIs there a clear documentatino on the authentification subject?
Thank youI can manage to connect to the nucleus server and list folders using the connect sample from the exchange library but it does not seem to work in python, the real error is the following:
[54,328ms] [Warning] [omni.client.plugin]  Tick: authentication: Discovery(wss://nucleus_address/omni/discovery): Timed out waiting for results for Nucleus Auth/CredentialsIs it a websocket issue? I wonder how I can debug itCan you double check that all your services are running (Discovery, Auth, etc), and validate those are accessible?
It looks like it took 54000+ ms to get a response, which makes it seem like either it’s not accessible or credentials were wrongPowered by Discourse, best viewed with JavaScript enabled"
410,bug-report-instances-batch-exported-as-references-payloads-explode-from-mesh,"Using version 200.3.0, when instanced objects are batch exported as references or payloads they are exploded out from the mesh as seen in the attached screenshot:
image (3)1223×672 175 KB
When exported without batching, the instanced objects are references in the stage, directing to a mesh entity and are in their correct locations.When exported as either references or payloads, the exported instanced objects reference an Xform entity containing a mesh entity. Upon testing, resetting the transform values of the Xform entity fixes the misplaced objects, however, this does move the “original” mesh entity to the scene root, away from where it should be. Here are some more images illustrating the point:
Pre-Change1392×878 110 KB

Post-Change1167×875 83.6 KB
It seems this rogue Xform entity is the cause of the issues where it should just reference to a prim as it does when exporting without the batch option selected. It goes without saying that the gizmo for the misplaced references appears in the correct place.
GizmoCorrect961×739 54 KB
It’s worth mentioning that just resetting the transform of the references entities does not fix the issue; they just occupy the exact location as the references transform when doing this, my only option is to reset the transform of the transform, misplacing the “original” mesh entity.I’ve uploaded a zip of the original 3DS Max scene as well as the USD exports with the exploded meshes as references and payloads.3DSMaxConnectorExportIssues.7z (37.2 MB)Thank you for reporting this @liam.c!  I’ve sent it over to the dev team to evaluate!A development ticket was created for this post: OM-81168: Instances batch exported as references/payloads explode from meshI’ve tested this in 3ds Max Connector 201.0 and cannot reproduce the issue. We did a lot of fixes in that release for batch+references.
Thank you!Powered by Discourse, best viewed with JavaScript enabled"
411,audio2face-2023-1-1-open-beta-released,"In this release - We’ve added the Omniverse Live Link plugin. Enabling the workflow to Live Stream animation from Audio2Face to Unreal Engine where it can be connected to a MetaHuman Character.The Audio2Face Live Link Plugin allows creators to stream animated facial blendshape weights and audio into Unreal Engine to be played on a character. Either Audio2Face or Avatar Cloud Engine (ACE) can stream facial animation and audio. With very little configuration a MetaHuman character can be setup to receive streamed facial animation, but the plugin may be used with many other character types with the correct mapping pose assets.More details about the plugin can be found here.
https://docs.omniverse.nvidia.com/audio2face/latest/user-manual/livelink-ue-plugin.html###TutorialsIn this series, we will cover how to use BlendShapes from Audio2Fact to Unreal MetahumanIn this series, we will cover how to use BlendShapes from Audio2Fact to Unreal MetahumanGreat, just remember to rebuild tensorRT… first time using it(2023.1.1).  dosent remind you if you have been using 2023.1.Great job. I have downloaded and try this version. Able to work fine for streaming from audio2face → Unreal Engine. (I am using 5.0.2)
However, i have one issue. I use female voice track in audio2face. Yet when it’s streamed into Unreal, the voice became male … Anyone know how to resolve this?@justin123 sounds like you’re using a different audio sample rate than your input audio? please try different rate like 48k Hz?
I can not find the new feature: AI model tool in 2023.1.1 linux version, and also can you tell me how to open the face model?
11848×1053 87.6 KBThanks for the answer. It’s indeed the Audio Sampling Rate (my wave file is in 22.05Khz)
I have another issue.
I am sending in a long wave track (50secs) via Headless Audio2Face API → Stream to Unreal.
After some intervals, the face & the audio out of sync.
I am thinking of chunking the long wave track into smaller pieces (by sentence wave) and then batch it to
audio2face. However, after combing through the API, i cannot find another start_play the root folder in batch mode. There’s only solve ExportBlendshapes API in batch mode. (which will batch the face weights to Unreal, But no audio stream)
Would there be a start_play API in batch mode? If not, what could be the possible solution in this use case?@justin123 Could you kindly start a new thread in the Audio2Face Forum regarding this issue? It would greatly improve visibility and increase the chances of getting helpful answers. Thanks!@zhangliyun9120 Could you kindly start a new thread in the Audio2Face Forum regarding this issue? Thanks!@Ehsan.HM this plugin only supports both 5.1.15 and 5.2.14 (the two of three subfolders from the “audio2face-ue-plugins” folder) and not backwards compatible, correct?p.s. wanted to clarify for users on the Discord as there was some confusion with how the subtitle was shown on the page 
image736×111 10.2 KBThat’s correct. The Audio2Face to UE livelink plugin only supports UE 5.1 and 5.2. That must’ve been a typo. Thanks for reporting it.@Ehsan.HM I have sent a PM regarding about the versions that Livelink supportsWe don’t currently have plans to support older versions of Unreal. We might do this in the future though, just no promises :)That said, the current released versions can be used as a reference to make an Unreal 4 compatible version. The name of the extension is omni.avatar.livelink and can be found on disc using extension manager.Screenshot_222400×1266 362 KBPowered by Discourse, best viewed with JavaScript enabled"
412,slow-startup-time,"Hi!I’m new to Omniverse in general.
I installed it in a 2xRTX 2080 Ti computer.I’m running Omniverse Create using the following command:./omni.create.singlegpu.sh --/renderer/activeGpu=1However, the startup process is particularly slow, ranging from ~450 to ~600s.Here’s the launch log:Is this an expected behaviour? What can I do to improve performance?Thanks!Hello @pbcorrea1!  I’ve notified the dev team about this post.  Thanks for reaching out!Hello @pbcorrea1, there is something very wrong with what you are seeing - Create can take a bit longer to start up the first time you run due to shader compilation, but generally 30s would be the the upper bound of what we’d expect to see.
Could you try sending us the more verbose log that Create writes? (You find it from the console - see the 2 rightmost little icons under the “Console” tab name).
Also, you could try attaching create to strace and see what it’s log says.If neither of those show anything, we can look at some additional profiling/debugging steps
Thanks
EoinTrying to cover as many possibilities as we can…another option is to record a Kit Profile and send that to us. See kit docs.
Thanks
EoinHello!I’ve had the same experience trying to run the enterprise install of Create on my team’s RTX server. The hardware is crazy fast but create takes 2+ minutes to open - I have an (almost) similar launch log to the one shared in the original post. Though the hold up for me looks like it occurs much earlierFiles also occasionally take longer than expected to open - but sometimes the same file opens normally. (This might be a different issue)Once launched and opened everything seems normal, but I’m left wondering what the hold up is to get running.Looking at the log file it seems the bulk of the startup time (from 8s to 100s) is between discovering GPUs. I’ve attached my launch log from last week.Idk if this is the verbose version or not, there was only one file in the log folder and it looks pretty similar to the command line output - let me know if there is any more helpful info I can provide.kit_20210902_100955.log (488.3 KB)One last thing to note is that I’ve tried running with the renderer.multiGpu.enabled = true even though I’m pretty sure this is deprecated - I’ve noticed that it picks up all GPUs either way.Thank you!I have the same issue and the following errors:image1117×148 21.5 KBThanks ruchirlives,
Any other info ? Just those two errors ? Does everything else run ok ? Can you provide more details ? Is this the latest Composer install, 2023.1.1 ?Yes, it’s the latest 2023 install. There are also 15 warnings in the log. It takes around 8-10 mins to load on initial startup, then around 30 secs to restart. I also get lots of crashes and freezes within the app.Please install the latest video drivers. Also go into Preferences and go to “Reset to Defaults” in blue right in the top left corner and let it reset the program.Powered by Discourse, best viewed with JavaScript enabled"
413,unable-to-export-usd-file-to-local-nucleus-server,"1651×879 62.1 KBI was able to confirm that the local Nucleus Service was successfully linked.（But I can access the local Nucleus server by opening the folder view through houdini and I can create folders normally）I have also tried running houdini with an administrator account (although I don’t think this method is relevant) and still can’t save to disk.
And I’ve also tried it with Unreal Engine 5.2, and I can access the local Omniverse server and read and write content normally.It is normal if the usd is saved locally. And in houdini, after creating omni linker, when I select usd, I can see the files on the local server normally but Houdini can’t read them.Nucleus Service Version：2023.1.0
System： WIndows 11
Houdini Version ：19.5.640
Houdini Connector Version：102.4.02830×517 34.9 KB
image746×311 10.8 KBPowered by Discourse, best viewed with JavaScript enabled"
414,where-does-the-shader-cache-live-on-linux,"Hello all, I am getting some odd results with a custom mdl on linux only ( this shader works fine on windows). I would like to delete the shader cache to force a fresh re-compile of the entire shader, but I cant find where these caches are stored.Does anyone know how to clear the converted shaders on linux?thanks,
KoenI also want to know thisPowered by Discourse, best viewed with JavaScript enabled"
415,product-configurator,"I was wondering if anyone has any additional documentation, tutorials, examples for creating a product configurator.  I’ve been following the “Product Configurator” page on Omniverse Docs, but it’s rather limited and not very clear.  (Product Configurator — workflows latest documentation)For example, a few of the confusing items in the documentation are:Anyway, I’m following the instructions in the Doc straight through as it’s written, and after I get to the part where I click the “Create Data Structure” button, then save the file,… the Product Configurator Panel pops open, but it’s empty.  The verbiage in the Doc makes it sound like I should see options for my variants in that panel, but it’s empty.Any help would be appreciated.  But I also kindly recommend that someone re-work the Doc page for clarity and to fix errors.Powered by Discourse, best viewed with JavaScript enabled"
416,requesting-a-community-extension,"We know there are creators out there with ideas for useful extensions and developers that want to help. If you have an idea for an extension – large or small – and are looking for a developer, you can create a new post in this forum section with your request. Here are some guidelines for how to structure your request:Developers and other interested users, can discuss the request by replying to the topic. Developers can share updates on in-progress work of the requested extension within the request topic. When an initial version of the request is completed and released, the developer and/or requester can create a topic follow the sharing guidelines here: Sharing a Community Extension.Powered by Discourse, best viewed with JavaScript enabled"
417,converting-the-example-to-a-standalone-application-using-jupyter-notebook-jupyter-notebook,"Hello everyone;
Unfortunately, I am not able to grant read and write permission to users, I think this may be caused

Screenshot from 2023-06-09 12-42-491902×709 63.5 KB

the following error after running the tutorial:Failed to save layer anon:0x1585af30:World0.usd to new path omniverse://localhost/Users/user/temp_jupyter_stage.usdthe buttons are blocked and the tutorial provided for the server is quite ambiguous can someone guide me on how to grant my user admin access to nucleus server?I am actually going through this documentation:https://docs.omniverse.nvidia.com/app_isaacsim/app_isaacsim/tutorial_core_hello_world.html#isaac-sim-app-tutorial-core-hello-worldI am trying to use jupyter notebook but I am not able to access the USD file in the terminal.I suspect this is because I do not have permission to read and write files in localhost, I did investigate the nucleus navigator but I could not change permissions! it is getting complicated can you guide me through the solution?
thanksHi @ahn.paf. Can you try writing the file to a directory that exists? I’m guessing this directory doesn’t exist: omniverse://localhost/Users/user. Try an existing user directory or create a new one and try again.Powered by Discourse, best viewed with JavaScript enabled"
418,mobile-manipulator-coupling-issue-in-isaac-sim-2022-2-0-version,"Hello, Currently, I would like to combine husky mobile robot from clearpath robotics and panda robot from franka emika to run the mobile manipulator. In the last six months, we’ve tried to combine the two robots, but they haven’t been successful.In the forum, as various experts say, we combine the top of the mobile base and the zero point of the manipulator with a fixed point, and we performed exclude from artisticization, but we failed. Also, if you put the panda model in husky model, no badies defined error will occur.We confirm that the mobile manipulator does not work on isaac sim, is it possible to succeed??Could you give us a specific solution??I would be very much obliged to you if you would help me.11237×675 128 KB

Hello, I also encountered the same problem, can you successfully couple the manipulator and the mobile base? @leejisueHello @1365351984 / @leejisue  - Are you both seeing this issue on Isaac Sim 2022.2.0 only or also on the latest Isaac Sim 2022.2.1?Yes. I can attach the each robot succefully. I miss the prim path, when i input the robot arm into mobile base.re you both seeing this issue on Isaac Sim 2022.2.0 onlyUnfortunately, I didn’t try both version. For now, I figure out what is the problem it is. So, I think that’s not the issues about version.thank you for your help :)Powered by Discourse, best viewed with JavaScript enabled"
419,why-simulation-becomes-slow-when-increasing-min-simulation-frame-rate,"I increased min_simulation_frame_rate and time_step_per_second from 30 to 400 to match the controller’s caculation rate, and the control system did work better, but I noticed that the simulation scene run much slower than real time physical environment,  which has bad effects on control. Can anyone help me with that?Sincerely,
LucyHi,
With this you are setting just the simulation frame rate to 400hz and telling the simulation to step just once. So the simulation wont match with the UI thread.
So you have two options:
a) set the min_simulation_frame_rate back to 30, this means several simulation steps will be done with the frequency 400hz to match up to 30hz UI frequency.
b) lock the UI thread to 400hz too, this can be done in edit->preferences->rendering
image1844×413 40.3 KBHope that helps, regards
AlesThis topic was automatically closed 14 days after the last reply. New replies are no longer allowed.Powered by Discourse, best viewed with JavaScript enabled"
420,the-difference-between-cpu-and-gpu-pipeline,"Hello,I hope this message finds you well. I recently encountered an issue while attempting to run a robot motion implementation with a ‘gpu’ pipeline. The implementation works perfectly fine when using the CPU, but when I switch the pipeline to ‘gpu’, the robot does not move. I’m writing to seek clarification regarding the potential differences between these two approaches.Could you please help me understand why this discrepancy might occur? I would greatly appreciate any insights or suggestions you can provide to help me resolve this issue. Below is the implementation I have been using:I eagerly look forward to your response and thank you in advance for your assistance.Best regards,
SMakolon385Hello!The previous issue has been resolved. It was simply due to calling the set_joint_positions function multiple times within a single loop, which resulted in overwriting the values.However, I have encountered another issue related to the behavior difference between the CPU pipeline and the GPU pipeline.
I am trying to recreate the Factory environment using the reference of the OmniIsaacGymEnvs Factory with a different robot. In the process of resetting the environment, when I use set_joint_positions to move the robot to its initial pose in the ArticulationView, the robot disappears inexplicably.
This issue does not occur when the pipeline is set to CPU, and the robot is properly initialized. However, using the CPU pipeline prevents me from simulating the SDF, which is necessary for replicating the Factory environment.After executing the following code, the robot disappears somewhere:Before calling it.

before966×587 110 KB

After calling it.

after966×587 70.6 KB
In this image, the robot and the table appear to overlap, so I initially thought that the robot might be getting displaced to an unintended location due to contact. However, even after removing the table, the same issue continues to occur.And here is the image when running with the CPU pipeline.
Before calling it.

before_cpu966×587 110 KB

After calling it.

after_cpu966×587 152 KB
Best regards,
SMakolon385Similar issues seem to have been reported for IsaacSim 2022.2.0 as well.Powered by Discourse, best viewed with JavaScript enabled"
421,bugs-in-parallel-environments-in-orbit-half-of-the-robots-lose-control,"I am creating a parallel environment for RL.
The problem is half of the robots act normally, while half of them lose control. Is there anything I can do about that?
(upload://tF485w0at7ofRclxYCJtSIe6LWL.py) (8.4 KB)I attached the USD
hsrb4s.usd (5.4 KB)
and the script used
hsr.py (8.4 KB)
lift_cfg.py (7.8 KB)
lift_env.py (24.0 KB)
(upload://tF485w0at7ofRclxYCJtSIe6LWL.py) (8.4 KB)
.
The scripts are actually the scripts from lift_env.py and lift_cfg.py from Orbit. I only changed the robot.Thank you!
Which Isaac Sim version are you using?Its okey, I have solved the problem, thank you.This topic was automatically closed 14 days after the last reply. New replies are no longer allowed.Powered by Discourse, best viewed with JavaScript enabled"
422,modulenotfounderror-no-module-named-omni-isaac-core-suddenly-appearred,"Hi,I suddenly got an below errors, though I can run my codes well just before.How can I fix it?I solved it by reinstalling 2022.2.1.This topic was automatically closed 14 days after the last reply. New replies are no longer allowed.Powered by Discourse, best viewed with JavaScript enabled"
423,export-lip-sync-animation-as-a-video-file-in-pythonic-way-automation-to-use-the-service-on-demand,"Hi, i would like to render the lip sync animation file on web. For this, i have used audio2face(used headless APIs) + blender to generate the lip sync animation and want to finally export the animation as an mp4 video on blender following the tutorial series Part 1 and 2 Omniverse Audio2Face and Blender | Part 2: Loading AI-Generated Lip Sync Clips - YouTube. But no luck in exporting the video output from Blender using “Render Animation”. Also checked for other options below:Can someone please guide me?Ref: How to export an audio2face as a video file?, Audio to face quick render option? - #3 by nic.tangheHello and welcome to the forums @snehareddypIs your animation fully functional in Blender? If so, are you planning to convert it into a video file for uploading to the web? or, do you intend to live stream the animation directly from Blender to the web?Or your animation is not functioning properly in Blender?Thanks for the reply Ehsan.
Yes, the animation is fully functional in the blender. The plan is to output a video file (preferably one with transparent background) to render it on the web.This issue seems more like a blender rendering issue. While it is possible to render things in Omniverse or Machinima, for your use Blender seems a better fit.Here are some tutorials I found on YouTube showing how to render animations in Blender:
How to Render Your 3d Animation to a Video File (Blender Tutorial) - YouTube
How to render animation as video in Blender 2.92 - YouTubeThank you. I have followed the steps in the vt videos to export a final video file. Im facing a few last minute issues with not able to see the emotions on the animationIn the a2f  - I generated the auto emotion keyframes in Auto Emotion. The emotions (and eye brow movements) are visible on mark’s face but not on the target skel(rain). The target skel only shows the lips movement and the eye blink is missing on the rain even with blink interval set to 3 seconds
Is this a limitation of the blender connector or can we render emotions(animate the brows and dynamic head movements) by adding any further steps?
Reference: Audio2Emotion Feature in Omniverse Audio2Face ~ Ai Facial Animation ~ Eyes, Skin, Tongue & Mouth - YouTubeThis is the final output file which doesnt show the emotions
Uploading: 0448-0733_out.webm…Hi @snehareddypAre you able to share your scene file? I’m not able to open the video file you’ve attached. Is it possible to change its format?Usually if eyes and brows don’t move, it means you’ve selected regular core instead of full face core. Can you confirm if this is set to full face?Powered by Discourse, best viewed with JavaScript enabled"
424,creating-a-scene-with-falling-objects-on-a-conveyor-through-a-script-node,"I want to create a script node that generates multiple falling objects on a conveyor. For instance, as a simulation starts running one box should be dropped from a certain height on a conveyor or ground for that matter and after a certain time period/delay another box need to be dropped from the same position. I am somehow not able simulate this action. I have tried numerous for looping attempts but it fails at saying “Cannot add object to the scene since its name is not unique”. Can anyone please share a sample working script node code.Below are the system specifications:Isaac Sim 2022.2.0 and Isaac Sim 2022.2.1 (tested on both)
Ubuntu - 20.04
CPU - Intel Core i7
Cores - 24
RAM - 32 GB
GeForce RTX 3060-Ti
VRAM - 16GB
Disk - 2TB SSDGood question.  I’d like to implement the exact same functionality.Hi @deveshkumar21398 - The error message you’re seeing is because you’re trying to add multiple objects with the same name to the scene. Each object in the scene must have a unique name.You can solve this by generating a unique name for each object. Here’s a simple example of how you can do this:from omni.isaac.dynamic_control import _dynamic_control
from omni.physx import _physxdc = _dynamic_control.acquire_dynamic_control_interface()
physx = _physx.acquire_physx_interface()for i in range(10):
# Generate a unique name for the box
box_name = f""box_{i}""This script will create 10 boxes, each with a unique name, and drop them from increasing heights every second. You can adjust the parameters to fit your specific use case.Powered by Discourse, best viewed with JavaScript enabled"
425,articulation-controller-works-in-standalone-python-script-but-not-in-extension,"Hello,I am currently working on implementing a robotics application in Omniverse. Initially, I started the development by creating a standalone script. However, now I want to integrate this functionality into an extension. Unfortunately, I’m encountering an issue where the rmpflow controller behaves unexpectedly in the extension.To provide a clearer understanding, I have recorded two videos. The first video demonstrates the application running as a standalone script:The second video showcases the same code integrated into an extension:For your convenience, here is the code for the standalone script:Additionally, here is the code for the main extension file. I based it on the bin filling Isaac example and incorporated my code into it:The only difference between the two approaches (except for the fact that the second version is executed as an extension) lies in the creation of the world instance and the world reset function.I’m unsure about the source of the problem. The controller is initialized in the exact same way and with the same configuration files. I would greatly appreciate any insights or suggestions regarding this issue.Thank you for your assistance.Kind regards,AxelI found the issue, the physics dt in the physics scene (240 tsps) was different than the physics dt of the motion policy (60), as set in:´´´
import omni.isaac.motion_generation as mg
self.articulation_rmp = mg.ArticulationMotionPolicy(robot_articulation, self.rmpflow, physics_dt)
´´´
Both need to have the same value, else rmpflow behaves like that. Interestingly, this inconsistency does not occur in the standalone version, despite having the same physics dt discrepancy.In my opinion it would be convenient to have the ability to change the physics dt without the need to reconfigure the motion policy. This would provide greater flexibility and convenience when adjusting the simulation settings.This topic was automatically closed 14 days after the last reply. New replies are no longer allowed.Powered by Discourse, best viewed with JavaScript enabled"
426,urdf-mesh-import-fails-in-latest-ros2-versions,"Hello,Latest ROS2 versions, require mesh file paths to start with file://, instead of package://, which no longer works (see Gazebo does not spawn mesh - ROS Answers: Open Source Q&A Forum).The urdf importer imports the filename with the file:// prefix, which is unsupported in Isaac sim.Output from terminal:Urdf importer should  trim this prefix.Hi - Sorry for the delay in the response. Let us know if you still having this issue with the latest Isaac Sim 2022.2.1 release.@rthaker Of course it still persists. Did anyone resolve / work on the bug on Nvidia’s side that you ask me if it still persists?Hi @omers - I checked internally. We should be able to fix it in the next release (around Aug timeframe)This topic was automatically closed 14 days after the last reply. New replies are no longer allowed.Powered by Discourse, best viewed with JavaScript enabled"
427,build-own-omniverse-connector-on-ios-macos,"Hi!
I would like to implement a new Omniverse Connector on iOS and macOS platforms. Are there any plans to make the necessary libraries (NVIDIA Omniverse Client Library) available on those platforms too? If I am not mistaken, currently, the library is limited to Windows / Linux development.
Thank you!Hello @geri!  Thank you for your interest!  We do have plans to develop for the iOS / macOS platform.  Right now, the demand for Windows / Linux development makes it higher priority.  I have added your request to the internal development ticket for the iOS / macOS platform development (for your reference OM-31163).Troubleshooting IssuesThis is great news. Thank you for your answer!Yo, I’m new here. Btw, may I know if Nvidia Omniverse are available for macbook?Not at the moment. Streaming is the only solution right now.Powered by Discourse, best viewed with JavaScript enabled"
428,omnivers-isaac-sim-replicator-palletizing-demo-py-on-the-webiner-by-andrei-haidu,"Hello,We’d like to run the demo on the below webiner by Andrei Haidu .Use Isaac Replicator to build production-quality synthetic datasets to train your deep learning perception models.Could you tell us where can we get those below python code?
Best regards,Hi there,here is the script from the webinar. The 2023.1.0 Isaac Sim release will include an updated version as well.Hi,Thank you very much for your kindness.
I really appreciate your help.I’d like to continue to check the behavior with your code.Best regards,Hi,Thank you very much for your kindness.
I really appreciate your help.I can run the demo with your code.
Thank you!Best regards,Powered by Discourse, best viewed with JavaScript enabled"
429,omniverse-white-paper,"Are there alternative sources for the John Peddie Omniverse Whitepaper linked here: Download NVIDIA Omniverse Whitepaper ? When I click the download button, I am just redirected to Nvidia’s website with no download initiated.Thanks!Hi, sorry for the wait, I checked in on this and that paper is out of date. Is there specific information you are looking for? I am happy to help fill in the gaps.A lot of the information covered in that doc has been expanded into this page 3D Design Collaboration with Omniverse Platform | NVIDIA. You will find many workflow videos as well as links to sessions from many production and engineering facilities.Thanks for getting back to me! I just wanted to use it as a reference for a broad overview of omniverse, but I’ve found other resources!This topic was automatically closed 14 days after the last reply. New replies are no longer allowed.Powered by Discourse, best viewed with JavaScript enabled"
430,audio2face2023-1-0-to-ue-get-stuck,"If the software is placed in the background or not in focus, it may get stuck.Thanks for reporting this @1757366814
We’ll investigate this.I can’t get Audio2Face to install. It appears to completely download according to green status bar in the top right of Omniverse launcher, but then it just gets stuck. I’ve tried several times and now trying an older Beta but has the same issue. Is there another way to get this?Audio2Face Stuck1374×585 104 KB@mp3rayne can you close out of the OV Launcher and provide the Launcher log: C:\Users\<UserName>\.nvidia-omniverse\logs\launcher.log (assuming you are on Windows)?It’s saying there is a “Buffer() is deprecated due to security and usability issues.
Please use the Buffer.alloc()”…Sounds too complicated for me. Will probably just wait till this is out of BetaUnderstood. I inquired because I was under the impression posting on the forum was to solicit help from the dev and community so you dont have to figure it all out yourself. If you have a change of mind, the support will be here.Regarding the message you got, I personally have seen it and hasn’t caused any installation issue. so, I think there are other reasons that are preventing you from installing the app from Launcher. And seeing the whole log will be the only way to see the whole big picture.Powered by Discourse, best viewed with JavaScript enabled"
431,how-to-add-surface-gripper,"Hello,
I’m trying to add a Surface Gripper to the fingertip link of the robot using the following code. However, I’m encountering the following error, and the gripper doesn’t behave as intended. The version of IsaacSim I’m using is 2022.2.1. Additionally, I’m trying to add the Surface Gripper to a parallelized learning environment based on OIGE.Furthermore, when I launch the simulation, the is_closed() function returns False , but then I get the following error, and it becomes True . I believe based on this post that the offset of the Surface Gripper might not be set correctly and it ends up being placed inside the hand. Is there a good way to debug the placement position of the Surface Gripper?I appreciate any assistance or insights you can provide. Thank you.According to the omni.isaac.examples.bin_filling.py script, I believe I can add a Suction Gripper similar to UR10e.However, I would like to use it in a parallelized learning environment similar to OIGE. Does the Surface Gripper support parallelization?Thank you.Hi @smakolon385 - Yes, the Surface Gripper in Isaac Sim does support parallelization. The Surface Gripper is implemented as an extension (omni.isaac.surface_gripper) and can be used in any environment, including those that are parallelized.However, it’s important to note that when using the Surface Gripper in a parallelized environment, each instance of the environment will have its own instance of the Surface Gripper. This means that the state of the Surface Gripper in one environment will not affect the state of the Surface Gripper in another environment.When using the Surface Gripper in a parallelized learning environment, you’ll need to ensure that the gripper’s state is properly initialized for each environment instance, and that any actions applied to the gripper are correctly applied in each environment instance.In the context of reinforcement learning, this means that the gripper’s state should be included in the environment’s state representation, and any actions applied to the gripper should be part of the action space. This will allow the reinforcement learning algorithm to learn how to control the gripper based on its state and the state of the environment.Hi, @rthaker - Thank you for your response. I will try using the Surface Gripper in a parallel environment.This topic was automatically closed 14 days after the last reply. New replies are no longer allowed.Powered by Discourse, best viewed with JavaScript enabled"
432,install-the-launcher-on-macos,"Is it part of your plans to offer the platform for MacOS? ThanksPowered by Discourse, best viewed with JavaScript enabled"
433,chicago-omniverse-developer-for-converting-my-script-into-a-youtube-video,"Hi there
I am retired from Palo Alto and living in downtown Chicago (gold coast).  I worked with an expert ( Syd Field) to develop a script for a movie. That didn’t go well so Syd helped me pivot to a pilot script for a netflix/Amazon pitch. I didn’t have the time to progress this as I sold a couple of startups and this took a back burner. I recently found the script again and want to progress it.  I can fund it and have my notes from Syd as to how to progress.  I would REALLY like to explore converting my script into workproduct completely generated by omniverse and maybe GPT. I am NOT a fan of remote (never have been) so this certainly is a face to face.  Happy to chat with anyone interested.1152dent@gmail.comHi! you can also post on our Discord server, you can find a lot of Omniverse users there as well. The “job-board” section in the Community category is ideal for this sort of request.Nvidia Omniverse Discord ServerPowered by Discourse, best viewed with JavaScript enabled"
434,livestream-getting-started-convai-omniverse-wed-july-19-11am-pst,"
nvidia-ace-for-games1920×1080 199 KB
Join the Convai team as we explore how to add AI characters to your virtual worlds in NVIDIA Omniverse!Add to your own calendar: AddEventWe will be broadcasting to:Join the Convai team as we explore how to add AI characters to your virtual worlds in NVIDIA Omniverse. As seen in the demo featured in the NVIDIA Keynote at...
All upcoming livestreams: AddEventPowered by Discourse, best viewed with JavaScript enabled"
435,publishing-rotation-static-transform,"Hello ,I want to publish a static transform, rotated a certain amount from a link.
I tried using ROS2PublishRawTransformTree , not sure how to set the rotation port.
What node can I use to generate a fixed rotation ?SandeepHello All,
Just to clarify , I want to achieve the equivalent of the following command in an action graph
“ros2 run tf2_ros static_transform_publisher 0 0 0 0 0 -1.571 Camera_IMU_1 Camera_IMU_1_rot”
this creates a frame “Camera_IMU_1_rot” which is rotated -90 deg on the X axis from Camera_IMU_1.Powered by Discourse, best viewed with JavaScript enabled"
436,how-to-transfer-the-isaac-sim-cortex-work-from-isacc-sim-2022-1-1-to-isacc-sim-2022-2-0,"Hi, my group and I want to follow up on the latest version of Isaac Sim (2022.2.0).We have a working version of our code in Isacc Sim 2022.1.1, but this code cannot run in Isacc Sim 2022.2.0. We have noticed the changes made to the Isaac Sim Cortex Libraries in Isacc Sim 2022.2.0. However, the method to launch the USD File for our simulation environment seems to have changed significantly from Isacc Sim 2022.1.1 to 2022.2.0. Can we still launch our USD File directly, or must we write the code to create the simulation environment?Could you please give us some suggestions for transferring our works from Isacc Sim 2022.1.1 to Isacc Sim 2022.2.0 more easily?Hi @Travis_Zhang - Someone from our team will review and provide answer to your question.Hi @ rthaker
Thank you so much for your reply! We are looking forward to hearing from you all.Hello, any updates?Hi Travis,Sorry things are changing. Cortex is an experimental API at the moment and we’re actively developing to improve it. The tutorials give an overview of the latest APIs and step through the examples in detail:https://docs.omniverse.nvidia.com/app_isaacsim/app_isaacsim/tutorial_cortex_1_overview.htmlYou should be able to load a scene from a USD file as before and control it using the Cortex pipeline. See the UR10 bin stacking demo for an example:https://docs.omniverse.nvidia.com/app_isaacsim/app_isaacsim/tutorial_cortex_5_ur10_bin_stacking.htmlCurrently, only standalone python apps are supported (you’d load the USD file from the standalone python app), but we’ll have support for the extensions workflow going out soon. We’ve integrated the Cortex pipeline more natively to the core API since the initial release so it’s easier and more flexible to work with to develop apps.Powered by Discourse, best viewed with JavaScript enabled"
437,nucleuss-status-is-stopped,"Nucleus’s status is stopped in http://localhost:3080/

image1180×918 142 KB
log:This was an issue we had in 2022.4 release. Please update to nucleus 2023.1.0.Powered by Discourse, best viewed with JavaScript enabled"
438,how-to-get-volume-of-object-in-scene,"Apologies since I am a new to Omniverse, but is there a way to directly return/calculate the volume of an object through a query?Looking through the Pixar USD library in addition to the Kit API, I haven’t found any success. I know that you could use a method that gets the volume of a bounding box of the object, but is not necessarily a good assumption.If there is a way, any help would be appreciated!I dHi @areddy74. I don’t think there is an API for that. You’d have to either include another library that does that or calculate it yourself at this time.Powered by Discourse, best viewed with JavaScript enabled"
439,build-and-package-errors-as-per-kit-app-template,"Hi there,we are trying to build and package kit APP, on ubuntu 22.04, as per template GitHub - NVIDIA-Omniverse/kit-app-template: Omniverse Kit App Template, but got two problems:
•	Build miss some files (repo.sh: not found), but seems success finally
•	Package fail due to missing PACKAGE-LICENSES fileCloud you give some clues/ideas that may cause these?
Thank you.Logs as below:
darien@universe:~/omniverse/kit-app-template$ ./build.shFetching all dependencies.
Before Pull Commands Step. Running ‘/home/darien/omniverse/kit-app-template/repo.sh link_app’…Found following Omniverse Apps:
0: cache-2022.2.0 (name: Cache, path: /home/darien/.local/share/ov/pkg/cache-2022.2.0)
1: nucleus-workstation-2022.4.2 (name: Nucleus Workstation, path: /home/darien/.local/share/ov/pkg/nucleus-workstation-2022.4.2)
2: isaac_sim-2022.2.1 (name: Isaac Sim, path: /home/darien/.local/share/ov/pkg/isaac_sim-2022.2.1)
3: create-2022.3.3 (name: Create, path: /home/darien/.local/share/ov/pkg/create-2022.3.3)
4: code-2022.3.3 (name: Code, path: /home/darien/.local/share/ov/pkg/code-2022.3.3)Selected app: create-2022.3.3 (name: Create, path: /home/darien/.local/share/ov/pkg/create-2022.3.3)
Creating a link ‘/home/darien/omniverse/kit-app-template/_build/linux-x86_64/release/baseapp’ → ‘/home/darien/.local/share/ov/pkg/create-2022.3.3’
Creating a link ‘/home/darien/omniverse/kit-app-template/_build/linux-x86_64/release/kit’ → ‘/home/darien/.local/share/ov/pkg/create-2022.3.3/kit’
Successfully linked an app: create-2022.3.3 (name: Create, path: /home/darien/.local/share/ov/pkg/create-2022.3.3)!After Pull Commands Step. Running ‘/home/darien/omniverse/kit-app-template/repo.sh precache_exts -c release’…
[2023-06-20 15:30:34,646][WARNING][omni.repo.build.pippull] Skipping pip file (doesn’t exist): /home/darien/omniverse/kit-app-template/deps/pip.toml
sh: 1: .//home/darien/omniverse/kit-app-template/repo.sh: not found
sh: 1: .//home/darien/omniverse/kit-app-template/repo.sh: not found
Building configurations…
Running action ‘gmake2’…
Generated _compiler/gmake2/Makefile…
Generated _build/linux-x86_64/debug/compile_commands.json…
Generated _build/linux-aarch64/debug/compile_commands.json…
Generated _build/linux-x86_64/release/compile_commands.json…
Generated _build/linux-aarch64/release/compile_commands.json…
Done (12ms).
Stage Files Step. doing file copy and folder linking.
File doesn’t exist: /home/darien/omniverse/kit-app-template/prebuild.toml. Skipping.
Processing file: /home/darien/omniverse/kit-app-template/_build/generated/prebuild.toml
VS Code setup. Writing: /home/darien/omniverse/kit-app-template/_build/linux-x86_64/release/setup_python_env.sh
VS Code setup. Writing: /home/darien/omniverse/kit-app-template/.vscode/settings.json
*** Building release ***
make: Nothing to be done for ‘all’.
BUILD SUCCEEDED (Took 0.71 seconds)darien@universe:~/omniverse/kit-app-template$ ./package.sh
Preparing files to package…
[2023-06-20 15:32:29,659][ERROR][omni.repo.man.fileutils] Error: Path does not exist: /home/darien/omniverse/kit-app-template/_build/linux-x86_64/release/PACKAGE-LICENSES
NoneType: None
Traceback (most recent call last):
File “tools/repoman/repoman.py”, line 28, in 
omni.repo.man.main(REPO_ROOT)
File “/home/darien/omniverse/kit-app-template/_repo/deps/repo_man/omni/repo/man/entry.py”, line 369, in main
run_tool()
File “/home/darien/omniverse/kit-app-template/_repo/deps/repo_man/omni/repo/man/nvteamcity.py”, line 145, in wrapper
result = func(*args, **kwargs)
File “/home/darien/omniverse/kit-app-template/_repo/deps/repo_man/omni/repo/man/entry.py”, line 368, in run_tool
raise raised
File “/home/darien/omniverse/kit-app-template/_repo/deps/repo_man/omni/repo/man/entry.py”, line 350, in run_tool
options.func(options)
File “/home/darien/omniverse/kit-app-template/_repo/deps/repo_man/omni/repo/man/entry.py”, line 201, in 
func=lambda args, func=func, config=merged_tool_config: func(args, config),
File “/home/darien/omniverse/kit-app-template/_repo/deps/repo_package/omni/repo/package/init.py”, line 998, in run_repo_tool
package(pkg_desc, configs)
File “/home/darien/omniverse/kit-app-template/_repo/deps/repo_package/omni/repo/package/init.py”, line 518, in package
src, dst, exclude_path_patterns=package_desc.files_exclude, symbol_check=package_desc.symbol_check, strip_to_first_wildcard=strip_to_first_wildcard
File “/home/darien/omniverse/kit-app-template/_repo/deps/repo_man/omni/repo/man/fileutils.py”, line 332, in copy_files
_do_copy_item(src, dst, allowed_paths)
File “/home/darien/omniverse/kit-app-template/_repo/deps/repo_man/omni/repo/man/fileutils.py”, line 100, in _do_copy_item
raise RuntimeError(“Path does not exist: %s” % src_path)
RuntimeError: Path does not exist: /home/darien/omniverse/kit-app-template/_build/linux-x86_64/release/PACKAGE-LICENSESHi @darien.wei. We have a new version of the app template coming out soon. This may end up being a non-issue then, but I’ll get the team to look at this just in case.Hi Mati,Thanks for the reply.
Is any update on this issue? and when will you deliver the new template?Regards,
DarienHey @mati-nvidia ,thanks for all the great videos about Kit App Template.Unfortunately build.bat doesn’t seem to find my installed and running Omniverse USD Composer 2023.1.1.
It gives me these errors:Before Pull Commands Step. Running ‘D:/Github/Nvidia/kit-app-template/repo.bat link_app’… [2023-07-19 11:57:41,177][WARNING][urllib3.connectionpool] Retrying (Retry(total=2, connect=None, read=None, redirect=None, status=None)) after connection broken by ‘NewConnectionError(’<urllib3.connection.HTTPConnection object at 0x000001671165FC88>: Failed to establish a new connection: [WinError 10061] No connection could be made because the target machine actively refused it’)‘: /components [2023-07-19 11:57:43,229][WARNING][urllib3.connectionpool] Retrying (Retry(total=1, connect=None, read=None, redirect=None, status=None)) after connection broken by ‘NewConnectionError(’<urllib3.connection.HTTPConnection object at 0x0000016711672448>: Failed to establish a new connection: [WinError 10061] No connection could be made because the target machine actively refused it’)‘: /components [2023-07-19 11:57:45,281][WARNING][urllib3.connectionpool] Retrying (Retry(total=0, connect=None, read=None, redirect=None, status=None)) after connection broken by ‘NewConnectionError(’<urllib3.connection.HTTPConnection object at 0x0000016711672B48>: Failed to establish a new connection: [WinError 10061] No connection could be made because the target machine actively refused it’)‘: /components Failed retrieving apps from an Omniverse Launcher, maybe it is not installed? Error: HTTPConnectionPool(host=‘127.0.0.1’, port=33480): Max retries exceeded with url: /components (Caused by NewConnectionError(’<urllib3.connection.HTTPConnection object at 0x0000016711677288>: Failed to establish a new connection: [WinError 10061] No connection could be made because the target machine actively refused it’))In repo.toml, is the name of the app still “create” since the change of the foundation apps names?
I’ve tried my luck randomly with other strings like ‘composer’, or ‘usd_composer’, etc. But No luck.I’m stuck at the very first step! I’ve also tried disabling entirely Windows’ firewall, still no luck. I’m a Tech Artist not a Dev, so, I’m sure I’m missing something obvious out ; ) Any idea? Cheers!I have also been trying to use the Kit app template, but it is not working with USD Composer (can’t find Omniverse extensions in a custom extension). It also seems that Omniverse Launcher v1.8.11 broke the configuration. It cannot find the installed apps (v1.8.7 was working fine). I raised an issue on GitHub:USD Composer · Issue #5 · NVIDIA-Omniverse/kit-app-template (github.com)@_stef @dvelazco Please hang tight. The new app template is coming and will work differently. It will no longer require a “baseapp” to link against. If you want to continue working with the current app template, please stick with the Kit 104.2-based apps (e.g. USD Composer 2022.3) for now.@dvelazco Let me know if you’re still having issues with Launcher 1.8.11, but linking against Composer 2022.3. This will be a non-issue with the new app template, but curious if something did change in the Launcher.Thanks for the update. Looking forward to the updated template!Yes, I have not been able to build the app since Launcher 1.8.11. You can reproduce this by simply cloning the kit-app-template and running the build.bat script (while having Launcher 1.8.11 installed). My coworker did not do the update and is able to build our custom app as usual.Hello @mati-nvidia, do you know of a way to install an earlier version of Omniverse Launcher? I am still having issues with v1.8.11. Thanks in advanced.Powered by Discourse, best viewed with JavaScript enabled"
440,jittering-issues-pivot-tool-issues-and-objects-w-colliders-phasing-into-each-other,"I am working with my institute on implementing Omniverse into a competitive robot design course, but my team and I have been facing issues when trying to get our demo scene to function properly in Omniverse Create. The issues are viewable in this video: Omniverse Create Issues Encountered - Georgia Tech ME 2110 Demo - YouTubeThe simplified version of the demo is to have the robot flip a ring onto the center pole and drop balls into the rotating hopper. As viewable in the video, the ring flipping mechanism is facing the most problems since it uses a joint (unlike the static ball dropping mechanism). The demo’s main goal is to demonstrate Omniverse’s physics capabilities, so complete robot control is not important (hence why Isaac Sim is not being used for this robot demo).Preferably, I am looking for solutions that are easy to follow so they can be used by future students who encounter the same issues (although it would be ideal if the issues could be resolved entirely in a future update). If the solutions (or future version solutions) cause too much user friction, then it would not be worthwhile for my institute to incorporate Omniverse into its curricula. I am ideally looking for a straightforward solution, but any help/guidance would be appreciated. The .USD of the demo scene is attached in the reply, but let me know if anything else is needed; thank you in advance for your assistance.ME2110Demo.usd (59.0 MB)@WendyGram who would be best to reach out to for this? Thank you!Trying this in Kit 105 (newest beta release) lets reconnect by friday 7/14Powered by Discourse, best viewed with JavaScript enabled"
441,is-it-possible-to-release-standalone-isaac-sim-library,"Hi,As far as I understand, the current release of Isaac Sim comes with its own bundle of python environment. I am wondering, if it is possible, to have a standalone library release of isaac gym in the future? It would be better if it can be published on PyPi.The benefits are:I am pretty sure this will make this nice library more popular among researchers and engineers.Thanks a lot!Hi - Sorry for the delay in the response. Let us know if you still having this issue/question with the latest Isaac Sim 2022.2.1 release.Hi, I think it still only supports up to python 3.8. Can you release a version that supports at least python 3.10?Hi @breakds - The upcoming Isaac Sim release (Aug/Sep) will support Python 3.10Powered by Discourse, best viewed with JavaScript enabled"
442,cant-download-any-apps-from-omniverse,"Can’t download any apps from omniverse, download will complete and stuck at 100%, tried on 3 different work station no luck also waited for 4 hours, am I missing something ? (nucleus installed)launcher.log (720.0 KB)Here’s an image

image1552×847 132 KB
System specs-
RTX 3090
AMD ryzen 5700x
windows 10 (1904) and also tried on win 11Hi @c6pros. I’ve moved this over to the launcher forum and reached out to the devs for you.Powered by Discourse, best viewed with JavaScript enabled"
443,video-textures,"Hi all,Does Omniverse Create support video textures? And if so what formats are supported?Greetings from Amsterdam, SanderIts on our list to support. One hack is to use UDIM textures and connect to time variable in MDL to advance the frames.We will make this easier when the material editor is in (next couple of months)Thank you for the info. Can’t wait for this feature it would make it the cherry on top for me. Unfortunately I am new to MDL and never had to use Udim before so that sounds like a no go for now.
Love the program so far btw, spectacular quality and performance.Thank you, this is just the beginning, so much in the works. Thank you for being a part of beta and helping shape the product.Hi Sander, thanks for the kind words!Until Omniverse fully supports video textures, we have a basic MDL material which can animate UDIM textures, creating the illusion of a video texture. You just have to assign the MDL material to a surface and point it to a folder containing a sequence of animated images named with consecutive numbers (e.g. frame001.png, frame002.png, …).You can see an example of this UDIM texture MDL in the video below:Let us know if this sounds interesting to you, and we can share the MDL file here.Ah that looks awesome, exactly what I need at the moment. Been using unreal for some real-time action but programs like that always break for me when you get into the nitty gritty like transparency/reflection etc… :/ Omniverse seems to have none of those drawbacks.Anyways, so the  marker is just a counter? A file would be much appreciated.
My interface looks a little bit different from yours. Can’t find a details tab for the life of me (no audio settings and utilities either. Are those plugins?).
I can’t yet wrap my head around the MDL shader. USD preview surface = MDL? Did you use omnigraph? (supercool btw coming from Houdini)So long story short, yes file please :)Sorry, another question. Do I need substance or similar tool to create a MDL or is this also possible inside Create? What is the easiest way to do that.That video is a bit outdated.  The Details panel is now called “Property”:

image1265×909 103 KB
The Audio panel is gone, because it’ now integrated into “Property”.  It will do the right thing when dealing with audio prims in USD.  Utilities were an old extension that have been integrated into other parts of the tool as well.I can’t yet wrap my head around the MDL shader. USD preview surface = MDL? Did you use omnigraph? (supercool btw coming from Houdini)USDPreviewSurface is an implementation of the USD standard material in MDL.  MDL is a general language that allows you to write any arbitrary material.  You can write them by hand or use a tool like Substance Designer to author new MDLs.  We are also close to adding  graph-based MDL.  Coming very soon.Hi SanderWelcome to Omniverse!I’m attaching the MDL material I’ve been using for our texture animations. You should be able to just drag and drop this into your stage and it will appear as a material. Haven’t tested it with the official beta build yet so let me know if there are any problems and I’ll make sure we get it to work for you.AnimUdimMaterial.mdl (3.5 KB)As mentioned before, this is a temporary solution. The renderer will load all UDIMs you’re referencing so this can easily exceed the maximum number of textures allowed or simply exceed your device memory. In this setup I believe the maximum number of frames possible are 640. I don’t remember the maximum number of texture resources. It might be a bit lower than that.Anyway, I hope this helps and let us know how it works out for you.Happy holidays!Best,
PatrikThank you so much for the workaround. It’s not going to cut it for now. 640 is not nearly 55000 haha. That will never fit in memory I guess. I hoped this to be a great step up from Eevee which just plays very long mp4’s no problem. Of course when you make great software somebody comes around with a very specific use case to pick it apart… sorry.But makes me very happy in showing what is possible. Can’t wait for the update :)Did a little render test and the performance is spectacular btw. This combined with Houdini Solaris is just a dream come true. Keep up the good work!Omniverse is a platform for loads of connectors between apps , from Unreal to Blender to Adobe tools orking in 3D. It wants NVIDIA’s (or AMD’s!) latest RTX GPUs. Now is another time to curse the names of whoever is buying the damned things up to resell them above cost or mine crypto or just build some really weird Microsoft Flight Simulator dungeon.I think it’s important to have image sequence support for textures. Also for animated HDR textures in a dome light. Looking forward!I had my UDIM animations working but not sure why they just don’t work like the example.usd provided. The example seems to work - how much does the resolution effect the animation? Does the UDIM length matter? Been attemping to solve this for over an hour. Thanks!

Q710×53 7.38 KB
@garretthoyos I try to help you here if I can :)See here:
The error you got in your screenshot is not the issue, I got it too and still it works.Most often I messed up the animated texture, is the odd naming thing.
Just use the example scene and keep the reference path of the “albedo map” as it is.Then just delete the example image 1000 png´s and put your own there with exaclty the same naming. Use free “bulk rename utility” to rename your png´s.So the texture to look for albedo map is always the:

image750×41 12.7 KB
Hope this helps…
image1737×1527 113 KB
Also, if your starting frame number of your timeline is something larger than 0, the animated texture jumps to there automatically. See here it is 434 so we have 1435 at animated seq ( on mobile phone screen ).Hello @garretthoyos!  Devs got back to me about your question:The warning message indicates he has more UDIM tiles than we support for that UDIM format. The limit of 640 is correct in this specific case.The warning message Kit gave is misleading. The UDIM file name format he’s using only supports a width of 10. Hence the max number of tiles is 10 * 64 = 640Internal Ticket to fix warning message: OM-37609The Devs would like to know if you remember what you were trying to do when you recieved the error message?haha! Just messed up my own current anim tex by renaming the png´simg.should be Img.maximum number of frames possible are 640I just saw this…
The custom solution I am talking here has 1000 frames.
It is here:Hope I did not waisted your time…Okay so I opened up the example.usd file (which I no longer see on this thread) - and I can see the UDIMs running okay on a flat plane (takes a bit to get the sequencer to get it working). Just updated to the newest version of omnivers create 2021.3.5 ---- currently using 2021.3.2  so maybe it will fix this problem if I update?
CONSOLE attached
Anyways here’s the error I get. kit_20210915_133247.log (381.9 KB)EDIT/UDPATE (even before posting this)
It now works… Not really sure what I did tbh. I had a jpg sequence and a png sequence in the same folder - I deleted the jpgs - perhaps that did the trick . my file’s are named fractal.1001.png and the mtl diffuse channel is named fractal..png – its working. :) thanks @pekka.varis and @WendyGramany idea how to make it loop?Powered by Discourse, best viewed with JavaScript enabled"
444,how-to-interface-usd-compser-and-legacy-system,"I want to create a Data Center digital twin with USD Composer.
And receive server status from the legacy system, and display it on the server.
As an interface method, we want to use websocket and restful http.
How can i implement it? Where can I find documentation or examples?Powered by Discourse, best viewed with JavaScript enabled"
445,sensors-tensorization-in-omniisaacgymenvs,"HI.I am trying to train my agent using DRL with observation data, such as RGB-D camera, lidar, etc…However, I could not find any example using tensorization of sensor data in OmniIsaacGymEnvs except for force data.Meanwhile, I found an answer from IsaacSim team of NVIDIA, and they said they are developing that function at that time (2 Nov 2022). Post linkIs it still being developed ?If it its, is there any way to accelerate DRL using sensor data, such as RGB-D, lidar, or point cloud data in OmniIsaacGymEnvs ?Thank you in advance!Powered by Discourse, best viewed with JavaScript enabled"
446,how-do-you-set-the-current-frame,"I know how to get the current frameHow do I set the current frame?ThanksPowered by Discourse, best viewed with JavaScript enabled"
447,ai-usage,"I’m wondering, are simple users able to use the AI?
If so, how?
Can we use it to create lifelike characters and creatures?
Are we able to give it a diagram and image and it produce a character?
Can it make anatomies of creatures? Such as for hunting games?
etcHi, Although 3D AI tools have come a long way they aren’t quite at the level you are looking for yet. Image generation tools for 2D images are quite photoreal at this point but AI-generated 3D models are still in the early stages.Here is an example of an AI model generator for animals available inside Omnivers today, as you see it’s a great example of where we are headed but not quite “game ready”. Omniverse Animal ExplorerFor characters, AI is being used to drive animation and provide interactive discussions as well as generate voices but there aren’t many tools yet to generate actual meshes. You can however use capture tools to generate meshes and animate them using AI.There are interesting tools that use text based AI to generate characters: Tafi - AI text to 3D previewThere are great tools to generate custom characters that can be imported into Omniverse like Reallusion iClone and Character Creator
We just did a great live stream on character creation and animation which you can watch here:  Designing 3D Characters With OpenUSD and Reallusion | Omniverse Live - YouTubeWe also have a great session on bringing characters to life using AI coming soon: Bringing NPC Characters to Life With Generative AI and NVIDIA ACE for Games | Community Stream - YouTubeThe great news is that any interesting tools you find out there to generate 3D models with AI will most likely be able to export to Omniverse where you can assemble and extend toolsets to your needs. Omniverse is a great platform to help build pipelines connecting 3rd party tools and enhancing it with your own extensions.I suppose what I’m looking for doesn’t exist yet. I’d love to know when it does.The AI would need to be able to take various 2D anatomy … blueprints? Charts? Whatever the word is… Picture? Whatever…
It would take those and scans of what the creature looks like and it would make a creature with anatomy.
Another tool, like a slider, with X, Y, Z, and rotation based, would allow to see the internals of the creature without damaging the mesh. Slide it to hide part of the creature, then slide it back to reveal the creature.
I’ve seen some models like that but I guess those took at least a few weeks to make by hand. I wish there was an AI that could take those inputs (anatomies, physiologies, looks) and make creatures from them.
That’d be good for hunting games at least. Better than current ones anyways. More realistic.And something for way later, interactiveness between meshes so they aren’t ignoring each other and flopping all over the place, for things like Fighting games. Make collisions more realistic. What a dream.Powered by Discourse, best viewed with JavaScript enabled"
448,tung-gum-teeth-on-non-human,"I have good result with non human mesh heads, cartoon and animals but have problems how to adjust tung and gum/teeth animation.
I going for more human like movments on mouth and try to have most lip movment in the front of long mouths.I would like to have some advice on making usable tung and gum teeth animation from non human meshes (cartoon and animals head) where the head shape have big difference to the human head. long noses, long tung, visible gum and teeth on the side?Is the new calculated pivot point for jaw, acesseible somewhere on result mesh and can i change it in a postprocess(maybe only for tung and gum/teeth)?I also trying different scenarios when gum and teeth is part of headmesh (not separate head, gum and teeth) and not have a2f  deform the gum and teeth but still have the lips move full and have the interpolation happen between inside lips and a bit up on outside gum and have the rest of gum and teeth stiff. Is there some way to weight paint attributes that a2f can pick up and blend mesh/point movments between affected and non affected mesh/point.I can change this after in the dcc,  in a pipline. But I would rather solve things directly in a2f in a post process and export the usd and blendchannels directly without doing manual work in dcc afterwards!Somtimes the mesh comes in reversed when imported, no problem i just revers it in dcc and reimport but havent figured why it happens randomly, I will try to track it down next time it happens (solaris houdini).Currently there are some simple post-processing options for changing tongue and teeth animation you can try.

Screenshot_13691×1244 56.9 KB
Is the new calculated pivot point for jaw, acesseible somewhere on result mesh and can i change it in a postprocess(maybe only for tung and gum/teeth)?We’ll discuss this internally. Thanks for bringing it up.scenarios when gum and teeth is part of headmesh (not separate head, gum and teeth) and not have a2f deform the gum and teethAs of now, A2F expects teeth and gum to be separate and we can’t guarantee it to work properly otherwise.reversed when importedI’m not sure if A2F touches surface normals. To make sure this is not happening in A2F, can you try and check normals in another DCC?This topic was automatically closed 14 days after the last reply. New replies are no longer allowed.Powered by Discourse, best viewed with JavaScript enabled"
449,unclear-unit-update-simulation-elapsed-time,"Hi everyone,are these parameters in seconds or in milliseconds? Its not clear for me.https://docs.omniverse.nvidia.com/kit/docs/omni_physics/latest/source/extensions/omni.physx/docs/index.html#omni.physx.bindings._physx.PhysX.update_simulationimage1351×648 53.1 KBHi,
these are in seconds, will update the doc, good point.Btw if you are looking for a way how to completely control physics simulation stepping independently you can use physx_simulation_interface instead, the full loop would look like this:Thanks for reporting.
Regards,
AlesHello,Thank you for the clarification.I do indeed need a way to control the physics steps.Do I always have to call the attach_stage and detach_stage methods? What happens without them?And do I need to configure the PhysicsScene component in a certain way for this to work?Powered by Discourse, best viewed with JavaScript enabled"
450,streaming-a2f-from-within-machinima,"I have a CC4 character transferred and working with the streaming player in A2F. I’d like to use that setup within Machinima to add additional animations etc. I see tutorials on how to export USD cache of the default player and bring those into Machinima, but I don’t see any documentation on how to use the streaming A2F player in Machinima. Is that possible?Powered by Discourse, best viewed with JavaScript enabled"
451,share-your-omniverse-creations-from-starttofinish,"Images courtesy of JDizzle made using Adobe Photoshop, Substance Painter and Designer, Unreal Engine, USD Composer, and Quixel.1920×1080 156 KBEvery great piece of art starts somewhere. We want to see you go from #StartToFinish.No matter what art you like to create, we want to see it as a part of our new #StartToFinish community challenge, running through Thursday, August 31.Those who own an NVIDIA Studio laptop, or an RTX card, can use the NVIDIA Omniverse platform in their workflow — and share their progress images on social media using the hashtag #StartToFinish — for a chance to be featured on NVIDIA Omniverse channels (Twitter, LinkedIn, Instagram, Threads) as well as NVIDIA Studio’s (Twitter, Facebook, Instagram, Threads).For questions, reach out on the Omniverse Discord server or forums.Featured images courtesy of JDizzle made using Adobe Photoshop, Substance Painter and Designer, Unreal Engine, USD Composer, and Quixel.Powered by Discourse, best viewed with JavaScript enabled"
452,launcher-unable-to-download-gdpr-agreements,"HelloI am trying to install Omniverse, and as soon I start the launcher and log in in to the account it gives the massege:“unable to download GDPR agreements and EULA. You may need to inspect your internet connection and restart the launcher”
image1920×1081 40.2 KB
I have tried to run a launcher (as an administrator) on 2 diffirent laptops, and with to diffierent internet connections, with firewall both on and off, but the issue remains.What should I do?CPU: 11th Gen Intel(R) Core™ i9-11950H @ 2.60GHz   2.61 GHz
GPU: NVidia RTX A5000 for laptopsHello @antonradzevitch!  Unfortunately this issue is with Amazon AWS in relation to recent US export laws and sanctions programs which prevents services in affected regions. This could mean that a part of your region communication is going through blocked servers.Will NVidia do something about it? Or is it Ok for NVidia?FIXED ITI had the same problem. Fixed it by changing time-zone of my computer from (GMT+5:30) to (GMT-12). Then RESTARTED my computer. Launched the Omniverse Launcher and it worked!After installing Cache and Nucleus, I changed time-zone to the correct one. Restarted my computer again and it works smoothly.Powered by Discourse, best viewed with JavaScript enabled"
453,enable-cad-converter-extension,"Hello,I found this link about a CAD converter extension: CAD Converter — extensions latest documentationI want to use that extension because I want to import a NX - Unigraphics Files (*.prt) file. It says in the link that you just need to activate it. But the problem is I can’t find it anywhere. Does it still exist and if it does how do I enable it?Thanks in advance.p.s. I need to do this because I can’t import step 214 and 242, isaac sim crashes if I do that.I found out that it is available on Windows but not on Linux. Is it possible to do it also on Linux?Powered by Discourse, best viewed with JavaScript enabled"
454,nucleus-server-is-not-working-at-localhost-ubuntu,"After installation both cache and nucleus server from omniverse launcher (following the current instruction), I am facing the issue that I can’t reach localhost. In the launcher I see:
Screenshot from 2023-07-27 16-03-571118×546 44.9 KBWhen I try to open settings through the hamburger menu, I can’t open the page in the browser and settings are unreachable:
Screenshot from 2023-07-27 16-03-401522×418 61.7 KBDuring the installation the firewall and VPN were inactive. The system is Ubuntu 20.04 and I installed the newest versions of each software.Interestingly enough, when I open Isaac Sim and log into the Nucleus Server in my organization, then everything is working as intended, but localhost still remain unreachable.After the installation I used Cleanup Tool and install it again, but didn’t help.Can you please validate all the services, especially auth and discovery, are running on your localhost?
You get to this from the hamburger menu->settings.As I wrote.
If I try to open settings (from hamburger menu) I receive the message:
Screenshot from 2023-07-27 16-03-401522×418 61.7 KBWhich means that I cannot open the settings using hamburger menu.Is there any other way to validate this?just curious, do you get the same message using Chrome or Edge?Yes.The same for all browsers:
Screenshot from 2023-07-28 09-57-56777×449 16.8 KB
Screenshot from 2023-07-28 10-00-541087×847 38.2 KBdo you happen to have a print out of the terminal/console/log?Launching an omniverse launcher from the terminal results in no logs, however I have something in ~/.nvidia-omniverse/logs. The content of this folder is in the attachment:
logs.zip (289.4 KB)just an observation - I am seeing a “Failed to load GDPR” error that results in a Http error 403 but not sure if it’s relatedand if it’s the case, I found this post from two years ago that may be relevant - Omniverse Launcher cannot be launched - #4 by rvinobhaIf you’re hitting this error, it’s likely your port is blocked somehow.
You’ll need to try and open the ports Nucleus needs.
It’s also possible Nucleus didn’t install correctly.
You can try uninstalling, then reinstalling and rebooting to see if that works as well.It turns out that antivirus blocked it.Powered by Discourse, best viewed with JavaScript enabled"
455,error-when-setting-up-blendshape-solver,"Hi there. I have completed the skin mesh fitting and transferred the face animations from mark to my face mesh successfully. i then used ‘blendshape generation’ and exported it to create a _usdSkel.usd file. I then imported this file into my project. I then Asigned the input anim mesh(the mesh which i transferred onto)  and the Blenshape mesh(the one i just imported). But when i click ‘setup blendShape solve’ I get this error: 2023-07-18 15:44:38  [Warning] [omni.deform.shared.scripts.utils] Could not find the output point attribute on Node(“/World/LazyGraph/chomper_low_001mesh_020__Audio2Face_EX_result_Visualizer”).
Any help would be very much appreciated. Thanks.I just tried following this tutorial. So I was using the two versions of mark which were included with the software, one with the animation and one with the blend shapes and when i tried to set up the blendShape solver i got this message:2023-07-19 15:43:45  [Warning] [omni.usd] Warning (secondary thread): in ProcessPropertyChange at line 519 of C:\b\w\ca6c508eae419cf8\USD\pxr\usdImaging\usdSkelImaging\skeletonAdapter.cpp – Unhandled ProcessPropertyChange callback for cachePath </World/male_bs_46/joint1/bs_anim> in UsdSkelImagingSkelAdapter.But the animation is transferred. When I do the same using my own models I get the same error but the animation is not transferred. Anyone have any idea why this could be? Thanks.Ive just tried the same process using the Mark model instead of my own. I imported it into blender and deleted all the shapekeys and then exported the USD(after setting up the face as a dynamic object). I then openeed the generated project om audio2face and did a character transfer from the version of mark with an animation. I then exported the blendshapes from the new model. I then imported this new model with the newly added blenshapes back into audio2face. i then did the blenshape conversion and it worked. So for some reason I can get it to work when using the mark model but not my own. One thing I have noticed is that when I import my model(the one with blenshapes) into the scene, its really small and I have to scale it up. The mark model was exactly the same so i just had to move it to the side. I cant figure out why its doing this but Im sure it must have something to do with why I cant sucessfully copy the animation over to the blendShape model…Any helo would be massively appreciated as I feel like i am so close and this workflow would be fantastic for the game im working on but need to get the animations working on a model with blenshapes.Ok so I finally got it working. I’m not sure exacly what it was but i think it had something to do with the models transforms. I positioned my own models head so that it matched the position of marks and then reset the transforms so that the position was at zero(it was quite heavily offset in the z place). I then went through the whole process again and this time the model with the blendshapes when imported was not tiny, so didnt need resizing. And yes, finally its now working. Hopefully this conversation with myself might come in useful for anyone expereiencing the same problem.It sounds like the scale of your model is a little too different from Audio2Face’s template models.
What you did is a good solution.
You can also change BlendShapeSolver parameters to achieve the same thing.This topic was automatically closed 14 days after the last reply. New replies are no longer allowed.Powered by Discourse, best viewed with JavaScript enabled"
456,quats-to-rot-matrices-not-found,"Hi, I’m trying to add a camera to simulation (taken from here except for the orientation calculation which gives the same error)But I get the following errors:File “/home/ahallak/.local/share/ov/pkg/isaac_sim-2022.2.1/exts/omni.isaac.sensor/omni/isaac/sensor/scripts/camera.py”, line 130, in init
self.set_world_pose(position=position, orientation=orientation)
File “/home/ahallak/.local/share/ov/pkg/isaac_sim-2022.2.1/exts/omni.isaac.sensor/omni/isaac/sensor/scripts/camera.py”, line 349, in set_world_pose
world_i_cam_i_R = self._backend_utils.quats_to_rot_matrices(orientation)
AttributeError: module ‘omni.isaac.core.utils.torch’ has no attribute ‘quats_to_rot_matrices’
Any ideas?Hi @ahallak  - Does this forum question helpful to you? Orientation of Camera - #2 by xuning.yangNo, but I managed to get it to work eventually.
It was because I’m using pytorch backend, but for some reason two functions were not implemented in the pytorch utils so I had to add them manually: quats_to_rot_matrices &  rot_matrices_to_quats.
I copied the two functions from ov/pkg/isaac_sim-2022.2.1/exts/omni.isaac.core/omni/isaac/core/utils/numpy/rotations.py to ov/pkg/isaac_sim-2022.2.1/exts/omni.isaac.core/omni/isaac/core/utils/torch/rotations.py,  and casted the input to numpy and the output to tensor so it wont crash.Powered by Discourse, best viewed with JavaScript enabled"
457,offline-pose-generation-generate-masks,"Hi,I have a question regarding offline pose generation. I have a specific requirement for my model where I need masks of objects in each frame. Currently, I have generated visibility masks, which are binary images representing the visible parts of each object. However, I also need the complete masks of objects, including the parts that are not visible. I was able to retrieve the visibility masks using the semantic segmentation annotator. Can anyone provide advice or suggestions on how to solve this problem?Hi there,would this be similar to these questions?If so, would this be a working solution for your case until the actual annotator is implemented? Isaac Sim 2022.1.1 Replicator Composer Missing Amodal & Occluded Instance Segmentation Sensor - #5 by ahaiduPowered by Discourse, best viewed with JavaScript enabled"
458,cannot-open-usd-files,"I want to open usd file that I saved last time, but failed.   sometimes is shows error log in the terminal.then the app is stoped.and sometimes it shows ""isaac sim "" is not responding in the GUI like picture below.
maybe there was a error when I launched the isaac sim APP,but I don’t known how to solve the problem.Hi @782365867 - Can you update the drivers as mentioned in the document for linux OS and see if that resolves the issue?
https://docs.omniverse.nvidia.com/app_isaacsim/app_isaacsim/requirements.htmlbreakpadmy drivers version is 530.41.03 ,and is newer than that mentioned in the document.
besides, the isaac sim(version 1.1) can open these usd files, but  isaac sim(version 2.1) cannot.Hi @782365867 - Can you update the driver mentioned in the document and let us know if the issue still persists?@rthaker Hi, I meet same problem with driver 530 and Ubuntu 20.04. The problem is solved when I downgrade driver to 525Hi @mlx12350  - Good to know. You should run the released version of Isaac Sim on the GPU driver mentioned in the docs. Currently the doc mentions “525.60.11” for Linux.Powered by Discourse, best viewed with JavaScript enabled"
459,adding-closed-links-to-robot-usd-produces-strange-outcome-in-spawning,"Hello,I’m working on developing a robot controller and have encountered an issue. I utilized my URDF file to import the robot and incorporated joints that create closed links using Isaac SIM. However, when I attempt to spawn it using Articulation and ArticulationView, it appears in an abnormal orientation, as shown in the video below:
Interestingly, this strange outcome doesn’t occur when I disable the added joints, as demonstrated in the following video:
I’m seeking a solution or workaround to address this problem. If anyone has any suggestions, I would greatly appreciate it.(FYI, it worked without problem in the USD scene from GUI.)
When you spawn the articulation view, and before you started the simulation, do you set the joint positions using set_joint_positions function?I once encountered the problem of robot flying away after spawn, I solved the problem by set joint positions (including the passive joints excluded from articulation). Since the passive joints are constrained by external forces, it will take some time to calculate the actual joint positions. If the initial positions of the passive joints are far from the proper ones, it will take too much time to converge to a normal value, and there might be something weird happening.I’ve already set default joint positions, but it did not work for my case.Hi @user15623  - The issue you’re encountering might be due to the initial pose of the robot not being set correctly when spawning it using Articulation and ArticulationView.When you spawn the robot, you can specify its initial pose (position and orientation) in the world frame. If this is not done, the robot might spawn with a default pose, which could be causing the abnormal orientation you’re seeing.Hi @user15623 - The issue you’re encountering might be due to the initial pose of the robot not being set correctly when spawning it using Articulation and ArticulationView.Hello, @rthaker. As I’ve already mentioned, the same configuration doesn’t generate any problem without closed links. It only makes problem when I add closed chain joints.Hi @user15623  - Adding closed links to a robot USD can indeed produce unexpected results. This is because the physics engine may interpret these closed links as a loop, which can lead to instability in the simulation.In the context of robot kinematics, a closed link or a loop is a sequence of joints and links that starts and ends at the same point. This can create ambiguity in the physics simulation because there are multiple paths between any two points in the loop.To avoid this issue, you should ensure that your robot’s kinematic structure is a tree, not a graph. This means that there should be a unique path between any two points in the structure. If your robot naturally has a closed-loop structure, you may need to introduce a “dummy” or “virtual” joint to break the loop for the purposes of the simulation.Powered by Discourse, best viewed with JavaScript enabled"
460,animation-graph-error,"I am trying to generate animation for people by using Animation Graph introduced by “Animation Graph Overview in Omniverse USD Composer”. I first loaded biped_demo.usd from omniverse://localhost/NVIDIA/Assets/AnimGraph/Characters/NVIDIA/. After that I created an animation graph and added it in biped_demo SkelRoot. However there was a warning “Animation graph ‘/World/AnimationGraph’ is not assigned a valid skeleton” when I ran the animation. Can anyone experienced this problem ?
image2508×1381 238 KB
image741×1349 115 KBI believe this means you need to set the ‘/World/biped_demo/Root’ as the skeleton in the AnimationGraph properties. Click on the AnimationGraph in the stage to select it, and in the properties window click ‘Add Target’ and select the SkelRoot in the Skeletal Binding section.Powered by Discourse, best viewed with JavaScript enabled"
461,dont-work-omniverse-launcher-unable-to-download-gdpr-agreements-and-eula,"Hi everyone. I from Ukraine and I need help. When I try to launch the Omniverse launcher, I get the following: Unable to download GDPR agreements and EULA.
How can i fix this?FIXED ITI had the same problem in India. Fixed it by changing time-zone of my computer from (GMT+5:30) to (GMT-12). Then RESTARTED my computer. Launched the Omniverse Launcher and it worked!Powered by Discourse, best viewed with JavaScript enabled"
462,issue-with-mapping-co-ordinates-for-created-lights-in-omniverse-extension,"Hi Team,
I have certain co-ordinate values in xformOP:transform attribute, from which I was trying to map xformOp:translate, xformOp:rotateXYZ, and xformOp:scale attributes after creating a light from the code. I copied the individual values from xformOp:transform and set them in the appropriate attributes (translate, rotateXYZ, and scale). As a result, I am now able to see the values in their properties(UI).However, despite successfully copying the values, I noticed that the placement of the created lights still remains at the default position (0, 0, 0), instead of at the translate co-ordinates which has been mapped. Please let me know the cause and resolution for the samePlease find attached below different code snippets that I tried to map the values1st Method:2nd Method:3rd Method:Powered by Discourse, best viewed with JavaScript enabled"
463,how-to-simulate-servo-motion-commands-of-real-robots,"Hi, I am planning to simulate the motion trajectory of  UR5 robot with isaac sim.  However,  I am wondering how to simulate the command servoj (a command that can be executed on real ur5 robot, see the figure below）？I hope to send commands to the robots in isaac sim and they can move like real robots, just like using UR sim.Hi @Robo_qq,We do not offer any custom simulation that directly mimics the interface and behavior of UR robots.We do have a generic trajectory generator that can be used for 6 and 7-dof robots: Lula Trajectory Generator — isaacsim 2022.2.1 documentationThis “Lula Trajectory Generator” can generate trajectories based on c-space (“joint”) or task space (“cartesian”) path specifications as documented in the above link.Behavior can be tuned by setting the velocity, acceleration and jerk limits for each joint. Velocity limits are set in URDF and acceleration/jerk are set in the robot description YAML file (Lula Robot Description Editor — isaacsim 2022.2.1 documentation).Hi,
thanks, Can I use moveit to connect to isaac sim to achieve this function?If you have an external controller model that provides desired joint positions, you can stream these to Isaac Sim.There is an updated tutorial for interfacing between MoveIt and Isaac Sim here: How To Command Simulated Isaac Robot — MoveIt Documentation: Rolling documentationPowered by Discourse, best viewed with JavaScript enabled"
464,waiting-for-a-request-that-has-taken-longer-than-0ms-to-complete,"Hi,I am running Kit headlessly in a container like this:As you can see, I am using two custom extensions. These are located on an external Nucleus instance. While the code seems to work, I am getting a lot of warnings:Any advice appreciated
BrunoHi Bruno,
I believe these to be related to connecting to the Nucleus Registry to pull and install the extensions, but it doesn’t seem to be taking too long or have any practical implications correct? It is basically trying to install the extension locally, similar way as it would if you would use the Extension Manager UI.A couple of questions:Once you have a more stable extension development, and if you are enabling all the time these extensions you can also add those by default to your container. But agreed, that having them in a Nucleus server registry is still helpful to install, push and pull updates though, and it should workThanks Teresa,it seems to happen only when running in a container, not if I run from a terminal or when running the UI.Best,
BrunoPowered by Discourse, best viewed with JavaScript enabled"
465,how-to-render-and-save-picture-with-create,"Nvidia made a really stunning ting with Omniverse. I have just started to learn it. My problem the documentation is very confusing, it is not linear, difficult to follow. But the real problem is I can’t find in it how to render a picture and how to save it. It’s getting to drive me crazy :) Does anyone know it? :)
Thanks.Hi @mypostafiok ,Easiest way is to use F10 to take a screenshot of the viewport but by default that saves the UI too, so turn it off under Edit → preference-> Capture Screenshot → check the checkbox and fill in the saving path.

image1048×701 50.9 KB
The other way you have to follow this documentation, which is using the movie capture tool where you could just render a frame or sequences.
https://docs.omniverse.nvidia.com/app_create/prod_extensions/ext_movie-capture.html
Rendering Basics — Omniverse Materials and Rendering documentationThank you. Yes that is the correct advise. Using the F10 key is the best way.Thank you very much.
Omniverse for Blender is brilliant. Since I have found it I deleted native Blender and I use only this version.Great. Glad you like itPowered by Discourse, best viewed with JavaScript enabled"
466,how-to-convert-the-point-cloud-to-a-right-handed-coordinate,"Hi,
I am publishing point cloud data from Isaac Sim using the depth_pcl type. When I display this point cloud in rviz2, the point cloud is generated with camera coordinate(z-axis forward) as below:

depth_pcl_issue1954×789 276 KB
Omnigraph is configured as follows:

depth_pcl_settings1520×470 63 KB
I can manually transform the point cloud data by fixing the SDGPipeline, but this isn’t very pleasant. Does anybody know how to deal with it?Hi there,currently manual post-process is needed to change the coordinate system. In future releases this can be provided as a built-in functionality.This topic was automatically closed 14 days after the last reply. New replies are no longer allowed.Powered by Discourse, best viewed with JavaScript enabled"
467,audio2face-without-rtx,"I would love to use Omniverse Audio2Face but I don’t have a RTX graphics card. Is there a way I can rent a VM or some computing power to be able to use the app through the cloud?Powered by Discourse, best viewed with JavaScript enabled"
468,adobe-substance-3d-painter-omniverse-connector-and-extension-updates,"
image1920×1080 154 KB
This topic contains information about all of the updates that we release for the Adobe Substance 3D Painter Connector + Extension.Here’s a video that gives an overview of how it works:101.1
We are pleased to announce the first version of the Substance 3D Painter Connector + Extension. The motivation behind this release is to provide creators with a way to interactively use Painter’s world leading 3D texturing and smart materials with USD meshes and MDL materials from Omniverse.  The Substance 3D Painter Connector works in tandem with the Create Painter Live Link extension to provide real time visualization of painted meshes in Omniverse Create. The Painter Connector enables these features:102.1 Point ReleaseThe emphasis for this release is on stability, bug fixes, and some light workflow improvement with the UI… we also love playing with the Create physics features while live-updating textures within a stage:FeaturesBug FixesChanges200.0 Release
substance3dPainter_Dragon1920×803 108 KB
We are excited to announce a new update to the Adobe Substance 3d Painter Connector & Extension!200.0
The emphasis for this release is a redesigned UI for an improved user experience.More details can be found in the link below.
Official Documentation 201.0 ReleaseNew releases for the Adobe Substance 3d Painter Connector & Extension are now available!AddedImprovedMore details can be found in the link below.
Official Documentation202.0 ReleaseNew releases for the Adobe Substance 3d Painter Connector & Extension are now available!FixedImprovedRemoved
connect_painter_plugin_panel (1)700×374 14 KB
More details can be found in the link below.
Official Documentation203.0 ReleaseUpdates for the Adobe Substance 3d Painter Connector & Extension are now available! The main feature of this release is that it leverages the new USD export feature available as of Substance Painter 8.3.0, which unlocks the ability to export textures using all built-in and user-defined texture sets.AddedImprovedRemovedMore details can be found in the link below.
Official DocumentationPowered by Discourse, best viewed with JavaScript enabled"
469,how-to-check-collision-of-objects-in-isaac-sim,"Hello!I defined a function in_collision() in the following code to detect when an object is in collision with other objects in isaac-sim.However, I am not quite sure how to implement the in_collision() function. Any guidance or examples you can provide on how to implement this function would be greatly appreciated. Thank you!Hello!I think the check_robot_collision function implemented in this repository might be helpful as a reference.Thank you, @smakolon385 !This topic was automatically closed 14 days after the last reply. New replies are no longer allowed.Powered by Discourse, best viewed with JavaScript enabled"
470,load-meshes-into-sim,"I can’t figure out out to load .obj files into sim. A previous question has references to converting the asset to usd. However, I can’t find out how to even load a usd through the api, much less do anything like adding collision or physics properties to it.Neither the standalone example, nor the referenced script editor conversion script seem to show how to load either a .usd or .obj file into simulation – even though you can drag and drop one into sim through the GUI and right click to add physics properties.The Shapenet situation seems similar – with a lot of information about how to use the GUI and the only thing referenced in the documentation is asset conversion. This would also be really useful if I could get those in and make them collision or physics objects.How can I load a .obj file? Or, if that isn’t possible through the api, how can I load a converted .usd asset, and add physics or collision to it? Additionally, where is this referenced in the API documentation?I referred the steps mentioned in this thread.So that would look like:Also where is the documentation for omni.physx.scripts?Powered by Discourse, best viewed with JavaScript enabled"
471,scripts-for-replicator-folder,"I am trying to find the Scripts for Replicator folder, because I want to take a look at the implemenation of the Basic Writer, but I am not able to find it. In the documentation there is a gif, that should show where this folder is located. Unfortunately it does not show the location of the folder. Maybe someone knows where I can find it and maybe correct the documentation as it is very confusing.

image1140×627 154 KB
Hi @julian.grimm thanks for pointing this out. I agree this isnt clear and I’ll make a ticket to get this addressed.In the interim open the extensions tab, under the menu Window>Extensions. In the Extensions tab, search for Repliactor, and select “Omni Replicator”. CLick on the folder or VS Code icons highlighted in green in the image below. Inside omni\replicator\core\scripts is the folder being referred to.
image1301×340 54.3 KB
Please also consider checking out our replicator github scripts repo here:Thank you for the info.This topic was automatically closed 14 days after the last reply. New replies are no longer allowed.Powered by Discourse, best viewed with JavaScript enabled"
472,blender-3-6-1-nucleus-unsupported,"Hello Omniverse Team. Hope you are doing well.I remember omniverse working great in previous Blender versions, but now it only seems to work on the 3.6 alpha.
I get a weird warning on my addon bar.
Am I doing something wrong?
I don’t want to use 3.6 alpha since I’m using 3.6.1 now.Thank you.Screenshot 2023-08-07 101723702×134 13.5 KBPowered by Discourse, best viewed with JavaScript enabled"
473,camera-output-lags-due-to-rendering-speed,"I am currently training an RL agent using the input from a camera. At each step of the training, the camera is moved. Before getting the output from the camera, I call world.step(). However, I have noticed that the camera output does not always match the expected position of the camera. I believe this is due to the slow rendering speed, but assumed that world.step() would block until rendering is finished.
I then moved to rep.orchestrator.step(), which after running multiple tests by comparing images of moving the camera over the same path multiple times, seems to block until rendering is finished.
There are two issues with rep.orchestrator.step():Are there any suggestions? And why does world.step() not block for one rendering_dt?Here is the camera setup:Hi @replicator.user.123  - It seems like you’re encountering a couple of issues related to rendering and camera updates in your reinforcement learning (RL) simulation. Here are a few suggestions that might help:Hi thank you for responding.Powered by Discourse, best viewed with JavaScript enabled"
474,isaac-2022-2-generalize-the-example-for-adding-noise-to-sensors,"Hi!I have been following the tutorial for adding noise to camera.Now, I am missing some more information about the general case. For instance, I was trying to add the same noise also for the BasicWriter class, so that I can choose to publish the image via ROS or save directly to a folder. However, I have run into issues trying to extend the approach, I cannot get the noise to apply to the BasicWriter.For instance, in the example, we substitute the annotator before calling writer.initialize(). However, in the case of BasicWriter this is not possible, as the annotators array gets filled only after it is initialized. So I try to replace it after initialization, but I don’t get any output.This is the code I added to the example:When I add prints to the function, I see that it is being invoked and no errors are printed. However, I do not get the images saved in the directory. What am I doing wrong here? Is there any documentation on the annotators used in different writers and the data being passed through there?Thanks!Hi there,sorry for the late reply:here is and example of how one can do this with the BasicWriter:Here is another example using annotator/writer combinations:In the next release this will be possible in a more straightforward way (e.g. avoiding accessing private members).Cheers,
AndreiThis topic was automatically closed 14 days after the last reply. New replies are no longer allowed.Powered by Discourse, best viewed with JavaScript enabled"
475,audio2gesture-animation-finger-issues-in-blender,"Hi guys. I was working around with fbx armature animation Nvidia machinima’s audio2gesture tab gives. When I imported the fbx file in my Blender project I’v surprisingly found out that the end fingers of the character are not bones, they are empties. Honestly saying it is looking uber weird as my German friend would say :DFor the fingers animation it is extra important for all the finger parts to animate correctly. Maybe you will fix this in newer versions of Machinima? Or maybe there is a way to transform empties into bones in Blender or maybe even in Machinima?I’m attaching a screenshot like you can understand where that numerous empties are situated.

2023-05-29_23-26-541911×1047 153 KB
I’m retargeting the animation from the fbx’s armature from audio2gesture to my character’s armature, you can have a look on the result I have, but as you can see my character is not closing the fingerst fully, because there is no bone for the end finger one, which is created as an empty.
Hi @zanarevshatyan ,
on the A2G armature, we don’t have the finger tip joint as they are not used.
The points on the last segment of the finger are all weighted to the last joint of the chain.
When setting up retargeting in kit, if your target character has extra finger tip joint, you can manually pick the right joint for it on the retarget UI.Check out this video where I explain the finger tagging (around 12’00""). A2G_retarget_machinima_2022_3_0.mp4 - Google DriveOmg, it sounds very complicated :DThanks for the reply so much. I’ll watch the video and try to understand it, if I will have some questions, can I ask them to you here?Maybe you know how I can make bones on the finger tip in Blender instead of that empties and animate them? Or this is not possible?You can probably add the finger tip joint, but animating them in theory won’t affect anything since the last segment of the finger are all weighted to the last articulation joint (which can be animated through rotation).
But maybe I was not understanding the issue clearly here.
Were you trying to export the A2G skeleton from ov as fbx, then when you load the fbx skeleton to Blender that the fingers exploded?I’ll ping our Blender expert to help chime in on the matter as well.Yeah, I’m exporting fbx file of a2g armature into Blender and trying to retargeting from a2g trained character to my character’s armature using another blender addon, that addon works fine, but there is no bone on a2g character finger tip, which is making the animation on character look weird for the fingers, they are not closing fully, you can have a look on the video I’ve also uploadead earlier in this topic. Hope guys can help me with Blender, waiting for them to answer, thanks.Hi there!Would you be able to upload that FBX for me to take a look at?Sure, here it is
World7.fbx (15.6 MB)Also I’m sending you the Blender project file with my character already retargeted from a2g armature using rokoko addon in BlenderGoogle Drive file.Thank you.please check my post, I am having same issue in metahuman and A2g, any help?Powered by Discourse, best viewed with JavaScript enabled"
476,isaac-sim-simulation-performance-down-by-time,"Hello everyone,
I found when I simulated robot motion after 6hrs, the fps were down obviously.
Eery 6hr the fps is down about 4fps. My project after simulating 24hrs only have 4fps.
I’m trying to simulated a product line’s UPH, but the fps down make the data not correct.
Any suggestion?Exactly same issue here. A memory leak is suspected.Hi @vic-chen / @kyungho.yoo  - Thank you for the information. Can you provide some more details like what Isaac Sim version are you using? code snippet to observe the memory leak just like this forum post: Isaac Sim OS (driver?) persistent memory leakHi @rthaker ,
The attachment records the GPU usage’s information.
My hardware spec is following:1.OS: Windows 10
2.CPU:  Intel(R) Xeon(R) Gold 6326 CPU @ 2.90GHz   2.90 GHz  (2 processes)
3.GPU: Nvidia RTX A6000 (2 GPUs)
4.RAM: 1TB
5.Isaac Sim version 2022.2.1
IsaacSim_FPS_Record.xlsx (86.4 KB)Powered by Discourse, best viewed with JavaScript enabled"
477,ticking-blueprint-with-import-usd-opens-the-usd-in-a-stage,"Is there a way to tick blueprint but not have the stage open up automatically in the viewport? I will not use the Import as Blueprint feature if it keeps doing this as it takes a lot of time to then open up the level I was just in to get back to what I was doing.  This video shows what I am talking about but I edited out the 3 minutes of waiting it takes for me to load the level I was in back up again which happens every time I import with import as blueprint enabled.Loving this functionality who ever is working on this, it’s fantastic !!! I have a few more suggestions I will post I hope you keep working on it as it is already a massive help to me as an indie game dev.Dirk, this is great feedback.  I have made internal issue OM-100664 to address this.  It may not make it into our upcoming update, but we just didn’t notice this while iterating on the import feature.  Thank you!This topic was automatically closed 14 days after the last reply. New replies are no longer allowed.Powered by Discourse, best viewed with JavaScript enabled"
478,materials-grayed-only-on-geometry-subsets-since-installing-update-2023-1-0,"I’m having this issue since installing USD Composer 2023.1.0
I have an a room which is a single mesh divided into multiple geometry subsets. I use these subsets to assign different materials to the various parts of the room.
Since the update, all materials assigned to geometry subsets are grayed out, whilst others that are assigned to a mesh that is not divided are working. Also, as you can see in the image, the material preview on the properties tab is also grayed out for these materials.
I have rolled back to the previous version (Create 2022.3.3) and the same usd works perfectly.
Any suggestions on how I should proceed with getting this fixed?OS:
|Processor|AMD Ryzen 5 5600 6-Core Processor                 3.50 GHz|
|Installed RAM|32.0 GB|
|System type|64-bit operating system, x64-based processor|GPU:
RTX 4090
Driver 536.40USD Composer Logs:
USD.Composer.zip (4.2 MB)Geometry Subsets No Material1663×535 131 KBThank you for the comment. We will start to look into this straight away.Hey Andy,Likely this is related to an update to USD that caused a very specific method of material binding/application to be no longer considered valid. To help, we specifically built the asset validator to fix these bindings and make your asset compliant to the updated versions of USD.You can read about it here! Materials and USD Compliance with 2023.1 - #2Hi Andy
As mentioned, certain older scenes files may exhibit material problems, and the best tool to solve this is the “Asset Validator” that is built into the software. Please see below for how to run it. It will find and fix a lot of these material issues, and several other issues as they occur.image1863×816 156 KBWe are aware of these issues and we are exploring more options to help fix scenes easily. Could you please run the Asset Validator tool and report back with your results ? That would be of great help.Thank you for your prompt response.
I have run the Asset Validator as instructed. However, it appears to have emptied my stage completely. (see attached image)My proceedure was:
Open the Stage containing the assets and materials
Save As “Stage_03” (Previously “Stage_02”)
Open the Asset Validator window
Click Enable All
Click Analyse
At this point the red writing appeared under the Asset column
I clicked on the checkbox next to the red writing to select it
Clicked Fix Errors on Selected
At this point the entire stage went black and all assets dissapeared from the stage tab. (Layers are still present)Log:
USD.Composer.zip (4.3 MB)Image:
Asset_Validator_1.02560×1080 459 KBI will experiment some more, but just wanted to keep you informed of each step of my process.
Thanks again for your help.Also, I’ve just discovered that the previous iteration, “Stage_02” has also been affected, leaving it empty of all assets except the HDRI I have on a Dome Light.Process:
Following the operation described in my previous comment, I closed “Stage_03” without saving.
I opened “Stage_02” to discover this:
Asset_Validator_1.12560×1080 312 KBHi Andy
That is not good. It should not be removing any assets. Can you send me your scene so I can experiment with it. Make sure you have backups. Our apologies for the inconvenience.Thanks Richard,
I will send you a link in a DM.I’m curious about the line at the top of the asset validator that says the stage is not available.Have you tried right clicking on the stage in the content browser and selecting, “validate asset”?It changes the validator from Stage mode to URI mode. The rules can also be run in dry mode to see what it finds before fixing.Hi fpliu,
The original stage has no assests left on it. However, I have copied one from my backup on an externalHD to follow your instructions.Process:
Duplicate folder containing all assets on externalHD
Move duplicate to the project folder on localhost
View project folder in browser in USD Composer
Open “Stage_02” to check if it contains assets as expected: It only shows the assets that were working before. The building that was grayed out by the update is not even present on the stage, though it is listed in the stage tab. (See image1)Right click “Stage_02”
Click “Validate USD”
Click Analyse
This lists “Stage_02” in the Asset column
Expanded list of Issues (See image1)
Selected All
Clicked “Fix Errors on Seleceted”
This leaves me with the Stage emptied again and the Stage tab depopulated. All that remains is the dome light with HDRI texture.
Also, two Issues remain listed in the Validator Asset column.
When I select these and click “Fix errors on Selected”, they remain and nothing further changes.I have attempted to “copy output” from the validator but am unable to paste the results in any format.
I have attempted to “save output” from the validator but no .csv file shows up in the file I save to.
So other than trying to screengrab the long list of issues listed, I am unable to send you a copy of the validator output.Thank you for your time. I hope this information gives you some clues.Validate_asset_from_browser1920×540 63.8 KBValidate_asset_from_browser_21920×540 59.9 KBHi @andygreen250
I took a look at your scene and we also consulted members of our engineering team regarding your setup. What was suggested is that you not run all the validation rules. In particular Default Prim Checker Rule.  That rule is the cause for making your geometry invisible.The suggested rules to run are only the Usd:Schema rules. You can Disable All rules. And then enable only the Schema rules.  Running this on your original data should fix all invalid USD realated to geomsubset family, material binding API and Dangling Material Bindings.It was noted that the data in your scene wasn’t actually deleted. It was invisible. To restore that visibility, you can go to the Script Editor and run the following on your sceneIn the future we’re going to change the default enabled rules in the Validator and we’re re-looking at the default prim checker to see if it’s overly aggressive.Hope this helps.@fpliuThank you fpliu and the engineering team.Apologies, but this is getting pretty complex now. My original issue with grayed out materials has snowballed. I’ll try to keep my response as clear as possible in the hope that we can glean something useful from this.For clarity: there are two versions of the scene that I have now shared with Richard:The original version: In a folder called “Layers” with usd’s called “Stage_01”, “Stage_02” and “Stage_03”The second version: called “Rooms_Modular_2.2_for_Richard”I am unsure which of these versions you have examined.So, following your message above, I went straight to the original version (“Stage_03”) that had been emptied by the Assest Validator to try out the script you suggested would restore the invisible data.Process:
Open the script editor window
Copy and paste the script from your message above into the Python tab
Click “Run”Result: It didn’t restore the data, the scene remained unchanged and empty. (Note: on this original, when I ran the asset validator on this scene, all assets dissapeared from the Stage tab as well as the viewport)I then opened the second version that I sent Richard, the usd called “Rooms_Modular_2.2” and ran the Asset Validator in the reduced way you suggested.Process:
I opened the usd, selected everything in the Stage tab,
opened asset validator window (in uri mode)
disabled all, renabled only options in the usd:schema set,
clicked analyse
selected everthing in the asset list
clicked fix errors on selectedThe result was that everything in the environment prim and everything in it has dissaperared and the building has turned red.I then tried on this scene the script you suggested might restore visbility to data made invisible by the asset validator. The scene remained unchanged.(See image 1)
After_usd_Schema_only3840×1080 452 KBOn a positive note: I have just experimented with a clean copy of the 2nd version of the scene and have begun to be able to restore the materials manually in a unexpected way, as follows:
1: Select the geometry subset of a greyed out object
2: Locate the MDL Source Asset
3: Drag the asset from the browser onto the mesh in the viewport
4: The wierd thing here is that doing this with only one material, this restores all other materials associated with all other geometry subsets on that mesh. (see image 2)I hope this might gives you some further insight into whatever has happened here.Material_restored_by Dragging_Dropping1920×540 84.6 KBAnyway, I’m very grateful for your time in trying to work this out and I hope that anyone reading the thread can learn from my mistakes. I have actually rebuilt the whole scene from scratch and so far it’s working in the updated USD Composer. For the benefit of anyone reading this thread, I’ll list some of my considerations in the rebuild. This time I’ve:So it’s been a tough but valuable lesson for me.
I understand if you don’t want to take this investigation further, as it’s all got a bit messy now. However, if it helps the devs to improve the software, then I’m happy to continue trying anything you might suggest.Thanks again
Andy(EDIT: Even stranger! I just reopened the version where I had begun to restore materials manually. When I closed it before, I had only restored materials to one room, as you can see in the picture above. On reopening it, all materials have been restored to all assets!! The scene now looks exactly as it did before I began this whole thread :) )Here’s a log of todays activity, incase it helps:
kit_20230706_113027.log (3.2 MB)
kit_20230706_120209.log (5.4 MB)I have exactly the same issue as described at the top. I opened my file using Code and all the material are textured correctly, in opened it in US Composer and they are all grey.Hi Zambetti1,
So could you follow the same advice.Hello!  I am experiencing the same issue with multiple stages.  I have attempted the afore-mentioned remedies and unfortunately, none of them have worked.  Notably, regardless of the file, the asset validator fails to apply the material binding API to any asset with the following result:

It does this with every stage on all assets.  It keeps failing on the “CloudSky.USD” file when it isn’t even the asset being evaluated.I am using the latest build 2023.1.1.Thank you!
-BrentPowered by Discourse, best viewed with JavaScript enabled"
479,can-i-use-omniverse-to-get-rotated-bounding-box,"I checked that I can get 3D box and segmentation information.
Does omniverse also provide information on rotated bounding boxes?
… rotated bounding box like the example in the figure below?
Hello @wjdrmstjrsla and thanks for reaching out! Replicator does not provide a rotated 2D bounding box annotator, but you may be able to get something reasonable by projecting the 3D bounding box into screenspace and using the furthest corners as your box vertices. This won’t produce a “tight” fit on all assets but should be sufficient in many cases.Powered by Discourse, best viewed with JavaScript enabled"
480,how-do-i-open-my-project-in-vs-code,"I knew how to do this a few months ago, but I forgot.I see my extension listed on the community tab if I click on 3rd party, but I have the code already on my hard drive.I added the location for Extensions to look in., but I only see the one on GitHub listed.Thanks, I think I asked this question before, but I forgot where the answer is.image1404×501 82 KBPowered by Discourse, best viewed with JavaScript enabled"
481,3d-bbox-with-3d-lidar,"Hi I’d like to generate 3D bounding boxes labels of my custom meshes with simulated point cloud. Would that be possible with ISAAC Sim?I also would like to know about this feature.Checkout the semantic schema editor and visualization tools. This should allow you to add labels to the mesh and get a bounding box around it.https://docs.omniverse.nvidia.com/app_isaacsim/app_isaacsim/ext_semantic_schema_editor.html?highlight=semantic
https://docs.omniverse.nvidia.com/prod_extensions/prod_extensions/ext_replicator/visualization.htmlIf you need that data in ROS, checkout the Bound Box 3d camera type in ROS2 Camera Helper
https://docs.omniverse.nvidia.com/app_isaacsim/app_isaacsim/tutorial_ros2_camera.htmlHas anyone of you solved the problem with 3d bounding boxes in lidar pointclouds?Hi @christof.schuetzenhoefer - Can you elaborate on your issue further?Powered by Discourse, best viewed with JavaScript enabled"
482,a2f-with-animation,"I’m new to the app and was able to follow the A2F tutorial with reallusion avatars. My question is once I have the transfer character set up, how do I merge it back into my original character? or set it up to perform animation while talking?So right now I could either use the transfer character to talk, or the original character for animation, but not both.Thanks in advance.

Untitled994×720 51.7 KB
Hi @fanyang1956 and welcome to the forums!As of now, you can’t have body motions in Audio2Face. But adding body motions is on the roadmap.In the meantime, you can export the facial animation from Audio2Face and apply it to your character in other 3D applications such as Maya, Blender or 3dsMax. I’m not quite sure if Reallusion has the ability to import facial animation, but that can be another option too.This tutorial is an example of using Audio2Face generated facial animation in Blender: Omniverse Audio2Face and Blender | Part 2: Loading AI-Generated Lip Sync Clips - YouTubeThis topic was automatically closed 14 days after the last reply. New replies are no longer allowed.Powered by Discourse, best viewed with JavaScript enabled"
483,cant-start-isaacsim-beacuse-of-x-error-of-failed-request-badlength,"image1341×551 33.9 KB
I use xshell to start Isaac in remote Linux server,but shows this error
X Error of failed request:  BadLength (poly request too large or internal Xlib length error)
Major opcode of failed request:  148 (GLX)
Minor opcode of failed request:  34 ()
Serial number of failed request:  161
Current serial number in output stream:  162Linux system: Ubuntu 20.04
Nvidia driver version:535.54.03Powered by Discourse, best viewed with JavaScript enabled"
484,lula-kinematics-velocity-control,"Hello,I’ve been experimenting with the Lula Trajectory Generator, and I couldn’t find any option to specify the target velocity for the movement. Based on the information provided in this post (How to control the movement and speed of Franka's end effector), it appears that there currently isn’t a way to control the movement speed for a trajectory.I’m curious to know if this feature is planned for development and if it will be added to the roadmap. Being able to control the movement speed for a trajectory is considered an important and very basic feature in classical robotics, and its inclusion in the Lula Trajectory Generator would be quite beneficial.Kind regardsAxelHi @axel.goedrich,Currently the Lula Trajectory Generator only supports generating a time-optimal trajectory based on the limits for joint velocity, acceleration and jerk. Internally, the trajectory for each joint is a piecewise cubic spline, with each segment saturating the limits for one of the aforementioned derivatives (velocity limits for each joint are required in the URDF and the acceleration/jerk limits are optional in the Robot Description YAML).Extending the Lula Trajectory Generator to support desired task space velocity is an oft-requested feature that we would like to add, but there is not a definitive planned release for this feature yet.Hi @bpeele,thank you for the clarification.I’m glad to know that you have plans to add this feature, even if it might take some time.Kind regardsAxelPowered by Discourse, best viewed with JavaScript enabled"
485,is-6sigmadcx-extension-still-available,"Does anyone know whatever happened to the 6SigmaDCX extension? After following through the prerequisite mentioned in the doc, are there other steps in making the extension show up in the Extensions dialog?Thanks,Powered by Discourse, best viewed with JavaScript enabled"
486,use-action-graph-and-ros-to-control-custom-robot-then-it-cant-work,"I use the
https://docs.omniverse.nvidia.com/isaacsim/latest/tutorial_ros_drive_turtlebot.html
method to control my custom robot ，then i bulid the  Action Graph accroding to   GUIDEand make someing change at joint name and wheel radius ,then i play simulate
,take the message :
Warning: in SdfPath at line 97 of /buildAgent/work/ca6c508eae419cf8/USD/pxr/usd/sdf/path.cpp – Ill-formed SdfPath <>: syntax error
and my robot didn’t work .( i have use the CORE API TUTORIALS to control and it can work)
forros0725.usd (6.9 KB)Hi @benson.ee12  - The error message you’re seeing suggests that there’s a problem with the SdfPath in your code. The SdfPath is used in USD to address scene components, and it should be a valid path to a component in your scene.Here are a few things you can check:Make sure that the joint names you’re using in your Action Graph match exactly with the joint names in your robot’s URDF file. Remember that these names are case sensitive.Check that the SdfPath you’re using to reference your robot and its components are correct. The path should start from the root of the stage (“/”) and include all parent components. For example, if your robot is named “my_robot” and it’s at the root of the stage, you would reference it as “/my_robot”.Ensure that the wheel radius you’re setting is a valid number and within the acceptable range for your robot’s design.Make sure that you’ve correctly set up the Action Graph. If you’ve made changes to the graph, it’s possible that you’ve accidentally broken a connection or set up a node incorrectly.Powered by Discourse, best viewed with JavaScript enabled"
487,right-click-context-menus-flickering-when-connecting-to-isaac-sim-from-omniverse-streaming-client,"When connecting to Isaac Sim (official NVidia image running in a container) through the Omniverse Streaming Client, certain UI panels such as Stage, Property and Action Graph has trouble handling mouse right clicks: the context menu shows for a short instance and then disappears.Right-click context menus in the Viewport and other parts of the UI work fine.The problem seems to go away if I simultaneously click the left AND right mouse button, which get registered as a normal right click and the context menu does not disappear.System: Ubuntu 22 with RTX 3060Isaac Sim: official NVidia image running locally in container (headless native mode)Streaming Client: version 103.1.1I’ve tested with multiple mouses and the problem persists, so it’s not a hardware issue.I’ve also tested connecting to Isaac Sim through websocket, and didn’t encounter the problem. This seems to suggest that it’s a bug with the Streaming Client’s mouse handling instead of Isaac Sim itself.No warning/error logs have been found when reproducing the right-click problems.Same issue here. Any response from the support/devs?Same issueHi. This is a known issue. We will be fixing this in the next version.Powered by Discourse, best viewed with JavaScript enabled"
488,nvidia-drive-missing,"Nvidia drive is missing from the Nucleus list. Tried to find new tutorials for Drive Beta 2.0 but couldn’t find anyPlease follow the documentationOmniverse Drive (Beta) — Omniverse Nucleus documentation (nvidia.com)Powered by Discourse, best viewed with JavaScript enabled"
489,how-export-teeth-results-to-maya,"I like the teeth results. How do I export them to Maya?export cache 11432×1026 130 KBOMG，I’m so dumb…You’re really great! Thank you!This topic was automatically closed 14 days after the last reply. New replies are no longer allowed.Powered by Discourse, best viewed with JavaScript enabled"
490,could-someone-tell-me-why-composer-and-code-render-this-so-different,"By Render, I just mean the away it appears in the view port.If you go to the Asset Store Beta, and search for Haunted, this building is free.image1337×415 196 KBAnd in this short video the windows look completely different. Is there a way to make Composer look like they do in Code.Thanks@DataJuggler I’ve seen this before when troubleshooting another Discord user’s scene. Long story short, the answer lies within the Render Setting; and, Composer would technically be the one with more correct representation.The specific settings found under “Indirect Diffuse Lighting” rollout were found to be the main attributing params that result in the visual discrepancies. Here were my findings using the default setting from both Code and Composer with the same haunted house USD opened as is (with no lighting or changes to scene elements):Code 2022.3.3:image1816×979 130 KBComposer 2023.1.1:image1814×978 187 KBThe reason why I said Composer is the more accurately depicted is because when there’re no lights in the scene in the original file, we shouldn’t be seeing any visible light/shadow. And when you add lights to the scene, you should expect them to calculate light bounces right away and not something you’d want to turn on manually that Code would have you do as an extra step. Because of no added ‘fake’ lighting that brighten the scene, shattered glass window panes are now visible; and allowing us to differentiate the glass from the background whereas it became extremely hard to see one separating from the other due to overall brighter scene.There could be other minute and subtle setting deviations between the two apps, but I found the render setting makes the most direct impact visually.So why is it? I can only speculate that Code’s intention wasn’t focused too much on doing final renders and making these little changes will allow people make doing their work easier (making extensions, tools, etc) without having to add lights to their scene all the time. Not sure how close my assumption is, but that’s simply one that I arrived at.If this becomes an annoyance or prove to be undesirable, you can simply match the setting on one app to the other (in this case, my recommendation would be to change the setting in Code to match that of Composer) and they should be brought to a more comparable level.Thank you for your response and detailed explanation.Thank for help answer the post. Yes you are correct. We are aware of slight differences between the way the default lighting in set up between the various kit apps. There were some changes made to Composer’s default lighting setup that have not been picked up yet by other apps. We are working to align them. Mostly in the secondary lighting, like GI and AO.The day after I posted this, Code was updated, and now the Windows look the same in Composer and Code.I guess I was just a day early .Powered by Discourse, best viewed with JavaScript enabled"
491,isaac-deformation,"Is there an easy way to give an object deformation in isaac sim?Hi @hvarpe - You can find more information about deformation here: Deformable-Body Simulation — extensions latest documentationPowered by Discourse, best viewed with JavaScript enabled"
492,how-do-i-set-camera-rotation-in-issac-sim-python-standalone-app,"Hi,I’m currently using Isaac Sim Python 2022.2.0.I would like to know how to adjust the rotation property of the camera, which can be seen on the screenshot:
However, I could only set the camera_position.
How can I set the initial camera rotation to [7, 0, -90] in Python standalone app?My code is:Hi @j.moon - If possible, I would suggest to update the Isaac Sim with the latest Isaac Sim 2022.2.1 release.I will assign the question to the right dev and he should be able to answer your question.Hi j.moon,The viewport uses /OmniverseKit_Persp prim.  Please, try setting the pose of that prim.  Adjusting the position and rotation of that prim should change the position of the camera that is used to render the viewport.Note, it looks like you are modifying the viewport position, instead of working with Isaac Sim camera API.  In Isaac Sim, a standard approach is to work with the cameras/sensors, instead of the viewport, which is designed for GUI.  An introduction to that approach is available here: Camera — isaacsim 2022.2.1 documentationThanks!Powered by Discourse, best viewed with JavaScript enabled"
493,omniverse-3d-model-in-browser,"Hi,
Is it possible to embed a 3d model from Omniverse Create into a webpage that people can interact with?
I’m new, so please excuse the basic question.
Thanks,
NHi,
Omniverse is a development platform built on USD that allows you to easily connect most digital content creation tools. It is cloud native so although today it’s something you download and use locally, in the near future it will be available as a cloud service. One use case is to run a kit-based (omniverse) app in the cloud and stream it to a browser.We have an example of that here: Rimac Automobili's Next-Generation 3D Car Configurator with NVIDIA Omniverse Cloud - YouTubeAs this isn’t quite available yet, you can also leverage the extensive connector ecosystem to bring your Omniverse asset into Unity or Unreal Engine to use web hosting solutions they offer today.Essentially Omniverse acts as a universal translator for all content creation tools allowing you to bring data into Omniverse but also out to any connected application.Powered by Discourse, best viewed with JavaScript enabled"
494,create-app-crashes-when-activating-fpga-scenario,"I am using the Create 2022.3.3 version
and testing modulus extension by referring to the documents.When running create app, the command line I used was:
~/.local/share/ov/pkg/create-2022.3.3/omni.create.sh --enable modulus_ext.coreThe following extensions are autoloaded.
modulus_ext.core, modulus_ext.ui, hpcvis.vtkm_bridge.coreSo far, there is no problem, but when I activate the FPGA scenario, the create app suddenly exits.The following is the error displayed in the console.Powered by Discourse, best viewed with JavaScript enabled"
495,any-support-to-import-a-corona-scene-into-omniverse,"Wondering if there will be any future update for importing Corona scenes into Omniverse? Considering a lot of Archiviz is now perfomed with Corona.We do not have plan in short term to support it yet.Powered by Discourse, best viewed with JavaScript enabled"
496,settings-omniverse-streaming-client,"Wat server setting do i need to set?
Nvidia SC789×805 43.1 KB
You will need to run an instance of Omniverse Create XR to stream a scene to the client. The server address would be the IP address of the system on your network that is rendering the scene. The XR app is a receiver but you need an RTX-capable machine to process the scene.You can find more information here Omniverse Create XR — Omniverse Create XR documentation (nvidia.com)I have an NVidia RTX 3080 in my machine and installed Omniverse create XR. What I would like to test is to stream an AR scene to an iPad (pro 11"") on the same network. Is this possible and do you have more information on how to do this? The info on the website is quite limited, and on youtube is also not much to find.Hope you can help me. thank you in advancePowered by Discourse, best viewed with JavaScript enabled"
497,isaac-read-rtx-lidar-data-node,"Hi!I try to implement a method to get 3d bounding box informations in a lidar pointcloud. However, I encountered the node from RTX Lidar Node Descriptions — Omniverse Robotics documentation and found the output of object ID’s, but do not know what it should represent? Is there a mapping from object ID to prim object? If yes, then I can easily compute the bounding box, knowing which points hit which objects in the scene.Best regards,
ChristofCan anyone help me here?Hi there,sorry for the delayed reply, I have internally forwarded the question to the relevant person. Until then, can you check if the Instance ID coincides with the Object Id? If so, you could read out the mapping from there.Update: the instance id mapping is different, I will loop back once I have an answer.Update 2:  the instance segmentation id is the same as the object id.Would the pointcloud, or the 3d bbox annotator work as a temporary workaround?Best,
Andreiokay, thank you!I have already tried the 3d bbox annotator, but I was not able to achieve any results. The data in the dictionary was always empty, so I guess it is not working for lidar pointclouds.Is it intended to work out-of-the-box for lidar pointclouds? Maybe there is bug somewhere that a connection in the omnigraph is not there anymore@ahaidu Any new information?Hi there,it seems the object_id is the same as the instance segmentation id. Using this function you can for example get the prim path:Thank you!I will try it out and come back to you if everything has worked as expected.Powered by Discourse, best viewed with JavaScript enabled"
498,isaac-sim-in-kubernetes,"I’m attempting to follow the guide to get headless isaac sim deployed on-prem. It seems that the sim doesn’t encounter any critical errors, but I never see the “Isaac Sim Headless Native App is loaded.” message pointed out in the docs. How would I go about debugging the deployment?Just to try my luck I tried connecting from the Omniverse Streaming Client - I get a black screen for ~10 seconds and a “failed to connect”. Blindly debugging, I wire-sharked to see what it tries to talk to to try to unclog the pipes. I had to expose ports 8211 and 48010 but I now see a bunch of packets flying on both. Still same black screen for ~10 seconds and “Unable to initiate a connection to the server” generic error banner.So I now have no clear indication of what might be wrong and the sim is not connecting / working. The websocket on 48010 seems to just terminate arbitrarily and the pod itself has no obvious errors.Ideas?Hi. Please confirm you have opened all ports needed by OV Streaming Client. See Using Omniverse Streaming Client — Omniverse Streaming Client documentationPlease also share your Isaac Sim logs.Powered by Discourse, best viewed with JavaScript enabled"
499,audio2face-api-input-mp3-output-blendshapes,"I’m wondering if Audio2Face can be available as a REST based API where one sends in a voice file (mp3 for example), to receive the blend shapes (asynchronously, is fine)?Hello @inac!  Thank you for your question.  I reached out to the Audio2Face team for an answer.  I will post back here when I have more information.Hi, I’m also interested in this as well. I was wondering if there has been an update to this. I also saw the release of the Rest API and Headless Mode, however, after watching the video I am still not sure if I am able to achieve the result of the original poster wanted to achieve. For me, I would just like to send a request from Unity using C# and receive the JSON list using the API alone, is this currently possible? Thank you.Still awaiting as well… been a long time!This video might help Audio2Face Headless and RestAPI Overview - YouTubeYou should consider having a Unity plugin for runtime support in UnityDo you mean something similiar to the Unreal Engine LiveLink plugin which was released in Audio2Face 2023.1.1?no, for unity, there should be support for dynamic creation at runtime - not just in editor. for example https://assetstore.unity.com/packages/tools/animation/salsa-lipsync-suite-148442Is your final goal feeding an audio file to Audio2Face and receiving BlendShape animation weights in Unity?
If so, then that’s what UE Livelink (Live Stream) plugin does. It’s an extension which can be used as a reference to build a similar one for Unity or any other app.No, the difference is fully autonomously at runtime instead of sending blendshapes to the editorSorry if I’m misunderstanding you and thanks for your patience.Do you mean you need to feed an audio clip to Audio2Face and receive the blendShape weights animation in Unity in runtime (or realtime)?  If so, then this is exactly what the Audio2Face LiveLink plugin for UE does.If that’s not what you mean, can you please provide more information of what the Unity Plugin you have in mind does?Hi Ehsan, the headless and RestAPI is missing the API to input audio and output blendshape weights. Will Omniverse Ace contain that in the future?No, the goal is to have it be self-contained without needing a separate app. Unity lets you build your app to publish anywhere from iOS to Android to consoles etc.In short: Unity app feeds in the audio input, Audio2Face Unity plugin provides blendshapes in Unity.Also, it would be preferable for blendshapes to be more universal than anger or happiness - rather eyeUp etc - for example as defined in https://developer.apple.com/documentation/arkit/arfaceanchor/blendshapelocationhttps://docs.omniverse.nvidia.com/audio2face/latest/user-manual/rest-api.html#headless-a2fI don’t think it’s possible to build a standalone app similar to what you can do with Unity or UE in the public Audio2Face. That said, this might be possible using NVIDIA ACE.Audio2Face uses as many blendShape targets as you like. The default blendShapes have 46 and 52 different face shape targets including eyeUp, etc.Anger, happiness and other emotions are added to those ~50 blendshape targets.I have built several standalone Unity apps that can do audio to blendshapes, so I can attest that it is absolutely possible. You can try out one recent app store app i made that does that here: https://apps.apple.com/us/app/shakespeare-vc/id6447998447 I used SALSA https://assetstore.unity.com/packages/tools/animation/salsa-lipsync-suite-148442Sorry for the confusion. That’s what I meant. Audio2Face can not do what Unity and UE can.Why not? it’s a REST based API that Unity can post data to and get data back?Powered by Discourse, best viewed with JavaScript enabled"
500,the-objects-i-imported-created-was-automatically-shifted-upwards-when-i-clicked,"In the Isaac Sim environment, Why all the objects I imported/created was automatically shifted upwards when I clicked on them?Before!
Screenshot from 2023-07-31 09-57-081442×988 170 KBAfter I clicked!
Screenshot from 2023-07-31 09-57-201442×988 226 KBclicked again
Screenshot from 2023-07-31 09-57-301408×734 220 KBHi @lily.chen  - This behavior is due to the Zero Gravity tool in Isaac Sim. The Zero Gravity tool is designed to help with positioning objects in a scene by temporarily adding physics properties to them, such as collision and gravity. When an object is selected with the Zero Gravity tool enabled, it is automatically lifted slightly to prevent it from immediately falling due to gravity.By disabling the Zero Gravity tool when not in use will prevent this behavior.Powered by Discourse, best viewed with JavaScript enabled"
501,realsense-splitter-no-definition-of-realsense2-camera-msgs-for-os-version-focal,"hello,
I have meet a error  in tutorial-realsenserealsense-viewer can work ,when I run
“cd /workspaces/isaac_ros-dev/ &&
rosdep install -i -r --from-paths src --rosdistro humble -y --skip-keys “libopencv-dev libopencv-contrib-dev libopencv-imgproc-dev python-opencv python3-opencv nvblox””I get error:
ERROR: the following packages/stacks could not have their rosdep keys resolved
to system dependencies:
realsense_splitter: No definition of [realsense2_camera_msgs] for OS version [focal]
Continuing to install resolvable dependencies…I use AGX xavier, ubuntu20.04, jetpack5.1.1, realsense D455,how to solve this ?Thanks, TomHi @user19235 - The error message indicates that the realsense2_camera_msgs package cannot be resolved for Ubuntu 20.04 (Focal). This package is likely a part of the realsense-ros package, which is the ROS wrapper for the Intel RealSense cameras.Here are some steps you can take to resolve this issue:solve my error !   thanks !!!This topic was automatically closed 14 days after the last reply. New replies are no longer allowed.Powered by Discourse, best viewed with JavaScript enabled"
502,need-some-help-with-frameprimscommand,"Hello,I would like to zoom the camera automatically to an optimal distance when rendering a scene. I use the FramePrimsCommand for this. But the Result is not exactly what I expected…My expectation would be that with zoom = 0 the camera would be positioned in a way that the target prims are exactly in the bounds of the viewport. But with zoom = 0 the camera is “in” the target object and when passing zoom > 0 the distance grows with the size of the bounding box of the target prims.Exmaple (zoom=0.3):
image1280×720 45.8 KB
image1280×720 34.8 KB
image1280×720 66.6 KBAs larger the prims I focus on the camera distance grows… can someone help?ThanksCarlPowered by Discourse, best viewed with JavaScript enabled"
503,a2f-to-unity-streaming,"Hey all,I am trying to build a pipeline that feeds audiotoface data directly into unity.
I’m talking an animated avatar that gets a speech audio file and talks live to you.
I have already set up a facial test in a2f which works great, i’m just unsure how the pipeline works towards unity streaming.
Any help will be greatly appreciated.
Cheers, KurtHello @soontekk!  I recommend that you join our discord community at discord.gg/nvidiaomniverse.  You can chat with other community members on the best way to setup your A2F stream!  In the meantime, I will forward this over to the dev team for some help!We also have a tutorial video that may help here:
https://docs.omniverse.nvidia.com/con_connect/con_connect/unity/tutorials.html#unity-and-audio2face-fbx-animation-transfer-workflowHi Wendy,
I’ll join the discord for sure and check the doc too.
Thank you so much.Currently, this is not possible. Thanks for bringing this up. We will discuss this internally.Thank you Ehsan, sorry for the late reply i thought i replied but seems not so ;)Just had a meeting with the team where we are still researching the unity and unreal routes.
Our dev found that it is not possible to feed audio into the A2F streaming to feed our avatars.His words:After looking into Nvidea’s audio2face, we discovered that it is presently not compatible with streaming into Unity. The missing component needed for streaming is the blendpose value keys, which the API is unable to retrieve. Nevertheless, we found a workaround by manually adding code to the facsSolver.py file to send the blendpose keys into Unity. This solution only works when audio is played through audio2face, a feature that is unfortunately not supported from the API.Is there any way we can access this information in a method that is automatable?Thanks @soontekkThere are 2 steps to resolve this:We’ll discuss adding Play/Stop API internally.Just had a meeting with the team where we are still researching the unity and unreal routes.
Our dev found that it is not possible to feed audio into the A2F streaming to feed our avatars.His words:After looking into Nvidea’s audio2face, we discovered that it is presently not compatible with streaming into Unity. The missing component needed for streaming is the blendpose value keys, which the API is unable to retrieve. Nevertheless, we found a workaround by manually adding code to the facsSolver.py file to send the blendpose keys into Unity. This solution only works when audio is played through audio2face, a feature that is unfortunately not supported from the API.Is there any way we can access this information in a method that is automatable?Brilliant, thank you @Ehsan.HMThis functionality is the holy grail for metaverse/gaming, totally reactive NPCs, leave your own Avatar on in the game 24x7 because it has trained on your prior experience and transfer learned from an LLM. Audio2Face becomes the most powerful tool in the world when this problem is solved and public.There is of course the Nvidia ACE and Tokkio programs, but I have not had a response from Nvidia on accessing them.Any ready-made solution for live streaming for audio face in local machine?Except [NVIDIA Omniverse Avatar Cloud Engine (ACE),@iwfaI’m not entirely sure what you mean, but if you’d like to generate animation from live stream audio on your machine you can follow this tutorial:Overview of Streaming Audio Player in Omniverse Audio2Face - YouTubeThanks for your reply.I have some questions about below link you provided.In this video, does this A2F model be accelerated by TensorRT ?
If does, how much times be accelerated by TensorRT ?
ThanksOverview of Streaming Audio Player in Omniverse Audio2Face - YouTubeYes the solve is built using TensorRT, but since we lack a CPU implementation, we can’t precisely measure the extent to which TensorRT accelerates computations. That said you can see the time in the UI.Powered by Discourse, best viewed with JavaScript enabled"
504,audio2face-no-mesh-showing-up,"I’m new to Omniverse and trying Audio2Face but I get no preview at all, only black screen - I have a GeForce MSi 4090 GPU so it should be working? What am I doing wrong?
Audio2Face-104.1+release.398.bbd11d352560×1369 204 KBMy latest Log file for you
kit_20230630_085744.log (666.2 KB)Hello and welcome to the forums @craig.guessfordIt seems you have 2 grpahics cards installed. Can you disable the 2nd one (AMD Radeon) to see if it works?OK you are gonna hate me but I’m newly converted mac user that now loves my new PC & window 11 pro - how do I do that???Haha, it’s ok. Here’s how to disable GPUsSweeeeet thx Obi-wan that was it and it is working!!!This topic was automatically closed 14 days after the last reply. New replies are no longer allowed.Powered by Discourse, best viewed with JavaScript enabled"
505,isaac-sim-camera-class,"Hi When using the Camera class extension via API,
the following error pops up. I am trying to use camera feature but the simulation stops due to the following error.
Screenshot from 2023-06-26 15-06-431449×172 25.2 KBHi @ashin.anandakrishnan  - The error message indicates that the quats_to_rot_matrices function is not found in the omni.isaac.core.utils.torch module. This could be due to several reasons:If none of these suggestions solve your issue, please provide more details about your Isaac Sim version and the exact code you’re trying to run.Powered by Discourse, best viewed with JavaScript enabled"
506,webrtc-livestream-cors-error-across-network,"I am trying to use the WebRTC Livestream extension across networks. I am doing this by port forwarding.
I am using http://<public_ip>:8211/streaming/webrtc-demo/?server=<public_ip>The web interface loads but the stream is blank and the console a CORS error.I tried to look into the WebRTC Backend Livestream code and handle the CORS policy myself but I can’t figure out how to add a middleware for it.It works fine locally.The docs: WebRTC Browser Client — Omniverse Create documentation (nvidia.com) don’t really say much besides explaining that it uses STUN servers. There is nothing in the extension.toml to configure for CORS.Am I doing something wrong in how I’m hosting the server/client? I’m not really doing anything special besides port forwarding.Hi @vlad.crehul. Let me check with the dev team.Can we get an update on this? I am having the same issue.Hi. Sorry for the delay. Here’s a suggestion that I got from the dev team:Perhaps you can attempt configuring CORS Headers for streaming using the following configuration settings for your app:Note that these settings would disable all CORS restrictions on your services, which may not be desirable for a production environment. If these happen to have a positive result for you, perhaps you may be inclined to configure these options with finer-grained CORS settings before deploying (perhaps with this reference for support: Cross-Origin Resource Sharing (CORS) - HTTP | MDN)I figured it out. So the extension where we need to configure it from is from the HTTP(S) SERVER: omni.services.transport.server.http it seems that extensions.toml are configured between Omniverse Code and Isaac Sim.So pretty much the settings you are mentioning but I would indeed avoid using wildcards.This topic was automatically closed 14 days after the last reply. New replies are no longer allowed.Powered by Discourse, best viewed with JavaScript enabled"
507,physics-and-simulation-parameters-for-grasping-and-picking-tasks-objects-slipping-away,"Hi everyone,I am having trouble finding a set of parameters for smooth simulation and physics. The goal is to find a set of parameters that allows both to detect collisions without issues like objects intersecting/penetrating each other and fine-grained tasks such as grasping/picking objects without the objects slipping away from the hand.Here are the 2 main problem I am facing:This are some of the resources I used so far to fix the issue, they have helped to improve the simulation and the physics but it’s still not good:Parameters I have been playing around with:Does anyone know a good setting for these parameters or which further parameters i could tune to improve the simulation and physics? Most importantly, I need a setting that allows for grasping and picking objects.Here is the set of parameters that i am currently using:Best regards,
SabinHi,I had faced a similar issue in the past where the grasped object would fall. In my case, I resolved it by increasing solver_position_iteration_count and solver_velocity_iteration_count to around 16 or 32 and setting static_friction and dynamic_friction within the object’s PhysicsMaterial to 1.0.
I understand that your environment and settings may vary, but here are the settings I used.I hope this information is helpful to you.Thanks @smakolon385!
Our issue right now is that, even with these settings above, the object doesn’t get picked up. It seems like it is a friction issue (See attached short video)
Our objects also seem to have static and dynamic friction in the Physics material set to 1.0 . So we think that the robot itself is the culprit. Do you mind sharing how you generated the USD of your Robot? And did you change any friction settings there?Thanks a ton @smakolon385 !
We were able to get a reliable grasping simulation working.
I think the information in this thread should be added as a tutorial in the Isaac Sim documentation.The key enablers for the grasping simulation are:Hope this helps someone elsePowered by Discourse, best viewed with JavaScript enabled"
508,error-while-running-ros2-carter-navigation-example-no-tf-data-actual-error-frame-map-does-not-exist,"OS: Ubuntu 20.04.6
ROS: ROS2 FoxySo, I’m following the docs and in the Carter Navigation example and I am unable to run it as the urdf file does not show correctly in RViz2.I don’t have much knowledge of ROS so I’m unable to fix this issue.
Screenshot from 2023-04-08 09-56-542560×1440 348 KB


Screenshot from 2023-04-08 09-58-001920×1080 242 KB
I also don’t really get how to use pose estimation and particle cloud features here.Hi @eaatre -  Please refer the similar thread No tf data received from Isaac SIM (ROS2 connection) - #16 by mikaneda.
Let us know if that helps resolving the issue you have.For the question regarding pose estimation and particle cloud feature in ROS, someone from our team will respond.HiMy environment is on the Anaconda , but for my understanding your situation looks function or component conflict issue…If you can read Japanese, you will be able to pick up my resolution process below.According Rviz2 screen, map server is not working abnormal.
Even if you don’t use Anaconda, Hope my example will be your help. ;)Hi @eaatre , can you provide more details for your issues? Here are the steps to keep in mind:Please let us know if this does not helpI don’t speak Japanese but will try following this, thanks.I have re-installed everything once already, did not help.Thank you for your response.Is there below module in your environment?pipリスト抜粋 is my success time installed module “pip list” typing result.PySide2 and PyQt5And for my reference is this pagehttps://docs.omniverse.nvidia.com/app_isaacsim/app_isaacsim/install_ros.htmlYou may need to setup middleware as above reference indicated.And, According to your picture, tf (map server ) information doesn’t work.
I think no response if you type command “ros2 topic echo /tf”
Btw, I added  English sentence on the key points in my case(Previous introduced Japanese site).Hope you will be able to resolve this issue.Hi @eaatre , can you please provide a repro of your exact steps? We can take a look and help.HI,I have met the same problem as u,have u known the solution? THANK U!!!I notice that in the EXAMPLE-ROS-NAVIGATION, the nodes of action graph is about ros1.However in the tutorial of ros2_navigation,you mentioned ros2_subscribe_twist…IS there any need to modify the nodes to ros2.I met the same problem as the title says:"" no tf data. Actual Error: Frame [map] does not exist""
Screenshot from 2023-07-31 18-54-341918×410 82.3 KB
Screenshot from 2023-07-31 18-54-461885×875 166 KBScreenshot from 2023-07-31 19-59-38710×438 38.9 KB
Hi,i try to change all the nodes which are related to ros1
to ros2.That works! If u still have the problem,try it! I have to say that this problem makes me so troublesome! Actually,it is so simple! I am so foolish that i lost so much time in solving it!!Powered by Discourse, best viewed with JavaScript enabled"
509,in-add-physics-menu-rigid-body-rigid-body-with-colliders-preset-in-gray,"questions1920×978 225 KBI have three questions about this:When we apply the “rigid”  attribute to Xform, I guess it was applied to the “mesh” which is the child of Xform? or applied to the Xform?why in add physics menu, “rigid body”  and “rigid body with colliders preset”  in gray?Usually , the collision bound created in Omni is red, But I import the URDF with collison description, it is in green.
If I can not create “rigid body” because it inherited greed collision boundy from original URDF? How can I remove the green bound box?Thank you !Hi,Regards,
AlesHello,For 3. how do you get the collision bounds to show? I’ve used Physics Debug>Simulation Debug Visualization>Collision Shapes, but the bounds are drawn in pink instead of green or red.Thanks!I guess green bound was inherited from  URDF， but not inherit collision from omni system.Describe the collision in URDF.You would have to use the inbuild collision visualization based on USD data:
image724×529 61.3 KBThe Physics Debug>Simulation Debug Visualization>Collision Shapes actually displays debug visualization coming directly from PhysX SDK, those colors come from the SDK directly.AlesPowered by Discourse, best viewed with JavaScript enabled"
510,how-to-set-script-editor-open-by-default-when-launching-omniverse,"I really can’t find anywhere to pin the windows let it open by default… Where are thoses settings?Powered by Discourse, best viewed with JavaScript enabled"
511,run-omniverse-create-on-server-remotely-using-vnc,"Hi there,I have a ubuntu server with Nvidia GPUs. I want clients to be able to run Omniverse apps remotely. To do this, I have configured the VNC server on the ubuntu server. But the problem is when users connect to the server by ‘vncviewer’, they cannot launch Omniverse apps. I found that the ‘nvidia-setting’ shows a blank screen. The following is the result of ‘nvidia-smi’:

Screen Shot 2022-06-09 at 9.46.59 PM1268×680 32 KB

I have also tried with ‘xrdp’ remote desktop
I would truly appreciate your time considering my case.
Best,
MatinHello @matin.moezzi1!If you are trying to setup a server that allows 3rd party customers to connect to it, this feature is not be supported with a standard Nucleus Workstation.  You might want to consider setting up an Omniverse Enterprise account which will give you access to the Enterprise Nucleus Server where people can connect to your private cloud behind a VPN.Another update from the dev team:There are many flavors of VNC servers and some do not use the GPU to create a desktop, so there is no hardware acceleration. Plus it can be an issue of “no monitor” plugged, where GPUs stop working when no screen is attached.Yes, right. Thank you for your response.I’m in a similar situation as matin.moezzi1.I’d like to understand how to get omniverse working for remote connections to a server where all the GPUs are.Does NVIDIA offer any guidance on how to do this? Your devs mentions that some VNC tools don’t allow pass-through… do you have a list that do support pass-through?I’m in the process of purchasing the Enterprise licenses and I’ll have a headless server with all the GPUs. So, how will my team be able to remote into the server and use OV Create and/or Isaac Sim?Thanks for any information you can share on this.Hi @grep-R
VNC servers like tigervnc and tightvnc create a virtual x server display that does not render GPU engines and API such as PhysX and Vulkan. Accordingly, you cannot run OV apps using these virtual vnc servers. However, X11vnc or x0vncserver share the real x display in which you can run OV but it needs the graphics card to be connected to a monitor (if you want to run in non-headless mode). Moreover, VirtualGl is a tool that enables you to render OpenGL in virtual vnc servers, but OV apps are not based on OpenGL but on PhysX and Vulkan. As a result, you cannot run OV in virtual vnc servers.
The other option is to execute some OV apps like IsaacSim in the headless mode which does not need the graphics card to be connected to a monitor.Thanks for posting and asking this question.  It’s actually one that we get fairly often.Two applications that many of us use for exactly this purpose are:NICE DCV - NICE DCV - High-End Remote Desktop | 2D/3D support | NI SP
Teradici - https://www.teradici.com/Both of them work very well for this purpose.I had also found this problem,and even more,when the omniverse had succeeded to  launched,its window was blanked. the omnivers seldom suceeded to launched once. who can help me solve this?
1eab68e391fb45d1b75f19919ffa50e1980×1280 28.2 KBIs this on an Ubuntu VM? Do you have proper graphics drivers installed and running properly?If this is running on Ubuntu VM, please attempt to run launcher with this flag (and turn off autostart)omniverse-launcher-linux.AppImage --no-sandboxno,it runned in ubuntu server throung VNC Client MobaXterm, not under the docker env. graphics drivers had been installed right  the system had told.
sometimes omniverse-launcher can’t start,sometimes can start succededly,and sometimes although it starts but it appeal its window is blank as aboveIf the app doesn’t always run, there may be another problem, which may be causing Launcher to display incorrectly.
Since you are running through VNC, you should attempt to run with the argument I supplied earlier.I had run omniverse app using the arguments you adivsed,it can run in host server machine,but can not run throug VNC-client(mobaxterm),wrong messages list as below pic. any new good idea,please.thanks
微信图片_20230629202051736×396 156 KBWe support launcher to run through direct desktop. VNC through xterm is unsupported.Powered by Discourse, best viewed with JavaScript enabled"
512,can-not-visualize-laser-scan-in-rviz2-when-i-try-to-follow-the-tutorial-ros2-sensors,"In the fixed frame of rviz2, there is no “turtle” to be chosen.And,when i add  the topic “laser_scan”,there was the error like the picture below.Need help!!!
Screenshot from 2023-08-01 15-47-081919×951 191 KBHi @gaozhao22  - Here are a few things that could potentially cause this issue:Powered by Discourse, best viewed with JavaScript enabled"
513,the-best-and-worst-of-the-unreal-connector,"Hello Unreal Connector Users!You are truly a great community, and we love making features and tools that empower you to make some of the coolest imagery and experiences out there. We are working on the next set of features and improvements, and nothing means more to us than your input.If you could reply with an answer to any or all of these questions, it would help us get you what you want.Thank you for helping us create the best experience for you and supercharge your workflows.Hi there. Thanks a lot for all the work you guys are doing!I’m probably not the best candidate to give feedback since I’m just starting out, but every little helps.I have struggled very much with the UE5 connector, the biggest frustration is the documentation / how to use it. What does a workflow with it look like? People need detailed instructions of actual workflows, step by step, start to end.Let me make you an example: I have an existing scene in Maya, that I would like to bring to Unreal. I exported an FBX and imported that with Omniverse Create, so that I end up having a USD stage. Now I can open that stage in Maya + Unreal and create a live session. So far everything works. However when I want to save in Unreal, the confusion starts, when I save, I have to save a “Level” though, not the USD. And sometimes when I reopen that Level, the whole USD scene is somehow invisible, it’s in the outliner, but I can not see it in the viewport of Unreal, whatever I try, the Level I saved is corrupted.Let’s assume we have an existing USD file, all I want is bring that into Unreal, what are the steps I need to take? Let’s assume there are no lights in the USD and we want to use the Unreal Light Mixer to create all the “basic” lighting. I open the USD file, add the lights, then save the “Level”.Another thing are broken Materials, I have tried for two days, the materials are severely broken, even the “parent” material is full of errors. Materials synced from Substance Painter to Unreal Engine are just bright white - #5 by 3dpxlI’m more than happy to go through these things with someone so I can explain everything in detail. Once I understand everything / everything works as intended, I’m more than happy to create educational videos.Hello @3dpxl.  Thank you for your feedback!  This is something we definitely want to work on.  The frustrations you mentioned are a combination of lack of workflow documentation, missing features and what sounds like a couple of bugs.  I would very much like to go over all the frustrations you are seeing as well as what you’d like to see or your ideal workflows.  I will send you a direct message so we can continue the conversation.Biggest killer feature:
Blender - Unreal Engine - A way to choose my own custom material instance from my own master material or to automagically swap out shaders on an incoming mesh to custom Unreal ones of my choosing. I would like to be able to send a USD from blender of a mesh with 1 or more materials applied with each material consisting of a simple Principled BSDF + textures for color, metal and so on for a PBR workflow. But when it gets into Unreal engine I would love to be able to configure the master material that will be chosen to create a material instance and have those textures automagically applied as parameters to it and have those settings stored so that if I made changes elsewhere in the USD and re-imported I would not need to set up the unreal shaders again.
It is difficult to customize and automate the materials workflow at the moment non destructively and for things like texture packing multiple materials into RGB for example I do not currently see a nice fast way to automate that with USD. Maybe there is a way but I do not see it yet.
The problem I have is I still have to go in and modify the materials so that they are more efficient or to customize by replacing blender shaders with unreal engine ones like substrate shaders. It would be amazing if I could have a way to swap out blender shaders with unreal ones of my choosing by configuring this once on the first import and then have Omniverse remember this so that if I changed another part of the model and re-imported I would not need to do all that work again. This kind of non destructive workflow could be pretty amazing for level building if I did not have to convert the USD to a static mesh as that breaks the interoperability between blender and unreal and makes the workflow destructive. Which is just like FBX or GLTF (Except Omniverse is a little bit faster and a lot more organized).Biggest frustration:
Lack of documentation and tutorials showing workflow and all settings in depth. And workflow from “install to import” could do with someone with “User Journey” in their job title to take a look at it. I have come across several weird blocking issues with settings that are really easy fixes when you know how but knowing how is not included in the user interface or documentation at the moment though there are some great video tutorials on the omniverse youtube which helped a lot.What workflows does omniverse enable me to have:
I can now create pre-fabs in Blender and then very quickly import sections of a large scene into the Unreal engine level piece by piece. This is fantastic for prototyping and testing how PBR materials will look in Unreal in terms of look and feel and color management or for quickly building scenes in Blender and sending the entire thing over to Unreal non destructively with a few clicks of a mouse is absolutely fantastic.Powered by Discourse, best viewed with JavaScript enabled"
514,vroid-studio-omniverse-with-hair-a-cloth-physics,"VRoid Studio (VRoid Studio) is free software from Pixiv for creating anime style characters. It outputs characters in a VRM (https://vrm-consortium.org/) format used by a number of VR apps (like VR Chat), which is actually GLB. If you rename a .vrm file to .glb, Create can import it successfully. It makes it quick to create 3d avatars with rigging and blend shapes.
image1027×1033 113 KB
I am experienced in Unity, but new to the Omniverse tools. I was exploring how hard it would be to add hair bone physics and/or cloth physics to characters. (For Unity, if you import a VRM file using the UniVRM package from GitHub, it adds crude hair and cloth physics automatically.) Reading around the forums I see threads basically saying “its hard” to add physics. Looking at the Bunny example_cloth sample in Create made it look possibly easy…?Are there any examples around on how to add hair and cloth physics to a model? VRoid Studio is nice because you can create avatars quickly with minimal knowledge, even if just for prototyping. All the characters also have very regular structure, making an automated script easier to write (e.g. to add standard capabilities). But without hair and cloth physics, they lack the depth to make them useful in a real project.
image1446×1177 70.1 KB
I’m also interested in this thread.
(Perhaps this importer for Unreal Engine could be beneficial: VRM4U/README_en.md at master · ruyo/VRM4U · GitHub)Could be worth a try for sure. I tried the Unity version (UniVRM), but the Unity/OV connector is newer and the character that came across did not work. The Unreal connector is more mature - definitely worth a try. I will however note that you still need some sort of behavioral code in OV to do the hair movement (mesh, bones, whatever).We are working on cloth and hair features, but we don’t have a release date yet. So please stay tuned!Until “the real thing” is available, are there any examples that could be used as a simple stop gap measure? Even a little bit of physics for hair bounce can make a huge difference. (So lots of stiffness and damping.)For example, here is a character with hair bones shown.

image1093×1006 72.2 KB

Here is the stage hierarchy

image907×993 102 KB
I was looking at the “Rigid-Body Ropes” physics sample and it looked pretty close. Adjustments I want to make are:I was just wondering if there was an existing sample that already does the above before I explore the rope demo deeper. Even without colliders (so hair disappears into the body) this would be useful. I was just not sure if Articulators or Physics Joints are closer to what I want. Hints in the right direction appreciated!Hi,
Right now, there is no way to connect the bodies with the SkelJoint other then using some separate python script to update the poses.
You would have to create a separate hierarchy through joints, like the rigid body rope demo does. Then the rigid body transformations would have to be applied to the SkelJoint.
For next release (should happen in few weeks) there should be a way to add rigid body directly to the skel joint, for the current release only a separate hierarchy would work I am afraid.AlesJust an update, I created a blog on my efforts to use particle cloth for hair with attachments to the skull. Some problems, some success. Kit 105 has some new capabilities which could be useful.But I think the short term solution is to implement some kind of behavioral script or action graph to implement slight movements and gravity based on bone animations. 105 seems to have improved collider support with skeleton animation making this at least feasible in concept.impressive work @alan.james.kent
looking forward to see the 1st eposide. Btw, it looks like that the Trailer  is missing from the website.And yes, for now you need to combine all animations generated in A2F in another DCC for full control.I am only maintaining the ‘blog’ section of the website at present. The videos there were rendered using Unity. I was trying to see if I could do them with Omniverse instead, but it’s been slow going. There are lots of nice technologies in Omniverse (beautiful cloth physics, great rendering, cool AI for audio to lip sync) but they are hard to combine in a reliable way into a complete solution.Oh, and trying to do it with free tools adds to the complexity! ;-)Just for clarity @Ehsan.HM , here are a list of some of the open Omniverse bugs in this area (retested with the final 105 release out today - still present). Just wanted to make sure these were on a bugs list somewhere to be addressed.Not sure if a bug or a feature:Here is a video as well, showing a series of things going wrong (it was 105 beta, but I have not seen anything fixed so reusing the same video). Note: some things do work, but as the video progresses more and more things go wrong (including an OV crash).Blog: Omniverse 105 (beta) Colliders for Particle Mesh Hair and Clothes – Extra Ordinary, the SeriesMade some progress. Mega capsule across shoulders just to prove it works.
image394×574 53.1 KBI need to research Rigid Body more as adding a Rigid Body on the character did two things:Hair physics also improved a lot when I increased the Particle System / Advanced / Solver Position Iteration Count from 16 to 32.Still having problems such as:Powered by Discourse, best viewed with JavaScript enabled"
515,mdl-creation,"Hello,
Is there a way we can convert sbsar or ssa file formats into mdl files directly? Also I wanted to ask if there is a tutorial on how one can create an mdl file from BaseColor, Metallic, Glosiness, Normal, Roughness, Occlusion png files.
Any support is much appreciated!Powered by Discourse, best viewed with JavaScript enabled"
516,rtx-loading-in-all-omniverse-apps,"
image1347×1026 87.7 KB

Hi,I am having problem when I open any of the Omniverse apps. I have newly installed NVIDIA GeForce RTX 3060. And it is showing the following error along with the screen just saying RTX Loading.Hi @akshay.goyal - Can you confirm that you have installed driver matches the requirements mentioned in this document?
https://docs.omniverse.nvidia.com/app_isaacsim/app_isaacsim/requirements.htmlHi @akshay.goyal. Please also share the full log file.
Yes. The driver is updated to the latest!Hi,
It seems that I had not installed Create and Code apps. Once I installed them, and then restarted, now Isaac Sim is working. Thanks for your time and help.Best regards,
AkshayThis topic was automatically closed 14 days after the last reply. New replies are no longer allowed.Powered by Discourse, best viewed with JavaScript enabled"
517,primvar-reader-for-points-prim-not-supported,"Hello Nvidia,
I am trying to color a point cloud. I have the values in the displayColor. These show up in usdview and in houdini, but not in omniverse. I added a primvar reader to the shader and connected this to the shader, but this still does not show up. Is there a way to read the per point colors on a points prim?thanks,
KoenPowered by Discourse, best viewed with JavaScript enabled"
518,approximating-physics-calculations-for-custom-imported-robot,"I’ve successfully imported a custom robot to Isaac Sim via URDF. The physics seem to be working properly, but the calculations are very slow. Is there any way to introduce approximations of the robot as a box (instead of a collection of complex components) to speed up the simulation?Hi,
I would be a bit surprised if the bottleneck would be the more complex collision approximations. You can eventually replace them by box approximation by selecting those and changing the approximation type. However first I would recommend you to enable omni.physx.fabric extension to speedup the results write. This would skip the USD update and write to Fabric.
If you enable the profiler extension and press F7 you should be able to see where the time goes, you will have increase the CPU depth for the profile zones.
Or feel free to send me the USD file through a private message I can take a look what could be improved and suggest improvements.
Regards,
Alesomni.physx.fabricIs there a way to enable the omni.physx.fabric extension from the Python API? I’m already importing physx with omni.physx.acquire_physx_interface(), but I haven’t been able to find much documentation on using it to enable/disable extensions via code.@AlesBorovicka
I can not find physx.fabric extension in isaac sim 2022.2.1 as follow:
Screenshot from 2023-07-27 11-09-181150×179 10.5 KBHow to enable it?Is there a way to enable the omni.physx.fabric extension from the Python API? I’m already importing physx with omni.physx.acquire_physx_interface(), but I haven’t been able to find much documentation on using it to enable/disable extensions via code.Yes, thats possible using the extension manager from python:AlesAh sorry you are using IsaacSim that is using a bit older version of kit (104.2) where this extension was called omni.physx.flatcache. This got rebranded in 105.0 to Fabric (Flatcache → Fabric).AlesPowered by Discourse, best viewed with JavaScript enabled"
519,i-tried-running-3-6-alpha-and-now-i-get-a-console-app-opening,"If I don’t touch the console app, I appear to be able to work in Blender 3.6 Alpha from Omniverse Launcher. If I close the console app, Blender closes.Earlier it showed me a message 'Don’t expect plug-ins to work in an experimental version.Is there a way to suppress this console app from launching. Is this supposed to happen? ThanksPowered by Discourse, best viewed with JavaScript enabled"
520,omniisaacgymenvs-skrl-for-iiwa-reaching-task,"Hi @toni.sm !I’m using the skrl you wrote really well.In particular, I am testing by customizing some based on your iiwa reaching task.I changed a part of def pre_physics_step of reaching_iiwa_omniverse_isaacgym_env.py as follows and added target orientation.My questions in this regard are:1. DLS IK convergence failure
The robot’s end-effector goes near the target, but does not decrease below the error threshold (<0.01).2. Orientation randomization
As a result of target orientation randomization evaluation, there was almost no convergence. I wondered if it was a joint limit problem with the robot, so I released all joint limits (-540 to 540) and evaluated it, but the results were similar.
Can you share any experiences with these issues?Thank you in advance !@toni.sm - Can you please review this question and help the user?Hi @swimparkSorry for the late reply, I had missed this topic in the notification history.Please, note that target (sphere) and iiwa_link_7 (used to compute IK) don’t have aligned coordinate systems. The robot will try to reach the sphere using the unusual joint configuration shown in your videos.Screenshot from 2023-07-06 21-20-50908×437 144 KBRegarding to IK convergence, the training is designed to learn how to move the iiwa_link_7 to a distance of at least 3.5 cm from the center of the sphere. Also note that iiwa_link_7 is 4.5 cm displaced in the z-axis of what should be the end-effector, which, being defined in the iiwa14.urdf file with a fixed joint, was merged with iiwa_link_7 under the same prim (because the option to merge fixed joints was activated during import).Regarding to the orientation randomization, what did you do to release all the joint limits?
This information may help to identify the problem@rthaker thanks for reminding me of this topicHi, @toni.sm !I fixed the problem by changing the stiffness and damping of the robot.It successfully worked, but I don’t know why the K and P values of the robot actuator affect the result.Thank you.Powered by Discourse, best viewed with JavaScript enabled"
521,module-import-error-win32-file-not-found-and-subprocess,"Hello,I am working on an extension that references a Python FMU library (FMPy); however, when I run related code in the extension, I get the following error (attached log file as well):win32Error1059×131 6.49 KB
win32Log.txt (1.4 MB)The library itself references the win32file module, but Omniverse is unable to recognize it. It seems that installing pywin32 using pyapi (pipapi.install) is a solution online, but that does not seem to fix the problem. I’ve additionally tried using a subprocess to call a separate file (subprocess.run/call) containing fmpy but no luck either.Any help would be appreciated!Powered by Discourse, best viewed with JavaScript enabled"
522,isaac-sim-license-release-benchmarking-dataset-using-isaac-sim,"Hello,
I’m planning to write the Dataset paper by utilizing the basic scenes (hospital, office, warehouse) provided in NVIDIA Isaac Simulator. At this time, it seems that the dataset built by Isaac Simulator should be released due to the dataset paper properties.In this case, I would like to ask if there are any other problems such as license even if the dataset built using Isaac Simulator is publicly released.
Can i release the benchmarking dataset publicly?Thank you :)Hi @phr0201  - You can find more information about licensing here: NVIDIA OMNIVERSE LICENSE AGREEMENT — isaacsim 2022.2.1 documentationLet us know if you have more questions.Powered by Discourse, best viewed with JavaScript enabled"
523,desynchronization-issue-with-animated-blend-shapes-eyebrow-and-eyelash-animations-are-out-of-sync-with-the-facial-animation-in-maya,"“Desynchronization Issue Using Animated Blendshapes: Eyebrow and Eyelash Animations are out of sync with the Facial Animation in Maya”Hello, I’m experiencing a problem with my head/face animation when using animated blend shapes. The animations for the eyelashes and brows are not synchronized with the facial animation. Additionally, they exhibit more deformation compared to the facial expressions. I’ve been grappling with this issue for weeks, so any ideas would be greatly appreciated. :) My project involves a basic custom Maya head mesh with distinct left and right eyes, eyebrows, and eyelashes. I’ve outlined my workflow below…Step 1: I import my custom head mesh (Not a Metahuman or Real Illusion Mesh) from Maya to A2F.Step 2: I create a Full Face Character Setup (Character Transfer), establish correspondences, and add the eyelashes and eyebrows as additional dynamic meshes.Step 3: I go through the Mesh Fit and Post Wrap processes.Step 4: In the Stage window, I select the Head result, Mesh, then choose the dynamic Eyelashes and Brows Meshes and apply the Prox UI.Step 5: I connect Audio2Face to the gray Mark head using the “english_voice_male_p3_neutral.wav” file using all of the default settings. As a result, everything, including the face, both eyes, eyebrows, and eyelashes, moves together as expected.Step 6: I generate Blendshapes clicking on the Preview in Stage, creating 46 NV Poses. I save those as a .usd file and import them into Maya.Step 7: I select all 46 heads, then the rig head; I choose “deform” and add blend shape. Similarly, I do the same for the brows and lashes, selecting all 46 of them, then the corresponding mesh parts, and applying the blend shape.Step 8: I create a group in Maya containing the head, brows, and lashes. I export this group using the Omniverse connector as a .usd Skeletal Mesh and then import it into my previous A2F scene.Step 9: For Blendshape Conversion, I select my custom result head mesh still driven by the grey Mark as the Input Anim Mesh. Then, I select the imported blend shape group head as the blend shape mesh. I set up the blend shape solve and export the JSON for the face at 30 fps. I repeat the process for the brows and eyelashes.Step 10: In Maya, I run the import JSON script for the face, brows, and lashes. This process successfully creates animated keys for all of the blend shapes but plays out of sync.Here is a video of the issue in Maya:
Powered by Discourse, best viewed with JavaScript enabled"
524,omni-isaac-sim-xr-steamvr-2022-2-1-and-depenencies-not-showing-inputs-from-vive-pro-2-controllers,"HiI am trying to find a way to interact with the robot scene and eventually control robots from within VR. that I built, Isaac sim does not update the inputs from the controllers. Any tips would greatly be appreciated!Hi, I am also stuck with this, it would be really great to get some insight about this issue.Dear dev team at NVIDIA Isaac Sim, please share some thought about this. Thanks!Hi @jmorrissette - Can you please elaborate “Isaac sim does not update the inputs from the controllers”?Hi, I can elaborate on that, the problem is that when I try to navigate through the scene using the VR controllers, there is not such navigation in there. Contrary to Isaac 2022.1.1, I can navigate without problems.My extension cannot migrate to Isaac 2022.1,1, I have to go with 2022.2.1I have also tried with Isaac 2022.2.0 but that is the same.If you need further details, please let me know, thanks!Hi I got in contact with some of the developers and they told me XR support for other applicaitons (isaac sim) with kit 105. However since it is still in beta it will probably need more time. In the mean time I found that creating a live sync session with create XR was the only reliable method for having VR working.Hello!As I mentioned in other forum post. Navigation issue will get fixed in upcoming Isaac Sim release.Powered by Discourse, best viewed with JavaScript enabled"
525,maya-omniverse-connector-error,"I don’t know what exactly happened, but once I downloaded the Maya Native Connector, I can’t open Maya correctly. But the background is still running, however I can’t open Maya’s window.  I have alreadly reinstalled the Maya2023.3 and uninstalled the connector, but the error is still there.
Can someone give me a little help?
bandicam 2023-06-06 12-26-29-7211091×472 135 KB
This is the error.
Powered by Discourse, best viewed with JavaScript enabled"
526,strange-behavior-when-trying-deformers-for-hair,"I am getting strange behavior in Create/USD Composer when adding deformer body to hair. I am trying to follow DiscordHere is my stage immediately after loading the file.

image1550×747 73.8 KB
I hit Play. The cube bounces and wobbles, but the hair falls through the ground. It does stop actually, but just below ground surface. Its like it is offset by some amount.

image1615×692 62.5 KB
I hit stop. The hair does not return to its start position (the cube does).

image1569×719 74.3 KB
I turn on the debug view for the deformable mesh. The hair jumps back to the top of the head.

image1418×661 48.8 KB
I turn the debug view off, the hair jumps back to the hips.I am trying to work out how to get the hair to attach to the top of the head to make the hair bounce around a bit. I have tried a few combinations of collisions, rigid body (the character body falls over!), and more. The video unfortunately rushed the bit about attaching the cape to the body (I know its a cloth instead, but was hoping the same thing would work for the deformable body).I expect some problems as the hair almost certainly goes through the body at points, so it might just be giving up and ignoring the collision stuff. But even if I lift the character off the ground, the hair still falls through the floor (the cube does not). Well, to be more precise, it falls through the floor if the debug mode is off. If its on, it stops at floor level.

image1286×642 37.5 KB
So putting the debug mode on seem to have zero offset, but when I turn it off, the hair offset is changed lower. Any ideas appreciated! I am trying to get a little bit of bounce and movement in the hair.Oh, and if I add too many colliders and things Omniverse Create locks up completely and I have to kill the process.Powered by Discourse, best viewed with JavaScript enabled"
527,omni-isaac-core-utils-torch,"https://docs.omniverse.nvidia.com/py/isaacsim/source/extensions/omni.isaac.core/docs/index.html?highlight=omni%20isaac%20core%20utils%20torch#module-omni.isaac.core.utils.torchthere is no document for this part?Hi @liuyuanyuan.sh - Thank you for reporting this. We are going to fix it in the next release.Powered by Discourse, best viewed with JavaScript enabled"
528,3ds-max-2024-1-fail-to-load-omniversemaxplugin-dll,"Omniverse: Fail to load C:\Users\vovap\AppData\Local\ov\pkg\3dsmax-connector-201.1.0\Max2024\Plugins\OmniverseMaxPlugin.dllSystem information
3ds max 2024.1 Build 26.1.0.2270
64-bit Windows 11 Pro 64-bit (6.2, Build 9200)
AMD Ryzen 7 4700G with Radeon Graphics          (16 CPUs), ~3.6GHz
32768MB RAM
NVIDIA GeForce RTX 3060 Drivers 31.00.0015.3640
2560 x 1440 (32 bit) (75Hz)Hello cin,
This is a strange problem. In my test, Max 2024.1 works well with Omniverse 3DS Max Connector. I guess it is something goes wrong with the settings of 3DS Max. How about you rename two 3DS Max folders and try again.
C:\Users\user name\AppData\Local\Autodesk\3dsMax\2024 - 64bit
C:\Users\user name\Documents\3ds Max 2024
You can rename the above two folders. Then 3DS Max will generate right settings.
If it does not work for you, you can delete the new generated folders and rename your folders back.I reinstall Windows. Install 3ds max 2024.1 and connector. Not help.You do not have to reinstall Windows and Max 2024.1. You can uninstall Connector and make sure Max runs well.
You do not have to go to Launcher. You can go the the installation path of Connector in the above picture. At the root of Connector, you can run MaxSetup.exe to uninstall Connector.
Maybe you have one old Max Connector on disk. You can run UninstallOld.exe which is at the same path as MaxSetup.exe.
And please run UninstallOld.exe with Admin. It will help remove old Connector.
After uninstallation with MaxSetup.exe and UninstallOld.exe, you can run Max and quit. Then you can install Connector again with MaxSetup.exe.If you formatted disk when installing Windows, it will not be the issue with old Connector. If not formatting disk, it could relate to old Connector.Which new version of connector available? I have 3dsmax-connector-201.1.0 for 3ds max 2024.1.
It not work on fresh installation of 3ds max 2024.1 which installed on fresh Windows 11.image1681×870 214 KBThis is a strange problem. In my test, Max 2024.1 works well with Omniverse 3DS Max Connector.Try on not development machine. May be it work only on your computer.You could be right. It may relate to Windows 11. I will find out a way to test it.I tested it on Windows 11 virtual PC. It runs well. Need think more on why it fails on your side.
image1024×768 81.2 KBHi Cin,
Do you have other plugins installed except the default things in Max 2024.1?No, I not install any plugins.
My order of actions:Hi Cin,DId you run Max before installing Omniverse? BTW, Do you mean Omniverse Launcher?Hi cin,
Thank you very much!Hi Cin,
Can you tell me which one is Russian?
image702×555 16.2 KBCould be русскийRussian is РусскийGet it. Thank you!Hi Cin,
I tested with Windows Russian and Japanese. But I did not reproduce the problem. I guess it could be special build of 3DS Max, e.g. Max for Russian.Solution: Enable UTF-8 support in Region Settings window.This topic was automatically closed 14 days after the last reply. New replies are no longer allowed.Powered by Discourse, best viewed with JavaScript enabled"
529,applying-transcluent-material-to-object-with-multiple-materials-turns-everything-black,"Hi all,as in the title, there is a bug that if I have a object with multiple materials and make one of them trascluent by appying OmniGlass / OmniSurface with trascluency to it, the entire object turns black.GLASS_MTL_BUG_021920×1053 131 KB
GLASS_MTL_BUG1920×1049 130 KBPowered by Discourse, best viewed with JavaScript enabled"
530,can-not-launch-any-python-file,"Hello, so I have few problem with isaac sim installation related hereBut i thing it’s ok, I can launch Isaac sim websocket. but now, I am pursuing with the tutorial of isaac sim.(https://docs.omniverse.nvidia.com/app_isaacsim/app_isaacsim/tutorial_required_workflows.html)So, I can manually launch the simulation  of follow target robot, by selecting it, in the menu at upper left of the screen in the Isaac examples menu.

isaac exe menu1705×894 183 KB

isaac followtarget1702×908 126 KB
But when I tape this command in the isaac folder: ./python.sh standalone_examples/api/omni.isaac.franka/follow_target.py
I have this error :CLI follow_target (11.2 KB)I don’t understand why. i can launch manually the simulation but not with command line and python file.Virtual machine Ubuntu 18.04 os
Quadro RTX 6000
driver : nvidia 470.94
cuda : 11.4Thank you for your help.I found this but i don’t know what to do with this information.
https://docs.omniverse.nvidia.com/app_isaacsim/app_isaacsim/known_issues.htmlWhen running samples from the standalone_examples via ./python.sh they might not exit/shutdown cleanly:[Warning] [carb.tasking.plugin] Leaking 1 carb::tasking::Counter instances!
Shutting Down Complete
./python.sh: line 27:  8164 Segmentation fault      (core dumped) $python_exe $@
There was an error running pythonThe standalone python samples cannot run headless by default.
You will need to modify the example to set
""headless"": False
to True
and then see Python Environment — Omniverse Robotics documentation
standalone_examples/api/omni.isaac.kit/livestream.py
on how to enable the livestream client for a standalone python sampleI also have the same issue, I can not even load the robot from the app itself!
I did change the
“Headless”: True
still encountering the same error in CLI!
why is this example still not loading even in the application?

Screenshot from 2023-06-05 17-22-361845×1012 141 KB
Hi @ahn.paf - Have you gone through the latest documentation?https://docs.omniverse.nvidia.com/app_isaacsim/app_isaacsim/manual_standalone_python.html#livestreamHi, I actually still cannot load the example through the code, I should only import it from examples in the toolbar, in this way, it is not tracking my curser as it should be.Powered by Discourse, best viewed with JavaScript enabled"
531,omniverse-to-unreal-engine-usd-workflow-not-usable,"I think there is a bug when it comes to exporting USD to Unreal Engine 5.2 via Omniverse Nucleus. The textures are not displaying when dragging the USD into the Viewport.When I export using Omniverse and Nucleus over localhost if I want to see the textures in the scene I have to manually convert the USD by right clicking on the USD file and selecting “import usd” to convert the USD over to a static mesh to be able to see the textures in the scene. I do not want to have to do this I want to be able to preview the USD in the viewport without unpacking it to static meshes and materials and textures in the unreal content browser.Alternatively if I just export to USD from Blender and skip using omniverse the meshes and textures do load in correctly and I do not need to make any extra steps to convert the USD over to a static mesh.But I was hoping that omniverse would act like a bridge between blender and unreal engine and allow me to send an entire USD scene over for complex scenes or groups of objects so I can preview them in Unreal in USD format in the viewport. Then allow me to make changes in blender to the meshes or materials and one click update them in Unreal to see those changes propagate in the viewport … then when 100% sure no more changes are needed to the meshes or position or textures/materials I could convert the USD over in the viewport to static meshes and materials. I think that would be a pretty neat workflow if other things could also be automated like converting materials over to a chosen material instance  and then assigning textures to the relevant parameters.If omniverse cannot do this then I do not see any benefit to using omniverse and nucleus? I might as well stick to USD or FBX ?What do you think?Hi @dirkteucher,I hope you don’t mind if I defer to my Omniverse colleagues as they know much more about the specific connectors to Blender and Unreal.I am sure @WendyGram will find the right people to comment and help.Thanks!The answer to this question is here Blender 3.6(Exchange version) to Omniverse to Unreal Engine 5.2 with Nucleus (Localhost) does not seem efficientThanks all who looked at itThis topic was automatically closed 14 days after the last reply. New replies are no longer allowed.Powered by Discourse, best viewed with JavaScript enabled"
532,hotkey-changes-for-2023-1-0-1-1,"“L” for Lights and “C” for cameras are now using the “Shift” key as well.
Shift + L"" for lights
Shift + C"" for Cameras
Grid remains as “G”Powered by Discourse, best viewed with JavaScript enabled"
533,systemerror-initialization-of-raised-unreported-exception-when-loading-a-schema-extension,"Hi all,I have created a schema extension to provide some project-specific prim types. The extension works fine when I configure it to be loaded in the Isaac Sim Extension Manager.Now I want to load it in a different context: I want to run kit from the command line in headless modeI get this as output:Thanks for any help
BrunoThis is the log output with info messagesInterestingly, the problem disappears if I add omni.usd.schema.physics as a dependency. Strangely, my schema extension should not rely on omni.usd.schema.physics…Hi Bruno. How are you creating the schema? In Kit 104, I don’t think we’ve provided a public way to compile the schemas for OV  and codeless schemas aren’t supported either.Hi @mati-nvidia,
I am using this: GitHub - NVIDIA-Omniverse/kit-extension-template-cpp: Omniverse Kit C++ Extension TemplateWith some help from Ales.Best,
BrunoHi Bruno,
is your track joint not derived from UsdPhysics Joint? Then the missing dependency would make sense. Note that in 105.0 the UsdPhysics will be part of stock USD, so this dependency would not be required.
It might be that if you tried to modify one of the schema examples that I gave you that the UsdPhysics is somewhere there added. It could be in your schema file for example at the top part.Regards,
AlesHi Ales,the extension I am talking about right now is not the TrackJoint extension, but something related. It only includes a couple of new prim types that have no Physx dependency.By the way, the problem also goes away if I enable omni.usd along with dbs.ipm.schema.tracksystem. The dbs.ipm.schema.tracksystem has only omni.usd.libs as a dependency.Powered by Discourse, best viewed with JavaScript enabled"
534,need-help-to-build-a-digital-twin-and-deploy-it-as-app-in-both-desktop-and-vr-headset,"Hello All,I am a compete beginner, but want to use okniverse to develop digital twin of a cpu box.I browsed through the courses offered by NVIDiA. However could not find much help.Can someone please guide me with this ?Thanks,
JayHi! If I were you, I would master a 3D modeling software like Maya, Blender, or SketchUp. And just import the CPU box model into Omniverse using the appropriate connector (but don’t forget to assign materials and textures to make your model realistic:) ).Thanks for your response. I have done that. Now, I want the end users to access it via either desktop or mobile phone. I am stuck here.Powered by Discourse, best viewed with JavaScript enabled"
535,would-isaac-sim-support-contact-force-tensor-on-rigid-bodies,"Hello, I am preparing to transfer from Isaac Gym to Issac Sim.Isaac Gym had a simple and user-familiar API for RL, and It has been of great help in my work.
However, transferring to Isaac Sim would be vital to merge visual perception in my workflow.I think Isaac Sim is more functional, but a bit trickier to do RL tasks with.
One trouble is that the API of Isaac Sim does not enclose all functions included in Isaac Gym API.
Especially, I used acquire_net_contact_force_tensor() in Isaac Gym to model 3-axis force sensors in RL.So far as I have experienced, it seems like there is no same function in Isaac Sim.
I guess contact sensors do not support the tensor API, and the articulated force sensor was unstable and not related to the contact forces.Is there any other way to get the contact force tensor that I could not find?
Or, would the future update will include the function?Hi - Sorry for the delay in the response. Let us know if you still having this issue/question with the latest Isaac Sim 2022.2.1 release.Hi - I  was also making the transition from Isaac Gym to Omniverse Isaac Gym Envs and the documentation is not that clear - but there is access to tensors for rigid body contact force  for multiple actors in rl learning.here is the gist…using the OmbiIsaacGymEnvs task framework e.g. anymal.py but no need anymore for the anymalview import which just serves to confuse and is nor helpful if you are using your own modelfrom omni.isaac.core.articulations import ArticulationView #if you need an articulation view (likely)
from omni.isaac.core.prims import RigidPrimViewdef set_up_scene(self, scene) → None:
…self._robots = ArticulationView(prim_paths_expr=“/World/envs/./biped"", name=“biped_view”)
…self._lfoot = RigidPrimView(prim_paths_expr=""/World/envs/./biped/LeftFoot”, name=“lfoot_view”, reset_xform_properties=False, track_contact_forces=True, prepare_contact_sensors=False)
…scene.add(self._robots)
…scene.add(self._lfoot)def update_buffers(self):
…self.contacts_left = (self._lfoot.get_net_contact_forces(clone=False)[:,2] > 0.00)
…print(self.contacts_left.shape)
… print(self.contacts_left)so this example takes the left foot prim from my biped.usd file for my 512 robot instances during training and detects if there is any contact force in the z plain (>0.00) that gives you a bool tensor to see if it is on the floor or not…in the anymalview example in omniisaacgym look like they do combine the articulation view and the net force contact sensors but this version works for me.Devs please look at documenting this better!! The two feature in the documentation that really are needed is explaining in general how views are handled, and how to obtain rigid body contact forces which is neede for pretty much all locomation tasks
Devs: does having fewer views/combining views make the rl more efficient?I found that the same trick works! I truly appreciate your generous assistance.Hi user15623 really glad to share!
its not really a trick as the contact forces must be calculated in order for the physics to work - it just exposes them which I think is NIVIDA’s intension but isaac sim is still in its maturing phase so they need to understand what we need.they have now implemented a RigidContactView now (see NVIDIDIA’s reply here)please share your project if possible(I apreciate you  may be limited due to reasearch restrictions) as it sounds really interesting. Presumably a manipulation task.sincerely
Sujit VasanthHi user15623 and sujitvasanth
As you have already figured out we do have isaac-sim APIs to get the contact data from the physics engine.
See this documentation page which explains about the different Isaac sim views. We are in the process of extending it to include more details. We are also extending the RigidContactView for the next release of Isaac sim to provide more detailed contact data.Powered by Discourse, best viewed with JavaScript enabled"
536,contact-sensor-reading-problem-can-not-get-the-right-data-from-contact-sensor,"Hey everyone:I’m trying to acquire the contact sensor data from Isaac sim.
First I add the sensor in isaac sim gui as the following:

1685007957767882×907 64.6 KB

Then I write the code in my task.py
The followings are my code:and the output is:
[]
and I also tried the function: get_contact_sensor_raw_data but not work.
So how can I get the data from the contact sensor?Thanks in advance!can anyone help me?Can you try following steps?Make sure that contact sensor is set up correctly in the scene. Make sure its position, orientation, and other settings.Contact Sensor — Omniverse Robotics documentationEnsure sensor path in your code matches the exact hierarchy of the contact sensor in your scene. You can do so by inspecting the scene hierarchy in Isaac Sim.Update the simulation steps if needed. Sometimes, sensor data may not be available immediately due to simulation steps being too low.Run the simulation and check the output of sensor_data. If the contact sensor is set up correctly and simulation is running then you should see the sensor readings in the sensor_data variable.Make sure to start the simulation before trying to get sensor readings. Let us know if you still see the issues.Powered by Discourse, best viewed with JavaScript enabled"
537,asset-validator-unable-to-repair-material-binding-on-any-usd-file,"Hello,Since the introduction of USD Composer 2023.1.0, I have had issues with elements in my stage not displaying with a material.  I can not apply new ones or get the existing ones to appear.  In another thread, this issue was brought up and understood to be related to a change in the Material Binding API and that the inbuilt Asset Validator can scan the stage or file from the content browser and apply the fix to conform the file to the new standard.My issue is that, no matter how I perform the scan and regardless of file, the Asset Validator fails to apply the fix due to an exception on a cloud asset not in the scene, nor even related to the element being repaired.I have tried to find this reference in the document, I have tried running the Validator on the USD file from the content browser instead of having it opened, and I have tried importing the asset in another application and exporting it to see if the other application would create the appropriate binding.If it helps in the diagnosis, the original USD was exported from the NVIDIA maintained USD branch of Blender 3.6 as well as 3d Studio MAX 2024.  I also have the previous release build of Omniverse Create 2022.3.3 still installed as it is the only version that will read our stages properly.Thank you.
BrentAn Update:  I just uninstalled all of the Omniverse apps and the launcher, deleted the remaining folders and registry entries, rebooted and installed fresh and I still have the issue.Screenshot 2023-07-21 1647191626×859 106 KBWhen I navigate to the mentioned path, https://omniverse-content-production.s3-us-west-2.amazonaws.com/Assets/Skies/2022_1/Skies/Dynamic/CloudySky.usd
I am able to download the asset so it is there.Not sure why validating the asset keeps reaching for this. And why it is failing to find it.Powered by Discourse, best viewed with JavaScript enabled"
538,how-can-i-generate-custom-label-for-semantic-segmentation,"I’ve applied semantic data via ‘Semantics Schema Editor’.
But when I try to record the data with ‘Synthetic Data Recorder’, the segmentation results are as below.
How does the ID is mapped by a class? Is it the sequence that I add the semantic schemes?{“0”: {“class”: “BACKGROUND”}, “3”: {“class”: “robot”}, “4”: {“class”: “objects”}, “6”: {“class”: “person”}, “2”: {“class”: “box”}, “7”: {“class”: “ceiling”}, “5”: {“class”: “fence”}, “9”: {“class”: “floor”}, “8”: {“class”: “wall”}}First, How do I customize the class ID number? For example I want to let floor class as class number 1.
Next, I can not find class “1”. Where can I find it? Is there any python scripts or something?Thank you.Hi there,the id is uniquely assigned for each prim, using the instance ID annotator you should be able to get the mapping from the id number to the prim path.Let me know if this does not work for you.Best,
AndreiPowered by Discourse, best viewed with JavaScript enabled"
539,documentation-url-404-not-found,"Hi there,
I got an error page as getting into some of your documentation URLs in these few days, not sure why this is.The error page content is shown below:Here are the errors I got from the URL:Regards,
Darien WeiPowered by Discourse, best viewed with JavaScript enabled"
540,exporting-point-cloud-to-ply-with-semantic-labels,"I have followed Example 2 of the “Point Cloud” section within “Annotators Information” documentation in order to export a .ply file of a scene using Open3D.  I had no issue retrieving the points and exporting the file, but I would like to include semantic labels (the object class each point belongs to, in other words), but have been unable to do it so far. Any help concerning this issue is greatly appreciated. Thanks in advance.Hi @p.faustino , it looks like open3d does not support adding semantic infos to each point. Maybe you can try other library that supports writing semantic data out to a ply file?Hi @jiehanw thank you for your answer. I was eventually able to achieve what I wanted to do by associating semantic information from the point cloud annotator to an RGB color and passing it to open3d as point colors. Maybe not the best approach but it suffices to have labeled point cloud data.Powered by Discourse, best viewed with JavaScript enabled"
541,save-usd-to-external-nucleus-from-python,"Hi,this is related to How can I write a file to nucleus?I am trying to write to Nucleus using omni.client.write_file() (seehttps://docs.omniverse.nvidia.com/kit/docs/client_library/latest/docs/python.html). I want to do this from within the Omniverse Kit Docker container. I have only enabled one extension: omni.client.This is the code:This is what happens:The container complains that xdg-open is not available (does write_file really use xdg-open??). So I install it into the container. Now I get this message:Also, how can I set the Nucleus credentials when using write_file?Thanks a lot
BrunoHi Bruno. I moved this to the Nucleus forum.I think I have found a solution.Firstly, it seems I can write a USD file to Nucleus using pxr.Usd.Stage.Save(). I was not aware of this, because my idea was that the Pixar USD library should not know how to save to a filename starting with “omniverse://”, but it is actually possible.The xdg-open messages only come when there is no authenticated user, so Omniverse tries to open a web dialog for authentication. When I set OMNI_USER and OMNI_PASS, this does not happen.This is the correct steps you should follow.@bruno.vetter For those of us playing along at home, could you post an example of the code you got working? I’m looking in to saving USD files on the Nucleus server as well but I’ve noticed that Usd.Stage.CreateNew(usd_file_name) is what creates the file. I’d like to avoid creating a file in one place and then saving a second file in another place. Do you specify the nucleus server in the path on CreateNew or do you do that in pxr.Usd.Stage.Save()?@archer-kgraves I specify the nucleus server in the CreateNew command like omniverse://servername/…As far as I know Stage.Save() will save the stage in the same place. Optionally you could create a stage in memory and then use Export to save it in an arbitrary place.This topic was automatically closed 14 days after the last reply. New replies are no longer allowed.Powered by Discourse, best viewed with JavaScript enabled"
542,ros2-need-help-with-completing-the-action-graph-for-my-turtlebot-and-launching-the-turtlebot3-navigation2-launch-file,"Hello.ROS2 Newbie here so please bear with me if this question is elementary.I am currently trying to get a turtlebot3 (burger model) to navigate around a warehouse floor on Isaac Sim but I keep running into the same error message on Rviz.I have created an occupancy map with the “Isaac Utils>Occupancy Map” extension and saved both the png file and the yaml file. And then launched the turtlebot3_navigation launch file with the following command:$ ros2 launch turtlebot3_navigation2 navigation2.launch.py use_sim_time:=true map:=warehouse_turtlebot_nav2.yamlwhenever I do this I keep getting the same error message that says: “Frame [map] does not exist”
image1528×957 135 KB

image1844×438 92.4 KB
I feel like the Action Graph that I have put together for the turtlebot is causing the problem, especially the transform nodes (since I am not so confident about it).
I currently have 1 transform tree node that has the following relationships and I feel like something is missing.

image675×958 57.6 KB
Can anyone share their Action Graph for a working turtlebot navigation scenario??
I have been stuck on trying to get this turtlebot to navigate for the past couple of days and any help would be greatly appreciated!(Also if you think theres something else other than the Action Graph that you think is causing this error, please feel free to point it out and let me know!)Thanks!Hi @jaeyeun - Have you referred to this document? 2. Drive TurtleBot via ROS2 messages — Omniverse Robotics documentationyes. ofcourse.
The tutorial on how to put together the OmniGraph for Navigation is not really that much helpful.Hi @jaeyeun , the Navigation sample contains the Carter robot with all the required ROS2 omnigraph Nodes. In the Image you have shared above, it seems the transforms between the links of the robot is not being published. You should verify if the ROS2PublishTransformTree and ROS2PublishRawTransformTree are set up correctly for your robot in simulation.Powered by Discourse, best viewed with JavaScript enabled"
543,audio2face-not-working-model-scaling-and-rotation-issue,"Can somebody please help me?Im experiencing an issue with Audio2Face. After selecting all of the correspondence points on my model and mark_openMouth’s face and select Begin Mesh Fitting, my model’s scale and rotation goes crazy, see attached images.This becomes “fixed” when I go through Post Wrap, and the Model returns to its original scale and rotation, but the points are now making the base of the model move instead of the face?More information, this occurs in the console after Begin Mesh Fitting finishes running
Scale after Mesh Fitting1920×1200 425 KB
This shows the absolute scale of the model when you zoom out after Mesh Fitting. Why?
Disappear after Mesh Fitting1920×1200 407 KB
This shows the a2f templates either side of where the model head used to be, before pressing Post-wrapHi @JackISO  and welcome to the forums!Instead of scaling your main mesh, can you try putting it under a new group and scale this new group to match Mark’s size before starting the character transfer process?Hello @Ehsan.HM , I am not scaling the main mesh at all, i am only translating the location of the a2fTemplates, not scaling or rotating anything else.I see… Are you scaling Mark? From the snapshot, it looks like Mark has been scaled. Can you reset their scale to 1.0 and scale your character group instead?Apologies, you are correct, in that screenshot mark has been scaled. However I have already attempted the same process with mark at scale 1.0 and it did not change the outcome.No worries, are you able to share your scene, or the model so I can try it too?bust.usda (26.8 MB)Included is the bust USDA File.Thanks for providing your file. When I added your mesh as Skin in Character Transfer tab, it errored (a red triangle appears), if you hover over that triangle, it shows you the error.Basically your mesh is made of multiple separate meshes, but Audio2Face needs one mesh with all vertices merged to work properly. You can separate and get rid of extra meshes you can follow the instructions given by the error.I have attempted this it has fixed the issue, thank you very much.However, I would like to add that I have attempted the to do this (without singular mesh) on another machine and it has worked fine, with no scaling errors and the mouth works as intended.But on this machine, it does not work without making it a singular mesh.
Could I provide you with some technical specifications of both machines to look at this in the future to help out?
The drivers used on both machines, the graphics cards? operating systems?Thanks again @Ehsan.HMGlad it works!
It’s strange if you were able it get Audio2Face to work propertly on a mesh with multiple geometry islands. Sure, would love to see both scene files and the machine specs for sure.I will provide these in this thread tomorrow, thanks for the help again 😊Machine 1 - Laptop - Razer Blade Pro 17 (2019).
Windows 11 Home (version 10.0.22621 Build 22621)
x64-Based
Intel i7-9750H @ 2.60GHz 6 Core
16GB RAM
NVIDIA GeForce RTX 2060 (laptop)
NVIDIA Studio Driver - Version 535.98This machine was able to complete the mesh and post-wrap no problems without separating the mesh.Machine 2 - Desktop PC
Windows 11 Pro (version 10.0.22621 Build 22621)
x64-Based
11th Gen Intel i9-11900F @ 2.50GHz 8 Core
32GB RAM
NVIDIA GeForce RTX 3090
NVIDIA Studio Driver 535.98This machine was not able to complete the mesh and post wrap without the mesh being separated, and the cause of this post.I have attached two files in an enclosed .zip. Machine 1 USDA and Machine 2 USDA. Thank you again for your help @Ehsan.HM. If you make any developments I’d be interested to hear why this was caused!Audio2Face USDA.zip (96.5 MB)This topic was automatically closed 14 days after the last reply. New replies are no longer allowed.Powered by Discourse, best viewed with JavaScript enabled"
544,nucleu-service-applications-could-not-be-started,"Hi @yinghuili  - Can you go to connections and do the login again and then try to restart the Nucleus Server?Powered by Discourse, best viewed with JavaScript enabled"
545,parallelisation-in-isaac-sim,"How can we add multiple robots in isaac-sim and give control to these robots in parallel making use of gpu?Thank you!Powered by Discourse, best viewed with JavaScript enabled"
546,system-monitor-does-not-exist-in-task-tray-from-nucleus-2023-1,"Hi,
I’m investigating the behavior of Nucleus 2023.1.It looks System Monitor is not shown in Task Tray from this Nucleus version.
Beforehand, we can confirm both Omniverse Launcher and System Monitor
in Task Tray.
I think this is because we can access to settings of Nucleus and Cache from
Omniverse Launcher Task from this version.But currently even if we close Omniverse Launcher task,
processes regarding System Monitor still exist.
In the old version, if we close System Monitor Task from Task Tray,
all of processes are closed.This does not affect our work but when we maintain Omniverse, we sometimes
close all omniverse processes and launch Omniverse again.
In this case, we close Omniverse Launcher and System Monitor.
But currently after we close Omniverse Launcher task, we need to close remained
tasks one by one.
Do we need to close by hand?Best Regards.===Masahiro SuzuokiHi,It looks if we select “Necleus => Exit” and “Cache => Exit”, and finally close Launcher
task itself, most of processes will be closed.
But, after that, Omniverse Tagging Service still remains.Best Regards.===Masahiro SuzuokiYes this is an update in nucleus 2023.1.0. We may have missed this update in the documentation and e will follow up to make sure it’s documented properly.This topic was automatically closed 14 days after the last reply. New replies are no longer allowed.Powered by Discourse, best viewed with JavaScript enabled"
547,time-step-index-is-2-at-simulation-start,"I am writing a custom Task for simulating pick and place with Franka (standalone application). BaseTask has a pre_step function that gets time_step_index argument. The value of this argument is 2 at the first call to pre_step. Why is it not 0?Thanks for your helpHi @prath4 - The pre_step function in the BaseTask class is called before each simulation step. The time_step_index argument represents the current simulation step index.The reason it starts from 2 instead of 0 is due to the initialization process of the simulation. The first two steps (0 and 1) are used for setting up the simulation and initializing various components. Therefore, the pre_step function is first called at the third step of the simulation, which is why time_step_index starts from 2.Thanks @rthaker for the quick reply. I would also like to know if that is deterministic and I will always get 2 for time_step_index in the first callback. Or the value is dependent on the simulation setup? What if I add a task in the middle of the simulation? Does task initialization always require a world reset? If I understand correctly the value counts steps from the last world reset.Thanks for your helpHi @prath4  - The time_step_index argument in the pre_step function of a BaseTask in Isaac Sim represents the current simulation step index. This index is incremented each time the simulation takes a step.The initial value of time_step_index being 2 is a result of the simulation initialization process and should be consistent across different simulation setups. However, this is an implementation detail of the simulation engine and could potentially change in future versions of the software.If you add a task in the middle of the simulation, the time_step_index for that task’s pre_step function will start from the current simulation step index, not from 0 or 2. This is because the time_step_index is tied to the simulation itself, not to individual tasks.Task initialization does not always require a world reset. You can add tasks to the simulation at any time, and they will start running from the current simulation step. However, if you want to reset the state of a task or the entire simulation, you can do so by resetting the world.You’re correct that the time_step_index counts steps from the last world reset. If you reset the world, the time_step_index will go back to 0, and the next call to pre_step will have a time_step_index of 2.Powered by Discourse, best viewed with JavaScript enabled"
548,issac-sim-people-simulation-character-initialization,"I am trying to simulate people in my custom usd files.
I’ve follow the tutorial (3. Omni.Anim.People — Omniverse Robotics documentation) But the warning keep occurs ‘Initialize the character’.
What does it mean?2023-06-08 12:47:11 [2,035,353ms] [Warning] [home_cilab___dot__local_share_ov_pkg_isaac_sim__dash__2022__dot__2__dot__1_extscache_omni__dot__anim__dot__people__dash__0__dot__1__dot__9_omni_anim_people_scripts.character_behavior] Initialize the characterHi @su-yeon.kim - Are you seeing any issues with your people simulation because of this warning or just curious about what this warning means?Powered by Discourse, best viewed with JavaScript enabled"
549,replicator-composer-physics-simulation-not-working,"I am trying to gather some synthetic data for training my jetbot for collision avoidance. I am using the replicator composer to generate random data with objects on the ground blocking the jetbot.However, I cannot collect data with object fell on the ground. Instead, I got only object floating around.
obj_physics is set to True; ‘scene_gravity’ is also set to True.I found this topic which faced a similar problem to mine (Object Physics (obj_physics) not working in Replicator Composer. Objects are still flying) , but I still cannot get my objects fall to the ground.Here is my input script:Hi there,from your script it seems you have all the necessary parameters set:
physics_simulate_time , scene_gravity, obj_physicsCan you test with the warehouse.yaml example to see if it works for you, and than building on top of that with your assets.Otherwise try sharing your jetbot_room.txt and simple_room(no_table).usd assets for us to test.Best,
AndreiUnfortunately, the warehouse.yaml  does not work for me.  Here are some pictures generated in warehouse.yaml:
rgb_41920×1080 152 KB
rgb_61920×1080 132 KBHere is my jetbot_room.txt
jetbot_room.txt (1.4 KB)And here is my simple_room(no_table).usd
simple_room(no_table).usd (1.3 MB)P.S. I am using the composer UI released on 2022.2.1 instead of the cmdThank you for your help!!Powered by Discourse, best viewed with JavaScript enabled"
550,replicator-how-to-change-skeleton-joint-rotation,"Ive been battling with Replicator and skeleton joints all day. At first, I had combined trying to rotate a skeleton joint with a script that also randomized the camera position and lighting scenario. On its own, the camera and lighting script worked. When I added the joint change and tested by stepping with Replicator, Replicator would show the desired change for one step then take a while and crash.I broke out the skeleton change to a separate script and it seemed to be fine. Then I started noticing the position of the joint was being reset back to (0,0,0) when modify.pose(rotation = (myvector)) was called. As I tried to troubleshoot that, the script completely stopped working and now wont update the joint rotation at all.Any ideas on why this code may not be working? I have also set up a cube in the same script to check behavior. The cube in this script will rotate as requested but the joint still remains in place. Also, the cube position is not reset like the joint was when it was working. I feel like Im seeing very different results with this through the day but cant figure out what may have changed to cause these issues.Powered by Discourse, best viewed with JavaScript enabled"
551,animation-graph-105-error-when-changing-type-of-variable,"I was playing with variables in Animation Graph in USD Composer 105. If you open the Animation Graph window, click “Add” to create a new variable it all works. But if you try to change the variable type from float to something else, you can an error in the console.image388×611 22.9 KBPowered by Discourse, best viewed with JavaScript enabled"
552,is-it-possible-to-use-a-tesla-k80-together-with-a-nvidia-rtx-3070-for-accelerate,"I can get from a friend a K80 Tesla cart.
Will it be of any use next to my RTX 3070
Will it also accelerate the render proces if it is capable?
I understand it sends data from the CPU to the Noth Bridge and then to the GPU (Grafics processor) for processingPowered by Discourse, best viewed with JavaScript enabled"
553,mesh-separate-stack-trace,"Back in VRoid Studio character and audio2face (does not work) - #7 by alan.james.kent I talked about a VRoid Studio character which has lots of disconnected meshes. I thought I reported here the problems I had with Mesh Separate, but cannot find it (maybe I just mentioned in Discord).Note that the mesh is badly disconnected so I know needs work to fix it up, but reporting some problems with Mesh Separate regardless in case helpful.Here is an example face before running mesh separate. (I hid the body and hair meshes)
image568×532 20.8 KB
Running mesh separate dumps stack traces with normals array accesses being out of bounds, so I commented out all the normals code just to see what would happen.Here is the face after running mesh separate with normals code commented out. It found the teeth, mouth cavity, tongue, eye sockets. The rest is not visible.
image798×832 36.4 KB
It did generate 26 separate meshes, but many seem to have lost the textures. So it is not just normals that is the only problem (probably).I think I am going to have to write custom code regardless - I need to stitch meshes together, but reporting the problem regardless in case helpful.Stack trace (with normals code left in place):Hi @alan.james.kentI guess this is the post you created before: VRoid Studio character and audio2face (does not work) - Apps / Audio2Face - NVIDIA Developer ForumsWhile A2F tries to tackle some common mesh problems, it’s not a good tool for fixing all of them. Its purpose is mostly deforming (standard) meshes based on audio, at least for now.Powered by Discourse, best viewed with JavaScript enabled"
554,can-audio2face-do-live-stream-to-metahuman,"I recommend to read the source codes and try making one by yourself.Did you release source code of Audio2face? If then, where can I download or refer to it?Python part of the A2F is accessible via your install directory. You can check the python extensions there and do whatever you want on top of the current implementation. :-)I found the directory, but I have no idea how to implement the function.
There are several tests in the directory and I investigate following files.exts\omni.audio2face.core\omni\audio2face\core\tests\test_a2f_core.py
exts\omni.audio2face.tool\omni\audio2face\tool\tests\test_a2f_usd_exporter.pyI want to implement grpc server which take wave file and stream it to usd format.
and I will connect it with metahuman.
But, I can’t find any solution for that with above files.
Would you suggest any example which I can refer?Hi heury,I am trying to animate a metahuman via a2f in real time, too. Have you been successful implementing this extension?
@NVIDIA: can you already tell when this update will be released?Hi, this will definetly be super useful, any update on the progress ?Hi, there is no progress in the feature unfortunately. We plan to work on this item within this year, but cannot confirm the exact date yet. Thanks for the interest!Bump. Any update on this feature? @yseol which users have implemented such a function themselves?Hello @feel.or.fake!  Here is a link to the latest releases to the Audio2Face app: Release Notes — Omniverse Audio2Face documentationStay tuned for some update announcements “very” soon.  Cheers.Hey @siyuen, any update on this?We announced ACE (Omniverse Avatar Cloud Engine) at Siggraph. This will allow live stream of A2F from cloud services to UE MH.We did a live recorded demo of that, check it out below. And if you missed the full Digital Human section of our special address, check out the link at the bottom too.Release date wise, we will have more updates on that at GTC in mid Sept. And our goal is as soon as we can. So stay tuned.ACENVIDIA Siggraph Special Address - Digital HumanWill Omniverse ACE be affordable or even available to hobbyists? My understanding was that it’s a business solution, but maybe I misunderstood.@siyuen Any updates on this ? December is coming soon.@WendyGram  Any updates on ACE ? Any pre-release plan ?We are entering limited early access this month on ACE.  Stay tuned for public release date.erstanding was that it’s a business solution, but maybe I misunderstood.@ yseol , any update on this?Except ACE (Omniverse Avatar Cloud Engine) solution.Any update for A2F realtime live link to the Metahuman in UE locally,?
ThanksWe’re working on this at the moment, and hopefully this should be available in the coming release.But, in the meantime, this tutorial might be a good short-term solution: AI-Powered Facial Animation — LiveLink with NVIDIA Audio2Face & Unreal Engine Metahuman - YouTubeHi, there is any update on this?We are really close to the release date (in about a couple of weeks). Stay tuned…Powered by Discourse, best viewed with JavaScript enabled"
555,the-nvidia-build-has-less-samplers,"Hello Nvidia, a blend file that works fine in default blender 3.6 does not work in the nvidia build, the shaders wont compile, I get this error:I would love to get the usdskel support and the extra shaders in a single application. Do you perhaps know what might cause this?
Thanks,
KoenERROR (gpu.shader): GPU_material_compile VertShader:
|
1560 | layout(binding = 20) uniform sampler2D samp20;
|
| Error: C7612: profile doesn’t support more than 32 samplersHi @koen2.  I just confirmed that the issue is fixed in our latest  Blender build for 4.0, which will be available in our next release.  This appears to have been an issue in the main Blender branch, nothing specific to the USD branch.  Thanks.Powered by Discourse, best viewed with JavaScript enabled"
556,how-do-you-add-an-sdf-collider-to-an-object-in-create-2022-3-0,"I’m playing with the “nuts and bolts” demo and I can’t find any collider in the “nut_bolt” physic properties. There’s only a “rigid body” component.From my understanding, aren’t we supposed to have a collider component?
image436×688 31.6 KB
I don’t find the aforementioned component in either nut nor bolt :
Hello @aws357!  I’ve shared your post with the dev team for further assistance.Hi @aws357 - you can access the collider properties by:We are working on an update to the docs about SDF.Thanks for the answers. I will check that up.hello @philipp.reist
I want to run the bolt and nut demo shown above in Isaacsim.
As a result of my search for bolts and nuts demos, I was only able to run them in the Omniverse showroom and did not have access to the stage like the author of this article.
where can i get it?I would like to run the sample files or sources for the Physics Demo Nuts and Bolts.https://docs.omniverse.nvidia.com/app_showroom/app_showroom/demos/physics-nuts-and-bolts.htmlHi @gudwnzm - you should be able to find the demo in Window → Simulation → Demo Scenes → Complex Showcases → Nuts and BoltsThen you can inspect the Python code of the demo.Hello @philipp.reistThank you for answer.
Thanks to that, I was able to run the sample.
I tried changing the collider of nut_bolt from SDF Mesh to Convex Hull to compare the difference between the sophisticated SDF Mesh and other Convex Hull and Triangle Mesh.
But I got an error message saying Can not edit attributes of instance proxy.How can I compare each collider?You can manually disable the instanceable flag on the asset, then you should be able to change to convex hull. But then it won’t be able to thread and just bump off the bolt.Powered by Discourse, best viewed with JavaScript enabled"
557,nvidia-issues,"i ran “nvidia-smi” and it says Nvidia-smi has failed because it couldn’t communicate with the NVIDIA driver and my graphics shows llvmpipe. my nvidia bug report log file is attached.
nvidia-bug-report.log.gz (173.4 KB)hello @cnptp - The GPUs you have in your system (GT 730) are not Omniverse capable . Omniverse requires NVIDIA RTX GPUs.  In addition, the OS GPU driver you have installed does not support those cards for any use. Snippet from your logs:Jun 20 12:41:11 teuchos kernel: [497090.335991] NVRM: The NVIDIA GeForce GT 730 GPU installed in this system is
Jun 20 12:41:11 teuchos kernel: [497090.335991] NVRM:  supported through the NVIDIA 470.xx Legacy drivers. Please
Jun 20 12:41:11 teuchos kernel: [497090.335991] NVRM:  visit Unix Drivers | NVIDIA for more
Jun 20 12:41:11 teuchos kernel: [497090.335991] NVRM:  information.  The 510.108.03 NVIDIA driver will ignore
Jun 20 12:41:11 teuchos kernel: [497090.335991] NVRM:  this GPU.  Continuing probe…To use Omniverse on a Linux system you need RTX cards and the proper driver installed.See following for Driver versions:
https://docs.omniverse.nvidia.com/platform/latest/common/technical-requirements.html#technical-requirementsThis topic was automatically closed 14 days after the last reply. New replies are no longer allowed.Powered by Discourse, best viewed with JavaScript enabled"
558,how-to-delete-old-version-of-any-app,"Hi everyone. How to delete old version of app? Audio2Face for example. Can’t find info on this topicIn your Omniverse launcher, go to Library and select the application you would like to uninstall click on the three bar menu (hamburger) next to the launch button and select “settings”. A pop-up menu with a per version “uninstall” button will appear.Powered by Discourse, best viewed with JavaScript enabled"
559,usdrt-and-character-models,"Hi,I am trying to spawn animated character models at runtime. How can we take advantage of USDRT API when working with animated character models?Spawning
Since USDRT works on a post-composition view of the USD stage should the characters be spawned using USD API. On trying with point instancer the animations were not working. What would be the best way to do it so that it is performant even when 1000’s of characters need to spawned.Animation/Manipulating Prim
When Fabric is enabled animation graph doesn’t work. But if the Skeletal Animation is directly set as animation source to the Skeleton the animation does work. Then I could move the skeleton of the character using the USDRT RtXformable schema while the looping animation played. Also had to manually adjust the timeline end frame and FPS so that the animation looped correctly. This also makes it difficult if the character needs to have different animations. Please suggest what would be the right way to approach this problem.Powered by Discourse, best viewed with JavaScript enabled"
560,enterprise-nucleus-issues-with-ssl,"The biggest problem is the confusing documentation.
If I dive into the other problems this post will get lengthy.I would appreciate if we could solve this problem, given the security issues involved.There are 3 official docs from Nvidia and they have conflicting information between each, some typos and syntax issues and deprecated stuff.Following the Nucleus Enterprise doc “Configuring SSL/TLS with NGINX”. On the Section  NGINX Configuration from steps 1 to step 3 the documentation tells me to Copy the nginx.ingress.router.conf file from your Nucleus Server to your NGINX Server and place it into the conf.d folder within the NGINX path.
So, basically put the nginx.ingress.router.conf  on /etc/nginx/conf.dhttps://docs.omniverse.nvidia.com/prod_nucleus/prod_nucleus/enterprise/installation/ssl_nginx.htmlOn the Official Nvidia blog post at AWS, which is even cited at Nvidia Docs, at the Section Configure The Reverse Proxy Server steps 2 to step 9 we are told to copy the nginx.ingress.router.conf into S3 named as nginx.conf and then download it at /etc/nginx named as nginx.conf.
And it fails to address all the rest of the required NGINX configuration and the errors within nginx.ingress.router.confIntroduction This post aims to get users up and running with NVIDIA Omniverse Enterprise Nucleus Server on Amazon Elastic Compute Cloud (Amazon EC2). Here I’ll outline the requirements for Enterprise Nucleus Server deployment and dive deep into the...The guide on Nucleus Enterprise to deploy on AWS.
It not only repeat the problems in the AWS post by telling us to update nginx.ingress.router.conf and place directly at /etc/nginx/nginx.conf but it has syntax errors by telling us to copy events { worker_connections 1024; } into the SSL/TLS server and there is a lose note about wrapping the config with http { } but it ignores all the other issues. To complement the CDK stack is has a few errors and the boto3 script is incomplete
https://docs.omniverse.nvidia.com/prod_nucleus/prod_nucleus/enterprise/cloud_aws_ec2.htmlThank you @Pappachuck_renan for your post.  I know we are working with you directly on your configuration, however I will also review each of your comments and we will adjust the documentation as and where needed.Thank you!
DrewPowered by Discourse, best viewed with JavaScript enabled"
561,modify-pose-empty-typename-for-xformoporder,"I got the following error message when trying the use randonmizerThe script is as followerror messageI believe this is happening because you are trying to modify the pose (i.e. xformOpOrder) on a material (/World/RubixCube/Cubies_11/RedStickers_Mat), which can’t have an xformOpOrder attribute. It is probably a result of using “rep.get.prims” which returns all the prims that match the path pattern, including children, since a ‘prim’ is the base of all objects.I would try using “rep.get.mesh” instead to filter out any materials that have part of the object path in the material path string.I tried to reproduce the error with your script and it does not produce this error if ‘/World/AlarmClock_Retro’ is a mesh (I replaced it with a Cube on my end), and the error seems to be related to the ‘/World/RubixCube’ object.Powered by Discourse, best viewed with JavaScript enabled"
562,no-module-named-omni-appwindow,"when i try to run a python file .py,which is to control the  jetbot through keyboard :when i run ‘~/.local/share/ov/pkg/isaac_sim-2022.2.1/python.sh’ keyboard_control.py  in terminal,it shows the error like 'No module named ‘omni.appwindow’ ,what is the matter? HELP!!!Hi @gaozhao22  - The error message “No module named ‘omni.appwindow’” indicates that Python cannot find the ‘omni.appwindow’ module. This could be due to several reasons:If none of these solutions work, then please reach out to us.I have the same problem and tried several imports.
from omni.isaac.kit import SimulationApp works for me.
But any other import fails.I tried using the python.sh file as described here:
https://docs.omniverse.nvidia.com/isaacsim/latest/install_python.htmland the “run single python file” configuration of VSCode as described here:
https://docs.omniverse.nvidia.com/isaacsim/latest/manual_standalone_python.html#visual-studio-code-vscode-supportThe following is my code:When i remove line 3 it works.
However most other modules seem to fail.The ‘omni.appwindow’ module is not in your Python path: Python uses the PYTHONPATH environment variable to determine where to look for modules. If the ‘omni.appwindow’ module is installed in a location that’s not in your PYTHONPATH, Python won’t be able to find it.I checked the launch.json and checked the python paths in the setup_python_env I then modified the path in the setup_python_env to make sure it pointed to $PYTHONPATH:$SCRIPT_DIR/exts/ which from my understanding means it should have all extensions in this folder in its path.I am not sure how I should install these separately, if you could provide a bit more context here. I only found information on how to install standard python packages via pip into omniverse code. But these are Omniverse packages.You’re using the wrong version of Python: The ‘omni.appwindow’ module might only be compatible with a specific version of Python. Make sure that you’re using the correct version.As i am using the provided vscode / python.sh file which from my understanding sets the python interpreter. Not sure if I understand this functionality correctly though.The ‘omni.appwindow’ module is not compatible with your operating system: Some Python modules are only compatible with certain operating systems. If you’re trying to use the ‘omni.appwindow’ module on an unsupported operating system, you might get this error.OS: Ubuntu 20.04I had a close look at the HelloWorld.py example:Basically what you have to do is to import the relevant packages, after the simulation has been started,
then the context is set and everything is available and works as intended.Powered by Discourse, best viewed with JavaScript enabled"
563,audio2face-not-installing,"Good day!I’ve been trying to download Audio2Face and I can’t seem to get it to do anything. I am installing into my L drive and it keeps giving me this error:Error occurred during installation of Audio2Face: EISDIR: illegal operation on a directory, symlink ‘L:\OMNIVERSE\Library\deps\6cd16432302a4b68763ef2c48f308f34’ → ‘L:\OMNIVERSE\Library\audio2face-2023.1.1’It has failed in multiple stages but this is the last one.Thank youHello and welcome to the forums @robertalomargonzalezCould you please send your latest Launcher log? It should be located inside C:\Users\(UserName)\.nvidia-omniverse\logs\launcher.logPowered by Discourse, best viewed with JavaScript enabled"
564,omniverse-houdini-connector-v102-4-release,"Omniverse Houdini Connector release v102.4We are pleased to announce our next update to the Omniverse Houdini Connector which includes some improvements and bugs fixes.Release NotesImprovedFixedPlease read the Houdini Connector Documentation.Thank you for checking out the Omniverse Houdini Connector!Powered by Discourse, best viewed with JavaScript enabled"
565,how-to-change-step-size,"Hello,I want to change step_size, but I don’t know how to change that.
Please tell me how to change step_size.from omni.isaac.examples.base_sample import BaseSample
import numpy as np
from omni.isaac.core.objects import DynamicCuboidclass HelloWorld(BaseSample):
def init(self) → None:
super().init()
returnHi @Yuya_t - To change the step_size in your script, you need to change the physics settings of the simulation. Here’s the modified scripts:This topic was automatically closed 14 days after the last reply. New replies are no longer allowed.Powered by Discourse, best viewed with JavaScript enabled"
566,error-omni-ext-plugin-ext-omni-isaac-repl-1-0-3-failed-to-startup-python-extension,"I tried to install Isaac REPL while following the tutorials, but it fails to start. It seems to be related to this:
ImportError: cannot import name ‘AnyFormattedText’ from ‘prompt_toolkit.formatted_text’I’m running on Ubuntu 20.04, version 2022.2.1Hi there,can you provide the steps on how you are running into the issue?Thanks!Sure, although unfortunately it’s not particularly interesting. I was following the tutorials, and got to the point where it tells you to go to Window > Extensions, search for Isaac REPL, and toggle it on. When I do that, I get the error.i enconuter the error:2023-03-25 09:21:14 [5,503ms] [Error] [omni.ext._impl.custom_importer] Failed to import python module omni.isaac.repl. Error: cannot import name ‘get_event_loop’ from ‘prompt_toolkit.eventloop.utils’ (/home/anson/.local/share/ov/pkg/isaac_sim-2022.2.1/kit/python/lib/python3.7/site-packages/prompt_toolkit/eventloop/utils.py). Traceback:
Traceback (most recent call last):
File “/home/anson/.local/share/ov/pkg/isaac_sim-2022.2.1/kit/kernel/py/omni/ext/_impl/custom_importer.py”, line 76, in import_module
return importlib.import_module(name)
File “/home/anson/.local/share/ov/pkg/isaac_sim-2022.2.1/kit/python/lib/python3.7/importlib/init.py”, line 127, in import_module
return _bootstrap._gcd_import(name[level:], package, level)
File “”, line 1006, in _gcd_import
File “”, line 983, in _find_and_load
File “”, line 967, in _find_and_load_unlocked
File “”, line 677, in _load_unlocked
File “”, line 728, in exec_module
File “”, line 219, in _call_with_frames_removed
File “/home/anson/.local/share/ov/pkg/isaac_sim-2022.2.1/exts/omni.isaac.repl/omni/isaac/repl/init.py”, line 10, in 
from .extension import *
File “/home/anson/.local/share/ov/pkg/isaac_sim-2022.2.1/exts/omni.isaac.repl/omni/isaac/repl/extension.py”, line 16, in 
from prompt_toolkit.eventloop.utils import get_event_loop
ImportError: cannot import name ‘get_event_loop’ from ‘prompt_toolkit.eventloop.utils’ (/home/anson/.local/share/ov/pkg/isaac_sim-2022.2.1/kit/python/lib/python3.7/site-packages/prompt_toolkit/eventloop/utils.py)2023-03-25 09:21:14 [5,503ms] [Error] [carb.scripting-python.plugin] Exception: Extension python module: ‘omni.isaac.repl’ in ‘/home/anson/.local/share/ov/pkg/isaac_sim-2022.2.1/exts/omni.isaac.repl’ failed to load.At:
/home/anson/.local/share/ov/pkg/isaac_sim-2022.2.1/kit/kernel/py/omni/ext/_impl/_internal.py(189): startup
/home/anson/.local/share/ov/pkg/isaac_sim-2022.2.1/kit/kernel/py/omni/ext/_impl/_internal.py(285): startup_extension
PythonExtension.cpp::startup()(2): 2023-03-25 09:21:14 [5,503ms] [Error] [omni.ext.plugin] [ext: omni.isaac.repl-1.0.3] Failed to startup python extension.same env: Ubuntu 20.04, version 2022.2.1it’s my fault for changing python env.Hi, I faced the same error.It seems that prompt_toolkit has removed ‘get_event_loop()’ and replaced it with ‘get_running_loop()’ function in their recent 3.0.37 release: python-prompt-toolkit/CHANGELOG at 960df477c31adf53a3fff98a5c212c71fbfd7a3e · prompt-toolkit/python-prompt-toolkit · GitHubNow I’m trying to revert my prompt_toolkit to 3.0.36 versionI tried something similar but that seemed to break other parts of the plugin. I was hoping that Nvidia would update the plugin to either use the new function, or require that it use prompt_toolkit <= 3.0.36 and figure out what other issues my result from that.Let me know if you have any luck.@ahaidu Any luck here?By default in 2022.2.1 the repl extension should package all of its dependencies in the pip_prebundle folder:and use prompt_toolkit-3.0.20its possible that if a newer prompt_toolkit was installed in the local python env that it might cause issues. if you run ./python.sh -m pip list is there another prompt-toolkit package installed and if so, does uninstalling fix the issue?I met exactly the same issue and i’ve reinstalled Isaac Sim several times. It didn’t work till now.
Everytime I launch Isaac Sim, here is the content from the terminal.2023-07-21 02:54:54 [6,527ms] [Error] [omni.ext._impl.custom_importer] Failed to import python module omni.isaac.repl. Error: cannot import name ‘get_event_loop’ from ‘prompt_toolkit.eventloop.utils’ (/home/walker2/.local/share/ov/pkg/isaac_sim-2022.2.1/kit/python/lib/python3.7/site-packages/prompt_toolkit/eventloop/utils.py). Traceback:
Traceback (most recent call last):
File “/home/walker2/.local/share/ov/pkg/isaac_sim-2022.2.1/kit/kernel/py/omni/ext/_impl/custom_importer.py”, line 76, in import_module
return importlib.import_module(name)
File “/home/walker2/.local/share/ov/pkg/isaac_sim-2022.2.1/kit/python/lib/python3.7/importlib/init.py”, line 127, in import_module
return _bootstrap._gcd_import(name[level:], package, level)
File “”, line 1006, in _gcd_import
File “”, line 983, in _find_and_load
File “”, line 967, in _find_and_load_unlocked
File “”, line 677, in _load_unlocked
File “”, line 728, in exec_module
File “”, line 219, in _call_with_frames_removed
File “/home/walker2/.local/share/ov/pkg/isaac_sim-2022.2.1/exts/omni.isaac.repl/omni/isaac/repl/init.py”, line 10, in 
from .extension import *
File “/home/walker2/.local/share/ov/pkg/isaac_sim-2022.2.1/exts/omni.isaac.repl/omni/isaac/repl/extension.py”, line 16, in 
from prompt_toolkit.eventloop.utils import get_event_loop
ImportError: cannot import name ‘get_event_loop’ from ‘prompt_toolkit.eventloop.utils’ (/home/walker2/.local/share/ov/pkg/isaac_sim-2022.2.1/kit/python/lib/python3.7/site-packages/prompt_toolkit/eventloop/utils.py)2023-07-21 02:54:54 [6,527ms] [Error] [carb.scripting-python.plugin] Exception: Extension python module: ‘omni.isaac.repl’ in ‘/home/walker2/.local/share/ov/pkg/isaac_sim-2022.2.1/exts/omni.isaac.repl’ failed to load.At:
/home/walker2/.local/share/ov/pkg/isaac_sim-2022.2.1/kit/kernel/py/omni/ext/_impl/_internal.py(189): startup
/home/walker2/.local/share/ov/pkg/isaac_sim-2022.2.1/kit/kernel/py/omni/ext/_impl/_internal.py(285): startup_extension
PythonExtension.cpp::startup()(2): 2023-07-21 02:54:54 [6,527ms] [Error] [omni.ext.plugin] [ext: omni.isaac.repl-1.0.3] Failed to startup python extension.BTW,  can I adjust the time show in the terminal?
thk u.Powered by Discourse, best viewed with JavaScript enabled"
567,creating-realistic-looking-glass-with-real-time-renderer,"Hi all,I am having a similar issue to @user79641 in his post here  but the topic is already closed so I could not reply there.Even after enabling caustics my glass is still black. Increasing ambient light value fixes the black but obviously brightens up the entire scene…@PhilippeR disabling cast shadows does not help in my case, right?I am curious about the World Epsilon Treshold parameter which fixes the issue up to degree. I don’t get why its description says “Lower values results in higher quality” but actually maximising it seems getting more image-based projection on specular surfaces.Here are some screenshots:
REAL_TIME_GLASS_011920×1082 135 KB
REAL_TIME_GLASS_01_CAUSTICS1920×1100 137 KB
REAL_TIME_GLASS_01_CAUSTICS_AMBIENT_LIGHT1920×1107 146 KBthanks in advance!best,
KarolHello @karol.osinski
Can you compare it to what it looks like in Interactive (Path Tracing)?Hi @PhilippeR ,It would look more or less like this:REAL_TIME_GLASS_01_PT1920×1043 149 KBI am doing some testing and changed the material type from OmniSurface to OmniGlass and boosted the IOR.It seems that the black appears where are the gaps between the outer and inner geometry shell.REAL_TIME_GLASS_GEO1920×1063 87.7 KB
REAL_TIME_GLASS_GLASS_MTL1920×1052 155 KBIf you use OmniGlass you’ll see that it looks similar in Path Tracing to what you see in Real-Time. The more complex OmniSurface is not well matched in this case in RTX Real-Time. You can increase the Max Refraction Bounces in the Real-Time render settings under Translucency, but it won’t fully solve this issue.In Interactive (Path Tracing) you would increase the Max Bounces and Max Specular Transmission Bounces.The real-time mode has more limitations when it comes to translucency.For World Epsilon Treshold, the lower the value, the less image-based reprojection is used to calculate refractions, so it will look more accurate, but I am not sure it will make much of a difference in this case.Thank you @PhilippeR . I will do more tests with the entire chandelier these days and will post the results here!I am using the OmniGlass material but the results are still not so nice. Trying extremly low IOR helps a bit but it is still far from ideal.Another issues is that the lights doesn’t really affect the glass using RealTime, caustics are working suprisingly well though.Aren’t there any settings overrides that could be applied to solve this issue?REAL_TIME_GLASS_CAUSTICS1920×1051 138 KB
REAL_TIME_GLASS_IOR_THIN_WALLED1920×1055 135 KB
REAL_TIME_GLASS_IOR_1.11920×1043 133 KB
REAL_TIME_GLASS_IOR_1.871920×1054 142 KBPowered by Discourse, best viewed with JavaScript enabled"
568,isaac-set-camera-omnigraph-warning-camera-prim-must-be-specified,"when try to follow the tutorial about ros2_camera, the ‘isaac set camera’ shows the warning’isaac_set_camera: OmniGraph Warning: Camera prim must be specified’.However,i have specified the camera prim in the property table,as the below pic shows.
Screenshot from 2023-07-26 11-34-311419×396 51.2 KB
Is there anything wrong which i did not mentioned? HELP!!!When i close and reopen the usd file,it works.But i still do not know why i met this problem.Hi @gaozhao22  - The fact that it works when you close and reopen the USD file suggests that the camera does exist and the path is correct, but there might be an issue with the timing or order of operations.To debug this, you could try the following:Check the execution order of your graphs. Make sure the graph that creates the camera is executed before the graph that uses the camera.Add a delay before the Isaac Set Camera node to give the camera time to be created.Use a conditional or loop to check if the camera exists before executing the Isaac Set Camera node.Remember that the Omniverse Kit is a real-time system, and the order and timing of operations can sometimes cause issues like this.Powered by Discourse, best viewed with JavaScript enabled"
569,onshape-importer-doesnt-support-variablestudio-element-type,"Hi,it looks like the latest onshape importer does not support this somewhat recent feature, and therefore fails to parse files.is there a fix coming?the error looks like this:
2023-05-22 20:17:03 [54,699ms] [Error] [omni.kit.app._impl] [py stderr]: Exception in thread Thread-188:
Traceback (most recent call last):
File “c:\users\shuau\appdata\local\ov\pkg\isaac_sim-2022.2.1\kit\python\lib\threading.py”, line 926, in _bootstrap_inner
self.run()
File “c:\users\shuau\appdata\local\ov\pkg\isaac_sim-2022.2.1\kit\python\lib\threading.py”, line 870, in run
self._target(*self._args, **self._kwargs)
File “c:\users\shuau\appdata\local\ov\pkg\isaac_sim-2022.2.1\exts\omni.isaac.onshape\omni\isaac\onshape\widgets\documents_widget.py”, line 92, in get_doc_type
self.document_id, self.get_wdid(), self.get_workspace()
File “c:\users\shuau\appdata\local\ov\pkg\isaac_sim-2022.2.1\exts\omni.isaac.onshape\omni\isaac\onshape\onshape_client\oas\api\documents_api.py”, line 2583, in call
return self.callable(self, *args, **kwargs)
File “c:\users\shuau\appdata\local\ov\pkg\isaac_sim-2022.2.1\exts\omni.isaac.onshape\omni\isaac\onshape\onshape_client\oas\api\documents_api.py”, line 1354, in __get_elements_in_document
return self.call_with_http_info(**kwargs)
File “c:\users\shuau\appdata\local\ov\pkg\isaac_sim-2022.2.1\exts\omni.isaac.onshape\omni\isaac\onshape\onshape_client\oas\api\documents_api.py”, line 2644, in call_with_http_info
collection_formats=params[“collection_format”],
File “c:\users\shuau\appdata\local\ov\pkg\isaac_sim-2022.2.1\exts\omni.isaac.onshape\omni\isaac\onshape\onshape_client\oas\api_client.py”, line 358, in call_api
_check_type,
File “c:\users\shuau\appdata\local\ov\pkg\isaac_sim-2022.2.1\exts\omni.isaac.onshape\omni\isaac\onshape\onshape_client\oas\api_client.py”, line 190, in __call_api
return_data = self.deserialize(response_data, response_type, _check_type)
File “c:\users\shuau\appdata\local\ov\pkg\isaac_sim-2022.2.1\exts\omni.isaac.onshape\omni\isaac\onshape\onshape_client\oas\api_client.py”, line 270, in deserialize
received_data, response_type, [“received_data”], True, _check_type, configuration=self.configuration
File “c:\users\shuau\appdata\local\ov\pkg\isaac_sim-2022.2.1\exts\omni.isaac.onshape\omni\isaac\onshape\onshape_client\oas\model_utils.py”, line 975, in validate_and_convert_types
inner_value, inner_required_types, inner_path, from_server, _check_type, configuration=configuration
File “c:\users\shuau\appdata\local\ov\pkg\isaac_sim-2022.2.1\exts\omni.isaac.onshape\omni\isaac\onshape\onshape_client\oas\model_utils.py”, line 937, in validate_and_convert_types
input_value, valid_classes, path_to_item, configuration, from_server, key_type=False, must_convert=True
File “c:\users\shuau\appdata\local\ov\pkg\isaac_sim-2022.2.1\exts\omni.isaac.onshape\omni\isaac\onshape\onshape_client\oas\model_utils.py”, line 880, in attempt_convert_item
raise conversion_exc
File “c:\users\shuau\appdata\local\ov\pkg\isaac_sim-2022.2.1\exts\omni.isaac.onshape\omni\isaac\onshape\onshape_client\oas\model_utils.py”, line 874, in attempt_convert_item
return deserialize_model(input_value, valid_class, path_to_item, check_type, configuration, from_server)
File “c:\users\shuau\appdata\local\ov\pkg\isaac_sim-2022.2.1\exts\omni.isaac.onshape\omni\isaac\onshape\onshape_client\oas\model_utils.py”, line 748, in deserialize_model
instance = used_model_class(**kw_args)
File “c:\users\shuau\appdata\local\ov\pkg\isaac_sim-2022.2.1\exts\omni.isaac.onshape\omni\isaac\onshape\onshape_client\oas\models\bt_document_element_info.py”, line 189, in init
setattr(self, var_name, var_value)
File “c:\users\shuau\appdata\local\ov\pkg\isaac_sim-2022.2.1\exts\omni.isaac.onshape\omni\isaac\onshape\onshape_client\oas\model_utils.py”, line 174, in setattr
self.set_attribute(name, value)
File “c:\users\shuau\appdata\local\ov\pkg\isaac_sim-2022.2.1\exts\omni.isaac.onshape\omni\isaac\onshape\onshape_client\oas\model_utils.py”, line 82, in set_attribute
check_allowed_values(self.allowed_values, (name,), value)
File “c:\users\shuau\appdata\local\ov\pkg\isaac_sim-2022.2.1\exts\omni.isaac.onshape\omni\isaac\onshape\onshape_client\oas\model_utils.py”, line 432, in check_allowed_values
% (input_variable_path[0], input_values, these_allowed_values)
omni.isaac.onshape.onshape_client.oas.exceptions.ApiValueError: Invalid value for element_type (VARIABLESTUDIO), must be one of [‘PARTSTUDIO’, ‘ASSEMBLY’, ‘DRAWING’, ‘FEATURESTUDIO’, ‘BLOB’, ‘APPLICATION’, ‘TABLE’, ‘BILLOFMATERIALS’, ‘UNKNOWN’]Exception in thread Thread-188:
Traceback (most recent call last):
File “c:\users\shuau\appdata\local\ov\pkg\isaac_sim-2022.2.1\kit\python\lib\threading.py”, line 926, in _bootstrap_inner
self.run()
File “c:\users\shuau\appdata\local\ov\pkg\isaac_sim-2022.2.1\kit\python\lib\threading.py”, line 870, in run
self._target(*self._args, **self._kwargs)
File “c:\users\shuau\appdata\local\ov\pkg\isaac_sim-2022.2.1\exts\omni.isaac.onshape\omni\isaac\onshape\widgets\documents_widget.py”, line 92, in get_doc_type
self.document_id, self.get_wdid(), self.get_workspace()
File “c:\users\shuau\appdata\local\ov\pkg\isaac_sim-2022.2.1\exts\omni.isaac.onshape\omni\isaac\onshape\onshape_client\oas\api\documents_api.py”, line 2583, in call
return self.callable(self, *args, **kwargs)
File “c:\users\shuau\appdata\local\ov\pkg\isaac_sim-2022.2.1\exts\omni.isaac.onshape\omni\isaac\onshape\onshape_client\oas\api\documents_api.py”, line 1354, in __get_elements_in_document
return self.call_with_http_info(**kwargs)
File “c:\users\shuau\appdata\local\ov\pkg\isaac_sim-2022.2.1\exts\omni.isaac.onshape\omni\isaac\onshape\onshape_client\oas\api\documents_api.py”, line 2644, in call_with_http_info
collection_formats=params[“collection_format”],
File “c:\users\shuau\appdata\local\ov\pkg\isaac_sim-2022.2.1\exts\omni.isaac.onshape\omni\isaac\onshape\onshape_client\oas\api_client.py”, line 358, in call_api
_check_type,
File “c:\users\shuau\appdata\local\ov\pkg\isaac_sim-2022.2.1\exts\omni.isaac.onshape\omni\isaac\onshape\onshape_client\oas\api_client.py”, line 190, in __call_api
return_data = self.deserialize(response_data, response_type, _check_type)
File “c:\users\shuau\appdata\local\ov\pkg\isaac_sim-2022.2.1\exts\omni.isaac.onshape\omni\isaac\onshape\onshape_client\oas\api_client.py”, line 270, in deserialize
received_data, response_type, [“received_data”], True, _check_type, configuration=self.configuration
File “c:\users\shuau\appdata\local\ov\pkg\isaac_sim-2022.2.1\exts\omni.isaac.onshape\omni\isaac\onshape\onshape_client\oas\model_utils.py”, line 975, in validate_and_convert_types
inner_value, inner_required_types, inner_path, from_server, _check_type, configuration=configuration
File “c:\users\shuau\appdata\local\ov\pkg\isaac_sim-2022.2.1\exts\omni.isaac.onshape\omni\isaac\onshape\onshape_client\oas\model_utils.py”, line 937, in validate_and_convert_types
input_value, valid_classes, path_to_item, configuration, from_server, key_type=False, must_convert=True
File “c:\users\shuau\appdata\local\ov\pkg\isaac_sim-2022.2.1\exts\omni.isaac.onshape\omni\isaac\onshape\onshape_client\oas\model_utils.py”, line 880, in attempt_convert_item
raise conversion_exc
File “c:\users\shuau\appdata\local\ov\pkg\isaac_sim-2022.2.1\exts\omni.isaac.onshape\omni\isaac\onshape\onshape_client\oas\model_utils.py”, line 874, in attempt_convert_item
return deserialize_model(input_value, valid_class, path_to_item, check_type, configuration, from_server)
File “c:\users\shuau\appdata\local\ov\pkg\isaac_sim-2022.2.1\exts\omni.isaac.onshape\omni\isaac\onshape\onshape_client\oas\model_utils.py”, line 748, in deserialize_model
instance = used_model_class(**kw_args)
File “c:\users\shuau\appdata\local\ov\pkg\isaac_sim-2022.2.1\exts\omni.isaac.onshape\omni\isaac\onshape\onshape_client\oas\models\bt_document_element_info.py”, line 189, in init
setattr(self, var_name, var_value)
File “c:\users\shuau\appdata\local\ov\pkg\isaac_sim-2022.2.1\exts\omni.isaac.onshape\omni\isaac\onshape\onshape_client\oas\model_utils.py”, line 174, in setattr
self.set_attribute(name, value)
File “c:\users\shuau\appdata\local\ov\pkg\isaac_sim-2022.2.1\exts\omni.isaac.onshape\omni\isaac\onshape\onshape_client\oas\model_utils.py”, line 82, in set_attribute
check_allowed_values(self.allowed_values, (name,), value)
File “c:\users\shuau\appdata\local\ov\pkg\isaac_sim-2022.2.1\exts\omni.isaac.onshape\omni\isaac\onshape\onshape_client\oas\model_utils.py”, line 432, in check_allowed_values
% (input_variable_path[0], input_values, these_allowed_values)
omni.isaac.onshape.onshape_client.oas.exceptions.ApiValueError: Invalid value for element_type (VARIABLESTUDIO), must be one of [‘PARTSTUDIO’, ‘ASSEMBLY’, ‘DRAWING’, ‘FEATURESTUDIO’, ‘BLOB’, ‘APPLICATION’, ‘TABLE’, ‘BILLOFMATERIALS’, ‘UNKNOWN’]
2023-05-22 20:17:03 [54,700ms] [Error] [omni.kit.app._impl] [py stderr]:the link where to look for the importer on the onshape website seems incorrect, and i can’t seem to be able to find.i tried contacting onshape but they seem very confused by all of it.can you provide the link to the older importer i should try?The onshape folks are saying:You are using our python client… which we dont support any longer and was way out of date. I would. suggest just making direct calls using the onshape.py file in the API Keys repo to the endpoint and hand building the body. Export is a pretty well documented endpoint.Hello! Thanks for bringing this to our attention. I’ll include support for variable studio features and an updated version of the extension will be issued.
Please be aware that extensions are libraries used inside omniverse apps, and are currently shipped with omniverse app releases, and not as downloadable executables.Once a new package is released I’ll update this thread with instructions to download.Powered by Discourse, best viewed with JavaScript enabled"
570,number-of-triangles-and-vertices-in-usd-how-are-they-calculated-and-why-the-results-are-different-to-blenders-result,"Hi!I have some questions on how the statistics of the numbers of triangles and vertices in Omniverse. As I am a new user that could not upload multiple images, I put all images into one and you could find Fig.1, Fig. 2, and Fig. 3 in the combine image. Sorry for the inconvenience.I read the discussion https://forums.developer.nvidia.com/t/how-to-get-statistic-information-like-blender-in-omniverse/251887 to show the number of triangles and vertices in Omniverse.I have a 3D model consists of multiple objects from other USD files. I check the statistic in Omniverse and it shows that there are 2.3Billions  total visible vertices for all mesh instances; but only 50Million vertices imported and 99.9Million vertices for all unique meshes. As shown in Fig. 1. So, I wonder what is the difference between ‘total visible triangles/vertices for all mesh instances’ and ‘triangles/vertices for all unique meshes’?If I import the USD file in blender, and check the statistic in blender, then I see different numbers, i.e. 61M Vertices and 92M triangles, as shown in Fig. 2.As I have triangulated all objects in the USDs, the number of triangles and faces are the same in blender statistics. However, 2.3B vertices give only 98M triangles in Omniverse, which does not make sense, because given that all faces are triangles, 2.3B vertices should have at least 2.3B/3>0.7B triangles.Then, if I export the scene in blender to another usd file and open it in Omniverse, and check the statistics info, the numbers of ‘vertices imported’ and ‘triangles for all unique meshes’ in Omniverse align with the number in Blender, as shown in Fig. 3.StatisticsFromOmniverseAndBlender.PNG1773×564 201 KBTherefore, I would like to know how the number of vertices and triangles are counted, and what might be the reason for the different results in Blender and Omniverse.Thank you for your time!Powered by Discourse, best viewed with JavaScript enabled"
571,download-latest-isaac-assets,"I have mount my nucleus to the isaac assets on Amazon S3. It seems that the files there are read only. I am wondering if it is possible to download the latest isaac assets to my pc.Yes, mounts are read only.
You can mount it to a different folder like “Isaac-mount” then go to the folder select all of it’s contents and right-click it. then select “Copy”.
Next, create a folder called “Isaac” and paste the contents there.
This a work around for the current version. We should have the download asset feature working in the next release.e.g.

Screenshot from 2022-03-11 09-21-501386×433 46.3 KB
@Sheikh_Dawood What is the current status with storing at s3 ?
ThanksHi @Andrey1984 Please clarify. We already have our assets stored on S3 and in the NVIDIA folder of every Nucleus.Powered by Discourse, best viewed with JavaScript enabled"
572,can-not-find-ai-search-in-nucleus-navigator,"Hi, I follow the instruction here, but there is no AI search option found, as shown below:image2560×1400 437 KBI can find the extension in Omniverse create… So what should I do to solve it? Thanks in advance.Deep Search is available for our enterprise customers and does not run on your local workstation.Oh I see. Thank you for the reply : )This topic was automatically closed 14 days after the last reply. New replies are no longer allowed.Powered by Discourse, best viewed with JavaScript enabled"
573,audio2face-installation-error,"Hello, when installing Audio2Face on Windows, I am getting this error when I click “install” or near the end of the installation, preventing A2F from installing:Error occurred during installation of Audio2Face: request to (url) failed, reason: getaddrinfo ENOTFOUND (url)(The actual urls in the error had to be cut because of the ""new users can only put one link in a post) limitationThe launcher logs are attached here:
launcher.log (1.5 MB)How should I fix this error?Hello and welcome to the forums @totallyalemmingIt might be worth updating the Launcher to see if this works.Can you also try and see if you can install any other applications from Launcher, e.g. USD Composer?Thanks for the reply. Selecting 'check for updates"" did not do anything, so I assume my launcher is currently up-to-date.Installing another application (Cache) did not work with the same error, and installing USD Composer also gave the same error as before. The launcher logs are here:
launcher.log (3.1 MB)Thanks for trying and sending the log file. We’ve created ticket OM-100886 for this issuePowered by Discourse, best viewed with JavaScript enabled"
574,omniverse-live-button-is-disable-in-maya,"I have faced problem with “Live” button in Legacy connector in Maya, that is not getting live on the file I opened from the server in Maya and Omniverse to concurrent work.
But “Live” button activates when I open another file from the same server. Not sure what is the issue.
image_maya011930×1083 249 KB
image_Omniverse012560×1373 300 KBSteps I followed:Maya version. 2023
Omniverse Create(USD Composer) version 2022.3.3
Maya Legacy Connector 200.4.0Does anyone faced similar issue.Hi vikas.vartak:
If USD up axis does not match the up axis in Maya, Live Sync will be disabled. How about you try again after making sure Maya uses the same up axis?
If it fails again, can you download your USD and send us for debug purpose?Hi,
I did check USD up axis in both files and those are matching. Hence sharing the collected file here.
Thanks.
Collected_Conveyor_forNvidia.zip (6.3 MB)Hi Vikas.vartak:I checked your USD. It uses animation which also prevents Live Sync.Hi Juma,
So do you mean, file with animation in it; could not sync in Maya and Omniverse respectively? and does it required only static mesh file to work “Live sync”.Right. Currently it is limited in this way.Powered by Discourse, best viewed with JavaScript enabled"
575,serious-issues-with-ghosting,"Hi all, I am having serious issues with ghosting, the only way I am able to fix this is to raise the items far off the table but I need the items on the table. Can anyone suggest the solution for this, project deadline is tomorrow and I have yet to be able to complete the dataset. There should only be 3 apples in each image. Out of 50 images only 5 do not have this. I have tried disabling Sampled Light etc. I am running on an A40 and also experience constant crashing and need to start new stage every time I want to rerun the code.rgb_00251024×1024 125 KB
rgb_00201024×1024 118 KB
rgb_00221024×1024 119 KB
rgb_00231024×1024 119 KB
rgb_00241024×1024 124 KBCode:It seems to be a combination of issues that make this happen. One is definitely is you try to get a close-up of an object, in this project I needed close-ups, and every attempt failed, as soon as you get closer to the object whether using camera placement or focal length this ghosting happens.OK so this issue only appears to happen when using RTX Real-Time, so far, changing rendering mode to RTX Interactive (Path Tracing) Seems to allow me to get much closer to subject and no more ghosting, so far. Hope it helps someone.Hi @AdamMiltonBarkerOfficial I def recommend trying subframes to combat ghosting. Check out the pages here:Rendering with Subframes Examples — Omniverse Extensions documentation (nvidia.com)Powered by Discourse, best viewed with JavaScript enabled"
576,omniverse-keeps-filling-up-my-c-drive,"Hello Nvidia,I have omniverse cache installed, and pointing at a drive I have plenty of space on. However, loading large scenes keeps filling up my c:\ drive with mostly texture cache data in C:\Users.…\AppData\Local\ov\cache\texturecacheis there anyway to redirect this to another location? ( and ideally limit that amount of data allowed to be cached)Thanks,
KoenHi @koen2. Let me check with the dev team to see if they have some guidance on this.I have the same issue.
Is it safe to manually delete the textures in this cache?Hi. Sorry for the delay. The team has chasing up a few additional questions around this. Should hopefully have an answer soon.There is a new version of Nucleus and Cache coming soon (Nucleus 2023.1.0). This includes a new Cache cleaner service Cache on Workstations — Omniverse Utilities documentation (nvidia.com)When enabled, this will automatically clean up your Kit based caches.One thing to keep in mind for caches, the cache location in this page only changes the Nucleus cache path. If you would like to update where your Kit based caches are, you’ll need to currently update the omniverse.toml file located in your user .nvidia-omniverse/config/ directory.In the [paths] section of the omniverse.toml you’ll need to add or modify the cache_root to be the location you’d like the kit cache to go (i.e. cache_root = ""D:\\OV_Cache"")Powered by Discourse, best viewed with JavaScript enabled"
577,enable-streaming-to-another-platform-unity-for-example-by-opening-a-udp-tcp-socket,"Hi, I wonder if we can enable streaming to another platform (unity for example) by opening a UDP/TCP socket using the same port number?Yes this is possible, but needs some development work on the Unity side.
At the moment Audio2Face 2023.1.0 can stream blendShape weights.We have an Unreal Engine plugin example which can read these streaming values and use them to drive MetaHuman chracters. This plugin will be released in a hotfix in a couple of days, possibly in 2023.1.1.This plugin can be used as an example to create similar tools for other apps e.g. Unity.The UE LiveLink Plugin which can be used as an example is not available on 2023.1.1
See for instructions: Audio2Face 2023.1.1 (Open Beta) Released - Apps / Audio2Face - NVIDIA Developer ForumsPowered by Discourse, best viewed with JavaScript enabled"
578,webrtc-browser-client-extension-doesnt-work-in-browsers-based-on-chromium-remote-connection,"Hello,
I have a Windows workstation on AWS. Omniverse USD Composer is launched on this workstation with enabled WebRTC Browser Client extension.
The issue is that the remote Streaming works normally in Firefox web browser, but not in Chrome/Edge browsers. In Chrome/Edge browsers the stream doesn’t start. Only gray window is presented.
chrome_webrtc1341×833 5.65 KBThe Chrome console contains warnings with 701 STUN host lookup received error
chrome_console.log (14.6 KB)Additional information:
Config of WebRTC Browser Client extension is default.
Chrome browser version: 115.0.5790.102
Firefox browser version: 115.0.2
Edge browser version: 114.0.1823.86Powered by Discourse, best viewed with JavaScript enabled"
579,simulating-a-quadcopter,"Hi everyoneI’ve been investigating the last days how to simulate a drone in isaac sim but there is not any information about it. I would like to simulate four rotors and change the velocity of each one.In gazebo I use this plugin. Is there a similar plugin in isaac sim?thank you in advanceUnfortunately, we currently don’t have a similar plugin in Isaac Sim.Hi @ltorabi, any plan on this? I’m trying to simulate a drone but, independently how fast the propellers rotate it is not flying :(still not in the plan for near future, probably some support in Q3 2022Hi @ltorabi @Hammad_M, anything like rotors_simulator (GitHub - ethz-asl/rotors_simulator: RotorS is a UAV gazebo simulator) instead? I’m looking into something similar myself.Pretty much every controller out there for dinner works with the propeller speeds, unless I’ve missed something.Is there any workaround to have a planning/navigation in 3D that can avoid obstacles? Even if it’s not super realistic it would be great.Hello, was anyone able to simulate this?@ltorabi Any news on the support for this?Thanks!“Big shout to get quadcopter support in IsaacSim”    : )Hi @Te77iiiii_Ho  - Please review this document for quadcopter. GitHub - PegasusSimulator/PegasusSimulator: A framework built on top of NVIDIA Isaac Sim for simulating drones with PX4 support and much moreOr GitHub - eliabntt/GRADE-RR: Generating Animated Dynamic Environments for Robotics ResearchPowered by Discourse, best viewed with JavaScript enabled"
580,liquid-simulation-is-not-reacting-with-blender-animations,"In Omniverse Create, the object animations which I have imported, it is not reacting with the Omniverse liquid simulation.
Uploading: Create 2022.3.3 - C__Users_Hariharan R_Downloads_Collected_Factory_Collected_Factory_Factory2.usd_ 2023-05-28 19-14-54.mp4…

Screenshot 2023-05-28 1915081920×1080 217 KB

you can see the error it throws.ThanksHi,
I am not able to view the video, but to be sure. Did you applied kinematic body with colliders to the animated objects? Note that for 104.2 release we support only animated transformations, if your mesh is changing points, this will not work. We will support animated mesh points through kinematic deformable bodies in upcoming release.Regards,
Aleskinematic body with collidersI added the Rigid body and collider preset and after your suggestion, I went and searched for it, I found it in the settings. By enabling the same, I am encountering different issues.
Thanks for helping out.
HariCan you please visualize where are the collisions, to identify where the particles do not collide?
Please see my video how to enable it:
Check please where are your collisions, to see what might be going wrong.Thanks, regards,
AlesPowered by Discourse, best viewed with JavaScript enabled"
581,failed-to-create-hydra-engine-omniverse-isaac-sim-need-help-thanks,"I have similar error, need someone’s help! ThanksContinuing the discussion from Failed to create Hydra Engine - Omniverse Code:--/renderer/multiGpu/enabled=falsekit_20230717_160302.log (1.3 MB)Hi @lily.chen  - The error message “Failed to create Hydra Engine” typically indicates an issue with the graphics rendering engine. The flag you’re using (--/renderer/multiGpu/enabled=false) is intended to disable multi-GPU rendering, which can sometimes resolve issues if your system is having trouble managing multiple GPUs.Here are a few things you can try to resolve this issue:Powered by Discourse, best viewed with JavaScript enabled"
582,code-2023-1-1-crashes-when-running-headless,"Today I updated to the new version of Code, but unfortunately Code crashes when I try to run the scripts I already created? Maybe you can provide any insight on what could possibly be wrong?crash_2023-08-07_13-12-32_5732.txt (8.8 KB)Powered by Discourse, best viewed with JavaScript enabled"
583,requesting-support-for-visualarq,"Hi,I’ve been messing around with VisualARQ (BIM addon) and it’s occurred to me that it does not work well with Omni/Rhino connector.For example, in Rhino, placing a cube and editing it in live mode is not a problem, but doing so in VisualArq causes an immediate crash every time.No rush, but I just wanted to request support for the plugin.Thanks.Powered by Discourse, best viewed with JavaScript enabled"
584,how-to-switch-between-different-robot-configurations-in-lula,"Hello,I am currently experimenting with lula kinematics and was wondering if it is possible to change the robot configuration that is resulting from the inverse kinematics in lula.Industrial robots can reach the same position with different robot configurations. Some configurations may lead to collisions between the robot and its environment, as seen in this example:
Is there an option to choose the robot configuration, for example through a seed that is passed to the controller?Kind regards
AxelThe warm_start param in compute_inverse_kinematics() can be used to set the initial c-space seed used for the IK solve: Motion Generation Extension [omni.isaac.motion_generation] — isaac_sim 2022.2.1-beta.29 documentationIf you know a “near solution”, you can pass this is in as a warm start.Another option is to solve IK multiple times using a broad sample of seeds in order to identify multiple solutions. In this case, you could take all the solutions from each run, identify the unique options and pick the one that is best for your application.Powered by Discourse, best viewed with JavaScript enabled"
585,rep-modify-pose-scale,"When I try modifying the scale of objects non-uniformly, the rotation matrix of these objects are no longer valid (ex: multiplied by its transpose is not the identity. Here’s an examplePowered by Discourse, best viewed with JavaScript enabled"
586,isaac-sim-on-jupyter-notebook,"please help me run isaac sim on jupyter notebookHi, please take a look at our docs.
https://docs.omniverse.nvidia.com/app_isaacsim/app_isaacsim/install_python.html
https://docs.omniverse.nvidia.com/app_isaacsim/app_isaacsim/tutorial_advanced_jupyter.html?highlight=jupyterI would also recommend trying out our tutorials first.
https://docs.omniverse.nvidia.com/app_isaacsim/app_isaacsim/tutorial_required_interface.htmlLet us know if you are stuck at any of the steps. Please include the Isaac Sim log file if you have issues.how do i run issac sim on windows.
i am using python.bat in the terminak to run it.
but i want to use VScode and use jupyter notebook in it.
i was assigning the python kernal to the path
C:\Users\AI_Admin\AppData\Local\ov\pkg\isaac_sim-2022.2.1\kit\python\python.exe.
but this doesnt recognise the omni packages and so on.
the python.bat file does.
is there a work around it to use .exe file of any sort to assign to vscode kernelI’m afraid we do not have instructions to run Jupyter notebook on Windows.
python.bat is recommended to be used to run Isaac Sim on Windows.This topic was automatically closed 14 days after the last reply. New replies are no longer allowed.Powered by Discourse, best viewed with JavaScript enabled"
587,is-there-someone-from-nvidia-i-can-pay-to-walk-me-through-setting-up-a-cc-character,"I have watched all the videos I can find on YouTube, and still can’t this working a year later. I think if you explain this to me, I can make a video that explains this better for Character Creator users.ThanksPowered by Discourse, best viewed with JavaScript enabled"
588,import-a-map-from-ue4-to-isaac-sim-on-linux,"Hi,Is there anyway to import a map that has been created on Unreal Engine (along with its textures) into Isaac Sim on Linux? I am currently running Isaac Sim on Ubuntu.Thank you for the help!Powered by Discourse, best viewed with JavaScript enabled"
589,animated-textures-in-usd-composer,"Hi,
Is this feature available in USD Composer now? NVIDIA Omniverse USD Composer 2022.3 Release Highlights - YouTube
Can’t find any documentation on this.Now its done this way:Not so easy but everybody should manage to follow that tut.Actually, this is the latest and greatest: Timesample Editor Overview - YouTubeWow that is nice update!
Any possibility to have same kinda short tutorial video on those VDB seqs please?This topic was automatically closed 14 days after the last reply. New replies are no longer allowed.Powered by Discourse, best viewed with JavaScript enabled"
590,popup-when-dragging-material-on-top-of-object,"This was a CAD import. I don’t know why the material is yellow. It is not yellow in the STEP file.
bild645×706 85 KB
When I drag an Omniverse material on top of the part I want to change, I get this popup. Pressing OK does seemingly nothing.
bild652×620 84 KB
Choosing anything from the drop down doesn’t give me any immediate visual feedback until I click OK and then it’s too late and I have to undo.This feels broken and the UX hasn’t been thought through. I dropped with my cursor on a point on the model. Omiverse should know what material is under the cursor on that point!(Can I guess that if I ever choose to re-import this STEP file, all the material assignments will be reset without a dialog asking me if I want to keep my replaced materials?)(Can I guess that if I ever choose to re-import this STEP file, all the material assignments will be reset without a dialog asking me if I want to keep my replaced materials?)No - not normally. The way the asset import workflow works is that it will convert the input file to USD, and then reference that USD into your scene.Any material assignments changes to the referenced asset will be USD “Overs”, saved into a USD layer. So your converted Asset should remain unchanged, and you should be able to re-convert that asset without losing your Overs (unless the prim paths have changed - orphaning the “overs”)The behavior of material drag+drop is dependent on your selection mode. There are two (in 2022.3.3): Prim, or Model.In Prim mode, it will drag and drop right onto the prim under your mouse.
In Model mode, the ‘model’ may contain multiple descendants and therefore you need to choose: assign to the top-level node (material assignments can be inherited) or you can assign to one of the descendants.That said - I spoke to one of our designer and there are planned improvements for this workflow to make it more intuitive.Powered by Discourse, best viewed with JavaScript enabled"
591,audio-to-gesture-to-metahuman-workflow-fingers-fix,"I am trying to transfer animations from audio to gesture to Metahuman in UE5.
I have been able to transfer the animation, however, there are some major issues going on with the fingers,
Any help on how this
can be fixed or what could be causing this?
have tried the same in blender as well and the same issue is coming.
Screenshot 2023-07-30 1654541094×837 56.3 KBHi @ayushsahai0074 ,
I’ve reached out to our team here to share some infos. I won’t be able to check this out for another week. Hope it’s resolved by then.@ayushsahai0074 while the dev reaches out, the explanation in this thread here sounds like the reason behind why you are experiencing this issue. Below is an excerpt:on the A2G armature, we don’t have the finger tip joint as they are not used.
The points on the last segment of the finger are all weighted to the last joint of the chain.
When setting up retargeting in kit, if your target character has extra finger tip joint, you can manually pick the right joint for it on the retarget UI.I am also interested in knowing if there’s a solution to this; let’s wait and see from the dev team.
this is the workflow that i have used, The fingers are looking really weird, Any help is appreciated.Fingers are very tricky to retarget. First the retarget pose has to match between two skeletons. For example, fingers in the bind pose could be narrow or wide or they might face different direction, which could cause twist and look weird. Also the way you look at this is that not looking at joints transform but looking at skinned hand posture, and see if they match relatively.Secondly, as @esusantolim pointed out in the other thread, the # of joints in the finger could impact, and how it’s skinned. The retargeter is FK chain retargeter, and how you pick your tags will create those chains and to match between.We’d like to create an easier way to retarget Audio2Geature to MetaHuman, but no immediate plan yet.Powered by Discourse, best viewed with JavaScript enabled"
592,ros2-nav2-and-isaac-sim-wizardvehicle,"Hi, I want to combine the wizard vehicle and NAV2. But I couldn’t figure out the steps for that. What is the best way to navigate a wizard vehicle?What I mean by wizard vehicle is at the link down below.
https://docs.omniverse.nvidia.com/prod_extensions/prod_extensions/ext_vehicle-dynamics.html#next-steps-pageNAV2:
https://docs.omniverse.nvidia.com/app_isaacsim/app_isaacsim/tutorial_ros2_navigation.htmlHi @MuratKOSE -  At this moment there is not a specific tutorial or document to integrate wizard vehicle with Nav2. But you can follow the general tutorials and guides for integrating any robot with Nav2 and apply it to your wizard vehicle. Below are the documents:Nav2 documentation: Configuration Guide — Navigation 2 1.0.0 documentationROS2 Nav2 tutorial: ROS2 Nav2 - Navigation Stack in 1 Hour [Crash Course] - YouTubeROS2 Nav2 example: navigation2/nav2_bringup at main · ros-planning/navigation2 · GitHubPlease take a look and let us know if you have follow-up questions.This topic was automatically closed 14 days after the last reply. New replies are no longer allowed.Powered by Discourse, best viewed with JavaScript enabled"
593,ros2-crashed-on-windows-10,"Hi,
I tried to control my robot through ROS.
My OS is Windows 10, in the following video I added ROS2 components to the action graph.
I found no matter what kind of ROS2 components I added, Isaac Sim always crash.
The following picture shows the error message!
It works on Ubuntu 18.04, but can’t run on Windows.

image1263×827 54.7 KB
Hi @vic-chen - I hope this document will help clarify ROS/ROS2 support
https://docs.omniverse.nvidia.com/app_isaacsim/app_isaacsim/install_ros.htmlHi @rthaker ,
I follow the ROS install instructions, the following picture shows the ROS talker and listener seems to work. I think the ROS should be installed success. But the Isaac Sim still crashed.
Ros2976×528 10.9 KB
The attachment is the error report.
crash_2023-06-28_13-54-16_20600.txt (8.4 KB)Powered by Discourse, best viewed with JavaScript enabled"
594,hello-teachers-why-dont-i-lead-to-the-usd-action,"Claire’s character guides USD to Ue5.2. The UE5.2 character has no expression. Do I need any more settings or plugins?
11796×845 217 KB21356×791 180 KBHello and welcome to the forums @261805624Here’s a short tutorial which should help: NVIDIA Omniverse Audio2face to Unreal Engine 5.2 Metahuman Tutorial - YouTube你好大佬！Can you export USD to use in UE5? Link mode is easy to drop frames!Powered by Discourse, best viewed with JavaScript enabled"
595,submitting-issues-in-isaac-sim-to-be-addressed-my-staff,"Hello!Is there a way to submit issues/bugs we have found in the Isaac Sim code to be addressed by staff? I don’t see a repo in Nvidia Omniverse GitHub in which to create bug reports.Thanks!Hi @jess7654 - You can provide a list of issues/bugs here. I can verify with the developers and create JIRAs for internal tracking.Powered by Discourse, best viewed with JavaScript enabled"
596,in-isaac-sim-docker-container-how-can-i-cache-on,"Hi I installed Isaac sim docker from “isaac-sim.docker.gui.sh” file below…isaac-sim.docker.gui.sh (1.5 KB)so in that container terminal
I tried below command and gui screen is loaded.
$ ./isaac-sim.sh

image1281×750 115 KB
However , as you can see in right upper side, the CACHE is OFF with red color text.I want to CAHE:ON to use jupyter notebook to step-by-stepy run my ipynb code below.

image1012×1224 147 KB
doyoung.ipynb (1.7 MB)when I run my doyoung.ipynb code in jupyter notebook, the code only run in headless mode I think. because there is no gui changes.As I know, jupyter notebook live syncs to the same USD so that changes are reflected in another Isaac Sim process launched through the terminal or the launcher. But, in my situation I can not synchronize between jupyter notebook and isaac sim like this below tutorial video in Nvidia document. so I guess this CACHE:OFF causes the problem.thank you!Hi. Do you have Cache and Nucleus installed from the Launcher on the host that is running that docker container?Hello! Thank you for reply… Im so sorry but I already installed Cache and Nucleus in my local host pc but I don’t know how to connect those with my docker container. can I get reference document or guidance from you ?And, also as I found some documents, Nucleus and Cache is no availabe in Isaac Sim container so there is no way to use Cache:On … right??
image1920×1474 179 KB

https://docs.omniverse.nvidia.com/app_isaacsim/app_isaacsim/install_faq.html@Sheikh_Dawood  hi… Im’ so sorry to bother you but can you reply my qustion…?Hi, apologies for the late reply.
Cache and Nucleus is no longer included in the Isaac Sim container. You can install them from any Workstation with Omniverse Launcher installed.
Please note that this Cache is not the same as the cache path in the container. The cache in the container is a shader cache while the Cache icon in Isaac Sim is for the Nucleus Cache which only works with the Workstation version of Isaac Sim.
However, you may run Isaac Sim on a container where he host have Cache and Nucleus setup locally and that icon may turn green.I have the same question as @user132705 .I have Cache and Nucleus setup locally on the host pc, which also contains the container as you described. However, the icon of the Isaac Sim within the container still does not turn green (the icon of the native Isaac Sim app turns green though). Is there a specific setting I need to make to make the Isaac Sim (in the container) connect to the Cache running on the host PC?Powered by Discourse, best viewed with JavaScript enabled"
597,audio2face-character-setup-tooltip-error-with-fix,"So I’m not a big fan Documentation :)
I hope this is okFixed version of the file:
ui.py (133.3 KB)file Location:
[omniverse_install]\Omni_App\pkg\audio2face-2022.2.1\exts\omni.kit.charTransfer\omni\kit\charTransfer\scripts\ui.pyExplanation of error and fixSo kind of odd if the video is low rez scrub it to the end and then back and it sharpens up.
yaaa technology.Thanks so much for reporting and posting the possible fix. We also found this bug and had it fixed a while ago. This will be released in a week or so.Powered by Discourse, best viewed with JavaScript enabled"
598,how-to-simulate-holes-when-importing-urdf-the-peg-can-not-insert-to-the-hole,"Hello,
I am now going to do a peg in hole tasks. However, when I import the urdf of peg and hole, the peg can not insert to the hole. I think it may related to the convex hull, but I don’t know how to configure it … can anyone help me ?image1196×662 78.5 KBHello,To modify the Collision Type after converting URDF to USD using the URDF Importer, you can follow these steps:I hope this helps!Hello，@ smakolon385
I followed your steps, but can not find “Collider section” in the Physics.
image1356×943 131 KBI apologize for the lack of explanation!Based on the URDF you provided, the collision tags and visual tags are defined within the URDF, so there should be a “collisions” stage along with the “visuals” stage in the USD generated by the URDF Importer. You should be able to find it by expanding the “+” icon next to “base_link” in the Stage tab on the right. Under “collisions,” you should see “mesh_0,” and I believe you will find “Physics → Collider” there.Thanks,
I can found the Physics → Collider now, But the peg still can not insert into the hole
image1736×948 87.6 KBI’m glad to hear that you were able to change the collision type successfully.Can you enable collision_visualization to check the collision mesh? You can enable collision_visualization by clicking on the eye icon in the top-left corner and then selecting Show By Type → Physics → Colliders → All. This should visualize the collision mesh as a green Triangle Mesh. Please take a look at the Collision Mesh and verify if there is a proper hole on the “hole” side.Thanks,
I find that the collision mesh of the hole was convex,  then I change the Collider setting to SDF Mesh in the visuals stage, and the peg can successfully insert into the hole!
However, when I try to import a smaller hole, the peg still can not insert to the hole.
image1756×983 95.8 KB
If it’s alright, could you please provide me with the URDF files for the Peg and Hole, as well as the corresponding STL files? Based on the images I’ve seen, it seems that they haven’t been converted to SDF Mesh yet. I would like to try converting them myself and verify the results. Once I have confirmed it, I will attach the USD files to this thread.sorry, Maybe I missed some steps, but now it’s okay again.
I have summarized the following steps:1, import the urdf , and checked “fixed based link” and “create physics scene” (is this necessary?)
2, Open the corresponding USD file
3, change the mesh type to SDF Mesh and check the mesh by “enable collision_visualization”, but do not add a rigid body with colliders present (Not verified, but it has been added before. I don’t know what’s going on, but it sometimes doesn’t work)
4, use “create_prim” to create the hole to the world.Thank you again for your answer!
By the way, from where you found that they haven’t been converted to SDF Mesh yet？That’s great to hear!The reason I thought the objects were not converted to SDF was because when I created an Insertion task in the past using SDF, the Triangle Mesh was provided in finer detail, and the mesh appeared on the object’s surface. However, when I looked at the images you provided, it seemed like the mesh was positioned away from the object’s surface, which led me to believe it wasn’t converted to SDF.The current mesh now is like this, I don’t know whether it provided fine details?
Because even if I increase the “SDF SOLUTION”, the mesh seems not change. Furthermore, when I set the value to 2500, the mesh disappeared…
hole (2).STL (7.8 KB)image1111×626 35.2 KB
image661×591 41.8 KB
image1168×647 90.3 KBI see.
It doesn’t seem to have the level of fine detail I expected. In my case, I would directly open the STL file in Blender or similar software and subdivide the mesh to make it finer.Okay, Thank youThis topic was automatically closed 14 days after the last reply. New replies are no longer allowed.Powered by Discourse, best viewed with JavaScript enabled"
599,how-to-introduce-a-new-usd-schema-that-built-in-c-dll-into-omniverse,"I am new to the Omniverse, I have a new usd schema and generated dll/lib that directly using pxr schema generation pipeline. It is used it in my project. I didn’t export it as pyd file since not needed yet.  But I don’t know how to introduce this new schema into Omniverse (so I can see the usd that containing new schema type displayed correctly with the new type instead of defaultPrim).
Do I have to export it as pyd to be picked up by Omniverse? Does regular C export to pyd will work if I do have to? or I have to use Omniverse Native Interface python binding(omni.bind) to export it?
This document mention the greet example, but I can’t find full source code even I download Kit, Create and Code. also I can’t find omni.bind that mentioned in the doc. https://docs.omniverse.nvidia.com/py/kit/source/extensions/omni.example.greet/docs/index.html
Is there a full example of creating such new schema and being used (in C++) in Omniverse?
Thanks.It should be possible to load custom USD schema into Omniverse. You need to create an extension, that will make sure that the USD schema is registered in USD. Also this extension needs to be loaded from start (you can check that box in Create->Extensions once you have it).
Here is a documentation for the USD schema usage in Omniverse. You can try to find the schema extensions within the create install directory, they all are starting with omni.usd.schema…USD Schema extensionsUSD libraries are part of omni.usd.libs extension and are loaded as one of the first extensions to ensure that USD dlls are available for other extensions.USD schemas itself are each in individual extension that can be part of any repository. USD schema extension is loaded after omni.usd.libs and ideally before omni.usd.Example of schema extension config.toml file:… code:: pythonSchema extension contains pxr::Schema, its plugin registry and config.toml definition file. Additionally it contains a loading module omni/schema/_schema_name that does have python init.py file containing the plugin registry code.Example:… code:: pythonpxr.UsdPhySo I have followed the omni.usd.schema.physics to set up the folder structure and .toml setup. I actually copy the entire folder and renamed the individual files and path) It is all loaded fine until I replace the actual dll to use my own built dll, then it gives me this error “Could not load dynamic library”. Strangely enough, I can register the exact dll plugin  in maya 22 usd and plain commandline prompt using pxr plug registerPlugins and Load() ),  That lead me to a conclusion that it might be because how I build dll (maybe linked to a library path missing?), So I copied the linker lib into bin folder with the dll, still no luck.   I think I need an example how to build dll in omniverse. Do I need to set up some macro or linked to omniverse pxr version?Yes, you will need to run the schema code gen against Omniverse pxr version. You can get it through CONNECT SAMPLE OMNIVERSE CONNECTOR. Though I am afraid I dont know the details how exactly run the schema gen with that version. Will try to ask our USD experts.Once you have the nv_usd package from the Connect sample, you can set up environment variables in your terminal to add its paths per the instructions here – USD Tutorials — Universal Scene Description 22.08 documentationThen you can run usdGenSchema per Pixar’s instructions at Generating New Schema Classes — Universal Scene Description 22.08 documentation and Universal Scene Description: Creating New Schema Classes with usdGenSchema (i.e., likely the same way you originally generated the dll/lib binaries for your schema.The following in on Windows10, used python 3.6.8, omniverse download from  https://developer.nvidia.com/usd#usd, build on vs2019.
I generate usdMy.dll with very simple schema:
#usda 1.0
(
“This is the USD schema for the usdMy dll. It provides all of our custom types and APIs for USD.”
subLayers = [
@usd/schema.usda@
]
)over “GLOBAL” (
customData = {
string libraryName = “usdSMS”
string libraryPath = “.”
}
)
{ }class “myGuidAPI” (
inherits = 
doc = “GUID API.”
customData = {
string className = “GuidAPI”
token apiSchemaType = “singleApply”
}
)
{
string guid (
customData = {
string apiName = “guid”
}
doc = “whatever”
)
}the project builds usdMy.dll succesfully, then I run usdGenSchema.cmd(from omniverse package) to generate bunch of .h, .cpp, generatedSchema.usda and plugInfo.json,
To test the extension, I just create a folder in exts directly
I copy over the dll into \AppData\Local\ov\pkg\code-2022.1.0\kit\exts\omni.usd.schema.my\bin\usdMy.dll
I copy over all generated .h, .cpp, generatedSchema.usda and pluginfo.json into \AppData\Local\ov\pkg\code-2022.1.0\kit\exts\omni.usd.schema.sms\plugins\UsdSMS\resources, the original schema.usda, i move to the \usdMy\ inside resources to be clear
My pluginfo.json, I tweak the last bits to be the following (physics extension sample, don’t understand why it is not /bin/usdMy.dll instead, but from the result, it seems able to locate the right one)
“LibraryPath”: “…/…/…/…/lib/usdMy.dll”,
“Name”: “usdMy”,
“ResourcePath”: “resources”,
“Root”: “…”,
“Type”: “library”My extension.toml, I don’t have pyd for my module, so just native
[core]
reloadable = true
order = -100
[package]
category = “Internal”
title = “USD My Test schema”
description=“USD My Test schema”
version = “1.0.0”
[dependencies]
“omni.usd” = {} # tried without this, still no luck
“omni.usd.libs” = {}[[native.library]]
path = “bin/${lib_prefix}usdMy${lib_ext}”my python __init__py that sit at \AppData\Local\ov\pkg\code-2022.1.0\kit\exts\omni.usd.schema.my\usd\schema\usdMy
import os
from pxr import Plug
pluginsRoot = os.path.join(os.path.dirname(file), ‘…/…/…/plugins’)
smsSchemaPath = pluginsRoot + ‘/UsdMy/resources’without above all, I use the launcher → library, Code, Extensions tab, find my schema, load using the toggle button
I got the error message in console:  2022-01-25 18:03:32  [Error] [omni.ext.plugin] Could not load the dynamic library from /appdata/local/ov/pkg/code-2022.1.0/kit/exts/omni.usd.schema.my/bin/usdMy.dll. Error: The specified module could not be found.some interesting detail: the following exact script:
import sys
sys.path.append(‘/AppData/Local/ov/pkg/code-2022.1.0/kit/exts/omni.usd.schema.my/plugins/UsdMy/resources’)
from pxr import Plug
Plug.Registry().RegisterPlugins(‘/AppData/Local/ov/pkg/code-2022.1.0/kit/exts/omni.usd.schema.my/plugins/UsdMy/resources’)
for plugin in Plug.Registry().GetAllPlugins():
print(“{} {}”.format(plugin.name, plugin.isLoaded))
print(Plug.Registry().GetPluginWithName(“usdMy”).Load())in python36 commandline prompt, (outside of omniverse code):
It will print out usdMy.dll in all the registered plugin
It will print out TrueBut in Omniverse Script Editor:
It won’t print out usdMy.dll as registered first of all,
Then of course It is None
To avoid confusion, I used the absolute path in plugininfo.json for library and also absolute path in python Plug Register functionThe USD binaries used in OV are not the binaries on usd.nvidia.com (the latter are built from the unmodified Pixar GitHub source). We are working to align OV’s USD packages with those of a standard Pixar distro, but don’t have a firm date on when that effort will be complete.To get the nv_usd package used in OV, you can get the Connect Samples from Launcher. Building those samples will download the nv_usd artifacts, which you can use to compile your schema.Hope that helps; sorry for the confusion.Thank you very much for the reply. I will take a look of the Connect Sample, see what nv_usd artifacts will be downloaded and if I can use itAfter I use everything (include usdGenSchema, include and libs) from nv_usd folder. the dll finally loaded! Thanks. I can see difference from a simple schema class I made between pxr vs ov. Thanks a lot,
There is a glitch of I put this schema in extension:
the schema extension will toggle on from Code->Extensions tab, will not be loaded (seem that what I put in the exts\omni.usd.schema.my\ folder the init.py won’t get called at all). The print GetAllPlugins() won’t have my dll either. but if in the python editor, I registerPlugins( with absolute path to the plugin.json). The module will be loaded. Is there some environment variable I need to set for omniverse?
Also, there is a console message, I am not sure if it is related "" [Warning] [omni.ext.plugin] [ext: omni.usd-1.4.7] ‘/appdata/local/ov/pkg/code-2022.1.0/kit/plugins’ in ‘[[native.plugin]]’ was not found.""Yes, you need to toggle the autoload, if you just enable the extension the USD wont register the schema correctly as it was already initialized. If you set autoload and open Create again, it should load the extension and register your schema correctly.Thanks for the reply. I am trying to understand how do I test if a custom schema really got picked up by omniverse. I used the schema that from pixar tutorial Generating New Schema Classes — Universal Scene Description 22.08 documentation, which contains simplePrim, complexPrim and paramsApi. my usd looks like the following:
#usda 1.0
(
defaultPrim = “MyFirstComplex”
metersPerUnit = 0.01
upAxis = “Y”
)def ComplexPrim “MyFirstComplex”
{
string complexString = “a really complex string”
int intAttr = 10
add rel target = 
}def Xform “MyTestObject” (
prepend apiSchemas = [“ParamsAPI”]
)
{
custom double params:mass = 1.0;
custom double params:velocity = 10.0;
custom double params:volume = 4.0;
}def Xform “Blahblah” (
prepend apiSchemas = [“UndefinedAPI”]
)
{
custom double params:what = 6.0;
}When I open the usd, omniverse seems have no problem display the undefinedAPI type, (which I didn’t introduce in schema.usda. It does show ComplexPrim as Type for the MyFirstComplex. Does it mean custom schema successfully got picked up?

image602×792 38.9 KB
I am also struggling with this. How does one “build against the nv_usd package”? Can a more specific walkthrough be described?Powered by Discourse, best viewed with JavaScript enabled"
600,blurry-camera-images,"Hello there,I am using an implementation similar to Isaac Orbit to create a camera for visualizing my reinforcement learning environment. The camera creation code looks like:But the captured images hence the video looked to have motion blurs:Google Drive file.I ensured physics_dt and rendering_dt of the SimulationContext are identical.What is the cause of the problem, and how can I address it?Thanks.Hi @btx0424  - The motion blur in your captured images could be due to a few reasons:To address the issue, you could try the following:After further playing with the settings, here are my findings:Can you offer some more specific guidelines?Powered by Discourse, best viewed with JavaScript enabled"
601,get-orientation-pointcloud-of-objects-in-camera-pixel,"Hello,In the YCB_Video Dataset there is a variable that is called “vertmap”. It has the shape (640,480,3) (pixel_x,pixel_y,point(x,y,z)) and shows the distance from the object center (3D) to the shown point in the image. So it is not a real pointcloud that has its center in the camera, but for each object it has its own point cloud that originate from the center of said object. Is there any internal function that can do something like this? Or should I start from scratch? My goal is to generate a dataset really close to the YCB_Video dataset, but with my own objects.If you have questions to understand what I mean, feel free to ask. I will attach a vertmap.npy and a png file, so if you want to further understand feel free to download.

000001-meta.mat (598.6 KB)
000001-vertmap.npy (3.5 MB)


000001-box.txt (123 Bytes)Thank you!Hi @valentinhendrik , have you checked the replicator’s pointcloud annotator? It should gives you the desired pointcloud. But that requires you to load your own asset and generate the data. You can take a look at offline_pose_generation.py which uses ycb_video_writer.py. To get the poincloud data, you need to add a arg that sets pointcloud=TrueYes I know that, but it is not using a regular pointcloud. What I need is a mapping of points in object coordinate system in the image array. Pointcloud annotator only gives out points in world coordinates.In that case, you can use the CameraParams annotator to get the cameraProjectionMatrix and cameraViewTransform matrix to convert the world points to object space, then to image space.This topic was automatically closed 14 days after the last reply. New replies are no longer allowed.Powered by Discourse, best viewed with JavaScript enabled"
602,i-installed-2023-1-and-now-l-for-lights-doesnt-work,"I finally got my computer back, and I can’t get the lights to turn off with L used to toggle it. G still works for grid.Is this a bug?Same hereYes sorry, it is now SHIFT+L, and SHIFT+C for cameras. Grid remains at G.OK, thanks.Powered by Discourse, best viewed with JavaScript enabled"
603,sharing-a-community-extension,"If you’ve developed a new community extension or have released an update to your extension, we encourage you to share and announce your extension in this forum. For more information about extensions and how to create one please check out these resources:These are the guidelines for sharing a Community Extension:Powered by Discourse, best viewed with JavaScript enabled"
604,how-to-properly-install-enterprise-launcher-on-a-local-network,"Hi all,
I’ve just got the 30 day trial of the Enterprise and I am trying to figure out how to properly set it up, starting with the Enterprise Launcher installation on our local network.We are trying it out because I was told that the free version of the Nucleus is not designed to be shared on the local network (it should be stored on the local workstation). And we really want to try out the collaboration features of Omniverse sharing content on our local server.Here is the post where @dlindsey  explains it to me.From what I understood so far watching this video that I the Enterprise Launcher serves as a deployment package that can be easily shared and updated with other users.Could someone please clarify some things to me?I would really appreciate that someone from Nvidia help me out with it because I want to make the most out of this 30-day trial as we are seriousy considering to use it as our base rendering platform if everything works well.Thanks in advance,
KarolHi Karol,The Enterprise Launcher (IT Managed Launcher) is similar to the Workstation Launcher, however it allows IT Administrators to restrict configuration changes and application installations.  (For example, the Workstation Launcher displays the Exchange tab, which is akin to an App Store for Omniverse, where the IT Managed Launcher does not. - Any applications needed by users will have to be pushed via a deployment script allowing the IT Administrator full control over what is installed.) - The Nucleus Server does not handle the deployment or upgrade of client applications.  We have seen customers integrate the deployment scripts for Omniverse applications with system management tools such as SCCM and other similar applications.Enterprise installations are typically tied to the additional installation of an Enterprise Nucleus Server which is the central server/location where all the user’s data will be stored.  This requirements for an Enterprise Nucleus Server is an Ubuntu or Red Hat Enterprise Linux Server running Docker/Docker Compose (Kubernetes coming soon!) allowing your users to work and save data in a single unified location (not tied to any workstation) to allow the very best options for collaboration.For pricing information, we can definitely have someone from our Sales Team reach out to you and share the latest pricing information and suggested resellers.  (I have your e-mail address through your user profile and we will follow up soon.)For all the users that want to use Omniverse Applications, the Launcher will need to be installed locally.  If users just want to copy files to (and from) an Enterprise Nucleus Server, they can do so through a web browser without installing anything, however they will need an account on the Nucleus Server.Local caching is always preferred and should remain enabled.Please let us know how we can assist further ensuring your trial meets your expectations.  If you need your trial extended, I would be more than happy to work with you on this!Powered by Discourse, best viewed with JavaScript enabled"
605,check-specific-link-velocity-and-pose,"Hello, may I know how to check the specific link facility and pose (in the world frame)?Thank you.Hello, may I know how to check the specific link facility and pose (in the world frame)?Hi @berternats - In Isaac Sim, you can get the pose of a specific link of a robot using the get_link_pose() method of the Articulation class.Powered by Discourse, best viewed with JavaScript enabled"
606,how-can-i-turn-off-snap-widget-ui,"Hello,
The snap widget follows me through every click and every interaction, yet I rarely need it. How can I turn it off so it doesn’t clutter my screen nor my visual focus?
Thanks for your help. Viva OV!

-ZiAIf you click on the small grey arrow it should collapse that menu

image833×447 58.5 KB
Thank you so much for the detailed reply. I understand, or believe that the snapping menu was added to help onboard users to this functionality because switching between global and local axis by double pressing ‘w’, ‘e’, ‘r’ on the keyboard may not be easily intuited.
Similarly, in Maya, we can manipulate the pivot of an object by pressing the ‘insert’ key. It is a great function, but it requires knowing someone who knows this, or reading the manual.
Modo came out around ~ 2013 (?) and someone once asked the CEO Brad Peebler what the hardest part of being CEO was. He replied,
“Not creating a button for every problem. Because as engineers, in charge of our own software platform, we can do that.”
Instead, his lead tool designer had to convince him to implement tools that could be used in combination with each other to achieve a variety of results. As opposed to designing individual buttons.
It is my hope that we can steer Omniverse down this same path and not clutter up the UI. I think the best UI’s are the one’s we don’t look at. How many great programmers spend time looking at their keyboard? Baseball players don’t stare at their bat while at the plate. We don’t drive by staring at our steering wheel. We look through the UI right? To the target of the action. So in this case, I want to be able to remove even the grey arrow from the viewport.
It occupies the viewport around 100% of the time? But I need it about  4% of the time. And even less after I learn the keyboard shortcuts.
In the early days of Google, a user would send the dev team an email with a number. No one could figure out what it was.
“Why is this guy sending us a number?”
There was never any explanation attached. The number sent at seemingly random and infrequent intervals. But the team plotted it and they noticed it always went up. Finally someone figured out that it was the count of the number of words on the home page. From then on they tried to keep the number consistently low.
In these early growth stages, Omniverse could become like Microsoft, a platform so dominate that everyone has to use them sooner or later.
Or we could become like Apple, whose product’s love is earned through design and thoughtful consideration of user’s experiences, current and future. Though my programming friend disagrees with the Microsoft characterization.
Microsoft - ‘Have to’,
Apple - ‘Love to’.
Omniverse - ‘Love to’ ?Hope this makes some sense as to why one would want to eliminate the ‘>’ completely.Cheers,
-ZiaPowered by Discourse, best viewed with JavaScript enabled"
607,play-and-pause-of-animation-that-is-done-in-blender-and-imported-to-nvidia,"Hello,I am struggling a bit for the animation which is done in blender and imported into nvidia omniverse. I want to know how can we play and pause the BLENDER model animations with click of a button that is created in action graph.I tried the following approaches for this:Tried to stop the animation using action graph where I created a button and by using ‘on widget clicked’ and connected it to the ‘Timeline(stop)’ input but this didn’t work for me. I even tried it with the ‘stage’ event to stop the animation while playing but even this didn’t work.Secondly we didn’t have any animation data to access and take control of the animation(play and pause it). Wanted to know how can we access the animation of the objects if the animation is done in blender not in omniverse.Tried converting the prims having animation to “USD TimeSamples to curves” i connected ‘on_tick’ as well as ‘On_Playback_Tick’ to the timeline node after converting but still it doesn’t work. Seems like it is not referring to the timeline.I just want to play and pause a animation of an object that is done in blender.
I am pretty new to Nvidia Omniverse, thus I need help with this.
Looking forward to your suggestions.zipped the images images.zip (469.6 KB)@vivek.chaini can you please share the USD file to take a look on what’s the real problem you are facing?Here is the USD
Collected_AnimationTestGraph.zip (224.5 KB)If you do know how to program a customized node, you may play around with the omni.timeline API in your node and then stop/play the time. If you don’t. We may address this type of issue in the next release. But in this release. I offer you some tricks to do so.
Thanks @jiayuan I will try this out. Great explaination.So I have some points that I have @jiayuan @mukundang:
image1477×697 119 KB
Screenshots:
MicrosoftTeams-ima45ge.zip (3.6 MB)Hello @jiayuan so in my project total length of the animation is 4500 but in the curve editor when i click timeline it shows  upto 240. Now if i play by clicking custom start button the animations are playing for a while then stops i dont know why that is happening.
If i click the animation data under the timeline it goes upto 4500 in curve editor.

image1920×999 146 KB
your timeline shows upto 240 frames because you set so. In your timeline prim, you set the length to 10.0 while the framerate to be 24, that’s 24*10.0(seconds)=240 frames.  If your animation is 4500 frames, and you want to start playingback the animation with your keyboard input, you’d construct a 4500 frames curve in your timeline prim.  That is you set the length to 4500/24.0 = 187.5 seconds then select the second key and set its value from (240.0, 240.0) to (4500.0, 4500.0)Powered by Discourse, best viewed with JavaScript enabled"
608,build-an-interactive-avatar-with-asr-chatgpt-tts-with-audio2face-from-renton-hsu-vfx,"Sharing from another thread, great example by RentonPowered by Discourse, best viewed with JavaScript enabled"
609,how-to-simulate-physics-material,"Hi everyone:
I’m trying to simulate a material’s extensibility.
The following video is I’m trying to simulate a plastic object and see how far it can be extended.
I found some material’s parameter from internet, like dynamic friction and young’s modulus. But I’m not sure if it is correct.
Also I’m not sure if Isaac Sim can simulate how far a object can be extended.
As the following video, the object seems can be extended unlimint.
I used Isaac Sim 2022.2.1 version. Anyone can give some advices?
And following video confused me. Why my object can’t extend unless I selected the show the type by deformable body mode and selected the object?
Hi @vic-chen ,
Two observations.Hi @SimonPhysX ,
Thanks for your reply, but I still don’t understand how to adjust parameter appropriately .
As the following video, I just simply add a deformable body to a cylinder object.
In the video, I don’t know which one is the real simulate effect.
When I open show by type to check deformable body effect, I think this should be the simluation result doesn’t it?
But you can see when I diselect the cylinder, the simulation effect looks weird.
And as you said, Isaac sim’s seems can’t simulate plastic deformatioin?
So that if I add any physics material to my object, will it have the pyhsics effect?
You need to change the resolution scales in the create → mesh → settings dialog and you will understand :)Hi @SimonPhysX ,
The following video I try to change the verts scales, but I still have some questions.Hi,Hi @SimonPhysX ,
Thanks for your answer. Hope one day that omniverse can simulate other physics materials.
Best regard with you.Hi @SimonPhysX ,
About my 6th question, you said that omniverse currently only supports elastic materials. The following questions I wanted to understand.Hi @vic-chen,
I will need some more information on what you are trying to achieve. There is no built in deformation limit or anything like that. However you could try to add plasticity yourself by changing the rest shape at runtime, but currently we don’t support that. We will likely add that soon. You would need to recreate the deformable in the deformed state right now.
Regarding your question 5. did you make sure to create a deformable body material?
Cheers,
SimonHi @SimonPhysX ,Powered by Discourse, best viewed with JavaScript enabled"
610,simulate-battery-charger,"Any suggestion on how to go about simulating a “battery charger” in Isaac Sim.Here is what I am trying to accomplish:
1 - Running ROS2 docking controller
2 - After docking to the battery charger successfull,  start charging, provide indication of coupling (light)
3 - Once the battery is fully charged (100%), stop the battery charger (decoupling (light))
4 - Undock the robot from the battery chargerI looked at the Samples (Clock, DeskFan, LightBulb and Maze) but not much documentation is provided. It would have been greate if there were some documentations (is there any documentation on it?).If anyone have ideas/suggestions/directions, it would be highly appreciated.Thanks.https://docs.omniverse.nvidia.com/kit/docs/omni.graph.docs/latest/Overview.htmlhttps://docs.omniverse.nvidia.com/extensions/latest/ext_omnigraph.html#what-is-omnigraphPowered by Discourse, best viewed with JavaScript enabled"
611,unable-to-download-gdpr-agreements-and-eula,"OMNIVERSE installation works fine on my home PC.
However, when installing on the company PC, the following error message occurs.Unable to download GDPR agreements and EULA. You may need to inspect your internet connection and restart the launcher. Try these troubleshooting links for Proxy and [Firewall]how to deal with?Hi @hyeyeongjo. I’m going to move this to the Launcher forum since it’s likely related to that.Hi @hyeyeongjo. You company may be block AWS or AWS may be blocking incoming traffic. Please have a look at this duplicate post: Omniverse Launcher cannot be launchedPowered by Discourse, best viewed with JavaScript enabled"
612,getting-rigid-body-error-when-starting-simulation,"Hi, I was working on the issac sim and following the tutorial of the documents provided by issac sim.I’ve tried your tutorial step by step which corresponding link says:
https://docs.omniverse.nvidia.com/app_isaacsim/app_isaacsim/tutorial_ros2_drive_turtlebot.html#isaac-sim-app-tutorial-ros2-drive-turtlebotAfter I put the turtlebot burger urdf file via import URDF, the error messages is keep occuring as below when I play the button.2023-06-05 16:28:43 [2,144,520ms] [Warning] [omni.physx.plugin] The rigid body at /World/turtlebot3_waffle/base_footprint has a possibly invalid inertia tensor of {1.0, 1.0, 1.0} and a negative mass, small sphere approximated inertia was used. Either specify correct values in the mass properties, or add collider(s) to any shape(s) that you wish to automatically compute mass properties for. If you do not want the objects to collide, add colliders regardless then disable the ‘enable collision’ property.
2023-06-05 16:28:43 [2,144,520ms] [Warning] [omni.physx.plugin] The rigid body at /World/turtlebot3_waffle/camera_depth_frame has a possibly invalid inertia tensor of {1.0, 1.0, 1.0} and a negative mass, small sphere approximated inertia was used. Either specify correct values in the mass properties, or add collider(s) to any shape(s) that you wish to automatically compute mass properties for. If you do not want the objects to collide, add colliders regardless then disable the ‘enable collision’ property.
2023-06-05 16:28:43 [2,144,520ms] [Warning] [omni.physx.plugin] The rigid body at /World/turtlebot3_waffle/camera_depth_optical_frame has a possibly invalid inertia tensor of {1.0, 1.0, 1.0} and a negative mass, small sphere approximated inertia was used. Either specify correct values in the mass properties, or add collider(s) to any shape(s) that you wish to automatically compute mass properties for. If you do not want the objects to collide, add colliders regardless then disable the ‘enable collision’ property.
2023-06-05 16:28:43 [2,144,520ms] [Warning] [omni.physx.plugin] The rigid body at /World/turtlebot3_waffle/camera_rgb_frame has a possibly invalid inertia tensor of {1.0, 1.0, 1.0} and a negative mass, small sphere approximated inertia was used. Either specify correct values in the mass properties, or add collider(s) to any shape(s) that you wish to automatically compute mass properties for. If you do not want the objects to collide, add colliders regardless then disable the ‘enable collision’ property.
2023-06-05 16:28:43 [2,144,520ms] [Warning] [omni.physx.plugin] The rigid body at /World/turtlebot3_waffle/camera_rgb_optical_frame has a possibly invalid inertia tensor of {1.0, 1.0, 1.0} and a negative mass, small sphere approximated inertia was used. Either specify correct values in the mass properties, or add collider(s) to any shape(s) that you wish to automatically compute mass properties for. If you do not want the objects to collide, add colliders regardless then disable the ‘enable collision’ property.
2023-06-05 16:28:43 [2,144,520ms] [Warning] [omni.physx.plugin] The rigid body at /World/turtlebot3_waffle/imu_link has a possibly invalid inertia tensor of {1.0, 1.0, 1.0} and a negative mass, small sphere approximated inertia was used. Either specify correct values in the mass properties, or add collider(s) to any shape(s) that you wish to automatically compute mass properties for. If you do not want the objects to collide, add colliders regardless then disable the ‘enable collision’ property.What does it mean?
Here is my asset layers.Hi,
This means that there are no mass properties defined for those rigid bodies.The mass properties are either defined from the collision shapes - using volume to compute the mass properties. If there are no colliders (which is most likely the case here) then there should be mass defined directly or as the warning says an approximation is used.I am not that much familiar with the URDF importer, but it looks like either there are no collisions or mass properties are missing on these bodies.Regards,
AlesPowered by Discourse, best viewed with JavaScript enabled"
613,getting-omniverse-flow-output-annotated,"I am developing a pipeline for generating synthetic smoke data. While using the particle emitter with the graph editor - I can achieve nice results, but I want to use the Omniverse Flow to create more realistic data. The problem - it seems to be that there is no way to get the annotation data for smoke and fire generated from Omniverse Flow. I have been through all the tutorials in Nvidia’s YouTube and site and couldn’t find any tutorial like this. Please help!Hi @guydada. Responding here mostly for others, since we talked about this offline. Currently this isn’t possible, but are looking into workarounds and or what the scope of this is with other omniverse teams. When I have more info I’ll follow up with you directly and in this forum post so its searchable.Hi @guydada, you can do it the same way I did here using zero shot detectors. (Fire Detector Using Omniverse Replicator And Arduino Nicla - Hackster.io). Hope it helps.That is very helpful! thank you for that first of all. The thing is I am looking for a scalable solution for thousands of different scenes, generated heedlessly. I am in contact with Nvidia and hopefully we will find some solution. I will give your code a try anyhow! thanks again.Powered by Discourse, best viewed with JavaScript enabled"
614,pvd-cannt-open-pxd2-file-the-file-is-corrupted,"HiI’am working Unreal Engine 4.26.1 which embedded PhysX 3.4.0. I created PvdFileTransport by PxDefaultPvdFileTransportCreate.However when i test the code on Windows, it saved pxd2 file successfully, but when i open the pxd2 file by Physx Visual Debugger(version 3.2018.04.23896843) , PVD says “the file is corrupted”.The corrupted .pxd2 file has been uploaded in the attachment. Is there anything wrong with my code?Any advice you could give would be much appreciated!Pvd_20230717164302.pxd2 (16.8 MB)I have loaded the attached pxd2 file into both PVD2 and PVD3 and both load the recording without problems.I have attached screens to show what I see.Is it possible that you have another instance of some software having a file lock on the file on your system?As seen in PVD2
pxd2_fine_pvd21920×1041 198 KBAs seen in PVD3
pxd2_fine_pvd31391×920 119 KBSometimes an instance of PVD is kept hanging in the background and has to be terminated manually using for example the Windows Task Manager./PabloThanks for your replay.
I had reboot window system to ensure there isn’t any PVD instance in the background and open the .pxd2 file again, but it still failed with the same message.
My PVD version is 3.2018.04.23896843, which version did you used to load the attached .pxd file?
I found that the graphic driver manager Geforce Experience also installed some physx file, does it related to this pvd file issue？
Is there any pvd log file that can be used to locate this issue？Mine is Version 3.2021.01.29532135
version2084×1378 227 KBYou can get it from here : Gameworks Download Center | NVIDIA Developer
pvd_download3217×1104 129 KBThe new version works.
Thank you very much!!!You’re very welcome :-) Always good to help! Could you close the thread/topic? I have no idea how to do it.Ah say you flagged it as “solved”. Perfect thanks!This topic was automatically closed 14 days after the last reply. New replies are no longer allowed.Powered by Discourse, best viewed with JavaScript enabled"
615,if-i-update-a-release-on-github-for-my-extension-does-it-get-picked-up-automatically,"Just curious if I have to do anything to have the latest version be the one Extensions Manager uses.ThanksPowered by Discourse, best viewed with JavaScript enabled"
616,from-audio2face-to-iclone-8-not-working-properly,"Hi, my name is Rafael, I’ve been trying to animate an avatar from iClone 8 to Audio2Face but I always get the same problem during the process. Every tutorial I try I stuck in the same problem. For example, just after I apply Post Wrap, the head of the avatar is misshapen and out of position, very different from the tutorial videos I’ve been following on youtube (like this one Audio2Face Nvidia Omniverse and iClone8 - YouTube) and after this problem I realize the JSON file won’t work well in iClone. Someone can help me?Welcome to the forums @biaimportsml
Are you able to share your resulting scene file so we can investigate?
problem1920×1852 188 KB

Thanks for responding me.My iClone was recently updated (version 8.23)Explaining a little more about this. First point, when exporting the USD file from iClone 8 using a default iClone avatar, the result generates a file with this name CC3_Base_Plus_A2F.usd and its related materials, so far it looks good.Following the latest tutorials available on Youtube (specially this one attached here previously), everything seems normal, but when you get to Post Wrap things get weird, for example, the teeth are detached from the head. The head is completely out of position, becoming misshapen.When adjusting the head close to the starting position, it is clearly possible to see that the teeth are outside the avatar.After exporting the JSON file and importing it into iClone, the avatar’s mouth has this strange photo format and obviously the animation doesn’t work.It looks like you’re using 2022.1.2 which is a little old. Can you try the latest version which is 2022.2.1?2022.2.1You’ve got the point… let’s try the new version, thanks for the hint.Powered by Discourse, best viewed with JavaScript enabled"
617,collision-detection-after-spawning-prim,"Hello,I want to check if my prims are spawned in another after creation, so I need something to check if they are colliding. I have only found solutions with Nodes and not with python code (Compute Geometry Penetrations — Omniverse Extensions documentation).Is there any python function to check if 2 prims are colliding and giving back a boolean value or a count?I want to generate a new scene if they are colliding.Thank you!Nobody got an answer?Hi @valentinhendrik - In Isaac Sim, you can use the PhysX extension to check for collisions between prims. However, this requires running a physics simulation, which might not be ideal if you just want to check for collisions after spawning prims.Unfortunately, there isn’t a built-in Python function in Isaac Sim that can directly check for collisions between prims without running a physics simulation.One possible workaround is to write a custom function that checks for collisions based on the bounding boxes of the prims. Here’s a simple example:def are_prims_colliding(prim1, prim2):
bbox1 = prim1.GetAttribute(‘primvars:displayColor’)
bbox2 = prim2.GetAttribute(‘primvars:displayColor’)This function assumes that the displayColor attribute of the prims contains their bounding boxes as six-element lists in the format [xmin, ymin, zmin, xmax, ymax, zmax]. You might need to adjust this depending on how the bounding boxes of your prims are defined.Please note that this is a very basic collision check and might not be accurate for complex shapes or rotations. For a more accurate collision check, you would need to use a physics engine like PhysX.Powered by Discourse, best viewed with JavaScript enabled"
618,cannot-install-isaac-sim,"Hello. I got an error while installing Isaac Sim on my computer. After few minutes of downloading and installing, it failed to be installed with an following error message.‘’’
Error occurred during installation of Isaac Sim: Command failed: “/home/minu/.local/share/ov/pkg/isaac_sim-2022.2.1/omni.isaac.sim.post.install.sh” >“/home/minu/.local/share/ov/pkg/isaac_sim-2022.2.1”/omni.isaac.sim.post.install.log
error: subprocess-exited-with-error× Getting requirements to build wheel did not run successfully.
│ exit code: 1
╰─> [1 lines of output]
error in gym setup command: ‘extras_require’ must be a dictionary whose values are strings or lists of strings containing valid project/version requirement specifiers.
[end of output]note: This error originates from a subprocess, and is likely not a problem with pip.
error: subprocess-exited-with-error× Getting requirements to build wheel did not run successfully.
│ exit code: 1
╰─> See above for output.note: This error originates from a subprocess, and is likely not a problem with pip.[notice] A new release of pip is available: 23.1.1 → 23.1.2
[notice] To update, run: /home/minu/.local/share/ov/pkg/isaac_sim-2022.2.1/kit/python/bin/python3 -m pip install --upgrade pip
‘’’I also found that ‘isaac-sim-2022.2.1’ wasn’t installed in  ‘/home/minu/.local/share/ov/pkg’.
Can I get some help? Thanks.Hi @cmw9903  - The error message suggests that there is an issue with the ‘extras_require’ field in the setup.py file of the gym package. This field should be a dictionary where the values are either strings or lists of strings containing valid project/version requirement specifiers.Here are a few things you can try to resolve this issue:Powered by Discourse, best viewed with JavaScript enabled"
619,problem-where-the-lidar-values-are-only-coming-out-as-negatives,"Hello, I’m currently conducting experiments with a husky robot equipped with a LiDAR sensor. My LiDAR model is OS1-64.However, when I attached the LiDAR to the robot and checked the values, they came out negative.When I visualized the LiDAR values, there seemed to be no problem.Could you possibly explain why this issue is occurring?
Lidar_Problem21312×788 119 KB
Those values look like all the horizontal angles that the rays are shooting out in.  The difference between each number is 0.00314158 radians, which is 0.18 degrees and is the correct horizontal_resolution that you set.Maybe you need to tap into a different part of the data?In typical circumstances, how are settings determined and what might other approaches entail?I appreciate your insight and guidance on this matter.You can read more about using Lidar in Isaac Sim in this documentation: Using Sensors: LIDAR — isaacsim latest documentationPowered by Discourse, best viewed with JavaScript enabled"
620,enterprise-size-vs-number-of-people-using-audio2face,"Hi! I was just wanting to further clarify this information about licensing and Audio2Face, based off this thread: Commerical use of Audio2FaceDoes the 2-person free commercial license qualify if the entire team is larger than 2 people, but no more than 1-2 people will be using Audio2Face? Or does the 2 person cutoff refer to the entire team?
The project would use A2F purely for content creation (generating lip sync animation, etc) Not necessarily Cloud or any other deployment options, just content generated by the A2F software itself :)Thanks!@wtelford1 might be the right person to answer thisHey amybuc6,
Audio2face is in beta and currently falls under the individual license terms. I’m not sure I understand the distinction between your two questions though. Currently two users using Audio2face to create content for your entity should be allowed. If you can tell me your use case and industry I can connect you with one of our devrels to clarify the details.Powered by Discourse, best viewed with JavaScript enabled"
621,modelling-a-conveyor-belt-in-isaac-sim,"Hello guys,I intend to build a model of a conveyor belt but I’m not quite sure how to realize it using ISAAC SIM.
I build models in many other robotic simulators which usually have one of these options:What would be a good idea for ISAAC Sim?grafik|690x371I found the following in some PhysX Documentation:
Screenshot from 2021-05-06 14-19-571465×170 26.8 KB
Is there any way to apply this function to a prim in ISAAC Sim?I’m curious if you ever found a solution to creating conveyors?  I will be looking into building conveyors in Isaac Sim in the near future.  If you learned anything helpful, I’d be happy to have any tips or suggestions.Apparently they have created an Extension simulating a conveyor belt now: https://docs.omniverse.nvidia.com/app_isaacsim/app_isaacsim/ext_omni_isaac_conveyor.htmlI have tried it, but it doesn’t seem to work with deformable bodies; they fall straight through just as with the “kinematic surface velocities”. I am looking forward to the day when this also works with deformable bodies.Fantastic!  Thanks so much for sharing.  This is going to be really useful.By chance have you played with it enough to know if curved conveyors can be created?  I ran into this issue when I used to use Unreal Engine.  I knew how to make straight conveyors but curved conveyors were a whole different animal.Wow, I wasnt thinking that asking for a conveyor belt would actually make the developers create something. I am looking forward to test the extension in my simulation model. Thank you for making me aware of its existenceHas anyone heard any updates from NVIDIA regarding curved conveyors?  I heard they were planning to release an update with that functionality in October but I haven’t seen anything yet.Hi - Sorry for the delay in the response. Let us know if you still having this issue with the latest Isaac Sim 2022.2.1 release.Hello! Curved conveyors are available since 2022.2.0 release! Please try it out!I will look into the deformables support for conveyorHi all,I appreciate you added a lot of conveyor support in the current Isaac version. You are now able to also simulated curved conveyor belts (in x-y-plane). However what I’m most interested in is the actual curvature that appears at the end of a conveyor belt (see my initial image). Approximating a conveyor with a moving plane is not sufficient for a realisitc simulation of the actual physics that happens at the conveyor edge. I hope you understand what I mean.Hello! The conveyor does not approximate to a plane, it actually conforms to the geometry of the conveyor. I have some interesting videos of a box stuck in the intersection of two belts like from your image, where the box just spins as the conveyors act on it. I’ll try to find it or replicate with other assetsHey, of course youre right that the velocity is applied to the whole mesh. The problem is, the velocity vector does not change with respect to the curvature of the belt. E.g., on a real belt, the velocity on the bottom of the belt would be in the opposite direction as the top side. Since you have some linear velocity and collision enabled, you will se some kind of movement in the examples you have in mind. However the forces apllied to the transfered objects are diffently in realitysame questionsHi @rgasoto , any update on this item?I’m also working on an application that requires a conveyor to transport a deformable body. Is there a way to configure the deformable body so it can collide with the conveyor?ThanksPowered by Discourse, best viewed with JavaScript enabled"
622,camera-clipping-not-working-on-iray,"Hi Omniverse Team,Recently had a project that I had utilize camera clipping, was working well in RTX realtime and path tracer but when turning render mode to IRAY the camera clipping stop working. Thank you.Thanks for the tip. I will check it out and file a bugPowered by Discourse, best viewed with JavaScript enabled"
623,epic-games-unreal-engine-omniverse-connector-updates,"102.2 4.27 Hotfix Release NotesNote, this hotfix is only for the 4.27 Preview 3 Unreal Editor.  Epic Games recently updated from Preview 2 to Preview 3 and our 102.1 release isn’t compatible, so we updated it.FeaturesBug FixesChanges102.3 Hotfix Release NotesSeparate MDL functions which require MDL 1.7 to a new core template which will be imported as needed. The main Ue4Function template maintains MDL 1.6 to compatible with older Kit SDK versions (OM-35152)This specifically addresses the issue where Machinima 2021.1 shows red materials with Unreal 102.1 and 102.2 exports
image1920×1082 115 KB
Fix a crash by adding error checking for the custom mdl function translatorAvoid repeatedly copying template MDL files when exporting materials102.5 (4.27 only) Hotfix Release Notes102.4 (4.27 only) Hotfix Release Notes102.6 Hotfix Release NotesFeaturesBug FixesChangesAdd support for referencing textures, materials, and USD stages with a checkpoint URL (file.png?checkpoint=2) (OM-34630)

image383×528 20.6 KB
Allow a material to load for a GeomSubset that doesn’t define the “familyName” attribute set to “materialBind” - still emit a warningAdd support for the USD Preview Surface “Transform2d” functionIf USD camera focus distance is zero use the setting from post-process on import (OM-35714)Export EXR files as 16 bit textures since they’re now supported by Create103.1 Update Release Notes103.2 Hotfix Release NotesFeaturesAllow users to export OmniPBR-based materials (OM-38620)Add live-edit support for USD Preview Surface material parameters (OM-39825)
Bug FixesChanges103.3 Hotfix Release Notes104.1 Update Release NotesFeaturesThe Omniverse Connector is released for Unreal Editor versions 4.26, 4.27, and 5.0 Preview 2Layer visibility and locked state is synced between Unreal and Create (OM-41867)Add support for loading MDL templates (OmniPBR and friends) as an instance based on Connector templatesAdd support for loading USDZ files from Nucleus (OM-43305)
image1024×900 65.5 KB
104.2 Hotfix Release NotesFeaturesBug FixesChanges104.3 Hotfix Release NotesBug FixesChanges
image1652×1102 151 KB
105.1 Update Release NotesFeaturesBug FixesChanges200.0 Update Release NotesFeaturesBug FixesChangesDeprecated200.1 Hotfix Release NotesBug FixesChanges200.2 Hotfix Release NotesFeaturesThe Omniverse Connector is released for Unreal Editor versions 4.27, 5.0, and 5.1Support loading thumbnails from Nucleus in the Content Browser for Omniverse USD and MDL (OM-34221)Bug FixesFix sky lighting issues when hiding/unhiding sublayersFix loading a material that has a Scope prim between the Material and Shader prims (OM-63457)Fix MDL errors: “DB element with tag still referenced” during startup (OM-63680)Disable Object related expression to make default 5.x floor material work (OM-66166)Support importing MDL functions with multiple array inputs (OM-67319)Export MDL constant for runtime virtual texture volume (OM-70927)An asset that fails to import should be destroyed (OM-67319)Fix setting wrong data size to image wrapper (OM-67000)Add error output when engine material functions can’t be loaded for MDLFix an issue with deleting Unreal actors after adding USD to an Unreal level. (OM-63202)Fix duplicating the root prim (OM-63738)Fix issues with properly fetching USD file changes with sublayers (OM-63838, OM-65069, OM-65783)Initialized UProperty for Omniverse Class and Struct to avoid build warningFix loading USD properly in Unreal Level (OM-63201)Fix crash when opening import/export dialog in the editor without viewportFix scaling mesh down to 0 resetting transforms (OM-65092)Fix exporting an animation sequence with no preview mesh. (OM-65158)Avoid loading the USD when adding a reference in the stage actor. (OM-65080)Fix crash when changing the OmniStageActor to a USD instead of OmniverseUSD and saving Unreal Level (OM-65667)Fix exporting level models (brushes) with multiple materials (OM-67443)Fix an issue where live sessions were stopped when leaving Play-In-Editor mode (OM-68945)Fix warning “SkeletalMesh has no skeleton. This needs to fixed before an animation can be set”Fix copying Omniverse URL from servers with mangled names (OM-71214)Check if a live session exist before creating (OM-71211)Fix an issue where a USD export fails to bind the OmniverseCameraShot as the default camera for CreateFix crash browsing a folder with a name like “_1” (OM-72684)Fix crash caused by invalid import package name (name.skel.usd) (OM-72722)Fix Unreal Text Render exporting (OM-72642)Fix warning: replace the deprecated DoesPackageExist()Fix warning: include the correct “Engine/Engine.h”ChangesUpdate some icons for the Content Browser and Omniverse Layers windowsRefactor the layer editor to fix bugs and improve perf (OM-63154, OM-63157)Properly loading a reference will load all related layers (OM-66300)Changing max influences for joint to 4 on importDon’t export normals from MeshDescription when they’re all (0, 0, 0)Use primvar displayColor and displayOpacity attributes to export meshes with vertex color (rather than texture coords) (OM-43093)Update USD resolver to 1.15, Omni client to 2.18, and add resolver version to About dialogUpdate the Core MDL Library to 0f84615b (and refresh all core MDLs when they’re updated) (OM-67680)Change the USD export prim kind hierarchy (OM-65886)Use mangle name for the asset name with unreal unsupported character (OM-72952)Update Omniverse USD Resolver Plugin to 1.11, Omniverse Client Library to 2.15201.0 Beta Update Notes
(The Orc now blinks due to blendshape importing)FeaturesBug FixesChanges201.1 Beta Hotfix Release NotesBug Fixes202.0 Beta Update Notes202.1 Hotfix Release NotesRelease Date: July 2023Powered by Discourse, best viewed with JavaScript enabled"
624,by-keyboard-input-how-to-change-from-one-scene-to-another-scene,"Can you guys please tell me how to change scene from one to another using keyboard input through action graphHi. I think I covered this here: How to switch from one environment/stage to another using action graphThis topic was automatically closed 14 days after the last reply. New replies are no longer allowed.Powered by Discourse, best viewed with JavaScript enabled"
625,following-object-problem,"I’m following the tutorial on the following link.
link : I’m following the tutorial on the following link.
There was a problem with the Adding a Follow target task.
please see 7. Modify the COBOTTA_SCRIPTS_PATH/follow_target_example.py script to follow the target using an IK solver.
In the video, There is a cube at the end of the link in the robot arm.
however, i want to put the cube in the middle of end effector.
do you know how to center the cube in the end effector?Hi @hoe8279  - To center the cube in the end effector, you would need to adjust the position of the cube relative to the end effector in the script. This is typically done by modifying the transformation matrix or the position vector that defines the cube’s position.Here’s a general idea of how you might do it:In this line, end_effector.position is the position of the end effector, and offset is a vector that defines how far and in what direction the cube is from the end effector.In this line, the cube’s position is set to be exactly the same as the end effector’s position, effectively centering the cube in the end effector.Please note that the actual code might look different depending on how the script is written.Powered by Discourse, best viewed with JavaScript enabled"
626,black-shadow-on-create-model,"can anyone help-what is this black shadow on my image in create modeler?

blackshadow909×563 86.2 KB
Hello,I’d suggest trying to set the normal map intensity to something very low, like 0.3 or set to 0 as a test.If you post a screen grab of the material parameters, we can look more closely.  Thanks!!Powered by Discourse, best viewed with JavaScript enabled"
627,monitors-turn-off-when-in-use-usd-composer,"Monitors turn off when in use USD Composer
Asus TUF-RTX4090-O24G-GamingVideo card passes all crash testsDrivers were installed differentOnly restarting the system helpshello @Polzovatel3971 -is this a windows or Linux based system?Also, can you do a nvidia-smi command prior to starting USD composer. Then the same command after the monitor turns off. reply with the dataWindowsPowered by Discourse, best viewed with JavaScript enabled"
628,audio2face-api-not-supporting-batch-play-audio,"Thanks for the answer. It’s indeed the Audio Sampling Rate (my wave file is in 22.05Khz)
I have another issue.
I am sending in a long wave track (50secs) via Headless Audio2Face API → Stream to Unreal.
After some intervals, the face & the audio out of sync.
I am thinking of chunking the long wave track into smaller pieces (by sentence wave) and then batch it to
audio2face. However, after combing through the API, i cannot find another start_play the root folder in batch mode. There’s only solve ExportBlendshapes API in batch mode. (which will batch the face weights to Unreal, But no audio stream)
Would there be a start_play API in batch mode? If not, what could be the possible solution in this use case?Thanks for bringing this to our attention. We’re investigating this interally and will update you soon.Powered by Discourse, best viewed with JavaScript enabled"
629,is-it-possible-to-run-audio2face-in-docker,"I wonder if Audio2Face can run in container. I have tried it but failed as I got some errors failed to load python modules or .so modules.Hello and welcome to the forum @gcsg1346
Our engineering team is running Audio2Face on OVC (Omniverse Cloud) which are docker container. Can you share the errors?Thanks for your reply. Here is the log when I run a2f in docker.
kit_20230725_052736.log (235.3 KB)
There are some errors that says “failed to load native plugin” or “ImportError: libpxOsd.so: cannot open shared object file” and something like that.
And here is my dockerfile if it helps.Any idea? Or maybe you can show me how to run A2F in container.Here’s a video to get you started with NVIDIA Docker Setup A2F NVIDIA Docker Setup - YouTubePowered by Discourse, best viewed with JavaScript enabled"
630,visualizing-the-output-folder-from-basic-writer,"I am working with visualizing the output folder and I was a bit confused where can I get the rgb_path in the figure ?

image1253×361 28.4 KB
rgb_path is defined earlier in the examples:Following this example PATH_TO_REPLICATOR_OUTPUT_DIR is defined above in the code as:output_dir=""out_dir_test_visualization""By default, non-absolute paths write files to ~/omni.replicator_out.  So for this example the files should be located in ~/omni.replicator_out/out_dir_test_visualization/Thank you very much, Dennish.
I am trying to visualize coordinates of objects. Can you suggest some ways to visualize them.For the RGB images, they are just PNG so a web-browser or built-in image viewer works.For other annotations, you can use numpy/matplotlibOr our own Replicator InsightPowered by Discourse, best viewed with JavaScript enabled"
631,streaming-audio-player-do-not-stream-sound-to-unreal-engine-via-omniverse-live-link-plugin,"Setup as follow:Kindly advised if the current version do not support Streaming Audio Player → Unreal Engine?I have the same question and wonder if there is a solutionCould you please try turning on Enable Audio Stream?Screenshot_171537×1224 104 KBI managed to solve it.
It’s the Player Setting in the StreamLiveLink instance.
need to match the Streaming Audio Player
“/World/audio2face/PlayerStreaming”Powered by Discourse, best viewed with JavaScript enabled"
632,isaac-sim-very-slow-compared-to-mujoco-or-pybullet-both-physics-and-rendering,"On 2.8GHz CPU and 3090 GPU, simulating the same quadruped robot,Isaac Sim takes 20x more time for physics or rendering compared to Mujoco or PyBullet.
With 20 quadrupeds, time taken is roughly 10x that of one quadruped for Isaac Sim, while Mujoco time increases linearly.  At this rate, there probably needs to be 1000s of robots for Isaac to catch up with the speed of Mujoco.Is this result expected?  Or did we not properly tune/use Isaac?Thanks,
LeI’m also curious what would be a good approach to optimize rendering for complex scenes to get real time performancesAny updates regarding this issue?Hi - Someone from our team will review this and respond back.Have there been any new updates @rthaker ?Powered by Discourse, best viewed with JavaScript enabled"
633,transferring-eye-tongue-and-jaw-animations-to-blendshape-model,"Hi there. Is it possible to transfer the eye,tongue and jaw animations to a blendshape model? So far i have only had success transferring the face. when i include the others i get an error. it seems that when i use blendshape generation to create a mesh it only exports the face…
Thanks.We deal with 3 different types of animation when using Audio2Face:Face mesh: vertices move around, so we can generate a blendShape using this.Eyes and Teeth: vertices do not move, only their tranform values (translate, rotate) changes. So we don’t really need blendShape for them.Tongue: the most complex…, both transform and vertices are animated. But we can generate blendShape for tongue using BlendShapeSolve, just like the face. You will need to create a few blendShapes for the tongue that covers almost all possible motions, e.g. TongueUp, TongueDown, TongueCurlUp, TongueCurlDown, TongueCurlLeft, TongueCurlRight, TongueStretchOut, TongueShorten, and possibly more if you need more detail. Then you can use the Data Convertion tab and generate blendShape animation weightsHi thanks for the reply. The reson I ask is because I need the teeth to be included in the blendshapes in order to use them in unity. If I cant include the teeth/jaw in the blendshapes, how would i go about exporting the transform data so that they can be used? Many thanks.Jaw and teeth should be relatively easy to setup as there are only 4 jaw movements in arkit, JawForward, JawLeft, JawRight, JawOpen. This means you can create a simple blendShape for your lower teeth in any DCC e.g. Maya. Then connect the face blendShape weights to this new jaw blendShape weights.For tongue, it’s a little different. You can solve blendShapes for it just like you can solve for the face.In conclusion, you will need 3 different blendShapes: face, tongue and lowerTeeth. The first 2 can get created using Audio2Face’s A2F Data Conversion tab. But you will need to create lowerTeeth blendShape node manually.Powered by Discourse, best viewed with JavaScript enabled"
634,skrl-a-modular-reinforcement-learning-library-with-support-for-nvidia-omniverse-isaac-gym,"Dear community,skrl is an open-source modular library for Reinforcement Learning written in Python (using PyTorch) and designed with a focus on readability, simplicity, and transparency of algorithm implementation. In addition to supporting the OpenAI Gym and DeepMind environment interfaces, it allows loading and configuring NVIDIA Isaac Gym and NVIDIA Omniverse Isaac Gym environments, enabling agents’ simultaneous training by scopes (subsets of environments among all available environments), which may or may not share resources, in the same runPlease, visit the documentation for usage details and exampleshttps://skrl.readthedocs.io/en/latest/ The current version 0.6.0 is now available (it is under active development. Bug detection and/or correction, feature requests and everything else are more than welcome: Open a new issue on GitHub! ). Please refresh your browser (Ctrl + Shift + R or Ctrl + F5) if the documentation is not displayed correctlyThis new version has focused on supporting the training and evaluation of reinforcement learning algorithms in NVIDIA Omniverse Isaac GymAddedDear community,skrl version 0.7.0 is now available (it is under active development. Bug detection and/or correction, feature requests and everything else are more than welcome: Open a new issue on GitHub! ). Please refresh your browser (Ctrl + Shift + R or Ctrl + F5) if the documentation is not displayed correctlyAddedNow, with the implementation of the standard scaler (adapted from rl_games), better performance is achieved…
Screenshot from 2022-07-11 23-46-411373×763 92.1 KB
E.g, for the Ant environment[ORANGE]: PPO agent with input preprocessor
[BLUE] PPO agent without input preprocessorsExciting work, thanks！
Can you add some algorithms and environments for multi-agent reinforcement learning?Hi @Mr.FoxThank you for giving the library a try.The idea is to add more algorithms to the library gradually :)Regarding environments, the development is focused on adding algorithms and functionalities to work with the relevant environments and interfaces in the RL field (such as Omniverse Isaac Gym, Isaac Gym, OpenAI Gym, or DeepMind, for example). Creating or providing environments as part of the library is not on my roadmap at the moment.Dear community,skrl version 0.8.0 is now available (it is under active development. Bug detection and/or correction, feature requests, and everything else is more than welcome: Open a new issue on GitHub! ). Please refresh your browser (Ctrl + Shift + R or Ctrl + F5) if the documentation is not displayed correctlyAddedChangedFixedRemovedAs a showcase for the basic Franka Emika real-world example, a simulated version of the environment for Isaac Gym and Omniverse Isaac Gym are provided to support advanced implementations :)Thank you for sharing this information, it is very helpfulDear community,skrl version 0.9.0 is now available (it is under active development. Bug detection and/or correction, feature requests, and everything else is more than welcome: Open a new issue on GitHub! ). Please refresh your browser (Ctrl + Shift + R or Ctrl + F5) if the documentation is not displayed correctlyAddedChangedFixedRemovedAs a showcase, a basic real-world example is provided where a KUKA LBR iiwa robot is controlled through a direct API and via ROS/ROS2. In addition, its simulated version, in Omniverse Isaac Gym, is provided to support advanced implementations :)Dear community,skrl version 0.10.0 is now available (it is under active development. Bug detection and/or correction, feature requests, and everything else is more than welcome: Open a new issue on GitHub! ). Please refresh your browser (Ctrl + Shift + R or Ctrl + F5) if the documentation is not displayed correctlyThis unexpected new version has focused on supporting the training and evaluation of reinforcement learning algorithms in NVIDIA Isaac OrbitAdded:
Screenshot from 2023-01-22 17-14-151100×597 223 KB
Dear developer,
I am currently using the multi-env framework with the skrl algorithm library. However, it’s strange that the performance of skrl is slightly worse than that of stable baselines3. I’m not sure if this is due to my configuration. Additionally, when clip_action=False, the actions outputted by skrl sometimes exceed the action space that I’ve defined. If clip_action=True, the actions will be clipped to the boundary values of -1 or 1. Could you please help me clarify these issues? Thank you very much.Hi @2456496590Can you please open a new discussion in the skrl repository to explore the topic there?
So we leave this post for announcements :)Explore the GitHub Discussions forum for Toni-SM skrl. Discuss code, ask questions & collaborate with the developer community.Dear community,skrl version 1.0.0-rc.1 is now available (it is under active development. Bug detection and/or correction, feature requests, and everything else is more than welcome: Open a new issue on GitHub! ). Please refresh your browser (Ctrl + Shift + R or Ctrl + F5) if the documentation is not displayed correctly.Among the major features of this new version are:Visit the library documentation to start training your favorite task in Isaac Gym preview, Isaac Orbit or Omniverse Isaac Gym using PyTorch or JAX!For questions and others, please open a new discussion in the skrl repository.
That way we leave this topic for announcements :)skrl-docs1459×829 198 KBperformance1543×873 56.3 KB
The execution times used to construct this graph are the complete execution of the script
(including environment loading and, in the case of JAX, the jit compilation of XLA, for example).Powered by Discourse, best viewed with JavaScript enabled"
635,rendering-flow-lens-distortion-and-depth-deep,"I’m wanting to render the natural phenomenon of lens distortion from heat, directly in Omniverse.Is there anyway to do this in Ominverse?  I added a Camera and I dont see where the OV Camera accepts a Material assignment.If I cant render this effect in OV, then I’d need to export it, but, to blend Flow with other CG elements in an external package, I’d need a depth map or, the best would be DEEP rendering - neither of which I could seem to figure out how to get working with Flow volumes and OV.Thanks.Powered by Discourse, best viewed with JavaScript enabled"
636,omniverse-code-dont-launch,"Hey, everyone. I am new to Omniverse and I could not discover a way to get Omniverse Code running back.After being a little time away from the screens, Omniverse code crashed. Then, I closed it and try to open again to go back to my project. After that, Omniverse Code does not even lauch anymore.My settings of my machine are:
Processor: AMD Ryzen 7 7700X 8-Core Processor 4.50 GHz
GPU: RTX 4090
Memory: 32 Gb ram
OS: Windows 10 Pro 21H2Follow attached the latest log of Omniverse Code on my machine.
kit_20230724_192459.log (133.7 KB)Can anybody help me or is someone having the same problem?Powered by Discourse, best viewed with JavaScript enabled"
637,using-python-virtual-environment-with-conda,"Using python virtual environment with conda？Are you experiencing an issue with Isaac ROS or Isaac Sim? It sounds like your question may be better suited for a Python- or Anaconda-specific forum. This forum exists for support with NVIDIA Isaac robotics products.I use “conda env create -f environment.yml；conda activate isaac-sim” created the virtual env.cd /home/lin/.local/share/ov/pkg/isaac_sim-2022.2.1new standalone_examples/test/arm.py file, the code as follow:I run python standalone_examples/test/arm.py ,but error as follow?from omni.isaac.examples.base_sample import BaseSample
ModuleNotFoundError: No module named ‘omni’ModuleNotFoundError: No module named ‘omni.isaac.core’Is there a shortcut to import Python Path and run it?I hope the official can provide a detailed case, and if it is convenient, a video case can be provided so that consumers can better enjoy isaac, rather than wasting time on environmental configuration. Thanks very much.There is no simple way to use our own conda environments. But I managed to import those packages with my own script. The key idea is to import the paths of the missing libraries one by one with “sys.path.append” command. The list of these paths can be extracted from the configuration of VScode. The list was intended for the syntax checker to ignore some alerts. But I found we can use it to run ISAAC SIM in my own python environment as well.Hi @904798869 - This forum post will be useful to you. Cannot import `omni` related modulesCould you please list the detail path of “sys.path.append”?From my understanding, the main problem with importing isaac python modules is that folders have been named in a psycho way. For example, “omni.isaac.franka” has inside the folders omni->isaac->franka, so the python interpreter, even if pointed to the right folder “exts” where “omni.isaac.franka”  is tries to enter into the folder “omni” (when importing "" . "" means enter in the folder )that does not exist.To solve it, I have created a Python script that scans the folder exts and create symbolic links for every module in a personal folder, and I have added that folder to the PYTHON_PATH variable (along with the other folder added in the “setup_python_env.sh”).
create_symlinks.py (5.4 KB)If anybody has a better solution, please share it with us.P.S. I cannot believe that Nvidia created such a mess on this, I am sincerely surprised that given this structure, Isaac itself is able to load the correct modules…Sorry to reply so late. I didn’t see your question earlier. 
To answer your question, I can’t provide you with my code, because I was using absolute path which contains my personal information. However, you can extract the path list from your_isaac_path/.vscode folder.Powered by Discourse, best viewed with JavaScript enabled"
638,can-i-simulate-a-tracked-robot-in-isaac-sim,"Hello!
Is it possible to simulate a tracked robot in Nvidia Isaac Sim.This Yahboom Jetson Robot is a crawler off-road robot based on ROS(robot operating system) and Python programming. It is equipped with Jetson NANO 4GB, Lidar, HD camera/depth camera, etc., which can realize remote communication, mapping navigation,...
Price: USD 749.99

This is the robot i’m talking about.Thank you.Powered by Discourse, best viewed with JavaScript enabled"
639,omniverse-toolbar-in-sketchup,"I already installed the Trimble SketchUp connector in Omniverse launcher but I don’t have the Omniverse toolbar in the list of toolbars in SketchUp. I would appreciate your input on this. Thanks!Powered by Discourse, best viewed with JavaScript enabled"
640,switching-to-enterprise-trial,"Hello, we decided to give a try to Enterprise version of Omniverse. I’ve been using Omniverse Create so far. Should I re-install the entire package in order to switch to Enterprise?thanks,
KarolI found this guide from 2021. Is it still valid?ENTERPRISE_011226×738 122 KB573.00 KBPowered by Discourse, best viewed with JavaScript enabled"
641,metahuman-eyebrow-eyelashes-not-working,"Hi Team,
I am using A2F 2022.2.1 . I am using Metahuman from Unreal engine to create A2F pipeline . After Connecting the pipeline ,All the Meshes are animating properly. But the Eyebrows and eyelashes are not animating after Prox UI.I am also tried to create a separate Eyebrow ,Eyelashes Mesh prim from DCC tools then import and tried the A2F pipeline. Then also not animating .I have attached the reference video.
Metahuman Error.mkv (14.4 MB)How can I proceed with this. Please do the Needful.Thanks in advance.There are a few steps you need to take to make sure eyebrows and eyelashes are setup properly. Here’s a tutorial video series which shows this in depth.
Camila Asset Pt 1: Overview with Omniverse Audio2Face and Reallusion Character Creator 4 - YouTube
Camila Asset Pt 2: Mesh Preparation with Omniverse Audio2Face and Autodesk Maya - YouTube
Camila Asset Pt 3: Tagging Meshes using Character Setup with Omniverse Audio2Face - YouTube
Camila Asset Pt 4: Character Transfer and Attaching Prims with Omniverse Audio2Face - YouTube
Camila Asset Pt 5: Tech Fix to Force Mesh Read when Audio is Playing in Omniverse Audio2Face - YouTube
Camila Asset Pt 6: Connect Character Setup Meshes to Drive the Full Body in Omniverse Audio2Face - YouTubeHope this help. Please reach out if you have any questions or issues following these tutorials.Thanks . I will going to check this videosPowered by Discourse, best viewed with JavaScript enabled"
642,i-had-this-error-with-cortex-control-package-when-building-the-ros-workspace-for-isaac-and-solved-it,"Hello everyone,I had this yaml-cpp related error when building the ros_workspace of Isaac simIt is a linking error, and as shown in the error message, yaml-cpp should use the -fPIC option.The way I solved it is by installing yaml-cpp from source and used the required flag as followsThis solved the above error with the cortex_control when building the ros_workspace.I hope this will help someone who encounters the same error.Best,Thanks a lot this helped us with build failures in isaac ros packages with latest updates which incorporated yaml cpp.Just the build directions above didn’t work for us, we did it as follows:Powered by Discourse, best viewed with JavaScript enabled"
643,error-in-installing-machinima-3-0,"Hi there, when I try to update Machinima within the launcher, it gives me this message:Error occurred during installation of Machinima Beta: invalid local file header signature: 0x0Hope you can help! thanksHello @jamestabbush!  Could you attach a copy of your launcher logs from here: C:\Users\<USERNAME>\.nvidia-omniverse\logs\launcher?  It will hopefully have more information on the error that is occurring.Hi @WendyGram , sure thing, here it is:
launcher.log (442.4 KB)Thank you @jamestabbush!  I looked through your logs and didn’t see where the issue might be.  I sent it over to the dev team to help us out!Hi there @WendyGram, any update on this? My projects are constantly crashing upon opening in Create, so it would be great to have an up to date Machinima to try them in.If you are trying to install Machinima Beta through the launcher, try downloading the installation file directly from the Machinima website and installing it manually.hi @vscoapk730 , sorry for the delay in replying, I’ve looked around and can’t see any direct link to download Machinima - perhaps you could help me out with a link if you have one? All I can find is links to download the Omniverse launcher.Powered by Discourse, best viewed with JavaScript enabled"
644,mtlx-primvar-reader-geo-property-value-is-supported-in-create,"Hello!We are trying to load one .usd asset exported from Houdini Solaris that contains a basic pighead geometry were the material emission intensity from the Standard Surface is driven by one primvar reader, but unfortunately lookslike in OV Create we can see the shaders correctly and even the primvar, but the emission “animation” doesn’t appear.So, I’m wondering if OV Create supports the mtlx Geometry Property Value node?I attached multiple .usd files below:scenes.rar (192.6 KB)On the version 05, you will find the .usd file using the “USD Primv Var Reader” node from Houdini. There is also a v05 with a MDL material reading the primvar data from the geometry and it is working correctly.And on the v08, the primvar is loaded using the “Geo Property Value” node from Houdini, unfortunately this one loks like isn’t working in OV Create.Probably we are missing something? Because we are still learning how to use OV Create, and for me is a extremely slow curve because I can’t use it on my machine si we are checking multiple things in other machine.Let me know,Thanks!Hello!I’m wondering if someone can check this? Thanks!Hello @cdordelly!  I’ve shared your post with the dev team for further assistance.Thanks @WendyGram !Hello @cdordelly
Sorry for the late reply. RTX Renderer doesn’t support MaterialX currently. It is on the road map to support MaterialX in the future. We will add warning messages to let user know that if there are shaders aren’t supported during exporting USD.Thank you for reporting this issue and please let us know if you have more questions,WayneHi
Now that MaterialX is officially supported ( Create 2022.3.0 Beta - Release Highlights — Omniverse Create documentation ) could you help about reading geompropvalue?I made a simple cube on which I added an int primvar called MAN_MD_body_textureIdx
I have the MaterialX Graph from houdini exported as .mtlx and loaded as Reference, as /MaterialX, then creating associated materials
Omniverse1901×667 181 KB
Whatever the chosen value for MAN_MD_body_textureIdx primvar, I get the green shading (i.e. first value in the switch node), even if I set a default value for the geompropvalue in the mtlx fileNote that if I remove the geompropvalue and use a default integer value on the switch node, it works. So the problem really stands in getting the value from the geompropvalueThis setup works perfectly in Houdini, hence my question, is geompropvalue supported in your MaterialX implementation?
If yes, how should the data be stored in order to be able to be read?Thanks a lot for your help,(sorry for mixing everything in 1 image, but I was not allowed to upload more than 1 image in my post)Powered by Discourse, best viewed with JavaScript enabled"
645,the-timeline-set-auto-update-false-no-longer-works,"As the title says I’m not able to get the set_auto_update() to work again.I triedandwithout success.What happened? this is fundamental since you don’t allow complete rendering with a single call! (i.e. i need to call render 64 times for path tracing and spp 1 totSpp 64)Hi @marc2002 - Can you try the following code and let us know if you are still having issues.Hi @rthaker
Do I need to do something specific to get those functions? because when I try the import I get an error (omni.timeline has no attriute named timeline) and omni.timeline hs no get_timeline_manager. I searched the other source codes and did not find a reference to that function either.Hi @marc2002 - I looked at your code/question again.The set_auto_update() function in the Omniverse Isaac Sim controls whether the timeline should automatically advance when play() is called. If set_auto_update(False) is called, the timeline will not advance automatically, and you will need to manually advance it by calling step_frame() .In your code, you’re calling play() after set_auto_update(False) , which means the timeline will not advance automatically. Then you’re calling update() on the SimulationApp object, but this does not advance the timeline. That’s why the time remains at 0.041 .If you want to manually advance the timeline, you should call step_frame() on the timeline object. Here’s how you can do it:Remember that step_frame() only advances the timeline by one frame. If you want to advance it by multiple frames, you will need to call step_frame() multiple times.Hi @rthaker. Yes, that’s the expected behaviour and what I was expecting to happen when I updated my code.However, when I call update() what happens is that the timeline gets updated anyway and brought forward.My previous process was similar to what you describedHowever, now if I do like this the timeline gets updated and advanced every simulation step. Which is wrong.
I’m on my phone and my working PC is occupied by some training so I can’t send you an explanation video and triple check this now, but can you please check this?
I ended up having to keep trace of how many simulations loop I’m doing and manually set the time with set_current_time().Also, do you have any news for the rendering? When doing multiple renders like that e.g. for path tracing with my code I’m losing cool features like motion vectors (which I found a cool way to get them anyway from the GRADE project) and motion blur. Any plan to get a render call that automatically completely renders the full scene?Hi @marc2002  - It would be great if you can clarify what exactly you are trying to achieve here. Whether you like the step time or not.Also, can you let us know who do you query the time? if you do it inside an update callback, it would return the end time of the frame.Also, regarding the rendering, how are you capturing the sequence in Path Tracing. Are you using the Movie Capture extension?Hi @rthakerIn v2021.2.1 what happened was the followingwhatever was done from this point onward, the timeline wouldn’t move if not withor similar.This means that kit.update(), sc.step(), sc.step(render=False), sc.render(), etc would not move the timeline and therefore the animations. Which is the desired behavior, i.e. the timeline does not auto update.In the latest version what is happening is what I described above.i.e. anything involvign rendering does advance and auto update the timeline.Log Example:Clearly, I can stop the timeline. But that will stop the simulation context, stopping the physics.Let’s hope I was clearer.As for the renderer: I capture the sequences either in RTX or Path Tracing. I use a custom loop that renders the scene using the simulation context since I need to control the simulation to move the robot around, and make changes. So I have my python script that runs a loop and a rendering call.Powered by Discourse, best viewed with JavaScript enabled"
646,nucleus-is-stopped-and-it-does-not-start-runnung,"Hello,
After I installed omniverse-launcher-linux.AppImage, Cashe, and NEUCLEUS, by following the exact documentation, I could not start the NUCLEUS, and each time I press start it goes back to STOPPED…
Nvidia1003×759 125 KB
Moreover, in my ubuntu
I get this error:
import omni.usd
ModuleNotFoundError: No module named ‘omni.usd’But if I try to install it like so pip install omni  I get the following:
Requirement already satisfied: omni in /usr/local/lib/python3.8/dist-packages (0.0.2)
Requirement already satisfied: wcfg>=4 in /usr/local/lib/python3.8/dist-packages (from omni) (6)
Requirement already satisfied: six>=1.8.0 in /usr/lib/python3/dist-packages (from omni) (1.14.0)
Requirement already satisfied: docopt>=0.6.2 in /usr/local/lib/python3.8/dist-packages (from omni) (0.6.2)
Requirement already satisfied: schema>=0.3.1 in /usr/local/lib/python3.8/dist-packages (from omni) (0.7.5)
Requirement already satisfied: WebOb>=1.4 in /usr/local/lib/python3.8/dist-packages (from omni) (1.8.7)
Requirement already satisfied: aiowsgi>=0.3 in /usr/local/lib/python3.8/dist-packages (from omni) (0.8)
Requirement already satisfied: contextlib2>=0.5.5 in /usr/local/lib/python3.8/dist-packages (from schema>=0.3.1->omni) (21.6.0)
Requirement already satisfied: waitress in /usr/local/lib/python3.8/dist-packages (from aiowsgi>=0.3->omni) (2.1.2)…
Any solution for both issues??
Thanks in advance,
EyasHi Eyas,Is this the first time installing Omniverse w/ Nucleus on this machine or did you have a previous version and upgraded recently?  Additionally, what version of Ubuntu are you running?  (20.04, 22.04, etc.)Thank you!Hello,
This is the first time I install Omniverse w/Nucleus on my machine:$lsb_release -a
No LSB modules are available.
Distributor ID:	Ubuntu
Description:	Ubuntu 20.04.6 LTS
Release:	20.04
Codename:	focalPowered by Discourse, best viewed with JavaScript enabled"
647,welcome-to-the-omniverse-developer-forum,"NVIDIA Omniverse™ is a scalable, multi-GPU real-time reference development platform for 3D simulation and design collaboration, and based on Pixar’s Universal Scene Description and NVIDIA RTX™ technology.  Download the Beta here: NVIDIA Omniverse DeveloperPowered by Discourse, best viewed with JavaScript enabled"
648,isaac-sim-inertia-treatment,"Dear Isaac Sim Team,I recently ported a MuJoCo model to Isaac Sim using the available importer. I noticed that the off-diagonal elements were simply ignored, so I added the information about the off-diagonal elements in the USD file via the physics:principalAxes element of the PhysicsMassAPI.Given that I was unsure whether I needed to present the quaternion in (w,x,y,z) or (x,y,z,w) convention, I simply tried both and looked at the simulation results. To my surprise, the results were the same.Intrigued by this observation and thinking that Isaac Sim simply ignores off-diagonal inertia elements, I decided to side-step the physics:principalAxes element and create an xForm element that carries the diagonal inertia information and is rotated to align with the inertia frame. As an example, here is the conversion I did:This led to a difference compared to the previous simulations. However, the simulation was again indifferent to whether I specified the rotation in (w,x,y,z) or (x,y,z,w) convention.For the MuJoCo simulation, using the different quaternions made a noticeable difference in simulation outcome.So I wonder, is Isaac Sim treating the off-diagonal elements at all? And how are nested bodies treated by the simulator?A short update: I realized that my issue may be very well related to this thread.I already confirmed that indeed, Isaac Sim ignores the off-diagonal elements when importing the USD model by calling the “get_body_inertias()” method of the ArticulationView.I will in the following days check whether I can, before starting the simulation, simply set the inertias with the “set_body_inertias()” method. However, I am afraid that this might not work since the Domain Randomization Page of IsaacSim only allows for the randomization of diagonal inertia elements and also only when simulating on the CPU (which I am not doing).I have tried in many ways to make the inertia tensor correctly set in Isaac Sim but without any success.The only way may be waiting for the team to fix this issue in the next release…Hi @AlanSunHR,funnily enough, I may have just fixed the problem (at least in my case).While I can confirm that I was not able to programmatically set the inertia tensors (neither via set_body_coms nor set_body_inertias), I found an issue in my USDA scene description.More precisely, I specified the principal axis of the inertia tensor as followsTurns out that the MassAPI specification wantsi.e. a quatf instead of a quat. Fixing this for all my bodies that use the MassAPI leads to the correct inertia tensors appearing after loading the model.Maybe this also works in your case.Best
PascalHi there, thanks for raising the issue and sharing your solutions. The MJCF importer is not currently taking inertia elements into consideration. We will work on improving this feature for the next release.Hi @pascal.klink ,I tried your method of add principleAxes in the USDA file. It correctly displayed the principle axes in Isaac Sim UI, but when I get the inertia matrix from the api ArticulationView.get_body_inertias (Core [omni.isaac.core] — isaac_sim 2022.2.1-beta.29 documentation), the calculated inertia matrices are still not correct.I don’t know if the inertia matrices can be ignored or not, since we don’t know in the simulation backend, if only the principle axes and diagonal inertia are used.But thanks any way for this workaround! At least we can make sure in the UI that the inertia is correct, and hope the team will fix the inertia issue soon.Powered by Discourse, best viewed with JavaScript enabled"
649,problem-in-connecting-the-ros-bridge-to-a-real-robot-and-sending-the-message,"Hi there,I am trying to synchronize the robot simulation on Isaac Sim and the real robot.
The robot is powered by a Jetson Nano board (ubuntu 18.04 and ROS melodic).I have created the Action-graph as below to control the robot to move forwards and backwards by pressing the keyboard. The simulation on Isaac-Sim works but the message couldn’t reach the ROS in the robot.
The rostopic cmd_vel was on.
image1926×1017 142 KB
I have inserted the ROS_MASTER_URL and ROS_IP to the isaac-sim.sh file.

Screenshot from 2023-05-19 13-16-46978×243 28.8 KB
On the other hand I can send the message to ROS from a custom mobile apps by the ROS bridge.
Could you tell me what might be the possible problems that the robot cannot receive signal from Isaac-Sim? ThanksHi @kahlamHave you also configured the ROS_MASTER_URI and ROS_IP (or ROS_HOSTNAME) in the robot?Btw, I think it is not necessary to modify the Isaac Sim startup script.
As long as there is a roscore running in some other terminal on the same workstation/container it is enough.Hi @toni.smNo I did not configure the ROS_MASTER_URI and ROS_IP on the robot side.
ROS was not installed on the workstation at that time so rosrun was not running.As my workstation OS is ubuntu 22.04 thus I installed ROS2 and tried to connect to robot through  ros1_bridge but somehow, I was not able to launch the ros1_bridge.Should I downgrade the workstation to Ubuntu 20.04 and install ROS1 noetic to connect to ROS1 melodic on the robot or install ROS2 on the robot and use ros2_bridge to receive message from Isaac-sim?Also, do anyone can share some samples of Action graphs that show how to publish message that make robot move?Thanks in advance!Hi @kahlamA possible solution in this scenario could be to run a ROS docker container on the workstation alongside Isaac Sim.Note: It should work, although I haven’t tested it because I don’t have Ubuntu 22.XX installed.Example (change the WORKSTATION_IP by the workstation IP address):Also, configure the ROS_MASTER_URI and ROS_IP in the terminal running Isaac Sim (I retract my previous comment) and in the robot (use the robot IP address for the ROS_IP in the robot computer) as wellHi @toni.smI reinstalled the OS and now using the Ubuntu 20.04.
I can successfully connect to the real robot using Isaac Sim ROS1 bridge by configuring both the ROS_MASTER_URI and ROS_HOSTNAME on both the issac-sim.sh and the robot side.
However, on the robot side I confirmed that it can receive the rostopic sent out from Isaac’s Action Graph, but it failed to receive the msgs by checking the rostopic echo (eg. sending linear velocity in cmd_vel to make the robot move )Here I set to use keyword to increase linear velocity to move the robot (TopicName is odom2)

Screenshot from 2023-06-05 19-50-013286×1396 498 KB
On Robot side I used Rostopic echo but no message is received

Screenshot from 2023-06-05 19-47-051920×1083 166 KB
What would be the problem? the rosbridge or action graph problem?
Thanks!HI @kahlamMake sure you can reach the ROS node by testing the connectivity with the IP or host name that appear when showing a topic or ROS node info:Example:Hi @toni.smYes the connectivity looks fine.
But no data received showing on rostopic echo.

Screenshot from 2023-06-12 13-03-121920×1083 261 KB
Hi @kahlamCan you test the communication in both directions/devices using for example the following commands?Also, can you provide a diagram showing the involved devices and their IP address, the ROS master location, the ROS-related environment variables configured on each device and any other relevant info?Hi @toni.smThe communication between the devices works fine.
After various experiments, I found that the connection of a ROS2 robot and Isaac ros2 bridge work properly.
However, the same setting does not work for the ROS1 robot and Isaac ros1 bridge.Hi @toni.smThe problem is solved by the following setting in the isaac-sim.sh file:
export ROS_MASTER_URI=http://(Robot IP):11311
export ROS_IP= (Host PC IP)Thanks so much!This topic was automatically closed 14 days after the last reply. New replies are no longer allowed.Powered by Discourse, best viewed with JavaScript enabled"
650,export-the-position-of-a-cylinder-as-a-python-variable,"Hi! I am working on building a robot with the fluid emitter at the end-effector. I am using a separate fluid engine called SPlisHSPlasH to generate fluid particles. Here’s the general explanation of my idea.First of all, I would like to know how I can export the end-effector position as a python file.Thanks in advance!Powered by Discourse, best viewed with JavaScript enabled"
651,trouble-importing-onshape-assembly-that-has-subassemblies,"I am trying to import an onshape assembly that has multiple sub-assemblies inside it. It imports the parts but not the assembly and just gets stuck like this
Capture1091×572 82.2 KB
and it doesn’t give any errors or warningsThe importing works if I suppress the sub-assemblies.Hi @isoahojesse  -  When importing an Onshape assembly into Isaac Sim, it’s important to note that the Onshape Importer uses the assembly mates to set up joints in the assembly. For convenience, every mate scans up the assembly tree and stops at the top-most assembly that does not contain some joint mate, and uses it as a reference instead of the mated part.Here are some steps you can take to troubleshoot and solve this issue:Check the Mates in Onshape: Ensure that the mates in your Onshape assembly are set up correctly. The Onshape Importer supports the following mates: Fastened, Revolute, Slider, Cylindrical, Planar, Ball, and Gear. If your assembly uses other types of mates, they may not be imported correctly.Check the Import Settings: In the Onshape Importer preferences, make sure that the “Rig Physics When Importing” option is enabled. This option tells the importer to create physics joints based on the mates in the Onshape assembly.Update the Onshape Document: If the assembly or sub-assemblies are not being imported correctly, you may need to update the Onshape document to ensure it’s compatible with the Onshape Importer. This could involve changing the types of mates used or the structure of the assembly.Hey thank you for the replyI only use fastened mates and have crated an configuration for the assebly where all the mates are suppressed.
I have enables this setting in the preferences
I have updated all the parts and assemblies
even with these things done the import doesn’t work
here is also a log file on trying to import it if this has any help
kit_20230727_064451.log (1.5 MB)Powered by Discourse, best viewed with JavaScript enabled"
652,bboxcache-computation-is-wrong-during-pose-randomization,"Hello!Following Situation:
I am importing usd objects into my simulation and want to change the pose of them using my own implemented OgnPerAxisPose node. In this node I am trying to get the bbox size of the object and do some overlap test like here Frequently Used Python Snippets — isaacsim latest documentation (without line 10, since it is not possible during randomization).The problem starts with the size of the bbox, because the results are not stable.For example:Since I want to change the pose of all objects and avoid overlaps with other objects, this is not really working for me.The following function is used for computation of the bbox:During the very first randomization, the size of the bbox seems to be good:Any recommendations for getting stable and correct results of the bbox size?Thanks in advance!Best Regards,
ChristofPowered by Discourse, best viewed with JavaScript enabled"
653,cannot-use-pointclouds-1-0-5-extension-on-create-xr-2022-2-0-rel-1,"I have used pointclouds extension with Create XR (2022.2.0-rel.1) to show point cloud files.
To do this, I add piontclouds extension (0.0.16) included in Create App (2022.3.3) to create a path to “C:\Users{$username}\AppData\Local\ov\pkg\create-2022.3.3\extscache”.
path_to_previous_extension1802×972 127 KBBut, after updated USD Composer (2023.1.0), Create XR (2022.2.0-rel.1) cannot use new pointclouds extension (1.0.5) included in that new USD Composer.
After I create a path to “C:\Users{$username}\AppData\Local\ov\pkg\create-2023.1.0\extscache” on Create XR, I cannot find pointclouds extension, and cannot launch Create XR again.
cannot_find_extension1802×972 138 KBFor now, I can show pointcloud files with Create XR via installing Create XR again, and using previous pointclouds extension.
But I can’t use new extension.Cloud you have some solution for this issue?Best regards,Powered by Discourse, best viewed with JavaScript enabled"
654,how-to-set-prim-location-in-isaac-sim,"It’s hard to find such simple function in omniverse…I found a way to get the prim and its transform matrix by the following codeHow can I set the location of a prim using python?Hi @joshchiu  - To set the prim location in Isaac Sim using Python, you can use the set_pose function from the omni.isaac.dynamic_control API. Here is an example:This script will move the specified prim to the new location. Make sure to replace “/path/to/your/prim” with the actual path to your prim, and new_location with the desired location. The new_rotation variable can be used to set the rotation of the prim.Hi @rthaker, I tried the script and got the following errorAnd I can’t find any document about TransformAPIHi @joshchiu  - I apologize for the confusion. The TransformAPI is not a part of the pxr.UsdGeom module. Instead, you should use the Gf.Matrix4d() function to create a new transformation matrix. Here’s how you can modify the script:Powered by Discourse, best viewed with JavaScript enabled"
655,how-to-use-real-time-lip-sync-on-unreal-engine-via-audio2face,"Hello, I’m interesting to make real-time lip sync character In Unreal engine. I saw audio2face live lip sync and I want to use that function in Unreal engine.  I found how to make this things, but I only found send animation with already recorded data. Is there anyone how to do it? or is it supported?This is the $64,000 question, and until now I have seen no answer to this question or seen any convincing third-party, independent demonstration of this.See the 8K member Virtual Beings Facebook group for more on this topic.Thanks.Hello @tuna83!  I will need to ask the dev team about this as I did not see a tutorial currently available.I did found a video involving the Audio2Face Live Mic Mode as well as setting up a streaming audio player and included them below!and a streaming audio playerYou can also use the Riva Extension to connect to the Riva TTS serverHi @tuna831 ,
Interesting subject. This feature surface up in internal discussions a few times recently.
Right now, there’s no option to do that, as it’s missing some features on the app side.
But we are working on adding those features now and it’s being tested internally. so hopefully this would be possible on the next release.This is now possible using Audio2Face UE Livelink plugin, available on 2023.1.1.
See for instructions: Audio2Face 2023.1.1 (Open Beta) Released - Apps / Audio2Face - NVIDIA Developer Forumsthank you for your service!Powered by Discourse, best viewed with JavaScript enabled"
656,open-audio2face-project-in-a-web-application,"Hello all,~I´m quite new on omniverse platform  so sorry if I´m making a basic question. I was trying to research how I can open an audiotoface scene in a browser. Ideia is to create a virtual assistant in a web app for customer support. Can I ask kindly to the expert community to give me some guidelines ?Thanks in advance.Pedro MartinsThis can be achieved using ACE Omniverse Avatar Cloud Engine (ACE) | NVIDIA Developer.While it’s still not available to public, you may register for an early access here: NVIDIA Omniverse ACE Early Access Program | NVIDIA DeveloperReally thanks for your support. I was previously reading documentation about ACE and I was trying to explore another possibility. I proposed my startup to have early acess to ACE. Thanks for your support.I’m anxious to see evidence of this new functionality. Short video demos are welcome on the 8K member Virtual Beings Facebook group.Hello all,Being ACE an option, is there other way to run our USD project build up on Audio2Face in a external app our web?Thanks and apologies for my basic questions.You should be able to use Rest API and WebRTC kit extension to stream videos in any web browser.
WebRTC Browser Client — Omniverse Extensions documentation (nvidia.com)Thanks for your feedback. I have a Audio2face project using stream audio. I want to create a local application running the avatar that will have a backend php script send audio to it in real time. On web probably it will be hard to implement if we have multiple user accessing it with different streaming input.
Thanks in advance your recommendations.Hi Nvidia.I submitted some months ago a request for early access to ACE but my request is still under evaluation. Is there any contact that we can use to know exactly the status ?Thank you.Pedro MartinsSorry for the inconvenience. But as ACE is relatively new, the access is gated, we only take a select number of accounts that match our criteria. But we have requested to send a notification email to those not accepted.This topic was automatically closed 14 days after the last reply. New replies are no longer allowed.Powered by Discourse, best viewed with JavaScript enabled"
657,access-to-world-object-or-simulationcontext-in-script-editor,"I am trying to do some prototyping using the Script Editor to locate objects and modify them in the physics loop. Is there an easy way to get access to the World and register a physics callback inside of a Script Editor Script, or do I need to move to a standalone application to get access to those objects?Hi @tshumay  - In the Isaac Sim Script Editor, you can access the World object and register a physics callback using the omni.isaac.core module. Here is an example of how you can do this:In this example, my_callback is a function that will be called at each physics step. The step_size argument is the time step size for the current physics step.Please note that the World object created in this way is a new, empty world. If you want to access the existing world that is currently being simulated in Isaac Sim, you would need to use a standalone application and use the get_world method of the SimulationApp class.Also, please note that the physics callback will only be called when the physics simulation is running. If the physics simulation is not running, the callback will not be called. You can start the physics simulation using the start_simulation method of the World object.Hi @rthaker !
I’m trying to get access to the current world the way you mentioned, but there does not seem to be a get_world as part of the SimulationApp class. Can you point me to the API documentation of that method?Thanks!Apologies. get_world method is not part of the SimulationApp class. It’s actually part of the omni.isaac.dynamic_control API. Here’s how you can use it:In this example, world is a reference to the existing world that is currently being simulated in Isaac Sim.Here is the link of Isaac Sim API doc link:
https://docs.omniverse.nvidia.com/py/isaacsim/source/extensions/omni.isaac.dynamic_control/docs/index.htmlPowered by Discourse, best viewed with JavaScript enabled"
658,moving-a-robots-arm-the-base-moves-along-but-shouldnt,"Hello,implementing a simulation of the RB-Kairos with an ur10 arm I got the problem, that the base moves along when I am moving some joints of the arm via joint state publisher.
The base is movable but when the arm is rotating it should stay at the same place.Does someone have this issue before or can help?
Thanks.implementing a simulation of the RB-Kairos with an ur10 arm I got the problem, that the base moves along when I am moving some joints of the arm via joint state publisher.
The base is movable but when the arm is rotating it should stay at the same place.Hi @ilka.jorissen - It sounds like the base of your robot is not properly fixed in the simulation, causing it to move when the arm moves. Here are a few steps you can take to troubleshoot this issue:Powered by Discourse, best viewed with JavaScript enabled"
659,errors-with-hydra-scene-graph-instancing,"I am using Orbit with the GridCloner while enabling the “hydra scene-graph instancing” and “flatcache” using the following:if sim.get_physics_context().use_gpu_pipeline:
sim.get_physics_context().enable_flatcache(True)
set_carb_setting(sim._settings,“/persistent/omnihydra/useSceneGraphInstancing”, True)I get two sets of warnings while running the simulation from a standalone python script:[Warning] [omni.hydra.scene_delegate.plugin] Calling getBypassRenderSkelMeshProcessing for prim /World/envs/env_0/table/leg_03.proto_leg_id0 that has not been populated[Warning] [omni.usd] Coding Error: in _GetProtoPrim at line 1866 of /buildAgent/work/ca6c508eae419cf8/USD/pxr/usdImaging/usdImaging/instanceAdapter.cpp – Failed verification: ’ r ’ – instancer = /World/envs/env_0/kona_module/bolts/bolt_M5X16_tight, cachePath = /World/envs/env_0/kona_module/bolts/bolt_M5X16_tight[Warning] [omni.usd] Coding Error: in ProcessPropertyChange at line 1313 of /buildAgent/work/ca6c508eae419cf8/USD/pxr/usdImaging/usdImaging/instanceAdapter.cpp – Failed verification: ’ proto.adapter ’ – /World/envs/env_0/kona_module/bolts/bolt_M5X16_tightduring scene loadingand/or[Warning] [omni.hydra.scene_delegate.plugin] InstanceAdapter - cannot find cache item for proto /World/envs/env_0/kona_module/bolts/bolt_M5X16_tight.proto_bolt_M5X10_tight_id0during the execution.Interestingly, the second set of warnings depends on the cloned environment number; some numbers do not create the warning. When these warnings are there, I see undesired effects, such that I cannot set the pose of the prims, etc.What do these warnings mean? How can I prevent them (and their effects)?Note: Isaac Sim 2022.2.1 docker container on Ubuntu 20.04. Nvidia driver Version: 525.60.11. CUDA Version: 12.0.Thanks in advance for the support.Any news on this?Hi @ozhanozenWe have asked the rendering to help answer this issue since it is more related to how that pipeline works.As a quick thing, can you clear your cache and see if that helps?Also can you please share your error log files?@ozhanozen I have been asked to share a repro. Can you make one for us to share internally please?Thanks for your reply @mmittal.We have noticed these warnings only occur for a specific asset and when we access the sub-elements of the asset using the API (e.g., using XformPrim.get_forld_pose to access the bolt_M5X16_tight element of kona_module). This asset is directly imported from Onshape using your Onshape importer. Moreover, the aforementioned undesired effects seem to be related to another problem.What do you mean by repro, a USD file? If so, how can we share one with you?Powered by Discourse, best viewed with JavaScript enabled"
660,bug-in-omni-anim-people-0-1-9,"Hello, I think I found a bug in the omni.anim.people extension v.0.1.9.The extension stopped working for me after updating.The issue seems to come from the following file:
omni.anim.people-0.1.9\omni\anim\people\scripts\commands\idle.pyAfter changing line 22 from
self.set_variable(""Action"", ""None"")
to
self.character.set_variable(""Action"", ""None"")
the issue was fixed.Please fix this in the upcoming version.Hi @jominga  - Someone from our team will review and respond to your question.Thats the correct fix. Idle is the default state regardless, so that character will go to Idle after finishing a command.We’ll add the fix in the next release.Hello,I got some early access to Isaac Sim 2022.2.1 and noticed this bug is not yet fixed.Also I found another bug:In the function get_avoid_angle in /omni.anim.people-0.1.9/omni/anim/people/scripts/navigation_manager.py the variable movement_vector is sometimes 0, which results in a division by 0. I noticed this when increasing the number of agents in the scene to more than 100. This results in a lot of agents staying put when they should move somewhere.I created a simple test scene where you can reproduce the second error. Here you have the usd file and cmd.txt file:cmd.txt (3.7 KB)
Example_omni.anim.people_error.usd (8.9 KB)The error log when you click on play:Powered by Discourse, best viewed with JavaScript enabled"
661,3d-connexion-mouse,"Are 3dconnexion devices supported?Hello enzino99,
Thank you for the post. I will bring your inquiry to the proper channels for review.
Thank you for your patience.Hi
It will depend on how this mouse can be a regular input device, but at best they can be used as a mouse.
if you wanted to use the special feature of the mouse for special actions you will have to write up some extensions to support the API using python maybe with something likePython interface to the 3DConnexion Space Navigatorwhat were you planning to do with the mouse ?
Thansk
damienDear NVIDA Team
The 3D Connexion 3D mouse has become a global standard in machine building industry.
I can’t imagine working in 3D without it.
I use it in all CAD programs as well as in 3DS Max.
You navigate the Spacemous with your left hand and the normal mouse with your right hand.
Using this mouse is an absolute must for any 3D software.
If they can integrate the support, Omniverse will surely gain many additional users.
Attached is a video where you can see how the whole thing looks.You are able to navigate with the 3D Connexion mouse in all 6 degrees of freedom in virtual space.Thank you for your supportMario Rothenbuehler
ROBO STUDIO GmbH
https://en.robostudio.swiss/Hello @3D-Artist!  I have some good news for you.  The development team is working on support for the 3D Connexion Mouse (Internal Ticket OM-35839).  I do not know when it will be released, but I am keeping my eye on the ticket and I will post here when I have more information!Dear Weny
Thank you for your very fast reply!
i will check this topic regularly for the update
have a nice day
/.mario’Any chance you can add Oculus Controller support to this request as well…or create another request?I can already stream OV to a browser and view on the Quest, for recording wrnch skeleton captures, but, would be nice to be able to use the Quest controllers in 3D space and map them to OV UI, for like stop/play and Camera movement.  I’m using a hand held mouse in the interim.My humble VR Studio would appreciate it.Hi @daryl.dunlap.ohio!  I am pretty sure our VR team is working on Oculus Controller support, but just in case, I did add this request to the dev ticket!Any updates on 3d mouse support?Hi @gavin.stevens. We’re currently thinking that this would make a good community extension.Still no options here? Spacemouse is how any serious professional does 3D. Omniverse sticks out like a sore thumb in any pipeline without it.August 13, 2022
I just downloaded omniverse was ready to get things going and noticed my space mouse is not working the way it should.cannot rotate around objects, pan, move up or down. Can only zoom in and out.  I quickly launched Blender to make sure it’s not my space mouse acting up, and it was not.Respectfully, why would you put out all this tremendous marketing, YouTube guides, community engagement…then when we get here you do not provide compatibility with our most used tool?come on now guys… huge disappointment. This is really silly to me.Wendy,Thanks for the feedback.StanFrom: [Wendy Gram via NVIDIA Developer Forums]
Sent: Monday, 15 August 2022 12:33 PM
To: [westan@carib-link.net]
Subject: [NVIDIA Developer Forums] [Omniverse/General Discussion] Use of 3DConnection mouse device in Omniverse
)
[WendyGram] Omniverse Community Manager
August 15Hello [ I have a request in with the development team to support the 3Dconnexion Mouse. I will add this request to the ticket!See this post for a possible work around: [3D connexion mouse - #3 by dfagnou]Another vote for SpaceMouse support.I dont use this but I did see some promotional material for the product and it looked pretty cool tbhif OV supported it I would buy itAny news on 3d mouse support?I second that, please support SpaceMouseNo updates here either, too badNo update on SpaceMouse support? We are just trying out Omniverse Create and having no support for SpaceMouse is a major productivity killer. Once you are used to it, there is no going back.Really thre isn’t a plugin for spacemouse in Omniverse? I’ve one spacemouse and used in every 3D program I’ve had, and I can’t do anything without it…Nvida, are you listening?Powered by Discourse, best viewed with JavaScript enabled"
662,new-nucleus-release-2023-1-0,"Hey Omniverse users! The latest release of Nucleus was released and is now available to download!For Omniverse Workstation users, upgrade your Nucleus version through the Nucleus tab within Launcher and as always, the latest updates for your installed client apps will be shown on the Library tab indicated by a green upgrade bell.For Enterprise Server users, the latest Nucleus package is available within the NVIDIA Licensing Portal. (2023.1.0)Powered by Discourse, best viewed with JavaScript enabled"
663,adding-icons-in-extension,"I’m trying to add svg icon in an extension using the following code:ui.get_custom_glyph_code(“my_icon.svg”)The icon is in the same path as extension.py but it always displays the question mark (?). I tried changing the path and different svg icons but the same issue occurs.Any help would be much appreciated here.ThanksHi @farman.ahmed. Have you tried providing an absolute path?Powered by Discourse, best viewed with JavaScript enabled"
664,python-package-shapely-version-conflict,"I saw that shapely is part of the default python environment in Isaac Sim. It seems with Isaac Sim 2022.2.1 the shapely version went up from 1.8.2 to 2.0.1.I am creating an extension that relies on a python library that will use shapely version 1 only. Do you have any recommendation how to resolve this?It would also be interesting to know which Omniverse extensions actually use shapely.Thanks much
BrunoHi @bruno.vetter - Shapely is not provided by the Omniverse python environment. There is no easy solution but if you pip install two versions of the same package, you will be able to try and import python and it will always import the newer one.Reference doc: linux - Force python to use an older version of module (than what I have installed now) - Stack OverflowThis topic was automatically closed 14 days after the last reply. New replies are no longer allowed.Powered by Discourse, best viewed with JavaScript enabled"
665,physics-in-vr,"Hi,Is it possible to enable physics so I can drop things when I am in VR?I haven’t tested but physics should definitely work in VR.Sounds good. I havent been able to set physics to the objects in the scene. Any tips and trix I can look at?Yep, just add a RB with collider preset on your object. Make sure you’re using a Physics Ground Plane.

image715×564 53.6 KB

Press play (space bar) to start the sim. Hold shift+LMB to drag the physics object around.I understand. But in the XR app you cant apply physics (see attached image).But maybe it is possible to do that in Create, save it and open the scene in XR. I have not tried that.It did not work unfortunately.Confirmed with the team, it’s in the queue for a future release. Sorry for the run-around.hi, is this still not possible to use physics information to be played in XR?Can confirm this works now in 2022.2Powered by Discourse, best viewed with JavaScript enabled"
666,cannot-login-to-nucleus-server,"Hi,
after about 3 weeks of not using Omniverse I cannot login to the Nucleus server, therefore can’t access anything from the Create app.What steps I can do to troubleshoot it?Please it is very urgent as I have to update the existing animation project…thanks in advance!Is this a localhost Nucleus server or a managed one?It is a localhost.Quick screen recording of what’s happening.I created the Nucleus server on my machine, that’s why I always used my IP or localhost in order to login to access it.Thanks in advance,
KarolSo, it looks like you credentials may be incorrect. Based on the video, it looks like you’re actually able to connect, you just can’t authenticate.
You may want to log is as the omniverse user to validate the user you’re attempting to log in as exists.Hi Karol,
just wondering if you solved this problem… I’m having the same error as you report and I don’t understand what’s going on. I’ll appreciate if you send me some steps on how you solved this.
Thanks in advance,
BorisHello All,Same for me ;)Hi guys, just to clarify. I set up a Nucleus server on the local server, not directly on the local machine. My biggest problem is that every time I reinstall Omniverse I cannot connect to that server. What I did last time was:So far, the whole Nucleus server experience is the biggest issue for us. Not the USD Composer itself…Now for example, I changed my computer in studio and it has a new IP number now. I reinstalled the entire Omniverse package again, set the location to the folder of the existing Nucleus server and I am again having problems with login.@karol.osinski  I’m not sure I understand what you mean, that you installed Nucleus server on a local server but not the local machine.When you install Nucleus, you need to install it on whatever system has the storage, we do not support using network storage as the location for Nucleus workstation installation.hi @dlindsey I’ve installed Nucleus on my local machine but chose the shared network folder as its location. It worked for us for one project but we had a lot of issues with it. Mostly login authentication and file paths which were either //localhost or //176.14.0.****(my PC ip number).So basically with free version of Omniverse, the Nucleus server cannot be shared between different users? I thought it is possible for 2 users.

COLLABORATION2520×1599 225 KB
@karol.osinski , you can share your Nucleus server with multiple users, however; the storage used for the Nucleus server must be on the local machine. We do not support utilizing network attached storage for a Nucleus server.Also, keep in mind, to share your Nucleus server, you will need to make sure every user is adding and using the Nucleus server with the external IP address. This way the server name is the same for everyone. Also, it’s highly recommended to use relative paths in your files, as this keeps your content the most portable.Powered by Discourse, best viewed with JavaScript enabled"
667,i-still-see-black-reflections,"Hi, please watch this:
What can be the solution to eliminate those black reflections?@pekka.varis ,Did you already try increasing the the path tracer max bounces and Max specular and transmission bounces ? The other one you can try is to make the objects not to cast shadows. Alternative try use Iray to render and see if the black spots are still there, I don’t think its a material problem as these black spots in reflections always got to do with light bounces.@pekka.varis
Hi Pekka and David:
I suggest using “clear” glass. I found other “glazing/tinted” shaders cause dark (black out) look when render in path trace but is correct if render in Iray. Attached is an image of each shader.  Hope that helps. In clear glass property, it has refraction and Opacity setting. once I diminish opacity, cast shadow is not an issue and I can use path trace setting.FrankGLASS COMPARISON1920×560 147 KB@fsheng21 this is a great visual breakdown and to see the difference between the glass materials and their behaviors, Frank! i am sure archviz folks would love these type of comparisons.the initial image depicts the blobs as fully metallic/reflective objects, so i wonder if he was imagining them turning to glass at some point, which would be able to leverage what you are showing here 👍. will have to ask Pekka himself 😁but, question for you, does the window panes in your example have thickness?@Simplychenable The glazing is an IGU. Two 1/4"" sandwich with 1/2"" air gap.
Another sample to the issue I was having with the glaze shaders.I was experimenting with percentage of shading and amount of interior sun light.Frank
GLASS COMPARISON-21920×1120 201 KBHi @fsheng21 , thanks for the great comparison ! Really useful! Lighting is so much more realistic in Iray, I would even dare to say that I found Iray lighting better than V-Ray, too bad it is like 3x slower haha. :)Powered by Discourse, best viewed with JavaScript enabled"
668,mjcf-conversion-problem,"The mjcf converter does not create Drive for the revolute joint, however, the 3D joint works well.The MJCF (MuJoCo XML) converter in Isaac Sim is designed to convert MJCF files into USD files that can be used in Isaac Sim. However, it might not support all features of MJCF, and there might be some differences in how joints are represented in MJCF and USD.In MJCF, a revolute joint is represented by a <joint> element with type=""hinge"". In USD, a revolute joint is represented by a RevoluteJoint prim.If the MJCF converter does not create a Drive for the revolute joint, it might be because the joint does not have a corresponding <motor> element in the MJCF file. The <motor> element in MJCF is used to specify the properties of the motor that drives the joint, and it is typically converted into a Drive in USD.Here are a few things you can check:This topic was automatically closed 14 days after the last reply. New replies are no longer allowed.Powered by Discourse, best viewed with JavaScript enabled"
669,createjoint-no-bodies-defined-at-body-and-body1-joint-prim,"Hi, I put the robotic arm into the robot asset and click on play. The following question will appear. How should I set it up?
2023-05-18_10-031770×884 390 KB
Hi,
this is most likely because you nested rigid bodies, so the nested rigid bodies were not create. Nesting rigid bodies is not allowed because in USD there is a local transformation stack, so we really dont know the order.To simulate something like this, what you can do is to put both robots on the same level in the USD hierarchy and connect them with a fixed joint.
You can either exclude the joint from the articulation (there is a bool on the joint) - this way two articulations will be created that are connected through a maximum coordinate joint. Or you dont exclude it, in which case one articulation is created for the whole topology.Regards,
AlesHello. Did you fixed this problem??Powered by Discourse, best viewed with JavaScript enabled"
670,new-release,"Hello!I am curious about the date for the new release of the replicator and which features it will (probably) include. Can someone give me an answer to it? Depending on the changes, I will wait for the release and save a lot of time.Thank you!Best,
ChristofNew release should be … imminent, with the update of Code. Don’t hold me to this, but I’d be surprised if a new version of replicator isn’t available next week. We’ll have a features and update list along with that release.Hello, maybe are there any updates regarding this topic. Because recently the documentation has been updated, but the release has not happend yet.  I would be very interested in using the new Replicator Materials and in particular the new Projection Materials. Maybe you can give a rough timeline on when the release is going to happen.Thank you, and I am looking forward to your reply.Kind  regards,
Julian@christof.schuetzenhoefer Its out now. Sorry for the delay from my initial estimate (that will teach me :D). @julian.grimm feel free to reach out to me on the material stuff if you’d like. The replicator materials is my area, and I would welcome any feedback or feature requests (you can also reach out to me on our discord server in the #synthetic-data-generation channel)Is the documentation also updated to the new release?And is it possible to use the new replicator version in isaac-sim?@christof.schuetzenhoefer Tutorial docs are updated, the API doc update will be going up shortly (today, I hear). I don’t believe the latest rep version from code will work in the current version of isaac - I’ll double check tho. My understanding is that the rep versions are dependent on the kit version, and kit in code 2023.1.1 is not the same version as the current Isaac release.Edit: Confirmed. My comments about rep’s dependency being on the kit version is correct.Okay, thank you for the clarification!@pcallender Thank you for coming back at this topic. I will try out the new features and if I have feedback, I will come back at you.Powered by Discourse, best viewed with JavaScript enabled"
671,can-articulation-force-sensor-be-applied-to-manipulator,"I want to get force sensor data attached to finger-tip during simulating manipulator to grip object.
So I attached “Articulation Force Sensor” to Franka Panda usd model in /Isaac/Robots/Franka/franka.usd.
And I change usd_path in “franka.py” to sensor attached model folder like;
usd_path = “/home/yoonjunheon/Isaac_Sim/panda_controller/model/panda_arm_hand_rev.usd”To check force data, I run “Franka Nut and Bolt” example in IsaacSim.
Have you solved? I also have the same problemNot yet…Hi @yoonjh98 - To get the force sensor data from the fingertip of the Franka Panda manipulator, you need to access the sensor data from your script. Here’s a general way to do it:Replace ""/path/to/force_sensor"" with the actual path to the force sensor in the USD hierarchy.This will give you the force and torque being applied at the force sensor’s location, in the sensor’s frame of reference.Thanks for your reply!
I had written a code in Python using the “ArticulationView” to retrieve force-torque (ft) sensor data with _physics_view.get_force_sensor_forces() method. However, even in that script, I was getting force-torque data close to zero even after the manipulator gripped the object, just as described in the previous question. To investigate the issue, I performed a test using a sample scene(bolt and nut) just like previos video.I attached ArticulationForceSensors to “panda_rightfinger” and “panda_leftfinger”. To verify if the sensors were receiving data before gripping the object, I manually dragged the fingers using the mouse to apply external forces. As shown in the video, forces were measured along the dragging axis.However, as seen in the video after gripping the object, the measured force values remained close to zero, contrary to what I expected. It seems that the problem cannot be resolved simply by using the script to retrieve values, as you mentioned in your response.Hello @ rthaker, when I use force_sensor_prim.GetAttribute(“force”).Get(), it return NONE
It seems that it can not get the force with GetAttribute.
image1131×697 118 KBPowered by Discourse, best viewed with JavaScript enabled"
672,modding-isaac-jetbot-code-to-fit-city-environment,"Hello,I have been following the tutorial for training the Jetbot in Isaac Sim using Reinforcement Learning and I am working on modifying the code to fit the NVIDIA Carter V1 robot navigating a city environment. My focus is on the simulation aspect and not on the physical robot that they have setup for the Jetbot. Currently, I have edited the Jetbot class to fit the Carter robot and modified the location of the files in the code. I have completed that step but I am facing a concern.I would like to use my own City USD file in the Enivronment code, but that code (referenced below) is creating an environment from scratch. I don’t want to create a new environment from scratch as I already have my USD file setup along with the robot in Isaac Sim. I was curious to know, is there a way that I can import my USD file into the code with a specific Omni Kit USD command, instead of having to build the environment completely from scratch? I look forward to hearing your advice.This is the tutorial that inspired me for my project: https://developer.nvidia.com/blog/training-your-jetbot-in-isaac-sim/
Github for IsaacSim Code: jetbot/isaacsim_patch at master · hailoclu/jetbot · GitHubis there a way that I can import my USD file into the code with a specific Omni Kit USD commandYou mean like this? Isaac Sim - How to import USD assets into a scene? - #2 by hoanggiangcan you tell me the alternatives to these deprecated library modulesfirstly where is this from jetbot_city.road_map import *  coming fromand these rest if possiblefrom pxr import PhysicsSchema, PhysicsSchemaTools, Semanticsimport atexitfrom omni.isaac.synthetic_utils import OmniKitHelperfrom omni.isaac.synthetic_utils import SyntheticDataHelperfrom jetbot import Jetbotfrom road_environment import Environmentfrom omni.isaac.utils.scripts.nucleus_utils import find_nucleus_serverfrom jetbot_city.road_map import *from jetbot_city.road_map_path_helper import *from jetbot_city.road_map_generator import *from omni.isaac.synthetic_utils import DomainRandomizationimport torch_wrapPowered by Discourse, best viewed with JavaScript enabled"
673,asset-converter-cant-find-shapenet-png-textures,"I’m trying to batch convert obj files in shapenet that is stored locally.
I’ve edited code on top of the conversion sample in Nvidia docs. It mostly seems to work, but I’m getting the following error.[Error] [omni.kit.asset_converter.impl.omni_client_wrapper] Cannot copy from /path/to/ShapeNetCore.v2/02958343/b3ffbbb2e8a5376d4ed9aac513001a31/models/images/alfa-romeo-159-2007_1_.PNG to /path/to/ShapeNetCore.v2/02958343/b3ffbbb2e8a5376d4ed9aac513001a31/models/materials/textures/alfa-romeo-159-2007_1_.PNG, error code: Result.ERROR_NOT_FOUND.I have two questionsWhen I check on assets already converted the model path in error shared above does show up. So does this error mean asset conversion has multiple stages and one of them failed or is it one shot?Is there anyway to skip problem files and try to convert other files? Here’s the async convert function I’m using.
I can share more of the code if necessary.Powered by Discourse, best viewed with JavaScript enabled"
674,issue-in-simplifying-the-model,"in the instruction of “running isaac sim for the first time”, there is a section called "" SIMPLIFYING THE MODEL"". It asked me to drag /Root/chassis/electronics into /Root/chassis/body, but it does not allow me to do so. In the Console of Isaac sim, it outputs “Cannot move/rename ancestral prim /Root/chassis/electronics”.Hope this can give more hints to solve the problem.Before this, when I tried to use STEP importer, after I click “Finish Import”, I can’t click “Save Flattened”. I can only do it after I select “Built in Step Files”.Hi,It seems you can not move/delete, etc. because the entities on your stage are references of external USD models… To modify them, you need to save the model as flattened (Save Flattened) to merge all components into one .usd. Then, open and work on this previously saved and flatten model…
flatten1304×1692 378 KB
Thank you. It works now.Hi Toni,I was trying to import urdf, and regroup it in USD. But , terminal  report   the error “cannot move/rename ancestral prim/world/mera_model/lsr_link”.
I  was trying to re-import mera_urdf by flatting the USD. But there is no “flatten ” button in URDF import window?

flatten696×428 49.2 KB
Hi @liuyuanyuan.shPlease note that when importing a URDF, the imported asset will be added to the current stage (cleared or not) as a reference (orange icon, in the Stage panel).To edit the imported URDF, go the output directory (same as source directory by default) and open the generated .usd file.In any case, you can always save the current stage as flattened using the File > Save Flattened As… menuThank you Toni.  It works now. I got it , I need one more step to flat it but not from import URDF window.Powered by Discourse, best viewed with JavaScript enabled"
675,mirrors-are-not-handled-correctly-for-depth-of-field-simulation,"When I have a camera with limited depth of field that is looking at a mirror at an angle, the sharpness of the image appears to be dictated by the distance of the cameras focal plane to the mirror, rather than the actual optical distance to the object.
I am using ISAAC sim 2022.2.1. This used to work correctly in 2021.2.1. Am I just missing a setting somewhere for this or is it a bug.
See the image for illustration.What I would expect:
Setting the camera focal distance to dist(O,A) + dist(A,B):  blue in focus, rest out of focus
Setting the camera focal distance to dist(O,A) + dist(A,C):  red in focus, rest out of focus
Setting the camera focal distance to dist(O,A) + dist(A,D):  black in focus, rest out of focus
Setting the camera focal distance to dist(O,A):  all out of focusWhat happens:
Setting the camera focal distance to dist(O,A):  all IN focus
All other settings: out of focus.
focalplanes615×612 10.5 KB
Here is another illustration:

image1344×830 57.1 KB
I think what may be happening is that first, the objects are projected onto the mirror without accounting for optical path (for purposes of depth of field simulation), and then the mirror is viewed as an image, leading to different optical paths for the different objects due to the between camera and mirror.Hi Patrick,I’m sorry, the question is not completely clear.  I understand that you are trying to model the RGB camera image, with the focus distance / fStep parameters.Since this scene is relatively difficult to reproduce, it’d be good, if you could attach the sample scene to the question. This should allow us to check the scene (and pass it to the RTX / rendering team).I am getting the correct result without a mirror.
In no_mirror, I have set up a camera with focal distance 300 units at 300 units distance to a line of small cubes. Their front face is in focus, as expected.In mirror, I have set up a mirror at a 45 degree angle 300 units away from the camera. 300 units away from the mirror, I have set up the same line of cubes. With a focal distance of 600 units. all of them should be in focus, however, none are.
When I set the focal distance to 300 units in this scenario, the central cube only is in focus.I hope this helps you to reproduce this bug.
no_mirror.usd (7.4 KB)
mirror.usd (9.4 KB)
Screenshot from 2023-06-14 18-49-281920×847 64.2 KB


Screenshot from 2023-06-14 18-49-581920×847 63.5 KB
Hi @patrickogfThank you very much for prepraring the sample scenes to reproduce.  It was really helpful for us to look into the issue you reported.Please see the attached images, I opened mirror.usd and rendered, one with RTX-Real-Time, the other with RTX-Interactive.As you can see, we can get expected result in RTX-Interactive (Path-Tracing). Would it be possible to use RTX-Interactive as a workaround for you now? Btw, currently it’s not possible to get the same/closer result with RTX-Real-Time.The dev team aware of this issue now, and will look into how we can improve in the future.mirror_RTX-Real-Time1920×1077 55.3 KBmirror_RTX-Interactive1920×1077 53.8 KBPowered by Discourse, best viewed with JavaScript enabled"
676,conversational-ai-healthcare,"As part of my upcoming internship evaluation with Life Singularity, I’m exploring the potential of Omniverse to create immersive patient experiences. I’m especially keen to understand how I can rapidly construct realistic scenes within Omniverse, possibly even with or without needing to create them first in Unity or Unreal engine.A key focus of my project involves creating real-time conversational AI. I am particularly interested in the Omniverse AI Conversation Engine (ACE) capabilities (Omniverse Avatar Cloud Engine ACE | NVIDIA Developer | NVIDIA Developer) as it appears to have potential for creating interactive and immersive healthcare simulations.Furthermore, I would also appreciate any insights you can share about integrating external APIs, such as ChatGPT, within the Omniverse environment. Given its AI conversational abilities, integrating ChatGPT could greatly enhance the user experience and realism of the healthcare simulations I am planning to develop.Powered by Discourse, best viewed with JavaScript enabled"
677,physx-error-supplied-pxgeometry-is-not-valid,"I get the following errors and warnings in my original reinforcement learning environment. But learning seems to be working. What does this error indicate? Also what is the solution?
This error does not occur when launching Isaac sim and loading the USD model.
It happens when I  use python script for learning.Hi @1616ttkj  - The error message “Supplied PxGeometry is not valid” is coming from the PhysX physics engine, which is used by Isaac Sim. This error typically occurs when there is a problem with the geometry of one or more of the collision shapes in your robot model.Here are a few things you can check:Make sure all your collision shapes have valid dimensions. For example, a box shape should have all its dimensions greater than zero, a sphere should have a radius greater than zero, and so on.Check if any of your collision shapes are too small. PhysX can have problems with shapes that are very small relative to other shapes in the scene. If you have any shapes that are much smaller than the others, try increasing their size to see if that fixes the problem.Check if any of your collision shapes are intersecting or overlapping with each other in the initial configuration of the robot. This can cause problems for the physics engine.If you are using mesh shapes for collision, make sure the mesh is valid. It should be a closed, manifold mesh with no self-intersections, degenerate triangles, or other geometric anomalies.If you are using a convex hull shape for collision, make sure the set of points you are using to define the hull is valid. The points should define a convex shape, and there should be at least four of them.If none of the above suggestions help, you might need to simplify your robot model or use a different method to simulate the tactile sensors.It seems that there is a conflict when reading usd, so I think the following is probably the cause.“Check if any of your collision shapes are intersecting or overlapping with each other in the initial configuration of the robot. This can cause problems for the physics engine.”I set different locations when resetting the learning environment as bellow code. However, on the screen of Isaac sim, it seems that all the models are placed in the same place near the origin for a moment when resetting. After that, it will be placed in the position I coded.Powered by Discourse, best viewed with JavaScript enabled"
678,time-to-move-camera-using-replicator-is-increasing-as-it-keeps-beeing-called,"Hi,I’m trying to get the image from the camera which attaches to a robot arm end-effector. The script I referred to uses the Isaac sim extension replicator and rep.create.modify.pose to change the camera’s location and rotation to help the camera track the end-effector. However, the executing time of rep.create.modify.pose is increasing as it is frequently called.This problem can be reproduced through this part of code:And the executing time is increasing: (like linearly)
time diff: 0.057644279091618955
time diff: 0.05855504795908928
time diff: 0.05905811698175967
time diff: 0.058225102024152875
time diff: 0.05821833002846688
time diff: 0.06227774091530591
time diff: 0.0584783450467512
time diff: 0.058484185952693224
time diff: 0.06052612408529967
time diff: 0.05977054499089718
time diff: 0.06175160000566393
time diff: 0.060659682960249484
time diff: 0.06187240604776889
time diff: 0.06093573791440576
time diff: 0.06167182105127722
time diff: 0.06161282095126808
time diff: 0.06197104905731976
time diff: 0.06258698797319084
time diff: 0.06255956797394902
time diff: 0.06283891701605171
time diff: 0.06408600404392928
time diff: 0.06407754798419774
time diff: 0.06379654398187995
time diff: 0.06398163898847997
time diff: 0.0644281420391053
time diff: 0.06441731099039316
time diff: 0.0661116229603067
time diff: 0.06483900803141296
time diff: 0.06525875104125589
time diff: 0.0660596649395302
time diff: 0.06606167904101312
time diff: 0.06671516795177013
time diff: 0.06762831006199121
time diff: 0.0665425059851259
time diff: 0.06702468392904848
time diff: 0.06854787608608603
time diff: 0.06778147700242698
time diff: 0.06848757294937968
time diff: 0.0682692660484463
time diff: 0.06836566491983831
time diff: 0.06882072507869452
time diff: 0.06984280096367002
time diff: 0.06901329394895583
time diff: 0.07001556304749101
time diff: 0.07009836798533797
time diff: 0.0695943069877103
time diff: 0.07127058401238173
time diff: 0.07076247700024396
time diff: 0.07301987602841109
time diff: 0.07155614695511758
time diff: 0.07189489807933569
time diff: 0.07193242793437093
time diff: 0.07240101299248636
time diff: 0.07414933305699378
time diff: 0.07292909699026495
time diff: 0.07310835202224553
time diff: 0.0731286599766463
time diff: 0.07368130097165704
time diff: 0.0738065029727295
time diff: 0.07577913405839354
time diff: 0.0743423729436472
time diff: 0.07493349607102573
time diff: 0.0746059239609167
time diff: 0.07567507994826883
time diff: 0.07404023909475654
time diff: 0.07543211593292654
time diff: 0.07637635699938983
time diff: 0.07596263498999178
time diff: 0.0760634740581736
time diff: 0.07742941996548325
time diff: 0.0775237079942599
time diff: 0.07746655005030334
time diff: 0.07721209595911205
time diff: 0.0783033010084182
time diff: 0.07781548204366118
time diff: 0.0779315069084987
time diff: 0.0785499420017004
time diff: 0.0787601521005854
time diff: 0.08062467095442116
time diff: 0.0791920879855752
time diff: 0.0791494200238958
time diff: 0.08052318100817502
time diff: 0.08036545698996633
time diff: 0.08004597201943398
time diff: 0.0814435079228133
time diff: 0.0815659670624882
time diff: 0.08121515193488449
time diff: 0.08016834000591189
time diff: 0.08103529701475054
time diff: 0.08118490502238274
time diff: 0.08250891603529453
time diff: 0.08279872499406338
time diff: 0.08345465897582471
time diff: 0.08310361497569829
time diff: 0.08350782806519419
time diff: 0.08400787191931158
time diff: 0.08572668605484068
time diff: 0.08446183800697327
time diff: 0.08457691199146211Can you help me solve this problem?Best,
ChayHi @hitzcy2016  - The issue you’re experiencing is likely due to the fact that you’re continuously updating the camera’s position and look_at parameters in a tight loop. This can cause a significant amount of overhead, as each call to rep.create.modify.pose will trigger a re-computation of the camera’s transformation matrix, which can be computationally expensive.One potential solution to this problem is to reduce the frequency at which you’re updating the camera’s pose. Instead of updating the pose on every iteration of the loop, you could update it only when the end-effector’s position has changed by a significant amount. This would reduce the number of calls to rep.create.modify.pose, and should therefore reduce the execution time.Another potential solution is to use a different method to track the end-effector. Instead of manually updating the camera’s pose, you could attach the camera to the end-effector using a parent-child relationship. This would automatically update the camera’s pose whenever the end-effector moves, without the need for explicit calls to rep.create.modify.pose.Here’s an example of how you might implement this:This will make the camera follow the end-effector, maintaining the same relative position and orientation. If you need the camera to have a different relative pose, you can adjust it using rep.create.modify.pose after setting the parent, and the changes will be preserved as the parent moves.Hi @rthaker ,Thanks for your solutions and suggestion. The code I’ve shown above is a simplified example of what I’m trying to do, and more importantly, from my intuition, the increase in time is not normal, which should be a constant.As you said, attaching the camera to a parent is a potential solution. But what I’m trying to implement is a multi-instances reinforcement learning training with visual input so either I need to create as many cameras as the number of instances or I need to move the camera from one position to another. Because from my another post Enable camera in headless model experience memory leak there seems a memory leak if I use the camera in training and the number of cameras is quite limited based on my previous test, around 50 cameras can be successfully created without any error.Do you have any suggestions for my specific problem?Best,
ChayHi there,the following code:will not randomize the camera for the given number of times, it will re-generate the Replicator Graph every iteration with new values on how to randomize the camera once you start your data generation (Replicator->Run).So the Replicator API scripts generate the randomization graphs that will be run during once Replicator is started.Here are some snippets explaining this: Isaac Sim/Robotics Weekly Livestream: Synthetic Data Generation - YouTubeHi ahaidu,The code I showed is just a simple example of the function that I want to achieve. I have successfully used the parent attribute of the replicator camera, attached it to my end-effector, and acquire images using AnnotatorRegistry. However, I’m experiencing a limited number of cameras. 60 cameras are reaching the limits in one environment so I can only have 60 robots in training right now.Is there any way I can increase the number of cameras?Best,
ChayPowered by Discourse, best viewed with JavaScript enabled"
679,could-not-login-the-omniverse-launcher,"Hi, all.
Last night I resized my ubuntu20.04 via GParted tool, and today the Omniverse Launcher could not login anymore.
However, the same count is accessible on the other PC.Here is the related msgs on the terminal:
walker2@walker2:~/appImages$ ./omniverse-launcher-linux.AppImage
10:54:58.953 › Omniverse Launcher 1.8.11 (production)
10:54:58.966 › Argv: /tmp/.mount_omniveaV52lW/omniverse-launcher
10:54:58.966 › Crash dumps directory: /home/walker2/.config/omniverse-launcher/Crashpad
10:54:59.197 › Running “/home/walker2/.local/share/ov/pkg/cache-2023.1.0/System Monitor/omni-system-monitor”
10:54:59.208 › Running “/home/walker2/.local/share/ov/pkg/nucleus-workstation-2023.1.0/System Monitor/omni-system-monitor”
10:54:59.216 › Reset current installer.
10:54:59.284 › Running production web server.
10:54:59.296 › HTTP endpoints listening at http://localhost:33480
10:54:59.305 › Sharing: false
10:54:59.411 › Started the Navigator web server on 127.0.0.1:34080.
10:55:00.371 › Saving omniverse-launcher.desktop file to /tmp/omniverse-launcher-FFgFLW…
10:55:00.372 ›
[Desktop Entry]
Name=omniverse-launcher
Exec=“/home/walker2/appImages/omniverse-launcher-linux.AppImage”  %u
Type=Application
Terminal=false
MimeType=x-scheme-handler/omniverse-launcher
10:55:00.567 › Saving omniverse.desktop file to /tmp/omniverse-launcher-Y0p5lV…
10:55:00.567 ›
[Desktop Entry]
Name=omniverse-launcher
Exec=“/home/walker2/appImages/omniverse-launcher-linux.AppImage”  %u
Type=Application
Terminal=false
MimeType=x-scheme-handler/omniverse
10:55:00.748 › (node:10735) UnhandledPromiseRejectionWarning: Error: ENOENT: no such file or directory, unlink ‘/home/walker2/.config/autostart/nvidia-omniverse-launcher.desktop’
(Use omniverse-launcher --trace-warnings ... to show where the warning was created)
10:55:00.749 › (node:10735) UnhandledPromiseRejectionWarning: Unhandled promise rejection. This error originated either by throwing inside of an async function without a catch block, or by rejecting a promise which was not handled with .catch(). To terminate the node process on unhandled promise rejection, use the CLI flag --unhandled-rejections=strict (see Command-line API | Node.js v20.5.0 Documentation). (rejection id: 1)
10:55:11.817 › Login URL opened.
10:55:11.823 › Waiting for login result…
10:56:16.621 › WEB /login-result?ui_locales=en&code=4LLpBhIb8lFCuZinakHh0kq7d6f0HrkRJo7Xt5mvDCecwVjHxmLetzdKEKu0xWK4gwfrZvJjpTk0TJUjD8jYaw
10:56:18.260 › (node:10735) UnhandledPromiseRejectionWarning: Error: ENOENT: no such file or directory, unlink ‘/home/walker2/.config/autostart/nvidia-omniverse-launcher.desktop’
10:56:18.260 › (node:10735) UnhandledPromiseRejectionWarning: Unhandled promise rejection. This error originated either by throwing inside of an async function without a catch block, or by rejecting a promise which was not handled with .catch(). To terminate the node process on unhandled promise rejection, use the CLI flag --unhandled-rejections=strict (see Command-line API | Node.js v20.5.0 Documentation). (rejection id: 2)
10:56:46.638 › Login cancelled.
10:56:59.569 › Error happened during the update:  Error: net::ERR_INTERNET_DISCONNECTED
at SimpleURLLoaderWrapper. (node:electron/js2c/browser_init:2:49549)
at SimpleURLLoaderWrapper.emit (node:events:513:28)
10:57:19.511 › Login URL opened.
10:57:19.517 › Waiting for login result…
10:57:30.454 › WEB /login-result?ui_locales=en&code=CpsQkBs5oEiE0KS4NZxcOtZ89B31KKxtGYQ2mGAK-SCrAr2u3B07yvz6L28fHvK3f3UaSw61-4xycSbxSuyo4Q
10:57:32.048 › (node:10735) UnhandledPromiseRejectionWarning: Error: ENOENT: no such file or directory, unlink ‘/home/walker2/.config/autostart/nvidia-omniverse-launcher.desktop’
10:57:32.048 › (node:10735) UnhandledPromiseRejectionWarning: Unhandled promise rejection. This error originated either by throwing inside of an async function without a catch block, or by rejecting a promise which was not handled with .catch(). To terminate the node process on unhandled promise rejection, use the CLI flag --unhandled-rejections=strict (see Command-line API | Node.js v20.5.0 Documentation). (rejection id: 3)
10:58:07.902 › Login cancelled.
11:00:10.978 › Login URL opened.
11:00:10.984 › Waiting for login result…
11:00:20.562 › WEB /login-result?ui_locales=en&code=zy1q0dl8aVnrUuh1WwaXci42CXOfoUm7HujTWX_82ZwNqgS0kKGrQP8TbuQFNNpKlGKRfEKUQFUaMIGFBi2s2Q
11:00:22.079 › (node:10735) UnhandledPromiseRejectionWarning: Error: ENOENT: no such file or directory, unlink ‘/home/walker2/.config/autostart/nvidia-omniverse-launcher.desktop’
11:00:22.080 › (node:10735) UnhandledPromiseRejectionWarning: Unhandled promise rejection. This error originated either by throwing inside of an async function without a catch block, or by rejecting a promise which was not handled with .catch(). To terminate the node process on unhandled promise rejection, use the CLI flag --unhandled-rejections=strict (see Command-line API | Node.js v20.5.0 Documentation). (rejection id: 4)
11:01:59.940 › Error happened during the update:  Error: net::ERR_INTERNET_DISCONNECTED
at SimpleURLLoaderWrapper. (node:electron/js2c/browser_init:2:49549)
at SimpleURLLoaderWrapper.emit (node:events:513:28)
11:02:59.945 › Error happened during the update:  Error: net::ERR_INTERNET_DISCONNECTED
at SimpleURLLoaderWrapper. (node:electron/js2c/browser_init:2:49549)
at SimpleURLLoaderWrapper.emit (node:events:513:28)
11:03:59.950 › Error happened during the update:  Error: net::ERR_INTERNET_DISCONNECTED
at SimpleURLLoaderWrapper. (node:electron/js2c/browser_init:2:49549)
at SimpleURLLoaderWrapper.emit (node:events:513:28)
11:04:38.783 › Failed to get GDPR, retrying in 2.5 seconds…
11:04:41.289 › Failed to get GDPR, retrying in 2.5 seconds…
11:05:06.722 › WEB /login-result?ui_locales=en&code=CpsQkBs5oEiE0KS4NZxcOtZ89B31KKxtGYQ2mGAK-SCrAr2u3B07yvz6L28fHvK3f3UaSw61-4xycSbxSuyo4Q
11:05:07.513 › WEB /login-result?ui_locales=en&code=CpsQkBs5oEiE0KS4NZxcOtZ89B31KKxtGYQ2mGAK-SCrAr2u3B07yvz6L28fHvK3f3UaSw61-4xycSbxSuyo4Q
11:05:08.312 › (node:10735) UnhandledPromiseRejectionWarning: Error: ENOENT: no such file or directory, unlink ‘/home/walker2/.config/autostart/nvidia-omniverse-launcher.desktop’
11:05:08.313 › (node:10735) UnhandledPromiseRejectionWarning: Unhandled promise rejection. This error originated either by throwing inside of an async function without a catch block, or by rejecting a promise which was not handled with .catch(). To terminate the node process on unhandled promise rejection, use the CLI flag --unhandled-rejections=strict (see Command-line API | Node.js v20.5.0 Documentation). (rejection id: 5)
11:05:08.927 › (node:10735) UnhandledPromiseRejectionWarning: Error: ENOENT: no such file or directory, unlink ‘/home/walker2/.config/autostart/nvidia-omniverse-launcher.desktop’
11:05:08.927 › (node:10735) UnhandledPromiseRejectionWarning: Unhandled promise rejection. This error originated either by throwing inside of an async function without a catch block, or by rejecting a promise which was not handled with .catch(). To terminate the node process on unhandled promise rejection, use the CLI flag --unhandled-rejections=strict (see Command-line API | Node.js v20.5.0 Documentation). (rejection id: 6)
11:05:26.618 › Login cancelled.
11:05:32.687 › Login URL opened.
11:05:32.692 › Waiting for login result…
11:05:44.768 › WEB /login-result?ui_locales=en&code=maCpMJwl19lktOKolqZA-5XrStPTopEpJNi3I2ggIGyPyS-okJp4YmIw9rSCqVqG7K2Z_qiqWzkEF_-JEZSTqg
11:05:46.409 › (node:10735) UnhandledPromiseRejectionWarning: Error: ENOENT: no such file or directory, unlink ‘/home/walker2/.config/autostart/nvidia-omniverse-launcher.desktop’
11:05:46.410 › (node:10735) UnhandledPromiseRejectionWarning: Unhandled promise rejection. This error originated either by throwing inside of an async function without a catch block, or by rejecting a promise which was not handled with .catch(). To terminate the node process on unhandled promise rejection, use the CLI flag --unhandled-rejections=strict (see Command-line API | Node.js v20.5.0 Documentation). (rejection id: 7)
11:08:00.248 › Error happened during the update:  Error: net::ERR_INTERNET_DISCONNECTED
at SimpleURLLoaderWrapper. (node:electron/js2c/browser_init:2:49549)
at SimpleURLLoaderWrapper.emit (node:events:513:28)
11:08:48.845 › Login cancelled.
11:08:50.185 › Login URL opened.
11:08:50.190 › Waiting for login result…
11:09:12.807 › WEB /login-result?ui_locales=en&code=a9b-juAxePAe5qz8x67A0HPP5io-UZt-y3UysQ7STokMmXdcVdQDqGQ56rGYD7pe4Dk1bE5nsh3GxqZQ7ZwtsA
11:09:14.371 › (node:10735) UnhandledPromiseRejectionWarning: Error: ENOENT: no such file or directory, unlink ‘/home/walker2/.config/autostart/nvidia-omniverse-launcher.desktop’
11:09:14.371 › (node:10735) UnhandledPromiseRejectionWarning: Unhandled promise rejection. This error originated either by throwing inside of an async function without a catch block, or by rejecting a promise which was not handled with .catch(). To terminate the node process on unhandled promise rejection, use the CLI flag --unhandled-rejections=strict (see Command-line API | Node.js v20.5.0 Documentation). (rejection id: 8)
11:09:28.701 › Login cancelled.
11:10:16.581 › Login URL opened.
11:10:16.587 › Waiting for login result…
11:11:56.147 › WEB /login-result?ui_locales=en&code=ca2-AmKVYl18wELcmDwPl9azDOss4eC3XbFXg0xh9LsZcFwRCnyDEVxFUx2gCNKKD8M92q-oUsUypd_EUCNvTQ
11:11:57.614 › (node:10735) UnhandledPromiseRejectionWarning: Error: ENOENT: no such file or directory, unlink ‘/home/walker2/.config/autostart/nvidia-omniverse-launcher.desktop’
11:11:57.615 › (node:10735) UnhandledPromiseRejectionWarning: Unhandled promise rejection. This error originated either by throwing inside of an async function without a catch block, or by rejecting a promise which was not handled with .catch(). To terminate the node process on unhandled promise rejection, use the CLI flag --unhandled-rejections=strict (see Command-line API | Node.js v20.5.0 Documentation). (rejection id: 9)And the launcher keeps turning around all the day with no sign of going any futher.
2023-07-26 11-22-26 的屏幕截图1250×700 64.1 KBCan you help me with the problem?I am also experiencing same issue. Launcher keeps rotating all the time with no signing in. Can some one please help on this!There were others that had luck copying the nvidia-omniverse-launcher.desktop file to the designated folder location (Failed to login(stuck after logging in web browser) - #6 by michele.angelini2). @SuxiangZ997 @shanker4red can you guys try it and see if it can address the issue?Powered by Discourse, best viewed with JavaScript enabled"
680,python-scripting-error,"When I make changes to the script I get these errors from the console
2023-06-20 09:58:18  [Error] [omni.kit.scripting.scripts.utils] Python Scripting Error:
2023-06-20 09:58:18  [Error] [omni.kit.scripting.scripts.utils] Traceback (most recent call last):
2023-06-20 09:58:18  [Error] [omni.kit.scripting.scripts.utils]   File “C:\create_docs_physx\new_script_1.py”, line 6, in on_init
2023-06-20 09:58:18  [Error] [omni.kit.scripting.scripts.utils]     print(f""{class.name}.on_init(paul_ja)->{self.prim_path}“)
2023-06-20 09:58:18  [Error] [omni.kit.scripting.scripts.utils]   File “c:\users\paul stefen\appdata\local\ov\pkg\create-2022.3.3\kit\exts\omni.kit.debug.python\debugpy_vendored\pydevd_pydevd_bundle\pydevd_io.py”, line 40, in write
2023-06-20 09:58:18  [Error] [omni.kit.scripting.scripts.utils]     r.write(s)
2023-06-20 09:58:18  [Error] [omni.kit.scripting.scripts.utils]   File “c:/users/paul stefen/appdata/local/ov/pkg/create-2022.3.3/kit/kernel/py\omni\kit\app_impl_init_.py”, line 137, in write
2023-06-20 09:58:18  [Error] [omni.kit.scripting.scripts.utils]     sys.stdout.write(msg)
2023-06-20 09:58:18  [Error] [omni.kit.scripting.scripts.utils] OSError: [Errno 22] Invalid argument
2023-06-20 09:58:18  [Error] [omni.kit.scripting.scripts.utils] Python Scripting Error:
2023-06-20 09:58:18  [Error] [omni.kit.scripting.scripts.utils] Traceback (most recent call last):
2023-06-20 09:58:18  [Error] [omni.kit.scripting.scripts.utils]   File “C:\create_docs_physx\new_script_1.py”, line 6, in on_init
2023-06-20 09:58:18  [Error] [omni.kit.scripting.scripts.utils]     print(f”{class.name}.on_init(paul_ja)->{self.prim_path}“)
2023-06-20 09:58:18  [Error] [omni.kit.scripting.scripts.utils]   File “c:\users\paul stefen\appdata\local\ov\pkg\create-2022.3.3\kit\exts\omni.kit.debug.python\debugpy_vendored\pydevd_pydevd_bundle\pydevd_io.py”, line 40, in write
2023-06-20 09:58:18  [Error] [omni.kit.scripting.scripts.utils]     r.write(s)
2023-06-20 09:58:18  [Error] [omni.kit.scripting.scripts.utils]   File “c:/users/paul stefen/appdata/local/ov/pkg/create-2022.3.3/kit/kernel/py\omni\kit\app_impl_init_.py”, line 137, in write
2023-06-20 09:58:18  [Error] [omni.kit.scripting.scripts.utils]     sys.stdout.write(msg)
2023-06-20 09:58:18  [Error] [omni.kit.scripting.scripts.utils] OSError: [Errno 22] Invalid argument
2023-06-20 09:58:18  [Error] [omni.kit.scripting.scripts.utils] Python Scripting Error:
2023-06-20 09:58:18  [Error] [omni.kit.scripting.scripts.utils] Traceback (most recent call last):
2023-06-20 09:58:18  [Error] [omni.kit.scripting.scripts.utils]   File “C:\create_docs_physx\new_script_1.py”, line 6, in on_init
2023-06-20 09:58:18  [Error] [omni.kit.scripting.scripts.utils]     print(f”{class.name}.on_init(paul_ja)->{self.prim_path}"")
2023-06-20 09:58:18  [Error] [omni.kit.scripting.scripts.utils]   File “c:\users\paul stefen\appdata\local\ov\pkg\create-2022.3.3\kit\exts\omni.kit.debug.python\debugpy_vendored\pydevd_pydevd_bundle\pydevd_io.py”, line 40, in write
2023-06-20 09:58:18  [Error] [omni.kit.scripting.scripts.utils]     r.write(s)
2023-06-20 09:58:18  [Error] [omni.kit.scripting.scripts.utils]   File “c:/users/paul stefen/appdata/local/ov/pkg/create-2022.3.3/kit/kernel/py\omni\kit\app_impl_init_.py”, line 137, in write
2023-06-20 09:58:18  [Error] [omni.kit.scripting.scripts.utils]     sys.stdout.write(msg)
2023-06-20 09:58:18  [Error] [omni.kit.scripting.scripts.utils] OSError: [Errno 22] Invalid argumentplease help me with this issue.
thank youHi @f20201269. This is a known issue and the team is working on a fix. For now, please use carb.log_info() insteawd of print() to work around the issue.This topic was automatically closed 14 days after the last reply. New replies are no longer allowed.Powered by Discourse, best viewed with JavaScript enabled"
681,what-nvidia-educational-resources-do-i-start-with-for-the-purpose-needed-as-described,"Hi,I submitted the following question to NVIDIA customer service but they directed me to post in the forums here. Would someone please point me in the right educational resources? Here the question asked below.""I have no tech background whatsoever. I recently watched a video on YouTube about the capabilities of NVIDIA Omniverse and NVIDIA Ace. My industry is marketing and advertising; and I do not know any programing languages. However, I would like to learn how to utilize NVIDIA Omniverse etc., for integration into my professional role. I’m not looking to become a computer programmer. My intent to use it would be just to integrate usage of the technology into a pre-existing role. I looked on your education pages and there are a lot of courses. I have no idea where to start or what to focus on. Would you please guide me as to what courses I should learn from NVIDIA so I can utilize the functionalities of NVIDIA Omniverse in my day-to-day job, without having to become a computer programmer? I would like to understand the capabilities of the platform and how to use it to generate conceptual briefs for clients, whether they are surreal or hyper-realistic. Thanks. ""Great question and thanks for your interest in Omniverse! As you have realized, NVIDIA Omniverse is a development platform so it is meant to be used as a foundation to build upon custom apps and tools. ACE provides all the tools necessary to create an AI-powered digital avatar but it’s not turnkey software.This said we do have a host of service integrators that can be contracted to do custom development and build the custom app you are looking for. To investigate this avenue, you can fill out the “try it free” form for Omniverse Enterprise and someone will be in touch to explore options. Omniverse Enterprise Trial | NVIDIAPowered by Discourse, best viewed with JavaScript enabled"
682,how-to-solve-graph-composer-startup-errors,"When I start the graph composer, there are many errors. How can I handle them?The following is my environment configuration:
deepstream-app version 6.2.0
DeepStreamSDK 6.2.0
CUDA Driver Version: 12.0
CUDA Runtime Version: 11.8
TensorRT Version: 8.5
cuDNN Version: 8.7
libNVWarp360 Version: 2.0.1d3@77438441 it may not correlate to the errors you are seeing, but which Kit version are you on?Powered by Discourse, best viewed with JavaScript enabled"
683,use-omniverse-api-to-add-camera-in-scene-got-rotation-problem,"i’m using the code below to create a camera, i set up the position and rotation tags, and then i find that a Camera_Xform is created, and a Camera instance is under it. For this step there’s no problem, and the position is only set to Camera_Xform, this also has no problem, but!! the rotation is set up to both Camera_Xform and Camera instance, which means that the camera is rotated twice, why and how to solve this problem?Hi @baikaixin. I am not able to reproduce that. When I run that in Code-2022.3 or Code-2022.2, I don’t see any Transform properties on the Camera prim. Which app are you using?Hi @mati-nvidia . I’m currently using isaac sim 2022.2.0. With the code “camera = rep.create.camera(position=(0.0, 0.0, 2), rotation=(0, 0, 0))”, we could find that the camera instance is created and the translation and rotation of the Camera_Xform is defined as what we expected. But if you go into the Camera instance under the Camera_Xform, you could easily find that the rotation is 90,0,90, which may lead to a wrong rotation, because the created camera has rotated twice.image413×514 23.1 KB
Ah, I see. I’ve moved this to the SDG forum where hopefully someone from the Replicator team can comment about that API.I tried to change the rotation of the camera to (0,0,0) using Pixel’s API or isaac sim replicator API, but I failed. I found that when I combined the camera creation code and the code to change the rotation using the prim path in a single Python file, the modification failed because the camera couldn’t be found through the prim path. However, if I split my code into two separate Python files - one for creating the camera and another for finding the camera through the prim path and modifying the rotation - then it works. However, I don’t want to split my code into multiple files and run them separately. Is there any way for me to immediately modify the incorrect rotation to (0,0,0) after creating the new camera? p.s. even if i set a sleep code between the creation and modification, it didn’t work.Hello,I believe this might be due to your world-up being different in Isaac Sim than the default Omniverse/USD UpAxis.When the UpAxis is different, Replicator will apply a 90 degree rotation so that the camera appears correct when viewed (camera frame up being world-up)If you want to reset the rotation to be truly (0,0,0) then you could use this example code as a start:But that might give you undesirable visual resultsThanks for your reply! But it doesn’t work.
with the code you offered as below, only the Xform’s rotation is changed, as i mentioned above, the problem is to change the rotation the the camera instance under the Camera_Xform tree.Powered by Discourse, best viewed with JavaScript enabled"
684,how-to-add-a-new-environment-in-isaac-sim,"Hi,
I want to create an ocean environment in Isaac sim. But I don’t know how to start. Do you have any relevant documents or other materials for reference?Hi @l3y1q2  - Creating an ocean environment in Isaac Sim can be a complex task, as it involves creating water physics, lighting, and other environmental factors. Unfortunately, as of now, Isaac Sim does not natively support fluid dynamics which are crucial for simulating an ocean environment.Thank you for your reply. Please remind me if this support is added in subsequent versions.
my email: 12231060@mail.sustech.edu.cnThis topic was automatically closed 14 days after the last reply. New replies are no longer allowed.Powered by Discourse, best viewed with JavaScript enabled"
685,usd-composer-2023-1-0-cannot-be-installed,"There is no file named “C:\Users\hogehoge\AppData\Local\ov\pkg\create-2023.1.0\pull_kit_sdk.bat”Can K-Kitta,
We are aware of some install issues. I cannot see and understand what you have posted above with all of the symbols. Can you post a clean log file from the Launcher, and show us the exact error. Are you running on a) a vpn, b) behind a firewall c) on an enterprise machine ?Can you run a cmd prompt and then type in “ping bootstrap.packman.nvidia.com”
Let me know if you connect. You should see 0% loss if you can. Let me know what your Average speed isApproximate round trip times in milli-seconds:
Minimum = 6ms, Maximum = 7ms, Average = 6msOne more thing… can you run a cmd prompt and run this file
C:\Users\hogehoge\AppData\Local\ov\pkg\create-2023.1.0\pull_kit_sdk.batThen look for errors in that and post back.Make sure you can access and download these files
https://bootstrap.packman.nvidia.com/python@3.7.13-nv1-windows-x86_64.cab
https://bootstrap.packman.nvidia.com/packman-common@6.55.1.zip
https://bootstrap.packman.nvidia.com/7za%4022.01-1.zip— bootstrap.packman.nvidia.com ping statistics —
378 packets transmitted, 378 received, 0% packet loss, time 377548ms
rtt min/avg/max/mdev = 7.209/9.109/42.214/4.141 ms
(base) root@kitta:/mnt/c/WINDOWS/system32#The directory named “create-2023.1.0” does not exist.
Only “packman-common@6.55.1.zip” was available for download. The other two files could not be downloaded.c) on an enterprise machineI think I missed the “https” off two of the links which is a problem. You can try these again if you want.https://bootstrap.packman.nvidia.com/python@3.7.13-nv1-windows-x86_64.cab
https://bootstrap.packman.nvidia.com/packman-common@6.55.1.zip
https://bootstrap.packman.nvidia.com/7za%4022.01-1.zipI was able to download all three links specified.Ok great, so now go to ‘c:/users/USER/appdata/local/ov’ and then delete the “packman-repo” folder and try the install againI got the same error as the first and could not install.Ok could you zip up and send us this folder please
C:\Users<USERNAME>.nvidia-omniverse\logs\launcherlauncherThere is no folder named launcher there. launcher.log is there. is launcher.log ok?
launcher.log (489.0 KB)Yes, send me all of those log files. From Launcher.log down to the bottom. Just zip them up together. ThanksUSD Composer 2023.11 BetaI tried USD Composer 2023.11 Beta and was able to install it. Thanks!Great !!This topic was automatically closed 14 days after the last reply. New replies are no longer allowed.Powered by Discourse, best viewed with JavaScript enabled"
686,about-object-api,"Hi, I am creating an environment where two robots grasp the ends of a long object and move the object together.To this end, I need to know which part of the object is in contact and how much force is being applied ti that particular area. May I know if we have this kind of ObjectAPI?Powered by Discourse, best viewed with JavaScript enabled"
687,error-starting-isaac-sim-from-docker,"Hi all,I have some problems while running Isaac Sim via docker and Websocket. I followed all the tutorials, and I managed to access the broadcast of Isaac’s GUI. However, while the GUI opens, in my terminal, I see this error:Does anyone know what does this error mean? How can I solve it?Best,
FrancescoHi. Please share your full log. Some questions:Thank you for the quick answer! Here I provide what you asked:Complete log fileBrowser
I am using Google ChromeDevice
I am running docker from a Mac connected via ssh to a Linux workstation which is in the same local network.WebRTC
WebRTC does not workThanks for the additional info.The log looks like you are running WebSocket mode. Please see Livestream Clients — Omniverse Robotics documentationTry the openh264 flag for websocket too. Try WebRTC too if possible.
Also share the error on the browser if it does not connect. Try a different browser is Chrome does not work.Powered by Discourse, best viewed with JavaScript enabled"
688,modelling-joint-motor-couplings-via-tendons,"Hello everyone,I have a question related to the modeling of two coupled motors in IsaacSim.In MuJoCo, I can model two coupled motors (let’s call the m0 and m1) that drive two joints (let’s call them j0 and j1) via two tendons, each of which is connected to the two motors and one of the joints.I am, however, having trouble creating such a setup in IsaacSim, since there seems to be a requirement that Tendons can only link joints in a tree structure with each other.However, in my setup, the motors would actually be decoupled from the robot joints (i.e., they would be in different subtrees). I already read here that this may only be a limitation of the USD parsing API and not of IsaacSim itself. Unfortunately, the URDF importer fails on my MuJoCo XML file (although I can simulate it with MuJoCo without any problems) such that I cannot “reverse” engineer a solution in that way (if one exists)So I wonder whether it is possible to specify said coupling, e.g., via the Python API.Best Regards
PascalHi Pascal,If you have joint drives on both j0 and j1 I would suggest mirroring the drive targets instead of using tendons.Alternatively, you could use a gear joint if the two target joints are revolute: Rigid-Body Simulation — Omniverse Extensions documentationPhilippPowered by Discourse, best viewed with JavaScript enabled"
689,consultation-needed,"Dear NVIDIA Community,I’m currently working on developing an Extended Reality (XR) solution that integrates conversational AI (similar to inworld/convai, but with some additional enterprise-specific requirements). I believe NVIDIA’s advanced solutions might offer the functionality and resources needed to accomplish this goal, however, I’m seeking expert insights to better understand the possibilities.I’m particularly interested in understanding whether NVIDIA’s technologies would support the seamless integration of conversational AI into an XR environment (HL2, Oculus Quest 2, desktop) and would appreciate if someone could guide me on the following:I am also open to having a detailed discussion to plan out the solution and would appreciate connecting with an expert in this field. Any guidance or direction towards available resources or individuals specializing in AI and XR integrations would be highly appreciated.Thank you for your time and consideration. I look forward to your valuable responses.Powered by Discourse, best viewed with JavaScript enabled"
690,how-to-create-a-points,"Hi. this is my code to generate points asset.But I want to create points using GUI. Are there related documents?thanks!Hi @PARKBONG - As per my knowledge, we don’t have documentation for it but you can create an extension for that functionality if you like. Here’s the doc for that: Getting Started with Extensions — kit-manual 105.0.1 documentationPowered by Discourse, best viewed with JavaScript enabled"
691,strong-differences-behavior-cpu-gpu-gear-joints,"Hello,I’m reporting here DRAMATIC differences which I have found whlie working with simualtion and gear joints, betweeb gpu and cpu simulation. Basically I work with a robot with 8 gear joints (with gear ratio -1)  and a loop joint also.I have tried to just send a sinusoid as targets to the gear joints and look at the average difference between the positions of each pair of joints. I have tried several setups:1/ without the gear option and sending opposite targets to each joints normally constrained by the gear ‘notgear’2 / 3/ 4/ 5 : with the gear option is activated i tried 4 settingsI then tried both in gpu and cpu, which is 10 settings, for which I report the error in real positions measured in simulation. Anything > 0.02 reports strong gear errors._32_16_notgear_10_0.2_cpu_dggFalse_sctTrue	0.026357614
_32_16_gear_10_0.2_cpu_dggTrue_sctTrue	    0.0055296985 –
_32_16_gear_10_0.2_cpu_dggTrue_sctFalse	0.0055296985 –
_32_16_gear_10_0.2_cpu_dggFalse_sctTrue	0.014545715
_32_16_gear_10_0.2_cpu_dggFalse_sctFalse	0.04246392_32_16_notgear_10_0.2_gpu_dggFalse_sctTrue	0.024247162
_32_16_gear_10_0.2_gpu_dggTrue_sctTrue	    0.081808455
_32_16_gear_10_0.2_gpu_dggTrue_sctFalse	0.081808455
_32_16_gear_10_0.2_gpu_dggFalse_sctTrue	0.021585925  –
_32_16_gear_10_0.2_gpu_dggFalse_sctFalse	0.06230142What we oberseve is that in CPU, the “best” setting is to disable the gains of one of the two joints, and it doesn’t matter whereas you send opposite targets or 0 then. This seem like the most logical case, where the gear constraint actually does its jobs quite well.In GPU however, this setting has MASSIVE ERRORS (0.08 is visually very big), however the best is to have BOTH joints in a gear constraint having gains and opposite targets sent to them.The solver iterations values are 32/16Conclusion : I do suspect there’s a BIG implementation issue of the Gear constraint in the GPU solver.RegardsHi @rochmollero ,That does sound like an issue we should have a look into.Would you be able to share a USD, plus ideally a Python script that we could use to reproduce the issue?Thank you,
PhilippPowered by Discourse, best viewed with JavaScript enabled"
692,glass-and-water-materials-can-i-stack-materials-on-top-of-each-other,"I have nice experiment with Glass and Water materials here, applied on alembic file import:I like that dirt feature in glass material!
Can I have these materials on top of each other?
Or even more efficient idea, how can I add this “dirt effect only” to my water material?PekkaHey Pekka. Have you tried playing around with the dirt node in the Material Graph Editor? You might be able to mix that in interactively.Powered by Discourse, best viewed with JavaScript enabled"
693,failed-to-download-audio2face,"hi, I can’t download Audio2Face .
pls help me…
RTX 3060ERROR
Error occurred during installation of Audio2Face: [2c6f0a44-2f64-46bc-8a68-3255a3d1ccd0] Cannot sign URL for download – FetchError: request to https://api.launcher.omniverse.nvidia.com/app/url?path=https%253A%252F%252Flauncher-prod.s3.us-east-2.amazonaws.com%252Faudio2face%252F2022.2.1%252Fwindows-x86_64%252F58a7f0d5273eb2952b7d3cfcf5e9746c.zip failed, reason: read ECONNRESETHi @max3347 and welcome to the forums!I can’t download the zip file you have in your post. Are you able to upload the Launcher log file which can be found here: C:\Users\(UserName)\.nvidia-omniverse\logs\launcher.log?I apologize for the delayed response. The previous issue has been resolved by continuously retrying the connection until it succeeded, which took up most of the afternoon.
Now, I’m diligently trying to install the Maya legacy Connector using this method. Wish me luck!
This time, I wanted to ask if there is a separate download link for the Maya legacy Connector？Currently, I am unable to install it and have been trying for a whole dayLauncher is the only official way to get the Connectors.
Are you able to share the Launcher log file which should be located here: C:\Users\(UserName)\.nvidia-omniverse\logs\launcher.loglauncher.log (2.2 MB)…yeah~,im done,thx!!!This topic was automatically closed 14 days after the last reply. New replies are no longer allowed.Powered by Discourse, best viewed with JavaScript enabled"
694,navmmesh-issue-with-warehouse-usd,"Hi,I am trying to make a custom industrial environment using warehouse.usd as a base. However, there is an issue when i create the Navmesh for this. The navmesh appears to be in a very weird structure. Picture has been attached. I have tried other sample warehouse structures and it works fine with exactly the same parameters.

navmesh.PNG1920×728 115 KB
Hi @a.imrantw - This document might be helpful to you.
https://docs.omniverse.nvidia.com/prod_extensions/prod_extensions/ext_navigation-mesh.htmlPowered by Discourse, best viewed with JavaScript enabled"
695,maya-connector-native-viewport-material-preview-best-way-to-export-materials,"Hi all,
after endless trials and errors trying to get the Legacy connector to work properly with Maya 2022/2023/2024 I gave up on it and Installed the newest version of the Native connector on Maya 2023.I see that there is no more possibility to assign the MDL materials.  Is there any work around for that?
So far what I did was to assign the Lambert material instead and instert a basic color texture etc. After opening the .usd file inside USD Composer I see the USD Preview material assigned with no textures whatsover and in order to assign Omni surface I select all the bounding objects with that material and assign the OmniSurface for example.This workflow is much more time consuming then when I worked with the Legacy connector. Am I missing something?Thanks,
KarolWe stops the development in Maya Legacy Connector. We are now making Maya Native to replace it. Maya Native 208 will support MDL.
Sorry for the trouble.
BTW, Maya Legacy has conflict with Maya USD plugin installed with Maya automatically. You have to rename the MayaUSD folder to make Maya fail to find it to avoid the conflict. Even if you do not load MayaUSD plugin, it still conflicts with Maya Connector because Autodesk sets the MayaUSD path to DLL searching path by default. So, you have to rename MayaUSD folder to make it fail to find it to avoid the conflict.Hi @juma
Thanks for your reply. I am looking forward to the 208 release then! ;)
I completely uninstalled Maya USD in order to make the Legacy connector to work, but I had to stick to the older version like 103 because the newer ones weren’t deteced by Maya.Powered by Discourse, best viewed with JavaScript enabled"
696,create-install-stuck-on-omni-create-warmup-bat,"I’ve installed a connector, cache, etc. but I’m having trouble installing ‘Create’. It seems to sit there on 'Create 2022.3.3 Installing OMNI.CREATE.WARMUP. If I reboot it then just spends another 20 mins downloading everything and trying again. Any tips/thoughts?
Quadro RTX 4000 laptopFYI I ran Omniverse Launch ‘as administrator’ and it installedI am glad that using Administrator to install worked well. That is a great idea.This topic was automatically closed 14 days after the last reply. New replies are no longer allowed.Powered by Discourse, best viewed with JavaScript enabled"
697,omniverse-blog-posts,"Industrial leader Siemens is accelerating development of defect detection models with 3D synthetic data generation from NVIDIA Omniverse, the latest manufacturing gains to emerge from an extended partnership for the industrial metaverse that aims to...
Est. reading time: 4 minutes
U.K.-based Rafi Nizam creates animated show ‘ArtSquad’ using Omniverse Create XR and content-creation apps from Adobe, Autodesk, Epic Games and more.
Est. reading time: 5 minutes
Yizhou Zhao, a doctoral candidate at UCLA, wins the #ExtendOmniverse contest with an ‘IndoorKit’ for robotics simulation environments.
Est. reading time: 4 minutes
30+ NVIDIA artists across time zones created the fully playable, simulated demo using nearly a dozen content-creation apps, all in just three months.
Est. reading time: 5 minutes
NVIDIA Omniverse ACE makes it easier to build and deploy intelligent virtual assistants and digital humans at scale.
Est. reading time: 4 minutes
Using NVIDIA Omniverse, DSD can develop highly capable perception and incident prevention and management systems to optimally detect and react to irregular situations during day-to-day railway operation.
Est. reading time: 3 minutes
HEAVY.AI’s framework built on NVIDIA Omniverse helps optimize wireless site placements to reduce the cost and complexity of network operations and improve the customer experience.
Est. reading time: 3 minutes
The technical director designs building renovations and more for enterprises, using applications including Trimble SketchUp, Autodesk Revit, PlantFactory and Omniverse Create.
Est. reading time: 5 minutes
Vanessa Rosa imbues traditional ceramics with an Omniverse-animated, sci-fi twist using Audio2Face AI, Blender software and more.
Est. reading time: 4 minutes
The NVIDIA Inception startup's Omniverse Extension uses mobile scanning to instantly create customizable, realistic avatars.
Est. reading time: 4 minutes
NVIDIA AI-enabled DeepSearch and Omniverse Enterprise help ILM rapidly search through massive asset library and create a captivating sky dome.
Est. reading time: 3 minutes
Applied robotics Ph.D. student Antonio Serrano-Muñoz creates an Omniverse Extension to use Robot Operating System software with NVIDIA Isaac Sim.
Est. reading time: 5 minutes
Animator Marko Matosevic is taking jokes from a children’s school dads’ group and breathing them into animated life with NVIDIA Omniverse.
Est. reading time: 3 minutes
NVIDIA Omniverse 3D design collaboration and simulation to help engineers work together on an ambitious effort to turn wind power into green hydrogen fuel.
Est. reading time: 4 minutes
UK-based Brett Danton makes ‘impossible camera moves’ possible for an automotive commercial using Autodesk Maya, Epic Games Unreal Engine and Omniverse Create.
Est. reading time: 4 minutes
NVIDIA Omniverse helps researchers accelerate the design and development of a full-scale fusion powerplant.
Est. reading time: 5 minutes
Toronto-based Omniverse user creates virtual representation of the ‘Big Apple’ with Autodesk 3ds Max, Maxon Redshift, Epic Games Unreal Engine and Omniverse Create.
Est. reading time: 4 minutes
More than 20 artists collaborate to cook up a stunning virtual restaurant using Adobe Substance 3D Painter, Adobe Substance Designer, Autodesk 3ds Max, Blender, Maxon Cinema 4D and ZBrush, SideFX Houdini and Omniverse Create.
Est. reading time: 5 minutes
Powered by Discourse, best viewed with JavaScript enabled"
698,diffrential-controller-speed,"Issue with navigation
I tried using the example on navigation for jetbot on the website but i want to change the speed coz the speed in slowi also tried articulation control but i can’t define a specific path
Can you please helpHello! Welcome to the Isaac Sim forums :D  Can you give me some more details about what you are working on?  are you using the omnigraph node?  or are you calling the controller in python?Powered by Discourse, best viewed with JavaScript enabled"
699,modulenotfounderror-no-module-named-subprocess-isaacsim-2022-2-1-crash-suddenly-appeared,"I have been facing an abrupt crash while opening Isaac Sim 2022.2.1. I am not sure how and why suddenly started. I have attached the console log. I am stuck and not able to figure out how to resolve it. Can anyone please help me fix this.Kit_Error_Console_Log (636.5 KB)Below are the system specifications:Isaac Sim 2022.2.1
Ubuntu - 20.04
CPU - Intel Core i7
Cores - 24
RAM - 32 GB
GeForce RTX 3060-Ti
VRAM - 16GB
Disk - 2TB SSDResolved it by re-installing the Isaac Sim.This topic was automatically closed 14 days after the last reply. New replies are no longer allowed.Powered by Discourse, best viewed with JavaScript enabled"
700,bugreport-on-machinima-2022-3-0-beta-solution,"I have found a bug in Machinima 2022.3.0. There is a problem with provided scipy-1.7.3 library in Audio2Face Player in directory “/opt/omniverse/ov/pkg/deps/a763c9e524972268d84832d5d2e5b4e1/extscache/omni.audio2face.player_deps-104.7.0+104.1.lx64/deps/py”:[Error] [omni.ext._impl.custom_importer] Failed to import python module omni.audio2face.player
Error: /opt/omniverse/ov/pkg/deps/a763c9e524972268d84832d5d2e5b4e1/extscache/omni.audio2face.player_deps-104.7.0+104.1.lx64/deps/py/scipy/li
nalg/_fblas.cpython-37m-x86_64-linux-gnu.so: ELF load command address/offset not properly aligned.I am running Linux version of the application on Ubuntu 20.04 LTS. The problem can be easily fixed by downloading same version of scipy-1.7.3 from pypi python repository and replacing affected scipy subpackage, thus modifying existing installation of Machinima. After replacing scipy with same version but from official source, the crash in terminal no longer happens and the bug is fixed. Your developers should do the same and fix the bug.Powered by Discourse, best viewed with JavaScript enabled"
701,multi-line-editor-widget,"I would like to offer a widget in my extension that lets the user enter multiline text, like some lines of Python code. Is there anything existing that I could use? So far I have only found a single line string field.Thanks for any help
BrunoI found a solution, it is described here:Fields and Sliders — omni.kit.documentation.ui.style 1.0.3 documentationUnder Multiline String FieldThanks
BrunoThis topic was automatically closed 14 days after the last reply. New replies are no longer allowed.Powered by Discourse, best viewed with JavaScript enabled"
702,nvidia-joins-alliance-for-openusd-aousd,"Alliance for OpenUSD1584×396 248 KBToday, NVIDIA announced our participation in the founding of the Alliance for OpenUSD (AOUSD), an open organization dedicated to fostering the interoperability of 3D content through OpenUSD (Universal Scene Description).Along with co-founders Pixar, Adobe, Apple, and Autodesk, the group hopes to standardize the 3D ecosystem to enable developers and content creators to more easily describe, compose, and simulate large-scale 3D projects and build an ever-widening range of 3D-enabled products and services.Don’t miss AOUSD-related activities happening next week:Read the full press release here.Powered by Discourse, best viewed with JavaScript enabled"
703,using-imu-in-rl-learning-training-for-a-walking-biped,"Hi, I have implemented a nice bipedal walking rl program for a custom-built robot on isaacGYmn Preview 3 but now wanted to add an IMU (ideally MPU 6050) into the rl training using omniverse isaac sim.I’ve got my robot up and running in Isaac sim now. I initially tried to add a contact sensor for the feet which wasn’t useful as I couldn’t find a way to implement the sensor into my rl pipeline  across my 512 actors due to no way of making a tensorised view? so went with getting the information from rigid body net contact forces.How do I implement the IMU across all the actors so it can be one of the observations for the rl learning?Screenshot from 2023-07-09 18-27-091920×1080 463 KBHi there, you can check out the RigidContactView APIs (Core [omni.isaac.core] — isaac_sim 2022.2.1-beta.29 documentation) for vectorized APIs on retrieving net contact forces on bodies.Thanks Kelly
I will look at the rigid contact view implementation - is this a new feature of v2022.2? its hard to tell as there doent seem to be version specific documentation? But I solved this already as stated in my original post.
But how do i use the IMU for rl training data in a tensorised form so I can feed it into the observations for training for all my 512 robots? - that was my question
sincerely
SujitHi @sujitvasanth -  To use the IMU for RL training data in a tensorized form, you would need to extract the IMU sensor data from each robot and convert it into a tensor that can be used as input for your RL model. Here’s a general approach on how you can do this:Extract IMU Data: For each robot, you would need to extract the IMU sensor data. In Isaac Sim, you can use the omni.isaac.sensor extension to simulate the IMU sensor and extract the linear acceleration and angular velocity of the robot.Extract IMU Data: For each robot, you would need to extract the IMU sensor data. In Isaac Sim, you can use the omni.isaac.sensor extension to simulate the IMU sensor and extract the linear acceleration and angular velocity of the robot.thanks @rthaker rthaker for the quick responseHi @sujitvasanth  - Apologies. Here is the updated response:Add IMU Sensor to Robots: First, you need to add an IMU sensor to each of your robots. In Isaac Sim, you can simulate the IMU sensor by extracting the linear acceleration and angular velocity of the body from the physics engine and apply local transformation of the sensor to simulate the sensor output via the omni.isaac.sensor extension.Retrieve Sensor Readings : To retrieve readings from the added sensors, you can create an ArticulationView and use the get_force_sensor_forces() API, which will return a buffer of dimension (num_articulations, num_sensors_per_articulation, 6)Tensorize the Data : Once you have the sensor readings, you can convert them into a tensor form suitable for feeding into your RL model. You can use libraries like NumPy or PyTorch for this. For example, if you’re using PyTorchFeed into Observations for Training: Now that you have your sensor readings in tensor form, you can feed them into your RL model as observations for training. The exact way to do this will depend on the specifics of your RL model and training setup.thanks but you have a sensor extension that specifically caters for imu’s Isaac Sensor Extension [omni.isaac.sensor] — isaac_sim 2022.2.1-beta.29 documentation
I have done the approach you have suggested previously in isaacgym  Raw accelerometer, gyroscope, magnetometer - #6 by sujitvasanth)So I have already attached one of these ?IMU sensors to my robot usd asset in omniversw but wanted to know how to tensorise them so can be used in training data…please be specific on how to do this in pytorch?Powered by Discourse, best viewed with JavaScript enabled"
704,lula-target-position-is-in-world-coordinates-but-orientation-is-relative-to-robot-base,"Hello,I experimented with lula kinematics and it appears that the position provided to lula kinematics is in world coordinates, while the orientation is relative to the robot’s base.Here is the code snippet I’ve been using:The robot base is shifted and rotated relative to the origin of the world coordinate system.When I set the target position to: [0, 0, 1.052] and the target orientation to: [0.49999997, 0.49999997, 0.49999997, -0.49999997], the gripper’s world-pose-values after the movement is completed are as follows:Position: [-1.4839604e-04, -6.5854285e-05, 1.0516999e+00]
Orientation: [6.1856705e-04, -1.1874572e-03, 7.0721060e-01, -7.0700163e-01]As you can see, the world position matches as expected, but the orientation does not.To address this issue, I found that subtracting the robot base orientation from the target orientation before sending it to the lula controller results in the correct world-pose-values for the gripper:Position: [-9.6186996e-05, 4.7650421e-05, 1.0518397e+00]
Orientation: [0.49978912, 0.49992058, 0.50019026, -0.50009996]This adjustment resolves the problem, but it is pretty confusing.
This inconsistency also applies to rmpflow.Kind regardsAxelBy the way, I have noticed an issue with the quat_to_euler_angles function from omni.isaac.core.utils.rotations. It appears that this function only works correctly for certain value ranges.During my experiments, I tested different gripper orientations by changing the orientations in 90-degree steps, ensuring that every value is a multiple of 90. The orientations are represented in quaternion format and then converted to euler angles using the quat_to_euler_angles function.For most orientations, the conversion is correct. However, when dealing with certain quaternion values, such as [7.0688874e-01, 1.3515353e-04, 7.0732468e-01, 1.2603216e-04], I get euler angles of [149.0697702, 89.95882564, 149.0692288] (in degrees).In comparison, when using a rotation converter from the internet ( 3D Rotation Converter (andre-gaschler.com)), I obtain the following values: [x: 0.0211482, y: 89.965736, z: 0], which definitely makes more sense.As I am not a mathematician, I am unsure if there are any constraints or limitations in converting quaternions to euler angles, but these are my observations.Kind regardsAxelPowered by Discourse, best viewed with JavaScript enabled"
705,big-shout-to-get-stability-in-isaacsim,"Hi all!
This is a great product and the effort from the developers and from the support team is faboulous.
HOWEVER
We, users, need stability, especially in the python API and in the documentation, and correctness of the simulation and of the expected behavior.Every time you go out with a new version, the documentation pages simply disappear!
Every new version also old pieces of code simply stop to work out of nowhere, without a single deprecation notice or any way to get around the issue. In the latest code for example, viewport legacy disappeared auto_update_timeline does not work anymore, and the RTX lidar seems to be usable only through the replicator core module.
This means that either we users spend days trying to figure out how to update our code (since there is no document “this has been changed with this”, see “step_clock_on_physics”) or simply keep an old and less powerful version of the engine.Please, try to be a bit more careful and helpful for us.
This has been a great product, but losing this much time to update a codebase because we need to figure out that the new system does not work as the old one without any single notice is not useful and will bring away customers!ThanksI believe there is a bug where RTX Lidar may not add correctly from the UI.
For a workaround until a new version can be release,  you can look at the standalone examples for rtx_lidar.  there are ones for ros_bridge, ros2_bridge, and debug_draw.
./python.sh standalone_examples/api/omni.isaac.debug_draw/rtx_lidar.pyIDK about this issue, what I’m sure about (been working with lidars the whole day yesterday) is that many things in the documentation don’t work.Take this as an example:Moreover:@eliabntt94  thank you for the report
We will make a ticket to go over these issuesPowered by Discourse, best viewed with JavaScript enabled"
706,ibis-model-for-rtx-a2000-display-port-1-4,"I am looking for IBIS-AMI model of teh Display Port 1.4 driver on the RTX A2000. Wanted to use the driver model to simulation Signal Integrity of the whole Display Port network.Would really appreciate your helpPowered by Discourse, best viewed with JavaScript enabled"
707,closed-articulation-simulation-problem,"Hello everybody,I have created a new model of the ANYmal C robot with the adaptive feet described in this paper.
The instanceable asset can be downloaded here
anymal_c_softfoot_q.zip (4.2 MB)
In the folder there is the instanceable asset anymal_c_softfoot_q.usd and the anymal_c_softfoot_q_meshes.usd file containing all the meshes referenced by the instanceable asset.
These adaptive feet have a closed kinematics structure. Each foot is made up of 3 chains (right, middle and left), each made up of 9 links (5 inner and 4 outer). Each chain has 10 joints (counting also the ones that attach the entire chain to the front and back roll links).
Now, the simulation perfectly works if I leave the open kinematics structure, without attaching the last link of each chain to the corresponding back roll link. But I cannot use the feet in these way, because they would not have the adaptive feature they were designed for. So I need to close the kinematic chain.
Let’s take, for example, the left chain of the Left Front foot.
I add the 10th joint of the chain by adding a joint between the back roll link and the 9th link of the left chain, as shown in this picture
closed_kinematic_chain1920×1000 120 KBI exclude the joint from the articulation to make it work, I set the maximum joint velocity and upper/lower limits like all the other joints of the same type, keeping the joint friction equal to 0 as the joint is excluded from the articulation.I press play and this is what happens
While disabling this last added joint everything works (except the correct behaviour of the adaptive feet of course)
What should I do to make everything work correctly, closing the kinematics of all the chains of all the feet as needed?Thanks a lot in advance for all your kind and precious replies.
If you could directly correct the usd file I shared and then share back the updated version I would be extremely happy and thankful. It would be greatly appreciated.@antonello.scaldaferri.as - can you please also post the version with the missing joints that are causing the issue? I can’t see them in the asset you uploaded.It’s easy to disable via the enable checkbox:
We’ll look into the issue.Powered by Discourse, best viewed with JavaScript enabled"
708,access-to-cpython-sensor-code-or-low-level-object-collision-information-to-model-custom-contact-sensor,"Hello!I’m trying to model a custom contact sensor that can detect normal and shear forces based on the deformation of the sensor. However, the contact sensor class in the omniverse documentation only appears to allow access to the force, impulse, and normal information.Are there methods of accessing the body collision information from:Thanks!A follow-up question:It seems that the Omniverse contact sensor class can only be spherical. Are there any plans to allow for the shape to be defined by a mesh?If not, do you have any recommendations for how to utilize the spherical setup to approximate a rounded but no spherical shape?For shear force you mean access to the tangential friction force? This is currently not possible, unfortunately, but it is on our list of features to add (I don’t have a timeline yet for you, unfortunately).Yes, tangential friction force.I’m having trouble getting normal force readings from adding the contact sensors to my robot. I think it may be due to the API’s I have applied to the prims in the USD because when I import the Ant USD and add contact sensors to the legs (like the contact sensor example) it works in my script.This is the USD/link I’m trying to apply the contact sensor to:}Is there anything you see wrong that I should change?No did you compare this to the working Ant example you have?I’ve tried comparing it to the Ant example, but because the ant USD is a .usd instead of .usda file, I have to open it in the Isaac Sim GUI application first and save it to a .usda file.I believe this process changes the file in a potentially important way because when I try to import the new .usda into my scene in the contact sensor example extension using the same method as the original ant.usd file, it throws an error.However, when just comparing the API’s and settings for the xforms and meshes/shapes in the ant and my robot USD files, I’ve already made the manual changes I think are necessary to have matching functionality to get the contact sensors to work on my robot.I’ve attached the USD files for my robot that I think should work and the converted .usda ant if they would be useful:simple_ant.usda (2.0 MB)
Arm_with_Two_Fingers_Manual_SDF_Clean_Manual_Just_Robot_Edit_To_Match_Ant.usda (40.0 KB)Just to clarify the message above, I haven’t found a solution to why the contact sensor won’t work on my robot and I’ve attached the full USD files. Can you please take a look to see what the problem may be?Powered by Discourse, best viewed with JavaScript enabled"
709,isaac-sim-2022-2-1-release-is-out-and-live-now,"Highlights of the Isaac Sim 2022.2.1 ReleaseKit SDK 104.2Ubuntu 18.04 is no longer supported.SensorsWarehouse logisticsROSUpdated ROS2 MoveIt tutorial.Deprecated ROS Migration Tutorial from Sim 2021.2.1 and Isaac Sim 2022.2.0.Custom python/C++ omnigraph nodes.Isaac GymOmniverse Isaac Gym Environments Added Factory Nut/Bolt Pick environment and SAC Ant/Humanoid environments.Added support for docker and live stream, along with multi-GPU trainingFixed run-time material randomization affecting friction and restitution.Bug fixesIsaac ReplicatorCortexDocstring, comments, type hints, remove unused functions/files.Convert all monitors to use add_monitors() API in examples.DfContext converted to DfRobotApiContext (base class) and DfBasicContext (instantiation)Backward compatible API update to CortexObjectsRos and CortexSimObjectsRos to fix hard coded in_coords.PeopleReduced steps for setting up characters by automating attaching behavior scripts and anim.graph.New spawn command introduced for characters which allows a simulation to be tied to only the command file and not a USD.Navigation API changes based on feedback from users.Added a new command text box as an alternative to provide commands from within the App.Revamped UIOrbitSupport for skrl library for reinforcement learning.Added utility to convert URDF to USD using command line arguments.Fixed issue with setting camera pose in ROS convention.Added support for other camera models offered by Replicator.ExtensionOmni.isaac.extension_templates UI to generate Extension templates to download python extension templates locally Extension templates significantly reduce the complexity of building a custom UI application in Isaac Sim.Omni.isaac.ui.element_wrappers Created a set of UI element wrappers in omni.isaac.ui that significantly reduce the complexity of making a UI-based extension in Isaac Sim that aesthetically matches other extensions Available wrappers are shown in the extension template “UI Component Library.”More information can be found at:Powered by Discourse, best viewed with JavaScript enabled"
710,urdf-importer-and-articulation-trouble,"Hi, I am having trouble importing my URDF with the appropriate physics and joint constraints. I have made a camera gimbal model and exported the model as a URDF using a solidworks to URDF. Then, when importing the URDF to Isaacsim, I am able to see it, but when I simulate it, the physics do not work at all. The gimbal always shifts to the same angle and gets stuck there and does not abide by the physics. Note: the baselink (top mounting bracket) is fixed for this model.The gimbal before looks normal (I would put a picture but cannot since I am a new user).
No matter the starting orientation before the simulation starts, here is the gimbal after:
image1009×765 54 KBI have confirmed it is not a problem with the model. This is probably a simple problem, but I have not been able to get it to work.All help is appreciated!Here is another example of where clearly the physics are wrong:

image934×724 43.8 KB
The top mounting bracket is fixed in space but the gimbal arms defy gravity.Hi,
please can you share the USD file, so that we can take a look how the simulation looks like?
Thanks,
Regards
AlesPowered by Discourse, best viewed with JavaScript enabled"
711,change-pivot-not-working,"Hey,I need to change the pivots of my generated usd files to use the rep.randomizer.scatter_2d function. So far the different objects with their different pivots appear under the plane and thus I’m not able to use the scatter function properly so far.I tried right click on the prim → add → TransformOp → Pivot and then tried to change  Translate:Pivot in the properties. Instead of moving the pivot, my object translates to that position and the pivot is the same.I’m using Isaac Sim 2022.2.0 if that helps.I’m happy for other workarounds, too.Thanks a lot!Hello @hugo.81243, thank you for reaching out. We will look at better handling pivot xform ops. For the time being, the suggested workaround is to use rep.modify.pose(pivot=(<relative pivot in x, y, z>)) command:Please let us know if this does not resolve the issue for you.Powered by Discourse, best viewed with JavaScript enabled"
712,errors-with-articulation-inspector-while-moving-a-robot-imported-from-an-external-urdf,"When I open the Articulation Inspector to inspect the robot joints, I get the following warning repeatedly, eventually causing the simulation to stop due to the huge number of warnings:[Warning] [omni.isaac.core.articulations.articulation] get_joint_efforts is deprecated. Please use get_applied_joint_efforts to get the applied joint efforts.In addition, I have imported a robot from a urdf file to Isaac Sim. When I use the Articulation Inspector, I have many different errors:2023-05-31 14:54:20 [55,061ms] [Error] [omni.physx.plugin] PhysX error: PxArticulationReducedCoordinate::copyInternalStateToCache() not allowed while simulation is running. Call will be ignored., FILE /buildAgent/work/16dcef52b68a730f/source/physx/src/NpArticulationReducedCoordinate.cpp, LINE 1962023-05-31 14:54:20 [55,061ms] [Error] [omni.physx.plugin] PhysX error: PxArticulationReducedCoordinate::applyCache() not allowed while simulation is running. Call will be ignored., FILE /buildAgent/work/16dcef52b68a730f/source/physx/src/NpArticulationReducedCoordinate.cpp, LINE 1632023-05-31 14:54:20 [55,061ms] [Error] [omni.physx.plugin] PhysX error: PxArticulationJointReducedCoordinate::setDriveTarget() not allowed while simulation is running. Call will be ignored., FILE /buildAgent/work/16dcef52b68a730f/source/physx/src/NpArticulationJointReducedCoordinate.cpp, LINE 3012023-05-31 14:54:20 [55,062ms] [Error] [omni.physx.plugin] PhysX error: PxArticulationJointReducedCoordinate::setDriveTarget() not allowed while simulation is running. Call will be ignored., FILE /buildAgent/work/16dcef52b68a730f/source/physx/src/NpArticulationJointReducedCoordinate.cpp, LINE 3012023-05-31 14:54:20 [55,062ms] [Error] [omni.physx.plugin] PhysX error: PxArticulationReducedCoordinate::copyInternalStateToCache() not allowed while simulation is running. Call will be ignored., FILE /buildAgent/work/16dcef52b68a730f/source/physx/src/NpArticulationReducedCoordinate.cpp, LINE 1962023-05-31 14:54:20 [55,062ms] [Error] [omni.physx.plugin] PhysX error: PxArticulationReducedCoordinate::applyCache() not allowed while simulation is running. Call will be ignored., FILE /buildAgent/work/16dcef52b68a730f/source/physx/src/NpArticulationReducedCoordinate.cpp, LINE 1632023-05-31 14:54:20 [55,062ms] [Error] [omni.physx.plugin] PhysX error: PxArticulationJointReducedCoordinate::setDriveTarget() not allowed while simulation is running. Call will be ignored., FILE /buildAgent/work/16dcef52b68a730f/source/physx/src/NpArticulationJointReducedCoordinate.cpp, LINE 3012023-05-31 14:54:20 [55,062ms] [Error] [omni.physx.plugin] PhysX error: PxArticulationJointReducedCoordinate::setDriveTarget() not allowed while simulation is running. Call will be ignored., FILE /buildAgent/work/16dcef52b68a730f/source/physx/src/NpArticulationJointReducedCoordinate.cpp, LINE 3012023-05-31 14:54:20 [55,063ms] [Error] [omni.physx.plugin] PhysX error: Concurrent API write call or overlapping API read and write call detected during fetchResults from thread -1744832768! Note that write operations to the SDK must be sequential, i.e., no overlap with other write or read calls, else the resulting behavior is undefined. Also note that API writes during a callback function are not permitted., FILE /buildAgent/work/16dcef52b68a730f/source/physx/src/NpCheck.cpp, LINE 952023-05-31 14:54:20 [55,063ms] [Error] [omni.physx.plugin] PhysX error: Leaving setDriveTarget on thread -1525627904, an overlapping API read or write by another thread was detected., FILE /buildAgent/work/16dcef52b68a730f/source/physx/src/NpCheck.cpp, LINE 127I have similar errors also with Gain Tuner with Lula robot Description Editor, I do not know if they are all related. It could be that something is wrong with the urdf of the robot.usd file.Any idea what could be wrong? Note: I am running Isaac Sim on a docker container.Powered by Discourse, best viewed with JavaScript enabled"
713,open-connection-for-writer-syncgate,"Hi!For my use case, I am using my own writer, but I am facing some problems. In the picture you can see my currently used omnigraph:
writer_problem1140×455 53 KB
My question is:Please help me out!Best regards,
ChristofHello @christof.schuetzenhoefer, Indeed you’re correct, the SyncGate’s ExecIn should be connected to the Sd On New Frame node’s ExecOut. Could you please share how you found yourself in this situation? This may point to a bug that we would like to address.Powered by Discourse, best viewed with JavaScript enabled"
714,how-to-use-differential-controller-with-acceleration-limit,"Hi, I have setup a 4-wheeled robot with differential controller for RL training. It works perfectly but I observed that my robot is a bit jerky. I know for differential controller of ROS there is acceleration limit.
so I am wondering how can I add acceleration limit to my differential controller or is there other workaround?
Hi @y.zganz.y  -  Can you try this?Modify the Robot’s Configuration File:Powered by Discourse, best viewed with JavaScript enabled"
715,can-nvidia-gup-a40-deploy-isaac-sim,"Can Nvidia GUP A40 deploy ISAAC SIM？Hi @hangfei.zong - Yes, A40 GPU should be able to run Isaac Sim.Powered by Discourse, best viewed with JavaScript enabled"
716,rtx-real-time-opacity-material,"Hi,I am trying to make a semi transparent curtain fabric using  OmniSurface MTL inside RTX Real Time renderer but changing the opacity down from 100% simply makes the object dissapear. Am I doing something wrong ?Thanks,
KarolHi @karol.osinskiThis post may help:Hi @toni.smThank you for a quick reply!
I am aware that PT results are much better but I gotta stick to RealTime renderer for this project. The opacity works now but the white fabric turns to black and my fps instantly went down by like 60%…REAL_TIME_OPACITY1920×989 142 KBHi @karol.osinskiI have little experience in rendering so let’s look for help.
@rgasoto can you please help us with this?By the way, black curtains look amazing!!!.. and provide higher solar protection for this summer 😎Powered by Discourse, best viewed with JavaScript enabled"
717,speed-up-the-simulation,"Hello I’m using Isaac Sim to simulate multi-robot control in a factory environment, but the problem is that the speed of the simulator is too slow.
Any tips on how to speed up the simulator would be greatly appreciated!!Hi,
just to be sure, is omni.physx.flatcache extension enabled when you are simulating in the editor environment?
Regards,
AlesJust to clarify @AlesBorovicka , should flatcache be enabled or disabled for faster performance? I’ve read a couple posts suggesting the same fix to accommodate for slow USD physics but I wasn’t sure whether this has been resolved in Isaac Sim 2022.2.1It should be enabled to skip USD updates, you can check if the extension is enabled or not.Powered by Discourse, best viewed with JavaScript enabled"
718,rtx-lidar-sdg-without-script-editor,"Hey all,I’ve finally got the RTX Lidar working via the script editor method in Marks video. Super helpful!I want to run everything from when the USD is loaded and not load via the script editor.If I open the USD the SDG graph is no longer there and if I try to run the script editor code after it has been saved with the RTX running it gives an error something about graph doesn’t exist.I dont need ROS I can rip the data straight from the point cloud data on the lidar node.I’m going to sit and have a play making my graph following the example in marks video. Got a feeling it wont be this simple but I’ll report back on that.Because I need to build a graph to export the SDG pipeline I dont want to have to build it every time I load the file.Just wanted to say thanks for all this as this is something I’ve been working on for years and finally making real progress.Props to the full team!Kind regards,SirensLittle more infoI tried building the graph and sensor from scratch.I opened a RTX lidar viaCreate->Isaac->Sensors->RTX Lidar->RotatingThen copied the action graph from Marks exampleI cant find how to select the sensor so that it delivers the output of the sensor just created.I can make the graph but I dont know where to link the sensor to the graph?I’m trying to make this from scratchtaPowered by Discourse, best viewed with JavaScript enabled"
719,import-robotiq-2f-85-urdf-in-isaac-sim,"Since the “mimic” tag has not been supported in Isaac Sim, after importing the Robotiq_2f_85 gripper URDF description into the Isaac Sim, the gripper cannot be controlled correctly as shown in the figure below.
robotiq_2f_85_gripper_issue722×454 88.9 KBHas anyone successfully imported and realized the control of this gripper?Looking forward to your help.Hi @im.renpeiI think your problem is that gripper has cyclical joints…
See Add gripper to UR5e - #2 by Hammad_M, particularly video 2Hi @toni.sm , thanks for your reply. The problem is similar to what happened in video 2. I am new to Isaac Sim, could you please provide some suggestions to solve this problem?Also looking forward to your @Hammad_M help.Hi @im.renpei , have you solve it ? I am now using the 2f85 and have the same questionPowered by Discourse, best viewed with JavaScript enabled"
720,animation-graphs-and-world-position-how-to-make-it-local-space,"Question/request from Discord thread (Discord).If you have an Xform with a character under it controlled by a Animation Graph playing an animation clip (e.g., walk, idle, etc), then if the parent Xform moves, the root bone of the child skeleton compensates in the opposite direction so the character does not move.Question: Is this on purpose?I used one of the NVIDIA samples for walking. I created a new Xform, moved the character under it, then moved the Xform while in Play mode. The Xform moved, but you can see the root bone offset move in the opposite amount resulting in the character not moving.If a character is sitting in a car and the car moves, how to get the character to move with the car?Powered by Discourse, best viewed with JavaScript enabled"
721,nucleus-connection-to-the-local-host,"The local serever has been created due to the invidia instraction but when it comes to connection error appears:  wrong credentials or the user does not exist.

image610×562 25.9 KB

After repeated unloading and loading of serever error remains the same, franklt speaking the error is not the metter of mistake in spelling of username or password.
After that i blocked windows fire wall in order to exclude it`s influance on the serever connection process but the error remains the same without any change.
Kindly ask you to advise the solution of right sequence in order to arrange connection of the local host, thank you in advance!Same here.
Local server is running and can’t log in as admin.Well… I was going to post a video of the process but apparently that was the solution!!!
Tried the process almost a dozen times and when I try to document it… voila!!.. it started working.Here it is miraculously working… Omniverse Nucleus localhost login problem... gone - YouTubeCan you validate all the services, especially the auth service is running when attempting to log in?Powered by Discourse, best viewed with JavaScript enabled"
722,demo-availability,"Hi. I can only see 2 demos in Showroom (Shader Balls and Zen Garden). That’s a bit like having two pictures in a gallery. And it seems a very abandoned place, too… the version of Showroom is 2021.3.0. We are almost in the middle of 2022, so it seems Showroom is dead. Is it?
OliverBump!Bump!! Still waiting - 137 days and counting since OP’s postThanks for you patience!  Showroom 2022.1.0 was released last week with 6 new demos. Enjoy!Only the Jade demo was cool among those 6 new demos, the rest of them was bleak and boring. Screwing on bolts? Flying over empty procedurally generated terrain? Seriously? Where’s the cool water demo, the ray-traced lego buggy and the Squashy spheres demo?? Showroom was advertised as having: deformable body physics, vehicle dynamics, fluid, smoke, fire demos and . Out of all those we only got a boring, non-interactive camp fire demo.These were teased to us 1,5 years ago, but what we got so far is a fraction of what was promised. What’s going on Nvidia?What I would like to see in Showroom:
Dip Pool
Piston Buggy
Squashy Spheres
Marbles RTX Night
Some cool interactive flow demos like these ones:
/watch?v=XGLAC69J-BM
/watch?v=1o0Nuq71gI4Pretty please?Still waiting for the ray-traced buggyPowered by Discourse, best viewed with JavaScript enabled"
723,use-of-3dconnection-mouse-device-in-omniverse,"Came across article in iClone newsletter using 3DConnection “mouse device”:
Looked it up and found their interesting “3D mouse” devices.
In the list of programs it supports I did not see Omniverse ~ Supported Apps - 3Dconnexion US

3DConnection~Report missing Supported Software1724×3324 168 KB
Hello @westan!  I have a request in with the development team to support the  3Dconnexion Mouse.  I will add this request to the ticket!See this post for a possible work around: 3D connexion mouse - #3 by dfagnouAny chance there will support soon?  These tools are so popular with 3D artists and AEC professionals like myself and there are 200 other platforms supported…Powered by Discourse, best viewed with JavaScript enabled"
724,how-to-permanently-attach-the-camera-on-franka-panda-end-effector,"Hello,I would like to add a camera to the panda end effector and record the camera images while the arm moves in the environment. I would also like the camera to move in sync with the end effector’s movements. I haven’t been able to find any specific tutorials on this topic. If there is anything related, could you please direct me there?Thank you.Hi @ksu1rng  - Adding a camera to the end effector of a robot arm and having it move in sync with the arm’s movements is a common requirement in many robotics and simulation tasks. Here’s a general way to do it in Isaac Sim:Please note that the exact method for doing this can vary depending on the specific version of Isaac Sim and the type of robot you are using.Powered by Discourse, best viewed with JavaScript enabled"
725,crash-during-suspension,"Hello all,Firstly I’d like to thank the NVIDIA developers for their great products.My first post is going to be a request for help. I’ve been using Fedora (38) for a while on my Lenovo P1 Gen5 laptop. Unfortunately there seems to be an issue when the computer goes into suspension mode, whether it be by activated by me manually or through inactivity. The issue is that the computer crashes with the following message on the display and the hardware starts running on full power because the fans start blowing as if an airplane is taking off.suspend_crash800×548 101 KBThis is the output from ‘uname -a’:Linux fedora.fritz.box 6.3.12-200.fc38.x86_64 #1 SMP PREEMPT_DYNAMIC Thu Jul  6 04:05:18 UTC 2023 x86_64 GNU/LinuxAnd this is the output of “lspci -k | grep -EA3 ‘VGA|3D|Display’”:Can anyone help?Thanks very much in advance.HristoPowered by Discourse, best viewed with JavaScript enabled"
726,omniverse-launcher-resets-start-up-application-status-on-reboot,"The Omniverse Launcher appears to be resetting my start up application settings on reboot. I installed the Launcher and it started on the next reboot, so I went through Windows Startup application settings and turned it off. That worked for one reboot, but it came back the next reboot. I’ve turned it off on start up, rebooted, then checked the settings and it’s enabled again. I’d really like it to respect those settings since I don’t use Omniverse related applications regularly and don’t need it to start after a reboot.OS: Windows 10 ProRepro steps:
Install Omniverse Launcher
Navigate to Windows Startup Apps and toggle Nvidia Omniverse Launcher to off
Reboot
Navigate to Windows Startup Apps and note that Nvidia Omniverse Launcher is toggled on again.Hi @archer-kgraves. Can you try unchecking this?
Powered by Discourse, best viewed with JavaScript enabled"
727,no-keyframe-animation-coming-to-blender-from-composer,"Hey, I have a problem with exporting animations from Composer to Blneder.See this:
I found this:But in my imported USD scene in blender, there is nothing at that modifier either.
Fast answer is needed, this is a work and deadline is coming soon :)Powered by Discourse, best viewed with JavaScript enabled"
728,multi-robot-navigation-example-doesnt-work,"Hey!I was hoping to do some testing with multi-robot navigation as one of our systems will eventually need command and plan multiple robot paths.I was going to use Isaac’s multi-robot navigation example as reference, but when I went to run it, it didn’t work.I’ve built the humble_ws. When I go to launch the office example I expect to see the three windows with the office map loaded for each one, as well as the correct transforms for each of the carter robots. Unfortunately, this doesn’t happen. The three Rviz windows are brought up, but none of them receive the occupancy map or the map transform.The following errors show up in the logger:Screenshot from 2023-06-22 17-12-311854×230 76.6 KB
Screenshot from 2023-06-22 17-11-231730×230 80.9 KB
Screenshot from 2023-06-22 17-12-451854×230 76.7 KB
Screenshot from 2023-06-22 17-12-531854×230 82.8 KBI have made no changes to the scene or to the code. I am attempting to run all of this with the ROS2 humble bridge extension in Isaac.Any insight as to what the problem would be is appreciated.Realized that this is an ongoing issue that won’t be patched until August.thanks!
i hope this issue with multi robot navigation with ROS2 Humble Bridge gets resolved in the next Isaac Sim update.For those wanting a solution to this problem, check out the linked topic above! I found a fix for the problem and posted it there.This topic was automatically closed 14 days after the last reply. New replies are no longer allowed.Powered by Discourse, best viewed with JavaScript enabled"
729,whats-the-eta-for-omniverse-unreal-engine-5-2-connector-please,"What’s the ETA for Omniverse Unreal Engine 5.2 connector, please?The Unreal Engine 5.2 Connector was released about 6 weeks ago.Thx.
The confusion come from updating the 5.1 connector and expecting 5.2 version.
Deleted 5.1 connector and added 5.2 connector.Thank you Ivan!  I’m glad you have it up and running.  Let me know if you run into any further issues.Powered by Discourse, best viewed with JavaScript enabled"
730,isaac-sim-startup-error-cannot-load-cublas-dll,"When I launch Isaac Sim from the Omniverse launcher, I get the following error:Apparently, he cublas64_11.dll fails to load when importing torch in some files of the isaac core extension. Isaac Sim starts nevertheless and I can use it to some extent, but I can’t use, for example, writers as they depend on the core module.
I’m working on a Windows 10 operating system with a RTX 3090. Here is the first line of the output of nvidia-smi:
NVIDIA-SMI 511.79       Driver Version: 511.79       CUDA Version: 11.6I’ve got CUDA 11.6 installed, as I need it for other applications. Could there be a problem with conflicting versions? If yes, how could I resolve this?Using a regular python install (or anaconda) if you install
torch==1.11.0+cu113 does it import correctly?Yes, if I use anaconda (python 3.7 environment, conda install pytorch==1.11.0 cudatoolkit=11.3 -c pytorch) and open a python shell, I can use import torch without problems.I’m not sure what the issue is currently, will see if I can find a better way to reproduce it on our end@ole.wegen does this issue persist with 2022.1.1?Unfortunately, yes. However, maybe it has something to do with my Windows version. I will update it and try again.We can reproduce this on Windows server 2019 but its not clear if its an older windows SDK issue being incompatible with pytorch somehow.
If you run the winver command what version does it report?Windows 10 Version 1809 (OS Build 17763.316)Is updating the windows version an option?interestingly the windows server version we can reproduce this on is also a very similar version to yours
Windows Server 2019 version 1809 (OS Build 17762.3046)Unfortunately I’m bound by institute guidelines regarding the use of specific OS versions, which makes updating windows challenging in my specific context. Nevertheless, I will see what I can do.Hi @ole.wegen - Can you please confirm if this issue still happening with latest Isaac Sim release? (Isaac Sim 2022.2.1)Still does not work. Now the cudart_64_110.dll fails to load and Isaac Sim even crashes on startup.
Unfortunately I was not able to upgrade my Windows version due to internal issues.Hi @ole.wegen - You can alternatively try docker container if it helps in your case.
https://docs.omniverse.nvidia.com/isaacsim/2022.2.1/install_container.htmlPowered by Discourse, best viewed with JavaScript enabled"
731,creating-a-web-based-virtual-tour,"Hello,I am recreating the apartament scene in USD Composer that initially did with Maya / V-Ray.
The goal is to get as the results as close as possible to V-Ray static render but using RTX Realtime engine which I presume could eventually be run inside the browser from any device.Something like thisIs it possible to create user navigable movement using an Action Graph? Something like click&go inside the Omniverse View but more elegant and customisable.What are the current Omniverse solutions for the Web-based viewable project?
I’ve seen NVIDIA Rimac Nevera Configurator the official sample, but it seems it still has a lot of things to improve, like slow loading textures and a lot of visual glitches (unless it is caused by my internet connection).Did anybody try something like this? Perhaps on bigger scale such as entire room / apartment?Thanks in advance,
Have a great week everyone!KarolHi Karol,
Yes you are correct. Eventually the idea is that we have fully interactive 3D with real time raytracing through a web browser. You can use omnigraph to program actions, navigation, etc for an interactive experience. This could be hosted by Nvidia cloud servers or by a private server. Officially the closest thing to this is the Rimac Configurator but more examples will surely be developed.Hi Richard.
Thanks. I would like to give it a try and upload my current project to Nvidia cloud. I already applied for an access multiple times but without any luck.Could you point me somebody who I can contact directly about the cloud services?Thanks in advance.
Have a great day,Powered by Discourse, best viewed with JavaScript enabled"
732,isaac-sim-2022-2-1-bult-in-quadruped-example-crash-on-loading-to-the-world,"Hello everyone,
I have installed Issac Sim 2022.2.1 and I try to load the bult-in Quadruped Example as the picture below. But it crashes and Isaac Sim exits automatically.
IsaacSim_quadrupedExample_crash_on_load904×815 66.6 KBHere is my crash report in Isaac Sim data folder:
crash_2023-07-29_09-56-00_4336.txt (4.9 KB)Could anyone has answer for my problems? I have tried to install and reinstall in both C drive and my external D drive many times, but it still crashes when I click load scene to the world.Thank you for taking your time to visit my post.Fortunately, I have found solution for this when I leave empty ROS Bridge Extension field in Isaac Sim App Selector.
IssacSim_509×564 39.1 KBNow I can load built-in Quadruped Example but it still crashes when I turn on ROS2 Bridge Extension. Here is my crash report in data folder:
crash_2023-07-29_10-22-40_13412.txt (4.1 KB)I think this is a bug and Isaac Sim developer team should mention this.Powered by Discourse, best viewed with JavaScript enabled"
733,help-with-simulating-an-os0-128-lidar,"I’m simulating a robot which has an ouster OS0-128 mounted sideways. So I have ±45 deg viewing angle on Z axis and 360 deg on Y (pitch) axis. Now the problem is that what ever I try the lidar (Create → Isaac → Sensors → Lidar → Rotating) will not go 360 degrees around. (see attached picture, in the picture both axis of the lidar have been set to “9999999999999999635896294965248” (which is the max value) but same result can be seen on anything higher than 360 on either of the axis). Even rotating the lidar manually using an revolute joint and smaller Fov’s doesnt work.
kuva608×571 38.6 KBFor anyone else having similar issues. This video should help “Isaac Sim/Robotics Weekly Livestream: Sensors - RTX Lidar”This topic was automatically closed 14 days after the last reply. New replies are no longer allowed.Powered by Discourse, best viewed with JavaScript enabled"
734,information-about-the-omniverse-platform,"Powered by Discourse, best viewed with JavaScript enabled"
735,async-sim-forcefields-results-in-cascade-of-errors,"Using any of the Physics Demo Scenes that utilize the set_physics_scene_asyncsimrender and the Forcefield extension will result in a plethora of errors before the simulation is terminated. To reproduce just place a Forcefield into any of the Demo Scenes that uses the asyncsimrender and hit play.This is just some of the errors. Most repeat:Hi,
thanks for reporting, the async sim is not working fully with all scenarios especially if you need to interact with the scene.
We are currently trying to put together requirements for more flexible update pipeline that would resolve this, though no fixed plans yet.
Will create internal jira ticket to see if we can fix it more easily.
Thanks for reporing
AlesPowered by Discourse, best viewed with JavaScript enabled"
736,getting-error-viewportwindow-object-has-no-attribute-render-product-path-while-migrating-from-viewport-legacy-to-viewport-utility,"Hello,I am modifying my code as per the new API i.e., from viewport_legacy to viewport.utility…After running my code I am getting error (refer attached screenshot)Could anybody please help me resolve the mentioned issue ?Error_TrainFork1902×327 87.5 KBHi @nilesh.patole1 - The error message you’re seeing is because the render_product_path attribute is not available in the ViewportWindow object of the new viewport.utility API.In the new viewport.utility API, the render_product_path is replaced by the get_render_product_path() function. You can use this function to get the render product path.Here is how you can modify your code:Please replace the old code with the new one and try running your script again. If you still encounter issues, please provide more details about your script and I’ll be happy to help further.Powered by Discourse, best viewed with JavaScript enabled"
737,how-to-customize-camera-projection-type-in-usd-composer,"I am using USD Composer to adapt the camera model, but the camera Projection Type in USD Composer is not consistent with the actual camera projection model I use. For example, my current camera projection model is y=f*tan(x). How can I add this camera Projection Type to the camera of USD Composer?
微信截图_20230714151019825×239 17 KBPowered by Discourse, best viewed with JavaScript enabled"
738,replicator-headless-example-is-overridden-in-a-stronger-layer-root-layer,"I am running the example described here:
https://docs.omniverse.nvidia.com/prod_extensions/prod_extensions/ext_replicator/headless_example.html#modifying-hello-world-script-for-running-headlesslyI am getting a lot of messages likeIt already happens after only part of the code is executed:
import omni.replicator.core as repI assume this is no big issue, because the example works fine, but I would like to understand why this happens. It looks like the camera xform is created in the replicator sublayer, and there is an “over” setting the translation in the root layer.In my opinion we should not see this kind of warnings when we create objects using the replicator.Thanks for any comment
BrunoHello @bruno.vetter,Apologies for the long delay in getting back to you. You are correct on both counts - the warning can largely be ignored, and indeed this is not something that should be raised when using Replicator. It comes from the fact that certain operations can often happen asynchronously over more than one frame. In such cases, the node execution occurs after the authoring layer has been switch back to “Root”. We are working on a better solution. Thank you!This topic was automatically closed 14 days after the last reply. New replies are no longer allowed.Powered by Discourse, best viewed with JavaScript enabled"
739,is-it-possible-to-implement-equipment-that-uses-a-pulley-method-in-isaac-sim-using-rigid-body-collider-and-joint,"Is it possible to implement equipment that uses a pulley method in Isaac Sim using Rigid Body, Collider, and Joint?Hello @toni.sm, Long time no see.
I’d like to physically implement something similar to a conveyor belt but with a different pulley system, and I’m not sure if that’s currently possible in Omniverse and Isaac Sim. And if this is possible, I’m not sure which Joint to use.This facility uses a pulley method to make the gripper body that picks up the box move up and down.Please refer to the photos and video below.
The movement of the equipment in the video was made to move with a simple action graph, and no physical rigid bodies were included.pulley belt.PNG1283×719 61.9 KBDue to the complex equipment and modeling, the frame falls with many colliders and rigid bodies, but the overall movement of the equipment is made to move by connecting revolute joints to the wheel and body.
As shown in the picture above, I want to physically move the pulley method as well.Wheel1017×540 197 KBHi,
We are planning to release C++ Omniverse PhysX Examples, where one can implement its own type of joint, for example pulley joint. (thats actually an example of a custom joint in PhysX SDK and will be available as an example in the released C++ Omniverse PhysX Examples). However till this is released you would have to cheat somehow by adding a prismatic joint to restrict the movement long the axis and using joint drive to drive towards a desired position.Regards,
AlesPowered by Discourse, best viewed with JavaScript enabled"
740,audio2face-not-launching-on-windows-11-because-of-python-asyncio-related-errors,"Can someone help me with this issue when launching audio2face, please?
image1461×1292 66.9 KB
I am using a Windows 11 machine with 64G RAM, a 13900K CPU, and two RTX 3090 GPUs. The same machine can run USD Composer with no issues. Also, I tried to disable one of the 3090 GPUs but still got the same error.
Also, I was able to run Audio2Face on the same machine in a Linux system.The version I try to launch is 2023.1.1 Beta.Quick update: by running .\audio2face.bat --/exts/omni.kit.async_engine/event_loop_windows=SelectorEventLoop I was able to launch the stage window, but I got another error when trying to load the AI model(Claire):
image1463×1166 91.4 KBPowered by Discourse, best viewed with JavaScript enabled"
741,velocities-are-being-throttled-on-physics-objects,"I am currently using USD Composer beta 2023.1.1 and I’ve noticed that when trying to run rigid-body simulations the maximum velocities are set at a very low number despite no configuration specified in the stage. Trying the same action in 2022.3.3 does not have this issue.Dragging the rigid body around does temporarily set velocities to reasonable rates but as soon as I let go, all non-gravity velocities get reset to zero.Also, trying to set any physics parameters seems to hide/remove all physics attributes.I had previously run into issues with trying to use a limit on an unrelated particle system, but have since used the Omniverse cleanup tool and reinstalled USD composer.Hi,
can you please elaborate more on this issue is? You mean the maxDepentrationVelocity is too low? Could it be that you are using assets with different MetersPerUnit?
Regards,
AlesThis issue is occurring on a brand new stage and with no imported assets. I just created a cube and applied the dynamic collider property. I’ll try to get a video of the process.https://vimeo.com/850209023?share=copyUff I really dont know what is going on there. This is really very strange, could you just save that file as USDA and paste here what such a simple scene contains? Something is not right, I dont see what though. Can you also try to simulate on the CPU? Create a physics scene and disable GPU dynamics, but that should not really be related. This is very strange.
Regards,
AlesbrokenScene.usda (8.3 KB)Here’s the scene as a USDA filekit_20230731_080807.log (2.4 MB)Disabling GPU Dynamics didn’t seem to change anything. Here’s the log file if it’s helpfulI am having a hard time reproducing it nor understand what is going on.
There might be some old settings or something bad written in your user config file. You can try to wipe the old config file (or rename it) to see if that help.
The user config file should be in your user directory:
AppData/Local/ov/data/Kit/USD.Composer/2023.1/user.config.json
Other then that I am really puzzled about what is going on.
AlesDeleting the config file fixed it! I’ll attach it here in case you want to look into it any further. Thanks!user.config.json (122.3 KB)Uff glad to hear that!Thanks for the config file, will try to figure out what was wrong.Regards,
AlesPowered by Discourse, best viewed with JavaScript enabled"
742,convert-step-to-usd-with-python-code,"Hello,I am trying to write a converter for converting a .STEP file to a .USB file. I want to make this a standalone application with python. I know that it is possible in the GUI of Isaac Sim but I want to do it in a standalone application. Is that possible?Thanks in advance.Hi @nickbakker40  - Yes, it is possible to write a standalone application to convert a .STEP file to a .USD file using the Omniverse Kit SDK. However, it’s important to note that the conversion process involves a few steps and requires a good understanding of both the STEP and USD file formats.Here’s a high-level overview of the process:Import the STEP file: You can use a library like pythonOCC or FreeCAD to import the STEP file and access its geometric and topological data.Create a new USD stage: Use the pxr.Usd.Stage API to create a new USD stage.Convert the STEP data to USD: This is the most complex part of the process. You’ll need to iterate over the geometric and topological data in the STEP file and create corresponding USD prims in the USD stage. The specifics of this process will depend on the complexity of your STEP files and how much detail you want to preserve in the USD file.Save the USD stage: Once you’ve created all the necessary USD prims, you can save the USD stage to a .usd file using the Save method of the pxr.Usd.Stage API.Here’s a very basic example of how you might structure your application:This is a very simplified example and doesn’t include the actual conversion of the STEP data to USD. The specifics of that process will depend on your particular use case and may require a good deal of custom code.Please note that the CAD Importer extension in Omniverse Kit provides a GUI-based tool for importing CAD files (including STEP files) into a USD stage. If you’re looking to automate this process, you might consider using the Omniverse Kit SDK to create a custom extension that uses the CAD Importer extension to perform the conversion.Powered by Discourse, best viewed with JavaScript enabled"
743,trouble-in-saving-map-when-i-follow-tutorial-for-visual-slam-with-isaac-sim,"I ran the command that 'ros2 action send_goal /visual_slam/save_map isaac_ros_visual_slam_interfaces/action/SaveMap “{map_url: /home/gaozhao/map}” '.In the terminal,it showed’success: true.However,i could not see the expected map in the target directory ‘home/gaozhao’ or in ‘home/gaozhao/map’.Except for this,anything else seems to go well!
HELP!!!What is the problem?
Screenshot from 2023-08-07 10-52-521920×1078 292 KB
Screenshot from 2023-08-07 10-45-531920×1078 277 KBScreenshot from 2023-08-07 10-53-411920×1078 331 KBPowered by Discourse, best viewed with JavaScript enabled"
744,enabling-livestream-native-with-usd-composer-2023-1-1-beta,"Hello, I’m running into an issue when trying to enable livestreaming from the latest BETA version of USD Composer.  I’m attaching the logs below, but it looks like it’s trying to get to an internal Nvidia host for some extensions (and from a prior post I saw with a similar issue)Any advice on how to mitigate would be great!Powered by Discourse, best viewed with JavaScript enabled"
745,api-or-sdk-for-calculating-the-current-fps-on-a-python,"I want to write a program in Python which will
1: calculate fps of the current active window (game) or window by pid.
2: display the number of current fps in the selected location, on top of all applications, etc. For start, you can display in the upper left corner, let’s say in orange.Hi @vladfesik36? Is this question in the context of Omniverse? Our Omniverse apps already have an FPS display.
image1673×603 116 KBPowered by Discourse, best viewed with JavaScript enabled"
746,isaac-sim-crashes-when-importing-urdf-with-many-links,"Since the contact sensor would only give the same value regardless of the location on a link, I created multiple links to simulate the contact values at multiple locations. This works when I tested it on a single-finger link. However, when I test this on the entire robot hand, Isaac Sim crashes immediately after initialization.I receive the following errorI was wondering if is this an expected behavior of Isaac Sim? My setup is 2xRTX6000, 128GB RAM. I monitor the RAM/VRAM and the utilization are low. Any suggestion is appreciated.Hi @lihongyu0807  - The crash might be due to the large number of links and joints you are creating. Each link and joint in a robot model adds to the complexity of the physics simulation, and there is a limit to how many can be handled efficiently.If you are trying to simulate a tactile sensor array, consider using a different approach. Instead of creating a separate link for each sensor, you could use a single link with a collision shape that covers the entire area you want to sense. Then, use a contact sensor to detect collisions with this link.You can also use a script to process the contact sensor data and determine the location of the contact within the link. This would give you the same information as having a separate sensor at each location, but with much less computational overhead.If you still want to use multiple links, try reducing the number of links and see if the simulation still crashes. If it doesn’t, you might need to find a balance between the number of links and the level of detail you need for your simulation.Also, make sure that your links and joints are defined correctly. Check for any errors or inconsistencies in your URDF file. A small mistake in the URDF can cause the simulation to crash.Thank you very much for the detailed reply. Indeed, when I reduce the number of joints/links to 20%, the simulation runs without any issues.Could you please elaborate on how to use a script to process the contact sensor and determine the location of the contact within the link? From my understanding, the contact sensor would only provide point collision information. Is this enough to detect the precise collision location on a robot’s finger tip?Hi @lihongyu0807  - In Omniverse Isaac Sim, a contact sensor measures the surface load applied to a body. It simulates the contact sensor by summing all forces applied on a given trigger spherical region intersected with the given body surface via the omni.isaac.sensor extension.However, the contact sensor does not provide detailed information about the precise location of the contact on the surface of the body. It only provides information about whether a contact has occurred within the defined region.If you need to determine the precise location of the contact on a robot’s fingertip, you might need to use a different approach. One possible method could be to use multiple smaller contact sensors distributed across the surface of the fingertip. By checking which of these sensors has detected a contact, you could infer the location of the contact on the fingertip.Here is a simple example of how you might implement this in a script:This script would print the name of any sensor that detects a contact, allowing you to determine the location of the contact on the fingertip.Please note that this is a simplified example and the actual implementation might be more complex, depending on the specific requirements of your application.Thank you very much for the detailed, helpful reply.This topic was automatically closed 14 days after the last reply. New replies are no longer allowed.Powered by Discourse, best viewed with JavaScript enabled"
747,list-of-icons,"Is there a list explaining the different icons that appears in Stage? It would be great to understand what they all mean.
Here are som examples of icons:
Hello @Jack_Jensen!  I wasn’t able to find a list of icons in our documentation so I reached out to our UI and Documentation teams.  I will post here once I have more information!  Great request, btw!Hi @Jack_Jensen!  Just wanted to give you an update on this.  The documentation team is working on an update that will include the stage icons and their meanings.  I will post back here when they have had a chance to publish it!Until then, please have a look at the Type column inside the Stage browser (which you cut off in your screenshot).
That tells what each primitive inside the stage is exactly and that should also make clear what the base icons mean.The orange arrow on top of an icon means it’s a Reference.
The blue “I” character on top of an icon means it’s Instanceable. (See the check box inside the property window.)E.g. in your screenshot the mesh primitive named “defaultobject” is under a referenced instanceable Xform, and that’s also why it’s greyed because you cannot edit sub-trees beneath referenced instanceable primitives.I was also looking for a list that explains the different icons.Hi there,
I know this is an old topic, but any update on the list of icons?
ThanksPowered by Discourse, best viewed with JavaScript enabled"
748,getting-an-error-in-audio2face-usd-export-unable-to-create-a-stage-for-writing-usdc-file,"testing audio2face for the first time.  Getting an error when writing the USD from BlenderUSD Export: unable to create a stage for writing whats causing this?Powered by Discourse, best viewed with JavaScript enabled"
749,outdated-omni-replicator-version-in-isaac-sim,"Hello!I noticed that my version of omni replicator is 1.7.8 and it is shown as up to date in the Isaac Sim extensions. Is there any other way to update replicator to the latest 1.9.8? I tried reinstalling, but still have 1.7.8 after installation.Thanks in advance.Hi @meiramovrahat787 - The latest replicator will come with latest Isaac Sim release around August timeframe. May I know where did you see the option to update the replicator to latest 1.9.8?I found that the latest version is 1.9.8 from replicator documentation here: Changelog — omni_replicator 1.9.8 documentationThe compatible version of Replicator with new Isaac Sim release is planned towards towards Aug/Sep timeframe.Powered by Discourse, best viewed with JavaScript enabled"
750,how-to-programmatically-modify-attibutes-especially-for-the-datatype-of-token-of-an-ogn,"Hello,I created an extension to programmatically connect some customized OGNs inside of a SDG Pipeline once an animation is triggered.Nevertheless, I also want to modify certain attribute, e.g., inputs:renderVar of this OGN called SD Render Var To Raw Array. Because for my project I need rendering output of HDRColor instead of the default LDRColor.However, the datatype of this input attribute is predefined as “token”, which means I can’t simply assign a string “HdrColor” to this input since the datatype doesn’t match to each other.Therefore, my question would be is it possible to programmatically modify/assign input variables with datatype defined as token?Thanks in advance for considering my request!Best regards,
Y.W.Hi there,can you provide details on how the SDG pipeline is created? Does the pipeline work is you use LdrColor or LdrColorSD instead of HdrColor?Best,
AndreiPowered by Discourse, best viewed with JavaScript enabled"
751,audio2face-failed-to-get-checkpoints-for-omniverse,"Audio2Face failed to get checkpoints for omniverse.
Unreal Engine 5 cannot load usd files
How can I solve this?Hi @whkdnwl7 - I moving this post to right forum.Can you please send your latest Audio2Face log file which can be found here: C:\Users\<USERNAME>\.nvidia-omniverse\logs\Kit\Audio2FacePowered by Discourse, best viewed with JavaScript enabled"
752,import-load-a-usd-file-containing-a-skeleton,"Hello,
i am trying to import / load usd file (e.g. omniverse://localhost/NVIDIA/Assets/Isaac/2022.2.1/Isaac/People/Characters/original_male_adult_construction_01/male_adult_construction_01.usd) via the python api (SimulationApp). All the other usds worked fine with create_prim(...). But when i load this contruction worker (or any other model with a skeleton / skelJoints) the skelJoints turn into xforms and are no longer moveable or rotatable - so the skeleton doesn’t work anymore. Also tried add_reference_to_stage(...)(same result as create_prim(...)) and found out about payloads which i don’t get to work with the python script BUT even when loading it via drag and drop (which should be payload) it again changes skelJoints to xforms. (in a new scene drag and drop works fine and the skelJoints are still moveable).
Is there any mistake that i do? The whole script is about 1000 lines, so i won’t post it - what information is necessary, so somebody could help me?would really appreciate some help - cheersAdditional info:
Also the final plan for this worker would be to move his hand via a leap motion controller (positions get transmitted via ros - everything ready and already moving a franka panda hand with it). When i can load the worker only need to move his hand according to (scaled) leap motion controller - hand- and finger-coordinates. But there is another Problem: (how) is it possible to setup an IK-Solver for a human-like arm? Also tried to import prerigged (with IK configured) .fbx files, but it doesn’t import/use the IK.
I am explaining this, because if this doesn’t work (yet?) - the import / load of the model wouldn’t matter anyways. Then i would use just simple geometries, that represent every fingerlink and a palm and move them.Hi @Buzz_T  - It seems like you’re experiencing an issue with importing USD files that contain skeletal animations using the Python API in Omniverse Kit. When you import these files, the skelJoints are being converted into Xforms, which makes them unmovable and unrotatable.This could be due to the way the USD file is being imported or referenced in your script. When you use the create_prim() or add_reference_to_stage() methods, it’s possible that the skeletal animation data is not being properly preserved.Here are a few things you could try:If none of these suggestions work, it would be helpful to see a simplified version of your script that reproduces the issue.Thank you @rthaker for your help! Sorry, for the late answer - just wanted to let you (and anyone who reads this) let you know that i was in a hurry, and so i went on with a simplified version. Couldn’t get it to work or find out what the problem was/is.Powered by Discourse, best viewed with JavaScript enabled"
753,starting-synthetic-data-recorder-through-script,"Hello, I am trying to start the synthetic data recorder at certain events, such as when a vehicle reaches a certain location. I am trying to do this within a python Omniverse object behavior script. Essentially, I would like to hit the start button for the synthetic data recorder through python scripting. What modules and commands do I need to make this work?Hi there,you can take a look at this example, it does something similar by accessing annotator data:The next Isaac SIm release (2023.1.0) will support custom trigger events on writers as well.Hello, thank you for the response. I have tried to implement this code in the on_update() portion of the object behavior script, but it appears to write empty data sets. The cam object is created as an Xform, but when I try to print the rgb_annot.get_data(), it prints “[  ]”. Any help would be appreciated, the code is provided below.def sdrecorder(self):You will need to either put rep.orchestrator.step() or world.step(render=True) to feed data to the annotator.Powered by Discourse, best viewed with JavaScript enabled"
754,pointcloud-isaac-sim-empty,"Hello there,My pointcloud as title suggest is empty!
Few background note:I have a isaac sim extension, where i placed a robot, the ground plane and a cloth-like object on a place to do some pick and place operation.
I therefore created a camera and created the annotator to obtained the rgb/depth/pointcloudWhile the RGB/D are working perfectly, point cloud is always empty. (I well aware of the fact that depth and ptc have the same information, but still it would be more conveniente to have the ptc direclty)What i am missing?thanksAccording to the documentation, by default, only objects with assigned semantic labels are included in the pointcloud.
You can either assing a label to the object you are trying to grasp, for example with:  omni.isaac.core.utils.semantics.add_update_semantics
Or you can customize the Pointcloud annotator to include unlabeled objects using: rep.AnnotatorRegistry.get_annotator(""pointcloud"", init_params={""includeUnlabelled"": True})That should fix your issue I think.I also recently started using the pointcloud annotator and am experiencing a huge frame rate decrease when its activated. Did you notice this too?Cheers
ThomasI found an alternative workaround, this adds the semantic to the primitive from the path. It was buried in the thousand of scripts…I do notice the same decreasing, but it’s more related to the rendering of the scene and not to the pointcloud itself…Powered by Discourse, best viewed with JavaScript enabled"
755,im-looking-for-force-sensor,"Hi NVIDIA Gym Team and friends in this community,I want to get the force value with a script.Has the current force sensor been updated like the answer on the site above?If updated, I’m looking for a way and an example how to write it.Thanks for reading.Hi  jinid231,If you are looking for force sensor in Isaac Sim, it will be in our next release, which is  in mid May.
But if you are looking for force sensor in Isaac gym,  please ask in the Isaac gym forum.Kindly,
Liila@ltorabi So is there any force torque sensor in Isaac Sim like 6d force sensor in IsaacGym rightnow?Force/Torque sensors are directly supported in PhysX now:
https://docs.omniverse.nvidia.com/app_create/prod_extensions/ext_physics.html#force-sensors@ltorabi  Thanks for replying. I’ve tried articulation force sensor , the Xform is set as articulation force sensor

WX20220303-1815081657×815 167 KB
However when the tool tip hits the table, the force torque sensor reading is alwasys zero

WX20220303-1820571697×845 91.5 KB
I’m wondering what’s the problem.Hi @ltorabi , it’s great to see that the force sensor is supported. But it seems that this sensor mainly records forces acting on the specific link. Normally, when grasping objects, the force on the active joint will change, I wonder if it is possible to attach a force sensor to a joint to read out the corresponding force value?@im.renpei  Hello, do you know why the force sensor reading is alwasy zero? What’s the force computing mechanism in physx.@im.renpei Hi, can you share how do you set articulation force sensor and make it work?The website is invalid，can you sen again？Here’s the updated link: Articulations — extensions latest documentationPowered by Discourse, best viewed with JavaScript enabled"
756,documentation-example-on-when-to-use-physicsapi-vs-physxapi,"Hello!I’m trying to troubleshoot a USD file I’m creating (specifically trying to figure out why the contact sensor attached is not reading any values even when the physics debugger is detecting contact) and I have noticed both PhysicsAPI and PhysXAPIs being applied to prims in USD files from the Omniverse Nucleus.While there is some documentation here on the different API’s, I don’t see anything that specifies when to use a PhysicsAPI vs a PhysXAPI. For example, PhysicsRigidBodyAPI vs PhysxRigidBodyAPI.Is this documented anywhere or are there any general principles for how to decide what API to apply?Thanks!Hi @jess7654  - The PhysicsAPI and PhysXAPI are both used in the Omniverse Nucleus, but they serve different purposes.The PhysicsAPI is a general API that is used to define physics properties that are common to all physics engines. This includes properties like mass, friction, and restitution.On the other hand, the PhysXAPI is specific to the PhysX physics engine. It is used to define properties that are specific to PhysX, like the PhysX material or the PhysX collision layer.In general, you should use the PhysicsAPI for properties that are common to all physics engines, and the PhysXAPI for properties that are specific to PhysX. If you’re not sure which one to use, it’s usually safe to start with the PhysicsAPI, and then switch to the PhysXAPI if you need to access PhysX-specific features.As for the contact sensor not reading any values, it could be due to a number of reasons. It could be that the contact sensor is not properly attached to the rigid body, or that the rigid body is not interacting with other bodies as expected. It could also be a bug in the physics engine or the contact sensor implementation. It would be helpful to have more information about your setup to diagnose the issue.Here is the .usda file for the robot I am trying to use the contact sensor with:
Arm_with_Two_Fingers_Manual_SDF_Clean_Manual_Just_Robot_Edit_To_Match_Ant_ToF_Contact_Sensor.usda (51.3 KB)Please let me know if any other information would be helpful!Also, is there a way to see the contact sensor values from the Isaac Sim GUI application? Right now I have to write and run from a Python script in order to troubleshoot the contact sensor readings.Someone from our team will review the file and provide you the solution.Hello,Has there been any progress on a solution?Hi,Just following up on this.Developers are currently busy with the upcoming release. Meanwhile, did you get a chance to review the doc for contact sensor? Contact Sensor — isaacsim latest documentationPowered by Discourse, best viewed with JavaScript enabled"
757,customwriter-for-lidar-pointclouds,"Hello all!For my object detection I want to generate from a lidar sensor 3D pointclouds and annotate them directly. However, I have not seen any opportunity to do so and want to kindly ask if someone can help me out where to start and what steps are required.Is it anyway possible to write the pointcloud data in a known format that can be further used for training?Thank you in advance!Best Regards,
Christof SchützenhöferIn addition, I would like to know how to annotate the pointclouds (3d bounding Box). Can I simply use the available annotator?Can anyone help me here?Hi Christof, the output of current point cloud annotator is calculated based on the distance from the camera to the object, so you can assume it is like a lidar 3D pointclouds. In terns of annotation, what data would you like to annotate to the points? Pointcloud and 3d bbox are two different annotators, but you  can find a mapping using their semantic ids.Thanks for the response!Using a lidar sensor, I would like to annotate objects in my pointcloud using bounding boxes.Unfortunately, it does not seem to work with the currently available annotators. I only got nodes in the omnigraph that are not connected.Do you have any working example for lidar sensors and 3D bounding boxes?I got the 3d bounding box annotator to work, but the result is not correct. It seems that it depends on the current viewpoint of the lidar sensor. However, rotating the sensor leads to some bounding boxes.Is there any implementation for lidar sensors to compute the bounding boxes based on the captured pointcloud?So you first capture the pointcloud given the prims using lidar sensors, then you place a 3D bounding box annotator to try to capture the 3D bounding box for the points? I don;t think the 3D bounding box annotator can work on pointcloud, but you can first use the 3D bbox annotator on prims to get the data, and then combine it with the pointcloud data to get the bbox of the pointcloud.Yes, this is what I want to do, but I need to know which objects are hit by the sensor. Is there any way to get that information from the sensor?Yes, there is a camera3dPositions, but we haven’t expose it as a annotator. It outputs a shape of (width, height, 4) array, which the first 3 column are points’ positions in camera space, and 4th column denote whether it is hit by the camera or not. We will expose it as a annotator so you can easily use the info.And this will also work for lidar sensors?You need to use the data along with the lidar sensor. camera3dPositions is giving you a mask that filters out points that are not hit.Okay, thank you!Is it already in the latest release or will it be in the next one?It will be in the next one. I can show you how to create an annotator that outputs this information but that’s pretty complicated.Depends on the date of the release. Maybe, it is worth that you show me how to do it (I really need it as soon as possible). I really appreciate your support! Thank youAny help or any news on it?Powered by Discourse, best viewed with JavaScript enabled"
758,does-omniverse-have-any-plans-for-a-web-based-version-that-runs-in-a-browser,"I know you can render videos already, but I am curious if anywhere on your roadmap is the ability to render Omniverse in a web browser?Just wondering if interactive websites could ever be possible.ThanksHello @DataJuggler!  Yes, it is possible with Omniverse Cloud.  We have an example of this here: Rimac Nevera 3D configurator.  You can Launch the Experience which will launch a web-based app inside of Omniverse Cloud.Fine print: Rimac Nevera 3D Configurator can be launched from the latest Google Chrome or Microsoft Edge Browsers on Windows, macOS, or Chrome OS devices.  Support for mobile phones and tablets is coming soon.Thank you. I will take a look.I am worried about the future of AI:
image1207×976 137 KB
Are you sure the link is correct, or maybe not public?ThanksA feature like this makes a lot of sense. One might assume it’s inevitable.My apologies, I  set that link to this: https://www.nvidia.com/en-us/omniverse/cloud/rimac-nevera-3d-configura instead of this https://www.nvidia.com/en-us/omniverse/cloud/rimac-nevera-3d-configurator/Guess I got too excited with the keyboard!  LOLI edited the link so it should work now!Thank you.Is it just me, or is there no way to exit the car after entering it?
Also, the h-Axis is inverted while the v-Axis is not inverted. Never came across such a strange setup. :PI know it’s just a demo, but hey, that’s a glimpse into the future, where everything is standardized, no? I am actually very much interested in Omniverse and creating apps that run with USD and Omnigraph and that will run anywhere some time in the future. :)Keep up the good work!Hi @WendyGram !Does Omniverse Cloud requires RTX gpu on the user side? Or is it entirely run using server GPUs?Thanks,
KarolAh sorry, I just read it can be launched from any device. :)sample model not working
image1057×360 13.5 KBPowered by Discourse, best viewed with JavaScript enabled"
759,omni-livestream-websocket-plugin-failed-to-initialize-encoder,"Environment details:I started isaac sim with isaac-sim headless web socket,and connected it using usl:http://192.***.100:8211/streaming/client/. the sever side terminal window displays as belows
image1682×1056 128 KBand the client side window displays as below:
image4960×2348 82.1 KBwho can help me to solve this problem?Hi @samitezhang1 - The error message “Failed to initialize encoder” suggests that there might be an issue with the video encoder used by the livestream websocket plugin in Isaac Sim. Here are a few troubleshooting steps you can try:Replace “470” with the version number of the latest driver available for your GPU.
2. Check your GPU capabilities: Make sure your GPU is capable of encoding video. The Nvidia Geforce RTX 3080 Ti should be capable of this, but it’s worth checking the specifications on the manufacturer’s website.
3. Check the plugin configuration: Make sure the livestream websocket plugin is correctly configured. There might be settings related to the video encoder that need to be adjusted.
4. Reinstall Isaac Sim: If none of the above steps work, try reinstalling Isaac Sim. There might be an issue with the installation of the software itself.Powered by Discourse, best viewed with JavaScript enabled"
760,composer-crashes-once-in-about-2-minutes-when-working-with-vdbs,"Very hard to make work.Please tell me where to send my logs?

image980×1356 63 KB
I zipped them all from the correct folder.
Size is 1.84 GtThis was solved by total clearing of my Win10 & runnign Omniverse cleanup tool.If I keep the viewport resolution as 25% I can work with 4 large VDB seqs without crashes!! Coool.This topic was automatically closed 14 days after the last reply. New replies are no longer allowed.Powered by Discourse, best viewed with JavaScript enabled"
761,running-simulation-in-unity-visualize-in-omniverse,"I’m trying to replicate something similar to Co-Simulation UE and Omniverse - ""SpaceVerse"" GTC demo - Return on Experience. | Tutorial only in Unity. What would be a workflow for this?Initially thought the idea was to simulate on the engine of choice (UE, Unity, etc.) utilizing the engine run-time and then live-sync would take care of it on the omniverse side. But, at least for Unity connector, it seems like that’s a hard limitation. Is run-time sync in the roadmap at all?In the above example, it seems like it’s either the UE engine running the movement within omniverse OR the custom app that’s processing the trajectories.csv. Some clarification would be amazing on which one of the two is running the simulation (movement) in omniverse.Powered by Discourse, best viewed with JavaScript enabled"
762,offset-problem-from-pick-and-place-example-when-using-ur5e-2f-150-end-effector,"Hi,I made a pick and place example with UR5e + 2f-140 gripper.
However, it has offset problem when it pick a cube. I think having the only z-offset is enough at end_effector_offsetbut, it has some proportional value on planar xy with the position of the cube.How can I fix this problem?
I attach my code below.image2168×1270 212 KBur5e_pick_place.7z (2.7 MB)ThanksPowered by Discourse, best viewed with JavaScript enabled"
763,duplicate-mesh-symmetrically,"Is there a tool that can symmetrically copy mesh along a certain coordinate axis as following link？https://download.autodesk.com/us/maya/maya_2014_gettingstarted_chs/index.html?url=files/Assigning_a_fur_description_Duplicating_objects_across_an_axis_of_symmetry.htm,topicNumber=d30e51949I think there is a long way to get such tool inside USD Composer, it is not meant to be a modelling package. I guess they would rather release their own modelling software directly connected to USD Composer at some point :)Thank you Karol.  :)Yes I also agree. Composer is not designed as a modeling package and any geometry command is best done in your dcc package before import.Powered by Discourse, best viewed with JavaScript enabled"
764,cannot-drag-assets-in-seq,"Hey, I cannot use mouse to drag my animated usd files in seq timeline. These are alembic file exports from Houdini, orginally.I can change the start time by tweaking the Sequence clip / Start slider as you can see from the video. But why the mouse dragging is not possible on these?Powered by Discourse, best viewed with JavaScript enabled"
765,fbx-exports-dont-work-and-i-dont-understand-the-omniverse,"Hello, I hope you don’t mind but I have some general questions about the omniverse and digital twins along with a more specific technical bug which I think should be answerable by anyone somewhat familiar with the the omniverse.I’ll ask the more techincal question first.
Notice how at the end it says it can’t find Rick_Roll_Bridge.usdz[0/RickRollBridge02_Model_2_u1_v1_baseColor.jpg]? Well, I used https://filext.com/file-extension/USDZ to open that file and that file DOES exist. …I have uninstalled and reinstalled the entire omniverse in an attempt to fix this problem. I am only using the default settings. But I’m not an animator so I don’t know a whole lot about these file formats and if this is a normal problem or not. Can you please advise me on how I can fix this problem? …it’s preventing me from doing this tutorial .My second question is also from a newcomers perspective so I hope you don’t mind if I group these two together. I’m having a hard time wrapping my mind around the omniverse. I guess my first question would be something like …“why is nvidea so convinced that digital twins will be relevant”? I watched Building Virtual Worlds with Omniverse  - GTC November 2021 Keynote Part 4 and it seems like NVIDEA believes there will be a new kind of internet, using proprietary NVIDEA technology, that perfectly models planet earth at a physics level. Plus there will be all kinds of other worlds. Am I understanding that right because it seems unrealistic to me. I could explain more than this but for one I think it has been established that people don’t want to live in VR. …how is NVIDEA planning to monetize creating a twin of the planet? How can they claim it is like HTML when it uses proprietary technology. …I’ll stop.I think if you look at the error message it failed to copy from that USDZ path to a destination path that seems to end with a square brackets but has no open square brackets. I think the square brackets is used for “this file is inside the USDZ (its a ZIP)”. It looks like something got confused and messed up the path names - e.g. it took the characters after the trailing “.” or the last “/” which (incorrectly) meant the closing “]” got included.Regarding the second question, Omniverse can be used for many different things, but that does not mean all those uses is why NVIDIA created it. It picked USD to support because USD is an industry standard adopted by many tools in the space. (Its from Pixar originally.)Digital twins does not necessarily mean modeling the whole world - it could be a factory floor. For example, train new staff with a 3D model before they visit the factory, so they know the safe paths when there are robots on the floor as well. Or check the safety of people walking around a factory before installing a new unit - does it result in people walking closer to the industrial robot arm that would injure people if they were hit by it.Or can I detect a faulty part due to a sensor raising an alarm - show me exactly where the part is so I can send a person to go repair it, and they know exactly where to go.It also can be useful for training self driving cars. Instead of real world conditions, create lots of different situations via 3D graphics (different weather, different road conditions, different times of day, etc) and check the self driving car models work on it well. Can it spot small children on the road at night in the rain correctly? If a car has to swerve to miss a child or a cat on the road, which does it pick?These are practical problems that are being faced today where it can reduce costs.Powered by Discourse, best viewed with JavaScript enabled"
766,blender-free-addon-for-landscape-3d-model,"Share this Blender free addon for Landscape 3D model, visit here, How to make Realistic Landscape Terrain with free Blender addon - cgian.comHow-to-make-Landscape-Terrain-in-Blender-cgian-800x445800×445 81.3 KBPowered by Discourse, best viewed with JavaScript enabled"
767,warning-omni-kit-notification-manager-manager-physx-error-pxrigiddynamic-body-must-be-non-kinematic,"Applying ArticulationAction on kinematic robotic arm generates this warning. It only happens when the device is CPU. For GPU PhysX device I dont get such error. Can somebody help me in comprehending why the behavior is different for CPU and GPU? What is the appropriate way to set joint positions for kinematic only prims?Hi @prath4  - The warning you’re seeing is due to a difference in how PhysX handles kinematic and dynamic bodies. In PhysX, a kinematic body is one that is not affected by forces or collisions, but instead has its motion explicitly controlled by setting its position or velocity. On the other hand, a dynamic body is one that is affected by forces, collisions, and can also have its motion controlled by setting its velocity.The setAngularVelocity function is intended for use with dynamic bodies, not kinematic ones. When you call this function on a kinematic body, PhysX issues a warning because it’s not appropriate to set the angular velocity of a body that isn’t affected by forces or collisions.The behavior difference between CPU and GPU might be due to different versions or configurations of PhysX being used for CPU and GPU simulations. It’s also possible that the GPU version simply doesn’t issue a warning in this case, even though the operation is still not appropriate.To control the motion of a kinematic body, you should use the setGlobalPose function to directly set its position and orientation. If you want to control the body’s motion using velocities, you might need to switch it to be a dynamic body.For setting joint positions for kinematic only prims, you can use the set_qpos function of the Articulation API. This function directly sets the joint positions of the articulation, and is appropriate for use with kinematic bodies. Here’s an example:In this code, new_joint_positions is an array of the new positions for each joint in the articulation. The order of the positions in the array should match the order of the joints in the articulation.Hi @rthaker thanks for your detailed response. I tried to look for set_qpos function in the core api docs but couldn’t find it. I am referring this link. Can you please point me to the correct documentation of the set_qpos function.Thanks for your helpPowered by Discourse, best viewed with JavaScript enabled"
768,isaac-sim-omnigraph-array-index-errors,"Hello,When using the ArrayIndex node, there is no protection from index out of range exceptions.
Causing the following error:For us, the use case is trying to extract the velocity command from a differential controller in order to control an 8-wheeled robot, but since the differential controller outputs an empty array at first, it throws the above error.Hi @omers - You can probably add a conditional check before accessing the array elements using the ArrayIndex node. You can use the omni.graph.nodes.Compare node to check if the array size is greater than the desired index before accessing the element.@rthaker Ok thanks. Do you think this should be added as part of the ArrayIndex node as well?Hi @omers - Yes, you can add it as a part of ArrayIndex node.You can add something like this:Powered by Discourse, best viewed with JavaScript enabled"
769,isaac-sim-installation-stuck,"Isaac Sim installation has been stuck while installing without any error message…Thanks in advance…Powered by Discourse, best viewed with JavaScript enabled"
770,updating-nodes-attributes-dynamically-from-code,"Hello,I wanted to use Replicator so that it either updates colours of all my prims individually (i.e. assigns a different colour to each prim) or have them all assigned to the same colour. I populated list of prims using rep.get.prim_at_path and then, I update inputs:paths attribute with the paths. However, I noticed that as soon as I pass “/Replicator” as the path (to update all prims to the same colour), Replicator maintains this setting despite me updating intputs:paths attribute of my prim_at_path node.I have uploaded an example Python code to illustrate the problem. What happens, in the first 10 frames each object has different colour assigned as expected. Then, for another 10 frames, they all receive the same colour. I would then expect the process to be repeated, i.e. each object to have colour assigned separately, but it is not the case. Colours of all prims is updated to the same value, despite changes to inputs:path attribute (see printed output of get_attribute(“inputs:paths”).get() )I would very much appreciate if someone could advise me on how to solve this problem. I would prefer to modify prims’ properties in a single simulation rather than restarting Replicator each time with new settings. Furthermore, I’d prefer not to change the layer as I’d need to re-load all assets which may be a bit time-consuming.Many thanks,
Mateuszedit: I forgot to mention that I’ve also tried creating two separate prim_at_path nodes for individual objects and for whole stage, and control the execution via omni.graph.action.Branch, but similarly, updating “inputs:condition” attribute didn’t seem to impact the execution paths. I also noticed that by default those multiple prim_at_path nodes were connected to each other, so I used connect and disconnect methods to wire all nodes.test.py (2.3 KB)Powered by Discourse, best viewed with JavaScript enabled"
771,replicator-documentation-offline,"Unfortunately I am not able to open the Omniverse Replicator documentation since last Friday, it seems like something is being changed. I wanted to ask when it will be available again, because I need to read something for documentation purposes.image1910×886 28.7 KBBest regards,
JulianAfter searching I found out that the link to this site has changed:https://docs.omniverse.nvidia.com/extensions/latest/ext_replicator.htmlLink forwarding of old bookmarks should be active now too.Thanks for your patience as we updated the documentation siteThis topic was automatically closed 14 days after the last reply. New replies are no longer allowed.Powered by Discourse, best viewed with JavaScript enabled"
772,cannot-launch-isaacsim-2022-2-1-vkresult-error,"Hi,I’m trying to run Isaac Sim on Ubuntu 20.04. I’m using A6000 with vGPU. The driver version 525.85.05.I’ve tried to run Isaac Sim from Omniverse Launcher and terminal. It can’t run with the following error. The log file is also attached.image944×68 38.8 KBkit_20230712_175215.log (802.2 KB)I’ve tried to reinstall the driver for several times and checked there is no duplicate icd.d in /usr/share/vulkan.The same issue still occurs even container is used.
Please help on this issue and how I can fix it.Thank you.Can anyone help? I’ve tried different methods but I still have no clues.Thank you.Hi. Can you run Create or Code from the Omniverse Launcher?I’ve tried run View from the Omniverse Launcher. It also can’t run.Thank you.This sounds like an issue with your setup or drivers. Please share the output of nvidia-smi.Try do clean install of the drivers and try install the latest production .run drivers too.
Delete the /usr/share/vulkan and /etc/vulkan folders after the uninstall.See also: Linux Troubleshooting — dev-guide 104.2.0 documentationI’ve tried to reinstall the driver and delete the mentioned folders. The same issue occurs.image1082×531 131 KBCan you try the latest drivers? 535.86.05Powered by Discourse, best viewed with JavaScript enabled"
773,action-graph-not-loading-properly-when-a-stage-is-loaded-from-standalone-script-in-isaac-sim,"I am trying to run a standalone python script that aims to open and load a USD stage and starts it running. The USD file that the script intends to open has been already worked upon and contains robot prims and some action graphs. The action graphs use Isaac ROS bridge and Semu Robotics bridge by @toni.sm to publish various information on ros topics and respective transforms. Now, whenever the USD stage is loaded from GUI, i.e., opening Isaac Sim from launcher and opening the USD file, everything runs fine. The transforms are published and I am able to control the robot via MoveIt. However, whenever I try to open and load the USD file from standalone script the MoveIt fails to connect as “Comput Graph” attribute from the Action Graph of all nodes are missing.The script used:I am also sharing the screenshots of a sample action graph node, where compute graph is missing when running from Standalone script.
From_Standalone554×542 23 KB

From_GUI559×805 43.4 KB
Below are the system specifications:Isaac Sim 2022.2.0 and Isaac Sim 2022.2.1 (tested on both)
Ubuntu - 20.04
CPU - Intel Core i7
Cores - 24
RAM - 32 GB
GeForce RTX 3060-Ti
VRAM - 16GB
Disk - 2TB SSDHi @deveshkumar21398Can you please share the logs?Yeah sure @toni.sm. Please find the log attached.kit_20230515_185952.log (1.4 MB)Hi @deveshkumar21398For some reason that I need to take time to inspect, when running from a Python standalone script the extension is not being loaded.
However, including the path to the extension, before loading it, as follow works:I have tested it with the following code and the semu.robotics.ros_bridge extension is loaded successfully this time.Hi @toni.sm thank you for your response. In my case, the semu.robotics.ros_bridge is already in isaac_sim-2022.2.1/exts folder. However, still I tried the above script by placing the extension in Kit path and including the path to the extension. However, still the issue remains same. I guess when I see the console it seems node starts (“SemuRosBridge node started” statement in below console) but problem seems to be somewhere else. See:Hi @deveshkumar21398The error you are getting now is related to the IsaacGetViewportRenderProduct OmniGraph node.Is your ActionGraph configured correctly?
Can you provide a minimal reproducible example?Hi @toni.sm now I tried to run not a custom USD file instead a sample stage from IsaacSim, i.e., carter_warehouse_navigation.usd in Samples, whose path is - omniverse://localhost/NVIDIA/Assets/Isaac/2022.2.0/Isaac/Samples/ROS/Scenario/ or omniverse://localhost/NVIDIA/Assets/Isaac/2022.2.1/Isaac/Samples/ROS/Scenario/Even such an example is not properly running with “Compute Graph” missing. Besides the same works fine if we run the example from GUI. It fails to load the attribute “Compute Graph” from standalone script. PFB the screenshots of comparispon.No “Compute Graph” screenshots running from standalone script:
No_Compute_AG1920×1200 137 KB

No_Compute_Node1920×1200 153 KB

No_Compute_RC1920×1200 137 KB
With “Compute Graph” screenshots running from GUI:
With_Compute_AG1920×1200 148 KB

With_Compute_Node1920×1200 160 KB
Also, the code is exactly similar to the one you posted above except the open stage command, which is changed to:I believe the issue is somewhere else, because all such USDs are missing “Compute Graph” attribute. Could you please help me with this.Hi @deveshkumar21398I cannot reproduce your issue.
For example, the following code runs perfectly on Isaac Sim 2022.2.1.Can you please upload the logs of this last example?Sure @toni.sm. Thank you for your response.Here is the log for the example in IsaacSim 2022.2.0:kit_20230527_184102.log (4.5 MB)Here is the log for the example in IsaacSim 2022.2.1:kit_20230527_184520.log (4.6 MB)now I tried to run not a custom USD file instead a sample stage from IsaacSim, i.e., carter_warehouse_navigation.usd in Samples, whose path is - omniverse://localhost/NVIDIA/Assets/Isaac/2022.2.0/Isaac/Samples/ROS/Scenario/ or omniverse://localhost/NVIDIA/Assets/Isaac/2022.2.1/Isaac/Samples/ROS/Scenario/Even such an example is not properly running with “Compute Graph” missing. Besides the same works fine if we run the example from GUI. It fails to load the attribute “Compute Graph” from standalone script. PFB the screenshots of comparispon.No “Compute Graph” screenshots running from standalone script:Hi @Hammad_M, could you please help us.
I cannot find any relevant info in the logs provided in the previous post… and I cannot reproduce the issue.Hi @deveshkumar21398 - It might be related to the sequence in your script. When you open the stage using open_stage, it might be overwriting the existing stage and causing the loss of the “Compute Graph” attribute from the Action Graph of all nodes. Can you try the following script and let me know if you were able to resolve the issue or not:Hi @rthaker thank you for your response.I tried running the above script, however, there is no such method as load_stage in the stage.py file. Therefore, it can’t import the same. Could you please check and confirm once from your end or if there is alternative way to load the stage such that it is not overwriting the stage.Moreover, in this process I found load_stage.py, a standalone script to the load the stage. Even running the same by giving the path to any USD file, the issue persists.Please check and confirm once.Hi @rthaker @Hammad_M  a gentle reminder just in case you missed on this issue. Please help us with the resolution as we are not able to execute any USD file with Omnigraph nodes using a standalone script. Any help would be appreciated. Thanks!elow are the system specifications:Isaac Sim 2022.2.0 and Isaac Sim 2022.2.1 (tested on both)
Ubuntu - 20.04
CPU - Intel Core i7
Cores - 24
RAM - 32 GB
GeForce RTX 3060-Ti
VRAM - 16GB
Disk - 2TB SSDHi @deveshkumar21398 - Can you try this script? if that doesn’t work then I will revert this question to devs who can help further.Hi @rthaker, I have tried running your script. However, it return an error message saying set_stage is not an attribute to UsdContext. I have attached the screenshot as well. Meanwhile, I tried set_stage_live as well, however, it returned as - the method is deprecated and is recommended to not use it.Error_Message1345×550 66 KBPowered by Discourse, best viewed with JavaScript enabled"
774,cant-get-the-render-farm-to-work-in-usd-composer,"Hi,  I posted it also there Job submitted to Farm but nothing happens but I think it is directly related to my USD Composer because I managed to submit the jobs for render but no window pops up inside USD Composer, basically nothing happens.Powered by Discourse, best viewed with JavaScript enabled"
775,use-audio2face-for-avatar-chatbot,"Hi, I would like to use audio2face to animate a chatbot I’m creating. I already have the NLP part of the chatbot, as well as the speech2text and text2speech parts. In fact, to interact with the bot locally, what I do is to launch an Audio2Face window, configure the streaming player and send audios to it via Python. However, I’d like to be able to integrate this into a webpage, rendering the animated face in real-time, as I do in the Audio2Face studio, but via web. I’ve seen that this must be possible  due to this demonstration video: Intelligent End-to-End AI Chatbot with Audio-Driven Facial Animation - YouTube However, I haven’t been able to guess how to do something like that in the video… I’ve seen you can export the usd cache from audio2face, but that’s not what I need, as I’d need to animate the usd in real-time; that’s useful only for applications where the audios are fixed, not generated by an AI model…Hello @usuario3863!  Welcome to the Community!  Wow super excited to hear what you are working on!  I send your questions over to the development team to help get you some answers!Hi @usuario3863 , thanks for the ping.You can use one of the streaming extensions Omniverse provide if you like to stream through a webpage.  We did recently find some of those extensions are not exposed in A2F and we have a hot fix patch coming that fixes that.Upon doing that, just put the a2f app to full screen and stream the output to the web, that should be it.  In the Misty demo above, that was just Omniverse running full screen viewport, that’s it.hope this helpsWould it be possible to create this kind of AI bot or like Misty but for an AR application? What’s the performance requirements to run this?Hey, I am working on the same project. It would be great if we could connect!@siyuen has this patch been updated?I am also working on similar project, is there any update for this? thanksFew options there, ACE allow you to do cloud based runtime of A2F but is in closed beta right now.  Public release will have to wait a bit.You can also look at the headless, rest API of A2F.  You can then stream those results to other engines
For using OV, you can build your own app with just the renderer and host in cloud and stream results out.  Those are all part of the latest A2F already released.Which update exactly do you mean?Is there any update to this thread ? I would like to know how to get some training on how to integrate python to the Audio2Face or a location where I can get some guidance on how to integrate and expose python for text to speech and speech to text integrations like mentioned above. It does not have to be a web application and could be just A2Face running at time of use. Please let me know how to get some help on thisHave you tried Audio2Face Headless and RestAPI? Audio2Face Headless and RestAPI Overview - YouTubePowered by Discourse, best viewed with JavaScript enabled"
776,omni-replicator-core-create-camera-projection-type,"This documentation (API — omni_replicator 1.6.4 documentation) states, that projection_type can be selected from [pinhole, fisheye_polynomial]. By intentionally producing an error, I was able to see, that [‘fisheyePolynomial’, ‘fisheyeOrthographic’, ‘fisheyeEquidistant’, ‘fisheyeEquisolid’, ‘fisheyeSpherical’, ‘fisheyeKannalaBrandtK3’, ‘fisheyeRadTanThinPrism’, ‘pinhole’] are also possible projection_types. I guess the documentation is not up-to-date and there are also other updated/changed parameters. When can I expect an update?Hi @sebastian.reicher - Someone from our team will review and respond back to you.Hi @sebastian.reicher and thanks for reaching out! You’re absolutely correct that these additional projection types were not yet implemented in 1.6.4. That has been addressed in versions >1.7.2 and should be reflected in the latest release of IsaacSim. If you still see a discrepancy, please let us know!Powered by Discourse, best viewed with JavaScript enabled"
777,cc-is-characters-are-finally-realtime,"Hello, exciting news! I got a tip from Benjamin Samar about using the Path Traced skin material in RTX realtime mode. It works finally and thank you very much Benjamin!See, this is my final realtime result:However, it did not came easy.
Rabbit Don’t Come Easy ( Helloween working music) !!The orginal native RTX shared looked like this:Here is the combination of all renders, you can see how the “Path Traced skin material in RTX realtime mode” solution kinda loose all the details of the skin:How can we get the details back?My solution was to combine in post the both realtime renders, and also added a lutify.me skintone correction adjustment layer. This can easily be done by everyone and also possible to do at cloud ( we are building our Cineshare product at Infinite-Compute, they are our partner ).I had a 60% transparency on the top layer ( sss shader hack )
layer opacity953×222 7.39 KBSo please join the conversation, what do you think of this?@weienchen @WendyGram @eetatwei @DavidDPDThere are so many variables at play here that it’s hard to know where to start, but I will share for the moment my attempts to maximize Camilla skin shaders generated via iclone path trace export, albiet brought into rtx mode w sss enabled. I agree that for sure, once the path trace prepared skin shaders are brought into rtx mode, there is a washing away of certain details. I will also submit though that I believe these normals and micro-normals details are still there, even upon leaving default settings as is, they just become more sensitive to the scenes lighting conditions, rendering settings, and settle latency. You can as an option crank up the normals and micro-normals settings a bit, but I’m not sure if that is the key here.There are other factors as well when it comes to taming and getting the most from the rt mode inside of composer. This will likely impact the performance level regarding fps, but if it is our goal to get the best possible quality result, I would suggest some of the following strategies.Eco mode disabled.I went to ‘rendering settings’ (by clicking on the rendering engine selection menu in the viewport) and selected ‘load from preset’ —> finalSet the super resolution mode from DLSS to DLAA. (yes this will impact vram so be advised, honestly I dont know how critical this is but I just wanted to remove this as a factor)I believe it’s important to disable the ‘experimental’ New Denoiser, in the Direct Lighting section. I love this feature but we don’t want it in this scenario.In  this example I’ve set my subsurface settings as such. I’m just cranking things up as a general strategy. Full disclosure I am not an expert on these settings:
sss_settings779×332 15.1 KBWith the rt mode, allowing more time for things to fall into place is part of what will give you this additional image detail. Raytracing settle latency. If you want to in the end render out a movie capture sequence with these more resource hungry settings, you will have to determine the ideal settle latency setting. Yes this is going to add some time to the render vs just a settle latency of zero. Maybe try starting out with an SL setting of 50, and increase from there after testing if it seems to make any difference. At a certain point, too much settle latency will create diminishing returns and become pointless, so its really going to depend on your hardware, resolution, the weight of your scene etc etc etcI spent time experimenting with many different hdri’s trying to find something that was complimentary. Here is the sample I have currently. Let’s focus on skin for now and not worry about issues with the eyebrows/hair/scalp line, we will get to all of that later, although I do want to see if its possible to improve the hair results by working with the omnihair shader.rtx_sss_v11920×3413 428 KBAgain just a v1 sample here, completely different lighting conditions etc so its not the most direct comparison to your test above Pekka. Just where I’m at with this so far.BONUS TIP: I think there is an issue with the tearline shaders that come out of iclone after conversion. It seems as though a shader is there but it remains invisible in both rt or pt mode. Correct me if I’m mistaken. My solution is to just drop the bundled water shader from the Composer material library onto these objects.For the Kevin it works even better!
The native RTX Realtime was so good I had only just a bit of hacked Path-Traced render over it. Thanks @deeplerningIf anyone has ideas to improve this, please drop a message here!Another way to “hack” this RT SSS shader.
I borrowed new A2F example character Claire´s complex material and just replaced the most obvious texture maps from CC4 Camilla export.Wow, it looks kinda good, what do you think? No post efx on this render.
If there is someone who knows better about the superior Claire RT shader, please write your thoughts, I would love to try to transport this shader in full effect, with all the texture maps in use…Powered by Discourse, best viewed with JavaScript enabled"
778,mouth-cavity-strange-animation,"My mouth cavity is animating in a strange way. (I hid the skin mesh so you can see it more easily)The opening at the front is where the mouth cavity joins up with the face skin - the opening at the front follows the lip movements (generally correctly). But if you watch the video, the back part of the mouth cavity jumps all around the place, including through the skin of the character at times.Is there a reason for this, or a way to stop it from happening?Here is an example of some of the blendshapes being generated.image919×340 49.3 KBCan this be due to the mouth cavity being too close to other parts of the face, e.g. under chin?So you think it is proximity of the mesh vertexes with other meshes being animated by audio2face? My goal is to keep the vertexes near the front of the cavity moving, but the ones at the rear of the cavity I want to get audio2face not to move (by making sure not close to other things).There are some challenges there - the quality of the mesh is not great at times. E.g. the teeth and mouth cavity might be intersecting.I cannot easily change the meshes individually (they are generated by another tool, and I am trying to simplify the process for everyone using that tool). I can however write automated scripts to do things to the meshes (I have had to write Python code to rebuild the meshes to remove unused vertexes already).  Are you suggesting sloping the rear of the mouth cavity up might help? (E.g. add a bit of a Y boost based on the distance from the front of the cavity mesh.) I can try that.  My alternative was to try and splice the mouth cavity into two meshes and only make the front mesh dynamic.It’s hard to tell for sure what the issue is without looking at the scene. But I assume there’s something wrong with the deformer bindings.
Another way is to find the culprit deformer and set the weights its problematic vertices to 0.0Powered by Discourse, best viewed with JavaScript enabled"
779,omniverse-could-not-connect-to-the-authentication-service,"Omniverse could not connect to the authentication service, I can not open the localhost.
I’m assuming this is localhost Nucleus.Can you go to http://localhost:3080/ in your browser and validate all the services are running?It displays ‘Cannot connect to System Monitor Backend.’

Error1048×372 17.7 KB
Please use the Nucleus → Launch button in the hamburger menu in Launcher and if you need to click the Restart All button.still can not. It seems there is something wrong with the internet connection. So strange. the Nucleus interface is like this. There is  no localhost under Omniverse option. And I have already created a a Server.

Error1824×753 33.1 KB
I have the same problem and I can’t find a solution. please help.

omniverse hata1920×1080 149 KB
Please open your system monitor by going to the hamburger menu-> settings.
You need to validate the server is running locally, as well as all the services included.Powered by Discourse, best viewed with JavaScript enabled"
780,how-can-i-resolve-the-access-violation-exception-when-opening-a-usd-file-using-my-custom-built-usd-c-library,"I am currently trying to use the USD C++ library in a custom C++ project on Windows 11, using Visual Studio 2019.From the USD GitHub repo https://github.com/PixarAnimationStudios/USD, I downloaded the latest source code release and built the lib using the build_usd.py located in the build_scripts/ directory. I built the libary with a few flags to avoid building unnecessary features (I basically only kept Alembic and HDF5 flags because I am planning to use the Alembic library in the same project).However, once the library is added to my project, I run this block of code :I get the following thrown exception :
Exception thrown at 0x00007FF810001190 (vcruntime140.dll) in alembicwrapper.exe : 0xC0000005 : Access violation reading location 0x00000219EF1A0000.No matter how many extra libraries I try to add or build options I change, I still encounter this error and I haven’t been able to find the answer on the issue section of the GitHub page.What am I doing wrong here ? Is this an issue that could be related to the compiler ?The first problem for me is that I have to include several directories to the project, and not only the USD include/ directory from the build/ folder, otherwise I get errors telling me some header files from the library can’t be located.What bugs me is that those extra directories are located in the USD include/ directory itself, the paths indicated in some of the source files in the #include <some-source-lib.h> line are not correct, which is why I need to add those.I also need to link against several libraries that are also located in the build/lib and build/bin dirs in order to avoid some LNK2019 errors.My goal here is just to open a USD file in order to check if the lib is working correctly. I know my USD file is correct since it is an official example from the USD website.Hi @hdebouard. I’m not sure about this one. Could you log the issue here? Issues · PixarAnimationStudios/USD · GitHubPowered by Discourse, best viewed with JavaScript enabled"
781,cad-importer-terminology,"CAD is a different world than gaming or research. There’s established industry standard practices in that world and I’m disappointed that Omniverse seemingly hasn’t done research into that.https://docs.omniverse.nvidia.com/prod_extensions/prod_extensions/ext_cad-importer.htmlLinear deflection - this should be called “linear tolerance” or “surface deviation”. Also, extremely important to explicitly say which unit it’s in!Angular deflection - this should be called “angular tolerance” or “angular deviation”. This doesn’t really exist in the CAD world. The tolerance is all that’s needed. But again, extremely important to explicitly say if it’s in radians or degrees. Unless I’m mistaken, right now it appears to be in radians, and literally nobody in the CAD world uses that.Minimum surface - again, tolerance is all that’s needed, but if you must have this please state the unit of measurement! (Also, since I know we are dealing with polygons here, I’m surprised there isn’t a maximum surface… quite a few tesselators have this.)CAD deals with real world objects which have absolute real world sizes. This gets lost in the UI in Omniverse currently which causes uncertainty. (For example, after import, I’d like to be able to select an object and see what it’s width, height and length is in millimeters, so I can confirm that it imported properly… all I see now is a very confusing transform matrix.)If everything I say gets ignored, at least please add unit labels to everything!Thank you - this is great feedback.Powered by Discourse, best viewed with JavaScript enabled"
782,audio2face-problem-failed-to-build-tensorrt-engine,"HiI get this message “Failed to build TensorRT Engine” using latest version as of today 230727. Using RTX 2060 cardAny clue what to check?regards
ErikHello and welcome to the forums Erik,Could you please send your latest Audio2Face log file? It should be located inside C:\Users\<USERNAME>\.nvidia-omniverse\logs\Kit\Audio2Face on Windows and ~/.nvidia-omniverse/logs/Kit/Audio2Face on Linux?Powered by Discourse, best viewed with JavaScript enabled"
783,replicator-composer-objects-is-not-dropped-and-overlapping,"Hi, I’m following Replicator Composer tutorial and tried to run sample file (warehouse.yaml) as belowbut when I open the result images the objects is floating. I want objects to be dropped and not to be overlapped. Even if I added “obj_physics: True” to warehouse.yaml, it wasn’t resolved. what to do?
rgb_01920×1080 181 KBThanks in advance.Hi @daisuk , this is because the scene’s gravity is by default set to 0. If you want the item to be dropped, you can add a new parameter in your yaml file. scene_gravity: 9.8, which is the earth’s gravity constant.Hi @jiehanw , thank you for your reply.
I tried that, but not improved. I checked the manual for the Replicator composer parameter list and found that the scene_gravity value should be set with a Boolean type, so maybe I should use boolean type?
I addedscene_gravity:True
obj_physics: True
physics_simulate_time:30to the warehouse.yaml file as well. but not improved. Is this a kind of bug?
image906×303 12.9 KBCame across this thread with a possible solution, potentially worth a try - Object Physics (obj_physics) not working in Replicator Composer. Objects are still flying - #12 by sebastian.reicherPowered by Discourse, best viewed with JavaScript enabled"
784,reallusion-characters-for-avatar-streaming,"Reallusion’s Character Creator 4 comes with an expanded list of blendshapes in a way that doesn’t trivially map to the 52 ARKit blendshapes. What would be the recommended flow for using these characters for avatar streaming? Thought about recreating ARKit blendshapes in Audio2Face, but decided there should be an easier way and I am missing something. Should I be adding a node into the solved mark blendshape file and map the 52 numbers to the 70 something blendshapes in my character somehow? Would appreciate some pointers.Or will there be a similar workflow video to the Camilla videos for custom characters from Reallusion?Thank you for all the hard work! The new updates are so exciting!Hello and welcome to the forums @OfozThis is possible and we have Siggraph talk which will happen in about a month, that shows the workflow.In the meantime we’re working with Reallusion team to get this more streamlined.Thanks so much for the response!For the Siggraph talk, should I be registering to get access to it?And in the meantime while waiting, any hints on the approach I can take? (Is it a mapping between the blendshapes or a way to export from CC4 with the Apple ARKit blendshapes only?)You don’t need to register for Siggraph for this.The solution is to create a script and match the Reallusion blendShape target names to Audio2Face blendShape target names. It needs a little bit of digging and finding out the missing (or added) blendShapes, merging the splitted ones and renaming them. Which is possible if one has enough knowledge of blendShapes in USD and Omniverse python scripting.Thanks again. I made some progress on this by switching back to Standard Profile in CC4 (which has a much better 1:1 mapping with the Apple blendshapes) and then editing the usd as usda inside Composer to reorder/reduce the blendshapes, so that the incoming array is mapped to a same size array. This moved things in the right direction, but still if I use Mark_solved in Audio2face and use a male reallusion character in avatarstream, the lip syncs are fairly off.I wanted to double check my understanding for the workflow with ARKit blendshapes: in audio2face side, we do not need to solve and obtain the blendshape weights with the exact same character we will use on the avatarstream side, right? When I try to stream from mark_solved on audio2face side to Claire on avatarstream side, it still maps pretty well, so that was my understanding. should this be true for reallusion characters as well? (And I am trying to get base_body working first. I understand I will have to deal with tongue, teeth meshes later still)In order to obtains best matching blendShape weights, the solver needs driver and driven meshes to be the same.Powered by Discourse, best viewed with JavaScript enabled"
785,view-failing-to-complete-install-stuck-on-omni-view-warmup-bat,"Just gets stuck on ‘\AppData\Local\ov\pkg\view-2022.3.7\omni.view.warmup.bat"" --/app/environment/name=‘launcher_warmup’ with no error. Tried running as Administrator but still no luck :(
Plenty of disk space, RTX Quad’ 4000, Win11Powered by Discourse, best viewed with JavaScript enabled"
786,sequencer-window-unexpected-behavior-view-start-offset-jumps-to-end-of-range-and-other-problems,"I am trying to convert my own character to comply with all the SkelRoot, SkelAnimation, etc standards so I can animate it using SkelAnimation clips. I probably still have something wrong with my character, but I don’t know what. Reporting the problems here together in case they are related.I am using 2023.1 (no idea if this happens in earlier versions).Before - note the view range at the bottom has view start 0.
image1816×580 37.7 KBWhen I drag a new character into the sequencer (the left panel, under the “motion…female” text), it creates a new track correctly, but the view start offset is changed to 300 at the same time. Is that a bug? (It is at least annoying as I keep having to remember to reset the view range back to start from 0). (Many other actions do the same after watching it for a while.)
image1816×592 32.9 KBUPDATE: For the second point, it turned out I missed the “Add Animation / SkelBinding” step on the animation clip I was using. The animation clip now works fine in the Sequencer, but I still get warnings such as 2023-07-01 03:12:23  [Warning] [omni.usd] Warning: in _CopyMetadata at line 5080 of W:\ac88d7d902b57417\USD\pxr\usd\usd\stage.cpp -- Failed copying metadata: Cannot set field 'variability' of type 'SdfVariability' to provided value '' because the value is an incompatible type 'void', on spec </World/Sequence/Liana_v1_22_1_fixed/stand_to_sit_251081_Clip/__sequence__clipAnim_43.animationSkelBinding:sourceSkeleton>. But another step forwards!Powered by Discourse, best viewed with JavaScript enabled"
787,isaac-sim-import-fbx-but-no-textures,"Hi there~I have a tunnel model with fbx like the first photo.
When I try to import to Isaac Sim, it becomes the second photo.
(isaac sim → File → Import)It seems like that it doesn’t import the textures successfully.
Is anyone know how to import fbx models correctly???
image836×467 80.1 KB

image1704×1052 94 KB
Have you tried importing this asset into other DCCs like Blender or 3ds Max?Some FBX files, depending on how they were exported, will not work because the texture paths are incorrect within the FBX or material definitions in the FBX are not defined.If it can import into other DCCs correctly and you can share the FBX with me I could take a look to understand why it’s not working.Thanks@a0939890213 , were you able to resolve the issue?, if not then please provide your response as requested in previous message.Meanwhile, you can try this video and see if it helps resolve your issue: Omniverse Create - Importing FBX Files | NVIDIA Omniverse Tutorials - YouTubeHi, I’m having the same problem.I have followed the video to import the fbx and alternatively I have imported my model as an stl.In both cases, the materials are not applied to the prims.
The fbx file is being exported from fusion 360. When I bring it into Unity, the materials work. Can anyone shed some light?ThanksHello - Can you please upload the FBX file here? so we can take a look at the problem and debug it.Hi @rthaker, I can also reproduce the behaviour for most FBX files. One example which does not work for me is i.e. this one: Free Magnetic Phone Car Mount 3D - TurboSquid 2019954If I import it to USD Composer or Isaac Sim it looks like this.
image1367×905 57 KBHope that helps, and looking forward for a hint how to solve this issue. Thanks!Powered by Discourse, best viewed with JavaScript enabled"
788,omniverse-open-stage-error,"Hello,I have a question or more a note I found a way around the error.When I want to open a stage with:I get the error: Inconsistency detected by ld.so: …/elf/dl-tls.c: 618: _dl_allocate_tls_init: Assertion `listp != NULL’ failed!The way I fixed it is to open the stage with:What is the difference between these two. Why does the second one work and the first one not.Thank you in advance.Hi @nickbakker40  - The open_stage function from omni.isaac.core.utils.stage and Usd.Stage.Open from pxr.Usd both open a USD stage, but they do so in slightly different ways and may have different behaviors depending on the context.Usd.Stage.Open is a function provided by the USD API (from Pixar). It opens a USD file and returns a Usd.Stage object that you can use to interact with the data in the file.On the other hand, open_stage from omni.isaac.core.utils.stage is a utility function provided by the Isaac SDK. It likely includes additional functionality or setup specific to the Isaac SDK. The error you’re seeing could be due to this additional functionality not being compatible with your specific setup or usage.The reason why Usd.Stage.Open works while open_stage does not could be due to a variety of factors, such as differences in your environment, the specific USD file you’re trying to open, or how you’re using the returned stage. Without more information, it’s hard to say exactly why one works and the other doesn’t in your specific case.Powered by Discourse, best viewed with JavaScript enabled"
789,point-cloud-create-crashes,"Hi, I imported a point cloud into Omniverse. I am able to open it and see it, but once I try to do anything else Create crashes and closes.Powered by Discourse, best viewed with JavaScript enabled"
790,how-to-open-usd-file-with-usd-api,"As the title, If I want OPEN the usd file (not load) by extension, is there have USD API that I can use? Thank you!!Screenshot from 2023-08-02 11-34-41698×461 48.3 KBPowered by Discourse, best viewed with JavaScript enabled"
791,torque-control-model-parameter,"@toni.sm @Ossama_Ahmed   I’m applying custom torque controller in IsaacSim. The controller works well in Bullet but not good in IsaacSim. The “damping” and “stiffness” parameters are set  0.1 and 1e4 as Bullet, the robot joints fluctuate as the following video when the controller starts.  In bullet, the robot will hold still with the same parameter.
Hello!  would you be able to share the USD file(s) for me to check? If the robot was imported using a URDF, the urdf source should suffice.in either case, here’s some things you can do to check the scene integrity:The robot drive Stiffness and Damping are PD parameters for the joint pose, respectively. so making a stiffness equal zero means your robot will only try to compensate for deviations on the joint velocity, which may lead to fluctuation. if your robot has a position drive, you need to set a Stiffness parameter for it to follow the position target.Thanks for replying. I want to clarify that If I’m aimed to applying pure torque control, the damping and stiffness have to be set 0?yes - that should work. you can have a small damping component to represent the internal energy loss of the system. This would help the physics to maintain simulation stability.Hi @rgasoto  I’ve tried all solutions above but didn’t work. The robot still fluctuates. Here is my urdf and generated usd file.
Uploading: flexiv_rizon4.usd…
flexiv_rizon4.urdf (7.2 KB)Hello, I was unable to download the USD file. it seems it didn’t come through.
for the URDF, are you able to package it on a zip folder containing  the robot meshes as well?@rgasoto  I’ve reuploaded them, the usd file is imported using urdf importer.
robot.zip (1.3 MB)
flexiv_rizon4_kinematics.usd (4.4 MB)The joint drive is a PD controller on the joint position and velocity target. The stiffness is the proportional gain to the position, and the damping is the Derivative gain on the position, and the proportional gain on the velocity target.For position target control set parameters for both Stiffness and Damping above zero.
For a velocity control, set the stiffness to zero and damping to something above zero.
For direct joint effort control, set both drive parameters to zero, and use the Advanced properties of the revolute joint tab to define a stiffness / damping (both values need to be above zero).
For absolute precision, set the stiffness to float_max (1e40), and add some damping for internal energy loss.Hello, @rgasoto . I am simulating force control to control the wheels of the car. I have set the stiffness and damping to zero，and the damping and stiffness in the advanced properties of the revolving joint tab are also zero.However,the damping and stiffness adjustments in the advanced properties of the revolving joint tab have no effect on the simulation.The car will first move along a straight line and then start to rotate.So I want to know what effect the stiffness and damping have on the direct force control in the advanced properties of the revolving joint tab.And how to control the wheels through direct force control to realize the straight driving of the car.
Thank you so much.Hi @1365351984  - Stiffness and damping are properties of a joint that control its behavior in response to forces and movements. Stiffness determines how much the joint resists deformation or displacement, while damping determines how much the joint resists changes in velocity.In the context of a revolving joint in a physics simulation, stiffness and damping can affect how the joint responds to forces applied to it. If the stiffness is high, the joint will resist being rotated by the applied force. If the damping is high, the joint will resist changes in its rotational speed.When you set the stiffness and damping to zero, you’re essentially making the joint freely movable, with no resistance to rotation or changes in rotational speed. This could explain why the car first moves along a straight line (since there’s no resistance to the wheels rolling) and then starts to rotate (since there’s no resistance to changes in rotational speed).To control the wheels through direct force control and realize the straight driving of the car, you would need to apply forces to the wheels in such a way that they cause the car to move in a straight line. This could involve applying equal forces to both wheels, or applying different forces to each wheel depending on the car’s current direction of movement.Thank you for your reply. But when I set the damping and stiffness to 10 in the advanced properties of the revolving joint tab. The car still will first move along a straight line and then start to rotate.how do i set it up？@rthakerPowered by Discourse, best viewed with JavaScript enabled"
792,cant-see-my-assembly-from-onshape,"I have created an assembly  using Onshape following the tutorials, however the assembly is the only thing that does not show up in the Onshape Importer. I can see all the individual parts, but the full assembly doesn’t even show up on the list. Any ideas?Thanks!For some reason a restart allowed me to see the assemblies - unsure why! Fixed though!I have created an assembly using Onshape following the tutorials, however the assembly is the only thing that does not show up in the Onshape Importer. I can see all the individual parts, but the full assembly doesn’t even show up on the list. Any ideas?It could be because Sometimes, the importer may not immediately reflect changes made in Onshape.Powered by Discourse, best viewed with JavaScript enabled"
793,usd-composer-2023-1-0-beta-now-available,"Welcome to the USD Composer 2023.1.0 Beta release!USD Composer 2023.1 marks the first release of the Foundation Applications based on Kit 105. Omniverse is a platform built to allow you to easily extend the Omniverse platform and build any experience you desire. The Foundation applications are based on specific workflows to provide a great starting point into the Omniverse ecosystem. In the Foundation applications you can expect tested and extendable extensions that we’ll continue to improve, document, and allow for easy entry points to create your own custom Omniverse application.With USD Composer 2023.1 we focused on a few core updates: USD/Python upgrade, USD validation tools, Configurator workflows, XR, and improved real time rendering.Jump below for highlights and the full release notes!image975×548 28.8 KBWe’ve built a suite of core extensions, workflows, and USD operations that will allow you to quickly generate configurators for a variety of purposes. From retail to industrial design, you’ll be able to generate a series of variants that can be manipulated from action graphs, extensions, and a new no code UI interface.A set of tools designed specifically to allow you both generate complex USD variants, but also easily review. Further, we’ve improved USD variant support from other tools that export USD.The new Variant Editor is a robust workflow for generating a large amount of variants quickly. Further, this will allow you to edit and adjust variants made in external applications on USD assets.With the Variant presenter, you can group and review variants from both generated directly from USD. These groupings can help to easily lock or unlock items you want your customers to be able to review and approve.Now you’ve generated your variants, let’s make sure you can accurately manipulate and adjust USD variants directly with a brand new set of action graph nodes & workflows.New “No Code” UI makes it extremely easy to generate in viewport UI that can call both python, actiongraph and other actions. These are stored in USD for easy transport around the Omniverse platform.
image975×477 31.2 KBA new denoiser and a greatly improved MDL distiller allows for a greater parity between RTX Realtime and RTX Interactive. Now you can run at high fidelity and high speed with RTX Realtime to quickly visualize your asset.image975×548 58.9 KBMake sure your asset has the perfect material application with UV generation provided by the Scene Optimizer.Put it all together in our indepth Configurator sample available in the examples browser. This sample shows you how to take an asset filled with variants and generate a no code UI with action graph.image975×575 96 KBA core part of the Omniverse ecosystem is USD (Universal Scene Description). We’re dedicated to expanding and interacting with the USD community and providing options on the Omniverse Platform to generate USD compliant assets and delightful stages.With this update, we’ve upgraded to a new version of python and USD and provided tools to continue to allow for transporting content in USD across multiple platforms.We’ve updated python to version 3.10 and USD to version 22.11. With both these updates we’re continuing on the path to keeping the Omniverse Platform marching forward with the progress of the greater ecosystem.A crucial component of a healthy ecosystem are healthy assets. With the new Asset Validator, you can quickly and easily review and fix all of your assets. Generate your own custom rules or use the many rules already available.The USD ecosystem is becoming more strict in enforcing standards. In some cases this can break older assets authored previous to these changes. We have introduced an Asset Validator Extension to help fix these issues and create more robust assets. If you suspect something is wrong when you load an assets or you see a warning, please run the extension which can be found under the Windows > Utilities > Asset Validator sub menu.One core breaking change was to dangling MDL bindings, if your materials load in 2022.3 and not in 2023.1, this is likely the reason! For more information, jump to this link:
https://docs.omniverse.nvidia.com/prod_materials-and-rendering/prod_materials-and-rendering/materials.html#asset-validation-for-materialsimage698×616 76.9 KBA huge bonus to the latest release is the ability to directly edit USD Preview surfaces in the Material Editor. Further, we have also added the ability to bake a USD Preview surface from an MDL.image975×583 56.4 KBWe’ve deprecated the Omniverse Create XR application and moved all of that functionality to the core Omniverse Platform. You can now launch XR directly in USD Composer!image959×567 134 KBTake your existing USD stages and immediately jump into them with a supported VR headset, or stream your viewport to an iPhone or iPad. With the combined power of USD and Raytracing, your data needs less prep and requires no baking to see it immersively with photorealistic lighting and materials. This initial release in Beta will be focused on initial integration. Follow closely for new updates this year!SteamVR integration of VRImproved Tablet AR connection experienceBetter compositing of shadows in AR modeimage975×548 104 KBWe’re dedicated to providing standards and examples of content we feel best exemplifies a great USD asset. We’re providing a few new content packs and extensions to help you compile stages and content that we truly feel is simulation ready.With the new sim ready asset browser, we’re providing assets that are built with proper scale, physics, and materials to work directly in simulation. These are built to exacting standards that we’ll be communicating and evangelizing to the greater USD community in the coming releases.This first release comes packed with over 1,000 assets that you are free to use in the Omniverse Ecosystem. We have many more assets on the way and they’ll automatically update in your SimReady Browser!image1230×420 83.7 KBEver worry about your asset unit type being miss-matched? We’ve got a solution with the Metrics assembler. Now, when you drag in an asset with a units type differing from your stage, we’ll automatically correct the scale and orientation to make sure it’s world scale matches.image975×352 34.8 KB
image975×123 34.4 KBA great pack of automotive assets that will give you a great starting point to setup any automotive design inside the Kit platform. This includes:A set of high quality dome maps and stages provided by CGBackgrounds.A new adaptable Domelight MDLAutomotive Uber MaterialA full pack of automotive materialsNew triplanar and box mapping MDL nodesThe Koenigsegg Ragnorak example stage.We continue to believe that the future of the digital workspace is a truly collaborative environment. As our workflows move more and more to the cloud we’ve doubled down on providing workflows that can be truly world wide, realtime, and remarkably effective.Live Collaboration is now enabled for reference and payload prims directly. Users can receive a live session in USD Composer on any USD file that is loaded as a reference or payload. This is a much more articulated and powerful way to iterate on specific assets as it allows for the USD Layers panel to be mainly used for aggregation and layout of assets, while the assets themselves can be handled with Live Prims as receive only assets.Here’s some of the key highlights.Quick join live sessionsShare and join session by linkOffscreen pointers for usersFollow userLive reference/payload primsAuto reload for prims and layersConsistent color allocations for user selectionsUI improvements in the stage view and viewport camera lists that indicate other collaborators.Updated and optimized name labels / name iconsWe’re continuing to improve the core animation runtimes and abilities for the Omniverse platform. With this release, we’ve added additional navigation methods with the new Navigation Mesh feature and a full Stage Recorder.Animated characters can now autonomously move around environments using the new Navigation Mesh (“Nav Mesh”) feature. The Nav Mesh is a pre-computed mesh that represents the traversable area for an agent, and unlocks the ability to build smart agents for simulation.A robust method for capturing all simulation and live session movement for later playback from disk or layer. Sessions can be recorded to disk or layer and allows users to record user input and specific time segments from the stage.image704×851 92.3 KBLots more to come in the full release notes as they post!
https://docs.omniverse.nvidia.com/app_composer/app_composer/release_notes.htmlPowered by Discourse, best viewed with JavaScript enabled"
794,omniverse-alias-connector-v201-0-release,"We are excited to announce that the latest release of Alias Connector 201.0 is on the Omniverse Enterprise Launcher!New Features:Bug Fixes:Read the full list of new features and functions in Alias Connector in the full release notes [here ]Documentation can be found [here]Powered by Discourse, best viewed with JavaScript enabled"
795,will-isaac-sim-run-on-cloud-h100,"---------is h100 supported?Hi @Andrey1984  - Currently Isaac Sim is not supported on cloud h 100.is it supported on GPU H100 [ not cloud]?Powered by Discourse, best viewed with JavaScript enabled"
796,im-trying-to-visualize-room-air-flowing-into-the-ventilator,"I’m trying to visualize room air flowing into the ventilatorI have already done a tutorial where prim and preset fire interact using flowAt first, I tested it by placing a fire on the cloth deck chair demo.
But the fire and wind from the fan didn’t interact.The next thing I tested was forcefieldwind demo, but likewise wind and fire didn’t interact.I need a good way to visualize fire and wind interacting or room air flowing into a vent.Powered by Discourse, best viewed with JavaScript enabled"
797,enable-camera-in-headless-model-experience-memory-leak,"Hi @kellyg ,I set enable_camera to True in Repo OmniIsaacGymEnvs, task FrankaCabinet without changing anything else and found out that the memory keeps increasing as training goes on. The training stopped when Python got killed due to a memory error. Here are some screenshots:
image1460×371 251 KB
image1476×469 127 KB
image1471×377 107 KB
image1454×370 112 KBIs there a bug in this module? And how can I fix it?Best,
ChayApologies, I’d like to provide some additional clarification. Currently, the environment is being executed on a server within a docker container, and as a result, there is no access to a monitor. To ensure the successful execution of the script, the configuration has been set with headless: TrueHi Chay, thanks for reporting the issue. I tested the FrankaCabinet environment with enable_camera set to True using our latest internal build and did not observe an increase in memory. It is likely that the issue has already been fixed and will be made available in the next Isaac Sim release.Hi kellyg,Upon reviewing my repository, I found that another env was imported via the task_util.py which resulted in this memory issue.These two lines of code I believe are responsible for increasing memory.Though I found another way to create multiple cameras, I’m still curious about how this would lead to the memory leak problem.,Best,
ChayPowered by Discourse, best viewed with JavaScript enabled"
798,unable-to-connect-to-nucleus-localhost,"Hello,I am unable to connect to localhost on my computer when I “Add Local Nucleus Service”.
I have tried many potential solutions reading other posts with similar issues on this forum, but nothing worked.I have tried:Additionally, most of the links posted by users on the forums which link to Nvidia resources are not working.Platform:Can you please post an image of the nucleus tab in launcher.Also can you please go to the hamburger menu under the nucleus tab and go to settings, that should launch system monitor. Can you please validate all the services are running?Is this an update or new install of nucleus?I have been experiencing the same issue since last Friday. For me, I am getting “This site can’t be reached” error when selecting the setting under the nucleus.image1100×217 13.6 KBBesides, I’ve been unable to access any documentation related to the Omniverse extension.After relaunching the Omniverse launcher, I’m now encountering a backend error from localhost:3080.image1506×523 21.2 KBHi @dlindsey,Here is the image of my Nucleus tab in Launcher:
Screenshot 2023-07-17 1053472571×1378 165 KBAll of the services are green when running the system monitor.As far as I can tell this is the latest install of Nucleus.Hi @dlindsey,I have an older version of Local Nucleus Server running on another PC and that seems to work fine (v.2022.4.2). Is there a way where I can downgrade Nucleus from v2023.1.0 to this older version until this issue gets resolved?I’m having the same problem… I tested 2 month ago on my Windows PC with version 2022.4.2 but when I switch to Linux and installed 2023.1.0 it’s showing the same problem as yours.The documentation issue should be fixed now and docs should be accessible now.As for this error, it looks like something didn’t install completely. Please uninstall and reinstall nucleus.Also please validate your are installing nucleus on a local drive.Can we please get the logs for the current version having issues?You should be able to install the old version by running this commandomniverse-launcher://install?slug=nucleus-workstation&version=2022.4.2Hi,
Thanks for the feedback. I looked at the admin file and found for some reason it didn’t save my username and password but only saved the default name: admin, psw: admin. After trying this I could finally log in yesterday… sorry I didn’t save the log.Hi @dlindsey,The solution that @yujing.liu provides allows me to log into the localhost server.
No matter what credentials are entered at server creation, the localhost only accepts:I am not sure where to find this admin file and locate the saved passwords to confirm @yujing.liu post, but the solution works.I will stick with the latest version of Nucleus and wait for a future fix to resolve this issue.On a related note to @Ripeg issues with broken links, under the Omniverse Navigator the “Link to Drivers” currently returns a 404 error. I tried to use this link to check for updated drivers.Attached are all the log files.
logs.zip (14.2 MB)I am having the same problem. I never get asked for a server name, I click Add Server, and it asks me to authenticate.image1056×644 41.9 KBI went through the setup once, then never could connect to the server, so I uninstalled / reinstalled Navigator. How do you uninstall the server so it starts over?Edit: It was asking for Server Name, I can’t read gray text on a white background. I opened magnifier and saw it was asking for Server Name, and admin / admin worked.Powered by Discourse, best viewed with JavaScript enabled"
799,internal-camera-problem,"I want to know how to output the camera internal parameter information in the simulation environment through code, such as fx, fy, cx, cy, camera factor and other parameter information.Hi @orangechen - Does this forum post help answer your question?Set exact intrinsic matrix of camera fails - #2 by rthakerPowered by Discourse, best viewed with JavaScript enabled"
800,how-to-speed-up-simulation-in-isaac-sim,"Hello!I have attached a simple script for a franka pushing a cube. I wanted to speed up the simulation by changing the physics step size and min_simulation_frame_rate. However, there is no change in the speed of the simulation.In addition, I am not quite sure whether stepping is using gpu or not. When I enable gpu dynamics from physics scene in the gui, the simulation gets slower.Here is the code and runtime information.Screenshot from 2023-07-24 16-49-59965×490 55.9 KBI would be grateful if you could give me any hints on how to speed up the simulation without affecting the physics. Thank you!Hi @hailegashaw12  - the speed of the simulation is primarily determined by the physics step size and the minimum simulation frame rate. However, these settings can be influenced by other factors such as the complexity of the scene, the number of physics objects, and the computational resources available.Here are some suggestions to speed up your simulation:@rthaker just wondering about the GPU dynamics piece … I’ve experimented with various simulation parameters (use_gpu_pipeline, use_flatcache, use_gpu, …) and tried other tricks (like overwriting the omni.physx GPU setting and increasing the thread count), but my code still runs slower on GPU than CPU.I’m confused because, according to nvidia-smi, my GPU (at ~70% usage) technically has greater remaining capacity than my CPU cores (often maxing out at 100%). My intuition says that GPU should be running faster; are there any other settings I should toggle or possible sources of slowness that I haven’t yet looked into?Additionally, is there a set_min_simulation_frame_rate (or similar) method for World? I haven’t been able to find this setting in the source code of at least Isaac Sim version 2022.2.1Thank you for your swift reply, @rthaker.My workstation spec:
GPU - GeForce RTX 3080
CPU - Intel(R) Xeon(R) W-2295 CPU @ 3.00GHzMy scene is a very simple scenario with one robot and a cube object. As @bubblefish said, I also think GPU should be running faster.Additional experiment:With 100 parallel robots with single object each → the simulation speed is same with or without gpu dynamics enabled.With less than 10 robots → enabling gpu dynamics made simulation slower.At this point, it seems that enabling gpu dynamics is no use. I don’t think this could be the case and may be I am doing something wrong. Kindly, are there any other settings to look into?Hello - Basically GPU simulation becomes faster only for larger environments. With smaller simulation size, there is a fixed cost for the GPU simulation.If you have one Franka then most likely CPU will be the fastest, maybe even a single threaded (set num threads for physics to 0) would be the fastest.Powered by Discourse, best viewed with JavaScript enabled"
801,help-regarding-starting-with-nvidia-omniverse,"Hello,
This might be a silly question but I am just trying to understand Nvidia Omniverse and see if that would be helpful for the company I work for. Especially on the thermal simulation side involving Nvidia Modulus as an extension.
Here are some of the questions I have:I am sorry for these questions. If this is not the right place for these questions, can someone point me to a forum where I can get more help. Any help is greatly appreciated.Thanks,
ManishHi, your assumption is correct, the viewport in Omniverse does require an RTX capable GPU as it heavily leverages the RT cores.We have announced Omniverse Cloud which will provide cloud based applications, this will provide remote desktop capabilities, streaming to any browser. Omniverse Cloud is still in early testing but if you click the “get in touch” you can submit your request for access.Hi,
Thank you for your response. I did submit the request twice. Never heard back from the support team.If you would like to DM me the email you used I’d be happy to check in with the cloud team but I believe only the cloud nucleus is currently being evaluated externally.An alternative to waiting for the cloud offering is to simply run an Omniverse workstation on AWS. You can find more information on that option here: Deploy an Omniverse Virtual Workstation on AWS (Windows) — Omniverse Nucleus documentation (nvidia.com)Powered by Discourse, best viewed with JavaScript enabled"
802,cannot-insert-duplicate-registry-entry-for-usd-layer-when-referencing-the-same-object-twice,"Hello all,I am having issues in Omniverse Code when I attempt to reference the same file in a scene in more than one place.  If I do the following in vanilla USD and usdview, it works fine:However if I do more or less the same in Omniverse (via the Script Editor):I get the following error:I can even open the file test_references.usda in OV Code with no issues. Did I make a mistake or could this be a bug? Is there an “Omniverse-safe” way of doing this?Seems like the issue might actually be the use of relative paths. The code I posted previously works when I use absolute paths. This also seems to work.Hi Mason. I see someone on usd-interest reported a similar issue back in April using a similar version to Omniverse. I wonder if this is something fixed in newer versions of USD. I’m guessing you’re using USDView from the launcher? I’ll check to see if anyone knows more about this.Huh. I actually can’t repro it.Strange. I installed new versions and still am able to see the issue in both Code 2022.1.3 and Create 2022.2.0. I am on Ubuntu if that makes a difference.To check in “vanilla” USD I actually used the pxr and usdview included in the nv_usd install in a Conda environment. I tried a few different USD files as well and had the same issue.I also found that post on usd-interest and downloaded their files. They seem to open fine in both usdview and Omniverse. If I manually add two references to the same file in OV it also works fine. The only issue crops up for me while scripting, whether I’m using the pxr or omni.kit.commands methods and only when using relative paths.Thanks for the additional info. Seems like a newer USD version isn’t necessarily the solution. I’ll check with the devs to see if they have any more ideas.I thought I had a potential solution and started drafting a post, but I realized as I was writing it and testing on my end that I didn’t understand the problem correctly. In case it’s helpful though, I ran your snippet with an adjustment to ref_path on Code 2022.1.3  and it worked fine for me. Also for the record I am on Windows 10, in case it is an OS-specific problem.I get 4 cubes nested inside of 4 Xforms as expected.
image2801×841 109 KB
I tried it on Code 2022.1.3 and Windows 10 and was able to reproduce the issue. However, I believe I discovered what was going on.It seems this only occurs whenever I have opened a brand new file and have not saved yet. If I open a USD file (such as cubetest.usd) and run the script, I get the expected result. My guess is that since relative pathing is based on the opened USD file, if we haven’t explicitly opened any USD file it does not know how to define references in Omniverse.Thanks Mason! I’m trying to get a hold of a Linux machine to try reproing again.hi,@ mati-nvidia
in the 2022.2.1,I got the same error in ubuntuPowered by Discourse, best viewed with JavaScript enabled"
803,use-rep-distribution-uniform-and-rep-distribution-choice,"Hello,I want to position one object in one of 3 areas randomly.
For the implementation, I combined rep.distribution.uniform() and rep.distribution.choice(). But the program just stops without an error message.To reproduce that issue, I created the following example:Using the scatter function is not an option, as the objects rotatation won’t vary.Happy about any information.Hello @hugo.81243 ,Unfortunately, randomizing distributions is not currently supported. Rest assured we have this feature high on our roadmap and understand its importance. Please let us know if this is a blocker for you and we can try to work together to determine a workaround that you can use in the mean time.Powered by Discourse, best viewed with JavaScript enabled"
804,generating-lens-flares,"Hey there,I was wondering if lens flares had been added into the Omniverse. I saw this old post from 2021 (Lens Flare and sun disk shape) and was wondering if there had been any development on this.Thanks in advance,Cheers,AntoineHi @ARi_31 In Render Settings tab → Post Processing there FFT Bloom option, there are a lot of settings to play around with, give it a try!Cheers,KarolPowered by Discourse, best viewed with JavaScript enabled"
805,loading-usd-file-with-error,"Hi everyone,
How to fixed the following picture’s error?
The message said that my usd to mdl with an error but I can’t find out where the problem is.
image1223×639 27.5 KB
Hi @vic-chenThis post may help:Hi @toni.sm ,
Thanks for your help, I have a question about the convert command.
1.usdcat -o whatever.usda input.usd
2.python process.py whatever.usda ← where is the process.py file location?
3.usdcat -o input.usd whatever.usdaIf I don’t execute 2 and 3 commands, will the convert process work?Hi @vic-chen@eliabntt94 Can you please shed some light on this discussion?Hi @toni.smThe main issue is that the new Isaac versions give errors on various fields while loading some (perhaps old) USDs. I’ve verified this also with the one-before-last omniverse blender connector and with various old USDs files in my possess.There are some fields (as the one share here) that are written as int within the USD file while the system expect float.
Moreover, even if the field is changed on float, if the number has no .xxx (decimal part) in it, it will trigger the same error.Idk if it’s a problem of the converter, of the MDL file itself, or of the USD loader (in Isaac2021.2.1 there was no such problem)To solve this what i did was to convert my binary USD files to text, process them, and reconvert them back.@vic-chen command 1 is to convert the binary USD to text USDA file. Command 2 is just an example on how to process the text file, this is a blueprint that you can follow, I’ll push an updated script sometime soon. At that point you can either use the new USDA file that you process with python (or manually), or convert it back to USD (command 3)Hi @eliabntt94Thank you for clarifying :)Hi @eliabntt94 and @toni.sm ,
Thanks for your help, I tried to convert my USD files, but unfortunately I still have not covert its success.
Because there are too many objects in my project, I’m not sure which one I should convert it. I found that even thought I didn’t convert the files, the project seems to can work in Isaac sim.
I’ll keep trying which USD file needs to be converted. Best regard to you.With a bash script you can process all USD files iteratively. See GRADE-RR/process_env_paths.sh at main · eliabntt/GRADE-RR · GitHubHi @eliabntt94 ,
Thanks for your reply, because my OS is Windows I need to change some path in your script. I can’t find folder path in the USD folder as the following picture shows.

image1196×782 37.1 KB

The image below shows my folder structure.

image854×506 28.3 KB
Best regard!Hi, for Windows I do not know what are the commands, I did not tried it out there. Those two (L10-11) come out from the installation of https://github.com/PixarAnimationStudios/USD that has the conversion tool (usdcat) and are needed to tell the system that usdcat exists and its location.You can see where I got those lines here. Those should be printed also for windows since the installation script is the same.
I would probably guess the path is something like C:\USD\install\lib\pythonPowered by Discourse, best viewed with JavaScript enabled"
806,reaction-video-made-in-omniverse,"I made this animated reaction video using omniverse real time mode with ray-tracing, I love how powerful it has gotten in the recent updates.https://www.youtube.com/watch?v=gWgwVab-qywPowered by Discourse, best viewed with JavaScript enabled"
807,documentation-of-omni-ui-on-web-is-not-redirected-to-a-newer-version,"I recently explored docs on omni.ui. I opened a dozen of tabs and left them intact. In one week they all render as unavailable with an error message, like:For example, I had a URL opened https://docs.omniverse.nvidia.com/kit/docs/omni.ui/2.15.4/omni.ui.html.
With a new release, version 2.15.4 become obsolete and page opens no more.
I have to manually try URL https://docs.omniverse.nvidia.com/kit/docs/omni.ui/2.15.5/omni.ui.html - it does not open.
Then I tried URL omni.ui — omni.ui 2.15.6 documentation - hooray, it works.
Is it possible to create a redirection, so that older documentation web pages are redirected to the latest available version?
Thank youPowered by Discourse, best viewed with JavaScript enabled"
808,real-behavior-of-a-lifting-mast,"Greetings,
I am trying to get a real behavior of a lifting mast (hydraulic cylinder) of a forklift. The Isaac Sim forklift is not holding the position after i stoped the force. When i increased the dampening, the lifting mast is moving slower down. And when the dampening is increased to one Million, it is nearly standing still. But this doesn’t look like a real behavior.
Does someone has already a solution for this problem?Hi @xadarius,Are you just applying a force plus using a velocity joint drive for damping?If yes, you should be using a position joint drive. It will do PD position control on a target position that you set.PhilippThank you,
that works.Powered by Discourse, best viewed with JavaScript enabled"
809,bug-in-get-intrinsics-matrix-of-camera-isaac-sim-python-api-wrong-vertical-aperture,"Isaac Sim 2022.2.1-rc.14 has a bug in get_intrinsics_matrix() where vertical_aperture incorrectly takes the value of horizontal aperture instead of vertical one:
exts/omni.isaac.sensor/omni/isaac/sensor/scripts/camera.pyThis impacts the correctnes of outputs for functions using the intrinsics (e.g.  get_image_coords_from_world_points and get_world_points_from_image_coords).Hi @nitr0x1de  - Thank you for reporting this. I have forwarded this to right team. They are looking into it.@rthaker on my side, I can confirm that changing to the expected vertical aperture produces correct results for the impacted functionsPowered by Discourse, best viewed with JavaScript enabled"
810,dynamiccuboid-falling-through-floor-when-make-kinematic-is-false,"What could be the reason for DynamicCuboids falling through the floor when make_kinematic is set to False? However, in the case of the last env_id (if there are 256 environments, it would be env_255), the DynamicCuboid contacts and correctly rests on the floor as intended. Could it be that the way objects are added to the environment is incorrect?This is how I add the objects in the environment.And regarding the physics settings added with apply_articulation_settings, I have them set as follows.Since there were reported issues with ArticulationView in IsaacSim2022.2.1, I am using the Docker image of IsaacSim2022.2.0 instead.Can somebody please tell me what causes this problem?
Thanks in advance.Hi @smakolon385 - The issue you’re experiencing might be due to the physics settings or the way the objects are added to the scene. Here are a few things you can try:Hello @rthaker,Thank you for your helpful suggestions! I followed your advice and made the following changes to resolve the issue:After implementing these changes, I’m happy to report that the issue has been successfully resolved. The objects now interact with the floor as expected, and the collision problems have been eliminated.Once again, thank you so much for your valuable suggestions and guidance.This topic was automatically closed 14 days after the last reply. New replies are no longer allowed.Powered by Discourse, best viewed with JavaScript enabled"
811,animated-m-v-made-in-path-tracing-mode,"Here’s my the animated music video made in blender and rendered completely in Omniverse path-trace mode.WHAT?!  THIS IS AMAZING!!  I also love the song!  Sharing with the team!!Also… LOVE THE FIRE!!Thank you so much ^^ Really appreciate the support and much love to the omniverse team <>Very nice!!!Powered by Discourse, best viewed with JavaScript enabled"
812,trying-to-cache-ray-tracing-shaders-inside-isaac-sim-container-using-simulationapp,"I am trying to set up a container image based on Isaac Sim that has shaders precompiled so I can run simulations instantly. I tried the following:What I planned to do is to compile the shaders once and then commit the changed Docker container to an image containing the compiled shaders.Any suggestions welcome
BrunoYes. This is the recommended way to cache your shaders. Run the container then save it and run again.Well I thought so, too. But as I mention in step 7, this does not seem to work.I call the SimulationApp from within Python and close it when it is done starting up, expecting that the shaders are somewhere persisted. Unfortunately, that does not seem to be the case. If I call the SimulationApp again afterwards, it will start compiling shaders again…Hi can you reproduce this same issue outside of docker too?The problem has somehow disappeared. I can now commit a Docker image with compiled shaders. I cannot say what the problem had been.Thanks everybody.This topic was automatically closed 14 days after the last reply. New replies are no longer allowed.Powered by Discourse, best viewed with JavaScript enabled"
813,control-the-car-forward-by-torque,"Hello everyone, I am using the differential car jetbot. I have two methods. One is through the omnigraph method, and the other is to write the control in the form of python code. But in the end, I couldn’t make the car move. Can you help me to see what’s wrong with the two methods? How to solve it?Any help will be appreciated.
Below is my omnigraph graph2023-07-12 18-09-13 的屏幕截图1920×1080 323 KBthis is python code:from omni.isaac.examples.base_sample import BaseSample
from omni.isaac.core.utils.types import ArticulationAction
from omni.isaac.core.utils.nucleus import get_assets_root_path
from omni.isaac.core.utils.stage import add_reference_to_stage
from omni.isaac.core.robots import Robot
import numpy as np
import carbclass HelloWorld(BaseSample):
def init(self) → None:
super().init()
returnNow, I can make the car move by applying force to the wheels, but sometimes the car bounces up during the movement, why is that?
2023-07-15 17-05-24 的屏幕截图1920×1080 174 KBNow, I solved the problem of the car bouncing up, but I encountered a new problem. When the car first started running, it walked in a straight line and then started to turn in a circle. I tried to use PID control, but it didn’t seem to work.Can someone help me? @toni.sm can you help me？code show as below.from omni.isaac.examples.base_sample import BaseSample
from omni.isaac.core.utils.types import ArticulationAction
from omni.isaac.core.utils.nucleus import get_assets_root_path
from omni.isaac.core.utils.stage import add_reference_to_stage
from omni.isaac.core.robots import Robot
from omni.isaac.dynamic_control import _dynamic_control
from omni.isaac.core import SimulationContext
import numpy as np
import carbclass HelloWorld(BaseSample):
def init(self) → None:
super().init()
returnSorry for the delayed response! I am trying to parse your code.  I would recommend against building extensions off of the current BaseSample.  Please checkout the extension templater under the Isaac utilities menu, as it’s more clear and easier to modify.When you run the jetbot does it drive in a tight circle or a broad (large) circle?  that is, is the deviation from “forward” severe or minor?edit: I see where you apply the action now (self.target).  I’ll see what it looks like on my sideThank you for your reply.when I have set the stiffness and damping to zero，and the damping and stiffness in the advanced properties of the revolving joint tab are also zero.Car no longer bounces during simulation.And when I modify the code, the car can also drive at the specified speed. But I have encountered a new problem now. When I imitate the example of Hello Robot, I need to manually modify the damping property in the diver of the car to zero every time before running the simulation. I want to use python.sh to run completely through the code, how to modify it? Below is my later code.
from omni.isaac.examples.base_sample import BaseSample
from omni.isaac.core.utils.types import ArticulationAction
from omni.isaac.core.utils.nucleus import get_assets_root_path
from omni.isaac.core.utils.stage import add_reference_to_stage
from omni.isaac.core.robots import Robot
from omni.isaac.dynamic_control import _dynamic_control
from omni.isaac.core import SimulationContext
from omni.isaac.franka import Franka
import numpy as np
import carbclass HelloWorld(BaseSample):
def init(self) → None:
super().init()
returnAnother question, I want to add a robotic arm to the car, how to combine them?Thank you very much.@mgussertI don’t follow.  do you mean you have to manually set a damping parameter to zero before pressing play?  which parameter in your code?  also, I could not load your video, sorry!Yes, because I want to control the wheels of the car directly through force control, so that the car can move forward. Then I saw someone reply in this post saying “For direct joint effort control, set both drive parameters to zero, and use the Advanced properties of the revolute joint tab to define a stiffness / damping (both values need to be above zero).”Torque Control Model Parameter - #9 by rgasoto. but I want to implement the modification of this parameter directly in the code, so I don’t need to manually modify it every time. And for the video problem, I’m sorry. I’ll try uploading it again. Thank you for your prompt reply @mgussertOr try to open this link to see if you can see the video, thank you. Isaac sim - YouTubeWhy not modify and save the USD directly? Why does it need to be done in code at all?  The parameters you want to modify are properties of the joint drive, and are defined as part of the asset.image1436×1213 212 KBI think the reason why you need to do it every time you run the sim is because the asset on the stage is configured one way, and that’s what gets simulated when you start.  If you modify the parameters before the sim is started, do you still have to set them every time play is called?I did some tinkering and I found a couple things that we can improve with the asset and the wheeled robot.  Consequently, I have a working extension for you that you might find useful!jetbot_extension.rar (19.7 KB)To use the extension, unpack it to a directory, and then in Isaac-Sim go to window->Extensions and click on the gear to open settings.  Add the path where you unpacked the extension to the list of discoverable paths, and you should be able to “enable” it.  click on “jetbot_extension” at the top menu bar and then press the load button.  if you press the “run” button you will see the Jetbot drive forward.some things you should know!this extension was made with the templater tools in the Isaac Utilities menu.  There are two files you want to pay attention to, scenario.py and ui_builder.py.  In the ui builder we define the function that loads the stage and the assets _setup_scene.  In the scenario.py we define what we DO with the assets, and in this case we set the drive mode to the jetbot to “efforts” and then apply a small action to go forward.The call we needed here was the switch_control_mode of the articulation controller in the robot class.  I “cheated” a little bit and referenced a private member of the wheeled robot to get this working.  we should probably expose that.the jetbot drifts to the right!! why!??!?!? it doesn’t appear to be an asset issue (you can verify this!  load the jetbot asset and set the velocity targets on the joint drives to 360 and the jetbot drives forward without drifting).  currently investigatingThank you again for your question and sorry it took so long to get back to you.  I hope this helps!
Gusthanks for your reply.Maybe I don’t know enough about Isaac sim, after I enable it, I can’t open jetbot_extension normally.
2023-07-27 15-13-49 的屏幕截图1920×1080 415 KB2023-07-27 15-15-25 的屏幕截图1920×1080 453 KBFor point 3 you mentioned：the jetbot drifts to the right.This is the problem I encountered before. But I also don’t understand why it drifts.Later, the error of the speed is used as the input of the PID control. After the applied force is used as the output, it no longer drifts.
But now when I put an arm on the car and combine the arm with the car and apply force to the wheels, the car doesn’t move at all.I am using the omni.isaac.dynamic_control api.
Thank you for the ideas you gave me on the previous question, that is very useful.@mgussertI have a local install of ubuntu 20.04 that I installed our latest release on.  After extracting the extension and loading it, I am able to run without issue.  I also don’t see that viewport error :(jetbot_extension_gui2482×1405 396 KBIt looks like you extracted and enabled the extension fine.  Does the viewport error happen when you enable? I don’t see the jetbot_extension tab anywhere so I’m assuming this error killed it.  if you disable autoload, restart isaac sim, and then load the extension (without loading anything else) does this error still occur?I disable autoload, restart isaac sim, and then load the extension (without loading anything else)  this error still occur.Maybe there is something wrong with my previous installation.But this question is not the most important question for me now. I have another problem now, I need to build the manipulator on the car to do a simulation experiment, but when I combine it as mentioned in other posts on the forum,like this Combining UR10 and Smart Transport Robot (STR) to get a mobile manipulator - #3 by Hammad_M result is that the two wheels of the car rotate at the specified speed. However, the car is in the original Man, just like the wheel slipping in real life, I don’t know if I did something wrong combining the two USD files or where, thank you very much and I am very sorry to reply you now. @mgussertHi @1365351984  - For any new questions, I would request you to start a new thread instead of combining them into one.Also, you can close this one if this is no longer an issue.Powered by Discourse, best viewed with JavaScript enabled"
814,dds-plug-in-no-longer-available,"After the most recent photoshop update the NVIDIA dds. plug-in no longer works. I reinstalled to no avail. It’s no longer a save option on PS. Anyone know a fix, or an alternative dds. plug-in?Hello @nrbailey2490!  I’ve contacted the dev team about this.  I’ll post back here as soon as I hear back!Hello @nrbailey2490!  Am I correct in assuming you are referring to this tool: Legacy Texture Tool?   If so, this tool was created many years before Omniverse was developed.  No one on the Omniverse team has worked on this tool, so I had to do some digging…Natively, Photoshop does not support DDS and it typically requires a 3rd party app to handle those files.  One such tool is our Legacy Texture Tool.   I was able to find the development team for that tool and I am reaching out to them for more information on this.  I will update this post as soon as I have more to share.Hi @nrbailey2490! This is Neil from the Texture Tools Exporter development team. I believe in Photoshop CC 2022 and 2023, the new version of the Texture Tools Exporter and the legacy version appear in the File > Save a Copy… menu instead of the File > Save As… menu, like this:However, it’s also the time of the year where many systems have both Adobe Photoshop 2022 and 2023 installed at the same time, and the installer might have installed the plugin to 2022’s directory while you’re trying to use 2023. Reinstalling the plugin should work; all you’ll need to do is change the Adobe Photoshop 2022 folder name here:to Adobe Photoshop 2023:If you restart Adobe Photoshop 2023, the plug-in should now appear.Finally, there’s a few ways to check that it’s installed in the right place. One is that you should see nvtt_export.8bi inside the folder C:\Program Files\Adobe\Adobe Photoshop [your Photoshop version]\Plug-ins\NVTT.  You should also see the plug-in appear in Photoshop’s Help > About Plug-ins… menu, like this:Hope this helps - let me know if I can answer any further questions!Update: The 2023.1.0 version (released February 24th, 2023) fixes this install path issue! The installer now first looks for Photoshop by iterating over the C:\Program Files\Adobe directory. Previously, its first choice was to look at the application registry key for Photoshop.exe – but it turns out sometimes this registry key points to old versions and sometimes odd locations. If it can’t find Photoshop in C:\Program Files\Adobe, it now falls back to the registry key; if the registry key isn’t there, it’ll prompt for a location.The plugin is not working nvidia stuff. It’s not starting! You can’t load a dds file in photoshop!BECAUSE PHOTOSHOP 2023 CHANGED THE FOLDER FROM 2022 TO 2023.Your installer for dds plugins when started CAN""T FIND PHOTOSHOP because IT IS BLIND AND DUMB .And don’t get me the “it’s free” BS. IT’S NOT! You locked the whole world in your BS you and BS “Adobe 23 programs needed to run in background big software company”.Maybe you have time to get to it next year after you finish buying and selling and buying back your stocks  a billion times.By the day you and adobe are less and less relevant and more and more just some annoying gate keepers.PS I don’t care about your answer. I will cancel PSP subscription.You right dude, i just copied  NVTT folder and DDS.8bi file from PS 2022 Plugin folder to PS 2023 plugin folder, and it works like a charm. The “experts” above are talking about other tool, not about plugin:D which is the reason to use Texture tool if all it does is save dds file texture to other format, you can do it with lot of software. but plugin is another story, you directly can open dds file in PS, and edit dds texture, then save it in different format, or as edited dds.Powered by Discourse, best viewed with JavaScript enabled"
815,unable-to-use-webrtc-when-i-run-runheadless-webrtc-sh-in-remote-headless-container,"I had the same problem.
I use native headless and connect it from the omniverse streaming client successfully, but websocket and webrtc show a gray screen after pressing the start button.Hi @tuu4208 , can you share the error logs with some more information?I have exactly the same problem.When I run ./runheadless.webrtc.sh on a container, the browser shows the same grey screen as above but it does nothing when I press the play button. This is the same even if I try to access the docker from the same PC (e.g., localhost).Some further information:Are there some further WebRTC settings needed specifically for the containers?In websocket modes.The final reason is that the webrtc mode communicates using fixed ports and does not provide a way to modify the configuration. These ports were not open in my container environment, which caused this problem.I think WebRTC method chooses a new port if the default one is taken (e.g., [omni.services.transport.server.http.server] http server was meant to start on 8211 but port is taken, starting on port 8034 instead).  All the ports WebRTC chooses should be open since we use the --network=host command while running the containers. Am I wrong?Did you solve the problem? If yes, how did you do exactly?
image1268×384 37.1 KB
My specific problem is that the container port mapping to the outside is random, so hard-coded port mapping to a different port causing the problem. Later, I used a virtual machine to ensure that ports are accurate, which can solve this problem.Maybe I don’t have the same problem (e.g., the websocket socket is directly mapped outside correctly: I guess should be the same with WebRTC).@Sheikh_Dawood @rthaker  do you have any info regarding this issue?I have exactly the same problem. Any updates?Any updates on this topic? @Sheikh_DawoodWhen I run the webrtc with the --/app/livestream/logLevel=verboseoption, I get the following errors during startup which I believe to be related to webrtc:2023-06-01 07:32:09 [5,265ms] [Warning] [omni.services.transport.server.http.server] http server was meant to start on 8211 but port is taken, starting on port 8068 instead
[5.298s] [ext: omni.kit.livestream.webrtc-0.1.0] startupActive user not found. Using default user [kiosk]2023-06-01 07:32:09 [5,288ms] [Fatal] [carb.livestream-rtc.plugin] StreamSDK: NvEncUtil: #6(M)[2023-06-01 07:32:09,419]=07:32:09=00000{1493626207712371803}&CC7C5461&        Computing SPS/PPS headers.
2023-06-01 07:32:09 [5,288ms] [Warning] [carb.livestream-rtc.plugin] StreamSDK: VideoFrameContext: #7(W)[2023-06-01 07:32:09,419]=07:32:09=00000{1493626207712371803}&0564BB28& Couldn’t find VideoFrameContext for this thread.
2023-06-01 07:32:09 [5,288ms] [Fatal] [carb.livestream-rtc.plugin] StreamSDK: AdapterContext: #9(M)[2023-06-01 07:32:09,419]=07:32:09=00000{1493626207712371803}&1E959A86&   Initializing Adapter context
2023-06-01 07:32:09 [5,288ms] [Fatal] [carb.livestream-rtc.plugin] StreamSDK: AdapterContext: #2(M)[2023-06-01 07:32:09,419]=07:32:09=00000{1493626207712371803}&F0E847F4&   Initialized context for adapter 0: 1280 x 720 @ 60.0 Hz
2023-06-01 07:32:09 [5,288ms] [Fatal] [carb.livestream-rtc.plugin] StreamSDK: AdapterContext: #4(M)[2023-06-01 07:32:09,419]=07:32:09=00000{1493626207712371803}&9A64D99C&   System is NOT co-proc.
2023-06-01 07:32:09 [5,337ms] [Fatal] [carb.livestream-rtc.plugin] StreamSDK: AdapterContext: #5(M)[2023-06-01 07:32:09,469]=07:32:09=00049{1493626207712371803}&21C9B327&   GPU memory usage: used = 12972.062500, free = 35696.500000 MB, total = 48668.562500 MB
2023-06-01 07:32:09 [5,338ms] [Fatal] [carb.livestream-rtc.plugin] StreamSDK: AdapterContext: #6(M)[2023-06-01 07:32:09,469]=07:32:09=00000{1493626207712371803}&DD40CDEB&   Initialized CUDA for device ‘NVIDIA RTX A6000’ (SM 8.6) in compute mode ‘CU_COMPUTEMODE_DEFAULT’.
2023-06-01 07:32:09 [5,338ms] [Fatal] [carb.livestream-rtc.plugin] StreamSDK: VideoEncoder: #8(M)[2023-06-01 07:32:09,469]=07:32:09=00000{1493626207712371803}&8BAD092A&     Initializing NvEnc10VideoEncoder.
2023-06-01 07:32:09 [5,338ms] [Fatal] [carb.livestream-rtc.plugin] StreamSDK: NvEnc10VideoEncoder: #9(M)[2023-06-01 07:32:09,469]=07:32:09=00000{1493626207712371803}&2A6D6983& Using NvEnc header version 10.0.
2023-06-01 07:32:09 [5,342ms] [Warning] [carb.livestream-rtc.plugin] StreamSDK: NvEncUtil: #0(W)[2023-06-01 07:32:09,474]=07:32:09=00000{1493626207712371803}&7F9BD44D&        Video format not supported, skipping preset.
2023-06-01 07:32:09 [5,343ms] [Warning] [carb.livestream-rtc.plugin] StreamSDK: NvEncUtil: #2(W)[2023-06-01 07:32:09,474]=07:32:09=00000{1493626207712371803}&7F9BD44D&        Video format not supported, skipping preset.
2023-06-01 07:32:09 [5,343ms] [Warning] [carb.livestream-rtc.plugin] StreamSDK: NvEncUtil: #4(W)[2023-06-01 07:32:09,474]=07:32:09=00000{1493626207712371803}&7F9BD44D&        Video format not supported, skipping preset.
2023-06-01 07:32:09 [5,343ms] [Warning] [carb.livestream-rtc.plugin] StreamSDK: NvEncUtil: #6(W)[2023-06-01 07:32:09,474]=07:32:09=00000{1493626207712371803}&7F9BD44D&        Video format not supported, skipping preset.
2023-06-01 07:32:09 [5,343ms] [Warning] [carb.livestream-rtc.plugin] StreamSDK: NvEncUtil: #8(W)[2023-06-01 07:32:09,474]=07:32:09=00000{1493626207712371803}&7F9BD44D&        Video format not supported, skipping preset.
2023-06-01 07:32:09 [5,343ms] [Warning] [carb.livestream-rtc.plugin] StreamSDK: NvEncUtil: #0(W)[2023-06-01 07:32:09,474]=07:32:09=00000{1493626207712371803}&7F9BD44D&        Video format not supported, skipping preset.
2023-06-01 07:32:09 [5,343ms] [Fatal] [carb.livestream-rtc.plugin] StreamSDK: AdapterContext: #1(M)[2023-06-01 07:32:09,474]=07:32:09=00000{1493626207712371803}&21C9B327&   GPU memory usage: used = 12972.062500, free = 35696.500000 MB, total = 48668.562500 MB
2023-06-01 07:32:09 [5,367ms] [Warning] [carb.livestream-rtc.plugin] StreamSDK: VideoEncoder: #5(W)[2023-06-01 07:32:09,499]=07:32:09=00000{1493626207712371803}&FB11B4FE&     VideoEncoder was not deinitialized prior to destruction!
2023-06-01 07:32:09 [5,367ms] [Fatal] [carb.livestream-rtc.plugin] StreamSDK: NvEncUtil: #6(M)[2023-06-01 07:32:09,499]=07:32:09=00000{1493626207712371803}&B4368309&        Finished computing SPS/PPS headers.
2023-06-01 07:32:09 [5,368ms] [Fatal] [carb.livestream-rtc.plugin] StreamSDK: Signaling: #1(M)[2023-06-01 07:32:09,499]=07:32:09=00000{8167984385094387598}&1165739F&        Signaling server started.
Streaming server started.And then the following errors when I try to connect from a browser:2023-06-01 07:35:02 [178,457ms] [Fatal] [carb.livestream-rtc.plugin] StreamSDK: AdapterContext: #0(M)[2023-06-01 07:35:02,588]=07:35:02=00000{8167984385094387598}&1E959A86&   Initializing Adapter context
2023-06-01 07:35:02 [178,457ms] [Fatal] [carb.livestream-rtc.plugin] StreamSDK: AdapterContext: #3(M)[2023-06-01 07:35:02,588]=07:35:02=00000{8167984385094387598}&F0E847F4&   Initialized context for adapter 0: 1280 x 720 @ 60.0 Hz
2023-06-01 07:35:02 [178,457ms] [Fatal] [carb.livestream-rtc.plugin] StreamSDK: AdapterContext: #5(M)[2023-06-01 07:35:02,589]=07:35:02=00000{8167984385094387598}&9A64D99C&   System is NOT co-proc.
2023-06-01 07:35:02 [178,522ms] [Fatal] [carb.livestream-rtc.plugin] StreamSDK: AdapterContext: #1(M)[2023-06-01 07:35:02,654]=07:35:02=00065{8167984385094387598}&21C9B327&   GPU memory usage: used = 7742.875000, free = 40925.687500 MB, total = 48668.562500 MB
2023-06-01 07:35:02 [178,522ms] [Fatal] [carb.livestream-rtc.plugin] StreamSDK: AdapterContext: #2(M)[2023-06-01 07:35:02,654]=07:35:02=00000{8167984385094387598}&DD40CDEB&   Initialized CUDA for device ‘NVIDIA RTX A6000’ (SM 8.6) in compute mode ‘CU_COMPUTEMODE_DEFAULT’.
2023-06-01 07:35:02 [178,523ms] [Warning] [carb.livestream-rtc.plugin] StreamSDK: WebRtc: #8(W)[2023-06-01 07:35:02,654]=07:35:02=00000{11115815066630238539}&00000000&           (pacing_controller.cc:154): Pacer min_packet_limit_2 disable_pacing_ 0
2023-06-01 07:35:02 [178,523ms] [Error] [carb.livestream-rtc.plugin] StreamSDK: PeerInstance: #8(E)[2023-06-01 07:35:02,655]=07:35:02=00000{8167984385094387598}&CBFBF6A7&     (Peer ID: 2) OnRenegotiationNeeded
2023-06-01 07:35:02 [178,524ms] [Warning] [carb.livestream-rtc.plugin] StreamSDK: WebRtc: #0(W)[2023-06-01 07:35:02,655]=07:35:02=00000{8167984385094387598}&00000000&           (used_ids.h:55): Duplicate id found. Reassigning from 96 to 127
2023-06-01 07:35:02 [178,524ms] [Warning] [carb.livestream-rtc.plugin] StreamSDK: WebRtc: #1(W)[2023-06-01 07:35:02,655]=07:35:02=00000{8167984385094387598}&00000000&           (used_ids.h:55): Duplicate id found. Reassigning from 98 to 126
2023-06-01 07:35:02 [178,524ms] [Warning] [carb.livestream-rtc.plugin] StreamSDK: WebRtc: #2(W)[2023-06-01 07:35:02,655]=07:35:02=00000{8167984385094387598}&00000000&           (used_ids.h:55): Duplicate id found. Reassigning from 100 to 125
2023-06-01 07:35:02 [178,524ms] [Warning] [carb.livestream-rtc.plugin] StreamSDK: WebRtc: #3(W)[2023-06-01 07:35:02,655]=07:35:02=00000{8167984385094387598}&00000000&           (used_ids.h:55): Duplicate id found. Reassigning from 102 to 124
2023-06-01 07:35:02 [178,540ms] [Fatal] [carb.livestream-rtc.plugin] StreamSDK: VideoEncoder: #7(M)[2023-06-01 07:35:02,672]=07:35:02=00000{11115815066630238539}&8BAD092A&     Initializing NvEnc10VideoEncoder.
2023-06-01 07:35:02 [178,540ms] [Fatal] [carb.livestream-rtc.plugin] StreamSDK: NvEnc10VideoEncoder: #8(M)[2023-06-01 07:35:02,672]=07:35:02=00000{11115815066630238539}&2A6D6983& Using NvEnc header version 10.0.
2023-06-01 07:35:02 [178,543ms] [Fatal] [carb.livestream-rtc.plugin] StreamSDK: NvEnc10VideoEncoder: #4(M)[2023-06-01 07:35:02,675]=07:35:02=00000{11115815066630238539}&A88A086D& Max number of reference frames configured 4 for 1920x1080 at frame: 0
2023-06-01 07:35:02 [178,543ms] [Fatal] [carb.livestream-rtc.plugin] StreamSDK: NvEnc10VideoEncoder: #5(M)[2023-06-01 07:35:02,675]=07:35:02=00000{11115815066630238539}&A08C34AC& H264 video format selected
2023-06-01 07:35:02 [178,543ms] [Fatal] [carb.livestream-rtc.plugin] StreamSDK: NvEnc10VideoEncoder: #6(M)[2023-06-01 07:35:02,675]=07:35:02=00000{11115815066630238539}&B5A73036& RateControl mode NV_ENC_PARAMS_RC_CBR is selected for 1080@60 at frame: 0
2023-06-01 07:35:02 [178,543ms] [Fatal] [carb.livestream-rtc.plugin] StreamSDK: NvEnc10VideoEncoder: #7(M)[2023-06-01 07:35:02,675]=07:35:02=00000{11115815066630238539}&E96C44F6& Multi pass encoding configured as NV_ENC_TWO_PASS_QUARTER_RESOLUTION at frame: 0
2023-06-01 07:35:02 [178,544ms] [Fatal] [carb.livestream-rtc.plugin] StreamSDK: NvEnc10VideoEncoder: #0(M)[2023-06-01 07:35:02,675]=07:35:02=00000{11115815066630238539}&4961242A& Encoder preset configured PRESET_LOW_LATENCY_HP
2023-06-01 07:35:02 [178,544ms] [Fatal] [carb.livestream-rtc.plugin] StreamSDK: NvEnc10VideoEncoder: #1(M)[2023-06-01 07:35:02,675]=07:35:02=00000{11115815066630238539}&C15E78BB& Encoder preset used PRESET_NVENC_P3
2023-06-01 07:35:02 [178,572ms] [Warning] [carb.livestream-rtc.plugin] StreamSDK: WebRtc: #7(W)[2023-06-01 07:35:02,704]=07:35:02=00000{11115815066630238539}&00000000&           (balanced_degradation_settings.cc:96): Unsupported size, value ignored.
2023-06-01 07:35:02 [178,572ms] [Warning] [carb.livestream-rtc.plugin] StreamSDK: WebRtc: #8(W)[2023-06-01 07:35:02,704]=07:35:02=00000{11115815066630238539}&00000000&           (balanced_degradation_settings.cc:96): Unsupported size, value ignored.
2023-06-01 07:35:02 [178,572ms] [Warning] [carb.livestream-rtc.plugin] StreamSDK: WebRtc: #9(W)[2023-06-01 07:35:02,704]=07:35:02=00000{11115815066630238539}&00000000&           (balanced_degradation_settings.cc:96): Unsupported size, value ignored.
2023-06-01 07:35:02 [178,573ms] [Warning] [carb.livestream-rtc.plugin] StreamSDK: WebRtc: #4(W)[2023-06-01 07:35:02,704]=07:35:02=00000{17797842939666438197}&00000000&           (forward_error_correction.cc:95): FEC encode configuration, min_fec_packets: 0 max_fec_packets: 48
2023-06-01 07:35:02 [178,573ms] [Warning] [carb.livestream-rtc.plugin] StreamSDK: WebRtc: #3(W)[2023-06-01 07:35:02,704]=07:35:02=00000{17797842939666438197}&00000000&           (rtp_video_sender.cc:107): Transmitting payload type without picture ID using NACK+ULPFEC is a waste of bandwidth since ULPFEC packets also have to be retransmitted. Disabling ULPFEC.
2023-06-01 07:35:02 [178,586ms] [Warning] [carb.livestream-rtc.plugin] StreamSDK: WebRtc: #6(W)[2023-06-01 07:35:02,717]=07:35:02=00010{16530539155952229181}&00000000&           (p2p_transport_channel.cc:1247): Failed to resolve ICE candidate hostname 87e9305f-2f75-450c-8083-5303f887b4e1.local with error -2
2023-06-01 07:35:02 [178,586ms] [Warning] [carb.livestream-rtc.plugin] StreamSDK: WebRtc: #7(W)[2023-06-01 07:35:02,717]=07:35:02=00000{16530539155952229181}&00000000&           (thread.cc:807): Waiting for the thread to join, but blocking calls have been disallowedHi. Does Websocket or Native livestreaming works? Are you able to reproduce the issue with Create or Code?Websocket works. I do not know about Native streaming as I can only check the streaming via a Mac. Please note that WebRTC works if I stream from the native app; the problem is specific to the Isaac Sim container, which is our main usage.I am not sure about the Create or Code, as I didn’t find their docker images in your database to test them out.Apologies for the late reply. Can you share the full Isaac Sim logs when running native and also when running from the container please?I have attached the two logs you asked for. Thank you in advance!kit_20230626_075153_docker.log (1.9 MB)
kit_20230626_093711_native.log (1.4 MB)@ozhanozen I am not reproduce those error with a container. It works on Safari on a Mac with Isaac Sim running in a container on Ubuntu.Do you run the native app on the same machine as the one with container?@Sheikh_Dawood, for me, it works neither from Safari nor Chrome. Also, I tested using other Macs (same host PC), and the results were the same.Yes, I run the native app on the same machine as the one with the container.Is it possible to try the WebRTC client on another Windows or Linux PC?
It could be a network or firewall issue on host.
If you have Omniverse Launcher installed on host, try running Create natively and start the webrtc client to test too.@Sheikh_Dawood I tried other PCs. As long as the WebRTC runs on a container, it is not possible to access it from anywhere, including the same PC (from outside the container: e.g., http://localhost:xxxx/streaming/webrtc-demo/?server=localhost), from other Macs, Linux or Windows PCs.If WebRTC service runs from a native app rather than a container, I can access it from the same PC or another PC on the same network. There is no problem here.I think the problem is something to do with the container. A firewall wouldn’t be an issue when trying to access the WebRTC of the container from the same PC; is this not correct?A firewall wouldn’t be an issue when trying to access the WebRTC of the container from the same PC; is this not correct?Yes, I believe that is correct.Are you running it this way?Then connect to http://127.0.0.1:8211/streaming/webrtc-client?server=127.0.0.1 for local PC?If it is container specific issue, it could be a problem with docker. Can you try reinstall docker and NVIDIA container toolkit?@Sheikh_Dawood , yes, I run like this or by running the runheadless.webrtc.sh script.I have reinstalled the docker and Nvidia container toolkit, but it didn’t help.Do not the error messages from my previous message give hints regarding the problem?When I run the webrtc with the --/app/livestream/logLevel=verboseoption, I get the following errors during startup which I believe to be related to webrtc:2023-06-01 07:32:09 [5,265ms] [Warning] [omni.services.transport.server.http.server] http server was meant to start on 8211 but port is taken, starting on port 8068 instead
[5.298s] [ext: omni.kit.livestream.webrtc-0.1.0] startupActive user not found. Using default user [kiosk]2023-06-01 07:32:09 [5,288ms] [Fatal] [carb.livestream-rtc.plugin] StreamSDK: NvEncUtil: #6(M)[2023-06-01 07:32:09,419]=07:32:09=00000{1493626207712371803}&CC7C5461&        Computing SPS/PPS headers.
2023-06-01 07:32:09 [5,288ms] [Warning] [carb.livestream-rtc.plugin] StreamSDK: VideoFrameContext: #7(W)[2023-06-01 07:32:09,419]=07:32:09=00000{1493626207712371803}&0564BB28& Couldn’t find VideoFrameContext for this thread.
2023-06-01 07:32:09 [5,288ms] [Fatal] [carb.livestream-rtc.plugin] StreamSDK: AdapterContext: #9(M)[2023-06-01 07:32:09,419]=07:32:09=00000{1493626207712371803}&1E959A86&   Initializing Adapter context
2023-06-01 07:32:09 [5,288ms] [Fatal] [carb.livestream-rtc.plugin] StreamSDK: AdapterContext: #2(M)[2023-06-01 07:32:09,419]=07:32:09=00000{1493626207712371803}&F0E847F4&   Initialized context for adapter 0: 1280 x 720 @ 60.0 Hz
2023-06-01 07:32:09 [5,288ms] [Fatal] [carb.livestream-rtc.plugin] StreamSDK: AdapterContext: #4(M)[2023-06-01 07:32:09,419]=07:32:09=00000{1493626207712371803}&9A64D99C&   System is NOT co-proc.
2023-06-01 07:32:09 [5,337ms] [Fatal] [carb.livestream-rtc.plugin] StreamSDK: AdapterContext: #5(M)[2023-06-01 07:32:09,469]=07:32:09=00049{1493626207712371803}&21C9B327&   GPU memory usage: used = 12972.062500, free = 35696.500000 MB, total = 48668.562500 MB
2023-06-01 07:32:09 [5,338ms] [Fatal] [carb.livestream-rtc.plugin] StreamSDK: AdapterContext: #6(M)[2023-06-01 07:32:09,469]=07:32:09=00000{1493626207712371803}&DD40CDEB&   Initialized CUDA for device ‘NVIDIA RTX A6000’ (SM 8.6) in compute mode ‘CU_COMPUTEMODE_DEFAULT’.
2023-06-01 07:32:09 [5,338ms] [Fatal] [carb.livestream-rtc.plugin] StreamSDK: VideoEncoder: #8(M)[2023-06-01 07:32:09,469]=07:32:09=00000{1493626207712371803}&8BAD092A&     Initializing NvEnc10VideoEncoder.
2023-06-01 07:32:09 [5,338ms] [Fatal] [carb.livestream-rtc.plugin] StreamSDK: NvEnc10VideoEncoder: #9(M)[2023-06-01 07:32:09,469]=07:32:09=00000{1493626207712371803}&2A6D6983& Using NvEnc header version 10.0.
2023-06-01 07:32:09 [5,342ms] [Warning] [carb.livestream-rtc.plugin] StreamSDK: NvEncUtil: #0(W)[2023-06-01 07:32:09,474]=07:32:09=00000{1493626207712371803}&7F9BD44D&        Video format not supported, skipping preset.
2023-06-01 07:32:09 [5,343ms] [Warning] [carb.livestream-rtc.plugin] StreamSDK: NvEncUtil: #2(W)[2023-06-01 07:32:09,474]=07:32:09=00000{1493626207712371803}&7F9BD44D&        Video format not supported, skipping preset.
2023-06-01 07:32:09 [5,343ms] [Warning] [carb.livestream-rtc.plugin] StreamSDK: NvEncUtil: #4(W)[2023-06-01 07:32:09,474]=07:32:09=00000{1493626207712371803}&7F9BD44D&        Video format not supported, skipping preset.
2023-06-01 07:32:09 [5,343ms] [Warning] [carb.livestream-rtc.plugin] StreamSDK: NvEncUtil: #6(W)[2023-06-01 07:32:09,474]=07:32:09=00000{1493626207712371803}&7F9BD44D&        Video format not supported, skipping preset.
2023-06-01 07:32:09 [5,343ms] [Warning] [carb.livestream-rtc.plugin] StreamSDK: NvEncUtil: #8(W)[2023-06-01 07:32:09,474]=07:32:09=00000{1493626207712371803}&7F9BD44D&        Video format not supported, skipping preset.
2023-06-01 07:32:09 [5,343ms] [Warning] [carb.livestream-rtc.plugin] StreamSDK: NvEncUtil: #0(W)[2023-06-01 07:32:09,474]=07:32:09=00000{1493626207712371803}&7F9BD44D&        Video format not supported, skipping preset.
2023-06-01 07:32:09 [5,343ms] [Fatal] [carb.livestream-rtc.plugin] StreamSDK: AdapterContext: #1(M)[2023-06-01 07:32:09,474]=07:32:09=00000{1493626207712371803}&21C9B327&   GPU memory usage: used = 12972.062500, free = 35696.500000 MB, total = 48668.562500 MB
2023-06-01 07:32:09 [5,367ms] [Warning] [carb.livestream-rtc.plugin] StreamSDK: VideoEncoder: #5(W)[2023-06-01 07:32:09,499]=07:32:09=00000{1493626207712371803}&FB11B4FE&     VideoEncoder was not deinitialized prior to destruction!
2023-06-01 07:32:09 [5,367ms] [Fatal] [carb.livestream-rtc.plugin] StreamSDK: NvEncUtil: #6(M)[2023-06-01 07:32:09,499]=07:32:09=00000{1493626207712371803}&B4368309&        Finished computing SPS/PPS headers.
2023-06-01 07:32:09 [5,368ms] [Fatal] [carb.livestream-rtc.plugin] StreamSDK: Signaling: #1(M)[2023-06-01 07:32:09,499]=07:32:09=00000{8167984385094387598}&1165739F&        Signaling server started.
Streaming server started.And then the following errors when I try to connect from a browser:2023-06-01 07:35:02 [178,457ms] [Fatal] [carb.livestream-rtc.plugin] StreamSDK: AdapterContext: #0(M)[2023-06-01 07:35:02,588]=07:35:02=00000{8167984385094387598}&1E959A86&   Initializing Adapter context
2023-06-01 07:35:02 [178,457ms] [Fatal] [carb.livestream-rtc.plugin] StreamSDK: AdapterContext: #3(M)[2023-06-01 07:35:02,588]=07:35:02=00000{8167984385094387598}&F0E847F4&   Initialized context for adapter 0: 1280 x 720 @ 60.0 Hz
2023-06-01 07:35:02 [178,457ms] [Fatal] [carb.livestream-rtc.plugin] StreamSDK: AdapterContext: #5(M)[2023-06-01 07:35:02,589]=07:35:02=00000{8167984385094387598}&9A64D99C&   System is NOT co-proc.
2023-06-01 07:35:02 [178,522ms] [Fatal] [carb.livestream-rtc.plugin] StreamSDK: AdapterContext: #1(M)[2023-06-01 07:35:02,654]=07:35:02=00065{8167984385094387598}&21C9B327&   GPU memory usage: used = 7742.875000, free = 40925.687500 MB, total = 48668.562500 MB
2023-06-01 07:35:02 [178,522ms] [Fatal] [carb.livestream-rtc.plugin] StreamSDK: AdapterContext: #2(M)[2023-06-01 07:35:02,654]=07:35:02=00000{8167984385094387598}&DD40CDEB&   Initialized CUDA for device ‘NVIDIA RTX A6000’ (SM 8.6) in compute mode ‘CU_COMPUTEMODE_DEFAULT’.
2023-06-01 07:35:02 [178,523ms] [Warning] [carb.livestream-rtc.plugin] StreamSDK: WebRtc: #8(W)[2023-06-01 07:35:02,654]=07:35:02=00000{11115815066630238539}&00000000&           (pacing_controller.cc:154): Pacer min_packet_limit_2 disable_pacing_ 0
2023-06-01 07:35:02 [178,523ms] [Error] [carb.livestream-rtc.plugin] StreamSDK: PeerInstance: #8(E)[2023-06-01 07:35:02,655]=07:35:02=00000{8167984385094387598}&CBFBF6A7&     (Peer ID: 2) OnRenegotiationNeeded
2023-06-01 07:35:02 [178,524ms] [Warning] [carb.livestream-rtc.plugin] StreamSDK: WebRtc: #0(W)[2023-06-01 07:35:02,655]=07:35:02=00000{8167984385094387598}&00000000&           (used_ids.h:55): Duplicate id found. Reassigning from 96 to 127
2023-06-01 07:35:02 [178,524ms] [Warning] [carb.livestream-rtc.plugin] StreamSDK: WebRtc: #1(W)[2023-06-01 07:35:02,655]=07:35:02=00000{8167984385094387598}&00000000&           (used_ids.h:55): Duplicate id found. Reassigning from 98 to 126
2023-06-01 07:35:02 [178,524ms] [Warning] [carb.livestream-rtc.plugin] StreamSDK: WebRtc: #2(W)[2023-06-01 07:35:02,655]=07:35:02=00000{8167984385094387598}&00000000&           (used_ids.h:55): Duplicate id found. Reassigning from 100 to 125
2023-06-01 07:35:02 [178,524ms] [Warning] [carb.livestream-rtc.plugin] StreamSDK: WebRtc: #3(W)[2023-06-01 07:35:02,655]=07:35:02=00000{8167984385094387598}&00000000&           (used_ids.h:55): Duplicate id found. Reassigning from 102 to 124
2023-06-01 07:35:02 [178,540ms] [Fatal] [carb.livestream-rtc.plugin] StreamSDK: VideoEncoder: #7(M)[2023-06-01 07:35:02,672]=07:35:02=00000{11115815066630238539}&8BAD092A&     Initializing NvEnc10VideoEncoder.
2023-06-01 07:35:02 [178,540ms] [Fatal] [carb.livestream-rtc.plugin] StreamSDK: NvEnc10VideoEncoder: #8(M)[2023-06-01 07:35:02,672]=07:35:02=00000{11115815066630238539}&2A6D6983& Using NvEnc header version 10.0.
2023-06-01 07:35:02 [178,543ms] [Fatal] [carb.livestream-rtc.plugin] StreamSDK: NvEnc10VideoEncoder: #4(M)[2023-06-01 07:35:02,675]=07:35:02=00000{11115815066630238539}&A88A086D& Max number of reference frames configured 4 for 1920x1080 at frame: 0
2023-06-01 07:35:02 [178,543ms] [Fatal] [carb.livestream-rtc.plugin] StreamSDK: NvEnc10VideoEncoder: #5(M)[2023-06-01 07:35:02,675]=07:35:02=00000{11115815066630238539}&A08C34AC& H264 video format selected
2023-06-01 07:35:02 [178,543ms] [Fatal] [carb.livestream-rtc.plugin] StreamSDK: NvEnc10VideoEncoder: #6(M)[2023-06-01 07:35:02,675]=07:35:02=00000{11115815066630238539}&B5A73036& RateControl mode NV_ENC_PARAMS_RC_CBR is selected for 1080@60 at frame: 0
2023-06-01 07:35:02 [178,543ms] [Fatal] [carb.livestream-rtc.plugin] StreamSDK: NvEnc10VideoEncoder: #7(M)[2023-06-01 07:35:02,675]=07:35:02=00000{11115815066630238539}&E96C44F6& Multi pass encoding configured as NV_ENC_TWO_PASS_QUARTER_RESOLUTION at frame: 0
2023-06-01 07:35:02 [178,544ms] [Fatal] [carb.livestream-rtc.plugin] StreamSDK: NvEnc10VideoEncoder: #0(M)[2023-06-01 07:35:02,675]=07:35:02=00000{11115815066630238539}&4961242A& Encoder preset configured PRESET_LOW_LATENCY_HP
2023-06-01 07:35:02 [178,544ms] [Fatal] [carb.livestream-rtc.plugin] StreamSDK: NvEnc10VideoEncoder: #1(M)[2023-06-01 07:35:02,675]=07:35:02=00000{11115815066630238539}&C15E78BB& Encoder preset used PRESET_NVENC_P3
2023-06-01 07:35:02 [178,572ms] [Warning] [carb.livestream-rtc.plugin] StreamSDK: WebRtc: #7(W)[2023-06-01 07:35:02,704]=07:35:02=00000{11115815066630238539}&00000000&           (balanced_degradation_settings.cc:96): Unsupported size, value ignored.
2023-06-01 07:35:02 [178,572ms] [Warning] [carb.livestream-rtc.plugin] StreamSDK: WebRtc: #8(W)[2023-06-01 07:35:02,704]=07:35:02=00000{11115815066630238539}&00000000&           (balanced_degradation_settings.cc:96): Unsupported size, value ignored.
2023-06-01 07:35:02 [178,572ms] [Warning] [carb.livestream-rtc.plugin] StreamSDK: WebRtc: #9(W)[2023-06-01 07:35:02,704]=07:35:02=00000{11115815066630238539}&00000000&           (balanced_degradation_settings.cc:96): Unsupported size, value ignored.
2023-06-01 07:35:02 [178,573ms] [Warning] [carb.livestream-rtc.plugin] StreamSDK: WebRtc: #4(W)[2023-06-01 07:35:02,704]=07:35:02=00000{17797842939666438197}&00000000&           (forward_error_correction.cc:95): FEC encode configuration, min_fec_packets: 0 max_fec_packets: 48
2023-06-01 07:35:02 [178,573ms] [Warning] [carb.livestream-rtc.plugin] StreamSDK: WebRtc: #3(W)[2023-06-01 07:35:02,704]=07:35:02=00000{17797842939666438197}&00000000&           (rtp_video_sender.cc:107): Transmitting payload type without picture ID using NACK+ULPFEC is a waste of bandwidth since ULPFEC packets also have to be retransmitted. Disabling ULPFEC.
2023-06-01 07:35:02 [178,586ms] [Warning] [carb.livestream-rtc.plugin] StreamSDK: WebRtc: #6(W)[2023-06-01 07:35:02,717]=07:35:02=00010{16530539155952229181}&00000000&           (p2p_transport_channel.cc:1247): Failed to resolve ICE candidate hostname 87e9305f-2f75-450c-8083-5303f887b4e1.local with error -2
2023-06-01 07:35:02 [178,586ms] [Warning] [carb.livestream-rtc.plugin] StreamSDK: WebRtc: #7(W)[2023-06-01 07:35:02,717]=07:35:02=00000{16530539155952229181}&00000000&           (thread.cc:807): Waiting for the thread to join, but blocking calls have been disallowedPowered by Discourse, best viewed with JavaScript enabled"
816,posetracker-to-unreal-in-realtime,"Hello guys,Pose Tracker is super cool! Is it possible to transmit the live mocap data generated from webcam, to UNREAL in realtime? Like a livelink.Im also very interested in this but for now id just like to know how you are able to use the recorded capture in unreal. All the other assets open up ok but i see nothing in sequencer or elsewhere?I haven’t tried it but it’s definitely worth a shot. If you get a chance to try it, let us know!It would be great to get this working with Unreal, but actually with any skeleton (i.e. Blender3D). Is there a guide for importing and using custom skeletons in machinima pose tracker? ThanksPowered by Discourse, best viewed with JavaScript enabled"
817,audio2face-for-linux,"Hi,
Is there a plan to release Audio2Face on Linux as well? If there is, when can we roughly expect it?ThanksHi @bart.sekura ,We have plans for Linux release but it won’t happen til a bit later.  Our best estimate at the moment is end of the year.  We will be testing on linux soon and can give a better estimate in a month or two.  Most of our efforts right now is for feature completion so users can drive a face fully with just audio.ThanksHi，I‘m still waiting for audio2face linux. Is there any news?Hello @770554498!  The development team is still working on Linux support for Audio2Face.  I do not have any more information on it at this time, but the QA team is currently testing it.  You can see what we are working on for the next release on our Omniverse Public Roadmap.We are working on a linux version at the moment.  Stay tuned.  A rough release timeframe is Q2 of this year.Thanks.Thank you for keeping us informed.
Can provide any updates on your progress for Linux compatibility.
If it isn’t too much trouble, could you also hint as to which distros you might test prior to release.Any update on this?
Thanks.Hello,
Since Q2 has passed could we get anny more info ?
I having to install windows just for audio 2 Face is quite a hastle.Ps annyone gotten a wine version working ?We have had to make some adjustments on our roadmap recently. Linux build of Audio2Face is still continuing and is not far off.  We will update with more precise release date soon.  Stay tuned.that’s great to hear.Linux version is coming out very soon, just a quick update.Hi! Is there any information on when the Linux version will be released? Good work!Today it has been released Audio2Face 2022.2 Now AvailableYes it has been released.  :)Do you have a Docker image with Audio2Face downloaded and up in headless mode? Is it possible to create such an image?Powered by Discourse, best viewed with JavaScript enabled"
818,omniverse-launcher-not-booting-not-finding-drivers-ubuntu-20-04,"Hi there, I’m having issues with booting Omniverse Launcher for the first time on this PC. If I click on the .AppImage I just get a grey screen which hangs indefinitely. I’ve put the output frome when I launch from terminal below:I noticed the “driver_name = (null)” in the output, and tried updating my drivers and am now on the latest 525.116.04, but still get the same issue. I then tried the launcher_cleanup tool with no luck.I saw this simillar issue on the forum, but that was related to not having an RTX capable GPU, I am using a NVIDIA A4500 so that shouldn’t be the cause.I’d appreciate any help getting this sorted :)AdamPowered by Discourse, best viewed with JavaScript enabled"
819,how-can-i-make-deformable-body-in-python,"I want to make deformable body in python program.
But,  I have the following error.
What should I do to make deformable body in python?
Please tell me the solution.[Error] [omni.physx.plugin] Deformable Body feature is only supported on GPU. Please enable GPU dynamics flag in Property/Scene of physics scene!Hi,
ah yes, this does ring a bell, I think this is a duplicate issue of this forum question:See the answer from Kelly at the end, I think its the same issue. The GPU dynamics on a physics scene needs to be enabled, but is somehow turned off by the Isaac scripts.Regards,
AlesThanks, Ales.I added SimulationContext(set_defaults=False), but I got the same error. Is there something wrong with my program’s coding?I think what you need are these lines:Thanks for the fast reply Ales.
I tried to add your lines, but I got same error.
Is it wrong to write the following?Hi @Yuya_t  - The error message you’re seeing is indicating that the GPU dynamics flag needs to be enabled in the physics scene properties. This is because deformable bodies in PhysX require GPU acceleration.You are already enabling GPU dynamics in your code with the line physics_context.enable_gpu_dynamics(True). However, it seems like this is not taking effect.One possible reason could be that the GPU dynamics flag needs to be set before the physics scene is created. If the physics scene is already created before this line is executed, the change might not take effect.You can try moving the enable_gpu_dynamics(True) line to be before the world.scene.add_default_ground_plane() line, like this:physics_context = world.get_physics_context()
physics_context.enable_gpu_dynamics(True)world.scene.add_default_ground_plane()
This ensures that the GPU dynamics flag is set before the physics scene is created.If this doesn’t solve the issue, it could be that your system doesn’t support GPU acceleration, or the necessary GPU drivers are not installed. You can check if your system supports GPU acceleration and if the necessary drivers are installed.Thank you for reply @rthaker .
I tried your opinion, but I didn’t solve error.I enabled gpu again at GUI, the problem was solved.Hi @Yuya_t  - There is a new Isaac Sim release coming around Aug/Sep timeframe.  I think this should not happen with that latest release.That’s good news! I am expectantly waiting for that.Powered by Discourse, best viewed with JavaScript enabled"
820,omniverse-glb-to-usd-converter-errors-with-readyplayer-me-avatar,"Just wanted to confirm the Omniverse GLB to USD converter is still broken in several ways. For example, it fails to correctly import ReadyPlayer.me avatars.Sample GLB file: ready-player-me-with-blendshapes.glb - Google DriveOmniverse
image974×1067 44.9 KBBlender (and all other apps I have tried)
image1656×1258 68.6 KBNote for textures that the hair and arms skin is wrong, but the short top is correct.(See also previous related thread GLTF/GLB character imports puts skeleton not at root - #14 by alan.james.kent)Steps to reproduce:image1026×874 117 KBPowered by Discourse, best viewed with JavaScript enabled"
821,blender-armature-export,"Hi!I want to export a USD file of an animated rigged character from Blender and import it to Isaac Sim! The animation of the import works in Isaac Sim but unfortunately there is no USDskel and some parts of the rendering is not right (hair has the wrong material).
I also tried to import a FBX file from Blender to Isaac Sim and it shows two different skeletons: one on the right place with the character and one with a much bigger scale and rotated.
I already saw the topic about the armature export: Armature Export of Parented MeshAre there any news considering the USD export with USDskel? And if not could you specify the solution proposed in the topic above (armature modifiers?)? Or is there an alternative way to add a skeleton in Isaac Sim to an animated character?Thanks a lot!Hello @ha.uyen.nguyen.  Thanks for reporting this.  It’s interesting that the skeleton in not included in the USD export from Blender.  Just to confirm, do you have exporting armatures enabled in the USD export options?  (The Armatures toggle under Rigging.)If you have this enabled and you’re still not getting the skeleton, would you be able to provide me with a Blender file to reproduce this, so that we can debug?  Please feel free to direct-message me to provide the file, if you prefer.  Thanks!Hi!
I’m currently using Blender 3.5 and there is no such option in the USD export window.

Are you using a different version? Or where can I find this option?
Thanks a lot!Hi @ha.uyen.nguyen.  It looks like you are using the official Blender 3.5 release version.  That version doesn’t yet have that feature.  The USD skeleton and blend shape animation feature is available on in the universal-scene-description Blender Alpha branch which you can get from the Omniverse Launcher.  The latest version is 3.6.0-usd.200.2:In this version, you will see options to enable exporting armatures and shape keys under the Rigging section:Please let us know if you have questions or encounter issues.have exporting armatures enabled in the USD export optionsHi !
Testing the same export process on version 3.6.0 usd.200.2, with exporting armatures enabled in the USD export options = no luck.
Any news considering the USD export ?
Sorry i cant attach the blend file because the file size is huge. But there is a blend file linked in the post by nguyen.
Any chance to get it working on future version ?
Thanks in advance !Powered by Discourse, best viewed with JavaScript enabled"
822,when-will-the-next-update-of-modulus-extension-be-released,"I want to create a new scenario using modulus extension.I found
‘API documentation and guides for creating your own scenario is forthcoming’
in the creating new scenario part of the modulus extension document.When will the next update of modulus extension be released?
Can I get a tutorial for vtk-m extension in addition?Hi @anmin621. Let me check on this for you.Hi @anmin621Presently development of the Modulus OV extension is on hold for the near future. For VTK related utilities consider having a look at the Paraview connector. Thanks.thank you for your kind replyThis topic was automatically closed 14 days after the last reply. New replies are no longer allowed.Powered by Discourse, best viewed with JavaScript enabled"
823,we-want-your-ideas-for-future-livestreams,"Let us know below!*You can watch us live on these channels Twitch  | YouTube(This was originally posted in the Announcements section, where comments are disabled.  Please feel free to comment here!)I would like to see more basic operations, omniverse is great as it adds so much flexibility. But for beginner devs this is brutal. Even how easy the tools are the overall experience isn’t intuitive.  For example just making a basic application or a basic function like lerp is hidden under mountains of documentation. If you aren’t a degree level programmer its just not accessible which is a real shameHello @Sirens_uk!  Thanks for the suggestion.  I will bring this up with the team.  In the meantime, where are you having difficulties getting started?  Is it with Python, USD, Kit, Omniverse Code? Are you creating an Extension, App, or Microservice, or Connector?We’ve done several tutorials for developers on how to get started with Ominverse, and several livestreams.  We also host weekly “Code with Matty” sessions where you can watch @mati-nvidia do live coding sessions on our discord server discord.gg/nvidiaomniverse.  I highly suggest you join the discord server because we have a full team of experts ready to help you get started coding in Omninverse!  You can find several tutorials under the Learning tab in the Omniverse Launcher as well!Check these out!DocumentationVideosI hope this helps!Ambassador´s gathering Live stream!live coding sessions on our discord server discord.gg/nvidiaomniverse. I highly suggest you join the discord server because we have a full team of experts ready to help you get started coding in Omninverse! You can find several tutorials under the Learning tab in the Omniverse Launcher as well!Thanks @WendyGram the team is great, currently trying to work out how to randomize transforms in Issac sim, within the action graph. Action graph is amazing as someone with a self taught coding background! Like incredible to be totally honest! hats off to all the team. I can see how this will just get better an better over time.I’ve been using interpolateTo, to great effect but I would like to loop these once target is reached and create a new random transform point to loop too. Not really sure how to do this?Joined the discord to so looking forward to learning there also!Live stream about using the CloudXR! step by step…SolidSpark had a great idea in Discord for these Future Livestreams:
“Can’t wait for the Ai Driven Physics Based Character Animation system. I hope it releases soon.”Would love to see some basic physics step-by-step.  For example a flag blowing in the wind.I would like some fluids - how to set up waterfalls, rivers, fountains, etc.Hi Wendy,
I pitched this idea to Edmar awhile back but I think this digital twin of an entire train system that is now deployed?? I think it is amazing - look at the edge cases they found, a trolley rolling across the platform… would love to hear more details from the creators of this twin.Using NVIDIA Omniverse, DSD can develop highly capable perception and incident prevention and management systems to optimally detect and react to irregular situations during day-to-day railway operation.
Est. reading time: 3 minutes
@WendyGram ,Would be great to see some exterior/interior renderings tips, tricks to achieving professional rendering in create and some quality comparison with offline renderer like Vray/corona/cycles and redshift/octane to judge the pros and cons with regards to quality and speed.Powered by Discourse, best viewed with JavaScript enabled"
824,edit-speed-of-rendered-video,"I am trying to render video in Omniverse Code. The problem is the videos rendered have quite fast pace. How can I adjust the speed of video in rendering setting or movie capture ?Hi,This topic belongs in the Omniverse category so the team has visibility. I will go ahead and move it over for you.Powered by Discourse, best viewed with JavaScript enabled"
825,enterprise-nucleus-server-release-notes-mistake-for-2022-2-0,"The Release notes say the problem was solved
Ingress config sample:What should it be then?
canon-name or canonical-nameObs.: this is from the stack named: nucleus-stack-2022.4.0+tag-2022.4.0-rc.1.gitlab.6522377.48333833Powered by Discourse, best viewed with JavaScript enabled"
826,mismatched-units,"I tried to bring in a pipe I created in Blender, and I get this message.Blender is set in meters and I am not sure how to find units in omniverse Composer. If I move the pipe, it disappears.Where are units in an Omniverse scene.ThansFor NEW files you can set the “Default Meters Per Unit” in both the Stage Prefences inside “Preferences>Stage”.
For existing files, you can edit the layer properties directly by right clicking on the layer, and choosing “Edit” at the bottom. This will open the USD file as an editable text file, in your default text editor (probably notepad). When I do this I see the following layer attributes…#usda 1.0
(
defaultPrim = “World”
endTimeCode = 100
metersPerUnit = 0.01
startTimeCode = 0
timeCodesPerSecond = 60
upAxis = “Y”
)Thank you.Powered by Discourse, best viewed with JavaScript enabled"
827,general-question-omni-anim-people,"Hello,I have a question regarding the omni.anim.people extension.
The extension utilizes the cmd.txt file to define the behaviors of characters in a given scene.
Specifically, I am interested in whether these behaviors are deterministic.While it is clear that with the same cmd.txt file, the characters will execute the same commands in the same order, I am curious as to whether the simulation outcome will be consistent across multiple runs.Is it possible that, due to factors such as collisions between characters, the navigation paths of my characters may differ slightly between simulations?Thank you in advance!Hi @jominga - Someone from our team will review and respond back to you.Hi @jominga - I received the response from the team.this makes the timeline play every frame, never skip oneThis makes delta time consistentPowered by Discourse, best viewed with JavaScript enabled"
828,camera-suddenly-stopped-working-failed-to-get-usd-attribute,"Hello,My camera suddenly stopped working. It was working all day but now at the end of the day it stopped working.I already tested it with some older code and with old usd files but that still does not work.One thing i notice is that first the camera model was visibile in isaac sim and now it is standard on invisible.Can anybody help me? I updated cache can this be the consequences of that?This is the same problem I think but there is no solution: Replicator sometimes seems to break the existing CameraI get this error:
2023-06-16 13:59:03 [83,075ms] [Error] [omni.graph.core.plugin] Failed to get USD Attribute for /Render/PostProcess/SDGPipeline/RenderProduct_omni_kit_widget_viewport_ViewportTexture_0_LdrColorSDExportRawArray.inputs:cudaStream of type uint64
2023-06-16 13:59:03 [83,075ms] [Error] [omni.graph.core.plugin] Failed to get USD Attribute for /Render/PostProcess/SDGPipeline/RenderProduct_omni_kit_widget_viewport_ViewportTexture_0_LdrColorSDExportRawArray.inputs:exec of type uint (execution)
2023-06-16 13:59:03 [83,075ms] [Error] [omni.graph.core.plugin] Failed to get USD Attribute for /Render/PostProcess/SDGPipeline/RenderProduct_omni_kit_widget_viewport_ViewportTexture_0_LdrColorSDExportRawArray.inputs:renderResults of type uint64
2023-06-16 13:59:03 [83,075ms] [Error] [omni.graph.core.plugin] Failed to get USD Attribute for /Render/PostProcess/SDGPipeline/RenderProduct_omni_kit_widget_viewport_ViewportTexture_0_LdrColorSDExportRawArray.inputs:swhFrameNumber of type uint64Pieces of my Code:Hi @nickbakker40 - The error messages you are seeing are indicating that the program is failing to get certain attributes from the USD file. These attributes include cudaStream , exec , renderResults , and swhFrameNumber .This could be due to a few reasons:The attributes might not exist in the USD file. Please check the USD file to ensure these attributes are present.The attributes are not accessible due to permissions or other restrictions. Make sure the program has necessary permissions to access the attributes.The types of the attributes might not be correct. The error messages indicate that the program is expecting cudaStream , renderResults , and swhFrameNumber to be of type uint64 and exec to be of type uint . If these attributes are not of the expected types, the program will fail to get them.Powered by Discourse, best viewed with JavaScript enabled"
829,my-extension-only-runs-when-the-mouse-cursor-is-over,"I want to make a realtime-streaming charthere is my code :I succeeded in executing the realtime streaming chart extension
without the ui freezing using threading,
but When mouse cursor leaves the UI of the extension, chart streaming stops.I wish it would work even if the mouse cursor goes out of the extension’s uiAny solution to this problem?here is the error displayed in the console:

1899×213 40.6 KB
I don’t know exactly why, but in create app it works in real time even if the cursor goes outWhen the mouse cursor is outside the extension ui, the only thing that stops my extension is the code appPowered by Discourse, best viewed with JavaScript enabled"
830,database-extension-omniverse,"I am new to integrating InfluxDB with Omniverse and would like to connect the two. Can someone provide me with a step-by-step overview of the process along with relevant links for guidance?Hi @Hellooo123. Welcome to the forums! I recommend starting with learning about Extensions: Extend Omniverse — Omniverse Kit documentationPowered by Discourse, best viewed with JavaScript enabled"
831,omniverse-faq,"*(SP) = Separate Post |  *(WBU) = Will be updated in future release | *(D) = DocumentationIf you are looking for where the applications are installed on your computer you can find out by opening the Omniverse Launcher > Library Tab > Click the Hamburger icon > SettingsThen you can see the path to where the application is downloaded.
wheredownload2971×535 92.1 KB
You can change where download paths by opening the Omniverse Launcher > User Icon > Settings
usersettings1301×697 194 KB
Yes!  Absolutely!  And we have a GREAT COMMUNITY of industry experts ready to help as well as our NVIDIA developers that occasionally prowl the chat rooms!  If you are a developer, there is a team of experts ready to help you get started!JOIN US!! discord.gg/nvidiaomniverseI recommend that you check this page out: Omniverse Development Resource CenterI also recommend that you join our discord server.RMC on any menu tab to open the Move to External Window option.You can open a 2nd Viewport by going to Window > Viewport and clicking the Viewport 2 option.
1viewport537×559 109 KB


2 viewport1493×713 78.8 KB
Powered by Discourse, best viewed with JavaScript enabled"
832,segmented-images-from-deepstream-segmentation-test,"Dear people.I installed deepstreamer in Nvidia jetson nano orin, i have run the application deepstream-segmentation-test with the command:./deepstream-segmentation-app dstest_segmentation_config_semantic.txt nv_2.jpgIt runs successfully, I can see on the screen the segmented image, however I am struggling to figure out how to save the segmented image in memory as png or  ppm  file.Thank you and regardsPowered by Discourse, best viewed with JavaScript enabled"
833,the-robot-does-not-follow-the-exact-target-position,"Hi,
I tried to follow the Adding a New Manipulator tutorial to build my custom robot. The following video is I create a custom robot, and it seems to work.
Here are three questions I hope someone can help me to figure out.Hi @vic-chen -
I think this one of the Isaac Sim livestream video will be helpful to you especially from 30:00 mins : Isaac Sim/Robotics Weekly Livestream: From Single to Multi Robot Environment - YouTubeHi @rthaker ,
Thanks for sharing the livestream video, I know how the deal with my second and third question. But my 1st question not mentioned in the video. Can you help me to know how to control the robot arm’s motion speed?how to speed up the robot arm’s motion?Hi @vic-chen - You can adjust the speed of the robot arm’s motion through the GUI by  modifying the joint velocity limits. You can select the particular joint from robot’s arm and then in property window, find the Velocity limit property and update the speed.Remember that speeding up a robot arm’s motion can increase the risk of instability or damage to the robot, so it’s important to do it carefully and monitor the robot’s performance closely.Hi @rthaker ,
Thanks for your suggestion. I know how to control it.
Best regard!This topic was automatically closed 14 days after the last reply. New replies are no longer allowed.Powered by Discourse, best viewed with JavaScript enabled"
834,emissive-material-not-visible-through-glass,"Hi,
I am having problems with seeing transparent glass material with the emissive background behind it.I had to turn off visibility of all the glasses in order to see my animated, emissive background. But because of it I am loosing all of the nice specular reflections of the interior windows.GLASS_EMISSIVE_TEXTURE_021920×1049 152 KB
GLASS_EMISSIVE_TEXTURE_011920×1057 153 KBPowered by Discourse, best viewed with JavaScript enabled"
835,hair-width,"Any idea how to change hair width in creator? I’m using an imported abc file with hair curve content. It shows up blocky in creator.Powered by Discourse, best viewed with JavaScript enabled"
836,cannot-jog-a-robot-through-multiple-poses,"Hi, I am working with Nvidia Isaac Sim importing the URDF file of my 7 DOF  robot by means of the commands:import omni
from omni.isaac.examples.base_sample import BaseSample
from pxr import UsdPhysics, Sdf, Gf, UsdLux
from omni.isaac.core.utils.stage import add_reference_to_stage
from omni.isaac.core.robots import Robot
from omni.isaac.dynamic_control import _dynamic_control
import numpy as npclass HelloWorld(BaseSample):
def init(self) → None:
super().init()
returnThe robot seems to be correctly imported in Isaac Sim. Now, I am trying to move to robot to specific poses, and to do that I have used:ArticulationAction
async def setup_post_load(self):
self._world = self.get_world()
self._robot = self._world.scene.get_object(“guidearm”)
self._robot_articulation_controller = self._robot.get_articulation_controller()
self._world.add_physics_callback(“sending_actions”, callback_fn=self.send_robot_actions)
returndef send_robot_actions(self, step_size):
target_joint_positions = np.array([0,math.degrees(90), 0, 0, 0, 0, 0])
target_joint_velocities = np.array([0, math.degrees(10), 0, 0, 0, 0, 0])Implementation of my own controller:
class CoolController(BaseController):
def init(self):
super().init(name=“my_cool_controller”)
returndef forward(self, command_position, command_velocity):
joint_positions = [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
joint_velocities = [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
# set the joint positions to be equal to the commanded values
joint_positions[0] = command_position[0]
joint_positions[1] = command_position[1]
joint_positions[2] = command_position[2]
joint_positions[3] = command_position[3]
joint_positions[4] = command_position[4]
joint_positions[5] = command_position[5]
joint_positions[6] = command_position[6]…
async def setup_post_load(self):
self._world = self.get_world()
self._robot = self._world.scene.get_object(“guidearm”)returnNone of the methods has resulted being successfull and all of them show the same behavior, with the robot jogging crazily through multiple poses, activating also joints that are not the commanded ones in a very fast way (see video)
I have read all of the documentation and I think I exhausted all the potential solutions on the web. The most common errors that were displayed to me when this type of behavior was happening were:
1)[Warning] [omni.isaac.core.utils.nucleus] /persistent/app/omniverse/mountedDrives setting not found
2) [Warning] [omni.isaac.core.simulation_context.simulation_context] A new stage was opened, World or Simulation Object are invalidated and you would need to initialize them again before using them.
3) (for method 1) :
PhysX error: Illegal BroadPhaseUpdateData
, FILE /buildAgent/work/16dcef52b68a730f/source/lowlevelaabb/src/BpBroadPhaseABP.cpp, LINE 4024 and consequenctly warning of this type [Warning] [omni.physx.plugin] Invalid PhysX transform detected for /World/Gen2_GuideArm/Link_5 for each link. The simulation stops when the PhysX error kicks in.Can you help me out figuring out what’s going on? Is it happening due to step size or am I forgetting to specify something in the scene to allow the robot to move smoothly?The erratic behavior of the robot could be due to several reasons. Here are a few things you could check:Regarding the warnings you are seeing:I hope this helps! If you are still having issues, please provide more details about your setup and I’ll be happy to help further.Thank you very much! I fixed the problem by setting the proper parameters in the urdf file. If possible, I would like to ask another question. I am jogging the robot through multiple poses now using the command ArticulationAction(jointpositions=…). I would like to measure the torque on each of the joints (or joint efforts) due to the mass and the inertia of the entire robot while jogging the arm. I tried to use the commands get_joint_efforts and get_applied_joint_efforts but I did not have success, resulting in really low values for the joints. According to the documentation, I found that the torque tau could be computed (at least until some months ago) by means of PD control (tau = stiffnessposition_error+ dampingvelocity_error). Is there any other way I can implement in my code in order to obtain an accurate measurement of the torque which my joints (position controlled) are exercising in order to move the arm to the desired joint angles?Best,Hi @rarduini  - The joint efforts or torques in a robot arm are indeed influenced by the mass and inertia of the robot, as well as the gravity, friction, and any external forces applied to the robot.The get_joint_efforts and get_applied_joint_efforts methods should give you the current joint efforts, but these values can indeed be quite low if the robot is not carrying any load or if it’s in a balanced position where the torques cancel out.If you’re not getting the expected values from these methods, here are a few things you can check:Powered by Discourse, best viewed with JavaScript enabled"
837,job-submitted-to-farm-but-nothing-happens,"Hi all,
I am having issues with the Farm. I installed both Queue and Agent on my machine following thistutorial . I’ve managed to connect with the localhost to the Queue and  to submit the job but nothing happens inside USD Composer. Also the Job Status remains as “submitted”.FARM_012770×535 319 KBPowered by Discourse, best viewed with JavaScript enabled"
838,crating-custom-rl-environments,"Hello, I would like to create a custom environment for training of quadrupedal robots with RL. My requirements are:I’m wondering which is the more appropriate way to proceed (by code? by custom .usd?). I previously used isaacgym envs but i’ve also seen orbit which claim better performance. Which one fit better the task? which is going to stay? and is orbit already stable?ThanksI’ll answer my own question for the ones interested. I frame the situation as it is today:p.s. thanks for the clear naming convention and overlapping products…This topic was automatically closed 14 days after the last reply. New replies are no longer allowed.Powered by Discourse, best viewed with JavaScript enabled"
839,omniverse-materials,"Hello,
I am trying to assign some of the predefined materials in omniverse, but for some reason all of the materials are locked (Image 1) and whenever I drag one of the materials to the asset, nothing happens (Image 2 after assigning Wood Bark material). Is it possible to remove this lock sign on the materials? I tried to change the permissions but it is not possible.
Thanks in advance!Image 11088×409 192 KB

Image 3.PNG1877×780 101 KBI believe the lock just means you can’t edit those assets since they are shipped with OV. I wouldn’t recommend disabling it (if it’s even possible).Personally, I don’t have issues dragging and dropping; that said, it does take a few seconds for it to be binded to the prim so try holding down the button a bit longer before you release it (you should be prompted a “Create Material” dialog). Alternatively, you can right click on any material and choose to “Bind material to selected prim(s)” option (see attached) if drag and drop doesn’t workAnother dev has also mentioned you can copy the assets into a separate location to allow read/write operations - Nvidia folder lock in the local host - #2 by dlindseyPowered by Discourse, best viewed with JavaScript enabled"
840,farm-agent-cannot-be-installed,"T_T…The rest of the Apps including farm queue install just fine.I checked the launcher.log During installation in Launcher,the corresponding log is recorded and the launcher itself is shut down.[[18060:0309/115520.076:ERROR:node_bindings.cc(146)] Fatal error in V8: Ineffective mark-compacts near heap limit Allocation failed - JavaScript heap out of memory]I am using windows 11.
My system RAM 128GB
GPU A6000 48GBI tested it on four of my workstations and they all got the same results…plz help…Hello @mangooo!  What version of Farm Agent are you installing.  I just tested Release 104.0.0 and it worked fine for me.  If you could, please share your full logs with us, it might shed more light onto why you are receiving this error.For the particular error you mentioned, it is a JavaScript Node.js issue where the default memory allocated by your system to Node.js is not enough to run a large project.  When I see this error (on a Windows machine) this is how I usually fix it:I believe you can also make this change in a PowerShell terminal by opening one up and typing this command then pressing Enter:
$env:NODE_OPTIONS=""--max-old-space-size=4096""If you want to increase the heap memory temporarily, you can run this command in the PowerShell:
set NODE_OPTIONS=--max-old-space-size=4096Let me know if this helps, if not, please share your logs with me so that we can help troubleshoot the issue!thx @WendyGramI installed both 104 and 103 versions.I did as you said and the problem went away. However, the following log appears and the launcher is terminated without installation as before.I am attaching the log that occurred when I hit the install command.[2023-03-17 16:58:14.422] [info]  Running /install command…
[2023-03-17 16:58:14.423] [debug] Enqueue [b8eb8855-a546-406c-9a41-792697bb81fd] .
[2023-03-17 16:58:14.423] [debug] Set current installer [b8eb8855-a546-406c-9a41-792697bb81fd] .
[2023-03-17 16:58:14.438] [debug] [b8eb8855-a546-406c-9a41-792697bb81fd] Extracting C:\Users\sales\Downloads\agent.zip to C:\Omniverse\library\OycNZ8…
[2023-03-17 16:58:14.459] [error] (node:2020) [DEP0005] DeprecationWarning: Buffer() is deprecated due to security and usability issues. Please use the Buffer.alloc(), Buffer.allocUnsafe(), or Buffer.from() methods instead.
(Use NVIDIA Omniverse Launcher --trace-deprecation ... to show where the warning was created)
[2023-03-17 16:58:33.116] [debug] [b8eb8855-a546-406c-9a41-792697bb81fd] Read description.toml file from C:\Omniverse\library\OycNZ8
[2023-03-17 16:58:33.120] [debug] [b8eb8855-a546-406c-9a41-792697bb81fd] Extracting C:\Omniverse\library\OycNZ8\windows-x86_64\package.zip to C:\Omniverse\library\OycNZ8\windows-x86_64…Hi Wendy,My team cannot install Farm agent.
Please see the attached logs.
Please advise!Farm Agent
Enterprise Branch version 104.0.0Error occurred during installation of Farm Agent: request to https://asset.launcher.omniverse.nvidia.com/farm-agent/104.0.0/windows-x86_64/jobs/stage-collect/11cdd4498b98eb35caf2b2b5f80d985b.zip?Expires=1682659715&Policy=eyJTdGF0ZW1lbnQiOlt7IlJlc291cmNlIjoiaHR0cHM6Ly9hc3NldC5sYXVuY2hlci5vbW5pdmVyc2UubnZpZGlhLmNvbS9mYXJtLWFnZW50LzEwNC4wLjAvd2luZG93cy14ODZfNjQvam9icy9zdGFnZS1jb2xsZWN0LzExY2RkNDQ5OGI5OGViMzVjYWYyYjJiNWY4MGQ5ODViLnppcCIsIkNvbmRpdGlvbiI6eyJEYXRlTGVzc1RoYW4iOnsiQVdTOkVwb2NoVGltZSI6MTY4MjY1OTcxNX19fV19&Signature=G7FTVPHX6mto2a-LHKwcV~wN7nq7Ou974VyGtatxubmlEWGDSme5Yk3Jm1QZytDFTJNkjnjRMUO8lYH-7OJ46YMJvEghey9AvcQlr6UEzblK8yVieh38etREZ0lP28kNCbbCh-MOZ3G8htg9sQGI9YGPNP7c07AqFDokWiSCF7USPAjBx1QBao5yFLU0F-~Ue8w2KTuupy3bKrOjIP-fUIVciRHcU5DRIWRRNZG-GtS8BWDF-ynDkita83g6Mo8xUDKq99yqBNHkH~DbJYhGiukDGfIGQuB46zRY1mR03upgCCMdBID5SyiQiKLOdfGP3xD1VOzOdNmx1DtMwoImrA__&Key-Pair-Id=K13PD0MHC2KFRP failed, reason: connect ETIMEDOUT 18.154.227.8:443Best,Alan Morales
Senior Media Production Specialist
BAE systemsPowered by Discourse, best viewed with JavaScript enabled"
841,running-fruitbox-tutorial-in-offline-mode-gets-stuck-at-rtx-ready-message,"When I run the fruitbox tutorial offline using the command below, it gets stuck at RTX ready message and doesn’t produce any results.Do you know what’s the reason and how it could be fixed?
I am able to use the same script and produce results in Omniverse Code Script Editor.Here’s the code:I ran it in terminal using the following command:Here’s where it gets stuck if run in offline mode:
image1477×956 165 KBHere’s what I see from my system
Screenshot from 2023-07-12 15-05-531851×956 189 KBScreenshot from 2023-07-12 15-07-02846×414 50.6 KBand here’s the full message:Before it gets stuck on “RTX Ready” message, I get this error message but searching on Web didn’t find much of a solution actually:This is what Claude 2 suggests:
Screenshot from 2023-07-13 11-06-59855×304 45.5 KBI am following this tutorial also for running the Omniverse Code in headless mode: https://docs.omniverse.nvidia.com/app_code/prod_extensions/ext_replicator/headless_example.htmlI am rerunning the algorithm again and here are results of $ tiptop and $ nvtop:
Screenshot from 2023-07-13 11-19-511842×835 218 KB
Screenshot from 2023-07-13 11-20-551842×835 57.7 KBDespite the progress shown above, no new output folder is created.Powered by Discourse, best viewed with JavaScript enabled"
842,cannot-connect-to-issac-sim-container-running-on-another-machine-on-the-lan,"I have the headless docker container for Issac Sim working fine. I can use the livestreaming client on the same machine where the docker container is running as 127.0.0.1.When I try to give the ip address of the machine running the container from another machine on the LAN it does not work. It says “Unable to initiate a connection to the server…”Pinging the machine ip address works so there is a network path. On the machine running the container I disabled the firewall(ufw).I am not sure where the logs go and how to check them. What port does it use? I checked with nmap and it seemed port 48010 was being opened when running the container.The client here is windows 11 and the server is ubuntu 22.04 having an RTX3080.Hi @sachinkundu  - The issue you’re experiencing might be due to the network configuration of your Docker container. By default, Docker containers are isolated from the host machine’s network and cannot be accessed from other machines on the LAN.To allow access to your Isaac Sim Docker container from other machines, you need to publish the necessary ports when you start the container. The --publish or -p option can be used to map a port inside the Docker container to a port on the host machine.The Isaac Sim livestreaming server uses port 47995 by default. So, you should publish this port when you start the Docker container. Here’s an example:In this command, -p 47995:47995 maps port 47995 inside the Docker container to port 47995 on the host machine. Now, you should be able to connect to the Isaac Sim livestreaming server from another machine on the LAN using the IP address of the host machine.If you’re still having trouble, check the Docker logs for any error messages. You can view the logs of a running Docker container using the docker logs command followed by the container ID or name. For example:Replace <container-id> with the ID or name of your Isaac Sim Docker container. The logs might provide more information about why the connection is failing.Powered by Discourse, best viewed with JavaScript enabled"
843,is-there-a-public-api-to-automate-setting-up-avatars-for-audio2face,"Is there a public API for Audio2Face so I can automate as much as possible the various steps to get blendshapes added to a character?Also, does this seem a useful process, or do things go wrong often enough that visual inspection of each step to verify correctness means automation is not useful. I realize adding the corresponding points probably needs to be done manually each time, but I want to import 20 to 30 characters. Repeating all the steps each time I often find is tedious and prone to human error. All the characters have identical bone structure etc.And with 105, are the tutorial videos likely to be updated? I found some of the buttons have moved or have different labels now compared to the videos from a few years back. (E.g. Is it worth waiting a week after 105 is out for new content to avoid wasting time on old videos, or should I just dive in now.)Thanks!This is a script used to setup the A2F pipeline for Reallusion characters. But it has quite a lot of hard coded parts that you need to update in order to make it work for your characters.
If you have a correspondence.json file for your characters, this should work nicely.
That said this only works with the latest 105 which is not out yet. So you might want to wait a week or so to try it out.
audio2face_autoSetup_v0001.py (3.2 KB)And yes,105 will have new tutorials for sure.This topic was automatically closed 14 days after the last reply. New replies are no longer allowed.Powered by Discourse, best viewed with JavaScript enabled"
844,how-to-trigger-action-graph-inside-simulationapp,"I am using a Python script instantiating a SimulationApp. I use SimulationContext.step() to step through my simulation. Part of the USD stage is an action graph to publish ROS2 messages. While I can trigger the publisher node successfully using On Tick or On Playback Tick, while running the simulation from the UI, the action graph does nothing when I run the scripted simulation.Any help appreciated
BrunoI am using a Python script instantiating a SimulationApp. I use SimulationContext.step() to step through my simulation. Part of the USD stage is an action graph to publish ROS2 messages. While I can trigger the publisher node successfully using On Tick or On Playback Tick, while running the simulation from the UI, the action graph does nothing when I run the scripted simulation.Hi @bruno.vetter - The issue you’re experiencing might be due to the difference in how the simulation is run in the UI versus in a script. When running the simulation from the UI, certain events like ticks or playback ticks might be automatically triggered, which might not be the case when running the simulation from a script.Here are a few things you can try:Thanks @rthaker,I want to try you suggesions 1 and 2, but I can find none of the functions you mention (trigger_tick(), add_action_graph()) in the api of SimulationContext: Core [omni.isaac.core] — isaac_sim 2022.2.1-beta.29 documentationHi @bruno.vetter  - You can try following instructions. If that doesn’t work for you then I will find the dev who can help you answer the question.Replace “/ActionGraph/OnImpulseEvent” with the path to your impulse event node.Powered by Discourse, best viewed with JavaScript enabled"
845,the-transform-window-now-resets-after-every-manipulation,"Ever since Kit 105, importing items from Blender is now vertical, instead of horizontal.Then I change it as shown in this short video, and the transform window resets after every change.Is this a bug?
Thanks if you can tell me why this resets? It used to stay after I changed something.Hi there,
So first the transform issue. You are in “Offset Mode”. So when you go to rotate your object by 270 degrees, it resets backt to 0.0 once done. And all of those values stay at 0.0. To switch back to “Transform mode” click the little icon in the upper right above the Z value.
image498×525 22.9 KBAs for your Y/Z axis problem, it could be a couple of things. 1) check how you are exporting your data from your DCC. It will say either Y or Z. Second check your Stage Preferences.
image1350×442 47 KBBefore this update I didn’t have to use the transform mode. I don’t know what the values mean when there are 4 of them in Transform mode?There are not even labels next to these values, and if I were to guess compared to the mode I like to use, it would be position, rotate, scale, what is the 4th one?Why aren’t there labels here? It was better to me the way it was before the update.We have not taken anything away from the system. Only added it. If you do not wish to use the Offset system just stick to the regular Transform system.Well you changed the offset mode, because before if I made a change, it stayed, it didn’t reset to 0.Ah I see what you are saying. No, that would not be the correct workflow. Much like other DCC software, offset is always working from 0.0. Once you type in an offset it should move and re-zero out. The change you enter should then update in the regular transform mode.So are you reporting this as a bug?No this is not a bug. It is a correction. It now works correctly.Well I liked it better working the way it was. I don’t know why there are 4 rows on transform view, and I can’t tell what value I set now because you erase it.I hate to go back to the old version, but I guess I will have to.I don’t see why you have 4 columns. It should be 3. X Y Z. Maybe reset your user preferences. I am not sure I understand why you need the offset to remain. That is not the way it is supposed to work. You have your offset and then you have your transform. As far as I know it has always worked that way.Here is the screenshot of 4 rows.I don’t know what this means?
I am not sure how you are getting this. Can you record a video of a totally fresh scene, add a cube and show me how you are getting this.Here is a video of me doing the same thing
The issue seems to happen only with items from Blender.
I took a simple object from Blender, I did apply rotation and scale and then triangulate and apply that before exporting as a usdc.The screen looks different with an object from Blender.Ok this seems like this is a blender/composer issue, not just a Composer issue. It may be a bug, but not with composer as such. Are you using an OV connector from Blender or just saving as a native USD ? Try some other format. Like fbx. Do you use any other DCC software ? Also try actually opening the usd file, not dragging it in. You seem to just keep dragging it in, but that is nesting one usd into another. Just save your blender file as a usd and then use File >  Open. Then go from there.What I want to see if native vs blender. So make a cube, make a sphere, native in Composer. Show me that working fine. Then drag in some assets from the nvidia library. Then open a blender file with the File > Open. Let’s isolate the issue to a blender file. Have you recently updated blender ? If you want to install the old one and show me the difference that is fine.Can you send me the USDc file please. Also why is it USDc ? Not just USD ?I opened the file this time as you suggested, instead of dragging and dropping, and that did fix the item moving weirdly.The Transform still appears with 4 rows.image1515×962 124 KBI use IClone, and in the last 24 hours found a program called Unigen, which looks promising also. I will try that and some other formats.Perhaps it is a Blender issue, but in the old version of Create this worked, so something changed.I will try some other formats and see if I get similar results.Thanks Corby@DataJuggler  It’s odd that you’re getting a single matrix for the transform.  Just to confirm, are you using the Blender build from the Omniverse Launcher?  If so, this version by default exports the transform as Scale, Rotate, Translate components, not a single matrix.  Can you verify that in the USD export option General -> Xform Ops is set to the default Scale, Rotate, Translate?Note that if you are using the mainline Blender release from the Blender Foundation (not from the Launcher), you won’t get this functionality and will always get the single matrix.I think you solved it, because no, I was using the 3.6 full release, since Omniverse 3.6 Blender is in alpha.I have both installed, and I imported the object I created earlier into the Omniverse version, and the transform node has 3 rows.I guess the 3.6 official release version of Omniverse has some issues.Thanks, I will stick with the Omniverse version. I don’t know if the Omniverse Blender team has any ties to the official Blender team, but they are quite different.CorbyI’m glad it’s working!  Indeed, we are working directly with the Blender Foundation to port the USD features in the Omniverse build to the official main branch.Powered by Discourse, best viewed with JavaScript enabled"
846,how-do-i-import-a2f-into-3ds-max,"My project uses 3dsmax.
There is no problem sending the modeling to a2f for face animation, but I couldn’t find a way to import it back to 3dsmax.
Isn’t there a good way?
I hope it’s not too complicated.want to import this as a 3dsmax morph.Here are 2 videos showing the full Blender -> Audio2Face -> Blender pipeline. While this is mainly focuses on Blender, the workflow can be applied in other DCCs.Audio2Face with Blender | Part 1: Generating Facial Shape Keys - YouTubeAudio2Face with Blender | Part 2: Loading AI-Generated Lip Sync Clips - YouTubePowered by Discourse, best viewed with JavaScript enabled"
847,none-type-object-has-no-attribute-get-force-sensor-forces,"Hi,
I followed the Force Sensor — isaacsim latest documentation (nvidia.com) to read a force sensros value, but It occur a error: None Type object has no attribute ‘get_force_sensor_forces’
As the figure below, I have added a Articulation Force Sensor to the panda_rightfinger, but can not read the force value.
image1325×911 75.5 KBHi @Robo_qq  - The error message you’re seeing suggests that the object you’re trying to call get_force_sensor_forces() on is None. This typically happens when the object (in this case, probably a robot or a sensor) hasn’t been properly initialized or doesn’t exist in the scene.Here are a few things you can check:Hi， @rthaker
I have check the things you suggested, but it still not work, can you have a look to the figure?The Atriculation Force Sensor have added to the “panda_rightfinger” and the simulation has started, you can see that there has force value showed on the right.16903327718431170×852 137 KBSee this documentation please
you need to add the ArticulationView object to the scene and use its APIs only after the reset.Powered by Discourse, best viewed with JavaScript enabled"
848,self-collision-issues-in-the-franka-panda-examples-franka-attractor,"Hi,I was adapting franka_attractor.py example and tested it in more challenging motions but I found that the simulator failed to accurately model collisions and the panda hand goes through the arm.
I suspect it happens when the hand is on top of the arm instead of the common use case where the hand is facing downwards.I did not change the API calls, only the targets for the controller (I attached the python file for reference).
franka_attractor.py (8.1 KB)Here is a screen capture that shows this behaviour:I’m using isaacgym 1.0rc4 with python 3.7.12, and using the Panda URDF that comes with the isaac-sim installation.Did anyone encounter such a problem and know how to fix it?Thanks,
TomPowered by Discourse, best viewed with JavaScript enabled"
849,cuda-processor-count-1-does-not-match-graphics-processor-count-2,"Hi folks, the last time I tried to work on Isaac SIM was some months ago, but I quit because I had a similar problem and I said to myself that I would just wait until an update solves the issue.
Probably an update solved the past issue where the GPU was not recognized at all and the program used to immediately close by itself after starting it.
Nowadays, the program doesn’t close by itself anymore, but the viewport is bugged and says “RTX Loading” forever.
I am using a laptop ASUS ROG Flow X16, it has an integrated GPU within the CPU (AMD R9 6000s) and a dedicated GPU NVIDIA RTX3070ti 8 GB.
This laptop has a MUX Switch, I don’t know… maybe this could be the cause since is a very recent technology. I don’t have an external display for testing. I tried also Ubuntu and I have the same issue.
By default, it utilizes
CWINDOWSsystem32cmd.exe.txt (54.2 KB)

Isaac Sim viewport1920×1200 99.6 KB

the AMD GPU to run the program, but since is very weak and without RTX maybe this is why is bugged because it could recognize the existence of an NVIDIA RTX GPU, but it uses the AMD. I have no clue, just supposing.Hi @marian.v.danci -  I am moving this question to right category and someone from our team will review and get back to you.Hi I have the same error:
2023-05-04 05:50:09 [716ms] [Error] [realm:gpu] could not open libcuda.so: Success
2023-05-04 05:50:09 [799ms] [Error] [gpu.foundation.plugin] CUDA processor count (0) does not match graphics processor count (1)In my case Im running it on Ubuntu 22.04 with a RTX 4070ti and a Ryzen 5 5600X.On /usr/lib/x86_64-linux-gnu i have 2 files libcuda.so:
libcuda.so.1
libcuda.so.530.30.02I don’t know if I can change the file required or i have to create the required file. I need helpHi, thank you! Sorry for the confusion, I didn’t know where to open this topicHello , i have the same problem when using the audio2face, ia have a laptop with AMD Ryzen 7 6800H with Radeon Graphics 3.20 GHz with RTX30702023-05-16 21:58:22  [Error] [gpu.foundation.plugin] CUDA processor count (1) does not match graphics processor count (2)coulyou help me find the solution pleasethanks youFor whatever reason this wasn’t an issue at one point, but these days, at least for me, if you have a non Nvidia card (or embedded graphics) as a second GPU, you have to select just the NVIDIA one with some special command line sauce. Appears to be too difficult for Omniverse to select just the card(s) that will actually work with Omniverse on it’s own - which makes sense, because for all it knows, maybe you are really intent on using the card without cuda that won’t work and you might feel really foolish when things do work, but secretly your AMD embedded CPU graphics are not actually being used. That would be tragic. So this opaque fail conveniently keeps you from suffering that.What’s that special command line sauce? I don’t remember. Audio2Face just updated and I don’t have it around anymore, so here I am looking for it again myself. Some developer whispers it somewhere in one of these forums.Same problem. Is there any way to solve it?
Runing on Ubuntu 20.04, rtx 4090, Core i9-12900k@rthaker hi, is there any news about it?I solved this problem by disabling the integrated gpu. Go to Device Manager → Display adaptors. Then right click on the first (if it is not the rtx one). Then re-installed Code or Isaac inside your omniverse. RestartFor a laptop is a very bad idea, since will drastically decrease the overall performance.
I don’t want to disable it, I want that someone from NVIDIA take some action since this problem persisted for an entire year.the same phenomenon as you given,but on  Ubuntu 20.04, rtx3080ti, Core i9-12900k.and when I run nvidia-settings,it displaye
202307191543501080×1440 172 KB
d:
nvidia-settings:GLib-GObject-CRITICAL **:g_object:unref:assertion ‘G_IS_OBJECT (object)’ failed.any resoved ways?Powered by Discourse, best viewed with JavaScript enabled"
850,nucleus-file-manipulation-in-python,"Is there a pythonic way to access a Nucleus server?Something like os.path.exists(), os.walk(), etc, but for files/folders on a Nucleus server?Powered by Discourse, best viewed with JavaScript enabled"
851,help-while-installing-usd-composer-it-stays-stuck-on-installing,"Help, while installing usd composer it stays stuck on installing.any help is appreciatedNoticed that it stays stuck on GPU_foundationthank you
Capture.PNG1920×531 36.7 KBPlease upload your launcher logs found in the launcher settings panel in the top right corner.launcher.log (279.6 KB)yes sir!Powered by Discourse, best viewed with JavaScript enabled"
852,how-can-i-uninstall-this-malware-called-omniverse,"#1 It’s nice to see that I am forced to submit in which area I work in order to access the forum.#2 It’s also nice to see that I am forced to say from which college I am.#3 I am forced to select in which area I develop. Even though I am a simple end user.#4 I uninstalled Omniverse via the normal Windows 10 process of opening the Preferences > Apps > Omniverse > UninstallThat was not sufficient.So I proceeded and used the Cleanup Tool:
https://docs.omniverse.nvidia.com/prod_launcher/prod_utilities/cleanup-tool.htmlNow Omniverse still pops up.Can I only remove Omniverse by switching to Mac and/or do a fresh Windows install with a fresh
AMD GPU?hello @missgeburtensalato -
I am sorry you are having issues with the uninstall. The steps you have taken should have resulted in the removal of the startup link.Can you check and see if this is still present (Startup Apps control panel). If present try and disable it.The current windows uninstall and cleanup tool does not remove the .exe and related files at the momentHello,I was (probably) able to fix the issue.#1 You have to deinstall it in Windows > Preferences > Apps > Omniverse > Uninstall
#2 https://docs.omniverse.nvidia.com/prod_launcher/prod_utilities/cleanup-tool.html
#3 Ctrl + Shift + Esc. Go to Autostart. Right-click on “Omniverse” and left-click on “Open file path”.
#4 Delete the folder.
#5 Hope and pray that all the junk is removed or do a fresh OS install.Powered by Discourse, best viewed with JavaScript enabled"
853,action-graph-car-configurator-tutorial,"Where can I find the tutorial for a car configurator in action graph that Dane Johnston mention in this talk about 30.07 in to the video?NVIDIA Omniverse Stream | Deep Dive - Omniverse Create 2022.1Hello @Jack_Jensen!  Let me know if this is the correct video you are referring to: Deep Dive Omniverse CreateIn the video that I referenced, at around 30.34, is talking about Action GraphHere is the tutorial he is referencing: https://docs.omniverse.nvidia.com/prod_extensions/prod_extensions/ext_omnigraph/car_customizer.htmlLet me know if this helps!Above link is not working . It is expired ?Powered by Discourse, best viewed with JavaScript enabled"
854,examples-for-the-deformable-liquid-particles-system-using-a-warp,"Hi. I found a very interesting example that uses warp.in the IsaacSim > Window > Warp > Scenes > Particle Simulation (please see the image).image1650×940 86.2 KBQ1. I tried to set it in my own environment, but I have no idea how to create “geompoints” on the Stage window.
Actually, this is the only part that I found about  geompoints.
Since I couldn’t find the helpful docs, I appreciate it If you give me some tips or detailed step-by-step documents.Q2. Is the warp version of the liquid particle simulation (i.e. omni-graph based) faster than the previous one (i.e., without omni-graph)?Thanks!… and I cannot find any information about ognParticleSolver on the Omnigraph documentation.Powered by Discourse, best viewed with JavaScript enabled"
855,issam-sim-camera-is-seeing-through-objects,"Hello,The camera in Issac SIM is looking through object, giving wrong depth values.

image1464×677 68.1 KB
On the left hand side you can see the robot (which i had to hide due to business limitations), is on top of a cube with full physical properties. It has a camera tilted down.
On the right hand side, you can see that the camera is looking straight through the cube. Yielding a depth of ~2 meters for the bottom pixel.Hi - Sorry for the delay in the response. Let us know if you still having this issue/question with the latest Isaac Sim 2022.2.1 release.@rthaker Not fixed! Not sure what is Nvidia’s workflow behind the scenes, but bugs will not fix themselves. In many of my open issues I was asked to check on latest version, and nothing was fixed.

image1032×441 37 KB
The camera in Issac SIM is looking through object, giving wrong depth values.Are you trying to use the GUI or python code? can you share the settings that you have for camera?@rthaker It happens both for manually created camera and python script created cameras.Here are the camera settings:

image536×780 30.4 KB
@omers How about reduce the minimum value of Clipping Range from 1.0 to 0.01?Powered by Discourse, best viewed with JavaScript enabled"
856,wind-effect,"Hi,
How is this tree leaves swaying effect achieved?
I tried wind force field but it blew the whole thing off. Couldn’t get it to work with deformable body as well.Hello @qazs!  If you haven’t already, please check out our documentation linked below and let me know if that helps.Hello,I’m attempting the same. While regular forces applied to deformable bodies have the expected effect, this has not been the case with force fields. In my attempts, the fields act on rigid bodies appropriately, but have no affect on the deformable ones. Are force fields and deformable bodies compatible?Any chance the tree-swaying has been achieved through animation?Powered by Discourse, best viewed with JavaScript enabled"
857,paraview-omniverse-connector-updates,"The ParaView 5.9 Omniverse Connector version 101.0.0 has just been released! New features include:Also, the robustness of the vdb output functionality is greatly improved.Note that this version still doesn’t support ParaView 5.10; we aim to release the connector for the latest version of ParaView very soon.For more details and example usage, see the updated documentation page!A new version of the ParaView Omniverse Connector has been released for both ParaView 5.9 (101.1.0) and ParaView 5.10 (101.2.0).Improvements mainly include:Known issues include:More details can be found in the updated documentation page.A new version of the ParaView Omniverse Connector has been released, making a full step to version 200.0.0. It is only available for ParaView 5.11.It introduces a number of improvements:Some bugs have been fixed:Known issues include:More details can be found in the updated documentation pages.Version 200.1.0 of the ParaView Omniverse Connector has been released for ParaView 5.11.The following are the most important improvements:Fixes include:Limitations regarding colormapping in USD Composer from the 200.0.0 release still remain for now, as per the previous post.For a detailed description of all changes, see the documentation pages.Powered by Discourse, best viewed with JavaScript enabled"
858,can-iray-in-omniverse-render-glass-dispersion-like-offline-render-should,"Hi Omniverse team,Can Iray in omniverse do this yet as shown in attached? Thank you.
image645×544 28.8 KB
Did a test, using OmniSurfaceBase with dispersion abbe set to >1 it is possible to get a glass dispersion effect as attached.
capture.2023-06-01 13.01.571920×810 26.6 KB
This topic was automatically closed 14 days after the last reply. New replies are no longer allowed.Powered by Discourse, best viewed with JavaScript enabled"
859,torque-control-implementation,"Hi all
I am trying to implement a simple torque control for a Franka robot in Isaac Sim. I am doing this by modifying franka_articulation.py.
It looks like dc.set_dof_position_target works fine and sends the robot to the desired position. However, I dont see any motions when calling dc.set_dof_velocity_target or dc.apply_dof_effort to control the joint velocity or torque, although both of these calls return true, i.e. success.Are there any resources/examples that can help clarifying these use cases?Any help will be appreciated.Here is the code:import os
import carb
from omni.isaac.python_app import OmniKitHelper
import time
import math
import numpy as np
CONFIG = {
“experience”: f’{os.environ[“EXP_PATH”]}/omni.isaac.sim.python.kit’,
“renderer”: “RayTracedLighting”,
“headless”: False,
}
if name == “main”:
# Example usage, with step size test
kit = OmniKitHelper(config=CONFIG)
import omni.physx
from omni.isaac.dynamic_control import _dynamic_control
from omni.isaac.utils.scripts.nucleus_utils import find_nucleus_server
stage = kit.get_stage()
asset_path=“/home/user/isaac_models/franka1.usd”
omni.usd.get_context().open_stage(asset_path)
kit.play()
# # perform step experiments
kit.update(1.0 / 10.0,1.0/500.0,50)
dc = _dynamic_control.acquire_dynamic_control_interface()
ar = dc.get_articulation(“/Franka”)
if ar == _dynamic_control.INVALID_HANDLE:
print(“*** ‘%s’ is not an articulation” % “/Franka”)
else:
root = dc.get_articulation_root_body(ar)
print(str(“Got articulation handle %d \n” % ar) + str(“— Hierarchy\n”))
body_states = dc.get_articulation_body_states(ar, _dynamic_control.STATE_ALL)
print(str(“— Body states:\n”) + str(body_states) + “\n”)
dof_states = dc.get_articulation_dof_states(ar, _dynamic_control.STATE_ALL)
print(str(“— DOF states:\n”) + str(dof_states) + “\n”)
dof_props = dc.get_articulation_dof_properties(ar)
print(str(“— DOF properties:\n”) + str(dof_props) + “\n”)
dof_ptr=[dc.find_articulation_dof(ar,“panda_joint”+str(i)) for i in range(1,8)]
q=[dc.get_dof_position(dof_ptr[i]) for i in range(7)]
qd=q.copy()
qd[1]=-1.5
for i in range(1000):
kit.update(1.0 / 10.0,1.0/500.0,50)
dc.wake_up_articulation(ar)
q=[dc.get_dof_position(dof_ptr[i]) for i in range(7)]
e=list(np.asarray(qd)-np.asarray(q))
k=10
tau_d=[k*e[i] for i in range(7)]
res=[dc.apply_dof_effort(dof_ptr[i],tau_d[i]) for i in range(7)]
print(res,tau_d)
# cleanup
kit.stop()
kit.shutdown()Thanks for the question @h.azimian will get back to you with a sample showing how to use position/velocity control. torque control should work, but I need to confirm.Hi @h.azimian , thanks for the question. Regarding the torque control you can use dc.apply_dof_effort or dc.apply_articulation_dof_efforts as you stated, the reason you didn’t see any motion is because of the scale, the USD of the franka arm has a stage scale of CM (we are in the process of changing this for the next release). So you will have to apply a larger magnitude to see any motion. You can use the following script in the script editor window after creating a franka arm and starting the simulation.from omni.isaac.dynamic_control import _dynamic_control
import numpy as np
dc = _dynamic_control.acquire_dynamic_control_interface()
art = dc.get_articulation(“/Franka”)
dc.wake_up_articulation(art)
joint_efforts = [-np.random.rand(9) * 1000 * (100**2)]
dc.apply_articulation_dof_efforts(art, joint_efforts)Regarding velocity control, there are several ways to achieve this. We will be adding some samples in the next release. For now you can use the following script to do so. Please note that you need to change the drive stiffness to zero for the corresponding joints before the articulation gets registered in dynamic_control so before this line specifically  - art = dc.get_articulation(“/Franka”) -from pxr import UsdPhysics
stage = omni.usd.get_context().get_stage()
for prim in stage.TraverseAll():
  prim_type = prim.GetTypeName()
  if prim_type in [“PhysicsRevoluteJoint” ,  “PhysicsPrismaticJoint”]:
    if prim_type == “PhysicsRevoluteJoint”:
      drive = UsdPhysics.DriveAPI.Get(prim,  “angular”)
    else:
      drive = UsdPhysics.DriveAPI.Get(prim,  “linear”)
    drive.GetStiffnessAttr().Set(0)
from omni.isaac.dynamic_control import _dynamic_control
import numpy as np
dc = _dynamic_control.acquire_dynamic_control_interface()
art = dc.get_articulation(“/Franka”)
dc.wake_up_articulation(art)
joint_vels = [-np.random.rand(9)*10]
dc.set_articulation_dof_velocity_targets(art, joint_vels)You can also see the attached videos for a step by step illustration through the UI, likewise you can use the same snippets of code if you are running IsaacSim headless and launching it from python. Please let us know if you have any other questions.This tutorial is nice!
The only thing I didnt get is: I have to replace dc.apply_articulation_dof_efforts(art, joint_efforts) to dc.set_articulation_dof_efforts(art, joint_efforts)
Is my usd version different or do I need to set something?And in addition, I tried to grasp deformable objects with Franka “torque-controlled” gripper.
the displacement seems not correct with the object, there is a gap between deformable object and the gripper fingers.

Screenshot from 2021-12-27 18-05-361381×699 157 KB
Can you try reducing the collision and rest offset parameters?https://docs.omniverse.nvidia.com/app_create/prod_extensions/ext_physics.html#collisionparametersThat fixed a similar issue for another userHow does joint controlled? (Franka Gripper)
I tried to set Franka Gripper joint position target and velocity target at the same time, the gripper moves.
However, I use dc.get_dof_effort(gripper_joint) it returns 0, is there no torque applied to control the joint?dc.set_dof_effort(joint, effort) seems not actually applying the commanded torque value.It seems the effort is calculated based on the target  position X and  velocity V. Effort= K_joint * (X_now- X) + D * (V_now - V). The equation is correct, but the joint seems to be not in torque control.I have a desired joint position and velocity, I use the equation Effort= K_joint * (X_now- X) + D * (V_now - V) to get my desired effort and do dc.set_dof_effort(joint, effort), but since the target position and velocity is predefined in USD file, I can not control the real desired joint position and velocity.What I want to implement is a simple PD controller for joint, which out puts desired torque to actuate the joint to get desired position and velocity.set_dof_effortI think the PD controller for joint is already implemented and you can directly set the desired postion and velocity with this function set_dof_state(...) (Dynamic Control [omni.isaac.dynamic_control] — isaac_sim 2022.1.1-release.1 documentationI can set desired position and velocity with dc.set_dof_position_target() and dc.set_dof_velocity_target(), the robot moves but when I call dc.get_dof_effort(gripper_joint) it returns 0. It seems there is no torque that actually controlling this motion.
And also, I want to add a contact sensor for finger tip, it also does not workDid you call get_dof_effort(...) before or after set_dof_state(...)? I had the same issue with get_rigid_body_linear_velocity(...) (How to use set_rigid_body_linear_velocity(self, arg0, arg1) from class DynamicControl? - #4 by hoanggiang). Could you try get_dof_state(...) (Dynamic Control [omni.isaac.dynamic_control] — isaac_sim 2022.1.1-release.1 documentation) also? Just to double check the position and velocity as well.I call get_dof_effort() after set_dof_state, and it returns nothingTo simplify my question:What I have tried for question 1 is that, I set K=0, i.e. a pure damping control. And I calculate desired torque with my PD controller. But this makes no sense, since the idea of PD control is to design your desired motion impedance, there will be two dynamics coupling each otherI conducted an experience and confirmed the torque can’t be applied to the joint via set_dof_torque(...) or set_dof_state(...). However, the position and velocity can be set by set_dof_state(...), no matter if the Angular Drive is mounted. If the Angular Drive is mounted, the position and velocity will be overwritten.
So, there is no direct torque controlBtw, what is the difference of set_dof_position and set_dof_position_target?And I dont understand how the joint is drived exactly.
get_dof_effort only returns effort = K_usd * (set_dof_position_target - init_position_usd) + D_usd * (set_dof_velocity_target - init_velocity_usd), where  init_position_usd and init_velocity_usd is defined as joint parameters in usd file. This is not the real torque the joint should output.Driving torque should be K_usd * (set_dof_position_target - position_now) + D_usd * (set_dof_velocity_target - velocity_now)This would be tricky for applications involving contacts.Will this be updated in future?If you set the drive stiffness and damping to zero you should be able to do torque control. This way the position/velocity target do night fight the specified torque value.Any yes, getting torque values out of sim for joints is missing some information. We are planning on exposing this in DynamicControl in the future, but there is an alternative way to access it via force sensors
https://docs.omniverse.nvidia.com/app_create/prod_extensions/ext_physics.html#force-sensorsHello, may i ask you some question?
Where did you find effort = K_usd * (set_dof_position_target - position_now ) + D_usd * (set_dof_velocity_target - velocity_now )? I just find this :stiffness * (position - target_position) + damping * (velocity - target_velocity) in create docs which is a resistance against motion. And when i use urdf files to import my own robot, i can also use set_dof_effort to control my robot. So i think maybe there is no target_pos and target_vel in usd files?
And i am trying to construct kinetic model of a wheel(such as T - Tload = Jdw/dt), i dont know how to make it.because i dont know how does friction, damping etc. work. maybe you have some advise?Hello, can you successfully build the dynamic model of the wheel?If successful, can I ask how you built it?
@user62652Powered by Discourse, best viewed with JavaScript enabled"
860,define-a-custom-physics-model-in-isaac-sim,"Hello everyone,is it possible to implement a customized physics model, for example a model with non constant gravity?
The idea would be to write our own equations of motion to move the objects in the scene.Thank you.Currently with PhysX SDK as the core solution one would have to disable gravity and apply forces to all the bodies in the stage every frame. This way you could emulate the desired behavior.Regards,
AlesPowered by Discourse, best viewed with JavaScript enabled"
861,add-viewport-issue-after-update-create-to-2022-1-0-beta,"Hi all
I have a test extension which create a new viewport window in a button click callback, I used omni.kit.viewport module, after update to Create 2022.1.0 Beta, this module seems change to omni.kit.viewport_legacy.So I only replace omni.kit.viewport to omni.kit.viewport_legacy and run my extension.
The console output this log every frame, and if I drag in viewport, the fps will reduce.2022-03-25 03:50:41 [52,495ms] [Error] [carb.events.python] AttributeError: ‘DrawSceneViewportManager’ object has no attribute ‘_skeleton’
At:
e:\insov\inssimulation\app\extscache\omni.anim.retarget.ui-103.1.6+103.1.cp37\omni\anim\retarget\ui\scripts\draw_scene_viewport.py(510): _on_updateHow can I fix this?Hello @user76666! I am passing this issue along to the development team. Could you please attach your full logs just in case there is something in them that can help us identify the issue?log.txt (1.1 MB)
Hi @WendyGram
Sorry for late reply, this log still print every frame after update to 2022.1.1 Beta
Reproduce path:I create a new viewport by snippet belowLook forward to your reply.What I did (given the viewport name):Powered by Discourse, best viewed with JavaScript enabled"
862,i-got-my-vdb-converted-to-mesh-but-exported-usd-crash-composer,"
image1920×1106 171 KB
I got my vdb converted to mesh, but exported USD just crashed Composer when I try to open it.What is the solution?
I made the mesh by following this tutorial:I have not baked anyhow the modifier in blender…PekkaReason for this is I need to work with still VDB clouds ( this one is anvil cumulus ) in real-time mode.Hello @pekka.varis.  Would you be able to provide the USD which crashes Create/Composer?  Please feel free to message me directly for this if you prefer.  Thank you!Powered by Discourse, best viewed with JavaScript enabled"
863,error-in-python-script-sample-for-4-3-introduction-to-cloner-tutorial-isaac-gym-tutorial,"For the “4. Getting Started with Cloner (Isaac Gym Tutorials) - 4.3. Introduction to Cloner” section of the tutorial, the provided sample python code throws the following error.link: 4. Getting Started with Cloner — Omniverse Robotics documentationIn “4.3. Introduction to Cloner” section of the tutorial, the instruction says:“Please make sure omni.isaac.cloner is enabled from the Extensions window before running the snippets.
Let’s first start with a simple use case of the Cloner interface. In this example, we will create a scene with 4 cubes.”…“We can add a transform to each cube, simply replace the last line of the previous code with the following:”so when I replace the last line of the first block of code with the second code block and run the scipt on the Script Editor extension it throws the following error and only 1 cube is generated when there should be 4 cubes.
image663×659 49.4 KB

image1706×648 230 KB
There is an error in the python script provided and it needs to be fixed.Powered by Discourse, best viewed with JavaScript enabled"
864,how-to-access-a-node-from-my-custom-writer-or-custom-annotator,"I am trying to find aspects of the simulation such as the distance of the object from the camera, the rotation of the object respective to the camera, and the illumination settings. I am unsure how to access this information from my custom writer - can a writer access an information bundle by itself or does it need a custom annotator?This is my AutoFunc node, I extract the prims locations from their names and compute the distanceLooking through the annotator registry files I see that an annotator is registered with a nodeHow should I access the output of my node omni.replicator.ComputeDistance in the writer?Unfortunately, because the annotators are executed asynchronously from the simulation, accessing the USD directly from an annotator can lead to issues where the annotator accesses data out of sync with the rest of the ground truth. There’s currently no easy way to get around this, but we are working on a better solution that will guarantee correct results.For your purposes though, I can suggest an alternative: In your custom writer, add the bounding_box_3d and camera_params annotators. Those will provide you with the world-space transforms of both any semantically labelled objects and of the current camera. From there a simple matrix multiplication will give you the transforms in camera-space. Please reply back if you run into any issue with this path and I’ll be happy to give you a more concrete example to try.Powered by Discourse, best viewed with JavaScript enabled"
865,export-urdf-from-nvidia-isaac-sim,"Continuing the discussion from Export URDF from NVidia ISaac Sim:Hello,
As this has been discussed, this feature is on the roadmap. Could you please provide a rough estimation of when this be available?Best regards,
YuriyHi @yfedi - We will try to enable this feature in the upcoming release around Aug/Sep time frame but if not in that release then the release after that.Powered by Discourse, best viewed with JavaScript enabled"
866,how-to-skip-frames-during-physics,"For the generation of synthetic images for bin picking, I want to drop an object into the bin, then capture several photos with different camera poses, and then drop another object into the bin. With the current code, 30 photos are taken while the object is falling into the bin.Love greetings Alexander
workflow2424×2578 214 KB


Scenario1430×995 29.2 KB
Hello @alexanderbischof255! As you’ve discovered, writers write on every frame. Writer triggers is a feature we are currently developing but which is not quite ready yet. I suggest as a workaround for the time being creating a custom writer which looks at the object’s transform (which you can get from the bounding_box_3d annotator) and skipping writing until the object’s Z coordinate falls to an acceptable threshold.Thank you for the feedback!This topic was automatically closed 14 days after the last reply. New replies are no longer allowed.Powered by Discourse, best viewed with JavaScript enabled"
867,adding-randomization-to-an-exiting-scene-and-saving-it,"Hello,I am trying to use Omniverse Replicator to add domain randomization to existing objects in a scene. I used to use the old randomization tools in previous versions of Isaac-Sim. Using these, I was able to add randomization components and save the scene without losing this information. I am trying to move to the newest version of Isaac Sim and use Replicator, however, I am having trouble saving whatever randomization I add to the scene. I get that Replicator is creating an action graph after I run a certain script, but whenever I save and reopen the scene, the graph will be corrupted and won’t work.Does anyone have a suggestion on the best way to achieve this? Is Replicator the way to go?Can you include an example USD scene (before/after) for what you mean by “corrupted”?There is no before USD scene because the moment I save the scene, the problem will occur. But I can share a screenshot of the stage and layer before saving and after saving.Before

before1125×737 63.5 KB
After

after1125×737 22.8 KB
Save Flattened As, would give

flat1126×736 60.4 KB
Flattening the scene will save the graph, and the randomization will keep working, however, it’s not a very convenient way of doing it since all the other layers in case of a complex scene will be lost.Thank you.Hello @anthony.yaghi, if flattening the layers is not an available solution for your use case, you can omit the with rep.new_layer(): line - this will write Replicator directly into the existing authoring layer, normally Root.Powered by Discourse, best viewed with JavaScript enabled"
868,a2f-2023-1-custom-character-questions,"Hello friends, I been trying to give my own character facial/lip movements with A2F. But its not working. I have watched tutorials yet still have these confusions…
1, do i have to create same blendshape&orders for my character before doing the transfer? i noticed in the newest version tutorial the custom character head mesh doesnt have same blendshapes before doing the transfer but i do it the same way my character is not moving with Clair or Mark. also i have issues create blendshapes for my character…
2, whats the difference between character transfer and arkit method?Thank you in advance! I’m really confused.Audio2Face can generate blendShapes for your character.Your character only needs to have blendShapes if you want to animate your character in another 3D application, e.g. Maya. Even then, having blendShapes is not necessary. You can export a cached version from Audio2Face into other 3D application.Arkit is the name of the collection of 52 blendShape targets that can be used to create facial animation. It was created by Apple I think for 3D avatars.You can use Audio2Face’s Character Transfer tab to generate Arkit BlendShapes for your characters.This topic was automatically closed 14 days after the last reply. New replies are no longer allowed.Powered by Discourse, best viewed with JavaScript enabled"
869,unable-to-login-to-omniverse-launcher-not-found-error,"Hi Omniverse team,Can seems to login any more to omniverse, keep showing this Not found page. I even did a reset of password to be safe it wasn’t password error but still same errorWe need more information about the error, at least a screenshot of the problem and maybe Launcher logs. Can you provide your launcher log at “C:\Users\USER.nvidia-omniverse\logs\launcher.log”@Richard3DI did a restart of the launcher app and is able to login already. Thank you.Great thanksThis topic was automatically closed 14 days after the last reply. New replies are no longer allowed.Powered by Discourse, best viewed with JavaScript enabled"
870,cad-import-takes-30-minutes,"I’m evaluating Omniverse and I just tried a medium sized STEP file we have and it took over 30 minutes (I walked away after a while so I’m not 100% sure)…The same file imports in Rhino in about a minute, and in Unreal about the same.Anyone else experienced extreme slowness of CAD imports in Omniverse? Is this common?Also, during all that importing time, the application is completely frozen up!@EDRobert  - I’m fairly sure you will not have this issue in the next release as we have changed the back end converter. I tested a 96mb step file and it imported in a few seconds. The older importer does indeed go way too slow (I’m still waiting for it to finish!).Powered by Discourse, best viewed with JavaScript enabled"
871,omni-isaac-ui-ui-utils-str-builder-on-clicked-fn-not-being-called,"I have created an ui element with str_builder and assigned a “test” function to the on_clicked_fn parameter. This test function has a simple print to test. But nothing i have tried resulted in any prints. I tried the same function on a checkbox (adding the positional argument it needs) and it printed perfectly. Even when selecting a file using use_folder_picker nothing is printed.Hi @DiscoFlower8890  - Can you please share the code that you have written? that will help diagnose the issue better.Hi @rthaker
Here:That should be enough as other parts of the extension work perfectly.Hi @DiscoFlower8890  - The str_builder function is used to create a string input field with an optional folder picker button. The on_clicked_fn function is called when the folder picker button is clicked, not when the string input field is clicked or changed.If you want to run a function when the string input field is changed, you might need to use a different function or method. For example, you might need to use a on_value_changed_fn function if one is available.Here’s an example of how you might do it:def test_change(value):
print(f""You changed the string box to: {value}"")with ui.HStack(height=20):
dome_texture = str_builder(
label=“HDRI texture path”,
tooltip=“Directory where the HDRI texture is stored. The path must not end in a slash.”,
use_folder_picker=True,
item_filter_fn=lambda item: item.is_folder or item.path.endswith(‘.hdr’),
folder_dialog_title=“Select Path”,
folder_button_title=“Select”,
on_value_changed_fn=test_change
)
In this code, test_change is called whenever the value of the string box is changed. Note that this is just an example and the actual code might look different depending on the specific functions and methods available in your UI library.Hi @rthaker
Ah, that explains half the issue. But I’m not getting any prints in console even when pressing the folder picker button. (Neither picking a full path an hitting select nor clicking the button and closing)
Video of me spamming the folder picker buttonAnd it seems str_builder doesnt have on_value_changed_fn parameter:Docs for the str_builderHi @DiscoFlower8890  - It seems like the str_builder function does not have an on_value_changed_fn parameter, as the error message suggests. This means that the function does not support running a function when the value of the string box is changed.As for the on_clicked_fn not working when the folder picker button is clicked, it could be due to a number of reasons. Here are a few things you could try:If none of this works then let me know and I can help you connect with right developer.Hi @rthaker - Yep that seems to be the case.I tried all of the things you suggested and got these results:and got:on_clicked_fn is mentioned at the top, but not under the “Args”.  Which is odd.None of the suggestions worked. Looking at the help print it I’d guess that it’s not implemented properly.@DiscoFlower8890  There is a newer set of UI utils that is generally much easier to use and manage.  Check out the docs for UI tools here: UI Element Wrappers scrolling down to see the newer UI tools (looking for StringField()).P.S. You can see a demonstration of an extension using all the new UI wrappers here: UI Component Library, but you’d have to download the extension as described in the linked tutorial to see the code.Powered by Discourse, best viewed with JavaScript enabled"
872,launcher-error-and-isaac-examples-error,"Hi there,I have 2 trouble.Thanks !!!Hi, what commands do you use to run Isaac Sim when you get those errors?
Can you try running with the --reset-user flag?
Please also share the full logs. There may be other clues in the logs.Powered by Discourse, best viewed with JavaScript enabled"
873,usd-composer-keeps-crashing-with-extension,"Hi,I’ve got an extension running that requires functions to run one after the other, but most operations in OV run in async functions as far as I have understood.To countermeasure, I’ve included several “asyncio.ensure_future(omni.kit.app.get_app().next_update_async())” lines after each important step, but as I added these, it’s causing the program to crash.Is there any particular reason for this, or any suggestion to keep Omniverse running while my app continues running? I’ve also checked the log files and there are no errors or warnings that could contribute to the shutdown of the operation, and there is ~7GB of free GPU memory and ~16GB of free RAM.I’m running USD Composer 2023.1.1 and my GPU is an RTX 3060.Thanks in advance for your help!Powered by Discourse, best viewed with JavaScript enabled"
874,how-to-programmatically-rotate-the-prim-correctly-with-the-new-pivot-position,"Hello everyone,I have encountered a challenge while automating the manipulation of a Primitive object. The Primitive has a noticeable difference in pivot position compared to its parent object’s xform. To ensure correct scaling, I utilized a pivot tool to adjust the pivot position of the parent object accordingly. With the help of Omniverse’s rotation tool, I have successfully achieved accurate rotation, translation, and scaling of the Primitive using the new pivot point.However, as I attempted to replicate these transformations programmatically using a Python script and a 4x4 transformation matrix for a -90 degree rotation around the Y-axis, I encountered unexpected stretching effects. Despite the Primitive rotating correctly, the resulting appearance is not as intended.I would greatly appreciate any guidance or insights on how to address this issue and achieve the desired transformation accurately without the presence of stretching effects. Thank you in advance for your assistance.
12076×1138 263 KB


22075×1134 273 KB


32054×1126 305 KB
import omni.usd
import math
from pxr import Gf, UsdGeomdef get_rotation_matrix(angle):
rotation = Gf.Rotation(Gf.Vec3d(0, 1, 0), angle)
matrix = Gf.Matrix4d(1.0)
matrix.SetRotate(rotation)
return matrixrotation_matrix = get_rotation_matrix(-90)stage = omni.usd.get_context().get_stage()
prim = stage.GetPrimAtPath(“/scene/Meshes/Sketchfab_model/root/GLTF_SceneRootNode/Box_28”)
transform_matrix = prim.GetAttribute(“xformOp:transform”).Get()prim.GetAttribute(“xformOp:transform”).Set(rotation_matrix * transform_matrix)Hi @renton.hsu.vfx. Have you seen this tutorial? Transformations, Time-sampled Animation, and Layer Offsets — Universal Scene Description 23.05 documentation I think this should give you an idea of how you could programmatically recreate the xformOps that you created manually using the pivot tool and transform manipulators.This topic was automatically closed 14 days after the last reply. New replies are no longer allowed.Powered by Discourse, best viewed with JavaScript enabled"
875,multi-level-semantic-segmentation,"Hi,I have a question regarding the current format of the semantic segmentation data. Right now, each point in the data is represented by a single ID in a 1-dimensional array. However, I’ve noticed that some points can belong to multiple classes. For example, let’s say we have a car and a wheel. The points that make up the wheel are also part of the car, so they should have two IDs: one for the car and one for the wheel.Is there a way to update the generated data from the annotator to include all the IDs associated with each point?Thanks for your help.Best regards,AnthonyHi @toninsemaan , in the case you describe above, the wheel will have a different semantic id than the car body. The id is assigned by the semantic label of a prim itself, combining all its parents’ semantic labels.To expand the case you described, if you you have the following structure.You will have a semantic id of [0, 1, 2] and a id to labels array that maps from semantic ids to semantic labels.
{0: [(""class"", ""car"")], 1: [(""class"", ""car, wheel"")], 2: [(""class"", ""car, seat"")]}
If you want a prim to have a list of semantic ids (based on itself’s semantic label and parents’ semantic labels), you can do it in the post-processing step.Powered by Discourse, best viewed with JavaScript enabled"
876,using-multiple-franka-robots-with-simulationapp,"Hello,I’m working on a project with a Franka robot and I have a question. I want to run lots of copies - maybe even thousands - of this robot at the same time with SimulationApp.I want to test different points where the robot can grab things in the same scene. I hope by having lots of robots running at once, I can test these points quicker.You can find my project code here: isaac-performance/nomlab_test.py at master · kciebiera/isaac-performance · GitHubSo, here’s what I need to know:Thanks for your help!Best,Krzysztof1- Yes, you can run lots of cloned robots concurrently. See here: 1. Overview & Getting Started — Omniverse Robotics documentation and GitHub - NVIDIA-Omniverse/OmniIsaacGymEnvs: Reinforcement Learning Environments for Omniverse Isaac Gym
2- You can see the minimum requirements for Isaac Sim here and the recommended ones: 1. Isaac Sim Requirements — Omniverse Robotics documentation
3- Run through our tutorials and envs provided first. Make sure to run your pipeline on the gpu by specifying the device in the SimulationContext.Let us know if you have more questions and make sure you post questions in the right forum here: Isaac Sim - NVIDIA Developer ForumsPowered by Discourse, best viewed with JavaScript enabled"
877,2d-and-3d-bounding-box-visualization-of-fruitbox-tutorial,"I have two requests:visualization of tight 2 bounding box in fruitbox demovisualization of 3D bounding box in fruitbox demoFurther, how can I save the camera intrinsic matrix (K) when running the script using BasicWriter?
I believe not having access to K is what viz of 2 is not working.With #1, problem is the 2D bounding box is not visualized where it should be.Screenshot from 2023-07-13 15-07-32631×552 149 KBas for 3d projected bounding box on the 2D image, I do need to have K. Here’s the code with a dummy K:can you please help figure how to access K, fix the visualization of tight 2D bounding box, as well as the 3D projected bounding box? I think I have an overall idea but need some guidance.Screenshot from 2023-07-13 15-56-591021×1077 98.1 KBHi @mona.jalal , for bbox 2D, I think y axis’ 0 starts from the top, so you don’t need to inverse it. For K, I assume you need the camera projection matrix? You can get the camera projection matrix from the cameraParam annotator.Powered by Discourse, best viewed with JavaScript enabled"
878,unable-to-connect-to-localhost,"Hello all.
I recently work on the Isaac Sim. However, the localhost label keeps unaccessible.here is the messages in the Console tab.
image1920×1080 449 KB
The Omniverse-Launcher/Nucleus keeps loading but not loaded yet.I want to create a local Server to share with others.
What should I do now?if there’s anything else need to be shared, I’d like to!
thk u!Can you please validate the services are running by going to the launcher, nucleus tab, hamburger menu, settings?it worked. thanks!Powered by Discourse, best viewed with JavaScript enabled"
879,link-to-kinetic-sculpture-digital-twin-example-source,"Would it be possible to get a link to the source code / files to the kinetic sculpture featured in the video below? It’s really cool and shows a lot of what Omniverse can do in a pretty isolated example.I couldn’t find it in the PhysX source on github - maybe I’m looking in the wrong spot.Thanks!Hi,
PhysX SDK is used here for the simulation purpose. The source USD file did have animation data stored in USD and individual meshes were simulated as SDF (signed distance field) collisions.
So in a sense the source for the simulation is the USD file itself, where it holds the animation information moving kinematic bodies and then a precise collision detection is done thanks to the SDF collisions.So in a sense if you have a model with animation you can assign kinematic rigid bodies to the individual pieces that should be moved by animation and then add dynamic bodies to pieces that should move. Eventually you might want to restrict movement with physics joints.We did not released this particular demo because of licensing, but we are planning to release similar demo for the upcoming release this month.We did not released this particular demo because of licensing, but we are planning to release similar demo for the upcoming release this month.Just following up if that similiar demo was in fact released already, and if so could you link it for us here?Thank you in advance!Yes, there should be a demo showcasing this now (in USD Composer 2023.1.0).
Open Window->Simulation->Demos, then Complex Showcases → Analog Digital Clock demo.Should look like this:
Regards,
AlesPowered by Discourse, best viewed with JavaScript enabled"
880,isaac-sim-jupyter-notebook,"Hello,I am currently doing the initial tutorials and there was an issue I couldn’t resolve.In this tutorial: Core API - Hello WorldI couldn’t find the live sync option, only the lightning symbol, where you can start a session.
In the 17. jupyter-notebook tutorial it is the same problem.When i execute the cells in the jupyter notebook, every cell executes fine, but I don’t get any updates in the Isaa-Sim app.
Could you maybe point me to some documentation/tutorial, that shows, how this works in the new Isaac-Sim version?Best regards!I have the same issue.Hi,We are looking into this and will get back to you.Is there any update on this issue?Any updates on this? Jupyter notebook still doesn’t work with Isaac Sim 2022.2. Frustrating.Powered by Discourse, best viewed with JavaScript enabled"
881,import-ue4-maps-to-isaac-sim,"Hi,I am currently running the Isaac Sim container (Omniverse launcher not installed), so I don’t have access to the Omniverse - Unreal connectors.Is there anyway to import UE4 maps to Isaac Sim without the connector? Thank you!Hi @j.t - I moving this discussion to Omniverse General thread. Someone will help you with this request.Thank youPowered by Discourse, best viewed with JavaScript enabled"
882,is-there-a-way-to-retrieve-collision-information-between-one-or-more-actors-in-simulation,"Hi,
I am using isaac_sim-2022.2.1 and the OmniIsaacGymEnvs to build my own robotics RL application.
Specifically, my agent needs information, for each robot link, on the contacts point (if any) with the rest of the environment, the contact normals and, possibly, the normal force.
Initially, I was using IsaacGym for this purpose and, in particular, I found the methods get_rigid_contacts() or get_env_rigid_contacts() to get the info I needed. The problem is that those methods are buggy (see this issue) and do not work when using the gpu pipeline. I consequently decided to switch to Omniverse hoping for a more complete API.
However, I have failed, up until now, to find a method/methods to retrieve the information I need in  IsaacSim. Can anybody help me on this?
Thanks in advanceHi @andreap - Following collision related forum questions might be helpful to answer your question:Powered by Discourse, best viewed with JavaScript enabled"
883,scene-optimizer-v105-not-available-despite-being-released-last-month,"Changes covered in release notes for v105 from April are still not available in Create, as the latest extension version in Create (2022.3.3) is v104.5. Any ideas when v105 will be available?
Screenshot 2023-05-25 174906915×489 53.9 KB
Hi @richard.frost. Those release notes when out a little early, sorry for confusion. v104.5 is still the latest. v105 will hopefully be out soon.This topic was automatically closed 14 days after the last reply. New replies are no longer allowed.Powered by Discourse, best viewed with JavaScript enabled"
884,having-low-hit-rate-using-isaac-sim-on-window,"Hello, I have been trying to follow the Github repo below:Contribute to makiJanus/Easy_DRL_Isaac_Sim development by creating an account on GitHub.to train the Jetbot to avoid object and to reach the target cube. However, the hit rate is very low (less than 20%) when I run the train_d.py from the repo. I tried to modify the code and the hit rate is still very low, some in the repo suggested that might be a scaling issue and the hit rate might be higher if we use Liunx rather than Window.May I know if there is some other way to solve the issue of low hit rate ?
Many thanksPowered by Discourse, best viewed with JavaScript enabled"
885,cannot-output-usd-file,"
20220802175557522×704 73.1 KB

windows11
houdini 19.0.657Hello @cmx278171032!  Without a log file, it looks like it is not able to connect to your localhost path.  Can you go into your content browser and right-click on your localhost server and click “Reconnect”?You can check your connections by going the Nucleus Tab on the Omniverse launcher, clicking  the “hamburger” icon and choose Settings.  Make sure all of your Nucleus services are running.  You can click on the “Restart All” icon to re-launch your services.
nucleus21254×702 72.5 KB


restartallnucleus1027×1121 143 KB
Let me know if this helped, if not, can you let me know what errors you ran into.  And also attach a copy of your log files for Houdini found here: C:\Users\<USERNAME>\Documents\Omniverse\<CONNECTORNAME>  Also, to be safe, if you could go to C:\Users\<USERNAME>\.nvidia-omniverse\logs\System Monitor and attach that log here as well!I will also need to know your current system environment.  Operating System, GPU / GPU Driver.MAYA works fine
Houdini log is empty
logs.txt (25.6 KB)
Houdini.7z (20.7 KB)
202208030942461122×960 118 KB

516.93-desktop-win10-win11-64bit-international-nsd-dch-whql
Windows 11 PRO 1000.22000.675.0found a bug
when i click ‘SAVE TO DISK’

The native Houdini Save to Disk for hip files to an Omniverse Nucleus server does not work using the native Houdini FS, known limitation in the docs. However, there is a Save Hip shelf button and menu option in the Omniverse menu as a work around.the error when I click ‘USD_ROP’ ‘save to disk’
not save hip
when i click just quit houdiniHello @cmx278171032
Thanks for reporting this. Could you try a couple things for me, please?The first option is a feature that Houdini has specifically, and that’s why you don’t see it in Maya.
From Houdini’s Doc USD (sidefx.com)There are several ways to get around  this issue, easiest one is uncheck the toggle like I suggested above. You can also config where to save the layer by using a  Configure Layer (sidefx.com) LOP. The other way is to change the “Save Style” to “Flatten Stage” on the USD ROP if you don’t need to preserve references and sublayers.The other issue I notice is the Pre-Render Script. The “q” will quit Houdini before writing out any data. I think it was accidentally put there?Please give the suggestions above a try and let me know if that works for you. I am happy to help if you are still experiencing the issues.thanks
But still not working

20220803152313613×538 46.4 KB


20220803152341522×704 74.7 KB
But ‘filecache’ node is working !

20220803153055889×274 42.3 KB


20220803153125620×833 72.2 KB
Hi @cmx278171032
Thanks for trying that. I have tested similar steps on my end and the USD ROP created the directories and wrote out the files correctly.
I know we have seen similar issues of creating directory on Nucleus in Houdini, and we are working closely with SideFX to sort out the issues. This might be one of them.
If you don’t mind, could you try something more for me, please?If all of above don’t give you a good result, we are working a new Connector for H19.5 and have directory creation bug fixes coming soon.Again, thank you for using Houdini Connector and report the issue. Please let us know if you have more questions and feedback. We love hearing from you.Still not working,but reduced one error message
And I also created the matarial folder, referring to the structure of the local USD file

202208041710581596×621 49.1 KB

Hello @cmx278171032
Thanks for testing that again.
I have tested other setups and still cannot reproduce the issue. There are a few things we can try more,If you use the same USD ROP, and change the output path to somewhere local (e.g. C:\). Does it work?If above works, then could you try change the output path to another omniverse:// path  (e.g. omniverse://localhost/testProjects/). Does it work? (Wondering if there are permission issues.)If none of above works, I suspect the original fbx file that attached in the hip you provide (E:/Project/2022.8.2_CAM_MEMO_USD/geo/PURE_CAM.fbx) might have a problem?omniverse:// omniverse://localhost/testProjects/hi walai,thanks for you reply！
Test 1-working
Test 2-not working same error log
Test 3-Even a sphere create by Houdini ,not working same error log
Test 4-Try running Houdini with administrator,not working same error log
(eg,Is it possible because my system language is Simplified Chinese？)system language is Simplified ChineseYour Omniverse path doesn’t contains any Chinese characters, so it should be ok. I need to test it with Windows 11 system.
I will update here again once I found the issue.
Thank you for your patience @cmx278171032WayneUpgraded to version 101.0.0, still not working
and ‘Omniverse Loader HDA’ ‘Omniverse Validator HDA’ node cannot be found



20220812100029654×719 38.7 KB
finally！windows10 solved the problem！
windows11 not working！pls fix！Hi @cmx278171032 ,We’ll keep investigating to see why the Win10 worked and Win 11 did not. Our testing with Win 11 so far have not reproduced the issue.As for the loader and validator nodes will be in the next release. We decided to hold off on them for now, release notes should be updated shortly.Thank you for your feedback and if you find any other issues please let us know. And if you have anything you can show off post that too!Best,
KevinHaving the same problem with and I’m on windows 11.

image1686×2070 239 KB
Having the same problem. an absolute path to my drive works. Keep in mind that it’s only the USD file that is not saving. I’m on Win10 btwPowered by Discourse, best viewed with JavaScript enabled"
886,materials-and-usd-compliance-with-2023-1,"Hey Everyone,An important update to USD has caused some specific ways of binding materials to no longer be valid. You may run into issues where your materials worked in older versions, but perhaps are not correctly displaying in 2023.1. Specifically showing grey or using their display color.To fix this issue, please use the asset validator. You can find more information about it here: Omniverse MDL Materials — Omniverse Materials and Rendering documentation2022.3
image1069×562 51.8 KB2023.1 before validation
image1088×576 36.2 KBWe highly suggest running the validator on older assets as a great way to keep them clean and as up-to-date with the USD spec. In this first release, we leaned towards a manual validation and letting our users decided when it was applicable to validate their content. We’ll be adding an automated process in future releases that can be selected as well.Let us know if you have any issues!There’s something about the shadows in that upper picture that makes it look extremely photorealisticThe Asset Validator fails to update the material binding on every scene I try to run it on.  It fails on every instance with this error:

Which is not even present in the scene I am attempting to correct.  What’s more, when I navigate to that path, the asset is available.I have created a thread dedicated to this issue.  Please provide some insight.Powered by Discourse, best viewed with JavaScript enabled"
887,retrieve-prim-path-from-replicatoritem,"How can I retrieve the prim path of a ReplicatorItem?
For example:If I run this code with Isaac GUI I can manually see that its prim path of torus is /Replicator/Torus_Xform/Torus, but how can I programmatically get it with python?Hey @federico.domeniconi. As I’ve mentioned before, anything related to Replicator is best asked in the SDG forum. I’ve move this topic there now.Sorry if I posted in the wrong section.I found the function omni.replicator.core.get.prim_at_path in the documentation but it doesn’t work.Morever I think that the name of the function and the documentation are misleading. In fact:but it also accept a ReplicatorItem as input.I tried this function in the Isaac GUI but it doesn’t work:It raises the following error:

image1037×137 18.7 KB
(sorry for posting the image of the error, but Isaac didn’t let me copy the text)Hello,Sorry for the confusion.get.prim_at_path() returns a prim object, not a path.Using your example:torus is already a prim object (wrapped in a ReplicatorItem), which is why get.prim_at_path(torus) does not work, since torus is not a path or a path wrapped by ReplicatorItem.If you are trying to get the path of the prim that was just created:And you can always work directly with the prim after its creation without knowing its path:I have gone through documentation for rep.utils.get_node_targets() where second input param we need to pass is attribute. Can you please mention, where can I find these attributes?They can be found on the prim object, either through the Omniverse User Interface, or via Python by using the dir() method to examine the object.Hovering over an attribute in the Property pane of the UI will give you its attribute name

image447×786 39.2 KB
The majority of the time, you should not have to bother with doing this, as the replicator API handles node connections for you.Powered by Discourse, best viewed with JavaScript enabled"
888,on-demand-playback-speed,"First let me say it is very cool that there is now a good deal of information to be gleaned from the on-demand videos.
However the options on playback speed don’t work very well.
(.5, 1.0, 2.0, 4.0)
The .5 is much too slow to understand and sounds annoying.
The 2.0 is much too fast and again is annoying. Not sure who is able to listen to 4.0.It would be great to add .75, 1.25, 1.5 and 1.75 to the dropdown box.
I find when watching a video that presents information that I know little about I need to run at about .75 to absorb the content.
When watching a video for a second or third time I am usually multitasking and just need a brush-up.
1.5 or 1.75 work great for this.Please Please Please add this support.Hi! This is excellent feedback and I totally agree with you on the benefits of more granular speed options. I’m relaying this to our video platform team and hopefully, we can get those extra speeds added. In the meantime, we post all the on-demand videos on YouTube so you can always find the same video there and use the playback speeds they offer. NVIDIA Omniverse - YouTubePowered by Discourse, best viewed with JavaScript enabled"
889,i-want-to-instantiate-a-class-that-inherates-behaviorscript-on-my-extension-py,"I want to instantiate a class (that inherates BehaviorScript) on my extension.py
However on_update and other func do not run, with exception of on_initPowered by Discourse, best viewed with JavaScript enabled"
890,omnigraph-process-bundle-input,"I want to implement an OmniGraph node in Python. It should get a bundle as an input, this bundle can contain multiple prims.How can I extract the prims from the bundle input inside the node’s compute(db) function?Thanks
BrunoHi @bruno.vetter. Have you taken a look at these two tutorials?Yes, I have. These tutorials do not cover how to work with bundles containing a set of prims. For example, the Ros Publisher Nodes accept a bundle named targetPrims, containing one or more prims. When I try to process such a bundle input inside my compute function, the attributes property returns an empty list. Looking at the OmniGraph docs, there seem to be a concept of having bundles inside bundles, but I am not sure if this is what I am looking for. I haven‘t found any code samples covering this.Thanks Bruno. I’ll reach out to the OmniGraph team for you.Hi! Sincere apologizes for the delay. Here is an example OGN and Python node:The OgnBundleInputExample.ogn file:The OgnBundleInputExample.py file:Powered by Discourse, best viewed with JavaScript enabled"
891,support-for-force-sensors-in-omniverse-isaac,"I am wondering if there is any current support for force sensors that can be enabled, specifically for robot arms like the Franka (e.g. wrist force/torque sensor, joint torque sensors). I haven’t seen force information being exposed client side anywhere yet, either in the ROS bridge, Robot Engine Bridge, or the Python API. Do force sensors exist already or are they planned for development? And if they do not already exist, how could I go about trying to extract the force information myself?I’m afraid we currently do not support force sensors and might not be able to get any force information yet.I’ll file this as a feature request.I am curious if there has been any motion on this, e.g. a target release date when force data will be exposed. Doing manipulation research with Omniverse Isaac Sim is fairly limited without getting force and contact data.Hi Adamconkey,Unfortunately, we don’t have a date for it now.Kindly,
LiilaSo till now force measurement is still not supported or not in the future plan?Hi newuhe,
Yes, force sensor is on the roadmap. We will have it our our future release (2-3 months  from now).Kindly,
LiilaThanks for replaying. That’s great news.I believe this feature is now available…Hello everyone, I am interested in getting the contact points and forces between rigid bodies. Is there a recommended way to accomplish this? Thanks in advance!Hi @cpetersmeier  - Yes, you can accomplish this by using the Contact Sensor in Isaac Sim. Contact sensors measure the surface load applied to a body. In Isaac Sim, contact sensors are simulated by summing all forces applied on a given trigger spherical region intersected with the given body surface via the omni.isaac.sensor extension.Powered by Discourse, best viewed with JavaScript enabled"
892,how-are-people-debugging-their-replicator-code,"I’m trying to get a reliable methodology to debug my Replicator Python code.  The Script Editor in Code is insufficient, IMHO.One method I’ve been using with some success is to create an extension from the template in the Extensions menu:
then add my replicator code and use the VS Code debugger to step through the Python.  (I’ll be honest I forgot how I got VS Code to communicate with OV Code, I think it was all part of the prefab VS Code project created by the Extension Template) This works great… once, then Code crashes after a random number of times (1-3) of executing my extension.  (I’ve upload dozen of crash logs, still waiting on a developer to investigate)I’ve tried rearranging my code to release things after running, keeping an eye on memory on cpu and gpu but I’m not close to running out.  I’ve tried replacing the assets with a basic cube.Seems no matter what I do it crashes after a few runs.My point is not for you to debug my code, my question is, how do YOU in the Replicator community debug your Python?I have also created a duplicate headless mode script that does the same Replicator things and that works great but it’s not interactive and I haven’t  tried to link VS Code with that python script, although I’m gathering it’s possible.Again, my question is, how do you all debug your Replicator Python code?Thanks in advance,
DaveHello @holdendp!  I’ve shared your post with the dev team for further assistance.Now I remember I got the VS Code / OV Code integration working from Mati’s videoUpdate: It SEEMS to be stable as long as I create a new stage before each execution.  If I can figure out how to do that programmatically I might have a band aid.@holdendp Interesting discovery. Did you get this working reliably? I’m also trying to debug replicator in vs code and experience unpredictable weirdness and crashes. Would appreciate any solution you found.As long as I create a new (I go for empty) stage before I start debugging it’s been stable.  Haven’t had a crash in a long time.  I was wondering if they’d fixed things with automatic updates but it sounds like your machine is still crashing, so maybe not.Anyhow let me know if the “new stage” trick is working for you too.Yes, it does seem to work. I haven’t quite figured out how to create a new stage in my code though.Me neither.  If you figure it out let me know, it’d be appreciated.I appear to have figured it out. For me, the following lines of code reset the stage and make the replicator work properly:I wanted to add that, for me, replicator seems to work just fine unless I call randomizer.instantiate. At that point, Omniverse Code (as well as IsaacSim) has problems such as not running the code when expected, or only running it partially, or crashing. This workaround seems to fix that problem.I doubt it’s a good idea to call new_stage frequently, so I’ll keep looking for a better workaround.Awesome can’t wait to try it out. Thanks.edit:
On vacation till July 5, not ignoring.Powered by Discourse, best viewed with JavaScript enabled"
893,omni-ocean-samples-freeze-when-launched,"Hi.The Omni Ocean samples (ocean_medium and ocean_small) freeze instantly when they are launched. Please let me know on how to fix  this issue.Create version: 2022.3.3 (Linux)
Log file:
kit_20230601_101201.log (1.2 MB)Thanks.I have confirmed that the Omni Ocean samples can be launched on Create for Windows, although they don’t work for the insufficient GPU memory (16GB). So the issue may be specific to Linux platform.Powered by Discourse, best viewed with JavaScript enabled"
894,isaac-sim-too-slow-for-real-time-synchronization,"I’m trying to sync an Isaac Sim simulation with a real robot, and I need Isaac’s computations to run faster than the real robot. Both are looping over the same C++ controller code and so should produce the same output when the loop counts are lined up; I have successfully synchronized a PyBullet simulation and the real robot this way.The goal is run at 500 Hz, and while PyBullet is able to complete each controller loop within 0.001 seconds, Isaac takes anywhere from 0.0015 to 0.01 seconds – much too slow! The difference persists no matter what dt values I set for Isaac, so I think it comes down to the computation time: Isaac simply can’t keep up.I’ve tried everything I can think of to speed up Isaac – running Isaac headless, replacing self.world.step() with omni.physx.acquire_physx_interface.update_simulation(dt, self.loop_count*dt), setting frequency to 400 instead of 500 Hz – to no avail. I’ve also played with a variety of world simulation parameters like flatcache, backend, and thread count, though many of my optimization attempts seem to make things worse: using GPU instead of CPU, for example, quadruples my runtime per loop.Any suggestions for what to try to cut down Isaac’s runtime? (I’m running on Ubuntu 20.04 with 1.0 TB disk capacity, a 525.125.06 CUDA driver, 16 11th gen @ 2.30 GHz processor cores, and ~30 GB memory; I’ve noticed while running Isaac that individual cores are occupied up to 100% capacity regardless of the number of threads I use.)I have 16 cores, but only one or two cores are using 100%(others are not seem to be used) while running simulation. Is there a way to use multi-core as much as possible to speed up the simulation process?It turns out my particular runtime issue was due to a combination of other factors, but if anyone is still experiencing a similar issue, it unfortunately doesn’t look like Isaac 2022.2.1 is optimized for all use cases … How to speed up simulation in Isaac-Sim - #6 by rthakerFor the curious, I was able to shave off a couple crucial milliseconds by referencing the Isaac Sim source code (~/.local/share/ov/pkg/isaac_sim-2022.2.1/) to avoid unnecessary overhead.For example, I replaced the line self.position = self.robot.get_world_pose()[0] with the code below to earn back 0.003 seconds:Obviously, this kind of workaround isn’t ideal, especially since it almost defeats the purpose of having a tool as theoretically powerful as Isaac Sim. However, if you also have a small program that probably won’t have to be changed too often, I’d suggest trying this hack to help reduce runtime.Powered by Discourse, best viewed with JavaScript enabled"
895,why-the-human-body-skeleton-meshs-collider-does-not-move-together-with-the-animation,"I set collider as the type of ‘sdf mesh’.  Only the green area has the collision effect.Even the human was made sitting, under animations, the collision area keeps still in the original place.Hi @jasminewuyidata . Collision against animated meshes is not supported yet. We hope to solve this in a future version, however.Powered by Discourse, best viewed with JavaScript enabled"
896,restitution-has-weird-behaviour-around-0-2,"I want to compare the restitution of Unity’s and Isaac’s physics engines. Both Unity and Isaac use different version PhysX however I dont think this is the reason for the weird behaviour.
I am using GPU PhysX for Isaac.I made a test consisting of a sphere rigidbody falling from 5 units onto a plane.
The combined restitution is set to average.
The graph shows that as the restitution get closer to 0.2, the ball bounces higher (on isaac) and then as the restitution moves above 0.2 the ball bounces lower?This graph shows 11 tests of increasing restitutions in Isaac and Unity. Unity seems to have the correct behaviour (with very small bounces and small restitutions).
unity_isaac1920×1009 108 KBI also tested just with Isaac the motions of the ball with restitutions going all the way up to 1, with the motions of the balls overlayed on top of each other.
On this test the combined restitution of the ball is set to max.
The bottom graph shows the peaks of the first bounce. You would expect this value to gradually increase as the restitution increases but at the beginning there is this weird anomaly.
Figure_21846×982 127 KBThis behaviour doesn’t happen when running on CPU.
Is this a bug or can it be explained by a PhysX parameter I’m missing?
Thanks.Hi @peter226 - Thank you for the summary. I will forward this to right team and they will get back to you.Hi @peter226it sounds like you may be affected by a bug that caused updates to the restitution coefficient not to take effect in the GPU simulation.
This problem has been fixed, see e.g., Contact Restitution with PhysX in IsaacGym - #11 by gstate .Can you please verify that you’re using a version greater or equal Isaac Sim - 2022.2.1 ? If yes, and you still have the problem, could you please share how you’re updating the restitution coefficient?Powered by Discourse, best viewed with JavaScript enabled"
897,adding-materials-to-objects-generated-using-python-extension,"I’m working on a procedual forest generator extension for Isaac sim.
And I can’t find any documentation on how to apply materials to objects (in this case the procedually generated ground) using python.Loading materials with .usd files works fine. So I’m thinking I could load the material with an .usd file and then just apply it to the ground mesh but how?Does anyone have any idea or am I in a dead end here?Hi @DiscoFlower8890  - Unfortunately, as of now, Isaac Sim does not provide a direct python API to apply materials to objects. The materials are typically defined in the USD files.However, you can create a workaround by creating a template USD file with the desired material applied to a dummy object. Then, in your python script, you can generate a new USD file for each object in your scene, replacing the dummy object with the actual object geometry.Hi @rthaker
Yeah, that’s what I figured. I can see why there wouldn’t be much use for it. Only real use being my case where the mesh is completely procedual. Tho it would be nice. :)I’ll give the workaround a try.This topic was automatically closed 14 days after the last reply. New replies are no longer allowed.Powered by Discourse, best viewed with JavaScript enabled"
898,is-it-possible-to-run-audio2face-on-cloud,"Hi,
I’ve been searching around for questions similar to this one but haven’t seen anyone sharing a successful cloud deployment answer. What I know for now is we will have NVIDIA Ace in the future. However, it’s still unpublished and I would be grateful if someone could give me advice on running audio2face on cloud.Thanks!I’m also highly interested in such a possibility :) In theory, it can be possible to try to pack a Linux-based version inside Docker and try to up it in headless mode.There’s a system called OVC “Omniverse Cloud” which is fairly new. When that system is mature enough, doing things like this should be easier to achieve.Powered by Discourse, best viewed with JavaScript enabled"
899,issue-with-signing-in-omniverse-launcher,"Hello,I’m experiencing an issue when installing the launcher for omniverse. When I enter my username information this won’t log in (even if it tells me that I can continue to work on the application). The installation worked on another computer a few weeks ago, so I don’t understand where this issue is coming from.Maybe this is related to the recently released update of the launcher. Did someone already get that issue?Thanks a lot for your help! ;)
Best,
Nicolas

Capture omniverse 012946×1405 198 KB
Hello nicolas1.carlier,Thank you for posting, I will bring your issue to the attention of the developers.
Thank you for your patience.@nicolas1.carlier Can you please navigate to the following location on your machine:
C:\Users\username.nvidia-omniverse\logs
and please attach the file entitled launcher log
I will bring it to the developers for review.Thanks!launcher.log (1.8 KB)
Here is the file! Thank you ;)Thank you for attaching the log file, unfortunately it was unable to provide any information towards what you are experiencing. Can you please follow the below attached steps, at the developers request:Can you please post what is available in the “Network” tab in your browser,Hello,
Thank you very much for the help, but it the launcher is working fine now!
Best,
NicolasI am facing the same issue. Could you share how you got the login to work again?@clemens.linnhoff1 it would be useful to provide the Launcher log as well 🙂I got it to work again. In a different post I found the solution, that you have to add the .desktop file to the auto start folder in Ubuntu. Now it works again.Powered by Discourse, best viewed with JavaScript enabled"
900,does-omniverse-create-support-boolean-operation,"Does Omniverse Create support Boolean operation?This is very much a wanted feature.Boolean as in a modeling feature ? No it does not. Unfortunately it never will because we are not, and will not be a modeling software. We are only a composition software and work very closely with other software vendors to not be a modeling software.Powered by Discourse, best viewed with JavaScript enabled"
901,create-error-saving-layers-because-of-permissions,"Error saving layers because of permissions
Keeps coming up when trying to save file?2023-05-22 16:37:05  [Error] [omni.usd] ERROR! Failed to save layer omniverse://localhost/Projects/Ballina-St-project/Ballina_St_-_Apartment.usdz due to permission issue.Looks like a .usdz file type issue!!
Why are there letter extension like a and z on .usd files??usdz is a zipped usd file. Assets downloaded from sketchfab will often be in this format. Although the file can be opened and displayed you can’t make modifications inside the zip file. You can resave the file as a usd and reopen it or go into your file browser and unzip the usdz into a folder you can read and write from before opening it up in Omniverse.Essentially the usd is contained in a zip file and it won’t let you make edits to the content inside a zip. You can read up on USDZ here: Usdz File Format Specification — Universal Scene Description 23.05 documentation (openusd.org)Powered by Discourse, best viewed with JavaScript enabled"
902,autodesk-inventor-connector,"Wil there be a Autodesk Inventor Connector available? that would be great!Powered by Discourse, best viewed with JavaScript enabled"
903,i-would-like-to-know-the-custom-extension-apps-licensing-guide,"Hi.I know that a Create license is required when developing Kit-based extensions and apps.Based on Omniverse Enterprise, more than 3 users created KIT-based applications,If a View License user wants to use an app created in this way. (as an Extension in View, etc.) In this case, do View users also need a Create license?Extensions are available on all kit based apps, they can be used in Create or View or any other kit based app you have a license for (including the free “standard” license).Extensions are not application dependent, they will work on any app that uses the same kit version, they aren’t tied to licenses.Omniverse is free to develop on so you can use Omniverse Create (Soon to be renamed USD Composer) or better yet Omniverse Code which is specifically configured for developers to create them, you may also freely distribute them as you wish.Powered by Discourse, best viewed with JavaScript enabled"
904,bug-in-basicwriter-omnigraph-error,"Hello all!I was just trying out some writers (especially the BasicWriter) and was curious about the corresponding omnigraph. So, I have just used the offline_generation.py script to check it, but I am facing some issues.As soon as I open the omnigraph the following error appears:
BasicWriterErrorLog1680×457 106 KB
AND the omnigraph looks like the following:
BasicWriter_Problem1542×844 52.2 KB
I am using Isaac Sim 2022.2.1 natively.Does anyone else has the same error when doing this steps?Best regards,
ChristofHi @christof.schuetzenhoefer! Thank you for reporting this, te issue should be resolved in the upcoming releases. Please let us know if you still see errors when visualizing the graph then.Powered by Discourse, best viewed with JavaScript enabled"
905,unity-omniverse-export-causes-crash,"I am trying to export a Unity scene to USD (so I can start a live sync) using the Omniverse Connector and it crashes Unity. I have the Omniverse Connector installed. When I select the Omniverse menu and click Export it will think for a bit, then the Unity application closes entirely.When I open c:\users\username\AppData\Local\Unity\Editor\Editor.log, I see a series of errors:I removed the Omniverse Connector from Unity and Omniverse Launcher then reinstalled it and I get the same error.Can you tell us more information?OS version
OS language
Unity versionI assume this is any scene causes a crash, correct?Anything else about your scene or workflow that might help us debug this crash?Also we release Unity connector 200.1 today, could you please try that version and see if you have an issue?OS: Windows 10 Pro
GPU: Intel(R) UHD Graphics and NVIDIA Quadro RTX 5000 with Max-Q Design
Application: Unity, version 2021.3.24f1
I’m not sure what you mean by OS language?I updated to the new package version today and it still crashes. The crashes happen both on a Unity project that has game objects imported from USD and also on a Unity project that just has a cube (created from Unity menus) in it.Repro steps, on a project with no USD assets, where the Omniverse connector is already installedRepro steps, on a project with USD assets imported as game objects, where the Omniverse connector is already installedLet me know if there’s any other information I can provide.Thank you for the complete bug report with reproduction steps! 🌟 Everything looks in order.I assume you’re not able to share with us privately your project, but if I am wrong please let me know.Are you exporting to local disk, network share or Nucleus server?Can you try using Export Selected only from the settings and select a simple game object in your scene and export as USDA file?Did a USD stage try to write to disk as an empty .USD/USDA file?Also please try disabling Export Physics to see if that makes a difference.Do you have any custom (not out of the box Unity) Unity components that you or a 3rd party developed in your project? We’ve had cases were some custom nodes are causing crashes.I’ll talk to the Devs next week for other ideas on Debugging.For reference, I asked this question about a Unity project where I imported the contents from a USD file using the USD package. For the sake of science, I created a second project that only contains a camera and a light. Both projects have the Omniverse connector (com.nvidia.omniverse.connector Version 200.1.1) and the USD package (com.unity.formats.usd Version 3.0.0-exp.2) installed.Are you exporting to local disk, network share or Nucleus server?
When I try to export a scene with imported USD game objects, the export crashes before the window opens to pick the export location.
Originally, when I tried to export an empty scene (camera and light only) to the nucleus server, it would let me select the export location and click “Export” but would crash afterwards and no file was created. Unfortunately Unity has now consistently started crashing when I select “Export” from the Omniverse menu.Can you try using Export Selected only from the settings and select a simple game object in your scene and export as USDA file?
For the scene with USD objects in it, I checked export selected only, then selected one game object (a cube) and clicked Omniverse → Export. Unity crashes before opening the window to select the export location.Did a USD stage try to write to disk as an empty .USD/USDA file?
Though I was once able to get the window to come up to select the export location, Unity is now consistently crashing before this window comes up.Do you have any custom (not out of the box Unity) Unity components that you or a 3rd party developed in your project? We’ve had cases were some custom nodes are causing crashes.
I have the Omniverse connector and the unity USD package installed. For the scene imported from USD, all of the game objects are cubes.Also please try disabling Export Physics to see if that makes a difference.
Disabling physics results in crashes trying to export in both the USD project and the empty project.I discovered today that I’m way behind on Unity versions so I updated to version 2022.2.20f1. I created a new project using that version and reinstalled both the USD and Omniverse connector packages (same versions as mentioned earlier). I installed the USD package first and was able to import a USD file as game objects with no errors. When I then installed the Omniverse package, I started getting errors for the USD package.

image1320×347 31.3 KB
Unity still crashes when I try to export to USD (Omniverse menu → Export)@archer-kgraves There are library conflicts with the Unity-USD package (com.unity.formats.usd) therefore I recommend only using one Package at a time to avoid the conflict and crashes.We will try to debug so both can be active at the same time.I uninstalled the USD package and Unity still crashed trying to export to USD.Steps:@kecollins I created a basic project and exported to USD via the connector. This time, I was able to pick the save location before it crashed. How would you like me to send the project so it can be used for debugging?Steps@archer-kgraves
Yes please DM the project and we’ll take a look for debugging.Thank you very much!Powered by Discourse, best viewed with JavaScript enabled"
906,usd-composer-2023-1-1-usdz-exporter-extension-not-working,"Getting the following error when exporting USDZ from USD Composer.2023-07-12 23:14:04  [Error] [asyncio] Task exception was never retrieved
2023-07-12 23:14:04  [Error] [asyncio] future: <Task finished name=‘Task-14756’ coro=<usdz_export() done, defined at d:/nvidia/ov/data/kit/usd.composer/2023.1/exts/3/omni.kit.usdz_export-1.0.1+ed961c5c/omni/kit/usdz_export/layers_menu.py:29> exception=ErrorException(Error in ‘pxrInternal_v0_22__pxrReserved__::UsdZipFileWriter::AddFile’ at line 932 in file W:\ac88d7d902b57417\USD\pxr\usd\usd\zipFile.cpp : ‘Failed to map ‘https://omniverse-content-production.s3.us-west-2.amazonaws.com/Assets/Scenes/Templates/Default/SubUSDs/textures/CarLight_512x256.hdr’: Invalid argument’, Error in ‘pxrInternal_v0_22__pxrReserved__::UsdZipFileWriter::AddFile’ at line 932 in file W:\ac88d7d902b57417\USD\pxr\usd\usd\zipFile.cpp : ‘Failed to map ‘https://omniverse-content-production.s3.us-west-2.amazonaws.com/Assets/Scenes/Templates/Default/SubUSDs/textures/ov_uv_grids_basecolor_1024.png’: Invalid argument’)>
2023-07-12 23:14:04  [Error] [asyncio] Traceback (most recent call last):
2023-07-12 23:14:04  [Error] [asyncio]   File “d:/nvidia/ov/data/kit/usd.composer/2023.1/exts/3/omni.kit.usdz_export-1.0.1+ed961c5c/omni/kit/usdz_export/layers_menu.py”, line 85, in usdz_export
2023-07-12 23:14:04  [Error] [asyncio]     success = UsdUtils.CreateNewUsdzPackage(Sdf.AssetPath(target_in), local_out_path.str())
2023-07-12 23:14:04  [Error] [asyncio] pxr.Tf.ErrorException:
2023-07-12 23:14:04  [Error] [asyncio] 	Error in ‘pxrInternal_v0_22__pxrReserved__::UsdZipFileWriter::AddFile’ at line 932 in file W:\ac88d7d902b57417\USD\pxr\usd\usd\zipFile.cpp : ‘Failed to map ‘https://omniverse-content-production.s3.us-west-2.amazonaws.com/Assets/Scenes/Templates/Default/SubUSDs/textures/CarLight_512x256.hdr’: Invalid argument’
2023-07-12 23:14:04  [Error] [asyncio] 	Error in ‘pxrInternal_v0_22__pxrReserved__::UsdZipFileWriter::AddFile’ at line 932 in file W:\ac88d7d902b57417\USD\pxr\usd\usd\zipFile.cpp : ‘Failed to map ‘https://omniverse-content-production.s3.us-west-2.amazonaws.com/Assets/Scenes/Templates/Default/SubUSDs/textures/ov_uv_grids_basecolor_1024.png’: Invalid argument’Powered by Discourse, best viewed with JavaScript enabled"
907,error-carb-flatcache-plugin-error-world-task-00-manipulator-obs-cuboid-00-0-xformoporder-usdrelationship-not-supported-in-getprimarrayattr,"I get this error while adding a new dynamic cube to the scene. I have no idea why I get this error. FYI this only happens when the PhysX device is cuda. I dont get this error with cpu. I am using latest IsaacSim 2022.2.1For context, I am trying to remove an existing cube and add a new cube with the same prim path. I remove it with the scene.remove_object(object_name) function and I also delete the object instance for completeness. When I try to add a new cube with the same prim_path and same object_name I get this error. The weird thing is that I get this error only with the GPU backend device. I am using RTX3090 on a desktop computer with i9 13 gen processor.The problem is that after populating the new objects, they are unstable when I step the physics. These objects just fly around like being pushed by some force. It only happens after I delete the previous cube instance and respawn new cubes at the same location. The initial instances of those cubes do not flay around and they behave normally.Thanks for your help.Hi,
this error should be harmless, it failed to create some attribute in Flatcache, but this particular attribute is not used.Based on what you are describing, it looks like some physics remained in place of the original cube. It looks like some stuff was not removed properly from Flatcache, but maybe even physics? Without a repro hard to tell, you can enable physics debug visualization to see what is in physics (Windows->Physics/Simulation->Debug, physx debug visualization).Regards,
AlesThanks @AlesBorovicka for the quick response.
I tried to debug the issue and found that the robot arm configuration after the world reset changes and one of the child prims of the arm remains in collision with the cube at the start of the simulation. but this happens after first reset. My robot is an imported urdf and the default joint position are specified in it. I dont see the problem in the very first import where the joint positions are same as default values in urdf. The problem starts after first reset.The problem only occurs for cuda device and that is what confuses me. The same program works without any issue when device is set to cpu. Is there a way to assert and set default joint state after world reset?Thanks for your helpCan you please double check that physics setting Reset Simulation on Stop is set to true (Edit->Preferences)?
Just curious what is not resetting right, is it the arm or the cube? If its the cube, is the cube created before simulation starts or during simulation? Also is the cube a dynamic body or kinematic body? Kinematic bodies are not reset by physics since physics is not the one moving the kinematic bodies.Reset simulation on stop is checked. The arm is not resetting right. Actually after every reset a new arm is spawned at the same location with the same prim path after removing the old arm. My experiments require generating robotic arms on fly so every arm is different.The robotic arm and obstacles are removed from the stage and re-created within the “cleanup” function of a Task. So according to the control flow for tasks the objects are created before reset. The cube is dynamic.I see, so there is more to that then just play and stop. Please would you be able to send me the repro through a PM. I can take a look and try to fix it or ask IsaacSim team to fix it if its more related to IsaacSim.
Sorry about the trouble,
AlesPowered by Discourse, best viewed with JavaScript enabled"
908,what-are-deps-folders-and-can-i-delete-them,"Hi! Please see this image from my C: drive
image2185×851 379 KB
I have here 51 gigs of files in “deps” folder: C:\Users\Pekka\AppData\Local\ov\pkg\depsWhat are these and can I delete them?PekkaDid you find an answer to this?noGiving this a bump because I’ve got the same problem, 45gb of files that seem to be duplicates of the actual installs.yes, I think so. they are raw downloaded files. my “deps” folder size is about 40GB.
I also want to know what are the exact structure of the folders of OV?
which one can we remove and are cache or duplicate install files and which ones are necessary?So is it safe to clear?Hey, I just deleted them really it does effect on the OV Composer. It cannot boot:image898×543 53.9 KBI restored the files from trashbin and Composer boots again.
So looks like you cannot just delete all of them.Hey Pekka, the error location is not in the “deps” folder !!!
I think you deleted all pkg folder.
did you delete just “deps” folder ?I know bro! I am 100% sure that I deleted only the deps folder, it must have some re-direction / shortcut thing.I would also love to understand this! Omniverse only worked when installed onto my C drive, now I have nearly 80GB in ‘deps’ filling up half of the drive, and no idea how to clean it up. Any help, @WendyGram?Yep, that is a pretty big amount of data in C:
One thing I might try is to delete the folders one by one from there ( starting from larger ones ) and see if Crete still boots up :)Oh, yeah. that’s true bro. got it.
yes, it seems all folders in the pkg folders are just shortcuts with the correct names of “deps” folder.
so I think we should keep “deps” folder files.
but it is somehow weird in Windows properties of folders. when I got properties of all folders inside pkg it is about 40GB and when I just get the properties of “pkg” folder in top “ov” folder it is about 20GB, so I think windows shows the double size of files inside the folder!
yes, also for me drive C is almost full. it is good to know how to clean-up files in omniverse.Powered by Discourse, best viewed with JavaScript enabled"
909,skeleton-export-issue,"for some reason, when maya connector export maya rig to usd skeleton will get issue.
I am not sure why it happen, but when i manully binding skeleton to mesh, most skeleton rig was currect to binded to mesh. only 4 of the rig cost mesh offset.
I have check bindingtransform and resttransform, it seen like when maya export rig to usd skeleton ,some of the rig has scale matrix error.
All the mesh componet has freeze transformation already.this issue I already fix.
remake a new maya file, and open the skeleton usd file then export it, it automatic fix the problem…
I cannot share project file in public, can you guys DM a email so i can send file

dae4fc4c2304238d2a8d65772ec4cfa6.PNG818×1021 39.8 KB
All the mesh componet stay in world axis.
All the transformation has been freeze.btw, I connot found the post script on document, i need write some attribute when usd file has been exportedPowered by Discourse, best viewed with JavaScript enabled"
910,replicator-no-textures-on-instantiate,"Hi! I’m having an issue where I don’t want to get all the usd files from a single directory so get_usd_files(path, recursive = True) is not a possibility. Loading the usd files as a reference works fine but when loading them through rep.randomizer.instantiate does not load the textures.
I’m using “omniverse://localhost/NVIDIA…” paths.Any help would be appreciated!
Thanks!Hi @didiersurka. I’ve moved this question to the Synthetic Data Generation forum where the Replicator team will be able to help you.Yup, same issue.It takes 3 frames to load materials, but image is taken on first frame. Increased on_frame interval and now I got 2 frames without materials and last one is usable.Just retested and even if I go to replicator>Step, they never show up.
on_frame(interval = 10) doesn’t help either. RTX interactive or real-time, same result.So i’m guessing it’s how’s getting the files from nucleus.Hi @didiersurka I’m going to try and repro, but it would be helpful to know some info about the asset you’re trying to load. Is it a big asset? Are there any errors or warnings in the log when trying?I’m having the same issue. Objects with instantiate have no textures in the output RGB images the vast majority of the time, however they have textures in the GUI’s viewport. Additionally, when I try to give other things textures through randomizing the material, like a plane I am using for a floor the same thing occurs. It seems that less than 1/10 times the floor actually has texture in the output images even though the viewport image has it. I get no warnings or errors on running the replicator. The I get 2 warnings on running my script:[carb]Plugin interfaace for a client: omni.hydratexture.plugin was already released.
[omni.replicator.core.scripts.utils.utils]More than one downstream connection detected, graph scheduling may not produce expected resultsedit: I forgot to answer, these are not big assets I’m loading. Basic things like chairs and other furniture.Powered by Discourse, best viewed with JavaScript enabled"
911,objects-clipping-through-gripper,"Greetings,Currently, I am in the process of implementing a robotics application that involves inserting a pin into a hole. However, I am encountering a problem where the pin is passing through the gripper fingers, even though both the pin and gripper fingers are defined as sdf mesh. I have experimented with various settings such as altering the colliders of the gripper fingers and pin, adjusting the joint force of the gripper, modifying the time steps per second value, and locking all positional and rotational axes of the pin. Unfortunately, none of these measures have proven effective, and the pin continues to pass through the gripper fingers. It is worth noting that collisions seem to function properly when I designate the pin as a kinematic actor, but this is not a viable solution for simulation purposes. I have included a video below that showcases this behavior.I would appreciate any insights into the cause of this issue.Best regardsIf I use a cube as object, the collision works properly (convex hull collider on cube), a simple cylinder with convex hull does not work:Hi Axel,We have been working on improving SDF collision behavior for the next release so there are improvements coming.But I am still surprised that this case is not working for you. Just to confirm:I would like to make sure that we either have fixed your issue already or can address it in time for the next release. Is there any chance that you could share a minimal repro for the gripping issues?Do you see any warnings in the log that could hint an an issue? Notable warnings would include GPU contact buffers not being sized large enough.Thank you,
PhilippHi
The collider of the pin, visualized with the green lines, looks a bit like a triangle mesh collider and not an SDF collider but it is hard to see in the video. Triangle mesh colliders work on static or kinematic objects but not on dynamic rigid objects. Dynamic rigids need an SDF collider for the Pin and the Grippers.
Sharing an usd to reproduce the issue would be great. You could delete everything except the gripper and the pin. Saving using “Save Flattened as” would be useful since everything will end up in a single file that you can easily share. If you save as usda, you can even check in a text editor that there is no confidential data in the file that you share.Hi Phillip and twidmer,yes, that is correct:Here is the example usd:
example_gripper_collisions.usd (14.8 MB)Once the simulation is started, rotate the robot by 90°. This action will correctly position the robot. Additionally, I have prepared a small script for opening and closing the grippers:Here is a short video demonstrating the process:
The only warning I encountered, that could be connected to this, is something about inertia tensors. You will probably see it in the example.Thank you,
AxelHi Axel,Great, thank you very much for the repro - we will have a look and get back to you.PhilippHi,I tested a bit with the scene. I was not able to get it working but there are a couple of issues that need to get fixed. The console shows a couple of problems. I would recommend to load the console when running Isaac Sim since it sometimes provides useful debug information. The messages I get are the followingThis means that there are several rigid body APIs on meshes or XForms that don’t have a collider API (the collider API can be on the same prim or on a child prim). Rigid bodies must have a collider associated to calculate mass properties. Otherwise the rigid API does not really make sense or gets artificial mass values applied. The second and even more important point is that non-uniform scaling is only supported to some degree. If non-uniform scaling is applied to an xform and then child nodes use a rotation and another child node even further down the hierarchy uses scaling again, this can lead to skew. Skew is not supported for meshes that have physics applied. As a rule of thumb, non-uniform scaling should only be used on meshes that don’t have children or if there are child prims they should not have a transforms and inherit the transform from their parent.Hope that helps to get a step further with debugging.Hi twidmer,thank you for the tips.I have managed to fix the issue.I discovered that the SDF collision-body does not rotate along with the robot (if the simulation is running). You can observe the problem in this video:
To address this, I defined a new SDF collision body after rotating the robot. Although the SDF appeared in the correct position, it remained stationary and did not move as expected. You can view the behavior in this video:
If I solely utilized the joints to manipulate the robot, everything worked as intended:
I have provided the updated usd-file for your reference (also fixed the issues in the console):
example_gripper_collisions_2.usd (14.8 MB)I also noticed that the cube, originally defined as a static triangle mesh, does not collide properly with the gripper. However, when I define the cube as a convex hull instead, the collision issue is resolved. Are collisions between static triangle meshes and SDFs supported?Static triangle meshes can collide against SDFs. Older versions of the PhysX SDK (which powers Isaac Physics) might not generate enough contact points. Newer versions will fix that. You can try to work around it by using static triangle meshes with more/smaller triangles. But the collisions do work. I tested your scene, duplicated a gripper part, added its own rigid body API and it did collide with the cube. But I still see the problems I described before in the scene like a mesh with child nodes (that is not supported in usd) and a chain with multiple non-uniform scalings across the hierarchy.Thank you for sharing your findings and testing the scene.Regarding the issues you mentioned, I have a few questions:Newer Versions of Isaac Sim: Do you have any information on when the newer versions of the PhysX SDK, will be integrated into Isaac Sim? It would be helpful to know an estimated timeline for these improvements.Child Nodes of Meshes: When you referred to child nodes of meshes, do you mean the GeomSubset nodes? As far as I can see, I couldn’t find any other child nodes on meshes. The GeomSubset child nodes are automatically generated when converting a step file to USD using the CAD importer extension. If there are other types of child nodes causing issues, please let me know so that I can investigate further.Non-Uniform Scaling Prims: Besides the cube, I couldn’t locate any other prims with non-uniform scaling. If there are specific prims that exhibit this behavior, I would appreaciate if you could provide more details so that I can address and rectify the scaling inconsistency.Regarding the CAD Importer: I’ve noticed that when converting a STEP file (metric system) to USD, the resulting object is 100 times larger than in the CAD program. My colleagues have encountered the same issue. We’ve been using the unit converter to scale the individual objects back to the correct size. Is this a bug, or is there a parameter that can be adjusted to resolve this scaling discrepancy?Hi Axel,I am not familiar with the CAD importer, maybe @rthaker can help with this question.Thank you,
PhilippThank you for the clarification.I have conducted some experiments with the original implementation and managed to resolve the collision issues. However, I have to assign a physics material to the pin and the Gripper, switch to another collider type, and then switch back to the SDF collider in order for it to work. Interestingly, this step is not necessary in the simplified example and it works without assigning a physics material.I’m unsure if this behavior is normal, but it appears that the convex hull collision is more stable than the SDF collisions. However, both methods still produce suboptimal results.Here’s an example using the pin collider defined as convex hull and the gripper as SDF (with 120 time steps per second):And here’s another example with the pin collider defined as SDF (also with 120 time steps per second):As you can see, there is noticeable jittering. Are there any parameters that could potentially address this issue?Additionally, there seems to be significant penetration between the rigid bodies, as evident in the following example (both gripper and pin colliders are defined as SDF, with 240 time steps per second):Are there any parameters that I can adjust to mitigate this problem?However, I have to assign a physics material to the pin and the Gripper, switch to another collider type, and then switch back to the SDF collider in order for it to work.For what to work specifically? This sounds like a potential bug and if you could share detailed repro steps that would be appreciated.For the jitter, it’s not easy to say - do you have a similar simplified repro available for this as you provided earlier?Thank you,
PhilippFor what to work specifically?For the collisions to work properly. Otherwise the pin clips through the gripper, as seen in the video of the first post.I will try to reproduce it in a simplified version.Additionally, there seems to be significant penetration between the rigid bodies, as evident in the following example (both gripper and pin colliders are defined as SDF, with 240 time steps per second):Here is the file for this part:
example_gripper_collisions_3.usd (14.8 MB)Kind regardsAxelUnfortunately, I encountered difficulties while attempting to make the rmpflow-controller function in a simplified, standalone exampleI instead experimented in a standalone example without the controller, but could not reproduce the issues.Here is the example, maybe this helps to improve sdf-collisions:
simplified_example.usd (12.1 MB)test_robot_control.py (3.4 KB)Just save the usd-file in the same folder than the standalone python file, and then run the standalone script. For some reason gpu dynamics gets disabled in the standalone version and I could not change the physics scene attributes in python, so you have to manually activate gpu dynamics and set your desired time steps per seconds value.In the provided example, I noticed that assigning a material appeared to enhance the performance of sdf-collisions, as demonstrated in the accompanying video. Additionally, when using a higher physics dt (time step), the convex hull collider seemed to outperform the sdf-collider. The video was recorded with 60 time steps per second.I will provide an update if I manage to reproduce the issues in a simplified version.Kind RegardsFound the reason, why the physics scene attributes would not change in the script.
The physics scene prim is overwritten when defining my_world. Defining my_world before changing the physics scene attributes fixes this issue.Here is the updated standalone script:
test_robot_control.py (3.4 KB)I experimented a bit and found out, that some of the wired physics behavior and the jittering is caused by the rmpflow controller.For my experiment, I still used rmpflow to controll the robot and left everything the same, except for when the gripper should be closed or opened. Instead of controlling the gripper through rmpflow, I directly set the target position of the linear drive in the gripper.Those are the results (120 time steps per second), I would recomment to slow down the videos:SDF-Collider on the pin, without setting a material:Directly changing Target position:
rmpflow:
SDF-Collider on the pin, with material:Directly changing Target position:
rmpflow:
Just for reference, when I control the gripper through rmpflow, I get the the controller action through the forward method of the paraller gripper object and then apply them with the articulation controller:The results of the direct joint position control look very good, but the control through rmpflow is unusable.Thank you for the update - the material difference is still interesting to see for me; what is the difference between the material that you apply and the default material? Is the effective mass different for the case where you apply a material?For rmp flow, I think @rthaker could provide/obtain more information.I am not sure about the default material, but I can read the mass property of the pin:default material: 0.003030497347936034 kgnew material (steel): 0.023789402097463608 kgAnd here are the properties of the new material:Dynamic friction: 0.4
Static friction: 0.4
Restitution: 0.2
Density: 7850 kg/m³
Friction combine mode: average
Restitution combine mode: averageYou can also test this in the standalone example I posted earlier:Here is the example, maybe this helps to improve sdf-collisions:
simplified_example.usd (12.1 MB)Here is the updated standalone script:
test_robot_control.py (3.4 KB)The same effect can be observed here. When the new material (Steel_Gripper, already in the scene and applied to the gripper fingers) is applied to the pin, the collisions seem more realistic.Hi @axel.goedrich - then I would think that the better mass ratio between pin and robot gripper with the new material provides better collision fidelity. Do you need it to work well with the lower mass or not?Powered by Discourse, best viewed with JavaScript enabled"
912,exporting-physical-material-vray-material-ambient-occlusion-map-to-usd-composer,"Hello everyone, I have a question about exporting 3dsmax vray or physical materials to Omniverse:
where should I put ambient occlusion maps in physical or Vray materials to open correctly in Omniverse USD composer?
I tried all the different slots in physical material and Vray and just the ambient occlusion map does not export correctly.
I tried composite map and other slots and they do not work for me!!
Screenshot 2023-07-29 173909843×1277 163 KBIt is not supported by physical material and Vray. You can use MDL material in Max Connector via Max’s Material Editor, e.g. OmniPBR and set it to ORM_texture and make it enabled.Powered by Discourse, best viewed with JavaScript enabled"
913,usd-composer-2023-1-1-open-recent-menu-item-not-working,"The USD Composer 2023.1.1 is not storing the previously opened USD scenes in the “Open Recent” menu item under “File” menu.This happens when I close the entire USD Composer session (shutdown the window) and later on re-start the USD Composer.Powered by Discourse, best viewed with JavaScript enabled"
914,anyone-have-soft-cloth-hair-working,"Reallusion is taking forever to support this and I can’t see why we need to wait - there must be  a manual solution, but I have not been able to get it to work quite right.Whatever I do, the hair ends up at the feet of the character. It appears to be staying aligned correctly with the head, it’s just in the wrong position.I’ve tried adding a scalp object and giving that rigid collider physics and attaching the hair, but I have not gotten that to change anything. So close and yet the hair so fair away. Literally.Has anyone worked out how to get the hair in the correct position?Hello @markrmiller!  I forwarded this over to an expert to help!  What is your current workflow?  Are you experiencing this issue in Character Creator or are you importing it into another app?It wasn’t hair, but I tried doing the same with with a character from iClone8/CC4 with a cape and had the result was the same as yours, it was just dropped right down to their feet as soon as I started playing the scene.My process is to create characters using CC4, animate the scene in iClone 8 before exporting using the Omniverse plugin.When using iClone’s soft cloth physics, I always leave about 5 seconds before animating a character, to give the soft elements a chance to settle from their default position. I did the same thing with my test project but immediately the cape jumped down to the character’s feet.Hi @markrmiller,
by saying ‘manual solution’ , do you mean setup the PhysX property manually in Omniverse Create or Machinima?Hi @darth.angelus
the character exported currently doesn’t have collision shape, so a cape dropped on the groundRight, I’m trying to get cloth physics on hair using just Omniverse.I’m dying to make Omniverse my main rendering solution, but unfortunately, everything I’m currently working on involves a character with long hair and if it’s just static, that really prevents me from being able to render with Omniverse. It’s mostly action scenes.So I’ve been waiting, first for physics in Omniverse and now still for automatic support from the iClone connector. In the meantime, I’ve worked out things in Blender, but I recently thought, hmm, I could do this in Blender without iClone support, of course I should be able to in Omniverse.A while back, a lot of my physics experiments with soft cloth would crash, but I figured, let’s give it a try with the recent Create release. Crashed once or twice, but worked often enough for me to stick with it.The hair seems to be be able to do what I want, but of course it drops to the characters feet. Oddly, it still appears correctly oriented and moves with the head, just down at the feet. The drop at least though, was expected.In Blender, I’d do something like pin the hair to a vertex group on the head. So I went to the Omniverse documentation to find a similar solution. There it looked like the answer was pretty straightforward. To attach the hair, make another object a ridged collider, overlap it a bit with the soft cloth hair, select both and create an attachment.So I tried that, using a scalp object on the head as the ridged object. Unfortunately, I saw nothing different in behavior. Perhaps I just didn’t have them overlapping correctly? I would have kept trying, but I wasn’t sure I was quite on the right track, mainly because of how the hair still acted like it was soft cloth and connected to the head, but just down at the feet. It is what I want, just in the wrong location, but my expectation was that it would just drop to the ground like a blanket and sit there.  And so I was wondering if I was missing something that maybe someone else had already figured out.Now that you mention it,  I didn’t fully think through my lack of collision shapes, I hadn’t seen them mentioned in the doc in the classic “simpler shaped objects approximating the mesh”, but I figured I’d just try making more body parts ridged physics objects or something.I took another whack at this and I didn’t have any more luck, but I had a bit different experience and realized I was confusing some things.So I tried I different scene and first I just tried making the hair soft cloth. The girl is on a motorcycle that’s like 10 feet in the air. The hair fell down like cloth, fell as far as it could go. Okay, good.Now I had some trouble that makes experimenting hard. Previously, I’d use the physics debug panel and hit stop to reset the hair. This time that would not work. I had to reload each time from a fresh copy.Next I added a collider (which I realize now must be my collision shapes?) and rigid physics to the scalp. I turn off gravity for the scalp and tried both kinematic and non kinematic.Then I create a physics attachment between the hair and the scalp and bump up the attachment offset.This is where things get frustrating. Now the hair is doing what I want. It’s flowing, it’s got the right alignment. But it’s down at the characters foot. Not at the bottom of the scene, not on the ground plane. The character is on a motorcycle that is 10-15 feet in the air (static). And the hair is also in the air but down at the characters feet, doing exactly what I want - in the wrong place.Hi @markrmiller
I am not expert on PhysX neither,
Adam and Michelle are definitely exports on PhysX, but I am not sure whom from NVIDIA would have time to take a look
I think you could send the .usd file to @WendyGram first?
let the NVIDIA expert to comment on the right way to add PhysX manually?
and if PhysX hair can work with USD SkelAnimation?In this presentation, we will explore newly-unveiled capabilities of deformable simulation in Omniverse PhysicsThis session introduces the simulation capabilities of OmniverseThanks for the suggestion! At this point I’m likely back to waiting for reallusion support and doing some renders in Blender in the meantime.I tried flipping the order that I made the attachment (selecting the hair second I think) and that moved the hair from the feet to the head - but still not in the right position - it ended up behind the head, though at least at the right height. I got that idea from watching one of the physics tutorials, but I’m at a loss as to why the hair attaches with an offset. I know you can set an offset so that the attachment doesn’t actually have to overlap, but this happens even with that at 0, and I’d still expect the hair to attach at its original placement, not offset behind that original placement.I’m certainly missing something, but I’m a little burned out on getting it to work as I’m about as close at the end of my efforts as I was at the start.Yeah, to be honest, there is no Soft cloth export support at this time. You can import ‘Rigid Body’ physics (eg, like a box dropping) but not ‘Soft cloth’ physics. Plus, I’m hoping they can also support animated morph too in the future.I’ve actually made a FeedTracker request on the Reallusion site asking for Soft Cloth Physic support (or even baked physics) just so they know there is a huge request for it. I’m guessing they are probably working on it, but I think it’s still early at this stage.Hair and cloth physics exported to Omniverse - Feedback Tracker (reallusion.com)It turns out OmniVerse isn’t ready for it either. Physics colliders don’t follow characters, so you can’t attach cloth or or have it collide with the character. If you could, this could be done without iClone support. Someone mentioned they might have it in this coming release. I hope so, it’s a huge gap.I finally have a workflow that can do soft cloth for hair and clothes in OmniVerse. Not really for the faint of heart, I’d tried in the past without much success, but recently I managed to iron out the kinks I seemed to have been running into.In the surface it’s relatively straightforward. You bring soft cloth props into blender via addon that does that well, bake your physics, export to USD with the OmniVerse addon, import into OmniVerse to join the rest of your scene that you bring straight from iClone to OmniVerse via that plugin pipeline.Which seems relatively obvious, but like I said, in the past, I couldn’t get very far with it, the devil frolicking in the details. But is indeed, a kind of slow or minority painful solution with few limitations and great results.Hi @markrmiller , did you ever write up a blog or similar describing the process you are using in a bit more detail? Kit 105 has included some new support, but I am still getting the behavior you described above of the hair falling to the ground (for me it jumps up and down in unpredictable ways by turning on/off debug views). This is still happening in 105.I put together my experience with 105 in a blog post Omniverse 105 (beta) Colliders for Particle Mesh Hair and Clothes – Extra Ordinary, the Series - there is a video at the end showing all the strange behavior. (I was trying to show the “good” behavior, but lots of strange things and crashes happened while trying to follow the steps in the blog.)Am I understanding correctly that you basically do the physics in Blender, then export the animated mesh over into Omniverse for rendering, instead of relying on the physics in Omniverse?Oh man, that’s a bummer if 105 doesn’t support it. I think from where I ended up, all that was needed was for the colliders to actually move with characters, and a dev somewhere on the forums had said it was likely going to be in 105. Otherwise, I think I had it close to working. I would do something like (it’s been a bit): remove the hair from the character hiearachy, apply soft body to the hair, add a collider for the body, create an attatchment between the hair and the scalp. And then the problem was, a character’s body collider did not move or animate with the character. You could do only do anything with a character that was not animated. I was hoping that was coming in 105. I only tried for about 5 minutes when the new create (what’s it called now?) hit, and ran into trouble right away - maybe in attatching the hair, or something with creating the collider - I got some Physx error that said something about not supporting that kind of mesh. I figured I wait for Machinima to get 105 before I looked at it further.But yeah, I did have some success with Blender, though its not an ideal workflow. For example, you won’t get the same material conversion / transfer that iClone to Omniverse get’s you. In fact, I could’nt get any textures / materials for some reason, and I finally realized it was because prim paths where off in the USD exported from Blender, so I did a find / replace on the USD file to get it working. One thing that had always tripped me up in the past, and where I just kind of would give up, is that I thought you needed to bake everything with the Blender omniverse plugin before exporting over - and a bunch of reallusion character stuff can’t be baked. But I ended up exporting my scene fine without baking anything anyway.So basically, the simple version is, I export from iClone, use the Blender addon that does a great job importing CC/iClone characters, and then export to USD with the Omniverse Blender addon (USD export is in Blender now, but the Omniverse Blender version seems to keep enough ahead that it’s probably the best bet), then fix prim paths if needed for textures, it shouldn’t be, but I had to, and soft cloth animation came in fine - didn’t have to bake out anything extra in Blender beyond the physics simulation. I did this first for a full scene, and most materials and what not came in fine that way, but a lot of the character materials are lacking compared to the straight from iClone route (hair being the biggest offender). So I’d probably bring in everything from iClone, just export and bring in the soft cloth from blender to replace those, and then fix up the materials to match what iClone did instead of Blenders attempt.I think 105 has the required features - its just they are not reliable when I tried them. The colliders do seem to move with the bones now. But for me, the hair would drop to the ground after a while (height zero). It would follow the shape of the head (it thought it was on the head by the looks of things) but it was rendered at ground level. That is, it felt bugging rather than missing functionality. I reported in the forums, provided a test case etc, but no word. So I am not holding my breath. (See this thread for more info, especially towards the end VRoid Studio -> Omniverse with hair a cloth physics? )E.g.
That’s the behavior I got when I left the hair as part of the character hierarchy. That’s why I’d pull the hair out of the character hierarchy. I think i duplicated it and then removed the original. Then it wouldnt be influenced by the character anymore. Then I’d try and use an attatchement to attatch the hair to the characters scalp.Powered by Discourse, best viewed with JavaScript enabled"
915,issues-with-starting-isaac-sim-headless-in-a-docker-container-and-workstation-installation,"I tried running with both flags and checked with nvidia-smi if only one GPU is being used, but it still gives the same results when trying to run either ./isaac-sim.headless.native.sh --allow-root or ./runheadless.native.sh:I’m out of ideas. Can you describe your setup?
Are you running on bare-metal (like desktop/latop) and have a monitor connected to one of the GPUs? or in a Cloud. It looks like you are on a VM. How was it setup?The situation is like this: Im not working on my local machine, im using VNC to work remotely on a VM Ubuntu setup, that runs a VNC server.
On my local setup, which has Windows 10 and a RTX 2070 Super, i had no problems installing and using Omniverse.
I have also seen this post, it seems to describe a similar situation like i have with the Workstaion variant but not the Container version.
Could it be that VNC is making issues for the container to run Isaac Sim headless?Yes. You may be having and issue similar to that post. VNC may not be hardware accelerating the virtual display. Is it possible to try other streaming clients like RDP or No Machine?So apparently no other remote software tools have worked so far. I was told that i may have access soon to a server to use as a Desktop, where the options for remote software is wider.Im asking myself, if VNC is the problem for the container not running Isaac Sim, why is it still not working if im simply connecting through SSH and not using a VNC Client to connect? Is it because the VNC Server is still running on that machine?
And is VNC also responsible for the missing files when trying to use the Omniverse Launcher or is something else causing this problem?It could caused by how the VM was setup or drivers.
I find it odd that you could run the other containers and nvidia-smi but not Isaac Sim headless container.Can you try remove these two folders below and re-install the drivers again via the .run installer./etc/vulkan/
/usr/share/vulkan/I removed both folders before re-installing the driver again via the .run installer and some things changed.
First i checked ifexists on my host and it does but with different data now:I also made sure ifexist on my host machine, but it seems like its now a folder with not files in it, which i think is odd so the file does not exist.
Running nvidia-smi still works and gives the same results as always.Next i tried running this container:Which also worked with no problems. Now come the first problems, vkcube is not working anymore and i get this error message:After that i tried running the Isaac Sim container again, with these flags:but both files on my host /usr/share/vulkan/icd.d/nvidia_icd.json and /usr/share/vulkan/implicit_layer.d/nvidia_layers.json are now directories so i didn’t expect any success. So then i tried with these flags:because both of these files exist.
But no matter what flags i used i still get the same results when trying to run Isaac Sim headless. Which means when trying to run ./runheadless.native.sh i still get the EULA warning with the Illegal Instrunction (core dumped) message.
And when running ./isaac-sim.headless.native.sh --allow-root i still get only Illegal Instrunction (core dumped)I had a typo when trying to run vkcube, so it actually works like it did before the driver re-installation.The only things that changed is that both files on my host /usr/share/vulkan/icd.d/nvidia_icd.json and /usr/share/vulkan/implicit_layer.d/nvidia_layers.json are now directories with no content inside them.
And the value of “api_version” inside /etc/vulkan/icd.d/nvidia_icd.json changed.
The Isaac Sim container is still the only one not working like beforeThanks for trying. I’m not sure if the changes to our recent container is causing your issues.Can you try running a couple older versions of the Isaac Sim container?I tried the versions 2022.1.0 and 2022.1.1 of the Isaac Sim container but i still get Illegal instruction (core dumped), here is an example from the 2022.1.0 version:I wanted to try the version older then 2022 but i don’t know how to run Isaac Sim headless there@pat_n22 , is this still an issue for you?Yes the issue still remains @rthaker.
I recently assumed that maybe the docker default network configuration might be a problem because of past issues with docker containers, but that also didn’t helpHi. Were you able to resolve the issues installing Launcher? Can you run Create and Code natively from the Launcher?No i was not able to resolve the issue with installing the launcher.
When i run ./omniverse-launcher-linux.AppImage i still get the same issue as illustrated in my original post.
Therefore i can’t run Create and Code.Hi. Have you tried deleting the folders again below and reinstall the drivers?Hi @pat_n22 - Let us know if you were able to resolve the issue.On my repo I have a script under the dev tree docker folder.
You can use and adapt to install your Isaac Sim Container.devReinforcement Learning Environments for Omniverse Isaac GymHey, yes i deleted the folders and did a fresh reinstall with the provided driver, but i still get:Unfortunately, i wasn’t able to resolve the issue yet, there is also no VNC server present anymore on the machine so i don’t know what could cause this.
It could be some VM settings, but i didn’t setup this machine it was done by someone elseMaybe, you could add “–ignore-gpu-blocklist” while you run the AppImage, I guess. When I encounter the same error, so I reinstall the gpu driver according the omniverse documentation and add above startup option, then it works for me.Powered by Discourse, best viewed with JavaScript enabled"
916,omniverse-xr-with-oculus-quest2-question,"Hi!
I am designing app for an AAA client within automotive industry. Have modeled cars in CAD programs, and successfully prepared it and exported to 3ds max software linked to Omniverse. The model itself is very complex, with more than 10 millions of polygons, but havent any problem to export it and put materials and stuff in Omniverse. My goal is to prepare VR application to be used with Oculus quest 2. I read documentation for Create XR, downloaded Steam and installed steam VR. Installed Omniverse create XR and successfully open model in it. In viewport it looks just fine, but when I press ""open VR"", it crashes after 10 sec with error saying that I have to check if SteamVR is installed correctly. I checked it, and it is fine. Tried to open an empty scene in VR, and it crashes also in 10 sec...so I guess it is not due of heavy load scene. Also...I cannot pair my Oculus to SteamVR...it cannot recognize it, and just showing Please plug in your VR headset. I tried everything, but it just doesnt work. Read documentation. It said that minimum requirement is RTX 3090. I have RTX3080, so maybe that is a problem (other requirements meet my specs…64Gb ram, i9 12th gen).
So, please, can you describe me, step by step how to solve my problem and play my file (or app) in oculus quest 2.Thanks in advance
DubravkoI assume you have the Occulus app running and your quest 2 is connected successfully in the app.I found out that my USB-c cable was the problem. I even had it to work for few seconds, but the oculus app was saying it is USB 2 (not USB 3)…Didn`t know that my cable was only 2.0, and that I need 3.0 cable (not a plug itself)…so I bought new USB-c to USB-c 20Gb type of cable, and boom…running smooth as supposed :)Powered by Discourse, best viewed with JavaScript enabled"
917,audio2face-launching-but-freezes-and-will-auto-quit-not-responding,"Screenshot (2)1600×900 147 KBthe app wont start, it just freezes at 40%-60% then will auto closei have minimum requirements pci dont have any knowledge on programing, even basic i dont have. please helpHello and welcome to the forums @chokidolf
Can you please send your latest Audio2Face log file which can be found here: C:\Users\<USERNAME>\.nvidia-omniverse\logs\Kit\Audio2FaceScreenshot (7)1600×900 68.6 KB
sir the last 3 files only? or do i need to send all of those? here the last 3 logs:kit_20230716_091350.log (556.9 KB)
kit_20230716_091402.log (576.1 KB)
kit_20230716_091606.log (675.7 KB)ill give up then. going to uninstall it. Ill save money for Faceit addon instead.This might be because your GPU doesn’t have enough memory. Based on this page it looks like Audio2Face needs 10GB of GPU memory at the minimum.Powered by Discourse, best viewed with JavaScript enabled"
918,cant-load-local-usd-file-through-yaml-file-with-replicator-composer,"So I was following this tutorial: Isaac Sim: Generating Synthetic Data using Replicator Composer - YouTube  but the models weren’t loading. No matter what I write in the .yaml as a value for the parameter “scenario_model” it keeps returning this error:Sometimes, if I try to load another .yaml example instead of “NVIDIA/Assets/Isaac/2022.2.1/Isaac/Environments/Simple_Warehouse/warehouse.usd” the path it tries to open is instead “NVIDIA/Assets/Isaac/2022.2.1/ /Isaac/Environments/Simple_Warehouse/warehouse.usd” with an extra “/” between the model path and the nucleus server path.I have also tried linking some of my own .usd files in the .yaml file but I keep getting this same error message, as if some sort of wrong default path gets opened every time instead of what I’m asking.Powered by Discourse, best viewed with JavaScript enabled"
919,on-picked-beta-node-is-not-responding-after-adding-the-button-node,"On picked node is working before i added the UI. After adding the Button node into the scene, the node is not responding. Is there any workaround which needed to be done to solve this.Thanks.

nvd11103×485 35.9 KB
Is ‘enablePicking’ turned on in the SetViewportMode node that turns on ‘Scripted’ mode?Powered by Discourse, best viewed with JavaScript enabled"
920,isaac-sim-with-ros2-on-windows,"Hi all,
I am going through tutorial ROS2 Navigation 7. Multiple Robot ROS2 Navigation — Omniverse Robotics documentation, my OS - Windows 10.As per my understanding, since DWBLocalPlanner was used in this tutorial, robots were supposed to avoid dynamic obstacles. I ran experiment where robot’s paths will cross at some point in the Hospital environment, and they collided.I increased InflationLayer values as suggested here Tuning Guide — Navigation 2 1.0.0 documentation, but collision still happens.Question - does this behavior happens because Windows is still Beta version for Isaac Sim+ROS2 or I missed some important step in the tutorial? Thanks!Hi @slugovoy  - Someone from our team will review and respond back to you.Powered by Discourse, best viewed with JavaScript enabled"
921,synthetic-data-camera-and-post-processing,"Hello everyone!For the recording of synthetic data from a camera, the goal is to be as similar to the real camera images. Hence, I would like to change some parameters of the camera like contrast, saturation as well as other effects. I have seen that there is a post-processing available (Post-Processing — Omniverse Robotics documentation) and I want to know, how to set those parameters in a python script and if the settings affect all cameras or just individuals? Please give some example scripts.Thank you a lot!Best regards,
ChristofHi there,apologies for the late reply, here is an example script you can run int he Script Editor to set for example grain noise:Thank you!This setting affects all cameras right? Is it possible to have individual settings for the cameras?Correct, these settings are global. We offer an augmentation system for implementing custom per-camera augmentations. Here’s a super simple example taking an RGBA input and producing an RGB output array:You can find more info in the API docs (API — omni_replicator 1.6.4 documentation). We’re also planning more default augmentations and some more examples to cover more use cases. Let us know if you have any feedback or suggestions on the type of augmentations that you’d like to see us cover!Thank you!Here are some more examples: [Isaac 2022.2] Generalize the example for adding noise to sensors - #3 by ahaiduThis topic was automatically closed 14 days after the last reply. New replies are no longer allowed.Powered by Discourse, best viewed with JavaScript enabled"
922,contact-sensor-in-orbit-parallel-environment,"I am currently using contact sensor in my RL environment.The problem is the contact sensor seems to stay in the initial position and does not seem to move together with the gripper fingers, resulting in the API always returning zero values. May I know how should I solve it?Thank you in advance.Hi @berternats  - The contact sensor in Isaac Sim is simulated by summing all forces applied on a given trigger spherical region intersected with the given body surface via the omni.isaac.sensor extension. If the sensor is not moving with the gripper fingers, it’s likely that the sensor is not properly attached to the gripper in your setup.Here are some steps you can take to troubleshoot and solve this issue:Check the Parent-Child Relationship: Ensure that the contact sensor is a child of the gripper finger in the scene graph. This ensures that the sensor moves together with the gripper finger.Verify the Sensor Position: Make sure that the sensor is positioned correctly on the gripper finger. If the sensor is not in the correct position, it may not detect any contact.Update Sensor Position: If the sensor is a child of the gripper finger and is still not moving with it, you may need to programmatically update the sensor’s position based on the gripper finger’s position in each time step of your simulation.Check the Sensor Settings: Make sure that the settings of the contact sensor are correct. For example, the radius of the spherical region should be set appropriately to detect the contacts.Powered by Discourse, best viewed with JavaScript enabled"
923,is-it-possible-to-simulate-a-tracked-robot-in-isaac-sim,"We would like to create a training/simulation environment for a tracked robot.
Putting wheels under the model is not a satisfying option as the robot needs to overcome various obstacles including stairs.Is there a way in ISAAC Sim to facilitate such a feature?Hi @christian.rammelmueller - Can you elaborate little further to understand your use case?Hello,basically we have a tracked robot which should be used in rescue robotics.
The robot has to overcome various obstacles in real life including debris from collapsed buildings, staircases, …
When I use virtual wheels for moving the robot in the simulation, it can not overcome such obstacles.To Speed up development with SLAM/Navigation we would like to use Isaac Sim as a virtual training environment, but we want to be as close to reality as possible. Therefore we would need a true tracked robot simulation.In principle a pendant to this Gazebo feature: Ignition Gazebo: TrackedVehicle Class ReferenceThe robot looks somehow like that: Tank Platform | AMBOT | American Robot Company@christian.rammelmueller - Thank you for more information. Someone from our team will review this and get back to you.Powered by Discourse, best viewed with JavaScript enabled"
924,omniverse-launcher,"i already uninstalled onmiverse and i still see login window of omniverse launcher. I deleted all the folders relevant but why is this launcher still there? I see this every time i switch on computer.Powered by Discourse, best viewed with JavaScript enabled"
925,omniverse-custom-writer,"Hey everyone,
maybe you can help me.I work at an extension, where i programmed a remote UI for driving the Jetbot inside my environment:image1432×335 18.5 KBThe last step is, when the train mode is activated by pressing the tainings buttom, that the camera of the Jetbot collects the images and save them and in the same time, to collect the current speed values of the Jetbot wheels prims and save them together with the image IDs into a csv file.I noticed that i can’t  use the Basicwriter of Omniverse so i tried to adapt all necessary functions i need to copy the basicwriter  and to extend it for my own purpose and i face some issues:current writer:
image1098×807 140 KBoutput:
image1663×874 187 KBAs shown here,  the counter jumps sometimes several numbers. How can i fix this issue?GreetingsPowered by Discourse, best viewed with JavaScript enabled"
926,real-time-audio2face-to-unity,"I have to create a digital human assistant, the NLU and NLG parts are already done and I was trying to find out how can I use the Audio2Face features to lipsync the digial human model with the text or real time audio streaming generated by the NLG.
Is there a workflow to visualize in real time in Unity the facial animations generated by Audio2Face?Unfortunately, currently we don’t have this.There are 2 steps into make this possible:Thank you for your reply.How much time do you think it will take to release those features?To be honest, it’s not easy to predict when all of these are going to happen. But Unreal Engine connector should get updated in the coming days.I’m happy to say that the first part of the problem is resolved in the latest Audio2Face 2023.1.0 and we can stream blendShape weights.The 2nd part can be done by any user or by the Unity team.Thank you for the pleasant update.
Best regardsThis topic was automatically closed 14 days after the last reply. New replies are no longer allowed.Powered by Discourse, best viewed with JavaScript enabled"
927,why-i-have-to-re-download-after-failed-to-install-isaac-sim,"I was using the omniverse-launcher in ubuntu to install Isaac-sim, after downloaded nearly 6.31GB files, the installation stuck due to some pip install error, but when I want to reinstall Isaac-sim, the omniverse-launcher still need to re-download that 6.31GB files. I want to know is it normal or is it a bug?Hi @Te77iiiii_Ho  - I understand your frustration and have raised the question to the respective team. I will let you know as soon as I hear back from them.Powered by Discourse, best viewed with JavaScript enabled"
928,can-i-run-isaac-ros-on-jetson-and-get-the-image-and-data-from-x86-isaac-sim,"For tutorial for Visual SLAM with Isaac Sim and NVBLOX with Isaac Sim, I can run the Isaac SIM and Isaac ROS on the same X86_64 Platform.But if I want to run Isaac ROS on Jetson platform and get the images and data from X86 Isaac SIM, Does anyone know how I should configure it? Thank you.Hi @jamesw - Thank you for reaching out. You can review the blog and code below. Let us know if you have any follow-up questions.Blog post: Design Your Robot on Hardware-in-the-Loop with NVIDIA Jetson | NVIDIA Technical BlogCode: GitHub - NVIDIA-AI-IOT/isaac_demo: Set of demo to try Isaac ROS with Isaac SIMPowered by Discourse, best viewed with JavaScript enabled"
929,preserving-appearance-of-imported-solidworks-model,"I am working with my institute on integrating Omniverse into a Solidworks CAD course, but I have been having trouble with Omniverse Create preserving the appearance of my imported assembly. I have tried importing the model as an IGES file and an SLDPRT file, but both imports have visual issues. The IGES file is missing textures and some parts, while the SLDPRT file has textures applied completely wrong (which I cannot fix using Omniverse’s material applier since the original model has colors applied at a face/feature level). I should also note that the model itself is not consistent in its construction method, as it was made by a group of students who each created their own subassemblies and parts (so the application of materials is at a mix of part, subassembly, and assembly levels). The model was created using Solidworks 2021-2022, not the most recent version. I am hoping to find a solution that is relatively easy to follow, because if there is too much user friction in importing Solidworks parts into Create, it would not be worthwhile for students to use Omniverse in the course. Attached are some images of what the model is supposed to look like and what the imports look like, so I hope that will be of help if I can get some help. Thanks!Link to viewing images: Omniverse Support Forum Images - Preserving Appearance of Imported Solidworks Model - Album on ImgurCan you share your USD file? Materials are unlikely to look exactly the same in a CAD system and in Omniverse. The materials in CAD systems is usually much simpler than the material system in Omniverse. That said, depending on the structure you get when you import your CAD, we should be able to come up with a pipeline that makes the materials look really good with little effort from your students.ME 1670 Omniverse Demo - Mech Import.usd (12.1 KB)The .usd file is attached. Let me know if you need anything else from me. Thanks!Hey StrainFlow, just checking that you received the .USD file on your end via my previous message. If not, here is the link again:ME 1670 Omniverse Demo - Mech Import.usd (12.1 KB)@agraves36 : Are you able to share the source files? IGES or SLDPRTSolidworks colors dont come in as complex shaders in OV, sometimes its labled none if its a default color sometimes is a simple color shader. In either case, the colors from solidworks are simply that, just colors. They lack reflectivity, roughness, top coat, refraction etc. We need the shaders to look great in OV so that when we render with path tracing the product/render/scene look ultra realistic. Unfortunately, even solidworks integrated rendering tool, visualize and photoview, even require repainting with better shader to make the model look better/realistic. What i propose is to limit the amount of painting in solidworks, and instead use that time in Omniverse Composer. I have two videos i would like to share with you to help the way you paint/group etc. This will change the way you use composer 100% you have my email from our meeting today, please reach and I will send you a few links. -adamhHi Andrew,
Here is a .zip file containing the .IGS, .SLDPRT, and .SLDASM files of the model, as well as images of what the imported models looked like on my end (updated for USD Composer 2023.1):
Georgia Tech ME 1670 Solidworks Mech.zip (57.7 MB)Powered by Discourse, best viewed with JavaScript enabled"
930,omniverse-maya-native-connector-v207-0-beta,"We are pleased to announce the release of v207.0 of the Omniverse Maya Native Connector.HighlightsCheck out the complete Release Notes , and Documentation .Thank you!Powered by Discourse, best viewed with JavaScript enabled"
931,how-can-i-revise-and-synchronize-the-program-of-isaac-sim-hello-world-example-in-real-time-with-rendering,"If I want to modify the Hello world code and synchronize the rendering with the program in real time, how can I do that? Thank you very much!Screenshot from 2023-07-04 09-21-431094×690 265 KBI need help. Thanks!Hi @lily.chen  - Isaac Sim does not currently support real-time code modification with immediate rendering updates, also known as hot reloading. This means that any changes you make to the code will not be reflected in the simulation until you restart it.However, you can still make changes to your code and see the results relatively quickly by following these steps:Powered by Discourse, best viewed with JavaScript enabled"
932,issac-sim-not-launching,"Hello, I just installed Isaac Sim and trying to run it, but when I click the launch button, it loads and stops launching, nothing shows up.Here is the spec:Windows 11
i7-9750H CPU (6 cores)
12 GB Rams
GeForce RTX 2060 GPU
6.14 GB VRamsHi @jcsuh  - Here is the Isaac Sim HW requirements: 1. Isaac Sim Requirements — isaacsim 2022.2.1 documentationMeanwhile, if you have access to docker then you can try that.Powered by Discourse, best viewed with JavaScript enabled"
933,wrong-parallel-gripper-position-when-gripping-an-object,"Hello,I created the urdf of a cartesian robot with a schunk EGH gripper fixed on it:
cartesian_robot_EGH.usd (1.3 MB)
cartesian_robot_EGH.urdf (9.6 KB)When using this gripper in Isaac Sim, the closure of the gripper does not work as expected when closing the gripper to grasp an object.
Indeed, one of the tip reaches its target position but the other one does not and the gripper is not centered as it would be in reality. In the urdf, the “mimic” property is used to represent this behavior. In this issue Problem for isaacgym for mimic joint, it is said that there is no mimic property provided by default in isaacgym, is it the case in Isaac Sim now ?To solve the problem, this issue How to implement gripper open/close action? suggests to reduce the number of dof to have X dof for the joint of the arm + a dof for the gripper (and not 2 dofs for the 2 fingers of the gripper). However, I don’t know how to implement it knowing I used the SingleManipulator class for my robot and the ParallelGripper class to control my gripper.Does someone know how the gripper could have the good behavior ?Hi @alempereur - Someone from our team will review and respond back to you.Hi @rthaker, do you have any update from the person who looked at this ?Powered by Discourse, best viewed with JavaScript enabled"
934,how-to-destroy-a-render-product,"Hello.
I’ve been trying to make a big amount of camera work on isaacsim but i’m running in a problem here.
Steps to reproduce[in a terminal]At that point my python script crashes, saying :I figured with nvidia-smi that my gpu memory usage explodes at runtime.
And despite the fact that that i’m puting the force_new argument to false, it keeps creating new render products.
Worst thing is even after deleting the camea from the Usd Stage with, my gpu usage still does not decrease.
Deleting the render product by its path (/Render/RenderProduct_Replicator_0x) does not help either.
So here are my questions :Hi there,a render product is a combination of a camera and a resolution, if the camera and the resolution is the same, no new render product is created, only if the force_new flag is set to True.Currently there is no clean way for destroying a render product, in the next release this should be possible. I will try to look for a workaround for that if possible using the current version.As a workaround to your scenario, if the cameras (camera1, camera2, etc.) have the same properties only different locations, you can “re-use” a render product by pausing the simulation, moving the camera to the locations of the camera1, camera2, etc. and then un-pausing. Basically instead of collecting the data in parallel (which causes you to run out of memory) you would run it in series.Let me know if you have any questions.Best,
Andrei@ahaidu Thank you for your answers. I actually figured out a way to switch render product for a camera. I was inspired by the code of the omni.isaac.sensor 's Camera class. It goes like this :This works for me but detaching anotators makes it crash, but a least now my VRAM usage does not rise too much.
The other catch is that gpu power usage rises sharply at each annotator attached.Powered by Discourse, best viewed with JavaScript enabled"
935,wizard-vehicle-and-ros2-controller,"Hi, I want to drive my car with ROS2 using a wizard vehicle, but there is no joint. How can I drive my robot with an articulation controller or something else?https://docs.omniverse.nvidia.com/app_isaacsim/app_isaacsim/tutorial_ros2_drive_turtlebot.htmlWhat I mean by wizard vehicle is at the link down below.
https://docs.omniverse.nvidia.com/prod_extensions/prod_extensions/ext_vehicle-dynamics.html#next-steps-pageHello!I don’t think you can drive the vehicle with an articulation controller.If you create a vehicle using the Vehicle Wizard with a “Drive Type” of “None”, then each of the tires have a Vehicle Wheel Controller properties window where you can set the Steering, Wheel Torque and Brake Torque separately for each tire. You can then write a script or use OmniGraph nodes to control the car with a traditional proportional feedback controller.I hope I am answering your question as I don’t entirely understand the question.Hi, yes I think I could ride it with Omnigraph as you said. I will try…My plan is to drive the jetracer or a car like that while using a wizard vehicle with better physics.This topic was automatically closed 14 days after the last reply. New replies are no longer allowed.Powered by Discourse, best viewed with JavaScript enabled"
936,problem-about-force-sensor-in-parallel-env,"Hi, currently, I am using the force sensor for my robot.https://docs.omniverse.nvidia.com/isaacsim/2022.2.1/isaac_sim_sensors_physics_articulation_force.html?highlight=force%20sensorThe force sensor is working well.
But in the parallel environment (Orbit), the sensors only return zero values.
May I know how to solve the problem?I am using this API to listen to the force sensor;
prims_utils.get_prim_property(prim_path=path, property_name=“physxArticulationForceSensor:force”Thank you.Hi @berternats  - The issue you’re experiencing might be due to the fact that the Orbit parallel environment runs physics simulations on the server side, and the client side (where your script is running) might not have access to the real-time physics data.To solve this issue, you might need to modify your script to run on the server side. This can be done by using the omni.isaac.dynamic_control API to send commands to the server and receive responses. Here’s an example of how you might do this:In this example, articulation_handle is the handle of the articulation that the force sensor is attached to, and link_index is the index of the link that the force sensor is attached to. You can get these values using the dc.get_articulation_handle and dc.get_link_index methods, respectively.Please note that this is just a general suggestion and might need to be adapted to your specific setup._sensor_dataHi, May I know where is the docs of those APIs?I checked this website, but I could not find any related information.
https://docs.omniverse.nvidia.com/py/isaacsim/source/extensions/omni.isaac.dynamic_control/docs/index.html#Thank you.This topic was automatically closed 14 days after the last reply. New replies are no longer allowed.Powered by Discourse, best viewed with JavaScript enabled"
937,how-can-i-reuse-the-python-code-worked-on-isaac-sim-in-the-jetson-board-being-developed-based-on-ros,"Hi
I have a question about the continuity of development with isaac sim and jetson board.RMPflow lib (omni.isaac.motion_generation) is used for motion generation in isaac sim.
If I have created a custom robot in isaac sim and created motions using the RMPFlow library, how can I reuse the python code I worked on in isaac sim?I would like to port the motion code with this library to a real jetson board that works on ROS basis.Can you suggest some good tutorials?I would be interested in a similar problem… I have implemented a simulation with a universal robot and the RMPflow algorithm in Isaac Sim, which works quite well.My question now is how to best implement the RMPflow motion policy in a real robot. Have you found any possible solutions?Thanks a lotRMPflow motion policy in a real robot.Hi @robin-vetsch  - You can explore the Isaac Cortex documentation as a starting point.1. Isaac Cortex: Overview — isaacsim 2022.2.1 documentationPowered by Discourse, best viewed with JavaScript enabled"
938,permission-exception-when-importing-mc-widget-library-extension,"So I would like to use the “mc-widget-library” that Mati streamed about on YouTube. The repo is here:Contribute to mati-nvidia/mc-widget-library development by creating an account on GitHub.I cloned it and tried tried to import it with “Import Extension”
But then I get the dreaded “Permission Exception” that several have posted about here.It isn’t clear to me exactly which folder I should be clicking on, but it doesn’t seem to matter because they all lead to the same error.There are others who had the same problem, but the “answers” that were posted did not help.I tried a couple of other things, like adding the directory to the “Extension Search Paths”, but that led to the same error, or sometimes, nothing happening at all even when I “Refresh”.Yet people seem to be getting work done. I have tried importing other extensions, and I always get this “Permissions Exception” which seems to be a misleading error message, probably a message in a general “catch” block.Powered by Discourse, best viewed with JavaScript enabled"
939,new-weekly-isaac-sim-robotics-livestream-every-tuesday-at-11am-pst,"
capture.2023-03-01 16.25.581920×1080 84 KB
Please join a rotating cast of Isaac Sim developers for a new weekly livestream series every Tuesday at 11am PST! The developers will be answering forum questions, show new projects, and answer your questions live. For more background on Isaac Sim, please see: What Is Isaac Sim? — Omniverse Robotics documentationJoin the inaugural livestream on Tuesday April 4, at 11am PST! We’ll be broadcasting on:
YouTube: NVIDIA Omniverse - YouTube
Twitch: Twitch
Discord: DiscordAdd this inaugural livestream to your calendar: AddEventCheck out all upcoming events / livestreams: AddEventPowered by Discourse, best viewed with JavaScript enabled"
940,gv100-capable,"Is Omniverse compatible with Tesla V100? My computer only has GV100, how can I use Omniverse with it?Powered by Discourse, best viewed with JavaScript enabled"
941,pbd-deformable-cloth-manipulation,"Hello,
I’trying to set up a deformable object pick and place application in omniisaacgymenvs for a cloth manipulation task. However the cloth keep slipping out of the gripper in this way…Currently, i’m moving the joints using  Articulation view API to set the joint position to grasp the gripper. I also tried to set joint efforts during the grasping but it doen’t help.Am i missing something regards the grasp phase? I’ve tried severals set of parameters for the cloth without any success
Thank youHi @user126140 - Someone from our team will review and respond back to you.For cloth - it is recommended that you make a surface mesh only - no thickness. Also - the mesh needs to be tesselated to the particle dimension. Check the Particle Cloth example in Window->Simulation->(physics)Demo Scenes.This is how the generated mesh should look like, when you look at it with the wireframe mode enabled.
image1508×968 213 KB
This works for planar rectangular shapes. For Tshirts or other types of meshes it’s recommended to import it such that it’s a surface mesh (it can be a 3D mesh as long as it’s a 2D surface), and that the vertices are at a maximum distance according to the resolution.Thank you for the response, i tried both plane and mesh but no result.Using the default mesh provided in the example is not reliable and the result are not consistent for my application.So far i;ve the following questions:Thank you againYes - the code above generates a planar surface mesh. For other things you will need to use a tesselator that generates a mesh with a max distance between vertices so you get the desired granularity, and have a mesh that is made only of surface geometry.The problem with having a solid geometry is that it will generate something similar to a bag - then one side of the cloth will collide against the other, cause interpenetration, and it will be a mess - that’s what I can notice in your video when one side of the cube interweaves with the other as it’s folding the cloth - then the penetration forces are strong enough to cause the grasp to slip.that can be somewhat solved by tuning the particle size among other things too - if you send me your file I can take a look into how to make it perform betterWhat should be the best dimension for a cloth-like?
I’m using a drape of 30cm x 30cm with 100 particles per side.The dimensions and particle count for a cloth-like object in a simulation can vary greatly depending on the level of detail you need and the computational resources you have available.A 30cm x 30cm drape with 100 particles per side would give you a particle spacing of about 0.3cm, which should be sufficient for a fairly detailed simulation. However, keep in mind that increasing the number of particles will increase the computational load of the simulation, so you’ll need a more powerful system to handle it.If you find that the simulation is running too slowly or not providing enough detail, you could try adjusting the dimensions and particle count. For example, you could try increasing the particle count to 200 per side for more detail, or decreasing it to 50 per side for better performance. You could also try adjusting the dimensions of the drape if you need it to be larger or smaller.Ultimately, the best dimensions and particle count will depend on your specific needs and resources. It may take some trial and error to find the right balance between detail and performance.Powered by Discourse, best viewed with JavaScript enabled"
942,collision-spheres-of-lula-rmpflow,"Hi all,I have created a YAML file for a UR10 robot using the Lula Robot Description Editor, with corresponding collision spheres for the robot.If I now define the base position of the robot not at [0,0,0], but move the base position for example 1m in x direction [1.0, 0.0, 0.0] then the collision sphere does not longer match the robot (see image).How can I realise the dependency of collision Sphere from the YAML file with the robot?Many thanks for your helpCollision_Sphere_UR10.PNG1253×832 65.4 KBI have created a YAML file for a UR10 robot using the Lula Robot Description Editor, with corresponding collision spheres for the robot.If I now define the base position of the robot not at [0,0,0], but move the base position for example 1m in x direction [1.0, 0.0, 0.0] then the collision sphere does not longer match the robot (see image).How can I realise the dependency of collision Sphere from the YAML file with the robot?The collision spheres in the Lula Robot Description Editor are defined relative to the base frame of the robot. If you move the base position of the robot, the collision spheres should move with it. If they do not, it might be a bug or a misconfiguration in your YAML file.Here are a few things you can check:Powered by Discourse, best viewed with JavaScript enabled"
943,nvidia-keynote-at-siggraph-2023-tue-aug-8-8am-pst,"NVIDIA will kick off SIGGRAPH with a keynote on Tuesday, August 8 at 8 AM, that gives an exclusive look at some of our newest technologies, including award-winning research, USD developments, and the latest AI-powered solutions for content creation. Watch the live stream here: NVIDIA at SIGGRAPH 2023 ConferenceFor all things NVIDIA at SIGGRAPH, please register for the event here: https://s2023.siggraph.org/register/Powered by Discourse, best viewed with JavaScript enabled"
944,why-no-canvas-on-linux,"Why is there no NVIDIA Canvas implementation available on Linux?Powered by Discourse, best viewed with JavaScript enabled"
945,application-is-gpu-intensive-at-startup-gpu-cache-is-not-properly-released,"Hello,When starting up Issac Sim I noticed that the application heavily strains the GPU at the default blank environment. I have tried to mitigate this issue using the following method:The above helps only for a little while, I noticed that with the main viewport camera, GPU memory will continue to collect without freeing the GPU cache.A simple experiment that I ran to confirm this is adding multiple camera Prims (called in python using Camera class) and noticing the “GPU resources” tab adding ~1 Gb of memory for each camera. When I tried to delete the created camera prim via the GUI, I noticed the GPU memory not freeing the added memory.I have 2 questions:Platform:Hi @InfraredLaser  - There is a new Isaac Sim release coming around Aug/Sep timeframe. Please try that release and report back if you are seeing the same or other issues.Meanwhile, to answer your question in terms of Isaac Sim 2022.2.1 release:Freeing GPU Memory: Currently, there is no explicit way to free GPU memory without restarting Isaac Sim. The GPU memory management is handled by the underlying graphics API and the operating system. When you delete objects in the scene, the memory they occupied may not be immediately released back to the system, but should be reused when new objects are created.Saving Rendering Preferences: Isaac Sim should save your preferences automatically when you close the application, and restore them when you start it up again. If this is not happening, it could be a bug. As a workaround, you could create a startup script that sets your preferred settings each time Isaac Sim is launched. You can use the omni.kit.settings API to get and set various settings. For example, to set the UI FPS limit, you could use:Please replace “30” with your desired FPS limit. You can use similar code to set other settings. This script could be run automatically at startup by placing it in the ~/.config/ov/kit/startup directory (create this directory if it doesn’t exist).Powered by Discourse, best viewed with JavaScript enabled"
946,lidar-cannot-detect-the-collision-enabled-model-and-cannot-use-semantic-data-on-oige,"Hi,I’m trying to use lidar on OIGE. However, I am stuck on two problems.
moving_target_point_cloud.7z (28.0 MB)How can I solve those problems?ThanksI’ll try to pass this to someone else in the team that knows more, but is there any way you can get the data to be of type float64, float32, float16, complex64, complex128, int64, int32, int16, int8, uint8, and bool instead of uint16?  That data type is not supported.I change the data type to int64 viafrom exts/omni.isaac.sensor/omni/isaac/sensor/scripts/rotating_lidar_physX.py,
def _data_acquisition_callback(self, step_size: float) -> None:However, there is no semantics informations.I think the main problem is the ‘Lidar cannot detect the collision-enabled model.’ which is I mentions first.I changed LidarView to below.But it still not works. Here is my code.
moving_target_point_cloud.zip (14.4 MB)Hi there, if you are running from the OIGE framework, can you try setting the enable_scene_query_support variable in the task config file to True?Powered by Discourse, best viewed with JavaScript enabled"
947,unable-to-creat-project-folder,"when i click the icon “Publish the current design …” , it show me always "" unable to creat project folder"" . I can run the omniverser viewer (presenter). I dont know what i should do to render in the viewer. Could someone help me?Hi @atelier-li ,If this is about how to use the SketchUp connector I think you can refer to the first link but did you setup your nucleus server(local or otherwise) correctly ? If not I think you’ll need to setup nucleus first before you can port file and open it up in usd composer or view.https://docs.omniverse.nvidia.com/connect/latest/sketchup/manual.htmlhttps://docs.omniverse.nvidia.com/nucleus/latest/workstation/installation.htmlPowered by Discourse, best viewed with JavaScript enabled"
948,rules-and-regulations-of-shared-extensions,"CONTENT WITHIN THIS FORUM IS CONTRIBUTED BY SITE MEMBERSINTELLECTUAL PROPERTY
This site allows members to share their created Extensions with the Omniverse community.  Please be advised, NVIDIA does not vet the Extension content shared on this forum.By sharing content on this forum, you automatically grant the irrevocable and perpetual, non-exclusive, transferable, fully-paid, royalty-free, worldwide license, by ourselves or others, to use, copy, distribute, publicly perform, publicly display, print, publish, republish, excerpt (in whole or part), reformat, translate, modify, revise and incorporate into other works, that the content and any works derived from that content, in any form of media or expression, in the manner in which NVIDIA Omniverse permits content to be used, and to license or permit others to do so.Users have the right to sublicense content to other Users to permit their use of that content in the manner in which the service permits content to be used.LIABILITY
You must own all intellectual property in your own original content you contribute.  You must not upload or contribute any content not originally created by you, or any content which is not property licensed to you by someone else for uploading or contributing.ACCEPTABLE USEYes! Good! Do this!No! Bad! Do Not Do this!Powered by Discourse, best viewed with JavaScript enabled"
949,rendering-to-image-files-using-python,"Hi,I have set up a scene with physics enabled. I would like to start the timeline programmatically via a Python script running headlessly and render frames to image files.I know how to do this using Isaac Sim. Is there also a way to achieve this using Kit only, without any Isaac Sim dependencies/extensions?Best,
BrunoWhat I have achieved so far is this:I run a simple python script like this:In the script I open an existing USD scene with physics driven objects:I was also able to do some physics stepping like this, and retrieve an object’s position after each step:So here is another question: In this setup, is it possible to just start the timeline instead and use some callbacks to react on physics steps?Thanks for any help
BrunoSo here is another question: In this setup, is it possible to just start the timeline instead and use some callbacks to react on physics steps?Hi Bruno,
You should be able to start a timeline like this:
omni.timeline.get_timeline_interface().play()
(if async, add await omni.kit.app.get_app().next_update_async())You can also add callbacks to physics events like
physx_interface.subscribe_physics_step_events(callback_fn)Generally, I suggest you to look into World, SimulationContext and PhysicsContext classes of Isaac SIM to have an idea on how to do these. In IS we wrap these concepts around a simpler to use API but they use PhysX and Kit APIs under the hood. If you take a look into the classes you will find most of these functionalities and calls.Powered by Discourse, best viewed with JavaScript enabled"
950,combobox-autoclosing-when-mouse-moves-outside-of-modal-window,"I’m running into an issue with ComboBox and modal Windows:I’ve created a dialog by creating a ui.Window with the ui.WINDOW_FLAGS_MODAL flag set. In that window I’ve got a ComboBox, and when open, the ComboBox overflows the dialog. When the cursor moves outside of the dialog window (i.e. when trying to select options at the bottom of the ComboBox), the ComboBox automatically closes.Is this a bug or am I missing something? Does anyone know of a way to get around this problem?image723×477 19.4 KBHi @erica.brett. Thanks for reporting this. I’ve sent this along to the team and we’ll see what they say.Powered by Discourse, best viewed with JavaScript enabled"
951,isaac-sim-os-driver-persistent-memory-leak,"Every time I run Isaac Sim from python it causes about ~0.4GB of system memory to become persistently unavailable. This memory is locked up even after the run completes (with no explanation for its use that I can find). The only way I have found to restore the memory is to reboot the machine. My only guess is that the memory is being leaked by the driver.This occurs with code as simple as starting then closing SimulationApp. It occurs both natively and when run from within singularity container (converted from the docker image) where the host’s memory remains unavailable after the container is closed.My environment:Code to reproduce the memory leak:Can you try the latest Isaac Sim and see if you still see the leak?@eric.langlois Do you see the same leak when running Create/USD Composer?We tested on 2022.2.1 with that script and still have this issue. I haven’t tested Create / Composer yet.Hi @eric.langlois - Please let us know once you test script with USD Composer (previously called Create).Yes, I have this issue with Composer / Create (I could only find it under the name “Create” in the Exchange). I repeatedly launched Create 2022.3.3 from omniverse launcher, waited for everything to finish loading/rendering, closed it from the GUI and then measured my available memory:I ran a control without opening / closing anything and the available memory fluctuated by  ±50MB / 30s vs the consistent ~200MB decrease per Create run.Hi @eric.langlois - Thank you for checking this. I have raised this issue with internal team. I will keep you posted.@eric.langlois - We have few follow-up question that we want to ask:It does not matter to me (at present) whether it runs on one or both of the GPUs. Both GPUs are active on the system and I have not been taking any action to specify GPU(s) for it to use, instead assuming that Isaac Sim will select a sensible allocation of computation to the GPUs that I might later try to tweak if it proved suboptimal.Let me know if there is anything you would like me to try in terms of restricting the GPUs that it runs on, e.g. which environment variables would need to be set.Thank you for the response. Can you please run the same script by isolating one GPU (CUDA_VISIBLE_DEVICES=0 python.sh …) at a time and provide the results?Halving the number of GPUs turns out to halve the rate of memory leak. 200MB / run with 1 GPU instead of 400MB / run with both.Hello,Is there any update on this? On our shared machines, this causes a lot of issues with mandatory reboots, as system memory is “lost” after every run.Our configuration is very similar to OP,
Ubuntu 20.04 / Kernel 5.15.0-73
Nvidia driver 525.105.17
Isaac Sim 2022.2.0
CPU  AMD Ryzen 9 7950X
GPU: GeForce RTX 4090
We only have one GPU in this machine.We noticed the following:Take these numbers with a grain of salt - it doesn’t behave exactly the same way always. But most apparent is the ever increasing number of DirectMap4k, and it never gets released until reboot.Hi @pnm -  I would suggest to update your Isaac Sim to latest Isaac Sim 2022.2.1 release. Even with that version memory leak is known issue which is currently being worked on and hopefully will be fixed in the next Isaac Sim release (~Aug/Sep) timeframe.Powered by Discourse, best viewed with JavaScript enabled"
952,animate-a-character-with-ai-pose-and-stream-animation-into-unreal-engine,"Hi, is it possible to animate a character and stream animation data like a live link into Unreal Engine?
As Live Link does with other softwares, I was wondering to relationate these two software to stream animation into Unreal and animate a character. I saw that is possible to sync Machinima and Unreal to make the same things at the same time, but I was waiting a confirm before doing something with character animations.We don’t yet support live streaming of a USD Skel animation into Unreal.  USD static mesh transforms will stream live, but we have a bit of work to do before we can stream animation data into Unreal.  Thank you for asking your question, we’re starting to see more and more requests for things like this.  If you have any more information you’d like to share about your specific goals/plans/application we’d love to hear about it.  I’m currently tracking this feature with our internal issue OM-34784.@LouRohan Hello. It has been some time. Is there any official workflow on how to connect Pose Tracker data to Unreal Engine Live Link?Powered by Discourse, best viewed with JavaScript enabled"
953,failed-to-connect-to-the-server-nucleus-server-on-aws,"Hi there!
I deployed an Nucleus Service on AWS by following this:Deploying NVIDIA Omniverse Nucleus on Amazon EC2. Contribute to aws-samples/nvidia-omniverse-nucleus-on-amazon-ec2 development by creating an account on GitHub.It could run successfully but after I did some testing on user account created, it failed to connect to the server.
I tried to reboot the EC2 intances on AWS but it doesn’t help.
What would be the issue?
ThanksPowered by Discourse, best viewed with JavaScript enabled"
954,3dsmax-multi-material-can-not-show-the-bitmap,"
333.jpg1969×981 264 KB
The problem show in this video
https://youtu.be/uxZlVnA2k9sJust so so so simple I want a pillar with two mapThe BUG is after exported USD and MDL to OMN
multi-material can use 1 map only, not showing another mapif future project has many mapping in a poly, what can I do?This is not supported. You have to do it via polygon edit UI, you can set each face with one material ID. It will also support several materials on a mesh.image865×690 50 KBCould you please check the video i post before, you can see i already use material ID on different face to assign 2 maps on my box with muti-texture, after export to OMN, it doesn’t work -.-https://youtu.be/uxZlVnA2k9scan you try using default bitmap and not vraybitmap and see if it yields the same result?Powered by Discourse, best viewed with JavaScript enabled"
955,warp-extension-not-visible,"I cannot see warp extension in omniverse create xr. My version of omniverse is 2022.2.0-rel.1
Do I need to update to see the extension, can i install extension otherwise?Powered by Discourse, best viewed with JavaScript enabled"
956,replicator-how-to-get-cameraviewtransform,"Using Replicator, I’d like to capture the 6-D pose of an object in the local frame of the camera.    However, the output type to generate “camera_params” is missing.    What to do?Using the UI Isaac Sim and python to drive replication, “Synthetic Data Recorder” it is simple to check off “camera params”.   And indeed, this all works fine to allow me to find the pose of an object.    However, the power of replicator is difficult/complex to reproduce.So is there a way to get camera params with replicator?(Yes, I know I can’t change the camera, so I can probably guesstimate the transform.   Is that the only work around?)Hi there,can you check if the Offline Pose Estimation tutorial contains the function you are looking for.Best,
AndreiIMHO there are currently roughly three ways to generate synthetic data:btw, thanks for your response!
pthe links are not working. Could you please tell how I can access the camera parameters (K) if I have an script like this?with this code:everything is set to zero. So, not sure why proper values are not posted?My ideal goal is to be able to set the camera params to that of Intel RealSense D435.Hi there, can you try calling rep.orchestrator.step() once before calling get_data()?Powered by Discourse, best viewed with JavaScript enabled"
957,massive-memory-leak-under-linux,"Hello,we are seeing a massive memory leak wir USD Composer / kit under Linux.Starting and stopping USD-Composer adds round about 800mb (!) of memory load to the system. The memory load of other processes keeps constant so I think this could be a driver problem.This is the output from nvidia-smi:
WindowsTerminal_KABp62ZNVV808×513 12.1 KBWe are using version 2022.3.3 in a headless environment.What can we do? This should be a production maschine, but even the 512GB of ram ran out very fast…ThanksCarl@mati-nvidia Can you have a look at this? This is really urgent for usHi Carl. Are you able to test with the latest USD Composer 2023?I’ve tried already, but when using the latest version with our extension I’m getting a strange new error reporting that some omniverse client cannot reach a public nvidia resource… sorry I don’t have the exact message at the moment.Do you know about such a problem that could be corrected in the latest version or is it only guessing? If yes I would dive deeper into the problems with the newest version and try to figure out whats the problem…Do you remember this……perhaps this (the way we shut down the process) could be the problem.Powered by Discourse, best viewed with JavaScript enabled"
958,blender-branch-installation-freeze,"For Blender? No. followed the path and it quits instantly. OV Exchange still listed as install.OV starts up without a problem.do you see the Blender 3.6 alpha USD branch in the Library tab? If so, let’s uninstall it (click on the hamburger next to launch > Settings > Uninstall)No. OV library only lists Cache as installed.when you try to reinstall Blender from Exchange tab again, does it still hang?i am inclined to ask you to restart your machine in case there were processes that was conflicting with the install as well. also, do you have any antivirus?Now it doesn’t hang anymore, now it downloads, attempts and install and crashes out. Ill try another reboot now.once you reboot, try to reinstall again. upload the log when it crashes out.Install is back to hanging at the same spot.
launcher.log (355.0 KB)seems like it’s a different issue. do you have any active antivirus running at the moment?Yes, Microsoft antivirus and firewall.can we try something and disable them both for a minute to see if it affects the install?Okay, I tried disabling both anti-virus and firewall together. Same exact results. I was hopeful, but this is the kind of rabbit hole I try to avoid. Round and round until everyone is crazy.  This version of Blender is clearly still in Alpha and I should ignore it until its more mature. Glad it works for some, but I can’t waste anymore time on it. Thanks for your help.we’ll see if the dev can chime in with your latest uploaded log. my thoughts moving forward was to clear the cache and run the cleanup tool, but it’s always a challenge troubleshooting when pinpointing the problem on different setups and scenarios. (i am just a passerby, so it does take me longer to get to the same result 😔)hope eventually you can arrive at a resolution and get to use it, though.Ok, I have an update.
I went back and went to the OV Blender Installer and went through the “All Blender Builds” list. Starting  from the top, I tried the installs from top down. Version103.1 worked!  I consider my machine to be completely NVIDIA ready. I made it to be an Unreal Engine content creator workstation.|Processor|13th Gen Intel(R) Core™ i9-13900KS   3.20 GHz|
|Installed RAM|64.0 GB
GeForce RTX 4090.I don’t believe it’s a problem on my end.yes, the intent for the cleanup tool was to reset Launcher back as if it’s your first time using it, so it’s a way to soft-rest the state of things.personally, i think what you are experiencing isn’t a common occurrence; so, i wouldn’t lose all hope. at least, not until you hear back from the Nvidia devs since they are more knowledgeable with the apps/connectors whereas i am simply doing cross-elimination based on the log. but if you don’t feel comfortable with the state of things, it’ ultimately your call whether or not to proceed with it.your spec looks perfectly fine based on that list. but i know there are users that don’t have those hardware yet able to install without issue. so i can’t say the problem stems from the installer completely either.so none of the 200’s builds work for you, then?Correctmy observation of the 200’s and their commonality is that they are 3.6.0-usd builds. but not sure if they have any relevance. i’ll bow out of this thread and let the dev jump in here in case i lead you astray or go down the rabbit hole again. 🤐(the original intent of me jumping in was to facilitate the troubleshooting process since the last person who had the problem was back in March with no resolution.)Powered by Discourse, best viewed with JavaScript enabled"
959,global-volumetric-effects-cc4-bug,"Hey, if I use the awesome Global volumetric effects in Path traced rendering, I get this:

pe-Natasha.0374960×540 31.5 KB

See those white error lines in eye browses?
They disappear when camera comes closer… Is there some value I could change to totally fix this error?few frames later:

pe-Natasha.0420960×540 26.5 KB
Here you can see how the error disappears when camera flies close her…Wow! Looks like the new Composer 2023.1 has fixed this issue :)
Global volumetric effects are ON here, and this is rendered in Path-Traced.
No white artifacts!!Camilla_composer.00131920×1080 46.2 KBPekka, that skins looks fantastic!Can you share your shader/rendering settings?It is super simple. Just install the latest Character Creator or IClone and export one of the free CC4 models that come with it. This one is naturally Camilla ;)Export with Path Traced as a target. Leave all rest on default.Import the USD to Composer. IF there is still a problem with the white triangle, here is the fix for that:
https://forums.developer.nvidia.com/t/cc4-extras-white-geometry-in-eye-browses/214429/8Activate the path-tracing mode.Delete all lights that possibly came with USD export from Reallusion.For rest of the scene, here is my setup:Std_Skin_Head make sure these values are not higher:image627×1788 135 KBEnvironments/2023_1/DomeLights/Studio/lightbox02.hdrimage1642×1673 150 KBCamera:
image630×750 35.6 KBNative OV sphere light as main Key light.image1679×1638 127 KBFFT Bloom, FireFly Filterin, Denoising, Global Volumetric Effects all ON with Default settings!Thats all folks.Orginal render 6,66 Mb LOL4000×2250 496 KBThis topic was automatically closed 14 days after the last reply. New replies are no longer allowed.Powered by Discourse, best viewed with JavaScript enabled"
960,how-to-change-the-nut-to-another-object-in-the-example-factorytasknutboltpick,"Hello,I’m currently working with the OmniIsaacGymEnvs package / FactoryTaskNutBoltPick exampleI would like to modify the example code to pick an object(ex. harness) instead  of the nut(default picking target)
However, It seems to be facing a problem related to an .usd file format in Isaac Sim.What I’ve tried is[harness.usd and nut.usd stage tree]
harness.usd1387×596 141 KB
Screenshot from 2023-06-13 13-59-411387×591 75.5 KB[Isaac Sim. Output]
Screenshot from 2023-06-13 14-02-041922×1005 211 KBharness penetrating into the desk (No more falling down. fluttering around)
Screenshot from 2023-06-13 14-02-501294×639 81.7 KBerror messagePhysX error: PxgDynamicsMemoryConfig::collisionStackSize buffer overflow detected, please increase its size to at least (number) in the scene desc!Screenshot from 2023-06-13 14-02-161308×223 31 KBharness stage tree, while running example
Screenshot from 2023-06-13 14-04-501862×643 146 KBI want to make the harness 3D model interact with the environment exactly as its own shape.However, There is a collision calculation error, and it seems that the collision calculation is not being performed based on the shape of the harness. As a result, certain parts of the harness keep penetrating into the desk.Even after reading the documentation related to USD, I’m not entirely sure where to look for guidance or which specific part of the .usd file needs to be modified. I’m unsure about what steps to take next.Hi there, I would recommend first debugging the harness asset individually in Isaac Sim to make sure the collision shapes and physics is behaving as expected. Then, once imported into the Factory environment, you can first try to start off with a smaller number of environments to see if the collisions are working, then extend to a larger number of environment. It may be possible that the reset conditions or default transforms for the new asset are different than the nut. You can also try increasing the values for the GPU buffers in the task config file to resolve the warning messages.This topic was automatically closed 14 days after the last reply. New replies are no longer allowed.Powered by Discourse, best viewed with JavaScript enabled"
961,prim-custom-data-array,"I put some custom data in the “Prim Custom Data” property in the Isaac Sim GUI and I can access it via the Python API. This works fine until I enter an array in the “Prim Custom Data” field. Then the field does not display any data anymore and I can not edit it anymore. Closing and reopening Isaac Sim did not help. Even stranger: I can still access the data with the Python API.To give an example:
This works:
{
“test1”: “test”,
“test2”: “test”
}This results in the problems in the GUI:
{
“test1”: “test”,
“test2”: [“test”,“test”,“test”]
}Am I doing something wrong, or is this a bug in the Isaac Sim GUI?Powered by Discourse, best viewed with JavaScript enabled"
962,getting-camera-position,"I am trying to get the position of prims on my stage using this script: Get the Local Space Transforms for a Prim — Omniverse Kit documentationIt works, but the problem is that i always get the position (0, 0, 0) although I created the camera at (0, 0, 1200).Am I missing something here or is there another way to get the position of the camera?Thanks, best regards,
Julian GrimmI think my issue is related to this behavior: Can't obtaing transform matrix applied by rep.modify.pose - #2 by dennis.lynch
However if I add og.Controller.evaluate_sync() i get the error “Controller.evaluated was never awaited”.
Maybe someone has a solution for this?
image1496×1050 71 KB
Hi @julian.grimm and thanks for reaching out! When you run the with rep.trigger.on_frame block, what you’re actually doing is preparing a graph - but that graph hasn’t run yet. That’s why when you retrieve the transform, you get a translation of (0,0,0).You can run the graph for one step as follows:Thank you for your answer. It worked for me, however I am encountering another issue when i run it ussing step_async(). When I have multiple camera views (therefore having multiple render_products) the rendering gets very slow and I get the warning “Timed out awaiting frame”.
image1486×275 17.9 KB
What I am trying to do is to render a part in multiple positions for training a classification network. Some specific positions belong to one class and other positions belong to another class, so the rendered images should be saved into seperate folders. Here is the script that I have been using. Maybe someone knows a more better way to achieve this.Hello @julian.grimm, while I haven’t seen this error before, I think I can help on a few things that perhaps will work to avoid the issue.To capture two different views and save to different directories, I recommend simply creating two writers. What you’re doing by calling run_replicator in a loop is essentially re-creating and re-running the generation from scratch for each output path. This may be needed if you are seeing yourself run out of available VRAM, but otherwise it will be more efficient to capture in a single run.The next point is more for performance. Rather than step_async and a for-loop, you can simply set your trigger with the number of activations you want and then tell Replicator to run until completion. Replicator will capture every frame until all triggers have reached their maximum number of activations. We’re planning a tutorial in the near future to detail what happens behind the scenes and why this is more efficient so that these details are less obscure.Please let me know if these tips are helpful. Thanks for reaching out!Hi @jlafleche , thank you for answering. I already got this addressed by removing one camera view, because it was not absolutely necessary. I used the for loop, because I need to save the images in two different folders, based on the position and the rotation of the objects.Regarding your other recommendation: In the meantime, I started working on a new project where I implemented your suggestion. I was able to reduce the render time by 0.5s per frame, when rendering 2000 images. I have yet to test it with the project mentioned above.Powered by Discourse, best viewed with JavaScript enabled"
963,how-can-i-edit-the-mesh-point-surface-line-as-in-maya,"WeChat0b555f79740885d166204e29c24b241d782×438 65.5 KB
I  watched the tutorial, there used to be an extension named “metrics assemble UI”, which can edit the mesh points, surface, line, but can not find it in the extension list right now.
In the menu as the picture I posed, can not edit the point.Powered by Discourse, best viewed with JavaScript enabled"
964,information-about-omniverse-for-blender,"Hi, I’ve recently installed  Blender 3.6 through the Omiverse Launcher. The problem that I’m experiencing now is Blender has no Omniverse Panel due to this error: “No module name _ar”. Full error stack below:Traceback (most recent call last):
File “D:\NVIDIA_CACHE\Lib\blender-3.6.0-usd.200.0\Release\3.6\scripts\modules\addon_utils.py”, line 333, in enable
mod = import(module_name)
File “C:\Users\gaida\AppData\Roaming\Blender Foundation\Blender\3.6\scripts\addons\omni_nucleus_init_.py”, line 24, in 
from .registration import (
File “C:\Users\gaida\AppData\Roaming\Blender Foundation\Blender\3.6\scripts\addons\omni_nucleus\registration.py”, line 43, in 
from .operators import (
File “C:\Users\gaida\AppData\Roaming\Blender Foundation\Blender\3.6\scripts\addons\omni_nucleus\operators.py”, line 13, in 
from pxr import Ar
File “D:\Work\Pipeline\Packages\usd\build\lib\python\pxr\Ar_init_.py”, line 24, in 
import _ar
ModuleNotFoundError: No module named ‘_ar’Any help around this is much appreciated! I would very much like to follow the tutorials 1 by 1Powered by Discourse, best viewed with JavaScript enabled"
965,make-ros1camerahelper-publish-headers-with-system-time-and-not-simulation-time,"Hello, the ROS1CameraHelper publishes messages with a header that contains the current time, the problem is this time is simulation time and not system time. How do I change it.Thanks!What ROS1CameraHelper are you referring to?The ROS1CameraHelper node in the action graph as shown in the image belowimage2008×1042 206 KBMoving this question to Isaac Sim.Hi @montanacaleb  - The timestamp in the header of the ROS messages is typically set to the current simulation time in many robotics simulation environments. This is because the simulation might not run in real-time, so using the system time could lead to inconsistencies between the simulation and the ROS messages.If you want to change the timestamp to the system time, you would need to modify the code of the ROS1CameraHelper class (or whichever class is responsible for publishing the ROS messages).Here’s a general idea of how you might do it:In this line, time.time() returns the current system time in seconds since the epoch (January 1, 1970), and rospy.Time.from_sec() converts this to a rospy.Time object.Please note that this is a general suggestion and the actual code might look different depending on how the ROS1CameraHelper class is implemented.Powered by Discourse, best viewed with JavaScript enabled"
966,omniverse-connector-filling-up-cui-in-3dsmax-2022,"Hi,I have uninstalled Omniverse Connector because it was making 3DSMAx 2022 very unresponsive.
More details here: Solved: Re: 3DSMax 2022 - Customise User Interface menu running very slow: - Autodesk CommunityI do not know if I was running the latest release of Launcher or Connector, I forgot to look.
But if anyone else is experiencing a slowing down of 3DSMaxs CUI editor window, this is the cause.Regards,I’m able to replicate this in 3ds max 2023 as well - I am logging this defect.
Thanks for sharing this with us.many duplicate menus in 3dsmax because of omniverse plugin
I have this problem too. I searched very much on the internet for this problem. Yes, exactly the omniverse is the culprit of this problem. Also, I Update the connector and it did not fix this problem. Idk why should it re-create menus in every restart of 3dsmax ?!!! My startup menu UI file was more than 10mb.
also I changed below script but it did not help. currently, the only way is uninstalling omniverse.
https://www.scriptspot.com/3ds-max/scripts/cleaning-menu-duplicatesLatest release ( v201.0 ) addresses this issue. The extension will try to cleanup duplicate menus, but the names were not always unique in an abundance of caution it may leave some behind so that we don’t inadvertently delete menus that may have the same name.Powered by Discourse, best viewed with JavaScript enabled"
