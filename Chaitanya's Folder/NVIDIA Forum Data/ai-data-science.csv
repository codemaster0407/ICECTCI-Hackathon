,query,data
0,tensorrt-error-3-topklayer-h-22-error-code-3-api-usage-error,"I’m trying to convert an onnx model that was exported from tensorflow 2.11to a TRT engine.
I followed this colabI then downloaded and used tf2onnx to create an .onnx file as followThen I attempt to convert the test.onnx model to TRT engine using the following commandand got the following print in terminalTensorRT Version:  8.5
GPU Type:  Orin AGX
Nvidia Driver Version:
CUDA Version:
CUDNN Version:
Operating System + Version:  Docker Image nvcr.io/nvidia/l4t-tensorrt:r8.5.2.2-devel
Python Version (if applicable):
TensorFlow Version (if applicable):
PyTorch Version (if applicable):
Baremetal or Container (if container which image + tag):Please attach or include links to any models, data, files, or scripts necessary to reproduce your issue. (Github repo, Google Drive, Dropbox, etc.)test.onnxI ran the following command in terminalPlease include:Hi,The below links might be useful for you.This Best Practices Guide covers various performance considerations related to deploying networks using TensorRT 8.0.3. These sections assume that you have a model that is working at an appropriate level of accuracy and that you are able to...The programming guide to the CUDA model and interface.https://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__STREAM.htmlFor multi-threading/streaming, will suggest you to use Deepstream or TRITONFor more details, we recommend you raise the query in Deepstream forum.orraise the query in Triton Inference Server Github instance issues section.Triton Inference Server has 29 repositories available. Follow their code on GitHub.Thanks!Powered by Discourse, best viewed with JavaScript enabled"
1,tensorrt-cuda-11-x-availability-for-sbsa-systems,"Hello, everyone!TensorRT 8.6 (EA and later GA) ARM SBSA section of the download page lists only CUDA 12.0 & Ubuntu 20.04.Should we expect availability of TensorRT builds for CUDA 11.8 later on?CuDNN and CUDA are both available in CUDA 11.8 variant for ARM SBSA.Hi,In case of TensorRT, we only support one CUDA major version for SBSA platform per release.Thank you.This topic was automatically closed 14 days after the last reply. New replies are no longer allowed.Powered by Discourse, best viewed with JavaScript enabled"
2,tesla-v100-cuda-support,"Hi,I’m trying to utilize Tensorflow with CUDA for accelerated training.
GPU Model: Tesla V100 FHHL 16GB
I have installed CUDA and cuDNN using the run file and the copying the necessary libraries.CUDA 10.1, cuDNN 7.6
I get this error whenever I try to run the tensorflow CNN model:
Attempting to fetch value instead of handling error Internal: failed to get device attribute 13 for device 0: CUDA_ERROR_UNKNOWN: unknown error.NVIDIA-SMI give the following output:| NVIDIA-SMI 450.51.05    Driver Version: 450.51.05    CUDA Version: 11.0nvcc-V the following output:
nvcc: NVIDIA (R) Cuda compiler driver
Copyright (c) 2005-2019 NVIDIA Corporation
Built on Fri_Feb__8_19:08:17_PST_2019
Cuda compilation tools, release 10.1, V10.1.105Pls let tme know if this is a CUDA version mismatch or any other things need to be taken care of.Hi @anandshubham1998 ,
Can you please validate the CUDA installation steps from here and let us know.ThanksSo Tesla V100 do they have limitations on the CUDA, cuDNN compatibility? Can I install the latest CUDA and cuDNN test configurations for tensorflow?cuDNN8.1	CUDA11.2Powered by Discourse, best viewed with JavaScript enabled"
3,cudnn-8-samples-crash-with-illegal-instruction-error,"I just did a fresh install of CUDA-11.0 (Driver 450.51.06) together with cuDNN 8.0.5 on a machine with Ubuntu 18.04 LTS and a Titan RTX. Head of nvidia-smi:Running the CUDA samples works fine, but the cuDNN samples crash with an illegal instruction:Apart from the illegal instruction error, I’m confused that the times for cudnnGetConvolutionForwardAlgorithm_v7 are negative and fixed at -1.0.Does someone have a clue what is going on or knows how to debug this? If you need additional information about the system or output from another program, please let me know.Hi @meier.philip,
cudnnGet does not exercise any GPU code so there’s no measurement of each algo.
The illegal instruction seem to be related to CPU.Thanks!The illegal instruction seem to be related to CPU.True, but why? I built the cuDNN sample on the same machine I was running it. Doesn’t that mean that the build used an instruction that is not available on my CPU?Yes this might be related to compiler.  The compiler generates an instruction that is not available on your CPU.Hi @meier.philip did you solve your problem? I have exactly the same issue. I was wondering whether that was because of AVX instructions not available on my machine. I thought cudnn 8.05 did not require AVX. Any help would be greatly appreciated.Powered by Discourse, best viewed with JavaScript enabled"
4,can-i-use-maxine-sdk-when-i-use-linux-as-client-with-geforce-rtx-30xx,"Dear Maxine TeamI have a question.As according to Supported Hardware was posted on  “https://developer.nvidia.com/maxine-getting-started”, It says need a GPU for server such as V100, T4.Can I use maxine sdk when I use linux as client with GeForce RTX 30XX?regards,
Hoyeon.Yes. I’ve got the AigsEffectsApp running on Ubuntu 1804, with an GeForce RTX 3050 Ti Laptop GPU.NolanHello Nolan,did you do anything special to get it running on Ubuntu with your RTX card?Thank you!Hi Zeol!Are you having issues getting the SDKs running?Me too, I want to test on my ubuntu 18.04 system, my GPU is T4, but I have been applying for months from this link Project Maxine | Developer NVIDIA and it has not been approved.Hello zeol,Sorry, way late here. Yes, it’s now running on Ubuntu 20.04 with that GPU.NolanPowered by Discourse, best viewed with JavaScript enabled"
5,different-fp16-inference-behavior-on-four-gpus,"I have an onnx model exported from torchvision Faster-RCNN. When deploying the model using TensorRT-8.2’s trtexec, I obtained correct results under FP32, tested on four GPUs (namely, JetsonTX2-NX, RTX2080, RTX3060, RTX3080Ti). However, when switched to FP16 mode (i.e., --fp16), the results on JetsonTX2-NX and RTX2080 degrade equally and significantly from FP32, while RTX3060 and RTX3080Ti retain their accuracy.I installed and built trtexec on each device based on the same method. The CUDA versions on these devices are slightly different (e.g., CUDA10.x on JetsonTX2 and CUDA11.x on the other three devices). Since their FP32 results are all correct, I believe the slight differences in their CUDA, CUDNN, and OS versions did not cause the degradation under FP16.Instead, I am more inclined to belive that the issue is associated with the supported data type / precision on each device? I wonder if you have any explanation on this issue and perhaps some insights on how to address it?Many thanks in advance!TensorRT Version: 8.2.1.8
GPU Type: JetsonTX2-NX, RTX2080, RTX3060, RTX3080Ti
Nvidia Driver Version:
CUDA Version:  10 and 11
CUDNN Version: 8.6
Operating System + Version:  Ubuntu 18.04 / 20.04More than happy to provide my model via private messages. Also happy to provide detailed logs if needed (all four devices display different trtexec logs, which makes it quite difficult and potentially confusing to put here).Hi,Could you please let us know how big the divergence is between the devices?
Please share with us verbose logs and an issue repro.
Three of those devices are different generations with different hardware accelerations for FP16. That alone can cause some divergence, and even with a single device, building the same network twice can result in slightly different numerical accuracy.Thank you.Thank you for your response!It is perhaps more straightforward if I just focus on the problematic device here (i.e., Jetson TX2NX). The divergence between its FP32 and FP16 is highly significant (in my case the model is a detector, and the bounding boxes of FP16 are very poor).Through DM, I have shared with you my onnx model, TensorRT OSS source files (i.e., to build new libnvinfer_plugin.so, onnxparser, or even trtexec if needed), and a sampled image (.dat and .jpg). The OSS source files are needed as I had to include a custom plugin (I am using TensorRT-8.2.1.8 which doesn’t natively support RoiAlign in my model; the RoiAlign plugin was taken from a later version of TensorRT-OSS).Steps to reproduce:Build new shared libraries (.so) from the provided TensorRT-OSS files (following standard build steps).Update LD_LIBRARY_PATH to accommodate newly built .so files.Generate trt engine from the provided onnx model using the following commands:trtexec --onnx=model.onnx --loadInputs=“input0:test_image.dat” --dumpOutputtrtexec --onnx=model.onnx --loadInputs=“input0:test_image.dat” --dumpOutput --fp16The target device was a Jetson TX2NX. Outputs of the above two commands mismatch drastically (the first one corresponds to FP32 and is correct). I also provide you the verbose logs of the above two commands.Please help look into this matter and don’t hesitate to let me know if you seek more details. Thank you in advance!Hi,Sorry for the delayed response.
We could reproduce the same behavior.
Please allow us some time to work on this issue.Thank you.Powered by Discourse, best viewed with JavaScript enabled"
6,different-between-context-enqueue-enqueuev2-enqueuev3,"A clear and concise description of the bug or issue.TensorRT Version: 8
GPU Type: 2080Ti
Nvidia Driver Version: 470
CUDA Version: 11.4
CUDNN Version:
Operating System + Version:
Python Version (if applicable):
TensorFlow Version (if applicable):
PyTorch Version (if applicable):
Baremetal or Container (if container which image + tag):Can anyone explain for me: different between  context->enqueue,  enqueueV2,  enqueueV3 when use tensorrt inference. Which solution should I use?It appears all others except v3 are deprecated in the latest version TensorRT: nvinfer1::IExecutionContext Class Reference, but I don’t have any insight into why it was changed. I suppose the v3 API must be preferable in some way.enqueue, enqueueV2, enqueueV3Hi @vuminhduc9755 ,
enqueue: oldest api, support implicit batch, is deprecated.
enqueueV2: replacement of enqueue, support explict batch. is deprecated now.
enqueueV3: latest api, support data dependent shape, recommend to use now.Please check TensorRT: nvinfer1::IExecutionContext Class Reference for details.ThanksPowered by Discourse, best viewed with JavaScript enabled"
7,how-to-convert-yolov4-feature-vectors-to-bounding-boxes,"I implement object detection code for Yolov4 using tensorrt c++ pipeline. Although tensorrt c++ api is not user friendly at all, I managed to preprocess image and inference the model. I, however, have 3 outputs which are in shape of 18x52x52, 18x26x26 and 18x13x13. How can I convert these outputs to bounding boxes?For example my outputs for a person picture go like this: 0.103638 -0.646484 -0.632324 -0.922852 -0.981445 -0.602539 ... which has a size of 18x52x52 = 48672My guess is that I have to add layers at the end of the network to convert these vectors such as yolo plugins layers and some calculations layers but I couldn’t come up with an example.TensorRT Version: 8.6.1
GPU Type: Geforce 1650Ti
Nvidia Driver Version: 525.105.17
CUDA Version: 12.0
CUDNN Version:
Operating System + Version: Ubuntu 20.04
Python Version (if applicable): 3.10.9
TensorFlow Version (if applicable):
PyTorch Version (if applicable):
Baremetal or Container (if container which image + tag):Hi,Please refer to the following sample, which may help you:main/samples/python/yolov3_onnxNVIDIA® TensorRT™, an SDK for high-performance deep learning inference, includes a deep learning inference optimizer and runtime that delivers low latency and high throughput for inference applicat...Thank you.Powered by Discourse, best viewed with JavaScript enabled"
8,tensorrt-inference-is-taking-1-5-sec-to-inference-a-single-frame-i-want-to-speed-up-my-inference-how-can-i-do-that,"here is my tensorrt inference script.
with this script, inferencing one frame it is taking 1.5-sec which means 0.5fps. I want it t have a better fps.
I’m sharing my script below.particularly,context.execute(batch_size=1, bindings=[int(d_input_1), int(d_output)])
this line is taking 1.4sec to runimport tensorrt as trt
import pycuda.driver as cuda
import pycuda.autoinit
import numpy as npdef allocate_buffers(engine, batch_size, data_type):“”""
This is the function to allocate buffers for input and output in the device
Args:
engine : The path to the TensorRT engine.
batch_size : The batch size for execution time.
data_type: The type of the data for input and output, for example trt.float32.Output:
h_input_1: Input in the host.
d_input_1: Input in the device.
h_output_1: Output in the host.
d_output_1: Output in the device.
stream: CUDA stream.“”""h_input_1 = cuda.pagelocked_empty(batch_size * trt.volume(engine.get_binding_shape(0)), dtype=trt.nptype(data_type))
h_output = cuda.pagelocked_empty(batch_size * trt.volume(engine.get_binding_shape(1)), dtype=trt.nptype(data_type))d_input_1 = cuda.mem_alloc(h_input_1.nbytes)d_output = cuda.mem_alloc(h_output.nbytes)stream = cuda.Stream()
return h_input_1, d_input_1, h_output, d_output, streamdef load_images_to_buffer(pics, pagelocked_buffer):preprocessed = np.asarray(pics).ravel()
np.copyto(pagelocked_buffer, preprocessed)def do_inference(engine, pics_1, h_input_1, d_input_1, h_output, d_output, stream, batch_size, height, width):“”""
This is the function to run the inference
Args:
engine : Path to the TensorRT engine.
pics_1 : Input images to the model.
h_input_1: Input in the host.
d_input_1: Input in the device.
h_output_1: Output in the host.
d_output_1: Output in the device.
stream: CUDA stream.
batch_size : Batch size for execution time.
height: Height of the output image.
width: Width of the output image.Output:
The list of output images.“”""load_images_to_buffer(pics_1, h_input_1)with engine.create_execution_context() as context:cuda.memcpy_htod_async(d_input_1, h_input_1, stream)Hi,
Can you try running your model with trtexec command, and share the “”–verbose"""" log in case if the issue persistmaster/samples/trtexecNVIDIA® TensorRT™, an SDK for high-performance deep learning inference, includes a deep learning inference optimizer and runtime that delivers low latency and high throughput for inference applicat...You can refer below link for all the supported operators list, in case any operator is not supported you need to create a custom plugin to support that operationAlso, request you to share your model and script if not shared already so that we can help you better.Meanwhile, for some common errors and queries please refer to below link:This is the revision history of the NVIDIA TensorRT 8.5 Developer Guide.This is the revision history of the NVIDIA TensorRT 8.5 Developer Guide.Thanks!Here is my trtexec output,please check&&&& RUNNING TensorRT.trtexec [TensorRT v8201] # trtexec --loadEngine=unethalf_engine.engine --shapes=modelInput:137201280
[03/13/2023-18:55:38] [I] === Model Options ===
[03/13/2023-18:55:38] [I] Format: *
[03/13/2023-18:55:38] [I] Model:
[03/13/2023-18:55:38] [I] Output:
[03/13/2023-18:55:38] [I] === Build Options ===
[03/13/2023-18:55:38] [I] Max batch: explicit batch
[03/13/2023-18:55:38] [I] Workspace: 16 MiB
[03/13/2023-18:55:38] [I] minTiming: 1
[03/13/2023-18:55:38] [I] avgTiming: 8
[03/13/2023-18:55:38] [I] Precision: FP32
[03/13/2023-18:55:38] [I] Calibration:
[03/13/2023-18:55:38] [I] Refit: Disabled
[03/13/2023-18:55:38] [I] Sparsity: Disabled
[03/13/2023-18:55:38] [I] Safe mode: Disabled
[03/13/2023-18:55:38] [I] DirectIO mode: Disabled
[03/13/2023-18:55:38] [I] Restricted mode: Disabled
[03/13/2023-18:55:38] [I] Save engine:
[03/13/2023-18:55:38] [I] Load engine: unethalf_engine.engine
[03/13/2023-18:55:38] [I] Profiling verbosity: 0
[03/13/2023-18:55:38] [I] Tactic sources: Using default tactic sources
[03/13/2023-18:55:38] [I] timingCacheMode: local
[03/13/2023-18:55:38] [I] timingCacheFile:
[03/13/2023-18:55:38] [I] Input(s)s format: fp32:CHW
[03/13/2023-18:55:38] [I] Output(s)s format: fp32:CHW
[03/13/2023-18:55:38] [I] Input build shape: modelInput=1+1+1
[03/13/2023-18:55:38] [I] Input calibration shapes: model
[03/13/2023-18:55:38] [I] === System Options ===
[03/13/2023-18:55:38] [I] Device: 0
[03/13/2023-18:55:38] [I] DLACore:
[03/13/2023-18:55:38] [I] Plugins:
[03/13/2023-18:55:38] [I] === Inference Options ===
[03/13/2023-18:55:38] [I] Batch: Explicit
[03/13/2023-18:55:38] [I] Input inference shape: modelInput=1
[03/13/2023-18:55:38] [I] Iterations: 10
[03/13/2023-18:55:38] [I] Duration: 3s (+ 200ms warm up)
[03/13/2023-18:55:38] [I] Sleep time: 0ms
[03/13/2023-18:55:38] [I] Idle time: 0ms
[03/13/2023-18:55:38] [I] Streams: 1
[03/13/2023-18:55:38] [I] ExposeDMA: Disabled
[03/13/2023-18:55:38] [I] Data transfers: Enabled
[03/13/2023-18:55:38] [I] Spin-wait: Disabled
[03/13/2023-18:55:38] [I] Multithreading: Disabled
[03/13/2023-18:55:38] [I] CUDA Graph: Disabled
[03/13/2023-18:55:38] [I] Separate profiling: Disabled
[03/13/2023-18:55:38] [I] Time Deserialize: Disabled
[03/13/2023-18:55:38] [I] Time Refit: Disabled
[03/13/2023-18:55:38] [I] Skip inference: Disabled
[03/13/2023-18:55:38] [I] Inputs:
[03/13/2023-18:55:38] [I] === Reporting Options ===
[03/13/2023-18:55:38] [I] Verbose: Disabled
[03/13/2023-18:55:38] [I] Averages: 10 inferences
[03/13/2023-18:55:38] [I] Percentile: 99
[03/13/2023-18:55:38] [I] Dump refittable layers:Disabled
[03/13/2023-18:55:38] [I] Dump output: Disabled
[03/13/2023-18:55:38] [I] Profile: Disabled
[03/13/2023-18:55:38] [I] Export timing to JSON file:
[03/13/2023-18:55:38] [I] Export output to JSON file:
[03/13/2023-18:55:38] [I] Export profile to JSON file:
[03/13/2023-18:55:38] [I]
[03/13/2023-18:55:38] [I] === Device Information ===
[03/13/2023-18:55:38] [I] Selected Device: NVIDIA Tegra X1
[03/13/2023-18:55:38] [I] Compute Capability: 5.3
[03/13/2023-18:55:38] [I] SMs: 1
[03/13/2023-18:55:38] [I] Compute Clock Rate: 0.9216 GHz
[03/13/2023-18:55:38] [I] Device Global Memory: 3964 MiB
[03/13/2023-18:55:38] [I] Shared Memory per SM: 64 KiB
[03/13/2023-18:55:38] [I] Memory Bus Width: 64 bits (ECC disabled)
[03/13/2023-18:55:38] [I] Memory Clock Rate: 0.01275 GHz
[03/13/2023-18:55:38] [I]
[03/13/2023-18:55:38] [I] TensorRT version: 8.2.1
[03/13/2023-18:55:40] [I] [TRT] [MemUsageChange] Init CUDA: CPU +229, GPU +0, now: CPU 289, GPU 2525 (MiB)
[03/13/2023-18:55:40] [I] [TRT] Loaded engine size: 41 MiB
[03/13/2023-18:55:41] [I] [TRT] [MemUsageChange] Init cuBLAS/cuBLASLt: CPU +158, GPU +160, now: CPU 448, GPU 2686 (MiB)
[03/13/2023-18:55:42] [I] [TRT] [MemUsageChange] Init cuDNN: CPU +240, GPU +243, now: CPU 688, GPU 2929 (MiB)
[03/13/2023-18:55:42] [I] [TRT] [MemUsageChange] TensorRT-managed allocation in engine deserialization: CPU +0, GPU +41, now: CPU 0, GPU 41 (MiB)
[03/13/2023-18:55:42] [I] Engine loaded in 4.66617 sec.
[03/13/2023-18:55:42] [I] [TRT] [MemUsageChange] Init cuBLAS/cuBLASLt: CPU +0, GPU +0, now: CPU 647, GPU 2887 (MiB)
[03/13/2023-18:55:42] [I] [TRT] [MemUsageChange] Init cuDNN: CPU +0, GPU +0, now: CPU 647, GPU 2887 (MiB)
[03/13/2023-18:55:42] [I] [TRT] [MemUsageChange] TensorRT-managed allocation in IExecutionContext creation: CPU +0, GPU +192, now: CPU 0, GPU 233 (MiB)
[03/13/2023-18:55:42] [I] Using random values for input modelInput
[03/13/2023-18:55:43] [I] Created input binding for modelInput with dimensions 1x3x720x1280
[03/13/2023-18:55:43] [I] Using random values for output modelOutput
[03/13/2023-18:55:43] [I] Created output binding for modelOutput with dimensions 1x1x720x1280
[03/13/2023-18:55:43] [I] Starting inference
[03/13/2023-18:56:00] [I] Warmup completed 1 queries over 200 ms
[03/13/2023-18:56:00] [I] Timing trace has 10 queries over 14.5163 s
[03/13/2023-18:56:00] [I]
[03/13/2023-18:56:00] [I] === Trace details ===
[03/13/2023-18:56:00] [I] Trace averages of 10 runs:
[03/13/2023-18:56:00] [I] Average on 10 runs - GPU latency: 1450.18 ms - Host latency: 1451.62 ms (end to end 1451.63 ms, enqueue 13.3297 ms)
[03/13/2023-18:56:00] [I]
[03/13/2023-18:56:00] [I] === Performance summary ===
[03/13/2023-18:56:00] [I] Throughput: 0.68888 qps
[03/13/2023-18:56:00] [I] Latency: min = 1445.93 ms, max = 1456.88 ms, mean = 1451.62 ms, median = 1451.2 ms, percentile(99%) = 1456.88 ms
[03/13/2023-18:56:00] [I] End-to-End Host Latency: min = 1445.94 ms, max = 1456.9 ms, mean = 1451.63 ms, median = 1451.22 ms, percentile(99%) = 1456.9 ms
[03/13/2023-18:56:00] [I] Enqueue Time: min = 2.87402 ms, max = 17.4658 ms, mean = 13.3297 ms, median = 13.9668 ms, percentile(99%) = 17.4658 ms
[03/13/2023-18:56:00] [I] H2D Latency: min = 1.07715 ms, max = 1.08496 ms, mean = 1.08086 ms, median = 1.08057 ms, percentile(99%) = 1.08496 ms
[03/13/2023-18:56:00] [I] GPU Compute Time: min = 1444.49 ms, max = 1455.45 ms, mean = 1450.18 ms, median = 1449.76 ms, percentile(99%) = 1455.45 ms
[03/13/2023-18:56:00] [I] D2H Latency: min = 0.355469 ms, max = 0.358398 ms, mean = 0.357129 ms, median = 0.357422 ms, percentile(99%) = 0.358398 ms
[03/13/2023-18:56:00] [I] Total Host Walltime: 14.5163 s
[03/13/2023-18:56:00] [I] Total GPU Compute Time: 14.5018 s
[03/13/2023-18:56:00] [I] Explanations of the performance metrics are printed in the verbose logs.
[03/13/2023-18:56:00] [I]
&&&& PASSED TensorRT.trtexec [TensorRT v8201] # trtexec --loadEngine=unethalf_engine.engine --shapes=modelInput:137201280Hello @kalyanichagala11 ,
Have you tried model quantization? You can check it here. And more info about how it works here. You can do fp16 and int8, however support for both as well as the final speed up depends heavily on the hardware. Also when doing quatization try to benchmark the model accuracy before and after to see if there are noticiable accuracy losses.Regards,
Andres
Embedded SW Engineer at RidgeRun
Contact us: support@ridgerun.com
Developers wiki: https://developer.ridgerun.com/
Website: www.ridgerun.comPowered by Discourse, best viewed with JavaScript enabled"
9,cannot-use-cudnn-past-8-4-1,"Working with the Darknet/YOLO framework.  If using CUDNN v8.4.1 or older, everything is fine.  If we use CUDNN v8.5.0 or newer, Darknet crashes during training while calculating the mAP%.I’m looking through someone else’s code trying to understand.  At the stage where it fails, it logs this:Anyone familiar with the changes that happened in CUDNN between 8.4.1 and 8.5.0 that might shed some light on this?The file in question, line 735, is a call to cudaStreamSynchronize():  darknet/network_kernels.cu at master · AlexeyAB/darknet · GitHub(CUDNN error at training iteration 1000 when calculating mAP% · Issue #8669 · AlexeyAB/darknet · GitHub)One of the things I’m seeing in the 8.5.0 release notes is this text:A buffer was shared between threads and caused segmentation faults. There was previously no way to have a per-thread buffer to avoid these segmentation faults. The buffer has been moved to the cuDNN handle. Ensure you have a cuDNN handle for each thread because the buffer in the cuDNN handle is only for the use of one thread and cannot be shared between two threads.This looks promising.  If the different threads didn’t have unique cuDNN handles and previously shared one, would this cause a the error we’re seeing, “an illegal memory access was encountered”?What API do I need to search for that would show where threads obtain a cuDNN handle?Hi @stephanecharette ,
Apologies for delayed response.
Can you please try running your workload with compute-sanitizer to get the kernel causing the IMA.
If it’s a cuDNN kernel, plesae create a cuDNN API log: Developer Guide - NVIDIA Docs and share it with us.ThanksPowered by Discourse, best viewed with JavaScript enabled"
10,error-smclk-0-when-i-using-tensorrt8-6-with-cuda12-on-rtx4070,"An Error when use trtexec on RTX4070TensorRT Version: TensorRT-8.6.1.6.Windows10.x86_64.cuda-12.0
GPU Type: RTX4070
Nvidia Driver Version: 535
CUDA Version: 12.0.1_528
CUDNN Version: 8.9.2.26
Operating System + Version: Win10
Python Version (if applicable):
TensorFlow Version (if applicable):
PyTorch Version (if applicable):
Baremetal or Container (if container which image + tag): BaremetalUse this to Export yolov8s.onnx
yolov8s.onnx (42.8 MB)Export yolov8s.onnx from GitHub - ultralytics/ultralytics: NEW - YOLOv8 🚀 in PyTorch > ONNX > CoreML > TFLite by cmd  yolo detect export model=yolov8s.pt format=onnxthen copy to path of trtexec.exe and run cmd .\trtexec.exe onnx=yolov8s.onnx,I get an error Error[1]: Unexpected exception KTM assertion failure: C:\_src\externals\ktm\src\timingModel.cpp:382  smClk > 0here is my log
error.log (687.2 KB)Hi,
Request you to share the ONNX model and the script if not shared already so that we can assist you better.
Alongside you can try few things:This NVIDIA TensorRT 8.6.1 Quick Start Guide is a starting point for developers who want to try out TensorRT SDK; specifically, this document demonstrates how to quickly construct an application to run inference on a TensorRT engine.import sys
import onnx
filename = yourONNXmodel
model = onnx.load(filename)
onnx.checker.check_model(model).
2) Try running your model with trtexec command.master/samples/trtexecNVIDIA® TensorRT™, an SDK for high-performance deep learning inference, includes a deep learning inference optimizer and runtime that delivers low latency and high throughput for inference applicat...
In case you are still facing issue, request you to share the trtexec “”–verbose"""" log for further debugging
Thanks!yolov8s.onnx (42.8 MB)
Here is the onnx fileerror.log (687.2 KB)Hi,Based on the following logs, looks like CUDA report the device clock incorrectly.[07/05/2023-09:50:21] [I] === Device Information ===
[07/05/2023-09:50:21] [I] Selected Device: NVIDIA GeForce RTX 4070
[07/05/2023-09:50:21] [I] Compute Capability: 8.9
[07/05/2023-09:50:21] [I] SMs: 46
[07/05/2023-09:50:21] [I] Device Global Memory: 12281 MiB
[07/05/2023-09:50:21] [I] Shared Memory per SM: 100 KiB
[07/05/2023-09:50:21] [I] Memory Bus Width: 192 bits (ECC disabled)
[07/05/2023-09:50:21] [I] Application Compute Clock Rate: 0 GHz
[07/05/2023-09:50:21] [I] Application Memory Clock Rate: 0 GHz
[07/05/2023-09:50:21] [I]
[07/05/2023-09:50:21] [I] Note: The application clock rates do not reflect the actual clock rates that the GPU is currently running at.Could you please run the following CUDA sample and confirm if it works fine.The installation instructions for the CUDA Toolkit on Linux.//github.com/NVIDIA/cuda-samples/tree/master/Samples/1_Utilities/deviceQueryThank you.Powered by Discourse, best viewed with JavaScript enabled"
11,trying-to-deploy-gptj-with-no-success,"Hi
I am trying to deploy GPT- J model on the triton server but I am having no luck.
I have followed this blog post https://developer.nvidia.com/blog/deploying-gpt-j-and-t5-with-fastertransformer-and-triton-inference-server/
but this wget mystic.the-eye.eu/public/AI/GPT-J-6B/step_383500_slim.tar.zstd files doesn’t seem to exist if anyone who has set a triton server successfully can share any documentation that would be great.ThanksPowered by Discourse, best viewed with JavaScript enabled"
12,practical-aspects-about-neural-networks-quantization-with-tensorrt,"I am currently exploring the topic of deep learning model quantization techniques. In the official NVIDIA’s Tensor RT documentation, we can see that Tensor RT supports quantization and applies it to activation and weights of the provided model.I am working with a repository that proposes explicit (with the pytorch_quantization library) and implicit quantization (implemented by Builder object from Python API).I would like to better understand the details described in the Explicit Versus Implicit Quantization section.When performing explicit quantization, I provide an ONNX graph with Q/DQ nodes to be converted to a ‘.trt’ file. Can I assume that all fake quantization nodes (as seen by Netron, for example) will be replaced by real INT8 operations of the associated layers?Moreover, the following statement from the above documentation doesn’t provide too much detail about which operations are quantized or not in implicit quantization:“When processing implicitly quantized networks, TensorRT treats the model as a floating-point model when applying the graph optimizations, and uses INT8 opportunistically to optimize layer execution time. If a layer runs faster in INT8, then it executes in INT8. Otherwise, FP32 or FP16 is used. In this mode, TensorRT is optimizing for performance only, and you have little control over where INT8 is used - even if you explicitly set the precision of a layer at the API level, TensorRT may fuse that layer with another during graph optimization, and lose the information that it must execute in INT8. TensorRT’s PTQ capability generates an implicitly quantized network.”What strategies can I use for profiling an inference run from a serialized network generated by TensorRT?I have already tried Nsight Systems and trtexec with profiling options, but I only get timing information with these two tools. Would you have another profiling approach that could allow me to verify the operation precision in every model layer during an evaluation process?Hi,
Can you try running your model with trtexec command, and share the “”–verbose"""" log in case if the issue persistmaster/samples/trtexecNVIDIA® TensorRT™, an SDK for high-performance deep learning inference, includes a deep learning inference optimizer and runtime that delivers low latency and high throughput for inference applicat...You can refer below link for all the supported operators list, in case any operator is not supported you need to create a custom plugin to support that operationAlso, request you to share your model and script if not shared already so that we can help you better.Meanwhile, for some common errors and queries please refer to below link:This is the revision history of the NVIDIA TensorRT 8.6 Developer Guide.This is the revision history of the NVIDIA TensorRT 8.6 Developer Guide.Thanks!Powered by Discourse, best viewed with JavaScript enabled"
13,approval-pending,"Dear JAX developers, thanks for this container. I was wondering if I could be granted access to it (my request is pending). Thanks!Access granted! We’d love to hear about your use case and what JAX libraries you’re using.Powered by Discourse, best viewed with JavaScript enabled"
14,exclude-tacticsources-from-ibuilderconfig,"Hello!
Since my TensorRT code running on Nano based on: https://github.com/noahmr/yolov5-tensorrt is consuming cca 1 GB RAM I’d like to decrease memory usage by - according your advice - excluding CUDNN from “tactic sources”.
Problem is: I can’t find how exactly to perform this exclusion (neither in TRT doc nor forum)?I tried it inside /src/yolov5_builder.cpp (which is now part of my app) like this, but doesn’t seem to exclude:Beside CUDNN I’d like to attempt exclusion of CuBLAS/CuBLASLt, so question applies to that as well
Many thanks!TensorRT Version : 8.0.1
GPU Type : Jetson Nano (Maxwell)
CUDA Version : 10.2
CUDNN Version : 8.2.1
Operating System + Version : Ubuntu 18.04 (Jetpack 4.6)Hi,This looks like a Jetson issue. Please refer to the below samples in case useful.Hello AI World guide to deploying deep-learning inference networks and deep vision primitives with TensorRT and NVIDIA Jetson. - GitHub - dusty-nv/jetson-inference: Hello AI World guide to deployin...An easy to use PyTorch to TensorRT converter. Contribute to NVIDIA-AI-IOT/torch2trt development by creating an account on GitHub.For any further assistance, we will move this post to to Jetson related forum.Thanks!Sir, not sure if we understood well: inference is implemented and works OK I just want to decrease RAM usage by excluding tactic sources-an approach you suggested in my previous thread. I think I just need (correct) argument(s) for config->setTacticSources() call… And it is C++any opinions on this?Hi @JoleCRO ,
Quoting from TRT developer guide.TensorRT’s dependencies (NVIDIA cuDNN and NVIDIA cuBLAS) can occupy large amounts of device memory. TensorRT allows you to control whether these libraries are used for inference by using the TacticSources (C++, Python) attribute in the builder configuration. Note that some plugin implementations require these libraries, so that when they are excluded, the network may not be compiled successfully.Each of the tactic source is explained here along with their default statusPowered by Discourse, best viewed with JavaScript enabled"
15,calibration-file-problem-between-different-versions-of-tensorrt,"I have a calibration file generated on Tensorrt 8.5, int8 model with this file gives good accuracy. When I reuse this file to generate int8 model on Tensorrt 8.2, performance of model is reduced much. What is happen?Note: I calibrate int8 model on Tenssort8.2 but acc is not good. This is the reason why I reuse calibration file from Tensorrt8.5. My device is Jetson Xavier.Hi, Please refer to the below links to perform inference in INT8master/samples/sampleINT8NVIDIA® TensorRT™, an SDK for high-performance deep learning inference, includes a deep learning inference optimizer and runtime that delivers low latency and high throughput for inference applicat...1777.21 KB
Thanks!I have done calibration and inference. Are you bot?When I reuse this file to generate int8 model on Tensorrt 8.2Hi,The calibration cache data is portable across different devices as long as the calibration happens before layer fusion. Specifically, the calibration cache is portable when using the IInt8EntropyCalibrator2 or IInt8MinMaxCalibrator calibrators, or when QuantizationFlag::kCALIBRATE_BEFORE_FUSION is set. This can simplify the workflow, for example by building the calibration table on a machine with a discrete GPU and then reusing it on an embedded platform. Fusions are not guaranteed to be the same across platforms or devices, so calibrating after layer fusion may not result in a portable calibration cache. The calibration cache is in general, not portable across TensorRT releases.Please refer to the developer guide for more information.This is the revision history of the NVIDIA TensorRT 8.6 Developer Guide.Thank you.The calibration cache is in general, not portable across TensorRT releases.Thank you so much.This topic was automatically closed 14 days after the last reply. New replies are no longer allowed.Powered by Discourse, best viewed with JavaScript enabled"
16,give-me-a-serialized-model-how-to-determine-it-is-fp32-or-fp16,"give me a serialized model , how to determine it is  fp32 or fp16?   and i donnot know parameters in serializingTensorRT Version: 8.2.4
GPU Type: 1660ti
Nvidia Driver Version: 530
CUDA Version: 11.6
CUDNN Version: 8.2.1
Operating System + Version: win10
Python Version (if applicable): 3.9.7
TensorFlow Version (if applicable):
PyTorch Version (if applicable): 1.9.0
Baremetal or Container (if container which image + tag):Please attach or include links to any models, data, files, or scripts necessary to reproduce your issue. (Github repo, Google Drive, Dropbox, etc.)Please include:Hi,
Request you to share the ONNX model and the script if not shared already so that we can assist you better.
Alongside you can try few things:This NVIDIA TensorRT 8.6.0 Early Access (EA) Quick Start Guide is a starting point for developers who want to try out TensorRT SDK; specifically, this document demonstrates how to quickly construct an application to run inference on a TensorRT...import sys
import onnx
filename = yourONNXmodel
model = onnx.load(filename)
onnx.checker.check_model(model).
2) Try running your model with trtexec command.master/samples/trtexecNVIDIA® TensorRT™, an SDK for high-performance deep learning inference, includes a deep learning inference optimizer and runtime that delivers low latency and high throughput for inference applicat...
In case you are still facing issue, request you to share the trtexec “”–verbose"""" log for further debugging
Thanks!my serialized model is serialized with c++ project, can it is validated in python script?i run it ‘–verbose’,but cannot find  information about it

企业微信截图_16825024547520890×393 6.75 KB


企业微信截图_16825024944979952×471 11.6 KB


企业微信截图_16825025322335963×237 6.45 KB
Hi @shaoyan_shi93 ,
I am afraid that the data type is logged in only during the build phase.ThanksPowered by Discourse, best viewed with JavaScript enabled"
17,approcal-pending,"Hi, I was wondering if my request could be approved soon? It is currently pending. Thank you!Access granted! We’d love to hear about your use case and what JAX libraries you’re using.Powered by Discourse, best viewed with JavaScript enabled"
18,tensorrt-installation-with-cudnn-8-5-0,"I would like to install TensorRT on my system, but cannot find a version of TensorRT with cuDNN 8.5 inside its support matrix. Is there any backwards compatibility across cuDNN versions, or am I out of luck with my system configuration?Hi @acardaras ,
You can use the TRT container as mentioned here which matches to set of your requirement.This topic was automatically closed 14 days after the last reply. New replies are no longer allowed.Powered by Discourse, best viewed with JavaScript enabled"
19,yolov4-tensorrt-loading-iplugincreator-not-found-error,"I mad a yolov4 tensorrt engine file using cv notebook samples of tao toolkit in python virtual envirionment, and the model was validated by using tao inference. It worked fine.But When I tried to load in tensorRT. I got the error like this.<_io.BufferedReader name=‘./trt.engine’>
[03/31/2023-05:23:57] [TRT] [E] 1: [pluginV2Runner.cpp::load::300] Error Code 1: Serialization (Serialization assertion creator failed.Cannot deserialize plugin since corresponding IPluginCreator not found in Plugin Registry)
[03/31/2023-05:23:57] [TRT] [E] 4: [runtime.cpp::deserializeCudaEngine::66] Error Code 4: Internal Error (Engine deserialization failed.)What is the problem?TensorRT Version: 8.5.1
GPU Type: 3090 TI
Nvidia Driver Version: 530.30.02
CUDA Version: 12.1
CUDNN Version: 8.8.1
Operating System + Version: Ubuntu 22.04
Python Version (if applicable): 3.10
TensorFlow Version (if applicable):
PyTorch Version (if applicable):
Baremetal or Container (if container which image + tag):Please attach or include links to any models, data, files, or scripts necessary to reproduce your issue. (Github repo, Google Drive, Dropbox, etc.)Please include:Hi,
Please refer to below links related custom plugin implementation and sample:This Samples Support Guide provides an overview of all the supported NVIDIA TensorRT 8.6.0 Early Access (EA) samples included on GitHub and in the product package. The TensorRT samples specifically help in areas such as recommenders, machine...TensorRT is an SDK for high performance, deep learning inference. It includes a deep learning inference optimizer and a runtime that delivers low latency and high throughput for deep learning…While IPluginV2 and IPluginV2Ext interfaces are still supported for backward compatibility with TensorRT 5.1 and 6.0.x respectively, however, we recommend that you write new plugins or refactor existing ones to target the IPluginV2DynamicExt or IPluginV2IOExt interfaces instead.Thanks!Thanks for the quick response. I will take a look at the sample you gave me.This topic was automatically closed 14 days after the last reply. New replies are no longer allowed.Powered by Discourse, best viewed with JavaScript enabled"
20,cudnn-reduce-tensor-doesnt-respect-input-data-formats,"Hi,
I’m using cudnnReduceTensor to calculate channel-wise average of the input. To do that I set alpha to 1/n and cudnn reduce descriptor to ADD. My input and output tensors are set to NHWC format. Everything is in float. But cudnnReduceTensor treats the input array in NCHW format anyway. I’m using CuDNN (v8201). Is this a bug or something else?Hi @dhananjt
Can you please share with us an API and error log?ThanksHas this issue been resolved?  I’m having an issue that looks like what he’s talking about.  I’m just doing a simple average along the images dim in order to calc the average error over all the images in a batch.The resulting image that comes out looks like it’s not coming out as NHWC.EDIT:  I set the output tensor’s format to CUDNN_TENSOR_NCHW and it worked…Can you at least update the docs about how reductions output?  Although, this really doesn’t help as I’d like to then pass that avg error back through my network…and it’s all in NHWC.  Sheesh.EDIT:  And just to be more clear.  Even though I set the output tensors format to NCHW, I can still save the image as the data in the tensor is still NHWC.  Maybe I’m confused over how the format flag works?  I’d think it’d inform any function on how the data in the tensor is actually stored.  Very strange.I couldn’t solve my issue. I had tried everything (including ensuring tensors’ formats).
I ended up switching to PyTorch as it provides a wrapper around CUDA.Powered by Discourse, best viewed with JavaScript enabled"
21,how-to-contribute-to-quickstart-scripts,"I have noticed some small changes that could be made to improve the quickstart scripts.  Can I contribute to these some where?HI @ryeinThanks for your interest in RivaThanks for your extending help for contribution, we really appreciate,Apologies, the Riva code base is internal ,if you can share your changes/insights over a document (if possible), i can certainly take this up with Riva team and bring it up with themThanksin riva_start.sh line 93 if you aren’t running tegra it doesn’t actually connect the ports properly at least on my distro.so if you change the code to
docker_run_args=""-p 8000:8000 -p 8001:8001 -p 8002:8002""it works properlyNot sure if the devs are running their docker instances in another way that causes this issue, but if you run it as a normal user I had this issue.Powered by Discourse, best viewed with JavaScript enabled"
22,segmentation-fault-errors-and-some-other-deserialize-the-cuda-engine-failed,"Hello, I changed the config and everything else according to instructions from linghu8812/tensorrt_inference (github.com). Next, I ran commands for yolov7 and errors started popping up at startup.If you run without sudo, Segmentation Fault (core dumped) pops up.And if you run with sudo, then another error pops up.I attached a photo to demonstrate the errors.I would be grateful for help with solving the problem!I use a custom yolov7 model with its own weights.TensorRT Version: 7.2.2.3
GPU Type: NVIDIA RTX 2070
CUDA Version: 11.1
Operating System + Version: Ubuntu 20.04249795877-2e76949c-c7c6-4ec5-8914-7a2859cc45791920×1080 306 KBHi,It looks like you’re using a very old version of TensorRT. We highly recommend that you try  the latest TensorRT version 8.6 and let us know if you still face the same issue.Thank you.Powered by Discourse, best viewed with JavaScript enabled"
23,tensorrt-fp8-support,"Unable to run inference using TensorRT FP8 quantizationTensorRT Version: 8.6.1
GPU Type: RTX 4070 Ti
Nvidia Driver Version: 530
CUDA Version: 12.1
CUDNN Version: 8.9.2.26
Operating System + Version: Ubuntu 22.04 LTS
Python Version (if applicable): 3.10
TensorFlow Version (if applicable): —
PyTorch Version (if applicable): —
Baremetal or Container (if container which image + tag): baremetalNVIDIA claimed that the new 4th gen TensorCores support FP8 quantization.
So I have an RTX 4070 Ti (ada lovelace archi.) which has the 4th Gen of TensorCores and suport FP8 format. So I have installed CUDA 12.1 and TensorRT 8.6.1 which include kFP8 data type for FP8 format.However, I cannot successfully run inference using FP8. INT8 on the other hand works fine but FP8 is not working.Here I have found in limitations section that FP8 is not supported in TensorRT yet.So basically FP8 is implemented in TensorRT 8.6.1 but it is not supported yet?
Could you please explain to me what that means? Because for me it is not supported means it is not implemented as well… !!Thank you in advanced.Best regardsHi, Please refer to the below links to perform inference in INT8master/samples/sampleINT8NVIDIA® TensorRT™, an SDK for high-performance deep learning inference, includes a deep learning inference optimizer and runtime that delivers low latency and high throughput for inference applicat...1777.21 KB
Thanks!Hi,1 TensorRT 8.6 adds nvinfer1::DataType::kFP8 to the public API in preparation for the introduction of FP8 support in future TensorRT releases. However, FP8 (8-bit floating point) is not supported by TensorRT currently, and attempting to use FP8 will result in an error or undefined behavior.Please refer to the Developer Guide :: NVIDIA Deep Learning TensorRT Documentation for the same.Thank you.Powered by Discourse, best viewed with JavaScript enabled"
24,cannot-create-engine-for-a-model-with-softmaxlayer,"When building an engine from a network definition using the C++ API only using an 5D input and a softmax layer, the engine can only be created when the input shape is smaller or equal to (1, 3, 248, 248, 248). I added a minimal example to reproduce the issue. The error I get is: “Error Code 10: Internal Error (Could not find any implementation for node softmax.)”.**TensorRT Version8.6.1:
**GPU TypeRTX 2000 Ada Laptop GPU / RTX 5000 A:
**Nvidia Driver Version536.25:
**CUDA Version11.8:
**CUDNN Version8.9:
**Operating System + VersionWindows 10 / 11:
Python Version (if applicable):
TensorFlow Version (if applicable):
PyTorch Version (if applicable):
Baremetal or Container (if container which image + tag):softmax_bug.cpp (1.6 KB)Hi,Could you please share with us the complete verbose logs.Thank you.I also add the updated example file to reproduce this log.softmax_bug_verbose.cpp (1.6 KB)
]Verbose logs:
[MemUsageChange] Init CUDA: CPU +346, GPU +0, now: CPU 18570, GPU 1381 (MiB)
Trying to load shared library nvinfer_builder_resource.dll
Loaded shared library nvinfer_builder_resource.dll
[MemUsageChange
Init builder kernel library: CPU +1422, GPU +264, now: CPU 21129, GPU 1645 (MiB)
Original: 1 layers
After dead-layer removal: 1 layers
Graph construction completed in 0.0008457 seconds.
After Myelin optimization: 1 layers
Applying ScaleNodes fusions.
After scale fusion: 1 layers
After dupe layer removal: 1 layers
After final dead-layer removal: 1 layers
After tensor merging: 1 layers
After vertical fusions: 1 layers
After dupe layer removal: 1 layers
After final dead-layer removal: 1 layers
After tensor merging: 1 layers
After slice removal: 1 layers
After concat removal: 1 layers
Trying to split Reshape and strided tensor
Graph optimization time: 0.0025031 seconds.
Building graph using backend strategy 2
Local timing cache in use. Profiling results in this builder pass will not be stored.
Constructing optimization profile number 0 [1/1].
Applying generic optimizations to the graph for inference.
Reserving memory for host IO tensors. Host: 0 bytes
=============== Computing costs for softmax
*************** Autotuning format combination: Float(50331648,16777216,65536,256,1) → Float(50331648,16777216,65536,256,1) ***************
--------------- Timing Runner: softmax (CaskSoftMaxV2[0x80000040])
Skipping tactic 0x48c115a824ac468d due to exception shader run failed
softmax (CaskSoftMaxV2[0x80000040]) profiling completed in 0.0039532 seconds. Fastest Tactic: 0xd15ea5edd15ea5ed Time: inf
10: Could not find any implementation for node softmax.
10: [optimizer.cpp::nvinfer1::builder::cgraph::LeafCNode::computeCosts::3869] Error Code 10: Internal Error (Could not find any implementation for node softmax.)Any news on this topic?Powered by Discourse, best viewed with JavaScript enabled"
25,tensorrt-is-slower-than-pth,"When I convert the PTH model of U2Net to the TensorRT model, the TensorRT fp16 model is 10 times slower than the PTH inference model. The ONNX structure of the model does not have a special network layer. The profiler is used to find that the time consumption of some convolutional layer and resizing layers is abnormal.Hello @1462555784 and welcome to the NVIDIA developer forums.I took the liberty of moving your post to the dedicated TensorRT category. You will find more topic experts in there.Thanks!Powered by Discourse, best viewed with JavaScript enabled"
26,riva-start-sh-will-not-start-the-server,"Hardware - GPU RTX A6000
Hardware - AMD Ryzen Threadripper PRO 3945WX 12-Cores
Operating System - CentOS Stream 9
Riva Version - 2.12
How to reproduce the issue ?
When I run the riva_init.sh file with the following config file:riva_target_gpu_family=“tegra”riva_tegra_platform=“orin”service_enabled_asr=true
service_enabled_nlp=true
service_enabled_tts=true
service_enabled_nmt=truelanguage_code=(“en-US”)asr_acoustic_model=(“conformer”)use_asr_greedy_decoder=falsegpus_to_use=“device=0”MODEL_DEPLOY_KEY=“tlt_encode”riva_model_loc=“riva-model-repo”if [[ $riva_target_gpu_family == “tegra” ]]; then
riva_model_loc=“pwd/model_repository”
fiuse_existing_rmirs=falseriva_speech_api_port=“50051”riva_ngc_org=“nvidia”
riva_ngc_team=“riva”
riva_ngc_image_version=“2.12.1”
riva_ngc_model_version=“2.12.0”########## ASR MODELS ##########models_asr=()for lang_code in ${language_code[@]}; do
modified_lang_code=“${lang_code//-/_}”
modified_lang_code=${modified_lang_code,}donemodels_asr+=()########## NLP MODELS ##########if [[ $riva_target_gpu_family == “tegra” ]]; then
models_nlp=()
else
models_nlp=()
fi########## TTS MODELS ##########if [[ $riva_target_gpu_family == “tegra” ]]; then
models_tts=()
else
models_tts=()
fi######### NMT models ###############models_nmt=(#“${riva_ngc_org}/${riva_ngc_team}/rmir_nmt_en_de_24x6:${riva_ngc_model_version}”
#“${riva_ngc_org}/${riva_ngc_team}/rmir_nmt_en_es_24x6:${riva_ngc_model_version}”
#“${riva_ngc_org}/${riva_ngc_team}/rmir_nmt_en_zh_24x6:${riva_ngc_model_version}”
#“${riva_ngc_org}/${riva_ngc_team}/rmir_nmt_en_ru_24x6:${riva_ngc_model_version}”
#“${riva_ngc_org}/${riva_ngc_team}/rmir_nmt_en_fr_24x6:${riva_ngc_model_version}”
#“${riva_ngc_org}/${riva_ngc_team}/rmir_nmt_de_en_24x6:${riva_ngc_model_version}”
#“${riva_ngc_org}/${riva_ngc_team}/rmir_nmt_es_en_24x6:${riva_ngc_model_version}”
#“${riva_ngc_org}/${riva_ngc_team}/rmir_nmt_ru_en_24x6:${riva_ngc_model_version}”
#“${riva_ngc_org}/${riva_ngc_team}/rmir_nmt_zh_en_24x6:${riva_ngc_model_version}”
#“${riva_ngc_org}/${riva_ngc_team}/rmir_nmt_fr_en_24x6:${riva_ngc_model_version}”#“${riva_ngc_org}/${riva_ngc_team}/rmir_nmt_en_deesfr_24x6:${riva_ngc_model_version}”
#“${riva_ngc_org}/${riva_ngc_team}/rmir_nmt_en_deesfr_12x2:${riva_ngc_model_version}”
#“${riva_ngc_org}/${riva_ngc_team}/rmir_nmt_deesfr_en_24x6:${riva_ngc_model_version}”
#“${riva_ngc_org}/${riva_ngc_team}/rmir_nmt_deesfr_en_12x2:${riva_ngc_model_version}”#“${riva_ngc_org}/${riva_ngc_team}/rmir_megatronnmt_any_en_500m:${riva_ngc_model_version}”
#“${riva_ngc_org}/${riva_ngc_team}/rmir_megatronnmt_en_any_500m:${riva_ngc_model_version}”
)NGC_TARGET=${riva_ngc_org}
if [[ ! -z ${riva_ngc_team} ]]; then
NGC_TARGET=“${NGC_TARGET}/${riva_ngc_team}”
else
team=“""""”
fissl_server_cert=“”
ssl_server_key=“”
ssl_root_cert=“”image_speech_api=“nvcr.io/${NGC_TARGET}/riva-speech:${riva_ngc_image_version}”image_init_speech=“nvcr.io/${NGC_TARGET}/riva-speech:${riva_ngc_image_version}-servicemaker”riva_daemon_speech=“riva-speech”
if [[ $riva_target_gpu_family != “tegra” ]]; then
riva_daemon_client=“riva-client”
fiI believe that it does end successfully and I get this output:Logging into NGC docker registry if necessary…
Pulling required docker images if necessary…
Note: This may take some time, depending on the speed of your Internet connection.Pulling Riva Speech Server images.
Pulling nvcr.io/nvidia/riva/riva-speech:2.12.1. This may take some time…Downloading models (RMIRs) from NGC…
Note: this may take some time, depending on the speed of your Internet connection.
To skip this process and use existing RMIRs set the location and corresponding flag in config.sh.
2023-07-26 17:22:13 URL:https://prod-model-registry-ngc-bucket.s3.us-west-2.amazonaws.com/org/nvidia/team/ngc-apps/recipes/ngc_cli/versions/3.22.0/files/ngccli_arm64.
zip?response-content-disposition=attachment%3B%20filename%3D%22ngccli_arm64.zip%22&response-content-type=application%2Fzip&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Date
=20230726T172203Z&X-Amz-SignedHeaders=host&X-Amz-Expires=3600&X-Amz-Credential=AKIA3PSNVSIZ42OUKYPX%2F20230726%2Fus-west-2%2Fs3%2Faws4_request&X-Amz-Signature=4b532d2
f8d3f8dc5f7ac15ff4bef838fc4666219673d53cc0449b0b40f31b75a [85961696/85961696] → “ngccli_arm64.zip” [1]
/opt/rivabash: line 3: /usr/local/bin/ngc: cannot execute binary file: Exec format error
bash: line 3: /usr/local/bin/ngc: cannot execute binary file: Exec format error
bash: line 3: /usr/local/bin/ngc: cannot execute binary file: Exec format error
bash: line 3: /usr/local/bin/ngc: cannot execute binary file: Exec format error[[ tegra != \t\e\g\r\a ]][[ tegra == \t\e\g\r\a ]]‘[’ -d /root/Apps/riva_quickstart_v2.12.1/model_repository/rmir ‘]’[[ tegra == \t\e\g\r\a ]]‘[’ -d /root/Apps/riva_quickstart_v2.12.1/model_repository/prebuilt ‘]’echoecho ‘Riva initialization complete. Run ./riva_start.sh to launch services.’
Riva initialization complete. Run ./riva_start.sh to launch services.However, when I try to run the ./riva_start.sh command it never connects to the riva server and I receive the following:
Starting Riva Speech Services. This may take several minutes depending on the number of models deployed.
Waiting for Riva server to load all models…retrying in 10 seconds
Waiting for Riva server to load all models…retrying in 10 seconds
Waiting for Riva server to load all models…retrying in 10 seconds
Waiting for Riva server to load all models…retrying in 10 seconds
Waiting for Riva server to load all models…retrying in 10 seconds
Waiting for Riva server to load all models…retrying in 10 seconds
Waiting for Riva server to load all models…retrying in 10 seconds
Waiting for Riva server to load all models…retrying in 10 seconds
Waiting for Riva server to load all models…retrying in 10 seconds
Waiting for Riva server to load all models…retrying in 10 seconds
Waiting for Riva server to load all models…retrying in 10 seconds
Waiting for Riva server to load all models…retrying in 10 seconds
Waiting for Riva server to load all models…retrying in 10 seconds
Waiting for Riva server to load all models…retrying in 10 seconds
Waiting for Riva server to load all models…retrying in 10 seconds
Waiting for Riva server to load all models…retrying in 10 seconds
Waiting for Riva server to load all models…retrying in 10 seconds
Waiting for Riva server to load all models…retrying in 10 seconds
Waiting for Riva server to load all models…retrying in 10 seconds
Waiting for Riva server to load all models…retrying in 10 seconds
Waiting for Riva server to load all models…retrying in 10 seconds
Waiting for Riva server to load all models…retrying in 10 seconds
Waiting for Riva server to load all models…retrying in 10 seconds
Waiting for Riva server to load all models…retrying in 10 seconds
Waiting for Riva server to load all models…retrying in 10 seconds
Waiting for Riva server to load all models…retrying in 10 seconds
Waiting for Riva server to load all models…retrying in 10 seconds
Waiting for Riva server to load all models…retrying in 10 seconds
Waiting for Riva server to load all models…retrying in 10 seconds
Waiting for Riva server to load all models…retrying in 10 seconds
Health ready check failed.
Check Riva logs with: docker logs riva-speechWhen I run docker logs riva-speech I get no output.Hi @samfarfarThanks for your interest in Riva,Apologies the docker logs riva-speech gives no output,Quick Request, can we try to run docker logs riva-speech parallelly in conjuction with bash riva_start.sh in two terminals (parallely at same time) and let me know if it generates logs, if yes, kindly share the logsThanksThis is the output I am getting from the docker log command:NVIDIA Release 23.06.1 (build 64517154)Copyright (c) 2018-2023, NVIDIA CORPORATION & AFFILIATES. All rights reserved.Various files include modifications (c) NVIDIA CORPORATION & AFFILIATES.  All rights reserved.This container image and its contents are governed by the NVIDIA Deep Learning Container License.
By pulling and using the container, you accept the terms and conditions of this license:145.24 KBRiva waiting for Triton server to load all models…retrying in 1 second
I0728 15:57:41.968026 102 metrics.cc:864] Collecting metrics for GPU 0: NVIDIA RTX A6000
I0728 15:57:41.968322 102 metrics.cc:757] Collecting CPU metrics
I0728 15:57:41.968536 102 tritonserver.cc:2264]
±---------------------------------±-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+
| Option                           | Value                                                                                                                                                                                                |
±---------------------------------±-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+
| server_id                        | triton                                                                                                                                                                                               |
| server_version                   | 2.27.0                                                                                                                                                                                               |
| server_extensions                | classification sequence model_repository model_repository(unload_dependents) schedule_policy model_configuration system_shared_memory cuda_shared_memory binary_tensor_data statistics trace logging |
| model_control_mode               | MODE_NONE                                                                                                                                                                                            |
| strict_model_config              | 1                                                                                                                                                                                                    |
| rate_limit                       | OFF                                                                                                                                                                                                  |
| pinned_memory_pool_byte_size     | 268435456                                                                                                                                                                                            |
| cuda_memory_pool_byte_size{0}    | 1000000000                                                                                                                                                                                           |
| response_cache_byte_size         | 0                                                                                                                                                                                                    |
| min_supported_compute_capability | 6.0                                                                                                                                                                                                  |
| strict_readiness                 | 1                                                                                                                                                                                                    |
| exit_timeout                     | 30                                                                                                                                                                                                   |
±---------------------------------±-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+I0728 15:57:41.968543 102 server.cc:261] No server context available. Exiting immediately.
error: creating server: Invalid argument - --model-repository must be specifiedRiva waiting for Triton server to load all models…retrying in 1 secondPowered by Discourse, best viewed with JavaScript enabled"
27,unable-to-install-tensorrt-for-ubuntu-22-04-lts,"The module python3-libnvinfer need a python version <= 3.9
But Ubuntu 22.04 has a default python version 3.10
When will the new version of TensorRT for Ubuntu 22.04 be published?Hi,This support may be added in the 8.5 version which is tentatively available by this month’s end or by mid of the next month.
Meanwhile, you can try degrading the python version or use the TensorRT NGC container.NVIDIA TensorRT is a C++ library that facilitates high-performance inference on NVIDIA graphics processing units (GPUs). TensorRT takes a trained network and produces a highly optimized runtime engine that performs inference for that network.Thank you.Is there any update on this issue?  It looks like Python 3.10 support was not added to version 8.5.  Are there any plans to add it in future versions?Hi,
Please refer to below links related custom plugin implementation and sample:This Samples Support Guide provides an overview of all the supported NVIDIA TensorRT 8.5.2 samples included on GitHub and in the product package. The TensorRT samples specifically help in areas such as recommenders, machine comprehension, character...TensorRT is an SDK for high performance, deep learning inference. It includes a deep learning inference optimizer and a runtime that delivers low latency and high throughput for deep learning…While IPluginV2 and IPluginV2Ext interfaces are still supported for backward compatibility with TensorRT 5.1 and 6.0.x respectively, however, we recommend that you write new plugins or refactor existing ones to target the IPluginV2DynamicExt or IPluginV2IOExt interfaces instead.Thanks!Perhaps I am missing something, but neither link seems to answer my question.  My question is “when will libnvinfer support Python 3.10?”  To be more specific, I am talking about the packages python3-libnvinfer and python3-libnvinfer-dev in Ubuntu 22.10.FYI, I fixed this.  It turns out that I needed to change my Nvidia APT repository from pointing to this:
https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2004/x86_64/to pointing to this:
https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64/Powered by Discourse, best viewed with JavaScript enabled"
28,create-a-twin-model-with-tensorflow,"I would like to create a program in python that can detect deviation in images. There are subtle deviation but still. The regular deep learning method I use is training on a lot of images with a defect and classifying those.
I want to do the opposite. I want to tell the AI that this is good. Anything else is crap. How can I do that?
Anyone knows how? Where to start? I tried openAI but it just brings back a solution that always results in 0 prediction.TensorRT Version:  Not working
GPU Type:  ~
Nvidia Driver Version:
CUDA Version:
CUDNN Version:
Operating System + Version: Ubuntu 20.04
Python Version (if applicable):  3.8.10
TensorFlow Version (if applicable):  Latest
PyTorch Version (if applicable):
Baremetal or Container (if container which image + tag):Hi,
We recommend you to check the below samples links in case of tf-trt integration issues.
https://docs.nvidia.com/deeplearning/frameworks/tf-trt-user-guide/index.html#samplesThis NVIDIA TensorRT 8.6.1 Quick Start Guide is a starting point for developers who want to try out TensorRT SDK; specifically, this document demonstrates how to quickly construct an application to run inference on a TensorRT engine.https://docs.nvidia.com/deeplearning/frameworks/tf-trt-user-guide/index.html#integrate-ovr
https://docs.nvidia.com/deeplearning/frameworks/tf-trt-user-guide/index.html#usingtftrtIf issue persist, We recommend you to reach out to Tensorflow forum.
Thanks!Thanks. I was looking more for a general approach on what to use in order to make this anomaly detection. Normally you know what good is (thats what I have 99% of the times). So how, without having the faulty ones, can I get the anomalies.Hi,The above query looks generic and is not related to TensorRT.
Please refer to the following, which may help you.If you need further assistance, please reach out to the relevant forum.Thank you.Thank you for the response. I will look into that link.Powered by Discourse, best viewed with JavaScript enabled"
29,fail-to-run-mlperf-inference-resnet50-benchmars-on-docker,"Hi all,
I am newbie to the NVIDIA AI world. I want to check the mlperf inference resnet50 benchmark result on my GPU A30.
Follow below user guide to setup SUT for running resnet50 benchmark.master/closed/NVIDIAThis repository contains the results and code for the MLPerf™ Inference v2.1 benchmark. - inference_results_v2.1/closed/NVIDIA at master · mlcommons/inference_results_v2.1However I get RuntimeError(“Building engines failed!”) and I have no idea how to fix it. can anyone give me some suggestion. Thanks
Below is the output.(mlperf) test@mlperf-inference-test-x86_64:/work$ make run RUN_ARGS=“–benchmarks=resnet50 --scenarios=offline”
make[1]: Entering directory ‘/work’
[2023-05-12 05:35:37,521 main_v2.py:221 INFO] Detected system ID: KnownSystem.AMD_K905
[2023-05-12 05:35:39,726 generate_engines.py:172 INFO] Building engines for resnet50 benchmark in Offline scenario…
[2023-05-12 05:35:39,752 ResNet50.py:36 INFO] Using workspace size: 0
[05/12/2023-05:35:40] [TRT] [I] [MemUsageChange] Init CUDA: CPU +320, GPU +0, now: CPU 354, GPU 642 (MiB)
[05/12/2023-05:35:41] [TRT] [I] [MemUsageChange] Init builder kernel library: CPU +414, GPU +132, now: CPU 787, GPU 774 (MiB)
Process Process-1:
Traceback (most recent call last):My SUT config
OS               :ubuntu 22.04.2
GPU            :A30 x2
Driver         :510.39.01
Nvidia-ctk :1.13.1nvidia-smi.txt (14.8 KB)For this error , i find that the “python3 -m scripts.custom_systems.add_custom_system” create empty value for your sut config  and you need to fill necessary content for test to run.
The github website have detail information in section "" Adding a New or Custom System"". Follow the section , you can avoid this error.There are two way to create config file.You need to do is copy and modify the data in init.py into the custom.py.
You can modify the server_target_qps value to match your gpu number.Powered by Discourse, best viewed with JavaScript enabled"
30,autoencoder-multi-gpus-training,"Any way to utilize multiple GPUs on DFP training?  i am not sure if dfencode has this optionVer 23.03 on DGX 4xA100 ,
in order to train using all GPUs does it matter if using DFP jupyter production docker or morpheus-sdk docker?The current implementation of the DFEncoder targets a specific CUDA device but our engineers are going to take a look at the issue you created as an enhancement:i am working on the DFP using Nvidia DGX 
is it possible to utilize multiple GP…U's for training?Powered by Discourse, best viewed with JavaScript enabled"
31,why-tensorrt-8-2-1-1-cuda10-2-does-not-exist-for-arm64,"Hello,I wanted to know why Jetpack 4.6.2. comes with CUDA 10.2 and TensorRT 8.2.1 but there is actually no possibility to download TensorRT 8.2.1 with CUDA 10.2 from the TensorRT download page for
arm64 architecture: https://developer.nvidia.com/nvidia-tensorrt-8x-download.This causes all sorts of issues when trying to install TensorRT 8.2.1 at the container level using a Nvidia Cuda 10.2 base image.Thanks!Hi @bayab ,
You can use the below link to download trt 8.2 with CUDA 10.2
http://cuda-repo/release-candidates/Libraries/TensorRT/v8.2/ThanksHi AakankshaS,It seems the link you provided is broken?Hi @bayab ,
Updated response -Jetpack 4.6.2. comes with CUDA 10.2 and TensorRT 8.2.1 but there is actually no possibility to download TensorRT 8.2.1 with CUDA 10.2 from the TensorRT download page for
arm64 architecture: https://developer.nvidia.com/nvidia-tensorrt-8x-download aarch64 packages are only available through the Jetson SDK manager, hence cant be downloaded externally.ThanksThanks for your reply. Actually for other people trying to accomplish this, one way to do it is to flash your Jetson with Jetpack 4.6.2 and then manually copy all required nvidia libraries into a container then save that image, in this way you do not need to download tensorRT 8.2.1 and cuda 10.2 from external sources in your Dockerfile.This topic was automatically closed 14 days after the last reply. New replies are no longer allowed.Powered by Discourse, best viewed with JavaScript enabled"
32,pip-install-tensorrt-fails,"I’m running Windows 10 and have installed CUDA 11.1 and TensorRT 7.2.3.4 DDLs. I noted that unlike for TensorRT 8 there is no wheel file included in the .zip package for TensorRT 7.2.3.4. To get the python bindings, I tried installing via pip:pip install nvidia-pyindex
pip install tensorrtHowever, this second command fails:TensorRT Version: 7.2.3.4
GPU Type: RTX 3070
CUDA Version: 11.1
Operating System + Version: Windows 10
Python Version (if applicable): 3.8
PyTorch Version (if applicable): 1.8.0+cu111
Baremetal or Container (if container which image + tag): Bare metalHi,
Please refer to the installation steps from the below link if in case you are missing on anythingThis NVIDIA TensorRT 8.6.0 Early Access (EA) Installation Guide provides the installation requirements, a list of what is included in the TensorRT package, and step-by-step instructions for installing TensorRT.
Also, we suggest you to use TRT NGC containers to avoid any system dependency related issues.NVIDIA TensorRT is a C++ library that facilitates high-performance inference on NVIDIA graphics processing units (GPUs). TensorRT takes a trained network and produces a highly optimized runtime engine that performs inference for that network.Thanks!I have followed the instructions. Can you please confirm what Python versions are supported? I am using 3.8.10.Ideally, can you provide the relevant wheel file?Hi @eric.w.maynard ,
I see you are using TRT 7.2.3.4, which is an older version. We highly recommend you to try out the latest TRT.
However, to answer your query, TRT 7.2.3, The Windows zip package for TensorRT does not provide Python support.
Please check out the release notes and installation guide for the same version.
ThanksPowered by Discourse, best viewed with JavaScript enabled"
33,inference-time-when-using-multi-stream-in-tensorrt-is-much-slower-than-a-single-one,"A clear and concise description of the bug or issue.TensorRT Version: 7.2.3
GPU Type: Tesla T4
Nvidia Driver Version:440.44
CUDA Version: 10.2
CUDNN Version: 8.0
Operating System + Version: Centos7
Python Version (if applicable): 3.6
TensorFlow Version (if applicable):
PyTorch Version (if applicable): 1.6.0
Baremetal or Container (if container which image + tag):Hey:
When I am using multi stream which has an single context ，the inference speed is much slower than a single stream. I used nvprof to observe the gpu trace，each stream is executed alternately，not the parallel effect I want.
The question is if the multi stream are executed serially on the GPU. And how can i get the fastest speed if i have several engine that can inference meanwhile.Hi,You may need to create multiple IExecutionContext and assign each IExecutionContext with 1 CUDA stream when calling enqueueV2. They are independent of each other.BTW, we also have a Triton server built on top of Tensorrt, it has a built-in schedule to handle multiple engines/models.The Triton Inference Server provides an optimized cloud and edge inferencing solution.  - GitHub - triton-inference-server/server: The Triton Inference Server provides an optimized cloud and edge i...Standardizes model deployment and delivers fast and scalable AI in production.Thank you.I do create multiple IExecutionContext and assign each IExecutionContext with single CUDA stream, but the result is what I described . However, when I bind each IExecutionContext with different GPU, the inference time is close to that of a single stream.
Thanks.A: Each ICudaEngine object is bound to a specific GPU when it is instantiated, either by the builder or on deserialization. To select the GPU, use cudaSetDevice() before calling the builder or deserializing the engine. Each IExecutionContext is bound to the same GPU as the engine from which it was created. When calling execute() or enqueue(), ensure that the thread is associated with the correct device by calling cudaSetDevice() if necessary.In general, CUDA programming streams are a way of organizing asynchronous work. Asynchronous commands put into a stream are guaranteed to run in sequence but may execute out of order with respect to other streams. In particular, asynchronous commands in two streams may be scheduled to run concurrently (subject to hardware limitations).In the context of TensorRT and inference, each layer of the optimized final network will require work on the GPU. However, not all layers will be able to fully use the computation capabilities of the hardware. Scheduling requests in separate streams allows work to be scheduled immediately as the hardware becomes available without unnecessary synchronization. Even if only some layers can be overlapped, overall performance will improve.In addition to the within-inference streaming, you can also enable streaming between multiple execution contexts. For example, you can build an engine with multiple optimization profiles and create an execution context per profile. Then, call the enqueueV3() function of the execution contexts on different streams to allow them to run in parallel.Running multiple concurrent streams often leads to situations where several streams share compute resources at the same time. This means that the network may have less compute resources available during inference than when the TensorRT engine was being optimized. This difference in resource availability can cause TensorRT to choose a kernel that is suboptimal for the actual runtime conditions. In order to mitigate this effect, you can limit the amount of available compute resources during engine creation to more closely resemble actual runtime conditions. This approach generally promotes throughput at the expense of latency. For more information, refer to Limiting Compute Resources.It is also possible to use multiple host threads with streams. A common pattern is incoming requests dispatched to a pool of waiting for worker threads. **In this case, the pool of worker threads will each have one execution context and CUDA stream. Each thread will request work in its own stream as the work becomes available. Each thread will synchronize with its stream to wait for results without blocking other worker threads.Powered by Discourse, best viewed with JavaScript enabled"
34,no-option-for-ubuntu-22-10-in-cuda-12-2-download,"I have been trying to set up my system for Deep Learning (powered by GPU). I use the latest Ubuntu 22.10 version on my system, and have Tensorflow 2.13.0 version installed, which is working fine on CPU.I want to use GPU for my processes, however in the CUDA toolkit website, there is no option to download CUDA 12 for Ubuntu 22.10. The latest version of Ubuntu mentioned is 22.04.
What should I do now? I installed it previously and it did not support my system, and tensorflow couldn’t find any GPU drivers.
Also, in the tensorflow website (Build from source  |  TensorFlow), it says that tensorflow 2.13.0 supports only till CUDA 11.8 version, but I don’t know if this version of CUDA is supported by Ubuntu 22.10 or not. How should I setup my system with this?Hi,
Please refer to the installation steps from the below link if in case you are missing on anythingThis NVIDIA TensorRT 8.6.1 Installation Guide provides the installation requirements, a list of what is included in the TensorRT package, and step-by-step instructions for installing TensorRT.
Also, we suggest you to use TRT NGC containers to avoid any system dependency related issues.NVIDIA TensorRT is a C++ library that facilitates high-performance inference on NVIDIA graphics processing units (GPUs). TensorRT takes a trained network and produces a highly optimized runtime engine that performs inference for that network.Thanks!Powered by Discourse, best viewed with JavaScript enabled"
35,riva-diarization-does-not-match-nemo-diarization-accuracy,"Please provide the following information when requesting support.I just deployed a diarization model with the default model on ngc and also trained a custom model and both of them perform really bad compared to Nemo.¿Is the Riva diarization accuracy excpected to match Nemo accurary?I’m using the default commands on the pipelines guide ¿What can be wrong?Hi @carlfm01Thanks for your interest in RivaApologies for the delay,I will check with the team and provide answers, probably would need a little more information, will request when i connect with teamThanksPowered by Discourse, best viewed with JavaScript enabled"
36,riva-asr-streaming-wont-work,"i ve been trying to use the streaming automatic speech recognition, however it crashes every time i use streaming asr. it works normally when using offline speech recognition though. if logs are needed let me know.also here is the config.sh of the server:
config.sh (13.3 KB)here are the logs: the riva server is running on an A100 and the client on a desktop with ubuntu linux:ALSA lib pcm_dsnoop.c:601:(snd_pcm_dsnoop_open) unable to open slave
ALSA lib pcm_dmix.c:1032:(snd_pcm_dmix_open) unable to open slave
ALSA lib pcm.c:2664:(snd_pcm_open_noupdate) Unknown PCM cards.pcm.rear
ALSA lib pcm.c:2664:(snd_pcm_open_noupdate) Unknown PCM cards.pcm.center_lfe
ALSA lib pcm.c:2664:(snd_pcm_open_noupdate) Unknown PCM cards.pcm.side
Cannot connect to server socket err = No such file or directory
Cannot connect to server request channel
jack server is not running or cannot be started
JackShmReadWritePtr::~JackShmReadWritePtr - Init not done for -1, skipping unlock
JackShmReadWritePtr::~JackShmReadWritePtr - Init not done for -1, skipping unlock
Cannot connect to server socket err = No such file or directory
Cannot connect to server request channel
jack server is not running or cannot be started
JackShmReadWritePtr::~JackShmReadWritePtr - Init not done for -1, skipping unlock
JackShmReadWritePtr::~JackShmReadWritePtr - Init not done for -1, skipping unlock
ALSA lib pcm_oss.c:397:(_snd_pcm_oss_open) Cannot open device /dev/dsp
ALSA lib pcm_oss.c:397:(_snd_pcm_oss_open) Cannot open device /dev/dsp
ALSA lib confmisc.c:160:(snd_config_get_card) Invalid field card
ALSA lib pcm_usb_stream.c:482:(_snd_pcm_usb_stream_open) Invalid card ‘card’
ALSA lib confmisc.c:160:(snd_config_get_card) Invalid field card
ALSA lib pcm_usb_stream.c:482:(_snd_pcm_usb_stream_open) Invalid card ‘card’
ALSA lib pcm_dmix.c:1032:(snd_pcm_dmix_open) unable to open slave
Input audio devices:
13: HD-Audio Generic: ALCS1200A Analog (hw:2,0)
15: HD-Audio Generic: ALCS1200A Alt Analog (hw:2,2)
17: pulse
18: default
None
18
ALSA lib pcm_dsnoop.c:601:(snd_pcm_dsnoop_open) unable to open slave
ALSA lib pcm_dmix.c:1032:(snd_pcm_dmix_open) unable to open slave
ALSA lib pcm.c:2664:(snd_pcm_open_noupdate) Unknown PCM cards.pcm.rear
ALSA lib pcm.c:2664:(snd_pcm_open_noupdate) Unknown PCM cards.pcm.center_lfe
ALSA lib pcm.c:2664:(snd_pcm_open_noupdate) Unknown PCM cards.pcm.side
Cannot connect to server socket err = No such file or directory
Cannot connect to server request channel
jack server is not running or cannot be started
JackShmReadWritePtr::~JackShmReadWritePtr - Init not done for -1, skipping unlock
JackShmReadWritePtr::~JackShmReadWritePtr - Init not done for -1, skipping unlock
Cannot connect to server socket err = No such file or directory
Cannot connect to server request channel
jack server is not running or cannot be started
JackShmReadWritePtr::~JackShmReadWritePtr - Init not done for -1, skipping unlock
JackShmReadWritePtr::~JackShmReadWritePtr - Init not done for -1, skipping unlock
ALSA lib pcm_oss.c:397:(_snd_pcm_oss_open) Cannot open device /dev/dsp
ALSA lib pcm_oss.c:397:(_snd_pcm_oss_open) Cannot open device /dev/dsp
ALSA lib confmisc.c:160:(snd_config_get_card) Invalid field card
ALSA lib pcm_usb_stream.c:482:(_snd_pcm_usb_stream_open) Invalid card ‘card’
ALSA lib confmisc.c:160:(snd_config_get_card) Invalid field card
ALSA lib pcm_usb_stream.c:482:(_snd_pcm_usb_stream_open) Invalid card ‘card’
ALSA lib pcm_dmix.c:1032:(snd_pcm_dmix_open) unable to open slave
SystemError: PY_SSIZE_T_CLEAN macro must be defined for ‘#’ formatsHi @aggelosioannidhs21Thanks for your interest in RivaApologies you are facing issueI will try to reproduce the issue from my endRequest to kindly  provide the python script/notebook used for ASR Streaming inferenceThanksPowered by Discourse, best viewed with JavaScript enabled"
37,error-when-runing-nvidia-tao-with-mask-rcnn,"i ran this command to convert data to tfrecord :  sudo docker run -it --rm --gpus all -v /home/mj/TAO/workspace:/workspace nvcr.io/nvidia/tao/tao-toolkit:4.0.1-tf1.15.5  mask_rcnn dataset_convert  -i /home/mj/TAO/workspace/tao-experiments/data/raw-data/val2017 -a /home/mj/TAO/workspace/tao-experiments/data/raw-data/annotations/instances_val2017.json  -o /home/mj/TAO/workspace/tao-experiments/data --include_masks -t val -s 32NVIDIA Release 4.0.1-TensorFlow (build )
TAO Toolkit Version 4.0.1Various files include modifications (c) NVIDIA CORPORATION & AFFILIATES.  All rights reserved.This container image and its contents are governed by the TAO Toolkit End User License Agreement.
By pulling and using the container, you accept the terms and conditions of this license:135.02 KBNOTE: The SHMEM allocation limit is set to the default of 64MB.  This may be
insufficient for TAO Toolkit.  NVIDIA recommends the use of the following flags:
docker run --gpus all --ipc=host --ulimit memlock=-1 --ulimit stack=67108864 …Using TensorFlow backend.
2023-05-24 09:01:12.396899: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0
WARNING:tensorflow:Deprecation warnings have been disabled. Set TF_ENABLE_DEPRECATION_WARNINGS=1 to re-enable them.
/usr/local/lib/python3.6/dist-packages/requests/init.py:91: RequestsDependencyWarning: urllib3 (1.26.5) or chardet (3.0.4) doesn’t match a supported version!
RequestsDependencyWarning)
2023-05-24 09:01:15.556207: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0
2023-05-24 09:01:15.984804: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libnvinfer.so.8
2023-05-24 09:01:15.998817: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcuda.so.1
WARNING:tensorflow:Deprecation warnings have been disabled. Set TF_ENABLE_DEPRECATION_WARNINGS=1 to re-enable them.
/usr/local/lib/python3.6/dist-packages/requests/init.py:91: RequestsDependencyWarning: urllib3 (1.26.5) or chardet (3.0.4) doesn’t match a supported version!
RequestsDependencyWarning)
Using TensorFlow backend.
Traceback (most recent call last):
File “</usr/local/lib/python3.6/dist-packages/iva/mask_rcnn/scripts/dataset_convert.py>”, line 3, in 
File “”, line 415, in 
File “”, line 403, in 
File “”, line 292, in main
FileNotFoundError: [Errno 2] No such file or directory: ‘/home/mj/TAO/workspace/tao-experiments/data’
Telemetry data couldn’t be sent, but the command ran successfully.
[WARNING]: <urlopen error [Errno -2] Name or service not known>
Execution status: FAILthe path is correct i dont know why it returns "" No such file or directory"" how can i solve it ??Powered by Discourse, best viewed with JavaScript enabled"
38,how-to-make-tensor-cores-work,"Hi!
I’m trying to make my network utilize my A100 tensor cores. To debug I’ve created simple model with single convolutional layer.I’ve already tried:The problem was connected with environment setup.
I’ve set up all libraries in my working conda environment but dlprof didn’t log any info about kernel usage. After I’had switched to nvidia docker image as it was recommended in dlprof installation guide, kernel info started logging, dlprof started correctly displaying tc usage as torch profiler and the problem has gone.This topic was automatically closed 14 days after the last reply. New replies are no longer allowed.Powered by Discourse, best viewed with JavaScript enabled"
39,dali-model-not-able-to-run-on-a-triton-server-on-a-gpu-node,"Hello, Whenever I try to deploy my DALI model on a Triton Server which is running on a GPU kubernetes node, I am seeing an error trace as “Assert on “device != “gpu” || device_id_ != CPU_ONLY_DEVICE_ID” failed: Cannot add a GPU operator ExternalSource, device_id should not be equal CPU_ONLY_DEVICE_ID” which is coming from DALI end hereIs there anything I need to add from DALI model code my current DALI pipeline code has  device=‘gpu’ and device_id=0 .Any suggestions on this will be really helpful pleasePowered by Discourse, best viewed with JavaScript enabled"
40,any-good-practice-for-data-ingestion-for-asr,"I was Trying to find a way to get RIVA ASR working with AWS for transcription. The challenge I am currently facing is how to get real-time transcription working. I always get issue on either using gRPC or transforming the data.
I would like to know if anyone found an optimal way to do real-time transcription in AWS?Below there is some thoughts:
AWS Kineses Video Data Stream → (some data transformation) → Riva ASR (AWS Batch EKS)I might Edit the post as I go exploring and discovering new solutions.Hi @Pappachuck_renanThanks for your interest in RivaI will check regarding your request with the team and get backThanksHi @Pappachuck_renanI have some inputs from the teamPlease follow the steps from How to Deploy Riva at Scale on AWS with EKS — NVIDIA Riva for deploying Riva API on AWSQuestions from the teamPlease provide the above details, will get back to the internal team and provide updatesThanksYour example they used a Ingress through traefik and I assume they do transform the data on the cluster itself.
I have seen examples on TensorRT.
I was mostly curious about alternatives to what u present.
Ingress through a edge router is far from optimal.So far I am just playing with ideas. I thought I could use Kinesis so I can have a more robust integration. Firehose is used for some ELT pipelines, pretty popular. lambda would just convert the encoding and send the job request.
I did not set anything yet. It doesnt feel right yet. I need to read more and see what can be done.The ASR I cant get it working at all outside NeMo. I am following other post for that.Hi @Pappachuck_renanApologies on the delayCan you share the details on errors which you get with AWS during transciption ?, I guess they are interested in fixing that at first placeThanksPowered by Discourse, best viewed with JavaScript enabled"
41,bug-of-layernormplugin,"The output of LayerNormPlugin has large error compared to torch.nn.LayerNorm.official docker container 22.12related files: https://cloud.tsinghua.edu.cn/f/40c33b2aeb2347678cea/?dl=1It will output >11 absolute error.Hi,
Please refer to below links related custom plugin implementation and sample:This Samples Support Guide provides an overview of all the supported NVIDIA TensorRT 8.5.3 samples included on GitHub and in the product package. The TensorRT samples specifically help in areas such as recommenders, machine comprehension, character...TensorRT is an SDK for high performance, deep learning inference. It includes a deep learning inference optimizer and a runtime that delivers low latency and high throughput for deep learning…While IPluginV2 and IPluginV2Ext interfaces are still supported for backward compatibility with TensorRT 5.1 and 6.0.x respectively, however, we recommend that you write new plugins or refactor existing ones to target the IPluginV2DynamicExt or IPluginV2IOExt interfaces instead.Thanks!Well… However, I think official provided plugins should be bug-free at least…Hi @lqs1 ,
You should export your model in opset 17 in order to leverage the new normalization layer in TensorRT 8.6.ThanksPowered by Discourse, best viewed with JavaScript enabled"
42,cant-install-video-effects-sdk-on-ubuntu-20-04,"I am trying to install Maxine Video Effects SDK on Ubuntu 20.04.5 LTS but no luck.I am following this guide Video Effects SDK System Guide - NVIDIA Docsand I have an error during /usr/local/VideoFX/share/build_samples.sh build.Dependencies I installed:Please assistPowered by Discourse, best viewed with JavaScript enabled"
43,enable-neural-vad,"with the quickstart scripts how do you enable marble telephony?Hi @ryeinThanks for your interest in RivaI will check with the team and provide inputsThanksHI @ryeinI checked with the team,Currently we do not provide Marblenet telephony as a part of our quickstartsYou can deploy it using the servicemaker container and the steps in the riva documentation : Pipeline Configuration — NVIDIA RivaThanksPowered by Discourse, best viewed with JavaScript enabled"
44,trt-e-3-builderconfig-cpp-382,"[TRT] [E] 3: [builderConfig.cpp::canRunOnDLA::382] Error C                                ode 3: API Usage Error (Parameter check failed at: optimizer/api/builderConfig.c                                pp::canRunOnDLA::382, condition: dlaEngineCount > 0Jetson Nano, Jetpack 4.6.1, docker l4t-mlWhat does this error mean? Any hints? Thx!Hi,
Please check the below links, as they might answer your concerns.This is the revision history of the NVIDIA TensorRT 8.4 Developer Guide.This is the revision history of the NVIDIA TensorRT 8.4 Developer Guide.This is the revision history of the NVIDIA TensorRT 8.4 Developer Guide.
Thanks!Did you manage to resolve the error? Help me if possiblePowered by Discourse, best viewed with JavaScript enabled"
45,cudnn-cant-use-tensorcore,"I try to use tensor core to compute int8 2D convolution forward, but it seems that cudnn tends to use non-tensorcore kernels at first. Who can help me, thanks!My GPU is 3090.I already set convolution math type to CUDNN_TENSOR_OP_MATH, and cudnnGetConvolutionForwardAlgorithm_v7 gives me available algo with mathType = CUDNN_TENSOR_OP_MATH. But when I call cudnnConvolutionForward, cudnn always invoke cuda core kernel s (same algo) (performance is pretty low).Only when I set tensor type to CUDNN_TENSOR_NCHW_VECT_C and data type to CUDNN_DATA_INT8x32, which is only supported by tensor core kernels, cudnn uses tensor core kernels.here is my code:Output:INFO: cudnn iconv algo total = 10
INFO: cudnn iconv found 10 algo
Algo = 1, Time = -1.000000 ms, Memory = 3934336 Bytes, Determinism = 1, MathType = 1
Algo = 1, Time = -1.000000 ms, Memory = 3934336 Bytes, Determinism = 1, MathType = 0
Algo = 0, Time = -1.000000 ms, Memory = 0 Bytes, Determinism = 1, MathType = 0
Algo = 2, Time = -1.000000 ms, Memory = 0 Bytes, Determinism = 1, MathType = 0
Algo = 5, Time = -1.000000 ms, Memory = 0 Bytes, Determinism = 1, MathType = 0
Algo = 4, Time = -1.000000 ms, Memory = 0 Bytes, Determinism = 1, MathType = 0
Algo = 7, Time = -1.000000 ms, Memory = 0 Bytes, Determinism = 1, MathType = 0
Algo = 6, Time = -1.000000 ms, Memory = 0 Bytes, Determinism = 1, MathType = 0
Algo = 3, Time = -1.000000 ms, Memory = 0 Bytes, Determinism = 1, MathType = 0
Algo = 7, Time = -1.000000 ms, Memory = 0 Bytes, Determinism = 1, MathType = 1So tensor core kernels (mathtype=1) are supported! But I can only specify algo in cudnnConvolutionForward, and cudnn will automatically use cuda core kernels. How can I make cudnn use tensor core kernels?Powered by Discourse, best viewed with JavaScript enabled"
46,attn-digits-is-no-longer-supported,"DIGITS is no longer supported, and NVIDIA will no longer respond to questions in the forums.Powered by Discourse, best viewed with JavaScript enabled"
47,distributed-data-parallel-training-fails-nccl-warn-error-ring-0-does-not-contain-rank-1,"I am trying to run a DDP training with 4 nodes, each with 1 GPU, I am using PyTorch Lightning framework with strategy = “ddp”, the backend is nccl. I have one NVIDIA RTX 3090 in each of the node.
NCCL version 2.14.3+cuda11.7GPU Type:  3090 RTX
Nvidia Driver Version: 515.86.01
CUDA Version: 11.7
CUDNN Version:
Operating System + Version: Ubuntu OSD, 20.04
Python Version (if applicable):
TensorFlow Version (if applicable):
PyTorch Version (if applicable):
Baremetal or Container (if container which image + tag):in each of the nodes i gave the following commands,
export MASTER_ADDR=(I use eno2 from my ifconfig)
export MASTER_PORT= xxxx (free port)
export WORLD_SIZE=4
export NODE_RANK=correspoding node rank in each node. (0 to 3)
then,
NCCL_DEBUG=INFO python3 LIGHTININGSCRIPT.pyKOR-C-008J2:546882:546882 [0] NCCL INFO Bootstrap : Using eno2:10.165.178.196<0>
KOR-C-008J2:546882:546882 [0] NCCL INFO NET/Plugin : No plugin found (libnccl-net.so), using internal implementation
KOR-C-008J2:546882:546882 [0] NCCL INFO cudaDriverVersion 11070
NCCL version 2.14.3+cuda11.7
KOR-C-008J2:546882:547125 [0] NCCL INFO Failed to open libibverbs.so[.1]
KOR-C-008J2:546882:547125 [0] NCCL INFO NET/Socket : Using [0]eno2:10.xxx.xxx.xxx<0> [1]br-fb29d128b7b0:192.168.49.1<0>
KOR-C-008J2:546882:547125 [0] NCCL INFO Using network Socket
KOR-C-008J2:546882:547125 [0] NCCL INFO Setting affinity for GPU 0 to 0fffffff
KOR-C-008J2:546882:547125 [0] NCCL INFO Channel 00/02 :    0   0   0   0KOR-C-008J2:546882:547125 [0] graph/rings.cc:51 NCCL WARN Error : ring 0 does not contain rank 1
KOR-C-008J2:546882:547125 [0] NCCL INFO graph/connect.cc:317 → 3
KOR-C-008J2:546882:547125 [0] NCCL INFO init.cc:759 → 3
KOR-C-008J2:546882:547125 [0] NCCL INFO init.cc:1089 → 3
KOR-C-008J2:546882:547125 [0] NCCL INFO group.cc:64 → 3 [Async thread]
KOR-C-008J2:546882:546882 [0] NCCL INFO group.cc:421 → 3
KOR-C-008J2:546882:546882 [0] NCCL INFO group.cc:106 → 3
KOR-C-008J2:546882:546882 [0] NCCL INFO comm 0x55ae17d27580 rank 0 nranks 4 cudaDev 0 busId 21000 - Abort COMPLETE
/home/vjj2kor/miniconda3/envs/control/lib/python3.8/site-packages/pytorch_lightning/plugins/training_type/ddp.py:512: UserWarning: Error handling mechanism for deadlock detection is uninitialized. Skipping check.
rank_zero_warn(“Error handling mechanism for deadlock detection is uninitialized. Skipping check.”)
Traceback (most recent call last):
File “main.py”, line 124, in 
trainer.fit(model, data)
File “/home/vjj2kor/miniconda3/envs/control/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py”, line 735, in fit
self._call_and_handle_interrupt(
File “/home/vjj2kor/miniconda3/envs/control/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py”, line 682, in _call_and_handle_interrupt
return trainer_fn(*args, **kwargs)
File “/home/vjj2kor/miniconda3/envs/control/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py”, line 770, in _fit_impl
self._run(model, ckpt_path=ckpt_path)
File “/home/vjj2kor/miniconda3/envs/control/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py”, line 1132, in _run
self._call_setup_hook()  # allow user to setup lightning_module in accelerator environment
File “/home/vjj2kor/miniconda3/envs/control/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py”, line 1428, in _call_setup_hook
self.training_type_plugin.barrier(“pre_setup”)
File “/home/vjj2kor/miniconda3/envs/control/lib/python3.8/site-packages/pytorch_lightning/plugins/training_type/ddp.py”, line 405, in barrier
torch.distributed.barrier(device_ids=self.determine_ddp_device_ids())
File “/home/vjj2kor/.local/lib/python3.8/site-packages/torch/distributed/distributed_c10d.py”, line 3145, in barrier
work = default_pg.barrier(opts=opts)
RuntimeError: NCCL error in: …/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1269, internal error, NCCL version 2.14.3
ncclInternalError: Internal check failed.
Last error:
Error : ring 0 does not contain rank 1In another node,
KOR-C-008J0:2448267:2448393 [0] bootstrap.cc:75 NCCL WARN Message truncated : received 988 bytes instead of 984
KOR-C-008J0:2448267:2448393 [0] NCCL INFO bootstrap.cc:413 → 3
KOR-C-008J0:2448267:2448393 [0] NCCL INFO init.cc:672 → 3
KOR-C-008J0:2448267:2448393 [0] NCCL INFO init.cc:904 → 3
KOR-C-008J0:2448267:2448393 [0] NCCL INFO group.cc:72 → 3 [Async thread]RuntimeError: NCCL error in: /opt/conda/conda-bld/pytorch_1659484810403/work/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1191, internal error, NCCL version 2.10.3
ncclInternalError: Internal check failed. This is either a bug in NCCL or due to memory corruptionncclInternalError: Internal check failed. This is either a bug in NCCL or due to memory corruptionHi @s.sivaramakrishnan ,This forum talks about issues related to TensorRT.I believe you can get better assistance on the respective forum.ThanksPowered by Discourse, best viewed with JavaScript enabled"
48,can-we-integrate-nxp-4k-mipi-cmos-camera-module-with-jetson-nano,"Is it possible to integrate NXP 4K-MIPI-CMOS-CAMERA-MODULE with Jetson Nano Developer kitHi,This looks like a Jetson issue. Please refer to the below samples in case useful.Hello AI World guide to deploying deep-learning inference networks and deep vision primitives with TensorRT and NVIDIA Jetson. - GitHub - dusty-nv/jetson-inference: Hello AI World guide to deployin...An easy to use PyTorch to TensorRT converter. Contribute to NVIDIA-AI-IOT/torch2trt development by creating an account on GitHub.For any further assistance, we will move this post to to Jetson related forum.Thanks!Powered by Discourse, best viewed with JavaScript enabled"
49,maxine-audio-effects-upload-of-training-data,"Hello,There was previously a feature to upload audio data to help improve/train the algorithms used in the noise suppression feature in MAXINE. This seems to have disappeared, is there still a away for the community to submit data?Powered by Discourse, best viewed with JavaScript enabled"
50,fp64-computation-on-budget,"Hi,I am looking for a GPU to speed up a simple, but very large, matrix multiplication:
Basically, we have a matrix A that is 70,000,000 x 15,000 (~10 TB) and want to calculate transpose(A)*A.I did some simple tests in Python with the cupy library and a 10,000,000x15,000 FP32 matrix and found the GPU to be multiple times faster. I know it will depend on the CPU and GPU, but let’s just work under the assumption that the GPU will provide a speed-up.Unfortunately, the data I work with is FP64, which means that the GPUs at my disposal are not a big help.I was hoping to get some advice on what GPU to buy if you want to do FP64 computations? The budget is around 2000-3000$.
I guess I could pick the GPU with the largest amount of GFLOPS (FP64) within my price range, but how does VRAM and transfer speeds come into play?Thank you :)As you are probably already aware, relative FP64 throughput can be gauged across the different SM Compute Capabilities here. Unfortunately it’s only the models ending in “X.0” or Tesla class, that perform well.For your budget, realistically a second hand V100 is about the only option.Powered by Discourse, best viewed with JavaScript enabled"
51,hello-when-i-run-gstreamer-it-gives-me-an-error-below-what-is-problem-it-works-with-9-cameras-but-when-i-increase-camera-count-it-gives-an-error,"TIME:Fri May 26 12:12:37 2023ERRORRRR from element source: !Could not write to resource.!TIME:Fri May 26 12:12:37 2023Error details: gstrtspsrc.c(7425): gst_rtspsrc_setup_streams_start (): /GstPipeline:streamer-v3-pipeline/GstBin:source-bin-07/GstURIDecodeBin:uri-decode-bin/GstRTSPSrc:source:Error (500): Internal Server ErrorTIME:Fri May 26 12:12:37 2023ERRORRRR from element source: !Could not read from resource.!TIME:Fri May 26 12:12:37 2023Error details: gstrtspsrc.c(5704): gst_rtspsrc_loop_udp (): /GstPipeline:streamer-v3-pipeline/GstBin:source-bin-07/GstURIDecodeBin:uri-decode-bin/GstRTSPSrc:source:Could not receive message. (System error)TIME:Fri May 26 12:12:37 2023ERRORRRR from element source: !Internal data stream error.!TIME:Fri May 26 12:12:37 2023Error details: gstrtspsrc.c(6056): gst_rtspsrc_loop (): /GstPipeline:streamer-v3-pipeline/GstBin:source-bin-07/GstURIDecodeBin:uri-decode-bin/GstRTSPSrc:source:streaming stopped, reason error (-5)Powered by Discourse, best viewed with JavaScript enabled"
52,debugging-custom-plugins-enqueue-function,"I was converting my onnx file which contains custom operation to tensorRT engine using trtexec. It seems like even though there is some bugs in my enqueue function trtexec generated the .engine file. When I use this for inference using TensorRT python API, the bug in my enqueue function appears and complains about cuDA illegal memory access. How can I debug my custom shared plugin library (to see where exactly the illegal access occurs) if I am running inference through python API?TensorRT Version: 8.2
GPU Type: RTX 3080 Laptop
Nvidia Driver Version:  530.41.03
CUDA Version: 12.1
CUDNN Version:
Operating System + Version: Ubuntu 20.04
Python Version (if applicable): 3.8
TensorFlow Version (if applicable): NA
PyTorch Version (if applicable): 1.12
Baremetal or Container (if container which image + tag): Container - nvcr.io/nvidia/pytorch:22.02-py3Hi,
Please refer to below links related custom plugin implementation and sample:This Samples Support Guide provides an overview of all the supported NVIDIA TensorRT 8.6.1 samples included on GitHub and in the product package. The TensorRT samples specifically help in areas such as recommenders, machine comprehension, character...TensorRT is an SDK for high performance, deep learning inference. It includes a deep learning inference optimizer and a runtime that delivers low latency and high throughput for deep learning…While IPluginV2 and IPluginV2Ext interfaces are still supported for backward compatibility with TensorRT 5.1 and 6.0.x respectively, however, we recommend that you write new plugins or refactor existing ones to target the IPluginV2DynamicExt or IPluginV2IOExt interfaces instead.Thanks!Hallo Aakansha,Thank you for the quick reply. I wrote my Plugin already based on IPluginV2DynamicExt using these 2 links, which were mentioned in many posts. I followed the steps mentioned inA simple tool that can generate TensorRT plugin code quickly. - GitHub - NVIDIA-AI-IOT/tensorrt_plugin_generator: A simple tool that can generate TensorRT plugin code quickly.to create a standalone shared library of the TensorRT Custom Plugin and linked it to trtexec using --plugin flag during conversion. During inference I load it in python using dll open command. The problem is I am not able to test my Enqueue function whether it is producing the expected result. Is there some way to test my plugin ?Hi,We recommend you to please try on the latest TensorRT version 8.6 (nvcr.io/nvidia/tensorrt:23.06-py3), if you still face the same issue,  please share with us the minimal issue repro model, scripts, and complete error logs for better debugging?Thank you.Powered by Discourse, best viewed with JavaScript enabled"
53,nvidia-riva-fails-to-infer-full-audio-chunk,"Hardware - GPU (GTX 3090)
Hardware - CPU
Operating System Ubuntu 20.04
Riva Version 2.9.0I am using silero-vad to create small chunks (15s max) of large audio and infer those chunks using RIVA. It fails to transcribe the complete audio, in a few chunks, it misses a couple of words(4-6 words or more) of that audio.How I can solve this issue to infer a long audio file.HI @shihab2Thanks for your interest in RivaCan you share with usThanksI figured out the problem, It was because of LM. In a few chunks, it fails to transcribe because of the probability threshold(beam_threshold=20), I reduce the value and make all the chunks in the range of 7s, solving my problem.Thanks @shihab2 for your kind feedback
Really AppreciatePowered by Discourse, best viewed with JavaScript enabled"
54,engine-upgrade,"A clear and concise description of the bug or issue.TensorRT Version: 8.4
GPU Type: NX
Nvidia Driver Version:
CUDA Version: 10.2
CUDNN Version: 8.0
Operating System + Version:
Python Version (if applicable): 3.6
TensorFlow Version (if applicable):
PyTorch Version (if applicable): 1.7
Baremetal or Container (if container which image + tag):Please attach or include links to any models, data, files, or scripts necessary to reproduce your issue. (Github repo, Google Drive, Dropbox, etc.)I have several engine files generated from tensorrt 7.1.3 and tensorflow, now I need to convert them for tensorrt 8.5, is that possible if I have no original weights files?Hi ,
We recommend you to check the supported features from the below link.These support matrices provide a look into the supported platforms, features, and hardware capabilities of the NVIDIA TensorRT 8.6.0 Early Access (EA) APIs, parsers, and layers.
You can refer below link for all the supported operators list.
For unsupported operators, you need to create a custom plugin to support the operationThanks!Thank you. I am checking.Powered by Discourse, best viewed with JavaScript enabled"
55,text-to-speech-tts-websockets-example,"Please provide the following information when requesting support.Hello, I am trying to connect Riva TTS with IBM voice gateway, I need TTS to be able to communicate through websockets. Is there an example in node.js of TTS implementation using websockets?Hi @nharoThanks for your interest in RivaWe have websocket-bridge implementation available with usWebsockets <-> Riva proxy service. Audiocodes compatible. - GitHub - nvidia-riva/websocket-bridge: Websockets <-> Riva proxy service. Audiocodes compatible.if the above is not that your are looking for ,please let me know, I will check further with Riva TeamThanksHi, that github has the implementation of websockets only for ASR, it’s not what I’m looking for.Powered by Discourse, best viewed with JavaScript enabled"
56,whether-it-is-worthwhile-to-create-cuda-c-for-cnn-or-whether-torch-cuda-is-sufficient,"I’m new and have been learning cuda C. I have a minor question: because I know that pytorch.cuda is essentially a wrapper for Cuda , is it worthwhile to utilize Cuda C  to train a simple CNN or torch.cuda is enoughHi @ngogiahuy20012004 ,
This forum talks about cudnn related issues.
Cuda or Pytorch forums might be able to assist better.ThanksPowered by Discourse, best viewed with JavaScript enabled"
57,tensorrt-8-memory-usage-with-dynamic-input-shapes,"During integration of dynamic shape support for a detection algorithm, I’ve encountered an interesting behavior of TensorRT. It seems that the device memory consumption depends on the maximum input size across all optimization profiles and, particularly, not limited to the currently used profile or the currently used input resolution. I’ve tested this behavior by adding 3 profiles, selecting the 2nd profile and tweaking the max size for the 3rd profile (the actual input shape and 1/2nd profiles are kept unchanged).So, it looks like device memory is allocated according to the worst possible case (ie the upper boundary on the input shape across all profiles). Is this understanding correct? If so, is it possible to work around the limitation (for example, creating engine without memory and plugging a sufficient workspace buffer as needed)?P.S. The motivation is to allow working with a wide range of input resolutions, but pay (in terms of memory) for large resolutions only when we’ve encountered such large resolution.TensorRT Version: v8.2.2.1
GPU Type: RTX 2070
Nvidia Driver Version: 470.63.01
CUDA Version: v11.4.3
CUDNN Version: v8.2.4.15Any news on the topic?Hi,Sorry for the delay in addressing this issue. We will get back to you in 1 or 2 days.
Thank you.Hi,Following may help you.
https://docs.nvidia.com/deeplearning/tensorrt/api/c_api/classnvinfer1_1_1_i_cuda_engine.html#ab431cff77cee511d37747920f6c2276fThank you.Hi, @spolisettyThank you for replying.I’ve looked into the suggested API. As I understand, I’ll need to use IExecutionContext::setDeviceMemory() [1] method to provide my own device memory buffer.  The documentation of this method states that I would need at least ICudaEngine::getDeviceMemorySize() [2] bytes. But getDeviceMemorySize() does not takes any parameter that would allow me to select a particular optimization profile. And API for choosing optimization profiles is present in IExecutionContext class [3] and not ICudeEngine.In short, it seems that I can use my own device buffer, but its size does not depend on the optimization profile (and corresponds to the same worst-case profile logic). So, I’m effectively in the same sitation as without custom device buffers. Or did I miss something?[1]https://docs.nvidia.com/deeplearning/tensorrt/api/c_api/classnvinfer1_1_1_i_execution_context.html#a12f8214ba871e63ec9c4dac970bb9c39
[2]https://docs.nvidia.com/deeplearning/tensorrt/api/c_api/classnvinfer1_1_1_i_cuda_engine.html#a5dbb256ba0555c4e58eac5e4b876c7ee
[3]https://docs.nvidia.com/deeplearning/tensorrt/api/c_api/classnvinfer1_1_1_i_execution_context.html#a74c361a3d93e70a3164988df7d60a4ccHi,Good question, yes you’re right. Always allocating for the worst case is a simplifying assumption in the executor - we’re assuming that the application would need to  budget memory for your worst expected case. But this assumption isn’t always valid.Thank you.Hi, @spolisettyThank you for the information. Can we expect any improvements on the subject in the foreseeable future?Yes, this may be improved in the future releases.Thank you.Okay, thank you!Hi! Are there any advancements on the topic? TensorRT significantly reduces memory consumption for my model, but allocating memory for the worst case negates all improvements:(
I would like to be able to independently control the required memory.Bumping this, as this topic is of high concern to me. In my use case, I am using triton to serve ~40 computer vision models (mainly CNNs). The VRAM usage is much higher when converting the models to TensorRT engines as compared to when they were normal Tensorflow models, to the point that some models cannot be initialized when all could previously. This is quite a bummer as TensorRT was able to provide quite a decent speedup (~5x) to most of the models through my experiments.Powered by Discourse, best viewed with JavaScript enabled"
58,the-end-user-license-agreement-link-is-broken,"Hi, I would like to confirm the license for cuOpt, but when I try to access the URL below, it does not appear to be displayed.https://docs.nvidia.com/cuopt/NVIDIA_cuOpt_EULA.pdfCould you tell me where I can find the EULA?The EULA is available here now https://docs.nvidia.com/cuopt/pdfs/NVIDIA-cuOpt-EULA.pdfThis topic was automatically closed 14 days after the last reply. New replies are no longer allowed.Powered by Discourse, best viewed with JavaScript enabled"
59,nemo-and-4090,"I’ve been trying to work through this tutorial for Nemo:https://developer.nvidia.com/blog/deploying-a-1-3b-gpt-3-model-with-nvidia-nemo-megatron/ which suggested this in the System Requirements:However, when I tried to run the model conversion on my 4090-equipped Ubuntu 20.04 box, I got this error:The containers I pulled for training and inference had the tags 22.08.01-py3 and 22.08-py3, respectively.The article does suggest getting the latest tags, but I wasn’t sure how to find the latest tags.So, my question is, how do I find the latest tags?  And, is Nemo really not supported on the 4090?Thanks!Hi @rbgreenwayThanks for your interest in Riva,The request seems to be more inclined towards Nemo,I will try to check internally and get answers, in meantime i highly recommend to start a discussion or post your question in this below Nemo github linkExplore the GitHub Discussions forum for NVIDIA NeMo. Discuss code, ask questions & collaborate with the developer community.ThanksThis topic was automatically closed 14 days after the last reply. New replies are no longer allowed.Powered by Discourse, best viewed with JavaScript enabled"
60,tensorrt-models-not-compatible-with-on-another-device,"I have converted an onnx model to TRT 8.5.2 using trtexec.
the conversion occurred on a server and could run inference on NVIDIA GPU.
However, when deploying the model on Jetson AGX orin device, it shows error and cannot run.
Is it necessary to convert a model on the same device where inference will run?Is it necessary to convert a model on the same device where inference will run?Yes. For Jetson we need to build the TensorRT engine on the same device.
For more information, please refer to the following similar post:Thank you.This topic was automatically closed 14 days after the last reply. New replies are no longer allowed.Powered by Discourse, best viewed with JavaScript enabled"
61,memory-issue,"
image4032×3024 2.5 MB


image4032×3024 4.31 MB

How to clear memory?like there are no process running in the background but still the memory used is 1.6gb.Hello,Can you please tell me how this relates to NVIDIA? Maybe I can find a better category for your topic.Thanks,
TomPowered by Discourse, best viewed with JavaScript enabled"
62,rtx3090-vs-a100,"RTX3090 bad results comparing A100 detectionsTensorRT Version: 8.4.1.1
GPU Type: RTX3090 (on desktop) and A100 (on virtual machine)
Nvidia Driver Version: 525.60.13
CUDA Version: 12.0 (the docker images says it uses 11.7)
CUDNN Version: 8.4.1.67
Operating System + Version: Ubuntu 20.04
Python Version (if applicable): python3.8
TensorFlow Version (if applicable):
PyTorch Version (if applicable):
Baremetal or Container (if container which image + tag):I trained a yolov4 model on the desktop PC (RTX3090) with tao toolkit.
I converted the .etlt file to RTX3090 compatible tensorrt file (model.plan) on the desktop PC, inside a docker image. Then I converted the .etlt file to A100 compatible tensorrt file (model.plan) on the VM in the same docker image.I use the same triton based docker image for inferencing on desktop PC (RTX3090) and on the VM (A100) too. The RTX3090 detections are much more poor, not just the confidence levels are lower for the same object, but many times missing the object that has high confidence when I infer on A100 VM. A100 detections are acceptable, but RTX3090 is really noisy. Is this normal? Any advice how top improve the RTX3090 detections with the same model?Hi,This looks like a TAO Toolkit related issue. We will move this post to the TAO Toolkit forum.Thanks!I am not sure. Can you confirm that I should get the same detections on RTX3090 and on T4 also if I use the same CUDA driver?Powered by Discourse, best viewed with JavaScript enabled"
63,cuda-version-affects-inference-results-during-batching,"We are running an application in an ubuntu jammy docker image on an NVIDIA Quadro P2200, with driver version 515.65.01 and CUDA 11.7. We are doing classification using Linear regression, and are experiencing instabilites with the results when we do inference depending on the CUDA version we are running.We perform inference in batching, hence we process 8 lines from the image at a time. The issue only occurs when we perform inference during batching. Deactivating batching fixes the problem, but we are dependent on enabling batching due to processing restrictions. The inference model is an ONNX model loaded with opencv and generated from tensorflow. Running inference in python with onnxruntime yields correct results, ruling out any possibility that the problem comes from the model itself.The issue shows itself in that the classified images sent through inference do not yield updated results, ie, the annotated pixels are not updated and performed inference on. Instead, the pixels of the resulting image are all annotated to one class(defined as foreign) and are never changed no matter how many images we annotate and send through inference.We have encountered this issue for a while, and have tested the application on multiple computers with varying CUDA versions. The interesting thing is that most of the computers were able to obtain correct results with CUDA 10.2, and some also for CUDA 11.4. All other versions trigger the inference issue during batching.We get no warning or error message in our logs, and there are no indications that there is a mismatch between any packages or extensions.Is there anyone here who has encountered the same issue before - or might have some insight that is helpful to debug this problem?Hi @helene.minge.olsen ,Request you to raise the concern on CUDA Platform to get better assistanceThank youPowered by Discourse, best viewed with JavaScript enabled"
64,fail-in-convertion-maskrcnn-onnx-model-to-tensorrt-pad-string,"When I try to convert my model from onnx to tensorrt format I get the same error, I even tried to apply polygraphy but I did not get good results. To make the conversion from pytorch to onnx I used the opset version number 16:[02/11/2023-15:24:59] [E] [TRT] ModelImporter.cpp:775: input:“/transform/Gather_output_0”
input: “/transform/Cast_17_output_0”
input: “”
output: “/transform/Pad_output_0”
name: “/transform/Pad”
op_type: “Pad”
attribute {
name: “mode”
s: “constant”
type: STRING
}TensorRT Version:  8.4.3.1
GPU Type:  T4
Nvidia Driver Version: 510.47.03
CUDA Version: 11.6
CUDNN Version:  8.4
Operating System + Version:  Ubuntu 20.04.5
PyTorch Version: 1.13.1model in onnx formart: model.onnxafter applying polygraphy: model_san.onnxlogs-fileHi,We recommend that you try the most recent TensorRT version 8.5.3.
I hope the following similar posts will help you.Thank you.I tried with TensorRT version 8.5.3, now the error is as follows:any ideas or workaround you can suggest? this is with the model after applying polygraphy, thanks a lot!Hi,We could reproduce a different error on 8.5.3 version.
Please allow us some time to work on this issue.Thank you.I’ll be waiting, thanks a lot!Hi @isaac21 ,
Pytorch MaskRCNN has known issues when importing into TensorRT.
We are working on addressing this issue, and shall be available in future release.
Please stay tuned.ThanksI have seen that this problem has been going on for a while. Any possible release date?Hi @isaac21 , apologies for the inconvenience caused.The team is already working on the fixes, and we are targeting to provide a fix in upcoming releases.Thank you.any updates?any updates?Powered by Discourse, best viewed with JavaScript enabled"
65,cudnn-installing-error,"I am following the NVIDIA CUDNN documentation https://docs.nvidia.com/deeplearning/cudnn/install-guide/index.html#package-manager-ubuntu-install to install cudnn on my linux machine (Ubuntu20.04). However when i go to run → the following is outputted:Could anyone help with this?
Thanks.Hi,Could you please try the following.
https://chrisjean.com/fix-apt-get-update-the-following-signatures-couldnt-be-verified-because-the-public-key-is-not-available/Thank you.The same problem like above, but
sudo apt-key adv --keyserver keyserver.ubuntu.com --recv-keys 4EBDF3392063C34EreturnsPowered by Discourse, best viewed with JavaScript enabled"
66,trtexec-fails-with-null-pointer-exception-when-usedlacore-enabled,"trtexec fails with null pointer exception when useDLACore enabled
AGX Orin
TensorRT 8517
Linux Artax 5.10.104-tegra #1 SMP PREEMPT Wed Aug 10 20:17:07 PDT 2022 aarch64 aarch64 aarch64 GNU/Linux
Ubuntu “20.04.5 LTS (Focal Fossa)”
Jetpack 5.0.2 - L4T 35.1.0Full error: [03/02/2023-09:19:38] [W] --workspace flag has been deprecated by --memPoolSize - Pastebin.comTo reproduce:
using this source model:
rvm_mobilenetv3_fp32_input.onnx (14.3 MB)execute command:
trtexec --onnx=rvm_mobilenetv3_fp32_input.onnx --workspace=8000 --saveEngine=rvm_mobilenetv3_fp32_output.engine --verbose --useDLACore=0 --allowGPUFallbackresult:Hi,
Please refer to the installation steps from the below link if in case you are missing on anythingThis NVIDIA TensorRT 8.5.3 Installation Guide provides the installation requirements, a list of what is included in the TensorRT package, and step-by-step instructions for installing TensorRT.
Also, we suggest you to use TRT NGC containers to avoid any system dependency related issues.NVIDIA TensorRT is a C++ library that facilitates high-performance inference on NVIDIA graphics processing units (GPUs). TensorRT takes a trained network and produces a highly optimized runtime engine that performs inference for that network.Thanks!Hmmm, trtexec fails withwhen I attempt to build using the container nvcr.io/nvidia/tensorrt:23.02-py3.Even attempting to build the bundled python deps using /opt/tensorrt/python/python_setup.sh fails with : tech@Artax:/opt/metamirror/src/resources/builder$ ./test.shSending build conte - Pastebin.comI resolved the build failure by adding the tegra-gl folder into the LD_LIBRARY_PATH; however, the resulting model’s inference is dismal.
6.58 FPS with the DLACore enabled, 42.71 FPS without. Also with the DLACore enabled TensorRT seems to be creating an arbitrary GL context preventing business logic from creating one resulting in application crash.Correct me if I am wrong, but I was under the impression that running mixed precision across the dep learning cores with gpu fallback was expected to increase the performance of inference. (puzzled).What I am seeing seems to mirror what’s mentioned here: Run pure conv2d node on DLA makes GPU get slower - #10 by AastaLLLHi @manbehindthemadness ,Are you still facing the issue.ThanksThe issue itself (null pointer) was resolved by including the GL libraries in the LD_Library_Path; however, running small-batch real time inference is still painfully slow with mixed precision. Are the DLAs designed for training acceleration only?@manbehindthemadness
Please check out the DLA github page for samples and resources: Recipes and tools for running deep learning workloads on NVIDIA DLA cores for inference applications.We have a FAQ page that addresses some common questions that we see developers run into: Deep-Learning-Accelerator-SW/FAQThanks, I will read these resources and see what I can come up with.
Out of curiosity, where does the DLA exist physically? Is it part of the existing GPU/Tensor core arrangement, or does it have it’s own silicon?It has its own silicon.Oh, wow. So the 200 and change TOPS rating for the Orin module is derived from the DLA cores? Eg, without optimizing the engines to make use of them a vast portion of the potential performance would remain untapped… Definitely good to know.One final question, will optimizing my engines to make use of the DLA cores provide meaningful computation advantages when used in a real-time single-batch inference, or are they designed exclusively for multi-batch training applications?Sorry for the confusion - GPU/Tensor cores offer lot of the AI compute ( 2/3rds on the Jetson Orin AGX SoC) and the two DLA cores offer the rest 1/3. So, in cases where we dont have any DLA core like Nano, all of the AI compute is from GPU.DLA cores are optimized for real time inference.Thanks for the clarificationPowered by Discourse, best viewed with JavaScript enabled"
67,model-training-accuracy-issue,"Hi,
We are trying to detect foreign objects by using custom model. The model has 5 classes and for each class we have taken 48 images. We have trained our ssd model by using jetson-train github repo for around 1200 epoch. However at 1200 epoch our loss was around 1.64. My questions are:Thanks.Hi @CostGazelle ,
I would suggest you raise the concern on Jetson Platform to get better assistance
ThanksPowered by Discourse, best viewed with JavaScript enabled"
68,detectron-2-onnx-tensorrt-bug-keyerror-unknown-scalar-when-export-model-py,"1、I guarantee I followed the instructions on the README carefully.2、The only difference is I used my custom data, I’m wondering if the conversion can’t be done after using the custom data?
（According to my understanding, after using custom dataset, only the weight of the model has changed, but the structure of the model has not changed, so Detectron 2 → ONNX → TensorRT should work as well. I don’t know if the current conversion can only be done for Mask R-CNN R50-FPN 3x? Or we can also convert the transfer learning model with custom dataset?）.3、.yaml file changed a little bit for transfer learning.--------------My Code--------------------------%run /content/detectron2/tools/deploy/export_model.py \ --sample-image /content/new_1344_1344.jpg \ --config-file /content/output.yaml \ --export-method tracing \ --format onnx \ --output /content \ MODEL.WEIGHTS /content/output/model_final.pth \ MODEL.DEVICE cuda--------------My Code----------------------------------------The Bug--------------------------`KeyError Traceback (most recent call last)/content/detectron2/tools/deploy/export_model.py in
224 exported_model = export_scripting(torch_model)
225 elif args.export_method == “tracing”:
 → 226 exported_model = export_tracing(torch_model, sample_inputs)
227
228 # run evaluation with the converted model8 frames[/usr/local/lib/python3.8/dist-packages/torch/onnx/symbolic_opset9.py] in to(g, self, *args)
1994 # aten::to(Tensor, Tensor, bool, bool, memory_format)
1995 dtype = args[0].type().scalarType()
 → 1996 return g.op(“Cast”, self, to_i=sym_help.cast_pytorch_to_onnx[dtype])
1997 else:
1998 # aten::to(Tensor, ScalarType, bool, bool, memory_format)KeyError: ‘UNKNOWN_SCALAR’`--------------The Bug----------------------------------------Environment Information--------------------------PyTorch version: 1.10.1+cu111
Is debug build: False
CUDA used to build PyTorch: 11.1
ROCM used to build PyTorch: N/AOS: Ubuntu 20.04.5 LTS (x86_64)
GCC version: (Ubuntu 9.4.0-1ubuntu1~20.04.1) 9.4.0
Clang version: 10.0.0-4ubuntu1
CMake version: version 3.22.6
Libc version: glibc-2.31Python version: 3.8.10 (default, Nov 14 2022, 12:59:47) [GCC 9.4.0] (64-bit runtime)
Python platform: Linux-5.10.147±x86_64-with-glibc2.29
Is CUDA available: True
CUDA runtime version: 11.2.152
CUDA_MODULE_LOADING set to:
GPU models and configuration: GPU 0: Tesla T4
Nvidia driver version: 510.47.03
cuDNN version: Probably one of the following:
/usr/lib/x86_64-linux-gnu/libcudnn.so.8.1.1
/usr/lib/x86_64-linux-gnu/libcudnn_adv_infer.so.8.1.1
/usr/lib/x86_64-linux-gnu/libcudnn_adv_train.so.8.1.1
/usr/lib/x86_64-linux-gnu/libcudnn_cnn_infer.so.8.1.1
/usr/lib/x86_64-linux-gnu/libcudnn_cnn_train.so.8.1.1
/usr/lib/x86_64-linux-gnu/libcudnn_ops_infer.so.8.1.1
/usr/lib/x86_64-linux-gnu/libcudnn_ops_train.so.8.1.1
HIP runtime version: N/A
MIOpen runtime version: N/A
Is XNNPACK available: TrueCPU:
Architecture: x86_64
CPU op-mode(s): 32-bit, 64-bit
Byte Order: Little Endian
Address sizes: 46 bits physical, 48 bits virtual
CPU(s): 2
On-line CPU(s) list: 0,1
Thread(s) per core: 2
Core(s) per socket: 1
Socket(s): 1
NUMA node(s): 1
Vendor ID: GenuineIntel
CPU family: 6
Model: 85
Model name: Intel(R) Xeon(R) CPU @ 2.00GHz
Stepping: 3
CPU MHz: 2000.210
BogoMIPS: 4000.42
Hypervisor vendor: KVM
Virtualization type: full
L1d cache: 32 KiB
L1i cache: 32 KiB
L2 cache: 1 MiB
L3 cache: 38.5 MiB
NUMA node0 CPU(s): 0,1
Vulnerability Itlb multihit: Not affected
Vulnerability L1tf: Mitigation; PTE Inversion
Vulnerability Mds: Vulnerable; SMT Host state unknown
Vulnerability Meltdown: Vulnerable
Vulnerability Mmio stale data: Vulnerable
Vulnerability Retbleed: Vulnerable
Vulnerability Spec store bypass: Vulnerable
Vulnerability Spectre v1: Vulnerable: __user pointer sanitization and usercopy barriers only; no swapgs barriers
Vulnerability Spectre v2: Vulnerable, IBPB: disabled, STIBP: disabled, PBRSB-eIBRS: Not affected
Vulnerability Srbds: Not affected
Vulnerability Tsx async abort: Vulnerable
Flags: fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ss ht syscall nx pdpe1gb rdtscp lm constant_tsc rep_good nopl xtopology nonstop_tsc cpuid tsc_known_freq pni pclmulqdq ssse3 fma cx16 pcid sse4_1 sse4_2 x2apic movbe popcnt aes xsave avx f16c rdrand hypervisor lahf_lm abm 3dnowprefetch invpcid_single ssbd ibrs ibpb stibp fsgsbase tsc_adjust bmi1 hle avx2 smep bmi2 erms invpcid rtm mpx avx512f avx512dq rdseed adx smap clflushopt clwb avx512cd avx512bw avx512vl xsaveopt xsavec xgetbv1 xsaves arat md_clear arch_capabilitiesVersions of relevant libraries:
[pip3] mypy-extensions==1.0.0
[pip3] numpy==1.21.6
[pip3] torch==1.10.1+cu111
[pip3] torchaudio==0.10.1+rocm4.1
[pip3] torchsummary==1.5.1
[pip3] torchtext==0.14.1
[pip3] torchvision==0.11.2+cu111
[conda] Could not collect--------------Environment Information--------------------------Hi,We recommend you please reach out to Issues · NVIDIA/TensorRT · GitHub to get better help regarding the above sample setup.Thank you.Hi dear Moderator,I have already tried to solve this problem for a month…     But still didn’t achieve.I already reach out to Issues · NVIDIA/TensorRT · GitHub , but no one answer.Could you please help me…Hi @user58864I encounter exactly the same issue
Did you manage to solve ?Powered by Discourse, best viewed with JavaScript enabled"
69,how-to-apply-rights-to-access-the-page-https-catalog-ngc-nvidia-com-orgs-nvidia-teams-riva-resources-speechtotext-citrinet-notebook,"Dear,
When I’m trying to access the page: https://catalog.ngc.nvidia.com/orgs/nvidia/teams/riva/resources/speechtotext_citrinet_notebook, 404 error reported, and it seems a permissions issue, I need to join an org named nvidia or a team named riva, right? But how to join, could you please give me a guideline?HI @174362510Thanks for your interest in RivaApologies, That links seems to be not presentPlease use the below linkEnd to End workflow for speech to text citrinet training with TAO Toolkit and deployment using Riva.ThanksThis topic was automatically closed 14 days after the last reply. New replies are no longer allowed.Powered by Discourse, best viewed with JavaScript enabled"
70,can-cudnn8-run-on-tesla-card-410-396-driver-with-cuda10-2-docker,"docker nvidia/cuda:10.2-cudnn7-devel-ubuntu18.04 supports 396, 410 …
ENV NVIDIA_REQUIRE_CUDA=cuda>=10.2 brand=tesla,driver>=396,driver<397 brand=tesla,driver>=410,driver<411 brand=tesla,driver>=418,driver<419 brand=tesla,driver>=440,driver<441
while in docker  nvidia/cuda:10.2-cudnn8-devel-ubuntu18.04
ENV NVIDIA_REQUIRE_CUDA=cuda>=10.2 brand=tesla,driver>=418,driver<419
does this mean cudnn8 can’t run on 396 410 driver?
in cudnn docs, i didn’t see any driver requirements, there’s only cuda version requirements
i m also wondering if the nvidia/cuda:10.2-cudnn7 docker support driver 384.111+
because triton-server:20.03.1 support driver 384.111+ and it is base on cuda 10.2Hi @763730968
please refer to the below link -These support matrices provide a look into the supported versions of the OS, NVIDIA CUDA, the CUDA driver, and the hardware for the NVIDIA cuDNN 8.8.0 release.
where , For the CUDA driver requirements for a given CUDA Toolkit version, refer to the CUDA Compatibility documentation.thanks,
but i want to know the supported versions NVIDIA CUDA, the CUDA driver, and the hardware for the NVIDIA cuDNN 8.0.0 not 8.8.0
and cuda10.2.89 with cudnn7.2.1Hi @763730968 ,
If I understood the query correct, you can find them here.ThanksPowered by Discourse, best viewed with JavaScript enabled"
71,inference-with-different-cudnn-versions-gives-different-outputs,"Hi All,
I’m running a freezed pytorch model (in eval mode).
I get different output when running with different cudnn versions ~1e-5.
Looks like the difference starts from the first conv2d layer, and aggregates to a bigger change in output.used flages (that didn’t helped):
torch.backends.cudnn.deterministic = True
torch.backends.cudnn.benchmark = FalseTested on Ubuntu20.04:
torch 1.13, cuda11.7: cudnn8700, cudnn8902, cudnn8500.PS. I’ve also tried using different cuda 11.7 vs. 11.6, but haven’t observed the differences reported above.Hi @valtmaneddy ,different cudnn versions ~1e-5If anything in the hardware-software stack changes then floating-point arithmetic results cannot (currently) be guaranteed to be bit-identical. Changes in hardware-software stack include but are not limited to: chip architecture, chip version within architecture, amount of system memory, driver version, CUDA version, NCCL version, DL library version, CPU version, other software package version such as Numpy or other libraries, chip and/or node physical interconnect, multi-node/multi-GPU regime. To guarantee bit-exact results it’s necessary to freeze the software stack using containers and freeze the hardware versions, including all components involved in compute (CPU, GPU, DPU, interconnect, etc)ThanksPowered by Discourse, best viewed with JavaScript enabled"
72,tensorrt-inference,"Hello,I have trained the DeTr model with a custom data set. Then I converted the DeTr model to a TensorRT model to achieve a faster inference time. As a test, I wrote a script inside the target system to infer only one image.The problem is that the bounding boxes are not within a range of [0, 1] (see example below).
Do you know what could be the reason for this?For reference:preprocess:model deserialize:memory allocation:image:inference:output:Hi @ozan.anli ,
Can you please help us with the environment details,
Also the onnx model and reproducible script for the same.ThanksPowered by Discourse, best viewed with JavaScript enabled"
73,how-does-tensorrt-use-host-memory-ram-at-runtime,"According to the TensorRT documentation, you can expect high host memory (RAM) usage during the build phase. Then lower host memory usage during runtime. This is what’s I’d expect, as inference should mostly use device (GPU) memory. This is also corroborated here, which implies I can expect a fixed amount of host memory usage at runtime, the variable amount is in the build stage.However this is not something I’ve experienced using the TensorRT library. Our system uses relatively large amounts (~4GB) of host memory during runtime. I’ve also been able to replicate this using trtexec. We can actually observe a slight increase in host memory usage, after the build stage has finished using trtexec.Here’s the trtexec example, you can use any onnx model:I’ve also tried deserializing the runtime from an .engine plan file instead, to see if it was a result of the ONNX building stage. But it still uses the same amount of host memory during runtime.This is running on a dGPU, not jetson, so the host/device memory are separate. We are running a large/complex model with a large workspace, in case this is a factor on runtime host memory usage.TensorRT Version:  8.0.0
GPU Type: NVIDIA GeForce RTX 3070 Laptop GPU
Operating System + Version: Ubuntu 20.04Hi,
Request you to share the ONNX model and the script if not shared already so that we can assist you better.
Alongside you can try few things:This NVIDIA TensorRT 8.6.1 Quick Start Guide is a starting point for developers who want to try out TensorRT SDK; specifically, this document demonstrates how to quickly construct an application to run inference on a TensorRT engine.import sys
import onnx
filename = yourONNXmodel
model = onnx.load(filename)
onnx.checker.check_model(model).
2) Try running your model with trtexec command.master/samples/trtexecNVIDIA® TensorRT™, an SDK for high-performance deep learning inference, includes a deep learning inference optimizer and runtime that delivers low latency and high throughput for inference applicat...
In case you are still facing issue, request you to share the trtexec “”–verbose"""" log for further debugging
Thanks!This has been improved in TRT8.3, which now gives more control over how memory is allocated using setMemoryPoolLimit()Powered by Discourse, best viewed with JavaScript enabled"
74,nvcc-fatal-unsupported-gpu-architecture-compute-86,"root@ae2267ce9d1c:/opt/py-faster-rcnn/caffe-fast-rcnn/build# make -j20 && make pycaffe
[  0%] Running C++/Python protocol buffer compiler on /opt/py-faster-rcnn/caffe-fast-rcnn/src/caffe/proto/caffe.proto
Scanning dependencies of target proto
[  1%] Building CXX object src/caffe/CMakeFiles/proto.dir///include/caffe/proto/caffe.pb.cc.o
[  1%] Linking CXX static library …/…/lib/libproto.a
[  1%] Built target proto
[  2%] Building NVCC (Device) object src/caffe/CMakeFiles/cuda_compile.dir/util/cuda_compile_generated_im2col.cu.o
[  2%] Building NVCC (Device) object src/caffe/CMakeFiles/cuda_compile.dir/util/cuda_compile_generated_math_functions.cu.o
[  2%] Building NVCC (Device) object src/caffe/CMakeFiles/cuda_compile.dir/solvers/cuda_compile_generated_rmsprop_solver.cu.o
[  4%] Building NVCC (Device) object src/caffe/CMakeFiles/cuda_compile.dir/layers/cuda_compile_generated_inner_product_layer.cu.o
[  4%] Building NVCC (Device) object src/caffe/CMakeFiles/cuda_compile.dir/layers/cuda_compile_generated_reduction_layer.cu.o
[  4%] Building NVCC (Device) object src/caffe/CMakeFiles/cuda_compile.dir/layers/cuda_compile_generated_detection_output_layer.cu.o
[  4%] Building NVCC (Device) object src/caffe/CMakeFiles/cuda_compile.dir/layers/cuda_compile_generated_power_layer.cu.o
[  5%] Building NVCC (Device) object src/caffe/CMakeFiles/cuda_compile.dir/layers/cuda_compile_generated_deconv_layer.cu.o
[  5%] Building NVCC (Device) object src/caffe/CMakeFiles/cuda_compile.dir/layers/cuda_compile_generated_slice_layer.cu.o
[  5%] Building NVCC (Device) object src/caffe/CMakeFiles/cuda_compile.dir/layers/cuda_compile_generated_permute_layer.cu.o
[  8%] Building NVCC (Device) object src/caffe/CMakeFiles/cuda_compile.dir/layers/cuda_compile_generated_prelu_layer.cu.o
[  8%] Building NVCC (Device) object src/caffe/CMakeFiles/cuda_compile.dir/layers/cuda_compile_generated_concat_layer.cu.o
[  8%] Building NVCC (Device) object src/caffe/CMakeFiles/cuda_compile.dir/util/cuda_compile_generated_bbox_util.cu.o
[  8%] Building NVCC (Device) object src/caffe/CMakeFiles/cuda_compile.dir/layers/cuda_compile_generated_normalize_layer.cu.o
[  8%] Building NVCC (Device) object src/caffe/CMakeFiles/cuda_compile.dir/layers/cuda_compile_generated_lstm_unit_layer.cu.o
[  8%] Building NVCC (Device) object src/caffe/CMakeFiles/cuda_compile.dir/layers/cuda_compile_generated_cudnn_softmax_layer.cu.o
[  9%] Building NVCC (Device) object src/caffe/CMakeFiles/cuda_compile.dir/layers/cuda_compile_generated_cudnn_sigmoid_layer.cu.o
[  9%] Building NVCC (Device) object src/caffe/CMakeFiles/cuda_compile.dir/layers/cuda_compile_generated_eltwise_layer.cu.o
[ 10%] Building NVCC (Device) object src/caffe/CMakeFiles/cuda_compile.dir/layers/cuda_compile_generated_recurrent_layer.cu.o
[ 10%] Building NVCC (Device) object src/caffe/CMakeFiles/cuda_compile.dir/layers/cuda_compile_generated_crop_layer.cu.o
nvcc fatal   : Unsupported gpu architecture ‘compute_86’
CMake Error at cuda_compile_generated_detection_output_layer.cu.o.cmake:207 (message):
Error generating
/opt/py-faster-rcnn/caffe-fast-rcnn/build/src/caffe/CMakeFiles/cuda_compile.dir/layers/./cuda_compile_generated_detection_output_layer.cu.onvcc fatal   : Unsupported gpu architecture ‘compute_86’
CMake Error at cuda_compile_generated_inner_product_layer.cu.o.cmake:207 (message):
Error generating
/opt/py-faster-rcnn/caffe-fast-rcnn/build/src/caffe/CMakeFiles/cuda_compile.dir/layers/./cuda_compile_generated_inner_product_layer.cu.onvcc fatal   : Unsupported gpu architecture ‘compute_86’
CMake Error at cuda_compile_generated_rmsprop_solver.cu.o.cmake:207 (message):
Error generating
/opt/py-faster-rcnn/caffe-fast-rcnn/build/src/caffe/CMakeFiles/cuda_compile.dir/solvers/./cuda_compile_generated_rmsprop_solver.cu.osrc/caffe/CMakeFiles/caffe.dir/build.make:98: recipe for target ‘src/caffe/CMakeFiles/cuda_compile.dir/layers/cuda_compile_generated_detection_output_layer.cu.o’ failed
make[2]: *** [src/caffe/CMakeFiles/cuda_compile.dir/layers/cuda_compile_generated_detection_output_layer.cu.o] Error 1
make[2]: *** Waiting for unfinished jobs…
nvcc fatal   : Unsupported gpu architecture ‘compute_86’
nvcc fatal   : Unsupported gpu architecture ‘compute_86’
nvcc fatal   : Unsupported gpu architecture ‘compute_86’
nvcc fatal   : Unsupported gpu architecture ‘compute_86’
CMake Error at cuda_compile_generated_im2col.cu.o.cmake:207 (message):
Error generating
/opt/py-faster-rcnn/caffe-fast-rcnn/build/src/caffe/CMakeFiles/cuda_compile.dir/util/./cuda_compile_generated_im2col.cu.osrc/caffe/CMakeFiles/caffe.dir/build.make:189: recipe for target ‘src/caffe/CMakeFiles/cuda_compile.dir/layers/cuda_compile_generated_inner_product_layer.cu.o’ failed
make[2]: *** [src/caffe/CMakeFiles/cuda_compile.dir/layers/cuda_compile_generated_inner_product_layer.cu.o] Error 1
CMake Error at cuda_compile_generated_power_layer.cu.o.cmake:207 (message):
Error generating
/opt/py-faster-rcnn/caffe-fast-rcnn/build/src/caffe/CMakeFiles/cuda_compile.dir/layers/./cuda_compile_generated_power_layer.cu.oCMake Error at cuda_compile_generated_reduction_layer.cu.o.cmake:207 (message):
Error generating
/opt/py-faster-rcnn/caffe-fast-rcnn/build/src/caffe/CMakeFiles/cuda_compile.dir/layers/./cuda_compile_generated_reduction_layer.cu.oCMake Error at cuda_compile_generated_math_functions.cu.o.cmake:207 (message):
Error generating
/opt/py-faster-rcnn/caffe-fast-rcnn/build/src/caffe/CMakeFiles/cuda_compile.dir/util/./cuda_compile_generated_math_functions.cu.osrc/caffe/CMakeFiles/caffe.dir/build.make:511: recipe for target ‘src/caffe/CMakeFiles/cuda_compile.dir/solvers/cuda_compile_generated_rmsprop_solver.cu.o’ failed
make[2]: *** [src/caffe/CMakeFiles/cuda_compile.dir/solvers/cuda_compile_generated_rmsprop_solver.cu.o] Error 1
nvcc fatal   : Unsupported gpu architecture ‘compute_86’
nvcc fatal   : Unsupported gpu architecture ‘compute_86’
CMake Error at cuda_compile_generated_slice_layer.cu.o.cmake:207 (message):
Error generating
/opt/py-faster-rcnn/caffe-fast-rcnn/build/src/caffe/CMakeFiles/cuda_compile.dir/layers/./cuda_compile_generated_slice_layer.cu.oCMake Error at cuda_compile_generated_deconv_layer.cu.o.cmake:207 (message):
Error generating
/opt/py-faster-rcnn/caffe-fast-rcnn/build/src/caffe/CMakeFiles/cuda_compile.dir/layers/./cuda_compile_generated_deconv_layer.cu.osrc/caffe/CMakeFiles/caffe.dir/build.make:140: recipe for target ‘src/caffe/CMakeFiles/cuda_compile.dir/layers/cuda_compile_generated_power_layer.cu.o’ failed
make[2]: *** [src/caffe/CMakeFiles/cuda_compile.dir/layers/cuda_compile_generated_power_layer.cu.o] Error 1
src/caffe/CMakeFiles/caffe.dir/build.make:70: recipe for target ‘src/caffe/CMakeFiles/cuda_compile.dir/util/cuda_compile_generated_math_functions.cu.o’ failed
make[2]: *** [src/caffe/CMakeFiles/cuda_compile.dir/util/cuda_compile_generated_math_functions.cu.o] Error 1
src/caffe/CMakeFiles/caffe.dir/build.make:63: recipe for target ‘src/caffe/CMakeFiles/cuda_compile.dir/util/cuda_compile_generated_im2col.cu.o’ failed
make[2]: *** [src/caffe/CMakeFiles/cuda_compile.dir/util/cuda_compile_generated_im2col.cu.o] Error 1
src/caffe/CMakeFiles/caffe.dir/build.make:91: recipe for target ‘src/caffe/CMakeFiles/cuda_compile.dir/layers/cuda_compile_generated_reduction_layer.cu.o’ failed
make[2]: *** [src/caffe/CMakeFiles/cuda_compile.dir/layers/cuda_compile_generated_reduction_layer.cu.o] Error 1
nvcc fatal   : Unsupported gpu architecture ‘compute_86’
nvcc fatal   : Unsupported gpu architecture ‘compute_86’
nvcc fatal   : Unsupported gpu architecture ‘compute_86’
CMake Error at cuda_compile_generated_permute_layer.cu.o.cmake:207 (message):
Error generating
/opt/py-faster-rcnn/caffe-fast-rcnn/build/src/caffe/CMakeFiles/cuda_compile.dir/layers/./cuda_compile_generated_permute_layer.cu.oCMake Error at cuda_compile_generated_prelu_layer.cu.o.cmake:207 (message):
Error generating
/opt/py-faster-rcnn/caffe-fast-rcnn/build/src/caffe/CMakeFiles/cuda_compile.dir/layers/./cuda_compile_generated_prelu_layer.cu.oCMake Error at cuda_compile_generated_concat_layer.cu.o.cmake:207 (message):
Error generating
/opt/py-faster-rcnn/caffe-fast-rcnn/build/src/caffe/CMakeFiles/cuda_compile.dir/layers/./cuda_compile_generated_concat_layer.cu.onvcc fatal   : Unsupported gpu architecture ‘compute_86’
src/caffe/CMakeFiles/caffe.dir/build.make:161: recipe for target ‘src/caffe/CMakeFiles/cuda_compile.dir/layers/cuda_compile_generated_slice_layer.cu.o’ failed
make[2]: *** [src/caffe/CMakeFiles/cuda_compile.dir/layers/cuda_compile_generated_slice_layer.cu.o] Error 1
src/caffe/CMakeFiles/caffe.dir/build.make:105: recipe for target ‘src/caffe/CMakeFiles/cuda_compile.dir/layers/cuda_compile_generated_deconv_layer.cu.o’ failed
make[2]: *** [src/caffe/CMakeFiles/cuda_compile.dir/layers/cuda_compile_generated_deconv_layer.cu.o] Error 1
nvcc fatal   : Unsupported gpu architecture ‘compute_86’
CMake Error at cuda_compile_generated_eltwise_layer.cu.o.cmake:207 (message):
Error generating
/opt/py-faster-rcnn/caffe-fast-rcnn/build/src/caffe/CMakeFiles/cuda_compile.dir/layers/./cuda_compile_generated_eltwise_layer.cu.oCMake Error at cuda_compile_generated_bbox_util.cu.o.cmake:207 (message):
Error generating
/opt/py-faster-rcnn/caffe-fast-rcnn/build/src/caffe/CMakeFiles/cuda_compile.dir/util/./cuda_compile_generated_bbox_util.cu.osrc/caffe/CMakeFiles/caffe.dir/build.make:147: recipe for target ‘src/caffe/CMakeFiles/cuda_compile.dir/layers/cuda_compile_generated_prelu_layer.cu.o’ failed
make[2]: *** [src/caffe/CMakeFiles/cuda_compile.dir/layers/cuda_compile_generated_prelu_layer.cu.o] Error 1
src/caffe/CMakeFiles/caffe.dir/build.make:154: recipe for target ‘src/caffe/CMakeFiles/cuda_compile.dir/layers/cuda_compile_generated_permute_layer.cu.o’ failed
make[2]: *** [src/caffe/CMakeFiles/cuda_compile.dir/layers/cuda_compile_generated_permute_layer.cu.o] Error 1
src/caffe/CMakeFiles/caffe.dir/build.make:84: recipe for target ‘src/caffe/CMakeFiles/cuda_compile.dir/layers/cuda_compile_generated_concat_layer.cu.o’ failed
make[2]: *** [src/caffe/CMakeFiles/cuda_compile.dir/layers/cuda_compile_generated_concat_layer.cu.o] Error 1
nvcc fatal   : Unsupported gpu architecture ‘compute_86’
CMake Error at cuda_compile_generated_lstm_unit_layer.cu.o.cmake:207 (message):
Error generating
/opt/py-faster-rcnn/caffe-fast-rcnn/build/src/caffe/CMakeFiles/cuda_compile.dir/layers/./cuda_compile_generated_lstm_unit_layer.cu.osrc/caffe/CMakeFiles/caffe.dir/build.make:119: recipe for target ‘src/caffe/CMakeFiles/cuda_compile.dir/layers/cuda_compile_generated_eltwise_layer.cu.o’ failed
make[2]: *** [src/caffe/CMakeFiles/cuda_compile.dir/layers/cuda_compile_generated_eltwise_layer.cu.o] Error 1
nvcc fatal   : Unsupported gpu architecture ‘compute_86’
src/caffe/CMakeFiles/caffe.dir/build.make:77: recipe for target ‘src/caffe/CMakeFiles/cuda_compile.dir/util/cuda_compile_generated_bbox_util.cu.o’ failed
make[2]: *** [src/caffe/CMakeFiles/cuda_compile.dir/util/cuda_compile_generated_bbox_util.cu.o] Error 1
CMake Error at cuda_compile_generated_normalize_layer.cu.o.cmake:207 (message):
Error generating
/opt/py-faster-rcnn/caffe-fast-rcnn/build/src/caffe/CMakeFiles/cuda_compile.dir/layers/./cuda_compile_generated_normalize_layer.cu.onvcc fatal   : Unsupported gpu architecture ‘compute_86’
nvcc fatal   : Unsupported gpu architecture ‘compute_86’
CMake Error at cuda_compile_generated_crop_layer.cu.o.cmake:207 (message):
Error generating
/opt/py-faster-rcnn/caffe-fast-rcnn/build/src/caffe/CMakeFiles/cuda_compile.dir/layers/./cuda_compile_generated_crop_layer.cu.oCMake Error at cuda_compile_generated_recurrent_layer.cu.o.cmake:207 (message):
Error generating
/opt/py-faster-rcnn/caffe-fast-rcnn/build/src/caffe/CMakeFiles/cuda_compile.dir/layers/./cuda_compile_generated_recurrent_layer.cu.onvcc fatal   : Unsupported gpu architecture ‘compute_86’
src/caffe/CMakeFiles/caffe.dir/build.make:175: recipe for target ‘src/caffe/CMakeFiles/cuda_compile.dir/layers/cuda_compile_generated_lstm_unit_layer.cu.o’ failed
make[2]: *** [src/caffe/CMakeFiles/cuda_compile.dir/layers/cuda_compile_generated_lstm_unit_layer.cu.o] Error 1
CMake Error at cuda_compile_generated_cudnn_softmax_layer.cu.o.cmake:207 (message):
Error generating
/opt/py-faster-rcnn/caffe-fast-rcnn/build/src/caffe/CMakeFiles/cuda_compile.dir/layers/./cuda_compile_generated_cudnn_softmax_layer.cu.onvcc fatal   : Unsupported gpu architecture ‘compute_86’
CMake Error at cuda_compile_generated_cudnn_sigmoid_layer.cu.o.cmake:207 (message):
Error generating
/opt/py-faster-rcnn/caffe-fast-rcnn/build/src/caffe/CMakeFiles/cuda_compile.dir/layers/./cuda_compile_generated_cudnn_sigmoid_layer.cu.osrc/caffe/CMakeFiles/caffe.dir/build.make:112: recipe for target ‘src/caffe/CMakeFiles/cuda_compile.dir/layers/cuda_compile_generated_normalize_layer.cu.o’ failed
make[2]: *** [src/caffe/CMakeFiles/cuda_compile.dir/layers/cuda_compile_generated_normalize_layer.cu.o] Error 1
src/caffe/CMakeFiles/caffe.dir/build.make:133: recipe for target ‘src/caffe/CMakeFiles/cuda_compile.dir/layers/cuda_compile_generated_cudnn_softmax_layer.cu.o’ failed
make[2]: *** [src/caffe/CMakeFiles/cuda_compile.dir/layers/cuda_compile_generated_cudnn_softmax_layer.cu.o] Error 1
src/caffe/CMakeFiles/caffe.dir/build.make:182: recipe for target ‘src/caffe/CMakeFiles/cuda_compile.dir/layers/cuda_compile_generated_recurrent_layer.cu.o’ failed
make[2]: *** [src/caffe/CMakeFiles/cuda_compile.dir/layers/cuda_compile_generated_recurrent_layer.cu.o] Error 1
src/caffe/CMakeFiles/caffe.dir/build.make:168: recipe for target ‘src/caffe/CMakeFiles/cuda_compile.dir/layers/cuda_compile_generated_crop_layer.cu.o’ failed
make[2]: *** [src/caffe/CMakeFiles/cuda_compile.dir/layers/cuda_compile_generated_crop_layer.cu.o] Error 1
src/caffe/CMakeFiles/caffe.dir/build.make:126: recipe for target ‘src/caffe/CMakeFiles/cuda_compile.dir/layers/cuda_compile_generated_cudnn_sigmoid_layer.cu.o’ failed
make[2]: *** [src/caffe/CMakeFiles/cuda_compile.dir/layers/cuda_compile_generated_cudnn_sigmoid_layer.cu.o] Error 1
CMakeFiles/Makefile2:272: recipe for target ‘src/caffe/CMakeFiles/caffe.dir/all’ failed
make[1]: *** [src/caffe/CMakeFiles/caffe.dir/all] Error 2
Makefile:127: recipe for target ‘all’ failed
make: *** [all] ErSame Error， Any Solution?Hi @pavankumarsharma995 ,
This forum talks about issues related to Tensorrt,
However which CUDA version are you using?
Maybe you need to build only for lower SMs for existing cuda version.
If the issue persist, you may reach out to CUDA Forum.ThanksPowered by Discourse, best viewed with JavaScript enabled"
75,e0310-0743-295347-1-logging-cc-43-1-stdarchivereader-cpp-54-error-code-1-serialization-serialization-assertion-sizeread,"• Hardware Platform ( A10 PCIe )
• NVIDIA GPU Driver Version - 510.85.02
• CUDA 11.6
• TENSORRT8.2.3
An error is reported when deploying the model in tritonserver.The following is the error report content

image2072×1093 194 KB
E0310 07:37:43.295347 1 logging.cc:43] 1: [stdArchiveReader.cpp::StdArchiveReader::54] Error Code 1: Serialization (Serialization assertion sizeRead == static_cast<uint64_t>(mEnd - mCurrent) failed.Size specified in header does not match archive size)
E0310 07:37:43.295377 1 logging.cc:43] 4: [runtime.cpp::deserializeCudaEngine::50] Error Code 4: Internal Error (Engine deserialization failed.)
I0310 07:37:43.307673 1 tensorrt.cc:5343] TRITONBACKEND_ModelInstanceFinalize: delete instance state
I0310 07:37:43.307731 1 tensorrt.cc:5282] TRITONBACKEND_ModelFinalize: delete model state
E0310 07:37:43.308175 1 model_repository_manager.cc:1152] failed to load ‘firesmoke5’ version 1: Internal: unable to create TensorRT engineHi,
Please refer to below links related custom plugin implementation and sample:This Samples Support Guide provides an overview of all the supported NVIDIA TensorRT 8.5.3 samples included on GitHub and in the product package. The TensorRT samples specifically help in areas such as recommenders, machine comprehension, character...TensorRT is an SDK for high performance, deep learning inference. It includes a deep learning inference optimizer and a runtime that delivers low latency and high throughput for deep learning…While IPluginV2 and IPluginV2Ext interfaces are still supported for backward compatibility with TensorRT 5.1 and 6.0.x respectively, however, we recommend that you write new plugins or refactor existing ones to target the IPluginV2DynamicExt or IPluginV2IOExt interfaces instead.Thanks!thank you
This issue has been resolved because the environment settings were incorrect during Tensorrt installation. The issue was resolved after I reset the environmentPowered by Discourse, best viewed with JavaScript enabled"
76,run-peoplenet-tranformer-on-tensorrt,"I am trying to run PeopleNet Transformer using TensorRT. I’ve gotten PeopleNet to work fine with TensorRT but I can’t generate a .engine file for PeopleNet Transformer.I’ve downloaded resnet50_peoplenet_transformer.etlt from here but when I try to convert the model with the following command:./tlt-converter resnet50_peoplenet_transformer.etlt -k nvidia_tao -d 3,544,960I get the following error:So it seems TensorRT doesn’t support INT64 but the documentation claims PeopleNet Transformer can be run on TensorRT. So I am wondering how do I work around this? How do I convert the model to a .engine file so that can be run with TensorRT?Help appreciated.Hi @jedda ,
I believe you can get better assistance on Deepstream Forum on the topic.
ThanksThis topic was automatically closed 14 days after the last reply. New replies are no longer allowed.Powered by Discourse, best viewed with JavaScript enabled"
77,a-weird-bug-two-similar-onnx-but-one-engine-with-bug,"I have two simple onnx files, say o1.onnx and o3.onnx. o1.onnx is a subgraph of o3.onnx, where the only difference is that o3.onnx adds two more outputs than o1.onnx.When I transform o1.onnx to trt engine, everything works fine. However, when I transform o3.onnx to trt engine, the engine outputs large error.official docker container 22.12Related files: https://cloud.tsinghua.edu.cn/f/09c8c8a1d6a44fa0915a/?dl=1When BASE='o1', the max error is just 9e-6, while when BASE='o3', the max error is 30+.Moreover, the error only produced by my input npy file (which is the real input for my model). If I use polygraph run command, the output is normal.Hi,
Request you to share the ONNX model and the script if not shared already so that we can assist you better.
Alongside you can try few things:This NVIDIA TensorRT 8.5.3 Quick Start Guide is a starting point for developers who want to try out TensorRT SDK; specifically, this document demonstrates how to quickly construct an application to run inference on a TensorRT engine.import sys
import onnx
filename = yourONNXmodel
model = onnx.load(filename)
onnx.checker.check_model(model).
2) Try running your model with trtexec command.master/samples/trtexecNVIDIA® TensorRT™, an SDK for high-performance deep learning inference, includes a deep learning inference optimizer and runtime that delivers low latency and high throughput for inference applicat...
In case you are still facing issue, request you to share the trtexec “”–verbose"""" log for further debugging
Thanks!related files are in this link: https://cloud.tsinghua.edu.cn/f/09c8c8a1d6a44fa0915a/?dl=1Hi, I still face this issue. It’s werid because the downstream onnx node can affect upstream ones. Is there any bug with TensorRT?Powered by Discourse, best viewed with JavaScript enabled"
78,torchvision-faster-rcnn-failed-to-convert-to-tensorrt-engine,"When converting my onnx model to TensorRT engine (via pre-built trtexec), I ran into a number of issues and eventually the error “kOPT values for profile 0 violate shape constraints: If_525_OutputLayer dimensions not compatible for if-conditional outputs Condition ‘==’ violated”. This error took place not only in my minorly custom model, but also in torchvision’s trained fasterrcnn_resnet50_fpn.TensorRT Version: 8.5.3.1+cuda11.8
GPU Type: 1 NVIDIA GeForce GTX 1080
Nvidia Driver Version: 525.78.01
CUDA Version: 11.8
CUDNN Version: 8.6
Operating System + Version: Ubuntu 20.04
Python Version (if applicable): 3.10.8
TensorFlow Version (if applicable):
PyTorch Version (if applicable):  1.13.1 (Torchvision: 0.14.1)
Baremetal or Container (if container which image + tag):I am more than happy to provide relevant artifacts (e.g., onnx models) via private messages.Context: My model is a torchvision Faster R-CNN model where I used ResNet10 as the backbone, and configured the RCNN head to predict boxes of a single category C. Under pytorch, this means having class “background” and class “C”.  The trained pytorch model was first converted into onnx format via torch.onnx.export(…) and then into TensorRT engine. Different opset_version (11-17) all led to the same outcome.Step1: When converting the onnx model into TensorRT engine (via ./trtexec --onnx=model.onnx --saveEngine=model.trt), I came across the following error:Step 2: Following the hinted solution, I applied constant folding through Polygraphy surgeon sanitize (the same command as in the official example). When executing the folded onnx with trtexec again, I faced the following:Step 3: From the above error and some digging through the internet, it appears that TensorRT does not cope well with wildcard Reshape (i.e., -1). As I could infer the exact tensor shape at roi_heads, I replaced -1s by pre-determined values in the codes of pytorch’s Faster RCNN. This remedy resolved the reshape issue. The newly generated (and then Polygraphy folded) onnx model, however, raised the current issue when being converted into trt engine.After visualizing my onnx model with netron, it appears the above error was raised at which pytorch carries out NMS in RPN (for reducing number of proposals). Specifically, I deduced that the error took place at the line “if  boxes.numel() == 0” (where pytorch behaves differently when there remain zero proposals to filter). Nevertheless, I am not able to grasp how this line would raise a shape constraint error for TensorRT, nor did I see a similar post. I confirm that both my pytorch and onnx models generate reasonable outputs.I appreciate any suggestions and am happy to provide relevant artifacts/logs. Thanks in advance!Hi,Could you please share with us the above issue repro ONNX model here or via DM to try from our end for better debugging.Thank you.Hi,Thank you for your previous response!I shared with you a google drive link with access to  my onnx models via DM. Wondering if you had a chance to take a look?Thanks again!Powered by Discourse, best viewed with JavaScript enabled"
79,what-is-kernel-auto-tuning-i-need-more-information,"Continuing the discussion from what is `Kernel Auto-Tuning` and `Multi-Stream Execution`?:I really don’t understand about kernel auto-tuning, that topic can’t solve the problem.
I need more information about that content, it’s better to have a detailed example attached.Powered by Discourse, best viewed with JavaScript enabled"
80,how-to-submit-your-own-ideas-for-data-science-of-the-day," Data Science of the Day  is a fun way to share new, or perhaps even novel perspectives on data science related topics.The criteria for an idea to be shared:If you have a suggestion, just reply to this thread with the information outlined and it will get taken into consideration for posting.After you are logged in, you can subscribe to the  Data Science of the Day , click the little notification bell near the upper right and updates can be sent directly to your inbox.Note: A new post will be made every business weekday, and it will be published at 9a in the Eastern Time zone.Powered by Discourse, best viewed with JavaScript enabled"
81,make-default-context-wasnt-able-to-create-a-context,"I am facing the issue“RuntimeError: make_default_context() wasn’t able to create a context on any of the 1 detected devices “This process runs in kubernetes inside a pod. Sometimes restarting the pod’s resolves the issue and sometimes we need to move the process to a different gpu.TensorRT Version: tensorrt==7.1.3.4
GPU Type: T4
Nvidia Driver Version: 460.32.03
CUDA Version: 11.2
CUDNN Version: 8.0.4
Operating System + Version: Ubuntu 18.04.6 LTS
Python Version (if applicable): 3.6.9
TensorFlow Version (if applicable):
PyTorch Version (if applicable): 1.4.0
Baremetal or Container (if container which image + tag):Getting error on the import(pycuda.autoinit) itselfIt seems that PyCUDA is unable to detect any available devices to create a context on, and so it cannot initialize. Check that PyCUDA is properly installed and configured on your system.
You can try reinstalling PyCUDA and making sure that your environment variables are set up correctly.
Also, you can try using CUDA from the torch and removing Python.
https://pytorch.org/docs/stable/cuda.htmlThe issue is not cuda driver because a simple restart of pods resolves this  issue temporary.Nvidia team kindly suggest permanent solution for this.
Reinstalling of drivers is not the way forward.Regards,
Amit Dubea simple restart of pods resolves this issue temporary.This could be due to not having enough free GPU memory available to create a new context.
Check the usage of the GPU to know if it is overloaded or if there is not enough free memory available.
You can also check for other processes using the GPU and terminate them if necessary.Thank you.Powered by Discourse, best viewed with JavaScript enabled"
82,int8-calibration-in-python-with-tensorrt-8-6,"Hello,I’m trying to quantize in INT8 YOLOX_Darknet from ONNX, using TensorRT 8.6 in Python.Running it in TF32 or FP16 is totally fine.I found various calibrators but they are all outdated and using apparently deprecated code, like :-how to use tensorrt int8 to do network calibration | C++ Python. Computer Vision Deep Learning | KeZunLin's Blog (from 2018, error is “[ERROR] Exception caught in get_batch(): TypeError: PythonEntropyCalibrator.get_batch() missing 1 required positional argument: ‘names’” while building serialized network) → I searched and found this thread int8 calibration,meet error get_batch() takes 2 positional arguments but 3 were given, but the “newer” versions (seemingly from TRT 5) doesn’t work for me.-tensorrt-utils/int8/calibration/SimpleCalibrator.py at master · rmccorm4/tensorrt-utils · GitHub (from 2020, but it uses “config.get_calibration_profile()” which uses directly the “config” while in my understanding the calibrator was to be added to the config and not the opposite)-How to deploy an ONNX model with int8 calibration? · Issue #557 · NVIDIA/TensorRT · GitHub This thread gives dead link but my question is very close from this one, and at the time answers were available.-TensorRT/samples/python/int8_caffe_mnist/calibrator.py at 498dcb009fe4c2dedbe9c61044d3de4f3c04a41b · NVIDIA/TensorRT · GitHub is the most official python sample I found, I tried to tweak it’s not super clear what is missing or not working.-Finally, TensorRT/samples/sampleINT8 at master · NVIDIA/TensorRT · GitHub is the official sample but it’s in C++.So my question is, what should be in a calibrator, and do NVIDIA has an up to date example ? the documentation (Developer Guide :: NVIDIA Deep Learning TensorRT Documentation) is pretty small so I’d like to have more informations about how to create a calibrator in python.TensorRT Version: 8.6
GPU Type: RTX3060
Nvidia Driver Version: latest as of 23/06/2023
CUDA Version: 11.8
Operating System + Version: Win11
Python Version (if applicable): 3.10Official YOLOX_darknet ONNX from YOLOX repo : YOLOX/demo/ONNXRuntime at main · Megvii-BaseDetection/YOLOX · GitHubCreate a script to quantize the model from ONNX to TRT using INT8 precision.Hi, Please refer to the below links to perform inference in INT8master/samples/sampleINT8NVIDIA® TensorRT™, an SDK for high-performance deep learning inference, includes a deep learning inference optimizer and runtime that delivers low latency and high throughput for inference applicat...1777.21 KB
Thanks!Hello,Thank you for your answer, but I already mentionned this github in my original post, the sample is only in C++.
I guess this means that there is no up-to-date sample in Python ?And is it even possible to do it in Python with TensorRT 8.6, as many functions are now deprecated ?ThanksI guess this means that there is no up-to-date sample in Python ?Hi,Please to the following samples (build_engine.py) which may help you.main/samples/python/tensorflow_object_detection_apiNVIDIA® TensorRT™, an SDK for high-performance deep learning inference, includes a deep learning inference optimizer and runtime that delivers low latency and high throughput for inference applicat...main/samples/python/efficientnetNVIDIA® TensorRT™, an SDK for high-performance deep learning inference, includes a deep learning inference optimizer and runtime that delivers low latency and high throughput for inference applicat...main/samples/python/detectron2NVIDIA® TensorRT™, an SDK for high-performance deep learning inference, includes a deep learning inference optimizer and runtime that delivers low latency and high throughput for inference applicat...main/samples/python/efficientdetNVIDIA® TensorRT™, an SDK for high-performance deep learning inference, includes a deep learning inference optimizer and runtime that delivers low latency and high throughput for inference applicat...Thank you.Thank you ! Yeah the beginning of the script in build_python.py is exactly what I was looking for, an up-to-date sample of calibator to see what is expected in python.This topic was automatically closed 14 days after the last reply. New replies are no longer allowed.Powered by Discourse, best viewed with JavaScript enabled"
83,can-maxine-audio-effect-sdk-be-used-for-offline-audio-processing,"Can MAXINE Audio effect SDK be used for offline audio processing, like denoising m4a files?Hi there @seriphuspan and welcome to the NVIDIA developer forums.I hope it is ok if I pass this post on to our MAXINE experts? We have a dedicated category for this SDK.I am sure there will be someone able to help.Thanks!Yes, Maxine’s SDK has a sample application that leverages the background noise removal model.Powered by Discourse, best viewed with JavaScript enabled"
84,does-riva-support-nvidia-rtx-4090-gpu,"According to the documents ‘‘Riva is supported on any NVIDIA Volta or later GPU (NVIDIA Turing and NVIDIA Ampere GPU architecture)’’, NVIDIA RTX 4090 or other 40 series GPU are not listed as a Preffered Deployment Platform. Does 4090 with 24GB memory work on Riva?HI @lsmaThanks for your interest in RivaGenerally we list the Server level GPUs in the docsFor Riva, 16+ GB VRAM is recommended. Since 4090 has 24GB, it should work with RivaSo feel free to use 4090 for RivaThanksPowered by Discourse, best viewed with JavaScript enabled"
85,converting-pt-to-tensorrt-engine,"I have my own pretrained pytorch model that I want to convert to a TensorRT model (.engine), I run this python script:import torch
from torch2trt import torch2trt
model = torch.load(‘/home/tto/himangy_mt_server/OpenNMT-py/models/1.pt’,  map_location=torch.device(‘cpu’))
x = torch.ones((1, 3, 224, 224)).to(torch.device(‘cpu’))
m = torch2trt(model, )got this error
Traceback (most recent call last):
File “/home/tto/himangy_mt_server/OpenNMT-py/convert.py”, line 11, in 
m = torch2trt(model, )
File “/home/tto/himangy_mt_server/himangy-env/lib/python3.9/site-packages/torch2trt-0.4.0-py3.9.egg/torch2trt/torch2trt.py”, line 694, in torch2trt
outputs = module(*inputs)
TypeError: ‘dict’ object is not callablewhen i print the keys of the model i am getting 5 keys
dict[‘model’,‘generator’,‘vocab’,‘opt’,‘optim’]i tired of taking the model key in the dictionary like this
“”““model = model_dict[‘model’]””“”""import torch
from torch2trt import torch2trt
model = torch.load(‘/home/tto/himangy_mt_server/OpenNMT-py/models/1.pt’,  map_location=torch.device(‘cpu’))
model = model_dict[‘model’]
x = torch.ones((1, 3, 224, 224)).to(torch.device(‘cpu’))
m = torch2trt(model, )still getting same error, what causes this error??Hi,Please refer to the example here.An easy to use PyTorch to TensorRT converter. Contribute to NVIDIA-AI-IOT/torch2trt development by creating an account on GitHub.If you still face the issue, you can also try the Pytorch model → ONNX model → TensorRT conversion.This NVIDIA TensorRT 8.6.1 Quick Start Guide is a starting point for developers who want to try out TensorRT SDK; specifically, this document demonstrates how to quickly construct an application to run inference on a TensorRT engine.Thank you.Powered by Discourse, best viewed with JavaScript enabled"
86,maxine-ar-linux-documentation,"Hi, I am looking for Maxine AR linux documnetation, but couldn’t find it anywhere.Would be of great help if you could mention some links for it.
ThanksPowered by Discourse, best viewed with JavaScript enabled"
87,aigseffectsapp-fails-while-loading-trtmodel,"Hello,I’m using NVIDIA_VFX_SDK_Ubuntu18.04_0.6.5.0.tgz on Ubuntu-20.04 with NVIDIA GeForce RTX 3050 Ti Laptop GPU.I’ve also installedI am able to build the VideoFX samples and run them.Please don’t say to go use the container! I’m actually writing another app that needs to use this functionality.Thank you,
NolanI should correct the above. I’m on Ubuntu 21.10.I’ve also tried various other combinations such as cuda-11-6, TensorRT-8.4.0.6.Linux.x86_64-gnu.cuda-11.6.cudnn8.3.tar.gz, cudnn-linux-x86_64-8.4.0.27_cuda11.6-archive.tar.xz.Any help would be appreciated.Thank you,
NolanHi @user64586,
Did you solve the problem? I am facing the same issue.ThanksHello l.mikloskoUnfortunately not. I stuck to Ubuntu 20.04. I was also not able to use Ubuntu 22.04.It seems that there is a very hard requirement on the exact distribution version to be used, while using the libraries. There are some newer updates that are worth trying. Let me know.Thanks,
NolanHave the same issue.I have Ubuntu 20.04.5 LTS and am trying SuperRes.@l.miklosko @user64586 did you have any progress on this?Unfortunately not. I followed all the memos, although had to alter the installation process as I was using Mint - but guessed it would be the same as it uses Ubuntu repositories anyway. Strangely enough, I was able to run it in Windows 11 without much trouble. What a strange times we live in.Yeah. I ended up asking my friend to run it on Windows.i have the same trouble, my envs is CUDA 12.0 TensorRT 8.6.0.12 cudnn is 8.9.0 , and my exec can run in windows 11 sucessfully ,my program is run by 3rd party and i create the same env in linux ,but i just get error Serialization (Serialization assertion plan->header.magicTag == rt::kPLAN_MAGIC_TAG failed.) ,Can somebody help plz!!!Powered by Discourse, best viewed with JavaScript enabled"
88,route-solution-never-uses-more-than-one-vehicle,"Hi,
I’m using cuOpt for the first time and I’m overall a beginner with routing problems. I have a problem where I have 3 depots and want to service a few hundred clients from them. The main question is which depot to assign to each client, but a good route is great too.The solution I get looks nice, but only uses one depot. I realized that the default objective function puts the vehicle count first, so I tried changing that to TRAVEL_TIME or CUMUL_PACKAGE_TIME but it’s still only using one depot.My preferred objective would be to minimize the maximum of the total cost assigned to a depot. So the depot (vehicle) with the largest cost has as little cost as possible. But I’d be happy with anything that splits up the clients between the vehicles. Why is it only using one vehicle?The objective function gets used after initial phase where the optimization happens to use minimum vehicles. Since there is no time restriction on when the order needs to be delivered, a single vehicle is being used to deliver all. May be you can try using min_vehicles options. In that case you will get a solution with minimum vehicles being used. Another option would be max cost per vehicle, with which you can limit cost per vehicle.Awesome, thanks! Check out my routes!

image950×567 128 KB
This topic was automatically closed 14 days after the last reply. New replies are no longer allowed.Powered by Discourse, best viewed with JavaScript enabled"
89,tensorrt-fp16-conversion-issue,"•	Hardware Platform (GPU): Tesla T4
•	DeepStream Version: 6.2
•	TensorRT Version: 8.5.2.2-1+cuda11.8
•	NVIDIA GPU Driver Version: 515.65.01
•	Issue Type: QuestionHello,We have a custom onnx FP32 model. We are trying to convert it to tensorrt FP16. We are converting the model with this command.While we check model_b16.plan with this flag (–loadEngine), It is showing FP32. It’s not converting in FP16.So, Kindly guide us on how we can convert our custom onnx FP32 model into tensorrt FP16.Thanks,
Dax JainMoving to TensorRT forum for better support.@yingliu and Nvidia support team,
Kindly give us your thought on this raised query.Hi @daxjain987 ,
Any model can be converted to TRT FP16 by setting builder flag to use FP16
Are you using custom onnx operator? If yes, you will need to implement TRT plugin.ThanksAny model can be converted to TRT FP16 by setting builder flag to use FP16Yes, We did that.
Kindly look at that command which I have mentioned. We have followed the same.Are you using custom onnx operator?No, we don’t.
We have used Tensorrt from this linkHello @AakankshaS ,We are trying to convert the model.onnx FP32 to a model.plan FP16. And for that, we are using “trtexec” for FP16 model conversion.While we check the precision of the converted FP16 model using “–loadEngine” flag, it’s showing us FP32 only.Kindly let us know how to convert our model to FP16 or FP32+FP16 precision only.Thanks
Dax JainHi @daxjain987 ,
Can you share the onnx model with us, so that we can try reproducing the issue from our end?ThanksHi @AakankshaS ,You can download the model from here.We have tried this model to covert it in the FP16 model with this commandAfter this conversion, we checked this model with the below commandWe are getting FP32 precision in logs.

image1054×441 12.5 KB
Kindly help us to convert the YOLOv4.onnx model to TRT FP16 model conversion.Thanks,
Dax JainHi @AakankshaS,Have you checked the above-highlighted issue?Looking for your prompt response!!!Thanks,
Dax JainPowered by Discourse, best viewed with JavaScript enabled"
90,trtexec-converter-throws-an-error-when-trying-to-convert-a-model-with-input-type-int8-or-fp16,"I am converting the DeepSORT ReID model to an Tensorrt engine. The steps are follows:I have been able to perform the above successfully and use the DeepSORT ReID model with TensoRT and Triton server.
The above conversion steps with default options in .trtexec converter, convert the model with input type FP32.Now I want to convert the model with input type int8/fp16 (since unit8 input is not supported by TensorRT yet). Since .trtexec converter allows to change the input data type with --inputIOFormats argument, I tried the following commands.I am not successful in achieving this as I am seeing the following errors.I have tried with .uff file wich has input type fp32 and also input type int8 and int 16 (by specifying the input node’s data type in step 1), but got the same results.Can someone help me understand the cause for this error and if its possible to convert a model with int8/fp16 input data types with .trtexec. Thanks.Hi,We recommend you to please try on the latest TensorRT version 8.5.1 and let us know if you still face this issue.
UFF and Caffe Parser have been deprecated from TensorRT 7 onwards, hence request you to try ONNX parser.
Please export your model to ONNX format and try converting to the TensorRT.Thank you.Any luck with this? I got a similar error with ONNX on TRT8.5Powered by Discourse, best viewed with JavaScript enabled"
91,suppress-tensorrt-warning-and-logging-messages-in-terminal,"I’m running TensorRT models to inference images captured from a camera. However, I’m seeing a ton of terminal output which I suspect is slowing down the inference framerate. Sample terminal output is shown below - can I disable all TensorRT terminal logging and how?TensorRT Version: 8.4.1
GPU Type: Jetson Xavier NX 16GB production
CUDA Version: 11.4.14
CUDNN Version:  8.4.1
Operating System + Version: Jetson Linux 35.1
Python Version (if applicable): 3.8[03/07/2023-15:30:23] [TRT] [I] The logger passed into createInferRuntime differs from one already provided for an existing builder, runtime, or refitter. Uses of the global logger, returned by nvinfer1::getLogger(), will return the existing value.
[03/07/2023-15:30:23] [TRT] [I] [MemUsageChange] Init CUDA: CPU +0, GPU +0, now: CPU 1198, GPU 6153 (MiB)
[03/07/2023-15:30:23] [TRT] [I] Loaded engine size: 21 MiB
[03/07/2023-15:30:23] [TRT] [W] Using an engine plan file across different models of devices is not recommended and is likely to affect performance or even cause errors.
[03/07/2023-15:30:23] [TRT] [I] [MemUsageChange] Init cuDNN: CPU +1, GPU +1, now: CPU 1221, GPU 6154 (MiB)
[03/07/2023-15:30:23] [TRT] [I] [MemUsageChange] TensorRT-managed allocation in engine deserialization: CPU +0, GPU +22, now: CPU 0, GPU 227 (MiB)
[03/07/2023-15:30:23] [TRT] [I] [MemUsageChange] Init cuDNN: CPU +0, GPU +0, now: CPU 1199, GPU 6154 (MiB)
[03/07/2023-15:30:23] [TRT] [I] [MemUsageChange] TensorRT-managed allocation in IExecutionContext creation: CPU +0, GPU +3, now: CPU 0, GPU 230 (MiB)Hi @zhifeis ,
Can you please share the detailed logs with us?
For the TRT part code, youcan change the log level and can try setting the log level to kWARNINGThanksThe code responsible for the terminal output above is shown here:How would I set the log level? Where in the code would I do that?Hi @zhifeis ,Please refer to the below link for details on enabling the logger.This is the revision history of the NVIDIA TensorRT 8.6 Developer Guide.ThanksPowered by Discourse, best viewed with JavaScript enabled"
92,help-with-detectnet-v2-train-config-file-tao,"Overview:
I have been using TAO to train custom, single-class detectnet_v2 networks with a resnet18 backbone on 1080p RGB images. This is the object/target that I am training on:

Detection_Target1920×1080 170 KB

While the networks are not perfect, I have great success deploying them for our use case. However, there are a few issues/cases I am running into that I would like to fix.Behavior:
When the object/target is far away/small, the network renders a near-perfect bounding box encapsulating the target:

Old Network: Perfect Detection1920×1080 53.1 KB
However, as the target gets closer, the neural network loses detection completely or begins to “split” the target:

Old Network: Too Close -&gt; Split Detection1920×1080 52.8 KB


Old Network: Too Close -&gt; No Detection1920×1080 58.6 KB
Current Improvement:
Over the last couple days, I have been trying to learn about all the different parameters in the training config for Detectnet_V2 with some success. My training config file now looks like this:Changes from previous config file to this one:Epochs: 60 → 40
I was worried about the model overfitting on a dataset of only 60k images.Freeze blocks: 0,1,2 → 0,1dbscan_eps: 0.3 → 0.7
Since the network was seeming to splice the detection, I thought that it may have been due to detections not being clustered together properly, so I increased this per the description here (DetectNet_v2 — TAO Toolkit 3.22.05 documentation).deadzone_radius: 0.6 → 0.2
Since the target is a circle and the bounding box should ideally circumscribe the target/circle, I calculated the deadzone_radius as (1- (circle_area_of_radius_r / square_area_of_width_2r)) = 0.2 to give the area inside the bounding box that is not the target.cov_radius_x: 0.5 → 1.0cov_radius_y: 0.5 → 1.0
Since the bounding box should ideally circumscribe the target, the coverage radius for x and y should be 1.0vflip_probability: 0.0 → 0.5If my reasoning for changing any of these parameters is wrong, please correct me. Additionally, I have also been trying to look into coverage_foreground_weight, but the explanation (Tlt spec file - cost function - #4 by Morganh) confuses me as to what coverage_foreground_weight is supposed to representThe neural network trained on this config file (using the same dataset as before) was able to track the target when it was larger/closer and fixed some of the “splitting”. Here are some outputs:
(1)

(1) Improved Network:  Less splitting1920×1080 62.8 KB

(2)

(2) Improved Network: Can detect closer but still splitting1920×1080 70.3 KB

(3)

(3) Improved Network: Different Target, Less Splitting than before1920×1080 68.9 KB
Image (1) shows improvement in the “splitting” but still does not encompass the entire target.Image (2) shows that the new/improved network is able to detect on a larger/closer target but still exhibits the same issues as (1), but worse. The splitting gets worse as it gets closer and closer/larger and largerImage (3) is on a sub-class of the target it has also been trained to detect and redemonstrates what (1) shows but on a different target. The red bounding box is the output from the previous neural network and the green bounding box is the output from the current neural networkQuestions and Help:
If you could provide any guidance or critique of the train config file or other parts of the training process to help remedy any of the following issues:Additional Info:
All example images from the network output have been cut from their original 1080p image for internal reasons. If so desired, I can provide the full image in a private context.Our dataset is roughly 60k 1080p RGB images hand-labeled in the KITTI format with just the class name and bounding box fields being non-zero. While the dataset does not include a lot of close-up/large images of the target, I would still expect it to be able to. Here are some data on the distribution of target bounding boxes in the dataset:Width: Mean=103.318 px, Min=14, Max=1006
Height: Mean=75.932 px, Min=6, Max=1076Width/Height Distribution:

Does the tendency for the bounding box to be in the middle of the image and/or be smaller (100-200 width) have an effect on training? If so, can this be resolved with the augmentation_config’s zoom_min/max and translate_max_x/y properties?(I did discover just now when grabbing these statistics that there were some wrong bounding boxes (like <25 in a dataset of 60k) so I will be retraining this weekend just to be sure)I got similar problems, for detecting closer object,  any solutions?Powered by Discourse, best viewed with JavaScript enabled"
93,riva-issue-running-on-jetson-orin-nano-dev-kit,"I have  flashed the SD card for my Jetson Orin nano dev kit with the latest Jetpack 5.1.1 Image.I have downloaded this CLI image:
ngc registry resource download-version “nvidia/riva/riva_quickstart_arm64:2.10.0”I have followed the install instructions from the rivia embedded quick start :Scripts and utilities for getting started with Riva Speech Skills on Embedded platformsI do not have any problems with the install.
The issue is when I run “$ sudo bash riva_start.sh”
All I get is this:Waiting for Riva server to load all models…retrying in 10 seconds
Waiting for Riva server to load all models…retrying in 10 seconds
Waiting for Riva server to load all models…retrying in 10 seconds
Waiting for Riva server to load all models…retrying in 10 seconds
Waiting for Riva server to load all models…retrying in 10 seconds
Waiting for Riva server to load all models…retrying in 10 seconds
Waiting for Riva server to load all models…retrying in 10 seconds
Health ready check failed.
Check Riva logs with: docker logs riva-speechIt says check the logs but there is nothing in themHi @adventuredaisyThanks for your interest in RivaApologies you are facing the issue,Jetpack 5.1 is the correct version for Riva 2.10Request to kindly share the following information with usQuick Checkswe request to kindly check if you have set the default runtime to nvidia on the Jetson platform by adding the following line in the /etc/docker/daemon.json file
If not already changed please Restart the Docker service using sudo systemctl restart docker after editing the file.We request to run basic container with nvidia runtime to check things are fine, so kindly run docker run -it --rm --runtime nvidia ubuntu:20.04 and let us know if it worksThanksSo I was able to get Rivia to run on the Jetson Orin nano dev kit.
I had to load the minimum of services to get it to run and it still taxes the Jetson Orin Nano to the limit.Here is all the things I did but I really dont know which one worked{
“default-runtime”: “nvidia”,
“runtimes”: {
“nvidia”: {
“path”: “/usr/bin/nvidia-container-runtime”,
“runtimeArgs”: 
}
}
}2.config.shservice_enabled_asr=true
service_enabled_nlp=false
service_enabled_tts=false
service_enabled_nmt=false
and
gpus_to_use=“device=1”sudo chmod -R a+rwx /home/joev/riva_quickstart_arm64_v2.10.0/model_repositoryI think that running all the services at one is just to taxing for the Jetson Orin nano Dev kitI will be posting a video soon with a little more in depth explanasionHI @adventuredaisyThanks for the feedbackI will check with the internal team on.whether running all services at once will burden the Jetson Orin and provide feedbackI will also check on the lock symbol issue facedThanksHI @adventuredaisyI have updates from the internal team,That’s True, Jetson Orin has limited GPU Memory, so loading all the models at once is not recommended, So it is expensive/taxing and we recommended running the single/limited serviceRegarding models downloaded has the lock symbol, This is expected as the Models are downloaded inside of Docker Volume, so should be fine and not needed for them to be unblockedThanksThis topic was automatically closed 14 days after the last reply. New replies are no longer allowed.Powered by Discourse, best viewed with JavaScript enabled"
94,nemo-models-license,"Are Nvidia pretrained NeMo models from ngc covered by the Apache2.0 license only as it is mentioned in NeMo github?Example of ngc model:
https://api.ngc.nvidia.com/v2/models/nvidia/riva/speechtotext_pt_br_conformer/versions/trainable_v1.2/files/Conformer-CTC-L_bpe1024_pt-BR_1.2.nemoHi @tompavlThanks for your interest in RivaI guess each model carry’s a different License, kindly request to check the license section at the bottom of your Model’s NGC Catalog
image1415×353 31.9 KBThanksPowered by Discourse, best viewed with JavaScript enabled"
95,tensorrt-python-3-8-for-cuda-11-on-ubuntu-20-04,"TensorRT for Python 3.8 is requested.TensorRT Version: TensorRT-7.1.3.4
GPU Type: GeForce GTX 1050Ti
Nvidia Driver Version: NVIDIA-SMI 450.57
CUDA Version: CUDA Version: 11.0
CUDNN Version: 8.0.2.39
Operating System + Version: Ubuntu 20.04
Python Version (if applicable): 3.8.2
TensorFlow Version (if applicable): 2.4.0
PyTorch Version (if applicable): 1.7.0Will TensorRT provide a wheel for Python 3.8?Cheers
PeiHi @jiapei,
As of now i don’t have any info about the python 3.8 support to TRT.
Request you to stay tuned to the NVIDIA TensorRT official page for any updates.An SDK with an optimizer for high-performance deep learning inference.
Thanks!Do you have any information now? I need answer of question.Powered by Discourse, best viewed with JavaScript enabled"
96,error-code-10-internal-error-could-not-find-any-implementation-for-node-foreignnode-668-mul-497,"This onnx model can run with onnxruntime-gpu.
I also didn’t see any unsupported operators for onnx2trt.
But I cannot generate the trt model with trtexec. The full error message is belove:TensorRT Version: 8.5.3.1
GPU Type:  RTX 3080
Nvidia Driver Version: 516.94
CUDA Version: 11.1
CUDNN Version: 8.0.5
Operating System + Version: Win10
Python Version (if applicable): 3.8.10
TensorFlow Version (if applicable):
PyTorch Version (if applicable): 1.9.1
Baremetal or Container (if container which image + tag):Here is the onnx model link.Google Drive file.Hi,
Request you to share the ONNX model and the script if not shared already so that we can assist you better.
Alongside you can try few things:This NVIDIA TensorRT 8.6.0 Early Access (EA) Quick Start Guide is a starting point for developers who want to try out TensorRT SDK; specifically, this document demonstrates how to quickly construct an application to run inference on a TensorRT...import sys
import onnx
filename = yourONNXmodel
model = onnx.load(filename)
onnx.checker.check_model(model).
2) Try running your model with trtexec command.master/samples/trtexecNVIDIA® TensorRT™, an SDK for high-performance deep learning inference, includes a deep learning inference optimizer and runtime that delivers low latency and high throughput for inference applicat...
In case you are still facing issue, request you to share the trtexec “”–verbose"""" log for further debugging
Thanks!Hi
I have checked the model by check_model.py.
And the error message is generated from trtexec.I provided the onnx model already but not the script. The full scripts are too many and it’s not important because all the things are onnx model. It can be run with onnxruntime-gpu.Hi @p890040 ,
Apologies for delay,
Can you please try using the latest TRT version and let us know if problem still persist?Could not find any implementation for node {ForeignNode[668...Mul_497]}The fix has been addressed in newer releases.ThanksPowered by Discourse, best viewed with JavaScript enabled"
97,alternative-solutions-for-dataframe-rolling-apply-function-because-its-axis-1-is-not-supported-yet,"Greetings for the day!I want to ask the following:I have daily data from Apple stock as cudf.DataFrame type.Then I want to run a random forest algo together with a shap feature selection.I want to apply these algos daily. I know that the rolling.apply method doesn’t support the axis=1 yet, so I tried created a numba cuda kernel to do the whole backtesting for the entire dataset in parallel.I tried using a list comprehension, but it actually uses the CPU, which I don’t want. I want to do all in GPU.Could you help me, please?This is the whole code:The following code block impor the necessary libraries:The following block imports the data and prepare the features and the feature prediction:The following code block is the function which I want to implement daily in the df dataframe:The following code is what I want to do:However, as explained above, this is not supported yet. Besides, I tried with a list comprehension, which I was able to run it properly. However, it consumes 100% of the CPU with a subset of 100 daily observations and this maximum capacity CPU usage is not useful for me because I want to do this multiple times with monte-carlo-simulated prices.I hope you can help!Thanks in advance,Regards,José CarlosPowered by Discourse, best viewed with JavaScript enabled"
98,cant-find-speaker-diarization-notebook-from-the-given-link,") you have mentioned a link for the jupyter notebook in Nemo documentation of Speaker Diarization SpeakerNet. (https://github.com/NVIDIA/NeMo/blob/main/tutorials/speaker_recognition/Speaker_Diarization_Inference.ipynb). @Developer team can you please look into this and let me know where can I find this guide notebook?Hi @sajid.hameedThanks for your Interest in Rivaplease find the notebook linkThanksPowered by Discourse, best viewed with JavaScript enabled"
99,inconsistent-behaviour-of-plugin-enqueue-method-when-inputs-has-empty-shapes-i-e-0-on-batch-dimension,"i got a onnx model(maskrcnn) and converted it into trt engine by using trtexec, there are 2 custom ops in onnx model, so i implemented 2 corresponding plugins in tensorrt, one is NMSRotatedPlugin and another is ROIAlignRotatedPlugin. when testing inference on the converted trt engine, i found that when inputs has empty shapes, the inputs/outputs parameters of enqueue method of these 2 plugins are given differently by tensorrt.As for ROIAlignRotatedPlugin, it has 2 inputs and 1 output, when the 1st dimension of the 2nd input is 0(which means no rois), the output should be empty(0 in the 1st dim). the inputs/outputs parameter are non-null pointer and the “checked the 0 condition and return” logic works fine, as show in the following picture

image1059×1091 92 KB
As for NMSRotatedPlugin, it has 3 inputs and 2 outputs, when the 1st dim of the first  2 inputs are 0(which means no bbox were found), the 1st output should also be empty(0 in the 1st dim). but this time the inputs/outputs parameter are given null pointers, and after “checked the 0 condition and return” logic, tensorrt throws out  “illegal memory access” error, as show in followig pictures:

image1020×1081 91.7 KB


image1437×60 7.16 KB
I think the illegal memory access is related to the null-pointers and i don’t know why the parameter passing behaviour are different(i.e. one gets non-null pointers and the other get null) on these 2 plugins.TensorRT Version:  8.6.0
GPU Type: RTX 3090
Nvidia Driver Version: 470.129.06
CUDA Version: 11.4
CUDNN Version: 8.8.0
Operating System + Version: Ubuntu 20.04.4 LTS
Python Version (if applicable): 3.8
TensorFlow Version (if applicable): none
PyTorch Version (if applicable): 1.9
Baremetal or Container (if container which image + tag): containerPlease attach or include links to any models, data, files, or scripts necessary to reproduce your issue. (Github repo, Google Drive, Dropbox, etc.)Please include:Hi,
Please refer to below links related custom plugin implementation and sample:This Samples Support Guide provides an overview of all the supported NVIDIA TensorRT 8.6.0 Early Access (EA) samples included on GitHub and in the product package. The TensorRT samples specifically help in areas such as recommenders, machine...TensorRT is an SDK for high performance, deep learning inference. It includes a deep learning inference optimizer and a runtime that delivers low latency and high throughput for deep learning…While IPluginV2 and IPluginV2Ext interfaces are still supported for backward compatibility with TensorRT 5.1 and 6.0.x respectively, however, we recommend that you write new plugins or refactor existing ones to target the IPluginV2DynamicExt or IPluginV2IOExt interfaces instead.Thanks!thanks for response, but i go through these 2 articles and don’t see anything related to my question, i derived my plugins from IPluginV2DynamicExt already.Powered by Discourse, best viewed with JavaScript enabled"
100,how-to-convert-uff-model-to-onnx-format,"A clear and concise description of the bug or issue.TensorRT Version: 7.2.0
GPU Type: V100
Nvidia Driver Version:
CUDA Version: 12.0
CUDNN Version:
Operating System + Version: Ubuntu 18.04
Python Version (if applicable): 3.10
TensorFlow Version (if applicable): 2.0
PyTorch Version (if applicable): 1.10
Baremetal or Container (if container which image + tag):Hi i am trying to convert a uff model to onnx format can you let me knw the process or procedure , i do not have the base tensorflow / pytorch model with me …The model architecture is customized object detection modelThanks in advanceLoad the model in pytorch, then use the following export command:ch, thenHi ,As mentioned previously i donot have the pytorch model with me i only have the uff model  i want to convert uff–> onnxHi,
UFF and Caffe Parser have been deprecated from TensorRT 7 onwards, hence request you to try ONNX parser.
Please check the below link for the same.ONNX-TensorRT: TensorRT backend for ONNX. Contribute to onnx/onnx-tensorrt development by creating an account on GitHub.Thanks!onwardBut this example if for if i am having onnx model with me , i am having only uff model either i have to convert it to tensorrt and then to onnx or directly to onnx . Is there any example which u can share which performs the following processHi,Please try tf2onnx in case it helps you (uff → .pb → onnx).Convert TensorFlow, Keras, Tensorflow.js and Tflite models to ONNX - GitHub - onnx/tensorflow-onnx: Convert TensorFlow, Keras, Tensorflow.js and Tflite models to ONNXIf you still need further assistance, please reach out to the ONNX related platform to get better help.Open standard for machine learning interoperability - Issues · onnx/onnxThank you.Powered by Discourse, best viewed with JavaScript enabled"
101,p6000-tensorrt-too-slow-and-the-serialized-fp16-model-size-is-not-as-expected,"I am trying to run one segmentation ONNX model with trt.
The input shape: 1, 3, 1600, 832; the output shape: 1, 1, 1600, 832
First, I take a test on Ubuntu OS with A100-40G, and the results seems to be good. The inference time is around 40~50ms. And when I set the fp16 flag, the inference time of fp16-trt-model could be accelerated to around 25ms. The fp16-trt-model is nealy half of fp32-trt-model.But when I tried to move the same project to Windows10 with P6000, some strange things occured.
The inference time become 400ms for float model.
And when I set the fp16 flag, the model has no speedup (still ~400ms), and the ROM size of the serialized trt model is still close to fp32 model.**TensorRT Version: **: 8.4.1
GPU Type: A100 P6000
Nvidia Driver Version: 522.25
CUDA Version: 11.6
CUDNN Version: 8.2.1 / 8.4.1 / 8.5.1 (We tried three version, and the results are the same)
Operating System + Version: Ubuntu20.04 / Windows 10
Python Version (if applicable): 3.8
TensorFlow Version (if applicable): -
PyTorch Version (if applicable): 1.12
Baremetal or Container (if container which image + tag): -Some logs on Windows 10:
[InferenceHelper][117] Use TensorRT
[04/04/2023-18:26:31] [I] [TRT] [MemUsageChange] Init CUDA: CPU +293, GPU +0, now: CPU 19005, GPU 1156 (MiB)
[04/04/2023-18:26:38] [I] [TRT] Loaded engine size: 233 MiB
[04/04/2023-18:26:38] [I] [TRT] [MemUsageChange] Init cuBLAS/cuBLASLt: CPU +321, GPU +112, now: CPU 19647, GPU 1502 (MiB)
[04/04/2023-18:26:39] [I] [TRT] [MemUsageChange] Init cuDNN: CPU +269, GPU +82, now: CPU 19916, GPU 1584 (MiB)
[04/04/2023-18:26:39] [W] [TRT] TensorRT was linked against cuDNN 8.4.1 but loaded cuDNN 8.2.1
[04/04/2023-18:26:39] [I] [TRT] [MemUsageChange] TensorRT-managed allocation in engine deserialization: CPU +0, GPU +254, now: CPU 0, GPU 254 (MiB)
[04/04/2023-18:26:39] [I] [TRT] [MemUsageChange] Init cuBLAS/cuBLASLt: CPU +1, GPU +8, now: CPU 19678, GPU 1598 (MiB)
[04/04/2023-18:26:39] [I] [TRT] [MemUsageChange] Init cuDNN: CPU +0, GPU +8, now: CPU 19678, GPU 1606 (MiB)
[04/04/2023-18:26:39] [W] [TRT] TensorRT was linked against cuDNN 8.4.1 but loaded cuDNN 8.2.1
[04/04/2023-18:26:39] [I] [TRT] [MemUsageChange] TensorRT-managed allocation in IExecutionContext creation: CPU +0, GPU +743, now: CPU 0, GPU 997 (MiB)
[InferenceHelperTensorRt][359] num_of_in_out = 2
[InferenceHelperTensorRt][362] tensor[0]->name: input
[InferenceHelperTensorRt][363]   is input = 1
[InferenceHelperTensorRt][367]   dims.d[0] = 1
[InferenceHelperTensorRt][367]   dims.d[1] = 3
[InferenceHelperTensorRt][367]   dims.d[2] = 1600
[InferenceHelperTensorRt][367]   dims.d[3] = 832
[InferenceHelperTensorRt][371]   data_type = 0
[InferenceHelperTensorRt][362] tensor[1]->name: output
[InferenceHelperTensorRt][363]   is input = 0
[InferenceHelperTensorRt][367]   dims.d[0] = 1
[InferenceHelperTensorRt][367]   dims.d[1] = 1
[InferenceHelperTensorRt][367]   dims.d[2] = 1600
[InferenceHelperTensorRt][367]   dims.d[3] = 832
[InferenceHelperTensorRt][371]   data_type = 0
[InferenceHelperTensorRt][456] 3
[SegmentationEngine][148] 832 1600 3
[SegmentationEngine][167] here[InferenceHelperTensorRt][329] process
[InferenceHelperTensorRt][333] 5324800
cudaMemcpyAsync cost    124.891 [msec]
[InferenceHelperTensorRt][340] process_2
[InferenceHelperTensorRt][345] process_3
[InferenceHelperTensorRt][351] process_4
[SegmentationEngine][173] thereTotal:                1024.149 [msec]
Capture:              12.140 [msec]
Image processing:    833.516 [msec]
Pre processing:     10.712 [msec]
Inference:         682.913 [msec]
Post processing:    39.013 [msec]
=== Finished 0 frame ===Please include:Hi,Request you to share the model, script, profiler, and performance output if not shared already so that we can help you better.Alternatively, you can try running your model with trtexec command.master/samples/trtexecNVIDIA® TensorRT™, an SDK for high-performance deep learning inference, includes a deep learning inference optimizer and runtime that delivers low latency and high throughput for inference applicat...While measuring the model performance, make sure you consider the latency and throughput of the network inference, excluding the data pre and post-processing overhead.
Please refer to the below links for more details:This Best Practices Guide covers various performance considerations related to deploying networks using TensorRT 8.0.3. These sections assume that you have a model that is working at an appropriate level of accuracy and that you are able to...This Best Practices Guide covers various performance considerations related to deploying networks using TensorRT 8.0.3. These sections assume that you have a model that is working at an appropriate level of accuracy and that you are able to...Thanks!Powered by Discourse, best viewed with JavaScript enabled"
102,cant-transcribe-buffer-received-from-websocket,"Hardware - GPU (NVIDIA GeForce RTX 2080 Ti)
Hardware - CPU
Operating System - Ubuntu 20.04.6 LTS
Riva Version - riva_quickstart_v2.10.0I am working on this Speech to text problem in which I am receiving audio buffers continuously from flask_socketio. I can get any reference or example file or tutorial on nvidia plateform, where I can make this done.I have done all steps for riva,
bash riva_init.sh - DONE successfully
bash riva_start.sh - DONE successfully
My riva server is running on auth = riva.client.Auth(uri=‘localhost:50051’)I checked this by running example notebooks, those are working fine for transcription.For reference, which I have tried, I am linking my code.This code currently giving me this error,Looking forward for any help from community, experts. ThanksHI @ahmedshahzadaaaThanks for your interest in RivaI will check regarding this error with the team and provide fix for itThanksStill waiting for the response.Powered by Discourse, best viewed with JavaScript enabled"
103,calibration-and-int8-inference-on-onnx-model,"I use TensorRT to accelerate the inception v1 in onnx format, and get top1-accuracy 67.5% in fp32 format/67.5% in fp16 format, while get 0.1% in int8 after calibration.The image preprocessing of the model is in bgr format, with mean subtraction [103.939, 116.779, 123.680]. Since tensorrt is not opensourced, I’ve no idea what’s going on inside the calibration tools. The images fed into the calibration tools should be the same format with the ones for inference, right?Was there anything wrong when I was using the calibration or inference? Or this type of unnormalized image format not friendly as input?I attached my script , onnx weight and calibration cache below.https://drive.google.com/drive/folders/1niT1dvsUdHyfKoWWRwztcKL5V4Lp1UYd?usp=sharingCould you help to inspect it ? thanks.Actually the environment is the Flashed environment by Jetpack 4.4 on Jetson AGX Xavier.TensorRT Version 7: 7.1 (Flashed by Jetpack4.4)
GPU Type:  GPU of Jetson Xavier
Nvidia Driver Version:
CUDA Version:  10.2
CUDNN Version:
Operating System + Version: Ubuntu 18.04
Python Version (if applicable):  3.6
TensorFlow Version (if applicable):
PyTorch Version (if applicable):  1.2 aarch64, downloaded from this forum
Baremetal or Container (if container which image + tag):Hi @yeahfd,
Your query has been noted. Please allow us some time to check on this.
Thanks!Hi @yeahfd,
Looking at the calibration cache.seems to be wrong.
Can you please try IInt8EntropyCalibrator2 instead.
Thanks!Actually  I tried  both the two versions of  calibrators.Here is the IInt8EntropyCalibrator2 resultdata_0: 5c810a14
conv1/7x7_s2_1: 5db1c61c
conv1/7x7_s2_2: 5dbee29b
pool1/3x3_s2_1: 5dbee29b
pool1/norm1_1: 5d9d2427
conv2/3x3_reduce_1: 5cf8a62a
conv2/3x3_reduce_2: 5d110825
conv2/3x3_1: 5d452ca4
conv2/3x3_2: 5cf63814
conv2/norm2_1: 5d15bde1
pool2/3x3_s2_1: 5d15bde1
inception_3a/1x1_1: 5c175b59
inception_3a/1x1_2: 5bcabc1d
inception_3a/3x3_reduce_1: 5bc0e6af
inception_3a/3x3_reduce_2: 5bc0e6af
inception_3a/3x3_1: 5c037974
inception_3a/3x3_2: 5bcabc1d
inception_3a/5x5_reduce_1: 5bda8670
inception_3a/5x5_reduce_2: 5bda8670
inception_3a/5x5_1: 5bac1839
inception_3a/5x5_2: 5bcabc1d
inception_3a/pool_1: 5d15bde1
inception_3a/pool_proj_1: 5c13b2b4
inception_3a/pool_proj_2: 5bcabc1d
inception_3a/output_1: 5bcabc1d
inception_3b/1x1_1: 5bb19575
inception_3b/1x1_2: 5bb0e2d4
inception_3b/3x3_reduce_1: 5b940d1d
inception_3b/3x3_reduce_2: 5b94d34a
inception_3b/3x3_1: 5bfb13a4
inception_3b/3x3_2: 5bb0e2d4
inception_3b/5x5_reduce_1: 5bc2cdda
inception_3b/5x5_reduce_2: 5bc8017d
inception_3b/5x5_1: 5c00c944
inception_3b/5x5_2: 5bb0e2d4
inception_3b/pool_1: 5bcabc1d
inception_3b/pool_proj_1: 5c1d55b7
inception_3b/pool_proj_2: 5bb0e2d4
inception_3b/output_1: 5bb0e2d4
pool3/3x3_s2_1: 5bb0e2d4
inception_4a/1x1_1: 5be0dcf0
inception_4a/1x1_2: 5ba7eb6f
inception_4a/3x3_reduce_1: 5bc9bd32
inception_4a/3x3_reduce_2: 5bc76092
inception_4a/3x3_1: 5c2a6110
inception_4a/3x3_2: 5ba7eb6f
inception_4a/5x5_reduce_1: 5bca9d5a
inception_4a/5x5_reduce_2: 5bcbdddd
inception_4a/5x5_1: 5c0a8457
inception_4a/5x5_2: 5ba7eb6f
inception_4a/pool_1: 5bb0e2d4
inception_4a/pool_proj_1: 5c60767a
inception_4a/pool_proj_2: 5ba7eb6f
inception_4a/output_1: 5ba7eb6f
inception_4b/1x1_1: 5b3f666d
inception_4b/1x1_2: 5b472a31
inception_4b/3x3_reduce_1: 5b0f8d0f
inception_4b/3x3_reduce_2: 5b1ee009
inception_4b/3x3_1: 5b2b7c60
inception_4b/3x3_2: 5b472a31
inception_4b/5x5_reduce_1: 5b3f98dc
inception_4b/5x5_reduce_2: 5b4d370e
inception_4b/5x5_1: 5b8f33d6
inception_4b/5x5_2: 5b472a31
inception_4b/pool_1: 5ba7eb6f
inception_4b/pool_proj_1: 5bb0975c
inception_4b/pool_proj_2: 5b472a31
inception_4b/output_1: 5b472a31
inception_4c/1x1_1: 5b65285b
inception_4c/1x1_2: 5b3765ca
inception_4c/3x3_reduce_1: 5b126f16
inception_4c/3x3_reduce_2: 5b126f16
inception_4c/3x3_1: 5b4170f6
inception_4c/3x3_2: 5b3765ca
inception_4c/5x5_reduce_1: 5b283f35
inception_4c/5x5_reduce_2: 5aed9c76
inception_4c/5x5_1: 5ae3f2f0
inception_4c/5x5_2: 5b3765ca
inception_4c/pool_1: 5b472a31
inception_4c/pool_proj_1: 5b974eee
inception_4c/pool_proj_2: 5b3765ca
inception_4c/output_1: 5b3765ca
inception_4d/1x1_1: 5b3a49fe
inception_4d/1x1_2: 5b047fa2
inception_4d/3x3_reduce_1: 5b2a319f
inception_4d/3x3_reduce_2: 5b01e841
inception_4d/3x3_1: 5b256afe
inception_4d/3x3_2: 5b047fa2
inception_4d/5x5_reduce_1: 5ae99132
inception_4d/5x5_reduce_2: 5aba5146
inception_4d/5x5_1: 5ae2f90a
inception_4d/5x5_2: 5b047fa2
inception_4d/pool_1: 5b3765ca
inception_4d/pool_proj_1: 5b766c30
inception_4d/pool_proj_2: 5b047fa2
inception_4d/output_1: 5b047fa2
inception_4e/1x1_1: 5a95d64b
inception_4e/1x1_2: 5a8f0ef3
inception_4e/3x3_reduce_1: 5a2f0475
inception_4e/3x3_reduce_2: 5a3269be
inception_4e/3x3_1: 5a96c39f
inception_4e/3x3_2: 5a8f0ef3
inception_4e/5x5_reduce_1: 5a8b6f43
inception_4e/5x5_reduce_2: 5aa36478
inception_4e/5x5_1: 5b0103a0
inception_4e/5x5_2: 5a8f0ef3
inception_4e/pool_1: 5b047fa2
inception_4e/pool_proj_1: 5b0f6a32
inception_4e/pool_proj_2: 5a8f0ef3
inception_4e/output_1: 5a8f0ef3
pool4/3x3_s2_1: 5a8f0ef3
inception_5a/1x1_1: 5a7e9c35
inception_5a/1x1_2: 5a46681e
inception_5a/3x3_reduce_1: 5a46b6cf
inception_5a/3x3_reduce_2: 5a49f5eb
inception_5a/3x3_1: 5a4048cd
inception_5a/3x3_2: 5a46681e
inception_5a/5x5_reduce_1: 5a4cbc76
inception_5a/5x5_reduce_2: 5a7a87fa
inception_5a/5x5_1: 5a45d620
inception_5a/5x5_2: 5a46681e
inception_5a/pool_1: 5a8f0ef3
inception_5a/pool_proj_1: 5b11b4e7
inception_5a/pool_proj_2: 5a46681e
inception_5a/output_1: 5a46681e
inception_5b/1x1_1: 59dcad3b
inception_5b/1x1_2: 59768900
inception_5b/3x3_reduce_1: 5a00ebaa
inception_5b/3x3_reduce_2: 5998eefa
inception_5b/3x3_1: 5957e92e
inception_5b/3x3_2: 59768900
inception_5b/5x5_reduce_1: 59d7afc3
inception_5b/5x5_reduce_2: 59655347
inception_5b/5x5_1: 59296929
inception_5b/5x5_2: 59768900
inception_5b/pool_1: 5a46681e
inception_5b/pool_proj_1: 5a16b618
inception_5b/pool_proj_2: 59768900
inception_5b/output_1: 59768900
pool5/7x7_s1_1: 59768900
pool5/7x7_s1_2: 585216d3
OC2_DUMMY_0: 585216d3
(Unnamed Layer* 142) [Constant]_output: 3a87a23a
OC2_DUMMY_2: 3a87a23a
(Unnamed Layer* 144) [Matrix Multiply]_output: 58905ec7
(Unnamed Layer* 145) [Constant]_output: 3c6b9c60
(Unnamed Layer* 146) [Shuffle]_output: 3c6b9c60
loss3/classifier_1: 58905ec7
(Unnamed Layer* 148) [Shuffle]_output: 58905ec7
(Unnamed Layer* 149) [Softmax]_output: 3c010a14
prob_1: 3c010a14Hi @yeahfd,Could you please manually check whether the batch data that is getting generated is correct in “load_calibrate_data”?
Also please refer to below sample for your referenceThis Samples Support Guide provides an overview of all the supported NVIDIA TensorRT 8.4.3 samples included on GitHub and in the product package. The TensorRT samples specifically help in areas such as recommenders, machine comprehension, character...ThanksHi @SunilJB @yeahfd,
Has this issue been solved?the  batch data in calibration should be ok.
Actually the situation is likeHi @yeahfd,
Have you tried calibration cache creation on detector models??no.
actually the type of task does not affect the accuracy, but the type of preprocessing in the original model.
Since tensorRT is a black box, I just wonder if there any extra steps done inside the calibration tool.Hi @yeahfd
So are you saying if we change the classifier model to detector model it should work??nope… I just got confusion on the preprocessing stage. I think the type of the task does not have relationship with the problem.The quantization-related problem if you get in the classifier, when you switch to the detector, it will exist as well since the backbones in the detector are still these commonly used networks.Hi @yeahfd, @AakankshaS @SunilJB
are you supplying labels for your calibration files??labels are not necessary for calibration stage. Only images are fed into model to get the blob tensor of each convolutional layer.Oh ok@yeahfd
I am seeing the same calibration behaviour with TRT 7.1.3. A calibrator that follows normalisation used during training in pytorch does not make any sense and just sets all tensor ranges to [-inf,inf]. While if you just divide img array by 255 the calibrated model then performs well. Did you manage to get any explanation?Hi,
Request you to share the ONNX model and the script if not shared already so that we can assist you better.
Alongside you can try few things:This NVIDIA TensorRT 8.4.3 Quick Start Guide is a starting point for developers who want to try out TensorRT SDK; specifically, this document demonstrates how to quickly construct an application to run inference on a TensorRT engine.import sys
import onnx
filename = yourONNXmodel
model = onnx.load(filename)
onnx.checker.check_model(model).
2) Try running your model with trtexec command.
https://github.com/NVIDIA/TensorRT/tree/master/samples/opensource/trtexec
In case you are still facing issue, request you to share the trtexec “”–verbose"""" log for further debugging
Thanks!@NVES
Thanks for your reply. Onnx model https://drive.google.com/file/d/1JVUiIBysRZjAA0a9lK9vKjHWG88d-qls/view?usp=sharing, onnx.checker.check_model return no errors.The model was trained in pytorch with [0,1] input normalised with mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225].My env:Calibration script:Ok, now two cases.Case 1. Calibration without normalisation. Cache - classifier_int8_noqat_unnorm.cache (2.6 KB). Log - log_classifier_int8_noqat_unnorm.txt (1.8 MB).
As you can see from the log during calibration the engine sees the input in the range [0,1], which is not normalised. If we look further down the log then scales and activations ranges make sense. But what is interesting is the input range which is [-1.00393,1.00393] while the engine sees inputs in the range [0,1]. Is is doing some normalisation by default?Case 2. Calibration with normalisation. Using mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]. Cache - classifier_int8_noqat_norm.cache (2.6 KB). Log - log_classifier_int8_noqat_norm.txt (1.8 MB).
We can see from the log that during calibration the engine sees the normalised input in the range [-value, +value]. But then the actual scales and ranges does not make any sense with most of them set to [-inf, inf]. What is strange is the input range [-2.04692e+38,2.04692e+38].I have faced a similar problem on other detection task and it seems to me having the input normalised b/n 0-1 is the source of the problem in the first place in case you plan to use int8 inference. The int8 is an Integer type with 8 bytes; however, by normalising the input data from 0-255 into 0-1 it is actually being converted into a floating point decimals and in the first convolution itself the result will be in fractions and doing int8 conversion will result in a lot of information loss. Thus, I just went and tried to have the input data in its original range (0-255) and trained the model (torch), torch to engine conversion, and the results  after int8 calibration looks reasonable to me as compared to it counter part float32 version.Powered by Discourse, best viewed with JavaScript enabled"
104,cannot-install-tensorrt,"I am trying to install TensorRT via pip on windows, i am new to this but i ran “pip install nvidia-pyindex” which worked perfectly, then “pip install nvidia-tensorrt” which returned the error:“C:\Users\Magic\AppData\Local\Temp\pip-install-hmcfh88w\nvidia-tensorrt_f68a79bb55924f0d825cccc5f57aa354\setup.py”, line 150, in 
raise RuntimeError(open(“ERROR.txt”, “r”).read())
The package you are trying to install is only a placeholder project on PyPI.org repository.-Note: The reason i am trying to do this is because my custom yolo model only gets roughly 15 fps even though it should be running on my GPU, the custom model was trained on YoloV5n using pytorch.TensorRT Version:
**GPU TypeRtx 2070:
Nvidia Driver Version:
**CUDA Version11.7:
**CUDNN Version8.8.1.3:
**Operating System + VersionWindows 10:
**Python Version (if applicable)3.11:
**TensorFlow Version (if applicable)2.12:
**PyTorch Version (if applicable)2.0.0:
Baremetal or Container (if container which image + tag):Python_AI_Script.py (1.1 KB)Please attach or include links to any models, data, files, or scripts necessary to reproduce your issue. (Github repo, Google Drive, Dropbox, etc.)pip install tensorrt

image869×77 5.38 KB


image1211×600 25.4 KB
The model inference times are roughly 30-45ms btwI am trying to install TensorRT via pip on Windows. I am new to this, but I ran “pip install nvidia-pyindex,” which worked perfectly, then “pip install nvidia-tensorrt,” which returned the error:Hi,We recommend you refer to the official TensorRT installation guide.This NVIDIA TensorRT 8.6.1 Installation Guide provides the installation requirements, a list of what is included in the TensorRT package, and step-by-step instructions for installing TensorRT.Thank you.Powered by Discourse, best viewed with JavaScript enabled"
105,nvidia-t550-laptop-gpu-can-i-train-tensorflow-models-on-this-gpu,"I am Data Scientist. I am using windows 11. Installed CUDA 11.2. And CUDNN 8.1 is been configured. Followed the blog for installation
I have installed tensorflow and training deep learning model or neural network.
“NVIDIA T550 Laptop GPU” is my laptops GPU. can I use this GPU for training deep learning models?
While training model, while epocs are getting executed, I can see in taskbar “Dedicated GPU Memory Usage” at almost 60% but GPU activity % I can see 0%. I am now confused whether it is using GPU to train model. I am not able to understand whether there is something wrong with my GPU.Blog: Installing Latest TensorFlow version with CUDA, cudNN and GPU support on Windows 11 PC | by Gunter Pearson | MediumHi,Are you able to run nvidia-smi successfully?
Please refer to the following.If you need further assistance, we recommend you to please reach out to the TensorRT forum.Thank you.Hello, I have this configuration tensorflow version: 2.10.0, Python version: 3.10, CUDCNN version: 8.1 and CUDA version: 11.2 for my GPU T550 for windows 11. But, when i run this code ""print(""Num GPUs Available: “, len(tf.config.list_physical_devices(‘GPU’)))” i am getting 0 gpus as an output. and nvidia-smi isnt showing in the terminal either. but for, >nvcc --version
nvcc: NVIDIA (R) Cuda compiler driver
Copyright (c) 2005-2020 NVIDIA Corporation
Built on Mon_Nov_30_19:15:10_Pacific_Standard_Time_2020
Cuda compilation tools, release 11.2, V11.2.67
Build cuda_11.2.r11.2/compiler.29373293_0 i am getting this output can you please help me which cuda and cudcnn to install to make my code run in GPU.Powered by Discourse, best viewed with JavaScript enabled"
106,riva-tts-model-not-getting-readdy,"Please provide the following information when requesting support.Hardware - GPU T4
Riva Version 2.6.0When deploying the helm chart with nvidia/riva/rmir_asr_citrinet_1024_en_us_str:2.6.0 and nvidia/riva/rmir_tts_fastpitch_hifigan_en_us:2.6.0 models we are getting the following error:-

image1913×703 25.7 KB
@jimit-modiHi @bhooyas.kapadiaThanks for your interest in RivaApologies you are facing issueCan you share with usThanksPowered by Discourse, best viewed with JavaScript enabled"
107,cuda-installation-error-code-id-16552,"Error Code 16552. Trying to install CUDA so i can use deforum stable diffusionTensorRT Version:
GPU Type:
Nvidia Driver Version:
CUDA Version: 12.1
CUDNN Version:
Operating System + Version:
Python Version (if applicable):
TensorFlow Version (if applicable):
PyTorch Version (if applicable):
Baremetal or Container (if container which image + tag):Please attach or include links to any models, data, files, or scripts necessary to reproduce your issue. (Github repo, Google Drive, Dropbox, etc.)Please include:Hi,
Please refer to the installation steps from the below link if in case you are missing on anythingThis NVIDIA TensorRT 8.6.1 Installation Guide provides the installation requirements, a list of what is included in the TensorRT package, and step-by-step instructions for installing TensorRT.
Also, we suggest you to use TRT NGC containers to avoid any system dependency related issues.NVIDIA TensorRT is a C++ library that facilitates high-performance inference on NVIDIA graphics processing units (GPUs). TensorRT takes a trained network and produces a highly optimized runtime engine that performs inference for that network.Thanks!Powered by Discourse, best viewed with JavaScript enabled"
108,tensor-volume-exceeds-2-31-1,"I trained a Tacotron2 model refrence from GitHub - Rayhane-mamah/Tacotron-2: DeepMind's Tacotron-2 Tensorflow implementation.
I convert onnx sucessfully and the result is correct. Hower a large tensor error occured when convert onnx into trt.env:
trt: 8.2.3
cuda: 10.2
cudnn: 7.6.5
onnx: 1.10.0
opset: 13steps:[02/14/2022-07:28:27] [E] [TRT] ModelImporter.cpp:776: — End node —
[02/14/2022-07:28:27] [E] [TRT] ModelImporter.cpp:779: ERROR: ModelImporter.cpp:166 In function parseGraph:
[6] Invalid Node - generic_loop_Loop__183
[graphShapeAnalyzer.cpp::processCheck::581] Error Code 4: Internal Error ((Unnamed Layer* 755) [LoopOutput]_output: tensor volume exceeds (2^31)-1, dimensions are [2147483647,1,160])
[graphShapeAnalyzer.cpp::processCheck::581] Error Code 4: Internal Error ((Unnamed Layer* 755) [LoopOutput]_output: tensor volume exceeds (2^31)-1, dimensions are [2147483647,1,160])
[02/14/2022-07:28:34] [E] Failed to parse onnx file
[02/14/2022-07:28:34] [I] Finish parsing network model
[02/14/2022-07:28:34] [E] Parsing model failed
[02/14/2022-07:28:34] [E] Failed to create engine from model.
[02/14/2022-07:28:34] [E] Engine set up failedI have moved the topic to TensorRT -  This team may be in a better position to help.Hi,
We recommend you to check the below samples links in case of tf-trt integration issues.During the TensorFlow with TensorRT (TF-TRT) optimization, TensorRT performs several important transformations and optimizations to the neural network graph. This guide provides instructions on how to accelerate inference in TF-TRT.This NVIDIA TensorRT 8.4.3 Quick Start Guide is a starting point for developers who want to try out TensorRT SDK; specifically, this document demonstrates how to quickly construct an application to run inference on a TensorRT engine.During the TensorFlow with TensorRT (TF-TRT) optimization, TensorRT performs several important transformations and optimizations to the neural network graph. This guide provides instructions on how to accelerate inference in TF-TRT.During the TensorFlow with TensorRT (TF-TRT) optimization, TensorRT performs several important transformations and optimizations to the neural network graph. This guide provides instructions on how to accelerate inference in TF-TRT.If issue persist, We recommend you to reach out to Tensorflow forum.
Thanks!Hi,Are you using tf2onnx to generate ONNX file?Hi,
I am also facing the same error and I have used tf2onnx to convert
Any help would be appreciated!Hi,Currently, TRT does not support tensors with more than 2^31-1 elements.
We do not have a workaround except modifiying the network.Thank you.Hello,
Can you clarify please why this limitation exist?
what its root cause to limit the tensor volume size and raise a runtime exception while performing the model inference operation while there are still available GPU RAM?
Why not enable the inference operation till there will not be enough available RAM memory?
Thanks,Why is this the limit? When will it be increased?Powered by Discourse, best viewed with JavaScript enabled"
109,availability-of-tensorrt-for-ubuntu-22-04-sbsa,"TensorRT (libnvinfer8, libnvinfer-dev) is not yet available at Index of /compute/cuda/repos/ubuntu2204/sbsaIs there a timeframe by when TensorRT will be available for Ubuntu 22.04 on aarch64 (sbsa)?TensorRT Version: 8.5.3
GPU Type: Quadro RTX 4000
Nvidia Driver Version: 520.61.05
CUDA Version: See Container
CUDNN Version: See Container
Operating System + Version: See Container
Python Version (if applicable):
TensorFlow Version (if applicable):
PyTorch Version (if applicable):
Baremetal or Container (if container which image + tag): Container nvidia/cuda:11.8.0-cudnn8-devel-ubuntu22.04Hi @b-data_it ,
Apologies for the delay,
This is on our roadmap and shall be available soon. Please stay tuned for the updates.Thanks@AakankshaS Any update on this?Powered by Discourse, best viewed with JavaScript enabled"
110,tensorrt-python-api-inference-is-inconsistent-with-trtexec-inference,"I converted tensorflow based model to TensorRT (via onnx). This generated trt engine was inferred using the TensorRT python API and trtexec CLI while using the same input. It was observed that the outputs produced by the two inferencing ways were inconsistent.TensorRT Version: 8.5.2
Nvidia Driver Version: 516.94
CUDA Version: 11.7
Operating System + Version:  Windows 10
Python Version (if applicable): 3.8.10
TensorFlow Version (if applicable): 2.11.0Clone the repo GitHub - DwijayDS/TensorRT-consistency-issue: Reproducing TRT consistency issueConvert the tensorflow model to ONNX using tf2onnx. The tesnorflow model is defined in the file tf_model.pyUse trtexec to transform the ONNX model into a .trt file.Run python_infer.py to infer the model and compare outputs.The generated log will be similar to

image1518×692 37 KB
This shows that there is some inconsistency between the generated output.Let me know if you need any more details related to the issue.Hi,
Please refer to the installation steps from the below link if in case you are missing on anythingThis NVIDIA TensorRT 8.5.3 Installation Guide provides the installation requirements, a list of what is included in the TensorRT package, and step-by-step instructions for installing TensorRT.
Also, we suggest you to use TRT NGC containers to avoid any system dependency related issues.NVIDIA TensorRT is a C++ library that facilitates high-performance inference on NVIDIA graphics processing units (GPUs). TensorRT takes a trained network and produces a highly optimized runtime engine that performs inference for that network.Thanks!Powered by Discourse, best viewed with JavaScript enabled"
111,add-archives-of-cudnn-8-8-0-to-https-developer-download-nvidia-com-compute-redist-cudnn,"Hi! I help maintain the nvidia software stack for https://easybuild.io/, which manages various HPC software stacks.
To pull in software for the various flavors of linux running on HPC centers, we prefer using generic sources and archive versions rather than an already packaged version such as deb/rpm/etc.
For cuDNN <= 8.7.0 we managed to do this for cuDNN using the .tar.xz from Index of /compute/redist/cudnn, however for cuDNN 8.8.0 the archive is missing.
The archive is available when downloading from non-redist, but as we might need to pull in sources many many times during the build of a software stack, manually downloading every time is not feasible.Could you please add the (already existing) tar archives to redist?With regards,
Robert Jan Schlimbach
SURF AmsterdamHi @robertjan.schlimbach ,
Please stay tuned for the update. Will et back to you shortly on this.ThanksHi @robertjan.schlimbach ,
You can find it here.ThanksMuch appreciated 👍This topic was automatically closed 14 days after the last reply. New replies are no longer allowed.Powered by Discourse, best viewed with JavaScript enabled"
112,tensorrt-version-for-cuda-12-0,"The latest tensorRT version, TensorRT 8.5 GA Update 2 for x86_64 Architecture supports only till CUDA 11.8. What should I do if I want to install TensorRT but have CUDA 12.0 toolkit installed?https://developer.nvidia.com/nvidia-tensorrt-8x-downloadHi ,
We recommend you to check the supported features from the below link.These support matrices provide a look into the supported platforms, features, and hardware capabilities of the NVIDIA TensorRT 8.5.3 APIs, parsers, and layers.
You can refer below link for all the supported operators list.
For unsupported operators, you need to create a custom plugin to support the operationThanks!Powered by Discourse, best viewed with JavaScript enabled"
113,runutime-error,"Hey Guys!!I am working on a python  project where I am  trying to train a YOLO model but I am confronted with a runtime error which , after a very thorough check the error  is not from my code , this is the error I am receiving : error: an illegal memory access was encountered.after going through a few trouble shooting process like updating drivers,  reinstalling CUDA and checking the code , when I run the same python script again the program suddenly exits  with no errors on the console whatsoever , I am really confused on what is happening, I also check the memory using Nsight but couldn’t find anything there either.These are my env specifications:
CUDA Version: 11.7
Python Version: 3.11.3Here is where the code is suddenly suspended

image1716×830 54.7 KB
Kindly help me with this
Cheers (:Hi @narayan322007 ,did you try running your code with cuda-memcheck?ThanksPowered by Discourse, best viewed with JavaScript enabled"
114,bean-search-language-model-for-conformer-ctc-hindi-model,"Hi,I’ve trained a language model and I’m trying to use it over the pretrained model. Getting the following error:
ValueError: Decoding strategy must be one of [‘greedy’]. Given beam
I’ve trained the LM model by following the below link:
https://docs.nvidia.com/deeplearning/nemo/user-guide/docs/en/main/asr/asr_language_modeling.html
Also the link which was used for implementing the beam search:# What does this PR do ?

Add high level API for beam search using model.trans…cribe() and beam search strategy.
Currently only supports DeepSpeed beam search library, eventually can support more advanced libraries
like pyctcdecode.

**Collection**: [ASR]

# Changelog 
- Adds AbstractBeamCTCInfer and BeamCTCInfer classes to support beam search strategy
- Updated CTCDecoding and CTCBPEDecoding classes to support beam search strategy
- Adds tests for CTC Decoding classes

# Usage


``` python

import nemo.collections.asr as nemo_asr

filepath = ""/media/smajumdar/data/Datasets/Librispeech/LibriSpeech/test-other-processed/367-130732-0008.wav""
kenlm_path = ""/media/smajumdar/data/Datasets/ASR_SET_LM/ASR_SET_3.0/lm""

model = nemo_asr.models.ASRModel.from_pretrained('stt_en_conformer_ctc_large')  # type: nemo_asr.models.EncDecCTCModelBPE

# Create default decoding config
model.change_decoding_strategy(None)

# Update decoding config for beam search
decoding_cfg = model.cfg.decoding
decoding_cfg.strategy = ""beam""
decoding_cfg.beam.beam_size = 128
decoding_cfg.beam.return_best_hypothesis = False
decoding_cfg.beam.beam_alpha = 1.0
decoding_cfg.beam.beam_beta = 0.0
decoding_cfg.beam.kenlm_path = kenlm_path

# Prepare beam search decoding strategy
model.change_decoding_strategy(decoding_cfg)

# Transcribe speech with beam search
hypothesis = model.transcribe([filepath], return_hypotheses=True)
print(""Num hypothesis per sample :"", len(hypothesis[0]))
print(""Result type :"", hypothesis[0][0].__class__.__name__)
for candidate in range(len(hypothesis[0])):
    print(f""Beam Search {candidate + 1} :"", hypothesis[0][candidate].text, hypothesis[0][candidate].score)

```

# Before your PR is ""Ready for review""
**Pre checks**:
- [x] Make sure you read and followed [Contributor guidelines](https://github.com/NVIDIA/NeMo/blob/main/CONTRIBUTING.md)
- [x] Did you write any new necessary tests?
- [x] Did you add or update any necessary documentation?
- [x] Does the PR affect components that are optional to install? (Ex: Numba, Pynini, Apex etc)
  - [ ] Reviewer: Does the PR have correct import guards for all optional libraries?
  
**PR Type**:
- [x] New Feature
- [ ] Bugfix
- [ ] Documentation
Does the language model works on the stt_hi_conformer_ctc_medium (Hindi Model)? And if yes, can you provide the steps required to create the. language model as well as the resource to use it with the pre-trained/fine-tuned model?Hi @iamgarimanarangThanks for your interest in RivaI will check regarding the issue with the team and get backThanksHi @iamgarimanarangQuick Updates,This issue seems to be with Nemo, so i am internally moving the request from Riva and try to pitch with NemoThanksHI @iamgarimanarangI have inputs from the Nemo teamKindly to use main branch, this feature is not for 1.15, and we have not released 1.16 yetYes as long as steps are followed from the docs ASR Language Modeling — NVIDIA NeMo , the HindI ASR model will process the text and build manifest, beam search will work. But  may need to do hyper parameters search to get good beam scores.for large scale grid search of hyper parameters you should still use the eval scripts mentioned in the documentation ASR Language Modeling — NVIDIA NeMo, Once you have good hyper parameters, this high level API is meant for use with final best beam alpha and beta to do beam search with model.transcribe()Once we have 1.17 release, things will be smooth, We will add docs for high level APIThanksHi @rvinobhaThanks for the updates. Can you confirm and let us know the tentative date for the new version release?Kind Regards,
Garima NarangHi @rvinobha,Any updates on the 1.17 release?Kind Regards,
Garima NarangHi @iamgarimanarangApologies for the delay,The Release is tentatively expected on first or second week of AprilThanksPowered by Discourse, best viewed with JavaScript enabled"
115,could-libnvinfer-safe-be-made-available-in-the-public-apt-repo,"Hi!We fetch the Nvidia libraries from the public apt repo:
https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64/There, we obtain for example “libnvinfer-headers*.deb”, which internaly contains NvInferSafeRuntime.h, which is the header corresponding to libnvinfer_safe.so.However, that actual library, libninver-safe*.deb, is not published on this portal.Would it be possible to make it available there? We could obtain it from the DriveOS package, but it’s always older than “latest and greatest”, which limits us in testing the newest features and giving early feedback. Another drawback is that the one in the DriveOS package is built only for a specific version of Ubuntu, while in the apt repo one can choose the Ubuntu version of their choice.Thanks!Hi,Sorry, we do not have much information on the above. Could you please try reaching out to the CUDA-related forum to get better help?Thank you.This is a TensorRT related question, why should I ask in the CUDA forum?Hi,Sorry for the confusion.
Currently, we do not have plans to release the safe builds outside of DriveOS.Thank you.Powered by Discourse, best viewed with JavaScript enabled"
116,compile-cv-cuda-source-code-error,"cv-cuda/src/nvcv_types/include/nvcv/cuda/BorderWrap.hpp:514:14: error: no matching function for call to ‘IsOutside’
if ((IsOutside<kActiveDimensions[Is]>(c, Base::m_tensorShape[kMap.from[Is]]) || …))
^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
cv-cuda/src/nvcv_types/include/nvcv/cuda/BorderWrap.hpp:479:17: note: in instantiation of function template specialization ‘nvcv::cuda::BorderWrap<nvcv::cuda::TensorWrap<const unsigned char, -1, -1, 1>, NVCV_BORDER_CONSTANT, false, true, true>::doGetPtr<int, int, int, 0UL, 1UL, 2UL>’ requested here
p = doGetPtr(Is, c.z, c.y, c.x);
^
cv-cuda/src/cvcuda/priv/legacy/morphology.cu:55:38: note: in instantiation of function template specialization ‘nvcv::cuda::BorderWrap<nvcv::cuda::TensorWrap<const unsigned char, -1, -1, 1>, NVCV_BORDER_CONSTANT, false, true, true>::operator<int3, void>’ requested here
res     = cuda::max(res, src[coord]);
^
cv-cuda/src/cvcuda/priv/legacy/morphology.cu:117:9: note: in instantiation of function template specialization ‘nvcv::legacy::cuda_op::dilate<unsigned char, nvcv::cuda::BorderWrap<nvcv::cuda::TensorWrap<const unsigned char, -1, -1, 1>, NVCV_BORDER_CONSTANT, false, true, true>, nvcv::cuda::TensorWrap<unsigned char, -1, -1, 1>>’ requested here
dilate<<<grid, block, 0, stream>>>(src, dst, dstSize, kernelSize, kernelAnchor, val);Hi,This forum talks more about updates and issues related to TensorRT.
Please reach out to the CV CUDA related Issues · CVCUDA/CV-CUDA · GitHub to get better help.Thank you.Powered by Discourse, best viewed with JavaScript enabled"
117,when-will-a-op-with-2-4-sparsity-weight-call-sparse-conv-kernel-on-tensorrt,"Hi, guysI have a model with 91 sparsity ops(totally130 ops) generating via ASP Tool. Then convert sparsity onnx model to trt model, it just have 43 ops to call ""…sparse_conv… kernel "" to calculate.  The trtexec tool doesn’t convert all sparsity ops to trt sparse kernel. The hardware is RTX3080.The sparse kernel is like “sm80_xmma_fprop_sparse_conv_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize128x128x64_stage3_warpsize2x2x1_g1_sptensor16x8x32_t1r1s1_execute_kernel_trt”.
The dense kernel is like “trt_ampere_h1688cudnn_256x64_ldg8_relu_exp_small_nhwc_tn_v1” or “sm80_xmma_fprop_implicit_gemm_f16f16_f16f16_f16_nhwckrsc_nhwc_tilesize64x32x64_stage5_warpsize2x2x1_g1_tensor16x8x16_simple_t1r1s1_execute_kernel_trt”Please correct me if i’m wrong.The Sparse Conv2d kernel:

image1392×260 34.4 KB
The Dense Conv2d kernel:

image876×244 25.8 KB
So what conditions does the ops meet, it would call sparse kernel, thxHi, @spolisetty any comments on this topic?Hi,Usually, a dense conv kernel is faster than a sparse conv kernel for that specific conv, so TRT chooses the dense kernel.
Sparse kernels are only faster than dense kernels when the problem size is large enough. That means the channels (C and K) are greater than 256.Thank you.Powered by Discourse, best viewed with JavaScript enabled"
118,cudnn-enabled-increase-runtime-by-25,"Hi All,
Enviroments tested:
torch=1.11, cuda11.3, cudnn8200 (conda),
AND
torch_1.13, cu11.7, cudnn8500 (conda),
AND
pytorch-nvidia-docker 22.07
All with Ubuntu 20.04.Running a torch model in eval mode with CUDNN-enabled, consistently increase 25% runtime.CUDNN, CUDA Deep Neural Network, is a specialized library that should improve runtime of CUDA.I’ve attached the report from torch profiler with/without CUDNN.nvidia-issue.txt (5.6 KB)Hi @valtmaneddy ,
Can you please try utilizing NHWC format, i.e. channels_last (see (beta) Channels Last Memory Format in PyTorch — PyTorch Tutorials 2.0.1+cu117 documentation), cuDNN should be quite a bit faster than without.
Please let us know if issue still persist.
ThanksHI @AakankshaS ,
I’ve tested with torch2.01+cu117 as you’ve asked.
in torch2.01 there’s an improvement with channels last+cudnn over no-cudnn.
in torch1.13 there’s an improvement with no-cudnn over with-cudnn.
best is torch2+ with cudnn with channels last.runtime torch_CUDNN_DISABLED_1.13cu11.6cudnn8302_CHNNELS_LAST_NO, runtime: 28.96msec
runtime torch_CUDNN_DISABLED_1.13cu11.6cudnn8302_CHNNELS_LAST_YES, runtime: 30.55msec
runtime torch1.13cu11.6cudnn8302_CHNNELS_LAST_NO, runtime: 30.43msec
runtime torch1.13cu11.6cudnn8302_CHNNELS_LAST_YES, runtime: 29.91msecruntime torch_CUDNN_DISABLED_2.0cu11.7cudnn8500_CHNNELS_LAST_NO, runtime: 29.41msec
runtime torch_CUDNN_DISABLED_2.0cu11.7cudnn8500_CHNNELS_LAST_YES, runtime: 31.59msec
runtime torch2.0cu11.7cudnn8500_CHNNELS_LAST_NO, runtime: 27.84msec
runtime torch2.0cu11.7cudnn8500_CHNNELS_LAST_YES, runtime: 27.00msecIs there a way to get CUDNN improve runtime with torch<2?Powered by Discourse, best viewed with JavaScript enabled"
119,are-there-binary-utilities-for-tensorrt-engine,"Are there binary utilities for TensorRT Engine?I couldn’t find the binary utilities for TensorRT Engine.
Is there any plan to release it?TensorRT Version:
GPU Type:
Nvidia Driver Version:
CUDA Version:
CUDNN Version:
Operating System + Version:
Python Version (if applicable):
TensorFlow Version (if applicable):
PyTorch Version (if applicable):
Baremetal or Container (if container which image + tag):Please attach or include links to any models, data, files, or scripts necessary to reproduce your issue. (Github repo, Google Drive, Dropbox, etc.)Please include:Hi,
Please check the below link, as they might answer your concernsThis is the API Reference documentation for the NVIDIA TensorRT library. The following set of APIs allows developers to import pre-trained models, calibrate networks for INT8, and build and deploy optimized networks with TensorRT. Networks can be...
Thanks!Dear AakankshaSThank you for your information.
I checked your link.
However, I couldn’t find the information about binary utilities.Thanks!Hi @Hiromitsu.Matsuura ,
Let me check on this and get back to you.
Thank you for your patience.Dear @AakankshaS ,Do you have any update?Regards,
hiroYes, there are binary utilities available for working with TensorRT Engine. TensorRT is a deep learning inference optimizer and runtime library developed by NVIDIA. It is commonly used to optimize and deploy deep learning models for inference on NVIDIA GPUs.TensorRT provides several binary utilities that can be useful in different stages of the TensorRT workflow. Here are a few notable ones:Dear @matiashayes03,Thank you for your information.
Unfortunately, your information is not what I look for.I think there are following four CUDA binary tools.CUDA Binary Utilities (nvidia.com)And I want to know similar tools for TensorRT Engine.Regards,
hiroyeah, may be I have tried to give detail overview of binary utilities and was thinking it may be helpfull for you.Hi @Hiromitsu.Matsuura ,
Yes we do have Polygraphy for Tensorrt.
You can find the details here.
https://docs.nvidia.com/deeplearning/tensorrt/polygraphy/docs/index.htmlThanks.Dear @AakankshaS,Thank you for your information.
I will check it.Regards,
hiroPowered by Discourse, best viewed with JavaScript enabled"
120,cuopt-limited-trial-program-closed,"Thank you all for your interest in cuOpt.  We appreciate your engagement and highly value your feedback.The limited trial program is now closed. However, due to high demand for the product, we are actively evaluating  the option to provide a trial version of the cloud service in the near future.  Please continue to stay engaged on Github for ongoing updates and releases - GitHub - NVIDIA/cuOpt-Resources: A collection of NVIDIA cuOpt samples and other resources.You can email us at cuopt@nvidia.com for licensing requests. Please allow 7-10 business days for a response.Regards,
cuOpt TeamPowered by Discourse, best viewed with JavaScript enabled"
121,any-other-ways-to-check-gpu-utilization-rather-than-nvidia-smi,"We all know that the Nvidia-smi returns the percent of used time over the past sample period, but it cannot reflect how much the GPU is used, such as how many tensor cores are used.Are there any ways or tools to check the utilization of a process?Powered by Discourse, best viewed with JavaScript enabled"
122,how-to-deploy-custom-models-to-aws-eks,"Please provide the following information when requesting support.Hardware - GPU g5 instance
Hardware - CPU  g5 instance
Operating System
Riva Version 2.10
TLT Version
How to reproduce the issue ?I would like to understand more the deploy process related to custom rmir model file.I’m trying to deploy AWS EKS with my custom rmir files but I’m a bit confused with this:#for non ngc based images or custom images.
raw_server_image: “”
raw_servicemaker_image: “”¿What to put in raw vars and where I change to my custom rmir files?I got it working locally, I’m just a bit lost deploying to EKS. ¿Any help? Thanks.Hi @carlfm01Thanks for your interest in Riva,I will check with the team and provide updatesThanksHello ¿Any updates?Hi @carlfm01My Sincere Apologies for the delayKindly use the existing AWS EKS tutorial - tutorials/deploy-eks.md at main · nvidia-riva/tutorials · GitHubThanksPowered by Discourse, best viewed with JavaScript enabled"
123,cudnn-fused-conv-bias,"Hi!
I’m trying to implement working conv+bias fused operation via backend api, and try to use example provided in another topic (Cudnn backend api for fused op - #8 by gautamj), but on finalizing of execution plan there always CUDNN_STATUS_UNSUPPORTED. In our production code, i have workaround with adding conv+add+add graph (with zero alpha2 on first add), but in case of two operations (just conv + add) we also have same error.Can you suggest me what can be wrong?Tested on:
card -  Tesla T4/GTX 1080ti
cudnn - 8.2.2/8.2.4
cuda - 11.1/11.4 (with LD_PRELOAD for libnvrtc.so)fuseOpDemo.cpp (20.7 KB)Also I have a few questions:Thank you in advance.Hi @lxq2.t,Thank you for using our API and posting here. The runtime fusion engine that I used in the forum : Cudnn backend api for fused op - #8 by gautamj is only supported for Volta and later GPU’s. GTX 1080Ti is too old. The issue with T4 is that it only supports half datatype and the file uses float datatype for all the tensors.The corrected file is attached. fuseOpDemo_turing.cpp (20.7 KB) This should work on T4.
You can also look at other samples of fusion from our public repository and try samples from there:For your questions:
We are working on 1 and 2 and will provide an update.
A new feature in upcoming release 8.3.0 is error reporting which will give much more informative errors like data type and format issues  which might help you for 3.@gautamj,
Thank you for example!
We further investigate problem with multithreaded call of backendExecute, and found that it seems to be required to create execution plan on separate handles, i.e. for each cudaStream we need to use separate execution plan and we can’t use plan created once. In case where we use plan created once (for ex. on one cudnn handle) we have mismatch of results when plan executed from multiple threads.
Can you tell me, we really need to create a separate cudnnHandle with execution plan for each stream if plan executed in parallel, since I did not find information about this in the documentation?Hi @lxq2.t , can you try the latest cuDNN 8.3.1 release? we have fixed a issue that we suspect to have caused the mismatches that you observed. However we are still developing a better testing method to check whether there are other remaining multi-thread issues. We will know with more certainty soon.Powered by Discourse, best viewed with JavaScript enabled"
124,basic-vpi-questions,"Hi, I’m looking for using the VPI Optical Flow LK Tracker. I looked at the example on the site and I have some questions.
Beforehand I’ll specify that what I need is that the tracker will work asynchronously and without needing to download/upload data on the GPU. I’m working on a Linux 22 environment.Thanks.Update: I’m trying to wrap a cv::cuda::GpuMat with a VPIArray, but I keep getting an invalid argument error on vpiArrayCreateWrapper:VPIArray VpiLKTrackerGPU::CreateVpiArrayViewer(cv::cuda::GpuMat &mat) {
VPIArray vpiArray;
VPIArrayData vpiArrayData;
vpiArrayData.bufferType = VPI_ARRAY_BUFFER_CUDA_AOS;
vpiArrayData.buffer.aos.data = mat.cudaPtr();
switch (mat.type()) {
case CV_32FC2:
vpiArrayData.buffer.aos.type = VPI_ARRAY_TYPE_KEYPOINT_F32;
vpiArrayData.buffer.aos.strideBytes = sizeof(cv::Vec2f);
break;
case CV_8UC1:
vpiArrayData.buffer.aos.type = VPI_ARRAY_TYPE_U8;
vpiArrayData.buffer.aos.strideBytes = 1;
break;
default: throw std::invalid_argument();
}
vpiArrayData.buffer.aos.capacity = mat.rows;
vpiArrayData.buffer.aos.sizePointer = &mat.rows;}Now, the documentation for vpiArrayCreateWrapper itself is confusing as it lists VPI_ARRAY_BUFFER_CUDA_AOS as a legitimate type but the documentation itself specifies wrapping host memory, and running the code with regular cv::Mat works. The question is whether is that I’m missing something and if I’m not whether is it possible to make a VPIArray wrapper to device memory.Powered by Discourse, best viewed with JavaScript enabled"
125,iconvolutionlayer-cannot-be-used-to-compute-a-shape-tensor,"[01/05/2023-11:31:51] [V] [TRT] Parsing node: Range_492 [Range]
[01/05/2023-11:31:51] [V] [TRT] Searching for input: 1023
[01/05/2023-11:31:51] [V] [TRT] Searching for input: 1022
[01/05/2023-11:31:51] [V] [TRT] Searching for input: 1024
[01/05/2023-11:31:51] [V] [TRT] Range_492 [Range] inputs: [1023 → ()[INT32]], [1022 → ()[INT32]], [1024 → ()[INT32]],
[01/05/2023-11:31:51] [V] [TRT] Registering layer: Range_492 for ONNX node: Range_492
[01/05/2023-11:31:51] [E] Error[9]: [graph.cpp::computeInputExecutionUses::553] Error Code 9: Internal Error (Conv_344: IConvolutionLayer cannot be used to compute a shape tensor)
[01/05/2023-11:31:51] [E] [TRT] parsers/onnx/ModelImporter.cpp:773: While parsing node number 391 [Range → “1025”]:
[01/05/2023-11:31:51] [E] [TRT] parsers/onnx/ModelImporter.cpp:774: — Begin node —
[01/05/2023-11:31:51] [E] [TRT] parsers/onnx/ModelImporter.cpp:775: input: “1023”
input: “1022”
input: “1024”
output: “1025”
name: “Range_492”
op_type: “Range”[01/05/2023-11:31:51] [E] [TRT] parsers/onnx/ModelImporter.cpp:776: — End node —
[01/05/2023-11:31:51] [E] [TRT] parsers/onnx/ModelImporter.cpp:778: ERROR: parsers/onnx/ModelImporter.cpp:180 In function parseGraph:
[6] Invalid Node - Range_492
[graph.cpp::computeInputExecutionUses::553] Error Code 9: Internal Error (Conv_344: IConvolutionLayer cannot be used to compute a shape tensor)
[01/05/2023-11:31:51] [E] Failed to parse onnx file
[01/05/2023-11:31:51] [I] Finish parsing network model
[01/05/2023-11:31:51] [E] Parsing model failed
[01/05/2023-11:31:51] [E] Failed to create engine from model or file.
[01/05/2023-11:31:51] [E] Engine set up failed
&&&& FAILED TensorRT.trtexec [TensorRT v8402] # trtexec --onnx=fs_folded2.onnx --saveEngine=fs.trt --fp16 --verboseTensorRT Version: 8.4.2-1+cuda11.6
GPU Type: A100
Nvidia Driver Version: 465.19.01
CUDA Version: 11.3
CUDNN Version:
Operating System + Version:  tensorrt:22.08-py3
Python Version (if applicable):
TensorFlow Version (if applicable):
PyTorch Version (if applicable):
Baremetal or Container (if container which image + tag):Google Drive file.trtexec --onnx=fs_folded2.onnx --saveEngine=fs.trt --fp16 --verboseHi,When we try using the latest TensorRT version 8.5.2, we are facing a different error.
Please allow us some time to debug this issue.Thank you.Any update? thanks.My local test shows that the provided model cannot run with onnxruntime.Seems the input shape is necessary for a correct inference. Can you provide a valid shape and input data for the graph inputs?Powered by Discourse, best viewed with JavaScript enabled"
126,the-enqueue-method-has-been-deprecated-when-used-with-engines-built-from-a-network-created-with-networkdefinitioncreationflag-kexplicit-batch-flag,"A clear and concise description of the bug or issue.TensorRT Version: 8.4.1.5
GPU Type:  1660 Ti
Nvidia Driver Version:  515.65.01
CUDA Version:  11.7
CUDNN Version: 8.5.0
Operating System + Version:  Linux  5.15.0-46-generic #49~20.04.1-Ubuntu
Python Version (if applicable):
TensorFlow Version (if applicable):
PyTorch Version (if applicable):
Baremetal or Container (if container which image + tag):Please attach or include links to any models, data, files, or scripts necessary to reproduce your issue. (Github repo, Google Drive, Dropbox, etc.)Please include:I converted a model via /usr/src/tensorrt/bin/trtexec  command succesfully. But When I want to run project, for every inference it generates these messages:How could I disable these messages? My project has been wrote in C++.Hi,You can modify the ProfileVerbosity level to remove warning messages. Please refer to the following doc.
https://docs.nvidia.com/deeplearning/tensorrt/api/c_api/classnvinfer1_1_1_i_logger.htmlThank you.You can modify the ProfileVerbosity level to remove warning messages. Please refer to the following doc.
https://docs.nvidia.com/deeplearning/tensorrt/api/c_api/classnvinfer1_1_1_i_logger.html Changing class Logger : public nvinfer1::ILogger as follows:Powered by Discourse, best viewed with JavaScript enabled"
127,help-build-dynamic-ai-enabled-kiosks-and-mobile-ordering-solutions,"Anyone can help with  Enabling voice ordering on mobile devices using NVIDIA’s speech recognition technologyYou may be looking for NVIDIA RIVA for audio transcription: https://developer.nvidia.com/riva and NVIDIA NeMO for for conversational AI GitHub - NVIDIA/NeMo: NeMo: a toolkit for conversational AI , like this one: NVIDIA Intelligent Virtual Assistant AI Workflow I’m not on either project, but their tutorials and docs should helpPowered by Discourse, best viewed with JavaScript enabled"
128,failed-to-convert-nemo-model-to-riva-nemo2riva-asr,"Hardware -  NVIDIA GeForce RTX 2080
Hardware - Intel Core i7-9700k
Operating System - WSL with Ubuntu 20.04
nemo2riva - 2.11.0
nemo - 1.18.0rc0I used this command for the installation of nemo :
 BRANCH=main
!python -m pip install git+https://github.com/NVIDIA/NeMo.git@$BRANCH#egg=nemo_toolkit[all] After finetuning a conformer (stt_en_conformer_ctc_large_ls) model, I obtained a .nemo file that I fail to convert in .riva with nemo2riva.I executed the following command :nemo2riva --out {riva_file_path} {nemo_file_path} I followed this tutorial and modified the conf file, mainly by reducing the batch size : Example: Kinyarwanda ASR using Mozilla Common Voice Dataset — NVIDIA NeMoERROR: Export failed. Please make sure your NeMo model class (<class ‘nemo.collections.asr.models.ctc_bpe_models.EncDecCTCModelBPE’>) has working export() and that you have the latest NeMo package installed with [all] dependencies.Here is the full output :[NeMo W 2023-05-30 09:48:51 optimizers:54] Apex was not found. Using the lamb or fused_adam optimizer will error out.
[NeMo W 2023-05-30 09:49:08 experimental:27] Module <class ‘nemo.collections.asr.modules.audio_modules.SpectrogramToMultichannelFeatures’> is experimental, not ready for production and is not fully supported. Use at your own risk.
[NeMo W 2023-05-30 09:49:11 experimental:27] Module <class ‘nemo.collections.tts.models.fastpitch_ssl.FastPitchModel_SSL’> is experimental, not ready for production and is not fully supported. Use at your own risk.
[NeMo W 2023-05-30 09:49:11 experimental:27] Module <class ‘nemo.collections.common.tokenizers.text_to_speech.tts_tokenizers.IPATokenizer’> is experimental, not ready for production and is not fully supported. Use at your own risk.
[NeMo W 2023-05-30 09:49:11 experimental:27] Module <class ‘nemo.collections.tts.models.radtts.RadTTSModel’> is experimental, not ready for production and is not fully supported. Use at your own risk.
[NeMo W 2023-05-30 09:49:16 experimental:27] Module <class ‘nemo.collections.tts.models.ssl_tts.SSLDisentangler’> is experimental, not ready for production and is not fully supported. Use at your own risk.
[NeMo W 2023-05-30 09:49:16 experimental:27] Module <class ‘nemo.collections.tts.models.vits.VitsModel’> is experimental, not ready for production and is not fully supported. Use at your own risk.
[NeMo I 2023-05-30 09:49:16 nemo2riva:38] Logging level set to 20
[NeMo I 2023-05-30 09:49:16 convert:35] Restoring NeMo model from ‘models/finetuned/conformer_ctc_bpe_final_epoch100_tokenbpe.nemo’
GPU available: True (cuda), used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[NeMo W 2023-05-30 09:49:17 nemo_logging:349] /mnt/d/MAI/STT_PER_EV/NEMO/env/lib/python3.8/site-packages/pytorch_lightning/trainer/setup.py:176: PossibleUserWarning: GPU available but not used. Set accelerator and devices using Trainer(accelerator='gpu', devices=1).
rank_zero_warn([NeMo I 2023-05-30 09:49:22 mixins:170] Tokenizer SentencePieceTokenizer initialized with 128 tokens
[NeMo W 2023-05-30 09:49:22 modelPT:161] If you intend to do training or fine-tuning, please call the ModelPT.setup_training_data() method and provide a valid configuration file to setup the train data loader.
Train config :
manifest_filepath: ./manifests/train_manifest_final.json
sample_rate: 16000
batch_size: 4
shuffle: true
num_workers: 16
pin_memory: true
max_duration: 30.0
min_duration: 0.1
is_tarred: false
tarred_audio_filepaths: null
shuffle_n: 2048
bucketing_strategy: synced_randomized
bucketing_batch_size: null[NeMo W 2023-05-30 09:49:22 modelPT:168] If you intend to do validation, please call the ModelPT.setup_validation_data() or ModelPT.setup_multiple_validation_data() method and provide a valid configuration file to setup the validation data loader(s).
Validation config :
manifest_filepath: ./manifests/dev_manifest_final.json
sample_rate: 16000
batch_size: 4
shuffle: false
use_start_end_token: false
num_workers: 8
pin_memory: true[NeMo W 2023-05-30 09:49:22 modelPT:174] Please call the ModelPT.setup_test_data() or ModelPT.setup_multiple_test_data() method and provide a valid configuration file to setup the test data loader(s).
Test config :
manifest_filepath: ./manifests/test_manifest_final.json
sample_rate: 16000
batch_size: 4
shuffle: false
use_start_end_token: false
num_workers: 8
pin_memory: true[NeMo I 2023-05-30 09:49:22 features:291] PADDING: 0
ERROR: Call to cuInit results in CUDA_ERROR_NO_DEVICE
[NeMo I 2023-05-30 09:49:24 save_restore_connector:249] Model EncDecCTCModelBPE was successfully restored from /mnt/d/MAI/STT_PER_EV/NEMO/models/finetuned/conformer_ctc_bpe_final_epoch100_tokenbpe.nemo.
[NeMo I 2023-05-30 09:49:24 schema:148] Loaded schema file /mnt/d/MAI/STT_PER_EV/NEMO/env/lib/python3.8/site-packages/nemo2riva/validation_schemas/asr-scr-exported-encdecclsmodel.yaml for nemo.collections.asr.models.classification_models.EncDecClassificationModel
[NeMo I 2023-05-30 09:49:24 schema:148] Loaded schema file /mnt/d/MAI/STT_PER_EV/NEMO/env/lib/python3.8/site-packages/nemo2riva/validation_schemas/asr-stt-exported-encdecctcmodel.yaml for nemo.collections.asr.models.EncDecCTCModel
[NeMo I 2023-05-30 09:49:24 schema:148] Loaded schema file /mnt/d/MAI/STT_PER_EV/NEMO/env/lib/python3.8/site-packages/nemo2riva/validation_schemas/asr-stt-exported-encdectcmodelbpe.yaml for nemo.collections.asr.models.EncDecCTCModelBPE
[NeMo I 2023-05-30 09:49:24 schema:148] Loaded schema file /mnt/d/MAI/STT_PER_EV/NEMO/env/lib/python3.8/site-packages/nemo2riva/validation_schemas/nlp-isc-exported-bert.yaml for nemo.collections.nlp.models.IntentSlotClassificationModel
[NeMo I 2023-05-30 09:49:24 schema:148] Loaded schema file /mnt/d/MAI/STT_PER_EV/NEMO/env/lib/python3.8/site-packages/nemo2riva/validation_schemas/nlp-mt-exported-encdecmtmodel.yaml for nemo.collections.nlp.models.MTEncDecModel
[NeMo I 2023-05-30 09:49:24 schema:148] Loaded schema file /mnt/d/MAI/STT_PER_EV/NEMO/env/lib/python3.8/site-packages/nemo2riva/validation_schemas/nlp-mt-exported-megatronnmtmodel.yaml for nemo.collections.nlp.models.MegatronNMTModel
[NeMo I 2023-05-30 09:49:24 schema:148] Loaded schema file /mnt/d/MAI/STT_PER_EV/NEMO/env/lib/python3.8/site-packages/nemo2riva/validation_schemas/nlp-pc-exported-bert.yaml for nemo.collections.nlp.models.PunctuationCapitalizationModel
[NeMo I 2023-05-30 09:49:24 schema:148] Loaded schema file /mnt/d/MAI/STT_PER_EV/NEMO/env/lib/python3.8/site-packages/nemo2riva/validation_schemas/nlp-qa-exported-bert.yaml for nemo.collections.nlp.models.QAModel
[NeMo I 2023-05-30 09:49:24 schema:148] Loaded schema file /mnt/d/MAI/STT_PER_EV/NEMO/env/lib/python3.8/site-packages/nemo2riva/validation_schemas/nlp-tc-exported-bert.yaml for nemo.collections.nlp.models.TextClassificationModel
[NeMo I 2023-05-30 09:49:24 schema:148] Loaded schema file /mnt/d/MAI/STT_PER_EV/NEMO/env/lib/python3.8/site-packages/nemo2riva/validation_schemas/nlp-tkc-exported-bert.yaml for nemo.collections.nlp.models.TokenClassificationModel
[NeMo I 2023-05-30 09:49:24 schema:148] Loaded schema file /mnt/d/MAI/STT_PER_EV/NEMO/env/lib/python3.8/site-packages/nemo2riva/validation_schemas/tts-exported-fastpitchmodel.yaml for nemo.collections.tts.models.FastPitchModel
[NeMo I 2023-05-30 09:49:24 schema:148] Loaded schema file /mnt/d/MAI/STT_PER_EV/NEMO/env/lib/python3.8/site-packages/nemo2riva/validation_schemas/tts-exported-hifiganmodel.yaml for nemo.collections.tts.models.HifiGanModel
[NeMo I 2023-05-30 09:49:24 schema:148] Loaded schema file /mnt/d/MAI/STT_PER_EV/NEMO/env/lib/python3.8/site-packages/nemo2riva/validation_schemas/tts-exported-radttsmodel.yaml for nemo.collections.tts.models.RadTTSModel
[NeMo I 2023-05-30 09:49:24 schema:187] Found validation schema for nemo.collections.asr.models.EncDecCTCModelBPE at /mnt/d/MAI/STT_PER_EV/NEMO/env/lib/python3.8/site-packages/nemo2riva/validation_schemas/asr-stt-exported-encdectcmodelbpe.yaml
[NeMo I 2023-05-30 09:49:24 schema:216] Checking installed NeMo version … 1.18.0rc0 OK (>=1.1)
[NeMo I 2023-05-30 09:49:25 artifacts:59] Found model at ./model_weights.ckpt
INFO: Checking Nemo version for ConformerEncoder …
[NeMo I 2023-05-30 09:49:25 schema:216] Checking installed NeMo version … 1.18.0rc0 OK (>=1.7.0rc0)
[NeMo I 2023-05-30 09:49:25 artifacts:136] Retrieved artifacts: dict_keys([‘052b4d9f4d2c45b8938f6197bb348105_tokenizer.vocab’, ‘94a4475434b1450aad2638e1c2d7ebb9_vocab.txt’, ‘ddd360ec8832437bb5a8f6114fc91547_tokenizer.model’, ‘model_config.yaml’])
[NeMo I 2023-05-30 09:49:25 cookbook:71] Exporting model EncDecCTCModelBPE with config=ExportConfig(export_subnet=None, export_format=‘ONNX’, export_file=‘model_graph.onnx’, encryption=None, autocast=True, max_dim=100000)
[NeMo W 2023-05-30 09:49:26 nemo2riva:62] It looks like you’re trying to export a ASR model with max_dim=100000. Export is failing due to CUDA OOM. Reducing max_dim to 50000 and trying again…
[NeMo I 2023-05-30 09:49:26 convert:35] Restoring NeMo model from ‘models/finetuned/conformer_ctc_bpe_final_epoch100_tokenbpe.nemo’
GPU available: True (cuda), used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[NeMo W 2023-05-30 09:49:26 nemo_logging:349] /mnt/d/MAI/STT_PER_EV/NEMO/env/lib/python3.8/site-packages/pytorch_lightning/trainer/setup.py:176: PossibleUserWarning: GPU available but not used. Set accelerator and devices using Trainer(accelerator='gpu', devices=1).
rank_zero_warn([NeMo I 2023-05-30 09:49:31 mixins:170] Tokenizer SentencePieceTokenizer initialized with 128 tokens
[NeMo W 2023-05-30 09:49:31 modelPT:161] If you intend to do training or fine-tuning, please call the ModelPT.setup_training_data() method and provide a valid configuration file to setup the train data loader.
Train config :
manifest_filepath: ./manifests/train_manifest_final.json
sample_rate: 16000
batch_size: 4
shuffle: true
num_workers: 16
pin_memory: true
max_duration: 30.0
min_duration: 0.1
is_tarred: false
tarred_audio_filepaths: null
shuffle_n: 2048
bucketing_strategy: synced_randomized
bucketing_batch_size: null[NeMo W 2023-05-30 09:49:31 modelPT:168] If you intend to do validation, please call the ModelPT.setup_validation_data() or ModelPT.setup_multiple_validation_data() method and provide a valid configuration file to setup the validation data loader(s).
Validation config :
manifest_filepath: ./manifests/dev_manifest_final.json
sample_rate: 16000
batch_size: 4
shuffle: false
use_start_end_token: false
num_workers: 8
pin_memory: true[NeMo W 2023-05-30 09:49:31 modelPT:174] Please call the ModelPT.setup_test_data() or ModelPT.setup_multiple_test_data() method and provide a valid configuration file to setup the test data loader(s).
Test config :
manifest_filepath: ./manifests/test_manifest_final.json
sample_rate: 16000
batch_size: 4
shuffle: false
use_start_end_token: false
num_workers: 8
pin_memory: true[NeMo I 2023-05-30 09:49:31 features:291] PADDING: 0
[NeMo I 2023-05-30 09:49:33 save_restore_connector:249] Model EncDecCTCModelBPE was successfully restored from /mnt/d/MAI/STT_PER_EV/NEMO/models/finetuned/conformer_ctc_bpe_final_epoch100_tokenbpe.nemo.
[NeMo I 2023-05-30 09:49:33 schema:187] Found validation schema for nemo.collections.asr.models.EncDecCTCModelBPE at /mnt/d/MAI/STT_PER_EV/NEMO/env/lib/python3.8/site-packages/nemo2riva/validation_schemas/asr-stt-exported-encdectcmodelbpe.yaml
[NeMo I 2023-05-30 09:49:33 schema:216] Checking installed NeMo version … 1.18.0rc0 OK (>=1.1)
[NeMo I 2023-05-30 09:49:33 artifacts:59] Found model at ./model_weights.ckpt
INFO: Checking Nemo version for ConformerEncoder …
[NeMo I 2023-05-30 09:49:33 schema:216] Checking installed NeMo version … 1.18.0rc0 OK (>=1.7.0rc0)
[NeMo I 2023-05-30 09:49:33 artifacts:136] Retrieved artifacts: dict_keys([‘052b4d9f4d2c45b8938f6197bb348105_tokenizer.vocab’, ‘94a4475434b1450aad2638e1c2d7ebb9_vocab.txt’, ‘ddd360ec8832437bb5a8f6114fc91547_tokenizer.model’, ‘model_config.yaml’])
[NeMo I 2023-05-30 09:49:33 cookbook:71] Exporting model EncDecCTCModelBPE with config=ExportConfig(export_subnet=None, export_format=‘ONNX’, export_file=‘model_graph.onnx’, encryption=None, autocast=True, max_dim=50000)
[NeMo W 2023-05-30 09:49:38 nemo2riva:62] It looks like you’re trying to export a ASR model with max_dim=50000. Export is failing due to CUDA OOM. Reducing max_dim to 25000 and trying again…
[NeMo I 2023-05-30 09:49:38 convert:35] Restoring NeMo model from ‘models/finetuned/conformer_ctc_bpe_final_epoch100_tokenbpe.nemo’
GPU available: True (cuda), used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[NeMo W 2023-05-30 09:49:39 nemo_logging:349] /mnt/d/MAI/STT_PER_EV/NEMO/env/lib/python3.8/site-packages/pytorch_lightning/trainer/setup.py:176: PossibleUserWarning: GPU available but not used. Set accelerator and devices using Trainer(accelerator='gpu', devices=1).
rank_zero_warn([NeMo I 2023-05-30 09:49:44 mixins:170] Tokenizer SentencePieceTokenizer initialized with 128 tokens
[NeMo W 2023-05-30 09:49:44 modelPT:161] If you intend to do training or fine-tuning, please call the ModelPT.setup_training_data() method and provide a valid configuration file to setup the train data loader.
Train config :
manifest_filepath: ./manifests/train_manifest_final.json
sample_rate: 16000
batch_size: 4
shuffle: true
num_workers: 16
pin_memory: true
max_duration: 30.0
min_duration: 0.1
is_tarred: false
tarred_audio_filepaths: null
shuffle_n: 2048
bucketing_strategy: synced_randomized
bucketing_batch_size: null[NeMo W 2023-05-30 09:49:44 modelPT:168] If you intend to do validation, please call the ModelPT.setup_validation_data() or ModelPT.setup_multiple_validation_data() method and provide a valid configuration file to setup the validation data loader(s).
Validation config :
manifest_filepath: ./manifests/dev_manifest_final.json
sample_rate: 16000
batch_size: 4
shuffle: false
use_start_end_token: false
num_workers: 8
pin_memory: true[NeMo W 2023-05-30 09:49:44 modelPT:174] Please call the ModelPT.setup_test_data() or ModelPT.setup_multiple_test_data() method and provide a valid configuration file to setup the test data loader(s).
Test config :
manifest_filepath: ./manifests/test_manifest_final.json
sample_rate: 16000
batch_size: 4
shuffle: false
use_start_end_token: false
num_workers: 8
pin_memory: true[NeMo I 2023-05-30 09:49:44 features:291] PADDING: 0
[NeMo I 2023-05-30 09:49:46 save_restore_connector:249] Model EncDecCTCModelBPE was successfully restored from /mnt/d/MAI/STT_PER_EV/NEMO/models/finetuned/conformer_ctc_bpe_final_epoch100_tokenbpe.nemo.
[NeMo I 2023-05-30 09:49:46 schema:187] Found validation schema for nemo.collections.asr.models.EncDecCTCModelBPE at /mnt/d/MAI/STT_PER_EV/NEMO/env/lib/python3.8/site-packages/nemo2riva/validation_schemas/asr-stt-exported-encdectcmodelbpe.yaml
[NeMo I 2023-05-30 09:49:46 schema:216] Checking installed NeMo version … 1.18.0rc0 OK (>=1.1)
[NeMo I 2023-05-30 09:49:46 artifacts:59] Found model at ./model_weights.ckpt
INFO: Checking Nemo version for ConformerEncoder …
[NeMo I 2023-05-30 09:49:46 schema:216] Checking installed NeMo version … 1.18.0rc0 OK (>=1.7.0rc0)
[NeMo I 2023-05-30 09:49:46 artifacts:136] Retrieved artifacts: dict_keys([‘052b4d9f4d2c45b8938f6197bb348105_tokenizer.vocab’, ‘94a4475434b1450aad2638e1c2d7ebb9_vocab.txt’, ‘ddd360ec8832437bb5a8f6114fc91547_tokenizer.model’, ‘model_config.yaml’])
[NeMo I 2023-05-30 09:49:46 cookbook:71] Exporting model EncDecCTCModelBPE with config=ExportConfig(export_subnet=None, export_format=‘ONNX’, export_file=‘model_graph.onnx’, encryption=None, autocast=True, max_dim=25000)
[NeMo W 2023-05-30 09:49:46 nemo2riva:62] It looks like you’re trying to export a ASR model with max_dim=25000. Export is failing due to CUDA OOM. Reducing max_dim to 12500 and trying again…
[NeMo I 2023-05-30 09:49:46 convert:35] Restoring NeMo model from ‘models/finetuned/conformer_ctc_bpe_final_epoch100_tokenbpe.nemo’
GPU available: True (cuda), used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[NeMo W 2023-05-30 09:49:47 nemo_logging:349] /mnt/d/MAI/STT_PER_EV/NEMO/env/lib/python3.8/site-packages/pytorch_lightning/trainer/setup.py:176: PossibleUserWarning: GPU available but not used. Set accelerator and devices using Trainer(accelerator='gpu', devices=1).
rank_zero_warn([NeMo I 2023-05-30 09:49:52 mixins:170] Tokenizer SentencePieceTokenizer initialized with 128 tokens
[NeMo W 2023-05-30 09:49:52 modelPT:161] If you intend to do training or fine-tuning, please call the ModelPT.setup_training_data() method and provide a valid configuration file to setup the train data loader.
Train config :
manifest_filepath: ./manifests/train_manifest_final.json
sample_rate: 16000
batch_size: 4
shuffle: true
num_workers: 16
pin_memory: true
max_duration: 30.0
min_duration: 0.1
is_tarred: false
tarred_audio_filepaths: null
shuffle_n: 2048
bucketing_strategy: synced_randomized
bucketing_batch_size: null[NeMo W 2023-05-30 09:49:52 modelPT:168] If you intend to do validation, please call the ModelPT.setup_validation_data() or ModelPT.setup_multiple_validation_data() method and provide a valid configuration file to setup the validation data loader(s).
Validation config :
manifest_filepath: ./manifests/dev_manifest_final.json
sample_rate: 16000
batch_size: 4
shuffle: false
use_start_end_token: false
num_workers: 8
pin_memory: true[NeMo W 2023-05-30 09:49:52 modelPT:174] Please call the ModelPT.setup_test_data() or ModelPT.setup_multiple_test_data() method and provide a valid configuration file to setup the test data loader(s).
Test config :
manifest_filepath: ./manifests/test_manifest_final.json
sample_rate: 16000
batch_size: 4
shuffle: false
use_start_end_token: false
num_workers: 8
pin_memory: true[NeMo I 2023-05-30 09:49:52 features:291] PADDING: 0
[NeMo I 2023-05-30 09:49:54 save_restore_connector:249] Model EncDecCTCModelBPE was successfully restored from /mnt/d/MAI/STT_PER_EV/NEMO/models/finetuned/conformer_ctc_bpe_final_epoch100_tokenbpe.nemo.
[NeMo I 2023-05-30 09:49:54 schema:187] Found validation schema for nemo.collections.asr.models.EncDecCTCModelBPE at /mnt/d/MAI/STT_PER_EV/NEMO/env/lib/python3.8/site-packages/nemo2riva/validation_schemas/asr-stt-exported-encdectcmodelbpe.yaml
[NeMo I 2023-05-30 09:49:54 schema:216] Checking installed NeMo version … 1.18.0rc0 OK (>=1.1)
[NeMo I 2023-05-30 09:49:54 artifacts:59] Found model at ./model_weights.ckpt
INFO: Checking Nemo version for ConformerEncoder …
[NeMo I 2023-05-30 09:49:54 schema:216] Checking installed NeMo version … 1.18.0rc0 OK (>=1.7.0rc0)
[NeMo I 2023-05-30 09:49:54 artifacts:136] Retrieved artifacts: dict_keys([‘052b4d9f4d2c45b8938f6197bb348105_tokenizer.vocab’, ‘94a4475434b1450aad2638e1c2d7ebb9_vocab.txt’, ‘ddd360ec8832437bb5a8f6114fc91547_tokenizer.model’, ‘model_config.yaml’])
[NeMo I 2023-05-30 09:49:54 cookbook:71] Exporting model EncDecCTCModelBPE with config=ExportConfig(export_subnet=None, export_format=‘ONNX’, export_file=‘model_graph.onnx’, encryption=None, autocast=True, max_dim=12500)
[NeMo W 2023-05-30 09:49:54 nemo_logging:349] /mnt/d/MAI/STT_PER_EV/NEMO/NeMo/nemo/collections/asr/modules/conformer_encoder.py:466: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can’t record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!
if seq_length > self.max_audio_length:[NeMo W 2023-05-30 09:49:55 nemo_logging:349] /mnt/d/MAI/STT_PER_EV/NEMO/env/lib/python3.8/site-packages/torch/onnx/symbolic_opset9.py:2112: FutureWarning: ‘torch.onnx.symbolic_opset9._cast_Bool’ is deprecated in version 2.0 and will be removed in the future. Please Avoid using this function and create a Cast node instead.
return fn(g, to_cast_func(g, input, False), to_cast_func(g, other, False))============= Diagnostic Run torch.onnx.export version 2.0.1+cu117 =============
verbose: False, log level: Level.ERROR
======================= 0 NONE 0 NOTE 0 WARNING 0 ERROR ========================[NeMo E 2023-05-30 09:49:55 cookbook:122] ERROR: Export failed. Please make sure your NeMo model class (<class ‘nemo.collections.asr.models.ctc_bpe_models.EncDecCTCModelBPE’>) has working export() and that you have the latest NeMo package installed with [all] dependencies.
Traceback (most recent call last):
File “/mnt/d/MAI/STT_PER_EV/NEMO/env/bin/nemo2riva”, line 8, in 
sys.exit(nemo2riva())
File “/mnt/d/MAI/STT_PER_EV/NEMO/env/lib/python3.8/site-packages/nemo2riva/cli/nemo2riva.py”, line 49, in nemo2riva
Nemo2Riva(args)
File “/mnt/d/MAI/STT_PER_EV/NEMO/env/lib/python3.8/site-packages/nemo2riva/convert.py”, line 83, in Nemo2Riva
export_model(
File “/mnt/d/MAI/STT_PER_EV/NEMO/env/lib/python3.8/site-packages/nemo2riva/cookbook.py”, line 123, in export_model
raise e
File “/mnt/d/MAI/STT_PER_EV/NEMO/env/lib/python3.8/site-packages/nemo2riva/cookbook.py”, line 81, in export_model
_, descriptions = model.export(
File “/mnt/d/MAI/STT_PER_EV/NEMO/NeMo/nemo/core/classes/exportable.py”, line 113, in export
out, descr, out_example = model._export(
File “/mnt/d/MAI/STT_PER_EV/NEMO/NeMo/nemo/core/classes/exportable.py”, line 220, in _export
torch.onnx.export(
File “/mnt/d/MAI/STT_PER_EV/NEMO/env/lib/python3.8/site-packages/torch/onnx/utils.py”, line 506, in export
_export(
File “/mnt/d/MAI/STT_PER_EV/NEMO/env/lib/python3.8/site-packages/torch/onnx/utils.py”, line 1548, in _export
graph, params_dict, torch_out = _model_to_graph(
File “/mnt/d/MAI/STT_PER_EV/NEMO/env/lib/python3.8/site-packages/torch/onnx/utils.py”, line 1117, in _model_to_graph
graph = _optimize_graph(
File “/mnt/d/MAI/STT_PER_EV/NEMO/env/lib/python3.8/site-packages/torch/onnx/utils.py”, line 665, in _optimize_graph
graph = _C._jit_pass_onnx(graph, operator_export_type)
File “/mnt/d/MAI/STT_PER_EV/NEMO/env/lib/python3.8/site-packages/torch/onnx/utils.py”, line 1891, in _run_symbolic_function
return symbolic_fn(graph_context, *inputs, **attrs)
File “/mnt/d/MAI/STT_PER_EV/NEMO/env/lib/python3.8/site-packages/torch/onnx/symbolic_helper.py”, line 306, in wrapper
return fn(g, args, **kwargs)
File “/mnt/d/MAI/STT_PER_EV/NEMO/env/lib/python3.8/site-packages/torch/onnx/symbolic_opset14.py”, line 79, in batch_norm
return symbolic_helper._onnx_opset_unsupported_detailed(
File “/mnt/d/MAI/STT_PER_EV/NEMO/env/lib/python3.8/site-packages/torch/onnx/symbolic_helper.py”, line 657, in _onnx_opset_unsupported_detailed
raise errors.SymbolicValueError(
torch.onnx.errors.SymbolicValueError: Unsupported: ONNX export of BatchNormalization in opset 14. All input tensors must have the same dtype. Turn off Autocast or export using opset version 15… Please try opset version 15.  [Caused by the value 'input.51 defined in (%input.51 : Half(, 512, *, strides=[1600000, 3125, 1], requires_grad=0, device=cuda:0) = onnx::Conv[dilations=[1], group=512, kernel_shape=[31], pads=[0, 0], strides=[1]](%1122, %1121, %1120), scope: nemo.collections.asr.models.ctc_bpe_models.EncDecCTCModelBPE::/nemo.collections.asr.parts.submodules.conformer_modules.ConformerLayer::layers.0/nemo.collections.asr.parts.submodules.conformer_modules.ConformerConvolution::conv/nemo.collections.asr.parts.submodules.causal_convs.CausalConv1D::depthwise_conv # /mnt/d/MAI/STT_PER_EV/NEMO/env/lib/python3.8/site-packages/torch/nn/modules/conv.py:309:0
)’ (type ‘Tensor’) in the TorchScript graph. The containing node has kind ‘onnx::Conv’.]
(node defined in /mnt/d/MAI/STT_PER_EV/NEMO/env/lib/python3.8/site-packages/torch/nn/modules/conv.py(309): _conv_forward
/mnt/d/MAI/STT_PER_EV/NEMO/env/lib/python3.8/site-packages/torch/nn/modules/conv.py(313): forward
/mnt/d/MAI/STT_PER_EV/NEMO/NeMo/nemo/collections/asr/parts/submodules/causal_convs.py(148): forward
/mnt/d/MAI/STT_PER_EV/NEMO/env/lib/python3.8/site-packages/torch/nn/modules/module.py(1488): _slow_forward
/mnt/d/MAI/STT_PER_EV/NEMO/env/lib/python3.8/site-packages/torch/nn/modules/module.py(1501): _call_impl
/mnt/d/MAI/STT_PER_EV/NEMO/NeMo/nemo/collections/asr/parts/submodules/conformer_modules.py(374): forward
/mnt/d/MAI/STT_PER_EV/NEMO/env/lib/python3.8/site-packages/torch/nn/modules/module.py(1488): _slow_forward
/mnt/d/MAI/STT_PER_EV/NEMO/env/lib/python3.8/site-packages/torch/nn/modules/module.py(1501): _call_impl
/mnt/d/MAI/STT_PER_EV/NEMO/NeMo/nemo/collections/asr/parts/submodules/conformer_modules.py(211): forward
/mnt/d/MAI/STT_PER_EV/NEMO/env/lib/python3.8/site-packages/torch/nn/modules/module.py(1488): _slow_forward
/mnt/d/MAI/STT_PER_EV/NEMO/env/lib/python3.8/site-packages/torch/nn/modules/module.py(1501): _call_impl
/mnt/d/MAI/STT_PER_EV/NEMO/NeMo/nemo/collections/asr/modules/conformer_encoder.py(619): forward_internal
/mnt/d/MAI/STT_PER_EV/NEMO/NeMo/nemo/collections/asr/modules/conformer_encoder.py(508): forward_for_export
/mnt/d/MAI/STT_PER_EV/NEMO/NeMo/nemo/collections/asr/models/asr_model.py(192): forward_for_export
/mnt/d/MAI/STT_PER_EV/NEMO/env/lib/python3.8/site-packages/torch/nn/modules/module.py(1488): _slow_forward
/mnt/d/MAI/STT_PER_EV/NEMO/env/lib/python3.8/site-packages/torch/nn/modules/module.py(1501): _call_impl
/mnt/d/MAI/STT_PER_EV/NEMO/env/lib/python3.8/site-packages/torch/jit/_trace.py(118): wrapper
/mnt/d/MAI/STT_PER_EV/NEMO/env/lib/python3.8/site-packages/torch/jit/_trace.py(127): forward
/mnt/d/MAI/STT_PER_EV/NEMO/env/lib/python3.8/site-packages/torch/nn/modules/module.py(1501): _call_impl
/mnt/d/MAI/STT_PER_EV/NEMO/env/lib/python3.8/site-packages/torch/jit/_trace.py(1268): _get_trace_graph
/mnt/d/MAI/STT_PER_EV/NEMO/env/lib/python3.8/site-packages/torch/onnx/utils.py(893): _trace_and_get_graph_from_model
/mnt/d/MAI/STT_PER_EV/NEMO/env/lib/python3.8/site-packages/torch/onnx/utils.py(989): _create_jit_graph
/mnt/d/MAI/STT_PER_EV/NEMO/env/lib/python3.8/site-packages/torch/onnx/utils.py(1113): _model_to_graph
/mnt/d/MAI/STT_PER_EV/NEMO/env/lib/python3.8/site-packages/torch/onnx/utils.py(1548): _export
/mnt/d/MAI/STT_PER_EV/NEMO/env/lib/python3.8/site-packages/torch/onnx/utils.py(506): export
/mnt/d/MAI/STT_PER_EV/NEMO/NeMo/nemo/core/classes/exportable.py(220): _export
/mnt/d/MAI/STT_PER_EV/NEMO/NeMo/nemo/core/classes/exportable.py(113): export
/mnt/d/MAI/STT_PER_EV/NEMO/env/lib/python3.8/site-packages/nemo2riva/cookbook.py(81): export_model
/mnt/d/MAI/STT_PER_EV/NEMO/env/lib/python3.8/site-packages/nemo2riva/convert.py(83): Nemo2Riva
/mnt/d/MAI/STT_PER_EV/NEMO/env/lib/python3.8/site-packages/nemo2riva/cli/nemo2riva.py(49): nemo2riva
/mnt/d/MAI/STT_PER_EV/NEMO/env/bin/nemo2riva(8): 
)Hello,I have been trying for more than 7 days still getting same error. I tried with different python env like conda, venv and cuda versions with 11.4, 11.7 and 12. There is a same error.So, why don’t you try ‘nemo2riva --onnx-opset=15’ (or 16) ?Adding  --onnx-opset=15 worked for me, thanks !This topic was automatically closed 14 days after the last reply. New replies are no longer allowed.Powered by Discourse, best viewed with JavaScript enabled"
129,should-i-call-cudastreamsynchronize-before-executev2,"I have some code that copies some data to a GPU buffer and then runs a model, it looks like this:It runs well, but is there a possibility that execution might run in some circumstances before all of the input data has copied across? If so, should I use cudaMemcpy instead? or cudaStreamSynchronize(stream_); ? Would there be any meaningful difference?executeV2 TensorRT: nvinfer1::IExecutionContext Class ReferenceI assume that “runs  a model” means executing a kernel on the GPU. From the snippet it is not clear how that kernel is invoked. In other words, I do not recognize context_->executeV2 as a built-in CUDA mechanism and assume it is an abstraction layer you have built around kernel launches.Operations issued to the same CUDA stream will be executed by the hardware in the order they were enqueued by the software. By issuing the cudaMemcpyAsync and then launching the kernel, both to the same CUDA stream, it is ensured that the data transfer is completed prior to the kernel operating on that data.While simple CUDA programs may use synchronous cudaMemcpy calls, it is very common to see high-performance CUDA-accelerated applications make use of multiple CUDA streams and use cudaMemcpyAsync throughout. Often, this also requires some sort of inter-stream synchronization, see the documentation for that.executeV2 is part of the TensorRT API
https://docs.nvidia.com/deeplearning/tensorrt/api/c_api/classnvinfer1_1_1_i_execution_context.html#a50950856f23674d4cc3fe9b323a75101In that case I suggest asking on the TensorRT sub-forum.Powered by Discourse, best viewed with JavaScript enabled"
130,slow-inference-unet-industrial-tf-trt,"Hi,
I am new to NVIDIA tools, I am running the notebook example of training,  test and export to TF-TRT UNet Industrial on DAGM dataset provided at  UNet Industrial Inference Demo with TF-TRT.I was able to do all the pipeline and exported the checkpoint to inference with TF-TRT. I am running the inference in one image with the following command:It was supposed to be very fast (since that is the premise of doing inference on NGC Containers), but it takes 60~70 seconds to run the inference.What can I do to speed up this time?
Is there another way to load and predict my .pb model?
The model is attached in zip file.
TR-TRT-model-FP32.zip (6.5 MB)GPU Type:  NVidia RTX A6000
Container: ```
docker build . --rm -t unet_industrial:latestHi,Request you to share the model, script, profiler, and performance output if not shared already so that we can help you better.Alternatively, you can try running your model with trtexec command.master/samples/trtexecNVIDIA® TensorRT™, an SDK for high-performance deep learning inference, includes a deep learning inference optimizer and runtime that delivers low latency and high throughput for inference applicat...While measuring the model performance, make sure you consider the latency and throughput of the network inference, excluding the data pre and post-processing overhead.
Please refer to the below links for more details:This Best Practices Guide covers various performance considerations related to deploying networks using TensorRT 8.0.3. These sections assume that you have a model that is working at an appropriate level of accuracy and that you are able to...This Best Practices Guide covers various performance considerations related to deploying networks using TensorRT 8.0.3. These sections assume that you have a model that is working at an appropriate level of accuracy and that you are able to...Thanks!Powered by Discourse, best viewed with JavaScript enabled"
131,performance-regression-found-in-tensorrt-8-6-1-when-running-bert-on-gpu-t4,"We are using TensorRT via Triton to inference a BERT classification model.
Our model is simply a BERT base + classifier.
Previous we are using TRT 8.5.2, at tail of trtexec, the log showsAverage on 10 runs - GPU latency: 1.52078 ms - Host latency: 1.53953 ms (enqueue 1.5259 ms)But after we upgrade to TRT 8.6.1, at tail of trtexec, the log shows
Average on 10 runs - GPU latency: 4.13606 ms - Host latency: 4.15786 ms (enqueue 0.541821 ms)It shows alomost 3x latency regression, the source onnx model are same.
it seems the --fp16 didn’t work, I use nsys profile and see the 8.6.1 is always using fp32 cuda apisyou can see it’s using fp32 in 8.6.1
trt12158×510 37 KBbut it’s using fp16 in 8.5.2
trt21222×442 28.6 KBattached trtexec logs and nsight logsTensorRT Version: 8.6.1
GPU Type: T4
Nvidia Driver Version: 525.105.17
CUDA Version: 12.0
CUDNN Version:
Operating System + Version: Ubuntu 20.04
Python Version (if applicable):
TensorFlow Version (if applicable):
PyTorch Version (if applicable):
Baremetal or Container (if container which image + tag):Please attach or include links to any models, data, files, or scripts necessary to reproduce your issue. (Github repo, Google Drive, Dropbox, etc.)
TextAnalyzerV2.t4.852.trt.log (1.4 MB)
TextAnalyzerV2.t4.861.trt.log (1.3 MB)
trt852.nsys-rep (735.7 KB)
trt861.nsys-rep (1.2 MB)Please include:Hi,Request you to share the model, script, profiler, and performance output if not shared already so that we can help you better.Alternatively, you can try running your model with trtexec command.//github.com/NVIDIA/TensorRT/tree/master/samples/trtexecWhile measuring the model performance, make sure you consider the latency and throughput of the network inference, excluding the data pre and post-processing overhead.
Please refer to the below links for more details:This Best Practices Guide covers various performance considerations related to deploying networks using TensorRT 8.0.3. These sections assume that you have a model that is working at an appropriate level of accuracy and that you are able to...This Best Practices Guide covers various performance considerations related to deploying networks using TensorRT 8.0.3. These sections assume that you have a model that is working at an appropriate level of accuracy and that you are able to...Thanks!repro step:you can get my onnx model here model.test.nomask.op12.fp16.onnx - Google DriveHi @AakankshaS could you repro my issue by repro steps above?Hi @niuzheng168 ,We could reproduce the issue. Please allow us some time to work on this.
Thank you for reporting it to us.Powered by Discourse, best viewed with JavaScript enabled"
132,confusing-nms-layer-implementation-in-tensorrt-8-5,"We use NonMaxSuppression nodes in our ONNX models, and are then parsing them to build the TensorRT engine.
image1037×288 15.3 KBOur implementation using  v8.0.1 works great, the “EXPERIMENTAL” support for NMS in tensorrt-onnx uses the EfficientNMS_ONNX_TRT plugin.We are now upgrading to v8.5.3, where the INMSLayer has been introduced by TensorRT, and is now also used by onnx-tensorrt instead of the plugin. We are having a various issues with this implementation, and there are no examples of its usage.Previously the NMS implementation would output a fixed shape of indices, and the indices would be padded so you could detect the unique detections. Now the indices output shape is (-1, 3), requiring the more complex handling of dynamic outputs:This seems like unnecessary complexity, as I still have to allocate the memory for the maximum number of boxes, using the formula from the docs batchSize * numClasses * min(numInputBoundingBoxes, MaxOutputBoxesPerClass).According the documentation, there should be another output:
image1014×215 39.5 KB
However it appears onnx-tensorrt only uses the first output, see here:The old EfficientNMS_ONNX_TRT plugin had a param that was set outputONNXIndices, which ensure a single output, that conforms the ONNX specification. This is no longer the case with the name INMSLayer, which has two outputs.TensorRT Version: 8.5.3
GPU Type:  RTX 3070 Laptop GPU
Nvidia Driver Version: 525.125.06
CUDA Version: 11.4
Operating System + Version: Ubuntu 20.04Powered by Discourse, best viewed with JavaScript enabled"
133,cudnn8-0-5-calling-cudnncnninferversioncheck-will-increase-the-memory-usage-by-approximately-1-5g,"I am observing some slowdown on the first iteration of CNN training or inference. I fixed it with a call to cudnnCnnInferVersionCheck(). But calling cudnnCnnInferVersionCheck() will increase the memory usage by approximately 1.5G.  How can I delete it when my program is finished?
捕获796×245 27.6 KBHi @holymxl ,
Can you please help us with the cudnn version you are using?ThanksPowered by Discourse, best viewed with JavaScript enabled"
134,converting-nemo-encoder-decoder-rnnt-model-to-onnx,"Hello!
I am trying to convert nemo_asr.models.EncDecRNNTBPEModel to onnx with nemo.export(). I want to convert onnx model to tensorrt after that. The problem is that nemo.export() generates 2 different .onnx files for encoder and decoder. Separately I can convert them to trt format, but it can cause some difficulties in future. So, I want to have one model file, not 2. Will be grateful for any help.I just doimport nemo.collections.asr as nemo_asr
asr_model = nemo_asr.models.EncDecRNNTBPEModel.from_pretrained(“nvidia/stt_ru_conformer_transducer_large”)asr_model.export(‘mymodel3.onnx’)Hi @liamstartipsThanks for your interest in RivaThe Issue seems to be related to NemoPlease raise your question/query in the below Github linkExplore the GitHub Discussions forum for NVIDIA NeMo. Discuss code, ask questions & collaborate with the developer community.ThanksPowered by Discourse, best viewed with JavaScript enabled"
135,nvidia-mig-on-a100-supports-egl,"Hi everyone,
today I tried to split up some A100 40GB PCIe using MIG. I set up the GPU instance and the compute instance it it works fine for training some PyTorch models. However, I am using Mujoco (https://mujoco.org/) to generate data for my models and suddenly, Mujoco performs a magnitude worse (i.e. I had 500 frames/s before, now 40 frames/s).
If I disable MIG, it is all fine again and I am at full performance. I was wondering whether MIG has some restrictions on driver capabilities, since I used to have problems with Mujoco not using EGL properly, which resulted in similar problems.Best,
TimMIG does not support graphics APIs!See:User guide for Multi-Instance GPU on the NVIDIA® GPUs.This topic was automatically closed 14 days after the last reply. New replies are no longer allowed.Powered by Discourse, best viewed with JavaScript enabled"
136,unable-to-use-nvidia-maxine-ar-gazeredirect-due-to-error-in-line-77,"Every time I am trying to run the bat file but getting this error.ERROR: The GPU is not supported, line 77
ERROR: An error has occured while initializing Gaze Redirection
ERROR: an error has occured while creating a feature
[ WARN:0] terminating async callbackMy spec.
OS: Window 11 64x ; GPU: Nvidia GTX 1650 Ti with Max Q designPlease helphi,
The error message you’re encountering suggests that your GPU may not be fully supported for the task you’re trying to perform. Here are a few steps you can try to resolve the issue:Hi there @abhineshgotit welcome to the NVIDIA developer forums!I took the liberty of moving your post to the correct category under Maxine.Thanks!Thanks for the move! @guestblogcrowcrowcrow - 1650ti is not a supported GPU for use with Maxine. Maxine requires an NVIDIA GPU with Tensor cores. The 1650ti does not have Tensor cores.I’ve been able to get all the way through a bat.run command and had it create a file with Gaze at the end but upon clicking it it says file type not supported and says it might be a corrupted. I have used multiple different files all with the same outcome.
I have windows 11 and am using windows Media player to view them. The video can be watched BEFORE putting it through Maxine but not after.
I have a GTX 3070Any tips would be greatly appreciatedPlease delete the current output file, and reprocess your input file. Let me know if that resolves the situation.Also - sorry I am jacking this thread.I have tried that and it was a no go.I dont know if this is relevant but it’s a 5g file and it takes quite a bit of time to process. Upwards of 10 minutes for a 20 minute video. The videos I’ve seen of people doing it seem to be VERY fast comparativelyWhat is the resolution of the input file?1920x1080 shot on a GoPro hero 8 stationary with me sitting in front of it. 24fps 1080pThank you for that information.What is the command you’re using? When processing files I tend to use this :C:\Users\maxine\Desktop\ARX\samples\GazeRedirect>GazeRedirect.exe --model_path=…..\bin\models --offline_mode=true --split_screen_view=false --in=$INPUT_NAME.mp4 --out=$OUTPUT_NAME.mp4 --draw_vizualization=false --temporal=true --capture_outputs=falseGive that a go and see if it resolves anything. Please copy and paste your command for me to take a look, if you don’t mind.C:\Users\andre\OneDrive\Desktop\Gaze\MAXINE-AR-SDK-0.8.2.0\samples\GazeRedirect>run.batC:\Users\andre\OneDrive\Desktop\Gaze\MAXINE-AR-SDK-0.8.2.0\samples\GazeRedirect>SETLOCALC:\Users\andre\OneDrive\Desktop\Gaze\MAXINE-AR-SDK-0.8.2.0\samples\GazeRedirect>SET PATH=C:\Windows\system32;C:\Windows;C:\Windows\System32\Wbem;C:\Windows\System32\WindowsPowerShell\v1.0;C:\Windows\System32\OpenSSH;C:\Program Files\NVIDIA Corporation\NVIDIA NvDLISR;C:\Program Files (x86)\NVIDIA Corporation\PhysX\Common;C:\Users\andre\AppData\Local\Microsoft\WindowsApps;;…..\samples\external\opencv\bin;…..\bin;C:\Users\andre\OneDrive\Desktop\Gaze\MAXINE-AR-SDK-0.8.2.0\samples\GazeRedirect>SET NVAR_MODEL_DIR=…..\bin\modelsC:\Users\andre\OneDrive\Desktop\Gaze\MAXINE-AR-SDK-0.8.2.0\samples\GazeRedirect>GazeRedirect.exe --offline_mode --split_screen_view=false  --in=INPUTFILE.MOVThis is the code I’m using. all I type is run.bat at the end of the first line after typing cmd in the URL of the GazeRedirectsearchC:\Users\andre\OneDrive\Desktop\Gaze\MAXINE-AR-SDK-0.8.2.0\samples\GazeRedirect>run.batC:\Users\andre\OneDrive\Desktop\Gaze\MAXINE-AR-SDK-0.8.2.0\samples\GazeRedirect>SETLOCALC:\Users\andre\OneDrive\Desktop\Gaze\MAXINE-AR-SDK-0.8.2.0\samples\GazeRedirect>SET PATH=C:\Windows\system32;C:\Windows;C:\Windows\System32\Wbem;C:\Windows\System32\WindowsPowerShell\v1.0;C:\Windows\System32\OpenSSH;C:\Program Files\NVIDIA Corporation\NVIDIA NvDLISR;C:\Program Files (x86)\NVIDIA Corporation\PhysX\Common;C:\Users\andre\AppData\Local\Microsoft\WindowsApps;;…..\samples\external\opencv\bin;…..\bin;C:\Users\andre\OneDrive\Desktop\Gaze\MAXINE-AR-SDK-0.8.2.0\samples\GazeRedirect>SET NVAR_MODEL_DIR=…..\bin\modelsC:\Users\andre\OneDrive\Desktop\Gaze\MAXINE-AR-SDK-0.8.2.0\samples\GazeRedirect>GazeRedirect.exe --offline_mode --split_screen_view=false  --in=INPUTFILE.MOVHere is the video I got the code from How To Change Eye Contact On PRE-RECORDED VIDEOS! | NVIDIA MAXINE | Gaze Redirect - YouTubeThis is what happened when I tried your file. To be clear my file input is PFM49test.mov If I understand your code correctly am I meant to add a different name for the output? that’s why I added a 1. See belowC:\Users\andre>C:\Users\maxine\Desktop\ARX\samples\GazeRedirect>GazeRedirect.exe --model_path=……\bin\models --offline_mode=true --split_screen_view=false --in=$PFM49test.mov --out=$PFM49test1.mov --draw_vizualization=false --temporal=true
–capture_outputs=false
The system cannot find the path specified.Also Here is a screenshot of two parts, my Gaze file and my CMDNimbus Screenshot. Communicate & provide feedback faster!Switching to DMs :)I get this error when I try to run.bat on the cmd on a windows based system.Microsoft Windows [Version 10.0.19044.2846]
(c) Microsoft Corporation. All rights reserved.C:\Users\rhmn1010\Documents\BGIM\MAXINE-AR-SDK-0.8.2.0\MAXINE-AR-SDK-0.8.2.0\samples\GazeRedirect>run.batC:\Users\rhmn1010\Documents\BGIM\MAXINE-AR-SDK-0.8.2.0\MAXINE-AR-SDK-0.8.2.0\samples\GazeRedirect>SETLOCALC:\Users\rhmn1010\Documents\BGIM\MAXINE-AR-SDK-0.8.2.0\MAXINE-AR-SDK-0.8.2.0\samples\GazeRedirect>SET PATH=C:\WINDOWS\system32;C:\WINDOWS;C:\WINDOWS\System32\Wbem;C:\WINDOWS\System32\WindowsPowerShell\v1.0;c:\Program Files (x86)\Microsoft SQL Server\110\Tools\Binn;c:\Program Files (x86)\Microsoft SQL Server\110\DTS\Binn;C:\WINDOWS\System32\OpenSSH;C:\Users\rhmn1010\AppData\Local\Programs\Python\Python38-32\Scripts;C:\Users\rhmn1010\AppData\Local\Programs\Python\Python38-32;C:\Users\rhmn1010\AppData\Local\Microsoft\WindowsApps;C:\Users\rhmn1010\AppData\Local\atom\bin;%USERPROFILE%\AppData\Local\Microsoft\WindowsApps;;…..\samples\external\opencv\bin;…..\bin;C:\Users\rhmn1010\Documents\BGIM\MAXINE-AR-SDK-0.8.2.0\MAXINE-AR-SDK-0.8.2.0\samples\GazeRedirect>GazeRedirect.exe
Cannot create a cuda stream: CUDA driver version is insufficient for CUDA runtime version
ERROR: Initializing Gaze Engine failed
[ WARN:1] terminating async callbackC:\Users\rhmn1010\Documents\BGIM\MAXINE-AR-SDK-0.8.2.0\MAXINE-AR-SDK-0.8.2.0\samples\GazeRedirect>Can someone help me understand what’s going on with this? Thanks.Powered by Discourse, best viewed with JavaScript enabled"
137,network-has-dynamic-or-shape-inputs-but-no-optimization-profile-has-been-defined,"Please provide the following info (check/uncheck the boxes after creating this topic):
Software Version
 DRIVE OS Linux 5.2.6
[k] DRIVE OS Linux 5.2.6 and DriveWorks 4.0
 DRIVE OS Linux 5.2.0
 DRIVE OS Linux 5.2.0 and DriveWorks 3.5
 NVIDIA DRIVE™ Software 10.0 (Linux)
 NVIDIA DRIVE™ Software 9.0 (Linux)
 other DRIVE OS version
 otherTarget Operating System
[k] Linux
 QNX
 otherHardware Platform
[k] NVIDIA DRIVE™ AGX Xavier DevKit (E3550)
 NVIDIA DRIVE™ AGX Pegasus DevKit (E3550)
 otherSDK Manager Version
[k] 1.9.1.10844
 otherHost Machine Version
[k] native Ubuntu 18.04
 otherWhile using TesorRT Optimisation Tool. Getting error.
Converted keras tensorflow model to onnx model and trying to optimise onnx model.
Error and onnx model attached.

Error1299×741 169 KB

Mar3_3.onnx (1.9 MB)Hi,
Request you to share the ONNX model and the script if not shared already so that we can assist you better.
Alongside you can try few things:This NVIDIA TensorRT 8.5.3 Quick Start Guide is a starting point for developers who want to try out TensorRT SDK; specifically, this document demonstrates how to quickly construct an application to run inference on a TensorRT engine.import sys
import onnx
filename = yourONNXmodel
model = onnx.load(filename)
onnx.checker.check_model(model).
2) Try running your model with trtexec command.master/samples/trtexecNVIDIA® TensorRT™, an SDK for high-performance deep learning inference, includes a deep learning inference optimizer and runtime that delivers low latency and high throughput for inference applicat...
In case you are still facing issue, request you to share the trtexec “”–verbose"""" log for further debugging
Thanks!How to share the trtexec “”–verbose"""" log.
I am attaching screenshot of error.

Screenshot from 2023-03-10 16-19-461299×741 149 KB
Still facing the same issue.
Attaching the log file for debugging.
Check you help me for proceeding further.
log.txt (3.5 KB)When using runtime dimensions, you must create at least one optimization profile at build time. Please refer below link:This is the revision history of the NVIDIA TensorRT 8.6 Developer Guide.https://github.com/NVIDIA/TensorRT/blob/master/samples/opensource/sampleDynamicReshape/sampleDynamicReshape.cpp#L153Supported data format in TRT:This is the revision history of the NVIDIA TensorRT 8.6 Developer Guide.ThanksThanks @AakankshaS for information.
Could you help me how to create optimization profiles, if possible, share along with example model and optimization profile.Hi @alksainath.medam ,
Please refer to the following link for better understandingThis is the revision history of the NVIDIA TensorRT 8.6 Developer Guide.ThanksPowered by Discourse, best viewed with JavaScript enabled"
138,tensorrt-stable-diffusion-xl-support,"Nvidia stated that they worked on an extension for stable diffusion that implements TensorRT efficiently with no resolution limitations, when is that going to release?TensorRT version: latest
GPU Type: 4070ti
Nvidia Driver Version: latest
CUDA Version: 11.8
CUDNN Version: latest
Operating System + Version: window 11/10
Python Version (if applicable): 3.10.9
TensorFlow Version (if applicable): latest
PyTorch Version (if applicable): 2.0.1+cu118
Baremetal or Container (if container which image + tag):Attempted to convert the SDXL model into TensorRT, doesn’t work.Nvidia stated that they made a TensorRT inference for stable diffusion that has no shape limitations about 3 months ago. So why wouldn’t it work on SDXL? It’s a giant model capable of generating the highest quality of AI images. TensorRT increases performance on older versions of stable diffusion about ~80%<=, which is a dramatic performance increase without any degradation.Hi,Attempted to convert the SDXL model into TensorRT, doesn’t work.This is currently not supported. In future releases, support will be added.
Please stay tuned.Thank you.Powered by Discourse, best viewed with JavaScript enabled"
139,documentation-for-multi-model-serving-with-overcommit-on-triton,"I read in the seldon core documentation that multi-model serving with overcommit is available out of the box on nvidia tritonhttps://docs.seldon.io/projects/seldon-core/en/v2/contents/models/mms/mms.html?highlight=multi%20modal%20servingCan you please share documentation on how to configure and implement multi-model serving with overcommit using Nvida TritonHi,The below links might be useful for you.This Best Practices Guide covers various performance considerations related to deploying networks using TensorRT 8.0.3. These sections assume that you have a model that is working at an appropriate level of accuracy and that you are able to...The programming guide to the CUDA model and interface.https://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__STREAM.htmlFor multi-threading/streaming, will suggest you to use Deepstream or TRITONFor more details, we recommend you raise the query in Deepstream forum.orraise the query in Triton Inference Server Github instance issues section.Triton Inference Server has 30 repositories available. Follow their code on GitHub.Thanks!Powered by Discourse, best viewed with JavaScript enabled"
140,faster-inference-in-tensorrt-model,"I have an tensorrt engine. It is running fine, but the time of model inference fluctuates greatly. I want to know what factors lead to this resultHi there @ludi1 and welcome to the NVIDIA developer forums!I think your question is better asked in the TensorRT forums. If you don’t mind, I will move it over there.Thanks!Powered by Discourse, best viewed with JavaScript enabled"
141,tensorrt-inference-speed-is-lower-than-pytorch,"A clear and concise description of the bug or issue.TensorRT Version: 8.2.1
GPU Type: Jetson Xavier
Nvidia Driver Version:
CUDA Version: 10.2
CUDNN Version:
Operating System + Version:
Python Version (if applicable):
TensorFlow Version (if applicable):
PyTorch Version (if applicable):
Baremetal or Container (if container which image + tag):I have a point cloud classification model, written in pytorch:Model inference time is 18 ms on pytorch.
Than I convert it to onnx and after it to tensorrt through trtexec to have possibility to profile the model.
The inference time when convert in fp32 is 114 ms in fp16 is 91 ms.
Here is the profiling output of the model:There is the TopK operation that takes 34% of inference time aprox. 30 ms. But still if I’ll resolve the issue with this operations it will take 60 ms. that is 3 times greater than pytorch, How to understand where is the issue? I converted tons of models that work with images classification, detection and segmentation but all the models performed much better than pytorch models.P.S. I can not change the version of tensorrt to newer one because of dependencies.Hi,Request you to share the model, script, profiler, and performance output if not shared already so that we can help you better.Alternatively, you can try running your model with trtexec command.master/samples/trtexecNVIDIA® TensorRT™, an SDK for high-performance deep learning inference, includes a deep learning inference optimizer and runtime that delivers low latency and high throughput for inference applicat...While measuring the model performance, make sure you consider the latency and throughput of the network inference, excluding the data pre and post-processing overhead.
Please refer to the below links for more details:Page displayed for direct links that are not 
supported by current publishing formats.Page displayed for direct links that are not 
supported by current publishing formats.Thanks!trt tried (look at my post) and script of model have postedHi @andhover ,
Can you help us with the model so that we can try a repro at our end?ThanksPowered by Discourse, best viewed with JavaScript enabled"
142,deallocate-memory-assigned-using-iexecutioncontext,"In my application, I load multiple TensorRT engines and depending on certain conditions I use one of them. I wish to reduce memory consumption.
I see that I can use createExecutionContextWithoutDeviceMemory to allocate memory at a latter time but I’m not seeing an option to deallocate the assigned memory. The only option I’m seeing is to delete the current context and create a new one using the function mentioned above.
Is this the appropriate method and if not what alternative method should I follow?Thanks in advance!Jetpack Version: 4.6 / 5.0.2
TensorRT Version:  8.0.1 / 8.4.1
GPU Type:  Jetson NX
CUDA Version: 10.2 / 11.4
Operating System + Version:  Ubuntu 18.04/20.04Hi,This looks like a Jetson issue. Please refer to the below samples in case useful.Hello AI World guide to deploying deep-learning inference networks and deep vision primitives with TensorRT and NVIDIA Jetson. - GitHub - dusty-nv/jetson-inference: Hello AI World guide to deployin...An easy to use PyTorch to TensorRT converter. Contribute to NVIDIA-AI-IOT/torch2trt development by creating an account on GitHub.For any further assistance, we will move this post to to Jetson related forum.Thanks!Thanks for the reply!
Sorry, but as I understand it’s not specifically a Jetson issue but related to TensorRT itself.
The same issue will come on other devices with limited RAM.
The main issue is that I’m not able to find an API or class members that I could free to freeup some memory.the function: createExecutionContextWithoutDeviceMemory allows to allocate memory when we want but I couldn’t find a function to freeup the used up memory.I believe you would use cudaMalloc and cudaFree. You can pass the allocated memory using IExecutionContex::setDeviceMemory(). And you can get the size of memory needed for malloc using ICudaEngine::getDeviceMemorySize().setDeviceMemoryThanks a lot. I had mistaken that this function would allocate the memory itself but on checking the API reference I can see that we provide it the required memory block.
Thanks.I have one further question:
The description of the above mentioned function says:
The memory must be aligned with cuda memory alignment property. Would a memory block allocated using cudaMalloc (and size specified using getDeviceMemorySize())  satisfy the condition. I’m new to working with cuda so sorry if i’m asking something basic.I believe cudaMalloc would give you the correct alignment (I think that would be cruel if it didn’t, and they didn’t provide any more information on how to align it properly). But do not, for example, cudaMalloc a large block of memory (p) and pass in an arbitrary sub block of that memory (p+offset), unless you ensure that offset is a multiple of the proper alignment. To be honest, I’m not sure exactly what alignment requirement they are referring to, CUDA Runtime API :: CUDA Toolkit Documentation cudaMalloc documentation just says “The allocated memory is suitably aligned for any kind of variable”This topic was automatically closed 14 days after the last reply. New replies are no longer allowed.Powered by Discourse, best viewed with JavaScript enabled"
143,does-tensorrt-inference-app-eat-cuda-resources,"A clear and concise description of the bug or issue.TensorRT Version:
8.4.0
GPU Type:
Ampere Arch
Nvidia Driver Version:
470.82.01
CUDA Version:
11.4
CUDNN Version:
N/A
Operating System + Version:
Ubuntu 20.04 x86_64
Python Version (if applicable):
TensorFlow Version (if applicable):
PyTorch Version (if applicable):
Baremetal or Container (if container which image + tag):Please attach or include links to any models, data, files, or scripts necessary to reproduce your issue. (Github repo, Google Drive, Dropbox, etc.)Please include:
a732×926 139 KB
When running inference program using TensorRT, does the TensorRT lib eat FP32/INT32 HW resource?
Or it only eat TENSOR CORE resource? (above picture showed)
Thank you very much!Is there something you’re trying to achieve? Or is it just curiosity?I believe TensorRT will use whatever resources it can to maximize the inference speed of your layers. So it really depends. Maybe if certain precisions are enabled/disabled, something more concrete could be said.Maybe you’re hoping to share resources with another task? My impression is you’re better off assuming TensorRT is going to try to maximize your GPU usage while it’s running and there’s no room for another task, but I won’t say that for certain…Thanks， just want to know inside tensorRT.
for example:
if I run 4 tensorRT models, the cuda cores and tensor cores are serial or parallel running.
or tensorRT libs averaged assign hw resources for each model, parallel running.When you build the tensorrt engine, it will maximize inference speed, which means it will probably use as many resources as possible. This means if you try to run 4 models in parallel, they effectively will run in serial (this is unless your model is small and can’t use all resources). Don’t expect resources to be assigned differently at runtime, it’s all fixed when the engine is built as far as I know.Got it，thank you very much!Powered by Discourse, best viewed with JavaScript enabled"
144,tensorrt-plugin-if-the-dynamic-output-shape-can-only-be-determined-after-the-computation-of-the-plugin-how-to-handle-this-case,"Does tensorRT have a plan to support the situation where the output dimension is not known until the plugin is calculated? Now encountering this kind of problem, how can I solve it? Thanks a lot!Hi,
Please refer to below links related custom plugin implementation and sample:This Samples Support Guide provides an overview of all the supported NVIDIA TensorRT 8.5.3 samples included on GitHub and in the product package. The TensorRT samples specifically help in areas such as recommenders, machine comprehension, character...TensorRT is an SDK for high performance, deep learning inference. It includes a deep learning inference optimizer and a runtime that delivers low latency and high throughput for deep learning…While IPluginV2 and IPluginV2Ext interfaces are still supported for backward compatibility with TensorRT 5.1 and 6.0.x respectively, however, we recommend that you write new plugins or refactor existing ones to target the IPluginV2DynamicExt or IPluginV2IOExt interfaces instead.Thanks!Powered by Discourse, best viewed with JavaScript enabled"
145,build-engine-from-onnx-model-can-not-set-dynamic-shape-right,"I try to export my onnx(set dynmiac axes already) model to trt engine with dynamic shapes. I try to configured optimized profile to set the dynamic shapes, but failed.TensorRT Version: 8…6.1
GPU Type: RTX3090
Nvidia Driver Version: 11.3
CUDA Version: 11.7
CUDNN Version:
Operating System + Version: ubuntu 20.04
Python Version (if applicable): 3.8.10
TensorFlow Version (if applicable):
PyTorch Version (if applicable): 1.12
Baremetal or Container (if container which image + tag):Please attach or include links to any models, data, files, or scripts necessary to reproduce your issue. (Github repo, Google Drive, Dropbox, etc.)
code:
test_export_trt.py (4.5 KB)model:
decoder.onnx (2.4 MB)Change the onnx_model_path and run, check engine.get_profile_shapes and profile.get_shape if are samePlease include:Hi,
Request you to share the ONNX model and the script if not shared already so that we can assist you better.
Alongside you can try few things:This NVIDIA TensorRT 8.6.1 Quick Start Guide is a starting point for developers who want to try out TensorRT SDK; specifically, this document demonstrates how to quickly construct an application to run inference on a TensorRT engine.import sys
import onnx
filename = yourONNXmodel
model = onnx.load(filename)
onnx.checker.check_model(model).
2) Try running your model with trtexec command.master/samples/trtexecNVIDIA® TensorRT™, an SDK for high-performance deep learning inference, includes a deep learning inference optimizer and runtime that delivers low latency and high throughput for inference applicat...
In case you are still facing issue, request you to share the trtexec “”–verbose"""" log for further debugging
Thanks!The model and code I’ve already sent in Relevant Files block.
And I tried trtexec to convert the onnx, if I not set the min/opt/max dynamic shapes with commandI will got:
image1398×161 37.9 KBIf I set the dynamic shapes with command:And I got this:
image2022×304 67.2 KB
Whole trtexec log:
trtexec.log (10.0 KB)Hi,Sorry for the delayed response, Are you still facing this issue?Thank you.That’s okay,
Already found operator problem and replace it, thanks for reply.Powered by Discourse, best viewed with JavaScript enabled"
146,unable-to-access-cuda-using-torch-on-igx-orin-dev-kit,"Hardware - GPU (A6000)
Hardware - CPU
Operating System - custom yocto build, architecture: aarch64-poky-linuxI am unable to access CUDA using Pytorch. When I check torch.cuda.is_available(), I always get “False” although I have CUDA installed.
Also, I am unable to install libtorch on it. I get an error saying file format is wrong.Powered by Discourse, best viewed with JavaScript enabled"
147,i-cant-wait-to-get-access-sounds-amazing-could-you-approve,"My approval is still pending. I would love to get access. Is there anything I can do, any extra info you need ?Powered by Discourse, best viewed with JavaScript enabled"
148,ngc-is-broken-to-download-riva-cant-download-riva-skills-quick-start-please-help-as-i-have-to-create-tts-poc,"ngc registry resource download-version “nvidia/riva/riva_quickstart:2.11.0”Download status: FAILED
Downloaded local path resource: C:\Users\rskliar\PycharmProjects\text-to-speech-finetune\riva_quickstart_v2.11.0
Total files downloaded: 0
Total transferred: 0 B
Started at: 2023-06-20 10:27:24
Completed at: 2023-06-20 10:27:25
Duration taken: 1sHello,This may have been related to an issue with our download site over the weekend. Let us know if you still have issues.Best,
TomStill having the same issue - I am receiving the same error message.PS C:\Users\rskliar\PycharmProjects\text-to-speech-finetune> ngc registry resource download-version nvidia/riva/riva_quickstart:2.11.0
Getting files to download…
⠹ ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ • 0.0/141.5 KiB • Remaining: -:–:-- • ? • Elapsed: 0:00:01 • Total: 26 - Completed: 0 - Failed: 0Error getting pagination for file download:‘{“timestamp”:1687330630230,“status”:404,“error”:“Not Found”,“path”:“/v1/org/nvidia/team/riva/resources/riva_quickstart/2.11.0/files”}’
⠸ ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ • 0.0/141.5 KiB • Remaining: -:–:-- • ? • Elapsed: 0:00:01 • Total: 26 - Completed: 0 - Failed: 0==================================I am having the same problem:Can you please check it out?This looks like the same issue as described in this thread: 404 error.Unable to downloadTry downgrading to CLI version 3.22.0.Hi @rskliar @Manchu505Apologies, the download seems to be working now, can you tryThanksThe issue is still present for me:
I have just tested it. Here is what I get:Powered by Discourse, best viewed with JavaScript enabled"
149,trying-to-complie-sample-ar-sdk-apps-error-lnk1104-cannot-open-file-opencv346-lib,"HI, I wonder if someone could give me some advice, I can’t seem to find this exact opencv lib to get the MAXINE AR SDK samples to compile.  I’ve got opencv 3.4.6… assuming that’s what’s missing?For any other nerds like me who are having trouble following the instructions to build the MAXINE AR SDK sample apps. You need to point the cMake source to the root of the SDK, not sure what OSS stands for but this is what you have to do!This topic was automatically closed 14 days after the last reply. New replies are no longer allowed.Powered by Discourse, best viewed with JavaScript enabled"
150,windows11-tensowflow-gpu,"This is the error you encountered when running tensorflow-gpu: “loaded cudnn version 8901 could not load library zlibwapi.dll. error code 193. please verify that the library is built correctly for your processor architecture (32-bit, 64-bit)”. You’re using Windows 11 with tensorflow-gpu==2.10.0, cuda==12.1, and cudnn==8.9.1. I want to ask you guys how to solve this problem. Thank you!could not load library zlibwapi.dll.I tried it, but I still get this errorhttps://docs.nvidia.com/deeplearning/cudnn/install-guide/index.html#install-zlib-windowsI did it with reference to this webpage, but it is still wrong，Is there any other way to solve it?After rebooting several times, it suddenly worked, thanks!This topic was automatically closed 14 days after the last reply. New replies are no longer allowed.Powered by Discourse, best viewed with JavaScript enabled"
151,word-boosting-in-indian-english,"Hi,I have been using RIVA 2.10 on Jetson xavier NX. In the client script (python) , i have tried to boost different words that is used in our native language  . But the RIVA is not able to recognise any of the words   . I have tried both Conformer and Citrinet  1024.  I have tried to modify the vocabulary but no luck , Can some one help me to resolve the issue ?Thanks,MaheshHI @maheshchandranThanks for your interest in RivaApologies on the delayCan you provide us information about  list of words that you want to boost
I will check further with the teamThanksPowered by Discourse, best viewed with JavaScript enabled"
152,hello-i-want-to-make-a-small-change-to-detectnet-py-file-in-jetson-inference-the-output-should-be-streamed-on-tcp,"Hello. I want to make a small change to detectnet.py file in jetson-inference. the output should be streamed on TCP can jetson raise tsp server in detectnet python?Hi @mr.kir141Sorry for the late response, for Jetson platform related questions, suggest to open a new topic at Latest Autonomous Machines/Jetson & Embedded Systems topics - NVIDIA Developer Forums, the Jetson support team will help to answer your question there.Powered by Discourse, best viewed with JavaScript enabled"
153,egpu-nvidia-rtx-4090-linux-ubuntu-laptop-with-existing-nvidia-gpu,"I want to buy an external GPU (eGPU) for a laptop running Linux (ubuntu) and already has one nvidia GPU (NVIDIA GeForce RTX 3080 Ti Laptop GPU). I want to use both GPUs to train AI models developed in pytorch. The laptop has Thunderbolt 4 Could you propose a suitable eGPU for this case (e.g. AORUS RTX 3080 Ti GAMING BOX, or, razer core x chroma external graphics enclosure with nvidia RTX 4090) ?TensorRT Version:
GPU Type:
Nvidia Driver Version:
CUDA Version:
CUDNN Version:
Operating System + Version:
Python Version (if applicable):
TensorFlow Version (if applicable):
PyTorch Version (if applicable):
Baremetal or Container (if container which image + tag):Please attach or include links to any models, data, files, or scripts necessary to reproduce your issue. (Github repo, Google Drive, Dropbox, etc.)Please include:Hi,This forum talks more about updates and issues related to the TensorRT.
We recommend you to please reach out to the right platform to get better helpPlease refer to the following support matrix document for information regarding hardware compatibility with TensorRTThese support matrices provide a look into the supported platforms, features, and hardware capabilities of the NVIDIA TensorRT 8.6.1 APIs, parsers, and layers.Thank you.Powered by Discourse, best viewed with JavaScript enabled"
154,illegal-memory-access-error,"Hello, I’m trying to do inference for a Convolutional Network on RTX 3060. When I ran my python script, I got a pyCUDA error: Illegal Memory Access. My test env is Ubuntu 22.04, and RTX 3060 GPU. I have tried to recompile my ONNX model and rebuild the TensorRT engine.This is the log I get:The “Illegal Memory Access” error typically occurs when the program tries to access memory that it does not have permission to access, or when the memory has been corrupted.
Check that your GPU memory is not being overloaded.Powered by Discourse, best viewed with JavaScript enabled"
155,maxine-sdk-download-link-is-dead,"
image1445×389 112 KB

Hello, when I clicked browser download, nothing happens.Anyone is experiencing the same problem?Hello, Welcome to the forums! Thanks for bringing this to our attention.
I have the team looking into this now and will update the thread when I have more info.Best,
TomAny ETA of when the download link will be back online? Or are there any alternative download available ?this is abysmal response time from nvidia. I have the same problem and cannot download from either ngc command line nor the website. Just update your URLs already.Hey there, I`m facing the same problem. Did you find any solution?I have contacted responsible parties for Maxine’s assets available through NGC. To set expectations - this may take some time as the owner is currently out of the office.Sorry, may I ask you to provide the alt link to download before you can fix it on your website?
I want to download NVIDIA_AFX_SDK_Linux_1.3.0.21.tar.gzI have tapped someone who should be able to assist, I’ll get back to this thread as soon as I have an update. I saw there was an issue with this download link a while back, but I totally dropped the ball after getting deeply involved in other projects. Sorry about this, folks!I am still working on this issue, I will let everyone know when it is resolved.The issue is likely to be resolved on 6/22, thank you for your understanding!Working on this now.This may be related to a known bug in the API. The workaround is to downgrade to CLI version 3.22.0. You can downgrade from the CLI, the command is ngc version upgrade 3.22.0.This issue will be fixed soon.any ETA on when the download button will be fixed?still not working:(I could not downgrade ngc to ersion 3.22.0, it doesn’t work unfortunately.
Also, the link to Maxine SDK is still not working.
Could you please share an alternate link to download the file?Hi all,I have alerted the team that manages the platform.Thanks for your patience.The NGC team hopes to have a resolution today (27JUL23). I will update when I know more.I have an update! The issue has been resolved!There may be a bit of oddity when downloading large files from NGC. If you notice that you’re stuck on “Preparing Download” please go over to the “File Browser” tab. You can download what you need from there.This topic was automatically closed 14 days after the last reply. New replies are no longer allowed.Powered by Discourse, best viewed with JavaScript enabled"
156,maxine-error-error-the-requested-feature-is-not-yet-implemented,"VideoEffectsApp.exe  --verbose  --progress --effect=SuperRes --mode=0 --model_dir=…..\bin\models --in_file=C:\vids\in.mp4 --resolution=1080 --out_file=C:\vids\out.mp4  --showEffects:
Transfer
ArtifactReduction
SuperRes
Upscale
GreenScreen
Denoisingframe rate 30.034
frame count  878
duration 00:00:29.234
Error: The requested feature is not yet implementedUpscale work ok, but no SuperResLatest driver, latest sdk, latest firmware on  EVGA 3070tiHI there! I’ll take a look at this and get back to you. Could you provide me with the version number from the version.h file?Hello#define NVIDIA_VIDEOEFFECTS_SDK_VERSION_MAJOR 0
#define NVIDIA_VIDEOEFFECTS_SDK_VERSION_MINOR 7
#define NVIDIA_VIDEOEFFECTS_SDK_VERSION_RELEASE 2
#define NVIDIA_VIDEOEFFECTS_SDK_VERSION_BUILD 0#define NVIDIA_VIDEOEFFECTS_SDK_VERSION 0,7,2,0
#define NVIDIA_VIDEOEFFECTS_SDK_VERSION_MAJOR_MINOR 0,7
#define NVIDIA_VIDEOEFFECTS_SDK_VERSION_STRING “0.7.2.0”
#define NVIDIA_VIDEOEFFECTS_SDK_VERSION_STRING_SHORT “0.7.2”
#define NVIDIA_VIDEOEFFECTS_SDK_VERSION_STRING_MAJOR_MINOR “0.7”I not compiled yet anything, just testing the app provided and not work.
I also test older sdk, and check on multiple machines all with RTX cards, and the result is VideoEffectsApp.exe always fai with same message error.Letmeknow anything you need.So was there a solution? I still get the same error.Powered by Discourse, best viewed with JavaScript enabled"
157,tensorrt-installation-version-issue-in-docker-container,"A clear and concise description of the bug or issue.TensorRT Version:  Installation issue
GPU: A6000
Nvidia Driver Version: = 520.61.05
CUDA Version:  =11.8
Docker Image: = nvidia/cuda:11.8.0-devel-ubuntu20.04Im using the docker image nvidia/cuda:11.8.0-devel-ubuntu20.04, when I install tensorrt it upgrades my CUDA version 12.1.  I want to stay at 11.8.  I tried to target tensorrt to a specific version but that did not work.  Is there anyway I can stay on CUDA 11,8 and get tensorrt support?Dockerfile information:
rom nvidia/cuda:11.8.0-devel-ubuntu20.04ARG DEBIAN_FRONTEND=noninteractive
RUN apt-get update
RUN apt-get install -y tensorrtHi,
Please refer to the installation steps from the below link if in case you are missing on anythingThis NVIDIA TensorRT 8.6.0 Early Access (EA) Installation Guide provides the installation requirements, a list of what is included in the TensorRT package, and step-by-step instructions for installing TensorRT.
Also, we suggest you to use TRT NGC containers to avoid any system dependency related issues.NVIDIA TensorRT is a C++ library that facilitates high-performance inference on NVIDIA graphics processing units (GPUs). TensorRT takes a trained network and produces a highly optimized runtime engine that performs inference for that network.Thanks!Just want to point out that I have an issue open for a similar problem where you can’t install an older version of tensorrt using the steps in the documentation.Thank you for pointing me to your problem, they are very similar.  I just think if you pull down a cuda-11.8 image from NIVIDA installing TensorRT package should not be upgrading cuda version.  At the very least It should allow you to install alternative.  CUDA 11.8 did run with a version of TensorRT.  By the way I can get all the correct package versions our company needs on the metal.This topic was automatically closed 14 days after the last reply. New replies are no longer allowed.Powered by Discourse, best viewed with JavaScript enabled"
158,parse-dict-to-cudf,"I have messages in kafka and I’m trying read that messages with cuStreamz, my problems is that when I see my cudf with the messages, the rows have dictionaries,  I’m trying parse that info into standard cudf.For example I tried the next code:import cudfdef f(row):
return row[‘a’][‘1’][‘box’][0]z = {‘1’:{‘box’:[0,1,2,3]}}
df = cudf.DataFrame({‘a’: [z,z,z]})
print(df.apply(f))But shows the next error message:… ValueError: user defined function compilation failed.Somebody have an idea about how parse this kind of info?RewardsPowered by Discourse, best viewed with JavaScript enabled"
159,error-in-riva-deployment-riva-deployment-aborted,"Hardware - GPU= RTX 3080 (driver version= 515.86.01)
Hardware - CPU= Intel® Xeon(R) W-2255 CPU @ 3.70GHz × 20
Operating System - Ubuntu 22.04.1 LTS
Riva Version- 2.7.0
NeMo Version - 1.15Note: This may take some time, depending on the speed of your Internet connection.Pulling Riva Speech Server images.
Image nvcr.io/nvidia/riva/riva-speech:2.7.0 exists. Skipping.
Image nvcr.io/nvidia/riva/riva-speech:2.7.0-servicemaker exists. Skipping.NVIDIA Release  (build 46434655)
Copyright (c) 2016-2022, NVIDIA CORPORATION & AFFILIATES. All rights reserved.Copyright (c) 2018-2022, NVIDIA CORPORATION & AFFILIATES. All rights reserved.An SDK with an optimizer for high-performance deep learning inference.Various files include modifications (c) NVIDIA CORPORATION & AFFILIATES.  All rights reserved.This container image and its contents are governed by the NVIDIA Deep Learning Container License.
By pulling and using the container, you accept the terms and conditions of this license:145.24 KBTo install Python sample dependencies, run /opt/tensorrt/python/python_setup.shTo install the open-source samples corresponding to this TensorRT release version
run /opt/tensorrt/install_opensource.sh.  To build the open source parsers,
plugins, and samples for current top-of-tree on master or a different branch,
run /opt/tensorrt/install_opensource.sh -b 
See GitHub - NVIDIA/TensorRT: NVIDIA® TensorRT™, an SDK for high-performance deep learning inference, includes a deep learning inference optimizer and runtime that delivers low latency and high throughput for inference applications. for more information./bin/bash: warning: setlocale: LC_ALL: cannot change locale (en_US.UTF-8)
2023-02-24 17:22:13,738 [INFO] Writing Riva model repository to ‘/data/models’…
2023-02-24 17:22:13,738 [INFO] The riva model repo target directory is /data/models
2023-02-24 17:22:23,173 [INFO] Using obey-precision pass with fp16 TRT
2023-02-24 17:22:23,174 [INFO] Extract_binaries for nn → /data/models/riva-trt-conformer-en-US-asr-streaming-am-streaming/1
2023-02-24 17:22:23,174 [INFO] extracting {‘onnx’: (‘nemo.collections.asr.models.ctc_bpe_models.EncDecCTCModelBPE’, ‘model_graph.onnx’)} → /data/models/riva-trt-conformer-en-US-asr-streaming-am-streaming/1
2023-02-24 17:22:23,781 [INFO] Printing copied artifacts:
2023-02-24 17:22:23,781 [INFO] {‘onnx’: ‘/data/models/riva-trt-conformer-en-US-asr-streaming-am-streaming/1/model_graph.onnx’}
2023-02-24 17:22:23,781 [INFO] Building TRT engine from ONNX file
[02/24/2023-17:22:26] [TRT] [W] parsers/onnx/onnx2trt_utils.cpp:364: Your ONNX model has been generated with INT64 weights, while TensorRT does not natively support INT64. Attempting to cast down to INT32.
[02/24/2023-17:22:26] [TRT] [W] parsers/onnx/onnx2trt_utils.cpp:392: One or more weights outside the range of INT32 was clamped
[02/24/2023-17:22:26] [TRT] [W] parsers/onnx/onnx2trt_utils.cpp:392: One or more weights outside the range of INT32 was clamped
[02/24/2023-17:22:26] [TRT] [W] parsers/onnx/onnx2trt_utils.cpp:392: One or more weights outside the range of INT32 was clamped
[02/24/2023-17:22:26] [TRT] [W] parsers/onnx/onnx2trt_utils.cpp:392: One or more weights outside the range of INT32 was clamped
[02/24/2023-17:22:26] [TRT] [W] parsers/onnx/onnx2trt_utils.cpp:392: One or more weights outside the range of INT32 was clamped
[02/24/2023-17:22:26] [TRT] [W] parsers/onnx/onnx2trt_utils.cpp:392: One or more weights outside the range of INT32 was clamped
[02/24/2023-17:22:26] [TRT] [W] parsers/onnx/onnx2trt_utils.cpp:392: One or more weights outside the range of INT32 was clamped
[02/24/2023-17:22:26] [TRT] [W] parsers/onnx/onnx2trt_utils.cpp:392: One or more weights outside the range of INT32 was clamped
[02/24/2023-17:22:26] [TRT] [W] parsers/onnx/onnx2trt_utils.cpp:392: One or more weights outside the range of INT32 was clamped
[02/24/2023-17:22:26] [TRT] [W] parsers/onnx/onnx2trt_utils.cpp:392: One or more weights outside the range of INT32 was clamped
[02/24/2023-17:22:26] [TRT] [W] parsers/onnx/onnx2trt_utils.cpp:392: One or more weights outside the range of INT32 was clamped
[02/24/2023-17:22:26] [TRT] [W] parsers/onnx/onnx2trt_utils.cpp:392: One or more weights outside the range of INT32 was clamped
[02/24/2023-17:22:26] [TRT] [W] parsers/onnx/onnx2trt_utils.cpp:392: One or more weights outside the range of INT32 was clamped
[02/24/2023-17:22:26] [TRT] [W] parsers/onnx/onnx2trt_utils.cpp:392: One or more weights outside the range of INT32 was clamped
[02/24/2023-17:22:26] [TRT] [W] parsers/onnx/onnx2trt_utils.cpp:392: One or more weights outside the range of INT32 was clamped
[02/24/2023-17:22:26] [TRT] [W] parsers/onnx/onnx2trt_utils.cpp:392: One or more weights outside the range of INT32 was clamped
[02/24/2023-17:22:26] [TRT] [W] parsers/onnx/onnx2trt_utils.cpp:392: One or more weights outside the range of INT32 was clamped
[02/24/2023-17:22:26] [TRT] [W] parsers/onnx/onnx2trt_utils.cpp:392: One or more weights outside the range of INT32 was clamped
[02/24/2023-17:22:26] [TRT] [W] parsers/onnx/onnx2trt_utils.cpp:392: One or more weights outside the range of INT32 was clamped
[02/24/2023-17:22:26] [TRT] [W] parsers/onnx/onnx2trt_utils.cpp:392: One or more weights outside the range of INT32 was clamped
[02/24/2023-17:22:26] [TRT] [W] parsers/onnx/onnx2trt_utils.cpp:392: One or more weights outside the range of INT32 was clamped
[02/24/2023-17:22:26] [TRT] [W] parsers/onnx/onnx2trt_utils.cpp:392: One or more weights outside the range of INT32 was clamped
[02/24/2023-17:22:26] [TRT] [W] parsers/onnx/onnx2trt_utils.cpp:392: One or more weights outside the range of INT32 was clamped
[02/24/2023-17:22:26] [TRT] [W] parsers/onnx/onnx2trt_utils.cpp:392: One or more weights outside the range of INT32 was clamped
[02/24/2023-17:22:26] [TRT] [W] parsers/onnx/onnx2trt_utils.cpp:392: One or more weights outside the range of INT32 was clamped
[02/24/2023-17:22:26] [TRT] [W] parsers/onnx/onnx2trt_utils.cpp:392: One or more weights outside the range of INT32 was clamped
[02/24/2023-17:22:26] [TRT] [W] parsers/onnx/onnx2trt_utils.cpp:392: One or more weights outside the range of INT32 was clamped
[02/24/2023-17:22:26] [TRT] [W] parsers/onnx/onnx2trt_utils.cpp:392: One or more weights outside the range of INT32 was clamped
[02/24/2023-17:22:26] [TRT] [W] parsers/onnx/onnx2trt_utils.cpp:392: One or more weights outside the range of INT32 was clamped
[02/24/2023-17:22:26] [TRT] [W] parsers/onnx/onnx2trt_utils.cpp:392: One or more weights outside the range of INT32 was clamped
[02/24/2023-17:22:26] [TRT] [W] parsers/onnx/onnx2trt_utils.cpp:392: One or more weights outside the range of INT32 was clamped
[02/24/2023-17:22:26] [TRT] [W] parsers/onnx/onnx2trt_utils.cpp:392: One or more weights outside the range of INT32 was clamped
[02/24/2023-17:22:26] [TRT] [W] parsers/onnx/onnx2trt_utils.cpp:392: One or more weights outside the range of INT32 was clamped
[02/24/2023-17:22:26] [TRT] [W] parsers/onnx/onnx2trt_utils.cpp:392: One or more weights outside the range of INT32 was clamped
[02/24/2023-17:22:26] [TRT] [W] parsers/onnx/onnx2trt_utils.cpp:392: One or more weights outside the range of INT32 was clamped
[02/24/2023-17:22:26] [TRT] [W] parsers/onnx/onnx2trt_utils.cpp:392: One or more weights outside the range of INT32 was clamped
[02/24/2023-17:27:42] [TRT] [W] Output type must be INT32 for shape outputs
2023-02-24 17:27:43,097 [INFO] Mixed-precision net: 7115 layers, 7115 tensors, 1 outputs…
2023-02-24 17:27:43,273 [INFO] Mixed-precision net: 1592 layers / 1592 outputs fixed
[02/24/2023-17:30:14] [TRT] [W] Myelin graph with multiple dynamic values may have poor performance if they differ. Dynamic values are:
[02/24/2023-17:30:14] [TRT] [W]  (# 0 (SHAPE audio_signal))
[02/24/2023-17:30:14] [TRT] [W]  (# 0 (SHAPE length))
python3: /root/gpgpu/MachineLearning/myelin/src/compiler/optimizer/const_ppg.cpp:2815: void myelin::ir::copy_slice_data(myelinType_t, void*, const void*, const symbolic_shape_t&, const symbolic_shape_t&, const symbolic_shape_t&, const symbolic_shape_t&, const symbolic_shape_t&, const symbolic_shape_t&, const int_const_shape_t&): Assertion 0' failed. /usr/local/bin/deploy_all_models: line 21:    98 Aborted                 riva-deploy $FORCE find $rmir_path -name *.rmir -printf ""%p:${MODEL_DEPLOY_KEY} ""` $output_pathI get this error by running the “riva_init.sh” in the Riva quick-start folder version 2.7.0. The model is a fine-tuned stt_en_conformer_ctc_large model using this training notebook in the Riva tutorials NeMo version 1.15.0. With only changes to config being data, learning rate, vocab.
My nemo2riva and riva-build commands do not give any errors.My riva-build command before running riva_init.sh:
riva-build speech_recognition 
/data/rmir/stt_en_conformer_ctc_large.rmir 
/servicemaker-dev/Conformer-CTC-BPE.riva 
–name=conformer-en-US-asr-streaming 
–featurizer.use_utterance_norm_params=False 
–featurizer.precalc_norm_time_steps=0 
–featurizer.precalc_norm_params=False 
–ms_per_timestep=40 
–endpointing.start_history=200 
–nn.fp16_needs_obey_precision_pass 
–endpointing.residue_blanks_at_start=-2 
–chunk_size=0.16 
–left_padding_size=1.92 
–right_padding_size=1.92 
–decoder_type=greedy 
–language_code=en-USAny help on this would be much appreciated.Hi @adjohnsonThanks for your interest in RivaI guess it is enough to mount the /data folder in start, in place of /data/models and /data/rmir that was usedPlease verify the /data folder to be mounted has the models folder and subsequent required models inside iti.e after your deployriva-deploy -f <rmir_filename>:<encryption_key> /data/modelsUseplease find the document reference
https://docs.nvidia.com/deeplearning/riva/user-guide/docs/model-overview.html#custom-modelsThanksTo update:
I ran the same deployment process using Riva 2.9.0 and the model successfully deployed and ran. My initial guess is that the documentation is wrong on what versions of NeMo that the models were created on is supported by Riva?This topic was automatically closed 14 days after the last reply. New replies are no longer allowed.Powered by Discourse, best viewed with JavaScript enabled"
160,cuopt-microservice-was-not-found,"I tried to deploy cuOpt container (cuopt:22.12) on WSL2/Ubuntu to test cuopt extensions on Omniverse. I manage to establish the connection with the cuopt microservice and I get the IP address and port number. However, when I try to start the cuopt extension on Omniverse, it doesn’t work and can’t find the microservice. I can’t figure out what the problem is, can you help me?
cuopt connexion912×107 4.46 KB
cuopt omniverse572×608 31.5 KBPowered by Discourse, best viewed with JavaScript enabled"
161,something-wrong-with-riva-quickstart,"Tried on two different systems and riva quickstart keeps failing to launch.Hi @ryeinThanks for your interest in RivaCan you please share with us theThanksNot sure if it’s same issue, but I also had trouble with:riva_init.sh was failing with:What I did to get it to proceed was to modify the docker run calls in riva_init.sh to use “–privileged” anywhere that call also used --gpus (there were 2 of such calls)Context:It now seems to be doing a whole lot of something… and I can see it’s using my GPU resources, so, that’s good. (not sure if I will run into issues with riva_start.sh yet, I have not gotten that far)Hopefully that helps someone…follow up on my previous post - I had to do the opposite on riva_start.sh - by adding “–gpus all” to the docker command which only had “–privileged” in it…but then it did all seem to work, and I was able to run examples.Note: I’m on a 4090, which doesn’t have enough ram to run all models at same time either, so I also disabled nlp and tts services at the top of config.sh and did a riva_clean.sh and then re-initialized.Thanks for the info.  I for sure need to play with it more.  I’m sure it was user error.From what I can tell, Riva will not yet run gpu’s beyond the Ampere architecture (30 series).  If true, Riva does not support the 40 series.  Note in your log above:Thanks so much @rbgreenway and @jason.greyThanks for your kind inputs, Really appreciateSincere Apologies for the long delayI will confirmThanksPowered by Discourse, best viewed with JavaScript enabled"
162,tensorrt-conversion-from-onnx-for-keras-ocr-models-fails-because-of-int32-input-in-intermediate-layers,"I am using polygraphy to convert my onnx model to tensorrt. However it fails because it complains that -[E] In node 95 (notInvalidType): UNSUPPORTED_NODE: Found invalid input type of INT32
this is an input to a matmul layer in the modelTensorRT Version: 8.5.3.1
GPU Type: T4
Nvidia Driver Version: 510
CUDA Version: 11.6
CUDNN Version: 8.0
Operating System + Version: ubuntu 18.04
Python Version (if applicable): 3.9
TensorFlow Version (if applicable): 2.7
PyTorch Version (if applicable):
Baremetal or Container (if container which image + tag):recognizer.onnx (33.6 MB)run in terminalError trace:Hi,
Request you to share the ONNX model and the script if not shared already so that we can assist you better.
Alongside you can try few things:This NVIDIA TensorRT 8.5.3 Quick Start Guide is a starting point for developers who want to try out TensorRT SDK; specifically, this document demonstrates how to quickly construct an application to run inference on a TensorRT engine.import sys
import onnx
filename = yourONNXmodel
model = onnx.load(filename)
onnx.checker.check_model(model).
2) Try running your model with trtexec command.master/samples/trtexecNVIDIA® TensorRT™, an SDK for high-performance deep learning inference, includes a deep learning inference optimizer and runtime that delivers low latency and high throughput for inference applicat...
In case you are still facing issue, request you to share the trtexec “”–verbose"""" log for further debugging
Thanks!Powered by Discourse, best viewed with JavaScript enabled"
163,nvidia-rapids-ai-data-s3-bucket-permission-denied,"I am deploying digital fingerprinting model with Nvidia Morpheus on a local machine, and I’m using the “fetch_example_data.py” script to download Azure training data, but I do not have sufficient permissions to pull down the training data from the rapids-ai S3 bucket.TensorRT Version: N/A
GPU Type: Nvidia 3080 Ti
Nvidia Driver Version: 536.23
CUDA Version: 12.2
CUDNN Version: N/A
Operating System + Version:  Ubuntu
Python Version (if applicable): 3.10
TensorFlow Version (if applicable): N/A
PyTorch Version (if applicable): N/A
Baremetal or Container (if container which image + tag): Docker Container#!/usr/bin/env python3import argparse
import os
from os.path import dirname
from os.path import existsimport s3fsAZURE_TRAINING_FILES = [
‘AZUREAD_2022-08-01T00_03_56.207Z.json’,
‘AZUREAD_2022-08-01T03_05_08.046Z.json’,
‘AZUREAD_2022-08-01T06_16_33.925Z.json’,
‘AZUREAD_2022-08-01T09_03_21.665Z.json’,
‘AZUREAD_2022-08-01T12_18_15.395Z.json’,
‘AZUREAD_2022-08-01T15_05_33.622Z.json’,
‘AZUREAD_2022-08-01T18_01_39.553Z.json’,
‘AZUREAD_2022-08-01T21_36_12.642Z.json’,
‘AZUREAD_2022-08-02T00_03_57.781Z.json’,
‘AZUREAD_2022-08-02T03_14_51.398Z.json’,
‘AZUREAD_2022-08-02T06_08_43.120Z.json’,
‘AZUREAD_2022-08-02T09_04_46.164Z.json’,
‘AZUREAD_2022-08-02T12_16_42.973Z.json’,
‘AZUREAD_2022-08-02T15_01_57.753Z.json’,
‘AZUREAD_2022-08-02T18_07_04.839Z.json’,
‘AZUREAD_2022-08-02T21_04_39.405Z.json’,
‘AZUREAD_2022-08-03T00_10_42.770Z.json’,
‘AZUREAD_2022-08-03T03_04_39.062Z.json’,
‘AZUREAD_2022-08-03T07_03_02.647Z.json’,
‘AZUREAD_2022-08-03T09_12_42.431Z.json’,
‘AZUREAD_2022-08-03T12_01_59.616Z.json’,
‘AZUREAD_2022-08-03T15_04_36.967Z.json’,
‘AZUREAD_2022-08-03T18_33_53.602Z.json’,
‘AZUREAD_2022-08-03T21_10_53.763Z.json’,
‘AZUREAD_2022-08-04T00_47_51.564Z.json’,
‘AZUREAD_2022-08-04T03_29_10.364Z.json’,
‘AZUREAD_2022-08-04T06_22_30.326Z.json’,
‘AZUREAD_2022-08-04T09_01_47.489Z.json’,
‘AZUREAD_2022-08-04T12_00_37.255Z.json’,
‘AZUREAD_2022-08-04T15_06_58.553Z.json’,
‘AZUREAD_2022-08-04T18_20_25.773Z.json’,
‘AZUREAD_2022-08-04T21_13_42.613Z.json’,
‘AZUREAD_2022-08-05T00_14_48.503Z.json’,
‘AZUREAD_2022-08-05T03_27_16.392Z.json’,
‘AZUREAD_2022-08-05T06_14_02.065Z.json’,
‘AZUREAD_2022-08-05T09_19_35.102Z.json’,
‘AZUREAD_2022-08-05T12_24_54.388Z.json’,
‘AZUREAD_2022-08-05T15_02_19.596Z.json’,
‘AZUREAD_2022-08-05T18_07_31.442Z.json’,
‘AZUREAD_2022-08-05T21_10_10.626Z.json’,
‘AZUREAD_2022-08-06T00_08_48.348Z.json’,
‘AZUREAD_2022-08-06T03_00_47.733Z.json’,
‘AZUREAD_2022-08-06T06_00_32.252Z.json’,
‘AZUREAD_2022-08-06T09_21_44.486Z.json’,
‘AZUREAD_2022-08-06T12_06_11.372Z.json’,
‘AZUREAD_2022-08-06T15_00_53.066Z.json’,
‘AZUREAD_2022-08-06T18_15_37.469Z.json’,
‘AZUREAD_2022-08-06T21_04_29.994Z.json’,
‘AZUREAD_2022-08-07T00_00_23.959Z.json’,
‘AZUREAD_2022-08-07T03_11_01.088Z.json’,
‘AZUREAD_2022-08-07T06_02_39.472Z.json’,
‘AZUREAD_2022-08-07T09_05_02.341Z.json’,
‘AZUREAD_2022-08-07T12_02_57.483Z.json’,
‘AZUREAD_2022-08-07T16_00_01.986Z.json’,
‘AZUREAD_2022-08-07T18_27_37.071Z.json’,
‘AZUREAD_2022-08-07T21_28_01.308Z.json’,
‘AZUREAD_2022-08-08T00_16_13.439Z.json’,
‘AZUREAD_2022-08-08T03_00_06.848Z.json’,
‘AZUREAD_2022-08-08T06_10_54.304Z.json’,
‘AZUREAD_2022-08-08T09_07_13.422Z.json’,
‘AZUREAD_2022-08-08T12_02_00.653Z.json’,
‘AZUREAD_2022-08-08T15_09_32.434Z.json’,
‘AZUREAD_2022-08-08T18_05_08.444Z.json’,
‘AZUREAD_2022-08-08T21_22_25.239Z.json’,
‘AZUREAD_2022-08-09T00_23_17.790Z.json’,
‘AZUREAD_2022-08-09T03_09_35.759Z.json’,
‘AZUREAD_2022-08-09T06_01_19.285Z.json’,
‘AZUREAD_2022-08-09T09_17_46.111Z.json’,
‘AZUREAD_2022-08-09T12_04_05.262Z.json’,
‘AZUREAD_2022-08-09T15_07_52.196Z.json’,
‘AZUREAD_2022-08-09T18_22_41.664Z.json’,
‘AZUREAD_2022-08-09T21_08_50.818Z.json’,
‘AZUREAD_2022-08-10T00_16_26.105Z.json’,
‘AZUREAD_2022-08-10T03_08_34.627Z.json’,
‘AZUREAD_2022-08-10T06_18_05.500Z.json’,
‘AZUREAD_2022-08-10T09_01_38.710Z.json’,
‘AZUREAD_2022-08-10T12_00_06.306Z.json’,
‘AZUREAD_2022-08-10T15_22_39.506Z.json’,
‘AZUREAD_2022-08-10T18_02_01.694Z.json’,
‘AZUREAD_2022-08-10T21_25_24.068Z.json’,
‘AZUREAD_2022-08-11T00_07_04.185Z.json’,
‘AZUREAD_2022-08-11T03_03_44.313Z.json’,
‘AZUREAD_2022-08-11T06_20_48.479Z.json’,
‘AZUREAD_2022-08-11T09_14_40.692Z.json’,
‘AZUREAD_2022-08-11T12_06_40.970Z.json’,
‘AZUREAD_2022-08-11T15_00_12.795Z.json’,
‘AZUREAD_2022-08-11T18_04_26.402Z.json’,
‘AZUREAD_2022-08-11T21_03_53.754Z.json’,
‘AZUREAD_2022-08-12T00_04_59.094Z.json’,
‘AZUREAD_2022-08-12T03_02_32.601Z.json’,
‘AZUREAD_2022-08-12T06_20_35.849Z.json’,
‘AZUREAD_2022-08-12T09_21_57.950Z.json’,
‘AZUREAD_2022-08-12T12_00_16.796Z.json’,
‘AZUREAD_2022-08-12T15_00_32.506Z.json’,
‘AZUREAD_2022-08-12T18_00_10.559Z.json’,
‘AZUREAD_2022-08-12T21_05_00.919Z.json’,
‘AZUREAD_2022-08-13T00_17_54.530Z.json’,
‘AZUREAD_2022-08-13T03_26_09.056Z.json’,
‘AZUREAD_2022-08-13T06_00_54.755Z.json’,
‘AZUREAD_2022-08-13T09_28_02.146Z.json’,
‘AZUREAD_2022-08-13T12_36_25.676Z.json’,
‘AZUREAD_2022-08-13T15_07_29.394Z.json’,
‘AZUREAD_2022-08-13T18_43_43.943Z.json’,
‘AZUREAD_2022-08-13T21_02_34.964Z.json’,
‘AZUREAD_2022-08-14T00_35_43.377Z.json’,
‘AZUREAD_2022-08-14T03_05_34.273Z.json’,
‘AZUREAD_2022-08-14T06_29_54.324Z.json’,
‘AZUREAD_2022-08-14T09_11_35.224Z.json’,
‘AZUREAD_2022-08-14T12_22_29.216Z.json’,
‘AZUREAD_2022-08-14T15_21_13.429Z.json’,
‘AZUREAD_2022-08-14T18_12_20.996Z.json’,
‘AZUREAD_2022-08-14T21_00_53.772Z.json’,
‘AZUREAD_2022-08-15T00_12_27.596Z.json’,
‘AZUREAD_2022-08-15T03_21_24.074Z.json’,
‘AZUREAD_2022-08-15T06_05_11.987Z.json’,
‘AZUREAD_2022-08-15T09_03_38.276Z.json’,
‘AZUREAD_2022-08-15T12_00_09.975Z.json’,
‘AZUREAD_2022-08-15T15_01_22.571Z.json’,
‘AZUREAD_2022-08-15T18_04_07.706Z.json’,
‘AZUREAD_2022-08-15T21_16_21.836Z.json’,
‘AZUREAD_2022-08-16T00_17_08.148Z.json’,
‘AZUREAD_2022-08-16T03_04_14.916Z.json’,
‘AZUREAD_2022-08-16T06_00_05.126Z.json’,
‘AZUREAD_2022-08-16T09_20_00.713Z.json’,
‘AZUREAD_2022-08-16T12_21_42.855Z.json’,
‘AZUREAD_2022-08-16T15_07_58.824Z.json’,
‘AZUREAD_2022-08-16T18_26_19.807Z.json’,
‘AZUREAD_2022-08-16T21_10_37.846Z.json’,
‘AZUREAD_2022-08-17T00_07_39.058Z.json’,
‘AZUREAD_2022-08-17T03_00_28.451Z.json’,
‘AZUREAD_2022-08-17T06_13_41.197Z.json’,
‘AZUREAD_2022-08-17T09_21_28.995Z.json’,
‘AZUREAD_2022-08-17T12_09_04.770Z.json’,
‘AZUREAD_2022-08-17T15_18_32.828Z.json’,
‘AZUREAD_2022-08-17T18_02_43.590Z.json’,
‘AZUREAD_2022-08-17T21_03_50.905Z.json’,
‘AZUREAD_2022-08-18T00_04_04.684Z.json’,
‘AZUREAD_2022-08-18T03_36_06.261Z.json’,
‘AZUREAD_2022-08-18T06_02_47.638Z.json’,
‘AZUREAD_2022-08-18T09_39_38.604Z.json’,
‘AZUREAD_2022-08-18T12_17_15.899Z.json’,
‘AZUREAD_2022-08-18T15_38_44.291Z.json’,
‘AZUREAD_2022-08-18T18_16_39.557Z.json’,
‘AZUREAD_2022-08-18T21_01_07.323Z.json’,
‘AZUREAD_2022-08-19T00_03_26.920Z.json’,
‘AZUREAD_2022-08-19T03_05_56.636Z.json’,
‘AZUREAD_2022-08-19T06_15_54.060Z.json’,
‘AZUREAD_2022-08-19T09_19_05.120Z.json’,
‘AZUREAD_2022-08-19T12_17_08.196Z.json’,
‘AZUREAD_2022-08-19T15_15_11.004Z.json’,
‘AZUREAD_2022-08-19T18_01_30.625Z.json’,
‘AZUREAD_2022-08-19T21_01_20.621Z.json’,
‘AZUREAD_2022-08-20T00_19_23.348Z.json’,
‘AZUREAD_2022-08-20T03_04_23.422Z.json’,
‘AZUREAD_2022-08-20T06_02_30.851Z.json’,
‘AZUREAD_2022-08-20T09_23_15.870Z.json’,
‘AZUREAD_2022-08-20T12_02_07.076Z.json’,
‘AZUREAD_2022-08-20T15_02_02.602Z.json’,
‘AZUREAD_2022-08-20T18_41_15.371Z.json’,
‘AZUREAD_2022-08-20T21_04_09.868Z.json’,
‘AZUREAD_2022-08-21T00_21_03.378Z.json’,
‘AZUREAD_2022-08-21T03_01_03.612Z.json’,
‘AZUREAD_2022-08-21T06_31_46.721Z.json’,
‘AZUREAD_2022-08-21T09_00_25.563Z.json’,
‘AZUREAD_2022-08-21T12_00_28.297Z.json’,
‘AZUREAD_2022-08-21T15_06_02.955Z.json’,
‘AZUREAD_2022-08-21T18_02_36.810Z.json’,
‘AZUREAD_2022-08-21T21_11_47.527Z.json’,
‘AZUREAD_2022-08-22T00_38_39.339Z.json’,
‘AZUREAD_2022-08-22T03_12_35.427Z.json’,
‘AZUREAD_2022-08-22T06_10_22.996Z.json’,
‘AZUREAD_2022-08-22T09_01_26.005Z.json’,
‘AZUREAD_2022-08-22T12_30_36.375Z.json’,
‘AZUREAD_2022-08-22T15_11_57.786Z.json’,
‘AZUREAD_2022-08-22T18_06_12.318Z.json’,
‘AZUREAD_2022-08-22T21_06_16.397Z.json’,
‘AZUREAD_2022-08-23T00_01_32.097Z.json’,
‘AZUREAD_2022-08-23T03_13_34.617Z.json’,
‘AZUREAD_2022-08-23T06_12_04.524Z.json’,
‘AZUREAD_2022-08-23T09_06_36.465Z.json’,
‘AZUREAD_2022-08-23T12_23_47.260Z.json’,
‘AZUREAD_2022-08-23T15_07_25.933Z.json’,
‘AZUREAD_2022-08-23T18_06_17.979Z.json’,
‘AZUREAD_2022-08-23T21_10_23.207Z.json’,
‘AZUREAD_2022-08-24T00_17_52.726Z.json’,
‘AZUREAD_2022-08-24T03_08_04.379Z.json’,
‘AZUREAD_2022-08-24T06_12_36.113Z.json’,
‘AZUREAD_2022-08-24T09_02_01.941Z.json’,
‘AZUREAD_2022-08-24T12_05_38.515Z.json’,
‘AZUREAD_2022-08-24T15_00_59.959Z.json’,
‘AZUREAD_2022-08-24T18_16_26.757Z.json’,
‘AZUREAD_2022-08-24T21_01_57.951Z.json’,
‘AZUREAD_2022-08-25T00_19_39.558Z.json’,
‘AZUREAD_2022-08-25T03_00_11.988Z.json’,
‘AZUREAD_2022-08-25T06_09_30.965Z.json’,
‘AZUREAD_2022-08-25T09_06_29.348Z.json’,
‘AZUREAD_2022-08-25T12_21_23.282Z.json’,
‘AZUREAD_2022-08-25T15_18_51.497Z.json’,
‘AZUREAD_2022-08-25T18_08_51.671Z.json’,
‘AZUREAD_2022-08-25T21_16_57.141Z.json’,
‘AZUREAD_2022-08-26T00_04_23.818Z.json’,
‘AZUREAD_2022-08-26T03_35_07.038Z.json’,
‘AZUREAD_2022-08-26T06_09_07.095Z.json’,
‘AZUREAD_2022-08-26T09_13_45.158Z.json’,
‘AZUREAD_2022-08-26T12_04_24.131Z.json’,
‘AZUREAD_2022-08-26T15_04_03.469Z.json’,
‘AZUREAD_2022-08-26T18_11_20.205Z.json’,
‘AZUREAD_2022-08-26T21_10_28.387Z.json’,
‘AZUREAD_2022-08-27T00_06_18.712Z.json’,
‘AZUREAD_2022-08-27T03_04_39.876Z.json’,
‘AZUREAD_2022-08-27T06_25_32.944Z.json’,
‘AZUREAD_2022-08-27T09_11_24.479Z.json’,
‘AZUREAD_2022-08-27T12_15_05.051Z.json’,
‘AZUREAD_2022-08-27T15_13_13.225Z.json’,
‘AZUREAD_2022-08-27T18_03_47.921Z.json’,
‘AZUREAD_2022-08-27T21_09_47.684Z.json’,
‘AZUREAD_2022-08-28T00_06_04.882Z.json’,
‘AZUREAD_2022-08-28T03_11_50.094Z.json’,
‘AZUREAD_2022-08-28T06_22_12.988Z.json’,
‘AZUREAD_2022-08-28T09_06_53.959Z.json’,
‘AZUREAD_2022-08-28T12_34_13.560Z.json’,
‘AZUREAD_2022-08-28T15_01_28.549Z.json’,
‘AZUREAD_2022-08-28T18_00_44.018Z.json’,
‘AZUREAD_2022-08-28T21_13_56.458Z.json’,
‘AZUREAD_2022-08-29T00_13_43.369Z.json’,
‘AZUREAD_2022-08-29T03_45_56.645Z.json’,
‘AZUREAD_2022-08-29T06_27_25.048Z.json’,
‘AZUREAD_2022-08-29T09_21_38.347Z.json’,
‘AZUREAD_2022-08-29T12_05_54.323Z.json’,
‘AZUREAD_2022-08-29T15_02_31.484Z.json’,
‘AZUREAD_2022-08-29T18_06_13.069Z.json’,
‘AZUREAD_2022-08-29T21_21_41.645Z.json’
]AZURE_INFERENCE_FILES = [
‘AZUREAD_2022-08-30T00_17_05.561Z.json’,
‘AZUREAD_2022-08-30T03_14_27.626Z.json’,
‘AZUREAD_2022-08-30T06_15_21.422Z.json’,
‘AZUREAD_2022-08-30T09_21_58.312Z.json’,
‘AZUREAD_2022-08-30T12_05_53.775Z.json’,
‘AZUREAD_2022-08-30T15_05_34.679Z.json’,
‘AZUREAD_2022-08-30T18_39_54.214Z.json’,
‘AZUREAD_2022-08-30T21_01_48.448Z.json’,
‘AZUREAD_2022-08-31T00_21_46.153Z.json’,
‘AZUREAD_2022-08-31T03_08_27.951Z.json’,
‘AZUREAD_2022-08-31T06_20_09.178Z.json’,
‘AZUREAD_2022-08-31T09_01_27.089Z.json’,
‘AZUREAD_2022-08-31T12_02_02.230Z.json’,
‘AZUREAD_2022-08-31T15_03_06.756Z.json’,
‘AZUREAD_2022-08-31T18_03_06.102Z.json’,
‘AZUREAD_2022-08-31T21_13_44.759Z.json’
]DUO_TRAINING_FILES = [
‘DUO_2022-08-01T00_05_06.806Z.json’,
‘DUO_2022-08-01T03_02_04.418Z.json’,
‘DUO_2022-08-01T06_05_05.064Z.json’,
‘DUO_2022-08-01T09_55_08.757Z.json’,
‘DUO_2022-08-01T12_09_47.901Z.json’,
‘DUO_2022-08-01T15_08_57.986Z.json’,
‘DUO_2022-08-01T18_05_32.818Z.json’,
‘DUO_2022-08-01T21_17_59.018Z.json’,
‘DUO_2022-08-02T00_37_03.298Z.json’,
‘DUO_2022-08-02T03_36_35.233Z.json’,
‘DUO_2022-08-02T06_26_03.986Z.json’,
‘DUO_2022-08-02T09_01_18.144Z.json’,
‘DUO_2022-08-02T12_08_27.244Z.json’,
‘DUO_2022-08-02T15_07_44.984Z.json’,
‘DUO_2022-08-02T18_56_34.378Z.json’,
‘DUO_2022-08-02T21_10_29.396Z.json’,
‘DUO_2022-08-03T00_01_48.778Z.json’,
‘DUO_2022-08-03T03_05_22.026Z.json’,
‘DUO_2022-08-03T06_00_14.663Z.json’,
‘DUO_2022-08-03T09_14_08.835Z.json’,
‘DUO_2022-08-03T12_17_51.740Z.json’,
‘DUO_2022-08-03T15_17_09.808Z.json’,
‘DUO_2022-08-03T18_17_59.005Z.json’,
‘DUO_2022-08-03T21_02_12.484Z.json’,
‘DUO_2022-08-04T00_16_45.964Z.json’,
‘DUO_2022-08-04T03_20_44.449Z.json’,
‘DUO_2022-08-04T06_05_25.390Z.json’,
‘DUO_2022-08-04T09_06_36.229Z.json’,
‘DUO_2022-08-04T12_12_42.099Z.json’,
‘DUO_2022-08-04T15_09_12.877Z.json’,
‘DUO_2022-08-04T18_13_07.708Z.json’,
‘DUO_2022-08-04T21_08_34.357Z.json’,
‘DUO_2022-08-05T00_39_29.224Z.json’,
‘DUO_2022-08-05T03_58_45.946Z.json’,
‘DUO_2022-08-05T06_22_42.332Z.json’,
‘DUO_2022-08-05T09_31_47.259Z.json’,
‘DUO_2022-08-05T12_05_50.568Z.json’,
‘DUO_2022-08-05T15_00_17.239Z.json’,
‘DUO_2022-08-05T18_05_56.244Z.json’,
‘DUO_2022-08-05T21_03_44.044Z.json’,
‘DUO_2022-08-06T00_04_07.964Z.json’,
‘DUO_2022-08-06T03_00_14.884Z.json’,
‘DUO_2022-08-06T06_14_11.811Z.json’,
‘DUO_2022-08-06T09_17_14.197Z.json’,
‘DUO_2022-08-06T13_00_53.987Z.json’,
‘DUO_2022-08-06T15_04_28.652Z.json’,
‘DUO_2022-08-06T18_11_42.754Z.json’,
‘DUO_2022-08-06T21_01_46.563Z.json’,
‘DUO_2022-08-07T01_30_43.028Z.json’,
‘DUO_2022-08-07T03_59_14.016Z.json’,
‘DUO_2022-08-07T06_38_45.747Z.json’,
‘DUO_2022-08-07T09_12_23.830Z.json’,
‘DUO_2022-08-07T12_09_20.360Z.json’,
‘DUO_2022-08-07T15_01_43.630Z.json’,
‘DUO_2022-08-07T18_25_51.363Z.json’,
‘DUO_2022-08-07T21_06_39.592Z.json’,
‘DUO_2022-08-08T00_03_47.268Z.json’,
‘DUO_2022-08-08T03_08_12.355Z.json’,
‘DUO_2022-08-08T06_08_16.424Z.json’,
‘DUO_2022-08-08T09_35_08.045Z.json’,
‘DUO_2022-08-08T12_37_44.191Z.json’,
‘DUO_2022-08-08T15_34_42.886Z.json’,
‘DUO_2022-08-08T18_02_49.470Z.json’,
‘DUO_2022-08-08T21_01_26.207Z.json’,
‘DUO_2022-08-09T00_02_38.932Z.json’,
‘DUO_2022-08-09T03_06_46.584Z.json’,
‘DUO_2022-08-09T06_15_35.216Z.json’,
‘DUO_2022-08-09T09_42_11.514Z.json’,
‘DUO_2022-08-09T12_00_29.207Z.json’,
‘DUO_2022-08-09T15_25_00.138Z.json’,
‘DUO_2022-08-09T18_00_22.295Z.json’,
‘DUO_2022-08-09T21_06_00.541Z.json’,
‘DUO_2022-08-10T00_06_51.783Z.json’,
‘DUO_2022-08-10T03_12_39.799Z.json’,
‘DUO_2022-08-10T06_02_17.599Z.json’,
‘DUO_2022-08-10T09_02_53.713Z.json’,
‘DUO_2022-08-10T12_22_27.222Z.json’,
‘DUO_2022-08-10T16_18_07.097Z.json’,
‘DUO_2022-08-10T18_03_12.024Z.json’,
‘DUO_2022-08-10T21_05_02.676Z.json’,
‘DUO_2022-08-11T00_14_46.056Z.json’,
‘DUO_2022-08-11T03_09_03.122Z.json’,
‘DUO_2022-08-11T06_13_34.994Z.json’,
‘DUO_2022-08-11T09_07_33.742Z.json’,
‘DUO_2022-08-11T12_15_05.859Z.json’,
‘DUO_2022-08-11T15_08_54.468Z.json’,
‘DUO_2022-08-11T18_03_29.894Z.json’,
‘DUO_2022-08-11T21_07_36.914Z.json’,
‘DUO_2022-08-12T00_00_36.965Z.json’,
‘DUO_2022-08-12T03_09_13.102Z.json’,
‘DUO_2022-08-12T06_23_37.076Z.json’,
‘DUO_2022-08-12T09_05_25.800Z.json’,
‘DUO_2022-08-12T12_00_02.586Z.json’,
‘DUO_2022-08-12T15_07_04.329Z.json’,
‘DUO_2022-08-12T18_06_29.281Z.json’,
‘DUO_2022-08-12T21_06_02.030Z.json’,
‘DUO_2022-08-13T00_01_19.915Z.json’,
‘DUO_2022-08-13T03_19_52.793Z.json’,
‘DUO_2022-08-13T06_07_44.115Z.json’,
‘DUO_2022-08-13T09_30_28.265Z.json’,
‘DUO_2022-08-13T12_05_54.411Z.json’,
‘DUO_2022-08-13T15_32_27.481Z.json’,
‘DUO_2022-08-13T18_30_04.101Z.json’,
‘DUO_2022-08-13T21_08_32.899Z.json’,
‘DUO_2022-08-14T00_28_59.030Z.json’,
‘DUO_2022-08-14T03_10_53.115Z.json’,
‘DUO_2022-08-14T06_15_59.959Z.json’,
‘DUO_2022-08-14T09_05_12.470Z.json’,
‘DUO_2022-08-14T12_07_08.971Z.json’,
‘DUO_2022-08-14T15_18_53.132Z.json’,
‘DUO_2022-08-14T18_34_28.408Z.json’,
‘DUO_2022-08-14T22_07_37.863Z.json’,
‘DUO_2022-08-15T00_15_04.480Z.json’,
‘DUO_2022-08-15T03_01_24.327Z.json’,
‘DUO_2022-08-15T06_04_36.244Z.json’,
‘DUO_2022-08-15T09_14_15.659Z.json’,
‘DUO_2022-08-15T12_42_54.708Z.json’,
‘DUO_2022-08-15T15_26_14.651Z.json’,
‘DUO_2022-08-15T18_29_34.302Z.json’,
‘DUO_2022-08-15T21_03_55.385Z.json’,
‘DUO_2022-08-16T00_18_15.026Z.json’,
‘DUO_2022-08-16T03_00_38.697Z.json’,
‘DUO_2022-08-16T06_10_49.268Z.json’,
‘DUO_2022-08-16T09_11_31.564Z.json’,
‘DUO_2022-08-16T12_08_44.078Z.json’,
‘DUO_2022-08-16T15_00_07.447Z.json’,
‘DUO_2022-08-16T18_02_26.218Z.json’,
‘DUO_2022-08-16T21_13_00.224Z.json’,
‘DUO_2022-08-17T00_04_52.924Z.json’,
‘DUO_2022-08-17T03_37_32.167Z.json’,
‘DUO_2022-08-17T06_08_04.647Z.json’,
‘DUO_2022-08-17T09_02_22.845Z.json’,
‘DUO_2022-08-17T12_02_54.475Z.json’,
‘DUO_2022-08-17T15_15_30.542Z.json’,
‘DUO_2022-08-17T18_04_42.665Z.json’,
‘DUO_2022-08-17T21_07_41.110Z.json’,
‘DUO_2022-08-18T00_00_50.864Z.json’,
‘DUO_2022-08-18T03_03_50.747Z.json’,
‘DUO_2022-08-18T06_00_37.820Z.json’,
‘DUO_2022-08-18T09_04_22.633Z.json’,
‘DUO_2022-08-18T12_09_18.662Z.json’,
‘DUO_2022-08-18T15_14_45.798Z.json’,
‘DUO_2022-08-18T18_17_27.739Z.json’,
‘DUO_2022-08-18T21_46_46.184Z.json’,
‘DUO_2022-08-19T00_01_58.530Z.json’,
‘DUO_2022-08-19T03_02_51.459Z.json’,
‘DUO_2022-08-19T07_06_56.960Z.json’,
‘DUO_2022-08-19T09_00_18.242Z.json’,
‘DUO_2022-08-19T12_20_43.912Z.json’,
‘DUO_2022-08-19T15_08_51.811Z.json’,
‘DUO_2022-08-19T18_09_33.257Z.json’,
‘DUO_2022-08-19T21_04_15.361Z.json’,
‘DUO_2022-08-20T00_03_51.763Z.json’,
‘DUO_2022-08-20T03_19_11.469Z.json’,
‘DUO_2022-08-20T06_21_39.116Z.json’,
‘DUO_2022-08-20T09_59_11.004Z.json’,
‘DUO_2022-08-20T12_01_02.871Z.json’,
‘DUO_2022-08-20T15_08_10.265Z.json’,
‘DUO_2022-08-20T18_09_04.018Z.json’,
‘DUO_2022-08-20T21_05_22.435Z.json’,
‘DUO_2022-08-21T00_12_40.168Z.json’,
‘DUO_2022-08-21T03_40_42.920Z.json’,
‘DUO_2022-08-21T06_19_14.897Z.json’,
‘DUO_2022-08-21T09_04_54.519Z.json’,
‘DUO_2022-08-21T12_02_14.649Z.json’,
‘DUO_2022-08-21T15_03_59.410Z.json’,
‘DUO_2022-08-21T18_40_23.810Z.json’,
‘DUO_2022-08-21T21_19_39.429Z.json’,
‘DUO_2022-08-22T00_00_32.032Z.json’,
‘DUO_2022-08-22T03_15_10.168Z.json’,
‘DUO_2022-08-22T06_14_12.112Z.json’,
‘DUO_2022-08-22T09_40_53.927Z.json’,
‘DUO_2022-08-22T12_09_11.101Z.json’,
‘DUO_2022-08-22T15_04_12.123Z.json’,
‘DUO_2022-08-22T18_04_02.147Z.json’,
‘DUO_2022-08-22T21_07_41.681Z.json’,
‘DUO_2022-08-23T00_45_51.610Z.json’,
‘DUO_2022-08-23T03_15_17.210Z.json’,
‘DUO_2022-08-23T06_09_53.853Z.json’,
‘DUO_2022-08-23T09_03_35.671Z.json’,
‘DUO_2022-08-23T12_20_07.116Z.json’,
‘DUO_2022-08-23T15_19_13.637Z.json’,
‘DUO_2022-08-23T18_23_01.179Z.json’,
‘DUO_2022-08-23T21_30_50.797Z.json’,
‘DUO_2022-08-24T00_35_12.724Z.json’,
‘DUO_2022-08-24T03_07_48.905Z.json’,
‘DUO_2022-08-24T06_52_26.658Z.json’,
‘DUO_2022-08-24T09_02_32.667Z.json’,
‘DUO_2022-08-24T12_36_29.646Z.json’,
‘DUO_2022-08-24T15_14_52.489Z.json’,
‘DUO_2022-08-24T18_10_31.894Z.json’,
‘DUO_2022-08-24T21_23_58.385Z.json’,
‘DUO_2022-08-25T00_38_52.875Z.json’,
‘DUO_2022-08-25T03_09_40.392Z.json’,
‘DUO_2022-08-25T06_16_01.732Z.json’,
‘DUO_2022-08-25T09_15_31.540Z.json’,
‘DUO_2022-08-25T12_32_54.214Z.json’,
‘DUO_2022-08-25T15_04_51.656Z.json’,
‘DUO_2022-08-25T18_09_25.686Z.json’,
‘DUO_2022-08-25T21_30_36.994Z.json’,
‘DUO_2022-08-26T00_03_13.149Z.json’,
‘DUO_2022-08-26T03_02_51.844Z.json’,
‘DUO_2022-08-26T06_15_36.205Z.json’,
‘DUO_2022-08-26T09_12_38.216Z.json’,
‘DUO_2022-08-26T12_18_33.577Z.json’,
‘DUO_2022-08-26T15_12_34.213Z.json’,
‘DUO_2022-08-26T18_26_38.686Z.json’,
‘DUO_2022-08-26T21_00_41.408Z.json’,
‘DUO_2022-08-27T00_17_07.536Z.json’,
‘DUO_2022-08-27T03_08_55.907Z.json’,
‘DUO_2022-08-27T06_01_59.976Z.json’,
‘DUO_2022-08-27T10_04_58.247Z.json’,
‘DUO_2022-08-27T12_00_48.542Z.json’,
‘DUO_2022-08-27T15_03_54.319Z.json’,
‘DUO_2022-08-27T18_04_17.126Z.json’,
‘DUO_2022-08-27T21_07_43.779Z.json’,
‘DUO_2022-08-28T00_26_28.894Z.json’,
‘DUO_2022-08-28T04_04_08.201Z.json’,
‘DUO_2022-08-28T06_15_53.091Z.json’,
‘DUO_2022-08-28T09_33_01.613Z.json’,
‘DUO_2022-08-28T12_38_30.312Z.json’,
‘DUO_2022-08-28T15_38_27.355Z.json’,
‘DUO_2022-08-28T18_03_05.686Z.json’,
‘DUO_2022-08-28T21_10_45.915Z.json’,
‘DUO_2022-08-29T00_07_32.403Z.json’,
‘DUO_2022-08-29T03_51_25.190Z.json’,
‘DUO_2022-08-29T06_06_27.003Z.json’,
‘DUO_2022-08-29T09_26_52.838Z.json’,
‘DUO_2022-08-29T12_09_06.633Z.json’,
‘DUO_2022-08-29T15_00_36.208Z.json’,
‘DUO_2022-08-29T18_02_32.315Z.json’,
‘DUO_2022-08-29T21_10_08.476Z.json’
]DUO_INFERENCE_FILES = [
‘DUO_2022-08-30T00_26_18.130Z.json’,
‘DUO_2022-08-30T03_33_23.301Z.json’,
‘DUO_2022-08-30T06_38_13.972Z.json’,
‘DUO_2022-08-30T09_14_52.103Z.json’,
‘DUO_2022-08-30T12_12_56.353Z.json’,
‘DUO_2022-08-30T15_42_24.545Z.json’,
‘DUO_2022-08-30T18_35_34.524Z.json’,
‘DUO_2022-08-30T21_02_48.060Z.json’,
‘DUO_2022-08-31T00_01_31.947Z.json’,
‘DUO_2022-08-31T03_06_14.133Z.json’,
‘DUO_2022-08-31T06_04_50.555Z.json’,
‘DUO_2022-08-31T09_04_50.225Z.json’,
‘DUO_2022-08-31T12_00_48.690Z.json’,
‘DUO_2022-08-31T15_00_12.020Z.json’,
‘DUO_2022-08-31T18_00_01.228Z.json’,
‘DUO_2022-08-31T21_01_09.338Z.json’
]DFP_DATASET_FILES = {
‘azure’: (AZURE_TRAINING_FILES, AZURE_INFERENCE_FILES), ‘duo’: (DUO_TRAINING_FILES, DUO_INFERENCE_FILES)
}S3_BASE_PATH = “/rapidsai-data/cyber/morpheus/dfp/”def fetch_dataset(dataset):def parse_args():
argparser = argparse.ArgumentParser(“Fetches training and inference data for DFP examples”)
argparser.add_argument(“data_set”, nargs=‘*’, choices=[‘azure’, ‘duo’, ‘all’], help=“Data set to fetch”)def main():
args = parse_args()
if args.data_set == [‘all’]:
ds_list = DFP_DATASET_FILES.keys()
else:
ds_list = args.data_setif name == “main”:
main()python fetch_example_data.py azurePlease include:Hi,This forum talks more about updates and issues related to TensorRT, we recommend you reach out to the Morpheus related platform to get better help.Morpheus SDK. Contribute to nv-morpheus/Morpheus development by creating an account on GitHub.Thank you.Powered by Discourse, best viewed with JavaScript enabled"
164,tensorrt-installation-fails-missing-libnvinfer-samples,"I am trying to install TensorRT (cuda11.4-trt8.2.0.6-ea-20210922) but I keep getting the error :The following packages have unmet dependencies:
tensorrt : Depends: libnvinfer-samples (= 8.2.0-1+cuda11.4) but it is not going to be installed
E: Unable to correct problems, you have held broken packages.Could the cause be that I am Using CUDA version 11.5 and the TenosrRT is for 11.4?Greetings and thanks in advance.TensorRT Version:  nv-tensorrt-repo-ubuntu2004-cuda11.4-trt8.2.0.6-ea-20210922_1-1_amd64
GPU Type:  NVIDIA Corporation TU117GLM [Quadro T1000 Mobile] / Quadro T1000/PCIe/SSE2
Nvidia Driver Version: 495.29.05
CUDA Version:  11.5
CUDNN Version: –
Operating System + Version: Ubuntu 20.04
Python Version (if applicable):  3.8.10os=“ubuntu2004”
tag=“cuda11.4-trt8.2.0.6-ea-20210922”
sudo dpkg -i nv-tensorrt-repo-${os}-${tag}_1-1_amd64.deb
sudo apt-key add /var/nv-tensorrt-repo-${os}-${tag}/7fa2af80.pub
sudo apt-get update
sudo apt-get install tensorrtReading package lists… Done
Building dependency tree
Reading state information… Done
Some packages could not be installed. This may mean that you have
requested an impossible situation or if you are using the unstable
distribution that some required packages have not yet been created
or been moved out of Incoming.
The following information may help to resolve the situation:The following packages have unmet dependencies:
tensorrt : Depends: libnvinfer-samples (= 8.2.0-1+cuda11.4) but it is not going to be installed
E: Unable to correct problems, you have held broken packages.Hi,
Please refer to the installation steps from the below link if in case you are missing on anythingThis NVIDIA TensorRT 8.4.1 Installation Guide provides the installation requirements, a list of what is included in the TensorRT package, and step-by-step instructions for installing TensorRT.
Also, we suggest you to use TRT NGC containers to avoid any system dependency related issues.NVIDIA TensorRT is a C++ library that facilitates high-performance inference on NVIDIA graphics processing units (GPUs). TensorRT takes a trained network and produces a highly optimized runtime engine that performs inference for that network.Thanks!I had to install cuda 11.4 then it worked.For this I followed CUDA Toolkit 11.4 Downloads but specified cuda 11.4 in the last command.sudo apt-get -y install cuda-11.4I have cuda-11.5 and met the same problemI have this problem for tensorrt 8.2.5-1 and cuda11.6 tensorrt : Depends: libnvinfer-samples (= 8.2.5-1+cuda11.4) but it is not going to be installedIt was solved by installing cuda 11.7 and tensorrt 8.4.1.5.Powered by Discourse, best viewed with JavaScript enabled"
165,failed-to-download-riva-asr-conformer-xl-offline-model,"I’m trying to install a latest version of Riva quickstart, but for some reason I can’t download the Riva ASR Conformer-XL Offline model:I also tried to download it with ngccli, but the result is the same:It happens only with the “nvidia/riva/rmir_asr_conformer_xl_en_us_ofl:2.12.0”, because the “nvidia/riva/rmir_asr_conformer_xl_en_us_str:2.12.0” was downloaded without any issues.How can I fix this issue?Thanks in advance!Hi @vbilousThanks for reporting this issue,
I tried to reproduce from my end, it happens in 2.12.0 too,
I will report to the Engineering and provide updates from themThanksPowered by Discourse, best viewed with JavaScript enabled"
166,how-to-get-file-profile-metadata-json-to-use-trt-engine-explorer,"I generated .engine model from .onnx by this tool TensorRT-For-YOLO-Series/export.py at main · Linaom1214/TensorRT-For-YOLO-Series · GitHub. This repo uses tensorrt, not trtexec to generate .engine model. From the generated .engine, I can generate 2 json files: profile.json and graph.json by command:This is an output of the above command:How I can get file profile.metadata.json from generated .engine model?I saw that the repo TensorRT/tutorial.ipynb at main · NVIDIA/TensorRT · GitHub needs 3 json file. If we generate .engine model not by using trtexec, how we can get full json files for Tensorrt engine explorer?
ThanksHi,
Please check the below link, as they might answer your concernsThis is the API Reference documentation for the NVIDIA TensorRT library. The following set of APIs allows developers to import pre-trained models, calibrate networks for INT8, and build and deploy optimized networks with TensorRT. Networks can be...
Thanks!Hi,
image1116×161 9.92 KB
process_engine.py will do this automatically,Thank you.Powered by Discourse, best viewed with JavaScript enabled"
167,build-engine-with-1gb-gpu-cudnn-status-alloc-failed,"Hello, I am trying to build a yolov7 engine with 1GB of GPU memory available. I know for a fact that while building the engine the maximum amount of GPU memory it consumes is 600MB, and once the engine is created the algorithm consumes 890MB of GPU memory.Building the engine on the same GPU although with 2GB of memory instead of only 1GB, works. (unfortunately, the engine created in the machine that uses the 2GB GPU does not deserialize on the machine that uses the 1 GB GPU, it throws a cudnn initialization error.)THIS IS THE ERROR I GET WHEN I TRY BUILDING THE ENGINE IN A A16 1GB GPU:root@ai-1gb:/home/ubuntu/TRT/yolov7/build# sudo ./yolov7 -s yolov7.wts yolov7.engine v7
Loading weights: yolov7.wts
Building engine, please wait for a while…
[07/26/2023-20:12:52] [W] [TRT] TensorRT was linked against cuBLAS/cuBLAS LT 11.5.1 but loaded cuBLAS/cuBLAS LT 11.4.2
[07/26/2023-20:12:52] [W] [TRT] Detected invalid timing cache, setup a local cache instead
[07/26/2023-20:12:52] [E] [TRT] 1: [convolutionRunner.cpp::executeConv::458] Error Code 1: Cudnn (CUDNN_STATUS_ALLOC_FAILED)
[07/26/2023-20:12:52] [E] [TRT] 2: [builder.cpp::buildSerializedNetwork::417] Error Code 2: Internal Error (Assertion enginePtr != nullptr failed.)
Build engine successfully!
yolov7: /home/ubuntu/tensorrtx/yolov7/main.cpp:38: void serialize_engine(unsigned int, std::string&, std::string&, std::string&): Assertion `serialized_engine != nullptr’ failed.
AbortedTensorRT Version:  8.0.1.6
GPU Type:  A16
Nvidia Driver Version:  525.85.05
CUDA Version:  11.3
CUDNN Version:  8.8.0
Operating System + Version:  UBuntu 22.04
Python Version (if applicable):  3.8.0
TensorFlow Version (if applicable):
PyTorch Version (if applicable):I am including the CmakeList and the main.cpp file.CMakeLists.txt (1.4 KB)main.cpp (7.6 KB)GIt clone and follow the steps of this Tutorial to reproduce the error,//github.com/wang-xinyu/tensorrtx/tree/master/yolov7The error happens when the command sudo ./yolov7 -s is executed.Why can’t I initialize CUDNN in a 1GB GPU machine?(unfortunately, the engine created in the machine that uses the 2GB GPU does not deserialize on the machine that uses the 1 GB GPU; it throws a cudnn initialization error.)Hi,The generated plan files are not portable across platforms or TensorRT versions. Plans are specific to the exact GPU model they were built on (in addition to the platforms and the TensorRT version) and must be re-targeted to the specific GPU in case you want to run them on a different GPU.Achieving this can be done by following certain rules in the most recent version of TensorRT, which is 8.6.1. For further information, please refer to the accompanying document.This is the revision history of the NVIDIA TensorRT 8.6 Developer Guide.Thank you.Powered by Discourse, best viewed with JavaScript enabled"
168,jetson-xavier-with-installed-tensorrt-code-in-anaconda-environment-output-no-module-named-tensorrt,"Hello, I have just ran the command:sudo apt install nvidia-tensorrtto install TensorRT on my Jetson Xavier.
When run the code with the system default python3.6.9,import tensorrtis fine, and when running the code in conda environment, it throws the error:ModuleNotFoundError: No module named ‘tensorrt’Is there any thing I need to do to link the system TensorRT to conda environment?Hi,This looks like a Jetson issue. Please refer to the below samples in case useful.Hello AI World guide to deploying deep-learning inference networks and deep vision primitives with TensorRT and NVIDIA Jetson. - GitHub - dusty-nv/jetson-inference: Hello AI World guide to deployin...An easy to use PyTorch to TensorRT converter. Contribute to NVIDIA-AI-IOT/torch2trt development by creating an account on GitHub.For any further assistance, we will move this post to to Jetson related forum.Thanks!Thanks, I will take a look!Powered by Discourse, best viewed with JavaScript enabled"
169,riva-streaming-recognize-returning-failure,"Hardware - GPU T4 (AWS g4dn)
Hardware - CPU
Operating System ubuntu 20.04
Riva Version 2.7.1 - Conformer-en-US-asr-streaming - Latency configs - Normal quickstartEvery few days my Riva instance just stops sending back results, I can’t understand the cause, It may be CPU related as my CPU usage gets stuck at 100% but my GPU is at 50% usage. The problem also is that using the health checker returns “serving” even though the docker isn’t transcribing anything which is a problem as AWS can’t remove the instance as it’s still seen as healthy.I usually only have to stop the docker and start again and then boom everything works for a few more days…If any advice could be given on how to avoid this error and also would it be possible to “fix” the grpc health proto to pick up the errorHere are the docker logs when it stops working:Hi @HansieBThanks for your interest in RivaI will check with the internal team and let you knowThanksHi @HansieBThe internal team told me that this issue won’t happen with our latest release,Please try our latest Riva and let us know the feedbackThanksPowered by Discourse, best viewed with JavaScript enabled"
170,tensorrt-slowdown-for-buildserializednetwork,"We’ve been using TensorRT for a couple of years now, and recently updated TensorRT from 8.0.3 to 8.5.1. The update went great and our functional tests have identical results, but we have noticed slower processing for some functions. One in particular is 2x to 4x slower in TensorRT 8.5: buildSerializedNetwork()This is quite annoying for our functional tests, since we are running many different models, some of which are quite large: the greatest slowdown is 120s → 450sThis behaviour was seen across platforms (Desktop Linux, Jetson Linux, Windows), across multiple arch (1080, 2080, 3080, Xavier).NVIDIA: can you explain this slowdown?TensorRT Version: 8.5.1.7
GPU Type: 1080, 2070, 2080, 3080, Xavier AGX (TensorRT 8.4)
Nvidia Driver Version: nvidia-driver-525
CUDA Version: 11.8.0
CUDNN Version: 8.6.0.163
Operating System + Version: Ubuntu 20.04
Baremetal or Container (if container which image + tag): Ubuntu 20.04 baremetalCode Sample:
trt_sample.cpp (2.1 KB)
CMakeLists.txt (707 Bytes)
FindTensorRT.cmake (3.2 KB)This slowdown was consistent across all of our own models.
I was able to reproduce it with public models from the onnx/models github repo, such as:onnx/models/vision/classification/caffenet/model/caffenet-12.onnx
onnx/models/vision/classification/vgg/model/vgg19-bn-7.onnx
onnx/models/vision/classification/zfnet-512/model/zfnet512-12.onnx
onnx/models/vision/object_detection_segmentation/duc/model/ResNet101-DUC-12.onnx
v v v v vA collection of pre-trained, state-of-the-art models in the ONNX format  - GitHub - onnx/models: A collection of pre-trained, state-of-the-art models in the ONNX formatOn my current setup (Intel + RTX 2070), I am running TensorRT 8.5.1 baremetal, and a docker to run the old TensorRT 8.0.3 (nvidia/cuda:11.3.1-cudnn8-devel-ubuntu20.04)All results are reproducible on baremetal or nvidia containers.Note that I did not cherrypick these models, these are the first four that I’ve tried. Here are the results for 2 of them.Initially, I added timing printfs in our codebase to find which function was slower: here only the function time is calculated (withchrono::steady_clock::now() ).Here’s the cleaned output of functional tests:
ctest | grep buildSerializedNetworkNotice the 2x to 4x slowdown through all the calls (use diff/meld to compare)
trt8517.txt (3.6 KB)
trt8034.txt (3.6 KB)Hi,Request you to share the model, script, profiler, and performance output if not shared already so that we can help you better.Alternatively, you can try running your model with trtexec command.master/samples/trtexecNVIDIA® TensorRT™, an SDK for high-performance deep learning inference, includes a deep learning inference optimizer and runtime that delivers low latency and high throughput for inference applicat...While measuring the model performance, make sure you consider the latency and throughput of the network inference, excluding the data pre and post-processing overhead.
Please refer to the below links for more details:Page displayed for direct links that are not 
supported by current publishing formats.Page displayed for direct links that are not 
supported by current publishing formats.Thanks!Hi,I’ve already made sure everything is included in my original post.Thanks,Hi NVES,The original post contains a very concise code sample that shows how to reproduce the problem, have you looked at it?Hi @fl932471 ,
In the updated release a lot more kernels/backends from 8.0 to 8.5 has been introduced which increased the auto-tuning cost. Can you please try with PreviewFeature kDISABLE_EXTERNAL_TACTIC_SOURCES_FOR_CORE_0805 enabled?ThanksAlso if you can try TRT 8.6 as build performance has been improvised there.ThanksCan you please try with PreviewFeature kDISABLE_EXTERNAL_TACTIC_SOURCES_FOR_CORE_0805 enabled?Thanks @AakankshaS for the follow-up!Unfortunately, setting this saves about <1% to 10 %, far from the ~2x we experience: for example, vgg19-bn-7.onnx goes from 34 to 32 sec, but TRT 8.0.3 has 19 sec). Furthermore, we are currently using cuDNN, so using this option is not possible.Indeed, we also noticed the new parameters for TRT 8.6 to specify the Builder optimization level, but this requires a thorough investigation as the network performance can be modified. It’ll be on our todo list.For now we are relying on a caching mechanism so we can prevent rebuilding the network.Powered by Discourse, best viewed with JavaScript enabled"
171,probable-bug-for-onnx-range-op,"When running a onnx model, it gives an error:But in the document it seems Range Op should be supported : onnx-tensorrt/operators.md at 3b008c466bcb7375aaf5cabf51b289fd34d40c44 · onnx/onnx-tensorrt · GitHub
And I think the error message means the Range Op’s shape has been set twice, which may be a bug.TensorRT Version: 8.5
GPU Type: Tesla T4
Nvidia Driver Version: 460I cut a subgraph of the origin onnx for your convenience to reproduce
subgraph.onnx (898 Bytes)trtexec --onnx=subgraph.onnxI just tried TensorRT8.6 EA. This problem goes away, but I didn’t find such fix in the TRT8.6 release notes. Anyway, the next problem using the trt8.6 is an error below:The bytes are too much for the GPU memory, how can I fix this ? this is actually a popular language model.We could successfully build the engine.
Could you please try mentioning the workspace option and make sure enough GPU memory is available.Hi， thanks for reply, I have no doubt the subgraph can be built with TRT8.6. I mentioned two questions in this post( My fault, maybe I should have opened a new one) The first question is that under trt8.5, the range op has problem. And the second quiestion is about the myelin, it happens on the whole graph, not in this subgraph. Maybe I should open a new post for my second problem and provide the whole onnx graph. And if you would like to see to the first problem, (trt8.5) might be useful .Powered by Discourse, best viewed with JavaScript enabled"
172,pythonapi-multi-outputs-for-tensorrt,"Hi, I’m referring TensorRT/quickstart/SemanticSegmentation
/tutorial-runtime.ipynb to run TensorRT inference with python. However, when I use a TensorRT model (yolov5s) with 3 outputs, the code only returns 1 output instead of 3. How should I modify it so that I can get 3 outputs?GPU Type: 2070
Nvidia Driver Version: 525
Baremetal or Container (if container which image + tag): nvcr.io/nvidia/deepstream:6.1-develHi,Following may help you.Thank you.Hi @spolisetty, the link you sent seems to work for C++. However, I’m trying to use TensorRT python API, what should I do?Hi,Please refer to the Python API document below.
https://docs.nvidia.com/deeplearning/tensorrt/api/python_api/infer/Graph/Network.html#tensorrt.INetworkDefinition.mark_outputThank you.Powered by Discourse, best viewed with JavaScript enabled"
173,can-we-use-nvidia-maxine-in-ubuntu-22-04-with-cuda-12-1,"I have a question about using Nvidia Maxine SDK in Ubuntu 22.04 with Cuda 12.1. Is it possible to use this version of OS and Cuda for the development?Hi BehKal! Welcome to the NVIDIA Developer Forums! Yes, Maxine will run on 22.04 and CUDA 12.1.Hello Mark. Thank you for your message. I tried to test Video effects SDK in Ubuntu 22.04, with Cuda 12.1, TensorRT 8.6.1.6 and cuDNN 8.9.2. In building the samples I faced an error:
Screenshot from 2023-06-21 02-02-15986×1006 278 KB
How can I resolve the issue? I would appreciate your assistance.In addition I modified the default versions (Cuda, TensorRT and cuDNN) for building the samples.Please give me some time, I would like to test this again on one of my bench machines. I will try to get back to you soon!Sure. I will be waiting for your reply. The priority is to run NVIDIA Maxine in Ubuntu 22, the CUDA version can be 11.8. Also, I tried it with Cuda Toolkit 11.8 and TensorRT 8.5.1.7 in Ubuntu 22, but I encountered this error:
image820×1022 175 KB
Thank you.Thank you, I have not forgotten! I am working on several projects. I should have some time today.Hello again. Is there any new update about using Maxine in Ubuntu 22.04?Not at this time, I apologize for the delay. I will work on this as I can. For now I suggest dropping down to 20.04 as I haven’t had time to do a deep dive.I see. The problem in Ubuntu 20.04 is related to the GStreamer version. GStreamer version is 1.16 in Ubuntu 20.04, and for using the “nvcodec” plugin of Nvidia, I have to use a newer version like GStreamer version 1.20, which is available in Ubuntu 22.04. Upgrading GStreamer version in Ubuntu 20.04 to 1.20 has some negative effects on my project, and I encounter some problems with compatibility.Powered by Discourse, best viewed with JavaScript enabled"
174,do-we-have-to-use-visual-studio-for-cudnn,"I was trying to install cuDNN but I don’t use Visual Studio and in the tutorial there is no instruction for people use other platforms. Do we really have to use Visual Studio. I normally use Jupyter notebook. Hi @baranaldemir1032 ,
A host compiler is needed on windows, VS has cl.exe which is used for compiling cpp source code, hence is suggested to use.Thanks!Hi,
I have using Window, do I need to install the Visual Studio?
Btw, your cuDNN manual is not updated… v8? Now it is v12. And some path is wrong too. :<
ThxPowered by Discourse, best viewed with JavaScript enabled"
175,does-tensorrt-rewrite-onnx-models-to-nhwc,"We are training with our convolutional networks tensorflow 2.3 and are exporting our models to onnx using keras2onnx.
A visualization of the beginning of the onnx model can be seen below.
The input is in NHWC, but since onnx uses NCHW it adds a transpose layer before the convolutions.
I would expect that tensorrt removes this transpose layer and executes the convolutions with NHWC on GPUs.
However, when profiling with trtexec it shows a PushTranspose Layer (see below) that also consumes time.Does this mean the convolutions are indeed executed with NCHW or how can I know what is going on?
I am certain that the GPU is used since I saw activity with nvidia-smi../trtexec --onnx=<model_path.onnx> --int8 --shapes=input_1:1x704x1280x3 --exportTimes=trace.json --dumpProfile --exportProfile=prof.jsonOnnx model visualized with Netron:

model_start552×1028 30.6 KB
TensorRT Version:  7.1.3.4
GPU Type: RTX 2080Ti
Nvidia Driver Version: 460
Operating System + Version: Ubuntu 18.04
Python Version (if applicable): 3.6
TensorFlow Version (if applicable): 2.3
Baremetal: YesHi, Request you to share the ONNX model and the script so that we can assist you better.Alongside you can try validating your model with the below snippetcheck_model.pyimport sys
import onnx
filename = yourONNXmodel
model = onnx.load(filename)
onnx.checker.check_model(model).Alternatively, you can try running your model with trtexec command.
https://github.com/NVIDIA/TensorRT/tree/master/samples/opensource/trtexecThanks!Hi, thanks for your reply, is there a way to privately share the model, if yes I can provide you with an example onnx model.The model is valid. As I already described I used trtexec to get the profiling. I will share the model with randomly initialized weights, I just have to export it again, I will get back to you in a couple of hours.Thanks!Hi @jean.wanka,Please DM by attaching the model.Thank you.Thanks, I shared the model in the DM.
If it is necessary I can also create a script that creates a reduced version of this model and uses keras2onnx to export it.Is there any update on this?
The main point I’m trying to understand is what the engine Builder (IBuilder) does in detail and how it rewrites and optimizes the graph.Is it able to:thanks!Hi @jean.wanka,We have ONNX GraphSurgeon that can modify the onnx file manually.
For the FP16, conv + leakyReLu can be fused together. For Int8 in some case we did not fuse conv and activation together because of register pressure (the extra requested register file will decrease the occupancy).master/tools/onnx-graphsurgeonTensorRT is a C++ library for high performance inference on NVIDIA GPUs and deep learning accelerators.Thank you.Hi @spolisetty ,
thanks for the update!One thing that is still not clear for me is the channels first/last question.
I’ve read in your documentation that channels last (NHWC) is preferred.
ONNX, however, only uses channels first layout, does this mean the tensorrt engine is also always in channels first layout?
Is there a way to change this or are the benefits not significant?any update on this would be much appreciated.thanks!Hi @jean.wanka,TRT engine always doesn’t use channels first layout.
It depends on the kernel implementation, TRT will always insert reformat when the adjacent layers has mismatched kernel I/O.Thank you.@spolisetty
Is it better to use NHWC layout with ONNX to prevent reformat layer?Powered by Discourse, best viewed with JavaScript enabled"
176,nvidia-technical-resources,"This section highlights developer resources available for NVIDIA Developers to get started with Deep Learning using NVIDIA GPUs.Get started with Documentation:
Mixed Precision Training
Frameworks
Automatic Mixed Precision
Automatic Mixed Precision in PyTorch
Automatic Mixed Precision in TensorFlow
TensorRT Developer GuideFind more information on NVIDIA Developer Zone:
 Deep Learning Overview for DevelopersTensor Cores for Developers
Tensor Core Optimized Examples
Deep Learning Frameworks
NGC Model Scripts
NVIDIA TensorRT
Automatic Mixed PrecisionRead technical NVIDIA AI/ Deep Learning blogs and check out NVIDIA Developer News Center to learn about developers building amazing GPU applications for the AI ready world.Powered by Discourse, best viewed with JavaScript enabled"
177,riva-en-us-when-using-lm-interim-results-with-stability-change-drop-already-predicted-but-less-stable-words,"Environment:The model was deployed with the following pipeline configurationThe test file was a section of Bill Gates’ Harvard Commencement Speech.In the trace below there is a noticeable discontinuity between the end of segment that reached stability 0.9 and start of segment with stability 0.1 (marked with the pipe symbol in the following trace):The discontinuity becomes evident only when one uses the flashlight decoder, with a greedy decoder there is no such problem. In fact no stability change occurs at all, in the case of a greedy decoder the stability=0.1 for the entire duration of interim results.How should I change the pipeline configuration parameters to diminish/remove this discontinuity (i.e. not have lost word/s), and decrease the latency to the 0.1/0.9 split (in the trace above approx. 1.5s of the most recent transcript has stability 0.1, older transcripts have stability 0.9). I would like to reduce this number as much as possible.Hi @ilbThanks for your interest in RivaI will check regarding this issue with the internal teamThanks@rvinobha any news on this? For my use case it is really problematic.Hi @ilbApologies for the delay,The Internal team are still debugging the issue, Once i have an update will provide youThanks@rvinobha any updates?Hi @ilbSincere Apologies, I still don’t have updates on this thread, have asked again the concerned team to check and provide feedback at earliestThanks@rvinobha any progress? Actually any info would be really welcome, as this is quite a major drawback for my use case.HI @ilbSincere Apologies, i have pushed for updates, will push again,
Still I am waiting for an update on this internal ticket createdThanks@rvinobha I’ve tested with the latest riva version (2.9.0) and I still have the same issue.  Any news from the team? If a different model would be more suitable, please tell.With the transcribe_file.py example script, the issue can be seen as follows. Assume an intermediate transcript where some of it has stability 0.9 and some stability 0.1. When time progresses and new intermediate results are displayed, the portion with stability 0.9 will become longer, individual words from the start of portion with stability 0.1 will get removed from stability 0.1 and get appended to the portion with stability 0.9. However, oftentimes they (even if the word was correct in the first place) first change to something else or disappear completely, before they reappear in the correct form at the end of the transcript with stability 0.9.Based on experiments there seems to be a relation with the  parameters left_padding_size, right_padding_size and chunk_size, i.e. the stability 0.1 seems to contain right_padding_size audio data, followed by chunk_size data, a default value of 0.16, which could lead to “dissapearing words” (most but the shortest words take more than 0.16s)?Hi @ilbApologies for the long delayI have feedback on this issue from the internal team“Yes we expect stability to improve with cache-aware.  The decoder could be tuned to give better stability e.g. reducing beam-size, and beam-size-token.   partial transcripts are being returned here also.  We could just look at final transcripts which are stable.  Using the streaming-throughput configuration should also produce more stable results given the longer chunk size.”let me know if you have further question, I will address it with the teamPowered by Discourse, best viewed with JavaScript enabled"
178,cudnn-samples-for-tar-installation-method,"Hello,I manage a small cluster and we keep around a few CUDA toolkit versions and cuDNN versions. This requires us to use tar files (I believe) but I also need to verify cuDNN works for each CUDA toolkit. Previously I did this using the samples.In version 8.3 the samples had tar versions which made life easy. Now it seems the samples are only found in the local deb installer, and there is only a single version currently available (libcudnn8-samples=8.8.1.3-1+cuda11.8).Are there tar samples anywhere I can download? If not, how can I verify cuDNN works for each toolkit (e.g. 11.7, 11.8, 12.0)?Thank you,
-CollinMy current work around is to use the local deb installer for the latest samples, then copy from /usr/src/cudnn_v8_samples somewhere else, then remove libcudnn8, libcudnn8-dev and libcudnn8-samples, then copy the samples to a toolkit-specific directory (and hope they build/run succesfully). It’s a pain and more importantly I’m not sure if I can assume samples from 8.8.1.3-1+cuda11.8 will work with 11.7/11.8/12.0 potentially running different cuDNN versions.I would be tempted to use the old samples from 8.3 but I want a workflow that will work for cuDNN 9+ in the future.Powered by Discourse, best viewed with JavaScript enabled"
179,engine-generated-on-device-a-can-t-be-deployed-to-device-b,"A clear and concise description of the bug or issue.TensorRT Version: 7.2.2-1
GPU Type: GeForce GTX 1650 Ti Mobile
Nvidia Driver Version: 525.85.12
CUDA Version: 11.1
CUDNN Version: 8.0.5.39
Operating System + Version: Linux 22.04
Python Version (if applicable):
TensorFlow Version (if applicable):
PyTorch Version (if applicable):
Baremetal or Container (if container which image + tag):Please attach or include links to any models, data, files, or scripts necessary to reproduce your issue. (Github repo, Google Drive, Dropbox, etc.)I did generated an YOLOv4 .engine file on a given system. However, .engine does not work in similar setup, where just the GPU differs (RTX3080 8GB).
Maybe, you have suggestions why this happens?Thanks in advance!Engines aren’t supposed to work on different devices. Maybe you can build engine with hardware compatibility Developer Guide :: NVIDIA Deep Learning TensorRT Documentation, but I think it can hurt performanceThanks!
I also found some additional information Developer Guide :: NVIDIA Deep Learning TensorRT DocumentationThis topic was automatically closed 14 days after the last reply. New replies are no longer allowed.Powered by Discourse, best viewed with JavaScript enabled"
180,getting-error-runtimeerror-cudnn-error-cudnn-status-execution-failed-while-running-a-basic-rnn-model,"This is my code for a basic RNN model and I am using the MNSIT dataset. My ultimate goal is to train this model on a custom dataset however I am trying to run this model on the MNSIT dataset so that I can be sure that the code and the model are running properly before I try to run my model.When I run this model on my GPU I get the error that has been pasted below. However interestingly, When I run my model on CPU instead of my GPU, the model runs perfectly.And After running the code for about 9 mins, I get this error:A few things I have considered after looking at other posts:After running Nvidia-smi I get:After running nvcc --version I get:And after running  print(torch.version) in python, i get:
 1.5.1+cu101Hi,Based on the info provided, it doesn’t look like TensorRT related issue. The following may help you. If you have further queries, we recommend you to post your concern on related platform.I am trying to run a simple RNN model with LSTM unit but I am getting cuda error (same code is working fine with CPU)  Rnn model is like below  class BiRNN(nn.Module):    ... ...     def forward(self, x):         # Set initial states         h0 =...
Reading time: 3 mins 🕑
Likes: 25 ❤
@ptrblck here the gist:   Installing Conda will take time, I have to install it without affecting the system python.Thank you.Hi,I have the same error. I have broken package??
I have another computer using Cuda 11.6 and it runs ok. Following this link for segmentation, https://www.highvoltagecode.com/post/edge-ai-semantic-segmentation-on-nvidia-jetsonAnd I just bought a new computer and installed 12.1, same code but I got this error.
So, is it the pytorch or cuda problem? I am confused.
I don’t mind to downgrade my cuda to 11.6, I have tried several links but failed to follow. Do you have any working link for downgrading? Thx
I followed this link, but I cannot install the lineHi,Here is  the errorPowered by Discourse, best viewed with JavaScript enabled"
181,unknown-behavior-for-retinaface-mobilenet-face-detection-in-deepstream-6-2,"I’m trying to do face detection via  deepstream 6.2 but I confront with unexpected errors while I have not these errors or warnings in deepstream 6.0 .  Does this problem related to the face detection engine or libnvds_nvmultiobjecttracker.so. It’s possible to resolve it without doing extra changes in face detection model?Deepstream Version: 6.2
TensorRT Version: 8.5.2
GPU Type: RTX 4090
Nvidia Driver Version: 525
CUDA Version: V11.8.89
CUDNN Version: 8.8.0
Operating System + Version:  Ubuntu 20.04
Python Version (if applicable):  Python 3.8.10
TensorFlow Version (if applicable):
PyTorch Version (if applicable):
Baremetal or Container (if container which image + tag):TensorRTx -  Implementation of popular deep learning networks with TensorRT network definition API RetinaFace Pth ModelsGenerated RetinaFace: Mobilenet engine via mentioned instruction in retinaface subdirectory.
retinaface.wts (3.7 MB)I do this work in this environments successfully for deepstream 6.0TensorRT Version: 8.2.0
GPU Type: 2080Ti, 1080Ti, RTX 3060, 1650 Ti
Nvidia Driver Version: 470
CUDA Version: V11.4.152
CUDNN Version: 8.2.4
Operating System + Version:  Ubuntu 20.04 , Ubuntu 18.04So, my problem is related to the newest environment RTX 4090../retina_mnet -sAnyway it generates an engine file with these warnings and I could run face detection for sample images./retina_mnet -dand these warning has been generated:And it shows that engine has been loaded successfully.When I use CUDA_MODULE_LOADING=LAZY in commandCUDA_MODULE_LOADING=LAZY ./retina_mnet -ssome of those warning removed :while non of these warnings was generated in the previous environments.And I got these warnings or errors by running project:And bysetting CUDA_MODULE_LOADING=LAZYHi,This looks like a Deepstream related issue. We will move this post to the Deepstream forum.Thanks!Have you check this challenges and does NVidia has any plan to resolve that?Powered by Discourse, best viewed with JavaScript enabled"
182,cudnn-status-not-supported-with-point-wise-operation-fusion-before-matmul,"Using the new backend cudnn API I am getting CUDNN_STATUS_NOT_SUPPORTED whenever I call cudnnBackendFinalize() on the CUDNN_BACKEND_EXECUTION_PLAN_DESCRIPTOR - when trying to fuse a non-broadcasted point wise addition before a matmul.I can successfully fuse a relu before the matmul, I can also fuse a broadcasted point wise addition before the matmul. The same non-broadcasted point wise addition also works perfectly if fused after the matmul.The datatype of all tensors in the fusion are fp16 and compute type is fp32 as per the fusion engine limitations.This error sounds like I am running up against one of the fusion engines limitations; but having studied them I cannot seem to figure what I am doing wrong. Any suggestions would be greatly appreciated.Hi @jamie.mcculloch21 ,
Can you please share the api/error logs with us for better assistance.ThanksPowered by Discourse, best viewed with JavaScript enabled"
183,pytorch-container-windows,"I was interested in using the nVIDIA PyTorch container with Docker Desktop for Windows and the WSL2 back end enabled.  Since I am a novice, I wanted to know if this is possible, and if so, some guidance on how to proceed with pointers if possible.  For example, do I also need to install the nVIDIA Container Toolkit in order to make use of my nVIDIA GPUs?  Any assistance would be much appreciated…This user guide indicates how to set up WSL2 for NGC container usage.Dear Robert:Thanks for the quick and helpful response.  I will study the guide and then come back with further questions as appropriate.  My intention is to try and use Docker Desktop for Windows to run a containerized Python code that calls your PyTorch container in order to make use of nVIDIA Quadro RTX5000 GPU cores in my workstation.  I will let you know what happens…
Best Regards,
ChristopherI’m not really familiar with docker desktop for windows.  It’s not necessary to use that to launch a NGC container in a WSL2 partition.  If you decide to use it anyway, I don’t know what issues you may run into, and this wouldn’t be the place to ask questions about docker desktop for windows.If you follow the guide I linked, you should be able to launch docker containers in WSL2 using a methodology identical to how you would do it on linux.  That is the only functionality/modality supported by NVIDIA.Thanks for that information — you are already helping me much since I am frankly overwhelmed by all the possibilities and associated documentation.  Again, I will now concentrate on the guide you recommended and see what happens.Powered by Discourse, best viewed with JavaScript enabled"
184,cuda-error-depending-on-batchsize-of-engine-file,"Hello,I am using the Nvidia Jetson AGX Developer Kit:Futhermore I am using a yolov5 small modell trained in pytorch for fp16 and imagesize (416,416). Now I wanted to convert the pt files to engine files and run inference with them. So I used the export.py script from ultralytics/yolov5 repository and convert the yolov5 modell to onnx. After that a converted the modell from onnx to engine on two different ways./usr/src/tensorrt/bin/trtexec --onnx=/path/to/onnx --fp16I have done this for batchsize 1,2,4,8,12,16,32. Than a made an inference with an big image (4864,3232) and crop down the images to (416,416) with a little overlap and put those in batches and feed them to the neural network. For batchsize 1,2,4 it work fine. But for batchsize 8,12,16,32 Cuda throws a big error message:Here is the code, with lead to the error messages. It is only a part of the real code with does the inference for a larger scaled image.I would really appreciating, if someone could help me or even tell me what could be the reason for this error.Yours
PatrickHi,
Request you to share the ONNX model and the script if not shared already so that we can assist you better.
Alongside you can try few things:This NVIDIA TensorRT 8.5.3 Quick Start Guide is a starting point for developers who want to try out TensorRT SDK; specifically, this document demonstrates how to quickly construct an application to run inference on a TensorRT engine.import sys
import onnx
filename = yourONNXmodel
model = onnx.load(filename)
onnx.checker.check_model(model).
2) Try running your model with trtexec command.master/samples/trtexecNVIDIA® TensorRT™, an SDK for high-performance deep learning inference, includes a deep learning inference optimizer and runtime that delivers low latency and high throughput for inference applicat...
In case you are still facing issue, request you to share the trtexec “”–verbose"""" log for further debugging
Thanks!Powered by Discourse, best viewed with JavaScript enabled"
185,riva-server-once-created-is-never-destroyed,"I am using the riva quickstart to test and even sending end() never actually kills the riva_server process.Hi Ryein,Thanks for your interest in RivaTo Stop the running containers we can use bash riva_stop.shTo remove the images and perform cleanup we can use bash riva_clean.shIf your request is different or not related to above ones, please let me knowThanksI’m talking about the thread that gets created.  The old threads are never destroyed.Hi @ryeinMy Apologies for wrong understanding,
Can you provide more inputs/insights, like is the thread Riva related or Triton, Model used etc.ThanksThanksUsing htop you can see riva_server processes being ran.   Every time you create a call with grpc even if you end it the thread isn’t destroyed so if you had multiple calls it will have hundreds of threads.Powered by Discourse, best viewed with JavaScript enabled"
186,nvidia-tesla-a10-normal-working-temperature-and-required-air-flow-through-the-gpu-case-in-cubic-feet-of-air-per-minute-cfm-units,"Hello,we are a small IT startup that wants to make a workstation with one nVidia Tesla A10 Pny for the tasks of training computer vision models. We also want to place this GPU into a ordinary “tower” computer case as lukas.koch in his post (link)We are also concerned about ensuring proper cooling for this graphics card. According to the user manual at link
(nvidia tesla A10 user guide)
“The NVIDIA A10 PCIe card employs a bidirectional heat sink, which accepts airflow either left to-right or right-to-left directions.”According to the answer in the post (link).“The A100 was designed for data center and hence rack mounted servers, however - as long as there is sufficient airflow, the actual chassis doesn’t matter, so a tower installation is an option. There are thermal sensors on the card too - but better cooling translates to better performance.”This means that the installation of this video card in a regular PC case is possible.We would like to get some reference points of the parameters to be sure that we were able to ensure normal unanimous cooling of this video card.If we focus on nVidia servers with nVidia Tesla A10 or A100, what is the temperature of the video cards considered to be the normal mode of their operation? There is this table in the user manual.
If I understand correctly, the temperature of the card should not exceed 55 degrees Celsius? And we should strive for this threshold?I also would like to know how the cooling of video cards of this class is arranged in professional servers for installation in which they are designed?What speed of air flow through the card case should the fans provide so that the GPU feels comfortable with a 24/7 load? Airflow expressed in cubic feet of air per minute (CFM units) (This is the main parameter that we are very concerned about). Knowing this parameter, we would be able to choose the optimal coolers that provide this airflow for its cooling.We would also be very grateful if you would share these information or indicate where it can be read.
We will be happy to share our results on installing this video card in a regular PC case. =)Sorry you didn’t get a response to your post.
Did you get any help through your distribution channel for the NVIDIA cards?
If you still need some help let me know and I can try to get some help.
ThanksHello,Could this issue be reponed? I have a similar use case. I will have a server with 2 A10 cards but the server has to be placed near people and admittedly, for a 2U chassis 80mm fans spinning at 13K RPM is very loud and annoying - specced out by our system integrator. Can nVidia provide any numbers on how to cool the cards?
Or is there a list of partner card manufacturers such as gigabyte/asus that design the GPU with active cooling?
Another potential solution for us could be liquid cooling.Powered by Discourse, best viewed with JavaScript enabled"
187,custom-plugin-supporting-int8-i-o-type-check-fail,"I’m trying to create a custom plugin which takes one int8 tensor and one int32 tensor as input. It should output one int8 tensor and one int32 tensor. (without calibration)However, when I try to build the engine, inside supportsFormatCombination, it shows the input and output which supposed to have int8 type having float32. I’m not sure what to do here.Here is a link to my onnx model: spconv_relu_int8_modified.onnx - Google Drive. Can anyone please check?TensorRT Version: 8.5.1.7
GPU Type: 3090
Nvidia Driver Version: 515.105.01
CUDA Version: 11.2
CUDNN Version:
Operating System + Version: ubuntu18.04
Python Version (if applicable):
TensorFlow Version (if applicable):
PyTorch Version (if applicable):
Baremetal or Container (if container which image + tag):Please attach or include links to any models, data, files, or scripts necessary to reproduce your issue. (Github repo, Google Drive, Dropbox, etc.)
Screenshot from 2023-05-10 16-12-391072×477 71.2 KB

Screenshot from 2023-05-11 10-05-231261×146 24.1 KB
Please include:Hi,We tried reproducing the above error, but we are facing a different error, as follows:
Please use the latest TensorRT version, 8.6.1.The error states that the importer could not find the plugin named “Plugin” with version 1. Could you please make sure the plugin name and functionality are correctly implemented/configured.Hi,
Please refer to below links related custom plugin implementation and sample:This Samples Support Guide provides an overview of all the supported NVIDIA TensorRT 8.6.1 samples included on GitHub and in the product package. The TensorRT samples specifically help in areas such as recommenders, machine comprehension, character...TensorRT is an SDK for high performance, deep learning inference. It includes a deep learning inference optimizer and a runtime that delivers low latency and high throughput for deep learning…While IPluginV2 and IPluginV2Ext interfaces are still supported for backward compatibility with TensorRT 5.1 and 6.0.x respectively, however, we recommend that you write new plugins or refactor existing ones to target the IPluginV2DynamicExt or IPluginV2IOExt interfaces instead.Thanks!Powered by Discourse, best viewed with JavaScript enabled"
188,convtranspose-add-slow,"I upgraded TensorRT from 8.2.1 to 8.6.1 recently. The inference time for the same onnx model takes 2x time more than before. After overriding IProfiler class to print the time consumption for each layer, it shows that ConvTranspose + Add in 8.6.1 takes much more time than which in 8.2.1.8.6.1:
Reformatting CopyNode for Input Tensor 0 to ConvTranspose_102 + Add_103 0.006144ms
ConvTranspose_102 + Add_103 2.00294ms8.2.1
ConvTranspose_102 0.072704ms
Add_103 0.0256msIs that a bug? How can I solve this problem?
Thanks for any help.TensorRT Version: 8.6.1
GPU Type:  NVIDIA GeForce RTX 3060 Laptop GPU
Nvidia Driver Version: 528.79
CUDA Version: 11.8
CUDNN Version: 8.9.0
Operating System + Version:  Windows 11 home 22h2Hi,Request you to share the model, script, profiler, and performance output if not shared already so that we can help you better.Alternatively, you can try running your model with trtexec command.master/samples/trtexecNVIDIA® TensorRT™, an SDK for high-performance deep learning inference, includes a deep learning inference optimizer and runtime that delivers low latency and high throughput for inference applicat...While measuring the model performance, make sure you consider the latency and throughput of the network inference, excluding the data pre and post-processing overhead.
Please refer to the below links for more details:This Best Practices Guide covers various performance considerations related to deploying networks using TensorRT 8.0.3. These sections assume that you have a model that is working at an appropriate level of accuracy and that you are able to...This Best Practices Guide covers various performance considerations related to deploying networks using TensorRT 8.0.3. These sections assume that you have a model that is working at an appropriate level of accuracy and that you are able to...Thanks!8.6.1 trtexec results:
[07/04/2023-15:52:15] [I] === Model Options ===
[07/04/2023-15:52:15] [I] Format: ONNX
[07/04/2023-15:52:15] [I] Model: test.onnx
[07/04/2023-15:52:15] [I] Output:
[07/04/2023-15:52:15] [I] === Build Options ===
[07/04/2023-15:52:15] [I] Max batch: explicit batch
[07/04/2023-15:52:15] [I] Memory Pools: workspace: default, dlaSRAM: default, dlaLocalDRAM: default, dlaGlobalDRAM: default
[07/04/2023-15:52:15] [I] minTiming: 1
[07/04/2023-15:52:15] [I] avgTiming: 8
[07/04/2023-15:52:15] [I] Precision: FP32
[07/04/2023-15:52:15] [I] LayerPrecisions:
[07/04/2023-15:52:15] [I] Layer Device Types:
[07/04/2023-15:52:15] [I] Calibration:
[07/04/2023-15:52:15] [I] Refit: Disabled
[07/04/2023-15:52:15] [I] Version Compatible: Disabled
[07/04/2023-15:52:15] [I] TensorRT runtime: full
[07/04/2023-15:52:15] [I] Lean DLL Path:
[07/04/2023-15:52:15] [I] Tempfile Controls: { in_memory: allow, temporary: allow }
[07/04/2023-15:52:15] [I] Exclude Lean Runtime: Disabled
[07/04/2023-15:52:15] [I] Sparsity: Disabled
[07/04/2023-15:52:15] [I] Safe mode: Disabled
[07/04/2023-15:52:15] [I] Build DLA standalone loadable: Disabled
[07/04/2023-15:52:15] [I] Allow GPU fallback for DLA: Disabled
[07/04/2023-15:52:15] [I] DirectIO mode: Disabled
[07/04/2023-15:52:15] [I] Restricted mode: Disabled
[07/04/2023-15:52:15] [I] Skip inference: Disabled
[07/04/2023-15:52:15] [I] Save engine: test.trt
[07/04/2023-15:52:15] [I] Load engine:
[07/04/2023-15:52:15] [I] Profiling verbosity: 0
[07/04/2023-15:52:15] [I] Tactic sources: Using default tactic sources
[07/04/2023-15:52:15] [I] timingCacheMode: local
[07/04/2023-15:52:15] [I] timingCacheFile:
[07/04/2023-15:52:15] [I] Heuristic: Disabled
[07/04/2023-15:52:15] [I] Preview Features: Use default preview flags.
[07/04/2023-15:52:15] [I] MaxAuxStreams: -1
[07/04/2023-15:52:15] [I] BuilderOptimizationLevel: -1
[07/04/2023-15:52:15] [I] Input(s)s format: fp32:CHW
[07/04/2023-15:52:15] [I] Output(s)s format: fp32:CHW
[07/04/2023-15:52:15] [I] Input build shapes: model
[07/04/2023-15:52:15] [I] Input calibration shapes: model
[07/04/2023-15:52:15] [I] === System Options ===
[07/04/2023-15:52:15] [I] Device: 0
[07/04/2023-15:52:15] [I] DLACore:
[07/04/2023-15:52:15] [I] Plugins:
[07/04/2023-15:52:15] [I] setPluginsToSerialize:
[07/04/2023-15:52:15] [I] dynamicPlugins:
[07/04/2023-15:52:15] [I] ignoreParsedPluginLibs: 0
[07/04/2023-15:52:15] [I]
[07/04/2023-15:52:15] [I] === Inference Options ===
[07/04/2023-15:52:15] [I] Batch: Explicit
[07/04/2023-15:52:15] [I] Input inference shapes: model
[07/04/2023-15:52:15] [I] Iterations: 10
[07/04/2023-15:52:15] [I] Duration: 3s (+ 200ms warm up)
[07/04/2023-15:52:15] [I] Sleep time: 0ms
[07/04/2023-15:52:15] [I] Idle time: 0ms
[07/04/2023-15:52:15] [I] Inference Streams: 1
[07/04/2023-15:52:15] [I] ExposeDMA: Disabled
[07/04/2023-15:52:15] [I] Data transfers: Enabled
[07/04/2023-15:52:15] [I] Spin-wait: Disabled
[07/04/2023-15:52:15] [I] Multithreading: Disabled
[07/04/2023-15:52:15] [I] CUDA Graph: Disabled
[07/04/2023-15:52:15] [I] Separate profiling: Disabled
[07/04/2023-15:52:15] [I] Time Deserialize: Disabled
[07/04/2023-15:52:15] [I] Time Refit: Disabled
[07/04/2023-15:52:15] [I] NVTX verbosity: 0
[07/04/2023-15:52:15] [I] Persistent Cache Ratio: 0
[07/04/2023-15:52:15] [I] Inputs:
[07/04/2023-15:52:15] [I] === Reporting Options ===
[07/04/2023-15:52:15] [I] Verbose: Disabled
[07/04/2023-15:52:15] [I] Averages: 10 inferences
[07/04/2023-15:52:15] [I] Percentiles: 90,95,99
[07/04/2023-15:52:15] [I] Dump refittable layers:Disabled
[07/04/2023-15:52:15] [I] Dump output: Disabled
[07/04/2023-15:52:15] [I] Profile: Disabled
[07/04/2023-15:52:15] [I] Export timing to JSON file:
[07/04/2023-15:52:15] [I] Export output to JSON file:
[07/04/2023-15:52:15] [I] Export profile to JSON file:
[07/04/2023-15:52:15] [I]
[07/04/2023-15:52:15] [I] === Device Information ===
[07/04/2023-15:52:15] [I] Selected Device: NVIDIA GeForce RTX 3060 Laptop GPU
[07/04/2023-15:52:15] [I] Compute Capability: 8.6
[07/04/2023-15:52:15] [I] SMs: 30
[07/04/2023-15:52:15] [I] Device Global Memory: 6143 MiB
[07/04/2023-15:52:15] [I] Shared Memory per SM: 100 KiB
[07/04/2023-15:52:15] [I] Memory Bus Width: 192 bits (ECC disabled)
[07/04/2023-15:52:15] [I] Application Compute Clock Rate: 1.702 GHz
[07/04/2023-15:52:15] [I] Application Memory Clock Rate: 7.001 GHz
[07/04/2023-15:52:15] [I]
[07/04/2023-15:52:15] [I] Note: The application clock rates do not reflect the actual clock rates that the GPU is currently running at.
[07/04/2023-15:52:15] [I]
[07/04/2023-15:52:15] [I] TensorRT version: 8.6.1
[07/04/2023-15:52:15] [I] Loading standard plugins
[07/04/2023-15:52:15] [I] [TRT] [MemUsageChange] Init CUDA: CPU +284, GPU +0, now: CPU 12241, GPU 1072 (MiB)
[07/04/2023-15:52:19] [I] [TRT] [MemUsageChange] Init builder kernel library: CPU +1162, GPU +264, now: CPU 14543, GPU 1336 (MiB)
[07/04/2023-15:52:19] [W] [TRT] CUDA lazy loading is not enabled. Enabling it can significantly reduce device memory usage and speed up TensorRT initialization. See “Lazy Loading” section of CUDA documentation CUDA C++ Programming Guide
[07/04/2023-15:52:19] [I] Start parsing network model.
[07/04/2023-15:52:19] [I] [TRT] ----------------------------------------------------------------
[07/04/2023-15:52:19] [I] [TRT] Input filename:   test.onnx
[07/04/2023-15:52:19] [I] [TRT] ONNX IR version:  0.0.6
[07/04/2023-15:52:19] [I] [TRT] Opset version:    11
[07/04/2023-15:52:19] [I] [TRT] Producer name:    pytorch
[07/04/2023-15:52:19] [I] [TRT] Producer version: 1.9
[07/04/2023-15:52:19] [I] [TRT] Domain:
[07/04/2023-15:52:19] [I] [TRT] Model version:    0
[07/04/2023-15:52:19] [I] [TRT] Doc string:
[07/04/2023-15:52:19] [I] [TRT] ----------------------------------------------------------------
[07/04/2023-15:52:19] [I] Finished parsing network model. Parse time: 0.110906
[07/04/2023-15:52:19] [I] [TRT] Graph optimization time: 0.0062036 seconds.
[07/04/2023-15:52:19] [I] [TRT] Local timing cache in use. Profiling results in this builder pass will not be stored.
[07/04/2023-15:52:45] [I] [TRT] Detected 1 inputs and 1 output network tensors.
[07/04/2023-15:52:45] [I] [TRT] Total Host Persistent Memory: 262480
[07/04/2023-15:52:45] [I] [TRT] Total Device Persistent Memory: 5120
[07/04/2023-15:52:45] [I] [TRT] Total Scratch Memory: 8652800
[07/04/2023-15:52:45] [I] [TRT] [MemUsageStats] Peak memory usage of TRT CPU/GPU memory allocators: CPU 25 MiB, GPU 116 MiB
[07/04/2023-15:52:45] [I] [TRT] [BlockAssignment] Started assigning block shifts. This will take 115 steps to complete.
[07/04/2023-15:52:45] [I] [TRT] [BlockAssignment] Algorithm ShiftNTopDown took 6.9475ms to assign 10 blocks to 115 nodes requiring 45090304 bytes.
[07/04/2023-15:52:45] [I] [TRT] Total Activation Memory: 45088768
[07/04/2023-15:52:45] [I] [TRT] [MemUsageChange] TensorRT-managed allocation in building engine: CPU +0, GPU +116, now: CPU 0, GPU 116 (MiB)
[07/04/2023-15:52:46] [I] Engine built in 30.9293 sec.
[07/04/2023-15:52:46] [I] [TRT] Loaded engine size: 117 MiB
[07/04/2023-15:52:46] [I] [TRT] [MemUsageChange] TensorRT-managed allocation in engine deserialization: CPU +0, GPU +115, now: CPU 0, GPU 115 (MiB)
[07/04/2023-15:52:46] [I] Engine deserialized in 0.0321676 sec.
[07/04/2023-15:52:46] [I] [TRT] [MemUsageChange] TensorRT-managed allocation in IExecutionContext creation: CPU +0, GPU +43, now: CPU 0, GPU 158 (MiB)
[07/04/2023-15:52:46] [W] [TRT] CUDA lazy loading is not enabled. Enabling it can significantly reduce device memory usage and speed up TensorRT initialization. See “Lazy Loading” section of CUDA documentation CUDA C++ Programming Guide
[07/04/2023-15:52:46] [I] Setting persistentCacheLimit to 0 bytes.
[07/04/2023-15:52:46] [I] Using random values for input input
[07/04/2023-15:52:46] [I] Input binding for input with dimensions 1x3x512x512 is created.
[07/04/2023-15:52:46] [I] Output binding for output with dimensions 1x64x128x128 is created.
[07/04/2023-15:52:46] [I] Starting inference
[07/04/2023-15:52:49] [I] Warmup completed 6 queries over 200 ms
[07/04/2023-15:52:49] [I] Timing trace has 89 queries over 3.0655 s
[07/04/2023-15:52:49] [I]
[07/04/2023-15:52:49] [I] === Trace details ===
[07/04/2023-15:52:49] [I] Trace averages of 10 runs:
[07/04/2023-15:52:49] [I] Average on 10 runs - GPU latency: 33.0017 ms - Host latency: 33.5733 ms (enqueue 33.4361 ms)
[07/04/2023-15:52:49] [I] Average on 10 runs - GPU latency: 34.3227 ms - Host latency: 34.8911 ms (enqueue 34.0019 ms)
[07/04/2023-15:52:49] [I] Average on 10 runs - GPU latency: 35.0873 ms - Host latency: 35.6554 ms (enqueue 35.6215 ms)
[07/04/2023-15:52:49] [I] Average on 10 runs - GPU latency: 33.5733 ms - Host latency: 34.1425 ms (enqueue 34.2074 ms)
[07/04/2023-15:52:49] [I] Average on 10 runs - GPU latency: 32.4652 ms - Host latency: 33.0368 ms (enqueue 32.9492 ms)
[07/04/2023-15:52:49] [I] Average on 10 runs - GPU latency: 34.9549 ms - Host latency: 35.5234 ms (enqueue 35.1269 ms)
[07/04/2023-15:52:49] [I] Average on 10 runs - GPU latency: 35.5306 ms - Host latency: 36.0989 ms (enqueue 36.6388 ms)
[07/04/2023-15:52:49] [I] Average on 10 runs - GPU latency: 32.14 ms - Host latency: 32.709 ms (enqueue 32.3849 ms)
[07/04/2023-15:52:49] [I]
[07/04/2023-15:52:49] [I] === Performance summary ===
[07/04/2023-15:52:49] [I] Throughput: 29.0328 qps
[07/04/2023-15:52:49] [I] Latency: min = 29.62 ms, max = 40.5161 ms, mean = 34.3186 ms, median = 34.5432 ms, percentile(90%) = 37.9171 ms, percentile(95%) = 38.6229 ms, percentile(99%) = 40.5161 ms
[07/04/2023-15:52:49] [I] Enqueue Time: min = 22.5655 ms, max = 44.9667 ms, mean = 34.2031 ms, median = 33.9287 ms, percentile(90%) = 39.4993 ms, percentile(95%) = 40.7681 ms, percentile(99%) = 44.9667 ms
[07/04/2023-15:52:49] [I] H2D Latency: min = 0.245361 ms, max = 0.272644 ms, mean = 0.246794 ms, median = 0.246399 ms, percentile(90%) = 0.24707 ms, percentile(95%) = 0.247925 ms, percentile(99%) = 0.272644 ms
[07/04/2023-15:52:49] [I] GPU Compute Time: min = 29.0519 ms, max = 39.9473 ms, mean = 33.7494 ms, median = 33.9743 ms, percentile(90%) = 37.3484 ms, percentile(95%) = 38.0539 ms, percentile(99%) = 39.9473 ms
[07/04/2023-15:52:49] [I] D2H Latency: min = 0.321533 ms, max = 0.346924 ms, mean = 0.322436 ms, median = 0.321777 ms, percentile(90%) = 0.322266 ms, percentile(95%) = 0.325439 ms, percentile(99%) = 0.346924 ms
[07/04/2023-15:52:49] [I] Total Host Walltime: 3.0655 s
[07/04/2023-15:52:49] [I] Total GPU Compute Time: 3.00369 s
[07/04/2023-15:52:49] [W] * Throughput may be bound by Enqueue Time rather than GPU Compute and the GPU may be under-utilized.
[07/04/2023-15:52:49] [W]   If not already in use, --useCudaGraph (utilize CUDA graphs where possible) may increase the throughput.
[07/04/2023-15:52:49] [W] * GPU compute time is unstable, with coefficient of variance = 8.9478%.
[07/04/2023-15:52:49] [W]   If not already in use, locking GPU clock frequency or adding --useSpinWait may improve the stability.
[07/04/2023-15:52:49] [I] Explanations of the performance metrics are printed in the verbose logs.8.2.1 trtexec results:
[07/04/2023-16:00:00] [I] === Model Options ===
[07/04/2023-16:00:00] [I] Format: ONNX
[07/04/2023-16:00:00] [I] Model: test.onnx
[07/04/2023-16:00:00] [I] Output:
[07/04/2023-16:00:00] [I] === Build Options ===
[07/04/2023-16:00:00] [I] Max batch: explicit batch
[07/04/2023-16:00:00] [I] Workspace: 16 MiB
[07/04/2023-16:00:00] [I] minTiming: 1
[07/04/2023-16:00:00] [I] avgTiming: 8
[07/04/2023-16:00:00] [I] Precision: FP32
[07/04/2023-16:00:00] [I] Calibration:
[07/04/2023-16:00:00] [I] Refit: Disabled
[07/04/2023-16:00:00] [I] Sparsity: Disabled
[07/04/2023-16:00:00] [I] Safe mode: Disabled
[07/04/2023-16:00:00] [I] DirectIO mode: Disabled
[07/04/2023-16:00:00] [I] Restricted mode: Disabled
[07/04/2023-16:00:00] [I] Save engine: test.trt
[07/04/2023-16:00:00] [I] Load engine:
[07/04/2023-16:00:00] [I] Profiling verbosity: 0
[07/04/2023-16:00:00] [I] Tactic sources: Using default tactic sources
[07/04/2023-16:00:00] [I] timingCacheMode: local
[07/04/2023-16:00:00] [I] timingCacheFile:
[07/04/2023-16:00:00] [I] Input(s)s format: fp32:CHW
[07/04/2023-16:00:00] [I] Output(s)s format: fp32:CHW
[07/04/2023-16:00:00] [I] Input build shapes: model
[07/04/2023-16:00:00] [I] Input calibration shapes: model
[07/04/2023-16:00:00] [I] === System Options ===
[07/04/2023-16:00:00] [I] Device: 0
[07/04/2023-16:00:00] [I] DLACore:
[07/04/2023-16:00:00] [I] Plugins:
[07/04/2023-16:00:00] [I] === Inference Options ===
[07/04/2023-16:00:00] [I] Batch: Explicit
[07/04/2023-16:00:00] [I] Input inference shapes: model
[07/04/2023-16:00:00] [I] Iterations: 10
[07/04/2023-16:00:00] [I] Duration: 3s (+ 200ms warm up)
[07/04/2023-16:00:00] [I] Sleep time: 0ms
[07/04/2023-16:00:00] [I] Idle time: 0ms
[07/04/2023-16:00:00] [I] Streams: 1
[07/04/2023-16:00:00] [I] ExposeDMA: Disabled
[07/04/2023-16:00:00] [I] Data transfers: Enabled
[07/04/2023-16:00:00] [I] Spin-wait: Disabled
[07/04/2023-16:00:00] [I] Multithreading: Disabled
[07/04/2023-16:00:00] [I] CUDA Graph: Disabled
[07/04/2023-16:00:00] [I] Separate profiling: Disabled
[07/04/2023-16:00:00] [I] Time Deserialize: Disabled
[07/04/2023-16:00:00] [I] Time Refit: Disabled
[07/04/2023-16:00:00] [I] Skip inference: Disabled
[07/04/2023-16:00:00] [I] Inputs:
[07/04/2023-16:00:00] [I] === Reporting Options ===
[07/04/2023-16:00:00] [I] Verbose: Disabled
[07/04/2023-16:00:00] [I] Averages: 10 inferences
[07/04/2023-16:00:00] [I] Percentile: 99
[07/04/2023-16:00:00] [I] Dump refittable layers:Disabled
[07/04/2023-16:00:00] [I] Dump output: Disabled
[07/04/2023-16:00:00] [I] Profile: Disabled
[07/04/2023-16:00:00] [I] Export timing to JSON file:
[07/04/2023-16:00:00] [I] Export output to JSON file:
[07/04/2023-16:00:00] [I] Export profile to JSON file:
[07/04/2023-16:00:00] [I]
[07/04/2023-16:00:00] [I] === Device Information ===
[07/04/2023-16:00:00] [I] Selected Device: NVIDIA GeForce RTX 3060 Laptop GPU
[07/04/2023-16:00:00] [I] Compute Capability: 8.6
[07/04/2023-16:00:00] [I] SMs: 30
[07/04/2023-16:00:00] [I] Compute Clock Rate: 1.702 GHz
[07/04/2023-16:00:00] [I] Device Global Memory: 6143 MiB
[07/04/2023-16:00:00] [I] Shared Memory per SM: 100 KiB
[07/04/2023-16:00:00] [I] Memory Bus Width: 192 bits (ECC disabled)
[07/04/2023-16:00:00] [I] Memory Clock Rate: 7.001 GHz
[07/04/2023-16:00:00] [I]
[07/04/2023-16:00:00] [I] TensorRT version: 8.2.1
[07/04/2023-16:00:00] [I] [TRT] [MemUsageChange] Init CUDA: CPU +277, GPU +0, now: CPU 10622, GPU 1072 (MiB)
[07/04/2023-16:00:11] [I] [TRT] [MemUsageChange] Init builder kernel library: CPU +1199, GPU +264, now: CPU 12819, GPU 1336 (MiB)
[07/04/2023-16:00:11] [W] [TRT] CUDA lazy loading is not enabled. Enabling it can significantly reduce device memory usage and speed up TensorRT initialization. See “Lazy Loading” section of CUDA documentation CUDA C++ Programming Guide
[07/04/2023-16:00:11] [I] Start parsing network model
[07/04/2023-16:00:11] [I] [TRT] ----------------------------------------------------------------
[07/04/2023-16:00:11] [I] [TRT] Input filename:   test.onnx
[07/04/2023-16:00:11] [I] [TRT] ONNX IR version:  0.0.6
[07/04/2023-16:00:11] [I] [TRT] Opset version:    11
[07/04/2023-16:00:11] [I] [TRT] Producer name:    pytorch
[07/04/2023-16:00:11] [I] [TRT] Producer version: 1.9
[07/04/2023-16:00:11] [I] [TRT] Domain:
[07/04/2023-16:00:11] [I] [TRT] Model version:    0
[07/04/2023-16:00:11] [I] [TRT] Doc string:
[07/04/2023-16:00:11] [I] [TRT] ----------------------------------------------------------------
[07/04/2023-16:00:11] [I] Finish parsing network model
[07/04/2023-16:00:11] [I] [TRT] Graph optimization time: 0.0065841 seconds.
[07/04/2023-16:00:11] [I] [TRT] Local timing cache in use. Profiling results in this builder pass will not be stored.
[07/04/2023-16:00:33] [I] [TRT] Detected 1 inputs and 1 output network tensors.
[07/04/2023-16:00:33] [I] [TRT] Total Host Persistent Memory: 260208
[07/04/2023-16:00:33] [I] [TRT] Total Device Persistent Memory: 236032
[07/04/2023-16:00:33] [I] [TRT] Total Scratch Memory: 2098176
[07/04/2023-16:00:33] [I] [TRT] [MemUsageStats] Peak memory usage of TRT CPU/GPU memory allocators: CPU 25 MiB, GPU 119 MiB
[07/04/2023-16:00:33] [I] [TRT] [BlockAssignment] Started assigning block shifts. This will take 110 steps to complete.
[07/04/2023-16:00:33] [I] [TRT] [BlockAssignment] Algorithm ShiftNTopDown took 6.444ms to assign 8 blocks to 110 nodes requiring 45089280 bytes.
[07/04/2023-16:00:33] [I] [TRT] Total Activation Memory: 45088768
[07/04/2023-16:00:33] [I] [TRT] [MemUsageChange] TensorRT-managed allocation in building engine: CPU +0, GPU +115, now: CPU 0, GPU 115 (MiB)
[07/04/2023-16:00:33] [I] [TRT] Loaded engine size: 117 MiB
[07/04/2023-16:00:33] [I] [TRT] [MemUsageChange] TensorRT-managed allocation in engine deserialization: CPU +0, GPU +114, now: CPU 0, GPU 114 (MiB)
[07/04/2023-16:00:33] [E] Error[3]: [runtime.cpp::nvinfer1::Runtime::~Runtime::346] Error Code 3: API Usage Error (Parameter check failed at: runtime.cpp::nvinfer1::Runtime::~Runtime::346, condition: mEngineCounter.use_count() == 1. Destroying a runtime before destroying deserialized engines created by the runtime leads to undefined behavior.
)
[07/04/2023-16:00:33] [E] Error[3]: [builder.cpp::nvinfer1::builder::Builder::~Builder::341] Error Code 3: API Usage Error (Parameter check failed at: builder.cpp::nvinfer1::builder::Builder::~Builder::341, condition: mObjectCounter.use_count() == 1. Destroying a builder object before destroying objects it created leads to undefined behavior.
)
[07/04/2023-16:00:33] [I] Engine built in 33.3251 sec.
[07/04/2023-16:00:33] [I] [TRT] [MemUsageChange] TensorRT-managed allocation in IExecutionContext creation: CPU +0, GPU +43, now: CPU 0, GPU 157 (MiB)
[07/04/2023-16:00:33] [W] [TRT] CUDA lazy loading is not enabled. Enabling it can significantly reduce device memory usage and speed up TensorRT initialization. See “Lazy Loading” section of CUDA documentation CUDA C++ Programming Guide
[07/04/2023-16:00:33] [I] Using random values for input input
[07/04/2023-16:00:33] [I] Created input binding for input with dimensions 1x3x512x512
[07/04/2023-16:00:34] [I] Using random values for output output
[07/04/2023-16:00:34] [I] Created output binding for output with dimensions 1x64x128x128
[07/04/2023-16:00:34] [I] Starting inference
[07/04/2023-16:00:37] [I] Warmup completed 10 queries over 200 ms
[07/04/2023-16:00:37] [I] Timing trace has 148 queries over 3.03358 s
[07/04/2023-16:00:37] [I]
[07/04/2023-16:00:37] [I] === Trace details ===
[07/04/2023-16:00:37] [I] Trace averages of 10 runs:
[07/04/2023-16:00:37] [I] Average on 10 runs - GPU latency: 21.458 ms - Host latency: 22.0232 ms (end to end 22.1509 ms, enqueue 21.1855 ms)
[07/04/2023-16:00:37] [I] Average on 10 runs - GPU latency: 20.8364 ms - Host latency: 21.4018 ms (end to end 21.5103 ms, enqueue 20.0202 ms)
[07/04/2023-16:00:37] [I] Average on 10 runs - GPU latency: 19.4282 ms - Host latency: 19.9976 ms (end to end 20.1049 ms, enqueue 19.8673 ms)
[07/04/2023-16:00:37] [I] Average on 10 runs - GPU latency: 19.3706 ms - Host latency: 19.9353 ms (end to end 20.0339 ms, enqueue 18.2692 ms)
[07/04/2023-16:00:37] [I] Average on 10 runs - GPU latency: 21.314 ms - Host latency: 21.8795 ms (end to end 22.0339 ms, enqueue 20.7183 ms)
[07/04/2023-16:00:37] [I] Average on 10 runs - GPU latency: 19.4384 ms - Host latency: 20.0034 ms (end to end 20.101 ms, enqueue 19.0662 ms)
[07/04/2023-16:00:37] [I] Average on 10 runs - GPU latency: 18.748 ms - Host latency: 19.3135 ms (end to end 19.4005 ms, enqueue 19.0619 ms)
[07/04/2023-16:00:37] [I] Average on 10 runs - GPU latency: 18.4604 ms - Host latency: 19.0252 ms (end to end 19.11 ms, enqueue 16.8491 ms)
[07/04/2023-16:00:37] [I] Average on 10 runs - GPU latency: 18.3616 ms - Host latency: 18.9261 ms (end to end 19.0176 ms, enqueue 18.4146 ms)
[07/04/2023-16:00:37] [I] Average on 10 runs - GPU latency: 20.4157 ms - Host latency: 20.9807 ms (end to end 21.1246 ms, enqueue 19.4072 ms)
[07/04/2023-16:00:37] [I] Average on 10 runs - GPU latency: 19.2681 ms - Host latency: 19.8331 ms (end to end 19.9717 ms, enqueue 18.7269 ms)
[07/04/2023-16:00:37] [I] Average on 10 runs - GPU latency: 20.3905 ms - Host latency: 20.9555 ms (end to end 21.0898 ms, enqueue 20.3416 ms)
[07/04/2023-16:00:37] [I] Average on 10 runs - GPU latency: 20.8597 ms - Host latency: 21.425 ms (end to end 21.5364 ms, enqueue 21.4828 ms)
[07/04/2023-16:00:37] [I] Average on 10 runs - GPU latency: 19.6132 ms - Host latency: 20.1783 ms (end to end 20.2763 ms, enqueue 19.0764 ms)
[07/04/2023-16:00:37] [I]
[07/04/2023-16:00:37] [I] === Performance summary ===
[07/04/2023-16:00:37] [I] Throughput: 48.7872 qps
[07/04/2023-16:00:37] [I] Latency: min = 16.9426 ms, max = 25.7742 ms, mean = 20.3844 ms, median = 21.1941 ms, percentile(99%) = 25.6042 ms
[07/04/2023-16:00:37] [I] End-to-End Host Latency: min = 17.0652 ms, max = 25.8479 ms, mean = 20.4956 ms, median = 21.315 ms, percentile(99%) = 25.7107 ms
[07/04/2023-16:00:37] [I] Enqueue Time: min = 9.98462 ms, max = 27.6577 ms, mean = 19.4512 ms, median = 19.9514 ms, percentile(99%) = 24.9242 ms
[07/04/2023-16:00:37] [I] H2D Latency: min = 0.243652 ms, max = 0.251709 ms, mean = 0.24467 ms, median = 0.244629 ms, percentile(99%) = 0.245728 ms
[07/04/2023-16:00:37] [I] GPU Compute Time: min = 16.3779 ms, max = 25.2097 ms, mean = 19.819 ms, median = 20.6289 ms, percentile(99%) = 25.0398 ms
[07/04/2023-16:00:37] [I] D2H Latency: min = 0.320068 ms, max = 0.361877 ms, mean = 0.32066 ms, median = 0.320313 ms, percentile(99%) = 0.32251 ms
[07/04/2023-16:00:37] [I] Total Host Walltime: 3.03358 s
[07/04/2023-16:00:37] [I] Total GPU Compute Time: 2.93322 s
[07/04/2023-16:00:37] [W] * Throughput may be bound by Enqueue Time rather than GPU Compute and the GPU may be under-utilized.
[07/04/2023-16:00:37] [W]   If not already in use, --useCudaGraph (utilize CUDA graphs where possible) may increase the throughput.
[07/04/2023-16:00:37] [I] Explanations of the performance metrics are printed in the verbose logs.The onnx model:Google Drive file.Hi,We have a similar known issue; please allow us some time to work on this issue.Thank you.Powered by Discourse, best viewed with JavaScript enabled"
189,is-it-safe-to-deallocate-nvinfer1-iruntime-after-creating-an-nvinfer1-icudaengine-but-before-running-inference-with-said-icudaengine,"I have structured my application such that I create an IRuntime as follows:I then use the IRuntime to deserailize the model and to create an ICudaEngine:Due to the way my application is structured, the  runtime variable falls out of scope and is deallocated before I have a chance to run inference using the m_engine. Is this safe? Or must the IRuntime instance remain in scope until inference is complete?Hi,IRuntime object in TensorRT should not be deallocated or destroyed before using any engine that was deserialized using that runtime.Thank you.Powered by Discourse, best viewed with JavaScript enabled"
190,cuda-and-nvidia-470-ge-710-tensorflow,"I have just installed Ubuntu 22.04. My Nvidia card is a Geforce GT 710. Nvidia libaries installed fine.nvidia-smi output is±----------------------------------------------------------------------------+
| NVIDIA-SMI 470.182.03   Driver Version: 470.182.03   CUDA Version: 11.4     |
|-------------------------------±---------------------±---------------------+
| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |
|                               |                      |               MIG M. |
|===============================+======================+======================|
|   0  NVIDIA GeForce …  Off  | 00000000:01:00.0 N/A |                  N/A |
| 50%   47C    P0    N/A /  N/A |    234MiB /  2000MiB |     N/A      Default |
|                               |                      |                  N/A |
±------------------------------±---------------------±---------------------+±----------------------------------------------------------------------------+
| Processes:                                                                  |
|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |
|        ID   ID                                                   Usage      |
|=============================================================================|
|  No running processes found                                                 |
±----------------------------------------------------------------------------+Its working fine.On Ubuntu 20 I had the CUDA libraries (toolkit 11) installed and was able to compile tensorflow using docker.I wish to do the same for 22.04. However when installing cuda using apt-get install cuda it wants to uninstall the 470 drivers for my card and install the 530 drivers and toolkit 12. So,I have noted that there is a version of cudnn for toolkit 11 and downloaded, but not installed, the link
cudnn-local-repo-ubuntu2204-8.9.2.26_1.0-1_amd64.deb. If this is available, why can’t I find the link to the 11.xThanks for your helpIf you want to use CUDA 11.x, remove any old packages and follow these instructions.Or you can stick with CUDA 12.x (highly suggested) and use the cuDNN for CUDA 12.x locate here.Thanks, will do.A question. Does this cause the 530 driver to be installed? If so, I assume this is compatible with the GT 710?Finally, I should have mentioned I’m using Lubuntu. Works very well on my old machine.Powered by Discourse, best viewed with JavaScript enabled"
191,nvidia-flare-limitations-and-potentials,"I am interested in understanding the limitations of Nvidia Flare. I’m aware that it supports several frameworks, such as PyTorch, TensorFlow, Numpy, or MONAI. I am wondering if any model built using these frameworks would be able to function in a federated manner. Is there a particular subset of models that would be better suited for use in this platform?Moreover, I relatively sure that I can construct models such as neural networks, logistic regression, linear regression, support vector machines, k-means, XGBoost, and random forests using Nvidia Flare. However, I am curious about the feasibility of implementing other models, such as Naive Bayes, decision trees, and gradient boosting. Would these models also be compatible with Nvidia Flare?Thanks in advance!Powered by Discourse, best viewed with JavaScript enabled"
192,trt-repeated-layer-name-stage2-split-1-layers-must-have-distinct-names,"Hi,tf → onnx → trt, it’s ok when I use tensorrt_7.0.0.11 with FP32 mode, but something wrong with int8 calibration:[TensorRT] INTERNAL ERROR: Assertion failed: d.nbDims >= 1To solve the problem, I try the tensorrt_7.1.3 with FP32, the same code/onnx file :   ./trtexec --onnx=./test.onnx, however:[TRT] Repeated layer name: stage2/split_1 (layers must have distinct names)I check the onnx, but all split_xx node with different names, not repeated, and tensorrt_7.0.0.11 with FP32 mode is ok, no error!the test.onnx is converted by tf2onnx, I  test the other onnx file (from pytorch), TensorRT-7.1.3.4 runs successfully.Finally, I replace the  TensorRT-7.1.3.4/lib/libnvonnxparser.so.7.1.xxx with TensorRT-7.0.0.11/lib/libnvonnxparser.so.7.0.xxx, everything works —FP32/16 and int8 calib.this is confusing!TensorRT Version: 7.1.3
GPU Type: T4
Nvidia Driver Version: 440.82
CUDA Version: 10.2
CUDNN Version: 8.0
Operating System + Version: centos7
Python Version (if applicable): 3.6.4
TensorFlow Version (if applicable): 1.14Hi @1269745339
Can you share your onnx model and the script so that i can assist you better.I replaced libnvonnxparser.so.7.1.3 with libnvonnxparser.so.7.1.0 which was built with the latest(2020/9/2) onnx-tensorrt code, then it worked.Hi @chienkan
It seems I have a very similar problem: layers must have distinct namesMay I asked how you changed the version of the onnx parser?Just build libnvonnxparser.so.7.1.3 with the open source code:
Releases · onnx/onnx-tensorrt (github.com)Powered by Discourse, best viewed with JavaScript enabled"
193,error-code-1-internal-error-error-weights-of-same-values-but-of-different-types-are-used-in-the-network,"I’m trying to convert a Pytorch model to onnx then convert it to a tensorRT engine using trtexec and got the following errorHere’s the full output:Device: Orin AGX
Dockerfile Content:Link to .onnx fileDownload the .onnx file & Run the command below in terminal
RetinaNet_(1, 3, 1080, 1440)_15.log (5.4 KB)Hi,
Request you to share the ONNX model and the script if not shared already so that we can assist you better.
Alongside you can try few things:This NVIDIA TensorRT 8.6.0 Early Access (EA) Quick Start Guide is a starting point for developers who want to try out TensorRT SDK; specifically, this document demonstrates how to quickly construct an application to run inference on a TensorRT...import sys
import onnx
filename = yourONNXmodel
model = onnx.load(filename)
onnx.checker.check_model(model).
2) Try running your model with trtexec command.master/samples/trtexecNVIDIA® TensorRT™, an SDK for high-performance deep learning inference, includes a deep learning inference optimizer and runtime that delivers low latency and high throughput for inference applicat...
In case you are still facing issue, request you to share the trtexec “”–verbose"""" log for further debugging
Thanks!The ONNX model is in the shared link. Can’t upload due to file size.Were you ever able to resolve this?Hi,Please try on the latest TensorRT version 8.6.1, and let us know if you still face the same issue.Thank you.Powered by Discourse, best viewed with JavaScript enabled"
194,onnx-to-tensorrt-conversion,"The issue I am facing is that when I try to convert one ONNX file, named “AP.onnx,” it is successfully converted to TensorRT format. However, when I attempt to convert another RGB.onnx file, the conversion process fails.To address the issue, I have attached both the RGB.onnx file, AP.onnx,and attached error message as error.txt at the end.
The error says that fp16 is not configured in the builderTensorRT Version: 8.4.1
GPU Type: NVIDIA GeForce RTX 3050
Nvidia Driver Version: 532.03
CUDA Version: 11.7
CUDNN Version: 8.5
Operating System + Version: Ubuntu 20.04 in wsl2 on windows 11
Python Version (if applicable): 3.8.10
Error.txt (726 Bytes)
test.py (2.2 KB)AP.onnx (1.3 MB)
RGB_9d9bb888-6342-466a-8705-cf3e7e99677e_2021-07-08_21-37-00.onnx (12.9 MB)Hi,We recommend you to use the latest TensorRT version 8.6.
Using the latest version, we could successfully build the engine.Thank you.Powered by Discourse, best viewed with JavaScript enabled"
195,fatal-error-nvinfer-h-no-such-file-or-directory,"I’m trying to run setup.py from torch2trt folder for trt_pose model.
I’m  on a Tesla V100 GPU azure cloud instance and I’m getting the error "" fatal error: NvInfer.h: No such file or directory"" when I try to run the setup.py file using command “python3 setup.py install --plugins”.
Can someone please help with what I am missing in the process.Please note: I currently don’t have a jetson device and and have not installed jetpack jdk.TensorRT Version:  8.5.3.1
GPU Type:   Tesla V100
Nvidia Driver Version:  520.61.05
CUDA Version:   11.8
CUDNN Version:  8.9.0
Operating System + Version: Linux Ubuntu 18.04
Python Version (if applicable):  3.6Refer following link:I follow the step ""Compiling the Project""  but get
~/jetson-inference/build$ ma…ke
[  2%] Building CXX object CMakeFiles/jetson-inference.dir/segNet.cpp.o
In file included from /home/jacob/jetson-inference/segNet.h:27:0,
                 from /home/jacob/jetson-inference/segNet.cpp:23:
/home/jacob/jetson-inference/tensorNet.h:27:21: fatal error: NvInfer.h: No such file or directory
 #include ""NvInfer.h""
                     ^
compilation terminated.
CMakeFiles/jetson-inference.dir/build.make:1409: recipe for target 'CMakeFiles/jetson-inference.dir/segNet.cpp.o' failed
make[2]: *** [CMakeFiles/jetson-inference.dir/segNet.cpp.o] Error 1
CMakeFiles/Makefile2:67: recipe for target 'CMakeFiles/jetson-inference.dir/all' failed
make[1]: *** [CMakeFiles/jetson-inference.dir/all] Error 2
Makefile:127: recipe for target 'all' failed
make: *** [all] Error 2

I have tried to download TensorRT 5 again but does not solve the problem.  I search the system and do not find NvInfer.h

JacobAnd verify if you define a proper tensorrt path.BTW, JetPack is for Jetson series products, not V100.Moving to TensorRT forum for better support, thanks.I’ve installed tensorrt using the tar file installation method and I’ve added the /lib path to LD_LIBRARY_PATH environment variable before running the setup fileI’ve followed the exact steps as mentioned in the below link for tar installation methodThis NVIDIA TensorRT 8.6.1 Installation Guide provides the installation requirements, a list of what is included in the TensorRT package, and step-by-step instructions for installing TensorRT.Hi.For those who are facing the problem, please try out the below link. That is what worked for me.Dear all,

I manage to install TensorRT using a tar file by referring to the […official NVIDIA site](https://docs.nvidia.com/deeplearning/tensorrt/install-guide/index.html#installing-tar) and try to install the torch2trt with plugins by referring to the [official torch2trt github](https://github.com/NVIDIA-AI-IOT/torch2trt).
I installed the torch2trt installation without plugins without any problems.
However, I fail to install the torch2trt with plugins. To be more specific, I'm having a problem with a Nvinfer.h.
I think the Nvinfer.h is not existed in my machine.

I installed below packages such as GPU driver, CUDA toolkit, cuDNN and TensorRT by referring to the [DL installation](https://github.com/vujadeyoon/DL-UbuntuMATE18.04LTS-Installation) and [TensorRT-Torch2TRT](https://github.com/vujadeyoon/TensorRT-Torch2TRT).
My machine environments are as follows:

- Operating System (OS): Ubuntu MATE 18.04.3 LTS (Bionic)
- Graphics Processing Unit (GPU): NVIDIA TITAN RTX, 4ea- 
- GPU driver: Nvidia-440.100
- CUDA toolkit: 10.1 (default), 10.2
- cuDNN: cuDNN v7.6.5
- PyTorch: 1.3.0
- TensorRT: 7.0.0.11
- torch2trt: 0.1.0

The debug information is as below when installing the torch2trt with plugins. In other words I run the command, sudo python setup.py install --plugins.

```bash
running install
running bdist_egg
running egg_info
writing torch2trt.egg-info/PKG-INFO
writing dependency_links to torch2trt.egg-info/dependency_links.txt
writing top-level names to torch2trt.egg-info/top_level.txt
reading manifest file 'torch2trt.egg-info/SOURCES.txt'
writing manifest file 'torch2trt.egg-info/SOURCES.txt'
installing library code to build/bdist.linux-x86_64/egg
running install_lib
running build_py
running build_ext
building 'plugins' extension
x86_64-linux-gnu-gcc -pthread -DNDEBUG -g -fwrapv -O2 -Wall -g -fstack-protector-strong -Wformat -Werror=format-security -Wdate-time -D_FORTIFY_SOURCE=2 -fPIC -I/usr/include/aarch64-linux-gnu -I/home/vujadeyoon/.local/lib/python3.6/site-packages/torch/include -I/home/vujadeyoon/.local/lib/python3.6/site-packages/torch/include/torch/csrc/api/include -I/home/vujadeyoon/.local/lib/python3.6/site-packages/torch/include/TH -I/home/vujadeyoon/.local/lib/python3.6/site-packages/torch/include/THC -I/usr/local/cuda/include -I/usr/include/python3.6m -c torch2trt/plugins/interpolate.cpp -o build/temp.linux-x86_64-3.6/torch2trt/plugins/interpolate.o -DUSE_DEPRECATED_INTLIST -DTORCH_API_INCLUDE_EXTENSION_H -DTORCH_EXTENSION_NAME=plugins -D_GLIBCXX_USE_CXX11_ABI=0 -std=c++11
torch2trt/plugins/interpolate.cpp:6:10: fatal error: NvInfer.h: No such file or directory
 #include <NvInfer.h>
          ^~~~~~~~~~~
compilation terminated.
error: command 'x86_64-linux-gnu-gcc' failed with exit status 1
```

My questions are below:
1. How to install the torch2trt with plugins.
2. Could not the Nvinfer.h be installed when installing TensorRT using a tar file?
3. Please give me any other advice. If you have succeeded in installing the torch2trt with plugins, please give your way for the installation.

Best regards,
VujadeyoonPowered by Discourse, best viewed with JavaScript enabled"
196,tritonserver-segmentation-fault-when-running-diarization-on-long-audio-1h,"Please provide the following information when requesting support.Hardware - NVIDIA GeForce RTX 2080
Operating System - WSL with Ubuntu 20.04
Riva Version - 2.10How to reproduce the issue ? (This is for errors. Please share the command and the detailed log here)I tried to run asr + diarization on long audio (1 hour long) and the triton server crashed.
I used a finetuned model based on Conformer and I also tried with Citrinet_1024.
The asr task works fine with both model on the long audio. The diarization succeed on short audio.Here are the logs + config file :
config.sh (12.7 KB)
logs.txt (117.6 KB)Hi @mel.adlThanks for your interest in RivaThanks for sharing the logs,The Segmentation fault seems related to GPU and its UsageCare must be taken to not exceed the memory available when selecting models to deploy. 16+ GB VRAM is recommended.Please refer
https://docs.nvidia.com/deeplearning/riva/user-guide/docs/support-matrix.html#data-centerThanksHi @rvinobha and thank you for your help.In the meanwhile, I’ve switched to a RTX 3080 and I still encounter the same trouble.Powered by Discourse, best viewed with JavaScript enabled"
197,cuda-version-conumdrum-pytorch-tensorflow-and-transformers,"Hello,Transformers relies on Pytorch, Tensorflow or Flax. I typically use the first.In any case, the latest versions of Pytorch and Tensorflow are, at the time of this writing, compatible with Cuda 11.8.Lucky me, for Cuda 11.8 is supposed to be the first version to support the RTX 4090 cards.Well, not fully, apparently:I believe the 4090 to be an Ada Lovelace, not a Hopper.Will the fact that the card is not correctly identified by Cuda have any effect in resource utilisation and/or performance?Is there anything we could do about that?Does anyone know if Torch works with a more recent Cuda? Or can the MapSMtoCores and MapSMtoArchName variables be somehow hard-coded? Or is this completely irrelevant?Best,EdFor Pytorch, the nightly version is compatible with Cuda 12.1, which fully supports the card and simplifies things considerably.Powered by Discourse, best viewed with JavaScript enabled"
198,riva-2-9-0-nmt-does-not-produce-translated-output,"Please provide the following information when requesting support.
Hardware - Nvidia A100
Hardware - AMD EPYC 7413
Operating System - Ubuntu 20.04.5
Riva Version - 2.9.0
TLT Version - N/a
How to reproduce the issue ? (This is for errors. Please share the command and the detailed log here)python function, that does not produce error or output:Attached are docker logs for riva. At the end of the file you can see that I tried twice with en → ru, and once with en → es. Both produced this error in docker logs and returned nothing to my python script.docker_riva_logs.txt (52.5 KB)Hi @armstrongsam25Thanks for your interest in RivaWe will try to try this from our end, can you kindly share your config.sh usedThanksconfig.sh (12.8 KB)@rvinobha sorry, didn’t reply to your comment or @ you. My bad.Hi @armstrongsam25Apologies for the delayFound the Cause if the problem
the text (even if a single sentence must be provided as a list)I also faced the same issue when I passes the text as string,  it returned empty response and attribute error in riva-speech logsPassing text as list will resolve the issue and provide transcriptresp = nmt_service.translate(text, model=model, source_language=""en-US"", target_language=target_lang)i.e first param text needs to be of type listPlease try and let me know if it worksThanks for your patiencePowered by Discourse, best viewed with JavaScript enabled"
199,megatron-gpt-p-tuning,"Hi,I am trying out p-tuning for megatron model. For my task, I finetuned a model on two classes.However, if a text is passed to the model which doesn’t relate to the two categories, the prediction is still generated, classifying the text to either of them. I was wondering if there is a way to know the confidence or some score to discard the result in that case?Any help on this please??I would recommend tuning the models on 3 classes instead: class A, class B and third one - “neither A nor B”. Since you already have examples for A and B, gathering examples for “neither A nor B” should be easyHi,I am trying out p-tuning for megatron model. For my task, I finetuned a model on two classes.However, if a text is passed to the model which doesn’t relate to the two categories, the prediction is still generated, classifying the text to either of them. I was wondering if there is a way to know the confidence or some score to discard the result in that case?Yes, it is possible to get a confidence score or probability for the predictions made by your fine-tuned Megatron model. Many machine learning models, including transformer models like Megatron, include a predict_proba() method which can be used to get the probability or confidence score for each of the classes the model is able to predict.For example, if you are using a fine-tuned Megatron model to classify texts into two categories, you could use the predict_proba() method to get a probability score for each of the two categories. If the probability score for one of the categories is very low, it may indicate that the model is not confident in its prediction for that text and you may want to discard the prediction.Keep in mind that the confidence scores or probabilities generated by the predict_proba() method should be used with caution and may not always be reliable. The accuracy and reliability of the probabilities generated by the method will depend on the quality and diversity of the training data used to fine-tune the model, as well as the complexity of the task and the limitations of the model itself.Hi,I am trying out p-tuning for megatron model. For my task, I finetuned a model on two classes.However, if a text is passed to the model which doesn’t relate to the two categories, the prediction is still generated, classifying the text to either of them. I was wondering if there is a way to know the confidence or some score to discard the result in that case?When using a fine-tuned model, it’s possible for the model to generate predictions even for inputs that are unrelated to the trained categories. To access the confidence or score associated with these predictions, you can utilize the probability values assigned to each class by the model. By examining these probabilities, you can set a threshold value to determine when to discard the results. For instance, if the highest probability for a prediction is below a certain threshold, you can consider it as an indication of low confidence and choose to discard the result.Powered by Discourse, best viewed with JavaScript enabled"
200,cudnnactivationforward-doesnt-work-with-cudnn-activation-identity,"As the title says.cudnnSetActivationDescriptor(desc, CUDNN_ACTIVATION_IDENTITY, CUDNN_NOT_PROPAGATE_NAN, 0.0f);This causes cudnnActivationForward to return Bad Param.  Oddly enough, it you used cudnnConvolutionBiasActivationForward() it worked fine.Please fix or explain why.  Thanks,
-ChrisEDIT:  Messing around, I’m noticing a lot of differences between cudnnActivationForward and cudnnConvolutionBiasActivationForward.  Why is cuDNN such a mess?Hi @ezbDoubleZero ,
Can you please share the logs with us so we can help better.
Also cudnnActivationForward and cudnnConvolutionBiasActivationForward(API Reference - NVIDIA Docs) are explained here for your reference.ThanksthanksPowered by Discourse, best viewed with JavaScript enabled"
201,a100-performance,"Hello hope your day is going well.We’ve been stress testing 3 new nodes that have 4x A100 GPUs and have some questions about power utilization and the effects on our A100 GPUs. We seem to be hitting 100% utilization (per nvidia-smi monitoring) regardless of the circumstances I describe below, but I’m hoping to verify we’re not jeopardizing card performance.
The power utilization for each card seems to drop when the card hits 84 degrees C – understood that this may be some sort thermal limit, but I just wanted to make sure I understood the ramifications?
We checked and monitored inlet air temps in the datacenter when this was happening and confirmed that source air was actually cooler when the cards were pegged at 84C.Any help is much appreciated!Powered by Discourse, best viewed with JavaScript enabled"
202,could-not-find-any-supported-formats-consistent-with-input-output-data-types,"Try to test “CustomSkipLayerNormPluginDynamic” for int8. I can build the engine for fp16/32, but will get [PluginV2DynamicExt]: could not find any supported formats consistent with input/output data types for int8docker image: tensorrt-ubuntu20.04-cuda12.0:latestGPU Type: A30Please attach or include links to any models, data, files, or scripts necessary to reproduce your issue. (Github repo, Google Drive, Dropbox, etc.)LogCode:Hi,
Please refer to below links related custom plugin implementation and sample:This Samples Support Guide provides an overview of all the supported NVIDIA TensorRT 8.6.0 Early Access (EA) samples included on GitHub and in the product package. The TensorRT samples specifically help in areas such as recommenders, machine...TensorRT is an SDK for high performance, deep learning inference. It includes a deep learning inference optimizer and a runtime that delivers low latency and high throughput for deep learning…While IPluginV2 and IPluginV2Ext interfaces are still supported for backward compatibility with TensorRT 5.1 and 6.0.x respectively, however, we recommend that you write new plugins or refactor existing ones to target the IPluginV2DynamicExt or IPluginV2IOExt interfaces instead.Thanks!Powered by Discourse, best viewed with JavaScript enabled"
203,riva-faces-difficulty-in-encoding-a-specific-word,"Hardware - GPU (T4)
Hardware - CPU (n1-standard-4)
Operating System - Linux
Riva Version - 2.8.1
TLT Version (if relevant)
How to reproduce the issue ? (This is for errors. Please share the command and the detailed log here)Currently we are using Riva ASR to infer our audios. I attempted to enhance specific words using the method outlined in this article:
https://docs.nvidia.com/deeplearning/riva/user-guide/docs/tutorials/asr-python-advanced-wordboosting.html#asr-inference-with-word-boostingI provided a word list to enhance the ASR transcription for specific words, which worked successfully for all words except one: ডিপিএসের. Unfortunately, an error was encountered during the process, displaying the following message:
E0531 07:38:56.434091 91 ctc-decoder.cc:328] Inference failed in ASR decoder: Unable to encode the word ডিপিএসেরIt is important to note that no errors occur when using the base word ডিপিএস, but the error arises when attempting to use ডিপিএসের. I tried using this word by normalizing with NFKC, NFKD, NFD. But none of these approaches resolved the issue.I would greatly appreciate receiving your insights and responses on this matter. Thank you.Powered by Discourse, best viewed with JavaScript enabled"
204,how-to-set-nvcvimage-members-when-the-pixelformat-is-yuv420p,"Hi,I want to  call NvCVImage_Transfer to transfer from YUV420P to NVCV_RGBA, I known that one YUV420P pix use 1 + 0.25 + 0.25 bytes , it likes the the following three members can only be set to integers：So how to set the values?Please Help, thanks.upPowered by Discourse, best viewed with JavaScript enabled"
205,riva-asr-issue-on-transcribing-demo-audio,"Please provide the following information when requesting support.Hardware - GPU Tesla P100
Hardware - CPU  Intel(R) Xeon(R) CPU @ 2.30GHz
Operating System Debian GNU/Linux 11
Riva Version v 2.10.0
How to reproduce the issue ? (This is for errors. Please share the command and the detailed log here)I follow the Riva Quick Start Guide to deploy the ASR models on the sever. When running “Offline recognition for English”, the issue, as shown in the attached snapshot, pops up.  All previous steps are executed without error.Specific configs in the Riva server config.sh file areCould you please troubleshoot the issue?
Screenshot 2023-04-19 at 4.13.43 PM1524×886 68.8 KB
Hi @xin.zhang3Thanks for your interest in RivaI will reproduce from my end and check this further with the team,request to kindly provideThanksHi there,Thanks for the quick response!I have attached 3 files below as requested. Can you please take a look?asr-basics.ipynb (192.2 KB)
config.sh (12.6 KB)
log1.txt (22.3 KB)Hi @rvinobhaAll required files have been uploaded. Could you please take a look?Thanks!Powered by Discourse, best viewed with JavaScript enabled"
206,ram-usage-doesnt-go-down-for-asr-could-it-be-because-of-tritons-requests-cache-if-so-why-dont-the-metrics-reflect-that,"Please provide the following information when requesting support.Hardware - GPU (T4)
Operating System - Container-Optimized OS with containerd (cos_containerd) (on a k8s cluster)
Riva Version 2.8.1I’m experimenting with a deployment on a K8s cluster, using Helm charts. I’m only using a subset of the models, and when they’re loaded and Triton starts, the RAM (not vRAM) usage sits at around 5.8GBAfter I send requests for offline ASR, the RAM usage spikes, then goes down, then it hits a plateau of about 8.8 GB, and doesn’t go down back to 5.8GBThe following image shows the Triton metric for RAM consumption nv_cpu_memory_used_bytes and shows the RAM spike, then plateau at 8.8GB
Hi @hkhairyThanks for your interest in RivaI will check regarding this query with the Riva Team and get backThanksI have noticed the same thing.  riva_server never gets killed.  I am calling for an end() and it never kills the process.Powered by Discourse, best viewed with JavaScript enabled"
207,does-tensorrt-support-geforce-and-quadro-boards,"Hi all,
I want to use TensorRT tool to optimize the inference of my algorithm.
I have two PC and each of them has an Nvidia board (PCI):
1-  NVIDIA GP 104 (GeForce GTX 1070)
2-  NVIDIA GM 206 GL (Quadro M2000)
Does TensorRT support these boards?
Thank you.Hi ,
We recommend you to check the supported features from the below link.These support matrices provide a look into the supported platforms, features, and hardware capabilities of the NVIDIA TensorRT 8.6.0 Early Access (EA) APIs, parsers, and layers.
You can refer below link for all the supported operators list.
For unsupported operators, you need to create a custom plugin to support the operationThanks!Powered by Discourse, best viewed with JavaScript enabled"
208,problems-running-unet-industrial-inference,"I am running the notebook example of training and test UNet Industrial on DAGM dataset provided at DeepLearningExamples/blob/master/TensorFlow/Segmentation/UNet_Industrial.I was able to train the model correctly, but when i am going to test it gives the error:How do I solve that?ValueError: Horovod has not been initialized; use hvd.init().Hi,Please reach out to Issues · NVIDIA/DeepLearningExamples · GitHub to get better help.Thank you.Powered by Discourse, best viewed with JavaScript enabled"
209,pycuda-driver-memoryerror-cumemhostalloc-failed-out-of-memory-yolov7,"I converted ONNX to .engine by commandAnd evaluate .engine model, a part of code as followBut I got errorMy GPU has 11 GB ram, i think it’s sufficient to run evaluate .engine model. Please help me.Hi,Please check the GPU memory available and make sure no other task is consuming the available resources.pycuda._driver.MemoryError: cuMemHostAlloc failed: out of memoryThis error can also happen if you pass a negative size in cuda.mem_alloc, could you please check.Thank you.Powered by Discourse, best viewed with JavaScript enabled"
210,cuopt-service-closed,"I got an Error message this Morning like below:
NVIDIA cuOpt failure - Limited version of NVIDIA cuOpt has expired\nObtained 35 stack framesIs there any way I can purchase this? or any other early access version I can upgrade?Since I couldn’t find the cuopt docker image in the nvcr.io today, I would like to know why this happens and how I can use this beautiful fuctions again.Hi,We have addressed this concern via email and have directed you to the sales lead. Thank you for your continued interest in cuOpt.Regards,
cuOpt TeamHi,
I have received a similar message:
NVIDIA cuOpt failure - Limited version of NVIDIA cuOpt has expiredCould you also direct me to the sales lead.
Thanks
Davidhello i have same message.
i m testing application with cuopt,
could you guide about sales lead as mail?ThxPlease reach out to cuopt@nvidia.com with details and we will connect you with a sales representative for licensing options.Thank You,
cuopt TeamHi David,Please reach out to cuopt@nvidia.com with details and we will connect you with a sales representative for licensing options.Thank You,
cuopt Teamthanks to your kind and immediate explanation, I could manage to contact cuopt team through email and they told me that it will take 7~10 days for the license assessment. However, still, there’s no action on it.only email forwarding to Korean sales team and no feedback.Is this what Nvidia intended to leveraging Opensource strategy?Powered by Discourse, best viewed with JavaScript enabled"
211,cudnn-installation-for-ubuntu-20-04-mint-21-1-for-a-1070-cuda-8-6-compat-question,"From the matrix is shows the 1070 can only deal with CUDA 8.6.  The libraries I found are for ubuntu 16.04.  I shouldn’t be able to use those libraries on 20.04 should it?  Can I use the latest CUDA on an older card and it work at all?  If not, is there a way to get the source and compile it myself?  Or can I only use older hardware with Windows since the CUDA 8.6 supports WIndows 10.
Sorry for all the questions at once, just want to get this in the first pass.Hi @andrew.falgout ,
Based on our support matrix: Support Matrix :: NVIDIA cuDNN Documentation, our latest release still supports the 1070 (Pascal Architecture).
So you can grab the latest release and choose the installer that matches your OS.
ThanksPowered by Discourse, best viewed with JavaScript enabled"
212,cuda-cudnn-and-visual-studio-2022-enable-gpu-processing,"I’m trying to build my first machine learning program in visual studio and i’m trying to get it to process my model using my gpu instead of cpu.  I have visual studio 2019 and 2022, along with cuda 11.8 and cudnn 8.6.  I’ve installed python 3.9 3.10 and 3.11, followed the instructions i found online here but still doesnt recognize my gpu which i have an rtx 3080?  Any assistance pointing me in the right direction to get it working correctly would be greatly appreciated.https://www.yodiw.com/install-tensorflow-cudavisual-studio-2022-in-windows-11-for-gpu-modelling/Hi @mike11d11 ,
Please check the cudnn-cuda version compatibility from here.Also check if your drivers are updated?ThanksYes this is what i looked at and several other sources online.  This is what i’ve got so far, i just can’t figure out what i’m doing wrong…Windows 10 pro fresh install
Visual studio 2019 and 2022 community edition
Cuda 11.8
cudnnn 8.6.0.163
copy dll’s and alos create directory for the cudnn
made environment system variables
anaconda 3.9.12What else could i be missing?I run this in anaconda but still get False for available GPU’s when i have 2 rtx 3060ti’s isntalled.python
import tensorflow as tf
tf.version
len(tf.config.list_physical_devices(‘GPU’))>0Hi @mike11d11 ,
Can you please share the error logs with us?ok this is what i ended up having to do in order to get it running with gpu on my machine.python 3.8.5 downgrade
pip install tensorflow-gpu==2.7.1
pip install protobuf 3.20.1
pip install grpcio==1.48.2
pip install pandas --user
pip install scikit-learn
pip install pyodbc
pip install sqlalchemy
pip install numpy==1.21I’m now seeing an issue that it will only process using 1 of my gpu’s, if I include both of my rtx 3060ti’s then i get this error below?No OpKernel was registered to support Op ‘NcclAllReduce’ used by {{node Adam/NcclAllReduce}} with these attrs: [reduction=“sum”, shared_name=“c1”, T=DT_FLOAT, num_devices=2]
Registered devices: [CPU, GPU]Powered by Discourse, best viewed with JavaScript enabled"
213,problem-with-structured-sparsity-and-explicit-quantization-ptq-on-tiny-yolov7,"Hi, I am trying to use TensorRT to execute a Tiny-Yolov7 model with structured sparsity (2:4) and explicit quantization (PTQ). I have successfully trained and deployed on TensorRT a sparse network following this page :○ TensorRT is an SDK for high-performance deep learning inference, and TensorRT 8.0 introduces support for sparsity that uses sparse tensor cores on NVIDIA Ampere GPUs. It can accelerate networks by…Then, I followed this tutorial to perform PTQ step on my sparse model : yolo_deepstream/yolov7_qat at main · NVIDIA-AI-IOT/yolo_deepstream · GitHubIf I use “–sparsity=enable”, you will see that no sparse implementation were picked (see ‘log_sparse_ptq.txt’). With “–sparsity=force”, I see an error happening but the engine is generated and evaluated (see ‘log_force.txt’). Why? The error is :[04/19/2023-11:08:04] [E] Error[3]:[convolutionLayer.h::setKernelWeights::30] Error Code 3: API Usage Error (Parameter check failed at: /_src/build/x86_64-gnu/release/optimizer/api/layers/convolutionLayer.h::setKernelWeights::30, condition: kernelWeights.values != nullptrIf I inspect with netron the ONNX, the weights in QuantizeLinear layers have the same zeros values of my sparsified only model. I don’t know what happened. If you could help me, it will be highly appreciated ^^TensorRT Version: 8.5.3
GPU Type: RTX A5000-24GB
Nvidia Driver Version: 520.61.05
CUDA Version: 11.8
CUDNN Version: 8.6.0
Operating System + Version: Ubuntu 22.04 LTS
Python Version (if applicable): 3.10.6
TensorFlow Version (if applicable): not applicable
PyTorch Version (if applicable): 2.0.0
Baremetal or Container (if container which image + tag): not applicableptq-sparse-640.onnx (24.0 MB)
log_sparse_ptq.txt (5.2 MB)
log_force.txt (5.2 MB)trtexec --onnx=ptq-sparse-640.onnx --saveEngine=ptq-tiny-yolov7-sparse-640.trt --int8 --fp16 --sparsity=enable --useCudaGraph --verbose
or
trtexec --onnx=ptq-sparse-640.onnx --saveEngine=ptq-tiny-yolov7-sparse-640.trt --int8 --fp16 --sparsity=force --useCudaGraph --verboseHi,This looks like a Deepstream related issue. We will move this post to the Deepstream forum.Thanks!Hi,
Thanks for the quick answer.It’s clearly a TensorRT problem. There is several parts in the github and I’m focus on the tensorRT and PTQ/QAT technic.Regards,Hi @TakeThat42 ,
Apologies for the delayed response, i am checking on this and will update you soon.ThanksHi @AakankshaS,Thanks for your concern. I will be waiting your answer.ThanksHi @AakankshaS,Do you have any update on my problem? I’m still waiting your answer.ThanksPowered by Discourse, best viewed with JavaScript enabled"
214,strange-cnn-inference-latency-behavior-with-cuda-and-tensorrt,"When using CNNs on my GPU, I’m getting a strange latency increase, if the last inference was >= 15s ago. The jitter also explodes. Please have a look at the graph included.
I’m using the TensorRT backend of OnnxRuntime, I double checked with their CUDA-Module - which provides the same latency anomaly, but with a bigger baseline latency.Things which I took care of:I’d like to know, if there is any HW-Schedulder on the GPU, which might be responsible for these dropouts? Do you have any ideas of what could possibly create such a strange behaviour?
trt_lat1202×899 60.9 KB

The term “Input Latency” refers to the period of two inferences.
The term “Output Latency” refers to the overall latency of the inference.TensorRT Version: 8.4.2.4
GPU Type: Quadro RTX 4000
Nvidia Driver Version: 515.43.04
CUDA Version: 11.7
CUDNN Version: 8.4.1
Operating System + Version: Debian 11, Kernel 5.10 PREEMPT_RTIt’s not easy to reproduce. If necessary, I provide a C++ snippet for thatHi,Could you please try on the latest TensorRT version 8.5.1 and let us know if you still face this issue.
If possible could you please share with us the minimum issue repro model/script for better debugging.Thank you.Hi,
thank you for your reply.I doubt the problem arises from the used TensorRT version, because of my counter measurements with the Onnxruntime CUDA module - which shows the same behaviour but with more jitter.
Bildschirmfoto vom 2022-12-21 11-08-031199×897 78 KB
In my opinion we are searching for some scheduling things or similar in hardware or driver.Upgrading to the most recent version of TensorRT will cost much time. I’ll give it a shot, but it will take some time until I have results.Creating a debugging repro will take some time, too - but I will come back when it’s done.Hi,
I created a minimal example for testing. It’s based on the TensorRT samples, I just altered the code of the MNIST-ONNX sample. Using this example, I could reproduce that behaviour. With my RTX3080, the latency increase is worse compared to the Quadro RTX4000.
I tested on a whole different machine, with energy savings enabled. The measurements were carried out with the more compute intensive ResNet-101. In the graph are only 75 samples, but the behaviour is clearly visible.
resnet101788×443 11.1 KB
TensorRT Version: 8.5.2.2
GPU Type: GeForce RTX3080
Nvidia Driver Version: 520.61.05
CUDA Version: 11.8
CUDNN Version: 8.7.0
Operating System + Version: Debian 11, Kernel 6.0 SMP PREEMPT_DYNAMICtrt_minimal_v2.7z (45.3 KB)TLDR:
Different system, different GPU and most recent driver and sdk versions. Problem still existing.I carried out some measurements with a windows 10 machine:
res18_win1000×664 15.4 KB
It has the same behaviour with ResNet-18.TensorRT Version: 8.5.2.2
GPU Type: Quadro RTX 4000
Operating System + Version: Windows 10Hi,Sorry for the delay in response. We are looking into this internally. Will get back to you.Thank you.Hi,thank you for investigating this! I’m really looking forward on your findings.I would have have liked to try a recent Tesla GPU, but unfortunately we don’t have one due to their price tags.
It would be interesting for us, if they might perform better in these scenarios, because maybe our final product would allow us to afford one.Hi,It’s likely not a TRT bug but a CUDA driver issue.
Could you share details on how you locked the GPU clocks? Did you use sudo nvidia-smi -lgc <sm_clk> to lock it?Thank you.Hi,really interesting!I provided the script “launchTest.sh” - I tweaked until the GPU never really lowered it’s clocks.
For the RTX3080 it’s for Linux:id=0
nvidia-smi -i $id -pm ENABLED#fan 100%
nvidia-settings -a ‘[gpu:’$id’]/GPUFanControlState=1’
nvidia-settings -a ‘[fan:0]/GPUTargetFanSpeed=100’
nvidia-settings -a ‘[fan:1]/GPUTargetFanSpeed=100’#powermizer (Maximum Performance)
nvidia-settings -a ‘[gpu:’$id’]/GPUPowerMizerMode=1’#clocks (Please find the maximum clocks for your card via <nvidia-smi -q -d SUPPORTED_CLOCKS>)
nvidia-smi -ac 9501,2100 -i $id
nvidia-smi  --lock-gpu-clocks=2100 -i $id
nvidia-smi  --lock-memory-clocks=9501 -i $id
nvidia-smi -q -d CLOCK -i $idHi @BitShifter,Could you please share complete output logs (stdout outputs) when you execute the above script.
We want to see if the clock is really locked or if it is prohibited because it is a GeForce GPU.Thank you.Hi,I did it for both GPUs for completeness.
logrtx4000.log (7.9 KB)
logrtx3080.log (7.9 KB)Hi @spolisetty,it’s been a while since your last post. Could you find something?Thanks :)Hi @spolisetty,two months went by, could you find something? Do we have a possible fix or workaround?Powered by Discourse, best viewed with JavaScript enabled"
215,nvidia-maxine-on-geforce-gtx-1070ti,"Is there any chance to launch NVIDIA Maxine on Geforce GTX 1070ti?1070ti is not supported due to the lack of tensor cores. Maxine will run on any graphics card with an RTX moniker along with datacenter GPUs with tensor cores. For graphics cards that includes GeForce RTX, Quadro RTX, and RTX A series or the GV100. Enterprise cards would include Volta, Turing, Ampere and newer architectures. That being said… there could be some cases where a tensor core enabled GPU will not run Maxine but it is highly unlikely.Hi,
Does NVIDIA Maxine RTX 3050 supports NVIDIA Maxine?Powered by Discourse, best viewed with JavaScript enabled"
216,gaze-redirect-error-cannot-create-a-cuda-stream,"Hi,
wanted to use Gaze Redirect for pre-recorded videos, but I get the error:

Screenshot 2023-01-25 1152081729×850 56.6 KB
I use a RTX A4500 and the latest CUDA version was just installed (12.0)Why do I still get the error that a cuda stream canot be created, and that the dynamic library cannot be loaded?I’m not experienced at all!I have the same problem with RTX 3050. Latest cuda installed, and also nvidia broadcast Version 0.7.2 (beta). I am trying to use v0.8.2.0 maxine ar SDK pack.“Cannot create a cuda stream: Error loading the dynamic library
ERROR: Initializing Gaze Engine failed.”
GazeRedirect App appear with grey background and suddenly disappear.This appears to be an error as the system cannot locate the dlls in the “bin” directory. Does the SDK sample run when using the run_local.bat ?Thank you for your help!You were right the “bin” directory was missing before. This video also helped me a lot How To Install Eye Contact AI | NVIDIA MAXINE | Gaze Redirect - YouTubethanks for linking my video happy to help :)I have this error that says my cuda version is insufficient. I installed the cuda toolkit and I don’t know what seems to be the issue here.

image960×405 13.5 KB
having the same issue, I do have the bin folder in theredownloaded from this link:
https://drive.google.com/drive/folders/1EH-oSgPOSV_U3vaGVrB6pvJ42_gM48zw
(there will be 2 zip file, extract both of them, else it will give error for missing files)changed the run.bat as:SETLOCAL
SET PATH=%PATH%;…..\samples\external\opencv\bin;…..\bin;
SET NVAR_MODEL_DIR=…..\bin\models
GazeRedirect.exe --offline_mode --split_screen_view=false  --in=example.mp4placed the sample video as example.mp4 in the GazeRedirect folderworks fine with  mePowered by Discourse, best viewed with JavaScript enabled"
217,trtexec-convert-bloomz-7b1-failed-due-to-no-implementation-for-reshape-7,"I’m trying to convert bigscience/bloomz-7b1 llm from onnx format to trt format on Jetson AGX Orin 64G, and it failed with following log:several problems showed in it:
1.Device memory is insufficient to use tactic.
2.(Could not find any implementation for node {ForeignNode[/transformer/h.10/self_attention/Cast…/transformer/h.10/self_attention/Reshape_7]}.)For OOM warning, my device is 64GB ram version and I set 64GB swap, during the convert, it cost 59GB ram and 41GB swap, still 20GB swap left, I have no idea about this problem. You can see memory usage in the memory_usage.log attachment.
For no implementation of Reshape_7 node, I found several issues on github and it said limit memory usage with --workspace parameter, but I also found that --workspace is useless after TensorRT8.4 in the release note.TensorRT Version:  8.5.2.2
GPU Type:  Jetson AGX Orin(64GB ram)
Nvidia Driver Version:
CUDA Version:
CUDNN Version:
Operating System + Version:  Ubuntu20.04
Python Version (if applicable): 3.8
TensorFlow Version (if applicable):  -
PyTorch Version (if applicable):  2.0
Baremetal or Container (if container which image + tag):  -memory_usage.log (645.1 KB)
convert.log (5.4 KB)You can download the bloomz-7b1 model from hugging face, and export it to onnx and convert the onnx with trtexec.
Export pt to onnx:convert onnx to trtexec:Hi,Could you please share the issue repro ONNX model here.
Please upload here or share via Google Drive or other platforms.Thank you.Hi,
Please refer to the below link for Sample guide.This Samples Support Guide provides an overview of all the supported NVIDIA TensorRT 8.6.1 samples included on GitHub and in the product package. The TensorRT samples specifically help in areas such as recommenders, machine comprehension, character...
Refer to the installation steps from the link if in case you are missing on anythingThis NVIDIA TensorRT 8.6.1 Installation Guide provides the installation requirements, a list of what is included in the TensorRT package, and step-by-step instructions for installing TensorRT.
However suggested approach is to use TRT NGC containers to avoid any system dependency related issues.NVIDIA TensorRT is a C++ library that facilitates high-performance inference on NVIDIA graphics processing units (GPUs). TensorRT takes a trained network and produces a highly optimized runtime engine that performs inference for that network.In order to run python sample, make sure TRT python packages are installed while using NGC container.
/opt/tensorrt/python/python_setup.shIn case, if you are trying to run custom model, please share your model and script with us, so that we can assist you better.
Thanks!Hi,The model file it really large so I updated failed several times. Can you follow the reproduce steps, downloads the model from hugging face and export it to onnx use my python script? It shall faster than get model from me.Thanks.Hi,We could reproduce a similar error.
Please allow us some time to work on this issue.Thank you.Any progress?bloom-7b1 and bloomz-7b1 will be supported in TensorRT’s future major releases.Powered by Discourse, best viewed with JavaScript enabled"
218,asr-conformer-ctc-audio-file-length-and-sampling-rate,"Is there a duration limit for the audio file ? What is the maximum length that a model can get?
 → Conformer-CTC uses self-attention which needs significant memory for large sequences. We trained the model with sequences up to 20s and they work for larger sequences but memory may not allow to go very large. For such large sequences two options are available:
1-Segment the sequence into smaller parts, perform the inference, merge the results.
2-Use Citrinet as its memory consumption in linear to the input length and can handle large audios in one shot.Regarding the sample rate - the conformer requires 16k. For data that is 8k, should it be resampled 16k in order to meet the requirements? In this notebook, it is mentioned that loading the weights of a 16 kHz model as initialization helps the model to converge faster with better accuracy. Does the 16k model perform better on 8k data (that was resampled to 16k) than the 8k model?
 → The best accuracy possible to get is with a model that was initialized with weights from one of our pretrained models, this is true for both upsampled 16 kHz data and 8 kHz data. Upsampling the data and keeping the model at 16 kHz also helps the fine-tuning converge faster.
For CTC models, in order to transcribe very long audio sequences, you can follow the streaming tutorial here.
In order to use on a large dataset, you can use: link
For Transducer Models, we currently do not support streaming inference in Nemo, but we are actively working on it. Also note that transducer Models are not currently supported in RIVA.Thank you for sharing the links.I was working on training with the conformer ctc xlarge model.  It is a huge model and I could only do a batch of two after I did extensive data segmentation to cut down phrase segments to around less than 8 seconds.  Even then a batch size of only 2 is all I could do with my 3090s.  Luckily with lighting I had 2 3090s so it could go kinda fast.  Even then while training the predictions were pretty terrible.Also it seems like conformer hallucinates a lot.  Citrinet is much more reliable at least on noisy audio.Is this a result of my low batch size?  Does it need a larger batch size to predict well during the training process?Powered by Discourse, best viewed with JavaScript enabled"
219,does-tensorrt-suport-dropout,"Hi there !I’m relatively new to TensorRT so maybe the answer will strike you as evident. For research purposes I am building Neural Networks which perform dropout on inference using Pytorch.Now I would like to embed these networks in a Jetson and, to improve performance, I would like to convert them to TensorRT engines. The issue is, I am not entirely sure this is supported, so I am asking if anyone has tried converting a Neural Network with dropout enabled into a TensorRT engine, and what the mandatory steps are.Thank you !Hi,TensorRT supports dropout. Please refer to the support matrix doc.Thank you.Powered by Discourse, best viewed with JavaScript enabled"
220,riva-quick-start-for-embedded-platforms-failed-to-run-on-jetson,"Hardware - Jetson NX 8GB
Riva Version - 2.9.0I am following the instructions from https://catalog.ngc.nvidia.com/orgs/nvidia/teams/riva/resources/riva_quickstart_arm64As per the docs jetson device is running at 15W 6 core mode, and the default runtime for docker is set to nvidia.When I execute bash riva_init.sh command it showsHere is my config file: config.sh (12.5 KB)Before this, I also tried Riva Version 2.0.0 Read this threadPlease help me resolve the issue.ThanksHi @NitinRaiThanks for your interest in RivaApologies you are facing the IssueI will check with the internal team on this error and get backThanksHI @NitinRaiI have some updates from Riva Teamyour config.sh seems perfect, only simple container start is still failing for the user.as next steps, please try the below and share the feedback with uslet us know the feedback
ThanksPowered by Discourse, best viewed with JavaScript enabled"
221,unified-memory-support-in-tensorrt,"I have a basic question about the interaction between TensorRT and unified memory. I would like to oversubscribe GPU memory while running inference. Could someone please explain or point me to docs (that I’m not finding) for the following?Hi @anand.raja
Your query has been noted.
Please allow me some time to check on this.
Thank you!Hi @anand.raja,How do I “tell” TensorRT that I would like to use unified memory - do I just pass in buffers that come from cudaMallocManaged or is there some other setting?You can use unified memory as input/output buffer .
However, i am afraid, we do not have any documents published around that.
Thanks!@AakankshaS, thanks for checking up and responding.If I’m not mistaken, TensorRT makes additional allocations while the engine is running, beyond the buffers you provide. Is there a way to use unified memory for these buffers?Thanks!Hi @anand.raja,Is there a way to use unified memory for these buffers?Sorry,but end user can’t manage this memory as this is managed by TRT itself.
Thanks!@AakankshaS I believe your response is incorrect. The setDeviceMemory API provides a way to specify an application-managed buffer. On my tests, this allows use of unified memory.I am trying to use unified memory, can you share your code？Powered by Discourse, best viewed with JavaScript enabled"
222,multi-model-parallel-inferencing,"how to parallel inferencing multi models ?  demo codeHi,The below links might be useful for you.This Best Practices Guide covers various performance considerations related to deploying networks using TensorRT 8.0.3. These sections assume that you have a model that is working at an appropriate level of accuracy and that you are able to...The programming guide to the CUDA model and interface.https://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__STREAM.htmlFor multi-threading/streaming, will suggest you to use Deepstream or TRITONFor more details, we recommend you raise the query in Deepstream forum.orraise the query in Triton Inference Server Github instance issues section.Triton Inference Server has 30 repositories available. Follow their code on GitHub.Thanks!Powered by Discourse, best viewed with JavaScript enabled"
223,tensorflow-gpu-installation-issue,"Dear all,
I recently bought a brand new computer with a RTX 3060 graphic card in order to do deep learning.
I try to install the GPU but I have some issues.
I installed tensorflow followig the procedure:
But when I write the following command:I have the following message:Anyone can help me ?Hi there,I was getting similar messages to your first couple. Specifically around libnvinfer.so.7.The problem seems to be incompatible versions of TensorRT and TensorFlow 2.11.
To fix this problem I had to install tf-nightly and add “$CONDA_PREFIX/lib/python3.10/site-packages/tensorrt/” the LD_LIBRARY_PATH env variable.I found some useful info in these issue on github:
https://github.com/tensorflow/tensorflow/issues/57671,https://github.com/tensorflow/tensorflow/issues/57671Apparently it will be fixed in the Tensorflow2.12 stable release whenever that is. There is also the option of downgrading to various python versions and package versions however I have a slightly abnormal setup (I’m running arch on wsl2) so getting previous versions of the nvidia libraries was a nightmare.Hope this helps!Thank you for your answer.
I don’t wan’t to downgrade python as I know it will bring new issues with other softwares.
I don’t understand the sentence: add “$CONDA_PREFIX/lib/python3.10/site-packages/tensorrt/” the LD_LIBRARY_PATH env variable.
Could you explain better ?
Thanks
ThibautHi,No problem. If you look at your error message it is looking for libnvinfer.so.7 in the directory in the environment variable $LD_LIBRARY_PATH.When you installed tensor flow you probably copied the script that creates this variable everytime you activate your virtual environment. You will need to go to where that script is located (refer to the steps you took to create it) and add the
location of the libnvinfer.so.8 library which for me was in:$CONDA_PREFIX/lib/python3.10/site-packages/tensorrt/$CONDA_PREFIX is an environment variable that points to your env root folder.If you uninstall tensorflow 2.11 and install the pip package “tf-nightly” you shouldn’t have to downgrade. It is the development branch of tensorflow and this dependency mismatch has been fixed there  but not in the current stable release.Try to read through the links I posted. There is a lot of info in there and people who have had and fixed the same problem in various ways.Sorry I am lost !
The libnvinfer.so.8 is located in the following folder:
/home/jacquin/anaconda3/envs/tf-py38/lib/python3.8/site-packages
I did not find the libnvinfer.so.7 file
$LD_LIBRARY_PATH is /home/jacquin/miniconda3/lib/tensorRT on system. python 3.10+ RT 8+
cd /usr/lib/x86_64-linux-gnu
ln -s libnvinfer.so.8 libnvinfer.so.7
ln -s libnvinfer_plugin.so.8 libnvinfer_plugin.so.7
ln -s libnvonnxparser.so.8 libnvonnxparser.so.7
ln -s libnvparsers.so.8 libnvparsers.so.7For virtual environment  only create symbolic links.Hi,No issue. If you look at your error message, libnvinfer.so is being sought after. LD LIBRARY PATH’s environment variable lists 7 in the directory.You duplicated the script that produces this variable each time your virtual environment is activated when you installed TensorFlow, most likely. You must visit the location of the script (remember the procedures you used to generate it) and add theMaybe there is software fault in your graphic card and you can ask a professional testing institution to test the graphic card or install the GPU instead of you. If any company wants to test and validate the performance of some chip, module and software, please drop me a direct message or write to me via e-mail: runlin.guo@thundersoft.comPowered by Discourse, best viewed with JavaScript enabled"
224,onnx-runtime-with-cuda-execution-provider-mismatch-with-cpu-execution-provider-results,"Dear CuDNN team,I find a mismatch of ONNX Runtime with CUDA Execution Provider and with CPU Execution Provider on op AveragePool with asymmetric padding (pads=[0, 0, 2, 2]).libcudnn8=8.5.0.96-1+cuda11.7Can you help to take a look? Thanks!
image2215×1841 289 KB
Hi @user9611 ,
I doubt if this issue is related to cudnn.
I may suggest you to reach out to onnxruntime issues to get better assistance.thanksPowered by Discourse, best viewed with JavaScript enabled"
225,no-output-streaming-mode-when-building-model-with-custom-vocabulary,"Please provide the following information when requesting support.Hardware - GPU (A100/A30/T4/V100) : Geforce 3080 RTX
Hardware - CPU : Intel i9
Operating System : WSL 2
Riva Version : 2.11.0
TLT Version (if relevant)
How to reproduce the issue ? (This is for errors. Please share the command and the detailed log here)I tried to add a custom vocabulary to transcribe audio in streaming mode but It seems that the model is not built correctly, I don’t have any output after transcription.The model used is a citrinet_1024 in English. I also tried with the citrinet_512 and conformer.
The offline model seems to run perfectly.Here is the command I used to build the model :Language model used : Riva ASR English LM, version = deployable_v4.1
The vocabulary file “vocab_lm.txt” contains a list of words, one word = one line.
I also tried without the “nn.use_trt_fp32” flag.config.sh (12.7 KB)Hi @mel.adlThanks for your interest in RivaApologies for the delayCan you kindly share the custom vocabulary used with us for reproductionThanksHi @rvinobha
Here is the custom vocabulary
vocab_lm.txt (213 Bytes)Powered by Discourse, best viewed with JavaScript enabled"
226,what-could-cause-negative-sized-output-bindings,"I am trying to run inference using a trt engine, but when allocating buffers it is showing my output bindings as negative sized, and preventing me from running the engine. What could cause this?Here is the snippet to get sizes:Output:TensorRT Version : 8.5
GPU Type : Jetson Xavier
Nvidia Driver Version :
CUDA Version : 11.4
CUDNN Version :
Operating System + Version : Ubuntu 20
Python Version (if applicable) : 3.8.10
TensorFlow Version (if applicable) :
PyTorch Version (if applicable) : 1.13
Baremetal or Container (if container which image + tag) :Hi,Could you please confirm that you using the dynamic shape?Thank you.I am not setting a dynamic shape explicitly, the input is a fixed size and batch size of 1. I can understand how the output size could be dynamic since there could be any number of detections in the image.Hi,This may happen due to dynamic dims, which are represented with -1.
To get the size of a tensor with dynamic dims, you will need to query the min/opt/max sizes by checking the optimization profiles.Thank you.Powered by Discourse, best viewed with JavaScript enabled"
227,why-weights-of-enigne-model-is-int8-while-bias-is-float-32,"I converted .onnx to engine file, but when I check graph json file, I saw that weights of layer have int8 format, bit bias has float format as follow:I think in one generated layers, weights and bias have to have the same format. How can I force to generate bias with int8 format to increase speed of engine model?
ThanksI converted .onnx to engine file, but when I check graph json file,Hi,Could you please share with us complete verbose logs while building the engine and graph json file for better help.Thank you.Hi,Could you please share with us complete verbose logs while building the engine and graph json file for better help.Thank you.@spolisetty I attached verbose logs when building engine and graph.json. I use this repo to build engine, it used Tensorrt Python API. It seems that, all biases is FP32 in engine model. Thanks
graph.json (239.4 KB)log.md (7.2 KB)Hi,
Request you to share the ONNX model and the script if not shared already so that we can assist you better.
Alongside you can try few things:This NVIDIA TensorRT 8.6.1 Quick Start Guide is a starting point for developers who want to try out TensorRT SDK; specifically, this document demonstrates how to quickly construct an application to run inference on a TensorRT engine.import sys
import onnx
filename = yourONNXmodel
model = onnx.load(filename)
onnx.checker.check_model(model).
2) Try running your model with trtexec command.master/samples/trtexecNVIDIA® TensorRT™, an SDK for high-performance deep learning inference, includes a deep learning inference optimizer and runtime that delivers low latency and high throughput for inference applicat...
In case you are still facing issue, request you to share the trtexec “”–verbose"""" log for further debugging
Thanks!trtexec “”–verbose""""I use this repo (Tensorrt Python API GitHub - Linaom1214/TensorRT-For-YOLO-Series: tensorrt for yolo series (YOLOv8, YOLOv7, YOLOv6....), nms plugin support)  to convert .onnx to .engine. If I enable -v for verbose in the repo, log is very long in terminal and I can not save all. Currently, I am not using trtexec to convert .onnx model to fp32 and .int8 engine.import sys
import onnx
filename = yourONNXmodel
model = onnx.load(filename)
onnx.checker.check_model(model).@AakankshaS I checked with this code, nothing is in output. It means that my onnx model is valid.In case you are still facing issue, request you to share the trtexec “”–verbose"""" log for further debugging
Thanks!@spolisetty @AakankshaS
Thanks. I used this repo GitHub - Linaom1214/TensorRT-For-YOLO-Series: tensorrt for yolo series (YOLOv8, YOLOv7, YOLOv6....), nms plugin support (tensorrt python API to convert .onnx to .int8 model) and I can log all verbose output. I attach .onnx file, graph.json file (weights is INT8 and bias is FLOAT in the same). How to fix in the Tensorrt Python API?
graph_new.json (238.5 KB)log.txt (12.0 MB)Because file .onnx is large (>100 MB), I attach drive link here to .onnx yolov7_cach3_KOdynamic.onnx - Google Drive you can download it.@spolisetty @AakankshaS
Sorry, but is there any update? I am waiting for your guide.(weights is INT8 and bias is FLOAT in the same)Hi,This is an expected behavior,. INT8 conv/gemm kernels use FP32 bias. Forcing bias into INT8 does not increase speed because the bias is already fused into conv/gemm kernels.Thank you.Thank you so much.This topic was automatically closed 14 days after the last reply. New replies are no longer allowed.Powered by Discourse, best viewed with JavaScript enabled"
228,suitable-cuda-toolkit-and-cudnn-for-windows-laptop,"Hi,
I have following below specs of my laptop can anyone help me to redirect which cudnn and cuda toolkit will work, I am tired of this installation, i tried with 11 and 12 version but nothing worked as gpu is still not getting detected.
I have uninstalled evrything and going for reinstallation,  need guidance for that.Laptop specs:
os: windows 11
GPU: RTX 3060 (16 GB)Sorry for this naive question, very new to Data science.Hi ,
We recommend you to check the supported features from the below link.These support matrices provide a look into the supported platforms, features, and hardware capabilities of the NVIDIA TensorRT 8.6.1 APIs, parsers, and layers.
You can refer below link for all the supported operators list.
For unsupported operators, you need to create a custom plugin to support the operationThanks!Powered by Discourse, best viewed with JavaScript enabled"
229,tensorrt-conversion-of-a-model-for-jetson-nano-on-host-device,"If Tensorrt conversion of a model for jetson nano on host device is possible having same environment or any kind of simulated environment
A clear and concise description of the bug or issue.TensorRT Version:
GPU Type:
Nvidia Driver Version:
CUDA Version:
CUDNN Version:
Operating System + Version:
Python Version (if applicable):
TensorFlow Version (if applicable):
PyTorch Version (if applicable):
Baremetal or Container (if container which image + tag):Please attach or include links to any models, data, files, or scripts necessary to reproduce your issue. (Github repo, Google Drive, Dropbox, etc.)Please include:Hi,The generated engine files are not portable across platforms or TensorRT versions. TRT engine files are specific to the exact GPU model they were built on (in addition to the platforms and the TensorRT version) and must be re-targeted to the specific GPU in case you want to run them on a different GPU.Thank you.Hi @spolisetty thanks for the reply. We want to generate specific model for jetson nano … doesnt need it to be portable but if it is possible to generate through simulated nano hw or any other technique possible rather generating directly on nanoSorry, we have to generate an engine on Jetson Nano.Hi NVIDIA,
Can you please verify if TensorRT model conversion (creation of .engine-file)) MUST take place on the “hardware platform”, even with the latest releases of Jetpack/TensorRT? If so, are there any plans to make model conversion in a virtual environment on another “hardware platform” (or will that never be possible)?
Thanks.Powered by Discourse, best viewed with JavaScript enabled"
230,could-not-parse-onnx-model-from-file,"Hello!
When trying to convert ONNX file into TensorRT engine file on my Jetson Nano I get following error:Detailed log of conversion is attached.
As can be seen in log, initial Pytorch model was generated using Yolo5 and converted to ONNX through: https://github.com/noahmr/yolov5-tensorrtTensorRT Version: 8.0.1
GPU Type: Jetson Nano (Maxwell)
CUDA Version: 10.2.300
CUDNN Version: 8.2.1
Operating System + Version: Ubuntu 18.04 (Jetpack 4.6)
PyTorch Version (if applicable): 1.12.1Please attach or include links to any models, data, files, or scripts necessary to reproduce your issue. (Github repo, Google Drive, Dropbox, etc.)TensorRT conversion log.txt (104.6 KB)Hi,
Request you to share the ONNX model and the script if not shared already so that we can assist you better.
Alongside you can try few things:This NVIDIA TensorRT 8.6.0 Early Access (EA) Quick Start Guide is a starting point for developers who want to try out TensorRT SDK; specifically, this document demonstrates how to quickly construct an application to run inference on a TensorRT...import sys
import onnx
filename = yourONNXmodel
model = onnx.load(filename)
onnx.checker.check_model(model).
2) Try running your model with trtexec command.master/samples/trtexecNVIDIA® TensorRT™, an SDK for high-performance deep learning inference, includes a deep learning inference optimizer and runtime that delivers low latency and high throughput for inference applicat...
In case you are still facing issue, request you to share the trtexec “”–verbose"""" log for further debugging
Thanks!Hello!
Here are:
the ONNX model:
CantConvert.onnx (27.2 MB)and conversion script:
Convert_PT_to_ONNX.py (22.8 KB)The log of failed ONNX to TRT conversion is already attached in my previous post.
Thanks!Hello!
Any opinion?Hi @JoleCRO ,
By looking at the error, the issue looks like coming from the opset.
This post might be relevant to resolve this issue.ThanksHello!
Link you provided seems outdated and it uses TRT7 and I use TRT8.
Nevertheless, I tried  all eight possible (and impossible) Opset versions (from ver.9 till 16) and still keep getting the same error regardless the opset I try ?
The same ONNX to TRT conversion script (which I have provided above) did successful job when I got the PT model from some other Pytorch version (don’t know which it was), but why my current  1.12.1 would generate problematic model whose ONNX cannot be successfully converted to TRT ??Powered by Discourse, best viewed with JavaScript enabled"
231,torch-quantization-examples-for-manual-q-dq-control,"I want to learn about the manual method to add q/dq layers between operations similar to your tensorrt developer guide. Do you have any examples on how to reproduce the layers in Figure 1 - Figure 10?My tensorrt engine is slower than the default fp16 engine without q/dq layers because there are a lot and excessive input/output scaling between the operations. From your documentation, it’s recommended to be conservative about adding the q/dq operations, but in the tensorrt examples and quantization source codes, there is no specific function to add or remove q/dq operations between input/output. All operations are by defualt quantize the input, but I want to know how to quantize only in the first layer, keep in running in int8 until the last layer and finally convert the output using dequantize to get the float32 data like in figure 8 or figure 9,Powered by Discourse, best viewed with JavaScript enabled"
232,how-to-set-exactly-number-of-runs-for-profiling-engine-model,"I used command to see all option with trtexec but it only contains --avgRuns. It seems that trtexec automatically radomly choose number of runs greater than specific number.I want to set exactly number of runs = 100 for profiling my engine models. I need to set same number of runs for fair comparison. I saw that each run to profile, trtexec choose different number of iterations to profile layers. Could you give an advice?Hi,Please try using the --iterations flag. Refer to the following document for more details:This is the revision history of the NVIDIA TensorRT 8.6 Developer Guide.Thank you.Thanks for quick response. I did it.This topic was automatically closed 14 days after the last reply. New replies are no longer allowed.Powered by Discourse, best viewed with JavaScript enabled"
233,tts-input-text-too-long,"Hardware - GPU(T4
Hardware - CPU
Operating System ubuntu 20.04
Riva Version 2.10I deployed the TTS Spanish models from Nemo to Riva through a docker container, when synthesizing audio through a very long text it gives me the following error in docker logs:I’m using this notebook code:I have also used online synthesis with this code:With online synthesis  I got this error:How can I continue using SSML tags and make TTS capable of supporting long texts?HI @nharoThanks for your interest in RivaI will try to reproduce the issue internally within NvidiaRequest to kindly share the NGC link of Nemo TTS Model usedThanksI will try to reproduce the issue interSure, this is the link to the model: TTS Es Multispeaker FastPitch HiFiGAN | NVIDIA NGCPowered by Discourse, best viewed with JavaScript enabled"
234,is-that-possible-add-ds-example-opencv-lib-add-in-python-deepstream,"A clear and concise description of the bug or issue.Nvidia Driver Version:  515
CUDA Version: 11.7
Operating System + Version: ubuntu 20
Python Version (if applicable): pyhon3.8For the face blur effect, I simply want to add ds-example to the Python deepstream process.
That seems possible.Hi,This looks like a Deepstream related issue. We will move this post to the Deepstream forum.Thanks!Powered by Discourse, best viewed with JavaScript enabled"
235,getting-error-on-command-bash-riva-init-sh,"Please provide the following information when requesting support.Hardware - GPU (RTX-8000)
Operating System: Ubuntu 22.04
Riva Version: riva_quickstart_v2.9.0
TLT Version (if relevant): Not sureHow to reproduce the issue ? :My aim: I want to create an application that does the speech recognition. Speech to text and text to speech.What am I trying: Get a local Docker using Quick Start scripts to set up a local workstation and deploy the Riva services using Docker.Following: https://docs.nvidia.com/deeplearning/riva/user-guide/docs/quick-start-guide.htmlAlready installed: NVIDIA Driver, CUDA, Docker and downloaded the scripts for 2.9.0Getting error on: bash riva_init.sh (error shown below)Logging into NGC docker registry if necessary…Pulling required docker images if necessary…Note: This may take some time, depending on the speed of your Internet connection.Pulling Riva Speech Server images.
Image nvcr.io/nvidia/riva/riva-speech:2.9.0 exists. Skipping.
Image nvcr.io/nvidia/riva/riva-speech:2.9.0-servicemaker exists. Skipping.Downloading models (RMIRs) from NGC…Note: this may take some time, depending on the speed of your Internet connection.To skip this process and use existing RMIRs set the location and corresponding flag in config.sh.docker: Error response from daemon: could not select device driver “” with capabilities: [[gpu]].Error in downloading models.FYI: I am totally new to Riva. I am from windows background and new to Linux OS too. So if you need more info on anything please give the exact script to extract the info from the prompt.Hi @kavithapThanks for your interest in RivaWe will help you setup/guide the RivaQuick check, can you run the following docker command in your machine and share the output (either as screenshot as image and uplaod or as text)sudo docker run --rm --gpus all nvidia/cuda:11.0-base nvidia-smiif some error happens in running the above command, Please do the following installationhttps://docs.nvidia.com/datacenter/cloud-native/container-toolkit/archive/1.8.1/install-guide.html#setting-up-nvidia-container-toolkitAfter installing nvidia-docker2 and restart, i hopefully believe the Riva will start working,if you face issues in the process, please let me know, will be happy to helpThanks
image1366×145 8.9 KB
I tried to install nvidia-docker2, but that seems to have some conflict. Please the text copied from the prompt below.root@riva-vm-01:/home/aptask# distribution=$(. /etc/os-release;echo $ID$VERSION_ID) \&& curl -s -L https://nvidia.github.io/nvidia-docker/gpgkey | sudo apt-key add - 
&& curl -s -L https://nvidia.github.io/nvidia-docker/$distribution/nvidia-docker.list | sudo tee /etc/apt/sources.list.d/nvidia-docker.list
Warning: apt-key is deprecated. Manage keyring files in trusted.gpg.d instead (see apt-key(8)).
OK
deb https://nvidia.github.io/libnvidia-container/stable/ubuntu18.04/$(ARCH) /
#deb https://nvidia.github.io/libnvidia-container/experimental/ubuntu18.04/$(ARCH) /
deb https://nvidia.github.io/nvidia-container-runtime/stable/ubuntu18.04/$(ARCH) /
#deb https://nvidia.github.io/nvidia-container-runtime/experimental/ubuntu18.04/$(ARCH) /
deb https://nvidia.github.io/nvidia-docker/ubuntu18.04/$(ARCH) /
root@riva-vm-01:/home/aptask# sudo apt-get update
E: Conflicting values set for option Signed-By regarding source https://nvidia.github.io/libnvidia-container/stable/ubuntu18.04/amd64/ /: /usr/share/keyrings/nvidia-container-toolkit-keyring.gpg !=
E: The list of sources could not be read.
root@riva-vm-01:/home/aptask# sudo apt-get install -y nvidia-docker2
E: Conflicting values set for option Signed-By regarding source https://nvidia.github.io/libnvidia-container/stable/ubuntu18.04/amd64/ /: /usr/share/keyrings/nvidia-container-toolkit-keyring.gpg !=
E: The list of sources could not be read.
root@riva-vm-01:/home/aptask#Apologies, Some older tags that used to work have been depreciated,Please try the following command (as your OS is Ubuntu 22.02, giving this)sudo docker run --rm --gpus all nvidia/cuda:11.8.0-base-ubuntu22.04 nvidia-smiand share me the outputThanksroot@riva-vm-01:/home/aptask# sudo docker run --rm --gpus all nvidia/cuda:11.8.0-base-ubuntu22.04 nvidia-smi
Unable to find image ‘nvidia/cuda:11.8.0-base-ubuntu22.04’ locally
11.8.0-base-ubuntu22.04: Pulling from nvidia/cuda
677076032cca: Already exists
bc572704fd22: Pull complete
82ca2dd0fe9d: Pull complete
335006729f70: Pull complete
1b9f8e302abf: Pull complete
Digest: sha256:3a85383782012c87cd78f9b47c33b6105b2b6e17feadf475d5e835234f87a7be
Status: Downloaded newer image for nvidia/cuda:11.8.0-base-ubuntu22.04
docker: Error response from daemon: could not select device driver “” with capabilities: [[gpu]].
root@riva-vm-01:/home/aptask#Thanks, I guess the nvidia-docker2 is missing, Please install via instructions provided in below linkhttps://docs.nvidia.com/datacenter/cloud-native/container-toolkit/archive/1.8.1/install-guide.html#setting-up-nvidia-container-toolkitAfter Completing all steps of installing nvidia-docker2, please try riva setup again and let me know if it worksThanksOk I shall do all the steps from starting again.Commandline.txt (10.6 KB)Please find the txt file that reflects my issues. I am not sure what is it and how to proceed. Is it possible to share screen or remote connect?I am trying to set up the server to run the speech recognition code.
For that I have downloaded NVIDIA Riva Skills 2.9.0.
I have installed the prerequisites mentioned, There were too many issues I corrected it one by one with the help of forum and searching online. I could run the riva local docker also once, But now I am seeing different set of issues and not able to run bash riva_start.sh again. It comes like “Waiting for Riva server to load all models…retrying in 10 seconds
Health ready check failed.
Check Riva logs with: docker logs riva-speech”. Not sure what to do now.
What I need is just a small application exactly what is mentioned in Getting Started with NVIDIA Riva Speech Recognition - YouTube
The code is also readily available. But I am struggling with system settings and related errors and looks like compatibility issue or not sure what.
I am planning to scrap the current VM and take a fresh one.
I need help on setting the NVIDIA Riva Skills 2.9.0. on that.
Hardware - GPU (RTX-8000)
Operating System: Ubuntu 22.04 (if it is not compatible can choose a different one too)
What all do I need to install on that?, to get a simple ASR project run on that?HI @kavithapApologies on the delayYou are encountering some problem with the initial setup only and not Riva related,Nonetheless, The Problem you mentioned in your txt file can be resolved using below link reference
https://docs.nvidia.com/datacenter/cloud-native/container-toolkit/troubleshooting.html#conflicting-values-set-for-option-signed-by-error-when-running-apt-updatePlease try this and let me knowThanksPowered by Discourse, best viewed with JavaScript enabled"
236,cumemalloc-duration-is-too-large-cuda12-1-1-l4,"202306271127333251×917 206 KBHi all, I ran an AI inference program on my L4 machine(Driver Version: 525.116.03   CUDA Version: 12.0) and found the inference time was unexpectedly huge time to time. I expected the infernece was faster than T4 cuda11.4 machine.
I profiled the program using the nsight systems. It shows that cuda HW is idle due to cuMemAlloc opertation.
Can anyone explain the observation? Thanks.Hi,Could you please share with us the TensorRT version with which you’re facing the issue and issue repro model and relevant steps/scripts.Thank you.ok. I am using tensorrt_version_8_6_0_12. But the model and sample code are disclosed to external parties.
I wrote an simple demo which just read a serial of png files and upload to device side then release.
image1895×826 60.8 KBhere is the nsight systems report. cuMemAlloc function cost exceed 300ms very often. even though I did not use pinned memory, the memalloc on device is too slow. while I run the demo, there is no other process on the same card( another process running on the other gpu card).image934×696 24.4 KBdeviceQuery shows the cuda driver version is 12.0. While I have no clue the reason L4 is slower than T4, I will upgrade to 12.1 later and check the latency problem.While I have no clue the reason L4 is slower than T4, I will upgrade to 12.1 later and check the latency problem.Hi,Are you still facing the same issue on 12.1?
Please share the issue repro model, relevant scripts and steps if you still face the same issue.Thank you.Powered by Discourse, best viewed with JavaScript enabled"
237,running-dev-environments-and-ml-tasks-cost-effectively-in-any-cloud,"Hi everyone,I’m the core developer of dstack, an open-source tool that makes it very easy to run development environments and ML tasks in any cloud. It supports AWS, GCP, and Azure. Today, we’re excited to announce that we’ve added initial support for Lambda Cloud.If you’re interested in efficiently running ML workloads in the cloud, especially utilizing the cheapest cloud GPUs like Lambda Cloud, we invite you to give it a try!Here’s the repository with all the important links, including documentation, examples, and more: https://github.com/dstackai/dstackWe greatly appreciate everyone’s feedback!Powered by Discourse, best viewed with JavaScript enabled"
238,404-error-unable-to-download,"Please provide the following information when requesting support.Hardware - GPU (T4)
Operating System:Ubuntu, 20.04 LTS
Riva Version:2.6.0
TLT Version (if relevant)
How to reproduce the issue ? (
1.)Run below NGC command
2.)ngc registry resource download-version “nvidia/riva/riva_quickstart:2.6.0”
3.)The following error occurs
Getting files to download…
⠴ ━━━━━━━━━━━ • 0.0/105.5  • Remaining: -:–:-- • ? • Elapsed: 0:00:00 • Total: 18 - Completed: 0 - Failed: 0
KiB
Error getting pagination for file download:'{“timestamp”:1686821936861,“status”:404,“error”:“Not Found”,""path
⠴ ━━━━━━━━━━━ • 0.0/105.5  • Remaining: -:–:-- • ? • Elapsed: 0:00:00 • Total: 18 - Completed: 0 - Failed: 0
KiB)please guide to resolve this issueSame to me on mac m1:ngc registry resource download-version “nvidia/riva/riva_quickstart:2.11.0”Error getting pagination for file download:‘{“timestamp”:1686943143724,“status”:404,“error”:“Not Found”,“path”:“/v1/org/nvidia/team/riva/resources/riva_quickstart/2.11.0/files”}’
{
“download_end”: “2023-06-16 21:19:03”,
“download_start”: “2023-06-16 21:19:02”,
“download_time”: “1s”,
“files_downloaded”: 0,
“local_path”: “/Users/DEV/Python/NVIDIA/Riva/riva_quickstart_v2.11.0”,
“size_downloaded”: “0 B”,
“status”: “FAILED”
}but after researching a bit I have manually found the URL: Riva Skills Quick Start | NVIDIA NGCand also figured out that the actual resource path should be: /v2/resources/nvidia/riva/riva_quickstart/versions/2.11.0/files/ instead of “/v1/org/nvidia/team/riva/resources/riva_quickstart/2.11.0/files” but no idea how to change that for the “ngc registry download-version” command. It seems it is on the NGC backend sideHad the same issue on Win 10. Any success to resolve it?Apparently, we have to wait for that to be fixed in the “ngc registry download-version” command or download files manually otherwiseCannot download manually toobump for same problem with maxine:and no apparent way to download manually via the website either. it’s all brokenIf you visit the catalog with your browser.You will find ‘…’ next to each file where you can choose between copying a terminal command for downloading or downloading the file right in your browser.I have created a simple bash script to make it all at once and it worked for me (see attached file)
download-version.sh (3.4 KB)Just replace second line in the .sh file to the actual folder you want to useCatalog is here:Scripts and utilities for getting started with Riva Speech SkillsThanks for the script.I tried downloading the files with the script.However I am unable to initialize and start the riva scripts.
riva_init.sh: line 1: {requestStatus:statusCode:FORBIDDEN}: command not foundYep, same to me, unfortunately, the error is just written inside each downloaded file. Apparently, we’re out of options for now and have to wait while it is fixed by the NVIDIA teamAny update on this?  I’m having the same problem.  Seems like the riva scripts have not been downloadable for a week or more.Same here; albeit with a different package nvidia/maxine/maxine_windows_ar_sdk_ga:0.8.2"" UK, Windows 10.
Apologies if not the right place to bump, but I assume its not package specific, and as such this looks like a decent home for my post!
Download on the website:Attempt with ngcAs a side note, my original attempt with NGC ended up with this:I assumed a unicode issue, and shifted the ngc config over to JSON to get slightly further along :)
Looking forward to getting access!!!Downgrading to CLI version 3.22.0 is a workaround. You can use the command ‘ngc version upgrade 3.22.0’This workaround has actually worked. I am able to proceed furthur  :-)
Thank you so much…It worked for me as well. Thank you for thatPowered by Discourse, best viewed with JavaScript enabled"
239,onnx-model-and-tensorrt-engine-gives-different-output,"I have exported a PyTorch model to ONNX and the output matches, which means the ONNX model seems to be working as expected. However, after generating Tensorrt Engine from this ONNX file the outputs are different.TensorRT Version: 8.0.1.6
GPU Type: Tesla V100-PCIE-16GB-LS
Nvidia Driver Version: 470.129.06
CUDA Version: 10.2
CUDNN Version: CUDNN_MAJOR 8
Operating System + Version: Ubuntu 18.04.6 LTS
Python Version (if applicable): python3.6
TensorFlow Version (if applicable):
PyTorch Version (if applicable):
Baremetal or Container (if container which image + tag):https://drive.google.com/drive/folders/1-vsBozRxrbQ0WDDcMlBMpjOEXEC_ryTc?usp=sharingpython3 compare_pytorch_tensorrt.pyHi,
Request you to share the ONNX model and the script if not shared already so that we can assist you better.
Alongside you can try few things:This NVIDIA TensorRT 8.5.3 Quick Start Guide is a starting point for developers who want to try out TensorRT SDK; specifically, this document demonstrates how to quickly construct an application to run inference on a TensorRT engine.import sys
import onnx
filename = yourONNXmodel
model = onnx.load(filename)
onnx.checker.check_model(model).
2) Try running your model with trtexec command.master/samples/trtexecNVIDIA® TensorRT™, an SDK for high-performance deep learning inference, includes a deep learning inference optimizer and runtime that delivers low latency and high throughput for inference applicat...
In case you are still facing issue, request you to share the trtexec “”–verbose"""" log for further debugging
Thanks!I have already shared the ONNX and TensorRT models in their respective files. Please download both the models and scripts using the link provided.Have there been any updates?I found the problem may be caused by the operation of TopK. I set the k as 614. It seems a large K will result in error as showed in history posts.Powered by Discourse, best viewed with JavaScript enabled"
240,failure-to-convert-onnx-model-to-trt-parsing-failure-engine-setup-failure,"I am trying to convert the following model to onnx, then convert it to .trt:I import the pickled file then I convert it to onnx using the command:
torch.onnx.export(bfm_decoder,dummy_input,""bfm_noneck_v3.onnx"",input_names=['R','offset','alpha_shp','alpha_exp'],output_names=['output'],opset_version=11)This operation is provided in detail in the file:The I try to convert to trt using trtexec, but the trt conversion fails. I used the commands, their results are provided at the end:
./trtexec --onnx=bfm_noneck_v3.onnx  --saveEngine=bfm_noneck_v3.trt
./trtexec --onnx=bfm_noneck_v3.onnx  --saveEngine=bfm_noneck_v3.trt --verbose
./trtexec --onnx=bfm_noneck_v3.onnx  --saveEngine=bfm_noneck_v3.trt --shapes=R:3x3,offset:3x1,alpha_shp:40x1,alpha_exp:10x1TensorRT Version: 8.6.0.12
GPU Type: NVIDIA GeForce RTX 2060 with Max-Q Design
Nvidia Driver Version: nvidia-driver-530
CUDA Version: 12.1
CUDNN Version:
Operating System + Version: Ubuntu 22.04
Python Version (if applicable): 3.9.12
TensorFlow Version (if applicable):
PyTorch Version (if applicable):
Baremetal or Container (if container which image + tag):bfm_noneck_v3.onnx (22.4 MB)Hi,
Request you to share the ONNX model and the script if not shared already so that we can assist you better.
Alongside you can try few things:This NVIDIA TensorRT 8.6.0 Early Access (EA) Quick Start Guide is a starting point for developers who want to try out TensorRT SDK; specifically, this document demonstrates how to quickly construct an application to run inference on a TensorRT...import sys
import onnx
filename = yourONNXmodel
model = onnx.load(filename)
onnx.checker.check_model(model).
2) Try running your model with trtexec command.master/samples/trtexecNVIDIA® TensorRT™, an SDK for high-performance deep learning inference, includes a deep learning inference optimizer and runtime that delivers low latency and high throughput for inference applicat...
In case you are still facing issue, request you to share the trtexec “”–verbose"""" log for further debugging
Thanks!Hello,The onnx model is already attached, and the onnx checker passes okay. It is not possible to run the onnx from trtexec without converting it first.Hi @elight1 ,Would you like to check the relevant post.
This may help .ThanksPowered by Discourse, best viewed with JavaScript enabled"
241,conversion-error-of-mask-rcnn-onnx-model-for-different-types-weights,"The Torchvision Mask RCNN model cannot be converted in TensorRT engine for the following error:Please let me know on how to fix this issue.TensorRT Version: 8.5.2.2
GPU Type: Jetson Orin Nano
Nvidia Driver Version: JetPack 5.1.1
CUDA Version: 11.4.19
CUDNN Version: 8.6.0Operating System + Version: JetPack 5.1.1
Python Version (if applicable): 3.8
TensorFlow Version (if applicable):
PyTorch Version (if applicable): 2.0.0
Baremetal or Container (if container which image + tag): Baremetalmaskrcnn.onnx
maskrcnn_pth2onnx.py (1.9 KB)I’d like to have a TensorRT engine for Jetson Orin. But you can reproduce the issue on x86 platform too.Hi,
Request you to share the ONNX model and the script if not shared already so that we can assist you better.
Alongside you can try few things:This NVIDIA TensorRT 8.6.1 Quick Start Guide is a starting point for developers who want to try out TensorRT SDK; specifically, this document demonstrates how to quickly construct an application to run inference on a TensorRT engine.import sys
import onnx
filename = yourONNXmodel
model = onnx.load(filename)
onnx.checker.check_model(model).
2) Try running your model with trtexec command.master/samples/trtexecNVIDIA® TensorRT™, an SDK for high-performance deep learning inference, includes a deep learning inference optimizer and runtime that delivers low latency and high throughput for inference applicat...
In case you are still facing issue, request you to share the trtexec “”–verbose"""" log for further debugging
Thanks!@AakankshaS
Thank you for your reply.
I have already attached the ONNX model ‘maskrcnn.onnx’ at the 1st post.
I have validated the model with the way you suggested, please find the code in maskrcnn_pth2onnx.py which also attached at the 1st post.Hello. How are things going for this issue?The error message I had, said “Weights of same values but of different types are used in the network!”, so I suspected that the “value” attribute of /rpn/anchor_generator/ConstantOfShape was invalid, because the attribute type was INT64 but TensorRT is supporting FP32 only. Then I removed the “value” attribute with onnx-graphsurgeon so that TensorRT will treat it as FP32 according to the ONNX specification. But I still have the same error from trtexec.
Please let me know the meaning of the error message.Python code to remove the “value” attribute from the /rpn/anchor_generator/ConstantOfShape node:
modify_onnx_model.py (1.4 KB)trtexec log for the original model:
trtexec_log_org.txt (507.2 KB)trtexec log for the modified model:
trtexec_log_mod.txt (507.2 KB)Thanks.Hi,We tried the polygraphy tool, trtexec, and faced the following error. It looks like there is a problem with the model input.Please refer to the similar issues below.When I try to use trt to convert my crnn model, a problem happened.How to solve …it?

[05/10/2023-19:45:06] [I] TensorRT version: 8.5.1
[05/10/2023-19:45:07] [I] [TRT] [MemUsageChange] Init CUDA: CPU +383, GPU +0, now: CPU 6759, GPU 893 (MiB)
[05/10/2023-19:45:08] [I] [TRT] [MemUsageChange] Init builder kernel library: CPU +193, GPU +74, now: CPU 7062, GPU 967 (MiB)
[05/10/2023-19:45:08] [I] Start parsing network model
[05/10/2023-19:45:08] [I] [TRT] ----------------------------------------------------------------
[05/10/2023-19:45:08] [I] [TRT] Input filename:   D:/Dissertation/Vehicle-license-plates-recognition-master/Vehicle-license-plates-recognition-master/tools/inference/rec_onnx/model.onnx
[05/10/2023-19:45:08] [I] [TRT] ONNX IR version:  0.0.8
[05/10/2023-19:45:08] [I] [TRT] Opset version:    10
[05/10/2023-19:45:08] [I] [TRT] Producer name:
[05/10/2023-19:45:08] [I] [TRT] Producer version:
[05/10/2023-19:45:08] [I] [TRT] Domain:
[05/10/2023-19:45:08] [I] [TRT] Model version:    0
[05/10/2023-19:45:08] [I] [TRT] Doc string:
[05/10/2023-19:45:08] [I] [TRT] ----------------------------------------------------------------
[05/10/2023-19:45:08] [W] [TRT] onnx2trt_utils.cpp:377: Your ONNX model has been generated with INT64 weights, while TensorRT does not natively support INT64. Attempting to cast down to INT32.
[05/10/2023-19:45:08] [I] Finish parsing network model
[05/10/2023-19:45:08] [W] Dynamic dimensions required for input: x, but no shapes were provided. Automatically overriding shape to: 1x3x32x1
[05/10/2023-19:45:08] [E] Error[4]: [graphShapeAnalyzer.cpp::nvinfer1::builder::`anonymous-namespace'::ShapeNodeRemover::analyzeShapes::1872] Error Code 4: Miscellaneous (IRecurrenceLayer p2o.LSTM.4: reshape wildcard -1 has infinite number of solutions or no solution. Reshaping [0,1,2,48] to [0,0,-1].)
[05/10/2023-19:45:08] [E] Error[2]: [builder.cpp::nvinfer1::builder::Builder::buildSerializedNetwork::751] Error Code 2: Internal Error (Assertion engine != nullptr failed. )
[05/10/2023-19:45:08] [E] Engine could not be created from network
[05/10/2023-19:45:08] [E] Building engine failed
[05/10/2023-19:45:08] [E] Failed to create engine from model or file.
[05/10/2023-19:45:08] [E] Engine set up failed## Description

I'm trying to convert the faster-rcnn.pt file to the onnx and …use it with onnxruntime-gpu and tensorrt provider. When I check the converted onnx with trtexec, I get an Shape Error. One of the node has [0,16] input and that node tries to reshape it [0,-1]. Is there any way to assign the shape of the node manually ? It is because of the tools versions that I used ? 


## Environment

**TensorRT Version**: 8.6 
**NVIDIA GPU**: Quadro RTX 4000
**NVIDIA Driver Version**:  530.30.02
**CUDA Version**: 12.1
**CUDNN Version**: 
**Operating System**: Ubuntu 18.04
**Python Version (if applicable)**: 3.9
**Tensorflow Version (if applicable)**: 
**PyTorch Version (if applicable)**: 1.7.1
**Baremetal or Container (if so, version)**: 



## Steps To Reproduce


1.Exported onnx  from PyTorch weight file (faster-rcnn model)
dynamic_axes = {""input"":{0:""batch_size""}, 
                ""boxes"":{0:""batch_size""}, 
                ""labels"":{0:""batch_size""}, 
                ""scores"":{0:""batch_size""}}

torch.onnx.export(
    model.to(""cuda""),
    dummy_input.to(""cuda""),
    onnx_filename,
    verbose = True,
    opset_version = 12,
    input_names = input_names,
    output_names = output_names,
    dynamic_axes = dynamic_axes,
    do_constant_folding=True
)
2.Applied polygraphy surgeon
polygraphy surgeon sanitize n.onnx --fold-constants --output n2.onnx

3. Run trtexec on new onnx file
trtexec --onnx=n2.onnx --shapes=input:1x2x2024x2024 --verbose


ERROR: Reshape node, infinite solution for Reshaping [0,16] to [0,-1].
...
[03/22/2023-10:30:08] [E] [TRT] ModelImporter.cpp:770: input: ""2881""
input: ""2897""
output: ""2898""
name: ""Reshape_1605""
op_type: ""Reshape""

[03/22/2023-10:30:08] [E] [TRT] ModelImporter.cpp:771: --- End node ---
[03/22/2023-10:30:08] [E] [TRT] ModelImporter.cpp:774: ERROR: ModelImporter.cpp:195 In function parseGraph:
[6] Invalid Node - Reshape_1605
[shapeContext.cpp::operator()::3546] Error Code 4: Shape Error (reshape wildcard -1 has infinite number of solutions or no solution. Reshaping [0,16] to [0,-1].)
[03/22/2023-10:30:08] [E] Failed to parse onnx file
[03/22/2023-10:30:08] [I] Finished parsing network model. Parse time: 0.284967
[03/22/2023-10:30:08] [E] Parsing model failed
[03/22/2023-10:30:08] [E] Failed to create engine from model or file.
[03/22/2023-10:30:08] [E] Engine set up failed
&&&& FAILED TensorRT.trtexec [TensorRT v8600] # /usr/src/tensorrt/bin/trtexec --onnx=n2.onnx --shapes=input:1x2x2024x2024 --verboseThank you.Powered by Discourse, best viewed with JavaScript enabled"
242,tensorrt-engine-combining-the-output-dimentions-to-196608-0-instead-1-256-384-2,"A clear and concise description of the bug or issue.TensorRT Version: 8.5.3.1
GPU Type: RTX 3060
Nvidia Driver Version:  520.61.05
CUDA Version: 11.8
CUDNN Version: 10.1
Operating System + Version:  ubuntu 20.04
Python Version (if applicable):  3.8.16Hi, In below code i am trying to convert CRAFT model into tensorrt format, but in the output i am getting [(3145728,),(196608,)] instead of [(1, 32, 256, 384),(1, 256, 384, 2)]. as you can see it’s combining all the dimensions into one, i am using below code to do this,please let me know what i am doing wrong here.Please check the expected dimensions of the output of the ONNX model.
Could you please share with us the minimal issue repro ONNX model, engine building command/script and script to try from our end?Thank you.Powered by Discourse, best viewed with JavaScript enabled"
243,cuopt-23-04,"there’s an announcement on cuopt 23.04 (NVIDIA/cuOpt-Resources Announcements · Discussions · GitHub) in your github repository.
Where can we get this version ?Hi,Thank you for your interest in cuOpt. We continuously upgrade our solvers to enrich features and incorporate feedback. All updated releases will be announced on open forums to inform the community. Open access to cuOpt is closed as this is a paid offering.Please reach out to cuopt@nvidia.com for additional details on licensing options so we can connect you to the appropriate sales teams.Thanks,
cuOpt TeamPowered by Discourse, best viewed with JavaScript enabled"
244,eula-research-using-geforce-cards,"I work as a machine learning researcher at a university. I want to use RTX 4090 cards (or possible the new upcoming Titan) for machine learning research activities. The models will only be accessible to myself and a colleague. We want to build a machine, on prem, with up to 8 geforce cards. Is this usage covered by your license? I’m happy to provide any further information you require.I think our usage is one of the scenarios covered by your spokesperson as quoted here : Nvidia: Using cheap GeForce, Titan GPUs in servers? Haha, nope! • The RegisterHello, Is this the wrong forum for this question?  If so please point me in the right direction.Powered by Discourse, best viewed with JavaScript enabled"
245,accuracy-drop-in-resize-op-when-converting-from-onnx-to-trt-fp32,"There is an accuracy drop when converting a simple resize op from ONNX to TRT.
I’ve noticed a general performance drop in few of my models and used polygraphy to debug.
I was able to trace the root cause to a Resize node and was able to reproduce this in a simple example (see relevant files and steps to reproduce in the appropriate sections)the resize op is defined in the ONNX model as follows:when i run polygraphy run model.onnx --workspace 1G --atol 1e-1 --rtol 1e-1  --trt --onnxrt --trt-outputs mark all  --onnx-outputs mark all i get the following accuracy issue:Also notice the high tolerance picked for mistakes, the default is 1e-5.I believe the issue is in building the TRT op from the current attributes of the ONNX op. causing some other mathematic operation to be done that causes the difference in the output.
Since the actual implementation of the TRT op isnt available, I hope you could find the bug and solve it.I would also appreciate a workaround until it is solved.Thanks in advanceTensorRT Version : 7.1.3
NVIDIA GPU : Jetson AGX Xavier
NVIDIA Driver Version : l4t 32.5.1
CUDA Version : 10.2
CUDNN Version :
ONNX runtime : onnxruntime 1.10.0
ONNX opset : 11
Operating System : Ubuntu 18.04.5 LTS
Python Version (if applicable) : 3.6.9
Tensorflow Version (if applicable) : 2.3 (original model was created with TF, then converted to ONNX)
PyTorch Version (if applicable) :
Baremetal or Container (if so, version) : custom containerlink to a ONNX model that has the issue when converted to TRTHi,We recommend that you please try the latest TensorRT version 8.5 and let us know if you still face this issue.
If you’re interested, you can also try the TensorRT NGC container to avoid setup related issues.NVIDIA TensorRT is a C++ library that facilitates high-performance inference on NVIDIA graphics processing units (GPUs). TensorRT takes a trained network and produces a highly optimized runtime engine that performs inference for that network.Thank you.Hey @spolisetty and thank you for the quick response.Unfortunately I don’t have any option to upgrade to the latest jetpack at this time.
Is there a way to upgrade my TRT version without upgrading the jetpack?
Do you know the root cause of the issue? what has changed between my version and version 8.5 for a Resize op? which “minimal” version of TRT solves the issue?Hi @weissrael ,
Are you still facing the issue?Hi @AakankshaS
Yes, basically
I found a possible workaround in terms of onnx<–>TRT compatibility for TRT version 7.1- adjusting the resize interpolation method to “bilinear” instead of “nearest_neighbor”. Problem is, this workaround requires finetune training the model more, otherwise the model isn’t usable in terms of reasonable predictions.
It takes too much time so I’d rather have a solution that doesn’t require more finetune training.
If there’s a workaround to upgrade TRT without upgrading the Jetpack, and having the original model I have without training it more, this would be ideal- either for me or for other people who might have this issue.@weissrael did you find a solution for this?  I am currently facing something similarPowered by Discourse, best viewed with JavaScript enabled"
246,tensorrt-inference-is-taking-1-5-sec-to-inference-a-single-frame-i-want-to-speed-up-my-inference,"here is my tensorrt inference script.
with this script, inferencing one frame it is taking 1.5-sec which means 0.5fps. I want it t have a better fps.
I’m sharing my script below.particularly,context.execute(batch_size=1, bindings=[int(d_input_1), int(d_output)])
this line is taking 1.4sec to runimport tensorrt as trt
import pycuda.driver as cuda
import pycuda.autoinit
import numpy as npdef allocate_buffers(engine, batch_size, data_type):“”""
This is the function to allocate buffers for input and output in the device
Args:
engine : The path to the TensorRT engine.
batch_size : The batch size for execution time.
data_type: The type of the data for input and output, for example trt.float32.Output:
h_input_1: Input in the host.
d_input_1: Input in the device.
h_output_1: Output in the host.
d_output_1: Output in the device.
stream: CUDA stream.“”""h_input_1 = cuda.pagelocked_empty(batch_size * trt.volume(engine.get_binding_shape(0)), dtype=trt.nptype(data_type))
h_output = cuda.pagelocked_empty(batch_size * trt.volume(engine.get_binding_shape(1)), dtype=trt.nptype(data_type))d_input_1 = cuda.mem_alloc(h_input_1.nbytes)d_output = cuda.mem_alloc(h_output.nbytes)stream = cuda.Stream()
return h_input_1, d_input_1, h_output, d_output, streamdef load_images_to_buffer(pics, pagelocked_buffer):preprocessed = np.asarray(pics).ravel()
np.copyto(pagelocked_buffer, preprocessed)def do_inference(engine, pics_1, h_input_1, d_input_1, h_output, d_output, stream, batch_size, height, width):“”""
This is the function to run the inference
Args:
engine : Path to the TensorRT engine.
pics_1 : Input images to the model.
h_input_1: Input in the host.
d_input_1: Input in the device.
h_output_1: Output in the host.
d_output_1: Output in the device.
stream: CUDA stream.
batch_size : Batch size for execution time.
height: Height of the output image.
width: Width of the output image.Output:
The list of output images.“”""load_images_to_buffer(pics_1, h_input_1)with engine.create_execution_context() as context:cuda.memcpy_htod_async(d_input_1, h_input_1, stream)here is my trtexec output&&&& RUNNING TensorRT.trtexec [TensorRT v8201] # trtexec --loadEngine=unethalf_engine.engine --shapes=modelInput:13 7201280
[03/13/2023-18:55:38] [I] === Model Options ===
[03/13/2023-18:55:38] [I] Format: *
[03/13/2023-18:55:38] [I] Model:
[03/13/2023-18:55:38] [I] Output:
[03/13/2023-18:55:38] [I] === Build Options ===
[03/13/2023-18:55:38] [I] Max batch: explicit batch
[03/13/2023-18:55:38] [I] Workspace: 16 MiB
[03/13/2023-18:55:38] [I] minTiming: 1
[03/13/2023-18:55:38] [I] avgTiming: 8
[03/13/2023-18:55:38] [I] Precision: FP32
[03/13/2023-18:55:38] [I] Calibration:
[03/13/2023-18:55:38] [I] Refit: Disabled
[03/13/2023-18:55:38] [I] Sparsity: Disabled
[03/13/2023-18:55:38] [I] Safe mode: Disabled
[03/13/2023-18:55:38] [I] DirectIO mode: Disabled
[03/13/2023-18:55:38] [I] Restricted mode: Disabled
[03/13/2023-18:55:38] [I] Save engine:
[03/13/2023-18:55:38] [I] Load engine: unethalf_engine.engine
[03/13/2023-18:55:38] [I] Profiling verbosity: 0
[03/13/2023-18:55:38] [I] Tactic sources: Using default tactic sources
[03/13/2023-18:55:38] [I] timingCacheMode: local
[03/13/2023-18:55:38] [I] timingCacheFile:
[03/13/2023-18:55:38] [I] Input(s)s format: fp32:CHW
[03/13/2023-18:55:38] [I] Output(s)s format: fp32:CHW
[03/13/2023-18:55:38] [I] Input build shape: modelInput=1+1+1
[03/13/2023-18:55:38] [I] Input calibration shapes: model
[03/13/2023-18:55:38] [I] === System Options ===
[03/13/2023-18:55:38] [I] Device: 0
[03/13/2023-18:55:38] [I] DLACore:
[03/13/2023-18:55:38] [I] Plugins:
[03/13/2023-18:55:38] [I] === Inference Options ===
[03/13/2023-18:55:38] [I] Batch: Explicit
[03/13/2023-18:55:38] [I] Input inference shape: modelInput=1
[03/13/2023-18:55:38] [I] Iterations: 10
[03/13/2023-18:55:38] [I] Duration: 3s (+ 200ms warm up)
[03/13/2023-18:55:38] [I] Sleep time: 0ms
[03/13/2023-18:55:38] [I] Idle time: 0ms
[03/13/2023-18:55:38] [I] Streams: 1
[03/13/2023-18:55:38] [I] ExposeDMA: Disabled
[03/13/2023-18:55:38] [I] Data transfers: Enabled
[03/13/2023-18:55:38] [I] Spin-wait: Disabled
[03/13/2023-18:55:38] [I] Multithreading: Disabled
[03/13/2023-18:55:38] [I] CUDA Graph: Disabled
[03/13/2023-18:55:38] [I] Separate profiling: Disabled
[03/13/2023-18:55:38] [I] Time Deserialize: Disabled
[03/13/2023-18:55:38] [I] Time Refit: Disabled
[03/13/2023-18:55:38] [I] Skip inference: Disabled
[03/13/2023-18:55:38] [I] Inputs:
[03/13/2023-18:55:38] [I] === Reporting Options ===
[03/13/2023-18:55:38] [I] Verbose: Disabled
[03/13/2023-18:55:38] [I] Averages: 10 inferences
[03/13/2023-18:55:38] [I] Percentile: 99
[03/13/2023-18:55:38] [I] Dump refittable layers:Disabled
[03/13/2023-18:55:38] [I] Dump output: Disabled
[03/13/2023-18:55:38] [I] Profile: Disabled
[03/13/2023-18:55:38] [I] Export timing to JSON file:
[03/13/2023-18:55:38] [I] Export output to JSON file:
[03/13/2023-18:55:38] [I] Export profile to JSON file:
[03/13/2023-18:55:38] [I]
[03/13/2023-18:55:38] [I] === Device Information ===
[03/13/2023-18:55:38] [I] Selected Device: NVIDIA Tegra X1
[03/13/2023-18:55:38] [I] Compute Capability: 5.3
[03/13/2023-18:55:38] [I] SMs: 1
[03/13/2023-18:55:38] [I] Compute Clock Rate: 0.9216 GHz
[03/13/2023-18:55:38] [I] Device Global Memory: 3964 MiB
[03/13/2023-18:55:38] [I] Shared Memory per SM: 64 KiB
[03/13/2023-18:55:38] [I] Memory Bus Width: 64 bits (ECC disabled)
[03/13/2023-18:55:38] [I] Memory Clock Rate: 0.01275 GHz
[03/13/2023-18:55:38] [I]
[03/13/2023-18:55:38] [I] TensorRT version: 8.2.1
[03/13/2023-18:55:40] [I] [TRT] [MemUsageChange] Init CUDA: CPU +229, GPU +0, now: CPU 289, GPU 2525 (MiB)
[03/13/2023-18:55:40] [I] [TRT] Loaded engine size: 41 MiB
[03/13/2023-18:55:41] [I] [TRT] [MemUsageChange] Init cuBLAS/cuBLASLt: CPU +158, GPU +160, now: CPU 448, GPU 2686 (MiB)
[03/13/2023-18:55:42] [I] [TRT] [MemUsageChange] Init cuDNN: CPU +240, GPU +243, now: CPU 688, GPU 2929 (MiB)
[03/13/2023-18:55:42] [I] [TRT] [MemUsageChange] TensorRT-managed allocation in engine deserialization: CPU +0, GPU +41, now: CPU 0, GPU 41 (MiB)
[03/13/2023-18:55:42] [I] Engine loaded in 4.66617 sec.
[03/13/2023-18:55:42] [I] [TRT] [MemUsageChange] Init cuBLAS/cuBLASLt: CPU +0, GPU +0, now: CPU 647, GPU 2887 (MiB)
[03/13/2023-18:55:42] [I] [TRT] [MemUsageChange] Init cuDNN: CPU +0, GPU +0, now: CPU 647, GPU 2887 (MiB)
[03/13/2023-18:55:42] [I] [TRT] [MemUsageChange] TensorRT-managed allocation in IExecutionContext creation: CPU +0, GPU +192, now: CPU 0, GPU 233 (MiB)
[03/13/2023-18:55:42] [I] Using random values for input modelInput
[03/13/2023-18:55:43] [I] Created input binding for modelInput with dimensions 1x3x720x1280
[03/13/2023-18:55:43] [I] Using random values for output modelOutput
[03/13/2023-18:55:43] [I] Created output binding for modelOutput with dimensions 1x1x720x1280
[03/13/2023-18:55:43] [I] Starting inference
[03/13/2023-18:56:00] [I] Warmup completed 1 queries over 200 ms
[03/13/2023-18:56:00] [I] Timing trace has 10 queries over 14.5163 s
[03/13/2023-18:56:00] [I]
[03/13/2023-18:56:00] [I] === Trace details ===
[03/13/2023-18:56:00] [I] Trace averages of 10 runs:
[03/13/2023-18:56:00] [I] Average on 10 runs - GPU latency: 1450.18 ms - Host latency: 1451.62 ms (end to end 1451.63 ms, enqueue 13.3297 ms)
[03/13/2023-18:56:00] [I]
[03/13/2023-18:56:00] [I] === Performance summary ===
[03/13/2023-18:56:00] [I] Throughput: 0.68888 qps
[03/13/2023-18:56:00] [I] Latency: min = 1445.93 ms, max = 1456.88 ms, mean = 1451.62 ms, median = 1451.2 ms, percentile(99%) = 1456.88 ms
[03/13/2023-18:56:00] [I] End-to-End Host Latency: min = 1445.94 ms, max = 1456.9 ms, mean = 1451.63 ms, median = 1451.22 ms, percentile(99%) = 1456.9 ms
[03/13/2023-18:56:00] [I] Enqueue Time: min = 2.87402 ms, max = 17.4658 ms, mean = 13.3297 ms, median = 13.9668 ms, percentile(99%) = 17.4658 ms
[03/13/2023-18:56:00] [I] H2D Latency: min = 1.07715 ms, max = 1.08496 ms, mean = 1.08086 ms, median = 1.08057 ms, percentile(99%) = 1.08496 ms
[03/13/2023-18:56:00] [I] GPU Compute Time: min = 1444.49 ms, max = 1455.45 ms, mean = 1450.18 ms, median = 1449.76 ms, percentile(99%) = 1455.45 ms
[03/13/2023-18:56:00] [I] D2H Latency: min = 0.355469 ms, max = 0.358398 ms, mean = 0.357129 ms, median = 0.357422 ms, percentile(99%) = 0.358398 ms
[03/13/2023-18:56:00] [I] Total Host Walltime: 14.5163 s
[03/13/2023-18:56:00] [I] Total GPU Compute Time: 14.5018 s
[03/13/2023-18:56:00] [I] Explanations of the performance metrics are printed in the verbose logs.
[03/13/2023-18:56:00] [I]
&&&& PASSED TensorRT.trtexec [TensorRT v8201] # trtexec --loadEngine=unethalf_engine.engine --shapes=modelInput:1 3720 1280TensorRT Version: 8.2.x
GPU Type: Jetsonnano
Nvidia Driver Version:
CUDA Version:
CUDNN Version:
Operating System + Version:
Python Version (if applicable):
TensorFlow Version (if applicable):
PyTorch Version (if applicable):
Baremetal or Container (if container which image + tag):Hi,
Can you try running your model with trtexec command, and share the “”–verbose"""" log in case if the issue persistmaster/samples/trtexecNVIDIA® TensorRT™, an SDK for high-performance deep learning inference, includes a deep learning inference optimizer and runtime that delivers low latency and high throughput for inference applicat...You can refer below link for all the supported operators list, in case any operator is not supported you need to create a custom plugin to support that operationAlso, request you to share your model and script if not shared already so that we can help you better.Meanwhile, for some common errors and queries please refer to below link:This is the revision history of the NVIDIA TensorRT 8.5 Developer Guide.This is the revision history of the NVIDIA TensorRT 8.5 Developer Guide.Thanks!Powered by Discourse, best viewed with JavaScript enabled"
247,could-i-use-riva-in-unity,"I would be very grateful if we could ask when you are available.I’m currently developing in Unity and are looking for a way to enable ASR / TTS in an offline environment.
Is it possible to use Riva in Unity (C#)? If so, what would be the appropriate method?I know that there is implementations for VOSK, but I could not find it for Riva.
Since I am a beginner engineer, I am sorry that I cannot implement it myself.Thank you very much in advance for your time.
If this is an inappropriate question, you may close it.Hi @yaohd71532Thanks for your interest in RivaI am not sure we support Unity, I will confirm from my team
I guess we may have other options,Can you let us know the use case/example to get more understandingThanksSo, does RIVA support Unity 3D?FollowingHi @peter170 @shunzi3Apologies, Currently we don’t have support for Riva in UnityThanksHelloStill no support for Riva in unity? I have to use Riva TTS and ASR in unity project. Please guide me ASAPHI @hashir.kamalApologies, We still do not support Riva in UnityThanksPowered by Discourse, best viewed with JavaScript enabled"
248,whats-the-rules-that-cudnn-convolution-select-fp32-vector-core-and-tf32-tensor-core,"Hi,device: A100;For cudnn function “CuDNN (v8400) function cudnnConvolutionForward()”  and convoluton params:
x, w, y dataType =CUDNN_DATA_FLOAT ;
mode: CUDNN_CROSS_CORRELATION;
mathType: CUDNN_FMA_MATH;
layout: NCHW;I found that cudnn will select fp32 vector core or tf32 tensor for different shape(n c h w, r, s, stride and dilation);
What’s rules that cudnn convoluton select fp32 vector core and tf 32 tensor core based on shapes?Hi @zhang662817 ,Apologies for the delay.
Are you still facing the issue?Powered by Discourse, best viewed with JavaScript enabled"
249,inference-yolov8s-custom-train-model-getting-low-fps,"Device  : Jeston Xavier NX (16 GB RAM), trt version:8.2.1.9, jetpack:4.6.1
converted yolov8s into engine(fp=16, imgsz=640) after train
at the time of inferencing on video and camera geting just 12-14 fps,
how can i imrove fps??
*Yolov4-tiny gives same fps on same deviceHi @uditbajpai0495,this forum is specifically for cuOpt questions.  Please visit https://forums.developer.nvidia.com/ and look for a forum that is aligned to Jetson Xavier and/or inferencing.Powered by Discourse, best viewed with JavaScript enabled"
250,error-cannot-deserialize-plugin-since-corresponding-iplugincreator-not-found-in-plugin-registry-with-yolov4-tensorrt-engine,"I trained a Yolov4 model with TAO toolkit and exported it to .etlt model.
I wanted to run inference with TensorRT Python API.Able to generate engine with the above command. But I’m unable to deserialize the engine when I try to do inference with Python API. Attached the code to run inference on the generated engine.trt_loader.py (9.3 KB)TensorRT Version: 8.2.2
GPU Type: RTX 3070
Nvidia Driver Version: 530.30.02
CUDA Version: 11.6
CUDNN Version: 8.3.2
Operating System + Version: Ubuntu 20.04
Python Version (if applicable): 3.8
TensorFlow Version (if applicable): NA
PyTorch Version (if applicable): NA
Baremetal or Container (if container which image + tag): Container version 22.01Please include:Kindly help me fix this issue.Hi,
Please refer to below links related custom plugin implementation and sample:This Samples Support Guide provides an overview of all the supported NVIDIA TensorRT 8.6.0 Early Access (EA) samples included on GitHub and in the product package. The TensorRT samples specifically help in areas such as recommenders, machine...TensorRT is an SDK for high performance, deep learning inference. It includes a deep learning inference optimizer and a runtime that delivers low latency and high throughput for deep learning…While IPluginV2 and IPluginV2Ext interfaces are still supported for backward compatibility with TensorRT 5.1 and 6.0.x respectively, however, we recommend that you write new plugins or refactor existing ones to target the IPluginV2DynamicExt or IPluginV2IOExt interfaces instead.Thanks!Hi,
Thanks for your reply. I think yolov4 model trained on TAO toolkit just requires installation of TensorRT OSS. Please refer this link:
https://docs.nvidia.com/metropolis/TLT/tlt-user-guide/text/object_detection/yolo_v4.html#tensorrt-open-source-software-oss
There isn’t any mention on writing the custom plugin for this model.Hi Team, Any lead on this please.Hi @arivarasan.e ,
Are you registering the plugins before deserializing TRT engine in python?
Maybe you should add following code before deserializing engine.
trt.init_libnvinfer_plugins()ThanksThis resolved the issue. Thanks for the help :)This topic was automatically closed 14 days after the last reply. New replies are no longer allowed.Powered by Discourse, best viewed with JavaScript enabled"
251,model-conversion-to-tensorrt,"Hello, I want to convert my .pth model to .trt model, WillTorch-TensorRT technique(Accelerating Inference Up to 6x Faster in PyTorch with Torch-TensorRT)Will this technique work ?Hi,
Request you to share the ONNX model and the script if not shared already so that we can assist you better.
Alongside you can try few things:This NVIDIA TensorRT 8.6.1 Quick Start Guide is a starting point for developers who want to try out TensorRT SDK; specifically, this document demonstrates how to quickly construct an application to run inference on a TensorRT engine.import sys
import onnx
filename = yourONNXmodel
model = onnx.load(filename)
onnx.checker.check_model(model).
2) Try running your model with trtexec command.//github.com/NVIDIA/TensorRT/tree/master/samples/trtexec
In case you are still facing issue, request you to share the trtexec “”–verbose"""" log for further debugging
Thanks!Powered by Discourse, best viewed with JavaScript enabled"
252,verify-cudnn-install-failed,"After install cuda 11.2 and cudnn 8.1.0, can’t make mnistCUDNNHi @1chimaruGin ,
Can you please validate your steps from the installation guide.This cuDNN 8.5.0 Installation Guide provides step-by-step instructions on how to install and check for correct operation of NVIDIA cuDNN on Linux and Microsoft Windows systems.Thanks!Facing the same issue. Please guide me if there is a solution already available.This worked for meThank you very much!Powered by Discourse, best viewed with JavaScript enabled"
253,int8-callibration-with-point-cloud,"A clear and concise description of the bug or issue.TensorRT Version: 8.2.1
GPU Type: Jetson Xavier
Nvidia Driver Version:
CUDA Version: 10.2
CUDNN Version:
Operating System + Version:
Python Version (if applicable):
TensorFlow Version (if applicable):
PyTorch Version (if applicable):
Baremetal or Container (if container which image + tag):I want to convert my model from onnx to trt with int8 precision. When one want’s to convert model that precesses images he/she should made calibration on images. But I’m working with point clouds the dimensions of input tensor is (batch_size, 3, 1024) 1024 points with distance in x, y, z directions.Is there some guide or suggestions how to calibrate on point clouds?Hi, Please refer to the below links to perform inference in INT8master/samples/sampleINT8NVIDIA® TensorRT™, an SDK for high-performance deep learning inference, includes a deep learning inference optimizer and runtime that delivers low latency and high throughput for inference applicat...1777.21 KB
Thanks!Powered by Discourse, best viewed with JavaScript enabled"
254,runtimeerror-tensorflow-has-not-been-built-with-tensorrt-support,"I am trying to convert the saved_model format into TensorRT using google colab, for that, I’m referring to the post (Accelerating Inference in TensorFlow with TensorRT User Guide :: NVIDIA Deep Learning Frameworks Documentation).
And it is giving me the below error:
RuntimeError: Tensorflow has not been built with TensorRT support.Converting the model on google colab is a proper way or do I need to use anaconda to install TensorRt and then convert it?Thank you!Hi,Could you please make sure you installed tensorflow-gpu.
If interested you can also use NVIDIA NGC containers for TensorFlow, which are built and tested with TF-TRT support enabled.Thank you.tensorflow-gpu is removed rightPowered by Discourse, best viewed with JavaScript enabled"
255,cannot-allocate-memory-in-static-tls-block,"On my jetson xavier nx, I am trying to get an image from gstreamer and then use my tensorrt engine (detectron2) for object detection.Problem is that under a few circumstances, it throws this error.

model1848×94 23.9 KB
Basically, I will provide a rough code:If I import this file on the top before cv2.videoCapture, it throws the above error.But if I import it later, it works fine.Now in my actual code, I cannot actually import it later since I need to call it in a class’s init.I saw some solutions online especially the libgomp one but that isn’t the case here.Any advices?TensorRT Version: 8.5.2.2
Jetson & Jetpack: Nvidia Jetson Xavier Jetpack 5.1
CUDA Version:  11.4
Operating System + Version:  Ubuntu 20.04
Python Version (if applicable):  3.8.10
PyTorch Version (if applicable): 1.14.0nvm, it’s solved.This topic was automatically closed 14 days after the last reply. New replies are no longer allowed.Powered by Discourse, best viewed with JavaScript enabled"
256,riva-asr-quickstart-throws-cudaerror-an-illegal-memory-access-was-encountered,"Environment:Steps to reproduce:And start producing any ASR streaming requests from client (nvidia-riva-client==2.10.0)Results
Riva server can’t process any ASR requests and throwing a lot of errors like the following:I also tried setting gpus_to_use=“all”, but nothing changed.
I want to note that this problem occurs with Riva versions starting from 2.8.0, while 2.7.0 works without any issues with the same configuration.Hope these files could help during the investigation.config.sh (12.7 KB)
nvidia-smi.txt (2.3 KB)
riva_init.log (150.2 KB)
riva_speech.log (2.0 MB)
riva_start.log (265 Bytes)Any help on this would be much appreciated.
Thanks in advance!Hi @vbilousThanks for your interest in RivaApologies you are facing issue,
Thanks for sharing the logs, I will check with the Riva team and provide updatesThanksWe are also tried to run Riva on a different VM, but the result is still the same.Environment:config.sh (13.3 KB)
nvidia-smi.log (1.5 KB)
riva_init.log (171.5 KB)
riva_speech.log (365.4 KB)
riva_start.log (333 Bytes)Thanks in advanceHI @vbilousSincere Apologies for the delay,I have not reached the complete triage and solution,But i have a point to testThe Current CUDA version you have at your end is 12.0Can you kindly downgrade to CUDA 11.8 and check
Riva works with CUDA 11.8Please try and let us know, while i get more informationThanksHi @vbilousApologies on the delayThis issue has been fixed and won’t happen in next release of RivaNext Release should be out by tentatively in first week of JulyThanksHi @vbilousI hope the 2.12.0 release did’nt help solve the issue,
One Question, can you confirm whether you are using a vGPU setup or a normal Baremetal Setup
If vGPU can you share the driver installed at guest and hostThanksHi @vbilousCan you perhaps once enable UVM and let me know if the issue still persists
Doc Reference : Virtual GPU Software User Guide :: NVIDIA Virtual GPU Software DocumentationThanksPowered by Discourse, best viewed with JavaScript enabled"
257,how-can-i-convert-tensorflow-object-detection-model-ssd-mobilenet-to-tensorrt-model-for-inference-on-jetson-tx1,"Hello
I have trained (Tuned) the custom tensorflow object detection model from the custom dataset and I want to convert it to a TensorRT model for inferencing on Jetson TX1 Board.
I have to try inferencing without converting it to TensorRT but I get 3 or 4 FPS on board which is very slow…
I have to convert this model to the TensorRT model.
how can I do that?
I just have some weight and checkpoint files and nothing more…
can anyone help me with how can I convert the model?TensorRT Version: 8.2.1
GPU Type: Jetson TX 1
Nvidia Driver Version: I don’t know
CUDA Version: 10.2
CUDNN Version: 8.2.1
Operating System + Version: Ubuntu 18.04 (L4T 32.7.1)
Python Version (if applicable): 3.6Hi,This looks like a Jetson issue. Please refer to the below samples in case useful.Hello AI World guide to deploying deep-learning inference networks and deep vision primitives with TensorRT and NVIDIA Jetson. - GitHub - dusty-nv/jetson-inference: Hello AI World guide to deployin...An easy to use PyTorch to TensorRT converter. Contribute to NVIDIA-AI-IOT/torch2trt development by creating an account on GitHub.For any further assistance, we will move this post to to Jetson related forum.Thanks!@NVES
Actually, I ask this issue in one of the topics on Jetson Community and they say “this is an AI issue” 😄😄
anyway, thanks for your help and links.
if I have any questions, I will ask you about them.
Good Luck 🌸✋@NVES
thank you for your response
I have run some ssd-mobilenet models on the docker container.
and it was very fast (almost 50fps)
but I want to train a model on the custom dataset and convert that to a TensorRT model for fast inferencing.
what can I do?Hi @Hamzeh.nv ,
You can check on the below approaches to proceed with tensorflow model to trt .
1 - You can try using tf-trt conversion using the mentioned link.
2 - You can try converting your tf model to onnx and the further do the onnx to trt conversion.
Thank you.Powered by Discourse, best viewed with JavaScript enabled"
258,riva-quickstart-2-11-fails-on-xavier-nx,"Please provide the following information when requesting support.Hardware - Jetson Xavier NX
Hardware - CPU
Operating System jetpack 5.1.1
Riva Version 2.11.0
TLT Version (if relevant)
How to reproduce the issue ? (This is for errors. Please share the command and the detailed log here)
$ sudo bash riva_init.sh
[sudo] password for valentin:
Please enter API key for ngc.nvidia.com:
Logging into NGC docker registry if necessary…
Pulling required docker images if necessary…
Note: This may take some time, depending on the speed of your Internet connection.Pulling Riva Speech Server images.
Image nvcr.io/nvidia/riva/riva-speech:2.11.0-l4t-aarch64 exists. Skipping.Downloading models (RMIRs) from NGC…
Note: this may take some time, depending on the speed of your Internet connection.
To skip this process and use existing RMIRs set the location and corresponding flag in config.sh.[[ tegra != \t\e\g\r\a ]][[ tegra == \t\e\g\r\a ]]‘[’ -d ‘/home/valentin/Downloads/riva3/files(2)/model_repository/rmir’ ‘]’[[ tegra == \t\e\g\r\a ]]‘[’ -d ‘/home/valentin/Downloads/riva3/files(2)/model_repository/prebuilt’ ‘]’echo ‘Converting prebuilts at /home/valentin/Downloads/riva3/files(2)/model_repository/prebuilt to Riva Model repository.’
Converting prebuilts at /home/valentin/Downloads/riva3/files(2)/model_repository/prebuilt to Riva Model repository.docker run -it -d --rm -v ‘/home/valentin/Downloads/riva3/files(2)/model_repository:/data’ --name riva-models-extract nvcr.io/nvidia/riva/riva-speech:2.11.0-l4t-aarch64docker exec riva-models-extract bash -c ‘mkdir -p /data/models; 
for file in /data/prebuilt/*.tar.gz; do tar xf $file -C /data/models/ &> /dev/null; done’docker container stop riva-models-extract‘[’ 0 -ne 0 ‘]’echoecho ‘Riva initialization complete. Run ./riva_start.sh to launch services.’
Riva initialization complete. Run ./riva_start.sh to launch services.
valentin@ubuntu:~/Downloads/riva3/files(2)$ sudo bash riva_start.sh
Starting Riva Speech Services. This may take several minutes depending on the number of models deployed.
Waiting for Riva server to load all models…retrying in 10 seconds
Waiting for Riva server to load all models…retrying in 10 seconds
Waiting for Riva server to load all models…retrying in 10 seconds
Waiting for Riva server to load all models…retrying in 10 seconds
Waiting for Riva server to load all models…retrying in 10 seconds
Waiting for Riva server to load all models…retrying in 10 seconds
Waiting for Riva server to load all models…retrying in 10 seconds
Waiting for Riva server to load all models…retrying in 10 seconds
Waiting for Riva server to load all models…retrying in 10 seconds
Waiting for Riva server to load all models…retrying in 10 seconds
Waiting for Riva server to load all models…retrying in 10 seconds
Waiting for Riva server to load all models…retrying in 10 seconds
Waiting for Riva server to load all models…retrying in 10 seconds
Waiting for Riva server to load all models…retrying in 10 seconds
Waiting for Riva server to load all models…retrying in 10 seconds
Waiting for Riva server to load all models…retrying in 10 seconds
Waiting for Riva server to load all models…retrying in 10 seconds
Waiting for Riva server to load all models…retrying in 10 seconds
Waiting for Riva server to load all models…retrying in 10 seconds
Waiting for Riva server to load all models…retrying in 10 seconds
Waiting for Riva server to load all models…retrying in 10 seconds
Waiting for Riva server to load all models…retrying in 10 seconds
Waiting for Riva server to load all models…retrying in 10 seconds
Waiting for Riva server to load all models…retrying in 10 seconds
Waiting for Riva server to load all models…retrying in 10 seconds
Waiting for Riva server to load all models…retrying in 10 seconds
Waiting for Riva server to load all models…retrying in 10 seconds
Waiting for Riva server to load all models…retrying in 10 seconds
Waiting for Riva server to load all models…retrying in 10 seconds
Waiting for Riva server to load all models…retrying in 10 seconds
Health ready check failed.
Check Riva logs with: docker logs riva-speechHI @exaflo16Thanks for your interest in RivaApologies you are facing issue,Request to kindly share the output of command
docker logs riva-speech
and share with us the config.sh usedWe have seen riva_start in past with customer due to Jetpack Version, you JetPack version seems to be correct  - JetPack 5.1.1One Check,can you kindly verify the default runtime is set to nvidiaYou have set the default runtime to nvidia on the Jetson platform by adding the following line in the /etc/docker/daemon.json file. Restart the Docker service using sudo systemctl restart docker after editing the file.""default-runtime"": ""nvidia""Thankscan you kindly verify the default runtime is set to nvidiahere and the default time is nvidia:sudo docker logs riva-speech
[sudo] password for valentin:NVIDIA Release 23.04 (build 59018715)Copyright (c) 2018-2023, NVIDIA CORPORATION & AFFILIATES. All rights reserved.Various files include modifications (c) NVIDIA CORPORATION & AFFILIATES.  All rights reserved.This container image and its contents are governed by the NVIDIA Deep Learning Container License.
By pulling and using the container, you accept the terms and conditions of this license:145.24 KBNVIDIA Tegra driver detected.Warning: ‘–strict-model-config’ has been deprecated! Please use ‘–disable-auto-complete-config’ instead.Riva waiting for Triton server to load all models…retrying in 1 second
Error: Failed to initialize NVML
W0609 14:54:17.102185 109 metrics.cc:785] DCGM unable to start: DCGM initialization error
I0609 14:54:17.103258 109 metrics.cc:757] Collecting CPU metrics
I0609 14:54:17.104303 109 tritonserver.cc:2264]
±---------------------------------±-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+
| Option                           | Value                                                                                                                                                                                                |
±---------------------------------±-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+
| server_id                        | triton                                                                                                                                                                                               |
| server_version                   | 2.27.0                                                                                                                                                                                               |
| server_extensions                | classification sequence model_repository model_repository(unload_dependents) schedule_policy model_configuration system_shared_memory cuda_shared_memory binary_tensor_data statistics trace logging |
| model_control_mode               | MODE_NONE                                                                                                                                                                                            |
| strict_model_config              | 1                                                                                                                                                                                                    |
| rate_limit                       | OFF                                                                                                                                                                                                  |
| pinned_memory_pool_byte_size     | 268435456                                                                                                                                                                                            |
| cuda_memory_pool_byte_size{0}    | 1000000000                                                                                                                                                                                           |
| response_cache_byte_size         | 0                                                                                                                                                                                                    |
| min_supported_compute_capability | 6.0                                                                                                                                                                                                  |
| strict_readiness                 | 1                                                                                                                                                                                                    |
| exit_timeout                     | 30                                                                                                                                                                                                   |
±---------------------------------±-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+I0609 14:54:17.104423 109 server.cc:261] No server context available. Exiting immediately.
error: creating server: Invalid argument - --model-repository must be specifiedRiva waiting for Triton server to load all models…retrying in 1 second
Triton server died before reaching ready state. Terminating Riva startup.
Check Triton logs with: docker logs
/opt/riva/bin/start-riva: line 1: kill: (109) - No such processconfig.shriva_target_gpu_family=“non-tegra”riva_tegra_platform=“orin”service_enabled_asr=true
service_enabled_nlp=true
service_enabled_tts=true
service_enabled_nmt=truelanguage_code=(“en-US”)asr_acoustic_model=(“conformer”)gpus_to_use=“device=0”MODEL_DEPLOY_KEY=“tlt_encode”riva_model_loc=“riva-model-repo”if [[ $riva_target_gpu_family == “tegra” ]]; then
riva_model_loc=“pwd/model_repository”
fiuse_existing_rmirs=falseriva_speech_api_port=“50051”riva_ngc_org=“nvidia”
riva_ngc_team=“riva”
riva_ngc_image_version=“2.11.0”
riva_ngc_model_version=“2.11.0”########## ASR MODELS ##########models_asr=()for lang_code in ${language_code[@]}; do
modified_lang_code=“${lang_code/-/_}”
modified_lang_code=${modified_lang_code,}donemodels_asr+=()########## NLP MODELS ##########if [[ $riva_target_gpu_family == “tegra” ]]; then
models_nlp=()
else
models_nlp=()
fi########## TTS MODELS ##########if [[ $riva_target_gpu_family == “tegra” ]]; then
models_tts=()
else
models_tts=()
fi######### NMT models ###############models_nmt=(#“${riva_ngc_org}/${riva_ngc_team}/rmir_nmt_en_de_24x6:${riva_ngc_model_version}”
#“${riva_ngc_org}/${riva_ngc_team}/rmir_nmt_en_es_24x6:${riva_ngc_model_version}”
#“${riva_ngc_org}/${riva_ngc_team}/rmir_nmt_en_zh_24x6:${riva_ngc_model_version}”
#“${riva_ngc_org}/${riva_ngc_team}/rmir_nmt_en_ru_24x6:${riva_ngc_model_version}”
#“${riva_ngc_org}/${riva_ngc_team}/rmir_nmt_en_fr_24x6:${riva_ngc_model_version}”
#“${riva_ngc_org}/${riva_ngc_team}/rmir_nmt_de_en_24x6:${riva_ngc_model_version}”
#“${riva_ngc_org}/${riva_ngc_team}/rmir_nmt_es_en_24x6:${riva_ngc_model_version}”
#“${riva_ngc_org}/${riva_ngc_team}/rmir_nmt_ru_en_24x6:${riva_ngc_model_version}”
#“${riva_ngc_org}/${riva_ngc_team}/rmir_nmt_zh_en_24x6:${riva_ngc_model_version}”
#“${riva_ngc_org}/${riva_ngc_team}/rmir_nmt_fr_en_24x6:${riva_ngc_model_version}”#“${riva_ngc_org}/${riva_ngc_team}/rmir_nmt_en_deesfr_24x6:${riva_ngc_model_version}”
#“${riva_ngc_org}/${riva_ngc_team}/rmir_nmt_en_deesfr_12x2:${riva_ngc_model_version}”
#“${riva_ngc_org}/${riva_ngc_team}/rmir_nmt_deesfr_en_24x6:${riva_ngc_model_version}”
#“${riva_ngc_org}/${riva_ngc_team}/rmir_nmt_deesfr_en_12x2:${riva_ngc_model_version}”#“${riva_ngc_org}/${riva_ngc_team}/rmir_megatronnmt_any_en_500m:${riva_ngc_model_version}”
#“${riva_ngc_org}/${riva_ngc_team}/rmir_megatronnmt_en_any_500m:${riva_ngc_model_version}”
)NGC_TARGET=${riva_ngc_org}
if [[ ! -z ${riva_ngc_team} ]]; then
NGC_TARGET=“${NGC_TARGET}/${riva_ngc_team}”
else
team=“""""”
fissl_server_cert=“”
ssl_server_key=“”
ssl_root_cert=“”image_speech_api=“nvcr.io/${NGC_TARGET}/riva-speech:${riva_ngc_image_version}”image_init_speech=“nvcr.io/${NGC_TARGET}/riva-speech:${riva_ngc_image_version}-servicemaker”riva_daemon_speech=“riva-speech”
if [[ $riva_target_gpu_family != “tegra” ]]; then
riva_daemon_client=“riva-client”
fican you please answer?Powered by Discourse, best viewed with JavaScript enabled"
259,eglstream-cuda-cv-gpumat-using-argus-nppi,"I’m trying to retrieve a cv::cuda::GpuMat from CUDA EGLStream.My code is based on jetson_multimedia_api/argus/samples/syncSensorThe current setting of my stream isI’ve made some modifications on ScopedCudaEGLStreamFrameAcquire::generateHistogramThe function terminates successfully, but throws an error on the next call at gpuMat.create()I haven’t called any OpenCV function outside generateHistogram()Any help will be appreciated!I’ve just noticed there’re some OpenCV codes left.After removing them, the code fails on the second execution with NPP_CUDA_KERNEL_EXECUTION_ERRORA small update on my progress so far.unspecified launch failure is returned by cuGraphicsResourceGetMappedEglFrame().The ret val of cuGraphicsResourceGetMappedEglFrame() is 719, which is CUDA_ERROR_LAUNCH_FAILEDWhen nppiNV21ToBGR_8u_P2C4R_Ctx() is called to convert a CUeglFrame into cv::cuda::GpuMat (BGR),
cuEGLStreamConsumerReleaseFrame() fails.I then tried to gpuMat.download(cpuMat) and cv::imwrite(cpuMat), it throws unspecified launch failure as well.I’ve attached my code for anyone interested in.Always, appreciate your help!syncSensor.zip (11.4 KB)Hi, I’m also try to do the same job (Argus → gpumat).
I’m really curious what happened since the last comment.
did you have some good result after all?Powered by Discourse, best viewed with JavaScript enabled"
260,is-there-any-way-to-install-a-higher-version-than-8-0-of-tensorrt-through-tar-without-changing-the-cuda11-3-version,"I want to install TensoRT version greater than 8.0 through tar. The corresponding version of cuda is 11.3, but the version of the tar package provided by cuda11.3 on the official website is only 8.0. Is there any way to install a higher version of TensorRT through tar without changing the cuda version?Hi ,
We recommend you to check the supported features from the below link.These support matrices provide a look into the supported platforms, features, and hardware capabilities of the NVIDIA TensorRT 8.6.1 APIs, parsers, and layers.
You can refer below link for all the supported operators list.
For unsupported operators, you need to create a custom plugin to support the operationThanks!Powered by Discourse, best viewed with JavaScript enabled"
261,tensorrt-efficientnms-plugin-fp16-inconsistent-but-valid-results,"Hello!I encountered an issue with the EfficientNMS plugin and its FP16 mode and I’m wondering whether this can be treated as a bug or it is somehow expected/can be explained.Running the inference multiple times on the same image in the float16 mode very often results in a slightly different output, one or few pixels off. Example:
1st run [104 x 75 from (142, 156)] vs  2nd run [105 x 73 from (141, 158)] with same confidence score.
Also sometimes the order of sorted bounding boxes with the same score is different than in the previous iteration.I consider the detections valid as they are pretty close. This is a problem for my testing plan as I assumed that when re-using the same TensorRT plan file I would always get deterministic results which has been the case until now.
Are there any ideas how to explain this behavior?
If it’s a bug I’ll try to prepare reproduction using publicly available models/data.Isolation:Thank you!TensorRT Version: 8.4 - 8.5.1.7 ( previous versions don’t work because of other issue )
GPU Type: RTX2070
Nvidia Driver Version: 525.60.13
CUDA Version: 11.8
CUDNN Version: 8.6.0
Operating System + Version: Ubuntu 18.04
Python Version (if applicable):
TensorFlow Version (if applicable):
PyTorch Version (if applicable):
Baremetal or Container (if container which image + tag): baremetalHi,We are checking on this issue internally.
Could you please share with us minimal issue repro model/script for better debugging.Thank you.Thank you for the reply.
I will try to create some minimum setup to reproduce this issue based on publicly available data. Please allow me some time as I cannot share our projects code/models here.Hi,I created a small app to show the problem. What it does:In my case I get different results almost each run for the boxes output.Thanks!repro_fp16_nms_issue.tar.gz (613.8 KB)@spolisetty Hi! Did you have a chance to check the issue internally? Thanks
KamilPowered by Discourse, best viewed with JavaScript enabled"
262,conda-env-compatible-nvidia-driver-cuda-cuddn-version,"Hi, I was wondering what is the role of conda environment in cuda and cudnnLets describe:
I’m on ubuntu 20.04 and i have installed latest Nvidia Driver (535), then i’ve used CUDA toolkit runfile for installing CUDA 11.2$ nvidia-smi
Sun Jul  2 13:21:13 2023
±--------------------------------------------------------------------------------------+
| NVIDIA-SMI 535.54.03              Driver Version: 535.54.03    CUDA Version: 12.2     |
|-----------------------------------------±---------------------±---------------------+
| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |
|                                         |                      |               MIG M. |
|=========================================+======================+======================|
|   0  NVIDIA GeForce GTX 1080        Off | 00000000:01:00.0 Off |                  N/A |
| N/A   34C    P8               4W / 200W |    320MiB /  8192MiB |      0%      Default |
|                                         |                      |                  N/A |
±----------------------------------------±---------------------±---------------------+
|   1  NVIDIA GeForce GTX 1080        Off | 00000000:02:00.0 Off |                  N/A |
| N/A   38C    P8               4W / 200W |      8MiB /  8192MiB |      0%      Default |
|                                         |                      |                  N/A |
±----------------------------------------±---------------------±---------------------+±--------------------------------------------------------------------------------------+
| Processes:                                                                            |
|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |
|        ID   ID                                                             Usage      |
|=======================================================================================|
|    0   N/A  N/A      1059      G   /usr/lib/xorg/Xorg                           84MiB |
|    0   N/A  N/A      1395      G   /usr/bin/gnome-shell                         55MiB |
|    0   N/A  N/A      4226      C   …/sl/miniconda3/envs/pose/bin/python      176MiB |
|    1   N/A  N/A      1059      G   /usr/lib/xorg/Xorg                            4MiB |
±--------------------------------------------------------------------------------------+So Nvidia Driver is installed and confirmed! for the cuda toolkit i can also confirm via:$nvcc -V
nvcc: NVIDIA (R) Cuda compiler driver
Copyright (c) 2005-2021 NVIDIA Corporation
Built on Sun_Feb_14_21:12:58_PST_2021
Cuda compilation tools, release 11.2, V11.2.152
Build cuda_11.2.r11.2/compiler.29618528_0it shows that version 11.2 is successfully installed.
After these i get into the cudnn installation by downloading and extracting the TAR file into /usr/local
( where the cuda is installed {/usr/local/cuda-11.2}, So that cudnn*.h files and libcuddn copyed into the /usr/local/cuda/{include,lib64}i wont be able to confirm the installation by
$ dpkg -l libcudnn*
and
$ cat /usr/include/cudnn_version.h | grep CUDNN_MAJOR -A 2define CUDNN_VERSION (CUDNN_MAJOR * 1000 + CUDNN_MINOR * 100 + CUDNN_PATCHLEVEL)endif /* CUDNN_VERSION_H */Also i added
export LD_LIBRARY_PATH=/usr/local/cuda/lib64:$LD_LIBRARY_PATH
export PATH=/usr/local/cuda/bin:$PATHafter that i have created a conda environment using conda create -n test python=3.10So it creates an environment while there are no way for me to test the real cudnn version, because /usr/src/samples are not available!(is there any way to test the cudnn?then i searched the net and found a code that can be used to test the cudnn as bellow:CODE:
“”“”“”“”“”“”“”“”“”“”""
import numpy as np
import ctypescudnn_lib = ctypes.cdll.LoadLibrary(‘libcudnn.so.8’)cudnnDataType_t = ctypes.c_void_p
cudnnTensorDescriptor_t = ctypes.c_void_p
cudnnConvolutionDescriptor_t = ctypes.c_void_p
cudnnFilterDescriptor_t = ctypes.c_void_p
cudnnConvolutionFwdAlgo_t = ctypes.c_void_pcudnnDataType = {
‘float32’: 0,
‘float64’: 1,
‘float16’: 2,
‘int8’: 3,
‘int32’: 4,
‘int8x4’: 5,
‘uint8’: 6,
‘uint8x4’: 7
}cudnnConvolutionFwdAlgo = {
‘implicit_gemm’: 0,
‘implicit_precomp_gemm’: 1,
‘gemm’: 2,
‘direct’: 3,
‘fft’: 4,
‘fft_tiling’: 5,
‘winograd’: 6,
‘winograd_nonfused’: 7,
‘count’: 8
}input_dim = (1, 3, 5, 5)
filter_dim = (2, 3, 3, 3)input_data = np.random.randn(*input_dim).astype(np.float32)
filter_data = np.random.randn(*filter_dim).astype(np.float32)input_desc = cudnnTensorDescriptor_t()
cudnn_lib.cudnnCreateTensorDescriptor(ctypes.byref(input_desc))
cudnn_lib.cudnnSetTensor4dDescriptor(input_desc,  # tensor descriptor
cudnnDataType[‘float32’],  # data type
input_dim[0],  # batch size
input_dim[1],  # channels
input_dim[2],  # height
input_dim[3])  # widthfilter_desc = cudnnFilterDescriptor_t()
cudnn_lib.cudnnCreateFilterDescriptor(ctypes.byref(filter_desc))
cudnn_lib.cudnnSetFilter4dDescriptor(filter_desc,  # filter descriptor
cudnnDataType[‘float32’],  # data type
filter_dim[0],  # output channels
filter_dim[1],  # input channels
filter_dim[2],  # filter height
filter_dim[3])  # filter widthconv_desc = cudnnConvolutionDescriptor_t()
cudnn_lib.cudnnCreateConvolutionDescriptor(ctypes.byref(conv_desc))
cudnn_lib.cudnnSetConvolution2dDescriptor(conv_desc,  # convolution descriptor
1,  # pad height
1,  # pad width
1,  # vertical stride
1,  # horizontal stride
1,  # dilation height
1,  # dilation width
1,  # mode (CuDNN_CONVOLUTION)
cudnnDataType[‘float32’])  # data typeoutput_dim = (input_dim[0], filter_dim[0], input_dim[2]-filter_dim[2]+1, input_dim[3]-filter_dim[3]+1)output_data = np.zeros(output_dim, dtype=np.float32)output_desc = cudnnTensorDescriptor_t()
cudnn_lib.cudnnCreateTensorDescriptor(ctypes.byref(output_desc))
cudnn_lib.cudnnSetTensor4dDescriptor(output_desc,  # tensor descriptor
cudnnDataType[‘float32’],  # data type
output_dim[0],  # batch size
output_dim[1],  # channels
output_dim[2],  # height
output_dim[3])  # widthconv_algo = cudnnConvolutionFwdAlgo_t()
cudnn_lib.cudnnGetConvolutionForwardAlgorithm(ctypes.byref(conv_algo),
input_desc,
filter_desc,
conv_desc,
output_desc,
cudnnConvolutionFwdAlgo[‘fft’],
0,
0)cudnn_lib.cudnnConvolutionForward(cudnn_lib._handle,  # CuDNN handle
ctypes.byref(ctypes.c_float(1.0)),  # alpha
input_desc,  # input tensor descriptor
input_data.ctypes.data,  # input data
filter_desc,  # filter descriptor
filter_data.ctypes.data,  # filter data
conv_desc,  # convolution descriptor
conv_algo,  # convolution algorithm
ctypes.byref(ctypes.c_void_p(0)),  # workspace
0,  # workspace size
ctypes.byref(ctypes.c_float(0.0)),  # beta
output_desc,  # output tensor descriptor
output_data.ctypes.data)  # output datacudnn_lib.cudnnDestroyTensorDescriptor(input_desc)
cudnn_lib.cudnnDestroyFilterDescriptor(filter_desc)
cudnn_lib.cudnnDestroyConvolutionDescriptor(conv_desc)
cudnn_lib.cudnnDestroyTensorDescriptor(output_desc)“”“”“”“”“”“”“”“”“”“”“”“”“”“”“”“”“”""but it produce an error that indicate the cudnn is not installed!AttributeError                            Traceback (most recent call last)
Cell In[4], line 97
95 # Initialize CuDNN convolution forward algorithm
96 conv_algo = cudnnConvolutionFwdAlgo_t()
—> 97 cudnn_lib.cudnnGetConvolutionForwardAlgorithm(ctypes.byref(conv_algo),
98                                                input_desc,
99                                                filter_desc,
100                                                conv_desc,
101                                                output_desc,
102                                                cudnnConvolutionFwdAlgo[‘fft’],
103                                                0,
104                                                0)
106 # Perform convolution using CuDNN
107 cudnn_lib.cudnnConvolutionForward(cudnn_lib._handle,  # CuDNN handle
108                                   ctypes.byref(ctypes.c_float(1.0)),  # alpha
109                                   input_desc,  # input tensor descriptor
(…)
118                                   output_desc,  # output tensor descriptor
119                                   output_data.ctypes.data)  # output dataFile ~/miniconda3/envs/pose/lib/python3.10/ctypes/init.py:387, in CDLL.getattr(self, name)
385 if name.startswith(‘‘) and name.endswith(’’):
386     raise AttributeError(name)
 → 387 func = self.getitem(name)
…
 → 392     func = self._FuncPtr((name_or_ordinal, self))
393     if not isinstance(name_or_ordinal, int):
394         func.name = name_or_ordinalAttributeError: /usr/local/cuda-11.2/targets/x86_64-linux/lib/libcudnn.so: undefined symbol: cudnnGetConvolutionForwardAlgorithmAfter all, my purpose is to use cuda version 11.2 with cudnn 8.1.0.77 for a project that requires these specific versions! it is better for me to install it in the env instead of the base system!Can anyone help me to figure this out?I also have another question about the frameworks!As i also want to use pytorch in my conda env, does the base system needs to have the same version of the pythorch cuda?for example, as i installed cuda 11.2, but the latest version of pythorch installes cuda 11.8, do they have conflicts? should those be the same? how about the cudnn version?I’m not sure what is the relationship between conda environment cuda, cudnn version with the base system ( while conda env is deactivated) cuda, cudnn version!Thanks
Best regards@AakankshaS @Ming_Q @nadeemm
can you explain it please?Powered by Discourse, best viewed with JavaScript enabled"
263,linux-repository-key-rotation-most-repo-users-will-have-to-update-keys,"If you are using the CUDA Linux repos for accessing CUDA Toolkit or many of our libraries you will need to take action to update the keys, full details in this post:Notice: CUDA Linux Repository Key Rotation - CUDA / CUDA Setup and Installation - NVIDIA Developer ForumsPowered by Discourse, best viewed with JavaScript enabled"
264,the-engine-plan-file-is-generated-on-an-incompatible-device-expecting-compute-6-1-got-compute-8-6-please-rebuild,"Error:the engine plan file is generated on an incompatible device expecting compute 6.1 got compute 8.6,please rebuild.
engine.cpp::nvinfer1::rt::deserializeEngine::934] Error code 2:Internal Error (Assertion engine->deserialize (start,size,allocator,runtime) failed).1.In Win10 environment, I used a graphics card RTX3090, which has 8.6 arithmetic power, and I trained the model and generated an engine model from the generated pt model via tensorrt to facilitate accelerated inference.
But I deployed it on RTX1060, which has 6.1 arithmetic power. running exe gives me the same error as posted above.
I have kept the cuda version all the time, and the training and deployment computers are on version 12 of cuda.
2.How should I deal with this error, can I make it compatible with RTX1060 by setting the arithmetic power in the process of generating the engine with tenosorrt? Or do I have to generate it again on the RTX1060?Hi,The generated TensorRT engine files are not portable across platforms or TensorRT versions. Plans are specific to the exact GPU model they were built on (in addition to the platforms and the TensorRT version) and must be rebuilt on the specific GPU in case you want to run them on a different GPU.TensorRT 8.6 and later versions support hardware and version compatibility.
Please refer to the following document, which may help you resolve the above issue:This is the revision history of the NVIDIA TensorRT 8.6 Developer Guide.Thank you.I actually tested it. Accelerated inference compatibility is still there, I tested it on 3060 as long as the cuda version is the same, it is possible to run. Because the 3060 and 3090 have the same arithmetic power, both are 8.6.
I hope to help others by putting the record here.
Thank you.Powered by Discourse, best viewed with JavaScript enabled"
265,estimated-processing-power-required-for-l5-driving-and-how-many-tops-is-sufficient,"Hello All,Can Nvidia drive orin soc with 254 TOPS used to achieve L5 driving. How much processing power will be required for L5 driving. I have searched in the Nvidia website the Orin with 254 TOPS can be used in L5 driving but in sources its written as to achieve we need up to one or more PetaFLOPS at L4/L5.Can you please provide me the rough estimated TOPS to achieve L5 driving or some documents in which I can refer.Thank you in adavance.Best regards
HarsihHi @harish.ujjeliThis forum is for NVIDIA cuOpt, the one you are looking for I believe is this:Powered by Discourse, best viewed with JavaScript enabled"
266,how-to-set-parameters-of-batchednmsplugin,"I have a model that is exported to ONNX with BatchedNMSPlugin, I then load this onnx and convert to TRT to execute. Is it possible to change the parameters (eg. confidence threshold) of BatchedNMSPlugin at runtime? eg. after the TRT has been built?I see there is TensorRT: nvinfer1::plugin::NMSParameters Struct Reference but can’t find any examples of use, also not clear if this is only for exporting.Hi,
Please refer to below links related custom plugin implementation and sample:This Samples Support Guide provides an overview of all the supported NVIDIA TensorRT 8.6.0 Early Access (EA) samples included on GitHub and in the product package. The TensorRT samples specifically help in areas such as recommenders, machine...TensorRT is an SDK for high performance, deep learning inference. It includes a deep learning inference optimizer and a runtime that delivers low latency and high throughput for deep learning…While IPluginV2 and IPluginV2Ext interfaces are still supported for backward compatibility with TensorRT 5.1 and 6.0.x respectively, however, we recommend that you write new plugins or refactor existing ones to target the IPluginV2DynamicExt or IPluginV2IOExt interfaces instead.Thanks!thanks for reply. I’m not trying to implement or modify a custom plugin, I’m trying to set the parameters of an existing plugin after the model has been exported to ONNX or built into a trt engine, even. I’m not sure if these links are relevant to that?Hi @lukee2ni6 ,Is it possible to change the parameters (eg. confidence threshold) of BatchedNMSPlugin at runtime? eg. after the TRT has been built?You can try reading it from environment variable inside the enqueue function, or can try reading it from any configuration file.
Please let us know if this works?ThanksPlease let us know if this works?My question is how to do this? Is there any documentation or example? Just to reemphasise - this is not a plugin I have written, it is an off the shelf plugin that comes with TRT and my question is how can it be configured without having to re-export the ONNX and rebuild the TRT from pytorchHi @lukee2ni6 ,
Apologies for the delayed response.
You may need to access and modify the already available plugin code during inference to do the desired runtime changes.ThanksPowered by Discourse, best viewed with JavaScript enabled"
267,kernel-launch-implemented-in-tensorrt-plugin-is-very-slow,"I found that kernel launch implemented in TensorRT Plugin is very slow.
For checking this problem clearly, I test kernel launch with/without tensorrt(plugin) using simply matrix multiplication(AB=C) kernel with same grid/block dimensions.And I found for-loop in kernel causes performance drop.
Is it a bug ?Test condition is below:Profiling result is here:TensorRT Version: 8.4.1.5 (i test on all newer versions, but result is same; 8.5.1.7, 8.5.2.2, 8.5.3.1, 8.6.0.12)
GPU Type: RTX 3080
Nvidia Driver Version: 525.85.12
CUDA Version: CUDA 11.8
CUDNN Version:  cuDNN 8.6.0
Operating System + Version: ubuntu 20.04.5 LTS
Python Version (if applicable):
TensorFlow Version (if applicable):
PyTorch Version (if applicable):
Baremetal or Container (if container which image + tag):matmul.cu (5.0 KB)
matmul_dynamic.h (3.0 KB)
matmul_dynamic.cu (7.4 KB)Refer attached filesHi,Request you to share the model, script, profiler, and performance output if not shared already so that we can help you better.Alternatively, you can try running your model with trtexec command.master/samples/trtexecNVIDIA® TensorRT™, an SDK for high-performance deep learning inference, includes a deep learning inference optimizer and runtime that delivers low latency and high throughput for inference applicat...While measuring the model performance, make sure you consider the latency and throughput of the network inference, excluding the data pre and post-processing overhead.
Please refer to the below links for more details:This Best Practices Guide covers various performance considerations related to deploying networks using TensorRT 8.0.3. These sections assume that you have a model that is working at an appropriate level of accuracy and that you are able to...This Best Practices Guide covers various performance considerations related to deploying networks using TensorRT 8.0.3. These sections assume that you have a model that is working at an appropriate level of accuracy and that you are able to...Thanks!matmul-kernel-test.nsys-rep (351.4 KB)
matmul-plugin-test.nsys-rep (406.4 KB)I attach profiling output files. (nsight system version is 2022.4.2)This problem is caused by wrong usage of cmake script. (resolved)This topic was automatically closed 14 days after the last reply. New replies are no longer allowed.Powered by Discourse, best viewed with JavaScript enabled"
268,welcome-to-nvidia-maxine,"Hello @Andrey1984.There are two ways to utilize the capabilities of Maxine.  One is using the Broadcast App, which does not have a broadcasting capability in itself, but can be used with thousands of different applications as a source for broadcasting.   The NVIDIA Broadcast App allows you to select a source and creates a virtual device (camera or microphone) that can be used in a variety of apps such as video conferencing, OBS, and others.   The app is only available for Windows.The other way to use the Maxine capabilities is via the SDKs.   None of the SDKs have built in broadcasting capabilities, they provide the AI processing capabilities and can be integrated into broadcast applications.  There are different input, processing and output resolution capabilities for the different SDKs, but in general, yes, 4k resolution is supported.   One example of this is Super Resolution, which supports conversion of 1080 video into 4k.  The SDKs are available for both Windows and Linux.  They are currently only supported on x86 with Turing and Ampere GPUs with Tensor Cores.You are correct, the Broadcast in the name is referring to use in broadcasting applications, not in providing the broadcasting capability itself.  There are many organizations with whom we’ve collaborated to integrate these capabilities into their broadcasting apps.we are asked to look into project proposal that is based on MaxineCould you advise on feasibility of the requirements? Will it be possible to implement it with Maxine? Will you support such development effort?
ThanksAndrey, you can implement Maxine capabilities either on the client or the server.   From your message, it seems that you want to implement Maxine on the server.  This would enable any device, whether it has a GPU or not, to have the background removed through the Maxine SDK.   To do this, you need a video conferencing infrastructure that is an MCU.   An SFU infrastructure would not allow you to access the video to run it through the SDK and remove the background or add other Maxine quality improvement or effects.@jkrinitt
thank you for following up
could you extend MCU/SFU meaning please?
what is MFU
what is SFU?
Given there is  a cloud server with supported GPU how to start implementing?@jkrinitt we can not install broadcast app on Windows 10 Pro with T4;
is that particular envirnment not supported? for the broadcast app? for video sdk? for both? neither of the two?

1624050003070.remmina-2021-6-19-23:58:56.0810031920×1080 48.2 KB


1624050003070.remmina-2021-6-20-0:0:28.4936461920×1080 88 KB

this one is Quadro RTX solutionu by Nvidia offered at Google CloudSpend smart, procure faster and retire committed Google Cloud spend with Google Cloud Marketplace. Browse the catalog of over 2000 SaaS, VMs, development stacks, and Kubernetes apps optimized to run on Google Cloud.Seriously, someone use Deep Learning to create an automated cloth weight painting application…I just spent an entire week trying to perfect the weight map of one skirt…Hi @Andrey1984 to know more about MCU and SFU architectures for video conferencing, you can check this linkHi all,Good to see you helping out each other here. Since we don’t find a Audio post we post here in the general topics here.We are a start-up with a focus of deploying a new-age AR chatroom project on the cloud. We tried out several technologies and zeroed-in on NVIDIA Maxine and its siblings. We downloaded the resources and configured the binaries to our needs, but have been facing trouble with the NVIDIA Audio SDK. We tried the Samples codes on our GCP instances.Though the binaries write out the processed audio files almost all the time, the execution ends with an error “NvAFX_Run() failed”(Ref. tailing error log)Since the execution control depnds almost entirely on configurations we just tried on other instances and found the same error. Our local machine is of a Pascal architecture and so unsuitable for the Maxine technology. We are in a confusion as to how to move ahead. We have enough know-how in CUDA and other matters but since the code is not available we could not proceed. We posted on the Developer’s forum but there is not much response.We would like to request you to kindly show us some light with regard to this issue so that we can stick on to Maxine and its promising capabilities.Best Regards,
Muthu
(For Kickback.Space Inc.,)Hi @asawarkar
Thank you for your response.
I understand that nvidia won’t support / share an example for neither MCU, nor SFU, right?“NvAFX_Run() failed”(Ref. tailing error log)Hello @muthu3Thank you for trying out NVIDIA Maxine Audio SDK. Unfortunately, Maxine SDKs don’t work on Pascal architecture. I’d encourage you to try running the SDK on either of these hardware platforms: V100, T4, A10, A100, A30. Maxine SDK docker containers are now available with pre-packaged dependencies which are much more convenient to pull from NGC and run on your cloud instance. For Maxine Audio SDK, please check this link and download the containers from NGC. Let me know if this helps thanks!@Andrey1984 There is an early access version of MCU reference pipeline with a sample video conferencing application with NVIDIA Deepstream container. You’d have to apply for early access here: link@asawarkar the lins that is shared by you opens the download page
could you extend which exactly file to download to try the MCU?Hi @asawarkar , Thanks a lot for the response. I had just mentioned that since my machine is a Pascal I did not try Maxine on it. I tried it on GCP with Tesla T4 GPUs. Thanks for your suggestion to try out the Docker versions. I will check out and get back with a hopefully positive note.You may want to spin up a new topic to discuss this. I think others will be interested in this discussion.Excuse me for cutting in.
When will  DeepStream Maxine containers be access?
I  applied for early access.  I can’t find it in the NGC catalog.
There are two containers.
Maxine Audio Effects SDK version 1.0
Maxine Video Effects SDK Beta v0.6@TomNVIDIA  is there a contact that I can meet with to discuss this technology more?Hi James,Your best bet is to chat with the one of the employees here in the forums.
I would start with @asawarkar or @lingq .Best regards,
Tom KThank you Tom!@asawarkar @lingq I was trying to direct message you in the platform to connect but I do not see that option. Can you reach out to connect with me further? I appreciate all of your assistance.Powered by Discourse, best viewed with JavaScript enabled"
269,cant-find-libnvinfer-so-7,"I have the problem where the Tensorflow Python package wants version 7 of libnvinfer.so when all I can find to install is version 8 (libnvinfer.so.7 versus libnvinfer.so.8). Like these people:/forums.developer.nvidia.com/t/tensorflow-looking-for-libnvinfer-so-7-when-i-have-libnvinfer-so-8-installed/200007and others on StackOverflow, but I’m new and can only put in one link.Here’s what Tensorflow says:I have libnvinfer.so.8:Details of my platform and installations below.Can someone help?Thanks,Platform Details:ThinkStation P520 Workstation with a Nvidia GeForce RTX 3080 GPU.I installed Rocky Linux 8 on it:I followed the instructions to install CUDA at CUDA Installation Guide for Linux and I seem to have done that correctly (several times, in fact).The cuda-samples/Samples/1_Utilities/deviceQuery program says:If I search for libnvinfer, I get:And the contents of libnvinfer8 are not what Tensorflow wants:I’m happy to update this with any additional information you need.Hi @scott.anderson ,
This forum talks about issues and updates related to TensorRT.I believe you may raise your concern on Tensorflow github page.ThanksPowered by Discourse, best viewed with JavaScript enabled"
270,always-return-nvcv-err-missinginput-when-run-face-expression,"I integrated NVAR into Unreal Engine 5 and at 5.0 it worked well, but there were already some very strange issues, when I upgraded to UE5.1, the same codes didn’t work at all, I thought it was UE that affected it, so I wrapped it into a separate DLL. I think, the code is very simple, this SDK is very easy to use, but its error message is not detailed enough.
I debugged every line of code to make sure they didn’t return an error, and when it go to the line of "" NvAR_Run"", it return an error status, and I couldn’t known by the return code what the problem was.solve function:Parameter “bytes” is a uint8 array. Please point out what I missed, thanks .NvAR_Run return NVCV_ERR_MISSINGINPUT (-15).Doesn’t anyone have a response to this question?OS：Windows 11 x64
Driver Version: 528.24
CUDA Version: 12.0
AR SDK: 0.8.2
GPU: 2060Powered by Discourse, best viewed with JavaScript enabled"
271,cugraph-live-ama-with-engineering-team,"Join us for a live AMA with the core cuGraphic engineering team.Bring along questions and have responses realtime live.Learn more : AMA with the cuGraph engineering team - April 12, 2023, 9am (PDT) - Connect With Experts - AMA / AMA cuGraph: Graph analysis and GNN - NVIDIA Developer ForumsPowered by Discourse, best viewed with JavaScript enabled"
272,error-code-10-internal-error-could-not-find-any-implementation-for-node-foreignnode-lm-head-bias-cast,"Hi there,i want to convert an static quantized transformer model to trt. It is an CodeGenForCausalLM from transformers library.   I used the following command for converting the model.The error is as follows:Docker Image: nvcr.io/nvidia/pytorch:22.12-py3
TensorRT Version            8.501
Python                              3.8.10
Pips:
transformers                   4.26.1
optimum                          1.7.1
onnx                                 1.12.0
onnxruntime-gpu          1.14.1
pytorch-quantization    2.1.2
pytorch-triton                 2.0.0+b8b470bc59
torch                                 1.13.1
torch-tensorrt                 1.3.0
torchtext                          0.13.0a0+fae8e8c
torchvision                       0.15.0a0Thank you very much for any help!Hi ,
We recommend you to check the supported features from the below link.These support matrices provide a look into the supported platforms, features, and hardware capabilities of the NVIDIA TensorRT 8.6.0 Early Access (EA) APIs, parsers, and layers.
You can refer below link for all the supported operators list.
For unsupported operators, you need to create a custom plugin to support the operationThanks!Thank you for your reply.It seems that lm_head.bias.../Cast is not in the list of supported files, right?Powered by Discourse, best viewed with JavaScript enabled"
273,is-there-any-configuration-to-limit-the-maximum-number-of-concurrent-requests-processed-in-riva,"If GPU/memory resource is enough, is there any configuration to limit the maximum number of concurrent requests processed in one Riva ASR streaming recognition server instance?HI @174362510Thanks for your interest in RivaI will check regarding this query with Riva team and update youThanksHello rvinobha,
Is there any update for this issue? It seems that we can config triton related parameters such as PREFERRED_BATCH_SIZE in config.sh under riva_quickstart folder, right? Could you explain it in detail?Hello @rvinobha , sorry for disturbing you, is there any update for this issue?Hi @174362510 and @whz796100Apologies for the delay,I will check again today with internal team and get backThanksPowered by Discourse, best viewed with JavaScript enabled"
274,attributeerror-nonetype-object-has-no-attribute-create-execution-context,"Hello,I am trying to run from ONNX to tensorRTwhile doing that conversion I was getting this errorcontext = engine.create_execution_context()
AttributeError: ‘NoneType’ object has no attribute ‘create_execution_context’Hi,
It seems TRT engine is not initialized properly.Could you please share the script and model file so we can help better?
Also, can you provide details on the platforms you are using:
o Linux distro and version
o GPU type
o Nvidia driver version
o CUDA version
o CUDNN version
o Python version [if using python]
o Tensorflow and PyTorch version
o TensorRT versionThanksHi,
Issue seems to be due to “EXPLICIT_BATCH” setting in the code.
In TRT 7, ONNX parser supports full-dimensions mode only. Your network definition must be created with the explicitBatch flag set (when using ONNX parser).Since you are using TRT 6, please replace it with below codeI tested on both TRT 6 (After code changes) and TRT 7 (without changes), it seems to be working fine on my test onnx model.ThanksHi,I think I am not able to resolve the issue after changing the code too.Can you please help me resolve this.I have attached the onnx file to this.Hello,I did the changed the code according but still finding the issue. I have attached which version of tensorRT I have the screenshot of the installed tensorRtPlease find code changed belowHi,Try changing the workspace size, something likeIf issue persist, could you please share the complete error log and model file so we can better help?ThanksHello,I issue still persist this is the complete error log file.[b]/usr/bin/python3.5 /home/bplus/Desktop/UNIT_MASTER_THESIS/UNIT/unit1/UNIT_Working/sample_code.py
Engine Created : <class ‘NoneType’>
[TensorRT] ERROR: Network must have at least one output
Traceback (most recent call last):
File “/home/bplus/Desktop/UNIT_MASTER_THESIS/UNIT/unit1/UNIT_Working/sample_code.py”, line 62, in 
context = engine.create_execution_context()
AttributeError: ‘NoneType’ object has no attribute ‘create_execution_context’Process finished with exit code 1[/b]Please find the attachments of the model file belowHi,This specific issue is arising because the ONNX Parser isn’t currently compatible with the ONNX models exported from Pytorch 1.3 - If you downgrade to Pytorch 1.2, this issue should go away.Or Upgrade to TRT 7. Latest TRT 7 supports Pytorch 1.3.ThanksHello,I tried upgrading the tensorRT version to 7 and changed the pytorch version to 1.3 please find the attachment of the screenshot of the tensorRT 7 version and pytorch but still finding the issue with it.Please help me to resolve thisHi,If possible, could you please share your model file along with error log so we can further debug the issue?
Or
To validate/debug the your ONNX model you can use “trtexec” command in --verbose mode and share the output log.“trtexec” command line tool can be used for benchmarking & generating serialized engines from models.
https://docs.nvidia.com/deeplearning/sdk/tensorrt-archived/tensorrt-601/tensorrt-developer-guide/index.html#command-line-programs
https://github.com/NVIDIA/TensorRT/tree/master/samples/opensource/trtexec#example-4-running-an-onnx-model-with-full-dimensions-and-dynamic-shapes
In TRT 7 you need to use the explicit mode while running the command.ThanksHello ,I have tried it by using the explicit mode please find the  error log please find the attachments of model and onnx file[TensorRT] ERROR: Network must have at least one output
[TensorRT] ERROR: Network validation failed.
Engine Created : <class ‘NoneType’>
Traceback (most recent call last):
File “/home/bplus/Desktop/UNIT_MASTER_THESIS/UNIT/unit1/UNIT_Working/sample_code.py”, line 61, in 
context = engine.create_execution_context()
AttributeError: ‘NoneType’ object has no attribute ‘create_execution_context’
Hi,Could you please share the generated ONNX file?In earlier post explicit mode was mentioned to be used while using trtexec command.Meanwhile, please try to validate/debug the your ONNX model you can use “trtexec” command in --verbose mode and share the output log.“trtexec” command line tool can be used for benchmarking & generating serialized engines from models.
https://docs.nvidia.com/deeplearning/sdk/tensorrt-archived/tensorrt-601/tensorrt-developer-guide/index.html#command-line-programs
https://github.com/NVIDIA/TensorRT/tree/master/samples/opensource/trtexec#example-4-running-an-onnx-model-with-full-dimensions-and-dynamic-shapesThanksHello,I am not able to resolve this so can you please help resolve this .Please find the attachment of the onnx modelHi,For some reason, attached ONNX model file is not visible to me.
Could you please re-upload the model file?Also, could you please share the error log that you got while running trtexec command?ThanksHi,I don’t know I am not able to upload the onnx file.Can you please say anyway I can uploadHi,I don’t know I am not able to upload the onnx file.Can you please say anyway I can uploadHi,You can try uploading the zip file as done earlier for .pt model or you can upload the model to third party drive and share the link in the comment.ThanksHi,Please find the below for the converted onnx.Please let me know if you require anything.Hi,The issue seems to be due to the ONNX model optimization failure.
ONNX model is using “Pad” operation in “reflect” mode, and TensorRT supports “Pad” operation only in “constant” mode.Either you update the “Pad” operation to use “constant” mode or create a custom plugin needs to be replace “Pad” operation with “reflect” mode.
Please refer below link for more info regarding “Pad” operation:
https://github.com/onnx/onnx/blob/master/docs/Operators.md#Pad“trtexec” command output using current model:ThanksPowered by Discourse, best viewed with JavaScript enabled"
275,tensorrt-gives-diffent-results-than-onnx-and-pytorch,"When creating a TensorRT engine from an ONNX file, and comparing the inference outputs from the two formats I receive different results (The difference is significant and not due to precision/optimizations).TensorRT Version: 8.6.1.0
GPU Type: NVIDIA RTX A3000
Nvidia Driver Version: 535.54.03
CUDA Version: 12.2
CUDNN Version: 8.9.2
Operating System + Version: Ubuntu 20.04
Python Version (if applicable): 3.8.10
TensorFlow Version (if applicable): N/A
PyTorch Version (if applicable): 2.0.1+cu118
Baremetal or Container (if container which image + tag): N/Atrtexec --onnx=model_640x640_FULL_STAT.onnx --saveEngine=model_640x640_fp16_stat.engine --useSpinWait --fp16
( I also tried without --fp16)
2. run the attached python script (comp_onnx_trt.py): loads the ONNX and trt models, inject the same image file (attached) as input to them, compare the results and print the maximum difference between them.
3, The printed result is : max diff:  6.824637
4. when I compare the ONNX results to Pytorch using the same input, the difference is  low (~0.01)comp_onnx_trt.py (3.1 KB)
model_640x640_fp16_stat.engine (5.2 MB)
model_640x640_FULL_STAT.onnx (7.9 MB)
zidane1280×720 163 KBHi @erez.h,We could reproduce the issue. This is a known issue and will be fixed in future major releases. As a temporary workaround, we recommend you to use the FP32 precision.Thank you.Can we get engine output with the py file in this attachment? Also, when I run the cmd code, I get the error tensorrt trtexec (tensorrt v8502)usr/src/… is there a solution?
note=what model did you use exactly?Hi,
I also tried using FP32 precision (removing “–fp16” flag) and the results are the same.
Do I need to use different configuration for the trtexec ? (than ""trtexec --onnx=model_640x640_FULL_STAT.onnx --saveEngine=model_640x640_stat.engine --useSpinWait)
Thanks,
Ereztrtexec --onnx=model_640x640_FULL_STAT.onnx --saveEngine=model_640x640_stat.engine --useSpinWaitThe above command should work fine.
If we don’t specify precision, TensorRT will use the “FP32” precision by default.Thanks.
As I wrote I already tried this and got the same resultsPowered by Discourse, best viewed with JavaScript enabled"
276,tensor-flow-with-gpu-support-for-gtx-1650-max-q,"TUF-Gaming-FX505DT-FX505DT:lspci | grep VGA01:00.0 VGA compatible controller: NVIDIA Corporation TU117M [GeForce GTX 1650 Mobile / Max-Q] (rev ff)
05:00.0 VGA compatible controller: Advanced Micro Devices, Inc. [AMD/ATI] Picasso/Raven 2 [Radeon Vega Series / Radeon Vega Mobile Series] (rev c2)I have recently ordered a gtx 3060 + R5 7600x system , it will reach in 1-2 week before it arrives i thought to try tensorflow on my laptop ,i currently have an asus tuf fx 505DT laptop with 1650 max qi am trying to install tensorflow with gpu support , but i am facing many issues from choosing which cuda, cudnn , gcc version to choose and how to install them. i am on ubuntu 22.04. Can some one post all steps , including version to install.Hi ,
We recommend you to check the supported features from the below link.These support matrices provide a look into the supported platforms, features, and hardware capabilities of the NVIDIA TensorRT 8.6.0 Early Access (EA) APIs, parsers, and layers.
You can refer below link for all the supported operators list.
For unsupported operators, you need to create a custom plugin to support the operationThanks!Powered by Discourse, best viewed with JavaScript enabled"
277,installed-cudnn-8-8-on-my-ubuntu-machine-pytorch-not-detecting-gpu-now,"Hi there, looking for some help to get my pytorch running again.Context
I first checked my installed version using nvccI then downloaded cuDNN 8.8 for 11.x and installed it.
This is the output I’m getting when I attempt to start Stable Diffusion now.How can I diagnose the core issue and fix this? The error doesn’t really lead me in a way I understand to follow.Hi @paul.gibler ,
Issue seems like pytorch is not able to detect the GPU.
This issue doesnt look like a cudnn issue.
However can you please check if one of this available solution works for you?
ThanksPowered by Discourse, best viewed with JavaScript enabled"
278,unable-to-quantization-fp8-in-tensorrt,"Unable to run inference using TensorRT FP8 quantizationTensorRT Version: 8.6.1
GPU Type: RTX 4070 Ti
Nvidia Driver Version: 530
CUDA Version: 12.1
CUDNN Version: 8.9.2.26
Operating System + Version: Ubuntu 22.04 LTS
Python Version (if applicable): 3.10
TensorFlow Version (if applicable): —
PyTorch Version (if applicable): —
Baremetal or Container (if container which image + tag): baremetalNVIDIA claimed that the new 4th gen TensorCores support FP8 quantization.
So I have an RTX 4070 Ti (ada lovelace archi.) which has the 4th Gen of TensorCores and suport FP8 format. So I have installed CUDA 12.1 and TensorRT 8.6.1 which include kFP8 data type for FP8 format.However, I cannot successfully run inference using FP8. INT8 on the other hand works fine but FP8 is not working.Here I have found in limitations section that FP8 is not supported in TensorRT yet.So basically FP8 is implemented in TensorRT 8.6.1 but it is not supported yet?
Could you please explain to me what that means? Because for me it is not supported means it is not implemented as well… !!Thank you in advanced.Best regardsHi,1 TensorRT 8.6 adds nvinfer1::DataType::kFP8 to the public API in preparation for the introduction of FP8 support in future TensorRT releases. However, FP8 (8-bit floating point) is not supported by TensorRT currently, and attempting to use FP8 will result in an error or undefined behavior.Please refer to the Developer Guide :: NVIDIA Deep Learning TensorRT Documentation for the same.Thank you.Powered by Discourse, best viewed with JavaScript enabled"
279,tensorrt-nchw-vs-cudnn-nhwc,"Hi everybody, I have a question regarding tensor memory layout in TensorRT. Some frameworks use a NCHW layout, others use NHWC layout.
Reading cuDNN documentation it seems 2d convolutions on tensor cores achieve best performance with NHWC memory layout. This thread though explicitly states that TensorRT implementation is NCHW.
Since I have tensor cores available, is there a way in which my CNN can take advantage of the extra performance provided by the NHWC memory layout? Is it automatically taken care of by Tensor RT?Hi @albecenz,Yes it automatically taken care by TRT,
TRT internally tries all kinds of tensor layouts.Thank you.Thanks@spolisettyI beleive Reformat (like NCHW to NHWC) is still a cost for large input,  so is it better to convert your model to NHWC first before feeding to tensorrt converter? But to the best of my knowledge, I have not seen any onnx model with NHWC format. Do you have any idea?Powered by Discourse, best viewed with JavaScript enabled"
280,could-not-initialize-cublas-please-check-cuda-installation,"I get below error when I try to create an engine from .etlt model using tao_converter.TensorRT Version: 8.2.3
GPU Type: Geforce RTX 3060
Nvidia Driver Version: 530.30.02
CUDA Version: 11.6.1
CUDNN Version: 8.3.3
Operating System + Version: Ubuntu 20.04
Python Version (if applicable): 3.8
TensorFlow Version (if applicable): NA
PyTorch Version (if applicable): NA
Baremetal or Container (if container which image + tag): Container - nvcr.io/nvidia/tensorrt:22.03-py3This is my Dockerfile:The image got built without any error, but I get the above mentioned error when I try to convert the etlt model to engine using tao_converter(8.2 version). Kindly help me fix this issue.Hi,
Please refer to the installation steps from the below link if in case you are missing on anythingThis NVIDIA TensorRT 8.6.0 Early Access (EA) Installation Guide provides the installation requirements, a list of what is included in the TensorRT package, and step-by-step instructions for installing TensorRT.
Also, we suggest you to use TRT NGC containers to avoid any system dependency related issues.NVIDIA TensorRT is a C++ library that facilitates high-performance inference on NVIDIA graphics processing units (GPUs). TensorRT takes a trained network and produces a highly optimized runtime engine that performs inference for that network.Thanks!Hi @AakankshaS , Thanks for the reply. I’m already using the TensorRT container from NGC. Kindly refer the my comment in the environment section. The container version I’m using - nvcr.io/nvidia/tensorrt:22.03-py3Hi Team, Any lead on this please?Hi @arivarasan.e ,
In the container, and your environment details, i see a TRT and CUDA version
mismatch from what the container holds.[quote=“arivarasan.e, post:1, topic:250843”]
TensorRT Version: 8.2.3
GPU Type: Geforce RTX 3060
Nvidia Driver Version: 530.30.02
CUDA Version: 11.6.1
[/quote].
Can you please check the container components and retry again.The TensorRT container is an easy to use container for TensorRT development. The container allows you to build, modify, and execute TensorRT samples. These release notes provide a list of key features, packaged software in the container, software...ThanksPowered by Discourse, best viewed with JavaScript enabled"
281,batching-preprocess-in-triton,"Hello,I am using the Triton server ensemble, which involves two main steps: preprocessing using .so file(s) (libtriton_.so) and executing an ONNX model compiled to a TensorRT model plan.Currently, both the custom backend and the model in the ensemble processes one input at a time (batch size = 1). My goal is to create a version that can handle multiple inputs in parallel (batched version).According to the Triton documentation, the server performs batching on incoming requests. What is the correct way to approach the batching of preprocessing?Powered by Discourse, best viewed with JavaScript enabled"
282,problem-in-starting-riva-speech-container,"Hardware - GPU (NVIDIA GeForce RTX 3050 Ti Laptop GPU)
Hardware - CPU (Intel® Core™ i7)
Operating System: Windows
Riva Version: latesthi ,i purchased this course https://courses.nvidia.com/courses/course-v1:DLI+S-FX-04+V1/
and i followed the notebooks in iti tried to start riva speeech by typing this command in git bash (./riva_start.sh config.sh)but getting this error:docker logs:Please help me to cater this problemHi @m.salamaThanks for your interest in RivaQuick Check,ThanksPowered by Discourse, best viewed with JavaScript enabled"
283,error-could-not-find-any-implementation-for-node-foreignnode,"Hi, I’m trying to convert my ONNX model to TensorRT format, but encounter an error as below.
Can anyone help me to solve the problem?
My ONNX model is here [https://83516952-my.sharepoint.com/:u:/g/personal/eddie_hsiao_insign-medical_com/ESS-N89ev6JIgnqv9O5TjzMBu1JbjaDy2VkQqhJEq1K0wQ?e=RrnLTA](https://onnx model )TensorRT Version: 8.5.3.1
GPU Type: RTX3090
Nvidia Driver Version: 510.108.03
CUDA Version: 11.6
CUDNN Version:
Operating System + Version:  Ubuntu20
Python Version (if applicable): 3.8
PyTorch Version (if applicable):my python scriptexecute command:Please include:Hi,
Request you to share the ONNX model and the script if not shared already so that we can assist you better.
Alongside you can try few things:This NVIDIA TensorRT 8.5.3 Quick Start Guide is a starting point for developers who want to try out TensorRT SDK; specifically, this document demonstrates how to quickly construct an application to run inference on a TensorRT engine.import sys
import onnx
filename = yourONNXmodel
model = onnx.load(filename)
onnx.checker.check_model(model).
2) Try running your model with trtexec command.master/samples/trtexecNVIDIA® TensorRT™, an SDK for high-performance deep learning inference, includes a deep learning inference optimizer and runtime that delivers low latency and high throughput for inference applicat...
In case you are still facing issue, request you to share the trtexec “”–verbose"""" log for further debugging
Thanks!Powered by Discourse, best viewed with JavaScript enabled"
284,does-tensorrt-exploit-parallelism-in-a-computational-graph-during-inference,"I have a pytorch model, the forward pass looks roughly like the following which I convert to TensorRT 8.4During inference, is TensorRT smart enough to know that the lidar encoder and camera encoder can be run at the same time on the GPU, but then a sync needs to be inserted before the torch.stack? Does it do this automatically?TensorRT Version: 8.4Hi,
Can you try running your model with trtexec command, and share the “”–verbose"""" log in case if the issue persistmaster/samples/trtexecNVIDIA® TensorRT™, an SDK for high-performance deep learning inference, includes a deep learning inference optimizer and runtime that delivers low latency and high throughput for inference applicat...You can refer below link for all the supported operators list, in case any operator is not supported you need to create a custom plugin to support that operationAlso, request you to share your model and script if not shared already so that we can help you better.Meanwhile, for some common errors and queries please refer to below link:This is the revision history of the NVIDIA TensorRT 8.6 Developer Guide.This is the revision history of the NVIDIA TensorRT 8.6 Developer Guide.Thanks!In my experience it doesn’t. You can run NSightSystems (or probably another profiler of your choice) and see the timeline. I think you can imagine that TensorRT is maximizing GPU usage on each layer, so it wouldn’t have any compute power left for running both branches “at the same time” anyway. Also I think you would need to have separate streams to achieve concurrent execution of layers, which as far as I know, tensorrt is only executing in one stream.If each branch isn’t very computationally intensive, maybe using cuda graphs will help? Developer Guide :: NVIDIA Deep Learning TensorRT DocumentationYou could also try to split your model into two pieces and execute them in separate streams. But again, if each branch has maximized GPU usage, there won’t be any performance difference.Powered by Discourse, best viewed with JavaScript enabled"
285,nvinfer1-deserializecudaengine-fails,"I am getting the following error when executing the nvinfer1::IRuntime::deserializeCudaEngine command:This error seems to cover a lot of ills.What else does it cover?Am I wrong when I interpret the size param to be the size of the serialized graph passed via the first argument? What is this error really trying to tell me?The graph runs fine in python3.Environment:问题解决了吗Powered by Discourse, best viewed with JavaScript enabled"
286,riva-tts,"Hello
I have my own Arabic data set, and I need to train Riva TTS with this data, is there any article or document that can help me to train TTSThank youPowered by Discourse, best viewed with JavaScript enabled"
287,nvidia-data-science-workbench,"NVIDIA Data Science Workbench
*IMPORTANT NOTE: A completely revised Workbench release is planned. To learn more, watch GTC Session S51283 – “A Path to Democratized Data Science in Any Cloud”.What is NVIDIA Data Science Workbench?What does Workbench do?What are the key features in Workbench?Getting StartedHere are installation instructions for Workbench. If you need support along the way,please reach out to NVIDIA via the email address provided at the end of the Installation section.What you will need:What to expect:You will be running command line tools such as git, pip, nvidia-smi, ngc, kaggle, aws-cli in your favorite shell (usually bash). You will be asked to enter your sudo password during the initial install of Workbench during the pip3 installation. Workbench automatically clones the NVIDIA Data Science Stack (GitHub - NVIDIA/data-science-stack: NVIDIA Data Science stack tools) to facilitate NVIDIA Linux driver installation.Your current NVIDIA driver will be upgraded to support the most up-to-date NGC Deep Learning framework (TensorFlow, PyTorch) containers. NVIDIA updates these containers once a month.When Workbench updates, older versions of the containers will still be available in docker images reporting. After Workbench is installed and running, look for an NVIDIA icon in the upper right-hand side of the Ubuntu toolbar near the networking icon. To start Workbench, click on the NVIDIA icon and a pulldown menu of data science tasks will appear.Installation steps:Steps to successfully loading NVIDIA Data Science Workbench:sudo apt install python3 pip (# may be required if pip3 is not already installed)pip3 install nvdsw (# sudo passwd may be required)nvdsw-setuprebootAfter a successful reboot, the NVIDIA Data Science Workbench logo will appear in the upper right-hand corner of Ubuntu desktop toolbar.For support and feedback, reach out to us by email at NVDSWTechSupport@nvidia.com.Powered by Discourse, best viewed with JavaScript enabled"
288,cudnn-tag-trt-tag-tensorrt-version-compatibility-against-cuda-ubuntu-cuda-arch-being-known,"Hello. I am trying to expand on a .sh script for the installation of CUDA, OpenCV with CUDA module and TensorRT. I am trying to implement download functionality for different graphics cards however am struggling to determine the compatible versions and tags for the three variables below.All the resources i have reference to are listed here.Examples of how the Tags are used below:If anyone can provide a solution to this and explain how to read off these dependencies from the above referenced links or provide the ones that are missing to resolve the issue, it would be greatly appreciated.Hi @adav0033 ,
I believe you can find a common version between the CUDa and TRT for your hardware from the below link in addition.These support matrices provide a look into the supported platforms, features, and hardware capabilities of the NVIDIA TensorRT 8.6.1 APIs, parsers, and layers.Also it is recommended to use latest TRT version for optimized performance, as support for TRT 6 has been discontinued.Thanks
Thanks.Powered by Discourse, best viewed with JavaScript enabled"
289,cannot-broadcast-shapes-that-have-different-ranks-onnx-tensorrt,"I have SavedModel format of a Tensorflow2 model and I follow the guide: https://developer.nvidia.com/blog/speeding-up-deep-learning-inference-using-tensorflow-onnx-and-tensorrt/When I try to build engine (from onnx to tensorrt), I get the error from parser:
In node 66 (isBroadcastValid): UNSUPPORTED_NODE: Cannot broadcast shapes that have different ranks!I also try it with trtexec, but the result is same.Any help is appreciated.Module: Jetson Nano ( 4GB)
Jetpack: 4.6
TensorRT Version:  8.0.1.6
GPU Type:  Tegra X1
Nvidia Driver Version:
CUDA Version: 10.2.300
CUDNN Version: 8.2.1.32
Operating System + Version: Ubuntu 18.04 Bionic Beaver
Python Version (if applicable):  3.6.9
TensorFlow Version (if applicable): 2.5.0+nv21.8summary of the model:Google Drive file.savedmodel:
https://drive.google.com/drive/folders/1Fut3t5JwMHd_IkmnhtAAjMR2vMCqg7Sh?usp=sharingonnx:Google Drive file.As GitHub - onnx/tensorflow-onnx: Convert TensorFlow, Keras, Tensorflow.js and Tflite models to ONNX indicates for a SavedModel:
python3 -m tf2onnx.convert --saved-model tensorflow-model-path --output model.onnxthen,
trtexec --onnx=onxx-model&&&& RUNNING TensorRT.trtexec [TensorRT v8001] # trtexec --onnx=/home/jetson/Desktop/DIP/trt/aocr/lpr.onnx
[03/09/2023-09:51:20] [I] === Model Options ===
[03/09/2023-09:51:20] [I] Format: ONNX
[03/09/2023-09:51:20] [I] Model: /home/jetson/Desktop/DIP/trt/aocr/lpr.onnx
[03/09/2023-09:51:20] [I] Output:
[03/09/2023-09:51:20] [I] === Build Options ===
[03/09/2023-09:51:20] [I] Max batch: explicit
[03/09/2023-09:51:20] [I] Workspace: 16 MiB
[03/09/2023-09:51:20] [I] minTiming: 1
[03/09/2023-09:51:20] [I] avgTiming: 8
[03/09/2023-09:51:20] [I] Precision: FP32
[03/09/2023-09:51:20] [I] Calibration:
[03/09/2023-09:51:20] [I] Refit: Disabled
[03/09/2023-09:51:20] [I] Sparsity: Disabled
[03/09/2023-09:51:20] [I] Safe mode: Disabled
[03/09/2023-09:51:20] [I] Restricted mode: Disabled
[03/09/2023-09:51:20] [I] Save engine:
[03/09/2023-09:51:20] [I] Load engine:
[03/09/2023-09:51:20] [I] NVTX verbosity: 0
[03/09/2023-09:51:20] [I] Tactic sources: Using default tactic sources
[03/09/2023-09:51:20] [I] timingCacheMode: local
[03/09/2023-09:51:20] [I] timingCacheFile:
[03/09/2023-09:51:20] [I] Input(s)s format: fp32:CHW
[03/09/2023-09:51:20] [I] Output(s)s format: fp32:CHW
[03/09/2023-09:51:20] [I] Input build shapes: model
[03/09/2023-09:51:20] [I] Input calibration shapes: model
[03/09/2023-09:51:20] [I] === System Options ===
[03/09/2023-09:51:20] [I] Device: 0
[03/09/2023-09:51:20] [I] DLACore:
[03/09/2023-09:51:20] [I] Plugins:
[03/09/2023-09:51:20] [I] === Inference Options ===
[03/09/2023-09:51:20] [I] Batch: Explicit
[03/09/2023-09:51:20] [I] Input inference shapes: model
[03/09/2023-09:51:20] [I] Iterations: 10
[03/09/2023-09:51:20] [I] Duration: 3s (+ 200ms warm up)
[03/09/2023-09:51:20] [I] Sleep time: 0ms
[03/09/2023-09:51:20] [I] Streams: 1
[03/09/2023-09:51:20] [I] ExposeDMA: Disabled
[03/09/2023-09:51:20] [I] Data transfers: Enabled
[03/09/2023-09:51:20] [I] Spin-wait: Disabled
[03/09/2023-09:51:20] [I] Multithreading: Disabled
[03/09/2023-09:51:20] [I] CUDA Graph: Disabled
[03/09/2023-09:51:20] [I] Separate profiling: Disabled
[03/09/2023-09:51:20] [I] Time Deserialize: Disabled
[03/09/2023-09:51:20] [I] Time Refit: Disabled
[03/09/2023-09:51:20] [I] Skip inference: Disabled
[03/09/2023-09:51:20] [I] Inputs:
[03/09/2023-09:51:20] [I] === Reporting Options ===
[03/09/2023-09:51:20] [I] Verbose: Disabled
[03/09/2023-09:51:20] [I] Averages: 10 inferences
[03/09/2023-09:51:20] [I] Percentile: 99
[03/09/2023-09:51:20] [I] Dump refittable layers:Disabled
[03/09/2023-09:51:20] [I] Dump output: Disabled
[03/09/2023-09:51:20] [I] Profile: Disabled
[03/09/2023-09:51:20] [I] Export timing to JSON file:
[03/09/2023-09:51:20] [I] Export output to JSON file:
[03/09/2023-09:51:20] [I] Export profile to JSON file:
[03/09/2023-09:51:20] [I]
[03/09/2023-09:51:20] [I] === Device Information ===
[03/09/2023-09:51:20] [I] Selected Device: NVIDIA Tegra X1
[03/09/2023-09:51:20] [I] Compute Capability: 5.3
[03/09/2023-09:51:20] [I] SMs: 1
[03/09/2023-09:51:20] [I] Compute Clock Rate: 0.9216 GHz
[03/09/2023-09:51:20] [I] Device Global Memory: 3964 MiB
[03/09/2023-09:51:20] [I] Shared Memory per SM: 64 KiB
[03/09/2023-09:51:20] [I] Memory Bus Width: 64 bits (ECC disabled)
[03/09/2023-09:51:20] [I] Memory Clock Rate: 0.01275 GHz
[03/09/2023-09:51:20] [I]
[03/09/2023-09:51:20] [I] TensorRT version: 8001
[03/09/2023-09:51:21] [I] [TRT] [MemUsageChange] Init CUDA: CPU +203, GPU +1, now: CPU 221, GPU 2592 (MiB)
[03/09/2023-09:51:21] [I] Start parsing network model
[03/09/2023-09:51:21] [I] [TRT] ----------------------------------------------------------------
[03/09/2023-09:51:21] [I] [TRT] Input filename:   /home/jetson/Desktop/DIP/trt/aocr/lpr.onnx
[03/09/2023-09:51:21] [I] [TRT] ONNX IR version:  0.0.7
[03/09/2023-09:51:21] [I] [TRT] Opset version:    13
[03/09/2023-09:51:21] [I] [TRT] Producer name:    tf2onnx
[03/09/2023-09:51:21] [I] [TRT] Producer version: 1.13.0 2c1db5
[03/09/2023-09:51:21] [I] [TRT] Domain:
[03/09/2023-09:51:21] [I] [TRT] Model version:    0
[03/09/2023-09:51:21] [I] [TRT] Doc string:
[03/09/2023-09:51:21] [I] [TRT] ----------------------------------------------------------------
[03/09/2023-09:51:21] [W] [TRT] onnx2trt_utils.cpp:364: Your ONNX model has been generated with INT64 weights, while TensorRT does not natively support INT64. Attempting to cast down to INT32.
[03/09/2023-09:51:21] [E] [TRT] ModelImporter.cpp:720: While parsing node number 66 [If → “If__282:0”]:
[03/09/2023-09:51:21] [E] [TRT] ModelImporter.cpp:721: — Begin node —
[03/09/2023-09:51:21] [E] [TRT] ModelImporter.cpp:722: input: “Equal__275:0”
output: “If__282:0”
name: “If__282”
op_type: “If”
attribute {
name: “then_branch”
g {
node {
input: “StatefulPartitionedCall/model/tf.ones/ones:0”
output: “Identity__277:0”
name: “Identity__277”
op_type: “Identity”
domain: “”
}
name: “tf2onnx__276”
doc_string: “graph for If__282 then_branch”
output {
name: “Identity__277:0”
type {
tensor_type {
elem_type: 1
shape {
dim {
dim_param: “unk__1024”
}
dim {
dim_value: 10
}
}
}
}
}
}
type: GRAPH
}
attribute {
name: “else_branch”
g {
node {
input: “StatefulPartitionedCall/model/tf.ones/ones:0”
input: “const_axes__260”
output: “Unsqueeze__280:0”
name: “Unsqueeze__280”
op_type: “Unsqueeze”
domain: “”
}
name: “tf2onnx__279”
doc_string: “graph for If__282 else_branch”
output {
name: “Unsqueeze__280:0”
type {
tensor_type {
elem_type: 1
shape {
dim {
dim_param: “unk__1025”
}
dim {
dim_param: “unk__1026”
}
dim {
dim_param: “unk__1027”
}
}
}
}
}
}
type: GRAPH
}
domain: “”[03/09/2023-09:51:21] [E] [TRT] ModelImporter.cpp:723: — End node —
[03/09/2023-09:51:21] [E] [TRT] ModelImporter.cpp:726: ERROR: onnx2trt_utils.cpp:190 In function isBroadcastValid:
[8] Cannot broadcast shapes that have different ranks!
[03/09/2023-09:51:21] [E] Failed to parse onnx file
[03/09/2023-09:51:21] [I] Finish parsing network model
[03/09/2023-09:51:21] [E] Parsing model failed
[03/09/2023-09:51:21] [E] Engine creation failed
[03/09/2023-09:51:21] [E] Engine set up failedHi,
Request you to share the ONNX model and the script if not shared already so that we can assist you better.
Alongside you can try few things:This NVIDIA TensorRT 8.5.3 Quick Start Guide is a starting point for developers who want to try out TensorRT SDK; specifically, this document demonstrates how to quickly construct an application to run inference on a TensorRT engine.import sys
import onnx
filename = yourONNXmodel
model = onnx.load(filename)
onnx.checker.check_model(model).
2) Try running your model with trtexec command.master/samples/trtexecNVIDIA® TensorRT™, an SDK for high-performance deep learning inference, includes a deep learning inference optimizer and runtime that delivers low latency and high throughput for inference applicat...
In case you are still facing issue, request you to share the trtexec “”–verbose"""" log for further debugging
Thanks!Hi,I attached onnx and saved-model.in the mean time, check_model does not throw any error.output of --verbose:
https://drive.google.com/drive/folders/1vMhIVxgkmdeTMR_KbLntA9yk5b9qauiY?usp=sharingThanks in advanceHi @dogu.budak ,
Apologies for the delay, are you still facing the issue?Hi,Yes, I am still facing the issuePowered by Discourse, best viewed with JavaScript enabled"
290,jarvis-quick-start-installation-fails,"Hi all,I am trying to install Jarvis, but I am getting the following error:Error occurred downloading 'nvidia/jarvis/jmir_jarvis_asr_citrinet_1024_asrset1p7_streaming:1.2.0-beta'. Exiting.Is there a solution?Regards,
GeorgeHi @petasisg,Could you please share the complete error log/command output so we can help better? Also please share docker logs jarvis-speech output as well.ThanksThe logs are:There is no container jarvis-speech yet, the procedure fails in the init script.Hi @petasisg ,Client Error: 403 Response: It seems to access issue, could you please cross check if NGC key is set correctly.Docker version insufficient. Please use Docker 19.03 or laterWill request you upgrade the docker to supported version.
Also, please run jarvis_clean.sh before retrying the deployment. Do let me know if you still face while setting up the service?ThanksDear @SunilJB,Yes, ngc key is correct, and if I modify config.sh to not include these citrinet packages, the rest download ok.The docker warnings are irrelevant, I am using podman in fedora :-).Facing the same issue. The models are not downloading.
`bash jarvis_init.sh
Logging into NGC docker registry if necessary…
Pulling required docker images if necessary…
Note: This may take some time, depending on the speed of your Internet connection.Pulling Jarvis Speech Server images.
Pulling nvcr.io/nvidia/jarvis/jarvis-speech:1.1.0-beta-server. This may take some time…
Pulling nvcr.io/nvidia/jarvis/jarvis-speech-client:1.1.0-beta. This may take some time…
Pulling nvcr.io/nvidia/jarvis/jarvis-speech:1.1.0-beta-servicemaker. This may take some time…Downloading models (JMIRs) from NGC…
Note: this may take some time, depending on the speed of your Internet connection.
To skip this process and use existing JMIRs set the location and corresponding flag in config.sh.NVIDIA Release  (build 21060478)Copyright (c) 2018-2021, NVIDIA CORPORATION.  All rights reserved.Various files include modifications (c) NVIDIA CORPORATION.  All rights reserved.
NVIDIA modifications are covered by the license terms that apply to the underlying
project or file.NOTE: The SHMEM allocation limit is set to the default of 64MB.  This may be
insufficient for the inference server.  NVIDIA recommends the use of the following flags:
nvidia-docker run --shm-size=1g --ulimit memlock=-1 --ulimit stack=67108864 …/data/artifacts /opt/jarvisDownloading nvidia/jarvis/jmir_punctuation:1.0.0-b.1…
Downloaded 0 B in 5s, Download speed: 0 B/s
Client Error: 403 Response:  - Request Id:  Url: https://ngc.download.nvidia.com/models/org/nvidia/team/jarvis/models/jmir_punctuation/versions/1.0.0-b.1/files.zip?ak-token=exp=1622813401~acl=%2Fmodels%2Forg%2Fnvidia%2Fteam%2Fjarvis%2Fmodels%2Fjmir_punctuation%2Fversions%2F1.0.0-b.1%2Ffiles.zip*~hmac=1751bcc2767a59c4416de7f32925d65209633e5b3c23cb30a755626c998cdff1
Downloaded 0 B in 6s, Download speed: 0 B/sAttempt 1 out of 3 failed
Trying again…
Downloaded 0 B in 6s, Download speed: 0 B/s
Client Error: 403 Response:  - Request Id:  Url: https://ngc.download.nvidia.com/models/org/nvidia/team/jarvis/models/jmir_punctuation/versions/1.0.0-b.1/files.zip?ak-token=exp=1622813418~acl=%2Fmodels%2Forg%2Fnvidia%2Fteam%2Fjarvis%2Fmodels%2Fjmir_punctuation%2Fversions%2F1.0.0-b.1%2Ffiles.zip*~hmac=094a1e25d63e86f6e3023bc10bcd6a80eeeed3fd48a67203b1fb0685096b0681
Downloaded 0 B in 7s, Download speed: 0 B/sAttempt 2 out of 3 failed
Trying again…
Downloaded 0 B in 6s, Download speed: 0 B/s
Client Error: 403 Response:  - Request Id:  Url: https://ngc.download.nvidia.com/models/org/nvidia/team/jarvis/models/jmir_punctuation/versions/1.0.0-b.1/files.zip?ak-token=exp=1622813437~acl=%2Fmodels%2Forg%2Fnvidia%2Fteam%2Fjarvis%2Fmodels%2Fjmir_punctuation%2Fversions%2F1.0.0-b.1%2Ffiles.zip*~hmac=51d78a87421d44dc584b75178dd891a8fcbb4e4578e3fc2a236ae49aaed999d7
Downloaded 0 B in 7s, Download speed: 0 B/sAttempt 3 out of 3 failed
Error occurred downloading ‘nvidia/jarvis/jmir_punctuation:1.0.0-b.1’. Exiting.
Error in downloading models.
`faced the same issue on 1.2.0-beta@SunilJB  Hi, any estimate as to when this will be fixed, ngc giving problems downloading the models 403 errors when trying to download the punctuation model in beta 1.1 and 1.2. Thanks in advanceFixed now. Sorry about that!thanks, but its not working for me still. What can i do to fix this? Please help. Thanks

image1618×447 20.4 KB
@ellengsu I recommend first pulling the latest quickstart (1.2.0-beta). Check that you have either $NGC_API_KEY variable set or the ngc utility installed and logged in (you would have a file $HOME/.ngc/config). Keep in mind that if you go to the NGC website to retrieve your API key, it will invalidate any previously used keys and you will need to update $NGC_API_KEY or login with the ngc command again. The 403 Response you are seeing is NGC reporting that you are not authorized to download the model. These models are available for any registered user, so it should work for you once the proper API key is used.The problem earlier in the thread was a permission problem on NVIDIA’s side which prevented download of the new CitriNet model.@rleary  Hi, unfortunately I can’t even pull the quickstart anymore, using the same command I’ve been using for a month, even made a new API key and config set it. ngc registry resource download-version nvidia/jarvis/jarvis_quickstart:1.2.0-beta  just returning failed.@rleary I’m in the same boat as HansieB, I was able to pull the quickstart yesterday, and attempting it today it  fails. I also tried a new API key and config set it.ngc registry resource download-version nvidia/jarvis/jarvis_quickstart:1.2.0-betaThanks rleary,
I did have the ngc api key in place and ngc utility installed and logged in. And to be safe, i followed the instruction on this page to ensure the ngc configured properly.
https://ngc.nvidia.com/setup/installers/cliBut when running jarvis_init.sh, it’s the same 403 error like above.
Same error when trying to get the latest quick start guide. ngc registry resource download-version nvidia/jarvis/jarvis_quickstart:1.2.0-betaI was able to pull the quickstart 1.2 a day or two ago but it has been failing to download models with the same 403 error.  I also just want back to pull the quickstart again on a cloud gpu instead of local machine and I can’t even download that anymore.$ docker login nvcr.io
Authenticating with existing credentials…
WARNING! Your password will be stored unencrypted in /.docker/config.json.
Configure a credential helper to remove this warning. See
docker login | Docker DocumentationLogin Succeeded
$
$ ngc registry resource download-version “nvidia/jarvis/jarvis_quickstart:1.2.0-beta”`Nothing works. Not even this:Hi, same problem here, looks like nvidia don’t want jarvis to conquer the world…I’m also not able to download jarvis_quickstart with NGC.  I downloaded all the files manually from the web site, then I cannot pull the following docker image during jarvis_init.sh:docker pull nvidia/jarvis/jmir_punctuation:1.2.0-beta
Error response from daemon: pull access denied for nvidia/jarvis/jmir_punctuation, repository does not exist or may require ‘docker login’: denied: requested access to the resource is deniedHere’s the full output since I know you will ask:Logging into NGC docker registry if necessary…
Pulling required docker images if necessary…
Note: This may take some time, depending on the speed of your Internet connection.Pulling Jarvis Speech Server images.
Image nvcr.io/nvidia/jarvis/jarvis-speech:1.2.0-beta-server exists. Skipping.
Image nvcr.io/nvidia/jarvis/jarvis-speech-client:1.2.0-beta exists. Skipping.
Image nvcr.io/nvidia/jarvis/jarvis-speech:1.2.0-beta-servicemaker exists. Skipping.Downloading models (JMIRs) from NGC…
Note: this may take some time, depending on the speed of your Internet connection.
To skip this process and use existing JMIRs set the location and corresponding flag in config.sh.NVIDIA Release devel (build 22382700)Copyright (c) 2018-2021, NVIDIA CORPORATION.  All rights reserved.Various files include modifications (c) NVIDIA CORPORATION.  All rights reserved.
NVIDIA modifications are covered by the license terms that apply to the underlying
project or file.… I had to remove this stuff because the forum complains that I am a new user and posting too many links …Attempt 1 out of 3 failed
Trying again…
…etc… 2 more times
Attempt 3 out of 3 failed
Error occurred downloading ‘nvidia/jarvis/jmir_punctuation:1.2.0-beta’. Exiting.
Error in downloading models.It seems clearly someone in the server development team has broken it for now. I changed https:// to h***s:/ / because I’m a new user and not allowed to post so many links.It appears to work again:Powered by Discourse, best viewed with JavaScript enabled"
291,profiling-error,"Hello I am unable to run profiling ( not Perfetto neither tensorboard) I get
double free or corruption (!prev) ; I use oficial container nvcr.io/nvdlfwea/jax/jax:23.03-py3; Nvidia Rtx 3090 GPUAborted (core dumped)
error
What does it mean?The same on 23.04Powered by Discourse, best viewed with JavaScript enabled"
292,triton-crashes-with-signal-11,"We noticed couple of times that triton is crashing with signal 11. RAM, CPU and GPU are not spiking before the crash.Trition verison : 22.12 (nvcr.io/nvidia/tritonserver:22.12-py3)
Python Backend verison : r21.08
TensorRT Version : 8.5.1
GPU Type : GeForce RTX 2080 SUPER
Nvidia Driver Version : 510.108.03
CUDA Version : 11.8
CUDNN Version : 8.7.0 GA
Operating System + Version : Ubuntu 20.04
Python Version (if applicable) : 3.7image1781×335 113 KBimage898×114 31.4 KBRAM usage:
image893×305 18.5 KBCPU:
image888×316 73.8 KBGPU:
image879×310 95.6 KBWe couldn’t find the exact reason for the crash, so cannot specify the steps to reproduce. This signal is observed mostly on long runs (8-12 hrs) of Triton Server.Hi,We recommend you to please reach out to Issues · triton-inference-server/server · GitHub to get better help on Triton related issues.Thank you.Powered by Discourse, best viewed with JavaScript enabled"
293,error-in-provided-code-in-deep-learning-data-science-course,"In the 7th section of the "" Accelerating End-to-End Data Science Workflows"" in the deep learning institute the provided code in the notebook yields the following error, could anyone guide/assist me with this issue?Screen Shot 2023-06-27 at 12.38.08 PM2294×1178 486 KBPowered by Discourse, best viewed with JavaScript enabled"
294,onnx-output-differs-largely-to-trt-engine-output,"I have an onnx model, whose output has beed verified to be almost identical with my original PyTorch model.After I convert it to tensorrt engine, the output changes too much. I don’t know if there is any tool I can use to debug and locate the place where error produced between onnx and trt?nvidia docker container 22.12Here is the onnx model: https://cloud.tsinghua.edu.cn/f/8e1a7623952946c7bb76/?dl=1Use this script to reproduce:Before run it, you should pip install onnxruntime-gpu.It shows the absolute error is ~4.6 that large.Also refer to Onnx output differs largely to TRT engine outputWe have a tool called polygraphy can be used to debug accuracy issues, see TensorRT/tools/Polygraphy/examples/cli/run/01_comparing_frameworks at main · NVIDIA/TensorRT · GitHub
And for your case,  you can first try to run with FP32 precision and see if the accuracy issue only happens to FP16? If only FP16 fails, it’s usually caused by FP16 overflow, e.g. the output of some accumulate operations is larger than 65504. This can be fixed by forcing FP32 precision for those problematic layers/tensors.I have checked that there’s no fp16 overflow… So is there any other possible reason for the output error?This is the simplified problem for now:I have a very simple onnx file (Where I locate the problematic sub-network): https://cloud.tsinghua.edu.cn/f/5db9c79dc5a841ada575/?dl=1When I use polygraphy run onnx/sample.onnx --trt --fp16 --onnxrt to test it. The output engine makes very large error.So how to fix this problem?The output engine makes very large error.Can you share the FP32/FP16 error report from Polygraphy?
And you can use ""  --trt-outputs mark all  --onnx-outputs mark all "" to dump per-layer accuracy result, it can help you root cause the problematic layer and understand the error porpogation.Thank you for your suggestion. As the per-layer polygraphy shows, InstanceNormalization contributes most error:Is there any bug for InstanceNormalization layer in TensorRT? Because my InstanceNorm layer is a 32 group GroupNorm actually.This is a common case that InstanceNormal causes accuracy issues in various models. The key problem is when using FP16 precision, instanceNorm kernel needs to accumulate per channel/batch elements under FP16 precision. Although the final result of InstanceNorm will not overflow but the accumulcator result might overflow before executing the division operation. That causes the accuracy issue.The recommanded way to fix this issue is to mark the precision of InstanceNorm layer as FP32. Polygraphy has provided such functionality with “–layerPrecision”/“–tensorPrecision”. You can check the Polygraphy --help to see how to use it. And trtexec should also has similiar options.Thank you for your explanation! However, I have a very large model. Is there any example of using python to optimize the onnx to trt? Otherwise, I have to list all InstanceNorm layer names in trtexec command line… (Or if the trtexec command line flags support regex expression?)You can use graph surgeon to optimize your ONNX model, it’s easy to use. See TensorRT/tools/onnx-graphsurgeon/examples/04_modifying_a_model at main · NVIDIA/TensorRT · GitHubAs for your second question, as far as I know, trtexec does not support regex expressions. We might add it in future. As for now, you can write a script to search all instance norm layers and generate the trtexec cmd.I tried the layer precision flag in polygraphy. However, I still got large error when norm becomes fp32. Is there anything wrong with my command?Can you add ""-v -v -v "" options to Polygraphy and share the whole Polygraphy log to me?Here is the log: https://cloud.tsinghua.edu.cn/f/85eee484ca024503bd31/?dl=1It seems that this time the error becomes incredibly larger…Yes, I think this seems like a TRT bug. I can repro it in TRT 8.5, but seems this issue has been fixed in TRT 8.6. You can wait the TRT 8.6 release to verify it, which should be released next month.Alright… Looking forward to the new release!Moreover, is there any workaround before the new release comes up? I notice there are group normalization plugins in the repo. However, they don’t have any docs or readmes. I wonder if I can use them? And If yes, how?Powered by Discourse, best viewed with JavaScript enabled"
295,installing-cudnn,"I’m on step 3 of section 3.2 of this installation guide. Installation Guide :: NVIDIA Deep Learning cuDNN Documentation . My goal is to be able to load the dynamic library cudnn64_8.dll . The instructions say to copy some files into some destination folders. I have the files. But I don’t have the destination folders. Should I create the destination folders such as C:\Program Files\NVIDIA\CUDNN\v8.x\bin or was I supposed to get these folders from somewhere else? To be clear, I don’t even have C:\Program Files\NVIDIA\. I only have C:\Program Files\NVIDIA Corporation and C:\Program Files\NVIDIA GPU Computing Toolkit.
I’m running some TensorFlow code and I get this warning.
2022-10-02 22:39:44.797031: W tensorflow/stream_executor/platform/default/dso_loader.cc:60] Could not load dynamic library 'cudnn64_8.dll'; dlerror: cudnn64_8.dll not found
What should I do?Hi @urkchar ,
Which version of cudnn you are using?
I believe this post should be able to help you.Thanks!I solved the issue using miniconda3 to install TensorFlow and most of its requirements. This topic may be closed now. Thank you.I have this exact same problem. What should I do?I have the same problem. I have installed CUDA Toolkit 11.8. Am trying to install cuDNN 8.4.1.50. Can downlod the file. The instructions ask you to unzip it in the . What  Do I create a new folder called NVIDIA|cuDNN\v.8.4.1 in my program files? The instructions are extremely convoluted. I wish someone would post clear instructions. I will try using miconda3. However, do I not need this to be installed and have an environment path for me to use it in Tensorflow 2.10? Thanksyou have to put environment variables in windows.
First create for example a tools folder in C.
Put in cudnn and tensorrt folder separately.
In environment variables windows put in path:
cudnn/lib
cudnn/include
and tensorrt the sameCame here with the same question. Sadly I don’t see an answer that is relevant. If I figure it out, I’ll post the answer. The instructions need to be updated.I figured out the answer, there is a new name for the folder.For me, instead ofCopy bin\cudnn*.dll to C:\Program Files\NVIDIA\CUDNN\v8.x\bin.I had to doCopy bin\cudnn*.dll to C:\Program Files\NVIDIA GPU Computing Toolkit\CUDNN\v12.x\bin.This also applies to the ‘include’ and ‘lib’ folder.If people are curious about my version, I’m on 12.1I had the same problem and no answer to be found.
So to install cudnn I just use what I have, I have this in my machine “C:\Program Files\NVIDIA GPU Computing Toolkit\CUDA\v12.1\bin” so I used it and it seems like working.I said it was working as I was able to compile get the darknet.exe based on the steps belowYOLOv4 / Scaled-YOLOv4 / YOLO - Neural Networks for Object Detection (Windows and Linux version of Darknet ) - GitHub - AlexeyAB/darknet: YOLOv4 / Scaled-YOLOv4 / YOLO - Neural Networks for Object ...Before I install cudnn, I used to get the error “Could NOT find CUDNN (missing: CUDNN_INCLUDE_DIR CUDNN_LIBRARY)” in the cMake logs.I have the same problem,I don’t have the floder “C:/NVIDIA”,so sad.It waste a lot of timeSame problem here, don’t have C:\Program Files\NVIDIA\CUDNN\ or C:\Program Files\NVIDIA GPU Computing Toolkit\CUDNN\v12.x. OMG Nvidia just please double-check the installation guide before publishing.Powered by Discourse, best viewed with JavaScript enabled"
296,error-an-iloopoutputlayer-cannot-be-used-to-compute-a-shape-tensor,"Our model is not able to run in TensorRT,  I have tried both running in ORT with TensorrtExecutionProvider and using trtexec to convert it to a plan.For compliance reason, I cannot share my full model, but I can build a mini one to repro my issue.please check the model binary directly:
tiny_model.onnx (15.9 KB)
OR build from the script:
tiny_model_builder.py (1.1 KB)Using ORT + TensorrtExecutionProvider reports error:Error Code 4: Internal Error ((Unnamed Layer* 23) [LoopOutput]: an ILoopOutputLayer cannot be used to compute a shape tensor)
ORT_repro.py (690 Bytes)Using  trtexec --onnx=tiny_model.onnx --saveEngine=tiny_model.trt --memPoolSize=workspace:10000 --minShapes=input:1x1 --maxShapes=input:1x512 --optShapes=input:1x64 --device=0 --verbose will show below error:
[02/21/2023-14:36:04] [E] Error[2]: [topSort.cpp::trivialChoice::314] Error Code 2: Internal Error (Assertion c != kNoColor failed. )  
 [02/21/2023-14:36:04] [E] Error[2]: [builder.cpp::buildSerializedNetwork::738] Error Code 2: Internal Error (Assertion engine != nullptr failed. )TensorRT Version:  8.5.0
GPU Type: T4
Nvidia Driver Version:  470.161.03
CUDA Version:  11.8
CUDNN Version:
Operating System + Version:  Ubuntu20.04
Python Version (if applicable):
TensorFlow Version (if applicable):
PyTorch Version (if applicable):
Baremetal or Container (if container which image + tag):Please attach or include links to any models, data, files, or scripts necessary to reproduce your issue. (Github repo, Google Drive, Dropbox, etc.)Please include:Hi @niuzheng168 ,
Apologies for the delayed response.
Can you please share the verbose logs with us, meanwhile we are trying to reproduce the issue from our end.ThanksPowered by Discourse, best viewed with JavaScript enabled"
297,setting-rapidsai-node-on-centos-ec2,"Hi,
I am trying to setup a dev environment using rapidsai/node on EC2 machine but having build errors when when run yarn cpp:build. It looks like the git url for cumlprims_mg is set for internal use only.Powered by Discourse, best viewed with JavaScript enabled"
298,pluginv2runner-cpp-265-error-code-2-internal-error-assertion-status-kstatus-scuess-failed,"Hi,
I upgraded my tensorRT 7 plugin c++ code so it’s compatible to tensorRT 8, and I keep getting this error while building  engine.I’m hoping someone can tell me more about this error msg, or point me to documents that can explain it. Does it mean that the building process failed due while processing a pluginV2?Any suggestion is highly appreciated. Thanks in advance.Hi,Could you please give us more details.
TensorRT version, GPU, Platform, CUDA version, Driver version.
Please try the following and share with us complete logs and if possible minimum issue repro ONNX model and scripts.## Description

Hi, I have build a model that has three inputs with tensorrt p…ython api layer by layer, and I develop a 3d grid sample plugin to build voxel. Everything runs well when building an engine, but when the engine is used fo inference there's an error thrown, Is anyone knows what's the problem. Part of the  log is below:
 Layer(Constant): (Unnamed Layer* 329) [Constant], Tactic: 0,  -> (Unnamed Layer* 329) [Constant]_output[Float(1,32,1)]
Layer(PointWiseV2): PWN(PWN(PWN((Unnamed Layer* 331) [ElementWise], (Unnamed Layer* 332) [ElementWise]), (Unnamed Layer* 333) [ElementWise]), (Unnamed Layer* 334) [ElementWise]), Tactic: 6, (Unnamed Layer* 327) [Reduce]_output[Float(1,32,1)], (Unnamed Layer* 329) [Constant]_output[Float(1,32,1)], (Unnamed Layer* 328) [Constant]_output[Float(1,32,1)], (Unnamed Layer* 330) [Constant]_output[Float(1,32,1)], (Unnamed Layer* 325) [ElementWise]_output[Float(1,32,199680)] -> (Unnamed Layer* 334) [ElementWise]_output[Float(1,32,199680)]
Layer(Scale): (Unnamed Layer* 336) [Scale], Tactic: 0, (Unnamed Layer* 335) [Shuffle]_output[Float(1,64,160,624)] -> (Unnamed Layer* 336) [Scale]_output[Float(1,64,160,624)]
Layer(ElementWise): (Unnamed Layer* 338) [ElementWise] + (Unnamed Layer* 339) [Activation], Tactic: 1, (Unnamed Layer* 337) [Resize]_output[Float(1,64,160,624)], (Unnamed Layer* 336) [Scale]_output[Float(1,64,160,624)] -> (Unnamed Layer* 339) [Activation]_output[Float(1,64,160,624)]
Layer(Reduce): (Unnamed Layer* 512) [Reduce], Tactic: 1, (Unnamed Layer* 511) [Shuffle]_output[Float(1,32,199680)] -> (Unnamed Layer* 512) [Reduce]_output[Float(1,32,1)]
Layer(ElementWise): (Unnamed Layer* 513) [ElementWise], Tactic: 1, (Unnamed Layer* 511) [Shuffle]_output[Float(1,32,199680)], (Unnamed Layer* 512) [Reduce]_output[Float(1,32,1)] -> (Unnamed Layer* 513) [ElementWise]_output[Float(1,32,199680)]
Layer(Constant): (Unnamed Layer* 518) [Constant], Tactic: 0,  -> (Unnamed Layer* 518) [Constant]_output[Float(1,32,1)]
Layer(Constant): (Unnamed Layer* 516) [Constant], Tactic: 0,  -> (Unnamed Layer* 516) [Constant]_output[Float(1,32,1)]
Layer(Reduce): (Unnamed Layer* 514) [ElementWise] + (Unnamed Layer* 515) [Reduce], Tactic: 2, (Unnamed Layer* 513) [ElementWise]_output[Float(1,32,199680)] -> (Unnamed Layer* 515) [Reduce]_output[Float(1,32,1)]
Layer(Constant): (Unnamed Layer* 517) [Constant], Tactic: 0,  -> (Unnamed Layer* 517) [Constant]_output[Float(1,32,1)]
Layer(PointWiseV2): PWN(PWN(PWN((Unnamed Layer* 519) [ElementWise], (Unnamed Layer* 520) [ElementWise]), (Unnamed Layer* 521) [ElementWise]), (Unnamed Layer* 522) [ElementWise]), Tactic: 6, (Unnamed Layer* 515) [Reduce]_output[Float(1,32,1)], (Unnamed Layer* 517) [Constant]_output[Float(1,32,1)], (Unnamed Layer* 516) [Constant]_output[Float(1,32,1)], (Unnamed Layer* 518) [Constant]_output[Float(1,32,1)], (Unnamed Layer* 513) [ElementWise]_output[Float(1,32,199680)] -> (Unnamed Layer* 522) [ElementWise]_output[Float(1,32,199680)]
Layer(Scale): (Unnamed Layer* 524) [Scale], Tactic: 0, (Unnamed Layer* 523) [Shuffle]_output[Float(1,64,160,624)] -> (Unnamed Layer* 524) [Scale]_output[Float(1,64,160,624)]
Layer(ElementWise): (Unnamed Layer* 526) [ElementWise] + (Unnamed Layer* 527) [Activation], Tactic: 1, (Unnamed Layer* 525) [Resize]_output[Float(1,64,160,624)], (Unnamed Layer* 524) [Scale]_output[Float(1,64,160,624)] -> (Unnamed Layer* 527) [Activation]_output[Float(1,64,160,624)]
Layer(CaskConvolution): (Unnamed Layer* 528) [Convolution], Tactic: 2775507031594384867, (Unnamed Layer* 527) [Activation]_output[Float(1,64,160,624)] -> (Unnamed Layer* 528) [Convolution]_output[Float(1,32,160,624)]
Layer(CaskConvolution): (Unnamed Layer* 340) [Convolution], Tactic: 2775507031594384867, (Unnamed Layer* 339) [Activation]_output[Float(1,64,160,624)] -> (Unnamed Layer* 340) [Convolution]_output[Float(1,32,160,624)]
Layer(Reduce): (Unnamed Layer* 342) [Reduce], Tactic: 1, (Unnamed Layer* 341) [Shuffle]_output[Float(1,32,99840)] -> (Unnamed Layer* 342) [Reduce]_output[Float(1,32,1)]
Layer(ElementWise): (Unnamed Layer* 343) [ElementWise], Tactic: 1, (Unnamed Layer* 341) [Shuffle]_output[Float(1,32,99840)], (Unnamed Layer* 342) [Reduce]_output[Float(1,32,1)] -> (Unnamed Layer* 343) [ElementWise]_output[Float(1,32,99840)]
Layer(Constant): (Unnamed Layer* 348) [Constant], Tactic: 0,  -> (Unnamed Layer* 348) [Constant]_output[Float(1,32,1)]
Layer(Constant): (Unnamed Layer* 346) [Constant], Tactic: 0,  -> (Unnamed Layer* 346) [Constant]_output[Float(1,32,1)]
Layer(Reduce): (Unnamed Layer* 344) [ElementWise] + (Unnamed Layer* 345) [Reduce], Tactic: 2, (Unnamed Layer* 343) [ElementWise]_output[Float(1,32,99840)] -> (Unnamed Layer* 345) [Reduce]_output[Float(1,32,1)]
Layer(Constant): (Unnamed Layer* 347) [Constant], Tactic: 0,  -> (Unnamed Layer* 347) [Constant]_output[Float(1,32,1)]
Layer(PointWiseV2): PWN(PWN(PWN((Unnamed Layer* 349) [ElementWise], (Unnamed Layer* 350) [ElementWise]), (Unnamed Layer* 351) [ElementWise]), (Unnamed Layer* 352) [ElementWise]), Tactic: 5, (Unnamed Layer* 345) [Reduce]_output[Float(1,32,1)], (Unnamed Layer* 347) [Constant]_output[Float(1,32,1)], (Unnamed Layer* 346) [Constant]_output[Float(1,32,1)], (Unnamed Layer* 348) [Constant]_output[Float(1,32,1)], (Unnamed Layer* 343) [ElementWise]_output[Float(1,32,99840)] -> (Unnamed Layer* 352) [ElementWise]_output[Float(1,32,99840)]
Layer(Scale): (Unnamed Layer* 354) [Scale], Tactic: 0, (Unnamed Layer* 353) [Shuffle]_output[Float(1,32,160,624)] -> (Unnamed Layer* 354) [Scale]_output[Float(1,32,160,624)]
Layer(Reduce): (Unnamed Layer* 530) [Reduce], Tactic: 1, (Unnamed Layer* 529) [Shuffle]_output[Float(1,32,99840)] -> (Unnamed Layer* 530) [Reduce]_output[Float(1,32,1)]
Layer(ElementWise): (Unnamed Layer* 531) [ElementWise], Tactic: 1, (Unnamed Layer* 529) [Shuffle]_output[Float(1,32,99840)], (Unnamed Layer* 530) [Reduce]_output[Float(1,32,1)] -> (Unnamed Layer* 531) [ElementWise]_output[Float(1,32,99840)]
Layer(Constant): (Unnamed Layer* 536) [Constant], Tactic: 0,  -> (Unnamed Layer* 536) [Constant]_output[Float(1,32,1)]
Layer(Constant): (Unnamed Layer* 534) [Constant], Tactic: 0,  -> (Unnamed Layer* 534) [Constant]_output[Float(1,32,1)]
Layer(Reduce): (Unnamed Layer* 532) [ElementWise] + (Unnamed Layer* 533) [Reduce], Tactic: 2, (Unnamed Layer* 531) [ElementWise]_output[Float(1,32,99840)] -> (Unnamed Layer* 533) [Reduce]_output[Float(1,32,1)]
Layer(Constant): (Unnamed Layer* 535) [Constant], Tactic: 0,  -> (Unnamed Layer* 535) [Constant]_output[Float(1,32,1)]
Layer(PointWiseV2): PWN(PWN(PWN((Unnamed Layer* 537) [ElementWise], (Unnamed Layer* 538) [ElementWise]), (Unnamed Layer* 539) [ElementWise]), (Unnamed Layer* 540) [ElementWise]), Tactic: 5, (Unnamed Layer* 533) [Reduce]_output[Float(1,32,1)], (Unnamed Layer* 535) [Constant]_output[Float(1,32,1)], (Unnamed Layer* 534) [Constant]_output[Float(1,32,1)], (Unnamed Layer* 536) [Constant]_output[Float(1,32,1)], (Unnamed Layer* 531) [ElementWise]_output[Float(1,32,99840)] -> (Unnamed Layer* 540) [ElementWise]_output[Float(1,32,99840)]
Layer(Scale): (Unnamed Layer* 542) [Scale], Tactic: 0, (Unnamed Layer* 541) [Shuffle]_output[Float(1,32,160,624)] -> (Unnamed Layer* 542) [Scale]_output[Float(1,32,160,624)]
Layer(Resize): (Unnamed Layer* 558) [Resize], Tactic: 1, (Unnamed Layer* 542) [Scale]_output[Float(1,32,160,624)] -> (Unnamed Layer* 558) [Resize]_output[Float(1,32,320,1248)]
Layer(Resize): (Unnamed Layer* 370) [Resize], Tactic: 1, (Unnamed Layer* 354) [Scale]_output[Float(1,32,160,624)] -> (Unnamed Layer* 370) [Resize]_output[Float(1,32,320,1248)]
Layer(Reduce): (Unnamed Layer* 357) [Reduce], Tactic: 1, (Unnamed Layer* 356) [Shuffle]_output[Float(1,32,399360)] -> (Unnamed Layer* 357) [Reduce]_output[Float(1,32,1)]
Layer(ElementWise): (Unnamed Layer* 358) [ElementWise], Tactic: 1, (Unnamed Layer* 356) [Shuffle]_output[Float(1,32,399360)], (Unnamed Layer* 357) [Reduce]_output[Float(1,32,1)] -> (Unnamed Layer* 358) [ElementWise]_output[Float(1,32,399360)]
Layer(Constant): (Unnamed Layer* 363) [Constant], Tactic: 0,  -> (Unnamed Layer* 363) [Constant]_output[Float(1,32,1)]
Layer(Constant): (Unnamed Layer* 361) [Constant], Tactic: 0,  -> (Unnamed Layer* 361) [Constant]_output[Float(1,32,1)]
Layer(Reduce): (Unnamed Layer* 359) [ElementWise] + (Unnamed Layer* 360) [Reduce], Tactic: 2, (Unnamed Layer* 358) [ElementWise]_output[Float(1,32,399360)] -> (Unnamed Layer* 360) [Reduce]_output[Float(1,32,1)]
Layer(Constant): (Unnamed Layer* 362) [Constant], Tactic: 0,  -> (Unnamed Layer* 362) [Constant]_output[Float(1,32,1)]
Layer(PointWiseV2): PWN(PWN(PWN((Unnamed Layer* 364) [ElementWise], (Unnamed Layer* 365) [ElementWise]), (Unnamed Layer* 366) [ElementWise]), (Unnamed Layer* 367) [ElementWise]), Tactic: 6, (Unnamed Layer* 360) [Reduce]_output[Float(1,32,1)], (Unnamed Layer* 362) [Constant]_output[Float(1,32,1)], (Unnamed Layer* 361) [Constant]_output[Float(1,32,1)], (Unnamed Layer* 363) [Constant]_output[Float(1,32,1)], (Unnamed Layer* 358) [ElementWise]_output[Float(1,32,399360)] -> (Unnamed Layer* 367) [ElementWise]_output[Float(1,32,399360)]
Layer(Scale): (Unnamed Layer* 369) [Scale], Tactic: 0, (Unnamed Layer* 368) [Shuffle]_output[Float(1,32,320,1248)] -> (Unnamed Layer* 369) [Scale]_output[Float(1,32,320,1248)]
Layer(ElementWise): (Unnamed Layer* 371) [ElementWise] + (Unnamed Layer* 372) [Activation], Tactic: 1, (Unnamed Layer* 370) [Resize]_output[Float(1,32,320,1248)], (Unnamed Layer* 369) [Scale]_output[Float(1,32,320,1248)] -> (Unnamed Layer* 372) [Activation]_output[Float(1,32,320,1248)]
Layer(Reduce): (Unnamed Layer* 545) [Reduce], Tactic: 1, (Unnamed Layer* 544) [Shuffle]_output[Float(1,32,399360)] -> (Unnamed Layer* 545) [Reduce]_output[Float(1,32,1)]
Layer(ElementWise): (Unnamed Layer* 546) [ElementWise], Tactic: 1, (Unnamed Layer* 544) [Shuffle]_output[Float(1,32,399360)], (Unnamed Layer* 545) [Reduce]_output[Float(1,32,1)] -> (Unnamed Layer* 546) [ElementWise]_output[Float(1,32,399360)]
Layer(Constant): (Unnamed Layer* 551) [Constant], Tactic: 0,  -> (Unnamed Layer* 551) [Constant]_output[Float(1,32,1)]
Layer(Constant): (Unnamed Layer* 549) [Constant], Tactic: 0,  -> (Unnamed Layer* 549) [Constant]_output[Float(1,32,1)]
Layer(Reduce): (Unnamed Layer* 547) [ElementWise] + (Unnamed Layer* 548) [Reduce], Tactic: 1, (Unnamed Layer* 546) [ElementWise]_output[Float(1,32,399360)] -> (Unnamed Layer* 548) [Reduce]_output[Float(1,32,1)]
Layer(Constant): (Unnamed Layer* 550) [Constant], Tactic: 0,  -> (Unnamed Layer* 550) [Constant]_output[Float(1,32,1)]
Layer(PointWiseV2): PWN(PWN(PWN((Unnamed Layer* 552) [ElementWise], (Unnamed Layer* 553) [ElementWise]), (Unnamed Layer* 554) [ElementWise]), (Unnamed Layer* 555) [ElementWise]), Tactic: 6, (Unnamed Layer* 548) [Reduce]_output[Float(1,32,1)], (Unnamed Layer* 550) [Constant]_output[Float(1,32,1)], (Unnamed Layer* 549) [Constant]_output[Float(1,32,1)], (Unnamed Layer* 551) [Constant]_output[Float(1,32,1)], (Unnamed Layer* 546) [ElementWise]_output[Float(1,32,399360)] -> (Unnamed Layer* 555) [ElementWise]_output[Float(1,32,399360)]
Layer(Scale): (Unnamed Layer* 557) [Scale], Tactic: 0, (Unnamed Layer* 556) [Shuffle]_output[Float(1,32,320,1248)] -> (Unnamed Layer* 557) [Scale]_output[Float(1,32,320,1248)]
Layer(ElementWise): (Unnamed Layer* 559) [ElementWise] + (Unnamed Layer* 560) [Activation], Tactic: 1, (Unnamed Layer* 558) [Resize]_output[Float(1,32,320,1248)], (Unnamed Layer* 557) [Scale]_output[Float(1,32,320,1248)] -> (Unnamed Layer* 560) [Activation]_output[Float(1,32,320,1248)]
Layer(CudnnConvolution): (Unnamed Layer* 561) [Convolution], Tactic: 6, (Unnamed Layer* 560) [Activation]_output[Float(1,32,320,1248)] -> (Unnamed Layer* 561) [Convolution]_output[Float(1,32,320,1248)]
Layer(CudnnConvolution): (Unnamed Layer* 373) [Convolution], Tactic: 6, (Unnamed Layer* 372) [Activation]_output[Float(1,32,320,1248)] -> (Unnamed Layer* 373) [Convolution]_output[Float(1,32,320,1248)]
Layer(Reduce): (Unnamed Layer* 375) [Reduce], Tactic: 1, (Unnamed Layer* 374) [Shuffle]_output[Float(1,32,399360)] -> (Unnamed Layer* 375) [Reduce]_output[Float(1,32,1)]
Layer(ElementWise): (Unnamed Layer* 376) [ElementWise], Tactic: 1, (Unnamed Layer* 374) [Shuffle]_output[Float(1,32,399360)], (Unnamed Layer* 375) [Reduce]_output[Float(1,32,1)] -> (Unnamed Layer* 376) [ElementWise]_output[Float(1,32,399360)]
Layer(Constant): (Unnamed Layer* 381) [Constant], Tactic: 0,  -> (Unnamed Layer* 381) [Constant]_output[Float(1,32,1)]
Layer(Constant): (Unnamed Layer* 379) [Constant], Tactic: 0,  -> (Unnamed Layer* 379) [Constant]_output[Float(1,32,1)]
Layer(Reduce): (Unnamed Layer* 377) [ElementWise] + (Unnamed Layer* 378) [Reduce], Tactic: 1, (Unnamed Layer* 376) [ElementWise]_output[Float(1,32,399360)] -> (Unnamed Layer* 378) [Reduce]_output[Float(1,32,1)]
Layer(Constant): (Unnamed Layer* 380) [Constant], Tactic: 0,  -> (Unnamed Layer* 380) [Constant]_output[Float(1,32,1)]
Layer(PointWiseV2): PWN(PWN(PWN((Unnamed Layer* 382) [ElementWise], (Unnamed Layer* 383) [ElementWise]), (Unnamed Layer* 384) [ElementWise]), (Unnamed Layer* 385) [ElementWise]), Tactic: 6, (Unnamed Layer* 378) [Reduce]_output[Float(1,32,1)], (Unnamed Layer* 380) [Constant]_output[Float(1,32,1)], (Unnamed Layer* 379) [Constant]_output[Float(1,32,1)], (Unnamed Layer* 381) [Constant]_output[Float(1,32,1)], (Unnamed Layer* 376) [ElementWise]_output[Float(1,32,399360)] -> (Unnamed Layer* 385) [ElementWise]_output[Float(1,32,399360)]
Layer(Scale): (Unnamed Layer* 387) [Scale] + (Unnamed Layer* 388) [Activation], Tactic: 0, (Unnamed Layer* 386) [Shuffle]_output[Float(1,32,320,1248)] -> (Unnamed Layer* 388) [Activation]_output[Float(1,32,320,1248)]
Layer(Reduce): (Unnamed Layer* 563) [Reduce], Tactic: 1, (Unnamed Layer* 562) [Shuffle]_output[Float(1,32,399360)] -> (Unnamed Layer* 563) [Reduce]_output[Float(1,32,1)]
Layer(ElementWise): (Unnamed Layer* 564) [ElementWise], Tactic: 1, (Unnamed Layer* 562) [Shuffle]_output[Float(1,32,399360)], (Unnamed Layer* 563) [Reduce]_output[Float(1,32,1)] -> (Unnamed Layer* 564) [ElementWise]_output[Float(1,32,399360)]
Layer(Constant): (Unnamed Layer* 569) [Constant], Tactic: 0,  -> (Unnamed Layer* 569) [Constant]_output[Float(1,32,1)]
Layer(Constant): (Unnamed Layer* 567) [Constant], Tactic: 0,  -> (Unnamed Layer* 567) [Constant]_output[Float(1,32,1)]
Layer(Reduce): (Unnamed Layer* 565) [ElementWise] + (Unnamed Layer* 566) [Reduce], Tactic: 2, (Unnamed Layer* 564) [ElementWise]_output[Float(1,32,399360)] -> (Unnamed Layer* 566) [Reduce]_output[Float(1,32,1)]
Layer(Constant): (Unnamed Layer* 568) [Constant], Tactic: 0,  -> (Unnamed Layer* 568) [Constant]_output[Float(1,32,1)]
Layer(PointWiseV2): PWN(PWN(PWN((Unnamed Layer* 570) [ElementWise], (Unnamed Layer* 571) [ElementWise]), (Unnamed Layer* 572) [ElementWise]), (Unnamed Layer* 573) [ElementWise]), Tactic: 6, (Unnamed Layer* 566) [Reduce]_output[Float(1,32,1)], (Unnamed Layer* 568) [Constant]_output[Float(1,32,1)], (Unnamed Layer* 567) [Constant]_output[Float(1,32,1)], (Unnamed Layer* 569) [Constant]_output[Float(1,32,1)], (Unnamed Layer* 564) [ElementWise]_output[Float(1,32,399360)] -> (Unnamed Layer* 573) [ElementWise]_output[Float(1,32,399360)]
Layer(Scale): (Unnamed Layer* 575) [Scale] + (Unnamed Layer* 576) [Activation], Tactic: 0, (Unnamed Layer* 574) [Shuffle]_output[Float(1,32,320,1248)] -> (Unnamed Layer* 576) [Activation]_output[Float(1,32,320,1248)]
Layer(CaskConvolution): (Unnamed Layer* 577) [Convolution], Tactic: -3946921629105938337, (Unnamed Layer* 576) [Activation]_output[Float(1,32,320,1248)] -> (Unnamed Layer* 577) [Convolution]_output[Float(1,32,320,1248)]
Layer(CaskConvolution): (Unnamed Layer* 389) [Convolution], Tactic: -3946921629105938337, (Unnamed Layer* 388) [Activation]_output[Float(1,32,320,1248)] -> (Unnamed Layer* 389) [Convolution]_output[Float(1,32,320,1248)]
Layer(Constant): (Unnamed Layer* 616) [Constant], Tactic: 0,  -> (Unnamed Layer* 616) [Constant]_output[Float(72,1)]
Layer(Constant): (Unnamed Layer* 617) [Constant], Tactic: 0,  -> (Unnamed Layer* 617) [Constant]_output[Float(72,1)]
Layer(ElementWise): (Unnamed Layer* 618) [ElementWise], Tactic: 1, (Unnamed Layer* 617) [Constant]_output[Float(72,1)], (Unnamed Layer* 616) [Constant]_output[Float(72,1)] -> (Unnamed Layer* 618) [ElementWise]_output[Float(72,1)]
Layer(PluginV2): (Unnamed Layer* 620) [PluginV2DynamicExt], Tactic: 0, (Unnamed Layer* 389) [Convolution]_output[Float(1,32,320,1248)], (Unnamed Layer* 577) [Convolution]_output[Float(1,32,320,1248)], (Unnamed Layer* 619) [Shuffle]_output[Float(1,72,1,1)] -> (Unnamed Layer* 620) [PluginV2DynamicExt]_output_0[Float(64,72,80,312)]
Layer(CudnnConvolution): (Unnamed Layer* 622) [Convolution], Tactic: 57, (Unnamed Layer* 621) [Shuffle]_output[Float(1,64,72,80,312)] -> (Unnamed Layer* 622) [Convolution]_output[Float(1,32,72,80,312)]
Layer(Constant): (Unnamed Layer* 623) [Constant], Tactic: 0,  -> (Unnamed Layer* 623) [Constant]_output[Float(32,1)]
Layer(Constant): (Unnamed Layer* 624) [Constant], Tactic: 0,  -> (Unnamed Layer* 624) [Constant]_output[Float(32,1)]
Layer(PluginV2): (Unnamed Layer* 625) [PluginV2DynamicExt], Tactic: 0, (Unnamed Layer* 622) [Convolution]_output[Float(1,32,72,80,312)], (Unnamed Layer* 623) [Constant]_output[Float(32,1)], (Unnamed Layer* 624) [Constant]_output[Float(32,1)] -> (Unnamed Layer* 625) [PluginV2DynamicExt]_output_0[Float(1,32,72,80,312)]
Layer(PointWiseV2): PWN((Unnamed Layer* 626) [Activation]), Tactic: 8, (Unnamed Layer* 625) [PluginV2DynamicExt]_output_0[Float(1,32,72,80,312)] -> (Unnamed Layer* 626) [Activation]_output[Float(1,32,72,80,312)]
Layer(CudnnConvolution): (Unnamed Layer* 627) [Convolution], Tactic: 57, (Unnamed Layer* 626) [Activation]_output[Float(1,32,72,80,312)] -> (Unnamed Layer* 627) [Convolution]_output[Float(1,32,72,80,312)]
Layer(Constant): (Unnamed Layer* 628) [Constant], Tactic: 0,  -> (Unnamed Layer* 628) [Constant]_output[Float(32,1)]
Layer(Constant): (Unnamed Layer* 629) [Constant], Tactic: 0,  -> (Unnamed Layer* 629) [Constant]_output[Float(32,1)]
Layer(PluginV2): (Unnamed Layer* 630) [PluginV2DynamicExt], Tactic: 0, (Unnamed Layer* 627) [Convolution]_output[Float(1,32,72,80,312)], (Unnamed Layer* 628) [Constant]_output[Float(32,1)], (Unnamed Layer* 629) [Constant]_output[Float(32,1)] -> (Unnamed Layer* 630) [PluginV2DynamicExt]_output_0[Float(1,32,72,80,312)]
Layer(PointWiseV2): PWN(PWN((Unnamed Layer* 631) [Activation]), (Unnamed Layer* 632) [ElementWise]), Tactic: 8, (Unnamed Layer* 630) [PluginV2DynamicExt]_output_0[Float(1,32,72,80,312)], (Unnamed Layer* 626) [Activation]_output[Float(1,32,72,80,312)] -> (Unnamed Layer* 632) [ElementWise]_output[Float(1,32,72,80,312)]
Layer(CudnnConvolution): (Unnamed Layer* 633) [Convolution], Tactic: 1, (Unnamed Layer* 632) [ElementWise]_output[Float(1,32,72,80,312)] -> (Unnamed Layer* 633) [Convolution]_output[Float(1,64,36,40,156)]
Layer(Constant): (Unnamed Layer* 634) [Constant], Tactic: 0,  -> (Unnamed Layer* 634) [Constant]_output[Float(64,1)]
Layer(Constant): (Unnamed Layer* 635) [Constant], Tactic: 0,  -> (Unnamed Layer* 635) [Constant]_output[Float(64,1)]
Layer(PluginV2): (Unnamed Layer* 636) [PluginV2DynamicExt], Tactic: 0, (Unnamed Layer* 633) [Convolution]_output[Float(1,64,36,40,156)], (Unnamed Layer* 634) [Constant]_output[Float(64,1)], (Unnamed Layer* 635) [Constant]_output[Float(64,1)] -> (Unnamed Layer* 636) [PluginV2DynamicExt]_output_0[Float(1,64,36,40,156)]
Layer(PointWiseV2): PWN((Unnamed Layer* 637) [Activation]), Tactic: 8, (Unnamed Layer* 636) [PluginV2DynamicExt]_output_0[Float(1,64,36,40,156)] -> (Unnamed Layer* 637) [Activation]_output[Float(1,64,36,40,156)]
Layer(CudnnConvolution): (Unnamed Layer* 638) [Convolution], Tactic: 57, (Unnamed Layer* 637) [Activation]_output[Float(1,64,36,40,156)] -> (Unnamed Layer* 638) [Convolution]_output[Float(1,64,36,40,156)]
Layer(Constant): (Unnamed Layer* 639) [Constant], Tactic: 0,  -> (Unnamed Layer* 639) [Constant]_output[Float(64,1)]
Layer(Constant): (Unnamed Layer* 640) [Constant], Tactic: 0,  -> (Unnamed Layer* 640) [Constant]_output[Float(64,1)]
Layer(PluginV2): (Unnamed Layer* 641) [PluginV2DynamicExt], Tactic: 0, (Unnamed Layer* 638) [Convolution]_output[Float(1,64,36,40,156)], (Unnamed Layer* 639) [Constant]_output[Float(64,1)], (Unnamed Layer* 640) [Constant]_output[Float(64,1)] -> (Unnamed Layer* 641) [PluginV2DynamicExt]_output_0[Float(1,64,36,40,156)]
Layer(PointWiseV2): PWN((Unnamed Layer* 642) [Activation]), Tactic: 8, (Unnamed Layer* 641) [PluginV2DynamicExt]_output_0[Float(1,64,36,40,156)] -> (Unnamed Layer* 642) [Activation]_output[Float(1,64,36,40,156)]
Layer(CudnnConvolution): (Unnamed Layer* 643) [Convolution], Tactic: 1, (Unnamed Layer* 642) [Activation]_output[Float(1,64,36,40,156)] -> (Unnamed Layer* 643) [Convolution]_output[Float(1,64,18,20,78)]
Layer(Constant): (Unnamed Layer* 644) [Constant], Tactic: 0,  -> (Unnamed Layer* 644) [Constant]_output[Float(64,1)]
Layer(Constant): (Unnamed Layer* 645) [Constant], Tactic: 0,  -> (Unnamed Layer* 645) [Constant]_output[Float(64,1)]
Layer(PluginV2): (Unnamed Layer* 646) [PluginV2DynamicExt], Tactic: 0, (Unnamed Layer* 643) [Convolution]_output[Float(1,64,18,20,78)], (Unnamed Layer* 644) [Constant]_output[Float(64,1)], (Unnamed Layer* 645) [Constant]_output[Float(64,1)] -> (Unnamed Layer* 646) [PluginV2DynamicExt]_output_0[Float(1,64,18,20,78)]
Layer(PointWiseV2): PWN((Unnamed Layer* 647) [Activation]), Tactic: 8, (Unnamed Layer* 646) [PluginV2DynamicExt]_output_0[Float(1,64,18,20,78)] -> (Unnamed Layer* 647) [Activation]_output[Float(1,64,18,20,78)]
Layer(CudnnConvolution): (Unnamed Layer* 648) [Convolution], Tactic: 1, (Unnamed Layer* 647) [Activation]_output[Float(1,64,18,20,78)] -> (Unnamed Layer* 648) [Convolution]_output[Float(1,64,18,20,78)]
Layer(Constant): (Unnamed Layer* 649) [Constant], Tactic: 0,  -> (Unnamed Layer* 649) [Constant]_output[Float(64,1)]
Layer(Constant): (Unnamed Layer* 650) [Constant], Tactic: 0,  -> (Unnamed Layer* 650) [Constant]_output[Float(64,1)]
Layer(PluginV2): (Unnamed Layer* 651) [PluginV2DynamicExt], Tactic: 0, (Unnamed Layer* 648) [Convolution]_output[Float(1,64,18,20,78)], (Unnamed Layer* 649) [Constant]_output[Float(64,1)], (Unnamed Layer* 650) [Constant]_output[Float(64,1)] -> (Unnamed Layer* 651) [PluginV2DynamicExt]_output_0[Float(1,64,18,20,78)]
Layer(PointWiseV2): PWN((Unnamed Layer* 652) [Activation]), Tactic: 8, (Unnamed Layer* 651) [PluginV2DynamicExt]_output_0[Float(1,64,18,20,78)] -> (Unnamed Layer* 652) [Activation]_output[Float(1,64,18,20,78)]
Layer(CudnnDeconvolution): (Unnamed Layer* 653) [Deconvolution], Tactic: 0, (Unnamed Layer* 652) [Activation]_output[Float(1,64,18,20,78)] -> (Unnamed Layer* 653) [Deconvolution]_output[Float(1,64,35,39,155)]
Layer(Constant): (Unnamed Layer* 654) [Constant], Tactic: 0,  -> (Unnamed Layer* 654) [Constant]_output[Float(1,836160)]
Layer(Reformat): (Unnamed Layer* 655) [Shuffle]_output copy, Tactic: 0, (Unnamed Layer* 655) [Shuffle]_output[Float(1,13540800)] -> (Unnamed Layer* 656) [Concatenation]_output[Float(1,13540800)]
Layer(Reformat): (Unnamed Layer* 654) [Constant]_output copy, Tactic: 0, (Unnamed Layer* 654) [Constant]_output[Float(1,836160)] -> (Unnamed Layer* 656) [Concatenation]_output[Float(1,836160)]
Layer(Constant): (Unnamed Layer* 658) [Constant], Tactic: 0,  -> (Unnamed Layer* 658) [Constant]_output[Float(64,1)]
Layer(Constant): (Unnamed Layer* 659) [Constant], Tactic: 0,  -> (Unnamed Layer* 659) [Constant]_output[Float(64,1)]
Layer(PluginV2): (Unnamed Layer* 660) [PluginV2DynamicExt], Tactic: 0, (Unnamed Layer* 657) [Shuffle]_output[Float(1,64,36,40,156)], (Unnamed Layer* 658) [Constant]_output[Float(64,1)], (Unnamed Layer* 659) [Constant]_output[Float(64,1)] -> (Unnamed Layer* 660) [PluginV2DynamicExt]_output_0[Float(1,64,36,40,156)]
Layer(ElementWise): (Unnamed Layer* 661) [ElementWise] + (Unnamed Layer* 662) [Activation], Tactic: 1, (Unnamed Layer* 660) [PluginV2DynamicExt]_output_0[Float(1,64,36,40,156)], (Unnamed Layer* 642) [Activation]_output[Float(1,64,36,40,156)] -> (Unnamed Layer* 662) [Activation]_output[Float(1,64,36,40,156)]
Layer(CudnnDeconvolution): (Unnamed Layer* 663) [Deconvolution], Tactic: 0, (Unnamed Layer* 662) [Activation]_output[Float(1,64,36,40,156)] -> (Unnamed Layer* 663) [Deconvolution]_output[Float(1,32,71,79,311)]
Layer(Constant): (Unnamed Layer* 664) [Constant], Tactic: 0,  -> (Unnamed Layer* 664) [Constant]_output[Float(1,1687072)]
Layer(Reformat): (Unnamed Layer* 665) [Shuffle]_output copy, Tactic: 0, (Unnamed Layer* 665) [Shuffle]_output[Float(1,55820768)] -> (Unnamed Layer* 666) [Concatenation]_output[Float(1,55820768)]
Layer(Reformat): (Unnamed Layer* 664) [Constant]_output copy, Tactic: 0, (Unnamed Layer* 664) [Constant]_output[Float(1,1687072)] -> (Unnamed Layer* 666) [Concatenation]_output[Float(1,1687072)]
Layer(Constant): (Unnamed Layer* 668) [Constant], Tactic: 0,  -> (Unnamed Layer* 668) [Constant]_output[Float(32,1)]
Layer(Constant): (Unnamed Layer* 669) [Constant], Tactic: 0,  -> (Unnamed Layer* 669) [Constant]_output[Float(32,1)]
Layer(PluginV2): (Unnamed Layer* 670) [PluginV2DynamicExt], Tactic: 0, (Unnamed Layer* 667) [Shuffle]_output[Float(1,32,72,80,312)], (Unnamed Layer* 668) [Constant]_output[Float(32,1)], (Unnamed Layer* 669) [Constant]_output[Float(32,1)] -> (Unnamed Layer* 670) [PluginV2DynamicExt]_output_0[Float(1,32,72,80,312)]
Layer(ElementWise): (Unnamed Layer* 671) [ElementWise], Tactic: 1, (Unnamed Layer* 632) [ElementWise]_output[Float(1,32,72,80,312)], (Unnamed Layer* 670) [PluginV2DynamicExt]_output_0[Float(1,32,72,80,312)] -> (Unnamed Layer* 671) [ElementWise]_output[Float(1,32,72,80,312)]
Layer(Constant): (Unnamed Layer* 726) [Constant], Tactic: 0,  -> (Unnamed Layer* 726) [Constant]_output[Float(1,20,304,288,3)]
Layer(PluginV2): (Unnamed Layer* 727) [PluginV2DynamicExt], Tactic: 0, (Unnamed Layer* 671) [ElementWise]_output[Float(1,32,72,80,312)], (Unnamed Layer* 726) [Constant]_output[Float(1,20,304,288,3)] -> (Unnamed Layer* 727) [PluginV2DynamicExt]_output_0[Float(1,32,20,304,288)]
Layer(Myelin): {ForeignNode[(Unnamed Layer* 730) [ElementWise]]}, Tactic: 0, (Unnamed Layer* 727) [PluginV2DynamicExt]_output_0[Float(1,32,20,304,288)], (Unnamed Layer* 729) [Identity]_output[Float(1,1,20,304,288)] -> Box[Float(1,32,20,304,288)]
[TensorRT] INFO: [MemUsageSnapshot] Builder end: CPU 1563 MiB, GPU 9954 MiB
[TensorRT] INFO: [MemUsageChange] Init CUDA: CPU +0, GPU +0, now: CPU 1563, GPU 9955 (MiB)
[TensorRT] INFO: Loaded engine size: 157 MB
[TensorRT] INFO: [MemUsageSnapshot] deserializeCudaEngine begin: CPU 1720 MiB, GPU 10113 MiB
[TensorRT] VERBOSE: Using cublas a tactic source
[TensorRT] INFO: [MemUsageChange] Init cuBLAS/cuBLASLt: CPU +0, GPU +8, now: CPU 1726, GPU 10256 (MiB)
[TensorRT] VERBOSE: Using cuDNN as a tactic source
[TensorRT] INFO: [MemUsageChange] Init cuDNN: CPU +0, GPU +10, now: CPU 1726, GPU 10266 (MiB)
[TensorRT] INFO: [MemUsageChange] Init cuBLAS/cuBLASLt: CPU +0, GPU +0, now: CPU 1726, GPU 10261 (MiB)
[TensorRT] VERBOSE: Deserialization required 512539 microseconds.
[TensorRT] INFO: [MemUsageSnapshot] deserializeCudaEngine end: CPU 1726 MiB, GPU 10261 MiB
[TensorRT] INFO: [MemUsageSnapshot] ExecutionContext creation begin: CPU 1532 MiB, GPU 9842 MiB
[TensorRT] VERBOSE: Using cublas a tactic source
[TensorRT] INFO: [MemUsageChange] Init cuBLAS/cuBLASLt: CPU +0, GPU +0, now: CPU 1532, GPU 9842 (MiB)
[TensorRT] VERBOSE: Using cuDNN as a tactic source
[TensorRT] INFO: [MemUsageChange] Init cuDNN: CPU +0, GPU +7, now: CPU 1532, GPU 9849 (MiB)
[TensorRT] VERBOSE: Total per-runner device memory is 90608640
[TensorRT] VERBOSE: Total per-runner host memory is 109936
[TensorRT] VERBOSE: Allocated activation device memory of size 1228143616
[TensorRT] VERBOSE: myelinAllocCb allocated GPU (data-constants) 8 bytes at 0x226cc7500.
[TensorRT] INFO: [MemUsageSnapshot] ExecutionContext creation end: CPU 1537 MiB, GPU 11109 MiB
[TensorRT] VERBOSE: myelinAllocCb allocated GPU 63037440 bytes at 0x22ddea000.
[TensorRT] ERROR: 2: [pluginV2DynamicExtRunner.cpp::execute::115] Error Code 2: Internal Error (Assertion status == kSTATUS_SUCCESS failed.)

And the inference code snippet is:
def do_inference(context, host_in, host_out):
    
    engine = context.engine
    #assert engine.num_bindings == 6
    bindings = []
    bytes = 0
    
    stream = cuda.Stream()
    for i in range(len(host_in)):
        devide_in = cuda.mem_alloc(host_in[i].nbytes)
        cuda.memcpy_htod_async(devide_in, host_in[i], stream)
        bindings.append(int(devide_in))
    devide_out = cuda.mem_alloc(host_out.nbytes)
    bindings.append(int(devide_out))
    context.execute_async(bindings=bindings, stream_handle=stream.handle)
    cuda.memcpy_dtoh_async(host_out, devide_out, stream)
    stream.synchronize()

    runtime = trt.Runtime(TRT_LOGGER)

    assert runtime

    with open(ENGINE_PATH, ""rb"") as f:
        engine = runtime.deserialize_cuda_engine(f.read())
    assert engine

    context = engine.create_execution_context()
    assert context
    BATCH_SIZE = 1
    image_size = IMAGE_BATCH_SIZE * IMAGE_CHANNELS * IMAGE_HEIGHT * IMAGE_WIDTH 
    
    host_in = []
    
    
    left_image = cuda.pagelocked_empty(image_size,
                                    dtype=np.float32)
    right_image = cuda.pagelocked_empty(image_size,
                                    dtype=np.float32)
    calib = cuda.pagelocked_empty(BATCH_SIZE * 3 * 4, dtype=np.float32)
    
    data_left = cv2.imread(""./liga/test/left/000000.png"")
    data_right = cv2.imread(""./liga/test/right/000000.png"")
    calib_project = np.random.rand(12)
    data_left = cv2.resize(data_left, (IMAGE_HEIGHT, IMAGE_WIDTH)).flatten()
    data_right = cv2.resize(data_right, (IMAGE_HEIGHT, IMAGE_WIDTH)).flatten()
    
    np.copyto(left_image, data_left.ravel())
    np.copyto(right_image, data_right.ravel())
    np.copyto(calib, calib_project.ravel())
    host_in.append(left_image)
    host_in.append(right_image)
    host_in.append(calib)

    host_out = cuda.pagelocked_empty(BATCH_SIZE * 32 * 20 * 304*288, dtype=np.float32)

    do_inference(context, host_in, host_out)

    print(f'Output: \n{host_out[:5000]}\n{host_out[-10:]}')


## Environment

**TensorRT Version**: 8.0
**NVIDIA GPU**: AGX Xavier
**NVIDIA Driver Version**: 
**CUDA Version**: 10.2
**CUDNN Version**: 8.2
**Operating System**: JetPack 4.6
**Python Version (if applicable)**: 3.6
**Tensorflow Version (if applicable)**: 
**PyTorch Version (if applicable)**: 
**Baremetal or Container (if so, version)**: 


## Relevant Files




## Steps To Reproduce

<!-- 
  Craft a minimal bug report following this guide - https://matthewrocklin.com/blog/work/2018/02/28/minimal-bug-reports

  Please include:
  * Exact steps/commands to build your repro
  * Exact steps/commands to run your repro
  * Full traceback of errors encountered 
-->Thank you.mainboard.log.INFO.20221017-160406.3087681 (1.9 MB)
Here is the full log. The CUDA environment is installed through Jetpack 5.0. There is no repo onnx model available since the network is constructed using tensorrt c++ API and pytorch pt model.As for the reference, the issue posted in github happens during inference phase and the building phase was successfil, but my error occured during the building phase. So I don’t think checking enqueue function can solve my issue.Let me know if you have any suggestion. Many thanks.TRT: 8.4.0.9
GPU: Orin
CUDA: 11.4.166Hi,Based on the logs, the error timestamp looks different than other log messages.Could you please share with us the minimal issue repro script/model/steps to try from our end for better debugging.Thank you.Powered by Discourse, best viewed with JavaScript enabled"
299,cloud-vendor-agnostic-pytorch-cuda-docker-image,"How to create vendor agnostic  docker Pytorch image for Training in GCP  tesla GPU  and Prediction in Azure tesla GPU ?Powered by Discourse, best viewed with JavaScript enabled"
300,wrong-results-when-i-convert-movenet-models-from-onnx-to-tensorrt,"Hello, I’m trying to run Movenet models (thunder and lighting) in TensorRT. I’ve converted the model from TensorFlow saved model to ONNX,  and then, from ONNX to TensorRT (using trtexec) in 2 platforms, Jetson Nano (JetPack 4.6), and a Windows machine. I’ve successfully converted the model in both platforms, and then, loaded the engine in a TensorRT (python api) script.The Issue is: when I run the inferences with the TensorRT model, the outputs are wrong, I mean, output confidence values are lower than 0.007 and the detected keypoints are in wrong positions when I draw them in the image.To check the entire process (preprocessing, model inferences and drawing the inferences in the image) I’ve developed 2 scripts, the first one performs the inference in the original tensorflow model (reading the .pb), that worked well, scores are above 0.8 and keypoints are in correct positions in the image.
The second script performs the same pipeline, but with ONNX library in python, loading the converted ONNX model, this one also works well (scores above 0.8 for keypoints, and keypoints are correct in the image).  I don’t know why only the TensorRT model is doing wrong in both machines and, in both apis (python and C++ api)To check if was a data type problem, I’ve developed the same pipeline using TensorRT in C++ api , I can load the model, show some properties (binding sizes, number of bindings) all seems good, but, again, in both machines the C++ inferences are wrong (very low scores and the points are drawn wrong).TensorRT Version: 8.2.1.8
GPU Type:  Jetson Nano tegra210, and RTX3070ti
Nvidia Driver Version:  516.94 in windows
CUDA Version: 10.2.300 in jetson, 11.2 in windows
CUDNN Version: 8.2.1.8 in both machines
Operating System + Version:  Windows 10, Ubuntu 18.04
Python Version (if applicable): python 3.9(Windows) and python 3.6 (Jetson)Model (tf version , onnx and my windows machine TRT version)movenetThunder.onnx (24.0 MB)
movenetThunder.zip (23.2 MB)
movenetThunderX86_64Desktop.plan (13.6 MB)Code to run inferences in ONNX and tensorRT
onnx_thunder_inference.py (977 Bytes)
thunder_trt_script.py (1.7 KB)
helper function to handle trt model
trtClasses.py (2.6 KB)converting the tensorflow model with tf2trt:
$ python -m tf2onnx.convert opset 15 --saved_model (path) --output movenetThunder.onnx
and then converting the ONNX model with trtexec with:
$ trtexec.exe --onnx=model.onnx --saveEngine=output.planHi,We have not observed the accuracy difference in the latest TensorRT version 8.6. We recommend you to use the latest TensorRT version.Thank you.I’ve updated mi Windows machine with TensorRT 8.6.1.6, also I’ve converted the model again with trtexec (8.6.1.6) but I’m still watching the issue. I mean, also in your answer the confidence is very small (third column), thus the points are going to be wrong. This is an inference over an image with trt 8.6.1.6:
[0.125852,0.688647,0.0288986,]
[0.122909,0.728908,0.036514,]
[0.11892,0.694355,0.0257515,]
[0.115888,0.739533,0.038808,]
[0.125717,0.663324,0.0263936,]
[0.124831,0.696173,0.0333687,]
[0.143203,0.638394,0.0330626,]
[0.188329,0.672103,0.0321696,]
[0.230244,0.641809,0.0252251,]
[0.225168,0.694879,0.0366927,]
[0.226246,0.696824,0.0199303,]
[0.220067,0.650417,0.0466373,]
[0.236858,0.602306,0.0586232,]
[0.206621,0.608331,0.074691,]
[0.233311,0.616974,0.0466939,]
[0.229971,0.595934,0.0391641,]
[0.230496,0.60848,0.0411795,]Also, Is there a way to keep using TensorRT 8.2.1? I’m triying to deploy in Jeston devices with Jetpack 4.6, I think I cannot update cuda from 10.2.This is the result on the same model in ONNX, filtering inferences with confidence lower than 0.01:
[0.2121701  0.5908465  0.66256255]
[0.1769617 0.6184405 0.8438019]
[0.1736151  0.5571672  0.64806515]
[0.17624334 0.6419107  0.44044065]
[0.1765048  0.49498025 0.8007731 ]
[0.35050184 0.7078541  0.89948887]
[0.33293843 0.40762717 0.8267948 ]
[0.57030946 0.822819   0.76486623]
[0.54338384 0.32539034 0.7200603 ]
[0.51787597 0.8769932  0.7091793 ]
[0.7186349  0.20520929 0.54700714]
[0.83332926 0.64797986 0.78879374]
[0.83464724 0.45133153 0.7731846 ]
[0.9976017  0.6606558  0.02581054]
[0.9836272  0.4453248  0.03359169]
[1.0159359  0.64814013 0.00775313]
[1.0501901  0.5519477  0.00633091]
I would appreciate any help, thanksPowered by Discourse, best viewed with JavaScript enabled"
301,digits-error-check-failed-error-cudasuccess-30-vs-0-unknown-error,"I have follow the steps to install Nvidia CUDA 10.2 and cuDNN 7 for 10.2 from:Get the latest feature updates to NVIDIA's proprietary compute stack.https://developer.nvidia.com/rdp/cudnn-download
Then I have follow the steps in the two day demo (for the Jetson) to try to set DIGITS on my computer from:Hello AI World guide to deploying deep-learning inference networks and deep vision primitives with TensorRT and NVIDIA Jetson. - GitHub - dusty-nv/jetson-inference: Hello AI World guide to deployin...
I am running Ubuntu 18.04Everything was going well until I try to train a model.
I get the following error:
Check failed: error == cudaSuccess (30 vs. 0)  unknown errorI have checked online but I have not found any solution.
Running nvidia-smi gives me the following:

image1468×812 97.6 KB
Another problem I am seen which may or may not be related is that when I run deviceQuery I get the following:
:/usr/local/cuda/samples/bin/x86_64/linux/release$ ./deviceQuery
./deviceQuery Starting…CUDA Device Query (Runtime API) version (CUDART static linking)cudaGetDeviceCount returned 999
 → unknown error
Result = FAILI have seen someplace that mention to run deviceQuery  as sudo helps but it is not my case.Any information would be much appreciated.@guivi01 Thank you buddy, you literally solved my issue.P.S: solution of issue: just use sudo :)Powered by Discourse, best viewed with JavaScript enabled"
302,tensorrt-detectron2-webcam,"Hello Everyone!
So after a lot of tries, I was finally able to develop a TensorRT engine for the detectron2’s mask-rcnn in the docker.
While I did do inference to check the performance, I was wondering if you guys could advice me something.I want to do real-time inference with a webcam so if you know anything, it would be a big help.
Of course, I am trying to develop the code myself too but it would be a big help if you could perhaps point me to the right direction.I am also simultaneously doing it on Jetson now that I have updated my jetpack to 5.1.Thank you all!TensorRT Version:  8.5.3.1
CUDA Version: 11.8
CUDNN Version: 8.6
Operating System + Version: Ubuntu20.04
Python Version (if applicable): 3.8
PyTorch Version (if applicable): 2TensorRT for Detectionmain/samples/python/detectron2NVIDIA® TensorRT™, an SDK for high-performance deep learning inference, includes a deep learning inference optimizer and runtime that delivers low latency and high throughput for inference applicat...Docker Image for Jetpack 5.1PyTorch is a GPU accelerated tensor computational framework with a Python front end. This container contains PyTorch and torchvision pre-installed in a Python 3 environment to get up & running quickly with PyTorch on Jetson.Hi,This looks like a Jetson issue. Please refer to the below samples in case useful.Hello AI World guide to deploying deep-learning inference networks and deep vision primitives with TensorRT and NVIDIA Jetson. - GitHub - dusty-nv/jetson-inference: Hello AI World guide to deployin...An easy to use PyTorch to TensorRT converter. Contribute to NVIDIA-AI-IOT/torch2trt development by creating an account on GitHub.For any further assistance, we will move this post to to Jetson related forum.Thanks!I apologize if I did not make myself clear,
I already created the tensorrt engine for detectron2.I just want to run inference using a webcam so I was asking for ideas here.Hi @rajupadhyay59 ,
Please refer to the link for understanding and performing inference on TRT engine.Thanks@AakankshaS Thank you for the reply, but I already solved the problem.
Thanks, everyone!This topic was automatically closed 14 days after the last reply. New replies are no longer allowed.Powered by Discourse, best viewed with JavaScript enabled"
303,inputs-of-transducer-from-nemo,"
Screenshot 2023-03-11 at 22-01-56 decoder_joint-stt_ru_conformer_transducer_large.onnx1846×968 47.5 KB
Hi, I’m trying to implement ASR inference for onnxruntime and tensorrt using conformer - transducer model.  I read model description but don’t understand some points… Could you please explain me purposes of inputs and outputs of transducer?
What is TARGETS input? Is it Joint part of network?
What is ENCODER_OUTPUTS? Is this tensor of token’s probabilities from encoder?
Why do I need INPUT_STATES_1, INPUT_STATES_2 and OUTPUT_STATES_1, OUTPUT_STATES_2?Powered by Discourse, best viewed with JavaScript enabled"
304,using-trt-8-0-1-on-rtx-4070-unsupported-sm-error,"We are getting an error when serializing a TensorRT engine using RTX 4070:The documentation for 8.0.1 say the card supports >SM5.0, and the card is SM8.9. But this post implies cards also have minimum supported TensorRT/CUDA versions.What are the supported versions for the RTX 4070, and where can I find this information in the future?TensorRT Version: 8.0.1
GPU Type: RTX 4070
Nvidia Driver Version: 525
CUDA Version:  11.4.2
CUDNN Version: 8
Operating System + Version: ubuntu20.04Hi,We recommend that you try  TensorRT version 8.5 or above. Before version 8.5, SM 8.9 was not supported.
Please refer to the release notes for details.Thank you.Thanks for your quick response!I’ve read the release notes, but I cannot see anywhere that explains forward compatibility of TRT with the latest GPUs.For example, will TRT8.5 be supported on the next gen 50-series cards? Does a new SM version always require a new release of TRT?This is important to us as this affects the compatibility of our product releases.Thanks,
MagnusPowered by Discourse, best viewed with JavaScript enabled"
305,if-we-want-to-use-tensorflow-that-requires-cuda-11-but-we-have-cuda-12-system-wide,"What do we need to do to install a version of cuda 11 so that we can use tensorflow from the precompiled pip packages.My OS uses cuda 12 so I don’t want to install cuda 11 system wide and subvert my package manager.TensorRT Version:
3053
530.41.03
8.1
Fedora 37
3.11
2.12As it turns out. I found a version I downloaded a version of cuda 11.8 by using the search for “cuda 11.8.0 download”. The link from the version 12 download page to archived versions did not provide an actual working link but always ended back at 12. Then I used the run file with the --installpath option to install it to a local directory. I did not have to use sudo to install this, which I think is a change from older cuda runfiles.Hi @matthew.smith3 ,
It seems like you solved it, but in any case, take a look here, with the cuda containers you can avoid having to install locally and modify your current enviroment in order to run other CUDA versions.Regards,
Andres
Embedded SW Engineer at RidgeRun
Contact us: support@ridgerun.com
Developers wiki: https://developer.ridgerun.c om/
Website: www.ridgerun.com Powered by Discourse, best viewed with JavaScript enabled"
306,more-information-about-tactic-sources,"I was surprised to learn that cuDNN, cuBLAS, and cuBLASLt can be disabled as tactic sources (e.g. with the tacticSources= argument to trtexec). So clearly they aren’t essential to TensorRT.(I’m asking this question in general, not in relation to any bug/issue with a particular TensorRT version)Hi @adam.alcolado ,
Quoting from TRT developer guide.TensorRT’s dependencies (NVIDIA cuDNN and NVIDIA cuBLAS) can occupy large amounts of device memory. TensorRT allows you to control whether these libraries are used for inference by using the TacticSources (C++, Python) attribute in the builder configuration. Note that some plugin implementations require these libraries, so that when they are excluded, the network may not be compiled successfully.Each of the tactic source is explained here along with their default statusThanksThanks for pointing me to the part. I would like to lower my device memory usage, so I am inclined to exclude them. Is there any expected performance loss (inference speed) from not using cuDNN and cuBLAS? So far I haven’t observed any difference with the models I’m using, but I don’t know if I’m just getting lucky.Is there any way to tell if a cuDNN/cuBLAS tactic is being used? For example, from the output of trtexec --exportLayerInfo?Hi @adam.alcolado ,
Currently, there is no way to tell whether a given tactic source has been selected for an engine or not, but you can check whether a given tactic source is available for an engine by using the --dumpLayerInfo flag in trtexec.
ThanksThis topic was automatically closed 14 days after the last reply. New replies are no longer allowed.Powered by Discourse, best viewed with JavaScript enabled"
307,random-spikes-in-ram-while-using-triton-inference,"We are observing spikes in RAM (~40GB) while using Triton Inference server. We have python backend models (CPU and GPU) and TensorRT models. And we also use BLS in the pipeline.Trition verison: 22.12 (nvcr.io/nvidia/tritonserver:22.12-py3)
Python Backend verison: r21.08
TensorRT Version: 8.5.1
GPU Type: GeForce RTX 2080 SUPER
Nvidia Driver Version: 510.108.03
CUDA Version: 11.8
CUDNN Version: 8.7.0 GA
Operating System + Version: Ubuntu 20.04
Python Version (if applicable):  3.7image1778×723 35 KBWe couldn’t find the exact reason for the spikes, so cannot specify the steps to reproduce. This issue happens mostly on long runs (8-12 hrs) of Triton Server.Hi,We recommend you to please reach out to Issues · triton-inference-server/server · GitHub to get better help on Triton related issues.Thank you.Powered by Discourse, best viewed with JavaScript enabled"
308,trtexec-error-only-activation-types-allowed-as-input-to-this-layer,"TensorRT Version:  8.5.1.7
GPU Type:  NVIDIA T1200 Laptop GPU
Nvidia Driver Version: 522.06
CUDA Version: 11.8
CUDNN Version:  8.2
Operating System + Version:  Windows10
PyTorch Version (if applicable): 2.0.0Hello, I am new to tensorrt . When I try to  export an onnx model to tensorrt model, it crashed. Here is the error message.I have carried out int8 quantization operation for this origin model by pytorch(torch.ao.quantization)，Then I convert the model to onnx format. When I use trtexec to convert onnx2engine, an error is reported. The onnx model node where the error occurs is as follows:
image1151×798 33.9 KB
Here is the model I use, it is the mobileNetv2 model quantized by pytorch.
quantize_onnx_sim.onnx (2.4 MB)Download this tensorrt project then use the polygraphy executable to sanitise the onnx:release/8.6/tools/Polygraphy/binNVIDIA® TensorRT™, an SDK for high-performance deep learning inference, includes a deep learning inference optimizer and runtime that delivers low latency and high throughput for inference applicat...Use this command:Then run trtexec on the resulting onnx again.Hi,Could you please check if the input tensor is an INT32 tensor? Int32 are index types, not activation types.Thank you.Thanks for your help. I tried your suggestion, but it  seems  not work for me. After using this operation, and then using trtexec to convert, the same error will still be reported.Hi, thanks for your help.
As you said, the input is int32 type, not activation type.I don’t know how to deal with this kind of error. Can I change the type of input？ Is there any type conversion operation for onnx or tensorrt？
Thank you.Hi,Please try ONNX GraphSurgeon. I do not have a sample to share at this moment.
https://docs.nvidia.com/deeplearning/tensorrt/onnx-graphsurgeon/docs/index.htmlmaster/tools/onnx-graphsurgeonNVIDIA® TensorRT™, an SDK for high-performance deep learning inference, includes a deep learning inference optimizer and runtime that delivers low latency and high throughput for inference applicat...Thank you.Powered by Discourse, best viewed with JavaScript enabled"
309,too-long-riva-tts-model-downloading,"Hello! I’m trying to download and install the Riva TTS model. I have a Nvidia RTX A4000 GPU.
On the screen shot you can see my powershell moment of installation. After that I’ve been waiting for a long time but it still in that position. Maybe you can help me with it? Internet connection is good.
I also have tried it on RTX A5000, problem is the same

photo_2023-04-04_15-58-271184×668 103 KB
HI @medvedevmr.workThanks for your interest in RivaQuick Note, Riva server requires Linux x86_64.
reference
https://docs.nvidia.com/deeplearning/riva/user-guide/docs/support-matrix.html#server-hardware
It won’t work in Windows (i.e Powershell)So request to kindly try the setup in a Linux MachineAlso in general, the models are heavy in size, so it will take a good amount of time to download (couple of minutes to few hours based on your Internet bandwidth)ThanksPowered by Discourse, best viewed with JavaScript enabled"
310,trt-pose-model-integration-with-a-rest-api-in-python,"I want to integrate the trt_pose model with a REST API where I send an image and after running through the model, and send the result in the response in JSON. Can someone please guide me on how to move forward with the integration. I’m building the backend using nodejs and pythonTensorRT Version : 8.5.3.1
Nvidia Driver Version : 520.61.05
CUDA Version : 11.8
CUDNN Version : 8.9.0
Operating System + Version : Linux Ubuntu 18.04
Python Version (if applicable) : 3.6Hi,Please refer GitHub - NVIDIA-AI-IOT/trt_pose: Real-time pose estimation accelerated with NVIDIA TensorRT for using trt-pose.
You can try creating REST APIs using Python Flask or FASTAPI.Thank you.Powered by Discourse, best viewed with JavaScript enabled"
311,xgboost-in-cuda-c-for-inference,"Hi, is there a way or a library to perform only the inference part of XGBoost algorithm in CUDA C (I’m using Visual Studio)?
I downloaded the xgboost.dll library that supports GPU but it includes also the training part as I understood.Hi @john455
We have Forest Inference Library as part of cuML here: API Reference — cuml 23.06.00 documentation. Yes, it is python API, but that’s built on top of the underlying C++ implementation in cuML.
It is possible to build a libcuml.so that includes only FIL, and you should get better performance than with the native framework inference.However this channel talks about issues related to cudnn, and Rapids should be the right channel to get better assistance.Thank you.So, no way to do it in CUDA C?Powered by Discourse, best viewed with JavaScript enabled"
312,additional-lab-time-and-access-to-dli-course,"Need additional lab time and access to the Disaster risk monitoring using Satellite Imagery DLI course as I had only accessed and taken the lab for shorter than the stipulated time in my browser and it keeps displaying the notification “ Usage Limit reached” and since it’s my first DLI course I would like to ask for help with this issueHi,This forum talks about TensorRT related updates and issues.
We recommend you to reach out to the relevant support team for help.Thank you.Powered by Discourse, best viewed with JavaScript enabled"
313,error-with-repository-on-ubuntu-22-04,"I get an error when installing Package Manager. What to do with it?
enoch@enoch-srv:~$ sudo add-apt-repository “deb Index of /compute/cuda/repos/ubuntu2204/x86_64 /”
Repository: ‘deb Index of /compute/cuda/repos/ubuntu2204/x86_64 /’
Description:
Archive for codename: / components:
More info: Index of /compute/cuda/repos/ubuntu2204/x86_64
Adding repository.
Press [ENTER] to continue or Ctrl-c to cancel.
Found existing deb entry in /etc/apt/sources.list.d/archive_uri-https_developer_download_nvidia_com_compute_cuda_repos_ubuntu2204_x86_64_-jammy.list
Adding deb entry to /etc/apt/sources.list.d/archive_uri-https_developer_download_nvidia_com_compute_cuda_repos_ubuntu2204_x86_64_-jammy.list
Found existing deb-src entry in /etc/apt/sources.list.d/archive_uri-https_developer_download_nvidia_com_compute_cuda_repos_ubuntu2204_x86_64_-jammy.list
Adding disabled deb-src entry to /etc/apt/sources.list.d/archive_uri-https_developer_download_nvidia_com_compute_cuda_repos_ubuntu2204_x86_64_-jammy.list
Get:1 file:/var/cudnn-local-repo-ubuntu2204-8.9.1.23  InRelease [1 572 B]
Get:1 file:/var/cudnn-local-repo-ubuntu2204-8.9.1.23  InRelease [1 572 B]
Hit:2 http://ru.archive.ubuntu.com/ubuntu jammy InRelease
Get:3 http://ru.archive.ubuntu.com/ubuntu jammy-updates InRelease [119 kB]
Get:4 http://ru.archive.ubuntu.com/ubuntu jammy-backports InRelease [108 kB]
Get:5 http://ru.archive.ubuntu.com/ubuntu jammy-updates/main amd64 Packages [605 kB]
Hit:6 Index of /ubuntu jammy-security InRelease
Get:7 http://ru.archive.ubuntu.com/ubuntu jammy-updates/main i386 Packages [393 kB]
Get:8 http://ru.archive.ubuntu.com/ubuntu jammy-updates/universe amd64 Packages [905 kB]
Get:9 http://ru.archive.ubuntu.com/ubuntu jammy-updates/universe i386 Packages [611 kB]
Ign:10 https://developer.download.nvidia.com/compute/cuda/repos/-ubuntu2204-/x86_64  InRelease
Hit:11 Index of /compute/cuda/repos/ubuntu2204/x86_64  InRelease
Err:12 https://developer.download.nvidia.com/compute/cuda/repos/-ubuntu2204-/x86_64  Release
404  Not Found [IP: 152.199.20.126 443]
Reading package lists… Done
W: https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64/InRelease: Key is stored in legacy trusted.gpg keyring (/etc/apt/trusted.gpg), see the DEPRECATION section in apt-key(8) for details.
E: The repository ‘https://developer.download.nvidia.com/compute/cuda/repos/-ubuntu2204-/x86_64  Release’ does not have a Release file.
N: Updating from such a repository can’t be done securely, and is therefore disabled by default.
N: See apt-secure(8) manpage for repository creation and user configuration details.
enoch@enoch-srv:~$Hi @rusmartcom ,
This doesnt look like a cudnn issue,
however below link might help you in this case.ThanksPowered by Discourse, best viewed with JavaScript enabled"
314,is-enqueuev2-thread-safe,"The document clearly states that safe::IExecutionContext::enqueueV2 is NOT thread safe: linkBut there is no description for the regular version of “enqueueV2”.Currently, I’m using multithreading:
each thread creates its own nvinfer1::IExecutuionContext and cudaStream.
But I got wrong results with some random values.If I make the calling of enqueueV2 sequentially, the result is OK:TensorRT Version:
GPU Type:
Nvidia Driver Version:
CUDA Version:
CUDNN Version:
Operating System + Version:
Python Version (if applicable):
TensorFlow Version (if applicable):
PyTorch Version (if applicable):
Baremetal or Container (if container which image + tag):Please attach or include links to any models, data, files, or scripts necessary to reproduce your issue. (Github repo, Google Drive, Dropbox, etc.)Please include:Hi,The below links might be useful for you.This Best Practices Guide covers various performance considerations related to deploying networks using TensorRT 8.0.3. These sections assume that you have a model that is working at an appropriate level of accuracy and that you are able to...The programming guide to the CUDA model and interface.https://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__STREAM.htmlFor multi-threading/streaming, will suggest you to use Deepstream or TRITONFor more details, we recommend you raise the query in Deepstream forum.orraise the query in Triton Inference Server Github instance issues section.Triton Inference Server has 29 repositories available. Follow their code on GitHub.Thanks!Based on the source code  of trtexec, it looks like ExecutionContext::enqueueV2 IS thread safe because this is no mutex. Is that correct?Powered by Discourse, best viewed with JavaScript enabled"
315,does-maxine-ar-sdk-body-pose-estimation-have-3d-position-depth-data,"Hi there,I would like to know if SDK Body Pose Estimation have 3D position (Depth data), as we only see 2D position right now.Thank you.Yes, the SDK does provide 2D and 3D positioning.Powered by Discourse, best viewed with JavaScript enabled"
316,could-not-load-dynamic-library-libnvinfer-so-7,"I have a laptop  with Ubuntu 20 Geforce RTX 3060, drivers 470, cuda 11.4,  CUDNN 8.1, Python 3.8 and Tensorflow 2.10, I hace also instaled tensoRT using:pip install nvidia-pyindex
pip install nvidia-tensorrt
When I try to run tensorflow I have te next messages:2023-05-28 22:08:39.240187: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library ‘libnvinfer.so.7’; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/cuda/lib64
2023-05-28 22:08:39.240222: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library ‘libnvinfer_plugin.so.7’; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/cuda/lib64
2023-05-28 22:08:39.240226: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.I read a lot of blog however I could not solve this problema.TensorRT Version:
GPU Type:
Nvidia Driver Version:
CUDA Version:
CUDNN Version:
Operating System + Version:
Python Version (if applicable):
TensorFlow Version (if applicable):
PyTorch Version (if applicable):
Baremetal or Container (if container which image + tag):Please attach or include links to any models, data, files, or scripts necessary to reproduce your issue. (Github repo, Google Drive, Dropbox, etc.)Please include:Hi,It looks like some TensorRT is not correctly installed.
Please refer to the installation steps from the below link if in case you are missing on anythingThis NVIDIA TensorRT 8.6.1 Installation Guide provides the installation requirements, a list of what is included in the TensorRT package, and step-by-step instructions for installing TensorRT.
Also, we suggest you use TRT NGC containers to avoid any system dependency related issues.NVIDIA TensorRT is a C++ library that facilitates high-performance inference on NVIDIA graphics processing units (GPUs). TensorRT takes a trained network and produces a highly optimized runtime engine that performs inference for that network.Thanks!TRT NGCThank you very much for your support. Before I try to install by Installation Guide :: NVIDIA Deep Learning TensorRT Documentation , I check if I have installed tensorrt obtaining::~$ python3
Python 3.8.10 (default, Mar 13 2023, 10:26:41)
[GCC 9.4.0] on linux
Type “help”, “copyright”, “credits” or “license” for more information.import tensorrt
print(tensorrt.version)
8.4.3.1
assert tensorrt.Builder(tensorrt.Logger())It looks like I have installed Tensorrt. However, running Python 3 with tensorflow I obtained the following error message:2023-06-06 19:58:53.291121: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library ‘libnvinfer.so.7’; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/cuda/lib64
2023-06-06 19:58:53.291192: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library ‘libnvinfer_plugin.so.7’; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/cuda/lib64
2023-06-06 19:58:53.291198: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.What can I do?Hi,We recommend you to uninstall existing and reinstall TensorRT correctly, also please use the latest version of the TensorFlow.Thank you.Powered by Discourse, best viewed with JavaScript enabled"
317,concatenating-two-cv-mat-to-get-format-1-2-128-128,"Im trying to run interference using a model that requires an input of [1,2,128,128] in C++.
I have generated two cv::Mat in the format [1,128,128]
I want to upload them to the GPU so i can run interference on them but first i have to concatenate them. How do I concatenate two [1,128,128] images to one [1,2,128,128] so i can upload it and run interference on it?TensorRT Version: 8.5.2.2
GPU Type: iGPU ga10b (i think)
Nvidia Driver Version: Jetson Orin Nano
CUDA Version: 11.4.315
CUDNN Version: 8.6.0.166
Operating System + Version: Ubuntu 20.04 focal
Python Version (if applicable):
TensorFlow Version (if applicable):
PyTorch Version (if applicable):
Baremetal or Container (if container which image + tag):Here is my preprocessing method that is supposed to upload the two images to the GPU so i can run interference on them:I want to upload them to the GPU so i can run interference on them but first i have to concatenate them. How do I concatenate two [1,128,128] images to one [1,2,128,128] so i can upload it and run interference on it?Hi,May be cv::hconcat() will help you. For more details please refer to the OpenCV documentation.Thank you.But if I use cv::hconcat() wont i get an input in the format [1,128,256]?
Or will I get the format [2,128,128]?It might result in an extended dimension. You can also check out cv::add() for element-wise addition. Please check OpenCV documentation.ill look into itPowered by Discourse, best viewed with JavaScript enabled"
318,is-there-an-example-of-a-node-js-engine-for-asr-tts,"Please provide the following information when requesting support.Hardware - GPU (A100/A30/T4/V100)
Hardware - CPU
Operating System
Riva Version
TLT Version (if relevant)I’m trying to connect Riva with the IBM voice gateway, this requires a Duplex NodeJS Stream. I would like to know if there is a similar implementation on which I can use as a guideline on how to implement for the integration with IBM.Hi @nharoThanks for your interest in RivaI will check with the team and let you knowThanksHI @nharoI have some updates from the teamWe can indeed leverage Riva Sample for referencehttps://docs.nvidia.com/deeplearning/riva/user-guide/docs/samples/sample-apps/riva-contact/README.htmlPlease find this blogpost (little old and outdated, but helps)Build a web app that can transcribe speech from a live video chat and tag key phrases in the transcript. We also show you how to train an NER model.ThanksPowered by Discourse, best viewed with JavaScript enabled"
319,gpu-benchmarking,"Hi!can anybody help me whether there is a page of benchmarks of different GPUs available where I can see how fast a model is executed on different GPU-s of nvidia? (E.g.: I would like to know how much the forward prop. path would take at a 3060 GPU vs 3090 GPU vs the new 4080/4090 GPU) - assuming that 3060 is 100%…or which formula would you recommend to calculate this?Thx.I personally don’t have those kinds of benchmarks of GPU performance on consumer GPUs, but the benchmarking exists for prosumer/enterprise GPU.  External references may be your best bet, like GPU Benchmarks for Deep Learning | Lambda.Fun fact: it is difficult to do a standard correlation across classes of GPUs as there are many internal differences between the GPUs as they are purpose built with different amounts of processor types, so you can probably only reasonably estimate generationally.  Example: I’ve seen a GV100 do in 3 seconds what takes an RTX8000 took 27 because the particular algo required heavy FP64 calculations.Powered by Discourse, best viewed with JavaScript enabled"
320,how-to-set-workspace-in-tensorrt-python-api-when-converting-from-onnx-to-engine-model,"I got warning when converting from .onnx to engineAnd this is my source codeIt seems that I had set maxixum workspace.
Am I wrong somewhere when setting workspace? Thanks.Hi,
Request you to share the ONNX model and the script if not shared already so that we can assist you better.
Alongside you can try few things:This NVIDIA TensorRT 8.6.1 Quick Start Guide is a starting point for developers who want to try out TensorRT SDK; specifically, this document demonstrates how to quickly construct an application to run inference on a TensorRT engine.import sys
import onnx
filename = yourONNXmodel
model = onnx.load(filename)
onnx.checker.check_model(model).
2) Try running your model with trtexec command.master/samples/trtexecNVIDIA® TensorRT™, an SDK for high-performance deep learning inference, includes a deep learning inference optimizer and runtime that delivers low latency and high throughput for inference applicat...
In case you are still facing issue, request you to share the trtexec “”–verbose"""" log for further debugging
Thanks!@AakankshaS  Thanks. Please focus on my question. But my questions is about how to set workspace in Tensorrt Python API. And I want to confirm that, my above setting is correct or not?@spolisetty Please help me with your guide. Thank you so much.Hi,Please refer to the following Python API document, which will help you.
https://docs.nvidia.com/deeplearning/tensorrt/api/python_api/infer/Core/BuilderConfig.html?highlight=workspace#tensorrt.MemoryPoolType
https://docs.nvidia.com/deeplearning/tensorrt/api/python_api/infer/Core/Engine.html?highlight=workspace#tensorrt.ICudaEngineThank you.Thanks for quick response.This topic was automatically closed 14 days after the last reply. New replies are no longer allowed.Powered by Discourse, best viewed with JavaScript enabled"
321,dynamic-batch-tensor-rt-inference-output-is-incorrect,"Hi, I have an issue regarding dynamic batch size on CRNN-like model with Bi-LSTM inside. I encounter incorrect output when I am using batch > 1 (please see Test 2,3, and 4), thank you for your help.FYI, I have tested other models (object detection, and other things with static shape), and they are working normally. Which means that the Tensor-RT+Python+OS has no issue (at least on CNN-like models).Note: I could send you the model through the email, kindly await for  your reply, thanks[Test 1]
If use fixed conversion, aka.trtexec --onnx=“ocr_recog.onnx” --shapes=‘x’:1x3x32x320  --saveEngine=“net_recog.trt” --explicitBatch --workspace=4096and do inference on batch=1, the output will be nonzeros (0 to 1), which is correct!
Snapshot with batch=1: 
test1_log_batch_1852×167 9.54 KB
[Test 2]
However, If I use model from [Test 1], then I modify the shape after the TRT-conversion using these:

self.context.active_optimization_profile = 0
orishape_ctx = self.context.get_binding_shape(0)
print('ori shape: ', orishape_ctx)
print('ori shape now: ', self.context.get_binding_shape(0), xshape)
self.allocate_buffers(x.shape)
The output will be correct for the first batch, not on batch > 1 (i.e. if I feed my model with 3 batch, batch 1’s output is correct, but batch 2 & 3 are incorrect).
Snapshot with batch>1:

test2_log_batch_gt1854×365 17.9 KB
[Test 3]
If I try to modify the fixed ONNX using build_engine as in this blog, this blog, or Nvidia repo, I could also built the model successfully.Then, I try to load the model, and change the “-1” as in the blog link above, then all returned inference output is zero. I’ve also tried to send only batch: 1, and it also returns zero results (the result shape is correct, but the min, max, and std values are 0.0).After that, I tries to use Logger with trt.Logger.VERBOSE, then I get these silent error:[01/27/2022-15:30:14] [TRT] [E] 3: [executionContext.cpp::nvinfer1::rt::ShapeMachineContext::resolveSlots::1480] Error Code 3: API Usage Error (Parameter check failed at: executionContext.cpp::nvinfer1::rt::ShapeMachineContext::resolveSlots::1480, condition: allInputDimensionsSpecified(routine)
)
[01/27/2022-15:30:14] [TRT] [E] 2: [executionContext.cpp::nvinfer1::rt::ExecutionContext::enqueueInternal::366] Error Code 2: Internal Error (Could not resolve slots: )Snapshot of [Test 3 and 4] with batch=1:

test3_4_logs_batch_1857×317 16.8 KB
Snapshot of [Test 3 and 4] with batch>1:

test3_4_logs_batch_gt1860×526 27 KB
[Test 4]
I’ve also tried to convert my text recognition model from ONNX to Tensor-RT with trtexec. It is successfully convereted. The command looks like this:trtexec --onnx=“ocr_recog_dnm.onnx” --optShapes=‘x’:50x3x32x320 --minShapes=‘x’:1x3x32x320 --maxShapes=‘x’:100x3x32x320 --saveEngine=“net_recog.trt” --explicitBatch --workspace=4096When I test the model with batch size: 1 or more, the output value will also always 0 (tested with min, max, mean, and std). And it also has the [Test 3]'s silent error.Windows 10.
CUDA 11.1
Tensor-RT 8.2.1.8
Python 3.8.10TensorRT Version:  8.2.1.8 (Windows version, after links the path to env. variable, then just pip install on the tensorrt_8xx/python/ folder).
GPU Type: RTX3070
Nvidia Driver Version: 471.41
CUDA Version: (11.1 on my env.path) (11.4 on nvidia-smi)
CUDNN Version: 8.2.1 (checked on cudnn_version.h in cudnn/include)
Operating System + Version: Win10 10.0.19044
Python Version (if applicable): 3.8.10
PyTorch Version (if applicable): 1.9.xPlease kindly send your email, then I will send it immediately.Hi,
Please refer to the below link for Sample guide.This Samples Support Guide provides an overview of all the supported NVIDIA TensorRT 8.4.3 samples included on GitHub and in the product package. The TensorRT samples specifically help in areas such as recommenders, machine comprehension, character...
Refer to the installation steps from the link if in case you are missing on anythingThis NVIDIA TensorRT 8.4.1 Installation Guide provides the installation requirements, a list of what is included in the TensorRT package, and step-by-step instructions for installing TensorRT.
However suggested approach is to use TRT NGC containers to avoid any system dependency related issues.NVIDIA TensorRT is a C++ library that facilitates high-performance inference on NVIDIA graphics processing units (GPUs). TensorRT takes a trained network and produces a highly optimized runtime engine that performs inference for that network.In order to run python sample, make sure TRT python packages are installed while using NGC container.
/opt/tensorrt/python/python_setup.shIn case, if you are trying to run custom model, please share your model and script with us, so that we can assist you better.
Thanks!Hi @ briliantnugraha, did you manage to resolve this error? I am getting the same behaviour and error. Is it caused by the TensorRT version. I am running using TensorRT 8.2.4.2Powered by Discourse, best viewed with JavaScript enabled"
322,cuspatial-demo-running-error,"Hi team,I was trying to use cuspatial hausdorff function for trajectory clustering. I am following a github page :wkelongws/trajectory_clustering_with_cuspatialBut there they are converting the trajectory dataset to image points before doing hausdorff distance calculation.
I wanted to do the hausdorff distance calculation on the original trajectory values (either lat/ long if permissible or on corresponding utm values) and then cluster it using DB scan as shown in example.
But when I tried with utm converted values am getting following error(rapids) [root@nithin-rapids cuspatial]# python ./python/cuspatial/demos/hausdorff_clustering_test_toy.py
Traceback (most recent call last):
File “/share/adas_coe_159/nithin/cuspatial/./python/cuspatial/demos/hausdorff_clustering_test_toy.py”, line 39, in 
distance = cuspatial.directed_hausdorff_distance(pnt_x, pnt_y, cnt)
File “/opt/conda/envs/rapids/lib/python3.10/site-packages/cuspatial/core/spatial/distance.py”, line 97, in directed_hausdorff_distance
return DataFrame(result)
File “/opt/conda/envs/rapids/lib/python3.10/contextlib.py”, line 79, in inner
return func(*args, **kwds)
File “/opt/conda/envs/rapids/lib/python3.10/site-packages/cudf/core/dataframe.py”, line 679, in init
new_df = self._from_arrays(data, index=index, columns=columns)
File “/opt/conda/envs/rapids/lib/python3.10/contextlib.py”, line 79, in inner
return func(*args, **kwds)
File “/opt/conda/envs/rapids/lib/python3.10/site-packages/cudf/core/dataframe.py”, line 5404, in _from_arrays
df._data[k] = column.as_column(
File “/opt/conda/envs/rapids/lib/python3.10/site-packages/cudf/core/column/column.py”, line 1887, in as_column
arbitrary = cupy.ascontiguousarray(arbitrary)
File “/opt/conda/envs/rapids/lib/python3.10/site-packages/cupy/_creation/from_data.py”, line 107, in ascontiguousarray
return _core.ascontiguousarray(a, dtype)
File “cupy/_core/core.pyx”, line 2673, in cupy._core.core.ascontiguousarray
File “cupy/_core/core.pyx”, line 2689, in cupy._core.core.ascontiguousarray
File “cupy/_core/core.pyx”, line 136, in cupy._core.core.ndarray.new
File “cupy/_core/core.pyx”, line 224, in cupy._core.core._ndarray_base._init
File “cupy/cuda/memory.pyx”, line 742, in cupy.cuda.memory.alloc
File “/opt/conda/envs/rapids/lib/python3.10/site-packages/rmm/rmm.py”, line 230, in rmm_cupy_allocator
buf = librmm.device_buffer.DeviceBuffer(size=nbytes, stream=stream)
File “device_buffer.pyx”, line 85, in rmm._lib.device_buffer.DeviceBuffer.cinit
MemoryError: std::bad_alloc: CUDA error at: /opt/conda/envs/rapids/include/rmm/mr/device/cuda_memory_resource.hpp
(rapids) [root@nithin-rapids cuspatial]# python ./python/cuspatial/demos/hausdorff_clustering_test_toy.pyCould you please let me know what could be the source of error?Thank you,
NithinOne thing observed is,
The function ‘cuspatial.directed_hausdorff_distance’ is working for a smaller dataset of size 25 trajectories. when I gave 50 trajectories I was getting above errorSorry for the delay, was on leave.  The example that you pointed to is 4 years old and most likely works with a 2.5 - 3 year old version of cuspatial, which is about the last time I ran it.  CuSpatial has been updated quite a few times since then.  It’s also on a private repo instead of the official repo.  Please look at cuspatial/notebooks at branch-23.04 · rapidsai/cuspatial · GitHub for the current latest notebooks as of March 2023.  You just have to keep edit that link to the latest branch (or go to GitHub - rapidsai/cuspatial: CUDA-accelerated GIS and spatiotemporal algorithms and click the notebooks folder)Powered by Discourse, best viewed with JavaScript enabled"
323,chunk-size-in-tts,"Please provide the following information when requesting support.Hardware - GPU T4
Hardware - CPU
Operating System Ubuntu 20.04
Riva Version
TLT Version (if relevant)Hello, I’m currently synthesizing audio from long texts with SSML tags in offline mode, however, the longer the duration of the audio, the worse the quality of the audio it generates. I’m thinking of using the online/streaming mode to split the text into smaller requests however the chunk sizes are very small, how can I change the chunk size to 10 seconds?These are the parameters with which a new model can be built, however the documentation does not indicate what values are currently being used and I do not fully understand how to increase the size of the chunks:–chunk_length CHUNK_LENGTH
Chunk length in mel frames to synthesize at one time
–overlap_length OVERLAP_LENGTH
Chunk length in mel frames to overlap neighboring
chunks
chunkerFastPitch:
–chunkerFastPitch.max_sequence_idle_microseconds CHUNKERFASTPITCH.MAX_SEQUENCE_IDLE_MICROSECONDS
Global timeout, in ms
–chunkerFastPitch.max_batch_size CHUNKERFASTPITCH.MAX_BATCH_SIZE
Default maximum parallel requests in a single forward
pass
–chunkerFastPitch.min_batch_size CHUNKERFASTPITCH.MIN_BATCH_SIZE
–chunkerFastPitch.opt_batch_size CHUNKERFASTPITCH.OPT_BATCH_SIZE
–chunkerFastPitch.preferred_batch_size CHUNKERFASTPITCH.PREFERRED_BATCH_SIZE
Preferred batch size, must be smaller than Max batch
size
–chunkerFastPitch.batching_type CHUNKERFASTPITCH.BATCHING_TYPE
–chunkerFastPitch.preserve_ordering CHUNKERFASTPITCH.PRESERVE_ORDERING
Preserve ordering
–chunkerFastPitch.instance_group_count CHUNKERFASTPITCH.INSTANCE_GROUP_COUNT
How many instances in a group
–chunkerFastPitch.max_queue_delay_microseconds CHUNKERFASTPITCH.MAX_QUEUE_DELAY_MICROSECONDS
Maximum amount of time to allow requests to queue to
form a batch in microseconds
–chunkerFastPitch.optimization_graph_level CHUNKERFASTPITCH.OPTIMIZATION_GRAPH_LEVEL
The Graph optimization level to use in Triton model
configurationIs there another way to avoid this loss of audio quality with long texts?Powered by Discourse, best viewed with JavaScript enabled"
324,can-we-use-the-nvidia-maxine-to-build-a-internal-web-mobile-conference-system,"Hi,My team is decided to build a internal web conferencing system, I planning to build it by using nvidia Maxine sdk.There are a few questions I want to ask.Can we used nvidia Maxine to build the internal web conference system, so my team members / staffs can join the web conference from their web and app.If we build the system by using nvidia maxine, is there any impacte.g.
. Can my staffs access the web conference, if their laptop does not have the nvidia graphic card?. Can mobile users access the web conference system by using the app, any impact to the mobile users.I already requested for the nvidia maxine early access program, but it is still waiting for someone from nvidia maxine to approval it, can someone from nvidia help to speed up for the approval process.What is the difference between nvidia Maxine and nvidia Maxine Microservice?Once the web conference system is built, my tech team usually will host it in the cloud, Is that necessary the system must host in the cloud services with GPU or can it host in any cloud services?Thank you,
TerryHi landmiles,To answer your questions :1 - Maxine can be integrated into any audio and video conferencing platform so long as it meets OS and hardware requirements.2 - This depends on the deployment type. The way you describe your application it does not appear to require any NVIDIA GPU on the client side.3 - I do not have any further information I can provide. NVIDIA Developer Relations will reach out when/if your application is approved.4 - Maxine can be run locally on a client or on a remote server through microservices, so long as either solution has a compatible OS and NVIDIA GPU. The Maxine microservices provide Maxine features to a connected client whether the client has an NVIDIA GPU or not.5 - To use Maxine features in the cloud your cloud instance must be NVIDIA GPU enabled. Maxine does not run on a CPU.I hope this helps!e. The way you describe your application it does not appear to require any NVIDIA GPU on thehi mark_tme,Thank you for sharing the info.We are worrying, if our client side do not have nvidia gpu, can the users join the video conference.Because we are going to build the Android and iOS conferencing app, mobile phone do not have the nvidia GPU,
web browser conference is another thing we will build, but some of the laptop do not have the nvidia gpu.so will this impact the quality of the video or the user will not able to get some of the nvidia maxine features e.g. noise cancellation and eye focus and etc.Thank you,
LandMilesMaxine will only work with hardware 2060 or above.
If you want to run it on anything without 2060, it will not function.The only go to solution is using cloud computing and render the graphics on a cloud 2060 GPU and send it back to the device that do not have the graphic card. But it will need a very reliable network speed.Powered by Discourse, best viewed with JavaScript enabled"
325,check-failed-start-event-nullptr-stop-event-nullptr,"i am using tensorflow 1.15
cuda 10
gpu rtx 4070 ti
2023-07-20 01:30:56.340808: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cudnn64_7.dll
2023-07-20 01:33:02.450081: W tensorflow/stream_executor/cuda/redzone_allocator.cc:312] Internal: Invoking ptxas not supported on Windows
Relying on driver to perform ptx compilation. This message will be only logged once.
2023-07-20 01:33:02.462977: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cublas64_100.dll
Exception: name ‘sel_samples’ is not defined
Exception: name ‘sel_samples’ is not defined
Exception: name ‘sel_samples’ is not defined
Exception: name ‘sel_samples’ is not defined
2023-07-20 01:34:23.662384: E tensorflow/stream_executor/cuda/cuda_blas.cc:428] failed to run cuBLAS routine: CUBLAS_STATUS_EXECUTION_FAILED
2023-07-20 01:34:23.663068: I tensorflow/stream_executor/stream.cc:4963] [stream=000001735FCE3160,impl=000001737426E0D0] did not memzero GPU location; source: 0000002EFBC3DDE8
2023-07-20 01:34:23.663286: I tensorflow/stream_executor/stream.cc:316] did not allocate timer: 0000002EFBC3DD90
2023-07-20 01:34:23.663387: I tensorflow/stream_executor/stream.cc:1964] [stream=000001735FCE3160,impl=000001737426E0D0] did not enqueue ‘start timer’: 0000002EFBC3DD90
2023-07-20 01:34:23.663589: I tensorflow/stream_executor/stream.cc:1976] [stream=000001735FCE3160,impl=000001737426E0D0] did not enqueue ‘stop timer’: 0000002EFBC3DD90
2023-07-20 01:34:23.663768: F tensorflow/stream_executor/gpu/gpu_timer.cc:65] Check failed: start_event_ != nullptr && stop_event_ != nullptrPowered by Discourse, best viewed with JavaScript enabled"
326,can-nvidia-tensorflow-1-x-be-used-with-rtx-4090,"Consider:An Open Source Machine Learning Framework for Everyone  - GitHub - NVIDIA/tensorflow: An Open Source Machine Learning Framework for Everyone
andIn this post I will show you how to install NVIDIA's build of TensorFlow 1.15 into an Anaconda Python conda environment. This is the same TensorFlow 1.15 that you would have in the NGC docker container, but no docker install required and no local...
Est. reading time: 6 minutes
I understand the above mentioned tensorflow can be used with RTX30 series GPU.Can this tensorflow be used with RTX4090?It is possible for NVIDIA version of  TF 1.15.x to run on a 4090.The only recommendation I have would be to use NGC.
The latest NGC container supports TF 1.15.5 and CUDA 12.  That will work on a 4090.I won’t be able to give you detailed instructions for usage.  There is a NGC section on these forums.Furthermore, if you don’t want to use NGC, I won’t be able to give you a detailed build recipe.  Nevertheless it should be possible to build TF 1.15.x for that target, and the NGC container is proof.Thank you for this answer.
It is clear and provides solid ideas to move forward with.I may try NGC on 3090 I have access to and at least get that going before I build and buy the rig with the 4090.Does anyone know if I can I rent a 4090 in the cloud at an hourly rate to try it?How is it going? Are you able to run NGC on 3090 for tensorflow 1.x?Confirmed running 4090 and 3060 with TF 1.15.5.
(NGC 23.01-tf1-py3)I’ve tried so many times with tensorflow official and other docker images but failed.
But NGC images works without any problem.
Thank you Robert_Crovella for saving my time.Powered by Discourse, best viewed with JavaScript enabled"
327,want-gaze-correction-not-gaze-redirection,"Hi,Summary
Is it possible in the AR/Gaze SDK to rotate eyes by by angular offsets I supply, rather than having the eye’s gaze always rotated towards the camera (as seems to be the case in the Gaze Redirection sample)?Additional Detail
In the sample app, the user’s eyes are transformed so that they seems to be looking at the remote party even if they are not.What I want instead is rotate the gaze vector, so that if the user is looking at the remote party’s eyes on screen, the remote party will perceive eye contact, but if they’re looking elsewhere, eye contact won’t be perceived. This is of course what happens in real world conversations – you can tell when someone is looking at your eyes, and when they are not, and this guides the conversation.In my application, I know the physical relationships between the camera, the user’s eyes, and the location of the remote party’s eyes on screen, so I know the angular offsets needed to rotate the gaze (vector) for gaze to be correct, rather than always showing the eyes as if they are looking at the remote party, even if they are not.Is it possible to rotate apparent eye orientation to achieve this using the SDK?Thanks.Bump?Hi there! I understand what you’re looking for. I will reach out to the Maxine engineering team and get back to you when I have a response!Terrific – very much appreciated Mark.This is exactly what we need as well.As soon as I know, you’ll know. I normally get a response in a day or so. If I don’t get anything back within 24 hours, I’ll ping again and keep you posted.Just an update for you - engineering is aware but hasn’t responded with the information requested. I’ll get back on Monday, 24OCT22, with any additional updates. Thank you for your patience and have a great weekend!Thanks again Mark. Looking forward to hearing if there’s a way to get the behavior we want.Hi folks, I didn’t want to spam anyone. Engineering has been tied up. I’ll ping the Maxine product manager and see if I can get some movement.Thanks Mark, very much appreciate the effort.Hi folks, I haven’t heard back. I have put it on the radar. I will get back as soon as I hear any commentary from engineering. I apologize for not getting back to you more quickly!Bump as well.Bumping. This would be enormously useful. There is good precedent for it here: GazeDirector: Fully Articulated Eye Gaze Redirection in Video (EG'18) - YouTubeI’ll follow up with engineering, been quite busy over the past few months and I appreciate the bumps.@fusterclucks @Plumertle @cameronscwhite @pakoconk    ---- Tagging all of you for optics on this reply.Could you elaborate on your use case in this thread? I bumped an email thread to the top of inboxes and engineering requires information about use cases.Thanks!Hey Mark,First, thanks for your continued help with this.For us, the application is gaze-correct videoconferencing. By “correct”, I mean that a remote participant sees the local participant’s gaze accurately: if the remote party’s eyes are is being looked at, they perceive eye contact; if they the gaze is directed elsewhere, say at the desk, the remote party can perceive that.This is important because gaze is a key social cue that helps regulate conversation. If you see that I’m looking at the desk in front of me, to use the example above, you understand that my attention is directed there (say to the widget we’re working on).MAXINE’s current implementation is actually worse than nothing at all for the general case (understanding where the remote party’s gaze is directed), since it falsely represents the gaze vector in situations where gaze is not actually directed at the other party’s eyes).Hope that helps.I’ve provided that information to our engineering team, thank you!Thanks Mark.I’m sure you’ve seen some of the recent reactions online to this feature, such as this one in Ars Technica that highlight the issue at hand. I hope the engineering team decides to act on the feedback.I agree, having the gaze look directly at the camera doesn’t make the conversation better. You can tell instantly if someone is using the feature and it makes the meetings creepy.It would be nice to be able to offset the eye gaze.Dear fusterclucks,I also tried to set the gaze direction to the NVIDIA Maxine SDK.
However, I could only set the vectors to draw the gaze direction and face pose.
Did you have any success in your implementation?BR,Powered by Discourse, best viewed with JavaScript enabled"
328,converted-model-is-broken-if-half-precision-with-dynamic-batch-size-and-batch-size-is-greater-than-1,"Converting fp16 ViT model from onnx to TensorRT with batch size>1 results in random model inference output. The conversion is successful, but the inference output is always roughly the same and random.
For example the output is for 4 images with broken converted model:The correct model output should be:This issue only occursThe same onnx model runs find in a local inference script or deployed with tritonserver regardless of the batch size and precision. Also I’m trying the same experiments with yolov7 segmentation model and both fp16 and fp32 models with dynamic batch size works properly.Any idea why?
Any help would be appreciated. I want to deploy my model in half precision because the speed difference is huge:TensorRT Version: 8.5.2.2
GPU Type: 2080ti, 3080
Nvidia Driver Version: 525.89.02
CUDA Version: 12.0
CUDNN Version: 8.7.0
Operating System + Version: Fedora Linux 37.20230223.0 (Silverblue)
Python Version (if applicable): 3.8.10
TensorFlow Version (if applicable): N/A
PyTorch Version (if applicable): 1.13.0
Baremetal or Container (if container which image + tag): nvcr.io/nvidia/tensorrt:23.01-py3,  nvcr.io/nvidia/tritonserver:23.01-py3I’m using a custom finetuned model, but the same is applicable to pre-trained model provided by pytorch-image-modelsWhen I use this model, both locally with a inference script that loads tensorrt engine, and deployed it with tritonserver, it always gives outputs like these for every image:
The output is for 4 images:The correct model output should be:If I change the batch_size in the model_warmup, or I sent a request for more than one image, the inference output is the same. If I convert the model with batch_size=1 with trtexec, or warmup with batch_size=1, the model generates proper outputs.This issue only occurs if the model is half precision, and batch_size is bigger than 1. If I use the exact same script to convert a fp32 model and use every config/command exactly the same except fp16 flags, model outputs proper inference output regardless of the batch size.I’m sure that this issue is not related to onnx conversion step because I can use the onnx model with a local script, and I can deploy the onnx model with tritonserver with the exact config minus gpu_execution_accelerator and model_warmup. Both local runs and tritonserver responses are perfectly fine.I also tried another model, yolov7. The problem doesn’t occur for yolov7 model for neither precision.Hi,
Can you try running your model with trtexec command, and share the “”–verbose"""" log in case if the issue persistmaster/samples/trtexecNVIDIA® TensorRT™, an SDK for high-performance deep learning inference, includes a deep learning inference optimizer and runtime that delivers low latency and high throughput for inference applicat...You can refer below link for all the supported operators list, in case any operator is not supported you need to create a custom plugin to support that operationAlso, request you to share your model and script if not shared already so that we can help you better.Meanwhile, for some common errors and queries please refer to below link:This is the revision history of the NVIDIA TensorRT 8.5 Developer Guide.This is the revision history of the NVIDIA TensorRT 8.5 Developer Guide.Thanks!Hi,
Thanks for the quick reply. I’ve added verbose log and my model.I’ve checked operators and every operator is supported for both fp32 and fp16 as far as I can see. As I’ve said, converting the same model in FP32 mode is working.Here is the verbose log:
trtexec_fp16.log (1.7 MB)Here is my model: model_best.pth.tar - Google DriveThanks.Hi @NVES,Any chance looking into this?Thanks!@NVES , @cagdas
Have you solved the issue?
I have faced same problem for VIT model, could not convert to TensorRT 16fp properly…Hi @nick_93,Unfortunately, no. I disabled tensorrt optimizations for the time being. If I can, I’ll certainly share from here.Bumping up the thread again @NVES . Any help would be appreciated.Hi @cagdas ,
Apologies for delayed response, can you please try the latest TRT version and confirm if the issue persist?Apologies for delayed response, can you please try the latest TRT version and confirm if the issue persist?@AakankshaS No worries about the delay. The issue got fixed. I tried out the latest TRT version like you suggested, and it’s running smooth now. Thanks for the help!Powered by Discourse, best viewed with JavaScript enabled"
329,segmentation-fault-error-while-runing-riva-start-sh,"Hello,I just want to run the riva English ASR service
Able to run bash riva_init.sh
but I got error when running bash riva_start.sh
/opt/riva/bin/start-riva: line 4:   108 Segmentation fault      (core dumped)
The details are shown in the attachmentEven if I disable all Riva services
I still get the same errorI also had tried to clean and re-init riva
but still have same issue.How much VRAM do I need to run a single English ASR model ?
How do I run it on multiple GPUs? (I have 4 RTX2080 )
Or does it only run on a single GPU?Hardware - GPU : RTX2080 * 4
Operating System : Ubuntu 20.04
Riva Version : riva_quickstart_v2.11.0docker_log.txt (163.5 KB)
config.sh (13.3 KB)Hi @arthurwu4workThanks for your interest in RivaThanks for sharing the config.sh and logsWe are not very sure about Multiple RTX 2080 support functioning for Riva,In General, usage of multiple GPU’s is an experimental feature and may result in undefined behaviours.Can you share with us the driver version used and also can you try modifying the config.shgpus_to_use =""device=0,1,2,3""and provide us feedbackThanksHi @rvinobha ,thank you for quickly reply!I had tried
gpus_to_use =""device=0,1,2,3""
but still get the same errorThis morning(2023/06/28) I tried changing the GPU to a single RTX2080TI and upgrading the driver from 460.73.01 to 535.54.03.
It worked!Is the main problem due to insufficient VRAM or driver version ?So currently RIVA can only use a single GPU for inference at runtime, right?thank youPowered by Discourse, best viewed with JavaScript enabled"
330,deserializecudaengine-failed-serialization-assertion-magictagread-kmagic-tag-failed-magic-tag-does-not-match,"When I try to deserialize trt engine file, I get the follow error message.the trt engine file is generated from onnx using trtexec.the onnx model file is generated from yolov5s.pt using the file export.py in the yolov5I also tried  to generate using the tensorrt’s api.It does not work. i got same error.the souce code  I tried to deserialize trt engine file:cuda cudnn and tensorrt I use come from  developer.download.nvidia.cn
I have check all the version of meta packages required by tensorrt, cuda, and cudnn.
TensorRT Version:  8.5.3.1-1+cuda11.8
GPU: 3060ti
Nvidia Driver Version:  nvidia-driver-530 530.30.02-0ubuntu1 amd64
CUDA Version:  cuda-drivers-530 530.30.02-1 amd64 cuda-driver-dev-11-8 11.8.89-1 amd64
CUDNN Version: libcudnn8 8.8.1.3-1+cuda11.8 amd64
Operating System + Version: ubuntu18 lts (up to date)
Python Version: 3.7
PyTorch Version: 1.13.1Please include:Hi,
Request you to share the ONNX model and the script if not shared already so that we can assist you better.
Alongside you can try few things:This NVIDIA TensorRT 8.6.0 Early Access (EA) Quick Start Guide is a starting point for developers who want to try out TensorRT SDK; specifically, this document demonstrates how to quickly construct an application to run inference on a TensorRT...import sys
import onnx
filename = yourONNXmodel
model = onnx.load(filename)
onnx.checker.check_model(model).
2) Try running your model with trtexec command.master/samples/trtexecNVIDIA® TensorRT™, an SDK for high-performance deep learning inference, includes a deep learning inference optimizer and runtime that delivers low latency and high throughput for inference applicat...
In case you are still facing issue, request you to share the trtexec “”–verbose"""" log for further debugging
Thanks!The onnx model I have been shared in previous message.I have fixed the issue. forgive my stupidity.Powered by Discourse, best viewed with JavaScript enabled"
331,pycuda-driver-logicerror-cumemcpydtohasync-failed-an-illegal-memory-access-was-encountered,"Hello,I am receiving this error in a very weird way, whenever I use batch_size = 10 in the below code I receive this error and for all other batch sizes, it works fine.error message :This error appears whenever I set batch_size = 10. Please can you help me identify the cause of this behavior?self.context.execute_v2(bindings=bindings)From the line “self.context.execute_v2(bindings=bindings)”, I am facing the same problem on jetson orin nano with my single batch output of “np.empty([1,7],dtype=np.float16)”For me, batch size of 1 does not work (2-3 and so on works)I reconverted my TF model to ONNX with fixed batch size as 1, then converted fixed batch size ONNX model to tensorrt with explicitBatch, problem is solved.This topic was automatically closed 14 days after the last reply. New replies are no longer allowed.Powered by Discourse, best viewed with JavaScript enabled"
332,tf-trtmodel-loading-time-is-very-slow,"I am converting my tensorflow SavedModel to TF-TRT using the following code and I am saving the converted model to SavedModel format.“”""
if precision==“FP32”:
precision = trt_convert.TrtPrecisionMode.FP32
elif precision==“FP16”:
precision = trt_convert.TrtPrecisionMode.FP16
else:
raise ValueError(‘Invalid Precision Mode’)“”""The issue is that the load time is very high (around 1 min) when I try to load the converted model in the SavedModel format. My system spec is Intel Xeon silver 32 cores, 32GB RAM, RTX 3030ti.TensorRT Version: 7.2.1.6
GPU Type: RTX 3080ti
Nvidia Driver Version: 470.103.01
CUDA Version: 11.4
CUDNN Version:
Operating System + Version: Ubuntu 18.04
Python Version (if applicable): 3.6.9
TensorFlow Version (if applicable): 2.3.1
PyTorch Version (if applicable):
Baremetal or Container (if container which image + tag): http://nvcr.io/nvidia/tensorflow:20.11-tf2-py3Hi,Request you to share the model, script, profiler, and performance output if not shared already so that we can help you better.Alternatively, you can try running your model with trtexec command.//github.com/NVIDIA/TensorRT/tree/master/samples/trtexecWhile measuring the model performance, make sure you consider the latency and throughput of the network inference, excluding the data pre and post-processing overhead.
Please refer to the below links for more details:This Best Practices Guide covers various performance considerations related to deploying networks using TensorRT 8.0.3. These sections assume that you have a model that is working at an appropriate level of accuracy and that you are able to...This Best Practices Guide covers various performance considerations related to deploying networks using TensorRT 8.0.3. These sections assume that you have a model that is working at an appropriate level of accuracy and that you are able to...Thanks!@AakankshaS I don’t have any issues with the throughput of the tf-trt model. The only problem that I face is that the load time of the serialized tf_trt-converted SavedModel is very high.Hi,We recommend that you please try the latest TensorRT version 8.6.1.
If you still face this issue, please share with us minimal issue repro model, scripts and steps.Thank you.Here’s the code to replicate my issue. It loads the serialized tf-trt SavedModel and then runs forward passes. It takes a minimum of 40 seconds to load the model. I have attached a converted Savedmodel that can be loaded and run with this code
converted_model.zip (75.8 MB)Hi,Could you please share the output of the above script on your system.Thank you.Please find the attached output file.
I added just one extra print line that prints the time taken from the start of the script up until the first forward pass, just to give you more context.output.txt (698 Bytes)Hi,If possible, could you please share with us the original model (not converted to TF-TRT) and time capturing for it for better debugging?Thank you.Please find the time_capture log and the unconverted TF saved_model zip folder.
output.txt (699 Bytes)
dummynet.zip (38.0 MB)Thank you very much for your time and effort in helping with this issue!Powered by Discourse, best viewed with JavaScript enabled"
333,failure-creating-a-trt-engine-file-with-batch-1-from-a-dynamic-onnx-file-caused-by-where-operator-error,"DescriptionUsing trtexec.exe to create a trt.engine file from the attached dynamic onnx file with batch > 1 fails with the following error:ERROR:
[E] Error[4]: [graphShapeAnalyzer.cpp::nvinfer1::builder::`anonymous-namespace’::ShapeNodeRemover::analyzeShapes::1872] Error Code 4: Miscellaneous (IISelectLayer /Where: broadcast dimensions must be conformable)EnvironmentGPU Type: NVIDIA RTX A3000 Laptop GPU
Nvidia Driver Version: 511.65
CUDA Version: 11.6, V11.6.124
CUDNN Version: 8.4.1
Operating System + Version: Windows 10
Python Version (if applicable): 3.8.8
TensorRT Version (if applicable): 8.5.1.7
PyTorch Version (if applicable): 1.13.1+cu117
Baremetal or Container (if container which image + tag): BaremetalRelevant Files - You will find the onnx file, the plugin dll and the c++ code that was used to build the dll in the following link:Google Drive file.Steps To ReproduceI ran trtexec.exe with the onnx model to create a trt.engine file whith batch=1, and the trt.engine file was created successfully.
This is the command I used:
trtexec.exe --onnx=model.onnx --saveEngine=model.trt.engine --plugins=Plugin.dll --minShapes=input:1x3x1024x1024 --optShapes=input:1x3x1024x1024 --maxShapes=input:1x3x1024x1024 –verboseI replaced the batch number 1 with 10, and ran the following command:trtexec.exe --onnx=model.onnx --saveEngine=model.trt.engine --plugins=Plugin.dll --minShapes=input:10x3x1024x1024 --optShapes=input:10x3x1024x1024 --maxShapes=input:10x3x1024x1024 –verboseThe creation of trt.engine file failed with the following error:[E] Error[4]: [graphShapeAnalyzer.cpp::nvinfer1::builder::`anonymous-namespace’::ShapeNodeRemover::analyzeShapes::1872] Error Code 4: Miscellaneous (IISelectLayer /Where: broadcast dimensions must be conformable).Thank you!Tanya Z.@orong13Hi,
Thank you for replying.
I did use onnx and also attached the onnx that I used to the topic.
Thank you.Hi,[E] Error[4]: [graphShapeAnalyzer.cpp::nvinfer1::builder::`anonymous-namespace’::ShapeNodeRemover::analyzeShapes::1872] Error Code 4: Miscellaneous (IISelectLayer /Where: broadcast dimensions must be conformable)As the log indicated, the /Where operation sees unconformable broadcast dims. For the Where operation, the condition should have a different rank from either X or Y , which is unsupported by TRT.
Could you please change keepdims to 1 in the ReduceMax operation shown below and let us know is you still face the issue?image828×476 17.7 KBThank you.Hi,This worked, this time the trt.engine file was created successfully!I appreciate the quick reply, thank you so much!TanyaThis topic was automatically closed 14 days after the last reply. New replies are no longer allowed.Powered by Discourse, best viewed with JavaScript enabled"
334,e-version-8-3-1-22-1-cuda10-2-for-libcudnn8-was-not-found,"I have followed the guide of installing cuDNN on linux
I’ve downloads cuDNN-version 8.3.1.22 ubuntu 16.04 for cuda 10.2, file name is [cudnn-local-repo-ubuntu1604-8.3.1.22_1.0-1_amd64.deb].
when it came to 2.3.2 - install the runtime library, i try
$ sudo apt-get install libcudnn8=8.3.1.22-1+cuda10.2
but
E: Version ‘8.3.1.22-1+cuda10.2’ for ‘libcudnn8’ was not found show up, i can’t figure out what is wrong. can anybody help me.Hi,Are you still facing this issue.
Please make sure you followed step 2, 3, 4 before that.This cuDNN 8.5.0 Installation Guide provides step-by-step instructions on how to install and check for correct operation of NVIDIA cuDNN on Linux and Microsoft Windows systems.Thank you.Hi I am facing a similiar issue with a different version mix:sudo apt-get install libcudnn8=8.4.0.27-1+cuda11.1
Reading package lists… Done
Building dependency tree
Reading state information… Done
E: Version ‘8.4.0.27-1+cuda11.1’ for ‘libcudnn8’ was not foundand I did follow steps 2, 3 and 4.Same issue.$ sudo apt-get install libcudnn8=8.4.0.27-1+cuda11.2
Reading package lists… Done
Building dependency tree… Done
Reading state information… Done
E: Unable to locate package libcudnn8One of our testers was able to reproduce the same error if they skipped step 4 in the instructions:This cuDNN 8.5.0 Installation Guide provides step-by-step instructions on how to install and check for correct operation of NVIDIA cuDNN on Linux and Microsoft Windows systems.
Please do this command:
sudo apt-get update
Before the install, and let us know if it works.
ThanksSame here.$ sudo apt-get install libcudnn8=8.4.0.27-1+cuda11.4
Reading package lists… Done
Building dependency tree
Reading state information… Done
E: Version ‘8.4.0.27-1+cuda11.4’ for ‘libcudnn8’ was not found*I already did
$ sudo apt-get update
, it just wont work.In the end I went to the cudnn archive and download older version
https://developer.nvidia.com/rdp/cudnn-archiveSame problem, version 8.3.2.44 is working … 8.4.0.27-1 is notThis might do the job.
After step -
sudo apt-key add /var/cudnn-local-repo-*/7fa2af80.pub
You will have the directory /var/cudnn-local-repo-ubuntu2004-8.4.0.27 (with your ubuntu version and cudnn downloaded)
Inside this directory, you will be having three .deb files.
just do for all the deb files-
sudo gdebi xxx.deb
which will install cudnn. ;)@dishant.daredevil yours is the working solution for me for installing on ubuntu20 .
just did
sudo  gdebi XX.deb
by going into dir /var/cudnn-local-repo…Hello
Try running the command as
sudo apt-get install libcudnn8i.e., without writing ‘=8.x.x.x-1+cudaX.Y’ this part.For 8.4.0.27, the cuda version expected is 11.6 even for my system with cuda 11.5
Therefore run the install command with cuda11.6 at the endYou need to update the public key for the repo and apt-update again.NVIDIA is updating and rotating the signing keys used by apt, dnf/yum, and zypper package managers beginning April 27, 2022.I wonder if there isn’t a existing verson of cudnn for CUDA11.7If you browse to the distro (Index of /compute/cuda/repos/ubuntu2004/x86_64 in my case), you can see what it is looking for in the list of links. For instance:
libcudnn8_8.4.1.50-1+cuda11.6_amd64.deb
libcudnn8_8.4.0.27-1+cuda11.6_amd64.debHope this helps someoneI was trying to install cudnn 8.4.1.50 on Ubutu20.04 with Cuda Toolkit 11.7, but there does not seem to be a version for 11.7 yet.though, as previous suggested, the following got the version for 11.6 for mesudo apt-get install libcudnn8Thanks a lot, it worked perfectly for me!sudo apt-get install libcudnn8Thank you it worked for me :)Even I had similar issues.Checked naming conventions of deb files in /var/cudnn-local-xx and used the same in apt-get install,sudo apt-get install libcudnn8=8.6.0.163-1+cuda11.8it worked !!That’s the  deal. It’s common mistake to confuse the versions of both the cuda version installed on computer and the libcudnn name (which refers to the most recent relead of cuda).One should look into var/cudnn-local-xx folder for the names of the deb to use.Powered by Discourse, best viewed with JavaScript enabled"
335,can-riva-tts-checkpoints-in-ckpt-be-converted-to-tlt,"For speech synthesis and finetuning we need pretrained checkpoints in .tlt format. can we create them in lj speech or any other dataset by simply converting the existing checkpoints?Please provide the following information when requesting support.Hardware - GPU (A100/A30/T4/V100)
Hardware - CPU
Operating System
Riva Version
TLT Version (if relevant)
How to reproduce the issue ? (This is for errors. Please share the command and the detailed log here)Hi @pragya.chaturvediThanks for your interest in Riva,can you kindly let me know what is the format of your current model checkpointsFor Riva TTS Deployment, we can deploy pre-trained models (including .nemo , .riva , .tlt and .rmir assets).Thanks.ckpt or .nemoPowered by Discourse, best viewed with JavaScript enabled"
336,tf-trt-model-recompiles-when-loading-it-in-a-different-python-scope,"When retrieving the callable tftrt function in a difference Python Scope as the actual inference scope, the model recompiles for several minutes before it can be executed. Self-explanatory example below.I use the latest TensorFlow NGC ContainerContainer :  nvcr.io/nvidia/tensorflow:23.06-tf2-py3
TensorRT Version: 8.6.1
GPU Type:  NVIDIA GeForce RTX 2080, compute capability: 7.5
Nvidia Driver Version: Driver Version: 525.125.06TFTRT_FP16_tensorflow_23_06_tf2_py3 - Model - Google DriveOutput:Output:Okay, I figured it out. saved_model_loaded needs to live when calling the callable infer.This solves the problemThe problem actually occurred for me when I stored the callable into a class attribute self.infer. So, make sure that you also store saved_model_loaded into a class attribute when you want to call the inference function somewhere else (self.saved_model_loaded).Should be added to the documentationhttps://docs.nvidia.com/deeplearning/frameworks/tf-trt-user-guide/index.html#python-examplePowered by Discourse, best viewed with JavaScript enabled"
337,tensorrt-not-installing-with-pip,"When I try to install tensorrt using pip in a python virtual environment, the setup fails and gives the following error:
ERROR: Failed building wheel for tensorrt.I have upgraded the version of the pip but it still doesn’t work.TensorRT Version: 8.5
CUDA Version: 11.4
CUDNN Version:  8.6
Operating System + Version: Jetson 35.2
Python Version (if applicable): 3.8Hi,We recommend that you please follow the installation guide and try the correct steps.This NVIDIA TensorRT 8.6.1 Installation Guide provides the installation requirements, a list of what is included in the TensorRT package, and step-by-step instructions for installing TensorRT.Thank you,Hi. I did follow this. Apparently, the architecture of the ORIN device is aarch64. When I flashed the ORIN device, tensorrt was already installed but not accessible in a Python virtual environment. So I created the virtual environment with --system-site-packages option and now it’s able to access tensorrt. I found this solution here: How to install nvidia-tensorrt? - #7 by dusty_nvI am getting the same errors with tensorrt, but how do I apply the :–system-site-packages you wrote for the solution?First check if tensorrt exists in your python 3.8 dist packages on your Linux machine.
The directory will be something like this: usr/lib/python3.8/dist-packagesSo when you use mkdirvirtualenv to create a new virtual environment, add --system-site-packages as an option.
Something like this: mkdirvirtualenv venv --system-site-packagesSo it will create a new virtual environment and have its own packages but will also use the packages found in the usr/lib/python3.8/dist-packages directory. Usually, tensorrt python package is there after you flash the ORIN device. So it should work.thank youPowered by Discourse, best viewed with JavaScript enabled"
338,how-to-choose-exact-tensorrt-version,"I downloaded Docker image Deepstream6.0:del but installed Tensort version is 8.0. But now I want to install Tensorrt 8.2.1.9 and I saw there is some options. But I dont know choose which to fit version 8.2.1.9. Is it TensorRT 8.2 GA Update 1?Please refer to the TensorRT release notes. Which may help you. Download the 8.2 GA update accordingly.NVIDIA TensorRT is a C++ library that facilitates high performance inference on NVIDIA GPUs. It is designed to work in connection with deep learning frameworks that are commonly used for training. TensorRT focuses specifically on running an already...We recommend you use the latest TensorRT version 8.6 to get better stability and performance.Thank you.@spolisetty
Thanks. I found that NVIDIA provided not all TensorRT version. For example, I can find TRT8.2.1.8 but TRT8.2.1.9, but I think it is not much different.This topic was automatically closed 14 days after the last reply. New replies are no longer allowed.Powered by Discourse, best viewed with JavaScript enabled"
339,yolov7-inferencing-using-multiprocess-and-tensorrt,"Hi,
we are running yolov7 tensorrt engine for inferencing. If we are running it without multiprocessing it is working fine. But when we are running it in a process using multiprocessing and shared queue of python, it shows error as shown in shared screenshot.

WhatsApp Image 2023-04-28 at 09.39.391280×960 187 KB
#enviroment
Docker image: nvcr.io/nvidia/tensorrt: 23.03-py3
tensorrt: 8.5.3.1
Cuda:12.0Please attach or include links to any models, data, files, or scripts necessary to reproduce your issue. (Github repo, Google Drive, Dropbox, etc.)Please include:Hi,The below links might be useful for you.This Best Practices Guide covers various performance considerations related to deploying networks using TensorRT 8.0.3. These sections assume that you have a model that is working at an appropriate level of accuracy and that you are able to...The programming guide to the CUDA model and interface.https://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__STREAM.htmlFor multi-threading/streaming, will suggest you to use Deepstream or TRITONFor more details, we recommend you raise the query in Deepstream forum.orraise the query in Triton Inference Server Github instance issues section.Triton Inference Server has 30 repositories available. Follow their code on GitHub.Thanks!Powered by Discourse, best viewed with JavaScript enabled"
340,cant-run-riva-tts,"Please provide the following information when requesting support.Hardware - GPU RTX3060
Hardware - CPU AMD Ryzen 5 3600-6core
Operating System Gigabyte
Riva Version
TLT Version (if relevant)
How to reproduce the issue ? (This is for errors. Please share the command and the detailed log here)When I tried to bash riva_start.sh, this happens :$ bash riva_start.sh
Starting Riva Speech Services. This may take several minutes depending on the number of models deployed.
ae38663e0f590e88948844ad62613c77d9c218f4c2f20ef24a48b566864e0dd8
OCI runtime exec failed: exec failed: unable to start container process: exec: “C:/Program Files/Git/usr/bin/grpc_health_probe”: stat C:/Program Files/Git/usr/bin/grpc_health_probe: no such file or directory: unknown
Waiting for Riva server to load all models…retrying in 10 seconds
Waiting for Riva server to load all models…retrying in 10 seconds
Waiting for Riva server to load all models…retrying in 10 seconds
Waiting for Riva server to load all models…retrying in 10 seconds
Waiting for Riva server to load all models…retrying in 10 seconds
Waiting for Riva server to load all models…retrying in 10 seconds
Waiting for Riva server to load all models…retrying in 10 seconds
Waiting for Riva server to load all models…retrying in 10 seconds
Waiting for Riva server to load all models…retrying in 10 seconds
Waiting for Riva server to load all models…retrying in 10 seconds
Waiting for Riva server to load all models…retrying in 10 seconds
Waiting for Riva server to load all models…retrying in 10 seconds
Waiting for Riva server to load all models…retrying in 10 seconds
Waiting for Riva server to load all models…retrying in 10 seconds
Waiting for Riva server to load all models…retrying in 10 seconds
Waiting for Riva server to load all models…retrying in 10 seconds
Waiting for Riva server to load all models…retrying in 10 seconds
Waiting for Riva server to load all models…retrying in 10 seconds
Waiting for Riva server to load all models…retrying in 10 seconds
Waiting for Riva server to load all models…retrying in 10 seconds
Waiting for Riva server to load all models…retrying in 10 seconds
Waiting for Riva server to load all models…retrying in 10 seconds
Waiting for Riva server to load all models…retrying in 10 seconds
Waiting for Riva server to load all models…retrying in 10 seconds
Waiting for Riva server to load all models…retrying in 10 seconds
Waiting for Riva server to load all models…retrying in 10 seconds
Waiting for Riva server to load all models…retrying in 10 seconds
Waiting for Riva server to load all models…retrying in 10 seconds
Waiting for Riva server to load all models…retrying in 10 seconds
Waiting for Riva server to load all models…retrying in 10 seconds
Health ready check failed.
Check Riva logs with: docker logs riva-speechI already tried to bash config > clean and rebash riva_init.sh
**but still have same issue. **below is the logs of container when I bash riva_start.sh:2023-04-28 10:52:05 ERROR: ld.so: object ‘[FATAL’ from LD_PRELOAD cannot be preloaded (cannot open shared object file): ignored.
2023-04-28 10:52:05 ERROR: ld.so: object ‘tini’ from LD_PRELOAD cannot be preloaded (cannot open shared object file): ignored.
2023-04-28 10:52:05 ERROR: ld.so: object ‘(7)]’ from LD_PRELOAD cannot be preloaded (cannot open shared object file): ignored.
2023-04-28 10:52:05 ERROR: ld.so: object ‘exec’ from LD_PRELOAD cannot be preloaded (cannot open shared object file): ignored.
2023-04-28 10:52:05 ERROR: ld.so: object ‘C’ from LD_PRELOAD cannot be preloaded (cannot open shared object file): ignored.
2023-04-28 10:52:05 ERROR: ld.so: object ‘/Program’ from LD_PRELOAD cannot be preloaded (cannot open shared object file): ignored.
2023-04-28 10:52:05 ERROR: ld.so: object ‘Files/Git/usr/bin/bash’ from LD_PRELOAD cannot be preloaded (cannot open shared object file): ignored.
2023-04-28 10:52:05 ERROR: ld.so: object ‘failed’ from LD_PRELOAD cannot be preloaded (cannot open shared object file): ignored.
2023-04-28 10:52:05 ERROR: ld.so: object ‘No’ from LD_PRELOAD cannot be preloaded (cannot open shared object file): ignored.
2023-04-28 10:52:05 ERROR: ld.so: object ‘such’ from LD_PRELOAD cannot be preloaded (cannot open shared object file): ignored.
2023-04-28 10:52:05 ERROR: ld.so: object ‘file’ from LD_PRELOAD cannot be preloaded (cannot open shared object file): ignored.
2023-04-28 10:52:05 ERROR: ld.so: object ‘or’ from LD_PRELOAD cannot be preloaded (cannot open shared object file): ignored.
2023-04-28 10:52:05 ERROR: ld.so: object ‘directory’ from LD_PRELOAD cannot be preloaded (cannot open shared object file): ignored.
2023-04-28 10:52:05 ERROR: ld.so: object ‘[FATAL’ from LD_PRELOAD cannot be preloaded (cannot open shared object file): ignored.
2023-04-28 10:52:05 ERROR: ld.so: object ‘tini’ from LD_PRELOAD cannot be preloaded (cannot open shared object file): ignored.
2023-04-28 10:52:05 ERROR: ld.so: object ‘(7)]’ from LD_PRELOAD cannot be preloaded (cannot open shared object file): ignored.
2023-04-28 10:52:05 ERROR: ld.so: object ‘exec’ from LD_PRELOAD cannot be preloaded (cannot open shared object file): ignored.
2023-04-28 10:52:05 ERROR: ld.so: object ‘C’ from LD_PRELOAD cannot be preloaded (cannot open shared object file): ignored.
2023-04-28 10:52:05 ERROR: ld.so: object ‘/Program’ from LD_PRELOAD cannot be preloaded (cannot open shared object file): ignored.
2023-04-28 10:52:05 ERROR: ld.so: object ‘Files/Git/usr/bin/bash’ from LD_PRELOAD cannot be preloaded (cannot open shared object file): ignored.
2023-04-28 10:52:05 ERROR: ld.so: object ‘failed’ from LD_PRELOAD cannot be preloaded (cannot open shared object file): ignored.
2023-04-28 10:52:05 ERROR: ld.so: object ‘No’ from LD_PRELOAD cannot be preloaded (cannot open shared object file): ignored.
2023-04-28 10:52:05 ERROR: ld.so: object ‘such’ from LD_PRELOAD cannot be preloaded (cannot open shared object file): ignored.
2023-04-28 10:52:05 ERROR: ld.so: object ‘file’ from LD_PRELOAD cannot be preloaded (cannot open shared object file): ignored.
2023-04-28 10:52:05 ERROR: ld.so: object ‘or’ from LD_PRELOAD cannot be preloaded (cannot open shared object file): ignored.
2023-04-28 10:52:05 ERROR: ld.so: object ‘directory’ from LD_PRELOAD cannot be preloaded (cannot open shared object file): ignored.
2023-04-28 10:52:05 ERROR: ld.so: object ‘[FATAL’ from LD_PRELOAD cannot be preloaded (cannot open shared object file): ignored.
2023-04-28 10:52:05 ERROR: ld.so: object ‘tini’ from LD_PRELOAD cannot be preloaded (cannot open shared object file): ignored.
2023-04-28 10:52:05 ERROR: ld.so: object ‘(7)]’ from LD_PRELOAD cannot be preloaded (cannot open shared object file): ignored.
2023-04-28 10:52:05 ERROR: ld.so: object ‘exec’ from LD_PRELOAD cannot be preloaded (cannot open shared object file): ignored.
2023-04-28 10:52:05 ERROR: ld.so: object ‘C’ from LD_PRELOAD cannot be preloaded (cannot open shared object file): ignored.
2023-04-28 10:52:05 ERROR: ld.so: object ‘/Program’ from LD_PRELOAD cannot be preloaded (cannot open shared object file): ignored.
2023-04-28 10:52:05 ERROR: ld.so: object ‘Files/Git/usr/bin/bash’ from LD_PRELOAD cannot be preloaded (cannot open shared object file): ignored.
2023-04-28 10:52:05
2023-04-28 10:52:05 ==========================
2023-04-28 10:52:05 === Riva Speech Skills ===
2023-04-28 10:52:05 ==========================
2023-04-28 10:52:05 ERROR: ld.so: object ‘failed’ from LD_PRELOAD cannot be preloaded (cannot open shared object file): ignored.
2023-04-28 10:52:05 ERROR: ld.so: object ‘No’ from LD_PRELOAD cannot be preloaded (cannot open shared object file): ignored.
2023-04-28 10:52:05 ERROR: ld.so: object ‘such’ from LD_PRELOAD cannot be preloaded (cannot open shared object file): ignored.
2023-04-28 10:52:05 ERROR: ld.so: object ‘file’ from LD_PRELOAD cannot be preloaded (cannot open shared object file): ignored.
2023-04-28 10:52:05 ERROR: ld.so: object ‘or’ from LD_PRELOAD cannot be preloaded (cannot open shared object file): ignored.
2023-04-28 10:52:05 ERROR: ld.so: object ‘directory’ from LD_PRELOAD cannot be preloaded (cannot open shared object file): ignored.
2023-04-28 10:52:05 ERROR: ld.so: object ‘[FATAL’ from LD_PRELOAD cannot be preloaded (cannot open shared object file): ignored.
2023-04-28 10:52:05 ERROR: ld.so: object ‘tini’ from LD_PRELOAD cannot be preloaded (cannot open shared object file): ignored.
2023-04-28 10:52:05 ERROR: ld.so: object ‘(7)]’ from LD_PRELOAD cannot be preloaded (cannot open shared object file): ignored.
2023-04-28 10:52:05 ERROR: ld.so: object ‘exec’ from LD_PRELOAD cannot be preloaded (cannot open shared object file): ignored.
2023-04-28 10:52:05 ERROR: ld.so: object ‘C’ from LD_PRELOAD cannot be preloaded (cannot open shared object file): ignored.
2023-04-28 10:52:05 ERROR: ld.so: object ‘/Program’ from LD_PRELOAD cannot be preloaded (cannot open shared object file): ignored.
2023-04-28 10:52:05 ERROR: ld.so: object ‘Files/Git/usr/bin/bash’ from LD_PRELOAD cannot be preloaded (cannot open shared object file): ignored.
2023-04-28 10:52:05 ERROR: ld.so: object ‘failed’ from LD_PRELOAD cannot be preloaded (cannot open shared object file): ignored.
2023-04-28 10:52:05 ERROR: ld.so: object ‘No’ from LD_PRELOAD cannot be preloaded (cannot open shared object file): ignored.
2023-04-28 10:52:05 ERROR: ld.so: object ‘such’ from LD_PRELOAD cannot be preloaded (cannot open shared object file): ignored.
2023-04-28 10:52:05 ERROR: ld.so: object ‘file’ from LD_PRELOAD cannot be preloaded (cannot open shared object file): ignored.
2023-04-28 10:52:05 ERROR: ld.so: object ‘or’ from LD_PRELOAD cannot be preloaded (cannot open shared object file): ignored.
2023-04-28 10:52:05 ERROR: ld.so: object ‘directory’ from LD_PRELOAD cannot be preloaded (cannot open shared object file): ignored.
2023-04-28 10:52:05 ERROR: ld.so: object ‘[FATAL’ from LD_PRELOAD cannot be preloaded (cannot open shared object file): ignored.
2023-04-28 10:52:05 ERROR: ld.so: object ‘tini’ from LD_PRELOAD cannot be preloaded (cannot open shared object file): ignored.
2023-04-28 10:52:05 ERROR: ld.so: object ‘(7)]’ from LD_PRELOAD cannot be preloaded (cannot open shared object file): ignored.
2023-04-28 10:52:05 ERROR: ld.so: object ‘exec’ from LD_PRELOAD cannot be preloaded (cannot open shared object file): ignored.
2023-04-28 10:52:05 ERROR: ld.so: object ‘C’ from LD_PRELOAD cannot be preloaded (cannot open shared object file): ignored.
2023-04-28 10:52:05 ERROR: ld.so: object ‘/Program’ from LD_PRELOAD cannot be preloaded (cannot open shared object file): ignored.
2023-04-28 10:52:05 ERROR: ld.so: object ‘Files/Git/usr/bin/bash’ from LD_PRELOAD cannot be preloaded (cannot open shared object file): ignored.
2023-04-28 10:52:05 ERROR: ld.so: object ‘failed’ from LD_PRELOAD cannot be preloaded (cannot open shared object file): ignored.
2023-04-28 10:52:05 ERROR: ld.so: object ‘No’ from LD_PRELOAD cannot be preloaded (cannot open shared object file): ignored.
2023-04-28 10:52:05 ERROR: ld.so: object ‘such’ from LD_PRELOAD cannot be preloaded (cannot open shared object file): ignored.
2023-04-28 10:52:05 ERROR: ld.so: object ‘file’ from LD_PRELOAD cannot be preloaded (cannot open shared object file): ignored.
2023-04-28 10:52:05 ERROR: ld.so: object ‘or’ from LD_PRELOAD cannot be preloaded (cannot open shared object file): ignored.
2023-04-28 10:52:05 ERROR: ld.so: object ‘directory’ from LD_PRELOAD cannot be preloaded (cannot open shared object file): ignored.
2023-04-28 10:52:05 ERROR: ld.so: object ‘[FATAL’ from LD_PRELOAD cannot be preloaded (cannot open shared object file): ignored.
2023-04-28 10:52:05 ERROR: ld.so: object ‘tini’ from LD_PRELOAD cannot be preloaded (cannot open shared object file): ignored.
2023-04-28 10:52:05 ERROR: ld.so: object ‘(7)]’ from LD_PRELOAD cannot be preloaded (cannot open shared object file): ignored.
2023-04-28 10:52:05 ERROR: ld.so: object ‘exec’ from LD_PRELOAD cannot be preloaded (cannot open shared object file): ignored.
2023-04-28 10:52:05 ERROR: ld.so: object ‘C’ from LD_PRELOAD cannot be preloaded (cannot open shared object file): ignored.
2023-04-28 10:52:05 ERROR: ld.so: object ‘/Program’ from LD_PRELOAD cannot be preloaded (cannot open shared object file): ignored.
2023-04-28 10:52:05 ERROR: ld.so: object ‘Files/Git/usr/bin/bash’ from LD_PRELOAD cannot be preloaded (cannot open shared object file): ignored.
2023-04-28 10:52:05 ERROR: ld.so: object ‘failed’ from LD_PRELOAD cannot be preloaded (cannot open shared object file): ignored.
2023-04-28 10:52:05 ERROR: ld.so: object ‘No’ from LD_PRELOAD cannot be preloaded (cannot open shared object file): ignored.
2023-04-28 10:52:05 ERROR: ld.so: object ‘such’ from LD_PRELOAD cannot be preloaded (cannot open shared object file): ignored.
2023-04-28 10:52:05 ERROR: ld.so: object ‘file’ from LD_PRELOAD cannot be preloaded (cannot open shared object file): ignored.
2023-04-28 10:52:05 ERROR: ld.so: object ‘or’ from LD_PRELOAD cannot be preloaded (cannot open shared object file): ignored.
2023-04-28 10:52:05 ERROR: ld.so: object ‘directory’ from LD_PRELOAD cannot be preloaded (cannot open shared object file): ignored.
2023-04-28 10:52:05 ERROR: ld.so: object ‘[FATAL’ from LD_PRELOAD cannot be preloaded (cannot open shared object file): ignored.
2023-04-28 10:52:05 ERROR: ld.so: object ‘[FATAL’ from LD_PRELOAD cannot be preloaded (cannot open shared object file): ignored.
2023-04-28 10:52:05 ERROR: ld.so: object ‘tini’ from LD_PRELOAD cannot be preloaded (cannot open shared object file): ignored.
2023-04-28 10:52:05 ERROR: ld.so: object ‘tini’ from LD_PRELOAD cannot be preloaded (cannot open shared object file): ignored.
2023-04-28 10:52:05 ERROR: ld.so: object ‘(7)]’ from LD_PRELOAD cannot be preloaded (cannot open shared object file): ignored.
2023-04-28 10:52:05 ERROR: ld.so: object ‘(7)]’ from LD_PRELOAD cannot be preloaded (cannot open shared object file): ignored.
2023-04-28 10:52:05 ERROR: ld.so: object ‘exec’ from LD_PRELOAD cannot be preloaded (cannot open shared object file): ignored.
2023-04-28 10:52:05 ERROR: ld.so: object ‘exec’ from LD_PRELOAD cannot be preloaded (cannot open shared object file): ignored.
2023-04-28 10:52:05 ERROR: ld.so: object ‘[FATAL’ from LD_PRELOAD cannot be preloaded (cannot open shared object file): ignored.
2023-04-28 10:52:05 ERROR: ld.so: object ‘C’ from LD_PRELOAD cannot be preloaded (cannot open shared object file): ignored.
2023-04-28 10:52:05 ERROR: ld.so: object ‘C’ from LD_PRELOAD cannot be preloaded (cannot open shared object file): ignored.
2023-04-28 10:52:05 ERROR: ld.so: object ‘/Program’ from LD_PRELOAD cannot be preloaded (cannot open shared object file): ignored.
2023-04-28 10:52:05 ERROR: ld.so: object ‘/Program’ from LD_PRELOAD cannot be preloaded (cannot open shared object file): ignored.
2023-04-28 10:52:05 ERROR: ld.so: object ‘Files/Git/usr/bin/bash’ from LD_PRELOAD cannot be preloaded (cannot open shared object file): ignored.
2023-04-28 10:52:05 ERROR: ld.so: object ‘Files/Git/usr/bin/bash’ from LD_PRELOAD cannot be preloaded (cannot open shared object file): ignored.
2023-04-28 10:52:05 ERROR: ld.so: object ‘tini’ from LD_PRELOAD cannot be preloaded (cannot open shared object file): ignored.
2023-04-28 10:52:05 ERROR: ld.so: object ‘failed’ from LD_PRELOAD cannot be preloaded (cannot open shared object file): ignored.
2023-04-28 10:52:05 ERROR: ld.so: object ‘failed’ from LD_PRELOAD cannot be preloaded (cannot open shared object file): ignored.
2023-04-28 10:52:05 ERROR: ld.so: object ‘(7)]’ from LD_PRELOAD cannot be preloaded (cannot open shared object file): ignored.
2023-04-28 10:52:05 ERROR: ld.so: object ‘No’ from LD_PRELOAD cannot be preloaded (cannot open shared object file): ignored.
2023-04-28 10:52:05 ERROR: ld.so: object ‘No’ from LD_PRELOAD cannot be preloaded (cannot open shared object file): ignored.
2023-04-28 10:52:05 ERROR: ld.so: object ‘exec’ from LD_PRELOAD cannot be preloaded (cannot open shared object file): ignored.
2023-04-28 10:52:05 ERROR: ld.so: object ‘such’ from LD_PRELOAD cannot be preloaded (cannot open shared object file): ignored.
2023-04-28 10:52:05 ERROR: ld.so: object ‘such’ from LD_PRELOAD cannot be preloaded (cannot open shared object file): ignored.
2023-04-28 10:52:05 ERROR: ld.so: object ‘C’ from LD_PRELOAD cannot be preloaded (cannot open shared object file): ignored.
2023-04-28 10:52:05 ERROR: ld.so: object ‘file’ from LD_PRELOAD cannot be preloaded (cannot open shared object file): ignored.
2023-04-28 10:52:05 ERROR: ld.so: object ‘/Program’ from LD_PRELOAD cannot be preloaded (cannot open shared object file): ignored.
2023-04-28 10:52:05 ERROR: ld.so: object ‘Files/Git/usr/bin/bash’ from LD_PRELOAD cannot be preloaded (cannot open shared object file): ignored.
2023-04-28 10:52:05 ERROR: ld.so: object ‘or’ from LD_PRELOAD cannot be preloaded (cannot open shared object file): ignored.
2023-04-28 10:52:05 ERROR: ld.so: object ‘file’ from LD_PRELOAD cannot be preloaded (cannot open shared object file): ignored.
2023-04-28 10:52:05 ERROR: ld.so: object ‘failed’ from LD_PRELOAD cannot be preloaded (cannot open shared object file): ignored.
2023-04-28 10:52:05 ERROR: ld.so: object ‘directory’ from LD_PRELOAD cannot be preloaded (cannot open shared object file): ignored.
2023-04-28 10:52:05 ERROR: ld.so: object ‘or’ from LD_PRELOAD cannot be preloaded (cannot open shared object file): ignored.
2023-04-28 10:52:05
2023-04-28 10:52:05 NVIDIA Release 22.07 (build 42007163)
2023-04-28 10:52:05 Riva Speech Server Version 2.4.0
2023-04-28 10:52:05
2023-04-28 10:52:05 Copyright (c) 2018-2022, NVIDIA CORPORATION & AFFILIATES. All rights reserved.
2023-04-28 10:52:05
2023-04-28 10:52:05 Various files include modifications (c) NVIDIA CORPORATION & AFFILIATES.  All rights reserved.
2023-04-28 10:52:05
2023-04-28 10:52:05 This container image and its contents are governed by the NVIDIA Deep Learning Container License.
2023-04-28 10:52:05 By pulling and using the container, you accept the terms and conditions of this license:
2023-04-28 10:52:05 https://developer.nvidia.com/ngc/nvidia-deep-learning-container-license
2023-04-28 10:52:05 ERROR: ld.so: object ‘No’ from LD_PRELOAD cannot be preloaded (cannot open shared object file): ignored.
2023-04-28 10:52:05 ERROR: ld.so: object ‘directory’ from LD_PRELOAD cannot be preloaded (cannot open shared object file): ignored.
2023-04-28 10:52:05 ERROR: ld.so: object ‘such’ from LD_PRELOAD cannot be preloaded (cannot open shared object file): ignored.
2023-04-28 10:52:05 ERROR: ld.so: object ‘file’ from LD_PRELOAD cannot be preloaded (cannot open shared object file): ignored.
2023-04-28 10:52:05 ERROR: ld.so: object ‘or’ from LD_PRELOAD cannot be preloaded (cannot open shared object file): ignored.
2023-04-28 10:52:05 ERROR: ld.so: object ‘directory’ from LD_PRELOAD cannot be preloaded (cannot open shared object file): ignored.
2023-04-28 10:52:05 ERROR: ld.so: object ‘[FATAL’ from LD_PRELOAD cannot be preloaded (cannot open shared object file): ignored.
2023-04-28 10:52:05 ERROR: ld.so: object ‘tini’ from LD_PRELOAD cannot be preloaded (cannot open shared object file): ignored.
2023-04-28 10:52:05 ERROR: ld.so: object ‘(7)]’ from LD_PRELOAD cannot be preloaded (cannot open shared object file): ignored.
2023-04-28 10:52:05 ERROR: ld.so: object ‘exec’ from LD_PRELOAD cannot be preloaded (cannot open shared object file): ignored.
2023-04-28 10:52:05 ERROR: ld.so: object ‘C’ from LD_PRELOAD cannot be preloaded (cannot open shared object file): ignored.
2023-04-28 10:52:05 ERROR: ld.so: object ‘/Program’ from LD_PRELOAD cannot be preloaded (cannot open shared object file): ignored.
2023-04-28 10:52:05 ERROR: ld.so: object ‘Files/Git/usr/bin/bash’ from LD_PRELOAD cannot be preloaded (cannot open shared object file): ignored.
2023-04-28 10:52:05 ERROR: ld.so: object ‘failed’ from LD_PRELOAD cannot be preloaded (cannot open shared object file): ignored.
2023-04-28 10:52:05 ERROR: ld.so: object ‘No’ from LD_PRELOAD cannot be preloaded (cannot open shared object file): ignored.
2023-04-28 10:52:05 ERROR: ld.so: object ‘such’ from LD_PRELOAD cannot be preloaded (cannot open shared object file): ignored.
2023-04-28 10:52:05 ERROR: ld.so: object ‘file’ from LD_PRELOAD cannot be preloaded (cannot open shared object file): ignored.
2023-04-28 10:52:05 ERROR: ld.so: object ‘or’ from LD_PRELOAD cannot be preloaded (cannot open shared object file): ignored.
2023-04-28 10:52:05 ERROR: ld.so: object ‘directory’ from LD_PRELOAD cannot be preloaded (cannot open shared object file): ignored.
2023-04-28 10:52:05 ERROR: ld.so: object ‘[FATAL’ from LD_PRELOAD cannot be preloaded (cannot open shared object file): ignored.
2023-04-28 10:52:05 ERROR: ld.so: object ‘tini’ from LD_PRELOAD cannot be preloaded (cannot open shared object file): ignored.
2023-04-28 10:52:05 ERROR: ld.so: object ‘(7)]’ from LD_PRELOAD cannot be preloaded (cannot open shared object file): ignored.
2023-04-28 10:52:05 ERROR: ld.so: object ‘exec’ from LD_PRELOAD cannot be preloaded (cannot open shared object file): ignored.
2023-04-28 10:52:05 ERROR: ld.so: object ‘C’ from LD_PRELOAD cannot be preloaded (cannot open shared object file): ignored.
2023-04-28 10:52:05 ERROR: ld.so: object ‘/Program’ from LD_PRELOAD cannot be preloaded (cannot open shared object file): ignored.
2023-04-28 10:52:05 ERROR: ld.so: object ‘Files/Git/usr/bin/bash’ from LD_PRELOAD cannot be preloaded (cannot open shared object file): ignored.
2023-04-28 10:52:05 ERROR: ld.so: object ‘failed’ from LD_PRELOAD cannot be preloaded (cannot open shared object file): ignored.
2023-04-28 10:52:05 ERROR: ld.so: object ‘No’ from LD_PRELOAD cannot be preloaded (cannot open shared object file): ignored.
2023-04-28 10:52:05 ERROR: ld.so: object ‘such’ from LD_PRELOAD cannot be preloaded (cannot open shared object file): ignored.
2023-04-28 10:52:05 ERROR: ld.so: object ‘file’ from LD_PRELOAD cannot be preloaded (cannot open shared object file): ignored.
2023-04-28 10:52:05 ERROR: ld.so: object ‘or’ from LD_PRELOAD cannot be preloaded (cannot open shared object file): ignored.
2023-04-28 10:52:05 ERROR: ld.so: object ‘directory’ from LD_PRELOAD cannot be preloaded (cannot open shared object file): ignored.
2023-04-28 10:52:05 ERROR: ld.so: object ‘[FATAL’ from LD_PRELOAD cannot be preloaded (cannot open shared object file): ignored.
2023-04-28 10:52:05 ERROR: ld.so: object ‘tini’ from LD_PRELOAD cannot be preloaded (cannot open shared object file): ignored.
2023-04-28 10:52:05 ERROR: ld.so: object ‘(7)]’ from LD_PRELOAD cannot be preloaded (cannot open shared object file): ignored.
2023-04-28 10:52:05 ERROR: ld.so: object ‘exec’ from LD_PRELOAD cannot be preloaded (cannot open shared object file): ignored.
2023-04-28 10:52:05 ERROR: ld.so: object ‘[FATAL’ from LD_PRELOAD cannot be preloaded (cannot open shared object file): ignored.
2023-04-28 10:52:05 ERROR: ld.so: object ‘C’ from LD_PRELOAD cannot be preloaded (cannot open shared object file): ignored.
2023-04-28 10:52:05 ERROR: ld.so: object ‘/Program’ from LD_PRELOAD cannot be preloaded (cannot open shared object file): ignored.
2023-04-28 10:52:05 ERROR: ld.so: object ‘tini’ from LD_PRELOAD cannot be preloaded (cannot open shared object file): ignored.
2023-04-28 10:52:05 ERROR: ld.so: object ‘Files/Git/usr/bin/bash’ from LD_PRELOAD cannot be preloaded (cannot open shared object file): ignored.
2023-04-28 10:52:05 ERROR: ld.so: object ‘(7)]’ from LD_PRELOAD cannot be preloaded (cannot open shared object file): ignored.
2023-04-28 10:52:05 ERROR: ld.so: object ‘failed’ from LD_PRELOAD cannot be preloaded (cannot open shared object file): ignored.
2023-04-28 10:52:05 ERROR: ld.so: object ‘No’ from LD_PRELOAD cannot be preloaded (cannot open shared object file): ignored.
2023-04-28 10:52:05 ERROR: ld.so: object ‘exec’ from LD_PRELOAD cannot be preloaded (cannot open shared object file): ignored.
2023-04-28 10:52:05 ERROR: ld.so: object ‘such’ from LD_PRELOAD cannot be preloaded (cannot open shared object file): ignored.
2023-04-28 10:52:05 ERROR: ld.so: object ‘C’ from LD_PRELOAD cannot be preloaded (cannot open shared object file): ignored.
2023-04-28 10:52:05 ERROR: ld.so: object ‘/Program’ from LD_PRELOAD cannot be preloaded (cannot open shared object file): ignored.
2023-04-28 10:52:05 ERROR: ld.so: object ‘Files/Git/usr/bin/bash’ from LD_PRELOAD cannot be preloaded (cannot open shared object file): ignored.
2023-04-28 10:52:05 ERROR: ld.so: object ‘file’ from LD_PRELOAD cannot be preloaded (cannot open shared object file): ignored.
2023-04-28 10:52:05 ERROR: ld.so: object ‘failed’ from LD_PRELOAD cannot be preloaded (cannot open shared object file): ignored.
2023-04-28 10:52:05 ERROR: ld.so: object ‘or’ from LD_PRELOAD cannot be preloaded (cannot open shared object file): ignored.
2023-04-28 10:52:05 ERROR: ld.so: object ‘directory’ from LD_PRELOAD cannot be preloaded (cannot open shared object file): ignored.
2023-04-28 10:52:05 ERROR: ld.so: object ‘No’ from LD_PRELOAD cannot be preloaded (cannot open shared object file): ignored.
2023-04-28 10:52:05 ERROR: ld.so: object ‘such’ from LD_PRELOAD cannot be preloaded (cannot open shared object file): ignored.
2023-04-28 10:52:05 ERROR: ld.so: object ‘file’ from LD_PRELOAD cannot be preloaded (cannot open shared object file): ignored.
2023-04-28 10:52:05 ERROR: ld.so: object ‘or’ from LD_PRELOAD cannot be preloaded (cannot open shared object file): ignored.
2023-04-28 10:52:05 ERROR: ld.so: object ‘directory’ from LD_PRELOAD cannot be preloaded (cannot open shared object file): ignored.
2023-04-28 10:52:05 ERROR: ld.so: object ‘[FATAL’ from LD_PRELOAD cannot be preloaded (cannot open shared object file): ignored.
2023-04-28 10:52:05 ERROR: ld.so: object ‘tini’ from LD_PRELOAD cannot be preloaded (cannot open shared object file): ignored.
2023-04-28 10:52:05 ERROR: ld.so: object ‘(7)]’ from LD_PRELOAD cannot be preloaded (cannot open shared object file): ignored.
2023-04-28 10:52:05 ERROR: ld.so: object ‘exec’ from LD_PRELOAD cannot be preloaded (cannot open shared object file): ignored.
2023-04-28 10:52:05 ERROR: ld.so: object ‘C’ from LD_PRELOAD cannot be preloaded (cannot open shared object file): ignored.
2023-04-28 10:52:05 ERROR: ld.so: object ‘/Program’ from LD_PRELOAD cannot be preloaded (cannot open shared object file): ignored.
2023-04-28 10:52:05 ERROR: ld.so: object ‘Files/Git/usr/bin/bash’ from LD_PRELOAD cannot be preloaded (cannot open shared object file): ignored.
2023-04-28 10:52:05 ERROR: ld.so: object ‘failed’ from LD_PRELOAD cannot be preloaded (cannot open shared object file): ignored.
2023-04-28 10:52:05 ERROR: ld.so: object ‘No’ from LD_PRELOAD cannot be preloaded (cannot open shared object file): ignored.
2023-04-28 10:52:05 ERROR: ld.so: object ‘such’ from LD_PRELOAD cannot be preloaded (cannot open shared object file): ignored.
2023-04-28 10:52:05 ERROR: ld.so: object ‘file’ from LD_PRELOAD cannot be preloaded (cannot open shared object file): ignored.
2023-04-28 10:52:05 ERROR: ld.so: object ‘or’ from LD_PRELOAD cannot be preloaded (cannot open shared object file): ignored.
2023-04-28 10:52:05 ERROR: ld.so: object ‘directory’ from LD_PRELOAD cannot be preloaded (cannot open shared object file): ignored.
2023-04-28 10:52:05 ERROR: ld.so: object ‘[FATAL’ from LD_PRELOAD cannot be preloaded (cannot open shared object file): ignored.
2023-04-28 10:52:05 ERROR: ld.so: object ‘tini’ from LD_PRELOAD cannot be preloaded (cannot open shared object file): ignored.
2023-04-28 10:52:05 ERROR: ld.so: object ‘(7)]’ from LD_PRELOAD cannot be preloaded (cannot open shared object file): ignored.
2023-04-28 10:52:05 ERROR: ld.so: object ‘exec’ from LD_PRELOAD cannot be preloaded (cannot open shared object file): ignored.
2023-04-28 10:52:05 ERROR: ld.so: object ‘C’ from LD_PRELOAD cannot be preloaded (cannot open shared object file): ignored.
2023-04-28 10:52:05 ERROR: ld.so: object ‘/Program’ from LD_PRELOAD cannot be preloaded (cannot open shared object file): ignored.
2023-04-28 10:52:05 ERROR: ld.so: object ‘Files/Git/usr/bin/bash’ from LD_PRELOAD cannot be preloaded (cannot open shared object file): ignored.
2023-04-28 10:52:05 ERROR: ld.so: object ‘failed’ from LD_PRELOAD cannot be preloaded (cannot open shared object file): ignored.
2023-04-28 10:52:05 ERROR: ld.so: object ‘No’ from LD_PRELOAD cannot be preloaded (cannot open shared object file): ignored.
2023-04-28 10:52:05 ERROR: ld.so: object ‘such’ from LD_PRELOAD cannot be preloaded (cannot open shared object file): ignored.
2023-04-28 10:52:05 ERROR: ld.so: object ‘file’ from LD_PRELOAD cannot be preloaded (cannot open shared object file): ignored.
2023-04-28 10:52:05 ERROR: ld.so: object ‘or’ from LD_PRELOAD cannot be preloaded (cannot open shared object file): ignored.
2023-04-28 10:52:05 ERROR: ld.so: object ‘directory’ from LD_PRELOAD cannot be preloaded (cannot open shared object file): ignored.
2023-04-28 10:52:05 ERROR: ld.so: object ‘[FATAL’ from LD_PRELOAD cannot be preloaded (cannot open shared object file): ignored.
2023-04-28 10:52:05 ERROR: ld.so: object ‘tini’ from LD_PRELOAD cannot be preloaded (cannot open shared object file): ignored.
2023-04-28 10:52:05 ERROR: ld.so: object ‘(7)]’ from LD_PRELOAD cannot be preloaded (cannot open shared object file): ignored.
2023-04-28 10:52:05 ERROR: ld.so: object ‘exec’ from LD_PRELOAD cannot be preloaded (cannot open shared object file): ignored.
2023-04-28 10:52:05 ERROR: ld.so: object ‘C’ from LD_PRELOAD cannot be preloaded (cannot open shared object file): ignored.
2023-04-28 10:52:05 ERROR: ld.so: object ‘/Program’ from LD_PRELOAD cannot be preloaded (cannot open shared object file): ignored.
2023-04-28 10:52:05 ERROR: ld.so: object ‘Files/Git/usr/bin/bash’ from LD_PRELOAD cannot be preloaded (cannot open shared object file): ignored.
2023-04-28 10:52:05 ERROR: ld.so: object ‘failed’ from LD_PRELOAD cannot be preloaded (cannot open shared object file): ignored.
2023-04-28 10:52:05 ERROR: ld.so: object ‘No’ from LD_PRELOAD cannot be preloaded (cannot open shared object file): ignored.
2023-04-28 10:52:05 ERROR: ld.so: object ‘such’ from LD_PRELOAD cannot be preloaded (cannot open shared object file): ignored.
2023-04-28 10:52:05 ERROR: ld.so: object ‘file’ from LD_PRELOAD cannot be preloaded (cannot open shared object file): ignored.
2023-04-28 10:52:05 ERROR: ld.so: object ‘or’ from LD_PRELOAD cannot be preloaded (cannot open shared object file): ignored.
2023-04-28 10:52:05 ERROR: ld.so: object ‘directory’ from LD_PRELOAD cannot be preloaded (cannot open shared object file): ignored.
2023-04-28 10:52:05 ERROR: ld.so: object ‘[FATAL’ from LD_PRELOAD cannot be preloaded (cannot open shared object file): ignored.
2023-04-28 10:52:05 ERROR: ld.so: object ‘tini’ from LD_PRELOAD cannot be preloaded (cannot open shared object file): ignored.
2023-04-28 10:52:05 ERROR: ld.so: object ‘(7)]’ from LD_PRELOAD cannot be preloaded (cannot open shared object file): ignored.
2023-04-28 10:52:05 ERROR: ld.so: object ‘exec’ from LD_PRELOAD cannot be preloaded (cannot open shared object file): ignored.
2023-04-28 10:52:05 ERROR: ld.so: object ‘C’ from LD_PRELOAD cannot be preloaded (cannot open shared object file): ignored.
2023-04-28 10:52:05 ERROR: ld.so: object ‘/Program’ from LD_PRELOAD cannot be preloaded (cannot open shared object file): ignored.
2023-04-28 10:52:05 ERROR: ld.so: object ‘Files/Git/usr/bin/bash’ from LD_PRELOAD cannot be preloaded (cannot open shared object file): ignored.
2023-04-28 10:52:05 ERROR: ld.so: object ‘failed’ from LD_PRELOAD cannot be preloaded (cannot open shared object file): ignored.
2023-04-28 10:52:05 ERROR: ld.so: object ‘No’ from LD_PRELOAD cannot be preloaded (cannot open shared object file): ignored.
2023-04-28 10:52:05 ERROR: ld.so: object ‘such’ from LD_PRELOAD cannot be preloaded (cannot open shared object file): ignored.
2023-04-28 10:52:05 ERROR: ld.so: object ‘file’ from LD_PRELOAD cannot be preloaded (cannot open shared object file): ignored.
2023-04-28 10:52:05 ERROR: ld.so: object ‘or’ from LD_PRELOAD cannot be preloaded (cannot open shared object file): ignored.
2023-04-28 10:52:05 ERROR: ld.so: object ‘directory’ from LD_PRELOAD cannot be preloaded (cannot open shared object file): ignored.
2023-04-28 10:52:05 ERROR: ld.so: object ‘[FATAL’ from LD_PRELOAD cannot be preloaded (cannot open shared object file): ignored.
2023-04-28 10:52:05 ERROR: ld.so: object ‘tini’ from LD_PRELOAD cannot be preloaded (cannot open shared object file): ignored.
2023-04-28 10:52:05 ERROR: ld.so: object ‘(7)]’ from LD_PRELOAD cannot be preloaded (cannot open shared object file): ignored.
2023-04-28 10:52:05 ERROR: ld.so: object ‘exec’ from LD_PRELOAD cannot be preloaded (cannot open shared object file): ignored.
2023-04-28 10:52:05 ERROR: ld.so: object ‘C’ from LD_PRELOAD cannot be preloaded (cannot open shared object file): ignored.
2023-04-28 10:52:05 ERROR: ld.so: object ‘/Program’ from LD_PRELOAD cannot be preloaded (cannot open shared object file): ignored.
2023-04-28 10:52:05 ERROR: ld.so: object ‘Files/Git/usr/bin/bash’ from LD_PRELOAD cannot be preloaded (cannot open shared object file): ignored.
2023-04-28 10:52:05 ERROR: ld.so: object ‘failed’ from LD_PRELOAD cannot be preloaded (cannot open shared object file): ignored.
2023-04-28 10:52:05 ERROR: ld.so: object ‘No’ from LD_PRELOAD cannot be preloaded (cannot open shared object file): ignored.
2023-04-28 10:52:05 ERROR: ld.so: object ‘such’ from LD_PRELOAD cannot be preloaded (cannot open shared object file): ignored.
2023-04-28 10:52:05 ERROR: ld.so: object ‘file’ from LD_PRELOAD cannot be preloaded (cannot open shared object file): ignored.
2023-04-28 10:52:05 ERROR: ld.so: object ‘or’ from LD_PRELOAD cannot be preloaded (cannot open shared object file): ignored.
2023-04-28 10:52:05 ERROR: ld.so: object ‘directory’ from LD_PRELOAD cannot be preloaded (cannot open shared object file): ignored.
2023-04-28 10:52:05 ERROR: ld.so: object ‘[FATAL’ from LD_PRELOAD cannot be preloaded (cannot open shared object file): ignored.
2023-04-28 10:52:05 ERROR: ld.so: object ‘tini’ from LD_PRELOAD cannot be preloaded (cannot open shared object file): ignored.
2023-04-28 10:52:05 ERROR: ld.so: object ‘(7)]’ from LD_PRELOAD cannot be preloaded (cannot open shared object file): ignored.
2023-04-28 10:52:05 ERROR: ld.so: object ‘exec’ from LD_PRELOAD cannot be preloaded (cannot open shared object file): ignored.
2023-04-28 10:52:05 ERROR: ld.so: object ‘C’ from LD_PRELOAD cannot be preloaded (cannot open shared object file): ignored.
2023-04-28 10:52:05 ERROR: ld.so: object ‘/Program’ from LD_PRELOAD cannot be preloaded (cannot open shared object file): ignored.
2023-04-28 10:52:05 ERROR: ld.so: object ‘Files/Git/usr/bin/bash’ from LD_PRELOAD cannot be preloaded (cannot open shared object file): ignored.
2023-04-28 10:52:05 ERROR: ld.so: object ‘failed’ from LD_PRELOAD cannot be preloaded (cannot open shared object file): ignored.
2023-04-28 10:52:05 ERROR: ld.so: object ‘No’ from LD_PRELOAD cannot be preloaded (cannot open shared object file): ignored.
2023-04-28 10:52:05 ERROR: ld.so: object ‘such’ from LD_PRELOAD cannot be preloaded (cannot open shared object file): ignored.
2023-04-28 10:52:05 ERROR: ld.so: object ‘file’ from LD_PRELOAD cannot be preloaded (cannot open shared object file): ignored.
2023-04-28 10:52:05 ERROR: ld.so: object ‘or’ from LD_PRELOAD cannot be preloaded (cannot open shared object file): ignored.
2023-04-28 10:52:05 ERROR: ld.so: object ‘directory’ from LD_PRELOAD cannot be preloaded (cannot open shared object file): ignored.
2023-04-28 10:52:05 ERROR: ld.so: object ‘[FATAL’ from LD_PRELOAD cannot be preloaded (cannot open shared object file): ignored.
2023-04-28 10:52:05 ERROR: ld.so: object ‘tini’ from LD_PRELOAD cannot be preloaded (cannot open shared object file): ignored.
2023-04-28 10:52:05 ERROR: ld.so: object ‘(7)]’ from LD_PRELOAD cannot be preloaded (cannot open shared object file): ignored.
2023-04-28 10:52:05 ERROR: ld.so: object ‘exec’ from LD_PRELOAD cannot be preloaded (cannot open shared object file): ignored.
2023-04-28 10:52:05 ERROR: ld.so: object ‘C’ from LD_PRELOAD cannot be preloaded (cannot open shared object file): ignored.
2023-04-28 10:52:05 ERROR: ld.so: object ‘/Program’ from LD_PRELOAD cannot be preloaded (cannot open shared object file): ignored.
2023-04-28 10:52:05 ERROR: ld.so: object ‘Files/Git/usr/bin/bash’ from LD_PRELOAD cannot be preloaded (cannot open shared object file): ignored.
2023-04-28 10:52:05 ERROR: ld.so: object ‘failed’ from LD_PRELOAD cannot be preloaded (cannot open shared object file): ignored.
2023-04-28 10:52:05 ERROR: ld.so: object ‘No’ from LD_PRELOAD cannot be preloaded (cannot open shared object file): ignored.
2023-04-28 10:52:05 ERROR: ld.so: object ‘such’ from LD_PRELOAD cannot be preloaded (cannot open shared object file): ignored.
2023-04-28 10:52:05 ERROR: ld.so: object ‘file’ from LD_PRELOAD cannot be preloaded (cannot open shared object file): ignored.
2023-04-28 10:52:05 ERROR: ld.so: object ‘or’ from LD_PRELOAD cannot be preloaded (cannot open shared object file): ignored.
2023-04-28 10:52:05 ERROR: ld.so: object ‘directory’ from LD_PRELOAD cannot be preloaded (cannot open shared object file): ignored.
2023-04-28 10:52:05 ERROR: ld.so: object ‘[FATAL’ from LD_PRELOAD cannot be preloaded (cannot open shared object file): ignored.
2023-04-28 10:52:05 ERROR: ld.so: object ‘tini’ from LD_PRELOAD cannot be preloaded (cannot open shared object file): ignored.
2023-04-28 10:52:05 ERROR: ld.so: object ‘(7)]’ from LD_PRELOAD cannot be preloaded (cannot open shared object file): ignored.
2023-04-28 10:52:05 ERROR: ld.so: object ‘exec’ from LD_PRELOAD cannot be preloaded (cannot open shared object file): ignored.
2023-04-28 10:52:05 ERROR: ld.so: object ‘C’ from LD_PRELOAD cannot be preloaded (cannot open shared object file): ignored.
2023-04-28 10:52:05 ERROR: ld.so: object ‘/Program’ from LD_PRELOAD cannot be preloaded (cannot open shared object file): ignored.
2023-04-28 10:52:05 ERROR: ld.so: object ‘Files/Git/usr/bin/bash’ from LD_PRELOAD cannot be preloaded (cannot open shared object file): ignored.
2023-04-28 10:52:05 ERROR: ld.so: object ‘failed’ from LD_PRELOAD cannot be preloaded (cannot open shared object file): ignored.
2023-04-28 10:52:05 ERROR: ld.so: object ‘No’ from LD_PRELOAD cannot be preloaded (cannot open shared object file): ignored.
2023-04-28 10:52:05 ERROR: ld.so: object ‘such’ from LD_PRELOAD cannot be preloaded (cannot open shared object file): ignored.
2023-04-28 10:52:05 ERROR: ld.so: object ‘file’ from LD_PRELOAD cannot be preloaded (cannot open shared object file): ignored.
2023-04-28 10:52:05 ERROR: ld.so: object ‘or’ from LD_PRELOAD cannot be preloaded (cannot open shared object file): ignored.
2023-04-28 10:52:05 ERROR: ld.so: object ‘directory’ from LD_PRELOAD cannot be preloaded (cannot open shared object file): ignored.
2023-04-28 10:52:05 ERROR: ld.so: object ‘[FATAL’ from LD_PRELOAD cannot be preloaded (cannot open shared object file): ignored.
2023-04-28 10:52:05 ERROR: ld.so: object ‘tini’ from LD_PRELOAD cannot be preloaded (cannot open shared object file): ignored.
2023-04-28 10:52:05 ERROR: ld.so: object ‘(7)]’ from LD_PRELOAD cannot be preloaded (cannot open shared object file): ignored.
2023-04-28 10:52:05 ERROR: ld.so: object ‘exec’ from LD_PRELOAD cannot be preloaded (cannot open shared object file): ignored.
2023-04-28 10:52:05 ERROR: ld.so: object ‘C’ from LD_PRELOAD cannot be preloaded (cannot open shared object file): ignored.
2023-04-28 10:52:05 ERROR: ld.so: object ‘/Program’ from LD_PRELOAD cannot be preloaded (cannot open shared object file): ignored.
2023-04-28 10:52:05 ERROR: ld.so: object ‘Files/Git/usr/bin/bash’ from LD_PRELOAD cannot be preloaded (cannot open shared object file): ignored.
2023-04-28 10:52:05 ERROR: ld.so: object ‘failed’ from LD_PRELOAD cannot be preloaded (cannot open shared object file): ignored.
2023-04-28 10:52:05 ERROR: ld.so: object ‘No’ from LD_PRELOAD cannot be preloaded (cannot open shared object file): ignored.
2023-04-28 10:52:05 ERROR: ld.so: object ‘such’ from LD_PRELOAD cannot be preloaded (cannot open shared object file): ignored.
2023-04-28 10:52:05 ERROR: ld.so: object ‘file’ from LD_PRELOAD cannot be preloaded (cannot open shared object file): ignored.
2023-04-28 10:52:05 ERROR: ld.so: object ‘or’ from LD_PRELOAD cannot be preloaded (cannot open shared object file): ignored.
2023-04-28 10:52:05 ERROR: ld.so: object ‘directory’ from LD_PRELOAD cannot be preloaded (cannot open shared object file): ignored.
2023-04-28 10:52:05 ERROR: ld.so: object ‘[FATAL’ from LD_PRELOAD cannot be preloaded (cannot open shared object file): ignored.
2023-04-28 10:52:05 ERROR: ld.so: object ‘tini’ from LD_PRELOAD cannot be preloaded (cannot open shared object file): ignored.
2023-04-28 10:52:05 ERROR: ld.so: object ‘(7)]’ from LD_PRELOAD cannot be preloaded (cannot open shared object file): ignored.
2023-04-28 10:52:05 ERROR: ld.so: object ‘exec’ from LD_PRELOAD cannot be preloaded (cannot open shared object file): ignored.
2023-04-28 10:52:05 ERROR: ld.so: object ‘C’ from LD_PRELOAD cannot be preloaded (cannot open shared object file): ignored.
2023-04-28 10:52:05 ERROR: ld.so: object ‘/Program’ from LD_PRELOAD cannot be preloaded (cannot open shared object file): ignored.
2023-04-28 10:52:05 ERROR: ld.so: object ‘Files/Git/usr/bin/bash’ from LD_PRELOAD cannot be preloaded (cannot open shared object file): ignored.
2023-04-28 10:52:05 ERROR: ld.so: object ‘failed’ from LD_PRELOAD cannot be preloaded (cannot open shared object file): ignored.
2023-04-28 10:52:05 ERROR: ld.so: object ‘No’ from LD_PRELOAD cannot be preloaded (cannot open shared object file): ignored.
2023-04-28 10:52:05 ERROR: ld.so: object ‘such’ from LD_PRELOAD cannot be preloaded (cannot open shared object file): ignored.
2023-04-28 10:52:05 ERROR: ld.so: object ‘file’ from LD_PRELOAD cannot be preloaded (cannot open shared object file): ignored.
2023-04-28 10:52:05 ERROR: ld.so: object ‘or’ from LD_PRELOAD cannot be preloaded (cannot open shared object file): ignored.
2023-04-28 10:52:05 ERROR: ld.so: object ‘directory’ from LD_PRELOAD cannot be preloaded (cannot open shared object file): ignored.
2023-04-28 10:52:05 ERROR: ld.so: object ‘[FATAL’ from LD_PRELOAD cannot be preloaded (cannot open shared object file): ignored.
2023-04-28 10:52:05 ERROR: ld.so: object ‘[FATAL’ from LD_PRELOAD cannot be preloaded (cannot open shared object file): ignored.
2023-04-28 10:52:05 ERROR: ld.so: object ‘tini’ from LD_PRELOAD cannot be preloaded (cannot open shared object file): ignored.
2023-04-28 10:52:05 ERROR: ld.so: object ‘tini’ from LD_PRELOAD cannot be preloaded (cannot open shared object file): ignored.
2023-04-28 10:52:05 ERROR: ld.so: object ‘(7)]’ from LD_PRELOAD cannot be preloaded (cannot open shared object file): ignored.
2023-04-28 10:52:05 ERROR: ld.so: object ‘(7)]’ from LD_PRELOAD cannot be preloaded (cannot open shared object file): ignored.
2023-04-28 10:52:05 ERROR: ld.so: object ‘exec’ from LD_PRELOAD cannot be preloaded (cannot open shared object file): ignored.
2023-04-28 10:52:05 ERROR: ld.so: object ‘exec’ from LD_PRELOAD cannot be preloaded (cannot open shared object file): ignored.
2023-04-28 10:52:05 ERROR: ld.so: object ‘C’ from LD_PRELOAD cannot be preloaded (cannot open shared object file): ignored.
2023-04-28 10:52:05 ERROR: ld.so: object ‘C’ from LD_PRELOAD cannot be preloaded (cannot open shared object file): ignored.
2023-04-28 10:52:05 ERROR: ld.so: object ‘/Program’ from LD_PRELOAD cannot be preloaded (cannot open shared object file): ignored.
2023-04-28 10:52:05 ERROR: ld.so: object ‘Files/Git/usr/bin/bash’ from LD_PRELOAD cannot be preloaded (cannot open shared object file): ignored.
2023-04-28 10:52:05 ERROR: ld.so: object ‘/Program’ from LD_PRELOAD cannot be preloaded (cannot open shared object file): ignored.
2023-04-28 10:52:05 ERROR: ld.so: object ‘Files/Git/usr/bin/bash’ from LD_PRELOAD cannot be preloaded (cannot open shared object file): ignored.
2023-04-28 10:52:05 ERROR: ld.so: object ‘failed’ from LD_PRELOAD cannot be preloaded (cannot open shared object file): ignored.
2023-04-28 10:52:05 ERROR: ld.so: object ‘failed’ from LD_PRELOAD cannot be preloaded (cannot open shared object file): ignored.
2023-04-28 10:52:05 ERROR: ld.so: object ‘No’ from LD_PRELOAD cannot be preloaded (cannot open shared object file): ignored.
2023-04-28 10:52:05 ERROR: ld.so: object ‘No’ from LD_PRELOAD cannot be preloaded (cannot open shared object file): ignored.
2023-04-28 10:52:05 ERROR: ld.so: object ‘such’ from LD_PRELOAD cannot be preloaded (cannot open shared object file): ignored.
2023-04-28 10:52:05 ERROR: ld.so: object ‘such’ from LD_PRELOAD cannot be preloaded (cannot open shared object file): ignored.
2023-04-28 10:52:05 ERROR: ld.so: object ‘file’ from LD_PRELOAD cannot be preloaded (cannot open shared object file): ignored.
2023-04-28 10:52:05 ERROR: ld.so: object ‘file’ from LD_PRELOAD cannot be preloaded (cannot open shared object file): ignored.
2023-04-28 10:52:05 ERROR: ld.so: object ‘or’ from LD_PRELOAD cannot be preloaded (cannot open shared object file): ignored.
2023-04-28 10:52:05 ERROR: ld.so: object ‘or’ from LD_PRELOAD cannot be preloaded (cannot open shared object file): ignored.
2023-04-28 10:52:05 ERROR: ld.so: object ‘directory’ from LD_PRELOAD cannot be preloaded (cannot open shared object file): ignored.
2023-04-28 10:52:05 ERROR: ld.so: object ‘directory’ from LD_PRELOAD cannot be preloaded (cannot open shared object file): ignored.
2023-04-28 10:52:05 ERROR: ld.so: object ‘[FATAL’ from LD_PRELOAD cannot be preloaded (cannot open shared object file): ignored.
2023-04-28 10:52:05 ERROR: ld.so: object ‘[FATAL’ from LD_PRELOAD cannot be preloaded (cannot open shared object file): ignored.
2023-04-28 10:52:05 ERROR: ld.so: object ‘tini’ from LD_PRELOAD cannot be preloaded (cannot open shared object file): ignored.
2023-04-28 10:52:05 ERROR: ld.so: object ‘tini’ from LD_PRELOAD cannot be preloaded (cannot open shared object file): ignored.
2023-04-28 10:52:05 ERROR: ld.so: object ‘(7)]’ from LD_PRELOAD cannot be preloaded (cannot open shared object file): ignored.
2023-04-28 10:52:05 ERROR: ld.so: object ‘(7)]’ from LD_PRELOAD cannot be preloaded (cannot open shared object file): ignored.
2023-04-28 10:52:05 ERROR: ld.so: object ‘exec’ from LD_PRELOAD cannot be preloaded (cannot open shared object file): ignored.
2023-04-28 10:52:05 ERROR: ld.so: object ‘exec’ from LD_PRELOAD cannot be preloaded (cannot open shared object file): ignored.
2023-04-28 10:52:05 ERROR: ld.so: object ‘C’ from LD_PRELOAD cannot be preloaded (cannot open shared object file): ignored.
2023-04-28 10:52:05 ERROR: ld.so: object ‘/Program’ from LD_PRELOAD cannot be preloaded (cannot open shared object file): ignored.
2023-04-28 10:52:05 ERROR: ld.so: object ‘Files/Git/usr/bin/bash’ from LD_PRELOAD cannot be preloaded (cannot open shared object file): ignored.
2023-04-28 10:52:05 ERROR: ld.so: object ‘C’ from LD_PRELOAD cannot be preloaded (cannot open shared object file): ignored.
2023-04-28 10:52:05 ERROR: ld.so: object ‘/Program’ from LD_PRELOAD cannot be preloaded (cannot open shared object file): ignored.
2023-04-28 10:52:05 ERROR: ld.so: object ‘Files/Git/usr/bin/bash’ from LD_PRELOAD cannot be preloaded (cannot open shared object file): ignored.
2023-04-28 10:52:05 ERROR: ld.so: object ‘failed’ from LD_PRELOAD cannot be preloaded (cannot open shared object file): ignored.
2023-04-28 10:52:05 ERROR: ld.so: object ‘No’ from LD_PRELOAD cannot be preloaded (cannot open shared object file): ignored.
2023-04-28 10:52:05 ERROR: ld.so: object ‘failed’ from LD_PRELOAD cannot be preloaded (cannot open shared object file): ignored.
2023-04-28 10:52:05 ERROR: ld.so: object ‘such’ from LD_PRELOAD cannot be preloaded (cannot open shared object file): ignored.
2023-04-28 10:52:05 ERROR: ld.so: object ‘file’ from LD_PRELOAD cannot be preloaded (cannot open shared object file): ignored.
2023-04-28 10:52:05 ERROR: ld.so: object ‘No’ from LD_PRELOAD cannot be preloaded (cannot open shared object file): ignored.
2023-04-28 10:52:05 ERROR: ld.so: object ‘or’ from LD_PRELOAD cannot be preloaded (cannot open shared object file): ignored.
2023-04-28 10:52:05 ERROR: ld.so: object ‘directory’ from LD_PRELOAD cannot be preloaded (cannot open shared object file): ignored.
2023-04-28 10:52:05 ERROR: ld.so: object ‘such’ from LD_PRELOAD cannot be preloaded (cannot open shared object file): ignored.
2023-04-28 10:52:05 ERROR: ld.so: object ‘file’ from LD_PRELOAD cannot be preloaded (cannot open shared object file): ignored.
2023-04-28 10:52:05 ERROR: ld.so: object ‘or’ from LD_PRELOAD cannot be preloaded (cannot open shared object file): ignored.
2023-04-28 10:52:05 ERROR: ld.so: object ‘directory’ from LD_PRELOAD cannot be preloaded (cannot open shared object file): ignored.
2023-04-28 10:52:05 ERROR: ld.so: object ‘[FATAL’ from LD_PRELOAD cannot be preloaded (cannot open shared object file): ignored.
2023-04-28 10:52:05 ERROR: ld.so: object ‘tini’ from LD_PRELOAD cannot be preloaded (cannot open shared object file): ignored.
2023-04-28 10:52:05 ERROR: ld.so: object ‘(7)]’ from LD_PRELOAD cannot be preloaded (cannot open shared object file): ignored.
2023-04-28 10:52:05 ERROR: ld.so: object ‘exec’ from LD_PRELOAD cannot be preloaded (cannot open shared object file): ignored.
2023-04-28 10:52:05 ERROR: ld.so: object ‘C’ from LD_PRELOAD cannot be preloaded (cannot open shared object file): ignored.
2023-04-28 10:52:05 ERROR: ld.so: object ‘/Program’ from LD_PRELOAD cannot be preloaded (cannot open shared object file): ignored.
2023-04-28 10:52:05 ERROR: ld.so: object ‘Files/Git/usr/bin/bash’ from LD_PRELOAD cannot be preloaded (cannot open shared object file): ignored.
2023-04-28 10:52:05 ERROR: ld.so: object ‘failed’ from LD_PRELOAD cannot be preloaded (cannot open shared object file): ignored.
2023-04-28 10:52:05 ERROR: ld.so: object ‘No’ from LD_PRELOAD cannot be preloaded (cannot open shared object file): ignored.
2023-04-28 10:52:05 ERROR: ld.so: object ‘such’ from LD_PRELOAD cannot be preloaded (cannot open shared object file): ignored.
2023-04-28 10:52:05 ERROR: ld.so: object ‘file’ from LD_PRELOAD cannot be preloaded (cannot open shared object file): ignored.
2023-04-28 10:52:05 ERROR: ld.so: object ‘or’ from LD_PRELOAD cannot be preloaded (cannot open shared object file): ignored.
2023-04-28 10:52:05 ERROR: ld.so: object ‘directory’ from LD_PRELOAD cannot be preloaded (cannot open shared object file): ignored.
2023-04-28 10:52:05 ERROR: ld.so: object ‘[FATAL’ from LD_PRELOAD cannot be preloaded (cannot open shared object file): ignored.
2023-04-28 10:52:05 ERROR: ld.so: object ‘tini’ from LD_PRELOAD cannot be preloaded (cannot open shared object file): ignored.
2023-04-28 10:52:05 ERROR: ld.so: object ‘(7)]’ from LD_PRELOAD cannot be preloaded (cannot open shared object file): ignored.
2023-04-28 10:52:05 ERROR: ld.so: object ‘exec’ from LD_PRELOAD cannot be preloaded (cannot open shared object file): ignored.
2023-04-28 10:52:05 ERROR: ld.so: object ‘C’ from LD_PRELOAD cannot be preloaded (cannot open shared object file): ignored.
2023-04-28 10:52:05 ERROR: ld.so: object ‘/Program’ from LD_PRELOAD cannot be preloaded (cannot open shared object file): ignored.
2023-04-28 10:52:05 ERROR: ld.so: object ‘Files/Git/usr/bin/bash’ from LD_PRELOAD cannot be preloaded (cannot open shared object file): ignored.
2023-04-28 10:52:05 ERROR: ld.so: object ‘failed’ from LD_PRELOAD cannot be preloaded (cannot open shared object file): ignored.
2023-04-28 10:52:05 ERROR: ld.so: object ‘No’ from LD_PRELOAD cannot be preloaded (cannot open shared object file): ignored.
2023-04-28 10:52:05 ERROR: ld.so: object ‘such’ from LD_PRELOAD cannot be preloaded (cannot open shared object file): ignored.
2023-04-28 10:52:05 ERROR: ld.so: object ‘file’ from LD_PRELOAD cannot be preloaded (cannot open shared object file): ignored.
2023-04-28 10:52:05 ERROR: ld.so: object ‘or’ from LD_PRELOAD cannot be preloaded (cannot open shared object file): ignored.
2023-04-28 10:52:05 ERROR: ld.so: object ‘directory’ from LD_PRELOAD cannot be preloaded (cannot open shared object file): ignored.
2023-04-28 10:52:05 ERROR: ld.so: object ‘[FATAL’ from LD_PRELOAD cannot be preloaded (cannot open shared object file): ignored.
2023-04-28 10:52:05 ERROR: ld.so: object ‘tini’ from LD_PRELOAD cannot be preloaded (cannot open shared object file): ignored.
2023-04-28 10:52:05 ERROR: ld.so: object ‘(7)]’ from LD_PRELOAD cannot be preloaded (cannot open shared object file): ignored.
2023-04-28 10:52:05 ERROR: ld.so: object ‘exec’ from LD_PRELOAD cannot be preloaded (cannot open shared object file): ignored.
2023-04-28 10:52:05 ERROR: ld.so: object ‘C’ from LD_PRELOAD cannot be preloaded (cannot open shared object file): ignored.
2023-04-28 10:52:05 ERROR: ld.so: object ‘/Program’ from LD_PRELOAD cannot be preloaded (cannot open shared object file): ignored.
2023-04-28 10:52:05 ERROR: ld.so: object ‘Files/Git/usr/bin/bash’ from LD_PRELOAD cannot be preloaded (cannot open shared object file): ignored.
2023-04-28 10:52:05 ERROR: ld.so: object ‘failed’ from LD_PRELOAD cannot be preloaded (cannot open shared object file): ignored.
2023-04-28 10:52:05 ERROR: ld.so: object ‘No’ from LD_PRELOAD cannot be preloaded (cannot open shared object file): ignored.
2023-04-28 10:52:05 ERROR: ld.so: object ‘such’ from LD_PRELOAD cannot be preloaded (cannot open shared object file): ignored.
2023-04-28 10:52:05 ERROR: ld.so: object ‘file’ from LD_PRELOAD cannot be preloaded (cannot open shared object file): ignored.
2023-04-28 10:52:05 ERROR: ld.so: object ‘or’ from LD_PRELOAD cannot be preloaded (cannot open shared object file): ignored.
2023-04-28 10:52:05 ERROR: ld.so: object ‘directory’ from LD_PRELOAD cannot be preloaded (cannot open shared object file): ignored.
2023-04-28 10:52:05 ERROR: ld.so: object ‘[FATAL’ from LD_PRELOAD cannot be preloaded (cannot open shared object file): ignored.
2023-04-28 10:52:05 ERROR: ld.so: object ‘tini’ from LD_PRELOAD cannot be preloaded (cannot open shared object file): ignored.
2023-04-28 10:52:05 ERROR: ld.so: object ‘(7)]’ from LD_PRELOAD cannot be preloaded (cannot open shared object file): ignored.
2023-04-28 10:52:05 ERROR: ld.so: object ‘exec’ from LD_PRELOAD cannot be preloaded (cannot open shared object file): ignored.
2023-04-28 10:52:05 ERROR: ld.so: object ‘C’ from LD_PRELOAD cannot be preloaded (cannot open shared object file): ignored.
2023-04-28 10:52:05 ERROR: ld.so: object ‘/Program’ from LD_PRELOAD cannot be preloaded (cannot open shared object file): ignored.
2023-04-28 10:52:05 ERROR: ld.so: object ‘Files/Git/usr/bin/bash’ from LD_PRELOAD cannot be preloaded (cannot open shared object file): ignored.
2023-04-28 10:52:05 ERROR: ld.so: object ‘failed’ from LD_PRELOAD cannot be preloaded (cannot open shared object file): ignored.
2023-04-28 10:52:05 ERROR: ld.so: object ‘No’ from LD_PRELOAD cannot be preloaded (cannot open shared object file): ignored.
2023-04-28 10:52:05 ERROR: ld.so: object ‘such’ from LD_PRELOAD cannot be preloaded (cannot open shared object file): ignored.
2023-04-28 10:52:05 ERROR: ld.so: object ‘file’ from LD_PRELOAD cannot be preloaded (cannot open shared object file): ignored.
2023-04-28 10:52:05 ERROR: ld.so: object ‘or’ from LD_PRELOAD cannot be preloaded (cannot open shared object file): ignored.
2023-04-28 10:52:05 ERROR: ld.so: object ‘directory’ from LD_PRELOAD cannot be preloaded (cannot open shared object file): ignored.
2023-04-28 10:52:05 ERROR: ld.so: object ‘[FATAL’ from LD_PRELOAD cannot be preloaded (cannot open shared object file): ignored.
2023-04-28 10:52:05 ERROR: ld.so: object ‘tini’ from LD_PRELOAD cannot be preloaded (cannot open shared object file): ignored.
2023-04-28 10:52:05 ERROR: ld.so: object ‘(7)]’ from LD_PRELOAD cannot be preloaded (cannot open shared object file): ignored.
2023-04-28 10:52:05 ERROR: ld.so: object ‘exec’ from LD_PRELOAD cannot be preloaded (cannot open shared object file): ignored.
2023-04-28 10:52:05 ERROR: ld.so: object ‘C’ from LD_PRELOAD cannot be preloaded (cannot open shared object file): ignored.
2023-04-28 10:52:05 ERROR: ld.so: object ‘/Program’ from LD_PRELOAD cannot be preloaded (cannot open shared object file): ignored.
2023-04-28 10:52:05 ERROR: ld.so: object ‘Files/Git/usr/bin/bash’ from LD_PRELOAD cannot be preloaded (cannot open shared object file): ignored.
2023-04-28 10:52:05 ERROR: ld.so: object ‘failed’ from LD_PRELOAD cannot be preloaded (cannot open shared object file): ignored.
2023-04-28 10:52:05 ERROR: ld.so: object ‘No’ from LD_PRELOAD cannot be preloaded (cannot open shared object file): ignored.
2023-04-28 10:52:05 ERROR: ld.so: object ‘such’ from LD_PRELOAD cannot be preloaded (cannot open shared object file): ignored.
2023-04-28 10:52:05 ERROR: ld.so: object ‘file’ from LD_PRELOAD cannot be preloaded (cannot open shared object file): ignored.
2023-04-28 10:52:05 ERROR: ld.so: object ‘or’ from LD_PRELOAD cannot be preloaded (cannot open shared object file): ignored.
2023-04-28 10:52:05 ERROR: ld.so: object ‘directory’ from LD_PRELOAD cannot be preloaded (cannot open shared object file): ignored.
2023-04-28 10:52:05 ERROR: ld.so: object ‘[FATAL’ from LD_PRELOAD cannot be preloaded (cannot open shared object file): ignored.
2023-04-28 10:52:05 ERROR: ld.so: object ‘tini’ from LD_PRELOAD cannot be preloaded (cannot open shared object file): ignored.
2023-04-28 10:52:05 ERROR: ld.so: object ‘(7)]’ from LD_PRELOAD cannot be preloaded (cannot open shared object file): ignored.
2023-04-28 10:52:05 ERROR: ld.so: object ‘exec’ from LD_PRELOAD cannot be preloaded (cannot open shared object file): ignored.
2023-04-28 10:52:05 ERROR: ld.so: object ‘C’ from LD_PRELOAD cannot be preloaded (cannot open shared object file): ignored.
2023-04-28 10:52:05 ERROR: ld.so: object ‘/Program’ from LD_PRELOAD cannot be preloaded (cannot open shared object file): ignored.
2023-04-28 10:52:05 ERROR: ld.so: object ‘Files/Git/usr/bin/bash’ from LD_PRELOAD cannot be preloaded (cannot open shared object file): ignored.
2023-04-28 10:52:05 ERROR: ld.so: object ‘failed’ from LD_PRELOAD cannot be preloaded (cannot open shared object file): ignored.
2023-04-28 10:52:05 ERROR: ld.so: object ‘No’ from LD_PRELOAD cannot be preloaded (cannot open shared object file): ignored.
2023-04-28 10:52:05 ERROR: ld.so: object ‘such’ from LD_PRELOAD cannot be preloaded (cannot open shared object file): ignored.
2023-04-28 10:52:05 ERROR: ld.so: object ‘file’ from LD_PRELOAD cannot be preloaded (cannot open shared object file): ignored.
2023-04-28 10:52:05 ERROR: ld.so: object ‘or’ from LD_PRELOAD cannot be preloaded (cannot open shared object file): ignored.
2023-04-28 10:52:05 ERROR: ld.so: object ‘directory’ from LD_PRELOAD cannot be preloaded (cannot open shared object file): ignored.
2023-04-28 10:52:05 ERROR: ld.so: object ‘[FATAL’ from LD_PRELOAD cannot be preloaded (cannot open shared object file): ignored.
2023-04-28 10:52:05 ERROR: ld.so: object ‘tini’ from LD_PRELOAD cannot be preloaded (cannot open shared object file): ignored.
2023-04-28 10:52:05 ERROR: ld.so: object ‘(7)]’ from LD_PRELOAD cannot be preloaded (cannot open shared object file): ignored.
2023-04-28 10:52:05 ERROR: ld.so: object ‘exec’ from LD_PRELOAD cannot be preloaded (cannot open shared object file): ignored.
2023-04-28 10:52:05 ERROR: ld.so: object ‘C’ from LD_PRELOAD cannot be preloaded (cannot open shared object file): ignored.
2023-04-28 10:52:05 ERROR: ld.so: object ‘/Program’ from LD_PRELOAD cannot be preloaded (cannot open shared object file): ignored.
2023-04-28 10:52:05 ERROR: ld.so: object ‘Files/Git/usr/bin/bash’ from LD_PRELOAD cannot be preloaded (cannot open shared object file): ignored.
2023-04-28 10:52:05 ERROR: ld.so: object ‘failed’ from LD_PRELOAD cannot be preloaded (cannot open shared object file): ignored.
2023-04-28 10:52:05 ERROR: ld.so: object ‘No’ from LD_PRELOAD cannot be preloaded (cannot open shared object file): ignored.
2023-04-28 10:52:05 ERROR: ld.so: object ‘such’ from LD_PRELOAD cannot be preloaded (cannot open shared object file): ignored.
2023-04-28 10:52:05 ERROR: ld.so: object ‘file’ from LD_PRELOAD cannot be preloaded (cannot open shared object file): ignored.
2023-04-28 10:52:05 ERROR: ld.so: object ‘or’ from LD_PRELOAD cannot be preloaded (cannot open shared object file): ignored.
2023-04-28 10:52:05 ERROR: ld.so: object ‘directory’ from LD_PRELOAD cannot be preloaded (cannot open shared object file): ignored.
2023-04-28 10:52:05 ERROR: ld.so: object ‘[FATAL’ from LD_PRELOAD cannot be preloaded (cannot open shared object file): ignored.
2023-04-28 10:52:05 ERROR: ld.so: object ‘tini’ from LD_PRELOAD cannot be preloaded (cannot open shared object file): ignored.
2023-04-28 10:52:05 ERROR: ld.so: object ‘(7)]’ from LD_PRELOAD cannot be preloaded (cannot open shared object file): ignored.
2023-04-28 10:52:05
2023-04-28 10:52:05 ERROR: ld.so: object ‘exec’ from LD_PRELOAD cannot be preloaded (cannot open shared object file): ignored.
2023-04-28 10:52:05 ERROR: ld.so: object ‘C’ from LD_PRELOAD cannot be preloaded (cannot open shared object file): ignored.
2023-04-28 10:52:05 ERROR: ld.so: object ‘/Program’ from LD_PRELOAD cannot be preloaded (cannot open shared object file): ignored.
2023-04-28 10:52:05 ERROR: ld.so: object ‘Files/Git/usr/bin/bash’ from LD_PRELOAD cannot be preloaded (cannot open shared object file): ignored.
2023-04-28 10:52:05 ERROR: ld.so: object ‘failed’ from LD_PRELOAD cannot be preloaded (cannot open shared object file): ignored.
2023-04-28 10:52:05 ERROR: ld.so: object ‘No’ from LD_PRELOAD cannot be preloaded (cannot open shared object file): ignored.
2023-04-28 10:52:05 ERROR: ld.so: object ‘such’ from LD_PRELOAD cannot be preloaded (cannot open shared object file): ignored.
2023-04-28 10:52:05 ERROR: ld.so: object ‘file’ from LD_PRELOAD cannot be preloaded (cannot open shared object file): ignored.
2023-04-28 10:52:05 ERROR: ld.so: object ‘or’ from LD_PRELOAD cannot be preloaded (cannot open shared object file): ignored.
2023-04-28 10:52:05 ERROR: ld.so: object ‘directory’ from LD_PRELOAD cannot be preloaded (cannot open shared object file): ignored.
2023-04-28 10:52:05 ERROR: ld.so: object ‘[FATAL’ from LD_PRELOAD cannot be preloaded (cannot open shared object file): ignored.
2023-04-28 10:52:05 ERROR: ld.so: object ‘[FATAL’ from LD_PRELOAD cannot be preloaded (cannot open shared object file): ignored.
2023-04-28 10:52:05 ERROR: ld.so: object ‘tini’ from LD_PRELOAD cannot be preloaded (cannot open shared object file): ignored.
2023-04-28 10:52:05 ERROR: ld.so: object ‘tini’ from LD_PRELOAD cannot be preloaded (cannot open shared object file): ignored.
2023-04-28 10:52:05 ERROR: ld.so: object ‘(7)]’ from LD_PRELOAD cannot be preloaded (cannot open shared object file): ignored.
2023-04-28 10:52:05 ERROR: ld.so: object ‘(7)]’ from LD_PRELOAD cannot be preloaded (cannot open shared object file): ignored.
2023-04-28 10:52:05 ERROR: ld.so: object ‘exec’ from LD_PRELOAD cannot be preloaded (cannot open shared object file): ignored.
2023-04-28 10:52:05 ERROR: ld.so: object ‘C’ from LD_PRELOAD cannot be preloaded (cannot open shared object file): ignored.
2023-04-28 10:52:05 ERROR: ld.so: object ‘exec’ from LD_PRELOAD cannot be preloaded (cannot open shared object file): ignored.
2023-04-28 10:52:05 ERROR: ld.so: object ‘/Program’ from LD_PRELOAD cannot be preloaded (cannot open shared object file): ignored.
2023-04-28 10:52:05 ERROR: ld.so: object ‘Files/Git/usr/bin/bash’ from LD_PRELOAD cannot be preloaded (cannot open shared object file): ignored.
2023-04-28 10:52:05 ERROR: ld.so: object ‘C’ from LD_PRELOAD cannot be preloaded (cannot open shared object file): ignored.
2023-04-28 10:52:05 ERROR: ld.so: object ‘failed’ from LD_PRELOAD cannot be preloaded (cannot open shared object file): ignored.
2023-04-28 10:52:05 ERROR: ld.so: object ‘/Program’ from LD_PRELOAD cannot be preloaded (cannot open shared object file): ignored.
2023-04-28 10:52:05 ERROR: ld.so: object ‘Files/Git/usr/bin/bash’ from LD_PRELOAD cannot be preloaded (cannot open shared object file): ignored.
2023-04-28 10:52:05 ERROR: ld.so: object ‘No’ from LD_PRELOAD cannot be preloaded (cannot open shared object file): ignored.
2023-04-28 10:52:05 ERROR: ld.so: object ‘failed’ from LD_PRELOAD cannot be preloaded (cannot open shared object file): ignored.
2023-04-28 10:52:05 ERROR: ld.so: object ‘No’ from LD_PRELOAD cannot be preloaded (cannot open shared object file): ignored.
2023-04-28 10:52:05 ERROR: ld.so: object ‘such’ from LD_PRELOAD cannot be preloaded (cannot open shared object file): ignored.
2023-04-28 10:52:05 ERROR: ld.so: object ‘such’ from LD_PRELOAD cannot be preloaded (cannot open shared object file): ignored.
2023-04-28 10:52:05 ERROR: ld.so: object ‘file’ from LD_PRELOAD cannot be preloaded (cannot open shared object file): ignored.
2023-04-28 10:52:05 ERROR: ld.so: object ‘file’ from LD_PRELOAD cannot be preloaded (cannot open shared object file): ignored.
2023-04-28 10:52:05 ERROR: ld.so: object ‘or’ from LD_PRELOAD cannot be preloaded (cannot open shared object file): ignored.
2023-04-28 10:52:05 ERROR: ld.so: object ‘or’ from LD_PRELOAD cannot be preloaded (cannot open shared object file): ignored.
2023-04-28 10:52:05 ERROR: ld.so: object ‘directory’ from LD_PRELOAD cannot be preloaded (cannot open shared object file): ignored.
2023-04-28 10:52:05 ERROR: ld.so: object ‘directory’ from LD_PRELOAD cannot be preloaded (cannot open shared object file): ignored.
2023-04-28 10:52:05 ERROR: ld.so: object ‘[FATAL’ from LD_PRELOAD cannot be preloaded (cannot open shared object file): ignored.
2023-04-28 10:52:05 ERROR: ld.so: object ‘tini’ from LD_PRELOAD cannot be preloaded (cannot open shared object file): ignored.
2023-04-28 10:52:05 ERROR: ld.so: object ‘(7)]’ from LD_PRELOAD cannot be preloaded (cannot open shared object file): ignored.
2023-04-28 10:52:05 ERROR: ld.so: object ‘exec’ from LD_PRELOAD cannot be preloaded (cannot open shared object file): ignored.
2023-04-28 10:52:05 ERROR: ld.so: object ‘C’ from LD_PRELOAD cannot be preloaded (cannot open shared object file): ignored.
2023-04-28 10:52:05 ERROR: ld.so: object ‘/Program’ from LD_PRELOAD cannot be preloaded (cannot open shared object file): ignored.
2023-04-28 10:52:05 ERROR: ld.so: object ‘Files/Git/usr/bin/bash’ from LD_PRELOAD cannot be preloaded (cannot open shared object file): ignored.
2023-04-28 10:52:05 ERROR: ld.so: object ‘failed’ from LD_PRELOAD cannot be preloaded (cannot open shared object file): ignored.
2023-04-28 10:52:05 ERROR: ld.so: object ‘No’ from LD_PRELOAD cannot be preloaded (cannot open shared object file): ignored.
2023-04-28 10:52:05 ERROR: ld.so: object ‘such’ from LD_PRELOAD cannot be preloaded (cannot open shared object file): ignored.
2023-04-28 10:52:05 ERROR: ld.so: object ‘file’ from LD_PRELOAD cannot be preloaded (cannot open shared object file): ignored.
2023-04-28 10:52:05 ERROR: ld.so: object ‘or’ from LD_PRELOAD cannot be preloaded (cannot open shared object file): ignored.
2023-04-28 10:52:05 ERROR: ld.so: object ‘directory’ from LD_PRELOAD cannot be preloaded (cannot open shared object file): ignored.
2023-04-28 10:52:05 ERROR: ld.so: object ‘[FATAL’ from LD_PRELOAD cannot be preloaded (cannot open shared object file): ignored.
2023-04-28 10:52:05 ERROR: ld.so: object ‘tini’ from LD_PRELOAD cannot be preloaded (cannot open shared object file): ignored.
2023-04-28 10:52:05 ERROR: ld.so: object ‘(7)]’ from LD_PRELOAD cannot be preloaded (cannot open shared object file): ignored.
2023-04-28 10:52:05 ERROR: ld.so: object ‘exec’ from LD_PRELOAD cannot be preloaded (cannot open shared object file): ignored.
2023-04-28 10:52:05 ERROR: ld.so: object ‘C’ from LD_PRELOAD cannot be preloaded (cannot open shared object file): ignored.
2023-04-28 10:52:05 ERROR: ld.so: object ‘/Program’ from LD_PRELOAD cannot be preloaded (cannot open shared object file): ignored.
2023-04-28 10:52:05 ERROR: ld.so: object ‘Files/Git/usr/bin/bash’ from LD_PRELOAD cannot be preloaded (cannot open shared object file): ignored.
2023-04-28 10:52:05 ERROR: ld.so: object ‘failed’ from LD_PRELOAD cannot be preloaded (cannot open shared object file): ignored.
2023-04-28 10:52:05 ERROR: ld.so: object ‘No’ from LD_PRELOAD cannot be preloaded (cannot open shared object file): ignored.
2023-04-28 10:52:05 ERROR: ld.so: object ‘such’ from LD_PRELOAD cannot be preloaded (cannot open shared object file): ignored.
2023-04-28 10:52:05 ERROR: ld.so: object ‘file’ from LD_PRELOAD cannot be preloaded (cannot open shared object file): ignored.
2023-04-28 10:52:05 ERROR: ld.so: object ‘or’ from LD_PRELOAD cannot be preloaded (cannot open shared object file): ignored.
2023-04-28 10:52:05 ERROR: ld.so: object ‘directory’ from LD_PRELOAD cannot be preloaded (cannot open shared object file): ignored.
2023-04-28 10:52:05 ERROR: ld.so: object ‘[FATAL’ from LD_PRELOAD cannot be preloaded (cannot open shared object file): ignored.
2023-04-28 10:52:05 ERROR: ld.so: object ‘tini’ from LD_PRELOAD cannot be preloaded (cannot open shared object file): ignored.
2023-04-28 10:52:05 ERROR: ld.so: object ‘(7)]’ from LD_PRELOAD cannot be preloaded (cannot open shared object file): ignored.
2023-04-28 10:52:05 ERROR: ld.so: object ‘exec’ from LD_PRELOAD cannot be preloaded (cannot open shared object file): ignored.
2023-04-28 10:52:05 ERROR: ld.so: object ‘C’ from LD_PRELOAD cannot be preloaded (cannot open shared object file): ignored.
2023-04-28 10:52:05 ERROR: ld.so: object ‘/Program’ from LD_PRELOAD cannot be preloaded (cannot open shared object file): ignored.
2023-04-28 10:52:05 ERROR: ld.so: object ‘Files/Git/usr/bin/bash’ from LD_PRELOAD cannot be preloaded (cannot open shared object file): ignored.
2023-04-28 10:52:05 ERROR: ld.so: object ‘failed’ from LD_PRELOAD cannot be preloaded (cannot open shared object file): ignored.
2023-04-28 10:52:05 ERROR: ld.so: object ‘No’ from LD_PRELOAD cannot be preloaded (cannot open shared object file): ignored.
2023-04-28 10:52:05 ERROR: ld.so: object ‘such’ from LD_PRELOAD cannot be preloaded (cannot open shared object file): ignored.
2023-04-28 10:52:05 ERROR: ld.so: object ‘file’ from LD_PRELOAD cannot be preloaded (cannot open shared object file): ignored.
2023-04-28 10:52:05 ERROR: ld.so: object ‘or’ from LD_PRELOAD cannot be preloaded (cannot open shared object file): ignored.
2023-04-28 10:52:05 ERROR: ld.so: object ‘directory’ from LD_PRELOAD cannot be preloaded (cannot open shared object file): ignored.
2023-04-28 10:52:05 ERROR: ld.so: object ‘[FATAL’ from LD_PRELOAD cannot be preloaded (cannot open shared object file): ignored.
2023-04-28 10:52:05 ERROR: ld.so: object ‘tini’ from LD_PRELOAD cannot be preloaded (cannot open shared object file): ignored.
2023-04-28 10:52:05 ERROR: ld.so: object ‘(7)]’ from LD_PRELOAD cannot be preloaded (cannot open shared object file): ignored.
2023-04-28 10:52:05 ERROR: ld.so: object ‘exec’ from LD_PRELOAD cannot be preloaded (cannot open shared object file): ignored.
2023-04-28 10:52:05 ERROR: ld.so: object ‘C’ from LD_PRELOAD cannot be preloaded (cannot open shared object file): ignored.
2023-04-28 10:52:05 ERROR: ld.so: object ‘/Program’ from LD_PRELOAD cannot be preloaded (cannot open shared object file): ignored.
2023-04-28 10:52:05 ERROR: ld.so: object ‘Files/Git/usr/bin/bash’ from LD_PRELOAD cannot be preloaded (cannot open shared object file): ignored.
2023-04-28 10:52:05 ERROR: ld.so: object ‘failed’ from LD_PRELOAD cannot be preloaded (cannot open shared object file): ignored.
2023-04-28 10:52:05 ERROR: ld.so: object ‘No’ from LD_PRELOAD cannot be preloaded (cannot open shared object file): ignored.
2023-04-28 10:52:05 ERROR: ld.so: object ‘such’ from LD_PRELOAD cannot be preloaded (cannot open shared object file): ignored.
2023-04-28 10:52:05 ERROR: ld.so: object ‘file’ from LD_PRELOAD cannot be preloaded (cannot open shared object file): ignored.
2023-04-28 10:52:05 ERROR: ld.so: object ‘or’ from LD_PRELOAD cannot be preloaded (cannot open shared object file): ignored.
2023-04-28 10:52:05 ERROR: ld.so: object ‘directory’ from LD_PRELOAD cannot be preloaded (cannot open shared object file): ignored.
2023-04-28 10:52:05 ERROR: ld.so: object ‘[FATAL’ from LD_PRELOAD cannot be preloaded (cannot open shared object file): ignored.
2023-04-28 10:52:05 ERROR: ld.so: object ‘tini’ from LD_PRELOAD cannot be preloaded (cannot open shared object file): ignored.
2023-04-28 10:52:05 ERROR: ld.so: object ‘(7)]’ from LD_PRELOAD cannot be preloaded (cannot open shared object file): ignored.
2023-04-28 10:52:05 ERROR: ld.so: object ‘exec’ from LD_PRELOAD cannot be preloaded (cannot open shared object file): ignored.
2023-04-28 10:52:05 ERROR: ld.so: object ‘C’ from LD_PRELOAD cannot be preloaded (cannot open shared object file): ignored.
2023-04-28 10:52:05 ERROR: ld.so: object ‘/Program’ from LD_PRELOAD cannot be preloaded (cannot open shared object file): ignored.
2023-04-28 10:52:05 ERROR: ld.so: object ‘Files/Git/usr/bin/bash’ from LD_PRELOAD cannot be preloaded (cannot open shared object file): ignored.
2023-04-28 10:52:05 ERROR: ld.so: object ‘failed’ from LD_PRELOAD cannot be preloaded (cannot open shared object file): ignored.
2023-04-28 10:52:05 ERROR: ld.so: object ‘No’ from LD_PRELOAD cannot be preloaded (cannot open shared object file): ignored.
2023-04-28 10:52:05 ERROR: ld.so: object ‘such’ from LD_PRELOAD cannot be preloaded (cannot open shared object file): ignored.
2023-04-28 10:52:05 ERROR: ld.so: object ‘file’ from LD_PRELOAD cannot be preloaded (cannot open shared object file): ignored.
2023-04-28 10:52:05 ERROR: ld.so: object ‘or’ from LD_PRELOAD cannot be preloaded (cannot open shared object file): ignored.
2023-04-28 10:52:05 ERROR: ld.so: object ‘directory’ from LD_PRELOAD cannot be preloaded (cannot open shared object file): ignored.
2023-04-28 10:52:05 ERROR: ld.so: object ‘[FATAL’ from LD_PRELOAD cannot be preloaded (cannot open shared object file): ignored.
2023-04-28 10:52:05 ERROR: ld.so: object ‘tini’ from LD_PRELOAD cannot be preloaded (cannot open shared object file): ignored.
2023-04-28 10:52:05 ERROR: ld.so: object ‘(7)]’ from LD_PRELOAD cannot be preloaded (cannot open shared object file): ignored.
2023-04-28 10:52:05 ERROR: ld.so: object ‘exec’ from LD_PRELOAD cannot be preloaded (cannot open shared object file): ignored.
2023-04-28 10:52:05 ERROR: ld.so: object ‘C’ from LD_PRELOAD cannot be preloaded (cannot open shared object file): ignored.
2023-04-28 10:52:05 ERROR: ld.so: object ‘/Program’ from LD_PRELOAD cannot be preloaded (cannot open shared object file): ignored.
2023-04-28 10:52:05 ERROR: ld.so: object ‘Files/Git/usr/bin/bash’ from LD_PRELOAD cannot be preloaded (cannot open shared object file): ignored.
2023-04-28 10:52:05 ERROR: ld.so: object ‘failed’ from LD_PRELOAD cannot be preloaded (cannot open shared object file): ignored.
2023-04-28 10:52:05 ERROR: ld.so: object ‘No’ from LD_PRELOAD cannot be preloaded (cannot open shared object file): ignored.
2023-04-28 10:52:05 ERROR: ld.so: object ‘such’ from LD_PRELOAD cannot be preloaded (cannot open shared object file): ignored.
2023-04-28 10:52:05 ERROR: ld.so: object ‘file’ from LD_PRELOAD cannot be preloaded (cannot open shared object file): ignored.
2023-04-28 10:52:05 ERROR: ld.so: object ‘or’ from LD_PRELOAD cannot be preloaded (cannot open shared object file): ignored.
2023-04-28 10:52:05 ERROR: ld.so: object ‘directory’ from LD_PRELOAD cannot be preloaded (cannot open shared object file): ignored.
2023-04-28 10:52:05 I0428 01:52:05.632777 90 metrics.cc:650] Collecting metrics for GPU 0: NVIDIA GeForce RTX 3060
2023-04-28 10:52:05 I0428 01:52:05.688788 90 tritonserver.cc:2123]
2023-04-28 10:52:05 ±---------------------------------±---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+
2023-04-28 10:52:05 | Option                           | Value                                                                                                                                                                                        |
2023-04-28 10:52:05   > Riva waiting for Triton server to load all models…retrying in 1 second
2023-04-28 10:52:06   > Riva waiting for Triton server to load all models…retrying in 1 second
2023-04-28 10:52:05 ±---------------------------------±---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+
2023-04-28 10:52:05 | server_id                        | triton                                                                                                                                                                                       |
2023-04-28 10:52:05 | server_version                   | 2.21.0                                                                                                                                                                                       |
2023-04-28 10:52:05 | server_extensions                | classification sequence model_repository model_repository(unload_dependents) schedule_policy model_configuration system_shared_memory cuda_shared_memory binary_tensor_data statistics trace |
2023-04-28 10:52:05 | model_control_mode               | MODE_NONE                                                                                                                                                                                    |
2023-04-28 10:52:05 | strict_model_config              | 1                                                                                                                                                                                            |
2023-04-28 10:52:05 | rate_limit                       | OFF                                                                                                                                                                                          |
2023-04-28 10:52:05 | pinned_memory_pool_byte_size     | 268435456                                                                                                                                                                                    |
2023-04-28 10:52:05 | cuda_memory_pool_byte_size{0}    | 1000000000                                                                                                                                                                                   |
2023-04-28 10:52:05 | response_cache_byte_size         | 0                                                                                                                                                                                            |
2023-04-28 10:52:05 | min_supported_compute_capability | 6.0                                                                                                                                                                                          |
2023-04-28 10:52:05 | strict_readiness                 | 1                                                                                                                                                                                            |
2023-04-28 10:52:05 | exit_timeout                     | 30                                                                                                                                                                                           |
2023-04-28 10:52:05 ±---------------------------------±---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+
2023-04-28 10:52:05
2023-04-28 10:52:05 I0428 01:52:05.688832 90 server.cc:247] No server context available. Exiting immediately.
2023-04-28 10:52:05 error: creating server: Invalid argument - --model-repository must be specified
2023-04-28 10:52:06 ERROR: ld.so: object ‘[FATAL’ from LD_PRELOAD cannot be preloaded (cannot open shared object file): ignored.
2023-04-28 10:52:06 ERROR: ld.so: object ‘tini’ from LD_PRELOAD cannot be preloaded (cannot open shared object file): ignored.
2023-04-28 10:52:06 ERROR: ld.so: object ‘(7)]’ from LD_PRELOAD cannot be preloaded (cannot open shared object file): ignored.
2023-04-28 10:52:06 ERROR: ld.so: object ‘exec’ from LD_PRELOAD cannot be preloaded (cannot open shared object file): ignored.
2023-04-28 10:52:06 ERROR: ld.so: object ‘C’ from LD_PRELOAD cannot be preloaded (cannot open shared object file): ignored.
2023-04-28 10:52:06 ERROR: ld.so: object ‘/Program’ from LD_PRELOAD cannot be preloaded (cannot open shared object file): ignored.
2023-04-28 10:52:06 ERROR: ld.so: object ‘Files/Git/usr/bin/bash’ from LD_PRELOAD cannot be preloaded (cannot open shared object file): ignored.
2023-04-28 10:52:06 ERROR: ld.so: object ‘failed’ from LD_PRELOAD cannot be preloaded (cannot open shared object file): ignored.
2023-04-28 10:52:06 ERROR: ld.so: object ‘No’ from LD_PRELOAD cannot be preloaded (cannot open shared object file): ignored.
2023-04-28 10:52:06 ERROR: ld.so: object ‘such’ from LD_PRELOAD cannot be preloaded (cannot open shared object file): ignored.
2023-04-28 10:52:06 ERROR: ld.so: object ‘file’ from LD_PRELOAD cannot be preloaded (cannot open shared object file): ignored.
2023-04-28 10:52:06 ERROR: ld.so: object ‘or’ from LD_PRELOAD cannot be preloaded (cannot open shared object file): ignored.
2023-04-28 10:52:06 ERROR: ld.so: object ‘directory’ from LD_PRELOAD cannot be preloaded (cannot open shared object file): ignored.
2023-04-28 10:52:06 ERROR: ld.so: object ‘[FATAL’ from LD_PRELOAD cannot be preloaded (cannot open shared object file): ignored.
2023-04-28 10:52:06 ERROR: ld.so: object ‘tini’ from LD_PRELOAD cannot be preloaded (cannot open shared object file): ignored.
2023-04-28 10:52:06 ERROR: ld.so: object ‘(7)]’ from LD_PRELOAD cannot be preloaded (cannot open shared object file): ignored.
2023-04-28 10:52:06 ERROR: ld.so: object ‘exec’ from LD_PRELOAD cannot be preloaded (cannot open shared object file): ignored.
2023-04-28 10:52:06 ERROR: ld.so: object ‘C’ from LD_PRELOAD cannot be preloaded (cannot open shared object file): ignored.
2023-04-28 10:52:06 ERROR: ld.so: object ‘/Program’ from LD_PRELOAD cannot be preloaded (cannot open shared object file): ignored.
2023-04-28 10:52:06 ERROR: ld.so: object ‘Files/Git/usr/bin/bash’ from LD_PRELOAD cannot be preloaded (cannot open shared object file): ignored.
2023-04-28 10:52:06 ERROR: ld.so: object ‘failed’ from LD_PRELOAD cannot be preloaded (cannot open shared object file): ignored.
2023-04-28 10:52:06 ERROR: ld.so: object ‘No’ from LD_PRELOAD cannot be preloaded (cannot open shared object file): ignored.
2023-04-28 10:52:06 ERROR: ld.so: object ‘such’ from LD_PRELOAD cannot be preloaded (cannot open shared object file): ignored.
2023-04-28 10:52:06 ERROR: ld.so: object ‘file’ from LD_PRELOAD cannot be preloaded (cannot open shared object file): ignored.
2023-04-28 10:52:06 ERROR: ld.so: object ‘or’ from LD_PRELOAD cannot be preloaded (cannot open shared object file): ignored.
2023-04-28 10:52:06 ERROR: ld.so: object ‘directory’ from LD_PRELOAD cannot be preloaded (cannot open shared object file): ignored.
2023-04-28 10:52:07 ERROR: ld.so: object ‘[FATAL’ from LD_PRELOAD cannot be preloaded (cannot open shared object file): ignored.
2023-04-28 10:52:07 ERROR: ld.so: object ‘tini’ from LD_PRELOAD cannot be preloaded (cannot open shared object file): ignored.
2023-04-28 10:52:07 ERROR: ld.so: object ‘(7)]’ from LD_PRELOAD cannot be preloaded (cannot open shared object file): ignored.
2023-04-28 10:52:07 ERROR: ld.so: object ‘exec’ from LD_PRELOAD cannot be preloaded (cannot open shared object file): ignored.
2023-04-28 10:52:07 ERROR: ld.so: object ‘C’ from LD_PRELOAD cannot be preloaded (cannot open shared object file): ignored.
2023-04-28 10:52:07 ERROR: ld.so: object ‘/Program’ from LD_PRELOAD cannot be preloaded (cannot open shared object file): ignored.
2023-04-28 10:52:07 ERROR: ld.so: object ‘Files/Git/usr/bin/bash’ from LD_PRELOAD cannot be preloaded (cannot open shared object file): ignored.
2023-04-28 10:52:07 ERROR: ld.so: object ‘failed’ from LD_PRELOAD cannot be preloaded (cannot open shared object file): ignored.
2023-04-28 10:52:07 ERROR: ld.so: object ‘No’ from LD_PRELOAD cannot be preloaded (cannot open shared object file): ignored.
2023-04-28 10:52:07 ERROR: ld.so: object ‘such’ from LD_PRELOAD cannot be preloaded (cannot open shared object file): ignored.
2023-04-28 10:52:07 ERROR: ld.so: object ‘file’ from LD_PRELOAD cannot be preloaded (cannot open shared object file): ignored.
2023-04-28 10:52:07 ERROR: ld.so: object ‘or’ from LD_PRELOAD cannot be preloaded (cannot open shared object file): ignored.
2023-04-28 10:52:07 ERROR: ld.so: object ‘directory’ from LD_PRELOAD cannot be preloaded (cannot open shared object file): ignored.
2023-04-28 10:52:07   > Riva waiting for Triton server to load all models…retrying in 1 second
2023-04-28 10:52:07 ERROR: ld.so: object ‘[FATAL’ from LD_PRELOAD cannot be preloaded (cannot open shared object file): ignored.
2023-04-28 10:52:07 ERROR: ld.so: object ‘tini’ from LD_PRELOAD cannot be preloaded (cannot open shared object file): ignored.
2023-04-28 10:52:07 ERROR: ld.so: object ‘(7)]’ from LD_PRELOAD cannot be preloaded (cannot open shared object file): ignored.
2023-04-28 10:52:07 ERROR: ld.so: object ‘exec’ from LD_PRELOAD cannot be preloaded (cannot open shared object file): ignored.
2023-04-28 10:52:07 ERROR: ld.so: object ‘C’ from LD_PRELOAD cannot be preloaded (cannot open shared object file): ignored.
2023-04-28 10:52:07 ERROR: ld.so: object ‘/Program’ from LD_PRELOAD cannot be preloaded (cannot open shared object file): ignored.
2023-04-28 10:52:07 ERROR: ld.so: object ‘Files/Git/usr/bin/bash’ from LD_PRELOAD cannot be preloaded (cannot open shared object file): ignored.
2023-04-28 10:52:07 ERROR: ld.so: object ‘failed’ from LD_PRELOAD cannot be preloaded (cannot open shared object file): ignored.
2023-04-28 10:52:07 ERROR: ld.so: object ‘No’ from LD_PRELOAD cannot be preloaded (cannot open shared object file): ignored.
2023-04-28 10:52:07 ERROR: ld.so: object ‘such’ from LD_PRELOAD cannot be preloaded (cannot open shared object file): ignored.
2023-04-28 10:52:07 ERROR: ld.so: object ‘file’ from LD_PRELOAD cannot be preloaded (cannot open shared object file): ignored.
2023-04-28 10:52:07 ERROR: ld.so: object ‘or’ from LD_PRELOAD cannot be preloaded (cannot open shared object file): ignored.
2023-04-28 10:52:07 ERROR: ld.so: object ‘directory’ from LD_PRELOAD cannot be preloaded (cannot open shared object file): ignored.
2023-04-28 10:52:08   > Triton server died before reaching ready state. Terminating Riva startup.
2023-04-28 10:52:08 Check Triton logs with: docker logs
2023-04-28 10:52:08 /opt/riva/bin/start-riva: line 1: kill: (90) - No such processHi @yeriel29Thanks for your interest in RivaI may be wrong but initial guess looks like some problem within Docker VolumeKindly request to run bash riva_clean.sh and try againAlso we kindly request to share the output of the below command
docker volume ls
before and after running bash riva_clean.shThanksHello,
Thank you for reply!
I tried bash riva_clean.sh and run init and start again but still have a same issue.
Here is the output from command:Before bash riva_clean.sh:$ docker volume ls
DRIVER    VOLUME NAME
local     riva-model-repoAfter bash riva_clean.sh:$ docker volume ls
DRIVER    VOLUME NAMEI am having the same problem on windows. Did you find the solution ?No, still struggling with same issue.
I’ve delete all data about Riva and re-bash everything, but still same. Hope I can find a reason to solve it.I have tried following this on ubuntu and windows and ubuntu app on windows. Everywhere there is a separate issue preventing from running the server. I think Ubuntu app on windows was where it progressed the most then got stuck.Follow this process:Riva Python client API and CLI utils. Contribute to nvidia-riva/python-clients development by creating an account on GitHub.
Then this
https://ngc.nvidia.com/setup/installers/cli
Then initialize and start riva via shell … see maybe it helps. It hasn’t for me. Sometimes it says GPC key is invalid, on ubuntu OS, sometimes, it cant store credentials. On windows i get the error you posted about.Hey, i got it working atleast from that error. Do the following:distribution=$(. /etc/os-release;echo $ID$VERSION_ID)
curl -s -L https://nvidia.github.io/nvidia-docker/gpgkey | sudo apt-key add -
curl -s -L https://nvidia.github.io/nvidia-docker/$distribution/nvidia-docker.list | sudo tee /etc/apt/sources.list.d/nvidia-docker.listsudo apt-get update && sudo apt-get install -y nvidia-container-toolkit
sudo systemctl restart dockerthen bash riva_init.shIt takes so much time, if you are running it on your PC … since the version is for datacenter with cluster of GPUs. I was running this on laptop and gave upHi @bhatt226Apologies for the delay, Thanks for your kind inputs and feedback, Really appreciate your proactive feedbackYes, the nvidia-container-toolkit is required for docker to leverage GPU@yeriel29 , Apologies for the delayCan you try setting up nvidia-container-toolkit and tryhttps://docs.nvidia.com/datacenter/cloud-native/container-toolkit/archive/1.12.1/install-guide.html#setting-up-nvidia-container-toolkitThankshello im facing the same problem on windows 10
and i couldn’t find a way to install nvidia container toolkit on windows 10Powered by Discourse, best viewed with JavaScript enabled"
341,tensorrt-api-8-6-torch-multinomial,"Hi,Is there something that plays the role of torch.multinomial in tensorRT?I couldn’t find it on the site below.
https://docs.nvidia.com/deeplearning/tensorrt/api/c_api/index.htmlThanks!A clear and concise description of the bug or issue.TensorRT Version:  8.6
GPU Type:  rtx A6000
Nvidia Driver Version: 528.49
CUDA Version:  12.0
CUDNN Version:  X
Operating System + Version:
Python Version (if applicable):  3.9
TensorFlow Version (if applicable):
PyTorch Version (if applicable):
Baremetal or Container (if container which image + tag):Please attach or include links to any models, data, files, or scripts necessary to reproduce your issue. (Github repo, Google Drive, Dropbox, etc.)Please include:Hi,The below links might be useful for you.This Best Practices Guide covers various performance considerations related to deploying networks using TensorRT 8.0.3. These sections assume that you have a model that is working at an appropriate level of accuracy and that you are able to...The programming guide to the CUDA model and interface.https://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__STREAM.htmlFor multi-threading/streaming, will suggest you to use Deepstream or TRITONFor more details, we recommend you raise the query in Deepstream forum.orraise the query in Triton Inference Server Github instance issues section.Triton Inference Server has 30 repositories available. Follow their code on GitHub.Thanks!Powered by Discourse, best viewed with JavaScript enabled"
342,error-model-is-not-available-on-server,"Please provide the following information when requesting support.Device: Xavier NX
6 Core CPU
Operating System Ubuntu 18.04
Riva Version 2.0.0
TLT Version (if relevant)
How to reproduce the issue ? I setup Riva 2.0 and every thing run well. When i send request to riva server for Transcribe give me Error: Model is not available on server.
How can i solve this issue?Hi @Mr.razzaghiThanks for your interest in RivaApologies you are facing error,It would be kind if you share the config.sh used in case you followed riva quickstart or the complete riva-build command in case you used a custom modelPlease do share the output of docker logs riva-speechI would also appreciate to knowconfig.sh (10.3 KB)
Uploading: riva-speech.log…
I just got this error without more details: Model is not available on server
For now I want to test all available models to evaluate them accuracy.riva-speech.log (57.8 KB)
log attached in this postFacing the same issue, @rvinobha any solution?Powered by Discourse, best viewed with JavaScript enabled"
343,trying-to-build-an-older-version-of-dali,"I’m having problems building an older version of DALI (v1.0.0) from GitHub - NVIDIA/DALI: A GPU-accelerated library containing highly optimized building blocks and an execution engine for data processing to accelerate deep learning training and inference applications.. I’ve created a branch of the v1.0.0 tag.  I get the following error when I use the command:The file https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64/7fa2af80.pub does exist. So I’m not sure if I’m missing something.  Note that I’m able to build the main branch.Thanks
TonyI hacked the docker/Dockerfile.cuda112.x86_64.deps by adding
apt-key adv --keyserver keyserver.ubuntu.com --recv-keys A4B469963BF863CC  and got around this issue.This topic was automatically closed 14 days after the last reply. New replies are no longer allowed.Powered by Discourse, best viewed with JavaScript enabled"
344,can-the-trt-model-on-the-server-be-directly-transplanted-to-jetson-nx-for-use,"Please provide complete information as applicable to your setup.
• Hardware Platform (Jetson / GPU) Server:[GeForce RTX 3070 Ti]  and NX:Jetpack4.6
• DeepStream Version version 6.0.0
• JetPack Version (valid for Jetson only) Jetpack4.6
• TensorRT Version TensorRT-8.0.1.6
**• NVIDIA GPU Driver Version (valid for GPU only)**GeForce RTX 3070 Ti: CUDA Version: 11.5/ Jetpack4.6:CUDA Version:10.2.300
• Issue Type( questions, new requirements, bugs)
• How to reproduce the issue ? (This is for bugs. Including which sample app is using, the configuration files content, the command line used and other details for reproducing)
• Requirement details( This is for new requirement. Including the module name-for which plugin or for which sample application, the function description)Can the trt model on the server be directly transplanted to Jetson NX for use?
d41b341d55b233178a7369070e61f381662×337 102 KB
the server is faster than the NX to engine model, so I want to copy the model from the server to use in NXCan the trt model on the server be directly transplanted to Jetson NX for use?no,   tensorrt version, device are different.Thank you for your attention, tensorrt version I have installed the server and equipment as the same （TensorRT-Version:8.0.1.6)Let’s move it to TensorRT forum to get better support, thank you.Powered by Discourse, best viewed with JavaScript enabled"
345,how-to-measure-precision-recognition-accuracy-in-asr-models,"Please provide the following information when requesting support.I am using riva 2.6 with the ASR model in Spanish and I would like to know if there is information on the accuracy of the model for speech recognition or any parameter/measure that can be used to measure the effectiveness/precision/accuracy of the model when recognizing speech to text.Hi @nharoThanks for your interest in RivaI will check with the team regarding this and let you knowThanksPowered by Discourse, best viewed with JavaScript enabled"
346,how-many-request-a-t4-gpu-can-handle-at-a-time,"Hi,I have deployed Riva ASR with a single T4 GPU with 4 CPU and 16 CPU RAM.
How many requests this deployment can handThanksHI @mehadi.hasanThanks for your interest in RivaPlease find the requested details belowhttps://docs.nvidia.com/deeplearning/riva/user-guide/docs/asr/asr-performance.html#resultsPlease choose T4 GPU in the below tableThanksPowered by Discourse, best viewed with JavaScript enabled"
347,does-gtx-1050ti-or-1650-for-notebook-support-tensorflow-gpu,"I am planning to buy a laptop with Nvidia GeForce GTX 1050Ti or 1650 GPU for Deep Learning with tensorflow-gpu but in the supported list of CUDA enabled devices both of them are not listed.
Some people in the NVIDIA community say that these cards support CUDA can you please tell me if these card for laptop support tensorflow-gpu or not.
Link to the official list of CUDA enabled devices:Explore your GPU compute capability and CUDA-enabled products.Both the GTX 1050ti and GTX 1650 support CUDA, and either is new enough to be supported by TensorFlow. The 1050ti has compute capability (CC) 6.1 and the 1650 has CC 7.5. Tensorflow currently requires CC 3.5. If you are planning to run training (rather than just inference), you will want to make sure the frame buffer is large enough to support your models of interest.Thanks for the reply and clearence. Actually I asked this question from a customer support executive of NVIDIA and he said that these card doesn’t support tensorflow-gpu. So according to you I can install Tensorflow-gpu on a laptop with GTX 1050Ti or 1650 .
Please clearyfy further and if possible then please send me the links to check the compute capability [CC] of NVIDIA GPU cards
Thanks.The 1050 Ti and 1650 have limited memory capacities (~4GB I believe) and as such will only be appropriate for some DL workloads. As such we do not recommend these GPUs for Deep Learning applications in general. Also, laptops are not generally designed to run intensive training workloads 24/7 for weeks on end.That said, if your training task is reasonably small, these GPUs will certainly run TensorFlow.
Unfortunately, CUDA GPUs - Compute Capability | NVIDIA Developer needs to be updated. In the mean time, a list of compute capabilities is available at CUDA - WikipediaSo, will gtx 1660ti or rtx 2060 suffice for larger workloads?The 1660ti and 2060 with 6GB of memory will certainly be more flexible in addressing DL workloads than the 4GB 1050ti/1650. As points of reference, the professional-grade, server-class accelerators generally pack 16-32GB of memory while high-end desktop parts, like the 2080 or 1080Ti provide 11-12GB. Memory requirements are highly model-dependent. You will want to look at typical model sizes in your area of interest (or look at what hardware platforms reference models of interest to you have been train on).HelloI am trying to run some simple DL model in a Geforce GTX 1650Is there a tutorial to achive this ?Thanks in advanceHi @hardolfo7,You shouldn’t need to change your TF python scripts to start making use of GPUs. Since you are running on a laptop, I assume you’re GPU may also be used for rendering. In that case you may want to enable allow_growth option to keep TF from claiming too much of your GPU’s memory by default. See Use a GPU  |  TensorFlow CoreThe tensorflow pip packages for TF 2.1+ and 1.15 come with GPU support built in. If, however, you are running TF 2.0 or an older 1.x releaes you will want to install the tensorflow-gpu package instead.In order for TF to make use of your GPU you will also need to install the CUDA toolkit and CUDNN library. The versions you need depend on your TF version. Here are version lists for Linux and Windows packages.If running Docker containers is an option, you can simplify the installation process by using a TensorFlow image from NVIDIA’s GPU Cloud registry. These provide TF prepackaged with the latest cudnn and toolkit.I made an env with the instructions of this videoWorks perfectly, ThanksI have a laptop with Nvidia GeForce GTX 1050Ti, and I couldn’t work with my gpu in tensorflow, So after several tries, I achieve it. How? Well, first you need to create a new environment, with a python version equal to 3.6, next, you need to install tensorflow-gpu 1.19 version. and I recommend you that you follow the instructions contained inSetting up TensorFlow (GPU) on Windows 10 | by Peter Jang | Towards Data Sciencebut with the versions that I mentioned at first.
I atacched an image where we can see the correct behavior with this NVIDIA graphic target
Captura de pantalla 2021-03-17 08.27.211920×1080 223 KB
Hello,I am attempting to utilize my NVIDIA GTX 1050 Ti in TensorFlow, however TensorFlow does not appear to be recognizing my GPU despite installing the appropriate version of CUDA and cuDNN. Is there a solution to this issue?Hi @ibrahimsabri.belimi1997, can you provide details of your environment (e.g., Windows or Linux version, NVIDIA driver version installed, what toolkit/cudnn version you are using) where you are installing TensorFlow from (NGC or dockerhub container, pip package, building from source?), along with the output of running nvidia-smi on your system.I would recommend using the NGC Docker containers if you are not already, as these come with all library dependencies pre-installed.Powered by Discourse, best viewed with JavaScript enabled"
348,error-2-graphoptimizer-cpp-4222-error-code-2-internal-error-assertion-residual-extent-output-extent-failed,"the given model can be converted to tensorrt engine using tensorrt before8.4.1,
but the conversion failed in tensorrt8.5.2TensorRT Version: 8.5.2.1
GPU Type: Orin
Nvidia Driver Version:
CUDA Version: 11.4
CUDNN Version: 8.4.1
Operating System + Version: ubuntu 20.04.4
Python Version (if applicable):
TensorFlow Version (if applicable):
PyTorch Version (if applicable):
Baremetal or Container (if container which image + tag):./trtexec --onnx=test.onnx --saveEngine=test.onnx.engine --minShapes=feature:1x512x4,xyz0:1x512x3,xyz1:1x128x3,xyz2:1x32x3,xyz3:1x8x3,neigh_idx0:1x512x1,neigh_idx1:1x128x8,neigh_idx2:1x32x8,neigh_idx3:1x8x8,sub_idx0:1x128x1,sub_idx1:1x32x8,sub_idx2:1x8x8,sub_idx3:1x2x8,interp_idx0:1x512x1,interp_idx1:1x128x1,interp_idx2:1x32x1,interp_idx3:1x8x1 --optShapes=feature:1x51200x4,xyz0:1x51200x3,xyz1:1x12800x3,xyz2:1x3200x3,xyz3:1x800x3,neigh_idx0:1x51200x1,neigh_idx1:1x12800x8,neigh_idx2:1x3200x8,neigh_idx3:1x800x8,sub_idx0:1x12800x1,sub_idx1:1x3200x8,sub_idx2:1x800x8,sub_idx3:1x200x8,interp_idx0:1x51200x1,interp_idx1:1x12800x1,interp_idx2:1x3200x1,interp_idx3:1x800x1 --maxShapes=feature:1x153600x4,xyz0:1x153600x3,xyz1:1x38400x3,xyz2:1x9600x3,xyz3:1x2400x3,neigh_idx0:1x153600x1,neigh_idx1:1x38400x8,neigh_idx2:1x9600x8,neigh_idx3:1x2400x8,sub_idx0:1x38400x1,sub_idx1:1x9600x8,sub_idx2:1x2400x8,sub_idx3:1x600x8,interp_idx0:1x153600x1,interp_idx1:1x38400x1,interp_idx2:1x9600x1,interp_idx3:1x2400x1 --verbose
test.onnx (363.6 KB)[07/07/2023-11:55:37] [V] [TRT] After Myelin optimization: 113 layers
[07/07/2023-11:55:37] [V] [TRT] Applying ScaleNodes fusions.
[07/07/2023-11:55:37] [V] [TRT] After scale fusion: 113 layers
[07/07/2023-11:55:37] [V] [TRT] Running: SqueezePushDownFork on (Unnamed Layer* 11) [Shuffle]
[07/07/2023-11:55:37] [V] [TRT] -----------SqueezePushDown kSQUEEZE_FORK case: Conv_1 → (Unnamed Layer* 11) [Shuffle] → LeakyRelu_2
[07/07/2023-11:55:37] [V] [TRT] Running: ShuffleShuffleFusion on squeeze_after_LeakyRelu_2
[07/07/2023-11:55:37] [V] [TRT] ShuffleShuffleFusion: Fusing squeeze_after_LeakyRelu_2 with Unsqueeze_3
[07/07/2023-11:55:37] [V] [TRT] Running: ShuffleErasure on squeeze_after_LeakyRelu_2 + Unsqueeze_3
[07/07/2023-11:55:37] [V] [TRT] Removing squeeze_after_LeakyRelu_2 + Unsqueeze_3
[07/07/2023-11:55:37] [V] [TRT] Running: ReduceToPoolingFusion on ReduceMax_83
[07/07/2023-11:55:37] [V] [TRT] Swap the layer type of ReduceMax_83 from REDUCE to POOLING
[07/07/2023-11:55:37] [E] Error[2]: [graphOptimizer.cpp::matchDetails::4222] Error Code 2: Internal Error (Assertion residual->extent == output->extent failed. )
[07/07/2023-11:55:37] [E] Error[2]: [builder.cpp::buildSerializedNetwork::751] Error Code 2: Internal Error (Assertion engine != nullptr failed. )
[07/07/2023-11:55:37] [E] Engine could not be created from network
[07/07/2023-11:55:37] [E] Building engine failed
[07/07/2023-11:55:37] [E] Failed to create engine from model or file.
[07/07/2023-11:55:37] [E] Engine set up failedHi,Based on the above error, it looks like the model has a residual tensor and an output tensor that have incompatible shapes. On running the Polygraphy tool constant folding, we could successfully build the engine.Thank you.Tests ok.
Thanks a lot.This topic was automatically closed 14 days after the last reply. New replies are no longer allowed.Powered by Discourse, best viewed with JavaScript enabled"
349,riva-start-sh-fails-on-jetson-orin,"I bought a Jetson AGX Orin Development Kit recently and wanted to run some speech AI on it.I followed the official Quick Start Guide [cannot put the link due to being a new user].All steps went OK, until the “bas riva_start.sh” step. I got the following output on screen after issuing the command.~/riva_quickstart_arm64_v2.9.0$ bash riva_start.sh
Starting Riva Speech Services. This may take several minutes depending on the number of models deployed.
Waiting for Riva server to load all models…retrying in 10 seconds
Waiting for Riva server to load all models…retrying in 10 seconds
Waiting for Riva server to load all models…retrying in 10 seconds
Waiting for Riva server to load all models…retrying in 10 seconds
Waiting for Riva server to load all models…retrying in 10 seconds
Waiting for Riva server to load all models…retrying in 10 seconds
Waiting for Riva server to load all models…retrying in 10 seconds
Waiting for Riva server to load all models…retrying in 10 seconds
Waiting for Riva server to load all models…retrying in 10 seconds
Waiting for Riva server to load all models…retrying in 10 seconds
Waiting for Riva server to load all models…retrying in 10 seconds
Waiting for Riva server to load all models…retrying in 10 seconds
Waiting for Riva server to load all models…retrying in 10 seconds
Waiting for Riva server to load all models…retrying in 10 seconds
Waiting for Riva server to load all models…retrying in 10 seconds
Waiting for Riva server to load all models…retrying in 10 seconds
Waiting for Riva server to load all models…retrying in 10 seconds
Waiting for Riva server to load all models…retrying in 10 seconds
Waiting for Riva server to load all models…retrying in 10 seconds
Waiting for Riva server to load all models…retrying in 10 seconds
Waiting for Riva server to load all models…retrying in 10 seconds
Waiting for Riva server to load all models…retrying in 10 seconds
Waiting for Riva server to load all models…retrying in 10 seconds
Waiting for Riva server to load all models…retrying in 10 seconds
Waiting for Riva server to load all models…retrying in 10 seconds
Waiting for Riva server to load all models…retrying in 10 seconds
Waiting for Riva server to load all models…retrying in 10 seconds
Waiting for Riva server to load all models…retrying in 10 seconds
Waiting for Riva server to load all models…retrying in 10 seconds
Waiting for Riva server to load all models…retrying in 10 seconds
Health ready check failed.
Check Riva logs with: docker logs riva-speechThe output of the “docker logs riva-speech” command is attached here (logs.txt).
logs.txt (82.2 KB)Hope to get help here. Many thanks!Hi @yanchaoliuThanks for your interest in RivaI will check regarding this error with the Riva Team, in meantime one quick suggestionCan you try the setup again with a different port (i.e default port 50051), from logs i find you are using 8001 port,Kindly also share the config.sh usedThanksHI @yanchaoliuI have updates from Riva TeamThe Team analyzed the logs, The errors are due to unrecognized symbols from the TRT library, which happens due to TRT version mismatch b/w container and host, e.g.unable to load shared library: /usr/lib/aarch64-linux-gnu/libnvinfer.so.8: undefined symbol: _ZN5nvdla8IProfile17setUseSoftMaxOptzEbThey told me to verifywe strongly beleive it may be jetson relatedFor Riva 2.9.0 (which is your version I believe), Jetpack 5.0.2 should be used.If you are using latest Jetpack 5.1, Please switch to Riva 2.10.0 release.Please check and provide us the feedbackThanksI’m using nvidia agx orin
bash ./riva_init.sh
Logging into NGC docker registry if necessary…
Pulling required docker images if necessary…
Note: This may take some time, depending on the speed of your Internet connection.Pulling Riva Speech Server images.
Pulling nvcr.io/nvidia/riva/riva-speech:2.11.0. This may take some time…Downloading models (RMIRs) from NGC…
Note: this may take some time, depending on the speed of your Internet connection.
To skip this process and use existing RMIRs set the location and corresponding flag in config.sh.
docker: Error response from daemon: failed to create shim task: OCI runtime create failed: runc create failed: unable to start container process: error during container init: error running hook #0: error running hook: exit status 1, stdout: , stderr: Auto-detected mode as ‘csv’
invoking the NVIDIA Container Runtime Hook directly (e.g. specifying the docker --gpus flag) is not supported. Please use the NVIDIA Container Runtime instead.: unknown.
Error in downloading models.bash ./riva_start.shStarting Riva Speech Services. This may take several minutes depending on the number of models deployed.
Waiting for Riva server to load all models…retrying in 10 seconds
Waiting for Riva server to load all models…retrying in 10 seconds
Waiting for Riva server to load all models…retrying in 10 seconds
Waiting for Riva server to load all models…retrying in 10 seconds
Waiting for Riva server to load all models…retrying in 10 seconds
Waiting for Riva server to load all models…retrying in 10 seconds
Waiting for Riva server to load all models…retrying in 10 seconds
Waiting for Riva server to load all models…retrying in 10 seconds
Waiting for Riva server to load all models…retrying in 10 seconds
Waiting for Riva server to load all models…retrying in 10 seconds
Waiting for Riva server to load all models…retrying in 10 seconds
Waiting for Riva server to load all models…retrying in 10 seconds
Waiting for Riva server to load all models…retrying in 10 seconds
Waiting for Riva server to load all models…retrying in 10 seconds
Waiting for Riva server to load all models…retrying in 10 seconds
Waiting for Riva server to load all models…retrying in 10 seconds
Waiting for Riva server to load all models…retrying in 10 seconds
Waiting for Riva server to load all models…retrying in 10 seconds
Waiting for Riva server to load all models…retrying in 10 seconds
Waiting for Riva server to load all models…retrying in 10 seconds
Waiting for Riva server to load all models…retrying in 10 seconds
Waiting for Riva server to load all models…retrying in 10 seconds
Waiting for Riva server to load all models…retrying in 10 seconds
Waiting for Riva server to load all models…retrying in 10 seconds
Waiting for Riva server to load all models…retrying in 10 seconds
Waiting for Riva server to load all models…retrying in 10 seconds
Waiting for Riva server to load all models…retrying in 10 seconds
Waiting for Riva server to load all models…retrying in 10 seconds
Waiting for Riva server to load all models…retrying in 10 seconds
Waiting for Riva server to load all models…retrying in 10 seconds
Health ready check failed.
Check Riva logs with: docker logs riva-speechconfig.sh fileriva_target_gpu_family=“non-tegra”riva_tegra_platform=“orin”service_enabled_asr=true
service_enabled_nlp=true
service_enabled_tts=true
service_enabled_nmt=truelanguage_code=(“en-US”)asr_acoustic_model=(“conformer”)
gpus_to_use=“device=0”
MODEL_DEPLOY_KEY=“tlt_encode”riva_model_loc=“riva-model-repo”if [[ $riva_target_gpu_family == “tegra” ]]; then
riva_model_loc=“pwd/model_repository”
fiuse_existing_rmirs=falseriva_speech_api_port=“50051”riva_ngc_org=“nvidia”
riva_ngc_team=“riva”
riva_ngc_image_version=“2.11.0”
riva_ngc_model_version=“2.11.0”help me sort out my problem
Also it’s taking a lot more time to pull models while executing riva_init.sh fileHi @ritthyk5050Thanks for your interest in RivaApologies for the delayCan you kindly share the Jetpack version used
Riva 2.11 required  JetPack 5.1 or JetPack 5.1.1kindly set the default runtime to nvidia on the Jetson platform if no already doneYou have set the default runtime to nvidia on the Jetson platform by adding the following line in the /etc/docker/daemon.json file. Restart the Docker service using sudo systemctl restart docker after editing the file.""default-runtime"": ""nvidia""Please check the above and let us know if it helpsThanksJetson Model: AGX Orin
Jetpack Version: 5.1 (L4T 35.2.1)
Riva Version: 2.11.0I am having the same problem as OP. Nearly identical config. I have also done the ""default-runtime"": ""nvidia"" sugeestion. Running bash riva_start.sh produces same result. Runs until finally quitting and saying:I have included my config.sh and the output of docker logs riva-speech in the log.txt file.I am only trying to run the nlp models to test a simple sample script but can not get the Riva Service to work. Is Riva 2.10.0 more stable?config.sh (13.3 KB)
log.txt (13.0 KB)Hi @hpetty_c2iThanks for your interest in RivaCan you run the following command and share with us the outputAlso can you check if you are able to run any basic container with nvidia-runtime
2. docker run -it --rm --runtime nvidia ubuntu:20.04Thanksdocker info | grep -i runtimeAnd yes I am able to run the containerPowered by Discourse, best viewed with JavaScript enabled"
350,how-to-add-batchednmsdynamic-trt-plugin-to-a-onnx-model,"I want to add BatchedNMSDynamic_TRT plugin to my SSD ONNX model in the end. I want output model to be also in ONNX form only. (or any other format but not TRT model.) My current model has two output (boxes and sources), one related to coordinate of bbox and another is corresponding confidence scores. Please help me on this. Python code is preferred.• Hardware Platform (GPU)
• DeepStream Version 6.2
• TensorRT Version 8.5.2
• NVIDIA GPU Driver Version 525.85.12
• Issue Type (questions)Hi,
Please refer to below links related custom plugin implementation and sample:This Samples Support Guide provides an overview of all the supported NVIDIA TensorRT 8.6.0 Early Access (EA) samples included on GitHub and in the product package. The TensorRT samples specifically help in areas such as recommenders, machine...TensorRT is an SDK for high performance, deep learning inference. It includes a deep learning inference optimizer and a runtime that delivers low latency and high throughput for deep learning…While IPluginV2 and IPluginV2Ext interfaces are still supported for backward compatibility with TensorRT 5.1 and 6.0.x respectively, however, we recommend that you write new plugins or refactor existing ones to target the IPluginV2DynamicExt or IPluginV2IOExt interfaces instead.Thanks!Powered by Discourse, best viewed with JavaScript enabled"
351,nvidia-nemo,"I’m planning to use Nvidia Nemo framework for speech diarization, speech recognition and other NLP operations and right now I’m doubtful about the data privacy coz the data will have confidential information and i don’t want it to be leaked in any way. can you walk me through the entire process of encrytion and data’s lifecycle. Also will this be stored in your cloud.Powered by Discourse, best viewed with JavaScript enabled"
352,jax-container-early-access-approval,"Hi team NVIDIA,I wonder how long does it normally take for an approval to go through? Furthermore, I am curious, are there any specific criteria you check for? I’ve used NVIDIA TensorFlow containers for my experiments before, and they’ve proven really useful. Now I have one more upcoming using JAX, and it would be a game changer, if I don’t have to build one myself fully from scratch.All the best,
ArtemAccess granted! We’d love to hear about your use case and what JAX libraries you’re using.Powered by Discourse, best viewed with JavaScript enabled"
353,agx-orin-setup-riva-docker,"I’m new to Nvidia devices. I have a Jetson AGX Orin dev kit. Installing Ubuntu was easy enough, but I’m having problems setting up the dev environment.I installed JetPack:
sudo apt update
sudo apt dist-upgrade
sudo reboot
sudo apt install nvidia-jetpackThe SDK I need to run is Riva. I’ve downloaded the Riva files. When I run riva_init.sh, I get errors stating docker is missing.I’ve tried installing docker using the deployment guide, but when I run the command:
sudo apt-get install -y docker-ce docker-ce-cli containerd.io
I get the following errors:Package docker-ce is not available, but is referred to by another package.
This may mean that the package is missing, has been obsoleted, or
is only available from another source
However the following packages replace it:
docker-ce-cli:amd64
E: Package ‘docker-ce’ has no installation candidate
E: Unable to locate package docker-ce-cli
E: Unable to locate package containerd.io
E: Couldn’t find any package by glob ‘containerd.io’
E: Couldn’t find any package by regex ‘containerd.io’Any help on the proper steps to get Riva and Docker properly setup and running would be greatly appreciated.I’m not familiarwith Riva SDK, will forward this issue to internal team to have suggestions.This topic was automatically closed 14 days after the last reply. New replies are no longer allowed.Hi @lshoellApologies for the delayWill check with the internal team regarding your docker installation failure and updateThanksHi @lshoellApologies on the delayI have updates from the internal team>> I have a Jetson AGX Orin dev kit. Installing Ubuntu was easy enoughFrom this comment, it seems that you are installing a generic Ubuntu release image. But for Jetson, we need to use the L4T BSP image which is provided with Jetpack.We request to follow the instructions here to setup Orin Jetson AGX Orin Developer Kit User Guide - Two Ways to Set Up Software | NVIDIA Developer - “Optional Flow with Host Linux PC” section mentions the steps to install L4T BSP image that needs to be used. The user should install Jetpack 5.1 L4T image and SDK components. After installing the Jetpack successfully, the user needs to follow Riva quick start guide Quick Start Guide — NVIDIA Riva instructions.Please try the above and let us know if it helpsThanksPowered by Discourse, best viewed with JavaScript enabled"
354,using-tensorrt-batched-nms-plugin-with-scrfd-model-insightface,"How to use  BatchedNMS_TRT plugin in a SCRFD model (scrfd_500m)  ?
Screenshot 2023-06-13 134439678×521 28.5 KB
TensorRT Version: 8.6
GPU Type:   NVidia A100
Nvidia Driver Version: 530
CUDA Version:  12.1
CUDNN Version:
Operating System + Version: Ubuntu 18.04.5 LTS
Python Version (if applicable):  Python 3.7.10
TensorFlow Version (if applicable):
PyTorch Version (if applicable):  1.9.0master/detection/scrfd/toolsState-of-the-art 2D and 3D Face Analysis Project. Contribute to deepinsight/insightface development by creating an account on GitHub.release/8.6/plugin/batchedNMSPluginNVIDIA® TensorRT™, an SDK for high-performance deep learning inference, includes a deep learning inference optimizer and runtime that delivers low latency and high throughput for inference applicat...scrfd_500m.onnx (2.2 MB)Hi,
Please refer to below links related custom plugin implementation and sample:This Samples Support Guide provides an overview of all the supported NVIDIA TensorRT 8.6.1 samples included on GitHub and in the product package. The TensorRT samples specifically help in areas such as recommenders, machine comprehension, character...TensorRT is an SDK for high performance, deep learning inference. It includes a deep learning inference optimizer and a runtime that delivers low latency and high throughput for deep learning…While IPluginV2 and IPluginV2Ext interfaces are still supported for backward compatibility with TensorRT 5.1 and 6.0.x respectively, however, we recommend that you write new plugins or refactor existing ones to target the IPluginV2DynamicExt or IPluginV2IOExt interfaces instead.Thanks!Powered by Discourse, best viewed with JavaScript enabled"
355,cudaarray-surface-nvcvimage-cudaarray-surface,"I’m writing a plugin for a host application where I’m provided a cudaArray for incoming image data and another cudaArray (for which I have to make a surface) for outgoing image data.What is the best way to get data to/from a NvVFX effect? I initially thought I could use NvCVImage_Init() to point to the incoming data (the cudaArray directly, trying to avoid a copy). It doesn’t seem to be able to read the data in cudaArray (as one would require), and I can’t use a surface since the type is wrong.Now I’m copying from a cuda surface to a previously allocated NvCVImage running an effect. Then I copy the outgoing NvCVImage (previously allocated) back to another surface.A couple of things:I can’t seem to use any built-in cudaMemcpy functions, so I have to use a custom kernel for both copies. Is there something I’m missing?Does the first copy really need to happen, or the last? Since all data is accessed and modified via CUDA, it seems I should be able to at least read the source data directly in the effect.Are there any suggestions on how to properly read/write NvCVImage from/to cuda surfaces/cudaArrays?Real-time performance is a big factor, so it would be ideal to avoid as much copying and overhead as possibleThanks,
KeithPowered by Discourse, best viewed with JavaScript enabled"
356,unable-to-convert-t5-xl-model-to-tensorrt,"We tried converting T5 xl model to TensorRT as per instructions at https://developer.nvidia.com/blog/optimizing-t5-and-gpt-2-for-real-time-inference-with-tensorrt/
I am not able to build TensorRT OSS and running into following error:/urs/bin/aarch64-linux-gnu-g++ is not a full path to existing compiler tool.I tried converting encoder and decoder separately into tensorrt but could not run the converted models in triton server.TensorRT Version:
GPU Type:  A100
Nvidia Driver Version: 530.30.02
CUDA Version:  12.1
CUDNN Version:
Operating System + Version:
Python Version (if applicable):  3.9.2
TensorFlow Version (if applicable):
PyTorch Version (if applicable):
Baremetal or Container (if container which image + tag):Hi,I am not able to build TensorRT OSS and running into following error:Could you try again after installing g++ correctly, Please refer to the readme below (latest doc) to build TensorRT OSS.NVIDIA® TensorRT™, an SDK for high-performance deep learning inference, includes a deep learning inference optimizer and runtime that delivers low latency and high throughput for inference applicat...Thank you.Powered by Discourse, best viewed with JavaScript enabled"
357,fps-is-not-increasing-while-doing-the-inference-for-segmentation-with-tensorrt-am-getting-only-1-frame-per-2-seconds-i-need-2-fps,"import cv2
import numpy as np
from PIL import Image
import tensorrt as trt
from torchvision import transforms
import labels  # from cityscapes evaluation script
import engine as eng
import time
import inference as inf
#import keras
import argparse #import skimage.transform
import pycuda.driver as cuda
#import pycuda.autoinitcuda.init()
cuda_device = cuda.Device(0)
cuda_ctx = cuda_device.make_context()TRT_LOGGER = trt.Logger(trt.Logger.WARNING)
trt_runtime = trt.Runtime(TRT_LOGGER)
CLASSES = 1
HEIGHT = 720
WIDTH = 1280
new_frame = 0
prev_frame = 0def preprocess(image):def rescale_image(image, output_shape, order=1):
image=cv2.resize(image,output_shape)
return imagedef main(args,frame,stream,context):if name == “main”:
parser = argparse.ArgumentParser()
#parser.add_argument(‘–input_image’, type=str)
parser.add_argument(‘–engine_file’, type=str)A couple of suggestions NVdali can convert to fp32, normalize and rescale/resize your input image in GPU memory in preparation for inferencing with TensorRT. Here is some tutorials of various NVdali image manipulation routines in python. https://docs.nvidia.com/deeplearning/dali/user-guide/docs/examples/general/normalize.html NVdali could also be used to threshold the image and convert to 8bit on the backend of inference operation.The NVIDIA Deepstream SDK is made to handle this entire pipeline of inference operations on the GPU with the highest performance. Explanation of the Deepstream pipeline, https://youtu.be/hSegX0P170spreprocessing of the image is not a concern,
here is my tensorrt inference script.
with this script, inferencing one frame it is taking 1.5-sec which means 0.5fps. I want it t have a better fps.
I’m sharing my script below.particularly,context.execute(batch_size=1, bindings=[int(d_input_1), int(d_output)])
this line is taking 1.4sec to runimport tensorrt as trt
import pycuda.driver as cuda
import pycuda.autoinit
import numpy as npdef allocate_buffers(engine, batch_size, data_type):“”""
This is the function to allocate buffers for input and output in the device
Args:
engine : The path to the TensorRT engine.
batch_size : The batch size for execution time.
data_type: The type of the data for input and output, for example trt.float32.Output:
h_input_1: Input in the host.
d_input_1: Input in the device.
h_output_1: Output in the host.
d_output_1: Output in the device.
stream: CUDA stream.“”""h_input_1 = cuda.pagelocked_empty(batch_size * trt.volume(engine.get_binding_shape(0)), dtype=trt.nptype(data_type))
h_output = cuda.pagelocked_empty(batch_size * trt.volume(engine.get_binding_shape(1)), dtype=trt.nptype(data_type))d_input_1 = cuda.mem_alloc(h_input_1.nbytes)d_output = cuda.mem_alloc(h_output.nbytes)stream = cuda.Stream()
return h_input_1, d_input_1, h_output, d_output, streamdef load_images_to_buffer(pics, pagelocked_buffer):preprocessed = np.asarray(pics).ravel()
np.copyto(pagelocked_buffer, preprocessed)def do_inference(engine, pics_1, h_input_1, d_input_1, h_output, d_output, stream, batch_size, height, width):“”""
This is the function to run the inference
Args:
engine : Path to the TensorRT engine.
pics_1 : Input images to the model.
h_input_1: Input in the host.
d_input_1: Input in the device.
h_output_1: Output in the host.
d_output_1: Output in the device.
stream: CUDA stream.
batch_size : Batch size for execution time.
height: Height of the output image.
width: Width of the output image.Output:
The list of output images.“”""load_images_to_buffer(pics_1, h_input_1)with engine.create_execution_context() as context:
# Transfer input data to the GPU.
cuda.memcpy_htod_async(d_input_1, h_input_1, stream)Your batchsize is 1, imagesize is 1280x720 (large), float32 tensor formatted data input. What GPU are you using (nvidia-smi.exe) and what display driver( CUDA version). I think you want an Ampere or better GPU. Higher framerates may require smaller input images. There is a tool NVIDIA Nsight Compute (see guided analysis video) that will point out which CNN layer is your bottleneck. What CNN are you inferencing? In the NVIDIA transfer learning toolkit hosted on ngc.nvidia.com many trained networks (PeopleNet, VehicleNet, etc) are available for comparison in performance. Even YOLO typically starts with a 446x446 image.Powered by Discourse, best viewed with JavaScript enabled"
358,is-there-any-way-to-run-a-tensorrt-engine-without-tensorrt-environment-in-another-word-what-is-the-minimal-trt-runtime-environment,"I have two machine, say machine A and B, with entirely same hardware and cuda environment. But I only have one with TensorRT environment, say machine A. Now I have built an engine on machine A. How can I make it run on machine B with python API?Can I just install pip install tensorrt? If not, what else should I do?Hi,
Please refer to the installation steps from the below link if in case you are missing on anythingThis NVIDIA TensorRT 8.5.3 Installation Guide provides the installation requirements, a list of what is included in the TensorRT package, and step-by-step instructions for installing TensorRT.
Also, we suggest you to use TRT NGC containers to avoid any system dependency related issues.NVIDIA TensorRT is a C++ library that facilitates high-performance inference on NVIDIA graphics processing units (GPUs). TensorRT takes a trained network and produces a highly optimized runtime engine that performs inference for that network.Thanks!Powered by Discourse, best viewed with JavaScript enabled"
359,cudnn-not-working-with-egpu,"CUDNN not working with eGPU? eGPU is recognized but Pytorch will not use itHi @nicholas_williams
Can you please brief us about the issue and share api logs?Powered by Discourse, best viewed with JavaScript enabled"
360,nvidia-broadcast-to-process-my-videos,"Hello everybody !I make videos and I use Gaze Redirect to process my videos once I record them. Can I use Nvidia Broadcast to do the same ? (I only record with my iPhone, I can’t plug it to my computer)Broadcast doesn’t natively support this. There are ways to do accomplish it by delivering the video to Broadcast via a virtual webcam. You could give that a try.Powered by Discourse, best viewed with JavaScript enabled"
361,training-custom-model-with-efficientdet-tf2-objects-not-detected-after-training-with-custom-dataset,"We followed the link provided below to train a custom model using EfficientDet_tf2: TAO Toolkit Getting Started | NVIDIA NGCWe successfully trained the model using the COCO dataset provided in the EfficientDet_tf2 notebook. During inference, the model correctly detects objects.However, when we train the model with our own data, the training completes successfully, but it does not detect any objects.Here are the details of our custom dataset training:Apart from the above details, we kept almost all settings as they were.We would appreciate any insights or suggestions regarding the issue we are facing. Thank you.
train.json (2.4 MB)hey when I try to run tao efficientdet_tf2 data_convert but it cannot find efficientdet_tf2 giving this error: /bin/bash not found efficientdet_tf2,  do you have any idea?Please try efficientdet_tf2 notebook from:
wget --content-disposition https://api.ngc.nvidia.com/v2/resources/nvidia/tao/tao-getting-started/versions/4.0.1/zip -O getting_started_v4.0.1.zip
unzip -u getting_started_v4.0.1.zip -d ./getting_started_v4.0.1 && rm -rf getting_started_v4.0.1.zip && cd ./getting_started_v4.0.1Make sure that nvidia-tao is installed with “pip3 install nvidia-tao”.And its “dataset_convert” not “data_convert”Powered by Discourse, best viewed with JavaScript enabled"
362,best-practices-for-seamlessly-integrating-chatgpt-into-your-applications,"Hello everyone,I wanted to start a discussion on the best practices for integrating ChatGPT into various applications. As developers, we strive to create user-friendly and efficient systems that leverage the power of conversational AI. Here are a few key points to consider when integrating ChatGPT into your applications:Start with a clear objective: Clearly define the purpose and scope of integrating ChatGPT into your application. Understand how it can enhance the user experience and add value to your product.Design a conversational flow: Plan and design a natural and intuitive conversational flow that integrates ChatGPT seamlessly. Consider user prompts, handling user inputs, and generating accurate and relevant responses.Handle user input gracefully: Implement a robust input validation mechanism to handle different user inputs effectively. This can include filtering inappropriate content, detecting potential user errors, or clarifying ambiguous queries.Optimize performance: Pay attention to the response time and latency of ChatGPT API integration. Ensure that the integration doesn’t significantly impact the overall performance of your application. Explore techniques like batching requests or utilizing asynchronous processing to improve efficiency.Prioritize user privacy and data security: Ensure that you handle user data with utmost care and follow industry-standard security practices. Clearly communicate your data handling policies to users and obtain necessary consent when dealing with personal information.At Chetu, we offer ChatGPT API integration solutions that address these best practices and more. Our goal is to provide developers with a seamless integration experience while prioritizing user privacy and maintaining high-performance standards. You can learn more about our ChatGPT integration solutions here.I’m looking forward to hearing your thoughts on these best practices and any additional tips you have for integrating ChatGPT into applications. Let’s create amazing conversational experiences together!Best regards,
Jeff ParchetaSince this is not a question for the AMA team I shall move this move.
ThanksPowered by Discourse, best viewed with JavaScript enabled"
363,python-sample-code-for-dynamic-shape-inference,"Is there a tensorrt inference sample code in python , catering to the inference for dynamic batch sizes?I am running inference on Jetson AGX orin using Jetpack 5.1.Hi,
Please refer to the below link for Sample guide.This Samples Support Guide provides an overview of all the supported NVIDIA TensorRT 8.6.0 Early Access (EA) samples included on GitHub and in the product package. The TensorRT samples specifically help in areas such as recommenders, machine...
Refer to the installation steps from the link if in case you are missing on anythingThis NVIDIA TensorRT 8.6.0 Early Access (EA) Installation Guide provides the installation requirements, a list of what is included in the TensorRT package, and step-by-step instructions for installing TensorRT.
However suggested approach is to use TRT NGC containers to avoid any system dependency related issues.NVIDIA TensorRT is a C++ library that facilitates high-performance inference on NVIDIA graphics processing units (GPUs). TensorRT takes a trained network and produces a highly optimized runtime engine that performs inference for that network.In order to run python sample, make sure TRT python packages are installed while using NGC container.
/opt/tensorrt/python/python_setup.shIn case, if you are trying to run custom model, please share your model and script with us, so that we can assist you better.
Thanks!Powered by Discourse, best viewed with JavaScript enabled"
364,can-i-use-two-dynamic-shapes,"for example :
ITensor* shape = addConstant(Dims(4, {1,1,-1,-1}));
// Two of the shapes are variable, but both have the same value.IShuffleLayer* reshape_layer = addShuffle(*mask); //mask is Dims4 random tensor
reshape_layer->setInput(1, *shape)
//impossible//but
ITensor* shape = addConstant(Dims(4, {1,1,-1,13}));
//If I put one variable, actually i can’t figure out the shape
IShuffleLayer* reshape_layer = addShuffle(*mask); //mask is Dims4 random tensor
reshape_layer->setInput(1, *shape)
//possibleSo, can i use two dynamic shapes? and
How to use it?Thanks! Have a nice day!Hi @ghdgudfkrPlease refer to the below doc while working with dynamic shapesThis is the revision history of the NVIDIA TensorRT 8.6 Developer Guide.ThanksPowered by Discourse, best viewed with JavaScript enabled"
365,support-for-gather-with-dynamic-shapes,"I want to convert my trained model and optimize inference with TensorRT 8.0.
For this I use the following conversion flow: Pytorch → ONNX → TensorRTThe ONNX model can be successfully runned with onxxruntime-gpu, but failed with conversion from ONNX to TensorRT with trtexec.From debugging, I have found the problem place which is related with following original Pytorch code:def sample_points(points, idx):
idx = idx.view(-1).unsqueeze(1)
index = idx.expand(-1, points.size(-1))
res = torch.gather(points, 0, index)
return res.reshape(points.size(0), -1, points.size(1))This sample_points function is used as intermediate operation inside NN. Seems like problem is connected that I use torch.size to initialize dimension and produces non-static, dynamic input data for torch.gather.Does anyone know how to solve this problem? I very need your help. Many thanks!TensorRT Version 8.0:
GPU Type NVIDIA RTX 2080 Ti
Nvidia Driver Version 450.5106
CUDA Version 11.0
Operating System + Version Ubuntu 18.04 LTS
Python Version 3.6.10
PyTorch Version 1.9.0Hi,
Request you to share the ONNX model and the script if not shared already so that we can assist you better.
Alongside you can try few things:This NVIDIA TensorRT 8.4.3 Quick Start Guide is a starting point for developers who want to try out TensorRT SDK; specifically, this document demonstrates how to quickly construct an application to run inference on a TensorRT engine.import sys
import onnx
filename = yourONNXmodel
model = onnx.load(filename)
onnx.checker.check_model(model).
2) Try running your model with trtexec command.
https://github.com/NVIDIA/TensorRT/tree/master/samples/opensource/trtexec
In case you are still facing issue, request you to share the trtexec “”–verbose"""" log for further debugging
Thanks!Hi, NVES. Thanks for your response!
I checked and validated my model with onnx.checker.check_model and it works well without any warning messages.
After, when I run trtexec optimization process failed with following message:[W] [TRT] onnx2trt_utils.cpp:364: Your ONNX model has been generated with INT64 weights, while TensorRT does not natively support INT64. Attempting to cast down to INT32.
[W] [TRT] onnx2trt_utils.cpp:390: One or more weights outside the range of INT32 was clamped
[W] [TRT] onnx2trt_utils.cpp:390: One or more weights outside the range of INT32 was clamped
[E] [TRT] ModelImporter.cpp:720: While parsing node number 81 [GatherElements → “105”]:
[E] [TRT] ModelImporter.cpp:721: — Begin node —
[E] [TRT] ModelImporter.cpp:722: input: “83”
input: “104”
output: “105”
name: “GatherElements_81”
op_type: “GatherElements”
attribute {
name: “axis”
i: 0
type: INT
}
[E] [TRT] ModelImporter.cpp:723: — End node —
[E] [TRT] ModelImporter.cpp:726: ERROR: builtin_op_importers.cpp:1379 In function importGatherElements:
[8] Assertion failed: !isDynamic(daDims) && !isDynamic(idxDims) && “This version of TenosrRT does not support GatherElements on dynamic shapes!”
[E] Failed to parse onnx file
[E] Parsing model failed
[E] Engine creation failed
[E] Engine set up failedAdditionally, I’ve added log file with --verbose from trtexec and ONNX model.
trt_exec_log.txt (46.8 KB)
model_knn.onnx (155.8 KB)Hi @mityaginkir ,
In Current TRT release, GatherElements does not support dynamic shapes. This fix will be available in future release.
Please stay tuned for the updates.Thanks!As of 2023, is this feature now supported?Powered by Discourse, best viewed with JavaScript enabled"
366,omniverse-cuopt-integration,"Hi, I want to create a scenario like the link down below but I couldn’t find content about cuOpt and Isaac sim integration. Is there a prepared tutorial about how can I move my robot(could be Carter) according to cuOpt results?I could move my robot according to the results by coding it myself but I wanted to ask first if there is a doc or tutorial…Thank you for your question.  Using cuOpt and robot navigation are covered separately.  As you mentioned there are a number of options for navigation from writing a controller yourself to leveraging ROS/ROS2.The cuOpt tutorial is hereROS Navigation is hereROS Multi-Robot Navigation is hereROS2 Navigation is hereROS2 Multi-Robot Navigation is hereIf you would like to write your own controller information can be found hereThanks It was helpful.
Since there is no planned structure with cuOpt and Nav2, I could plan my structure as below. Is there something wrong or a more optimized way of doing this operation?I am planning to broadcast my robot location with a topic. I will code a  ros2 node that gets these locations and send it to the cuOpt server. The outcome of the cuOpt server will be my route for the robot.My robot will be getting these routes one by one as waypoints and trying to reach the waypoints.Note: I skipped using algorithms such as mapping-localization and detailed technical work in the explanation phase.Your approach seems reasonable.  A couple things that might help along the way:Have a look at the required data for a cuOpt problem.  This is shown explicitly across the 3 examples show in the cuOpt tutorial.  I would recommend finding the cuOpt example that is closest to your use case and take a look at the code for that example (in particular the data formatting).  If you find yourself with cuOpt specific questions please reach out here again.As you go through your navigation work have a look at the Isaac Sim forums and if you have questions about your Nav2 approach it is likely you can find an answer there either through existing questions or by asking a new one if needed.Good luck and thanks again for your questions.This topic was automatically closed 14 days after the last reply. New replies are no longer allowed.Powered by Discourse, best viewed with JavaScript enabled"
367,need-to-edit-in-yolo-classes,"This is General Deep Learning Question. In YOLO v8 I want to only the Person class after that I will want to custom train my object and I have the dataset for that. For example, if I want to train pen and charger I want to give a custom dataset for these two objects and Yolo’s pre-defined class person.At last, I wantHow to archive this? Only one or some pre-trained classes of YOLO and others are my custom objects.Hi,This forum talks more about updates and issues related to TensorRT. We recommend that you please reach out to the Yolo-related platform to get better help.
And following tutorial may help you.Train YOLOv8 on a custom pothole detection dataset. Training YOLOv8 Nano, Small, & Medium models and running inference for pothole detection on unseen videos.Thank you.Powered by Discourse, best viewed with JavaScript enabled"
368,jetson-nano-downloads-nvidia-riva-couldnt-find-another-way-to-report-it,"Nvidia Riva docker containers download to Jetson Nano despite being blocked. After several minor tweaks to the installation scripts, it downloads in several tries. I don’t know if this was intended so I thought that I should report it.Sorry fo the late response, currently at the moment, we support the following below embedded devices only for RivaReference: Support Matrix — NVIDIA RivaPerhaps I have phrased myself wrong, it shouldn’t, that’s the point. But it does. It does support Jetson Nano after tweaking the installation script. But it shouldn’t. Since it can download when it was not intended, I thought I must report it for security measures.Powered by Discourse, best viewed with JavaScript enabled"
369,unable-to-verify-cudnn-installation-on-linux-ubuntu-20-04,"I’m following this guide to install cudnn.This cuDNN 8.5.0 Installation Guide provides step-by-step instructions on how to install and check for correct operation of NVIDIA cuDNN on Linux and Microsoft Windows systems.I’ve able to perform instruction give under “2.3.4.1. Ubuntu Network Installation” section
https://docs.nvidia.com/deeplearning/cudnn/install-guide/index.html#package-manager-ubuntu-installI’m getting the following error when I’m trying to perform instruction give under “2.4. Verifying The Install On Linux” section for verifying installation.Inside my  /usr/src/cudnn_samples_v8 directory I only have one file “NVIDIA_SLA_cuDNN_Support.txt”Installed cudnn and cuda versions are as follows 8.3.2.44-1+cuda11.5Hi,Are you still facing this issue.
Which method of installation are you following. Could you run successfully prior steps as mentioned in the docs without any error ?This cuDNN 8.5.0 Installation Guide provides step-by-step instructions on how to install and check for correct operation of NVIDIA cuDNN on Linux and Microsoft Windows systems.Thank you.Are you still facing this issue.Yes.Which method of installation are you following.As I mentioned, I followed ""2.3.4.1. Ubuntu Network Installation” sectionCould you run successfully prior steps as mentioned in the docs without any error ?Yes. & if I try to reinstall it says : “libcudnn8 is already the newest version (8.3.2.44-1+cuda11.5)”I believe the issue I’m facing is related to “2.4. Verifying The Install On Linux” section.When I perform the 2nd step of the verification-procedure. “Go to the writable path.”
It says:So I believe mnistCUDNN is missing from the setup. or not being included anymore.Thank you.Hi,Are you still facing this issueYes.I have the same issue, do you have any kind of solution? I have performed multiples time the installation steps…I have the same Issue too , did anyone find a solution for it ? Note : I used the .tar file InstallationThe same here, I followed the steps but there is only the file NVIDIA_SLA_cuDNN_Support.txt.But now I tried to install dlib which CUDA support and it was found!Found CUDA: /usr (found suitable version ""11.5"", minimum required is ""7.5"")But it reaised another issue that I’ll handle now.*** CUDA was found but your compiler failed to compile a simple CUDA program so dlib isn't going to use CUDA.Same issue here. Any update?I have followed the installation instruction for the 1.3.4.1. Ubuntu Network Installation as well as 1.3.1. Tar File Installation and eventually the Debian installation. When it get to installing the library it says:Reading package lists… Done
Building dependency tree… Done
Reading state information… Done
Package libcudnn8-samples is not available, but is referred to by another package.
This may mean that the package is missing, has been obsoleted, or
is only available from another sourceE: Version ‘8.x.x.x-1+cudaX.Y~’ for ‘libcudnn8-samples’ was not foundCant Nvidia just put the sample code to github just like you did with the CUDA toolkit sample codes? Why all the hoop-jumping. And from what I’ve seen this is not a singular issue, a lot of people is having issue locating and finding the cudnn sample codes.Yes!First I installed Ubuntu 22.10 and for my happiness I noticed the SO had finally recognized my Geforce RTX 3070 Ti Laptop in all monitoring tools, including:nvidia-smi
sudo lshw -c video
ubuntu-drivers devices
neofetch
glances
nvtop
inxi -F
lspci -k | egrep -A 3 -i “vga|display|3d”It wasn’t been recognized in Ubuntu 22.04 LTS. Even the wifi was recognized at the end of the installation. The nvidia toolkit (nvidia-smi, …) was automatically installed too. :)Then I’ve followed this Installing dlib using conda with CUDA enabled · GitHubI’ve aborted the first installation because it complained about the lack of a package and I’ve decided to install it first and them restart the installation process.sudo nala install libopenblas-dev liblapack-devWhen it finished I could see the expected messages.– Found CUDA: /home/…/miniconda3/envs/CV (found suitable version “12.0”, minimum required is “7.5”)
– Found cuDNN: /home/…/miniconda3/envs/CV/lib/libcudnn.soUp to now I think changing to Ubuntu 22.10 was a good choice.That is what I did. Good luck!You can install with:Reboot.Save it.Reboot and enjoy it!Thanks for all for replying. I finally found out the issue.it has to do with the way you install the cuDNN, the sample code are only available if you are doing it with debian install (1.3.2)it will not be included if you do it with the tar file you download from nvidia, neither would it work if you install it from Package Manager (sudo apt install)the previous 2 methods will only install libcudnn8 and libcudnn8-dev packages, but not the libcudnn8-samples you needso you can install the libraries with whatever method you want, but you will need to follow 1.3.2 of the cudnn install guide to get the libcudnn8-samplesalso only cuda12.0 is available and not cuda12.1, so for e.g.
sudo apt-get install libcudnn8-samples=8.8.1.3-1+cuda12.0(i wish they do include it for package manager/tar at least, would make it much easier)Hi Guys:If you have the sample in a second computer (or peer’s computer), you can copy cudnn_samples_v8 onto your targeted computer.1. Move it to /usr/src$ sudo mv /home/user/Downloads/cudnn_samples_v8 /usr/src/2. Copy the cuDNN samples to a writable path$ cp -r /usr/src/cudnn_samples_v8/ $HOME3.Go to the writable path$ cd $HOME/cudnn_samples_v8/mnistCUDNN4. Compile the sample$ make clean && make5. Run the sample$ ./mnistCUDNN…
Test passed!
…Cheers,MikePowered by Discourse, best viewed with JavaScript enabled"
370,int8-calibration-cache-format-could-it-be-officially-documented,"Hi,Today, the INT8 Calibration Cache is a text file that looks like this:So the format is: <tensor_name>:<scale factor>. Where scale factor is (presumably) the hex representation of the floating point number in IEEE754 big endian format.Questions:Let’s say that we take the Calibration Cache and manually parse it without using any TensorRT API. Can we assume that the format that I mentioned above is stable and dependable? Or can the format change on a new TRT release without notice (since it’s undocumented)?If we can assume the format is stable - could it be documented in the official TensorRT documentation? So that we can depend on solid information instead of reverse engineering.Thanks!Hi, Please refer to the below links to perform inference in INT8master/samples/sampleINT8NVIDIA® TensorRT™, an SDK for high-performance deep learning inference, includes a deep learning inference optimizer and runtime that delivers low latency and high throughput for inference applicat...1777.21 KB
Thanks!Those resources do not answer my question.Hi,We always recommend that you please use the TensorRT API.
The format of the calibration cache used by TensorRT is not officially documented or supported for manual parsing without using the provided TensorRT APIs. As a result, relying on the format and attempting to parse it without the official APIs may lead to compatibility issues and unexpected behavior.TensorRT releases may introduce changes, improvements, or optimizations that could impact the calibration cache format.Thank you.Hi,I understand that, but TensorRT does not provide any API for extracting information from the calibration cache (other than reading it into an opaque data blob) - what should be used then?Let’s say I want to extract the scale factor of a given tensor name from the calibration cache - what API can I use to get that information?Thanks!Hi,At this moment, we do not have APIs available, we will provide them in future releases.Thank you.Powered by Discourse, best viewed with JavaScript enabled"
371,cudnn8-crash-with-stack-overflow,"interface: cudnnConvolutionForward
cuda:11.4
cudnn:8.24
Windows10, RTX2080Ti, driver version:471.41
I am trying to update my project from cuda10.2 to cuda11.4, I didn’t change any code. But the program crash with stack overflow by cudnn8 interface cudnnConvolutionForward.Unhandled exception at 0x00007FFC9A31A3C8 (cudnn_cnn_infer64_8.dll) in PerformanceTool.exe: 0xC00000FD: Stack overflow (parameters: 0x0000000000000001, 0x0000004067303000). occurredHi @lei.yan
Can you pls share the detailed logs with us for better assistance.Thanks!Hi, Thank you for your reply. My program didn’t output logs, does cuda have function to print logs?
My program is multiple cpu threads and create some cuda streams, cudnn handles for every cpu thread.
And call functions:
cudnnFindConvolutionForwardAlgorithmEx,
cudnnGetConvolutionForwardWorkspaceSize,
cudnnConvolutionForward etc.
The program crash with stack overflow. Does cudnn stack size changed from cudnn7 to cudnn8?My program didn’t output logs, does cuda have function to print logs?cuDNN has API logging, which you can enable with environment variables or by using the API.Thank you for your support. I got the logs file for cudnn7(OK) and cudnn8(NG).
Please help to check. Thanks a lot (1.3 MB)Hi @lei.yan ,
Looks like you may need to share the attachments again.
Thanks!Crach happens in my new laptop:cudnnCreateConvolutionDescriptor(&conv_desc)  triggers the failure.Information from the debugging outputs:
0x00007FFD5B3A62BD (cudnn64_8.dll) (RQNetd.exe 中)处有未经处理的异常: 请求了严重的程序退出。Information from console:
Could not load library cudnn_cnn_infer64_8.dll. Error code 126
Please make sure cudnn_cnn_infer64_8.dll is in your library path!Environments:
Windows 11
Visual Studio 2022
Cuda 11.5
Cudnn 8.3.0
RTX 3060cudnnCreateConvolutionDescriptorcudnn64_8.dll has error.
Cuda 10.2 and Cudnn 8.0.2 is rightsounds crazy but this worked for me and one other… Nvidia should be ashamed !copy zlibwapi.dll- from “C:\Program Files\Microsoft Office\root\Office16\ODBC Drivers\Salesforce\lib” (other guy installed using Microsoft 365 x64 in windows 11 but I already had it) and copy pasted this file into “C:\Program Files\NVIDIA GPU Computing Toolkit\CUDA\v11.6\bin”hey presto … no more crash when calling cudnnCreateConvolutionDescriptornow you wouldn’t have guessed that one !Copying zlibwapi.dll worked for me too. In hindsight it is explained hereThis cuDNN 8.5.0 Installation Guide provides step-by-step instructions on how to install and check for correct operation of NVIDIA cuDNN on Linux and Microsoft Windows systems.For reference I only encountered the crash after upgrading from Visual Studio 2019, Cuda 11.2 and cuDNN 8.2 to VS 2022, Cuda 11.7 and cuDNN 8.4.The same question bothered me ever. Confirm that you install the correct package “Microsoft.ML.OnnxRuntime.Gpu” rather than “Microsoft.ML.OnnxRuntime”!Hello.I can confirm that this crash still happens on Windows 11!Gary, thanks man - your solution worked for me! I could have spent days reinstalling toolkits and drivers. My crash was also in cudnnCreateConvolutionDescriptor call and fixes after I copied zlibwapi.dll and libs (downloaded with cuda toolkit) in CUDA install folders.I use laptop with:
RTX 3050TI
Cuda11.8
Cudnn 8.6.0/8,5.0
Visual Studio 2017,2022Powered by Discourse, best viewed with JavaScript enabled"
372,tensorrt-quantization-uses-int8-or-uint8,"TensorRT developer guide says the quantized range is [-128, 127], meaning it should use int8.
However, when I convert the tensorflow quantization-aware-trained model to ONNX, and then to TRT, such error comes out –ModelImporter.cpp:778: ERROR: builtin_op_importers.cpp:1173 In function QuantDequantLinearHelper:
[6] Assertion failed: shiftIsAllZeros(zeroPoint) && ""TRT only supports symmetric quantization - zeroPt must be all zerosDoes it mean I should use uint8 (0, 255)?TensorRT Version: 8.6
GPU Type: Nvidia a4000
Nvidia Driver Version:
CUDA Version: 11.8
CUDNN Version:
Operating System + Version:
Python Version (if applicable):
TensorFlow Version (if applicable): 2.12
PyTorch Version (if applicable):
Baremetal or Container (if container which image + tag):Hi, Please refer to the below links to perform inference in INT8master/samples/sampleINT8NVIDIA® TensorRT™, an SDK for high-performance deep learning inference, includes a deep learning inference optimizer and runtime that delivers low latency and high throughput for inference applicat...1777.21 KB
Thanks!Powered by Discourse, best viewed with JavaScript enabled"
373,problems-with-4090-cuda-samples-cudnn-sample-are-these-expected,"Greetings,
I am trying to use TensorRT in our Linux 22.04.1 device with an RTX 4090, however, running into several problems. Is there something I am doing wrong here in terms of compatibility? Can we do anything other than waiting for an update?
(1) When I tried to install CUDA 11.8 (cuda_11.8.0_520.61.05_linux.run) with run file and use the bundled driver 520.61.05, my screen goes blank.
(2) So I purged everything and installed 520.56.06 (NVIDIA-Linux-x86_64-520.56.06.run), then CUDA 11.8 (cuda_11.8.0_520.61.05_linux.run, driver is deselected). Performed post installation steps (adding to path and exporting the lib)
(3) To start with TensorRT, I have installed PyCuda and moved on to cuDNN.
(4) Zlib was already there used this deb: cudnn-local-repo-ubuntu2204-8.6.0.163_1.0-1_amd64.deb
(5) Tried to run MNIST sample but it sometimes gives this:
Executing: mnistCUDNN
cudnnGetVersion() : 8600 , CUDNN_VERSION from cudnn.h : 8600 (8.6.0)
Host compiler version : GCC 11.3.0There are 1 CUDA capable devices on your machine :
device 0 : sms 128  Capabilities 8.9, SmClock 2580.0 Mhz, MemSize (Mb) 24252, MemClock 10501.0 Mhz, Ecc=0, boardGroupID=0
Using device 0Testing single precision
Loading binary file data/conv1.bin
Loading binary file data/conv1.bias.bin
Loading binary file data/conv2.bin
Loading binary file data/conv2.bias.bin
Loading binary file data/ip1.bin
Loading binary file data/ip1.bias.bin
Loading binary file data/ip2.bin
Loading binary file data/ip2.bias.bin
Loading image data/one_28x28.pgm
Performing forward propagation …
Testing cudnnGetConvolutionForwardAlgorithm_v7 …
^^^^ CUDNN_STATUS_SUCCESS for Algo 1: -1.000000 time requiring 0 memory
^^^^ CUDNN_STATUS_SUCCESS for Algo 0: -1.000000 time requiring 0 memory
^^^^ CUDNN_STATUS_SUCCESS for Algo 2: -1.000000 time requiring 0 memory
^^^^ CUDNN_STATUS_SUCCESS for Algo 5: -1.000000 time requiring 178432 memory
^^^^ CUDNN_STATUS_SUCCESS for Algo 4: -1.000000 time requiring 184784 memory
^^^^ CUDNN_STATUS_SUCCESS for Algo 7: -1.000000 time requiring 2057744 memory
^^^^ CUDNN_STATUS_NOT_SUPPORTED for Algo 6: -1.000000 time requiring 0 memory
^^^^ CUDNN_STATUS_NOT_SUPPORTED for Algo 3: -1.000000 time requiring 0 memory
Testing cudnnFindConvolutionForwardAlgorithm …
ERROR: cudnn failure (CUDNN_STATUS_ALLOC_FAILED) in mnistCUDNN.cpp:589
Aborting…(6) Sometimes this:
Executing: mnistCUDNN
cudnnGetVersion() : 8600 , CUDNN_VERSION from cudnn.h : 8600 (8.6.0)
Host compiler version : GCC 11.3.0There are 1 CUDA capable devices on your machine :
device 0 : sms 128  Capabilities 8.9, SmClock 2580.0 Mhz, MemSize (Mb) 24252, MemClock 10501.0 Mhz, Ecc=0, boardGroupID=0
Using device 0Testing single precision
ERROR: cudnn failure (CUDNN_STATUS_INTERNAL_ERROR) in mnistCUDNN.cpp:414
Aborting…(7) Both are run with sudo(8)
±----------------------------------------------------------------------------+
| NVIDIA-SMI 520.56.06    Driver Version: 520.56.06    CUDA Version: 11.8     |
|-------------------------------±---------------------±---------------------+
| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |
|                               |                      |               MIG M. |
|===============================+======================+======================|
|   0  NVIDIA GeForce …  Off  | 00000000:01:00.0  On |                  Off |
|  0%   44C    P2   114W / 450W |    503MiB / 24564MiB |      0%      Default |
|                               |                      |                  N/A |
±------------------------------±---------------------±---------------------+±----------------------------------------------------------------------------+
| Processes:                                                                  |
|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |
|        ID   ID                                                   Usage      |
|=============================================================================|
|    0   N/A  N/A      2025      G   /usr/lib/xorg/Xorg                244MiB |
|    0   N/A  N/A      2282      G   /usr/bin/gnome-shell               81MiB |
|    0   N/A  N/A      3519      G   …8/usr/lib/firefox/firefox      154MiB |
|    0   N/A  N/A     34621      G   …mviewer/tv_bin/TeamViewer       23MiB |
±----------------------------------------------------------------------------+(9)
./deviceQuery Starting…CUDA Device Query (Runtime API) version (CUDART static linking)Detected 1 CUDA Capable device(s)Device 0: “NVIDIA GeForce RTX 4090”
CUDA Driver Version / Runtime Version          11.8 / 11.8
CUDA Capability Major/Minor version number:    8.9
Total amount of global memory:                 24252 MBytes (25430589440 bytes)
MapSMtoCores for SM 8.9 is undefined.  Default to use 128 Cores/SM
MapSMtoCores for SM 8.9 is undefined.  Default to use 128 Cores/SM
(128) Multiprocessors, (128) CUDA Cores/MP:    16384 CUDA Cores
GPU Max Clock rate:                            2580 MHz (2.58 GHz)
Memory Clock rate:                             10501 Mhz
Memory Bus Width:                              384-bit
L2 Cache Size:                                 75497472 bytes
Maximum Texture Dimension Size (x,y,z)         1D=(131072), 2D=(131072, 65536), 3D=(16384, 16384, 16384)
Maximum Layered 1D Texture Size, (num) layers  1D=(32768), 2048 layers
Maximum Layered 2D Texture Size, (num) layers  2D=(32768, 32768), 2048 layers
Total amount of constant memory:               65536 bytes
Total amount of shared memory per block:       49152 bytes
Total shared memory per multiprocessor:        102400 bytes
Total number of registers available per block: 65536
Warp size:                                     32
Maximum number of threads per multiprocessor:  1536
Maximum number of threads per block:           1024
Max dimension size of a thread block (x,y,z): (1024, 1024, 64)
Max dimension size of a grid size    (x,y,z): (2147483647, 65535, 65535)
Maximum memory pitch:                          2147483647 bytes
Texture alignment:                             512 bytes
Concurrent copy and kernel execution:          Yes with 2 copy engine(s)
Run time limit on kernels:                     Yes
Integrated GPU sharing Host Memory:            No
Support host page-locked memory mapping:       Yes
Alignment requirement for Surfaces:            Yes
Device has ECC support:                        Disabled
Device supports Unified Addressing (UVA):      Yes
Device supports Managed Memory:                Yes
Device supports Compute Preemption:            Yes
Supports Cooperative Kernel Launch:            Yes
Supports MultiDevice Co-op Kernel Launch:      Yes
Device PCI Domain ID / Bus ID / location ID:   0 / 1 / 0
Compute Mode:
< Default (multiple host threads can use ::cudaSetDevice() with device simultaneously) >deviceQuery, CUDA Driver = CUDART, CUDA Driver Version = 11.8, CUDA Runtime Version = 11.8, NumDevs = 1
Result = PASS(10) Should I be scared of this?
MapSMtoCores for SM 8.9 is undefined.  Default to use 128 Cores/SM
MapSMtoCores for SM 8.9 is undefined.  Default to use 128 Cores/SM(11)
./deviceQueryDrv Starting…CUDA Device Query (Driver API) statically linked version
Detected 1 CUDA Capable device(s)Device 0: “NVIDIA GeForce RTX 4090”
CUDA Driver Version:                           11.8
CUDA Capability Major/Minor version number:    8.9
Total amount of global memory:                 24252 MBytes (25430589440 bytes)
MapSMtoCores for SM 8.9 is undefined.  Default to use 128 Cores/SM
MapSMtoCores for SM 8.9 is undefined.  Default to use 128 Cores/SM
(128) Multiprocessors, (128) CUDA Cores/MP:     16384 CUDA Cores
GPU Max Clock rate:                            2580 MHz (2.58 GHz)
Memory Clock rate:                             10501 Mhz
Memory Bus Width:                              384-bit
L2 Cache Size:                                 75497472 bytes
Max Texture Dimension Sizes                    1D=(131072) 2D=(131072, 65536) 3D=(16384, 16384, 16384)
Maximum Layered 1D Texture Size, (num) layers  1D=(32768), 2048 layers
Maximum Layered 2D Texture Size, (num) layers  2D=(32768, 32768), 2048 layers
Total amount of constant memory:               65536 bytes
Total amount of shared memory per block:       49152 bytes
Total number of registers available per block: 65536
Warp size:                                     32
Maximum number of threads per multiprocessor:  1536
Maximum number of threads per block:           1024
Max dimension size of a thread block (x,y,z): (1024, 1024, 64)
Max dimension size of a grid size (x,y,z):    (2147483647, 65535, 65535)
Texture alignment:                             512 bytes
Maximum memory pitch:                          2147483647 bytes
Concurrent copy and kernel execution:          Yes with 2 copy engine(s)
Run time limit on kernels:                     Yes
Integrated GPU sharing Host Memory:            No
Support host page-locked memory mapping:       Yes
Concurrent kernel execution:                   Yes
Alignment requirement for Surfaces:            Yes
Device has ECC support:                        Disabled
Device supports Unified Addressing (UVA):      Yes
Device supports Managed Memory:                Yes
Device supports Compute Preemption:            Yes
Supports Cooperative Kernel Launch:            Yes
Supports MultiDevice Co-op Kernel Launch:      Yes
Device PCI Domain ID / Bus ID / location ID:   0 / 1 / 0
Compute Mode:
< Default (multiple host threads can use ::cudaSetDevice() with device simultaneously) >
Result = PASS(12)
./bandwidthTest
[CUDA Bandwidth Test] - Starting…
Running on…Device 0: NVIDIA GeForce RTX 4090
Quick ModeCUDA error at bandwidthTest.cu:686 code=46(cudaErrorDevicesUnavailable) “cudaEventCreate(&start)”Hi @volkan.dinc1 ,
Apologies for delayed response, we are checking on this and will get back to you.
Thank you for your patience.Hi @volkan.dinc1 ,
Can you please turn on api logging and attach a log . This will help to exactly see where in cuDNN the failures are happening.ThanksI don’t have problems with 11.8 and cudnn v8.7.0 in pytorch and tensorflow, but when cudnn release for cuda 12.0?I have the same issue with ubuntu22.04, cuda12.0 and cudnn8.8.First time I run the ./mnustCUDNN:second time:When trying to call jax.random.RNGKey(0) in python I get:I am also running into this problem ubuntu22.04:  driver525.85.05, cuda 12.0, cudnn 8.8 but I am running on an RTX 3060 XC from EVGA with the 12GB GDDR6 along with a pair of A2000’s with 12GB as well.Powered by Discourse, best viewed with JavaScript enabled"
374,retrain-model-using-less-categories-without-any-loss,"Hello, i have a pretrained yolov5 model trained on coco dataset with 80categories
currently i would like to remove from the model 78 categories and use just two of them, what’s the best way to do it?
do i need to download the coco dataset, select just the two categories of interest and retrain the model from scratch?
do i need to download the coco dataset, select just the two categories of interest and retrain the model starting from the checkpoint that i currently have?Is there any other way to reach this goal?**Training **: google colab
**Training **: NVIDIA Tesla
deploy
**TensorRT **:8
GPU Type:  Nvidia Jetson
CUDA Version:  10.2
Operating System: Ubuntu 1804
Python Version (if applicable):  3.6.9GitHub - ultralytics/yolov5: YOLOv5 🚀 in PyTorch > ONNX > CoreML > TFLite here you can find the model that i am usingThanks guysHi,
Request you to share the ONNX model and the script if not shared already so that we can assist you better.
Alongside you can try few things:This NVIDIA TensorRT 8.6.0 Early Access (EA) Quick Start Guide is a starting point for developers who want to try out TensorRT SDK; specifically, this document demonstrates how to quickly construct an application to run inference on a TensorRT...import sys
import onnx
filename = yourONNXmodel
model = onnx.load(filename)
onnx.checker.check_model(model).
2) Try running your model with trtexec command.master/samples/trtexecNVIDIA® TensorRT™, an SDK for high-performance deep learning inference, includes a deep learning inference optimizer and runtime that delivers low latency and high throughput for inference applicat...
In case you are still facing issue, request you to share the trtexec “”–verbose"""" log for further debugging
Thanks!hi,
does not matter the ONNX,  i think that the problem is model intependent, i need to know how retrain a model with a subset of it’s categories.Hi @Rik ,I believe you can raise the concern on the respective github page.This page talks about the issues related to TensorRT.ThanksPowered by Discourse, best viewed with JavaScript enabled"
375,topic,"Добрый день. У меня есть вопросы по распознаванию объектов и существующему принципу распознавания.
Есть собственное представление, о том, что можно сделать для этого, и я хочу проверить мое представление о решении вопроса, но есть вопросы:Изображение с камеры передается в виде кода, силуэт от фона отличается цветом, для точного создания выделения по силуэту объекта нужен точный код цвета силуэта объекта. Что дальше?У меня есть изображение сделанное с помощью фильтра, который, как я считаю, решение распознавания и есть ноутбук с видеокартой Nvidia 3050. Как можно с помощью этого провести тест?Фильтр с квадратной сеткой. Объект имеет сферическую форму - круглая лампочка, на изображении, визуально, одного цвета, но оттенок меняется. Может быть проверить есть ли зависимость в изменении оттенка, но фильтр самодельный, поэтому, возможна неоднородность и из-за него.Есть идея - я использовал фильтр определенного цвета для наиболее сложного цвета, как упоминалось ранее, с сеткой определенного цвета. Может быть от этой связки можно производить дальнейшие расчеты?Смысл идеи: “Уйти от 3d компьютерного зрения и прийти к обработке изображения с наложенным фильтром.”As far as I understand, I am interested in pattern recognition. If there is a specialist, I would talk to him.It is supposed to use a 2d picture scene: on a blue background, an ice melting inverted figure (icicle) picture, a yellow background with a white grid is superimposed on the picture
Blue is the most complex color, yellow filters it for clarity
, black is a simple color
recognition is assumed according to templates by counting the number of shades of an object’s spot and calculating the average weighted harmonicI conducted an experiment with a distorted blue inscription: I took it without a yellow filter and with a yellow filter, very similar photos turned out, then I uploaded both photos to an online translator and as a result, what was translated with a filter, what was not translated correctly without a filter - the writing was not recognized correctly
show the file with screenshots was not I can, because the inscription will be an advertisement, I can throw it to someone personally
бсф2910×393 291 KB
Addition: the exact definition of an object can be achieved by focusing on it (its border, etc.)Rather, even smooth focusingThis is a photo of a starЯ предполагаю, не могу точно проверить, что есть максимальный или пограничный оттенок, который может указывать на границу между частями объекта или объектами
I guess I can’t verify for sure that there is a maximum or border shade that can indicate a border between parts of an object or objectshttps://www.cyberforum.ru/projects/thread3017863.html
image1920×1412 40.8 KB
Powered by Discourse, best viewed with JavaScript enabled"
376,cross-correlation-layer-with-c-tensorrt-api,"I try to build a siamRPN Network with TensorRT C++ API, with a AlexNet Backbone. What I want is to do the cross correlation between two ITensor* of shape  [1, 126, 24, 24] and [256, 1, 4, 4]. Those ITensor are output of ILayers. I have the network working with pytorch in python. This cross Correlation layer is Implemented as followed in python:
torch.nn.fonctionnal.conv2D(tensor1, tensor 2, groups=nb_channel_of_tensor1).I don’t know if I need to do a plugin layer or if I can use network->addConvolution layer to do it.
I tried this solution :ILayer* siamRPN::crossCorrelation(INetworkDefinition network, ITensor input1, ITensor* input2) {
Weights emptywts{ DataType::kFLOAT, nullptr, 0 };
Weights emptybiais{ DataType::kFLOAT, nullptr, 0};
int16_t Nb_groups = 256;IConvolutionLayer* crossConv = network->addConvolutionNd(*input1, 256, DimsHW{4, 4}, emptywts, emptybiais);
crossConv->setNbGroups(Nb_groups);
crossConv->setInput(1, *input2);return crossConv;}But when building engine this error is returned
[06/06/2023-13:43:39] [I] [TRT] Graph optimization time: 0.0184208 seconds.
[06/06/2023-13:43:39] [I] [TRT] Local timing cache in use. Profiling results in this builder pass will not be stored.
[06/06/2023-13:43:51] [E] [TRT] 10: Could not find any implementation for node (Unnamed Layer* 46) [Convolution].
[06/06/2023-13:43:51] [E] [TRT] 10: [optimizer.cpp::computeCosts::3869] Error Code 10: Internal Error (Could not find any implementation for node (Unnamed Layer* 46) [Convolution].)TensorRT Version:  8.6.0.1
GPU Type: RTX3060Nvidia Driver Version: 530.30.02
CUDA Version:  12.1
CUDNN Version:
Operating System + Version: Linux Ubuntu 20.04Thanks in advance for the helpHi,
Please check the below link, as they might answer your concernsThis is the API Reference documentation for the NVIDIA TensorRT library. The following set of APIs allows developers to import pre-trained models, calibrate networks for INT8, and build and deploy optimized networks with TensorRT. Networks can be...
Thanks!Powered by Discourse, best viewed with JavaScript enabled"
377,convert-onnx-to-tensorrt-and-run-on-another-device,"Hi, I would like to convert ONNX model (2070 GPU) to TensorRT and run on another device (Jetson AGX Orin). Is it possible?Hi,Please refer to the following docs, which may help you.This is the revision history of the NVIDIA TensorRT 8.6 Developer Guide.This is the revision history of the NVIDIA TensorRT 8.6 Developer Guide.By default, TensorRT engines are only compatible with the type of device where they were built. With build-time configuration, engines can be built that are compatible with other types of devices. Currently, hardware compatibility is supported only for Ampere and later device architectures and is not supported on NVIDIA DRIVE OS or JetPack.Thank you.Powered by Discourse, best viewed with JavaScript enabled"
378,why-is-the-variable-nbins-in-compute-amax-entropy-determined-by-the-sign,"As I understand, HistogramCalibrator collects data distribution of each activation, and it is compressed into 2^bits bins for computing entropy.In the signed 8-bit setting, integer values among -128~127 can be used, and the number of candidate integers is 256.In the unsigned 8-bit setting, integer values among 0~255 can be used, and the number of candidate integers is also 256.So, I think nbins in _compute_amax_entropy should always be 256 regardless of the sign.However, the implementation in pytorch_quantization, nbins of signed 8-bit setting is half that of unsigned 8-bit setting.Can you inform me what I missed?Hi @dk.hong ,Apologies for the delay,Did you check the available doc?
ThanksPowered by Discourse, best viewed with JavaScript enabled"
379,riva-not-starting-up,"Hi to everyone,
I am trying to use riva on my jetson orin nano ( the 4gb module ), I have followed the quick start guide. However once i try to start via the “riva_start.sh” script, the associated riva_speech container stops as i cannot connect to a server on 127.0.0.1:8001.
So the scripts just waits until it stops.I also attached riva_speech container logs and config.sh.Thank you in advance for your help!
riva_speech.txt (96 Bytes)
config.sh (13.3 KB)HI @mattsaysThanks for your interest in RivaApologies you are facing issueRequest to kindly share details onPlease let me know if the above are set correct and still not workingThanksYes I am sure that I have the latest versions of Riva and JetPack and I also doublechecked the daemon.json option. However it still doesn’t work.Powered by Discourse, best viewed with JavaScript enabled"
380,load-csv-with-cudf-in-c,"Hi,
I have recently start using cudf for parsing csv file in c++. I’m able to build an example and get some info about the loaded data. However, I would like to specify data type for each column then extract the value at each index (i…n, j…m).Thanks,
DomMost RAPIDS requests are in python but the C++ interface is the Python impl foundation. This sample code is meant to be building blocks for your code already presented and it use generic auto data typing to skip past impl details of varing column data types. Being that your question was mainly about setting column dtypes, please look over this Issue which lists all the possible types, https://github.com/rapidsai/cudf/issues/1119 . For character strings see this more detailed developers guide for libcudf and string_scalar topic https://docs.rapids.ai/api/libcudf/stable/developer_guide | Use traits to set gdf_data elements and other typedefs · Issue #1119 · rapidsai/cudfOnce #892 is merged and after we move away from cffi in #599 , update the union gdf_data to use traits defined members. The code to be changed: typedef union { int8_t si08; /**< GDF_INT8 */ int16_t…github.com |#include <cudf/io/functions.hpp>
#include <cudf/io/csv.hpp>
#include <cudf/column/column_factories.hpp>
#include <cudf/types.hpp>std::unordered_map<std::string, std::vector<gdf_scalar>> read_csv_with_types(const std::string& filename, const std::unordered_map<std::string, gdf_dtype>& column_types)
{
// Create an empty unordered map to store the extracted data
std::unordered_map<std::string, std::vector<gdf_scalar>> data;// Read the CSV file using cuDF
cudf::io::csv_reader_options options = cudf::io::csv_reader_options::builder(cudf::io::source_info{filename});
for (const auto& column_type : column_types)
{
options.set_dtypes({column_type.first}, {column_type.second});
}
auto result = cudf::io::read_csv(options);// Iterate over each column in the result table
auto table = result.tbl;
for (cudf::size_type i = 0; i < table.num_columns(); ++i)
{
const auto& column = table.get_column(i);
const auto& column_name = column->name();
const auto& column_data = static_cast<const cudf::column_view&>(*column).begin<gdf_scalar>();// Store the column data in the unordered map
data[column_name] = std::vector<gdf_scalar>(column_data, column_data + column->size());
}return data;
}int main()
{
// Specify the filename of the CSV file
std::string filename = “your_file.csv”;// Specify the data types of each column
std::unordered_map<std::string, gdf_dtype> column_types;
column_types[“column1”] = GDF_INT32;
column_types[“column2”] = GDF_FLOAT64;
column_types[“column3”] = GDF_STRING;// Call the CSV file reader function
std::unordered_map<std::string, std::vector<gdf_scalar>> data = read_csv_with_types(filename, column_types);// Access the extracted data
for (const auto& column_data : data)
{
const std::string& column_name = column_data.first;
const std::vector<gdf_scalar>& column_values = column_data.second;std::cout << “Column '” << column_name << “’ data:” << std::endl;
for (const auto& scalar : column_values)
{
// Access scalar value using scalar.is_valid and scalar.data
// …
}
}return 0;
}Hi @bfurtaw,It looks like the gdf_data is not available in latest libcudf. However have done similar implementation using cudf::data_type but it gives segmentation fault when casting column value. Below code snippetSolved by copying data to host_vectorThis topic was automatically closed 14 days after the last reply. New replies are no longer allowed.Powered by Discourse, best viewed with JavaScript enabled"
381,tensorrt-faster-transformer-for-gpt2-mt-nlg-with-sparsity,"Hi,
Is NVIDIA working on TensorRT/Faster Transformer implementation for GPT2 or Other larger model e.g., Megatron-Turing Natural Language Generation model (MT-NLG) to support 2-4 Sparsity?As of now GitHub - NVIDIA/FasterTransformer: Transformer related optimization, including BERT, GPT states sparsity is available only for BERT and Encoder.Hi,Please refer to the following post,Thank you.Thank You,
As per the link the 2-4 Structured Sparsity is only for Megatron.
Is there any plan to have Sparsity for GPT2 6.7Billion model ?Hi,Currently, we are not sure about it, It may be available in future releases.Thank you.Hi， @spolisettyWhy NV doesn’t support Sparsity for onnx BERT model? Could you describe the reason? Thank you.Powered by Discourse, best viewed with JavaScript enabled"
382,mnistcudnn-test-failed,"Hello,
I am trying to run CUDA 10 toolkit + CUDNN 7.5.0 with RTX 3060 GPU.  I got totally stuck at CUDNN verifying.When I run $ ./mnistCUDNN i get:cudnnGetVersion() : 7500 , CUDNN_VERSION from cudnn.h : 7500 (7.5.0)
Host compiler version : GCC 6.5.0
There are 1 CUDA capable devices on your machine :
device 0 : sms 28  Capabilities 8.6, SmClock 1852.0 Mhz, MemSize (Mb) 12045, MemClock 7501.0 Mhz, Ecc=0, boardGroupID=0
Using device 0Testing single precision
Loading image data/one_28x28.pgm
Performing forward propagation …
Testing cudnnGetConvolutionForwardAlgorithm …
Fastest algorithm is Algo 0
Testing cudnnFindConvolutionForwardAlgorithm …
^^^^ CUDNN_STATUS_SUCCESS for Algo 1: 0.001024 time requiring 100 memory
^^^^ CUDNN_STATUS_SUCCESS for Algo 0: 0.009536 time requiring 0 memory
^^^^ CUDNN_STATUS_SUCCESS for Algo 4: 0.056032 time requiring 207360 memory
^^^^ CUDNN_STATUS_SUCCESS for Algo 2: 0.214880 time requiring 57600 memory
^^^^ CUDNN_STATUS_NOT_SUPPORTED for Algo 3: -1.000000 time requiring 0 memory
Resulting weights from Softmax:
0.0000000 0.9999399 0.0000000 0.0000000 0.0000561 0.0000000 0.0000012 0.0000017 0.0000010 0.0000000
Loading image data/three_28x28.pgm
Performing forward propagation …
Resulting weights from Softmax:
0.0000000 0.0000000 0.0000000 0.9999288 0.0000000 0.0000711 0.0000000 0.0000000 0.0000000 0.0000000
Loading image data/five_28x28.pgm
Performing forward propagation …
Resulting weights from Softmax:
0.0997550 0.0892810 0.1054468 0.1071846 0.0902156 0.1043498 0.0953814 0.0938049 0.1155469 0.0990339Result of classification: 1 3 8Test failed!
Prediction mismatch
mnistCUDNN.cpp:876
Aborting…Results of $ nvidia-smi :Thu Aug 26 23:58:30 2021
±----------------------------------------------------------------------------+
| NVIDIA-SMI 470.57.02    Driver Version: 470.57.02    CUDA Version: 11.4     |
|-------------------------------±---------------------±---------------------+
| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |
|                               |                      |               MIG M. |
|===============================+======================+======================|
|   0  NVIDIA GeForce …  Off  | 00000000:07:00.0  On |                  N/A |
|  0%   40C    P8    18W / 170W |    371MiB / 12045MiB |      2%      Default |
|                               |                      |                  N/A |
±------------------------------±---------------------±---------------------+±----------------------------------------------------------------------------+
| Processes:                                                                  |
|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |
|        ID   ID                                                   Usage      |
|=============================================================================|
|    0   N/A  N/A      1176      G   /usr/lib/xorg/Xorg                 26MiB |
|    0   N/A  N/A      1248      G   /usr/bin/gnome-shell               88MiB |
|    0   N/A  N/A      1514      G   /usr/lib/xorg/Xorg                146MiB |
|    0   N/A  N/A      1647      G   /usr/bin/gnome-shell               27MiB |
|    0   N/A  N/A      2042      G   …AAAAAAAAA= --shared-files       77MiB |
±----------------------------------------------------------------------------+And $ nvcc --version:
nvcc: NVIDIA (R) Cuda compiler driver
Copyright (c) 2005-2018 NVIDIA Corporation
Built on Sat_Aug_25_21:08:01_CDT_2018
Cuda compilation tools, release 10.0, V10.0.130Thank you in advance for any idea that might help. I reinstalled everything several times and followed official documentation. Maybe I have missed some compatibility issues, but hopefully not.Hi @HolJak ,
Can you plesae check if you have installed the driver properly. Use the method in the linux install guide. Perform all steps including verification, before attempting to do anything else (like install cudnn)Thanks!Thank you @AakankshaS for your answer. During installation of CUDA i verified with ./deviceQuery test with result = PASS. This morning tried again with just to be sure. Results here:./deviceQuery
./deviceQuery Starting…CUDA Device Query (Runtime API) version (CUDART static linking)Detected 1 CUDA Capable device(s)Device 0: “NVIDIA GeForce RTX 3060”
CUDA Driver Version / Runtime Version          11.4 / 10.0
CUDA Capability Major/Minor version number:    8.6
Total amount of global memory:                 12046 MBytes (12630884352 bytes)
MapSMtoCores for SM 8.6 is undefined.  Default to use 64 Cores/SM
MapSMtoCores for SM 8.6 is undefined.  Default to use 64 Cores/SM
(28) Multiprocessors, ( 64) CUDA Cores/MP:     1792 CUDA Cores
GPU Max Clock rate:                            1852 MHz (1.85 GHz)
Memory Clock rate:                             7501 Mhz
Memory Bus Width:                              192-bit
L2 Cache Size:                                 2359296 bytes
Maximum Texture Dimension Size (x,y,z)         1D=(131072), 2D=(131072, 65536), 3D=(16384, 16384, 16384)
Maximum Layered 1D Texture Size, (num) layers  1D=(32768), 2048 layers
Maximum Layered 2D Texture Size, (num) layers  2D=(32768, 32768), 2048 layers
Total amount of constant memory:               65536 bytes
Total amount of shared memory per block:       49152 bytes
Total number of registers available per block: 65536
Warp size:                                     32
Maximum number of threads per multiprocessor:  1536
Maximum number of threads per block:           1024
Max dimension size of a thread block (x,y,z): (1024, 1024, 64)
Max dimension size of a grid size    (x,y,z): (2147483647, 65535, 65535)
Maximum memory pitch:                          2147483647 bytes
Texture alignment:                             512 bytes
Concurrent copy and kernel execution:          Yes with 2 copy engine(s)
Run time limit on kernels:                     Yes
Integrated GPU sharing Host Memory:            No
Support host page-locked memory mapping:       Yes
Alignment requirement for Surfaces:            Yes
Device has ECC support:                        Disabled
Device supports Unified Addressing (UVA):      Yes
Device supports Compute Preemption:            Yes
Supports Cooperative Kernel Launch:            Yes
Supports MultiDevice Co-op Kernel Launch:      Yes
Device PCI Domain ID / Bus ID / location ID:   0 / 7 / 0
Compute Mode:
< Default (multiple host threads can use ::cudaSetDevice() with device simultaneously) >deviceQuery, CUDA Driver = CUDART, CUDA Driver Version = 11.4, CUDA Runtime Version = 10.0, NumDevs = 1
Result = PASSHi @HolJak ,
if possible, can you please share the API logs ?
Setting API logging:
https://docs.nvidia.com/deeplearning/sdk/cudnn-developer-guide/index.html#api-logging ThanksI encountered the exact same issue on WSL 2, where the predicted results were 1, 3, and 8, but the predictions were incorrect. This suggests that my cuDNN installation might have failed. Could you please share how you resolved it?Powered by Discourse, best viewed with JavaScript enabled"
383,ipluginv2-called-during-build-phase-and-building-failed,"I rewrote my old plugins from IPlugin class to IPluginV2 class so I can upgrade my tensorRT7 to tensorRT8.When I was running the usual pipeline (file parsing → engine building → inference), the process ended during building.After a bit more investigation, I found that some custom plugins were called during building which did not happen in tensorRT7.Can this be the reason why building failed? Is this normal for IPluginV2? Where do I start to debug?mainboard.log.INFO.20221017-160406.3087681 (1.9 MB)Jetpack 5.0
TRT: 8.4.0.9
GPU: Orin
CUDA: 11.4.166Hi,Looks like this post is a duplicate of [pluginV2Runner.cpp::execute::265] Error Code 2: Internal Error (Assertion status == kSTATUS_SCUESS failed. ).Thank you.Did you ever figure this out? I’m trying to do the same thing with the same error.Instead of using c++ API, it’s easier to convert model to onnx format first. I got around this issue by that.so you converted the caffe model into onnx first and then loaded it into tensorRT? I am also using apollo.Powered by Discourse, best viewed with JavaScript enabled"
384,cudnn-convolution-slow-since-8-4-0,"There are two  code snippets, run in 8 GPUS, windows 10  ,compiled by visual studio 2019.
just becasue large memory has been created in different location, the second code snippet is special slow.
first snippet:Running screenshot

Snipaste_2022-10-19_14-45-301394×586 78.7 KB

this running time is normal.Second snippet:Running screenshot

Snipaste_2022-10-19_14-50-00868×501 45 KB

this running time is more and more slow. it may be related to  zlib.
It  quickly occurred  to me that the program be running in 8gpus. this bug was disappeard in cudnn 8.2.2.my environment
cuda 11.4
cudnn 8.4.0
GPU: 8X RTX 3090
GPU driver:517.40-desktop-win10-win11-64bit-international-nsd-dch-whql
ram 768GB
visual studio 2019
windows 10if  my code snippets has bug , please  point out .
thanksLater, i use  cudaMallocHost  or windows api VirtualAlloc,VirtualLock, it is normal.
so it is related to page locked memory ?It is diffcult that i replaced all new  with cudaMallocHost,  spcieal, large  numbers of object and stl vector need to be created in heap.
this has already perplexed me for three months，please help me, thanks.Hi @ttt ,
We are checking on it, meanwhile can you please try out the following  and let us know the results?ThanksThe problem remainsHi @ttt , We are debugging this issue, and will get back to you as soon as possible.
Thank you for your patience.Hi @ttt
Apologies for delay.
We were not able to repro the issue.
However with the latest releases the problem should not persist and we recommend you to try the same.
ThanksPowered by Discourse, best viewed with JavaScript enabled"
385,can-riva-offline-models-be-used-for-language-recognition-with-a-microphone,"Hardware - AGX Orin
Hardware - CPU
Operating System：Ubuntu 20.04
Riva Version ：2.8.1Can RIVA offline models be used for language recognition with a microphone?
Are there any sample programs available for C++?HI @u9713112Thanks for your interest in RivaYes, of course, we can use RIVA offline models for language recognition, for ASR i.e speech to text,
For more details on ASR, please refer the below link
https://docs.nvidia.com/deeplearning/riva/user-guide/docs/asr/asr-overview.htmlOnce the Riva Model is deployed in your server, it can be access via C++ Client, Please find the repo belowSample C++ command-line Riva clients. Contribute to nvidia-riva/cpp-clients development by creating an account on GitHub.Hope this helpsThanksHI rvinobhaI saw the example of riva_asr_client from this website( GitHub - nvidia-riva/cpp-clients: Sample C++ command-line Riva clients.), but it can only perform speech recognition from a wav file. How can I modify it to use a microphone?ThanksHi @rvinobha
I have exact same question, that’s why pinging here,  But required in “PYTHON”Every thing is available, Deployed, Riva ready to use.But How can I use streaming service from a websocket to transcribe audio buffer in real time. (Not In device microphone, Not pre recorded file).Really looking for any help. Thank you.HI @u9713112 and @ahmedshahzadaaaThanks for your reply,I will further check with the team on how to live microphone instead of recorded filesThanks@rvinobha
I’m waiting for your message~ :)
If possible, could you provide me with a C++ example?Thanks@rvinobha Same waiting for you for python…
thanksHi @u9713112 and @ahmedshahzadaaaSincere Apologies for the delayFor live microphone use case using riva_streaming_asr_client with a streaming model would be the apt option, you can use --audio_device which will allow you to select the audio device to use.Thanksriva_streaming_asr_clientDoes the riva_asr_client example support the use of a microphone in non-streaming mode?
I want to implement riva asr functionality using a microphone without an internet connection.HI @u9713112Thanks for your interest in RivaI will check and confirm again with the teamThanksPowered by Discourse, best viewed with JavaScript enabled"
386,nchwtonhwc-kernel-launches,"Hello,
I have been profiling a network in Nsight-Compute and I noticed a lot of kernel launches with the name “nchwTonhwc”.
I assume this kernel is doing exactly what its name suggests: changing the order from NCHW to NHWC.
The input I give TensorRT is in NCHW format and I was under the impression that TensorRT also works with this format.
Can anyone explain what is happening?(PS: I use a lot of parametricReLU layers in the network. Maybe that is of any importance?)TensorRT Version:  7.0
GPU Type:  Quadro RTX 5000
Nvidia Driver Version: 440.40
CUDA Version:  10.2
Operating System + Version: Ubuntu 16.04a lot of kernel launches with the name “nchwTonhwc”TRT will choose the internal format of each layer, and find out the best one with best performance. This is the auto-tunning feature of TRT.Please refer below link for TRT supported data format:This is the revision history of the NVIDIA TensorRT 8.4 Developer Guide.Thanks@SunilJBCan we convert onnx model from NCHW to NHWC to prevent insertig reformat layer in TensorRT?Powered by Discourse, best viewed with JavaScript enabled"
387,issues-with-onnx-to-tensorrt-conversion-for-the-faster-r-cnn-mobilenet-v3-model,"Description:I am currently encountering difficulties when attempting to convert the Faster R-CNN Mobilenet V3 model from PyTorch to ONNX and subsequently to TensorRT. The PyTorch model I am using is the pretrained ‘fasterrcnn_mobilenet_v3_large_320_fpn’ model. The goal is to perform object detection in images.Firstly, I exported the model to ONNX format using the PyTorch ‘torch.onnx.export()’ function. The initial conversion from PyTorch to ONNX was successful, but I am having issues with the next stage: conversion from ONNX to TensorRT.While trying to convert the model to TensorRT format or while trying to infer with TensorRT provider in ONNX Runtime, I encounter the following error:To address this, I tried to run shape inference on the ONNX model using the onnx.shape_inference.infer_shapes() function. However, the issue persisted even after running shape inference.I suspect this issue could be due to certain operations in the Faster R-CNN model that may not be fully supported by TensorRT, but I would appreciate any guidance or insights on how to resolve this problem.I am currently using ONNX 1.10.1, PyTorch 1.11.0, and TensorRT 8.6.1 on a system with CUDA 11.5.Thank you in advance for your assistance.Hi,
Request you to share the ONNX model and the script if not shared already so that we can assist you better.
Alongside you can try few things:This NVIDIA TensorRT 8.6.1 Quick Start Guide is a starting point for developers who want to try out TensorRT SDK; specifically, this document demonstrates how to quickly construct an application to run inference on a TensorRT engine.import sys
import onnx
filename = yourONNXmodel
model = onnx.load(filename)
onnx.checker.check_model(model).
2) Try running your model with trtexec command.master/samples/trtexecNVIDIA® TensorRT™, an SDK for high-performance deep learning inference, includes a deep learning inference optimizer and runtime that delivers low latency and high throughput for inference applicat...
In case you are still facing issue, request you to share the trtexec “”–verbose"""" log for further debugging
Thanks!fasterrcnn_mobilenet_v3_large_320_fpn.onnx (74.2 MB)
output.txt (1.1 MB)Uploaded files aree the onnx model I use and the log from trtexec, based on pretrained model fasterrcnn_mobilenet_v3_large_320_fpn from Pytorch.As for the check_model.py, I ran it and nothing shows or any error raised.Thank you!Hi,Sorry for the delayed response
When we tried using the trtexec observed the following error.It looks like there is some problem with the model definition. When dealing with if statement, you have to make sure that both true and false block outputs have the same shape for every output.Thank you.Is it currently possible to export the model torchvision.models.detection.fasterrcnn_mobilenet_v3_large_320_fpn to tensorRT, or are there any difficulties? If so, how can it be done using the file. trtexecPowered by Discourse, best viewed with JavaScript enabled"
388,what-causes-the-deserializecudaengine-fail-and-how-to-get-the-error-message,"Env: RTX 3090, Cuda 11.4, TensorRT 8.2.3, Ubuntu 18.04**, Docker 19.03We have 2 TRT models, which both sizes are more than 200MB. When our C++ application loads these 2 models into GPU by deserializeCudaEngine(), everything is fine at first. After some time, we try to optimize out application, and we change some C++ code run on CPU, then one model is randomly loaded failed (The returned engine pointer is null). How can I know what causes the deserialization fail? How to get the error message? I just guess that might relate to the memory. Refer to the below issue:We try to use Google sanitizer to check the memory leak. At first with these 2 models, we can’t even start the application. Every time it is failed at deserializeCudaEngine(). Then we change the model file to another version with size around 120MB. That works and we can run the application to check the memory leak.How can we know how much the host/GPU memory used by the model is?Hi,The following few reasons could cause the deserializeCudaEngine() function to fail:You need to set the TRT logging to VERBOSE or DEBUG in order to get the error messages. Then the error messages and details about memory usage can be discovered in the logs.You can also use the TRT profiler to know how much host or GPU memory is used by the model.
Please refer Developer Guide :: NVIDIA Deep Learning TensorRT Documentation for more info.Thank you.So, where can I find the logs? Is it in the log file or any output? stdout or stderr?You can find the output logs in the console (STDOUT).@spolisetty , Thanks. Besides, does the trtexec --workspace number affect the trt inferencing GPU memory usage? Or the workspace size only affect the trt conversion?trtexec --workspace option can affect the both TRT inferencing GPU memory usage and TRT conversion.
TensorRT uses the workspace memory to store the intermediate results and to perform optimizations.@spolisetty So, if I convert an ONNX model to trt engine by trtexec --workspace 20480, then when I use this trt in a c++ applicaiton, it will cost more GPU memory than the trt engine by trtexec --workspace 1024, right? Can trtexec report the GPU memeory usage for trt inferencing if I use trtexec to run the trt inference?Hi,Sorry if my previous response did not convey clear information to you.
The amount of memory used for inference in a C++ application is not directly affected due to the workspace size specified during engine building using trtexec. The workspace size only impacts the temporary GPU memory that TensorRT uses when building engines.If we use the trtexec tool for both engine building and inference, then the workspace option will affect both, as I previously mentioned.The Trtexec tool logs report higher-level summaries. You can also use profiling tools to get detailed information about GPU memory usage.This is the revision history of the NVIDIA TensorRT 8.6 Developer Guide.Thank you.@spolisetty Thanks for your clarification.This topic was automatically closed 14 days after the last reply. New replies are no longer allowed.Powered by Discourse, best viewed with JavaScript enabled"
389,are-int8-calibration-cache-files-platform-independent,"If I create a int8 calibration cache on one machine (x86), can I use it on another machine to build the TRT engine (arm AGX)?I am creating the calib file using polygraphy:Then building the TRT engine:Or will this affect performance?Hi, Please refer to the below links to perform inference in INT8//github.com/NVIDIA/TensorRT/tree/master/samples/sampleINT81777.21 KB
Thanks!Hi,Also, please refer to the developer guide below, which may help you.This is the revision history of the NVIDIA TensorRT 8.6 Developer Guide.The calibration cache data is portable across different devices as long as the calibration happens before layer fusion. Specifically, the calibration cache is portable when using the IInt8EntropyCalibrator2 or IInt8MinMaxCalibrator calibrators, or when QuantizationFlag::kCALIBRATE_BEFORE_FUSION is set. This can simplify the workflow, for example by building the calibration table on a machine with a discrete GPU and then reusing it on an embedded platform. Fusions are not guaranteed to be the same across platforms or devices, so calibrating after layer fusion may not result in a portable calibration cache. The calibration cache is in general not portable across TensorRT releases.Thank you.Powered by Discourse, best viewed with JavaScript enabled"
390,how-to-use-dla-int8-i-o-reformatting,"Hi,We have a model with FP32 inputs and outputs, and want to run it on DLA in INT8 mode.If we ran it on GPU, TensorRT would insert reformating layers FP32 → INT8 at the input, and INT8 → FP32 at the output. However, this is not done on DLA, and the client is responsible for doing the appropriate conversion.How is this done in practice? The conversion FP32 → INT8 requires appropriate scale factors. Where are they obtained from? Should they be obtained from the Calibration Table that comes out of the engine build process? If not, is there a “standard practice” on how to do it?Thanks!Hi, Please refer to the below links to perform inference in INT8master/samples/sampleINT8NVIDIA® TensorRT™, an SDK for high-performance deep learning inference, includes a deep learning inference optimizer and runtime that delivers low latency and high throughput for inference applicat...1777.21 KB
Thanks!Powered by Discourse, best viewed with JavaScript enabled"
391,marshalling-interop-output-from-maxine,"so I’m trying to wrap the Maxine libraries from C# to make use of the expression featureI’ve tested all samples and they work just fine and my code seems to load and run just fine. I’ve confirmed with a reverse transfer that my image data appears to be flowing through correctly so I’m assuming the library is seeing my images and runningI’m just stuck on reading the output data out at the end. no matter how I implement my marshalling code, the data keeps reading zero. I’ve tried several marshalling methods so at this point I can’t tell if it’s my interop going wrong or if I’m perhaps missing something about how I’m using the librarymethods where I run the librarymy struct definitionsmy current marshalling method is copying to and from IntPtrs, like soand my API prototype classthis is currently running but only outputting zero across the boardI’ve tried a number of things, including using the API’s Get functions to try to poll the output data structures at run time but that always results in crashes for me and I’m unclear if that’s even supported
I’ve also tried what other people are doing in some of the existing github projects to wrap the Maxine libraries but nobody’s wrapped the AR module yet and trying to transfer what they’ve done with the Audio/Video modules didn’t work for meany indications if the problem is with how I’m using the library or with the interop would be great. I’ve tried enough things now at this point that I can’t be sure
it especially isn’t making sense to me because the SetF32Array should be even simpler, in that I expect default marshalling behavior to handle it perfectly but I still don’t get any output in those arraysPowered by Discourse, best viewed with JavaScript enabled"
392,serialization-error-in-verifyheader-0-crc-32-checksum-does-not-match-value-in-archive,"I have a running program, that usually works fine.  But sometimes after restarting it fails with logs below:terminate called after throwing an instance of ‘char const*’
ERROR: INVALID_CONFIG: Deserialize the cuda engine failed.
ERROR: INVALID_STATE: std::exception
ERROR: …/rtSafe/coreReadArchive.cpp (63) - Serialization Error in verifyHeader: 0 (CRC-32 checksum does not match value in archive).Then it can start again, without any issues. This happens not so often. From logs I see, that it happens in one of the functions below. In the first function I write an engine to a file, and in the second function I read it and use it further. What can cause this problem?1 - function
void serializeEngine(ICudaEngine* engine, const string& engineFilename) {
ofstream engineFile(engineFilename, ios::binary);
unique_ptr<IHostMemory, TensorRtDeleter> trtModelStream{engine->serialize(), TensorRtDeleter()};
engineFile.write((char*) trtModelStream->data(), trtModelStream->size());
if (engineFile.bad())
cerr << “Char_const. SerializeEngine: I/O error while write” << endl;
else if (engineFile.eof())
cerr << “Char_const. SerializeEngine: End of file reached successfully” << endl;
else if (engineFile.fail())
cerr << “Char_const. SerializeEngine: Non-integer data encountered” << endl;
else
cout << “Char_const. SerializeEngine: “<< trtModelStream->size() <<” bytes were written to enginefile” << endl;
}2 - function
ICudaEngine* readEngine(const string& engineFilename) {
ifstream engineFile(engineFilename);}TensorRT Version: 6.0.1.5
GPU Type: GeForce GTX 1050 Ti
Nvidia Driver Version: 455.23.05
CUDA Version: 11.1
CUDNN Version: 7.6.5.32
Operating System + Version: cudagl:10.1-devel-ubuntu16.04
Python Version (if applicable): -
TensorFlow Version (if applicable): -
PyTorch Version (if applicable): -
Baremetal or Container (if container which image + tag):-The error message indicates that the program failed to deserialize the CUDA engine. This can occur when the data being read from the file is not in the correct format or is corrupt.Please check if there are any memory allocation errors when creating or deserializing the engine. If it fails, please make sure the serialization of the engine works fine, or try to generate the engine again.Powered by Discourse, best viewed with JavaScript enabled"
393,freeimage-is-not-set-up-correctly-please-ensure-freeimae-is-set-up-correctly,"Hi, I followed the steps in:Installation Guide :: NVIDIA Deep Learning cuDNN Documentation to set up cudnn. Everything seems to be working fine but when I executed “make clean && make”, I got the above warning and no mnistCUDNN was created.Somebody suggested setting the CUDA_PATH using: export CUDA_PATH=/usr/local.cuda-9.0but it does not work. I also tried:sudo ln -s /usr/local/cuda-9.0/ /usr/local/cuda-9.0but same issue. Then, I tried:sudo ln -s /usr/local/cuda-9.0/ /usr/local/cudaAgain, same issue.I also executed the following line as somebody mentioned:cat /usr/include/x86_64-linux-gnu/cudnn_v7.h | grep CUDNN_MAJOR -A 2but I still get:make clean && make
rm -rf *o
rm -rf mnistCUDNN
/usr/bin/ld: -lcublas cannot be found
collect2: error: ld returned 1 exit statusWARNING - FreeImage is not set up correctly. Please ensure FreeImage is set up correctly. <<<
:
:
:Doing “sudo dpkg -i libcudnn7*.deb” and repeat Sec 2.4 Verifying cuDNN is installed resulted in the same error.Executing: nvcc --versionleds to ""Command ‘nvcc’ not found, but can be installed with:sudo apt install nvidia-cuda-toolkit""Not sure if I need to do that.Could you please help to resolve the issue?Also, I am concerned that I might set the wrong path and make the wrong symbolic links. Please let me know how to undo the above three lines of command that don’t work.I am using Pop OS which somebody said is basically ubuntu. Not sure if this is true.Thanks.Assuming you are using cuda 9.  In Ubuntu I put these exports in my .profile.  If your .profile is empty then this is not the place to put it.  You will have to figure that out yourself.  This is in the cuda documentation. You will want to leave out the numbers at the beginning.It might also be due to some 3rd party packages. In the cuda documentation it says to get these if you want to use the samples.I also did a quick google search and saw on the third link something about “How To Install and Use FreeImage” it gave a library. So if the other things didn’t work try using the packages below.If that doesn’t work use the Ubuntu that is supported with your cuda and cudnn version and do the things I put above. You probably don’t need to do the last one, though.I also did a quick google search and saw on the third link something about “How To Install and Use FreeImage” it gave a library. So if the other things didn’t work try using the packages below.If that doesn’t work use the Ubuntu that is supported with your cuda and cudnn version and do the things I put above. You probably don’t need to do the last one, though.I ran into the same issue and this fixed it for meSame here, this fixed it for me!This still does not work for me.  I have followed these instructions but continue to get the same error.It seems that the above-mentioned two commands have not made sense to me. It still has the persistent error as follows.Environment:
Ubuntu 18.04
Driver: 440.110 ppa
CUDA Toolkit 10.2
cuDNN 7.6.5/bin/sh: 1: cannot create test.c: Permission denied
/bin/sh: 1: cannot create test.c: Permission denied
g++: error: test.c: No such file or directory
g++: warning: ‘-x c’ after last input file has no effect
g++: fatal error: no input files
compilation terminated.WARNING - FreeImage is not set up correctly. Please ensure FreeImage is set up correctly. <<<It is quite strange. After I re-install the Ubuntu System, it has generated the error.I reinstalled Ubuntu 18.04 system,  Official Nvidia Driver 440.33, CUDA  Toolkit 10.2 and cuDNN 7.6.5.  It is quite strange I  got Test passed! after I added the super user command “sudo -i”Test passed!Ubunt ppa only allows 440.100 Driver. I had personally thought that it is the incompatibility between 440.100 Driver and CUDA Toollkit 10.2 that indicates the 440.33.01 driver with its detail listed as follows.cuda-repo-ubuntu1804-10-2-local-10.2.89-440.33.01_1.0-1_amd64.debHowever, it still had the error with no inputting “sudo -i”. It does not make sense for me to install  libfreeimage3 libfreeimage-dev in the new installed environment. What’s wrong with the error in the condition of no-inputting “sudo -i” command?My test of mnistCUDNN is finally passed. It is quite strange that the terminal sometimes pops up the requirement of sudo. After adding sudo before the command, it fails. However, the terminal does not require the permission of sudo this time. So it is ok for me right now.By the way, It is not necessary for me to install libfreeimage3 libfreeimage-dev with any scenarios.Test passed!Cheershello, running in same problem with cudnn_samples_v8. Chmod mnistCUDNN has no effect.
Ubuntu 18.04, Nvidia GTX650-TI-boost, Cuda Toolkit 11, 450.66 driversThx in advance for any helpsudo apt-get install libfreeimage3 libfreeimage-devThis solved my issue as well. Thanks @KingDudmanSo I followed the above suggestions and did the following -I even modified my paths:But this didn’t solve it for me.I have initially installed cuDNN via Tar file installation (see steps 2.3.1 in Installation Guide :: NVIDIA Deep Learning cuDNN Documentation ) so after apt-get install and path modification, I did the Debian installation (see steps 2.3.2) which also includes steps to download samples.This is helped solved the problem for me!I ran into the same issue and this fixed it for meThis fixed my issue also, Thanks!you are a saver manHi, I came into the same problem, and I tried to install libfreeimage but it doesn’t fix the problem. Also tried restart, same.I am installing cudnn on a freshly installed ubuntu 20.04 system, with nvidia driver 470.74 and cuda 10.1. ‘nvidia-smi’ and ‘nvcc --version’ both give correct results. When I run ‘make clean && make’ inside cudnn_samples_v8/mnistCUDNN folder, it gives error in linking as follows:rm -rf *o
rm -rf mnistCUDNN
Linking agains cublasLt = true
CUDA VERSION: 10010
TARGET ARCH: x86_64
HOST_ARCH: x86_64
TARGET OS: linux
SMS: 35 50 53 60 61 62 70 72 75
/usr/bin/ld: cannot find -lcublasLt
/usr/bin/ld: cannot find -lcublas
collect2: error: ld returned 1 exit statusWARNING - FreeImage is not set up correctly. Please ensure FreeImage is set up correctly. <<<
[@] /usr/local/cuda/bin/nvcc -ccbin g++ -I/usr/local/cuda/include -I/usr/local/cuda/targets/ppc64le-linux/include -IFreeImage/include -m64 -gencode arch=compute_35,code=sm_35 -gencode arch=compute_50,code=sm_50 -gencode arch=compute_53,code=sm_53 -gencode arch=compute_60,code=sm_60 -gencode arch=compute_61,code=sm_61 -gencode arch=compute_62,code=sm_62 -gencode arch=compute_70,code=sm_70 -gencode arch=compute_72,code=sm_72 -gencode arch=compute_75,code=sm_75 -gencode arch=compute_75,code=compute_75 -o fp16_dev.o -c fp16_dev.cu
[@] g++ -I/usr/local/cuda/include -I/usr/local/cuda/targets/ppc64le-linux/include -IFreeImage/include -o fp16_emu.o -c fp16_emu.cpp
[@] g++ -I/usr/local/cuda/include -I/usr/local/cuda/targets/ppc64le-linux/include -IFreeImage/include -o mnistCUDNN.o -c mnistCUDNN.cpp
[@] /usr/local/cuda/bin/nvcc -ccbin g++ -m64 -gencode arch=compute_35,code=sm_35 -gencode arch=compute_50,code=sm_50 -gencode arch=compute_53,code=sm_53 -gencode arch=compute_60,code=sm_60 -gencode arch=compute_61,code=sm_61 -gencode arch=compute_62,code=sm_62 -gencode arch=compute_70,code=sm_70 -gencode arch=compute_72,code=sm_72 -gencode arch=compute_75,code=sm_75 -gencode arch=compute_75,code=compute_75 -o mnistCUDNN fp16_dev.o fp16_emu.o mnistCUDNN.o -I/usr/local/cuda/include -I/usr/local/cuda/targets/ppc64le-linux/include -IFreeImage/include -L/usr/local/cuda/lib64 -L/usr/local/cuda/targets/ppc64le-linux/lib -lcublasLt -LFreeImage/lib/linux/x86_64 -LFreeImage/lib/linux -lcudart -lcublas -lcudnn -lfreeimage -lstdc++ -lmI also tried both cudnn version 7.6.5 and 8.0.5 with cuda10.1, and they gave the same error. Any hints?Hey… Thank you so much.
You saved me from so much trouble!If all else fails, you can try compiling from source.~/cudnn_samples_v8/mnistCUDNN$ less readme.txt
(you could use any reader to open it, I just used ‘less’ as an example)There will be another readme for your distro in the FreeImage directory after you unzip the download. “readme.<your_distro>”
In Linux I still have the FreeImage directory in my downloads folder and it found it.  That, I’m not sure why if found it, but it passed the test.
I moved it to my $HOME folder from Downloads and it’s still finding it.  So idk about pathing, but it’s finding it somehow.
Ubuntu 20.04This did work for me too.sudo apt-get install libfreeimage3 libfreeimage-devthis  didn’t work for me, and I got this error
Err:1 Index of /ubuntu focal/universe amd64 libfreeimage3 amd64 3.18.0+ds2-1ubuntu3
Connection failed [IP: 185.125.190.39 80]if anyone is facing this error after installing libfreeimage3sudo apt-get install libfreeimage3 libfreeimage-devmake sure to install the libcudnn-deveg.sudo apt-get install libcudnn8-devworked for me.Powered by Discourse, best viewed with JavaScript enabled"
394,migration-from-os2s-decoder-to-flashlight,"Hello,I am currently using RIVA 2.10.0 and attempting to migrate my ASR model from OpenSeq2Seq decoder to Flashlight due to future deprecation. I am currently using the Nemo Conformer large as the acoustic model, which is fine-tuned on my data. Additionally, I use KenLM as the language model, which was trained using this script provided by NeMo.However, the OpenSeq2Seq decoder does not support BPE tokens, so subwords are mapped to chars and KenLM is trained on these chars. Also I want to use a lexicon-free decoder because my word-level language models perform worse in comparison to subword models.I have attempted to migrate directly by running the riva-build script with parameters --decoder_type=flashlight and --flashlight_decoder.use_lexicon_free_decoding=True. I also replaced the riva_decoder_vocabulary.txt in the model repository with the encoded version. However, regardless of the decoder settings, transcriptions contain anything other than what sounded in the audio.My question is: Does a correct way to make migration exist? Alternatively, is it possible to train the LM and set up RIVA to work with subwords without encoding using a lexicon-free decoder?Hi @A.AbugalievThanks for your interest in RivaI will check with the team further regarding this query and get backThanksHello @rvinobha Do you have any news about my question?Hi @A.AbugalievMy Sincere Apologies for the delay in my part,I will check soon and provide updatesThanksPowered by Discourse, best viewed with JavaScript enabled"
395,tensorrt-8-2-1-convert-lstm-model-failed,"When I’m using the latest TensorRT (8.2.1) to convert a OCR model, an error happens in myelin assertion code. After some digging up, I’ve found that the problem occurs from the LSTM operator in the model. The error message:TensorRT Version:  8.2.1.8
GPU Type: Tesla T4
Nvidia Driver Version:  440.33.01
CUDA Version:  10.2
CUDNN Version:  8.2.1
Operating System + Version: CentOS 7
Python Version (if applicable):  3.7
TensorFlow Version (if applicable):
PyTorch Version (if applicable):  1.7.0
Baremetal or Container (if container which image + tag):Exported model lstm.onnx (99.0 KB)Full error log convert.log (15.5 KB)Minimal steps to reproduce the bug:Hi,
Request you to share the ONNX model and the script if not shared already so that we can assist you better.
Alongside you can try few things:This NVIDIA TensorRT 8.4.3 Quick Start Guide is a starting point for developers who want to try out TensorRT SDK; specifically, this document demonstrates how to quickly construct an application to run inference on a TensorRT engine.import sys
import onnx
filename = yourONNXmodel
model = onnx.load(filename)
onnx.checker.check_model(model).
2) Try running your model with trtexec command.
https://github.com/NVIDIA/TensorRT/tree/master/samples/opensource/trtexec
In case you are still facing issue, request you to share the trtexec “”–verbose"""" log for further debugging
Thanks!onnx checkerThe model holds with onnx checker. Actually, the same model file is converted successfully using TensorRT 7.1.3.4.Run with trtexec commandI run with the command: trtexec --onnx=lstm.onnx --verbose, the error log is almost the same as the convert.log attached above.@NVES Any updates?get the same log with youManaged to workaround the problem by changing onnx model:
image1136×1286 94.1 KB
Awesome, that works! Thank you!AwesomePowered by Discourse, best viewed with JavaScript enabled"
396,riva-installation,"Please provide the following information when requesting support.Hardware - GPU (A100/A30/T4/V100)
Hardware - CPU
Operating System
Riva Version
TLT Version (if relevant)
How to reproduce the issue ? (This is for errors. Please share the command and the detailed log here)
nexbie and frech if you can detail all the commans to install riva ,thanks and please!HI @exaflo16Thanks for your interest in RivaPlease use the below Quick Start Guide to install your setuphttps://docs.nvidia.com/deeplearning/riva/user-guide/docs/quick-start-guide.html#quick-start-guideThanksPowered by Discourse, best viewed with JavaScript enabled"
397,credit-card-fraud-detection-using-xgboost-smote-and-threshold-moving,"
Click the image to read the article
Find more #DSotD postsHave an idea you would like to see featured here on the Data Science of the Day?It’s definitely an important issue that financial institutions face. Thanks for sharing your insights on fraud detection.I stumbled upon this post and I just had to comment because credit card fraud is such a pervasive problem these days. It’s great to see people like you using machine learning techniques to tackle this issue head-on. Your approach of using XGBoost, SMOTE, and threshold moving is definitely interesting. I’m not too familiar with the technical aspects of it, but it sounds like a well-thought-out strategy. I am cautious about the choice of payments. And I very often refill my account at vclub. So far there have been no cases of cheating for me. It’s a convenient way to make transactions and has been gaining popularity in recent years.Powered by Discourse, best viewed with JavaScript enabled"
398,conversion-fails-with-weightsptr-h-144-error-code-2-internal-error-assertion-count-0-failed,"While converting one of our models to TensorRT, it fails with the following error:TensorRT Version: 8.6.0
GPU Type: 4090
Nvidia Driver Version: 520.61.05
CUDA Version: 11.8
CUDNN Version: 8.6.0
Operating System + Version: Ubuntu 22.04
Python Version (if applicable): 3.8
TensorFlow Version (if applicable):
PyTorch Version (if applicable):
Baremetal or Container (if container which image + tag):Attached is a script that will generate a minimal ONNX model which passes the model checker, but fails to convert to TensorRT and produces the above error.main.py (1.3 KB)Hi,
Request you to share the ONNX model and the script if not shared already so that we can assist you better.
Alongside you can try few things:This NVIDIA TensorRT 8.6.0 Early Access (EA) Quick Start Guide is a starting point for developers who want to try out TensorRT SDK; specifically, this document demonstrates how to quickly construct an application to run inference on a TensorRT...import sys
import onnx
filename = yourONNXmodel
model = onnx.load(filename)
onnx.checker.check_model(model).
2) Try running your model with trtexec command.master/samples/trtexecNVIDIA® TensorRT™, an SDK for high-performance deep learning inference, includes a deep learning inference optimizer and runtime that delivers low latency and high throughput for inference applicat...
In case you are still facing issue, request you to share the trtexec “”–verbose"""" log for further debugging
Thanks!Is there a better place to report TensorRT bugs?Powered by Discourse, best viewed with JavaScript enabled"
399,convert-int8-onnx-model-to-trt-engine,"I tried to convert model.int8.onnx to tensorRT engine, but encountered error…The model was already quantized in onnxRT framework and it has Quantized/Dequantized linear layer, whuch are supported by tensorRT accoring to this page (onnx-tensorrt/operators.md at main · onnx/onnx-tensorrt · GitHub)I used command:
./trtexec --onnx=test.int8-onnx-calibrated.onnx --saveEngine=engine.trtI thoght that it could be converted … But errors appeared, which are following.It’s not possible to convert int8-onnx model to trt engine?
Best regards.A clear and concise description of the bug or issue.TensorRT Version:
GPU Type:
Nvidia Driver Version:
CUDA Version:
CUDNN Version:
Operating System + Version:
Python Version (if applicable):
TensorFlow Version (if applicable):
PyTorch Version (if applicable):
Baremetal or Container (if container which image + tag):Please attach or include links to any models, data, files, or scripts necessary to reproduce your issue. (Github repo, Google Drive, Dropbox, etc.)Please include:Hi,We recommend that you try the latest TensorRT version 8.5.3 and please share with us complete verbose logs and issue repro ONNX model if you still face the issue.Thank you.I think it also encountered the same issuePlease share the issue repro ONNX model with us here or via DM for better debugging.Thank you.The suspicious reason is that conv has three input operators: input, weghit, bias in my onnx model.However, trtexec assumes two input channels: input, weight+bias(?). When using TensorRT/tools/pytorch-quantization at main · NVIDIA/TensorRT · GitHub and quantize the model, a generated onnx model has 2 input and it seems to be possible to convert to trt.engine via trtexec.No support of convirsion of “onnx-int8” model to trt.engine?No methods?Hi @Chubby ,
Are you still facing the issue?ThanksPowered by Discourse, best viewed with JavaScript enabled"
400,server-error-cuopt,"Hi,
When I try the following
sudo docker run --network=host -it --gpus all --rm nvcr.io/nvidia/cuopt/cuopt:22.12I get the following error:PS: Even with Python SDK, I have the same kind of issue.Best,
NaveenWhat kind of request are you sending it to the server?And are you running on WSL?the apis are available in http://0.0.0.0:5000/cuopt/for docs : http://0.0.0.0:5000/cuopt/redoc or http://0.0.0.0:5000/cuopt/docsFor examples on how to form the request data follow examples in cuOpt-Resources/notebooks/routing/microservice at branch-22.12 · NVIDIA/cuOpt-Resources · GitHubMake sure the request is goinf as HTTP and not HTTPS.I just started with cuOpt and run the first command mentioned under NVIDIA CUOPT CONTAINER Setup — cuOpt 22.12.00 documentation … So, I haven’t sent any request.
However, Jupyter notebook works fine as I can open it in the browser and run.May be then something trying to ping port 5000 and hitting this server, since request is not proper it is just giving error.Agree with above, it looks to me like the request is just wrong.  Here is my output, and the two commands that I sent. As you can see I got the 404 from the second request and the log message matches.If you go here on github, there are utility scripts for running cuopt and in the cuopt-api subdirectory there is a sample program for posting a problem to the server.Update, fixed:
Actually one mistake in post-cuopt.py which I will fix, line 101 should use ""set_cost_matrix’ instead of ‘add_cost-matrix’branch-22.12/docker-utilities/utilitiesA collection of NVIDIA cuOpt samples and other resources - cuOpt-Resources/docker-utilities/utilities at branch-22.12 · NVIDIA/cuOpt-ResourcesPowered by Discourse, best viewed with JavaScript enabled"
401,cudnnconvolutionforward-returns-cudnn-status-execution-failed-when-memory-is-low,"cudnnConvolutionForward() returns CUDNN_STATUS_EXECUTION_FAILED when the available memory on GPU is low, but we have preallocated the needed workspace and passed the pointer into it. What’s the reason why it still needs additional memory on GPU? Is it possible to make it only use the workspace?Hi @shshao ,
We suggest not to pre allocate all available GPU memory. CUDA operations such as loading modules and launching kernels may allocate memory under the hood and may fail if insufficient.ThanksPowered by Discourse, best viewed with JavaScript enabled"
402,riva-deployement-error-with-custom-ngram-langugae-model-and-custom-acoustic-model,"Hello,I have been facing this issue when I try to deploy riva with custom n-gram language model and acoustic model.Hardware - NVIDIA RTX A5000
Hardware - GPU
Operating System: Ubuntu: 20.04.3 LTS
Riva Version 2.9.0.I0607 12:05:02.027920 108 ctc-decoder-library.cc:23] TRITONBACKEND_ModelInstanceInitialize: offline_riva_ngram_lm_pipeline-ctc-decoder-cpu-streaming-offline_0 (device 0)
terminate called after throwing an instance of ‘std::invalid_argument’
what():  Unknown entry in dictionary: ‘<’
/opt/riva/bin/start-riva: line 4:   108 Aborted                 (core dumped) ${CUSTOM_TRITON_ENV} tritonserver --log-verbose=0 --strict-model-config=true $model_repos --cuda-memory-pool-byte-size=0:1000000000Powered by Discourse, best viewed with JavaScript enabled"
403,how-to-install-and-use-trtexec-in-google-colab-notebook,"Hi all, I tried installing the tensorrt in google colab and succeeded. But when tried using trtexec it is saying
/bin/bash: trtexec: command not foundLet me know how to install it.TensorRT Version:
GPU Type:
Nvidia Driver Version:
CUDA Version:
CUDNN Version:
Operating System + Version:
Python Version (if applicable):
TensorFlow Version (if applicable):
PyTorch Version (if applicable):
Baremetal or Container (if container which image + tag):Please attach or include links to any models, data, files, or scripts necessary to reproduce your issue. (Github repo, Google Drive, Dropbox, etc.)Please include:Hi,
Please refer to the installation steps from the below link if in case you are missing on anythingThis NVIDIA TensorRT 8.6.1 Installation Guide provides the installation requirements, a list of what is included in the TensorRT package, and step-by-step instructions for installing TensorRT.
Also, we suggest you to use TRT NGC containers to avoid any system dependency related issues.NVIDIA TensorRT is a C++ library that facilitates high-performance inference on NVIDIA graphics processing units (GPUs). TensorRT takes a trained network and produces a highly optimized runtime engine that performs inference for that network.Thanks!Powered by Discourse, best viewed with JavaScript enabled"
404,onnxruntime-in-docker,"I’m trying to use Onnxruntime inside a Docker container. The base image is l4t-r32 (from docker hub /r/stereolabs/zed/, Cuda 10.2) and so I installed onnxruntime 1.6.0 using binaries from Jetson Zoo. However, when trying to import onnxruntime, I get the following error:ImportError: cannot import name ‘get_all_providers’I also tried with onnxruntime 1.10.0, it installs and imports fine but when I run rt.get_device() it says GPU and rt.get_available_providers() returns ['TensorrtExecutionProvider', 'CUDAExecutionProvider', 'CPUExecutionProvider']. However, when I create an inference session, session.get_providers() returns only CPUExecutionProvider.My best guess is that the issue has something to do with cuDNN, but from my understanding the base image of l4t-r32.4 should come with cuDNN installed?TensorRT Version: None. Can it be installed in an arm64 container?
CUDA Version: 10.2
CUDNN Version:
Operating System + Version: Ubuntu 18.04
Python Version (if applicable): 3.6
Baremetal or Container (if container which image + tag): 3.8-py-devel-l4t-r32.4Hi,
Request you to share the ONNX model and the script if not shared already so that we can assist you better.
Alongside you can try few things:This NVIDIA TensorRT 8.6.0 Early Access (EA) Quick Start Guide is a starting point for developers who want to try out TensorRT SDK; specifically, this document demonstrates how to quickly construct an application to run inference on a TensorRT...import sys
import onnx
filename = yourONNXmodel
model = onnx.load(filename)
onnx.checker.check_model(model).
2) Try running your model with trtexec command.master/samples/trtexecNVIDIA® TensorRT™, an SDK for high-performance deep learning inference, includes a deep learning inference optimizer and runtime that delivers low latency and high throughput for inference applicat...
In case you are still facing issue, request you to share the trtexec “”–verbose"""" log for further debugging
Thanks!Hi,My ONNX model works fine (with onnxruntime GPU inference) on my host machine, it’s only in the docker container that it is restricted to CPU EP. So unfortunately I don’t think that sharing the model will help. As for scripts, when using onnxruntime version 1.6 (as at should be compatible with Cuda 10.2) the code breaks atwith error code as stated above, not much of a script unfortunately. When using onnxruntime version 1.10, a script to reproduce the error would beI’m assuming that the latter breaks because onnxruntime 1.10 is not the correct version for Cuda 10.2. However, I don’t know how to fix the ImportError when using 1.6.As for trtexec/TensorRT, the docker environment does not have TensorRT and all the binaries to install it appear to be for amd64, while the container is of arm64.Powered by Discourse, best viewed with JavaScript enabled"
405,performance-of-qat-yolov7-model-is-worse,"I follow this guide https://github.com/NVIDIA-AI-IOT/yolo_deepstream/tree/main/yolov7_qat to do QAT for YOLOv7 model. mAP is good, but inference time from profiling is bad. Inference time of int8 QAT engine is 2 times larger than inference time of int8 engine when using calibration with TRT API.
Could you give me some suggestion?Hi,Request you to share the model, script, profiler, and performance output if not shared already so that we can help you better.Alternatively, you can try running your model with trtexec command.//github.com/NVIDIA/TensorRT/tree/master/samples/trtexecWhile measuring the model performance, make sure you consider the latency and throughput of the network inference, excluding the data pre and post-processing overhead.
Please refer to the below links for more details:This Best Practices Guide covers various performance considerations related to deploying networks using TensorRT 8.0.3. These sections assume that you have a model that is working at an appropriate level of accuracy and that you are able to...This Best Practices Guide covers various performance considerations related to deploying networks using TensorRT 8.0.3. These sections assume that you have a model that is working at an appropriate level of accuracy and that you are able to...Thanks!@AakankshaS
I have 2 models
qat.engine (38.2 MB)
trt_api.engine (37.0 MB)Info of qat.engineInfo of trt.engine (int8 engine generated from yolov7 with calibration file in TRT API)inference time of qat.engine is much larger than inference time of trt.engine
There is big difference with inference time.Hi,Which version of TensorRT are you using? We also recommend that you share environment information such as GPU and CUDA details.
Please try on the latest TensorRT version 8.6 and if you still face the same please share with us issue repro model/commands and complete verbose logs.Thank you.My GPU is Gtx1050 Ti, Tensorrt 8.5.1.1. I am using Docker container, mAP of qat engine is better than Trt ptq engine but speed is worse than the one.Please try on the latest TensorRT version 8.6 and if you still face the same please share with us issue repro model/commands and complete verbose logs.@spolisetty  Thank you so much.
I checked with TRT8.6 in PC with Docker container. Performance of QAT engine model is a litte bit better (may be fluctuation), but it is bad compared to engine generated from calibration with TRT Python API.Remind that, in the above comment, I attached 2 engines with TRT8.5
Here I attach QAT engine model with TRT8.6.
qat_trt86.engine (38.3 MB)@spolisetty
I am also confused about this performance table. Performance of TRT PTQ and QAT engines are same in this table.I also try skipping rules (Nvidia recommended) in this line https://github.com/NVIDIA-AI-IOT/yolo_deepstream/blob/5af35bab7f6dfca7f1f32d44847b2a91786485f4/yolov7_qat/scripts/qat.py#L160 anh checked inference time from profiling. Inference time is almost same as applying rules.@spolisetty @mchi
Sorry. Is there any update?Here is command to convert qat.pt to engineIn this table (NVIDIA report)
Screenshot from 2023-08-03 15-07-461165×155 17.9 KBSpeed of TRT PTQ engine and QAT engine is almost same. But it is not true after checking on PC and Jetson board.Hi, @johnminho
in Github link, That perf is test on OrinX, which have higher int8/fp16 accelarate rate.if you want the performance the same as PTQ(Best performance). You should finetune the QDQ placement following the guidance: https://github.com/NVIDIA-AI-IOT/yolo_deepstream/blob/main/yolov7_qat/doc/Guidance_of_QAT_performance_optimization.md@haowang
Thanks for response.That perf is test on OrinX, which have higher int8/fp16 accelarate rate.I did not checked on OrinX, but I checked on Xavier NX, QAT engine is worse than PTQ engine. I will check on OrinX.Hi, Would you mind share your trtexec log & yolov7_qat_profile.json & yolov7_qat_layer.json here:trtexec --onnx=yolov7_qat.onnx --fp16 --int8 --verbose --saveEngine=yolov7_qat.engine --workspace=1024000 --warmUp=500 --duration=10  --useCudaGraph --useSpinWait --noDataTransfers --exportLayerInfo=yolov7_qat_layer.json --profilingVerbosity=detailed --exportProfile=yolov7_qat_profile.jsonHi @johnminho
I tried these two models on A2, PTQ and QAT perf are almost the same.$ /usr/src/tensorrt/bin/trtexec --onnx=yolov7_dy.onnx --int8 --best --optShapes=images:12x3x640x640 --saveEngine=yolov7_dy_bs12_best.plan
$ /usr/src/tensorrt/bin/trtexec --loadEngine=yolov7_dy_bs12_best.plan --batch=12
—> got 260.109 qps$ /usr/src/tensorrt/bin/trtexec --onnx=yolov7_dy.onnx --int8 --best --optShapes=images:12x3x640x640 --saveEngine=yolov7_dy_bs12_best.plan
$ /usr/src/tensorrt/bin/trtexec --loadEngine=yolov7_qat_bs12_best.plan --batch=12
 → got 266.843 qps@mchi
Thanks for checking again. Which version of TRT are you using?@haowangHi, Would you mind share your trtexec log & yolov7_qat_profile.json & yolov7_qat_layer.json here:Thanks. I will share you later. I will check for some devices and TRT versions. I think that the reasons may be hardware and TRT version. Until now, I checked RTX2080Ti and TRT8.2, speed of QAT engine is bad.
I will inform you as soon as possible.Please keep in touch, thanks.Which version of TRT are you using?  ==> TensorRT 8.5.3.Until now, I checked RTX2080Ti and TRT8.0, speed of QAT engine is bad.Please use Lastest TensorRT version for example TensorRT8.5.3 to align.
Thanks@mchiWhich version of TRT are you using? ==> TensorRT 8.5.3.I am using TRT8.2.@haowangPlease use Lastest TensorRT version for example TensorRT8.5.3 to align.I am going to check for TRT8.5.3Thank you very much.@haowang
I would like to send log and json files.
gtx1050Ti_trt8.6.zip (29.1 MB)I can confirm that, with GTX1050Ti + TRT8.6, speed of QAT engine model is bad compared to speed of TRT PTQ engine (throughput of QAT ~ 32 qps, throughput of TRT PTQ ~ 43 qps). The reason may be related to GTX1050Ti (not good support int8).
I will check for more devices.Powered by Discourse, best viewed with JavaScript enabled"
406,cant-install-pycuda-on-jetson-nano,"i try to install pycuda but terminal show this errorDefaulting to user installation because normal site-packages is not writeable
Collecting pycuda
Using cached pycuda-2021.1.tar.gz (1.7 MB)
Installing build dependencies … done
Getting requirements to build wheel … done
Preparing metadata (pyproject.toml) … done
Collecting pytools>=2011.2
Using cached pytools-2022.1.9-py2.py3-none-any.whl
Collecting mako
Using cached Mako-1.2.0-py3-none-any.whl (78 kB)
Collecting appdirs>=1.4.0
Using cached appdirs-1.4.4-py2.py3-none-any.whl (9.6 kB)
Requirement already satisfied: typing-extensions>=4.0 in ./.local/lib/python3.9/site-packages (from pytools>=2011.2->pycuda) (4.2.0)
Collecting platformdirs>=2.2.0
Using cached platformdirs-2.5.2-py3-none-any.whl (14 kB)
Collecting MarkupSafe>=0.9.2
Using cached MarkupSafe-2.1.1-cp39-cp39-manylinux_2_17_aarch64.manylinux2014_aarch64.whl (26 kB)
Building wheels for collected packages: pycuda
Building wheel for pycuda (pyproject.toml) … error
error: subprocess-exited-with-error× Building wheel for pycuda (pyproject.toml) did not run successfully.
│ exit code: 1
╰─> [147 lines of output]
***************************************************************
*** WARNING: nvcc not in path.
*** May need to set CUDA_INC_DIR for installation to succeed.
***************************************************************
*************************************************************
*** I have detected that you have not run configure.py.
*************************************************************
*** Additionally, no global config files were found.
*** I will go ahead with the default configuration.
*** In all likelihood, this will not work out.
***
*** See README_SETUP.txt for more information.
***
*** If the build does fail, just re-run configure.py with the
*** correct arguments, and then retry. Good luck!
*************************************************************
*** HIT Ctrl-C NOW IF THIS IS NOT WHAT YOU WANT
*************************************************************
Continuing in 10 seconds…
Continuing in 9 seconds…
Continuing in 8 seconds…
Continuing in 7 seconds…
Continuing in 6 seconds…
Continuing in 5 seconds…
Continuing in 4 seconds…
Continuing in 3 seconds…
Continuing in 2 seconds…
Continuing in 1 seconds…
/tmp/pip-build-env-9u5ovmop/overlay/lib/python3.9/site-packages/setuptools/_distutils/dist.py:257: UserWarning: Unknown distribution option: ‘test_requires’
warnings.warn(msg)note: This error originates from a subprocess, and is likely not a problem with pip.
ERROR: Failed building wheel for pycuda
Failed to build pycuda
ERROR: Could not build wheels for pycuda, which is required to install pyproject.toml-based projectsHi,
I’m not sure if this will work for you but you can try this as it worked for me when I had the same error yesterday.Open terminal on Jetson Nano, and write the following commands.echo “export CPATH=$CPATH:/usr/local/cuda-10.0/targets/aarch64-linux/include” >> ~./bashrcecho “export LIBRARY_PATH=$LIBRARY_PATH:/usr/local/cuda-10.0/targets/aarch64-linux/lib” >> ~./bashrcsource ~./bashrcpip3 install pycuda —userNote: where cuda-10.0 has been mentioned be sure to change it to your own version.
For eg. I had cuda-10.2 so just change cuda-10.0 to cuda-10.2 in the commands.
Hopefully this works for you!Hi @user153553For Jetson issues, please post in the Jetson category. We have engineering there to help.Discussions relating to the Jetson DevKits and other Embedded computing devicesPowered by Discourse, best viewed with JavaScript enabled"
407,performance-slowdown-during-distributed-training-with-4x-rtx-4090-gpus,"Hey, I am currently experiencing a considerable training slowdown while implementing Distributed Training with four RTX 4090 GPUs on various Computer Vision models, such as YOLO and ResNet50. After initiating the training process, I observed a significant drop in power usage from 450W to around 80-90W within just a few seconds, resulting in the training becoming approximately 6-8 times slower.My setup is liquid-cooled and I’ve verified that the temperature levels for both CPU and GPU are within the acceptable range, so overheating can be ruled out. To further diagnose this issue, I conducted training with Large Model Support (LLMS) and performed a GPU-Burn test. The GPUs operated normally, sustaining 450W power usage without any significant increase in temperature. This peculiar slowdown appears to be exclusive to training Computer Vision models, particularly when the data is distributed across multiple GPUs.Moreover, I have executed individual tests for CPU, GPU, and I/O and everything is working fine stand-alone, however as soon as you start training a CV model problem occurs. I tried different versions of YOLOs, Resnet50, and noticed similar behavior for all.System Specs:I’ve done some research and found that numerous people seem to be encountering similar issues and pointing out that p2p is disabled on Nvidia 40 series Cards:I’m considering if the peer-to-peer (p2p) aspect could contribute to this problem. If so, is it possible that an Nvidia driver software update could rectify this issue, or might this be a permanent limitation that hinders the utility of a quad 4090 GPU setup for Deep Learning applications in the future?I would be grateful for confirmation on whether RTX 4090 GPUs with Distributed Data-Parallel (DDP) are suitable for Deep Learning or if this is more likely to be a software-related issue. I’ve tried using the NVIDIA NGC Docker container with PyTorch2 and CUDA 11.8, as well as the TensorFlow Docker for the ResNet50 benchmark, but neither provided a solution.Your assistance in this matter is greatly appreciated.Make sure you have the latest drivers, 535 drivers were just released in Ubuntu packages. This drivers fix an issuer with p2p. The issue being there is no p2p in 4090s, but old drivers reported there was.
When training same model with pytorch lighting running from docker, dual is doubling my batches.
2.03it/s dual 4090s
2.25it/s single 4090sIf you post your docker bench marks command I can test.Hi @alaapdhall79 ,
I am checking on this. please allow me some time.
Thank you for your patience.Powered by Discourse, best viewed with JavaScript enabled"
408,missing-vc-runtime-library-of-nvonnxparser-dll-on-windows-7,"missing VC++ Runtime Library of nvonnxparser.dll on Windows 7api-ms-win-core-libraryloader-l1-2-0.dllI’m guessing this can be solved by installing the VC++ Redist, but it’s not clear to me which version should be installed, and the VC++ runtime doesn’t seem to be explained in the TensorRT documentationTensorRT Version: 7.2.3.4
GPU Type: unknown
Nvidia Driver Version:
CUDA Version: 11.1
CUDNN Version: 8
Operating System + Version:  Windows 7Hi,
Request you to share the ONNX model and the script if not shared already so that we can assist you better.
Alongside you can try few things:This NVIDIA TensorRT 8.6.0 Early Access (EA) Quick Start Guide is a starting point for developers who want to try out TensorRT SDK; specifically, this document demonstrates how to quickly construct an application to run inference on a TensorRT...import sys
import onnx
filename = yourONNXmodel
model = onnx.load(filename)
onnx.checker.check_model(model).
2) Try running your model with trtexec command.master/samples/trtexecNVIDIA® TensorRT™, an SDK for high-performance deep learning inference, includes a deep learning inference optimizer and runtime that delivers low latency and high throughput for inference applicat...
In case you are still facing issue, request you to share the trtexec “”–verbose"""" log for further debugging
Thanks!Sorry, I forgot to give the background of the question:We developed a C++ program for model inference on Windows PCs, which works fine on Windows 10. However, on Windows 7, there will be a problem that the program cannot be started because of missing DLLs, and Windows error messages are as follows
image918×377 34.4 KB
We checked with the Dependency Walker that the DLL was introduced by the nvonnxparser. So I want to know how to install the runtime environment nvonnxparser.dll  required in Windows 7.Thank you!Hi @739119168 ,
I see you are using a very old version of TRT, for updated features and performance, we highly recommend you to use latest version of TRT.Can you try upgrading trt if possible and let us know if issue persist?Powered by Discourse, best viewed with JavaScript enabled"
409,configuring-multiple-versions-of-tensorrt-and-tensorflow-on-hpc-share-cluster-tf-trt-warning-cannot-dlopen-some-tensorrt-libraries,"We use Bright Computing for provisioning nodes on RHEL 9 and have cuda 11.7 and cuda 11.8 available as modules, as well as cudnn 8.5 for cuda 11.7 and cudnn 8.8 for cuda 11.8. I also created a module for cutensor-cuda11.7.We also have various modules for Python, e.g., mamba with Python 3.11, Anaconda Python 3.9.10. Tensorflow 2.11.0 was installed via pip with --user.NVIDIA-SMI 520.61.05    Driver Version: 520.61.05    CUDA Version: 11.8 and NVIDIA RTX A6000 are the GPUsWhat’s the reason for TF not finding the GPUs?But at least on cuda 11.8 the GPU is found:Hi,
Please check the below links, as they might answer your concerns.This is the revision history of the NVIDIA TensorRT 8.6 Developer Guide.This is the revision history of the NVIDIA TensorRT 8.6 Developer Guide.This is the revision history of the NVIDIA TensorRT 8.6 Developer Guide.
Thanks!Nothing there about multiple versions. Any other specific suggestions?Hi @rk3199 ,We are checking on this. Will update you on the same.ThanksHi @rk3199 ,
Did you try completely removing CUDA and reinstall it again.ThanksNo as this is in a cluster a loaded as a module.Lower versions of Python,. e.g., 3.7 does not generate this error/warning.Hi @rk3199 ,
Can you please share the TRT version you are using?
Also if you can try an upgrade and let us know if issue is still there?ThanksCan you please share the TRT version you are using?pip list | grep -i tensorrt
WARNING: Ignoring invalid distribution -ensorflow (/path/to/me/.local/lib/python3.9/site-packages)
nvidia-tensorrt              8.4.3.1
tensorrt                     8.6.1
tensorrt-bindings            8.6.1
tensorrt-dispatch            8.6.0
tensorrt-lean                8.6.0
tensorrt-libs                8.6.1Also if you can try an upgrade and let us know if issue is still there?Upgrade what? We use modules so I can install specific versions.Hi, I have the same problem using pyhton 3.9.4, cuda and cudnn 11.5 on HPC. Any solution so far?Powered by Discourse, best viewed with JavaScript enabled"
410,cudnn-isnt-found-fwd-algo-for-convolution,"Hello please help me… I got this error during training my custom dataset… How to solve it?Hi @202606 ,
can you please brief about the error?ThanksPowered by Discourse, best viewed with JavaScript enabled"
411,inquiry-regarding-transformer-engine-model-acceleration-information,"I am reaching out to inquire about the capabilities of the Transformer Engine developed by NVIDIA. We are particularly interested in understanding the specific models that can be accelerated using the Transformer Engine.I am interested to know if the Transformer Engine developed by NVIDIA is capable of accelerating Vision Transformer (ViT) models, along with NLP Transformer models such as BERT, GPT-3, and T5.It would greatly benefit our organization and our customers to have accurate information regarding the models compatible with the Transformer Engine.Could you kindly provide us with an updated list of the models that can be accelerated using the NVIDIA Transformer Engine? Additionally, any detailed information or documentation you can provide about the compatibility and performance improvements achieved by the Transformer Engine for each supported model would be highly appreciated.We understand the importance of efficient model acceleration for various natural language processing tasks, and we believe that the Transformer Engine can significantly enhance the performance and productivity of our applications. Having a comprehensive understanding of the supported models will allow us to better serve our customers and optimize our development efforts.Thank you very much for your attention to this matter. We look forward to your prompt response and appreciate your assistance in providing the requested information.Powered by Discourse, best viewed with JavaScript enabled"
412,cudnn-install-fails-on-ubuntu-22-04-installing-from-deb-file,"Installing cuDNN from deb file on  Ubuntu 22.04
cuda installation already done, and verifiedfollowing the steps, once you have install of deb file for cuDNN
install:  (cuDNN)   toolkit,  samples,  docsMy paste-dump below  is a mix of my notes but clearly shows the commands and the output.
I’m New to this, but can see the error, How to fix  run cleanly ?
TIA - for any assistance or guide to help1.3.2. Debian Local InstallationDownload the Debian local repository installation package. Before issuing the following commands, you must replace X.Y and 8.x.x.x with your specific CUDA and cuDNN versions.X.Y  12.2
8.x.x.x  8.9.3.28sudo apt-get install libcudnn8=8.x.x.x-1+cudaX.Y
sudo apt-get install libcudnn8=8.9.3.28-1+cuda12.2sudo apt-get install libcudnn8-dev=8.x.x.x-1+cudaX.Y
sudo apt-get install libcudnn8-dev=8.9.3.28-1+cuda12.2sudo apt-get install libcudnn8-samples=8.x.x.x-1+cudaX.Y
sudo apt-get install libcudnn8-samples=8.9.3.28-1+cuda12.2base) peter@p1:~/Downloads$ sudo cp /var/cudnn-local-repo-ubuntu2204-8.9.3.28/cudnn-local-BD12C98D-keyring.gpg /usr/share/keyrings/
(base) peter@p1:~/Downloads$ sudo dpkg -i cudnn-local-repo-ubuntu2204-8.9.3.28_1.0-1_amd64.deb
(Reading database … 228174 files and directories currently installed.)
Preparing to unpack cudnn-local-repo-ubuntu2204-8.9.3.28_1.0-1_amd64.deb …
Unpacking cudnn-local-repo-ubuntu2204-8.9.3.28 (1.0-1) over (1.0-1) …
Setting up cudnn-local-repo-ubuntu2204-8.9.3.28 (1.0-1) …(base) peter@p1:~/Downloads$ sudo apt-get update
Get:1 file:/var/cuda-repo-ubuntu2204-12-2-local  InRelease [1,572 B]
…
Reading package lists… Done
W: https://packages.microsoft.com/repos/vscode/dists/stable/InRelease: Key is stored in legacy trusted.gpg keyring (/etc/apt/trusted.gpg), see the DEPRECATION section in apt-key(8) for details.sudo apt-get install libcudnn8=8.9.3.28-1+cuda12.2(base) peter@p1:~/Downloads$ sudo apt-get install libcudnn8=8.9.3.28-1+cuda12.2
Reading package lists… Done
Building dependency tree… Done
Reading state information… Done
Package libcudnn8 is not available, but is referred to by another package.
This may mean that the package is missing, has been obsoleted, or
is only available from another sourceE: Version ‘8.9.3.28-1+cuda12.2’ for ‘libcudnn8’ was not found(base) peter@p1:~/Downloads$ sudo apt-get install libcudnn8-dev=8.9.3.28-1+cuda12.2
Reading package lists… Done
Building dependency tree… Done
Reading state information… Done
Package libcudnn8-dev is not available, but is referred to by another package.
This may mean that the package is missing, has been obsoleted, or
is only available from another sourceE: Version ‘8.9.3.28-1+cuda12.2’ for ‘libcudnn8-dev’ was not found(base) peter@p1:~/Downloads$ sudo apt-get install libcudnn8-samples=8.9.3.28-1+cuda12.2
Reading package lists… Done
Building dependency tree… Done
Reading state information… Done
Package libcudnn8-samples is not available, but is referred to by another package.
This may mean that the package is missing, has been obsoleted, or
is only available from another sourceE: Version ‘8.9.3.28-1+cuda12.2’ for ‘libcudnn8-samples’ was not foundPowered by Discourse, best viewed with JavaScript enabled"
413,tensorrt-8-6-1-ga-aarch64-sbsa-packages-contain-x86-64-stub-libraries,"TensorRT 8.6 distribution for Linux includes stub static libraries which are used to disable dependencies on cuBLAS and cuDNN libraries:The problem is that these libraries are built for x86-64 (intel) despite being packaged for aarch64 sbsa (arm) hosts.TensorRT Version:  8.6.1 GAReproducer/analyzer script is at: GitHub - sergeev917/tensorrt-stub-wrong-arch: Reproducer for tensorrt packaging issueResult:Hi,
Please refer to the installation steps from the below link if in case you are missing on anythingThis NVIDIA TensorRT 8.6.1 Installation Guide provides the installation requirements, a list of what is included in the TensorRT package, and step-by-step instructions for installing TensorRT.
Also, we suggest you to use TRT NGC containers to avoid any system dependency related issues.NVIDIA TensorRT is a C++ library that facilitates high-performance inference on NVIDIA graphics processing units (GPUs). TensorRT takes a trained network and produces a highly optimized runtime engine that performs inference for that network.Thanks!Hello, @AakankshaS!Please refer to the installation steps from the below link if in case you are missing on anythingThis guide is not relevant to the described issue. In short, SBSA/ARM packages contain some static libraries for X86 architecture and not ARM architecture. I don’t see how this could be an intended outcome. Correct me, if I’m wrong.To be more precise, nv-tensorrt-local-repo-ubuntu2004-8.6.1-cuda-12.0_1.0-1_arm64.deb is the “repository” DEB package (i.e. contains other DEB packages) which can be downloaded from https://developer.nvidia.com/nvidia-tensorrt-8x-download. The problematic package inside is libnvinfer-dev_8.6.1.6-1+cuda12.0_arm64.deb – it contains /usr/lib/aarch64-linux-gnu/stubs/*.a libraries which are built for X86.The reproducer script shows that issue in details.Idea behind these static stub libraries is described here:Stubbed static libraries for cuDNN, cuBLAS, and cuBLASLt are provided in $(TRT_LIB_DIR)/stubs. When statically linking TensorRT with no requirement for cuDNN, cuBLAS, or cuBLASLt, the stubbed library can be used to reduce CPU and GPU memory usage.NVIDIA TensorRT is a C++ library that facilitates high performance inference on NVIDIA GPUs. It is designed to work in connection with deep learning frameworks that are commonly used for training. TensorRT focuses specifically on running an already...Hello, @spolisetty, @AakankshaS!Is there any news on the topic?Hi @sergeev917,Thank you for letting us know.
This will be fixed soon.Powered by Discourse, best viewed with JavaScript enabled"
414,tensorrt-parsing-onnx-model-error,"In my C++ code, I am calling TensorRT-8.0 api to load and parse an ONNX model. It ran into an error. The gist of this error is stated by“Invalid Node - model/flatten_3/Reshape
Attribute not found: allowzero”Exactly the same error message as this post: Error when transform large onnx model to trt - githubmemoryDoes anyone have any idea how this was resolved in the above post? Thanks!TensorRT 8.0:
**GPU GeForce **:
Nvidia Driver 465.89:
CUDA 11.3:
CUDNN 8.2.1:
Windows 10:Hi,
Request you to share the ONNX model and the script if not shared already so that we can assist you better.
Alongside you can try few things:This NVIDIA TensorRT 8.4.3 Quick Start Guide is a starting point for developers who want to try out TensorRT SDK; specifically, this document demonstrates how to quickly construct an application to run inference on a TensorRT engine.import sys
import onnx
filename = yourONNXmodel
model = onnx.load(filename)
onnx.checker.check_model(model).
2) Try running your model with trtexec command.
https://github.com/NVIDIA/TensorRT/tree/master/samples/opensource/trtexec
In case you are still facing issue, request you to share the trtexec “”–verbose"""" log for further debugging
Thanks!Hi NVES,Thanks very much.I ran check_model.py snippet on my ONNX model file. It ran and finished quietly. I assume this means it is validated successfully. The full code in my check_model.py is as below.
import sys
import onnx
filename = r""C:\mypath\mymodel.onnx""
model = onnx.load(filename)
onnx.checker.check_model(model, full_check=True)As to the full log, could I work with Nvidia directly, instead of sharing such information on the public forums?Ok, problem resolved.Hello, I have the same problem.please explain how did you resolve it.@Gudbach
can you explain ?I got the same error, for me it was because my ONNX model was using opset 14 (which is not currently supported by tensorRT), I lowered it to opset 13 and it workedHello, i have the same error. @NVES @Gudbach could you please share the solution?resolvedLiterally the worst type of person. Why would you say you solved it, but not provide an answer. UghPowered by Discourse, best viewed with JavaScript enabled"
415,a4000-training-benchmark,"May I know ,Is NVidia RTX A4000 supports NV Link? And also, please provide the ResNet -50 Training benchmark for RTX A4000. Will it support deep learning training?Hi,The RTX A4000 does not support NVLink. NVLink is a technology that allows two or more GPUs to communicate with each other directly, which can improve performance in some applications. However, the RTX A4000 has 16GB of GDDR6 memory, which is enough for most deep learning training tasks.
Please refer to the following specs.312.08 KBThank you.Powered by Discourse, best viewed with JavaScript enabled"
416,inference-with-tensorrt-model-pytorch-bert-model-onnx-tensorrt-inference,"I have converted my BERT-trained model from PyTorch to ONNX and from ONNX to TensorRt. The challenge I am having now is getting the TensorRt model to return the probability values given an input text. When using the PyTorch or ONNX versions, the models take as input the input_ids and attention mask and yield the predictions (input_text_prediction --see below). Given that the TensorRt is the final conversion of the original PyTorch model, my intuition tells me that the TensorRt also needs to take the same inputs. I have been reading/following this guideline, but it seems like their approach is too complex, or am I oversimplifying the inference process using TensorRt?Yes, I have done my Google/StackOverflow research and have not found an answer/guideline besides the above link. Any help/guidance resources are greatly appreciated.nd from ONNX to TensorRt. The challenge I am having now is getting the TensorRt model to return the probability values given an input text. When using the PyTorch or ONNX versions, the models take as input the input_ids and attention mask and yield the predictions (input_text_prediction --see below). Given thBERT works with tokens only, so I think you should use some autotokenizer before you put your data to model’s inputs.Here is my example with onnxruntime:Powered by Discourse, best viewed with JavaScript enabled"
417,cuopt-server-get-optimized-routes-sync-error,"hello dear all.i m testing cuopt server version method
“get_optimized_routes_sync”i got some error :
AttributeError: ‘NoneType’ object has no attribute ‘cost_matrix’(cost_matrix_data.cost_matrix is not None)
at  cuopt_amr_service.py : line 783, in get_optimized_routes_syncso i check that python file in containerdeclear graph asand check as like belowwhen i send graph(only one catagory)  ‘NoneType’ object has no attribute can happen always because i will send graph as one typeis that bug right?It is a bug and being worked on, for now, you can send cost_matrix_data as None to avoid this issue.thanks for reply.
How can i avoid in API version?
for example i send get_optimized_routes_sync AS below
opt_sync = {
“cost_waypoint_graph_data” : {
“waypoint_graph”: {
“0”: {
“offsets”: [0, 3, 5, 9, 11, 13, 15, 17, 18, 19, 20, 21],
“edges”: [1, 2, 9, 0, 7, 0, 3, 4, 10, 2, 4, 2, 5, 6, 9, 5, 8, 1, 6, 0, 5],
“weights”: [1, 1, 2, 1, 2, 1, 1, 1, 3, 2, 3, 2, 1, 2, 1, 3, 4, 2, 3, 1, 1]
}
}
},}
than NoneType object error happen. if i add
“cost_matrix_data”: None
or
“cost_matrix_data”: {
“cost_matrix”: {
“0”: [
[
0
]
],
},
“vehicle_type”: 0
},
also error shows up.How about “cost_matrix_data”:{“cost_matrix”:None}also error happen: requests.exceptions.JSONDecodeError: Expecting value: line 1 column 1 (char 0)it maybe happen format (“cost_matrix”:None) not allowed
only “cost_matrix”:{“0” : something""} format allowedso, when i did like below
“cost_matrix_data”:{“cost_matrix”:{
“0”: None
}},
also error hanppen
SOLVER RESPONSE: {‘detail’: [{‘loc’: [‘body’, ‘cost_matrix_data’, ‘cost_matrix’, ‘0’], ‘msg’: ‘none is not an allowed value’, ‘type’: ‘type_error.none.not_allowed’}]}I was able to do something  like thiscost_matrix = {0: [[0, 1, 1], [1, 0, 1], [1, 1, 0]]}routing_data = {
“cost_waypoint_graph_data” : {“waypoint_graph”:None},
“travel_time_waypoint_graph_data” : {“waypoint_graph”:None},
“cost_matrix_data”:{“cost_matrix”:cost_matrix},
“travel_time_matrix_data”:{“cost_matrix”:None},
“cost_matrix”: cost_matrix,
“task_data”: task_data,
“fleet_data” : fleet_data,
“solver_config” : solver_config,
}
​Powered by Discourse, best viewed with JavaScript enabled"
418,correctness-problem-using-tensorflow-with-rtx-4090,"I am writing to report a correctness issue I encountered while using the RTX 4090 GPU with Tensorflow.I have a Tensorflow model which gives very close results when running on my RTX 3090 GPU and CPU. However, when I run it on RTX 4090 with Nvidia Tensorflow (version: nv22.11), it gives significantly different results when compared to running on the CPU.Configurations:OS: Ubuntu 20.04.5 LTS
CPU: Intel(R) Core™ i9-13900K
GPU: NVIDIA GeForce RTX 4090
Docker image: nvidia tensorflow, tag: 22.11-tf1-py3My model is a regression U-net model. The change of results when switched to RTX4090 and this version of Tensorflow affects the performance. The range of the output of the model if about -3 to 3 and the difference in results running on different devices can be up to 0.08. When running on RTX3090 with older version of CUDA (11.1) and cuDNN (8.0), the difference is < 0.01.Difference of results between RTX 4090 and CPU:sample_program.zip (1.9 MB)
I attached here a sample program to repeat the problem. The script run.sh runs inference on a GPU and the CPU and compare the results.Since I will soon upgrade the GPU cards to newer models with new libraries. I am afraid that the performance will degrade. I would like to know that how I can get consistent result with the new GPU card. I look forward to hearing back from you soon.Hi @chydavy ,
Apologies for the delay.
Let me check on this with the Engg team and get back to you.
ThanksThank you Aakanksha! I look forwards to hearing from your team.Powered by Discourse, best viewed with JavaScript enabled"
419,inference-tensorrt-randomly-returns-nan,"I use onnx-tensorrt to inference my model.
In my python code, I am calling TensorRT-8.0 api to load and parse the ONNX model. It rans well. But in my C++ code. It randomly returns nan.TensorRT Version: 8.4.1
Nvidia Driver Version: 470.182.03
CUDA Version: 11.4
CUDNN Version: 8.4.0
Operating System + Version: linux18.04Hi,
Can you try running your model with trtexec command, and share the “”–verbose"""" log in case if the issue persistmaster/samples/trtexecNVIDIA® TensorRT™, an SDK for high-performance deep learning inference, includes a deep learning inference optimizer and runtime that delivers low latency and high throughput for inference applicat...You can refer below link for all the supported operators list, in case any operator is not supported you need to create a custom plugin to support that operationAlso, request you to share your model and script if not shared already so that we can help you better.Meanwhile, for some common errors and queries please refer to below link:This is the revision history of the NVIDIA TensorRT 8.6 Developer Guide.This is the revision history of the NVIDIA TensorRT 8.6 Developer Guide.Thanks!verbose from trtexec.odt (74.4 KB)
Hi,
I Have upload my verbose.Powered by Discourse, best viewed with JavaScript enabled"
420,a40-and-3090-gemm-performance-test-data,"environment：ubuntu 22.04 、cublasMatmulBench（The test has locked the GPU frequency）
A40 DATA：
|         INT8       |       FP16         |        TF32         |     FP32            |
| 163 TFLOPS |  116 TFLOPS |    70 TFLOPS  |  20.2 TFLOPS  |3090 DATA:
|        TF32            |         FP32         |
|    58.6 TFLOPS  |  21.1 TFLOPS  |The question is ：The data I measured myself is very different from the official PeakTFLOPS, what is the reason？Powered by Discourse, best viewed with JavaScript enabled"
421,when-i-used-the-onnx-model-fidtm-to-generate-the-tensorrt-engine-i-received-an-error-as-follows,"When I used the onnx model（FIDTM） to generate the tensorrt engine, I received an error as follows:
ERROR:
ERROR: builtin_op_importers.cpp:2651 In function importResize:
[8] Assertion failed: scales.is_weights() && “Resize scales must be an initializer!”
[03/02/2023-16:40:06] [E] Failed to parse onnx file
[03/02/2023-16:40:06] [E] Parsing model failed
[03/02/2023-16:40:06] [E] Engine creation failed
[03/02/2023-16:40:06] [E] Engine set up failedENVIRONMENT
TensorRT Version - 7.2.3.4
GPU Type - GeForce RTX 2080Ti
Nvidia Driver Version - 470.86
CUDA Version - 10.2.89
CUDNN Version - 8.0.5
Operating System + Version - ubuntu18.04
Python Version - 3.6.13
Onnx Version - 1.11.0ONNX IR version:  0.0.6
Opset version:    10
Producer name:    pytorch
Producer version: 1.6
Domain:
Model version:    0
Doc string:my model ：Google Drive file.
Please help me to solve this problem. Thanks!Hello and welcome to the NVIDIA Developer forums!
This topic belongs in the TensorRT forum, I will go ahead and move it over for you.Best,
TomPowered by Discourse, best viewed with JavaScript enabled"
422,loading-a-pre-trained-model-into-nemo,"Hello,I’m just getting started with NeMo and I’m trying to load a pre-trained model.  The steps I’d like to accomplish are:I’m using NeMo and I’m loading a Financial Megatron model finmegatron345m_gpt2_bpe.  I feel like my problem is in the configuration of NeMo.  I’m using the basic QA conf that is provided in the Github (qa_conf.yaml) and then following this Notebook (Question_Answering.ipynb).I’ve been tweaking the configuration to point to my pre-trained model but I’m getting a variety of errors, most recently that I need to load a language model checkpoint.My code is super-simpleWhen reading through the pre-trained model info, it doesn’t say anything about needing to include a language model checkpoint.  Is there any simple documentation on just loading up a pre-trained model in NeMo and running an inference pass?Thanks!.nemo file is just a archive that stores config, model, checkpoint. Try to unrar nemo file to know what files does it have. I think you should define full path to finmegatron_gpt2_en_uncase.nemoPowered by Discourse, best viewed with JavaScript enabled"
423,riva-can-be-deployed-free-of-charge-up-to-a-certain-usage-limit-clarifications,"Hi,
I was going through the Riva Enterprise offering, trying to understand what the offering is, and what it means for Riva that I currently deploy.
image1964×778 87.2 KB
Riva Enterprise FAQ says “Riva can be deployed free of charge up to a certain usage  limit” right after saying the SDK is free.I think they mean the Riva Enterprise offering can be deployed free for a trial, correct?  Can someone clarify this please. Is the SDK free, but NVIDIA will somehow charge for deploying it? Is Riva Enterprise a managed offering?We are inception program partners, but have not had much luck getting much expertise or assistance from NVIDIA regarding Riva, and would like to explore our options. If anyone has experience with this that they could share, do let us know!Hi @ShantanuNairThanks for your interest in RivaWe have few pointers below  regarding your queryLearn more about Riva Enterprise here.Thanks @rvinobha
Can you please clarify your first point. Is the ASR usage counted for riva self-hosted deployments? Right now we have deployed on AWS, and don’t have such limits. Does this change with future releases?If so, I need to update my team on this, rather bad news. It would be honestly, disappointing. Despite previous discussions being brought up about licensing and roadmaps, this was never hinted at.I’ve gone through all the Jarvis/Riva/Nemo seminars and articles, including marketing content, and never saw anything indicating that we wouldn’t be able to fully self-deploy Jarvis/Riva free of cost or that this is the direction it would take, in fact, quite the opposite.@ShantanuNair I would like to echo your comments.  This is a sudden surprise, and not a happy one.  Although I understand wanting to get paid for your efforts, as far as I can tell, there were no discernible warnings for this license change.  I sure hope the Enterprise program is fairly priced.  We are in the midst of product development in which ASR is key, including market/pricing models that are now suddenly wrong.
I’m curious how this effects the use of Riva models that have been customized with proprietary TAO training.According to an email I received yesterday, the usage limit only applies to the new Riva 2.0 software.Riva 2.0 is free of charge for both development and production use up to a daily usage limit. Earlier Riva versions will continue to work unchanged.Of course that could change between now and when the finalized license terms are eventually released.Hi @ShantanuNairThanks for your interest in RivaOur team had been working on determining the right business model for Riva as part of the Riva Public Beta program and we believe we have been able to put together an enterprise license that is fairly priced. ASR usage is counted for self hosted deployments as well - all deployments are self hosted - Nvidia does not have a managed service offering for ASR today.Hi @BGreenwayThanks for your interest in RivaASR Usage with Riva models that have been customized with proprietary TAO training also counts against the cap. We do believe that the Riva Enterprise License is fairly priced and delivers significant value for customers who would like to achieve best possible accuracy with customizations and flexibility of deployment.Hi, what is the current licensing situation for Riva ASR? The current EULA only authorises usage in Non-Production environments, and there doesn’t seem to be any mention of the 1000 hours cap for unlicensed instances.Hi @sjpritchardThanks for your interest in RivaI will check with the team and provide responseThanksHi, any update on this? I’ve tried contacting your sales team twice now via Riva Sales Contact me Form with no response.HI @sjpritchardI have earlier asked inputs on your query and now mentioned about the Riva Sales Contact me Form with no response to the Riva Internal team, will shortly get backHi @ryeinApologies on that,i will update the team again on thisHi @rvinobha. Your last response on this was a little while ago now. Is there any update on the licencing, free use of Riva, Enterprise Licence costs etc? I’m trying to find this out with zero success. I’m looking possibly to use Riva for ASR and need to understand what the potential cost implications will be for running in a production environment processing over 1000 hours of audio per day. I cannot find ANY information on the enterprise licence costs. I’ve submitted requests for information several times now and had no response. Any ideas?HI @david397Thanks for your interest in RivaCustomers can use Riva free version for POC/testing. Max daily usage limit is 1000 hours in production environment, this comes without any NVIDIA support. Customers can purchase Riva Enterprise with NVIDIA support for a feeplease reach out to the below link for Riva EnterpriseRiva Sales Contact me FormThanksHi @rvinobha. Many thanks for your response, that at least clarifies that we can create a POC without the licence, so thank you. The problem is I need to understand the potential costs of the enterprise licence to know whether this is viable for us as a long term solution. I’ve completed the sales contact form twice now without any response. Is there a phone number I can call to speak to someone directly? If I can’t understand the costs involved we simply can’t consider Riva as a potential solution.UPDATE: My boss has finally managed to make contact with the NVidia sales team (through other means) and we have a meeting to discuss commercials in a couple of weeks time. Thanks.Hi, Is there any information about the potential costs of using Nvidia Riva?  I completed the Sale Contact Form several times but no response yet. Please let me know how to contact the sale teamHI @david397 and @kyndApologies, I will let the team know that there are no responses from formThanksPowered by Discourse, best viewed with JavaScript enabled"
424,error-occurs-when-running-stable-diffusion-demo-on-v100-16g,"Hi, I tried running stable diffusion demo on V100 16G. The following error occurs:python3 demo_txt2img.py “a beautiful photograph of Mt. Fuji during cherry blossom” --hf -token=$HF_TOKEN -vLoading TensorRT engine: engine/vae.plan
[I] Loading bytes from engine/vae.plan
[E] 1: [defaultAllocator.cpp::allocate::21] Error Code 1: Cuda Runtime (out of memory)
[W] Requested amount of GPU memory (20401098752 bytes) could not be allocated. There may not be enough free memory for allocation to succeed.
[E] 2: [executionContext.cpp::ExecutionContext::436] Error Code 2: OutOfMemory (no further information)
Traceback (most recent call last):
File “demo_txt2img.py”, line 83, in
demo.loadResources(image_height, image_width, batch_size, args.seed)
File “/workspace/mydata/tensorrt-sd/TensorRT/demo/Diffusion/stable_diffusion_pipeline.py”, line 151, in loadResources
self.engine[model_name].allocate_buffers(shape_dict=obj.get_shape_dict(batch_size, image_height, image_width), device=self.device)
File “/workspace/mydata/tensorrt-sd/TensorRT/demo/Diffusion/utilities.py”, line 234, in allocate_buffers
self.context.set_binding_shape(idx, shape)
AttributeError: ‘NoneType’ object has no attribute ‘set_binding_shape’Then I added the –build-static-batch flag at the end of the command. The program worked fine, but I got an all-black image.python3 demo_txt2img.py “a beautiful photograph of Mt. Fuji during cherry blossom” --hf-token=$HF_TOKEN -v --build-static-batchDoes SD demo not work on V100 now? Could you give me some suggestion?Thank you!!TensorRT Version:  8.6
GPU Type:  V100 16G
Nvidia Driver Version:  530.30
CUDA Version:  12.1
CUDNN Version:  8
Operating System + Version:  Ubuntu 18.04
Python Version (if applicable):
TensorFlow Version (if applicable):
PyTorch Version (if applicable):
Baremetal or Container (if container which image + tag): Container (docker run --rm -it --gpus all -v $PWD:/workspace nvcr.io/nvidia/pytorch:23.02-py3 /bin/bash)Hi,This looks like a Jetson issue. Please refer to the below samples in case useful.Hello AI World guide to deploying deep-learning inference networks and deep vision primitives with TensorRT and NVIDIA Jetson. - GitHub - dusty-nv/jetson-inference: Hello AI World guide to deployin...An easy to use PyTorch to TensorRT converter. Contribute to NVIDIA-AI-IOT/torch2trt development by creating an account on GitHub.For any further assistance, we will move this post to to Jetson related forum.Thanks!Why Jetson issue?
The error occurs when I running tensorrt stable diffusion demo on V100…release/8.6/demo/DiffusionNVIDIA® TensorRT™, an SDK for high-performance deep learning inference, includes a deep learning inference optimizer and runtime that delivers low latency and high throughput for inference applicat...Hi @kun.he.love.u ,
Please share the steps to reproduce so that we can assist better.ThanksPowered by Discourse, best viewed with JavaScript enabled"
425,why-does-int8-quantization-occupy-more-gpu-graphics-memory-than-float16-tensorrt-quantization,"A clear and concise description of the bug or issue.TensorRT Version:
GPU Type:  NVIDIA GeForce RTX 3090
Nvidia Driver Version:  515.65.01
CUDA Version:  11.3
CUDNN Version:  8.4
Operating System + Version:  Centos7
Python Version (if applicable):  3.8
TensorFlow Version (if applicable):
PyTorch Version (if applicable): 1.11
Baremetal or Container (if container which image + tag):Please attach or include links to any models, data, files, or scripts necessary to reproduce your issue. (Github repo, Google Drive, Dropbox, etc.)Please include:model sizeint8        8.1M
float16     15.4MB
float32      29.9MHi, Please refer to the below links to perform inference in INT8master/samples/sampleINT8NVIDIA® TensorRT™, an SDK for high-performance deep learning inference, includes a deep learning inference optimizer and runtime that delivers low latency and high throughput for inference applicat...1777.21 KB
Thanks!Powered by Discourse, best viewed with JavaScript enabled"
426,does-nemo-fastconformer-and-transducer-model-support-in-riva,"Does this models support Riva?Hi @mehadi.hasanThanks for your interest in RivaI will check with the team and confirm shortlyThanksHi @mehadi.hasanApologies, they are not supported,
But we it is in our Roadmap for future supportThanksPowered by Discourse, best viewed with JavaScript enabled"
427,ama-with-cugraph-engineering-team,"If you have any interest in accelerated graph processing or are already a user of cuGraph, the core engineering team and product management will be hosting an AMA right here in the forums on Weds, April 12th. Find out more details : AMA with the cuGraph engineering team - April 12, 2023, 9am (PDT) - Connect With Experts - AMA / AMA cuGraph: Graph analysis and GNN - NVIDIA Developer ForumsHow we part of this team i cant understand the process.Powered by Discourse, best viewed with JavaScript enabled"
428,trt-w-cuda-initialization-failure-with-error-35-segmentation-fault-core-dumped,"I run the following code:import tensorrt as tr
trt_runtime = trt.Runtime(trt.Logger(trt.Logger.WARNING))Error:
[TRT] [W] CUDA initialization failure with error: 35
Segmentation fault (core dumped)TensorRT Version:  8.6.1
CUDA driver from nvidia-smi: 11.7
CUDA compiler from nvcc --version: 12.1
Driver Version: 515.76
GPU compute capability: 6.1
Operating System + Version: Ubuntu 20.04.4 LTSHi,It appears there is an issue in your CUDA setup.
Could you please uninstall CUDA and correctly reinstall only one version?Also, please note that generally, CUDA 12.x needs a driver version greater than 525.60.
If you plan to install CUDA 12, please upgrade the driver version.Thank you.Powered by Discourse, best viewed with JavaScript enabled"
429,tensorrt-inference-in-real-time,"I have converted my model.onnx to createdEngine.engine using the following command:/usr/src/tensorrt/bin/trtexec --onnx=/home/isl/fall_keras_model/keras_fall_model_onnx.onnx --saveEngine=/home/isl/success_engine.engineAlso in a python script I have the following code:import cv2
from cvzone.HandTrackingModule import HandDetector
from cvzone.ClassificationModule import Classifier
import numpy as np
import mathfinal_output = “”
letters =
count_frames = 20cap = cv2.VideoCapture(0)
detector = HandDetector(maxHands=1)
classifier = Classifier(“fall_keras_model.h5”, “fall_labels.txt”)offset = 50
imgSize = 300
counter = 0labels = [“A”, “B”, “back”, “C”, “D”, “E”, “F”, “G”, “H”, “I”, “J”, “K”, “L”, “M”,
“N”, “O”, “P”, “Q”, “R”, “S”, “space”, “T”, “U”, “V”, “W”, “X”, “Y”, “Z”] #back, space, j, zwhile True:
success, img = cap.read()
hands = detector.findHands(img, draw=False)
filtered = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)
filtered = cv2.GaussianBlur(filtered, (5, 5), 2)
filtered = cv2.adaptiveThreshold(filtered, 255, cv2.ADAPTIVE_THRESH_GAUSSIAN_C, cv2.THRESH_BINARY_INV, 11, 2)
ret, filtered = cv2.threshold(filtered, 170, 255, cv2.THRESH_BINARY_INV + cv2.THRESH_OTSU)cv2.imshow(“Original”, img)if hands:
hand = hands[0]
x, y, w, h = hand[‘bbox’]key = cv2.waitKey(1)
if key == ord(‘q’):
cap.release()
cv2.destroyAllWindows()ISSUE: i want to run this converted .engine tensorrt engine and pass my input image to get the prediction, so how should I load the engine in the above code and run it?TensorRT Version: 8.0.6.1
GPU Type: Jetson Nano
Nvidia Driver Version:
CUDA Version:
CUDNN Version:
Operating System + Version: Ubuntu 18.04+
Python Version (if applicable): 3.6.9
TensorFlow Version (if applicable):
PyTorch Version (if applicable):
Baremetal or Container (if container which image + tag):Please attach or include links to any models, data, files, or scripts necessary to reproduce your issue. (Github repo, Google Drive, Dropbox, etc.)Please include:Hi,
Request you to share the ONNX model and the script if not shared already so that we can assist you better.
Alongside you can try few things:This NVIDIA TensorRT 8.5.3 Quick Start Guide is a starting point for developers who want to try out TensorRT SDK; specifically, this document demonstrates how to quickly construct an application to run inference on a TensorRT engine.import sys
import onnx
filename = yourONNXmodel
model = onnx.load(filename)
onnx.checker.check_model(model).
2) Try running your model with trtexec command.master/samples/trtexecNVIDIA® TensorRT™, an SDK for high-performance deep learning inference, includes a deep learning inference optimizer and runtime that delivers low latency and high throughput for inference applicat...
In case you are still facing issue, request you to share the trtexec “”–verbose"""" log for further debugging
Thanks!Powered by Discourse, best viewed with JavaScript enabled"
430,model-interpretability-attention-mechanism-for-megatron-models,"Hi,I’m working on model interpretability (specifically visualizing the attention flow) for Megatron models like biomegatron345m_biovocab_30k_cased and biomegatron-bert-345m-cased. Has anyone worked on this topic before and can offer me some advice?Also, does anyone know where I can find the model scripts for the BioMegatron models?Thank you!Powered by Discourse, best viewed with JavaScript enabled"
431,jax-program-gives-cudnn-status-arch-mismatch-error,"I am trying to run a simple JAX program and I get this errorI have a Tesla K40c GPU card . I have installed CUDA Toolkit 11.4 . The CUDnn version from the cudnn_version.h file is 8800The first few lines of the deviceQuery is as followsA search on Stack Overflow indicated that the CUDA toolkit and the CUDNN version might have mismatches . But I specifically selected the CUDNN version compatible ""cuDNN 8.8.0 for CUDA 11.x	""Any pointers will be helpfulI am not able to understand this page’s compatibility matrixThese support matrices provide a look into the supported versions of the OS, NVIDIA CUDA, the CUDA driver, and the hardware for the NVIDIA cuDNN 8.8.0 release.Hi @hvram1CUDNN version compatible ""cuDNN 8.8.0 for CUDA 11.x ""This is correct as per the document,
However have you also verified the CUDA Compatibility matrix?ThanksPowered by Discourse, best viewed with JavaScript enabled"
432,how-to-use-the-trt-model-with-video-and-webcam,"I have a yolo model that I trained to use with Jetson nano. I converted this model first to onnx format and then to trt model. How can I use this trt model with a video or webcam?Hi,This looks like a Jetson issue. Please refer to the below samples in case useful.Hello AI World guide to deploying deep-learning inference networks and deep vision primitives with TensorRT and NVIDIA Jetson. - GitHub - dusty-nv/jetson-inference: Hello AI World guide to deployin...An easy to use PyTorch to TensorRT converter. Contribute to NVIDIA-AI-IOT/torch2trt development by creating an account on GitHub.For any further assistance, we will move this post to to Jetson related forum.Thanks!Powered by Discourse, best viewed with JavaScript enabled"
433,cuda-toolkit-version,"I have RTX3060 in my desktop, so which cuda toolkit should i install for machine learning/deep learning?The CUDA 12.1 SDK is the current toolkit for ML and DL whether you are Windows or LINUX OS this webpage (https://developer.nvidia.com/cuda-downloads) has an option for you. Just choose which OS and I’ll recommend the network installer it easier and quicker. We have options for installing CUDA under WSL2 on Win11 under the green LINUX button if WSL is your route. NVIDIA RAPIDS our ML/DL CUDAX-AI accelerated algorithms is OpenSource software (https://rapids.ai/start.html) that runs on in LINUX(WSL also) that makes processing datasets a snap. Preprocessing data is transformed in seconds instead of minutes. This can all be done RAPIDS using familiar code constructs from Pandas, numpy and Scikit - learn often only having to change module imports from pandas to cudf ( import pandas as pdf​ , becomes import cudf as pdf​) and your off and running with RAPIDS. I am a Ubuntu user on both Window WSL and LINUX what OS are you using?Installing CUDA on Windows is only 2 commands and on LINUX just 4 commands, the CUDA SDK will also update the display driver to be compatible with the CUDA toolkit version.
image.png2037×1348 89.2 KB
Powered by Discourse, best viewed with JavaScript enabled"
434,building-morpheus-nothing-provides-requested-mrc-23-11,"Hello, while building Morpheus on Jetson AGX Orin container build fail with following messages:Build info:Can you assist? I looked at forums but found no solution.Powered by Discourse, best viewed with JavaScript enabled"
435,looking-for-developers-to-help-build-dynamic-ai-enabled-kiosks-and-mobile-ordering-solutions,"Hello NVIDIA community!We’re a business looking to revolutionize the customer experience with dynamic AI-enabled kiosks and mobile ordering solutions. We believe that these types of solutions have enormous potential to improve customer satisfaction and drive sales, and we’re looking for talented developers to help us bring our vision to life.Our ideal candidate has experience working with NVIDIA’s Jetson platform and is passionate about building intelligent solutions that can interact with customers in a natural and personalized way. Specifically, we’re looking for developers who can help us with the following:If you’re a developer who is excited about the potential of these types of solutions and has experience working with NVIDIA’s technology, we’d love to hear from you! Please reach out to us with your resume and a brief summary of your experience and interest in this project.We’re looking forward to hearing from you and building something amazing together!Powered by Discourse, best viewed with JavaScript enabled"
436,centerpointpillar-trt-deployment,"Deer Tech team:
I am using TRT to do the centerpointpillar deployment, I met a problem as as below

image1170×658 73.7 KB
I trained the Lidar model and then I export the .pth model to .onnx with model simplification work. Unfortunately, it has the shape problem. I refered the CUDApointpillar project, it should be fine.
Here is my simplified onnx model. if there’s a problem please tell me.
pfe_centerpoint.onnx (19.2 MB)Hope you guys may provide me with some hint.
Thanks in advance.Best regards,
BarryPowered by Discourse, best viewed with JavaScript enabled"
437,how-to-enforce-convert-all-layers-to-int8-when-building-int8-engine-model,"I build int8 engine by using tensorrt Python API or trtexec. There are some fp32 and fp16 layers in my generated int8 model. How can I enforce convert all layers to INT8 when building int8 engine model? I want to check acc and speed of engine model with all INT8 layers. ThanksHi, Please refer to the below links to perform inference in INT8master/samples/sampleINT8NVIDIA® TensorRT™, an SDK for high-performance deep learning inference, includes a deep learning inference optimizer and runtime that delivers low latency and high throughput for inference applicat...1777.21 KB
Thanks!@AakankshaS
Thanks. I can run inference on INT8 engine. I mean that is there any way to force all fp32 layers (from onnx) to int8 layers in building process?Hi,Are you using implicit quantization or explicit quantization?Thank you.@spolisetty
I use implicit quantization with batch size = 1.We need to use the setPrecision and setOutputType APIs to force specific layers to use int8. TRT chooses FP32/FP16 over INT8 because at small batches, these run faster than INT8 (batch=1  some linear ops are faster on SM than TC, and using INT8 will require extra computation for quantization).This is the API Reference documentation for the NVIDIA TensorRT library. The following set of APIs allows developers to import pre-trained models, calibrate networks for INT8, and build and deploy optimized networks with TensorRT. Networks can be...This is the revision history of the NVIDIA TensorRT 8.6 Developer Guide.Powered by Discourse, best viewed with JavaScript enabled"
438,v2-camera-csi-snow-grain-denoicing,"Hello I buyed a v2 camera and when iI run it a noice graine was there it get worse when there is no light so i tried g-streamer tnr mode to try to cancel it but no effect so i would like to know if my camera have a probleme, or if its just cheap and if it is can you guides me to a super resolution super fps camera for outside activity and how to use it. thank youThe NVIDIA Maxine Video Effects SDK offers AI-based visual features that transform noisy, low-resolution video streams into pleasant user experiences.i saw this but i don’t know how to use docker or to install it via sdkmPowered by Discourse, best viewed with JavaScript enabled"
439,ngx-sample-output-frame-is-empty,"Hi,We built and are trying to run the sample up for image super resolution (isr.exe)we ran the command withisr.exe --factor 2 --input test.png --output out.pngthe resulting out.png is empty though (black frame).no other errors were observed. We are running and RTX 3090 using driver 511.09Same here! Compiled with VS 2019Windows 11, 511.23-desktop-win10-win11-64bit-international-dch-whqlIs cuda toolkit/driver required?Plus one here.
Windows 10 20H2
RTX 3070 Ti 511.23
VS 2017, tried suggested 10.0.10586 win10 sdk or retargeted to latest both output black images.Having the same problem with windows 10. tried everything. installed Cuda toolkit, visual studio 2022, 2017, etc. couldn’t solve.Bump!I’m having the same issue here, I have a 3080ti, newest drivers at the time of writing this, just getting black frames, I hope someone can help us out.I am bringing this thread to the NGX team - hopefully someone will get back to us soon.
Thanks for your patience.I have the same problem,
When i run ISR, i only get blank images.
The upscalling has been done because the 500x375 became 2000x1500“isr.exe --input lowres.jpg --factor 4 --output hres.jpg”Thanks.Any update from the NGX team? It has been 4 month. I have the same issue here.Is NGX even still supported at this point? The documentation states 430.64 as the newest driver version, that the SDK got tested with. However 430.64 doesn’t support RTX3000 series GPUs.@nadeemmIt’s been 4 months, can you tell us anything about this issue?Any news on this? Getting same issue…All I see is black frame when using isr.exe as well.Windows: 21H2
NVIDIA GeForce RTX 3080
Driver version 516.94
Visual Studio 2022Same here
RTX 3080
516.94
VS2017Is the NGX SDK being replaced by the DLSS SDK?
DLSS/include at main · NVIDIA/DLSS (github.com)
Because it seems you have more up-to-date files, the only problem is the lack of samples like the NGX SDK.Hi, getting same black output issue here on drivers 527.56. Are there working samples anywhere?
ThanksThe NGX samples appear to only work on Turing GPUs. I’m also getting a black image after swapping to an ampere GPU, the OS, drivers and everything else remaining the same.SloMo is the only example of the four presented as examples for the NGX SDK 1.1. that I have earned on my computer configuration, provided that the solution was built without problems.
In relation to isr.exe only buffer allocation and image saving with increased size works correctly, but all pixels are black.
It does not matter in which format I provide the image JPG or PNGNVIDIA NGX Video SloMo Sample
[INFO ] Media format: QuickTime / MOV (mov,mp4,m4a,3gp,3g2,mj2)
Session Initialization Time: 11 ms
Working on Frame: 441 FPS: 27.1387436 437 438 439 440
Session Deinitialization Time: 3 ms
System INFO: 13th Gen Intel(R) Core™ i5-13600KF 3.50 GHz, NVIDIA GeForce RTX 3060 12GB GDDR6, driver 528.49, VS 2015 SP3Powered by Discourse, best viewed with JavaScript enabled"
440,multiple-threads-running-inference-are-causing-a-slowdown,"We get cv::Mat frames using this OpenCV gstreamer pipeline:
filesrc location=""./video.mp4"" ! qtdemux ! h264parse ! queue ! nvv4l2decoder ! queue ! nvvideoconvert ! video/x-raw,format=BGRx ! videorate max-rate=30 ! videoscale ! video/x-raw,format=BGRx,width=1920,height=1080 ! queue ! videoconvert ! video/x-raw,format=BGR ! appsinkWe specify a model using Yolov7::Yolov7 after which we pass the cv::Mat frames to Yolov7::preProcess and then run Yolov7::infer and Yolov7::PostProcess.The inference works and everything runs fine at this point (around 31 seconds to process a 31 second video).
When we then spin up another thread that does the same thing in parallel, the combined process takes around 6 seconds longer than with a single thread.
For every additional thread after that, there is an additional 20-25 second increase in processing time.After further examination, the culprit lies somewhere within the method enqueueV2 mentioned in Yolov7.cpp
I traced it’s origin via NvInfer.h and  NvInferRuntime.h to NvInferImpl.h.
There the class class VExecutionContext : public VRoot has the method
virtual bool enqueueV2(void* const* bindings, cudaStream_t stream, cudaEvent_t* inputConsumed) noexcept = 0;
From there I can’t find further information nor definition of how it works and why it would be slowing down the overall process.Any idea of why this is happening?TensorRT Version:
TensorRT 8.4.1
GPU Type:
Jetson Orin AGX
Nvidia Driver Version:
CUDA Version:
Cuda SDK 11.4.14
CUDNN Version:
cuDNN 8.4.1
Operating System + Version:
Ubuntu 20.04.6 LTS  - JetPack 5.0.2-b231We are using the library called Yolov7 made by an Nvidia employee.Hi,The below links might be useful for you.This Best Practices Guide covers various performance considerations related to deploying networks using TensorRT 8.0.3. These sections assume that you have a model that is working at an appropriate level of accuracy and that you are able to...The programming guide to the CUDA model and interface.https://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__STREAM.htmlFor multi-threading/streaming, will suggest you to use Deepstream or TRITONFor more details, we recommend you raise the query in Deepstream forum.orraise the query in Triton Inference Server Github instance issues section.Triton Inference Server has 31 repositories available. Follow their code on GitHub.Thanks!Powered by Discourse, best viewed with JavaScript enabled"
441,failure-to-convert-torch-nn-functional-grid-sample-to-tensorrt-engine,"After debugging my network, I found the line which prevents it from being converted to TensorRT engine is a line which uses a ‘grid sample’ object:torch.nn.functional.grid_sample()After exporting the onnx and sanitising it, I run trtexec but I get the error:No importer registered for op: GridSample. Attempting to import as plugin.What is the solution please?Link to model:Google Drive file.TensorRT Version: 8.6.1.6
GPU Type: RTX 3080
Nvidia Driver Version:
CUDA Version: 11.6
CUDNN Version:
Operating System + Version: Windows 10
Python Version (if applicable):
TensorFlow Version (if applicable): 3.10.8
PyTorch Version (if applicable): 1.12.1
Baremetal or Container (if container which image + tag):Please attach or include links to any models, data, files, or scripts necessary to reproduce your issue. (Github repo, Google Drive, Dropbox, etc.)Please include:Hi,Could you please check the opset version you’re using while exporting the ONNX model.
Please make sure you are using the latest Opset version 17 and try again.
If you still face the issue, please share with us the ONNX model and complete verbose logs.Thank you.Hello,I just exported the onnx again using opset v. 17. After sanitising it, if I attempt to convert it to a trt engine, it gives this error:Here is the onnx model exported with opset v. 17:Google Drive file.Hi,We could successfully build the engine on TensorRT 8.6.Are you facing this issue only on windows ?Thank you.Thanks. Yes I’m using Windows.Good Morning,If this is an identified issue on Windows, is there an upcoming fix please?Thanks.Hi,Could you please reinstall TensorRT and try again to make sure the TensorRT libraries are correctly installed.This NVIDIA TensorRT 8.6.1 Installation Guide provides the installation requirements, a list of what is included in the TensorRT package, and step-by-step instructions for installing TensorRT.Please share with us the complete verbose logs if you face the issue again.Thank you.Powered by Discourse, best viewed with JavaScript enabled"
442,dynamic-engine-building-and-optimization-profile,"I have exported an ONNX model of SSD based from PyTorch. This .pth model and .onnx model are perfectly working fine. Input is (batch_size, 3, 300, 300).I have also built the TensorRT engine with dynamic shape (batch_size being dynamic) such that input layer is (-1, C, H, W).During memory allocation, on host and device, it is done based on engine binding shape i.e. (-1, C, H, W). Since, it is negative, it shows cuda out of memoryWays to resolve this (tried and failed):Created optimization profile, and activate it, and then set the binding shapes:
The profile has no effect on engine.Manually set binding’s shape to (1, C, H, W).
No error, but wrong inference.What am I doing wrong?TensorRT Version: 7.0
GPU Type: Quadro GV100, 32505 MiB
Nvidia Driver Version: 440.59
CUDA Version: 10.2
CUDNN Version:
Operating System + Version: Ubuntu 18.04
Python Version (if applicable): 3.6.9
TensorFlow Version (if applicable):
PyTorch Version (if applicable):
Baremetal or Container (if container which image + tag): nvcr.io/nvidia/tensorrt:20.03-py3Hi @anubhav4sachan,
Request you to share the verbose logs along with your onnx model so that we can assist you better on this.
Thanks!Google Drive Link for the .onnx model.The inference results are wrong [Log 1, for the given code.]size = trt.volume(engine.get_binding_shape(binding)) * -1 * engine.max_batch_sizeIf the quoted line is changed to size = trt.volume(engine.get_binding_shape(binding)) * engine.max_batch_size then there is cuda out of memory error. [Log 2]Hi @anubhav4sachan,
I could not reproduce the issue.
Can you please running your model with TRT trtexec command with the latest release.
https://github.com/NVIDIA/TensorRT/tree/master/samples/opensource/trtexecThanks!@anubhav4sachan were you ever able to resolve this?Powered by Discourse, best viewed with JavaScript enabled"
443,tts-could-not-find-0-in-the-dictionary,"Please provide the following information when requesting support.Hardware - GPU T4
Hardware - CPU
Operating System Ubuntu 20.04
Riva Version 2.10I am using the multispeaker model in Spanish from NeMo in Riva, when trying to synthesize “03” I get this error:E0630 15:00:56.133005 712188 character_mapping.cc:280] Could not find ‘0’ in the dictionary
E0630 15:00:56.133018 712188 character_mapping.cc:280] Could not find ‘3’ in the dictionaryCan someone tell me how I can modify the dictionary or how to add a whitelist that allows me to normalize those numbers to text?Hi @nharoThanks for your interest in RivaApologies on the delay,Let me check with the team and get back to youThanksPowered by Discourse, best viewed with JavaScript enabled"
444,tensorrt-c-not-working-as-python-version-and-gives-wrong-results,"I Converted TensorFlow weights to the ONNX model and tried it in C++. But the C++ results didn’t match the Python ones. So, I checked the Python version using TensorRT, and it worked right. You can check this too - I’ve put both Python and C++ codes in a GitHub repository. There, you can also find steps to try it yourself. I’ve made the example as simple as possible, so I think the issue is probably with TensorRT. Weights included in there.Contribute to robosina/C-Tensorrt-Issue development by creating an account on GitHub.TensorRT Version: TensorRT-8.4.3.1, also two different version tested
GPU Type: 2080ti
Nvidia Driver Version: 536.67
CUDA Version:  11.5
CUDNN Version: cudnn-windows-x86_64-8.6.0.163_cuda11
Operating System + Version: Windows 10
Python Version (if applicable): 3.9
TensorFlow Version (if applicable):
PyTorch Version (if applicable):
Baremetal or Container (if container which image + tag):Hi,We recommend that you please try the latest TensorRT version 8.6.1 and let us know if you still face the same issue. Also, please make sure your inference script and if your are doing post processing it is correct.Thank you.@spolisetty I appreciate your response. For your information, I have attempted multiple versions like TensorRT-8.6.1.6 and 8.2. Furthermore, to make testing straightforward, I have simplified the script, eliminating any pre-processing and post-processing in this scenario. The input binding will be filled with constant variable to make sure that we are not doing something wrong between python and c++.Using cudaMemset in here was wrong and now the C++ also generate same responses.Powered by Discourse, best viewed with JavaScript enabled"
445,list-model-that-fully-support-dla,"Hello, recently i worked with DLA and I used to convert some model such as (Yolo, Alphapose-Resnet50) to DLA. But there are some unsupported layer that cause the GPU fallback problem. Can you guys give me some some pretrained model that fully supported by DLA?Thank youHi,
Please check the below links, as they might answer your concerns.This is the revision history of the NVIDIA TensorRT 8.6 Developer Guide.This is the revision history of the NVIDIA TensorRT 8.6 Developer Guide.This is the revision history of the NVIDIA TensorRT 8.6 Developer Guide.
Thanks!@longvuvan083 Also check out the DLA github page for samples and resources. Here we have included some common reference models that you can run fully on DLA: Recipes and tools for running deep learning workloads on NVIDIA DLA cores for inference applications. This topic was automatically closed 14 days after the last reply. New replies are no longer allowed.Powered by Discourse, best viewed with JavaScript enabled"
446,converting-a-pt-file-to-a-engine-file-on-jetson-xavier-nx,"I have my own pretrained pytorch model that I want to convert to a TensorRT model (.engine), I run this python script:import torch
from torch2trt import torch2trtpytorch_model = torch.load(“crowdhuman1600x_yolov5l.pt”)
x = torch.ones((1, 3, 224, 224)).cuda()
trt_model = torch2trt(pytorch_model, [x])
with open(“model.engine”, “wb”) as f:
f.write(trt_model.engine.serialize())but everytime I run it, the following error occurs:Traceback (most recent call last):
File “/home/nvidia/torch2trt/export.py”, line 9, in  pytorch_model = torch.load(“crowdhuman1600x_yolov5l.pt”)*
*File “/home/nvidia/.local/lib/python3.6/site-packages/torch/serialization.py”, line 592, in load return_load(opened_zipfile, map_location, pickle_module, *pickle_load_args)
File “/home/nvidia/.local/lib/python3.6/site-packages/torch/serialization.py”, line 851, in _load
result = unpickler.load()
ModuleNotFoundError: No module named ‘models’How can I solve it?TensorRT Version:  8.2.1.9
GPU Type:  Xavier
CUDA Version:  10.2
Operating System + Version: Ubuntu 18.04
Python Version (if applicable):  3.6.9
PyTorch Version (if applicable):  1.8.0Hi,This looks like a Jetson issue. Please refer to the below samples in case useful.Hello AI World guide to deploying deep-learning inference networks and deep vision primitives with TensorRT and NVIDIA Jetson. - GitHub - dusty-nv/jetson-inference: Hello AI World guide to deployin...An easy to use PyTorch to TensorRT converter. Contribute to NVIDIA-AI-IOT/torch2trt development by creating an account on GitHub.For any further assistance, we will move this post to to Jetson related forum.Thanks!Powered by Discourse, best viewed with JavaScript enabled"
447,onnx-engine-initialisation-build-takes-significantly-longer-in-tensorrt-8-5-vs-8-0,"We have upgraded to 8.5.3 from 8.0.1, and have noticed it takes significantly longer to initialise, parse the onnx, build and serialize the engine. The inference is marginally faster, which is nice, but this slower initializing will cause issues for our tests and users.Is this expected behavior of this version, or a bug?
How can I fix this?Initialization with timing cache:Initialization without timing cache:We are using fp16, but I think a difference can be observed with any optimization profile.I was able to reproduce the increase with trt samples:Interestingly, I am not able to reproduce with trtexec. I can’t see what trtexec is doing differently to the samples, but trtexec takes excessively long compared which it think masks the issue:TensorRT Version: 8.5.3
GPU Type:  NVIDIA GeForce RTX 3070 Laptop GPU
Nvidia Driver Version: 525.125.06
CUDA Version: 11.4
Operating System + Version: Ubuntu 20.04Powered by Discourse, best viewed with JavaScript enabled"
448,profile-backward-pass-of-dnn,"Hi, I wanted to profile the per layer execution times for backward pass of a DNN. Existing profilers like pytorch profiler give operator-level stats. But, I wanted a layerwise time. So, I wanted to know if Nvidia has a profiler to do this ?
Does DLPROF : DLProf User Guide - NVIDIA Docs help with this ?Device : Orin Jetpack v5.0.1
Using : Pytorch v2.0.0@dusty_nv Would you be able to help please?Hi @vinayakah ,
Apologies for the delay.
Let me check on this, will get back to you shortly.thanksPowered by Discourse, best viewed with JavaScript enabled"
449,ssd-mobile-net-v3-to-tensorrt,"can i convert ssd-mobilenet-v3-small/ssd-mobilenet-v3-large (https://github.com/tensorflow/models/blob/master/research/object_detection/g3doc/detection_model_zoo.md) pretrained tensorflow model to TensorRT Model.i tried using this link(GitHub - NVIDIA-AI-IOT/tf_trt_models: TensorFlow models accelerated with NVIDIA TensorRT).but got error .def global_pool(input_tensor, pool_op=tf.compat.v2.nn.avg_pool2d): AttributeError: module ‘tensorflow._api.v1.compat’ has no attribute ‘v2’but it successfully work for ssd_mobilenet_v1_coco,ssd_inception_v2_cocoHi,Try converting your model to ONNX instead using tf2onnx and then convert to TensorRT using ONNX parser. Any layer that are not supported needs to be replaced by custom plugin.
https://github.com/onnx/tensorflow-onnx
https://github.com/onnx/onnx-tensorrt/blob/master/operators.mdYou can also convert the model to TRT using TF-TRT and serialize it to a .plan file.
Save model file using TF-TRT 2.0
https://docs.nvidia.com/deeplearning/frameworks/tf-trt-user-guide/index.html#worflow-with-savedmodelThanksHi,I converted successfully ssd-mobilenet-v3-large to tensorrt uff file. You can try it. Hope it will help you.convert-to-uff --input-file frozen_inference_graph.pb -O NMS -p config.pyIf you have issue with Cast operation, checkout tensorflow models to branch 1.12.0 (from 1.13.0, tensorflow will Cast operation automatically to trained model) , add some SSD Mobilenet V3 stuffs, then export frozen model and convert it to uff file.Hello @michaelnguyen1195
I have two questions
1: I have trained the ssd-mobilenet-v1 on a custom dataset
how can I convert that model to a TensorRT engine ?!
2:I have converted the yolov5 model to onnx and convert the .onnx model to tensorRT engine (.trt file),
now
how can I inference(make prediction, draw bounding box) ?!Powered by Discourse, best viewed with JavaScript enabled"
450,deploying-riva-models-on-vultr-a100-vgpu-instance-fails-with-cuda-errors,"Hardware - GPU (A100 vGPU - 10GB VRAM and 1/7 GPU allocated). Vultr hosting
Hardware - CPU
Operating System: Ubuntu 22.04
Riva Version: 2.11
Nvidia Driver Version: 525.85.05When deploying with the above build it fails with the following errors:[06/10/2023-10:33:04] [TRT] [E] 1: [graphContext.h::~MyelinGraphContext::35] Error Code 1: Myelin (No Myelin Error exists)
[06/10/2023-10:33:04] [TRT] [W] Skipping tactic 0x0000000000000000 due to Myelin error: CUDA error 800 failed to create CUDA stream[06/10/2023-10:33:18] [TRT] [E] 4: [optimizer.cpp::computeCosts::3710] Error Code 4: Internal Error (Could not find any implementation for node {ForeignNode[746 + (Unnamed Layer* 20) [Shuffle]…MatMul_269]} due to insufficient workspace. See verbose log for requested sizes.)
[06/10/2023-10:33:18] [TRT] [E] 2: [builder.cpp::buildSerializedNetwork::738] Error Code 2: Internal Error (Assertion engine != nullptr failed. )I’ve tried different models and all fail with the same error. I tried setting nn.trt_max_workspace_size to 6
GB because of the error about insufficient workspace but it made no difference. Running exactly the same build and deploy scripts locally on my home server with RTX3060 works fine.Hi @steve.pritchardThanks for your interest in RivaApologies on the error
525 Drivers come with Cuda 12 installed
Riva required CUDA 11.8.89 , can you downgrade the CUDA version and tryhttps://docs.nvidia.com/deeplearning/riva/user-guide/docs/support-matrix.html#id2The Release Notes for the CUDA Toolkit.ThanksThanks for the reply. Unfortunately the driver version is fixed on the Vultr vGPU platform (I checked with their support). However, I thought that the Nvidia host drivers were backwards compatible with docker container drivers, ie the docker CUDA 11.8.89 should work with host CUDA 12?Powered by Discourse, best viewed with JavaScript enabled"
451,how-to-specify-some-not-all-layer-precision-in-tensorrt,"When i used following codes to specify some layer precison to fp16, but i found all layer precision will be changed to fp16. Is there any wrong in my codes?
After build, i used inspector to print all layer and found all layer weight is Half.TensorRT Version:  TensorRT-8.6.1.6
GPU Type:  NVIDIA A10
Nvidia Driver Version: 525.105.17
CUDA Version: cuda_11.8.r11.8/compiler.31833905_0
CUDNN Version: 8.7.0
Operating System + Version: Ubuntu 18.04.6 LTS
Python Version (if applicable): python3.10
TensorFlow Version (if applicable):
PyTorch Version (if applicable): torch2.0
Baremetal or Container (if container which image + tag):Please attach or include links to any models, data, files, or scripts necessary to reproduce your issue. (Github repo, Google Drive, Dropbox, etc.)Please include:Hi,It appears you are doing config.set_flag(trt.BuilderFlag.FP16), which may apply FP16 precision to the entire network. You can remove that global flag and set the precision for individual layers, as you have done.
Also, when setting the precision for a specific layer, you need to ensure that the layer’s input and output types are also set accordingly.
For more information, please refer to:This is the revision history of the NVIDIA TensorRT 8.6 Developer Guide.Thank you.Thanks for answer.
But if i remove fp16 flag, will have following error:Found solution.## Description
When i used following codes to specify some layer precison to fp…16, but i found all layer precision will be changed to fp16. Is there any wrong in my codes?
After build, i used inspector to print all layer and found all layer weight is Half.
```
logger = trt.Logger(trt.Logger.VERBOSE)
builder = trt.Builder(logger)
ctypes.CDLL(plugin_path)
trt.init_libnvinfer_plugins(logger, '')
network = builder.create_network(1 << int(trt.NetworkDefinitionCreationFlag.EXPLICIT_BATCH))
parser = trt.OnnxParser(network, logger)
success = parser.parse_from_file(onnx_path)
for idx in range(network.num_layers):
  layer = network.get_layer(idx)
  if idx == 1600:    # for debug
    layer.precision = trt.float16
    layer.set_output_type(0, trt.DataType.HALF)
config = builder.create_builder_config()
config.max_workspace_size = 4 << 30  # 4GB
config.profiling_verbosity = trt.ProfilingVerbosity.VERBOSE
config.set_flag(trt.BuilderFlag.PREFER_PRECISION_CONSTRAINTS)
config.set_flag(trt.BuilderFlag.DIRECT_IO)
config.set_flag(trt.BuilderFlag.REJECT_EMPTY_ALGORITHMS)
config.set_flag(trt.BuilderFlag.STRICT_TYPES)
config.clear_flag(trt.BuilderFlag.TF32)
config.set_flag(trt.BuilderFlag.FP16)
serialized_engine = builder.build_serialized_network(network, config)
with open(trt_path, 'wb') as f:
  f.write(serialized_engine)

# Use inspector to print all layer, i found all layer weight is Half.
with open('./generator.trt', 'rb') as f:
  trt_engine = trt.Runtime(trt.Logger(trt.Logger.ERROR)).deserialize_cuda_engine(f.read())
  inspector = trt_engine.create_engine_inspector()
  print('trt_engine layer_info:\n{}'.format(
    inspector.get_engine_information(trt.LayerInformationFormat.JSON)
    ))
  trt_ctx = trt_engine.create_execution_context()
```


## Environment

**TensorRT Version**:  TensorRT-8.6.1.6
**GPU Type**:  NVIDIA A10
**Nvidia Driver Version**: 525.105.17
**CUDA Version**: cuda_11.8.r11.8/compiler.31833905_0
**CUDNN Version**: 8.7.0
**Operating System + Version**: Ubuntu 18.04.6 LTS
**Python Version (if applicable)**: python3.10
**TensorFlow Version (if applicable)**: 
**PyTorch Version (if applicable)**: torch2.0
**Baremetal or Container (if container which image + tag)**: 


## Relevant Files

Please attach or include links to any models, data, files, or scripts necessary to reproduce your issue. (Github repo, Google Drive, Dropbox, etc.)

## Steps To Reproduce



Please include:
  * Exact steps/commands to build your repro
  * Exact steps/commands to run your repro
  * Full traceback of errors encounteredThis topic was automatically closed 14 days after the last reply. New replies are no longer allowed.Powered by Discourse, best viewed with JavaScript enabled"
452,unable-to-serialize-vpi-warp-type-object-in-python-3-using-json-pickle-and-marshal-vision-programming-interface-jetson,"Hi,I tried serializing vpi.warpmap object using json.dumps(), pickle.dumps() and marshal.dumps() methods. However, I am getting the error - ‘Object of type WarpMap is not JSON serializable’.
Are VPI objects serializable? My end goal is to store the warpmap in a file and retrieve it later.
Can you kindly help.ThanksThe vast majority of vpi questions on these forums are on the Jetson forums.  You may wish to pick one of those forums to post vpi questions pertaining to jetson.Thanks for letting me know. I hope I posted it in the right forum now - Unable to serialize vpi.warp type object in python 3 using json, pickle and marshal - Vision Programming Interface, JetsonThis topic was automatically closed 14 days after the last reply. New replies are no longer allowed.Powered by Discourse, best viewed with JavaScript enabled"
453,unable-to-install-older-tensorrt-versions-using-nvidia-cuda-apt-repository,"Unable to install older TensorRT versions using the NVIDIA CUDA APT repository.When installing the tensorrt=8.5.1.7-1+cuda11.8 package, apt-get fails with the following.This is a new installation of 22.04 with no previous CUDA packages installed.  According to dpkg -l, there are no held or broken packages.It is desired to use this installation in a script to setup the appropriate environment for developers.  Access to the TensorRT tar archive or local deb repository is not permitted unless you are logged in, which makes it difficult to use that method.TensorRT Version: 8.5.1.7
GPU Type: 4090
Nvidia Driver Version: 525.78.01
CUDA Version: 11.8
CUDNN Version: 8.6.0.163
Operating System + Version: Ubuntu 22.04
Python Version (if applicable): N/A
TensorFlow Version (if applicable): N/A
PyTorch Version (if applicable): N/A
Baremetal or Container (if container which image + tag): BaremetalConfigure the CUDA repository by installing https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64/cuda-keyring_1.0-1_all.debInstall CUDA toolkit 11.8 and cuDNN 8.6.0:sudo apt-get install cuda-toolkit-11-8 libcudnn8=8.6.0.163-1+cuda11.8Install TensorRT 8.5.1.7sudo apt-get install tensorrt=8.5.1.7-1+cuda11.8Hi,
Please refer to the installation steps from the below link if in case you are missing on anythingThis NVIDIA TensorRT 8.5.3 Installation Guide provides the installation requirements, a list of what is included in the TensorRT package, and step-by-step instructions for installing TensorRT.
Also, we suggest you to use TRT NGC containers to avoid any system dependency related issues.NVIDIA TensorRT is a C++ library that facilitates high-performance inference on NVIDIA graphics processing units (GPUs). TensorRT takes a trained network and produces a highly optimized runtime engine that performs inference for that network.Thanks!I’ve read those documents and there doesn’t seem to be anything other than suggesting the packages are held after they are installed.  The problem is that it can’t be installed to begin with.If it helps, here is a Dockerfile that can be used to reproduce this issue in a clean environment:@hyoo_dv.This is because apt always tries to install the latest version.
Maybe Nvidia will fix the packages dependencies declaration one day but while we wait: It works if you specify all dependencies versions manually.Ex :All credits goes to Jesus Alvarez: Unmet dependencies issue when installing TensorRT from a cuda:10.2-cudnn7-devel-ubuntu18.04 (#75) · Issues · nvidia / container-images / cuda · GitLabPowered by Discourse, best viewed with JavaScript enabled"
454,tensorrt-int8-calibration-error-indexerror-map-base-at,"Hi, I’m trying to apply a Bisenetv2 model based on Python TensorRT 7. The model I’ve used is Bisenetv2, and it’s already converted from TensorFlow pb to onnx with tf2onnx. The model worked fine when I used FP32 and FP16 model.
However, when I use INT8, TensorRT outputs an error message “IndexError: _Map_base::at” when executing “builder.build_engine(network, config)”.The following information is the setting in my computer:
- Linux distribution: Ubuntu 16.04
- GPU: 2080Ti
- Nvidia driver version: 440.33.01
- CUDA version: 10.2
- CuDNN version: 8.0.4
- Python version: Python 3.5.2
- TensorFlow version: 1.14.0
- onnx version: 1.6.0
- tf2onnx version: 1.5.4
- TensorRT version: 7.1.3.4The attachment contains the code and the model I’ve used.
TensorRT7_bisenetv2_int8_issued.tar.gz (20.1 MB)I inspect the dimension of the input image and the model, but still cannot figure out the problem. Can someone help? Thank you.Hi @RahnRYHuang,Could you please share complete error logs and confirm whether are you able to run with onnx-runtime.Thank you.Meet same problem, it’s ok when parameter is fp32, but get error message when parameter is int8.Hello, did u solve that problem?Check if you loaded calibration set properly. In my case I was feeding model with empty calibration set (lenght = 0) so this error has appeared.hi, RahnRYHuang
I have the same problem. did you solved it?Powered by Discourse, best viewed with JavaScript enabled"
455,nvidia-maxine-ar-bodytrack-on-jetson-boards,"HelloWe would like to know whether it’s possible or not to run BodyTrack on Jetson family devices (ARM based) such as Jetson Nano and Xavier NX ?It might have a significant advantage over BlazePose/Mediapipe that also has 3d pose estimation using only 1 monocular camera.In addition, it would be really help to have an option to train the models for our custom annotated image DB (similar to this question How can we train and use the custom model for the maxine?).ThanksHi @neilqy60u ,We don’t currently support Maxine on Jetson devices. That being said, if you are interested in running models on them please check out NVIDIA’s DeepStream SDK.Link: https://developer.nvidia.com/deepstream-sdk
Link: NVIDIA DeepStream SDK Developer Guide — DeepStream 6.1.1 Release documentationHi @tvarshneyAs far as I know  and according to Nvidia Documentation (DeepStream and TRT_Pose) these are 2D human pose estimators but not 3D like Maxine AR BodyTrack. Am I mistaken somewhere ? We are interested in 3D joint positions but not 2D that is why I am asking about Maxine support on Jetson boards.Another question. Can TRT_Pose/DeepStream use Maxine AR BodyTrack models (.trtpkg) to estimate 3D  human pose? TRT_Pose uses .pth files and according to its documentation
“Training scripts to train on any keypoint task data in MSCOCO format. This means you can experiment with training trt_pose for keypoint detection tasks other than human pose.”Thankshttps://developer.nvidia.com/blog/creating-a-human-pose-estiation-application-with-deepstream-sdk/This is a sample DeepStream application to demonstrate a human pose estimation pipeline. - GitHub - NVIDIA-AI-IOT/deepstream_pose_estimation: This is a sample DeepStream application to demonstrate ...Real-time pose estimation accelerated with NVIDIA TensorRT - GitHub - NVIDIA-AI-IOT/trt_pose: Real-time pose estimation accelerated with NVIDIA TensorRTMight be I found an alternative here: GitHub - RArbore/Deep-Learning-Hearing-Aid: Various components for using an FCNN to de-noise audio in real-time on the Jetson Nano 2GB. . Anyway it has an issue, the model “model.pt” is lacking in the repository.Powered by Discourse, best viewed with JavaScript enabled"
456,vpi-template-matching,"Is it possible to get minimum and maximum correlation values after templating matching ? just like in opencv :  res = cv.matchTemplate(image,template,cv.TM_CCORR)
min_val, max_val, min_loc, max_loc = cv.minMaxLoc(res)
As per VPI documentation : vpi.Image.minmaxloc() return locations , just wanted to know is there any ways to get minimum/maximum values?Powered by Discourse, best viewed with JavaScript enabled"
457,torchtensorrt-lowering-performance-in-real-time-inference,"A clear and concise description of the bug or issue.TensorRT Version: 8.4.2
GPU Type: A100
Nvidia Driver Version: 465.19.01
CUDA Version: 11.3
CUDNN Version: 8
Operating System + Version: SLES “15-SP2” in host machine
Python Version (if applicable): 3.8
PyTorch Version (if applicable): 1.13.0a0+d321be6
Baremetal or Container (if container which image + tag): nvcr.io/nvidia/pytorch:22.08-py3Hi. I convert pytorch retinaface  and arcface model to TensorRT via  torch_tensorrt library. Everything is okay but after some iterations inference is freezing and the time for handling the image is badly increased (>10x).
Snippet of inference simulation is here:But after some iterations and time return this:ret time is : 0.1150820255279541 <-----
arc time is : 0.0020606517791748047
global time is : 0.11714410781860352I try changing the clock freq to the max of A100(1410MHz) but nothing changes from the default(765MHz).
It will be great if you support fixing this bug. Thanks in advance!!!Preformatted textHi,We recommend that you please reach out to Issues · pytorch/TensorRT · GitHub to get better help for the above issue.Thank you.Powered by Discourse, best viewed with JavaScript enabled"
458,getting-error-for-tensorrt-module-not-found,"while working with trt pose repository "" GitHub - NVIDIA-AI-IOT/trt_pose: Real-time pose estimation accelerated with NVIDIA TensorRT ""
getting error while running command : sudo python3 setup.py installerror : ModuleNotFoundError: No module named ‘tensorrt’.I tried installing tensorrt with this command : “pip install nvidia-pyindex
pip install nvidia-tensorrt” and its installed “nvidia-tensorrt-99.0.0 and tensorrt-8.6.1”.But after this also it is throwing the same error that no module named tensorrt.VM specifications for your reference : NVIDIA-SMI 525.85.12    Driver Version: 525.85.12    CUDA Version: 12.0what is the correct version of tensorrt that i need to install with this vm specification and what is the command for the installation.Hi,Please checkout Support Matrix :: NVIDIA Deep Learning TensorRT Documentation.Also, we recommend you follow the installation guide.This NVIDIA TensorRT 8.6.1 Installation Guide provides the installation requirements, a list of what is included in the TensorRT package, and step-by-step instructions for installing TensorRT.Thank you.To setup dGPU in Ubuntu 20.04 on Nvidia T4 machine
https://docs.nvidia.com/metropolis/deepstream/dev-guide/text/DS_Quickstart.htmlsudo apt-get updateInstall Dependencies
sudo apt install 
libssl1.1 
libgstreamer1.0-0 
gstreamer1.0-tools 
gstreamer1.0-plugins-good 
gstreamer1.0-plugins-bad 
gstreamer1.0-plugins-ugly 
gstreamer1.0-libav 
libgstreamer-plugins-base1.0-dev 
libgstrtspserver-1.0-0 
libjansson4 
libyaml-cpp-dev 
libjsoncpp-dev 
protobuf-compiler 
gcc 
make 
git 
python3Install CUDA Toolkit 11.8
sudo apt-key adv --fetch-keys https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2004/x86_64/3bf863cc.pub
sudo add-apt-repository “deb Index of /compute/cuda/repos/ubuntu2004/x86_64 /”
sudo apt-get update
sudo apt-get install cuda-toolkit-11-8Install NVIDIA driver 525.85.12
wget https://us.download.nvidia.com/tesla/525.85.12/NVIDIA-Linux-x86_64-525.85.12.run
chmod 755 NVIDIA-Linux-x86_64-525.85.12.run
sudo ./NVIDIA-Linux-x86_64-525.85.12.run --no-cc-version-checkInstall PyTorch
Start Locally | PyTorch
pip3 install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118Install tensorrt
Installation Guide :: NVIDIA Deep Learning TensorRT Documentation
a. wget https://developer.nvidia.com/downloads/compute/machine-learning/tensorrt/secure/8.6.1/tars/TensorRT-8.6.1.6.Linux.x86_64-gnu.cuda-12.0.tar.gz
b. tar -xzvf TensorRT-8.6.1.6.Linux.x86_64-gnu.cuda-12.0.tar.gz
c. cd TensorRT-8.6.1.6/lib
d. pwd → output (/home/mastekinnovation/TensorRT-8.6.1.6/lib)
e. export LD_LIBRARY_PATH=/home/mastekinnovation/TensorRT-8.6.1.6/lib
f. cd TensorRT-8.6.1.6/python
g. python3 -m pip install tensorrt-8.6.1-cp38-none-linux_x86_64.whl
h. cd TensorRT-8.6.1.6/uff
i. python3 -m pip install uff-0.6.9-py2.py3-none-any.whl
j. cd TensorRT-8.6.1.6/graphsurgeon
k. python3 -m pip install graphsurgeon-0.4.6-py2.py3-none-any.whl
l. cd TensorRT-8.6.1.6/onnx_graphsurgeon
m. python3 -m pip install onnx_graphsurgeon-0.3.12-py2.py3-none-any.whlInstall torch2trt
GitHub - NVIDIA-AI-IOT/trt_pose: Real-time pose estimation accelerated with NVIDIA TensorRT
git clone GitHub - NVIDIA-AI-IOT/torch2trt: An easy to use PyTorch to TensorRT converter
cd torch2trt
sudo python3 setup.py install --pluginsAt this point we are getting “tensorrt module not found”
Please helpHi,
Please refer to the installation steps from the below link if in case you are missing on anythingThis NVIDIA TensorRT 8.6.1 Installation Guide provides the installation requirements, a list of what is included in the TensorRT package, and step-by-step instructions for installing TensorRT.
Also, we suggest you to use TRT NGC containers to avoid any system dependency related issues.NVIDIA TensorRT is a C++ library that facilitates high-performance inference on NVIDIA graphics processing units (GPUs). TensorRT takes a trained network and produces a highly optimized runtime engine that performs inference for that network.Thanks!Powered by Discourse, best viewed with JavaScript enabled"
459,vad-endpointing-configurations,"Please provide the following information when requesting support.Hardware - T4
Hardware - CPU x86_64
Operating System - Debian GNU/Linux 11 (bullseye)
Riva Version: 2.12.1
TLT Version (if relevant)
How to reproduce the issue ? (This is for errors. Please share the command and the detailed log here)Hi! I’m building the speech-recognition pipeline, with the en-US conformer with the high throughput configurations.
I’m also using the marblenet VAD, and I have adjusted the endpointing to have a 2000ms stop_history.I have some questions/issue i would love to resolve:Many thanks.Additionaly, is the correct way to work using both endpointing and VAD or only one of them? documentation is quite sparse regarding this.Do I need to specify anything in the configuration when doing inference or will VAD automatically be chosen if it was passed to the riva build speech-recognition paramters?Powered by Discourse, best viewed with JavaScript enabled"
460,any-plans-for-a-tensorrt-plugin-that-can-directly-use-a-dynamic-shape-tensor,"(Note: This is question about a feature/plans for a feature. I have no issue implementing a custom plugin with dynamic shapes)I have a custom plugin layer, “MyPlugin” (Inherited from IPluginV2DynamicExt). I want the output of MyPlugin to be the same shape as some tensor, “X”, in my network (the shape of X can be dynamic). However, the output of MyPlugin does not directly depend on X. Regardless, I pass tensor X as an input to MyPlugin so that I can use it’s shape in MyPlugin::getOutputDimensions(). This is okay, but makes my plugin more confusing to use. Are there any plans for a feature that would allow me to directly just use the shape of X (similar to how IResize layer works)?TensorRT Version: 8.2.3.0Hi,
Please refer to below links related custom plugin implementation and sample:This Samples Support Guide provides an overview of all the supported NVIDIA TensorRT 8.5.3 samples included on GitHub and in the product package. The TensorRT samples specifically help in areas such as recommenders, machine comprehension, character...TensorRT is an SDK for high performance, deep learning inference. It includes a deep learning inference optimizer and a runtime that delivers low latency and high throughput for deep learning…While IPluginV2 and IPluginV2Ext interfaces are still supported for backward compatibility with TensorRT 5.1 and 6.0.x respectively, however, we recommend that you write new plugins or refactor existing ones to target the IPluginV2DynamicExt or IPluginV2IOExt interfaces instead.Thanks!Hi, thank you for the response. However, I don’t think my question was clear. I have implemented my plugin with IPluginV2DynamicExt already.I was wondering if, in the future, there would be support for a plugin that can use the output of an IShapeLayer to determine the output dimensions of the plugin. I.e. the way IResizeLayer and IShuffleLayer work Developer Guide :: NVIDIA Deep Learning TensorRT Documentation. In other words, I want to access the values of a shape tensor when determining the output dimensions of my plugin. The PluginV2DynamicExt interface only allows me to access the shape of my inputs, but the shape that I want isn’t necessarily the shape of one of my inputs.I want one of the inputs to my plugin to be “shape of X”, and I want the output dimensions of my plugin to depend on this shape.My workaround right now is to just pass in X to my plugin, but this doesn’t convey the meaning of my plugin very well.Hi @adam.alcolado ,
We have noted your suggestion and passed it to the Engg Team,
Please stay tuned to the TRT Release Notes for latest updates.ThanksThis topic was automatically closed 14 days after the last reply. New replies are no longer allowed.Powered by Discourse, best viewed with JavaScript enabled"
461,how-to-reduce-noise-in-asr-transcriptions,"Please provide the following information when requesting support.Hardware - GPU: T4
Operating System: Ubuntu 20.04
Riva Version: 2.10I""m using the CTC conformer models in Spanish (es-US) to do streaming recognition through a telephone line. However, when there is background noise, spurious words appear in ASR transcriptions. In the releases it is mentioned that there is an option to use the neural-based voice activity detector to avoid this problem, how can I use it? Is there any other way to suppress the noise without doing fine-tuning?Thanks.HI @nharoSincere Apologies for the delay,I will check with the internal team whether noise reduction is possibleThanksHi @nharoWe do have a Noise Robust es-US 3.1 model link below, could you confirm whether you are already using this model?ThanksYes, that is the model I am using, however the noise affects transcriptions. Sometimes it transcribes background noise when no one is speaking, and other times it transcribes both, noise and user audio. Is there a filter or some kind of score for streaming recognition?Powered by Discourse, best viewed with JavaScript enabled"
462,about-pip3-install-riva-api-2-8-1-py3-none-any-whl,"Please provide the following information when requesting support.Hardware - GPU (A100/A30/T4/V100) A10g
Hardware - CPU
Operating System Ubuntu
Riva Version 2.8.1
TLT Version (if relevant)
How to reproduce the issue ? (This is for errors. Please share the command and the detailed log here)
Hello everyone!During my Riva installation, everything works except this one “pip3 install riva_api-2.3.0-py3-none-any.whl”,Deployed with NVIDIA GPU-Optimized AMI on AWSReceived error message:
“ERROR: Could not install packages due to an EnvironmentError: [Errno 2] No such file or directory: ‘/home/ubuntu/riva_quickstart_v2.8.1/riva_api-2.8.1-py3-none-any.whl’”Could anyone help me on that?Thanks!Hi @hulkdesignThanks for your interest in RivaApologies, riva_api-<x.y.z>-py3-none-any.whl is not longer packed with riva quickstartAre you trying to use riva-sample app, can you share the link of the tutorial followedThanksGot you. The links are:Building a Speech-Enabled AI Virtual Assistant with NVIDIA Riva on Amazon EC2 | NVIDIA Technical BlogPlease advise how to proceed from there, thanks.Here’s another link, the system doesn’t allow more than one linksample-apps/virtual-assistant at main · nvidia-riva/sample-apps · GitHubHi @hulkdesignIn replacement of unavailable riva_api-<x.y.z>-py3-none-any.whlPlease install the below packageAnd use the following branchsven-update-riva-api-calls/virtual-assistantSample applications using NVIDIA Riva Skills. Contribute to nvidia-riva/sample-apps development by creating an account on GitHub.ThanksThanks for the input.Upon following each individual step and executing python3 main.py, it prompted “ModuleNotFoundError: No module named ‘riva_api’”Looks still like the riva api file issue. Could you help further on this?Thanks!BTW, just remembered that during the pip installation, it displayed like wheel file v2.8.0 but QuickStart version is 2.8.1. Not sure if the version mismatch is the reason.@hulkdesignApologies that you are facing “ModuleNotFoundError: No module named ‘riva_api’”
Can you share the complete tracebackAre you sure you are using sven-update-riva-api-calls branch
can you check with git branchThanksI have same error too, here is the complete tracebackPowered by Discourse, best viewed with JavaScript enabled"
463,inference-time-becomes-longer-when-doing-non-continuous-fp16-or-int8-inference,"Inference time becomes longer when doing “non-continuous” fp16 or int8 inference.I think the issue is not model correlated.
Please let me know if you cannot reproduce the issue.Xavier
TensorRT Version : 7.1.3-1
CUDA Version : cuda10.2
CUDNN Version : 8.0Hi, Please refer to the below links to perform inference in INT8
https://github.com/NVIDIA/TensorRT/blob/master/samples/opensource/sampleINT8/README.md1777.21 KB
Thanks!Thanks for the reply.
But I have no problem about the INT8 calibration.
This topic concerns about the inference performance.
Not only INT8 but also FP16 has the same issue.I see there is “warm-up” mechanism in some topics.Is it the reason that slow down the “non-continuous” inference?
Would you please explain more about “warm-up” mechanism? For example, under what conditions TensorRT will “warm-up” again.By the way I use FP32 input for both fp32 and fp16/int8 models. Please let me know if it is a problem.@zhaofengming.zfm,I believe they are referring to do a few warm-up runs of common.do_inference_v2() before starting the timing. The very first run usually takes a long time in setting up stuff.Thank you.@spolisettyThanks for the reply. I see what you mean.And I found there is same “warm-up” process in trtexec module when doing time measurement, refer to inferenceLoop function of /usr/src/tensorrt/samples/common/sampleInference.cppI totally understand that,
in order to obtain better and more accurate measurement results, it’s better to do a few warm-up before starting the timing, because “The very first run usually takes a long time in setting up stuff.”But I think these scenarios are all “continuous” inference scenarios which I mentioned in this topic. After a few warm-up I can get the best performance when doing “continuous” inference.In the case of “non-continuous” inference scenarios (the time between two inference will be 10ms or 20ms depending on preprocess time on each input), I observed that the “The very first run” and “warm-up” effects appeared again and again.And I also observed that when doing two inference parallelly, it can reduce the “The very first run” and “warm-up” effects. It seems that the two models using the GPU/TensorRT alternately, so the GPU/TensorRT didn’t “cold-down”, thus didn’t need “warm-up” again.So would you please let me know under what conditions (for example within xx ms that GPU/TensorRT is not working) TensorRT or GPU will “warm-up” again?Hi @zhaofengming.zfm,Sorry for the delayed response. It’s hard to provide suggestions based on this info, we recommend you to please share minimal issue repro scripts/model for better debugging.Thank you.Hi @spolisettyprofiler.7z (1.9 KB)Please check the attachment.I cannot upload the model file due to the size limitation.
Get it from here:GitHub - ultralytics/yolov5: YOLOv5 🚀 in PyTorch > ONNX > CoreML > TFLiteI think the issue is not model correlated.
You can use any model to reproduce it.yolov5.diff (447 Bytes)I met some issue when using the original script.
Apply the attachment patch before excuting the following script.
python3 export.py --weights yolov5s.ptHi,Have you observed same issue when you try using trtexec, if yes could you share both verbose logs.No.
trtexec do “continuous” inference additionally skip serveral premier frames so I haven’t seen any delay effect.Could you please share minimum issue repro script you’re using.Thank you.It had been shared on Aug 3. Please check the above reply.Sorry we couldn’t find inference script you’re using in 7z file you’ve shared. Please provide us inference script with sample to reproduce the issue.Thank you.profiler.7z.001 (10 MB)profiler.7z.002 (10 MB)profiler.7z.003 (10 MB)profiler.7z.004 (4.7 MB)Hi @spolisettyPlease download profiler.7z.001 - profiler.7z.004 and uncompress it.
cd profiler/build
./profilerPowered by Discourse, best viewed with JavaScript enabled"
464,outputs-of-cudnn-depthwise-conv-mismatch-tensorflow-depthwise-conv,"The outputs of cudnn depthwise conv mismatched the outputs of tensorflow layer.
Is there anything wrong when I used cudnn?
Thanks a lot for your help!conv_depthwise.cpp (6.1 KB)Hi @user126573 ,
Can you please help us with the API log and code to repro on Tensorflow?
Thanksconv.zip (13.9 KB)Unzip the directory and runCUDNN codes are in conv_depthwise.cpp. Thanks a lot for your help!Can anyone help me check the implementation?Powered by Discourse, best viewed with JavaScript enabled"
465,health-ready-check-failed,"Hardware - GPU (A100/A30/T4/V100): NVIDIA RTX 3080
Hardware - CPU: Intel(R) Xeon(R) W-2225 CPU @ 4.10GHz
Operating System: Ubuntu 20.04.4 LTS
Riva Version: 2.8.0
TLT Version (if relevant)
How to reproduce the issue ?When running the command
bash riva_start.sh
I get this error:
Health ready check failed.Have attached the detailed logs in the text file.
riva_log.txt (146.7 KB)Thank you so much for your help!Hi @lohwenyeThanks for your interest in RivaThanks for sharing the riva-speech docker logsI find that one of the model is not presentE1208 07:12:27.101070 106 model_repository_manager.cc:996] Poll failed for model directory 'riva-trt-conformer-en-US-asr-streaming-am-streaming': failed to open text file for read /data/models/riva-trt-conformer-en-US-asr-streaming-am-streaming/config.pbtxt: No such file or directoryIf you are using the default (found in riva_model_loc variable inside config.sh)Please run
docker volume rm riva-model-repo
and
bash riva_clean.shAnd retry the setup again and let us know if you still face the issueThanksHi Rvinobha,Thanks for the advice!I have done the steps as suggested, but still face the same issue with Health ready check failed.Have attached the latest docker logs.
riva_log2.txt (78.0 KB)Thank you so much for your help!HI @lohwenyeApologies that did’nt help
Will check with the internal team on thisThanks for sharing the docker logs riva-speech,
Request to also share theOnce shared I will check with the internal teamThanksI’m also experiencing the same “ Health Ready check failed” when running riva_start.sh. When checking the docker logs I see that it can’t connect to any of the remote hosts. I’m following the embedded QuickStart guide on Xavier NXPowered by Discourse, best viewed with JavaScript enabled"
466,graph-composer-2-0-0-v4l2src-typeerror-docker-workflow-option,"Attempting to create a simple pipeline in GC sing a USB camera (logitech brio) (have developed the same in python and successfully used with trained TAO model).Review GTC22 Video regarding getting started and created a v4l2src using the generate extension. Added  component to graph but will not run. Below errors2022-05-24 21:43:52  [Error] [asyncio] [/tmp/.tmpNvXgRaphComposer/run/kit/python/lib/python3.7/asyncio/base_events.py:1619] Task exception was never retrieved
2022-05-24 21:43:52  [Error] [asyncio] future: <Task finished coro=<RunGraphWindow.async_on_run_clicked() done, defined at /tmp/.tmpNvXgRaphComposer/run/composer/omni.exp.graph.composer/omni/exp/graph/composer/run_graph_window.py:356> exception=TypeError(‘can only concatenate str (not “NoneType”) to str’)>
2022-05-24 21:43:52  [Error] [asyncio] Traceback (most recent call last):
2022-05-24 21:43:52  [Error] [asyncio]   File “/tmp/.tmpNvXgRaphComposer/run/composer/omni.exp.graph.composer/omni/exp/graph/composer/run_graph_window.py”, line 359, in async_on_run_clicked
2022-05-24 21:43:52  [Error] [asyncio]     self._logger.info(""Running graph "" + self._graph_path)
2022-05-24 21:43:52  [Error] [asyncio] TypeError: can only concatenate str (not “NoneType”) to strIt does not matter what components I add after - it always fails with same error.Any help gratefully received.CheersDid you happen to find a solution for this? As of now I am not able to even create the extension for v4l2src.Powered by Discourse, best viewed with JavaScript enabled"
467,the-detected-cuda-version-12-0-mismatches-the-version-that-was-used-to-compile-pytorch-11-4-please-make-sure-to-use-the-same-cuda-versions,"Complete ERROR message :
RuntimeError:   The detected CUDA version (12.0) mismatches the version that was used to compile  PyTorch (11.4). Please make sure to use the same CUDA versions.
Hi Team,To solved this issue we installed cuda-12.0 from the below link but in nvcc --version cuda is still showing as 11.4.Resources CUDA Documentation/Release NotesMacOS Tools Training Sample Code Forums Archive of Previous CUDA Releases FAQ Open Source PackagesSubmit a BugTarball and Zip Archive DeliverablesAlso tried exporting the downloaded updated cuda location, still the nvcc version shows as 11.4, Please check and advice how to proceed from here:export CUDA_HOME=/usr/local/cuda-12.0/image764×233 46.7 KBPowered by Discourse, best viewed with JavaScript enabled"
468,triton-server-died-before-reaching-ready-state-terminating-riva-startup,"Please provide the following information when requesting support.Hardware - GPU (A100)
Hardware - CPU
Operating System: Ubuntu 20.04
Riva Version: 1.6.0-beta
TLT VersionHiI would like to test Riva on the workstation, since there is no internet access on this workstation, here is what I’ve done:then I got an errorTriton server died before reaching ready state. Terminating Riva startup.here is the complete log message:NVIDIA Release 21.09 (build 27567456)Copyright (c) 2018-2021, NVIDIA CORPORATION.  All rights reserved.Various files include modifications (c) NVIDIA CORPORATION.  All rights reserved.
NVIDIA modifications are covered by the license terms that apply to the underlying
project or file.Riva waiting for Triton server to load all models…retrying in 1 second
I1007 07:32:54.182862 70 metrics.cc:228] Collecting metrics for GPU 0: NVIDIA A100-SXM-80GB
I1007 07:32:54.183331 70 tritonserver.cc:1658]
±---------------------------------±---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+
| Option                           | Value                                                                                                                                                                                  |
±---------------------------------±---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+
| server_id                        | triton                                                                                                                                                                                 |
| server_version                   | 2.9.0                                                                                                                                                                                  |
| server_extensions                | classification sequence model_repository model_repository(unload_dependents) schedule_policy model_configuration system_shared_memory cuda_shared_memory binary_tensor_data statistics |
| model_control_mode               | MODE_NONE                                                                                                                                                                              |
| strict_model_config              | 1                                                                                                                                                                                      |
| pinned_memory_pool_byte_size     | 268435456                                                                                                                                                                              |
| cuda_memory_pool_byte_size{0}    | 1000000000                                                                                                                                                                             |
| min_supported_compute_capability | 6.0                                                                                                                                                                                    |
| strict_readiness                 | 1                                                                                                                                                                                      |
| exit_timeout                     | 30                                                                                                                                                                                     |
±---------------------------------±---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+I1007 07:32:54.183346 70 server.cc:231] No server context available. Exiting immediately.
error: creating server: Invalid argument - --model-repository must be specifiedRiva waiting for Triton server to load all models…retrying in 1 second
Riva waiting for Triton server to load all models…retrying in 1 second
Triton server died before reaching ready state. Terminating Riva startup.
Check Triton logs with: docker logs
/opt/riva/bin/start-riva: line 1: kill: (70) - No such processHi,Looks like your Triton server is down, due to models are not loaded using --model-repository option. Could you please share us docker log and config file/steps to understand the deployment here for better help.Thank you.Hi spolisetty
The log message that I provided earlier is the one that I get via running docker logs riva-speechHere is the content of config.sh that is under folder riva_quickstart_v1.6.0-betaBTW, here is the log while running bash riva_start.shand…here is another log while running bash riva_init.sh on my edge device (not workstation)and…here is another log while running bash riva_init.sh on my edge device (not workstation)Hi,Could you please let us know edge device are you using here. Just for your info please make sure, you’re using supported hardware. Please refer support matrix here Support Matrix — NVIDIA Riva
For your reference similar error,
Failed to run tensorrt docker image on Jetson Nano - #3 by AastaLLLThank you.Hi
the edge device that I used is Jetson Nano.
I know that Jetson is not a supported hardware for Riva, so I just use it to download docker images from NGC via running riva_init.sh.
Once the images are downloaded, I transfer them to workstation, where I plan to run Riva. (p.s. There is no internet access on Workstation).It seems that the models (RMIRs) are planned to be downloaded from NGC when running  riva_init.sh, is there any way that I could skip the model downloading process?And how could I download this models not via running riva_init.sh?Hi,Sorry for the delayed response. We can either comment out all the models or set all the service enabled flags to false. All of the models enumerated in config.sh correspond to paths in NGC. We can use the NGC CLI to download them manually.Thank you.Hi
Thanks for your reply, I have set all the service enable flags to false and use_existing_rmirs to true. I also comment out the portion of checking NGC API key in riva_init.sh, here I attached my new config.sh and riva_init.shHowever, I still cannot initialize Riva properly due to the following error after running riva_init.shfind: '/data/rmir': No sush file or directlyIt came from the last docker command in riva_init.shHere are my questions:
image1920×1080 368 KB
Config.shriva_init.shHi,Please refer following doc for more details.
https://docs.nvidia.com/deeplearning/riva/user-guide/docs/custom-model-deployment.html?highlight=model_loc#option-1-using-quick-start-scripts-to-deploy-your-models-recommended-pathThank you.Thanks, @spolisetty
I’m planning to host Riva on another server with internet to ease the setup process, however, the GPU spec isNVIDIA Tesla T4 16GB x1I’m wondering whether this kind of GPU is powerful enough to host Riva.According to the support matrix, only the following GPU is supporting Riva,Is NVIDIA Turing T4 as same as NVIDIA Tesla T4?I am having the same issue, as I run:/riva_quickstart_v2.3.0# bash riva_start.sh…then:/riva_quickstart_v2.3.0# docker logs riva-speechI0818 17:06:19.992784 105 model_repository_manager.cc:1132] successfully unloaded ‘citrinet-1024-en-US-asr-offline-ctc-decoder-cpu-offline’ version 1Riva waiting for Triton server to load all models…retrying in 1 secondI0818 17:06:20.434286 105 server.cc:267] Timeout 29: Found 0 live models and 0 in-flight non-inference requestserror: creating server: Internal - failed to load all modelsW0818 17:06:20.453564 105 metrics.cc:401] Unable to get power limit for GPU 0. Status:Success, value:0.000000W0818 17:06:20.453699 105 metrics.cc:419] Unable to get power usage for GPU 0. Status:Success, value:0.000000W0818 17:06:20.453771 105 metrics.cc:443] Unable to get energy consumption for GPU 0. Status:Success, value:0Riva waiting for Triton server to load all models…retrying in 1 secondW0818 17:06:21.453974 105 metrics.cc:401] Unable to get power limit for GPU 0. Status:Success, value:0.000000W0818 17:06:21.454103 105 metrics.cc:419] Unable to get power usage for GPU 0. Status:Success, value:0.000000W0818 17:06:21.454173 105 metrics.cc:443] Unable to get energy consumption for GPU 0. Status:Success, value:0Riva waiting for Triton server to load all models…retrying in 1 secondTriton server died before reaching ready state. Terminating Riva startup.Check Triton logs with: docker logs/opt/riva/bin/start-riva: line 1: kill: (105) - No such processMy config.sh:As other posts suggested, I ran clean-up:results via docker logs riva-speech:error: creating server: Invalid argument - --model-repository must be specified…as follows:
Screen Shot 2022-08-18 at 12.11.40 PM2860×1582 338 KB
Environment Details:
Ubuntu 22.04.1 LTS
NVIDIA Driver Version: 510.73.08 (GPU A100)
CUDA Version: 11.6
Docker: 20.10.17Any thoughts?Thank you.Hi @NSDBThanks for your interest in Riva,To triage the issue, we may need the following informationPlease clean the setup using bash riva_clean.shThanksPer your request:docker volume inspect riva-model-repo:ls /models:docker logs riva-speechThank you for your time and guideance.Any solution for this problem?Powered by Discourse, best viewed with JavaScript enabled"
469,an-important-skill-for-data-scientists-and-machine-learning-practitioners,"
Click the image to read the article
Find more #DSotD postsHave an idea you would like to see featured here on the Data Science of the Day?Powered by Discourse, best viewed with JavaScript enabled"
470,unable-to-remove-the-watermark-for-nvidia-maxine-gaze-redirect,"I am trying to run Nvidia Maxine gaze redirect locally on my system and I am unable to remove the watermark when I export the video. The text appears in green and I have already tried using the command provided, but it doesn’t seem to be working.Has anyone else experienced this issue?
Screenshot_1913746×161 58.9 KB
I searched a lot too and didn’t find it.  :(Easiest way is to crop the output in OBS or where ever your sending the redirectionIt`s possible to remove the watermark by modifying the code and rebuilding the GazeRedirect.exe file. Here are some pointers.Go to GazeRedirect.cpp and remove the following two lines.
snprintf(buf, sizeof(buf), “Redirected Output222”);
cv::putText(outputFrame, buf, cv::Point(80, 40), cv::FONT_HERSHEY_SIMPLEX, fontsize, cv::Scalar(0, 255, 0), 1);Make sure you have CMake installed, select the base folder for MAXINE-AR_SDK in Cmake source code, press on configure, generate and then open project. This should open the project in Visual Studio.In the solution explorer, right click on GazeRedirect and press on rebuild. This will rebuild the GazeRedirect.exe (the location will depend on your configuration but it might be somewhere like this for example C:\Program Files\CMake\bin\Release)Replace the old GazeRedirect.exe with the newly built one and that is it, no more watermark.Hey mate if you see this would you be able to contact me so i can learn how to do this please. i have many people asking me to change there videos and how to set up maxine on there computerPowered by Discourse, best viewed with JavaScript enabled"
471,trainer-train-stuck-with-rtx-a6000,"I found the following code stuck on RTX A6000 on Trainer.train() method, anyone knows how to fix this? I am sure it works for GeforceRTX2080Timain/examples/pytorch/multiple-choice🤗 Transformers: State-of-the-art Machine Learning for Pytorch, TensorFlow, and JAX.Run it on A6000 with following script:Anyone can help me with this?Hi @sym ,This forum talks about updates and concerns related to TRT.
I believe you can raise your concern on the Hardware Forum support.ThanksPowered by Discourse, best viewed with JavaScript enabled"
472,nemo-question,"Hi,I’m interested in trying out Nemo for Question Answering, as well as summarization .
I came across an excellent talk from GTC Fall 2022 (by Nirmalya De),In this talk, p-tuning is explained as the technique to customize LLMs for specific use-cases.Considering that this is from 6 months ago, I wonder if this is still the recommended approach, or if it has been superseded by something else.How can I try this out? I applied for Early acess on the Nemo page 3-4 days ago, but haven’t heard back yet.ThanksPowered by Discourse, best viewed with JavaScript enabled"
473,error-in-template-matching-in-vpi-samples-code,"
Screenshot from 2023-04-15 13-34-431600×900 156 KB
When using the template_matching sample code in C++(VPI), we see the following errors:VPI_ERROR_INVALID_ARGUMENT: MinMaxLoc input image size differs from the one specified in the payloadHi @hyashas89 ,
This doesn’t look like a cudnn issue.
Maybe you can raise it to the concerned forum for better assistance.ThanksPowered by Discourse, best viewed with JavaScript enabled"
474,how-to-use-batchednms-plugin-with-retinaface-detector,"I am using RetinaFaceNet for facial detection, which provides a score, a bounding box, and 5 facial landmarks for each detected face. These are represented as three separate vectors (or matrices): a score vector, a bounding box vector, and a landmark vector. I would like to apply non-maximum suppression (NMS) to filter out overlapping bounding boxes based on the scores, which is a common post-processing step in object detection.I am considering using the BatchedNMS Plugin in TensorRT for this task, as it provides an efficient, GPU-accelerated implementation of NMS. However, I am encountering a problem. While the BatchedNMS Plugin can provide the retained bounding boxes and their scores after NMS, it does not maintain a linkage to the original indices of these bounding boxes in the input data.This presents a challenge because I need to keep track of the facial landmarks associated with each bounding box. After applying NMS, I don’t know how to find the corresponding landmarks for the retained bounding boxes, as the original indices are lost in the process.I am looking for suggestions on how I can maintain the linkage between bounding boxes and their corresponding landmarks through the NMS process. Specifically, how can I find the corresponding landmarks for the bounding boxes retained after NMS when using the BatchedNMS Plugin in TensorRT? Any advice or alternative strategies would be greatly appreciated.Attached, you’ll find the ONNX model file and a screenshot of the model’s structure. The screenshot was created with the Netron tool.mobilenet_simplified.onnx (1.7 MB)
mobilenet_retinaFace1595×743 107 KB
Hi,
Please refer to below links related custom plugin implementation and sample:This Samples Support Guide provides an overview of all the supported NVIDIA TensorRT 8.6.1 samples included on GitHub and in the product package. The TensorRT samples specifically help in areas such as recommenders, machine comprehension, character...TensorRT is an SDK for high performance, deep learning inference. It includes a deep learning inference optimizer and a runtime that delivers low latency and high throughput for deep learning…While IPluginV2 and IPluginV2Ext interfaces are still supported for backward compatibility with TensorRT 5.1 and 6.0.x respectively, however, we recommend that you write new plugins or refactor existing ones to target the IPluginV2DynamicExt or IPluginV2IOExt interfaces instead.Thanks!Thanks, I will check these.Powered by Discourse, best viewed with JavaScript enabled"
475,unhandled-exception-in-nvinfer-h-in-visual-studio,"Hello,
I am fairly new to GPU Programming and I have been encountered with the following error while using cuDNN.This is the exception returned by Visual Studio:
Unhandled exception at 0x00007FFC862FDF51 (cudnn64_8.dll) in LibraryTest.exe: Fatal program exit requested.This is where the exception occurs (NvInfer.h):I have also installed TensorRT according to the documentation of NVIDIA.CUDA version: 11.6
TensorRT version: 8.5.2.2
OS: WIndows 10
GPU: NVIDIA Quadro M1000MIf anyone could help bring light to the situation, it would be great.
Thank you so much in advance!Hi @simona.simoni ,
Can you please check on the installation guide of cudnn and verify if it is installed correctly?
ThanksHello @AakankshaS ,yes, the installation is done correctly.Hi @simona.simoni s,
can you check if you have zlib available in your environment?
This was a new dependency introduced first in cuDNN 8.3.0. See the following guide for getting zlibThis cuDNN 8.9.0 Installation Guide provides step-by-step instructions on how to install and check for correct operation of NVIDIA cuDNN on Linux and Microsoft Windows systems.
ThanksPowered by Discourse, best viewed with JavaScript enabled"
476,version-problems-occurs-when-deserializing-engine-plane-file,"I have intalled TensorRT8.5.3.1 in Windows, but still got the error as below

image865×52 6.3 KB
TensorRT Version:  8.5.3.1
GPU Type: GeForce RTX 4090
Nvidia Driver Version:  31.0.15.2756
CUDA Version: 11.8
CUDNN Version: 8.60
Operating System + Version: Windows11专业版  操作系统版本 22621.1265
Python Version (if applicable):
TensorFlow Version (if applicable):
PyTorch Version (if applicable):
Baremetal or Container (if container which image + tag):more details please find below:
https://cxr7ii2pbr.feishu.cn/docx/MwEZdXzo6oKQAwxuP1Bcfa3PnNgPlease include:Hi ,
We recommend you to check the supported features from the below link.These support matrices provide a look into the supported platforms, features, and hardware capabilities of the NVIDIA TensorRT 8.5.3 APIs, parsers, and layers.
You can refer below link for all the supported operators list.
For unsupported operators, you need to create a custom plugin to support the operationThanks!The prolem still exist, if any more specifically suggestion can be given?Hi @liumin2086 ,
Please follow the installation steps from the below link along with the compatibility matrix  to have the setup ready.This NVIDIA TensorRT 8.5.3 Installation Guide provides the installation requirements, a list of what is included in the TensorRT package, and step-by-step instructions for installing TensorRT.
ThanksPowered by Discourse, best viewed with JavaScript enabled"
477,file-not-found-why,"Hello all,Please, before I proceed, I am quiet new to programming. I have been working on a project where I utilize ONNX and TensorRT, however, I am getting an error:FileNotFoundError: Could not find module ‘C:\Program Files\NVIDIA GPU Computing Toolkit\CUDA\v11.7\bin\nvinfer.dll’ (or one of its dependencies). Try using the full path with constructor syntax.Which means, the nvinfer.dll cannot be found in the folder, however, I did copy it there correctly as per my knowledge.I also did try to edit the system Environment Variables with following path: C:\Program Files\NVIDIA GPU Computing Toolkit\CUDA\v11.7\binI am using TensorRT 7.1.3.4. (I did have 8.5 version, but it is not supported by my GPU due to computing power [I am running 2060 super])
and cudnn 8.8.1.3Any advise?Thank you,
David

Capture675×622 37.8 KB
Hi @davidukus ,Please validate the installation steps from the below linkThis NVIDIA TensorRT 8.6.0 Early Access (EA) Installation Guide provides the installation requirements, a list of what is included in the TensorRT package, and step-by-step instructions for installing TensorRT.To find the compatible versions, please check the support matrix.For onnx to Tensorrt version, please findONNX-TensorRT: TensorRT backend for ONNX. Contribute to onnx/onnx-tensorrt development by creating an account on GitHub.ThanksPowered by Discourse, best viewed with JavaScript enabled"
478,finetuning-nemo-model,"Hello community,I am working on a project for Gujarati (Indic) Language ASR. I am using pre-trained English Quartznet 15*5 model. Because of small dataset (About 7 hours) (80-20 train-validation split), I freezed the encoder and unfreezed the decoder.
During training, the Validation loss gets stuck around 370-380 and training loss hovers around 250-300 – this despite running for almost 450+ epochs.(I took 2 minutes same train and validation data and overfitted to verify that Model is outputting Gujarati – it gave very good results.)Train Data contains 1732 files totalling 4.96 hours
Validation Data contains 433 files totalling 1.26 hours
The audio files are < 25 seconds and with sample rate of 22050 Hz.Am attaching the hyperparameters that I tried till now, the loss graphs along with the config file.
Can someone suggest some good hyperparameters to try out? Or any better Augmentation techniques?Augmentation:
target: nemo.collections.asr.modules.SpectrogramAugmentation
rect_freq: 50
rect_masks: 5
rect_time: 120
freq_masks: 2
freq_width: 25
time_masks: 10
time_width: 0.05config_final11.yaml (8.7 KB)
Loss curves3048×1400 212 KBHello Community,
Any updates or suggestions for this?Powered by Discourse, best viewed with JavaScript enabled"
479,onnx-tensorrt-int64-clamping-why,"Models exist which use INT64 values.
Inference with those models works on my 4090 GPU.
Hardware computation on a GPU or CPU involving INT64 values don’t care if the code invoking the computations is done via non-compiled python code or highly optimized TRT compiled code.
“Why” doesn’t TRT support INT64?Also, if there was a good reason, wouldn’t a conversion to float32 better represent large magnitude INT64 values.  I’m not sure if the “might” be a contributing factor in the poor quality of SD inferenced images when TRT is used.Hi,
Request you to share the ONNX model and the script if not shared already so that we can assist you better.
Alongside you can try few things:This NVIDIA TensorRT 8.6.1 Quick Start Guide is a starting point for developers who want to try out TensorRT SDK; specifically, this document demonstrates how to quickly construct an application to run inference on a TensorRT engine.import sys
import onnx
filename = yourONNXmodel
model = onnx.load(filename)
onnx.checker.check_model(model).
2) Try running your model with trtexec command.master/samples/trtexecNVIDIA® TensorRT™, an SDK for high-performance deep learning inference, includes a deep learning inference optimizer and runtime that delivers low latency and high throughput for inference applicat...
In case you are still facing issue, request you to share the trtexec “”–verbose"""" log for further debugging
Thanks!It is well known that tensorrt doesn’t support int64.  Are you doubting that non-tensorrt models sometimes use int64?  Do you really want me to post a 1.7GB unet_fp16.onnx model converted from huggingface runwayml/stable-diffusion-v1-5?I’m not trying to track down a bug for which providing a test case would be appropriate.  This situation is obvious.  TRT does NOT support int64.  Why?  My NVidia hardware doesn’t have a problem doing inference with non-tensorrt engine models which happen to have int64 values in them.Also, onnx-checker didn’t output anything as if there was no problem with the model.Hi,In future major release versions, INT64 support will be added. Please stay tuned for the update.Thank you.Powered by Discourse, best viewed with JavaScript enabled"
480,tensorrt-has-incorrect-output-for-networks-with-custom-plugins,"I have an onnx model with a total of three operators, among which operator 1 and operator 2 are my custom plug-ins. When parsing in tensorrt, the values ​​of output1 and output3 are both correct, but output2 is incorrect. But I copied the output pointer from the video memory with cudaMemcpy in enqueue() in Operator 2, and the printed value is correct.
I have used onnx.checker.check_model(model), it will report an error saying that there is no my custom plug-in. But I use print(model) to see the network is correct.
How can I debug this problem? Thanks.TensorRT Version:  8.2 GA
GPU Type: rtx 3060
Nvidia Driver Version: 31.0.15.1702
CUDA Version: 11.4
CUDNN Version: 11.4
Operating System + Version: Windows 11
PyTorch Version (if applicable):  1.9.0+cu111Hi,
Please refer to below links related custom plugin implementation and sample:This Samples Support Guide provides an overview of all the supported NVIDIA TensorRT 8.6.0 Early Access (EA) samples included on GitHub and in the product package. The TensorRT samples specifically help in areas such as recommenders, machine...TensorRT is an SDK for high performance, deep learning inference. It includes a deep learning inference optimizer and a runtime that delivers low latency and high throughput for deep learning…While IPluginV2 and IPluginV2Ext interfaces are still supported for backward compatibility with TensorRT 5.1 and 6.0.x respectively, however, we recommend that you write new plugins or refactor existing ones to target the IPluginV2DynamicExt or IPluginV2IOExt interfaces instead.Thanks!Powered by Discourse, best viewed with JavaScript enabled"
481,unable-to-start-en-gb-model-for-asr,"Hardware - GPU A10Question: How to run only one model using riva quickstart? I want to run the en-GB model without the en-US model.Details:I can successfully start riva ASR for en-US model. I have been trying for couple of days to start riva with the en-GB model and am not able to accomplish this.
I am using the riva quickstart repo 2.12.1 ( I have also tried with 2.11.0).Steps I am taking:I do not edit anything else.Ah, now that I have properly read the log I can I am running out of memory.
I see it is loading the en-US model as well, how can I prevent that?The en-US model by itself fits in my 24GB of memory. (I only have 16GB free, I need to run other processes in the GPU in parallel).The answer was that the docker service tries to run all the models in the model dir. I had some en-US models there from previous runs.Powered by Discourse, best viewed with JavaScript enabled"
482,riva-asr-not-working-in-esxi8-environment,"Please provide the following information when requesting support.Hardware - GPU (A100/A30/T4/V100) - grid_a100-20c
Hardware - CPU - AMD EPYC 7352 24-Core Processor
Operating System / VMware ESXi, 8.0.0, 21203435 - Virtual Machine(Ubuntu 20.04)
Riva Version - riva_quickstart:2.10.0
TLT Version (if relevant)
How to reproduce the issue ? (This is for errors. Please share the command and the detailed log here)Hi all.I’m trying to test ASR,TTS features etc with Nvidia riva quick start guide.I downloaded the datacenter riva_quirck_start:2.10.0 version and set only asr,nmt services to true in config.sh as the guide says.I only enabled the German translation part of NMT, the rest is the same as default.Running riva_start.sh works fine.Then I wrote and executed the guide code for ASR test, but I get the following error.------------------------------------ Error ----------------------------------------------{
“name”: “_InactiveRpcError”,
“message”: “<InactiveRpcError of RPC that terminated with:\n\tstatus = StatusCode.UNKNOWN\n\tdetails = ""in ensemble ‘conformer-en-US-asr-offline’, audio_signal: failed to perform CUDA copy: an illegal memory access was encountered""\n\tdebug_error_string = ""UNKNOWN:Error received from peer  {grpc_message:""in ensemble \‘conformer-en-US-asr-offline\’, audio_signal: failed to perform CUDA copy: an illegal memory access was encountered"", grpc_status:2, created_time:""2023-04-21T01:03:01.91677218+00:00""}""\n>"",
“stack”: ""\u001b[1;31m---------------------------------------------------------------------------\u001b[0m\n\u001b[1;31m_InactiveRpcError\u001b[0m                         Traceback (most recent call last)\nCell \u001b[1;32mIn[13], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m response \u001b[39m=\u001b[39m riva_asr\u001b[39m.\u001b[39;49moffline_recognize(content, config)\n\u001b[0;32m      2\u001b[0m asr_best_transcript \u001b[39m=\u001b[39m response\u001b[39m.\u001b[39mresults[\u001b[39m0\u001b[39m]\u001b[39m.\u001b[39malternatives[\u001b[39m0\u001b[39m]\u001b[39m.\u001b[39mtranscript\n\u001b[0;32m      3\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m""\u001b[39m\u001b[39mASR Transcript:\u001b[39m\u001b[39m""\u001b[39m, asr_best_transcript)\n\nFile \u001b[1;32m~\AppData\Local\Packages\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\LocalCache\local-packages\Python311\site-packages\riva\client\asr.py:362\u001b[0m, in \u001b[0;36mASRService.offline_recognize\u001b[1;34m(self, audio_bytes, config, future)\u001b[0m\n\u001b[0;32m    360\u001b[0m request \u001b[39m=\u001b[39m rasr\u001b[39m.\u001b[39mRecognizeRequest(config\u001b[39m=\u001b[39mconfig, audio\u001b[39m=\u001b[39maudio_bytes)\n\u001b[0;32m    361\u001b[0m func \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstub\u001b[39m.\u001b[39mRecognize\u001b[39m.\u001b[39mfuture \u001b[39mif\u001b[39;00m future \u001b[39melse\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstub\u001b[39m.\u001b[39mRecognize\n\u001b[1;32m–> 362\u001b[0m \u001b[39mreturn\u001b[39;00m func(request, metadata\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mauth\u001b[39m.\u001b[39;49mget_auth_metadata())\n\nFile \u001b[1;32m~\AppData\Local\Packages\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\LocalCache\local-packages\Python311\site-packages\grpc\channel.py:1030\u001b[0m, in \u001b[0;36m_UnaryUnaryMultiCallable.call\u001b[1;34m(self, request, timeout, metadata, credentials, wait_for_ready, compression)\u001b[0m\n\u001b[0;32m   1021\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__call\u001b[39m(\u001b[39mself\u001b[39m,\n\u001b[0;32m   1022\u001b[0m              request: Any,\n\u001b[0;32m   1023\u001b[0m              timeout: Optional[\u001b[39mfloat\u001b[39m] \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m,\n\u001b[1;32m   (…)\u001b[0m\n\u001b[0;32m   1026\u001b[0m              wait_for_ready: Optional[\u001b[39mbool\u001b[39m] \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m,\n\u001b[0;32m   1027\u001b[0m              compression: Optional[grpc\u001b[39m.\u001b[39mCompression] \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Any:\n\u001b[0;32m   1028\u001b[0m     state, call, \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_blocking(request, timeout, metadata, credentials,\n\u001b[0;32m   1029\u001b[0m                                   wait_for_ready, compression)\n\u001b[1;32m-> 1030\u001b[0m     \u001b[39mreturn\u001b[39;00m _end_unary_response_blocking(state, call, \u001b[39mFalse\u001b[39;49;00m, \u001b[39mNone\u001b[39;49;00m)\n\nFile \u001b[1;32m~\AppData\Local\Packages\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\LocalCache\local-packages\Python311\site-packages\grpc\_channel.py:910\u001b[0m, in \u001b[0;36m_end_unary_response_blocking\u001b[1;34m(state, call, with_call, deadline)\u001b[0m\n\u001b[0;32m    908\u001b[0m         \u001b[39mreturn\u001b[39;00m state\u001b[39m.\u001b[39mresponse\n\u001b[0;32m    909\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m–> 910\u001b[0m     \u001b[39mraise\u001b[39;00m _InactiveRpcError(state)\n\n\u001b[1;31m_InactiveRpcError\u001b[0m: <_InactiveRpcError of RPC that terminated with:\n\tstatus = StatusCode.UNKNOWN\n\tdetails = ""in ensemble ‘conformer-en-US-asr-offline’, audio_signal: failed to perform CUDA copy: an illegal memory access was encountered""\n\tdebug_error_string = ""UNKNOWN:Error received from peer  {grpc_message:""in ensemble \‘conformer-en-US-asr-offline\’, audio_signal: failed to perform CUDA copy: an illegal memory access was encountered"", grpc_status:2, created_time:""2023-04-21T01:03:01.91677218+00:00""}""\n>”
}Can you tell me if this is a CUDA version related issue?If it is a CUDA version issue, how can I resolve it?After executing riva_start_client.sh, the NMT Guide Code works normally.python3 /opt/riva/examples/nmt.py --model-name=en_de_24x6 --src-language=en --tgt-language=de --text=“I love you.” → Ich liebe dich.Thanks for reading and have a great day.Hi @mklee1Thanks for your interest in RivaRequest to share the complete log output of docker logs riva-speech in this threadQuick doubt, do we have multiple GPUs present in your machine, if yes can we try running only using a single GPUThanksHi @rvinobhaFirst of all, thank you for your reply.I am attaching the docker logs riva-speech log as a txt file.riva-speech-logs.txt (2.2 KB)I am using Single GPU for the VM.I am attaching the nvidia-smi screenshot.Thanks a lot.Have a nice day.I saw a similar inquiry on the forum.I downgraded riva to 2.7.0 version and ASR seems to be working fine.As for the log file, now that I look at it, it seems to be a miscommunication.Thanks.Hi @mklee1Thanks for proactively trying 2.7 and finding out it works, we will try to find why it didn’t work in 2.10Thanks for sharing the logs,Apologies, from the logs captured, I can find the riva-start has failed,Can you share the complete log output of riva-init to find some clue regarding the failure, as docker logs riva-speech currently shared does not have any detailsAlso when running riva-start, simultaenously can you parallelly run docker logs riva-speech simultaneously in another window and reshare againThanksThis topic was automatically closed 14 days after the last reply. New replies are no longer allowed.Powered by Discourse, best viewed with JavaScript enabled"
483,trying-to-convert-detectron2s-keypoint-rcnn-r-50-fpn-3x-to-tensorrt,"Hi, so following the Tensorrt’s Github page, I was successfully able to generate the TensorRT engine for mask_rcnn and faster_rcnn.But when it comes to keypoint_rcnn, while the engine did got created but the output I get is not actual keypoints but the keypoint heatmaps.Now I do know that the support for detectron2 to tensorrt is specifically supported for mask_rcnn but still, since I am not very skilled at this, I was hoping to get some ideas.Thank You.TensorRT Version: 8.6
GPU Type: NVIDIA GeForce RTX 3070 Ti
CUDA Version: 12.0
CUDNN Version:  8.8
Operating System + Version:  Ubuntu 20.04
Python Version (if applicable): 3.8
PyTorch Version (if applicable): 2.1Keypoint ONNX: https://drive.google.com/file/d/1po8GkgTc8lr020Wpnqjr5Wpqw9imr8_R/view?usp=share_linkKeypoint Converted ONNX:  https://drive.google.com/file/d/12Ncepg8qPmmIuCMzvNnHb5SdAN4cwgtU/view?usp=share_linkJust follow this linkrelease/8.6/samples/python/detectron2NVIDIA® TensorRT™, an SDK for high-performance deep learning inference, includes a deep learning inference optimizer and runtime that delivers low latency and high throughput for inference applicat...Hi,We couldn’t successfully build the TensorRT engine using the ONNX file shared.
Please share with us more details of the problem, logs with the issue repro model, and sample data.Thank you.Please request the converted keypoint onnx file one more time.
Thank You.Hi,Sorry for the delayed response. Could you please give more details on your query?
Please refer to the following (mentioned in the sample readme) to learn more about Detectron’s output.The outputs of the graph are the same as the outputs of the EfficientNMS_TRT plugin and segmentation head output, name of the last node is detection_masks , shape is [batch_size, max_proposals, mask_height, mask_width] , dtype is float32.Please check the below for sample output resultsrelease/8.6/samples/python/detectron2NVIDIA® TensorRT™, an SDK for high-performance deep learning inference, includes a deep learning inference optimizer and runtime that delivers low latency and high throughput for inference applicat...Thank you.Hi, I generated an onnx file for keypoint and then further generating the converted onnx for tensorrt engine.
Thing is that in the output, instead of keypoint x, y points, I get the keypoint heatmap values.I will attach the onnx and the converted onnx.Onnx:Google Drive file.Converted Onnx:Google Drive file.If you see the onnx file, at node ConstantOfShape_2057, I can see the xy_preds .
onnx2467×1259 70.3 KBBut on the converted onnx at the last node (gather), the output dimension is 100x17x56x56 which I think are heatmap values since I believe xy_preds should be 100x17x3x3.
I modified the converted onnx python code by doing something like this. (took reference from TensorRT’s github)Could you please help me with the converted onnx? Maybe I am doing something wrong?EDIT:
From what I understand the problem could be is that, xy_pred stuff happens after the  resize node but in the code, after getting the last resize, I am just gathering it.Somehow I want to insert the node ConstantOfShape_2057 but whenever I try to insert it in the code, it throws error.Thanks.Powered by Discourse, best viewed with JavaScript enabled"
484,nv-gesture-dataset,"I aim to use the [NVIDIA Dynamic Hand Gesture Dataset] in a project wherein Im bulding a nn architecture to classify hand gestures. For some reason I followed all the steps on these links and my computer was unable to open the dataset files at all. Anyone have any idea why this is?regards,
Varun
Online Detection and Classification of Dynamic Hand Gestures with Recurrent 3D Convolutional Neural Networks | Research (nvidia.com)Hi,This forum talks more about updates and issues related to the TensorRT.
We recommend that you please reach out to the relevant platform to get better help.Thank you.Powered by Discourse, best viewed with JavaScript enabled"
485,segmentation-fault-when-using-resize,"When converting Mask-RCNN from onnx to trt trtexec , the segfault (‘Segmentation fault (core dumped)’) happened. I check the size of tensor before resize is (1,1,30,30) and target is (1, 1, 275, 442). I used code base of torchvision when implementing Mask-RCNN. Any help would much appreciated!TensorRT Version: 8.6.0.12-1+cuda11.8
GPU Type: 3060 Ti
Nvidia Driver Version: 530.30.02
CUDA Version: 12.1
CUDNN Version:
Operating System + Version: Ubuntu 20.04 LTS
Python Version (if applicable):
TensorFlow Version (if applicable):
PyTorch Version (if applicable):
Baremetal or Container (if container which image + tag):ONNX file: LINK
Log of trtexec: LINK./trtexec --onnx=maskrcnn.onnx --verbose
image1370×378 175 KB
Hi,
Request you to share the ONNX model and the script if not shared already so that we can assist you better.
Alongside you can try few things:This NVIDIA TensorRT 8.6.0 Early Access (EA) Quick Start Guide is a starting point for developers who want to try out TensorRT SDK; specifically, this document demonstrates how to quickly construct an application to run inference on a TensorRT...import sys
import onnx
filename = yourONNXmodel
model = onnx.load(filename)
onnx.checker.check_model(model).
2) Try running your model with trtexec command.master/samples/trtexecNVIDIA® TensorRT™, an SDK for high-performance deep learning inference, includes a deep learning inference optimizer and runtime that delivers low latency and high throughput for inference applicat...
In case you are still facing issue, request you to share the trtexec “”–verbose"""" log for further debugging
Thanks!Powered by Discourse, best viewed with JavaScript enabled"
486,welcome-to-the-computer-vision-image-processing-forum,"What challenges are you facing with building Computer Vision solutions?Powered by Discourse, best viewed with JavaScript enabled"
487,status-health-of-riva-models,"Hi is there any whay to check if the ststus or the health of models loaded.
For example, if at some point the inference server stops, is there any way to know that.Hi @aman.rai1Thanks for your interest in Riva,
I will check for more inputs from the team,But for now you can get the status by running the below docker commanddocker logs riva-speechThanksHi @rvinobhayes, I have been using the same, but I would love to have something which is just as easy as pinging the server and analysing ther response code rather than monitoring the logs 24/7.Thankst I would love to have something which is just as easy as pinging the server and aThere’s a health proto? You can ping the /health endpoint. Check out grpccurl on github and you’ll be able to list the available endpointsDid you find a way to check the status?Hi @nharoApologies for the delay,I will check with the team again and confirm if we have anythingThanksRiva implements  GRPC health checking .  Using a tool like grpcurl you can easily check this from command line, or via k8s with liveness  probesWatch a container for changes:Powered by Discourse, best viewed with JavaScript enabled"
488,as-tensor-error-in-keras,"I am using keras/tensorflow in R and have an issue with as_tensor.My test data example looks like :X1 X2
-1 9837
-1 10000
-1 10154
-1 10000
-1 9894
-1 10000When I run, as_tensor, I receive the error :
Error: basic_string::substr: __pos (which is 2354882) > this->size() (which is 1177900)I am able to run as_tensor on the individual columns, but not on the whole dataset. All of the data is the same type, int. Also, I made sure I do have any NA.How do I debug this?ThanksWindows
R/Keras/Tensorflow
NVIDIA GeForce GTX 1650 Ti with Max-Q Design
Driver Version 517.13
tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX AVX2I tensorflow/core/common_runtime/gpu/gpu_device.cc:1510] Created device /device:GPU:0 with 2143 MB memory:  → device: 0, name: NVIDIA GeForce GTX 1650 Ti with Max-Q Design, pci bus id: 0000:01:00.0, compute capability: 7.5
TensorFlow built with CUDA:  TRUEtensorflow/core/common_runtime/gpu/gpu_device.cc:1510] Created device /device:GPU:0 with 2143 MB memory:  → device: 0, name: NVIDIA GeForce GTX 1650 Ti with Max-Q Design, pci bus id: 0000:01:00.0, compute capability: 7.5tensorflow::tf_version()
[1] ‘2.6’Hi @Kbulls1 ,
This forum talks about issues related to TensorRT.
Request you to raise the issue on Keras/Tensorflow Forum.
ThanksPowered by Discourse, best viewed with JavaScript enabled"
489,meet-nvidia-ai-developer-experts-march-20-23-2023,"Join virtually at GTC 2023 March 20-23 to discover the latest AI technologies including Generative AI, Speech AI, Ray Tracing, Real-Time Graphics, and Neural Graphics and see how they are accelerating digital human and professional visualization development.Don’t forget to add the GTC Keynote [S52226] on March 21st at 8:00 AM PT, to your calendar to get insight into the latest technology breakthroughs NVIDIA is making across industries.We have sessions for developers of all disciplines. Here’s a curated list of all the MUST-SEE sessions:Recommended Professional Visualization Developer SessionsRecommended Avatar SessionsRecommended Video Streaming and Conferencing SessionsLearn More: nvidia.com/gtcPowered by Discourse, best viewed with JavaScript enabled"
490,how-chunk-size-padding-size-and-other-build-configs-affect-behavior-of-streaming-asr,"Hardware - GPU T4
Hardware - CPU (Unknown, AWS g4dn.xlarge instance)
Operating System - Amazon Linux 2 ECS GPU optimized AMI
Riva Version - 2.0Our team has been working on optimizing the performance of Riva for our use case: Arabic streaming ASR.Specifically we’re trying to improve the latency of intermediate responses and the accuracy of partial transcripts.
We’re currently using a CitriNet model trained with NeMo.
There are many build parameters riva-build provides, which is great, but there is limited documentation on how each parameter affects the performance of the final model.We’ve found the parameters that affect the behavior the most are:The only resources we’ve found are:From our experimentation, we’ve observed the following behaviors:This seems to be in line with what the documentation describes as “low-latency” vs. “high-throughput” models, but brings up some questions:We’re hoping we can get more input from someone on the Riva team to provide more context on how these build configurations specifically, including others, affect the accuracy and latency of the deployed model.
3rd party resources would be helpful here as well.We’d also like to know which model configurations will fail at runtime, since we’ve found some configurations that result in models that don’t work!
An example of such a model is on with the following configuration:These configurations should be handled accordingly at build time and not run time.Hi @pineapple9011Thanks for your interest in Riva,Apologies for the delay,I will check regarding your questions/queries further with the team and get back to youThis post has the potential to be extremely useful to the company. @rvinobha can you please check with the team and have someone respond to this query? It would be central to successful usage of riva with TAO/NeMo as well!Hi @ShantanuNair and @pineapple9011Thanks for your interest in Riva,We will be updating the documentation to better explain the above discussed aspects, I will check with the team regarding the availability and let you knowThanksHello @rvinobha,I hope you have some updates regarding the topic discussed earlier. I am also currently working on optimizing the performance of the RIVA ASR pipeline, and I find the padding parameters in the feature extractor quite confusing.For instance, when I use the following settings in my ASR pipeline:everything works smoothly. However, if I make a change to the settings like this:I encounter the following message during Triton initialization: padding_factor*chunk_size must be greater than left_padding_size+right_padding_size in feature extractor. This confuses me because I actually decreased the values of left_padding_size and right_padding_size, so how is it possible that the condition becomes greater? Moreover, I’m not sure what the “padding_factor” refers to or where I can adjust it.I appreciate any insights or clarifications you can provide on this matter. Thank you in advance!Powered by Discourse, best viewed with JavaScript enabled"
491,how-to-inference-with-2-different-shape-outputs,"I used the BERT demo in TensorRT github,here come the linktrt6.0I got my bert model with 2 difference shape outputs, which i use network.mark_output() to make them become the output of engine one by one, it works and i successfully build the engine.
[TensorRT] INFO: Detected 3 inputs and 2 output network tensors.
[TensorRT] INFO: Detected 3 inputs and 2 output network tensors.
[TensorRT] INFO: Detected 3 inputs and 2 output network tensors.
[TensorRT] INFO: Saving Engine to bert_slot_384.engine
[TensorRT] INFO: Done.But problem happens when i use inference.py to do inference, i didn’t changed any code of cuda and memory part, which comes with such error message,and if only one output it can do inference.“”""
[TensorRT] ERROR: engine.cpp (165) - Cuda Error in ~ExecutionContext: 700 (an illegal memory access was
encountered)
[TensorRT] ERROR: INTERNAL_ERROR: std::exception
[TensorRT] ERROR: Parameter check failed at: …/rtSafe/safeContext.cpp::terminateCommonContext::165, condition:
cudaEventDestroy(context.start) failure.
[TensorRT] ERROR: Parameter check failed at: …/rtSafe/safeContext.cpp::terminateCommonContext::170, condition:
cudaEventDestroy(context.stop) failure.
[TensorRT] ERROR: …/rtSafe/safeRuntime.cpp (32) - Cuda Error in free: 700 (an illegal memory access was
encountered)
terminate called after throwing an instance of ‘nvinfer1::CudaError’
what():  std::exception
Aborted (core dumped)
“”""TensorRT Version: 6.0
GPU Type: 2080TI
Nvidia Driver Version: 418.39
CUDA Version: 10.1Hi, Request you to share your model and script, so that we can help you better.Alternatively, you can try running your model with trtexec command.
https://github.com/NVIDIA/TensorRT/tree/master/samples/opensource/trtexecThanks!here comes the code of get oputput from engine,just like the one in
https://github.com/NVIDIA/TensorRT/tree/master/samples/opensource/trtexecthanksProvided the code in my .py file. And i think that the code in inference.py was designed for one output, maybe we need to change it to apply for 2 different shape outputs. Do you have any experience or example, thanks a lot.Hi @501967143,We don’t have an example to provide.
As you already created one output, please create another output to just mimic d_output in the shared code.Thank you.I have created an open source, well documented project which demonstrates how you can run inference with single / multiple input - single / multiple output models with batching support in C++. It can be found here.Powered by Discourse, best viewed with JavaScript enabled"
492,tensorrt-conversion-from-tensorflow-with-custom-op,"Hi, I  am trying to convert my tensorflow model with a custom op to a tensorRT model. I use the trtexec commandline tool to do so. I first convert the model from tensorflow to ONNX with tf2onnx which has my custom op that I included by passing the .so file (custom op library file) to tf2onnx. Later when I try to convert it to a TRT model I am unable to do so. I use the -plugin command line option with trt exec to which I pass the .so file.
This the error that I am currently running into. It is unable to even read the .so file. It says its not at the location mentioned, even though it is there.
Am I doing something wrong here or is there anyother way where I can convert my TF model with custom op in .so file to TRT?
Here is the command I am running and below is the error log.trtexec --onnx=model_on.onnx --plugins=…/image_patch.so --saveEngine=engine.trt[08/03/2023-16:41:57] [I] TensorRT version: 8.6.1
[08/03/2023-16:41:57] [I] Loading standard plugins
[08/03/2023-16:41:57] [I] Loading supplied plugin library: …/image_patch.so
[08/03/2023-16:41:57] [E] Uncaught exception detected: Unable to open library: …/image_patch.so due to libtensorflow_framework.so.2: cannot open shared object file: No such file or directory
&&&& FAILED TensorRT.trtexec [TensorRT v8601] # trtexec --onnx=model_on.onnx --plugins=…/image_patch.so --saveEngine=engine.trtPowered by Discourse, best viewed with JavaScript enabled"
493,different-engines-give-different-inference-results-when-using-the-same-onnx-model-and-giving-the-same-input,"There are some custom operators in my onnx model, and I implemented their plugins on tensorrt. When I use tensorrt to read this onnx model and run it multiple times for the same input, different engines will be given. Some engines give correct inference results, while some engines give incorrect inference results.
This onnx file is exported with pytorch.
In this case, which aspect of the problem do I need to locate first? Thanks a lot.TensorRT Version: 8.2 GA
GPU Type: rtx 3060
Nvidia Driver Version: 31.0.15.1702
CUDA Version:  11.4
CUDNN Version: 11.4
Operating System + Version: Windows 11
PyTorch Version (if applicable): 1.9.0+cu111Hi,
Request you to share the ONNX model and the script if not shared already so that we can assist you better.
Alongside you can try few things:This NVIDIA TensorRT 8.6.0 Early Access (EA) Quick Start Guide is a starting point for developers who want to try out TensorRT SDK; specifically, this document demonstrates how to quickly construct an application to run inference on a TensorRT...import sys
import onnx
filename = yourONNXmodel
model = onnx.load(filename)
onnx.checker.check_model(model).
2) Try running your model with trtexec command.master/samples/trtexecNVIDIA® TensorRT™, an SDK for high-performance deep learning inference, includes a deep learning inference optimizer and runtime that delivers low latency and high throughput for inference applicat...
In case you are still facing issue, request you to share the trtexec “”–verbose"""" log for further debugging
Thanks!I have an onnx model with a total of three operators, among which operator 1 and operator 2 are my custom plug-ins. When parsing in tensorrt, the values ​​of output1 and output3 are both correct, but output2 is incorrect. But I copied the output pointer from the video memory with cudaMemcpy in enqueue() in Operator 2, and the printed value is correct.
How can I debug this problem? Thanks.There are my custom operators in onnx. I use onnx.checker.check_model(model), it will report an error saying that there is no custom plug-in. But I use print(model) to see the network is correct.Powered by Discourse, best viewed with JavaScript enabled"
494,are-those-cudnn-backend-apis-descriptions-conflict-with-each-other,"In cudnn Backend API references, The subsection 9.2.5.cudnnBackendGetAttribute() says that "" This function will return CUDNN_STATUS_NOT_INTIALIZED if the descriptor was already successfully finalized."". But in subsection 9.2.4. cudnnBackendFinalize() says that “Getting attributes using cudnnBackendGetAttribute() is only allowed when the finalized state of the descriptor is true .”
I think the above description are conlict with each other.  From some secrions, I can see that the cudnnBackendGetAttribute() after called after the cudnnBackendCreateDescriptor() and before calling cudnnBackendFinalize().
Besides the above questions, I also want to know the work that cudnnBackendInitialize and cudnnBackendFinalize actually do. Is cudnnBackendInitialize set some default values according to the type of the descriptors? Besides set the finalized flag, what happens when calling the cudnnBackendFinalize? Is cudnnBackendFinalize will check the status descriptor that exists in the attributes of current descriptor( current refers to the parameters of the API cudnnBackendFinalize)Hi @182yzh ,
This might be a bug. We are checking internally if this is the case.
Shall provide you the update soon
ThanksPowered by Discourse, best viewed with JavaScript enabled"
495,how-to-enable-dynamic-batching-for-models-on-triton-inference-server,"I am trying to host a custom model on Triton Inference server and I am trying to enable dynamic batching for the model.I am converting a pytorch model to onnx and enabling dynamic batching on the input and output nodes.I attach a copy of the script i use to convert from pytorch to onnx in the attachments and a copy of my config.pbtxt file.export_onnx.py (683 Bytes)
config.pbtxt (270 Bytes)E0522 09:23:57.598550 82 model_repository_manager.cc:1215] failed to load ‘par’ version 1: Invalid argument: model ‘par’, tensor ‘512’: for the model to support batching the shape should have at least 1 dimension and the first dimension must be -1; but shape expected by the model is [1,22]Powered by Discourse, best viewed with JavaScript enabled"
496,guidance-on-egpu-for-windows-laptop,"I have an HP Zbook with i9 , 32GB RAM and NVIDIA RTX A2000 GPU(4 GB) under Windows 10. Have Thunderbolt 4/USB C-type port on the laptop.What are my options on eGPU ?quite often I run out of memory while training. (Tensoflow/Keras) so was thinking of an eGPU .guidance appreciated.Check out: https://egpu.io/Thanx…will have a look at it.Powered by Discourse, best viewed with JavaScript enabled"
497,bash-riva-start-sh-never-complete-on-jetson-agx-orin,"Please provide the following information when requesting support.Hardware - Jetson AGX Orin
Hardware - CPU
Operating System: JetPack 5.0.2
Riva Version: riva_quickstart_arm64_v2.8.1
TLT Version (if relevant)
How to reproduce the issue ? (This is for errors. Please share the command and the detailed log here)Today I have flashed the latest JetPack 5.0.2 into the Jetson AGX Orin unit. Then I have downloaded the riva_quickstart_arm64_v2.8.1When I run the bash riva_init.sh,  it finishes without any error except the bash riva_start.sh.Can you please give me the suggestion how to solve the problem?Thanks
JimmyStarting Riva Speech Services. This may take several minutes depending on the number of models deployed.
53b96b57a1999e4006324210317be0156a8a3bb9cb64bb476dc1e620468be53c
docker: Error response from daemon: failed to create shim: OCI runtime create failed: runc create failed: unable to start container process: error during container init: error running hook #0: error running hook: exit status 1, stdout: , stderr: Auto-detected mode as ‘csv’
invoking the NVIDIA Container Runtime Hook directly (e.g. specifying the docker --gpus flag) is not supported. Please use the NVIDIA Container Runtime instead.: unknown.
Error response from daemon: Container 53b96b57a1999e4006324210317be0156a8a3bb9cb64bb476dc1e620468be53c is not running
Waiting for Riva server to load all models…retrying in 10 seconds
Waiting for Riva server to load all models…retrying in 10 secondsHi @jimmyqThanks for your interest in Riva,Please share yourThanks for your jist of log, i also recommend to check the NGC setup
the ngc files would be present in $HOME/.ngc
open the config file (PLEASE DO NOT EDIT THIS FILE), (PLEASE DO NOT SHARE THIS FILE AS IT CONTAINS SENSITIVE INFORMATION)
and check for format_type, if it is set as csv and not json,
Run ngc config set and change the type as json (prompted on step 3 after API key and ORG), and retry the setup again, and let us know if it worksThanksinit.log (18.3 KB)
start.log (2.8 KB)Hi @rvinobha,But I still have the same problem:  run riva_start.sh has the error as seen in the attached the start.logThanks so much for help
JimmyOr it may be the docker version related problem?Hi @jimmyqCan you kindly verify Step 6 from the below document has been performedScripts and utilities for getting started with Riva Speech Skills on Embedded platformsYou have set the default runtime to nvidia on the Jetson platform by adding the following line in the /etc/docker/daemon.json file. Restart the Docker service using sudo systemctl restart docker after editing the file.
""default-runtime"": ""nvidia""
and perhaps try againHi @rvinobha,Here is my /etc/docker/daemon.json file.{
“runtimes”: {
“nvidia”: {
“path”: “nvidia-container-runtime”,
“runtimeArgs”: ,
“default-runtime”: “nvidia”
}
}
}After I have done: sudo systemctl restart docker, I have done:bash riva_clean.sh
bash riva_init.sh
bash riva_start.shWhile first two commands has no problem, the riva_start.sh still face the same problem.I am thinking to re-flash the JetPack5.0.2 image into my Jetson AGX Orin and then to try the Riva again.  I guess that my docker may need to be downgraded?If you have new suggestions, please let me know since I need to make the RIVA working on the Jetson Orin.Thanks
JimmyHi @jimmyqWhat is the current version of your dockerThanksHi JimmyAre you sure your daemon.json is correct? Pleas take a look onto Upgrading to the NVIDIA Container Runtime for Docker :: DGX Systems Documentation Chapter 4{
“default-runtime”: “nvidia”,
“runtimes”: {
“nvidia”: {
“path”: “/usr/bin/nvidia-container-runtime”,
“runtimeArgs”: 
}
},}Just got the Jetson Orin nano dev kit.
I am having the exact same issue. Was this resolved and how?Hi @adventuredaisy  and @er-er-haThanks for your interest in RivaThe Problems could be because ofFor Riva 2.9.0, Jetpack 5.0.2 should be used.
But if you are using latest version Riva 2.10.0 release Jetpack 5.1, should be used.
https://docs.nvidia.com/deeplearning/riva/user-guide/docs/release-notes.html#key-features-and-enhancementsCan you check your Jetpack version and provide us feedbackThanksPowered by Discourse, best viewed with JavaScript enabled"
498,broken-link-to-source-code-on-dfp-example-docs,"wanted to have a look at the new source code but the link to the source code on the new morpheus docs for DFP inferencing example is broken
the link to the doc is : https://docs.nvidia.com/launchpad/ai/ai-workflow-dfp/latest/ai-workflow-dfp-step-02.html
the broken link to the source code is : https://catalog.ngc.nvidia.com/enterprise/orgs/nvaie/resources/cybersecurity_dfp_sourceThanks Tamir, you’ve identified a gap in our public LaunchPad documentation. We’re working on fixing that ASAP.Essentially, one has to register for NVIDIA AI Enterprise to access the workflow charts, containers, and resources from NGC. This will be made clearer in the pending doc update.Powered by Discourse, best viewed with JavaScript enabled"
499,system-reboot-when-running-tensorrt-on-536-40-driver,"I am running object detection on my GPU inside a container.  When the object detection runs, my system will hard reboot, no bluescreen, and no warnings in any system logs.I rolled back to driver version 528.49 and the issue goes away and object detection runs without issue.  I’m not yet sure where between 528 and 536 this starts happening.TensorRT Version: 8.4.1.5 - 8.6.1
GPU Type: GTX 1080ti
Nvidia Driver Version: 536.40
CUDA Version: 11.7 - 12.1
CUDNN Version: 8.7 - 8.9
Operating System + Version: Windows 11 + WSL2 (Ubuntu 20.04)
Python Version (if applicable): 3.9
TensorFlow Version (if applicable):
PyTorch Version (if applicable):
Baremetal or Container (if container which image + tag):  Package frigate · GitHubIssue observed using various yolo models generated from GitHub - yeahme49/tensorrt_demos: TensorRT MODNet, YOLOv4, YOLOv3, SSD, MTCNN, and GoogLeNet.
This model is automatically downloaded and converted when the above container runs.docker-compose.yml (640 Bytes)
config.yml (816 Bytes)Hi,We recommend you please install the TensorRT from https://developer.nvidia.com/nvidia-tensorrt-8x-download or TensorRT | NVIDIA NGC and try again.Thank you.Powered by Discourse, best viewed with JavaScript enabled"
500,dealing-with-data-dependent-slices-in-tensorrt-8-4,"I’m exporting a model that uses a custom operator (from pytorch_scatter), so I wrote a custom TRT plugin that implements the operator and I’m now trying to export a custom model with it.torch_scatter.scatter_sum (the function that I am trying to export) has dynamic output shape based on the data in one of its parameters and, since this doesn’t seem to be supported in TRT 8.4 (especially so for a TRT plugin), I thought of precalculating the value at which the output needs to be sliced, running the plugin op with a fixed output shape and following it with a sliceTo keep things extremely simple and only test the bare minumum, I wrote the following pytorch model:This can be exported successfully to ONNX. After applying the necessary node changes to place the TRT_PluginV2 ONNX node, we get this onnx model.Finally, I try to export the model with:This raises the following error:What is the meaning of the error? Am I doing something wrong or misunderstanding how Slice should be used?TensorRT Version: 8.4
GPU Type: Tesla V100
Nvidia Driver Version: 515.65
CUDA Version: 11.2
CUDNN Version: 8.1.1
Baremetal or Container (if container which image + tag): BaremetalHi,
Please refer to below links related custom plugin implementation and sample:This Samples Support Guide provides an overview of all the supported NVIDIA TensorRT 8.6.0 Early Access (EA) samples included on GitHub and in the product package. The TensorRT samples specifically help in areas such as recommenders, machine...TensorRT is an SDK for high performance, deep learning inference. It includes a deep learning inference optimizer and a runtime that delivers low latency and high throughput for deep learning…While IPluginV2 and IPluginV2Ext interfaces are still supported for backward compatibility with TensorRT 5.1 and 6.0.x respectively, however, we recommend that you write new plugins or refactor existing ones to target the IPluginV2DynamicExt or IPluginV2IOExt interfaces instead.Thanks!Hello,
I had already seen the Sample Support Guide (the one you link is for version 8.6 EA, while I’m on 8.4, but they seem to be equivalent for the problem at hand).The custom plugin is written for IPluginV2DynamicExt. From what I read in the developer guide, the *IOExt interface is intended for plugins needing INT8 support, which is not the case for me.Powered by Discourse, best viewed with JavaScript enabled"
501,nvaudioeffects-dll-missing-function-listed-in-programming-guide,"Hello,The Maxine Audio Effects SDK discusses VAD (Voice Activity Detection) algorithm as part of the noise suppression feature. The enabling/disabling of VAD is available in Windows however the function ‘NvAFX_GetBoolList’ to request the status is missing. The programming guide does not state this is Linux only.DLL Viewer644×518 27.7 KBany update on this?Sorry for the delay on response. I forwarded this to the Maxine engineering team on Thursday and they’re looking into it.Powered by Discourse, best viewed with JavaScript enabled"
502,cuda-error-out-of-memory-out-of-memory,"I was training a model and interrupted it during training to modify learning rate parameter. On next training, during model initialisation it started to throw errors:It tries to allocate the memory, sometimes it successfully gets to ~8gb and initialize the model, and training process goes as usual. But process is not stable.I’ve googled a bit, and found a kinda solutionIt successfully initialize the model, compiles it, but crashes on first epoch:File was still there. And it’s a bit strange that it starts to throw these errors. But I’ve reinstalled cuda, cudnn, tensorflow just in case.To check the correct cuda+cudnn+tensorflow installation, I’ve ran import tensorflow as tf; print(tf.reduce_sum(tf.random.normal([1000, 1000]))), which imports tensorflow and does some basic tensor manipulations.As you can see, it executes the function using cuda, but before that, throws the same error.I could think that this is a vram issue, but occt test OK(but it checks only 10gb), mods\mats check OK, gaming is OK. I couldn’t find any solution on how to test cuda perfomance with using batches of memory. I’ll get 2080ti soon to check if itll work on this machine to be sure if it’s a software issue or hardware issue. But again, gaming is ok, no artifacts.my specs:Hi @vladislav.abyshkin ,
While i am checking on this, in the meantime can you pls confirm if there are no other processes running on the system?
Also can you please check if the below file was at toolkit\cuda\bin ?File was still therePowered by Discourse, best viewed with JavaScript enabled"
503,how-to-build-ml-model-using-bigquery,"
Click the image to read the article
Find more #DSotD postsHave an idea you would like to see featured here on the Data Science of the Day?Thnx for sharing it! I find it really helpful! Maybe you have something similar to share with the rest of the world?Hey there! Thanks for sharing this article on building ML models using BigQuery. I’m new to this forum and just stumbled upon this post. I’m really interested in learning more about this topic, and your post has definitely helped me gain a better understanding.
I actually discovered another resource that might be useful for anyone: guide on ga4 to bigquery integration. It walks you through the process step by step. I found it to be really informative and thought it might be a helpful addition to what you’ve already shared.
I’m really excited to delve deeper into this topic and learn more about how to use BigQuery for ML modeling. It’s amazing how much data we have access to these days, and being able to use it to gain insights and make predictions is really exciting to me.Powered by Discourse, best viewed with JavaScript enabled"
504,need-help-for-nvidia-dali,"Hello,Can I have a working example of “nvidia.dali.fn.random_bbox_crop” with all the parameters?Thanks.Hi,This forum talks more about issues and updates related to TensorRT. We recommend that you please reach out to the DALI related platform to get better help.The following document may help you.
https://docs.nvidia.com/deeplearning/dali/user-guide/docs/operations/nvidia.dali.fn.random_bbox_crop.html#nvidia.dali.fn.random_bbox_cropThank you.This topic was automatically closed 14 days after the last reply. New replies are no longer allowed.Powered by Discourse, best viewed with JavaScript enabled"
505,no-result-when-specifying-offline-mode-and-streaming-false,"Please provide the following information when requesting support.Hardware - GPU - RTX 3060 12GB
Hardware - CPU - AMD Ryzen 3600
Operating System - Ubuntu 22.04
Riva Version - 2.11
TLT Version (if relevant)When configuring a model with riva-build for offline ASR, and specifying --streaming=false, no responses are returned when using RecognizeRequest. When --streaming=true, it works fine. I need to disable streaming to reduce GPU memory footprint. I’ve tried with the Conformer-CTC Large and XL models with the same result.Config (for Large model):Hi @steve.pritchardThanks for your interest in RivaI will check with the internal team regarding this issue and get back
If I need additional information midway I will let you knowKind Regards,
RoshandevHi, any feedback on this one? Cheers.Hi, following up on this. This is still true for 2.12.1Powered by Discourse, best viewed with JavaScript enabled"
506,getplugincreator-could-not-find-plugin-scatterelements-version-1,"When I depoly CodeFormer with tensorrt, got an error “could not find plugin: ScatterElements version: 1”. And with the limit of the environment, I can’t update tensorrt to 8.2+。
Then I try to convert scatterElement to scatterNd, but got an error: Invalid Node: concat_1022I locate the problem in this code:min_tmp = torch.ones(int(indices.shape[0]) * int(indices.shape[1]) * self.codebook_size).to(indices)
indices.flatten()
tmp = torch.add(self.tmp, indices)
min_tmp[tmp] = 1
min_tmp = min_tmp.view(256, self.codebook_size)The origin python code:ndices = indices.view(-1, 1)  # [batch * 256, 1]
min_encodings = torch.zeros(int(indices.shape[0]), self.codebook_size).to(indices)
min_encodings.scatter_(1, indices, 1)Thanks for your help.TensorRT Version:  8.0.1.6The onnx file CodeFormer.onnx - Google DriveHi,
Please refer to below links related custom plugin implementation and sample:This Samples Support Guide provides an overview of all the supported NVIDIA TensorRT 8.5.3 samples included on GitHub and in the product package. The TensorRT samples specifically help in areas such as recommenders, machine comprehension, character...TensorRT is an SDK for high performance, deep learning inference. It includes a deep learning inference optimizer and a runtime that delivers low latency and high throughput for deep learning…While IPluginV2 and IPluginV2Ext interfaces are still supported for backward compatibility with TensorRT 5.1 and 6.0.x respectively, however, we recommend that you write new plugins or refactor existing ones to target the IPluginV2DynamicExt or IPluginV2IOExt interfaces instead.Thanks!Powered by Discourse, best viewed with JavaScript enabled"
507,seqdata-and-multi-head-attention,"Hi,In cudnnSetAttnDescriptor() and the fwd, bwd weight, bwd data APIs.(1) What are the Q K V input data layouts required in the global memory if the corresponding Q, K, and V projection size is set to zero (the input is the data output by the specific linear layer)? To be specific, does vec_dim have the size [numHeads*headDim] with headDim continuous in the global memory?(2)  Why oSize equals to numHeads*vSize if both the linear layers for V and O are not included? This requirement seems to consume more global memory than normal attentions do (more precisely, numHeads times more).HI @jundaf3 ,
Below link should be able to assist you better.
https://docs.nvidia.com/deeplearning/cudnn/api/index.html#cudnnSetAttnDescriptorThanksPowered by Discourse, best viewed with JavaScript enabled"
508,miss-match-riva-k8s-deployment-documentation,"Try to follow docs: How to Deploy Riva at Scale on AWS with EKS — NVIDIA Rivariva helm version 2.12.1 does not containI think the documentation is not updated for 2.12.1  versionHi @mehadi.hasanThanks for your interest in RivaThanks for your feedback,I guess the documentation is now updated, can you check and let me know if it is fineThanksPowered by Discourse, best viewed with JavaScript enabled"
509,cannot-create-a-cuda-stream-error-loading-the-dynamic-library-windows-10-rtx-3070,"i download all this:cuda-downloads?target_os=Windows&target_arch=x86_64&target_version=10&target_type=exe_local
and
us.download.nvidia.com/Windows/536.67/536.67-desktop-win10-win11-64bit-international-nsd-dch-whql.exe
and
international.download.nvidia.com/Windows/broadcast/sdk/VFX/nvidia_video_effects_sdk_installer_v0.7.2_ampere.exethis is my run.batand show this errorhow fix this?Powered by Discourse, best viewed with JavaScript enabled"
510,torchpoints3d-pointnet2-deploy-to-tensorrt-with-custom-plugin,"Currently trying to export a model from the Torch Points 3D framework (PointNet2) to ONNX to then get to TensorRT where I can load and run inference in C++. There is an unsupported function three_interpolate that would require me to define it as a custom ONNX operator and then I would need to define it as a custom plugin in  TensorRT. Originally I attempted to go into a simpler route of setting up Torch Points 3D in the Jetson Xavier but had dependency issues that lead me to consider this as the main option.Would it be possible to implement this as a custom plugin (three interpolate related functions)?This is the original error when trying to export:For Training/Export:
Python – 3.8
PyTorch – 1.7.0
Docker Container in an Ubuntu System (from this dockerfile torch-points3d/Dockerfile.gpu at master · nicolas-chaulet/torch-points3d · GitHub, which is pretty much the work environment of the torch points 3d developers)Other Steps:
ONNX to TensorRT Serialization - Jetson Xavier
Inference Baseline - Jetson Xavier (ubuntu arm64/aarch64)
Model Trained - PointNet++ from the torch points 3d framework (GitHub - nicolas-chaulet/torch-points3d: Pytorch framework for doing deep learning on point clouds.)Thanks,MarkHi,
Below link might help you with your query, Kindly check below link for all 3d support layers:These support matrices provide a look into the supported platforms, features, and hardware capabilities of the NVIDIA TensorRT 8.4.3 APIs, parsers, and layers.This is the revision history of the NVIDIA TensorRT 8.4 Developer Guide.Thanks!Thanks for the links! Looking into that information it seems like I would need to prepare a custom plugin regardless and while talking to other NVIDIA representatives on the phone it seems like this is the best approach instead of setting up the Torch Points 3D environment in the Jetson.I will be updating this forum as I work through setting up the custom plugin and for now I will be following the steps metioned in: Comparison of the trtexec tool with plugins option and putting plugins in libProcessing: image.png…After working with the plan mentioned above I ran into another interesting problem… after implementing the ThreeInterpolate function I got to the point where I could compile it so I tried trtexec. After setting up the library path correctly the plugin was found but I stumbled with having another layer that is not supported either, OneHot. Considering it was a more recognizable layer I tried to find an implementation for it and there was one here: GitHub - hobrasoft/tensorrt-onehot: Implements OneHot plugin for Nvidia TensorRTAfter setting it up I tried trtexec again and kept getting an error about not having an op importer for it. I tried a few more things from the TensorRT side thinking that there was a problem there to soon realize it was onnx parser related (which made sense later considering the source file of the error). As of latest version of onnx/onnx_tensorrt there should be a function to register unknown operators straight to a TensorRT Plugin:But sadly since I am limited to using TensorRT 7.1.3 the version I have to work with doesn’t have that Fallback function implemented…:At this moment I find myself wondering if I could use one TensorRT to generate a serialized engine (newer enough to have that fallback function) and then a different one to load it?I am guessing it should be a no but I am running out of options.This is the trtexec log:Thanks,MarkHi Mark,OneHot operator is currently not supported in TensorRT. You may need to implement custom plugin.Please check following doc for supported operators.Thank you.Hi @spolisetty,As I stated in my message I had taken an approach to implement OneHot as a custom operator with one found in github so that was not the main issue metioned, the big issue was that for the TensorRT version I am using the functionality of “Fallback Function” was not implemented meaning that if there is no existing onnx parser importer available the trtexec tool fails even though I do have a custom operator available and identified. I was thought maybe performing the serialization with a newer version of that supports fallback functions and then loading the model in the version I intended to use but I would expect it to fail because of things being serialized/deserialized differently in the different versions… could it be possible though?Thanks,MarkHi,I have managed to pass the previous roadblock related to the onnx parser importer by defining built in importers following the code for the other TRT operators at the end of the builtin_op_importers.cpp file for the 7.1.3 version (main reference being the TRT_PluginV2 importer @ line 3584). Currently managed to get the importers to a point both custom plugins layers are registered and have succesfully passed the onnx parsing step. In the validation step I now find myself with this error:I verified that the condition tensor was of boolean type but this error might be in regards to the actual values inside of the tensor not being boolean. I find it weird that this is happening considering that layers right before it are not the new custom ones and are not having any problems being validated (meaning that the data should be in the right format by the time it gets here).I tried tracking the error in the source code to see where this error is brought up but did not manage to find its origin. I can track the flow of the code up until the onnx parsing ends and the conversion to an engine begins in these files but can’t get further from that:This would be the complete output of the trtexec tool at this time (there are some prints for debugging purposes that can be identified because they don’t have the TRT log pattern):trtexec_w_custom_plugins_2022-01-07.txt (295.9 KB)Thanks,MarkHi,Considering the built in Where layer implementation was giving issues I tried creating another Plugin for the Where function (as suggested by another NVIDIA representative) in an attempt to be more flexible with the data needed. I ended up getting a different error in the same place as the other one:It seems as the object passed between those layers have certain properties that can not necessarily be worked around in the middle of the process… the only way I managed to completely pass the error was to pass in one of the inputs as output (it being something that can be converted as a ShapeTensor and did not use any function that would require condition tensor to be a boolean). For now its more of a workaround to allow more progress in following parts but it seems like it could be something that’s greatly embedded and hard to manually modify…What should this output object contain besides corresponding shape and data type to be successful in this step?Thanks,MarkHi,Sorry for the delay in addressing this issue.
Could you please let us know and give more details, if you’re still facing above issue or your queries unanswered.Thank you.Hi @Mark.RiveraMelendez ,Any updates on your progress? Were you able to port the pointnet2 to TensorRT?Thank you in advance.Cheers,Hi @schennDue to many of these issues the deployment was continued c++ tensorflow. Can’t share the full details but as a general note, custom layer models are better deployed through serialization by rewriting it in a way you know they can be deployed. Frameworks can be too tricky to modify in order to achieve this type of deployment.Good luck!@Mark.RiveraMelendez
Hi Mark,Can you please explain what you mean by “custom layer models are better deployed through serialization”?Thanks!!@Mark.RiveraMelendez I also faced issues getting a custom plugin imported with older versions of TensorRT. My solution at the time was to export the supported pieces of the model in ONNX, and then connect the unsupported layers through the TensorRT network builder.For example, I would export an onnx model that has an output which is the input to that layer, and an input that is the output from that layer. Then I would parse the onnx field, find those inputs/outputs, and attach those tensors to the custom plugin.e.g. ONNX input -> A -> fake_output, fake_input -> B -> output
Then through the C++ api I would turn it into
input -> A -> custom_plugin -> B -> outputI can give more precise details if it’s helpfulPowered by Discourse, best viewed with JavaScript enabled"
511,nvidia-dali-filename-as-additional-dataloader-output,"Hello,I am using Nvidia Dali as a dataloader for loading train images and labels. However, I want to adapt the code so that the dataloader will also retrieve an additional attribute of the image, e.g. the filename or some other attribute which belongs to the image and is stored in a textfile.
Let’s assume that I want to retrieve the filename, how can I modify the code in order to receive the filename of each image as a third output in addition to images and labels?Please find my code snippet below:Hi,We recommend that you please refer to the DALI documentation.
https://docs.nvidia.com/deeplearning/dali/user-guide/docs/FAQ.htmlIf your query is still not answered, please reach out to the DALI related forum to get better help.A GPU-accelerated library containing highly optimized building blocks and an execution engine for data processing to accelerate deep learning training and inference applications. - GitHub - NVIDIA/...Thank you.Powered by Discourse, best viewed with JavaScript enabled"
512,unet-inference-on-jetson-nano-faces-problem-of-compiling-the-onnx-file-to-the-engine-file,"Hello all,I use jetson nano to convert my onnx file of Unet. However, I get the following error:
[08/03/2023-15:47:01] [TRT] [W] onnx2trt_utils.cp:366: Your ONNX model has been generated with INT64 weights, while TensorRT does not natively support INT64. Attemptting to cast down to INT32.
ERROR: Failed to parse the ONNX file.
In node 0 (parseGraph): INVALID_NODE: Invalid Node StatefulPartitionedCall/model_1/conv2d/BiasAdd__6
Attribut not found: allowzeroNvidia Driver Version: Jetson nano
CUDA Version: 10.2.300
CUDNN Version: 8.2.1.32-1+cuda10.2
Operating System + Version: Ubuntu 18.04
Python Version (if applicable): Python 3.6.9Could you help me to solve this problem?Hi,This looks like a Jetson issue. Please refer to the below samples in case useful.Hello AI World guide to deploying deep-learning inference networks and deep vision primitives with TensorRT and NVIDIA Jetson. - GitHub - dusty-nv/jetson-inference: Hello AI World guide to deployin...An easy to use PyTorch to TensorRT converter. Contribute to NVIDIA-AI-IOT/torch2trt development by creating an account on GitHub.For any further assistance, we will move this post to to Jetson related forum.Thanks!Powered by Discourse, best viewed with JavaScript enabled"
513,cudnnmultiheadattnforward-with-bad-params,"hi,I have a problem with wrong parameters when using cudnn api cudnnMultiHeadAttnForward(),  LogInfo is attached。
image1044×949 17.3 KB
Shape is as follows
bathsize=1
qo_beam_size=1
kv_beam_size=1
qo_seq_len=4
kv_seq_len=4
q_in_len=4   // vector size of each word
k_in_len=4
v_in_len=4
q_proj_len=4
k_proj_len=4
v_proj_len=4
o_proj_len=4I tried LogWarn and LogErr, but get nothing.CUDNN_STATUS_BAD_PARAMS occured when calling cudnnMultiHeadAttnForward() function.Everthing works fine when I set param currIndx=-1.  what is the limit between qkv shapes when curId>=0?thanksCan you please share the error logs with us?
ThanksPowered by Discourse, best viewed with JavaScript enabled"
514,segmentation-fault-core-dumped,"My code never reported an error in 3090, but it often did in 4090 “segmentation fault core dumped”. Why? How do I fix it?Hi @2651449412 ,
This might be a hardware issue.
please check the compatibility matrix of TRT and you can raise the concern to get better support on respective ForumThanksPowered by Discourse, best viewed with JavaScript enabled"
515,riva-2-11-0-nemo-23-03-unable-to-deploy-nmt,"Please provide the following information when requesting support.Hardware - GPU T4
Riva Version 2.11.0
Nemo Version 23.03Reproduce as:Expected behaviour, a successful export of a riva file, as per Custom Models — NVIDIA Riva.The issue is in
/usr/local/lib/python3.8/dist-packages/nemo2riva/patches/mtencdec.py where change_tokenizer_names is missing **kwargs, i.e. it shout beThe mentioned fix allows me to safely deploy en_de_24x6.nemo, but I have problems deploying a custom nmt model built with nemo:23.03.Riva does not start up, it lists the following error:I’ve tracked the issue down to line 96 in /usr/local/lib/python3.8/dist-packages/servicemaker/triton/nmt.py, which has a hardcoded value [-1,32000] for the classifier output.However, I can not find why my custom built model has a value of 64000 (if I modify the classifier .pbtxt by hand the model loads and works as it should). Is this the size of the target language tokenizer (I use 64000 tokens).Any help appreciated.Powered by Discourse, best viewed with JavaScript enabled"
516,cant-download-any-files-https-catalog-ngc-nvidia-com-orgs-nvidia-teams-maxine-resources-maxine-windows-ar-sdk-ga-files,"Hello I can’t download anything from Maxine Windows AR SDK | NVIDIA NGC
Can anyone help me or give me a valid download link?Hi, I am going to move this topic over to the Maxine forum for better visibility.Please provide more context. Are you signed into NGC?Yes I have logged in but when I click on download nothing happens.Tried on various android,ios,Windows and Mac OSs but can’t download it.Could you copy and paste what link, if any, is presented to you? Maybe a screenshot if you are comfortable with that?https://catalog.ngc.nvidia.com/orgs/nvidia/teams/maxine/resources/maxine_windows_ar_sdk_ga/files#
IMG_20230414_234927.jpg1920×952 108 KB
Could you please provide me a link that works ?Its emergency.Hello, Im facing the same problem, did you find a solution?Im facing the same problem did you guys find any solution?Tracking here : Maxine SDK Download Link is dead - Deep Learning (Training & Inference) / Maxine - NVIDIA Developer ForumsPowered by Discourse, best viewed with JavaScript enabled"
517,tensorrt-concurrent-or-parrellel-inference-in-one-gpu-in-jetson-platform,"TensorRT C/C++ problem: On the Jetson Orin device, I started multiple threads, each with a trt file for cyclic AI inference (apply memory ->inference ->release memory). The context used was enqueueV3’s inference method(context->enqueueV3), and the methods used for applying and releasing memory were cudaMallocManaged() and cudaFree(). After the program runs, the memory in both threads shows continuous growth （no releasing buffers of input and output pointers, maybe the volumes of input (in KB) and output (in Bytes) buffer are too tiny.）. That is “Memory Leak” ? !
Whatever Process I or II, they all reuslts in “Memory Leak”. However, the speed of memory leakage in Process II is faster than that in Process I.Process I：
nvinfer1::IRuntime *runtime=…;
nvinfer1::ICudaEngine engine =…;
while(1) { // do inference in infinite loop
nvinfer1::IExecutionContext context = engine->createExecutionContext();
…
cudaStream_t stream;
cudaStreamCreate(&stream);
context->setTensorAddress(INPUT_Name, (void *)inputPtr);
context->setTensorAddress(OUTPUT_Name, (void *)outputPtr);
context->enqueueV3(stream);context->destroy();
}
engine->destroy();
runtime->destroy();Process II：
nvinfer1::IRuntime *runtime=…;
nvinfer1::ICudaEngine engine =…;
nvinfer1::IExecutionContext context = engine->createExecutionContext();
while(1) { // do inference in infinite loop
…
cudaStream_t stream;
cudaStreamCreate(&stream);
context->setTensorAddress(INPUT_Name, (void *)inputPtr);
context->setTensorAddress(OUTPUT_Name, (void *)outputPtr);
context->enqueueV3(stream);
}
context->destroy();
engine->destroy();
runtime->destroy();JetPack Version: 5.1-b147
TensorRT Version: 8.5.2-1
GPU Type:  Jetson Orin NX 16GB
Nvidia Driver Version:
CUDA Version: 11.4
CUDNN Version: 8.6
Operating System + Version: Linux orinnx 5.10.104-tegraPlease attach or include links to any models, data, files, or scripts necessary to reproduce your issue. (Github repo, Google Drive, Dropbox, etc.)Please include:Hi,The below links might be useful for you.This Best Practices Guide covers various performance considerations related to deploying networks using TensorRT 8.0.3. These sections assume that you have a model that is working at an appropriate level of accuracy and that you are able to...The programming guide to the CUDA model and interface.https://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__STREAM.htmlFor multi-threading/streaming, will suggest you to use Deepstream or TRITONFor more details, we recommend you raise the query in Deepstream forum.orraise the query in Triton Inference Server Github instance issues section.Triton Inference Server has 31 repositories available. Follow their code on GitHub.Thanks!Powered by Discourse, best viewed with JavaScript enabled"
518,trt-cuda-lazy-loading-is-not-enabled,"export CUDA_MODULE_LOADING=LAZY
not workHi,Could you please share with us more details like complete verbose logs, minimal issue repro model/script and the following environment details,TensorRT Version :
GPU Type :
Nvidia Driver Version :
CUDA Version :
CUDNN Version :
Operating System + Version :
Python Version (if applicable) :
TensorFlow Version (if applicable) :
PyTorch Version (if applicable) :
Baremetal or Container (if container which image + tag) :Thank you.Hi, I have a same erro when i run the  demo(TensorRT/cpp) of YOLOX, and this is my environment details:
TensorRT Version : 8.5.1.7
GPU Type :RTX 3060
Nvidia Driver Version : 516.94
CUDA Version :v11.7
CUDNN Version :8.5.0.96
Operating System + Version : Windows11 22621.963
Python Version (if applicable) : not used
TensorFlow Version (if applicable) : not used
PyTorch Version (if applicable) : not usedHi,Looks like you’re using windows platform. Please add the environment variable correctly by following similar steps below.If the PATH, ORACLE_SID, and ORACLE_HOME environment variables do not exist, you must create them.Also please refer to the following doc for more details on Lazy loading.
https://docs.nvidia.com/cuda/cuda-c-programming-guide/lazy-loading.html?highlight=environment%20variable#lazy-loadingThank you.I’m also running TensorRT 8.5, but on linux.  I confirm not work.[12/26/2022-11:29:32] [TRT] [W] CUDA lazy loading is not enabled. Enabling it can significantly reduce device memory usage. See CUDA_MODULE_LOADING in CUDA C++ Programming Guide
Loading ONNX file from path …/yolov7/runs/train/yolov7_master3/weights/best.onnx…
Beginning ONNX file parsing
[12/26/2022-11:29:32] [TRT] [W] onnx2trt_utils.cpp:377: Your ONNX model has been generated with INT64 weights, while TensorRT does not natively support INT64. Attempting to cast down to INT32.
[12/26/2022-11:29:32] [TRT] [W] onnx2trt_utils.cpp:403: One or more weights outside the range of INT32 was clamped
Completed parsing of ONNX file
Building an engine from file …/yolov7/runs/train/yolov7_master3/weights/best.onnx; this may take a while…
Completed creating Engine
[12/26/2022-11:31:58] [TRT] [W] CUDA lazy loading is not enabled. Enabling it can significantly reduce device memory usage. See CUDA_MODULE_LOADING in CUDA C++ Programming Guide
[12/26/2022-11:31:58] [TRT] [W] The getMaxBatchSize() function should not be used with an engine built from a network created with NetworkDefinitionCreationFlag::kEXPLICIT_BATCH flag. This function will always return 1.
[12/26/2022-11:31:58] [TRT] [W] The getMaxBatchSize() function should not be used with an engine built from a network created with NetworkDefinitionCreationFlag::kEXPLICIT_BATCH flag. This function will always return 1.
[12/26/2022-11:31:58] [TRT] [W] The getMaxBatchSize() function should not be used with an engine built from a network created with NetworkDefinitionCreationFlag::kEXPLICIT_BATCH flag. This function will always return 1.
[12/26/2022-11:31:58] [TRT] [W] The getMaxBatchSize() function should not be used with an engine built from a network created with NetworkDefinitionCreationFlag::kEXPLICIT_BATCH flag. This function will always return 1.
[12/26/2022-11:31:58] [TRT] [W] The getMaxBatchSize() function should not be used with an engine built from a network created with NetworkDefinitionCreationFlag::kEXPLICIT_BATCH flag. This function will always return 1.I bet you can easily reproduce this by running the yolo sample packed with TensorRT.  Most people just ignore this message.  I suggest suppressing it if it’s not adding value.Any updates on this?Powered by Discourse, best viewed with JavaScript enabled"
519,tensorrt-installation-issue-on-agx-orin,"The default TensorRT version which is installed on ORIN with Jetpack 5.1/5.0 is 8.5( with ubuntu 20.04 , python 3.8) . When the OS is upgraded to Ubuntu 22.04, then the default python Version has 3.10.What would be the suitable solution for this bug ?Hi,
Please refer to the installation steps from the below link if in case you are missing on anythingThis NVIDIA TensorRT 8.6.0 Early Access (EA) Installation Guide provides the installation requirements, a list of what is included in the TensorRT package, and step-by-step instructions for installing TensorRT.
Also, we suggest you to use TRT NGC containers to avoid any system dependency related issues.NVIDIA TensorRT is a C++ library that facilitates high-performance inference on NVIDIA graphics processing units (GPUs). TensorRT takes a trained network and produces a highly optimized runtime engine that performs inference for that network.Thanks!Powered by Discourse, best viewed with JavaScript enabled"
520,bash-trtexec-command-not-found,"I tried to build trtexec in .../TensorRT/samples. I followed this git link for building the sample but it didn’t work. It looks like it’s not a valid command with the message :
bash: trtexec: command not foundTensorRT Version: 7.1.0:
GPU Type: Xavier:
Nvidia Driver Version: N/A:
CUDA Version: 10.2:
CUDNN Version: n/a:
Operating System + Version: Ubuntu 18.04:
Python Version (if applicable):
TensorFlow Version (if applicable):
PyTorch Version (if applicable):
Baremetal or Container (if container which image + tag):I am new to the platform. Thanks for your help!Please refer below link in case it helps:ThanksI think the building steps are similar and and intuitive with make commad. But I am not sure why I got the error after make the trtexec and I cannot use the trtexec command.Which Jetpack version you are currently using?
Can you try to reinstall the latest Jetpack?It should be located in  /usr/src/tensorrt/bin or /opt/tensorrt/binThanksHi,
I have the exact same problem that the command is not recognized. I am also using a xavier and i also tried it on tx2. The build using make in /usr/src/tensorrt/samples/trtexec/ looks to be succesful and I can see the files in /usr/src/tensorrt/bin but when I try to use trtexec command i get : -bash: trtexec: command not foundI have read somewhere that maybe I should add it to the path but I do not know how that works. Any idea how I can solve this ?
Thank you in advanceAfter the make command I ended up with creating an alias:alias trtexec=""/usr/src/tensorrt/bin/trtexec""Hi,
Request you to share the ONNX model and the script if not shared already so that we can assist you better.
Alongside you can try few things:import sys
import onnx
filename = yourONNXmodel
model = onnx.load(filename)
onnx.checker.check_model(model).
2) Try running your model with trtexec command.
https://github.com/NVIDIA/TensorRT/tree/master/samples/opensource/trtexec
In case you are still facing issue, request you to share the trtexec “”–verbose"""" log for further debugging
Thanks!I actually solved by problem by simply copying the trtexec from “/usr/src/tensorrt/bin/trtexec/” to  “/usr/local/bin/” I think there might have been a problem with linking the file.alias trtexec=""/usr/src/tensorrt/bin/trtexecthis worked for me, thanks a lotPowered by Discourse, best viewed with JavaScript enabled"
521,help-maxine-gaze-redirect-change-the-colors-of-my-video,"Hello ! I tried Gaze Redirect on a video i recorded today with my phone. The result is very “white” I don’t know why. How can I change this setting ?Thanks !

using gaze redirect1642×928 104 KB
This is without Gaze Redirect

without gaze redirect1187×665 55.5 KB
This is not an expected output, could you provide me with the command you used to process your video?Of course !Here is the command I use :
SETLOCAL
SET PATH=%PATH%;…..\samples\external\opencv\bin;…..\bin;
SET NVAR_MODEL_DIR=…..\bin\models
GazeRedirect.exe --offline_mode --split_screen_view=false  --in=IMG_4619.mp4I found a solution. I used HandBrake and I encoded it in H.256 !This topic was automatically closed 14 days after the last reply. New replies are no longer allowed.Powered by Discourse, best viewed with JavaScript enabled"
522,cudnnnormalizationforwardtraining-vs-cudnnbatchnormalizationforwardtraining,"I’m wondering what are the differences between cudnnNormalizationForwardTraining() and cudnnBatchNormalizationForwardTraining().Initially I thought cudnnNormalizationForwardTraining() would perform more general normalization than batch normalization, but its documentation seems to suggest that it performs just batch normalization.  Also, the documentation indicates that both functions are based on the same paper entitled “Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift”.What is the critical difference between the two functions?Hi @wonseok.shinWhat is the critical difference between the two functions?I believe there is little to none. cudnnNormalizationForwardTraining() was intended to be a general API which could potentially cover different kinds of normalizations.Powered by Discourse, best viewed with JavaScript enabled"
523,jetson-xavier-nx-devkit-and-riva-2-10-0,"Hi,I have a Jetson Xavier NX DevKit device with JP 5.1 installed. I have loaded the Riva 2.10.0 Quck Start and am trying to use the Question Answering model working. I have changed the config.sh file in this way:When I set riva_target_gpu_family=""tegra"" and do a sudo bash riva_init.sh the system downloads the preconfig models and the script terminates normally.  However, when I do a sudo bash riva_start.sh the container doesn’t come up.  Doing a sudo docker logs riva-speech says that a process was terminated (can’t recall exact message but could provide the complete log, if desired).I have successfully gotten Riva en-US ASR working on another NX DevKit and Riva en_US TTS working on yet another NX DevKit (I have 3) with the riva_target_gpu_family=""tegra"" in config.sh. I can’t seem to get more than one of these features working on the same NX or just NLP, ASR en_US to another language with or without this setting.Hi @daniel.levineThanks for your interest in RivaApologies you are facing issues,Jetpack 5.1 is the correct version for Riva 2.10,I will check regarding this further with the team, request to kindly provideWe also kindly request to check if  the default runtime is set to nvidia on the Jetson platform by adding the following line in the /etc/docker/daemon.json file if not already done. Restart the Docker service using sudo systemctl restart docker after editing the file if not done earlier.
reference
https://docs.nvidia.com/deeplearning/riva/user-guide/docs/quick-start-guide.html#embeddedOnce done, please verify by running docker run -it --rm --runtime nvidia ubuntu:20.04 and let us know if it worksThanksI did a riva_clean.sh (I kept the containers: riva-speech:2.10.0-l4t-aarch64 and riva-speech:2.10.0-servicemaker-l4t-aarch64) and removed all the models to start clean.  riva-Init.sh finished successfully. Only one model was downloaded: models_nlp_intent_slot_misty_bert_base_v2.10.0-tegra-xavier. Imust admit that I was expecting something like distilbert-base-uncased to get pulled down too for the Q&A capability.My intent is to just have the Q&A demos working in my Jetson Xavier NX DevKit so only the Riva NLP was enabled. I have attached my config.sh:
config.sh (12.8 KB)Anyway, I ran riva_start.sh and after trying and retrying for a while it stops trying to bring the containers up. The results of docker logs riva-speech are attached:
NLP.tegra.log (1.7 KB)I have already made that change to /etc/docker/daemon.json as a part of the Quick Start process. docker run -it --rm --runtime nvidia ubuntu:20.04 succeeds after downloading the Ubuntu container and giving me a root prompt in the container.I should add that whenever I run the riva_*.sh scripts or docker commands, I need to use sudo or thet don’t succeed.Note, I see this in the non-tegra path of config.sh:
### BERT Base Question Answering model fine-tuned on Squad v2.       ""${riva_ngc_org}/${riva_ngc_team}/rmir_nlp_question_answering_bert_base:${riva_ngc_model_version}"" 
I was hoping that there would be something like this in the tegra path of config.sh:
### BERT Base Question Answering model fine-tuned on Squad v2.       ""${riva_ngc_org}/${riva_ngc_team}/rmir_nlp_question_answering_distilbert_base:${riva_ngc_model_version}"" It would appear that the inability to connect to 127.0.0.1:8001 is not the reason it doesn’t work (as far as I can tell), because my other Jetson Xavier NX DevKit running only ASR en_US  supporting models works just fine and spits out those messages to the log as well.Hi @daniel.levineThanks for sharing the details,I will check further with the internal team and provide updateThanksI added the below to the tegra NLP models to be downloaded to see what happens and turned on both the ASR and NLPs:### BERT Base Question Answering model fine-tuned on Squad v2. ""${riva_ngc_org}/${riva_ngc_team}/rmir_nlp_question_answering_bert_base:${riva_ngc_model_version}"" I ran riva_init.sh and all the expected models got downloaded from nvidia.  It looked like it took the qa rmir (which is not what the tegra path normally gets) in stride and converted it to model_repository and the script finished. I then ran the riva_start.sh to see what happened.  To my delight it ran for a while and then dropped me into the client testing container, which was a step forward.  However running the client/riva_qa_nlp complained about the qa model.  I can’ recall the error.  I’m now working to see if I can figure out how to turn another qa model file ending in .riva from the nvidia model library into an rmir and then put that in the model_repository by hand and see if there’s any difference.I also tried downloading this RIVA Question Answering | NVIDIA NGCAnd followed this process to convert the encrypted .riva model to produce a rmir file.  But in Phase 2 my riva-build fails like this:riva-build qa /servicemaker-dev/rmir-questionanswering_squad_english_bert:tao-encode /servicemaker-dev/questionanswering_squad_english_bert.riva:tao-encode 023-04-10 19:58:28,508 [WARNING] Property 'encrypted' is deprecated. Please use 'encryption' instead. 2023-04-10 19:59:00,102 [INFO] Packing binaries for language_model/PyTorch : {'ckpt': ('nemo.collections.nlp.models.question_answering.qa_model.QAModel', 'model_weights.ckpt'), 'bert_config_file': ('nemo.collections.nlp.models.question_answering.qa_model.QAModel', 'bert-base-uncased_encoder_config.json')} 2023-04-10 19:59:00,103 [INFO] Copying ckpt:model_weights.ckpt -> language_model:language_model-model_weights.ckpt Traceback (most recent call last):   File ""/usr/local/bin/riva-build"", line 8, in <module>     sys.exit(build())   File ""/usr/local/lib/python3.8/dist-packages/servicemaker/cli/build.py"", line 102, in build     rmir.write()   File ""/usr/local/lib/python3.8/dist-packages/servicemaker/rmir/rmir.py"", line 159, in write     newfact = art.create(name=target_name, content=fact.get_content(**cb), **props)   File ""<frozen eff.core.file>"", line 307, in get_content   File ""<frozen eff.core.file>"", line 269, in get_handle   File ""<frozen eff.core.file>"", line 216, in decrypt   File ""<frozen eff.core.file>"", line 236, in check_decryption PermissionError: The provided passphrase is invalid 
So, I’m using the passphrase AFAIK for the model for both the input and the output (even though I probably don’t need to do this for the output).Haven’t gotten past this yet to move on to the rest of Phase 2 and Phase 3 of the procedure. :-/I found that config.sh uses tlt_encode and used that in the above command instead of tao-encode and got further, so I’m guessing the nvidia model documentation is out of date. :-/So now riva-build finishes. Yay!So now I did the next step:riva-deploy /servicemaker-dev/rmir-questionanswering_squad_english_bert:tlt_encode /data/models 
But that fails with:2023-04-10 21:05:17,978 [INFO] Writing Riva model repository to '/data/models'... 2023-04-10 21:05:17,979 [INFO] The riva model repo target directory is /data/models 2023-04-10 21:05:49,808 [INFO] Using obey-precision pass with fp16 TRT 2023-04-10 21:05:49,809 [WARNING] /data/models/riva-trt-riva_qa-nn-bert-base-uncased already exists, skipping deployment.  To force deployment rerun with -f or remove the /data/models/riva-trt-riva_qa-nn-bert-base-uncased 2023-04-10 21:05:49,810 [WARNING] /data/models/qa_tokenizer-en-US already exists, skipping deployment.  To force deployment rerun with -f or remove the /data/models/qa_tokenizer-en-US 2023-04-10 21:05:49,810 [WARNING] /data/models/qa_qa_postprocessor already exists, skipping deployment.  To force deployment rerun with -f or remove the /data/models/qa_qa_postprocessor 2023-04-10 21:05:49,811 [INFO] Extract_binaries for self -> /data/models/riva_qa/1 2023-04-10 21:05:49,812 [ERROR] Traceback (most recent call last):   File ""/usr/local/lib/python3.8/dist-packages/servicemaker/cli/deploy.py"", line 100, in deploy_from_rmir     generator.serialize_to_disk(   File ""/usr/local/lib/python3.8/dist-packages/servicemaker/triton/triton.py"", line 447, in serialize_to_disk     RivaConfigGenerator.serialize_to_disk(self, repo_dir, rmir, config_only, verbose, overwrite)   File ""/usr/local/lib/python3.8/dist-packages/servicemaker/triton/triton.py"", line 314, in serialize_to_disk     self.generate_config(version_dir, rmir)   File ""/usr/local/lib/python3.8/dist-packages/servicemaker/triton/nlp.py"", line 404, in generate_config     tokenizer._inputs[0].name: ""IN_QUERY_STR__0"", AttributeError: 'RivaQATokenizer' object has no attribute '_inputs' 
This may have been what the original rmir I downloaded failed with as well.  It felt like the same kind of thing.I’m going to try this, since the error seems similar, except their problem was with ASR and mine is with QA: https://forums.developer.nvidia.com/t/riva-speech-skills-initialisation-error-rivatokenizer-object-has-no-attribute/After doing a riva_clean.sh, ensuring that NLP is the only capability set to true in config.sh and with the BERT QA RMIR added to the tegra path in riva_init.sh and all docker run commands in the riva_*.sh scripts now have --privileged added to them.The riva_init.sh finished silently, but didn’t populate model_repository/models with anything.  So I went beack to where Phase 1, 2, and 3 of the by hand method are described.  All I needed to do was a riva_deploy, which resulted in:ModelOutput(name=""SEQ__0"", data_type=self.model.input_ids_type, dims=[max_seq_length]), AttributeError: 'QuestionAnswering' object has no attribute 'input_ids_type'Hey there, has anyone reproduced my issue?Hi @daniel.levineI have created an internal thread and provided all the information, once they provide inputs/updates i will reply backThanksThanks.I have also tried the AMDx64 version of Riva 2.10.0 to see how it behaves. I was able to get the en_US ASR  to work and TTS stuff to work just like on the Jetson Xavier NX.  However the BERT NLP QA won’t allow the riva_start.sh to come up.  Similarly, the en_es NMT ASR capability also prevents riva_start.sh from coming up.However, I was enabled the NLP Megatron QA model in the script instead of the BERT QA one and it did come up and I was able to exercise the demo using it on the AMDx64 platform.  I tried to reproduce the result on the Jetson Xavier NX, but it doesn’t look like to converted the rmir for the model to anything as fast as I can tell.  riva_start.sh doesn’t come up, I believe, because there are no models.Powered by Discourse, best viewed with JavaScript enabled"
524,tensorrt-8-5-1-7-not-soupport-topk-this-version-of-tensorrt-only-supports-input-k-as-an-initializer,"when I trying to convert onnx model to TensorRT, there is a error such as:[04/26/2023-11:27:14] [E] [TRT] ModelImporter.cpp:732: ERROR: ModelImporter.cpp:168 In function parseGraph:
[6] Invalid Node - TopK_573
This version of TensorRT only supports input K as an initializer. Try applying constant folding on the model using Polygraphy: TensorRT/tools/Polygraphy/examples/cli/surgeon/02_folding_constants at master · NVIDIA/TensorRT · GitHub
[04/26/2023-11:27:14] [E] Failed to parse onnx fileThen I used Polygraphy tools to process the onnx model according to the above link to convert the model again, but the problem still exists.Does anyone know how to repair this problem?TensorRT Version: 8.5.1.7
GPU Type: 3090 24G
Operating System + Version: WindowsHi,
Request you to share the ONNX model and the script if not shared already so that we can assist you better.
Alongside you can try few things:This NVIDIA TensorRT 8.6.0 Early Access (EA) Quick Start Guide is a starting point for developers who want to try out TensorRT SDK; specifically, this document demonstrates how to quickly construct an application to run inference on a TensorRT...import sys
import onnx
filename = yourONNXmodel
model = onnx.load(filename)
onnx.checker.check_model(model).
2) Try running your model with trtexec command.master/samples/trtexecNVIDIA® TensorRT™, an SDK for high-performance deep learning inference, includes a deep learning inference optimizer and runtime that delivers low latency and high throughput for inference applicat...
In case you are still facing issue, request you to share the trtexec “”–verbose"""" log for further debugging
Thanks!Thank you for your help.This is my onnx model:
rtmdetnano.onnx (3.8 MB)Did you solve the parsing problem?Use the latest Tensorrt version 8.6+Powered by Discourse, best viewed with JavaScript enabled"
525,dockerfile-for-nvidia-l4t-tensorrt-containers,"I am looking for the DockerFile used to create the following container: nvcr.io/nvidia/l4t-tensorrt:r8.2.1-runtime. Basically, I need to build my own image from this TensorRT image since it does not have essential libraries that I need (cmake, glog, etc).There is this DockerFile: TensorRT/ubuntu-20.04-aarch64.Dockerfile at release/8.2 · NVIDIA/TensorRT · GitHub but it is not the same TensorRT version and it does not seem to be the same thing since this one actually installs cmake whereas when I use nvcr.io/nvidia/l4t-tensorrt:r8.2.1-runtime, I get cmake: command not found.*Container (if container which image + tag)**: nvcr.io/nvidia/l4t-tensorrt:r8.2.1-runtimeDocker Pull nvcr.io/nvidia/l4t-tensorrt:r8.2.1-runtime, then try to build any files using cmake.Thanks!Hi,
Please refer to the installation steps from the below link if in case you are missing on anythingThis NVIDIA TensorRT 8.6.0 Early Access (EA) Installation Guide provides the installation requirements, a list of what is included in the TensorRT package, and step-by-step instructions for installing TensorRT.
Also, we suggest you to use TRT NGC containers to avoid any system dependency related issues.NVIDIA TensorRT is a C++ library that facilitates high-performance inference on NVIDIA graphics processing units (GPUs). TensorRT takes a trained network and produces a highly optimized runtime engine that performs inference for that network.Thanks!Thanks for your quick reply, I actually did check the first website. But what actually is the TensorRT container link that you just sent? it does not have the TensorRT versions that I am used to seeing (it goes from 17 to 23) and it uses python3?I am actually using these containers: NVIDIA L4T TensorRT | NVIDIA NGC because my Jetson is running on L4T. My main issue with the containers here are that they don’t include cmake for some reason so I just wanted to double check the Dockerfile used to create the images, in particular for http://nvcr.io/nvidia/l4t-tensorrt:r8.2.1-runtime.The main I have is that after running the Dockerfile:FROM nvcr.io/nvidia/l4t-tensorrt:r8.2.1-runtime
RUN apt-get update && apt-get install -y 
libgoogle-glog-dev 
build-essential 
sudo-ldap
RUN cd /tmp && 
wget https://github.com/Kitware/CMake/releases/download/v3.21.4/cmake-3.21.4-linux-aarch64.sh && 
chmod +x cmake-3.21.4-linux-aarch64.sh && 
./cmake-3.21.4-linux-aarch64.sh --prefix=/usr/local --exclude-subdir --skip-license && 
rm ./cmake-3.21.4-linux-aarch64.shand I try to use cmake, I get the following error:CMake Error at /usr/local/share/cmake-3.21/Modules/CMakeTestCCompiler.cmake:69 (message):
The C compiler
“/usr/bin/cc”
is not able to compile a simple test program.
It fails with the following output:
Change Dir: /workspace/inference/build/CMakeFiles/CMakeTmp
Run Build Command(s):/usr/bin/make -f Makefile cmTC_b43c9/fast && /usr/bin/make  -f CMakeFiles/cmTC_b43c9.dir/build.make CMakeFiles/cmTC_b43c9.dir/build
make[1]: Entering directory ‘/workspace/inference/build/CMakeFiles/CMakeTmp’
Building C object CMakeFiles/cmTC_b43c9.dir/testCCompiler.c.o
/usr/bin/cc    -o CMakeFiles/cmTC_b43c9.dir/testCCompiler.c.o -c /workspace/inference/build/CMakeFiles/CMakeTmp/testCCompiler.c
cc: error trying to exec ‘cc1’: execvp: No such file or directory
CMakeFiles/cmTC_b43c9.dir/build.make:77: recipe for target ‘CMakeFiles/cmTC_b43c9.dir/testCCompiler.c.o’ failed
make[1]: *** [CMakeFiles/cmTC_b43c9.dir/testCCompiler.c.o] Error 1
make[1]: Leaving directory ‘/workspace/inference/build/CMakeFiles/CMakeTmp’
Makefile:127: recipe for target ‘cmTC_b43c9/fast’ failed
make: *** [cmTC_b43c9/fast] Error 2Hi @bayab ,
Here you can find the contents of the image of TRT 8.2.1, and I believe you can create a Dockerfile referring the one you have shared.ThanksThanks! Can you explain the difference between these TensorRT containers:TensorRT | NVIDIA NGC (the one you pointed me to)and these ones:NVIDIA L4T TensorRT | NVIDIA NGC ?Can I still run the containers in here TensorRT | NVIDIA NGC  if my Jetson Xavier runs on L4T?ThanksHi @bayabCan I still run the containers in here TensorRT | NVIDIA NGC  if my Jetson Xavier runs on L4T?I believe this can be done.
However i will re-validate this .
ThanksPowered by Discourse, best viewed with JavaScript enabled"
526,cudnn-conv-bias-fusion-using-backend,"I am trying to implement conv+bias fusion using the backend api but I am unable to get it working. Specifically, I have been following some demo code from an old forum post:
fuseOpDemo.cpp (20.7 KB)closely. I noticed that the documentation now has CUDNN_ATTR_CONVOLUTION_SPATIAL_DIMS for CUDNN_BACKEND_OPERATION_CONVOLUTION_FORWARD_DESCRIPTOR now which is not present in the old blog. Could someone provide an updated version of this example code? Or any simple and clear example on how to do conv+bias fusion with just backend API calls?Hi @ryantong
Are you still facing the issue?Powered by Discourse, best viewed with JavaScript enabled"
527,tensorrt-8-5-2-1-cuda11-8-pycuda-driver-logicerror-cumemcpyhtodasync-failed-invalid-argument,"Dear Nvidia community,Could you please help me to fix the below issue? Thank in advance!The full code:Hi,This looks like a Jetson issue. Please refer to the below samples in case useful.Hello AI World guide to deploying deep-learning inference networks and deep vision primitives with TensorRT and NVIDIA Jetson. - GitHub - dusty-nv/jetson-inference: Hello AI World guide to deployin...An easy to use PyTorch to TensorRT converter. Contribute to NVIDIA-AI-IOT/torch2trt development by creating an account on GitHub.For any further assistance, we will move this post to to Jetson related forum.Thanks!Hi @NVES, Thank you for your response. The issue was resolved for the fp32 model, but I’m still not getting accurate results for the fp16 and int8 models. Is it possible that my model is not compatible with fp formats lower than fp32?models was converted using trtexec utlity.Powered by Discourse, best viewed with JavaScript enabled"
528,why-tensorrt-got-stuck-when-using-threadpool-in-python,"After reference this draft and this draft  I wrote codes as below. The code got stuck when using thread pool.Can any one help out how to make it work properly?And I won’t my model to serve by flask frame with multithreading.TensorRT Version:   8.6.1
GPU Type:  A6000
Nvidia Driver Version:
CUDA Version:  V11.2.152
CUDNN Version:
Operating System + Version:  Ubuntu 20.04
Python Version (if applicable):  3.8.13
TensorFlow Version (if applicable):
PyTorch Version (if applicable):
Baremetal or Container (if container which image + tag):  ContainerPlease attach or include links to any models, data, files, or scripts necessary to reproduce your issue. (Github repo, Google Drive, Dropbox, etc.)Just run the script with your engine files to reproduceHi,The below links might be useful for you.This Best Practices Guide covers various performance considerations related to deploying networks using TensorRT 8.0.3. These sections assume that you have a model that is working at an appropriate level of accuracy and that you are able to...The programming guide to the CUDA model and interface.https://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__STREAM.htmlFor multi-threading/streaming, will suggest you to use Deepstream or TRITONFor more details, we recommend you raise the query in Deepstream forum.orraise the query in Triton Inference Server Github instance issues section.Triton Inference Server has 31 repositories available. Follow their code on GitHub.Thanks!Powered by Discourse, best viewed with JavaScript enabled"
529,tensorflow-can-not-detect-my-gpu,"Hello,I installed Tensorflow 2.11.0 and cuda 11.2.2 and also  8.1.0.77 in Anaconda application. When I try check my GPU with code snippet which in below:
import tensorflow as tf;
tf.config.experimental.list_physical_devices(‘GPU’)It says there is no GPU in system. But I have Nvidia RTX 3060 on my pc. How can I solve this problem. I applied to steps in tensorflow website for this installation. Help me.GPU Type:  Nvidia RTX 3060
Nvidia Driver Version: 531.18
CUDA Version: 11.2.2
CUDNN Version: 8.1.0.77
Operating System + Version: Wİndows 11 Pro 22H2
Python Version (if applicable):  3.9.13
TensorFlow Version (if applicable): 2.11.0Hi,
We recommend you to check the below samples links in case of tf-trt integration issues.
https://docs.nvidia.com/deeplearning/frameworks/tf-trt-user-guide/index.html#samplesThis NVIDIA TensorRT 8.5.3 Quick Start Guide is a starting point for developers who want to try out TensorRT SDK; specifically, this document demonstrates how to quickly construct an application to run inference on a TensorRT engine.https://docs.nvidia.com/deeplearning/frameworks/tf-trt-user-guide/index.html#integrate-ovr
https://docs.nvidia.com/deeplearning/frameworks/tf-trt-user-guide/index.html#usingtftrtIf issue persist, We recommend you to reach out to Tensorflow forum.
Thanks!Powered by Discourse, best viewed with JavaScript enabled"
530,fp32-with-tf32-precision,"I’m using PyTorch with V100 GPU. As this GPU doesn’t support operations in TF32, I’m adjusting my x (input to the prediction model) and y (ground truth) tensors that are in FP32 to have 10-bit precision in the decimal places, the same way TF32 is represented, just using, for example, x = torch.round(x, decimals=4) (I’m using 4 decimal places following instructions from this site - FP64, FP32, FP16, BFLOAT16, TF32, and other members of the ZOO | by Grigory Sapunov | Medium, in the TF32 section). Would this rounding be enough for me to make the FP32 very close to what the TF32 would be? Doing that (considering that the way I’m doing is correct), should I also reduce model precision by performing model.half()? I’m making these adjustments because for some reason my model converges well on Ampere GPU (RTX A4000) but the same does not happen with Volta GPU (V100). I’m guessing it’s because I no longer use TF32 in the operations.
Thanks in advance.Powered by Discourse, best viewed with JavaScript enabled"
531,triton-inference-server,"Hi,I am running a Video analytics pipeline, where multiple cameras are sending requests to the Yolox_s model. It is using tensorrt c++ for getting the inference currently. I want to adapt to triton.It is a pytorch model. I want to deploy this model as tensorrt model, hence inside the NGC Container first I will be converting the onnx model to tensorrt engine file and configure config.pbtxt file where I can specify how many instances of the model we can initiate.How to route the client request to the model instances. Does it automatically get referred to the instance or we need to define it while sending requestsAlso, please suggest some reference for sending client side request in c++.Hi,Please refer to the following documentation.Triton Inference Server has 31 repositories available. Follow their code on GitHub.The Triton Inference Server provides an optimized cloud and edge inferencing solution.  - GitHub - triton-inference-server/server: The Triton Inference Server provides an optimized cloud and edge i...Triton Python, C++ and Java client libraries, and GRPC-generated client examples for go, java and scala. - GitHub - triton-inference-server/client: Triton Python, C++ and Java client libraries, and...Thank you.Powered by Discourse, best viewed with JavaScript enabled"
532,is-it-accurate-to-say-that-data-scientists-will-mainly-generate-insights-to-the-business,"Hi, I’ve seen few blog posts and posts in other forums out there talking about the difference between data scientists and machine learning engineers. As I understand, many of the skills necessary for doing both roles are very similar, and the actual difference between the skillset being very nuanced.Comment below if you consider yourself a data scientist, but you are also pushing fully featured APIs/applications to production. I’m pretty sure that people make the distinction I mentioned above just because most of the tools in the space are not very mature yet, and to push things to production there’s a lot of manual engineering work that’s still necessary. Any level of detail on your day-to-day would be appreciated.Powered by Discourse, best viewed with JavaScript enabled"
533,riva-skills-quickstart-error-in-jupyterlabs-4-0-quick-start-deploy-an-ootb-asr-pipeline-with-riva,"Received this error when doing the NGC ASR training from my home laptop:Downloading the Riva Skills Quick Start resource folder
{
“error”: “Invalid org - Org cannot be set if Authenticated with API Key or Starfleet.”
}
sed: can’t read riva_quickstart_v2.11.0/riva_init.sh: No such file or directoryThe three previous ipynb scripts ran without error.   I have valid NGC account setups and a valid api key.  The third script did the NGC CLI setups for using my new api key.   The  ‘ngc config current’  JSON shows the key value ‘org’  set to ‘nvidia’ .   All the python code is being run from the notebook which invokes the ubuntu cli.HI @rayvakThanks for your interest in RivaQuick Check- Did you receive an email after your new api key gen and did you accept itEmail would have “Welcome to NVIDIA NGC”Can you check it, accept and let me know if it worksThanksPowered by Discourse, best viewed with JavaScript enabled"
534,warning-freeimage-is-not-set-up-correctly-please-ensure-freeimage-is-set-up-correctly,"Hello!
Test fails from NVIDIA cuDNN Documentation Installation Guide :: NVIDIA cuDNN Documentation What could be the reason, can anyone help? Ubuntu 22.04.

Снимок экрана 2023-05-22 в 16.49.551275×813 141 KB
Hi @rusmartcom ,
Below are the possibilities we would like you to check upon.
1- Verify CUDA installation.
2 - seems like maybe either you don’t have the cuda toolkit installed, or it’s in a non-standard location.Can you please confirm on the same.ThanksPowered by Discourse, best viewed with JavaScript enabled"
535,tensorrt-same-yolo-model-with-different-weights,"I want to run same YOLOv3 model with different weights for specialized audience on Jetson Xavier NX using TensorRT 8.4.1.5. How should I approach this for the most optimized way?For example I want to load model and I want real-time images to being inferenced by weights A when the robot is moving > 5m/s and being inferenced by weights B when the robot is moving < 5m/s. So loading and unloading model is not an option because switch has to be quick.TensorRT Version: 8.4.1.5
GPU Type: Jetson Xavier NX
Nvidia Driver Version: Jetpack Release 35.1.0Hi,I hope the following approach will help you.Create two separate TensorRT engines for the Yolo V3 model, one for weights A and another for weights B. You can use the TensorRT C++ API, Python API, or trtexec to create these engines. A variety of settings can be selected through the engine creation process, such as the input and output model shapes, precision, and optimization parameters.Load both engines into memory. Switch to engine A if the robot is moving faster than 5 m/s. Otherwise, switch to engine B. Then perform the inference based on the robot’s speed of movement.Thank you.Powered by Discourse, best viewed with JavaScript enabled"
536,tensorrt-incorrect-predictions-for-segmentation,"I am trying to do a segmentation model tensorrt inference. It is giving incorrect results.
(tensorrt version: ‘8.4.1.5’)
tensorrt_result804×415 13.6 KB
Attached the necessary files to reproduce the issue.
cmap_cityscapes.npy (356 Bytes)
model.onnx (11.5 MB)
model_fp32.engine (12.5 MB)
Nvidia_TensorRT.ipynb (48.3 KB)

sample_input1024×512 41.7 KB
Hi @dheerajmadda ,
Apologies for the miss,
I am trying to reproduce the issue from my end, Will update you soon.ThanksAny update on this please? The tensorRT is producing zeros as outputs.Powered by Discourse, best viewed with JavaScript enabled"
537,riva-waiting-for-triton-server-to-load-all-models-retrying-in-1-second,"Please provide the following information when requesting support.Hardware - GPU RTX 3060
Hardware - CPU
Operating System
Riva Version 2.9.0I run this command
riva-build speech_recognition -f 
/servicemaker-dev/asr_bn.rmir /servicemaker-dev/conformer_ctc_med_bn1.riva 
–offline 
–name=conformer-bn-BD-asr-offline 
–featurizer.use_utterance_norm_params=False 
–featurizer.precalc_norm_time_steps=0 
–featurizer.precalc_norm_params=False 
–ms_per_timestep=40 
–endpointing.start_history=200 
–endpointing.residue_blanks_at_start=-2 
–nn.fp16_needs_obey_precision_pass 
–chunk_size=4.8 
–left_padding_size=1.6 
–right_padding_size=1.6 
–max_batch_size=16 
–featurizer.max_batch_size=512 
–featurizer.max_execution_batch_size=512 
–decoder_type=flashlight 
–decoding_language_model_binary=/servicemaker-dev/v4_lm_4gram_p0011.binary 
–decoding_vocab=/servicemaker-dev/bn_lexicon.txt 
–flashlight_decoder.lm_weight=0.2 
–flashlight_decoder.word_insertion_score=0.2 
–flashlight_decoder.beam_threshold=20. 
–language_code=bn-BD && riva-deploy -f /servicemaker-dev/asr_bn.rmir /data/modelsafter deployment when i tried to access that deployed model to transcribe it throws this errorriva_1                |   > Riva waiting for Triton server to load all models…retrying in 1 second
riva_1                |   > Riva waiting for Triton server to load all models…retrying in 1 second
riva_1                |   > Riva waiting for Triton server to load all models…retrying in 1 second
riva_1                |   > Riva waiting for Triton server to load all models…retrying in 1 second
riva_1                |   > Riva waiting for Triton server to load all models…retrying in 1 second
riva_1                |   > Riva waiting for Triton server to load all models…retrying in 1 second
riva_1                |   > Riva waiting for Triton server to load all models…retrying in 1 second
riva_1                |   > Riva waiting for Triton server to load all models…retrying in 1 second
riva_1                |   > Riva waiting for Triton server to load all models…retrying in 1 second
riva_1                |   > Riva waiting for Triton server to load all models…retrying in 1 second
riva_1                |   > Riva waiting for Triton server to load all models…retrying in 1 second
riva_1                |   > Riva waiting for Triton server to load all models…retrying in 1 second
riva_1                |   > Riva waiting for Triton server to load all models…retrying in 1 second
riva_1                |   > Riva waiting for Triton server to load all models…retrying in 1 second
riva_1                |   > Riva waiting for Triton server to load all models…retrying in 1 secondand thenriva_1                | [Trie] Trie label number reached limit: 6
riva_1                | [Trie] Trie label number reached limit: 6
riva_1                | [Trie] Trie label number reached limit: 6
riva_1                | [Trie] Trie label number reached limit: 6
riva_1                | [Trie] Trie label number reached limit: 6
riva_1                | [Trie] Trie label number reached limit: 6
riva_1                | [Trie] Trie label number reached limit: 6
riva_1                | [Trie] Trie label number reached limit: 6
riva_1                | [Trie] Trie label number reached limit: 6
riva_1                | [Trie] Trie label number reached limit: 6
riva_1                | [Trie] Trie label number reached limit: 6
riva_1                | [Trie] Trie label number reached limit: 6
riva_1                | [Trie] Trie label number reached limit: 6
riva_1                | [Trie] Trie label number reached limit: 6
riva_1                | [Trie] Trie label number reached limit: 6
riva_1                | [Trie] Trie label number reached limit: 6
riva_1                | [Trie] Trie label number reached limit: 6
riva_1                | [Trie] Trie label number reached limit: 6
riva_1                | [Trie] Trie label number reached limit: 6
riva_1                | [Trie] Trie label number reached limit: 6
riva_1                | [Trie] Trie label number reached limit: 6
riva_1                | [Trie] Trie label number reached limit: 6
riva_1                | [Trie] Trie label number reached limit: 6
riva_1                | [Trie] Trie label number reached limit: 6
riva_1                | [Trie] Trie label number reached limit: 6
riva_1                | [Trie] Trie label number reached limit: 6
riva_1                | [Trie] Trie label number reached limit: 6
riva_1                | [Trie] Trie label number reached limit: 6
riva_1                | [Trie] Trie label number reached limit: 6
riva_1                | [Trie] Trie label number reached limit: 6
riva_1                | [Trie] Trie label number reached limit: 6
riva_1                | [Trie] Trie label number reached limit: 6
riva_1                | [Trie] Trie label number reached limit: 6
riva_1                | [Trie] Trie label number reached limit: 6
riva_1                | [Trie] Trie label number reached limit: 6
riva_1                | [Trie] Trie label number reached limit: 6
riva_1                | [Trie] Trie label number reached limit: 6
riva_1                | [Trie] Trie label number reached limit: 6how to resolve it?Hi @shihab2Thanks for your interest in RivaApologies you are facing issueRequest to kindly shareThanksWell, the issue has been solved. I changed my lexicon.txt file and it worked for me.Powered by Discourse, best viewed with JavaScript enabled"
538,unexpected-opengl-compute-shader-difference-from-cpu-implementation-of-neural-network,"The full code for this project is available at https://github.com/echoline/fann. I am using two OpenGL compute shaders to run neural networks:and train neural networks:local_size_x and the threads variable are set to the x value of GL_MAX_COMPUTE_WORK_GROUP_SIZESSBO Shader Storage Buffer Objects are allocated only once, then used over and over by both compute shaders:the glDispatchCompute calls use only one workgroup:I have written a simple test program:When training the networks, it works the same as the CPU implementation until the network size gets larger and larger. when it is larger than about 5 layers with 1024 nodes per layer, the results start to be different than the CPU after about the third training iteration.it works fine with networks of 5 layers with 1000 neurons apiece:but these are the results with 5 layers of 2000 neurons apiece:My question is, why does the GPU start giving such different results with the larger neural networks after the third training iteration? It must not be a floating point calculation difference because the results are about the same until the network is large enough. It only starts going wrong after a few iterations.Powered by Discourse, best viewed with JavaScript enabled"
539,missing-words-in-citrinet-pretrained-model,"Hello all,I have been trying citrinet model with a language model to improve the decoding for certain words. But with the use of the language model, I am finding that citrinet is skipping some words completely. For example, if I speak “I want to open a bank account”, the output is “I want to account”.I am pasting a reference link here of another person having similar issues hereWhat is the solution for this problem? Any suggestions on what can be done here?Powered by Discourse, best viewed with JavaScript enabled"
540,install-tensorrt,"Hi all,I am using jetson TX1 with jetpack 4.4 and python 3.6.9
I cannot install tensorrt in python in order to test a neural network. It was not possible to install it using pip and I want To know:
1- Which tensorrt version should I install in this environment
2- How can I install tensorrt instead of using pipThank you very muchHi @mahdi041196 ,
Please follow the Installation Guide to set up TRT.
We always recommend to use the latest TRT release for improved performance.
However to get better understanding with the specific environment details, you can reach out to the Jetson forum.
ThanksPowered by Discourse, best viewed with JavaScript enabled"
541,how-to-support-dynamic-batch-size-for-tensorrt-engine,"In the doc, it’s said that EXPLICIT_BATCH must be specified for onnx exported network. So, is there any way to support dynamic batch size for engine built by this network?For example, I build this engine with batch size 16. Now I want to run it in Python with batch size 8. Is there any method?Hi,
Request you to share the ONNX model and the script if not shared already so that we can assist you better.
Alongside you can try few things:This NVIDIA TensorRT 8.5.3 Quick Start Guide is a starting point for developers who want to try out TensorRT SDK; specifically, this document demonstrates how to quickly construct an application to run inference on a TensorRT engine.import sys
import onnx
filename = yourONNXmodel
model = onnx.load(filename)
onnx.checker.check_model(model).
2) Try running your model with trtexec command.master/samples/trtexecNVIDIA® TensorRT™, an SDK for high-performance deep learning inference, includes a deep learning inference optimizer and runtime that delivers low latency and high throughput for inference applicat...
In case you are still facing issue, request you to share the trtexec “”–verbose"""" log for further debugging
Thanks!Powered by Discourse, best viewed with JavaScript enabled"
542,segment-fault-due-to-strlen-avx2-s-missing,"When I try to convert ONNX modal to TensorRT Engine with TensorRT 8.0.3.4, a segment fault would be reported.TensorRT Version:  8.0.3.4
GPU Type:  RTX 3070
Nvidia Driver Version: 465.19.01
CUDA Version: 11.3
CUDNN Version: 8.2.1 but complained that 8.1.1 is used
Operating System + Version: Ubuntu 20.04
Python Version (if applicable):
TensorFlow Version (if applicable):
PyTorch Version (if applicable): 1.7
Baremetal or Container (if container which image + tag):(ONNX Model)[share/model.onnx at master · lannyyip/share · GitHub]: share/model.onnx at master · lannyyip/share · GitHubPlease give me a hand on it. Thank you.LannyHi @lannyyip1,We could reproduce similar error using trtexec command, Please allow us sometime to get back on this.Thank you.Hi,Looks like we are facing different error when we tried trtexec command. Is input size of the Resize_151 along axis 3 zero in your ONNX model ?Error:&&&& RUNNING TensorRT.trtexec [TensorRT v8001] # /usr/src/tensorrt/bin/trtexec --onnx=/home/my_data/model_189181.onnx --verbose[09/15/2021-16:28:49] [E] Error[2]: [graphShapeAnalyzer.cpp::throwIfError::1306] Error Code 2: Internal Error (Resize_151: IResizeLayer requires that if input dimension is zero, output dimension must be zero too (axis = 3 input dimension = 0 output dimension = 1)
)
[09/15/2021-16:28:49] [E] Error[2]: [builder.cpp::buildSerializedNetwork::417] Error Code 2: Internal Error (Assertion enginePtr != nullptr failed.)Thank you.Thank you for your response.
Please add options “–optShapes=input0:1x1x1024x500” as there is some restriction when I converted model to onnx.
I run following cmd, there is no segment fault shown:But when I added --fp16 argument, as following show, the segment fault shows up.I used gdb to run, confirm that it is the same segment fault as mention in my first post.
It seems related to fp16 option.LannyHi,When we tried following command could successfully build engine.
./trtexec --onnx=model.onnx --optShapes=input0:1x1x1024x500 --fp16 --verboseCould you please share us complete error verbose logs of trtexec command.Thank you.Have you solved it? I’m facing the same problem, thanks a lot for any advice.Powered by Discourse, best viewed with JavaScript enabled"
543,kernel-launch-takes-too-much-time-in-deep-learning-model-training-and-much-time-gpus-are-idling,"When training a deep learning model in a parallel multi-gpu environment, say e.g. 4xA100 DGX, profiled and visualized in tensorboard, have found large part of time is spent in kernel launch, see kernel launch takes too much time 1(some steps in the middle of training) and kernel launch takes too much time 2(the overall training).
And most of the time gpus are idling, see gpus are idling.
Could anyone please inform of what features or optimization options could be enabled on the CUDA platform to reduce the delay caused by kernel launch and low gpu utilization rate? And is it possible that this problem is related only to some specific kernels?Powered by Discourse, best viewed with JavaScript enabled"
544,can-an-engine-have-multiple-inputs-and-outputs-icudaengine-getnbiotensors,"Sorry if this is a silly question. I’m  scratching the surface of AI and I’ve  started exploring the TensorRT C++ API to get my hands dirty. When I deserialize an engine using nvinfer1::IRuntime::deserializeCudaEngine() I can ask the deserialized engine the number of IO tensors using getNbIOTensors(). I understand that that the batch size can be larger than one, but that’s not what getNbIOTensors() returns. With my current underderstanding you can feed multiple batches during inference.What does it mean when the number of input tensors is larger than one?
What does it mean when the number of  output tensors is larger than one?Hi,The below links might be useful for you.This Best Practices Guide covers various performance considerations related to deploying networks using TensorRT 8.0.3. These sections assume that you have a model that is working at an appropriate level of accuracy and that you are able to...The programming guide to the CUDA model and interface.https://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__STREAM.htmlFor multi-threading/streaming, will suggest you to use Deepstream or TRITONFor more details, we recommend you raise the query in Deepstream forum.orraise the query in Triton Inference Server Github instance issues section.Triton Inference Server has 30 repositories available. Follow their code on GitHub.Thanks!Hi @AakankshaS was this answer for me? Although the links are good to get a general understanding, how do they answer my question? My question wasn’t that general I hope :)What does it mean when the number of input tensors is larger than one?
What does it mean when the number of output tensors is larger than one?The number of input tensors in a TensorRT engine refers to the number of tensors that are required to be passed to the engine in order to perform inference. If the number of input tensors is larger than one, it means that the engine can be used to perform inference on multiple inputs at the same time. When performing inference, you need to provide data for each of these input tensors separately.The number of output tensors refers to the number of tensors that are produced by the engine after inference is performed. If the number of output tensors is larger than one, it means that the engine can produce multiple outputs after inference is performed. The model will generate multiple sets of outputs, each corresponding to a different aspect of the model’s predictions or computations.The batch size is a separate concept from the number of input or output tensors. The batch size refers to the number of inputs or outputs that are processed by the engine at the same time. For example, if the batch size is 2, then the engine will process two inputs or outputs at the same time.Powered by Discourse, best viewed with JavaScript enabled"
545,minimum-gpu-to-train-yolov5-extra-large-version-6-model-with-4k-images,"HiI’m in a project that requires training a yolov5 model with 4K images. I have a RTX 3060 12 GB and I’m getting a cuda out of memory error. I’m wondering since the model is very heavy and the images are very large what would be the best or minimum GPU to use for training? Also I’ll need to convert the model (pytorch) into onnx. Then It should be deployed d using cudnn. The idea is to train the yolov5 in a machine and make the inference in another machine. What would be the best gpu for inference with 4K images? I mean I’ll need to have 2 GPUs one for training and other for inference. I just want the recommended GPUs for both cases.ThanksHi @p.carvalho ,Can you please share the detailed api logs with us?ThanksPowered by Discourse, best viewed with JavaScript enabled"
546,trtexec-multi-source-streams-and-multi-batch-performance-test-failed,"I want to test the performance of the model in multiple sources (streams) and batches with the trtexec command, and I test it with the following command/usr/src/tensorrt/bin/trtexec --loadEngine=yolov7_b16_int8_qat_640.engine --shapes=images:4x3x640x640 --streams=4ps:
The .engine model source is converted by the following command(dynamic batch)but the following error occurs.TensorRT Version : 8.5.2
GPU Type : J etson AGX Xavier
Nvidia Driver Version :
CUDA Version : 11.4.315
CUDNN Version : 8.6.0.166
Operating System + Version : 35.2.1 ( Jetpack: 5.1)
Python Version (if applicable) : Python 3.8.10
TensorFlow Version (if applicable) :
PyTorch Version (if applicable) : 1.12.0a0+2c916ef.nv22.3
Baremetal or Container (if container which image + tag) :Hi,Could you please try the latest TensorRT version 8.6 and let us know if you still face the same issue?
Please share with us the repro ONNX model to try from our end.For easy setup, you can also use the TensorRT NGC container.NVIDIA TensorRT is a C++ library that facilitates high-performance inference on NVIDIA graphics processing units (GPUs). TensorRT takes a trained network and produces a highly optimized runtime engine that performs inference for that network.Hi,@ spolisettyI tried “nvcr.io/nvidia/tensorrt:23.05-py3”
got  tensorrt-dev version is 8.6.1.2-1+cuda12.0I get an error message when running the trtexec commandMy platform is  ‘AGX Xavier’, shoud i use other version?

image1007×247 11.9 KB
Which version of CUDA are you using? Please refer to the container release notes and make sure the driver version requirements are satisfied.The TensorRT container is an easy to use container for TensorRT development. The container allows you to build, modify, and execute TensorRT samples. These release notes provide a list of key features, packaged software in the container, software...Thank you.CUDA Version : 11.4.315My cuda version should not support TensorRT 8.6 or higherPowered by Discourse, best viewed with JavaScript enabled"
547,cudnn-installation-error-in-miniconda,"Hi, I need to install cudnn on my miniconda3 environment but I keep getting (ERROR: Could not find a version that satisfies the requirement nvidia-cudnn-cu11==8.9.2.26 (from versions: 0.0.1.dev5)
ERROR: No matching distribution found for nvidia-cudnn-cu11==8.9.2.26)
according to Nvidia’s document, this cudnn version is compatible with my system and I even downloaded this archive from Nvidia’s website. Still, honestly, I don’t know what to do with it.
could somebody please help me I need this setup and I’m a complete nooby in codingHi @sam.myc.1la ,
Are you trying to set this up with tensorflow?
If yes, please make sure for tf version above 2.10, you are installing TensorFlow in WSL2, or tensorflow-cpu .
ThanksPowered by Discourse, best viewed with JavaScript enabled"
548,whats-the-meaning-of-word-timestamp-printed-by-riva-streaming-asr-client-py,"Hardware - GPU (T4)
Hardware - CPU
Operating System
Riva Version 2.9.0
TLT Version (if relevant)I modified the variable interim_results as false in the script: python-clients/scripts/asr/riva_streaming_asr_client.py. and placed a audio file named nlp.wav under the folder data/examples, whose duration is one hour, and  then run the command: cd python-clients; python scripts/asr/riva_streaming_asr_client.py --input-file data/examples/nlp.wav, and find the last timestamp printed is 112.24s, but it should be a value around 3600s, right? What’s the meaning of Time printed here?The following is the last few lines printed by the script:
Time 111.34s: Transcript 0: sub sample of some words where it works incredibly well it’s also true that when you really play around with it for a while you’ll find some things where like oh liked minus german goes to some crazy sushi term or something it doesn’t always make sense but there are a lot of them where it really is surprisingly Intuitive and so people essentially then came up with
Time 111.40s: Transcript 0: a dataset to try to see how often does it really appear
Time 111.43s: Transcript 0: and does it really
Time 111.63s: Transcript 0: Work this well and so they basically collected this word vector analogies task and these are some examples you can download all of them on this link here this is the again the original word paper that discovered
Time 111.82s: Transcript 0: and described these linear relationships and they basically look at chicago and Illinois and houston Texas and you can basically come up with a lot of different analogies where you know the city appears in that state
Time 112.13s: Transcript 0: of course there are some problems and you know as you optimize this metric more and more you will observe like oh well maybe that city name actually appears you know multiple different cities and different states have the same name and then it kind of depends on your corpus that you’re training on on whether this is being captured or not but still a lot of people
Time 112.24s: Transcript 0: it makes a lot of sense for most of them to optimize this at least for a little bit here are some other examples ofHi @174362510Thanks for your interest in RivaI will check with the internal team and let you knowThanksHello @rvinobha
Excuse me, is there any update to this question?Hi @174362510i have inputs form the teamTime printed here is the wall clock time elapsed since start of inference. See python-clients/asr.py at main · nvidia-riva/python-clients · GitHubIf you need word timestamps, --word-time-offsets flag should be passed.Thanks–word-time-offsetsokay, thank you very much @rvinobhaThis topic was automatically closed 14 days after the last reply. New replies are no longer allowed.Powered by Discourse, best viewed with JavaScript enabled"
549,transcribe-file-offline-py-gives-error-channel-count-must-be-1,"When I run the transcribe_file_offline.py on a WAV file I get the error:**Error: channel count must be 1**.The WAV file is as follows:Hi @tenjeeukThanks for your interest in RivaWe currently do no support audio file has more than a single channelRequest to kindly to convert the audio file to mono channel, ie in tool/converter for Channels option: please use mono as opposed to stereo etc,We recommend to record audio files with single/mono channel at first place, if not possible we can convert using some online audio format converter availableThanksPowered by Discourse, best viewed with JavaScript enabled"
550,gstreamer-plugin-using-maxine-sdk,"Hi,
recently i wrote a GStreamer plugin to use Maxine SDK in a generic pipeline. Now i want to publish it under LGPL license as advised here https://gstreamer.freedesktop.org/documentation/plugin-development/appendix/licensing-advisory.html?gi-language=c.
Are there any problems with Maxine SDK license?Hey @voidmainvoid95It is great to see your effort in this direction. We will get back to you soon with information about the same!@voidmainvoid95 Could you share for which SDK (Video Effects, Audio Effects, Augmented Reality) did you design the plugin? Do you have any specific targeted OS? Which feature plugins did you design/test? (AIGS, Super Resolution, Background Noise Reduction, etc)?Hi @tvarshney ,
At the moment the plugin implement each effect present in Video Effects SDK under Linux.
The desired effect can be selected using plugin prop and can be combined with others in the same pipeline.  I’ve also implemented a custom meta to pass foreground mask from GreenScreen to BackgroundBlur or other future effects involving the mask.At the moment the plugin implement each effect present in Video Effects SDK under Linux.
The desired effect can be selected using plugin prop and can be combined with others in the same pipeline. I’ve also implemented a custom meta to pass foreground mask from GreenScreen to BackgroundBlur or other future effects involving the m@tvarshney  Is the plugin published?@wawacry Yes, you can find it here!Powered by Discourse, best viewed with JavaScript enabled"
551,running-cuopt-on-wsl2,"I’ve been trying to deploy cuOpt container (cuopt:22.12) on WSL2/Ubuntu, but it doesn’t seem to work. However, it seems that GPU support on WSL2 is working as expected as I’ve tried running other containers using gpu, for instance:sudo docker run -it --gpus=all --rm nvcr.io/nvidia/k8s/cuda-sample:nbody nbody -benchmarkWindowed mode
Simulation data stored in video memory
Single precision floating point simulation
1 Devices used for simulation
GPU Device 0: “Pascal” with compute capability 6.1Compute 6.1 CUDA device: [NVIDIA GeForce GTX 1050 Ti]
6144 bodies, total time for 10 iterations: 5.727 ms
= 65.911 billion interactions per second
= 1318.219 single-precision GFLOP/s at 20 flops per interactionWhen I’m trying to deploy cuopt, docker just hangs without any feedback/error and container is not created. I’ve been using following parameters:docker run -it --gpus all --rm --network=host  /bin/bashEnvironment:
Windows 10 Enterprise 22H2
WSL2, Ubuntu-22.04
Docker 23.0.1, installed dicrectly on Ubuntu (no Docker Desktop)
NVIDIA GeForce GTX 1050 Ti, Driver Version: 531.18, CUDA Version: 12.1As of now cuOpt supports cuda 11.2 - 11.8, this could be an issue with that.And also if possible can you please try running with with docker in debug mode and share the logs on what happens ? How to Run Docker in Verbose Mode?Powered by Discourse, best viewed with JavaScript enabled"
552,accuracy-issues-with-riva-asr,"Hardware - AGX Orin
Hardware - CPU
Operating System：Ubuntu 20.04
Riva Version ：2.8.1Currently, when using a Mandarin model with a microphone, the following issues may arise:Thank you for your help.HI @u9713112Thanks for your interest in Riva,Kindly request to provide more informationPoor accuracy in single-person daily conversations - Can you record and share the voice samples used anlong with accuracy valuesUnsatisfactory results in multi-person conversations for speech recognition. -  Can you record and share the voice sample and provide more details on what is unsatisfactory in the resultsPlease find the word boosting exampleriva_streaming_asr_client --audio_file=audio_file.wav --boosted_words_file=boosted_words_file.txt --boosted_words_score=100. --interim_results=falseboosted_words_file.txt should contain the list of words that need to be boosted (one word on each line)Powered by Discourse, best viewed with JavaScript enabled"
553,what-is-the-fastest-way-of-spinning-up-an-inference-session,"Hi,I was wondering what is the fastest way to spin up a TensorRT inference session that remains unchanged in-between sessions. Currently, I start my inference sessions from a serialised .trt file with something likeHowever, this takes around ~3s on a Jetson TX2. Is there any way to speed up this process? A way “save and restore” a session? Pickle the state in some way? Save and reload the deserialised engine?Hi,Request you to share the model, script, profiler, and performance output if not shared already so that we can help you better.Alternatively, you can try running your model with trtexec command.master/samples/trtexecNVIDIA® TensorRT™, an SDK for high-performance deep learning inference, includes a deep learning inference optimizer and runtime that delivers low latency and high throughput for inference applicat...While measuring the model performance, make sure you consider the latency and throughput of the network inference, excluding the data pre and post-processing overhead.
Please refer to the below links for more details:This Best Practices Guide covers various performance considerations related to deploying networks using TensorRT 8.0.3. These sections assume that you have a model that is working at an appropriate level of accuracy and that you are able to...This Best Practices Guide covers various performance considerations related to deploying networks using TensorRT 8.0.3. These sections assume that you have a model that is working at an appropriate level of accuracy and that you are able to...Thanks!Powered by Discourse, best viewed with JavaScript enabled"
554,should-i-really-start-computer-vision-with-jetpack-for-learning-opencv,"My question is Im really going to deeep with tensorflow with Computer vision  so  Should I really start from jetpack to deeplearning with tensorflow ?
A clear and concise description of the bug or issue.TensorRT Version:
GPU Type:
Nvidia Driver Version:
CUDA Version:
CUDNN Version:
Operating System + Version:
Python Version (if applicable):
TensorFlow Version (if applicable):
PyTorch Version (if applicable):
Baremetal or Container (if container which image + tag):Please attach or include links to any models, data, files, or scripts necessary to reproduce your issue. (Github repo, Google Drive, Dropbox, etc.)Please include:Hi,This looks like a Jetson issue. Please refer to the samples below.Hello AI World guide to deploying deep-learning inference networks and deep vision primitives with TensorRT and NVIDIA Jetson. - GitHub - dusty-nv/jetson-inference: Hello AI World guide to deployin...An easy to use PyTorch to TensorRT converter. Contribute to NVIDIA-AI-IOT/torch2trt development by creating an account on GitHub.For any further assistance, please reach out to the Jetson related forum.Thanks!Powered by Discourse, best viewed with JavaScript enabled"
555,how-to-allocate-specific-resource-when-running-profile-engine-model,"I have 2 engine models and I want to compare inference time of the 2 engine models from profiling. But I recognize that results of profiling runs are different with large fluctuation. So I want to allocate specific resource for each run. Is there any way to do it? If no, so it is difficult to compare 2 engine models by using profiling.Hi,Sorry for the delayed response.
You can customize “CUDA_VISIBLE_DEVICES” for GPUs.
Example: You can make single GPU visible to the TensorRT NGC container and try profiling multiple times.Thank you.@spolisetty
Thanks.You can customize “CUDA_VISIBLE_DEVICES” for GPUs.Yes, I know this option. I mean that is there any way to allocate RAM, no. CPU core when profiling.Powered by Discourse, best viewed with JavaScript enabled"
556,enable-dynamic-ai-for-kiosks,"Hi guys,I am looking to develop dynamic AI interaction for kiosks and enable voice ordering for mobile apps and add characters to that.I am looking for developers to develop that for meHi @saif.gouraThanks for your interest in RivaWe can help regarding riva in this forumRiva Offersyou can let me know the areas of interest from aboveWe also have Audio2face which I guess may be helpful for your above use case (reference)
https://docs.omniverse.nvidia.com/app_audio2face/app_audio2face/overview.html
we have a forum too for itAudio2Face is a combination of AI based technologies that generates facial motion and lip sync that is derived entirely from an audio source.Let us know more brief details on your roadmapThanksHi,Thank you for getting back to me.Well now I am working on AI phone ordering call center for restaurants in Arabic and English so the AI will answer the call and take the order place the order and send it to the restaurants POS.Now what I also want to build and what’s on my road map is a big project but firstEnable voice ordering for multi restaurants food ordering apps and any app that would want to enable voice ordering ( for example imagine Uber eats app and we add the voice ordering app and customer can open the app and place orders and talk to the bot via the voice ordering.The exact same as number one but we want to add characters and make it dynamic ordering for example for Ubereats we do a character 3d or cartoon to customize it for Ubereats then customer can open the app and interact live with the character and place order and make it conversionalEnable dynamic live ordering for kiosks for restaurants and drive thruCall center for banks and hotelsAnd much moreHi @saif.gouraI pitched your points with the internal teamThey are interested to know about your company and aspirations
Can you share if possible details about your companyThanksPowered by Discourse, best viewed with JavaScript enabled"
557,instancenormalization-produces-nan-inf-on-tensorrt-by-my-model,"Hi NVidia Team,I converted the .onnx to TensorRT.
Converted engine generates nan,inf.
.onnx files do not generate these.After inspecting the output, I found that it is produced when running the InstanceNormalization op.
Is this a weight issue?I uploaded the .onnx that cuts out only the part that executes InstanceNormalization.Thank you.GPU Type: GeForce RTX 2060 SUPER
Nvidia Driver Version: 535.98
Baremetal or Container (if container which image + tag): nvcr.io/nvidia/pytorch:23.05-py3Contribute to sugi10fe/tensorrt-instancenormalization development by creating an account on GitHub.Hi,
Request you to share the ONNX model and the script if not shared already so that we can assist you better.
Alongside you can try few things:This NVIDIA TensorRT 8.6.1 Quick Start Guide is a starting point for developers who want to try out TensorRT SDK; specifically, this document demonstrates how to quickly construct an application to run inference on a TensorRT engine.import sys
import onnx
filename = yourONNXmodel
model = onnx.load(filename)
onnx.checker.check_model(model).
2) Try running your model with trtexec command.master/samples/trtexecNVIDIA® TensorRT™, an SDK for high-performance deep learning inference, includes a deep learning inference optimizer and runtime that delivers low latency and high throughput for inference applicat...
In case you are still facing issue, request you to share the trtexec “”–verbose"""" log for further debugging
Thanks!Hello.I shared .onnx in relevant files repo:Test scripts is also contains in this repo.
Please check these.Thank you.I checked repro.onnx. It produces error.Certainly TerrorRT itself supports up to 7 opsets.
On the other hand, it was stated that onnx_tensorrt supports up to 17, so if the model is converted by onnx_tensorrt, there seems to be no problem.
The imported onnx is the one included in the container.I also tried running trtexec with --verbose, but I don’t see anything suspicious, except the output.
verbose.log (2.0 MB)I hope the issue is resolved.
Thank you.Hello.I tried to implement repro.onnx’s InstanceNormalization by other ops.Converted model is here.So, I guess, InstanceNormalization_TRT plugin has issue of implements.
According to the onnx-tensorrt repo, it is used on InstanceNormalization parsing.
Maybe it has something to do with having two versions of InstanceNormalization_TRT loaded.
You can see this in the verbose.log uploaded in the previous post.Hope it helps you solve it.
Thank you.Hi,Sorry for the delayed response.
Are you still facing the issueHello.I still get around this with a workaround that replaces InstanceNormalization.
Since the number of nodes on onnx will increase, if possible, I am looking for a method that does not produce nan or inf while using InstanceNormalization.Thank you.Hi,We could reproduce similar behavior. Please allow us some to work on a fix.Thank you.Hi,As a workaround, we recommend that you please use the Native instance norm.
Please use --onnx-flags native_instancenorm in the Polygraphy command.The following may also be helpful to you:This is the revision history of the NVIDIA TensorRT 8.6 Developer Guide.Thank you.Powered by Discourse, best viewed with JavaScript enabled"
558,nvinfer-parameter-threshold-pre-cluster-threshold-post-cluster-threshold,"In nvinfer, there are three parameters：threshold pre-cluster-threshold Post-cluster-threshold . I understand that preClassPrecluster before clusting (such as cluster mode=2: nms),  Post-cluster-threshold  after clusting. For example,  the bbox confidence in yolov5 lesser than pre-cluster-threshold would be rejected, then Comprehensive probability = the bbox confidence * the class probability. the comprehensive probability lesser than Post-cluster-threshold  would be rejected. So what is this parameter threshold used for? Does it overlap with Post-cluster-threshold ?Powered by Discourse, best viewed with JavaScript enabled"
559,riva-tts-with-arabic,"Hello
I already installed and configured RIVA ASR with the Arabic model, and now trying to make the same thing with Riva TTS, is there a way to have  Riva TTS with Arabic?ThanksHi @ahmed.nawasrahThanks for your interest in Riva,Apologies, There is no Riva TTS with Arabic, Currently we only support en-US, de-DE, es for TTS in Mel-Spectrogram Generators in NemoThere are two main ways to load pretrained checkpoints in NeMo as described in Checkpoints. Using the restore_from() method to load a local checkpoint file (.nemo), or, Using the from_pretrained() ...ThanksPowered by Discourse, best viewed with JavaScript enabled"
560,how-can-build-plugin-efficientnmsplugin-on-tensorrt-7-1-3-jetpack-4-4-1,"Dear communityCould build efficientNMSPlugin on tensorRT 7.1.3 from sources?
I want to use this plugin for inference YOLO v7 bodel.Hi,
Please refer to below links related custom plugin implementation and sample:This Samples Support Guide provides an overview of all the supported NVIDIA TensorRT 8.6.1 samples included on GitHub and in the product package. The TensorRT samples specifically help in areas such as recommenders, machine comprehension, character...TensorRT is an SDK for high performance, deep learning inference. It includes a deep learning inference optimizer and a runtime that delivers low latency and high throughput for deep learning…While IPluginV2 and IPluginV2Ext interfaces are still supported for backward compatibility with TensorRT 5.1 and 6.0.x respectively, however, we recommend that you write new plugins or refactor existing ones to target the IPluginV2DynamicExt or IPluginV2IOExt interfaces instead.Thanks!Powered by Discourse, best viewed with JavaScript enabled"
561,export-tacotron2-to-onnx-nvidia-nemo,"Hello! I want to export Tacotron2 model from Nemo to ONNX model. Is it possible to export Tacotron2 model to onnx?Powered by Discourse, best viewed with JavaScript enabled"
562,onnx-model-and-tensorrt-engine-gives-different-output-for-parseq-model,"I converted parseq ocr model from pytorch to onnx and tested it on onnx model and every thing is ok, but when I convert onnx to fp32 or fp16 tensorrt engine, output of the model is very different from onnx model.
I use onnxsim to simplify onnx. if i dont use onnxsim all results are nan.model repo : GitHub - baudm/parseq: Scene Text Recognition with Permuted Autoregressive Sequence Models (ECCV 2022)TensorRT Version: TensorRT-8.6.1.6
GPU Type: RTX 3060
Nvidia Driver Version: 531.79
CUDA Version: cuda-12.0
CUDNN Version: cudnn-8.9.1.23_cuda12
Operating System + Version: win 10
Python Version: 3.8
Onnx opset: 14onnx model: test.onnx - Google Drivetrtexec.exe --onnx=parseq/test.onnx --workspace=10000 --saveEngine=parseq/test_fp32.trs --verbose
trt engine fp32: test_fp32.trt - Google Drive
trt engine fp32 log: test_fp32_log.txt - Google Drivetrtexec.exe --onnx=parseq/test.onnx --fp16 --workspace=10000 --saveEngine=parseq/test_fp16.trs --verbose
trt engine fp16: test_fp16.trt - Google Drive
trt engine fp16 log: test_fp16_log.txt - Google DriveI wrote a sample code to compare similarity of onnx and trt inference result. when I use real data, mean of similarity is 0.3 and when I use random number it is near 0.85sample code:Google Drive file.
sample real data:Google Drive file.I have same problem with VitStr based on timm vision transformerVitStr:masterPyTorch code of my ICDAR 2021 paper Vision Transformer for Fast and Efficient Scene Text Recognition (ViTSTR) - GitHub - roatienza/deep-text-recognition-benchmark: PyTorch code of my ICDAR 2021 pap...Vision transformer:It seems that the problem is vision transformer.Hi,
Request you to share the ONNX model and the script if not shared already so that we can assist you better.
Alongside you can try few things:This NVIDIA TensorRT 8.6.1 Quick Start Guide is a starting point for developers who want to try out TensorRT SDK; specifically, this document demonstrates how to quickly construct an application to run inference on a TensorRT engine.import sys
import onnx
filename = yourONNXmodel
model = onnx.load(filename)
onnx.checker.check_model(model).
2) Try running your model with trtexec command.//github.com/NVIDIA/TensorRT/tree/master/samples/trtexec
In case you are still facing issue, request you to share the trtexec “”–verbose"""" log for further debugging
Thanks!It is a bug in 8.6.1 version of tensorrt. and it will be fixed in the next release.## Description
I converted parseq ocr model from pytorch to onnx and tested it …on onnx model and every thing is ok, but when I convert onnx to fp32 or fp16 tensorrt engine, output of the model is very different from onnx model.
I use onnsim to simplify onnx. if i dont use onnxsim all results are nan.

model repo : https://github.com/baudm/parseq

<!--
  A clear and concise description of the issue.

  For example: I tried to run model ABC on GPU, but it fails with the error below (share a 2-3 line error log).
-->


## Environment



**TensorRT Version**: TensorRT-8.6.1.6

**NVIDIA GPU**: RTX 3060

**NVIDIA Driver Version**: 531.79

**CUDA Version**: cuda-12.0

**CUDNN Version**:cudnn-8.9.1.23_cuda12


Operating System: Win 10

Python Version: 3.8

PyTorch Version: 1.13

Onnx opset : 14


## Relevant Files
onnx model: https://drive.google.com/file/d/1CRXsD8Zk5Mo50JYCZytrAtBbFm2oOqvc/view?usp=sharing

trtexec.exe --onnx=parseq/test.onnx --workspace=10000 --saveEngine=parseq/test_fp32.trs --verbose
trt engine fp32: https://drive.google.com/file/d/17eecl4QrRrE1BiLqDE8HJT0wZCVm3BkB/view?usp=sharing
trt engine fp32 log: https://drive.google.com/file/d/1i9KkbKainaNIz5QQvolmScIu53DzFHHv/view?usp=sharing

trtexec.exe --onnx=parseq/test.onnx --fp16 --workspace=10000 --saveEngine=parseq/test_fp16.trs --verbose
trt engine fp16: https://drive.google.com/file/d/1CIzRZ-71a2hXZWnMNtWn7k2tuM3Pi6K_/view?usp=sharing
trt engine fp16 log: https://drive.google.com/file/d/15LOBtarM6RZiiyZaz66qt6Z8nu67JyrN/view?usp=sharing




## Steps To Reproduce
I wrote a sample code to compare similarity of onnx and trt inference result. when I use real data, mean of similarity is 0.3 and when I use random number it is near 0.85

sample code:
https://drive.google.com/file/d/1dLo9iD3ZUPVuvU6LNFnwQSCjcLDTiKQr/view?usp=sharing
sample real data:
https://drive.google.com/file/d/1VtQgOYw5ZYQSZmUOGyJ7xPKElC7caFMl/view?usp=sharingThis topic was automatically closed 14 days after the last reply. New replies are no longer allowed.Powered by Discourse, best viewed with JavaScript enabled"
563,gazeredirect-doesnt-work-on-specific-video-but-works-on-others,"So i have been able to use gazeredirect work on a 30fps 1080p but this video thats 24fps and custom resolution wont work. any ideas on how to fix? i tried to change the video to 30 fps and 1080p but that didnt work, so im not sure what is different about this video that it wont work. When it doesn’t work everything works like normally(no errors) but it doesn’t process as long and the file it creates can not be played.Powered by Discourse, best viewed with JavaScript enabled"
564,multiple-tensorrt-engine-contexts-for-different-models,"I want to have 5 models work together in a single stream.
Each model onnx file is separately loaded and parsed.Each of the model contexts is created and kept in an array.
For contexts at 2,3,4(0-based indexing), The results are fine for every batch.
But for contexts 0,1. The results are garbage.TensorRT Version:  8.0.1
GPU Type:  1050ti
Nvidia Driver Version: 470.82.01
CUDA Version: 11.4
CUDNN Version: NA
Operating System + Version: Ubuntu 18.04
Python Version (if applicable): 3.8
TensorFlow Version (if applicable):  NA
PyTorch Version (if applicable): NA
Baremetal or Container (if container which image + tag): NAmodel_1.onnx (462.8 KB)model_2.onnx (462.8 KB)model_3.onnx (462.9 KB)Hi,The below links might be useful for you.
https://docs.nvidia.com/deeplearning/tensorrt/archives/tensorrt-803/best-practices/index.html#thread-safetyThe programming guide to the CUDA model and interface.https://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__STREAM.htmlFor multi-threading/streaming, will suggest you to use Deepstream or TRITONFor more details, we recommend you raise the query in the Deepstream forum.orraise the query in the Triton Inference Server Github instance issues section.Triton Inference Server has 28 repositories available. Follow their code on GitHub.Thanks!The issue was with ONNX model. It had multiple unnecessary outputs also. Once they were removed, the issue got resolvedThis topic was automatically closed 14 days after the last reply. New replies are no longer allowed.Powered by Discourse, best viewed with JavaScript enabled"
565,hello-i-want-to-remove-the-output-of-the-image-to-the-display-in-the-detectnet,"I want to remove the output of the image to the display in the detectnet and immediately broadcast via RTSP, I did the broadcast, but the window with the video from the camera is displayed on the screenHi @mr.kir141 ,
This forum talks about support and issues related to TensorRT.
I believe you can reach out to Jetson forum for better assistance.ThanksPowered by Discourse, best viewed with JavaScript enabled"
566,cudnn-runs-pretty-slow,"I wrote a simple convolution program in cudnn but the elapsed time I measured al always ~280ms while my own implementation based on cufft just takes ~10ms. I am using cudnn 8.8.1.3 on CUDA 11.8, geforce 1080Ti.Something to mention. The elapsed time did not change much on different conv algorithms I specified, it even did not change when the batchSize was changed from10 to 1. I suspect that it is because the timer did not work correctly due to some concurrency issues.Here is my code:Problem solved. The answer is here: cuDNN8: extreamly slow first iteration of CNN training or inferenceJust add cudnnCnnInferVersionCheck(); to preload the kernel before the timer start, so it could correctly get only the convolution time.This topic was automatically closed 14 days after the last reply. New replies are no longer allowed.Powered by Discourse, best viewed with JavaScript enabled"
567,module-tensorrt-has-no-attribute-volume,"A clear and concise description of the bug or issue.TensorRT Version: 8.5.2.2
GPU Type: Jetson orin nano（8g）
Nvidia Driver Version: jetpack5.1.1
CUDA Version: 11.4.19
CUDNN Version: 8.6.0
Operating System + Version: Jetson Linux 35.3.1
Python Version (if applicable): 3.8
TensorFlow Version (if applicable):
PyTorch Version (if applicable): 1.12.0
Baremetal or Container (if container which image + tag):
DSC_00661920×1440 359 KB
Please attach or include links to any models, data, files, or scripts necessary to reproduce your issue. (Github repo, Google Drive, Dropbox, etc.)Please include:Hi,This looks like a Jetson issue. Please refer to the below samples in case useful.Hello AI World guide to deploying deep-learning inference networks and deep vision primitives with TensorRT and NVIDIA Jetson. - GitHub - dusty-nv/jetson-inference: Hello AI World guide to deployin...An easy to use PyTorch to TensorRT converter. Contribute to NVIDIA-AI-IOT/torch2trt development by creating an account on GitHub.For any further assistance, we will move this post to to Jetson related forum.Thanks!I have solved this problem, because I set up the virtual environment, when soft linking tensorrt, it didn’t include all the filesThis topic was automatically closed 14 days after the last reply. New replies are no longer allowed.Powered by Discourse, best viewed with JavaScript enabled"
568,tensorrt7-0-0-ngc-simple-tf-trt-test-problem,"I have problem with executing this simple test case with TensorRT7.0.0 from NGC docker image.If I run this with TensorRT7.1.3 (in non-docker environment on my host machine) it runs successfully:But when it’s run with NGC nvcr.io/nvidia/tensorflow:20.03-tf2-py3 then I got the following error:Btw, this forum software is awful for posting.Hey! Any update?Hi, I am going to move this topic to the TensorRT forum for better visibility.Sorry for bumping into an old conversation. One thing that comes to mind is that there might be a compatibility issue between the TensorRT version and the NGC docker image. Have you tried using a different version of TensorRT within the docker image? Upgrading or downgrading the TensorRT version might resolve the issue. Alternatively, you could use a service to test this program for you, like zaptest. I hope this helps!Powered by Discourse, best viewed with JavaScript enabled"
569,8xh100-server-training-time-higher-than-8xa100-server,"I have opened and described issue in details on Nvidia github page. link belowBased on additional details on the github issue I changed the category to Nsight Systems.I’ve handed it off to Leslie Monis for initial triage. You should hear back from him here.It looks like you have included the nsys profiler while measuring the time taken for training. Do you observe a similar difference in training times without the presence of the nsys profiler?@lmonis yes, I do observe similar difference in training times without nsys as well.A100 profiling (time taken to complete: w/ nsys :7m48.808s, w/o nsys : 3m51.057s)
H100 profiling (time taken to complete: w/ nsys : 23m16.4s, w/o nsys : 5m29.673s)Powered by Discourse, best viewed with JavaScript enabled"
570,how-to-link-tensorrt-to-a-virtualenv,"I have TensorRT installed in system level (installed from .deb file) and installed Tensorflow in a virtual environment using pip. But Tensorflow complains for missing TensorRT whenever I import tensorflow in my python app.Is it possible to link tensorrt (installed in system level) in a venv without reinstalling in venv (using - pip install tensorrt)? I’m asking it because, tensorrt does a redundant installation of CUDA and cuDNN libs too in the venv if installed from pip.Hi,
Please refer to the installation steps from the below link if in case you are missing on anythingThis NVIDIA TensorRT 8.6.0 Early Access (EA) Installation Guide provides the installation requirements, a list of what is included in the TensorRT package, and step-by-step instructions for installing TensorRT.
Also, we suggest you to use TRT NGC containers to avoid any system dependency related issues.NVIDIA TensorRT is a C++ library that facilitates high-performance inference on NVIDIA graphics processing units (GPUs). TensorRT takes a trained network and produces a highly optimized runtime engine that performs inference for that network.Thanks!Powered by Discourse, best viewed with JavaScript enabled"
571,how-to-change-chunk-size-in-quick-start-scripts,"Please provide the following information when requesting support.Hardware - CPU
Operating System
Riva Version: 2.6I’m doing audio to text transcription through the StreamingRecognize API, I have a lot of latency, I want to use chunks of 160ms size to improve latency. Is it possible to configure this from the riva quick start scripts or should I build new pipelines with riva-build?HI @nharoThanks for your interest in RivaApologies, unfortunately, we won’t be able to configure chuck size from quick start, it can only be tweaked manually using --chunk_size flag in  riva-build and deploy processhttps://docs.nvidia.com/deeplearning/riva/user-guide/docs/asr/asr-pipeline-configuration.htmlThanksI am trying audio to text transcription through the StreamingRecognize API, But it is never working, I have running container as well as riva-servicemaker,
I can run example notebooks perfectly fine.Can you tell your way of doing this, How you did transcription using chunks, as I am trying to do it with flask socketio audio buffer.Thank you.Hello, you can use the nvidia-riva/samples repository where there are examples of the use of the API in python or through the Riva Websocket Bridge repository for JavaScriptPowered by Discourse, best viewed with JavaScript enabled"
572,use-vpi-for-distortion-correction-with-pre-calculated-correction-matrix,"I want to replace my OpenCV code and use VPI for distortion correction.
I have pre-calculated matrix for the correction, using OpenCV remap:cv::Mat src = /* read image /;
cv::Mat dst(src.size(), src.type());
cv::Mat correction_x = / image x correction /;
cv::Mat correction_y = / image y correction */;remap(src, dst, correction_x, correction_y, INTER_LINEAR, BORDER_CONSTANT, Scalar(0, 0, 0));I want to use VPI Lens Distortion Correction or vpi remap examples but I don’t find a way to use my pre-calculated matrix without initializing the VPIWarpMap with a for loop.vpi1Hi,This forum talks more about TensorRT related updates and issues. We recommend you please reach out to the VPI related platform for better help.Helps developers to solve challenges when working with Jetson embedded devices.https://docs.nvidia.com/vpi/getting_started.htmlThank you.This topic was automatically closed 14 days after the last reply. New replies are no longer allowed.Powered by Discourse, best viewed with JavaScript enabled"
573,tensorrt-threads-affect-each-other-during-multithreaded-inference,"According to my understanding of NVIDIA Deep Learning TensorRT Documentation , it should be possible to build tensorRT engines concurrently from multiple threads. However, I did not get the expected resultson the above platform.
In my C++ program, which started multiple threads and initialized the Tensorrt context and cuda stream in each thread, I found in my testing that the time it took to start two threaded models to process a frame (by processing time I mean one of the two models processing a frame) was greater than the time it took to start only one threaded model(Thread models interact with each other,the more threads, the slower the model.).
Deepstream and triton-server are not suitable for my business, so I need to use TensorRT API for integration. I hope you can help me solve this problem.TensorRT Version: TensoRT-8.2.5
GPU Type: RTX3080
Nvidia Driver Version: 520.56.06
CUDA Version: cuda11.8
CUDNN Version: 8.6.0
Operating System + Version: centos7.9
Baremetal or Container (if container which image + tag): BaremetalThis is the simplest test program that contains the full C++ code and model .
Tensort_thead_test1.tar.gz (71.9 MB)
And my onnx model, if you need it
yolov5m.tar.gz (67.5 MB)Hi,The below links might be useful for you.This Best Practices Guide covers various performance considerations related to deploying networks using TensorRT 8.0.3. These sections assume that you have a model that is working at an appropriate level of accuracy and that you are able to...The programming guide to the CUDA model and interface.https://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__STREAM.htmlFor multi-threading/streaming, will suggest you to use Deepstream or TRITONFor more details, we recommend you raise the query in Deepstream forum.orraise the query in Triton Inference Server Github instance issues section.Triton Inference Server has 31 repositories available. Follow their code on GitHub.Thanks!I have checked a lot of relevant topics, and you all reply like this, but this is not helpful to me. I have read the relevant documents and confirmed that there is no problem in using them. I hope you can use the examples I provided for debugging.Deepstream or TRITON didn’t fit our business, so we built on Tensort’s api.Hi,Could you please try on the latest TensorRT version 8.6 and let us know if you still face the same issue.Thank you.I tried TensorRT 8.6.1 and cudn8.9.0 still had the same problem.hello, you haven’t replied to me for a few days, can you give me any useful help?I had the same problem. Is there any way to solve it? Where do you think the code call error?Hi,Sorry for the delay. Please allow us sometime to try reproducing this issue.Thank you.Excuse me, have you checked the result?Powered by Discourse, best viewed with JavaScript enabled"
574,conversion-from-onnx-to-tensorrt-engine,"while conversion from onnx to tensorrt """"ImportError: /home/amogh/scratch/anaconda3/envs/gbc/lib/python3.8/site-packages/pycuda/_driver.cpython-38-x86_64-linux-gnu.so: undefined symbol: cuDevicePrimaryCtxRelease_v2""""Note:
 UserWarning: Failed to import the CUDA driver interface, with an error message indicating that the version of your CUDA header does not match the version of your CUDA driver.TensorRT Version: 8.6.1
GPU Type: Quadro P5000-16GB
Nvidia Driver Version:
CUDA Version: 11.3
Operating System + Version: ubuntu 18.04
Python Version (if applicable): 3.8
PyTorch Version (if applicable): 1.13
Baremetal or Container (if container which image + tag): noneHi,The above issue looks setup-related. Ensure that you have the correct NVIDIA GPU driver installed that is compatible with the CUDA version. You can check your GPU driver version by running nvidia-smi. Please make sure you are able to run a CUDA sample correctly.Thank you.Powered by Discourse, best viewed with JavaScript enabled"
575,failed-cudnn-test-mnistcudnn,"has same problem.eh…I fix this problem by reboot my computer, and samples works fine now.rebooting fixed it for me as well.and me. arrrghRebooting does not work for me. I am using CUDA 9.0 with CuDnn 7.0.5.restart worked…However! Under System Settings → Software and Updates → Additional Drivers it looks as it changed the driver. I selected 390.48, but now it is set as 384.130, which is weird, as I think you need a 390.x driver due to cuda 9.0. Edit: Apparently it’s not a 100% requirement, just very recommended as newer Kernels will need the newer driver.(ubuntu 16.04)I disabled secure boot while rebooting and it worked for me.Just wanted to comment that I had disabled secure boot and got this same error.Restarting the computer, like others mentioned, made it work, getting the following from ./mnistCUDNN:
cudnnGetVersion() : 7301 , CUDNN_VERSION from cudnn.h : 7301 (7.3.1)
Host compiler version : GCC 5.4.0
There are 1 CUDA capable devices on your machine :
device 0 : sms 28  Capabilities 6.1, SmClock 1582.0 Mhz, MemSize (Mb) 11164, MemClock 5505.0 Mhz, Ecc=0, boardGroupID=0
Using device 0Testing single precision
Loading image data/one_28x28.pgm
Performing forward propagation …
Testing cudnnGetConvolutionForwardAlgorithm …
Fastest algorithm is Algo 1
Testing cudnnFindConvolutionForwardAlgorithm …
^^^^ CUDNN_STATUS_SUCCESS for Algo 0: 0.021504 time requiring 0 memory
^^^^ CUDNN_STATUS_SUCCESS for Algo 1: 0.027648 time requiring 3464 memory
^^^^ CUDNN_STATUS_SUCCESS for Algo 2: 0.034816 time requiring 57600 memory
^^^^ CUDNN_STATUS_SUCCESS for Algo 7: 0.069632 time requiring 2057744 memory
^^^^ CUDNN_STATUS_SUCCESS for Algo 4: 0.070656 time requiring 207360 memory
Resulting weights from Softmax:
0.0000000 0.9999399 0.0000000 0.0000000 0.0000561 0.0000000 0.0000012 0.0000017 0.0000010 0.0000000
Loading image data/three_28x28.pgm
Performing forward propagation …
Resulting weights from Softmax:
0.0000000 0.0000000 0.0000000 0.9999288 0.0000000 0.0000711 0.0000000 0.0000000 0.0000000 0.0000000
Loading image data/five_28x28.pgm
Performing forward propagation …
Resulting weights from Softmax:
0.0000000 0.0000008 0.0000000 0.0000002 0.0000000 0.9999820 0.0000154 0.0000000 0.0000012 0.0000006Result of classification: 1 3 5Test passed!Testing half precision (math in single precision)
Loading image data/one_28x28.pgm
Performing forward propagation …
Testing cudnnGetConvolutionForwardAlgorithm …
Fastest algorithm is Algo 1
Testing cudnnFindConvolutionForwardAlgorithm …
^^^^ CUDNN_STATUS_SUCCESS for Algo 0: 0.024480 time requiring 0 memory
^^^^ CUDNN_STATUS_SUCCESS for Algo 1: 0.027488 time requiring 3464 memory
^^^^ CUDNN_STATUS_SUCCESS for Algo 2: 0.047104 time requiring 28800 memory
^^^^ CUDNN_STATUS_SUCCESS for Algo 4: 0.068608 time requiring 207360 memory
^^^^ CUDNN_STATUS_SUCCESS for Algo 7: 0.069632 time requiring 2057744 memory
Resulting weights from Softmax:
0.0000001 1.0000000 0.0000001 0.0000000 0.0000563 0.0000001 0.0000012 0.0000017 0.0000010 0.0000001
Loading image data/three_28x28.pgm
Performing forward propagation …
Resulting weights from Softmax:
0.0000000 0.0000000 0.0000000 1.0000000 0.0000000 0.0000714 0.0000000 0.0000000 0.0000000 0.0000000
Loading image data/five_28x28.pgm
Performing forward propagation …
Resulting weights from Softmax:
0.0000000 0.0000008 0.0000000 0.0000002 0.0000000 1.0000000 0.0000154 0.0000000 0.0000012 0.0000006Result of classification: 1 3 5Test passed!And I wanted to shout out to Robert_Crovella, wrt his instructions before, that I had done all of these:
“install the driver properly. Use the method in the linux install guide. Perform all steps including verification, before attempting to do anything else (like install cudnn)”I came here after having done those.  And indeed, the restart at the end of everything is what was needed.  (That, and another pre-req is that secure boot is disabled.)I also have the same problem, and reboot can work !Hi I have the same issue withCUDA 9.0
cudnn 7.0
NVIDIA Driver 384.145  (but also tried 390)
Ubuntu 16.04
GPU 720MRebooting or using sudo for the cdnn test is not workingI still get thiscudnnGetVersion() : 7005 , CUDNN_VERSION from cudnn.h : 7005 (7.0.5)
Host compiler version : GCC 5.4.0
There are 1 CUDA capable devices on your machine :
device 0 : sms  2  Capabilities 2.1, SmClock 1250.0 Mhz, MemSize (Mb) 1985, MemClock 800.0 Mhz, Ecc=0, boardGroupID=0
Using device 0Testing single precision
CUDNN failure
Error: CUDNN_STATUS_ARCH_MISMATCH
mnistCUDNN.cpp:394
Aborting…Thanks a lot for your helpHello, im dealing qith the same isuue, im using a Nvidia Quadro 2000 with ubuntu 16.04+CUDA9.0+cudnn7.5
Any help or advice would be appreciated~/cudnn_samples_v7/mnistCUDNN$ ./mnistCUDNN
cudnnGetVersion() : 7500 , CUDNN_VERSION from cudnn.h : 7500 (7.5.0)
Host compiler version : GCC 5.4.0
There are 1 CUDA capable devices on your machine :
device 0 : sms  4  Capabilities 2.1, SmClock 1251.0 Mhz, MemSize (Mb) 962, MemClock 1304.0 Mhz, Ecc=0, boardGroupID=0
Using device 0Testing single precision
CUDNN failure
Error: CUDNN_STATUS_ARCH_MISMATCH
mnistCUDNN.cpp:394
Aborting…After some time hitting my head against my desk, i understand than in my particular case, Nvidia quaddro 2000 card does not support Cudnn. Beause of architecture stuff i dunno. This post in particular helped a lotError when I run main.py in  MNIST Convnets.  root@albert-PC:/media/ww/examples-master/mnist# python main.py  Traceback (most recent call last):   File ""main.py"", line 113, in      train(epoch)   File ""main.py"", line 83, in train     output =...
Reading time: 1 mins 🕑
Likes: 2 ❤
So i cheecked in wikipediaCUDA (or Compute Unified Device Architecture) is a parallel computing platform and application programming interface (API) that allows software to use certain types of graphics processing units (GPUs) for general purpose processing, an approach called general-purpose computing on GPUs (GPGPU). CUDA is a software layer that gives direct access to the GPU's virtual instruction set and parallel computational elements, for the execution of compute kernels.
 CUDA is designed to work with programming l...So now i know than nvidia quaddro 2000 card belongs to Fermi microarchitecture, which supports 2.1 arch. But cudnn needs 3.0 or more to work at runtime. So i guess im stopping in my attempts to make this cudnn work. Anyway, i still manage to use opencv with cuda with my card and i guess that might be enough for now. If anyone had the same issue and manage to make cudnn work out I’d like to know how to do that.Hello,You are right, cuDNN doesn’t work with Fermi architecture which is having the compute capability of 2.1. It is clearly mentioned in the Nvidia installation guide that cuDNN requires the GPU of the compute capability 3.0 or higher.This cuDNN 8.5.0 Installation Guide provides step-by-step instructions on how to install and check for correct operation of NVIDIA cuDNN on Linux and Microsoft Windows systems.I think it’s just a waste of time in searching for a solution.Rebooting worked ThanksI have the same problems as follows, even restart again.cudnnGetVersion() : 7605 , CUDNN_VERSION from cudnn.h : 7605 (7.6.5)
Cuda failurer version : GCC 7.4.0
Error: no CUDA-capable device is detected
error_util.h:93I used graphic card GTX 960M, cuda version 10.0, ubuntu 18.04. Rather than installing the cuda as mentioned in the linux installation guide, the cuda was installed through Jackson SDK because i try to work on Jackson Nano through Nsight eclipse . who can help to resolve! thanks a lot!solved by reboot +1Solved by rebootAt first I think my CUDA11.0 was corrupted and I installed it again, but it didn’t work.
REBOOT works, thank you!worked for me tooReboot Worked!!Reboot also worked for me. Please for the love of you choice in deity Nvidia, update your docsPowered by Discourse, best viewed with JavaScript enabled"
576,not-accessible-to-guests-when-downloading,"Please provide the following information when requesting support.Hardware - GPU 3080ti
Hardware - CPU Intel 12ks
Operating Sys2.11tem - Ubuntu 22.04
Riva Version
How to reproduce the issue ? (This is for errors. Please share the command and the detailed log here)I am trying to download Riva Quick start Project but am getting 403 errors. I have tried ngc-cli with app key and also direct download from webpage. I am logged in in all cases but the api end point does not seem to acknowledge me as logged in.I get an error saying 403 Not accessible to guests, but I am logged in. Not sure how I resolve this.having the same issueHi @willpowell8 and @imropanteraApologies, The Issue has been fixed now, can you try and let me know if you still face the isseThanksI am having the same issue when trying to download riva files from:Scripts and utilities for getting started with Riva Speech SkillsWhen trying to download with ngc-cli I get the
Error getting pagination for file download:‘{“timestamp”:1686943143724,“status”:404,“error”:“Not Found”,“path”:“/v1/org/nvidia/team/riva/resources/riva_quickstart/2.11.0/files”}’Powered by Discourse, best viewed with JavaScript enabled"
577,is-there-a-sip-interface-example-for-asr,"Hey, I am having good success with the Jarvis examples. I would like to replace the microphone/headset part with a SIP audio connection so I can setup a service with audio sessions from call centers. The part in the clients that uses a microphone/headset in the client container applications would be replaced with a SIP interface.I saw that the Clara Guardian example has a SIP interface so I have requested access to take a look so I could try to use this in my Jarvis testing. I would preferably want to use python.Is there an example of a client that connects to the audio source via SIP?Hi @rmcinnis1
We don’t have an existing SIP integration example but the team is looking into putting one together.ThanksWas a SIP interface ever developed?I would like to follow up on my original question although the updated package is named Riva. Is there a SIP interface available?Powered by Discourse, best viewed with JavaScript enabled"
578,cudnn-foward-and-backward-passes-help-me-understand,"Hey again everybody.  I’m trying to wrap my head around how cuDNN works as an overall network WITH backpropagation.Here’s my simple network:64x64x3 Image → Convolution → Activation → TransposeConvolution → Activation → 64x64x3 ImageThink of it as a simple autoencoder.  I give it an image, and it outputs an image.  The code is basically:cudnnConvolutionForward() → cudnnActivationForward() → cudnnConvolutionBackwardData() → cudnnActivationForward()Nice right?  Okay, that’s the easy part of course.  What I’m struggling with is how to back propagate the error (target image - output image).  This is how I think it should flow:cudnnActivationBackward() → cudnnConvolutionForward() → cudnnActivationBackward() → cudnnConvolutionBackwardData()What I don’t get is well, most of this.  If I do cudnnConvolutionBackwardData on the forward pass, do I do cudnnConvolutionForward on the backward pass to get the gradient?  IDK.  What about the filter(weights) update?  I see there is a cudnnConvolutionBackwardFilter function, but will that work on the filter used in the TransposeConvolution?There is very little direction here…none actually that I’ve seen.  It’s hard enough to understand how to get a forward pass set up, but backwards?..wow…IDK.Any help or pointers would be appreciated! :)
-ChrisOkay, don’t worry about it.  There is a thread on this forum that has a response by the moderator that makes transpose convolutions very confusing.  He talks about “dgrad”.  Ignore it.Read this: https://arxiv.org/pdf/1603.07285v1.pdfYou can do transpose convolutions using regular forward convolutions.  Read the paper.Messing around some more.  You can use the BackwardData function to do transpose convolution, and yes, you can use Forward to pass the gradient to the previous layer (haven’t confirmed the math though, so try it yourself).  You can also seemingly update the filter and bias using the BackwardFilter/Bias functions too.  Again, I haven’t done the math to confirm it, use at your own risk if nobody can confirm.  IDK. :PThis topic was automatically closed 14 days after the last reply. New replies are no longer allowed.Powered by Discourse, best viewed with JavaScript enabled"
579,about-the-codes-that-should-be-used-for-each-of-tensorrt-capabilities,"Hello. How’re you? I am a student and this semester my seminar topic is about TensorRT and I have been able to learn its capabilities very well and there are many valuable articles in this field. But in the field of code! I can’t find code for all TensorRT features on websites and articles, it’s not complete! For example, there is a code for “precise calibration”, but I can’t find the codes for all the features, and I’ve been searching the internet for a while, but I haven’t found anything acceptable that I can attribute to any of the features. , I have to write the code related to the activation or operation of that feature next to it so that I can defend my seminar. Can you help me with this? I would really appreciate it if you could help mePlease introduce me to sources. No matter how hard I try, I can’t find any resources related to all the properties of  TensorRT!Hi,The following resources may be helpful to you:
Developer guide, which covers information about all features of TensorRT.This is the revision history of the NVIDIA TensorRT 8.6 Developer Guide.This Samples Support Guide provides an overview of all the supported NVIDIA TensorRT 8.6.1 samples included on GitHub and in the product package. The TensorRT samples specifically help in areas such as recommenders, machine comprehension, character...//github.com/NVIDIA/TensorRT/tree/release/8.6/samples//github.com/NVIDIA/TensorRT/tree/release/8.6/demoThank you.Powered by Discourse, best viewed with JavaScript enabled"
580,getting-errors-while-trying-to-install-tensorrt-8-5-3-in-cuda-11-8-environment,"I am getting the following error while trying to run the command:sudo apt-get install tensorrt=8.5.3.1-1+cuda11.8Reading package lists… Done
Building dependency tree
Reading state information… Done
tensorrt is already the newest version (8.5.3.1-1+cuda11.8).
You might want to run ‘apt --fix-broken install’ to correct these.
The following packages have unmet dependencies:
tensorrt : Depends: libnvinfer8 (= 8.5.3-1+cuda11.8) but it is not going to be installed
Depends: libnvinfer-plugin8 (= 8.5.3-1+cuda11.8) but it is not going to be installed
Depends: libnvparsers8 (= 8.5.3-1+cuda11.8) but it is not going to be installed
Depends: libnvonnxparsers8 (= 8.5.3-1+cuda11.8) but it is not going to be installed
Depends: libnvinfer-bin (= 8.5.3-1+cuda11.8) but it is not going to be installed
Depends: libnvinfer-dev (= 8.5.3-1+cuda11.8) but it is not going to be installed
Depends: libnvinfer-plugin-dev (= 8.5.3-1+cuda11.8) but it is not going to be installed
Depends: libnvparsers-dev (= 8.5.3-1+cuda11.8) but it is not going to be installed
Depends: libnvonnxparsers-dev (= 8.5.3-1+cuda11.8) but it is not going to be installed
Depends: libnvinfer-samples (= 8.5.3-1+cuda11.8) but it is not going to be installed
E: Unmet dependencies. Try ‘apt --fix-broken install’ with no packages (or specify a solution).TensorRT Version:  8.5.3.1
GPU Type:  RTX 3080
Nvidia Driver Version: 520
CUDA Version: 11.8
CUDNN Version: 8.6.0.163
Operating System + Version: Ubuntu 20.04
Python Version (if applicable):
TensorFlow Version (if applicable):
PyTorch Version (if applicable):
Baremetal or Container (if container which image + tag):os=“ubuntu2004”
tag=“8.5.3.1-cuda-11.8”
sudo dpkg -i nv-tensorrt-local-repo-${os}-${tag}_1.0-1_amd64.deb
sudo cp /var/nv-tensorrt-local-repo-${os}-${tag}/*-keyring.gpg /usr/share/keyrings/
sudo apt-get update
sudo apt-get install tensorrt=8.5.3.1-1+cuda11.8Hi,
Please refer to the installation steps from the below link if in case you are missing on anythingThis NVIDIA TensorRT 8.6.0 Early Access (EA) Installation Guide provides the installation requirements, a list of what is included in the TensorRT package, and step-by-step instructions for installing TensorRT.
Also, we suggest you to use TRT NGC containers to avoid any system dependency related issues.NVIDIA TensorRT is a C++ library that facilitates high-performance inference on NVIDIA graphics processing units (GPUs). TensorRT takes a trained network and produces a highly optimized runtime engine that performs inference for that network.Thanks!Hello! I followed all the steps carefully but whenever I try to run
sudo apt-get install tensorrtIt tries to install 8.6.0.12-1+cuda12.0When I runapt list -a tensorrt:
I get:Listing… Done
tensorrt/unknown 8.6.0.12-1+cuda12.0 amd64 [upgradable from: 8.5.3.1-1+cuda11.8]
tensorrt/unknown 8.6.0.12-1+cuda11.8 amd64
tensorrt/unknown,now 8.5.3.1-1+cuda11.8 amd64 [installed,upgradable to: 8.6.0.12-1+cuda12.0]
tensorrt/unknown 8.5.3.1-1+cuda11.8 amd64
tensorrt/unknown 8.5.2.2-1+cuda11.8 amd64
tensorrt/unknown 8.5.1.7-1+cuda11.8 amd64
tensorrt/unknown 8.4.3.1-1+cuda11.6 amd64
tensorrt/unknown 8.4.2.4-1+cuda11.6 amd64
tensorrt/unknown 8.4.1.5-1+cuda11.6 amd64Powered by Discourse, best viewed with JavaScript enabled"
581,riva-start-sh-will-not-load-the-models,"Please provide the following information when requesting support.Hardware - Nvidia Geforce RTX 2060
Hardware - Intel(R) Core™ i7-10750H CPU @ 2.60GHz
Operating System fedora 38
Riva Version 2.12.0How to reproduce the issue ? (This is for errors. Please share the command and the detailed log here)Currently when I run riva_init.sh it does work however at the end of the output the follow is shown:qemu-aarch64-static: Could not open ‘/lib/ld-linux-aarch64.so.1’: No such file or directory
qemu-aarch64-static: Could not open ‘/lib/ld-linux-aarch64.so.1’: No such file or directory
qemu-aarch64-static: Could not open ‘/lib/ld-linux-aarch64.so.1’: No such file or directory
qemu-aarch64-static: Could not open ‘/lib/ld-linux-aarch64.so.1’: No such file or directory[[ tegra != \t\e\g\r\a ]][[ tegra == \t\e\g\r\a ]]‘[’ -d /home/samfarzamfar/Desktop/tests/ngc-cli/riva_quickstart_v2.12.0/model_repository/rmir ‘]’[[ tegra == \t\e\g\r\a ]]‘[’ -d /home/samfarzamfar/Desktop/tests/ngc-cli/riva_quickstart_v2.12.0/model_repository/prebuilt ‘]’echoecho ‘Riva initialization complete. Run ./riva_start.sh to launch services.’
Riva initialization complete. Run ./riva_start.sh to launch services.and when I run the riva_start.sh i get the following message:
Starting Riva Speech Services. This may take several minutes depending on the number of models deployed.
Waiting for Riva server to load all models…retrying in 10 seconds
Waiting for Riva server to load all models…retrying in 10 seconds
Waiting for Riva server to load all models…retrying in 10 seconds
Waiting for Riva server to load all models…retrying in 10 seconds
Waiting for Riva server to load all models…retrying in 10 seconds
Waiting for Riva server to load all models…retrying in 10 seconds
Waiting for Riva server to load all models…retrying in 10 seconds
Waiting for Riva server to load all models…retrying in 10 seconds
Waiting for Riva server to load all models…retrying in 10 seconds
Waiting for Riva server to load all models…retrying in 10 seconds
Waiting for Riva server to load all models…retrying in 10 seconds
Waiting for Riva server to load all models…retrying in 10 seconds
Waiting for Riva server to load all models…retrying in 10 seconds
Waiting for Riva server to load all models…retrying in 10 seconds
Waiting for Riva server to load all models…retrying in 10 seconds
Waiting for Riva server to load all models…retrying in 10 seconds
Waiting for Riva server to load all models…retrying in 10 seconds
Waiting for Riva server to load all models…retrying in 10 seconds
Waiting for Riva server to load all models…retrying in 10 seconds
Waiting for Riva server to load all models…retrying in 10 seconds
Waiting for Riva server to load all models…retrying in 10 seconds
Waiting for Riva server to load all models…retrying in 10 seconds
Waiting for Riva server to load all models…retrying in 10 seconds
Waiting for Riva server to load all models…retrying in 10 seconds
Waiting for Riva server to load all models…retrying in 10 seconds
Waiting for Riva server to load all models…retrying in 10 seconds
Waiting for Riva server to load all models…retrying in 10 seconds
Waiting for Riva server to load all models…retrying in 10 seconds
Waiting for Riva server to load all models…retrying in 10 seconds
Waiting for Riva server to load all models…retrying in 10 seconds
Health ready check failed.
Check Riva logs with: docker logs riva-speechand running docker logs riva-speech does not output anything. below is my configuration file:riva_target_gpu_family=“tegra”riva_tegra_platform=“orin”service_enabled_asr=true
service_enabled_nlp=true
service_enabled_tts=true
service_enabled_nmt=truelanguage_code=(“en-US”)asr_acoustic_model=(“conformer”)use_asr_greedy_decoder=falsegpus_to_use=“device=0”MODEL_DEPLOY_KEY=“tlt_encode”riva_model_loc=“riva-model-repo”if [[ $riva_target_gpu_family == “tegra” ]]; then
riva_model_loc=“pwd/model_repository”
fiuse_existing_rmirs=falseriva_speech_api_port=“50051”riva_ngc_org=“nvidia”
riva_ngc_team=“riva”
riva_ngc_image_version=“2.12.0”
riva_ngc_model_version=“2.12.0”########## ASR MODELS ##########models_asr=()for lang_code in ${language_code[@]}; do
modified_lang_code=“${lang_code//-/_}”
modified_lang_code=${modified_lang_code,}donemodels_asr+=()########## NLP MODELS ##########if [[ $riva_target_gpu_family == “tegra” ]]; then
models_nlp=()
else
models_nlp=()
fi########## TTS MODELS ##########if [[ $riva_target_gpu_family == “tegra” ]]; then
models_tts=()
else
models_tts=()
fi######### NMT models ###############models_nmt=(#“${riva_ngc_org}/${riva_ngc_team}/rmir_nmt_en_de_24x6:${riva_ngc_model_version}”
#“${riva_ngc_org}/${riva_ngc_team}/rmir_nmt_en_es_24x6:${riva_ngc_model_version}”
#“${riva_ngc_org}/${riva_ngc_team}/rmir_nmt_en_zh_24x6:${riva_ngc_model_version}”
#“${riva_ngc_org}/${riva_ngc_team}/rmir_nmt_en_ru_24x6:${riva_ngc_model_version}”
#“${riva_ngc_org}/${riva_ngc_team}/rmir_nmt_en_fr_24x6:${riva_ngc_model_version}”
#“${riva_ngc_org}/${riva_ngc_team}/rmir_nmt_de_en_24x6:${riva_ngc_model_version}”
#“${riva_ngc_org}/${riva_ngc_team}/rmir_nmt_es_en_24x6:${riva_ngc_model_version}”
#“${riva_ngc_org}/${riva_ngc_team}/rmir_nmt_ru_en_24x6:${riva_ngc_model_version}”
#“${riva_ngc_org}/${riva_ngc_team}/rmir_nmt_zh_en_24x6:${riva_ngc_model_version}”
#“${riva_ngc_org}/${riva_ngc_team}/rmir_nmt_fr_en_24x6:${riva_ngc_model_version}”#“${riva_ngc_org}/${riva_ngc_team}/rmir_nmt_en_deesfr_24x6:${riva_ngc_model_version}”
#“${riva_ngc_org}/${riva_ngc_team}/rmir_nmt_en_deesfr_12x2:${riva_ngc_model_version}”
#“${riva_ngc_org}/${riva_ngc_team}/rmir_nmt_deesfr_en_24x6:${riva_ngc_model_version}”
#“${riva_ngc_org}/${riva_ngc_team}/rmir_nmt_deesfr_en_12x2:${riva_ngc_model_version}”#“${riva_ngc_org}/${riva_ngc_team}/rmir_megatronnmt_any_en_500m:${riva_ngc_model_version}”
#“${riva_ngc_org}/${riva_ngc_team}/rmir_megatronnmt_en_any_500m:${riva_ngc_model_version}”
)NGC_TARGET=${riva_ngc_org}
if [[ ! -z ${riva_ngc_team} ]]; then
NGC_TARGET=“${NGC_TARGET}/${riva_ngc_team}”
else
team=“""""”
fissl_server_cert=“”
ssl_server_key=“”
ssl_root_cert=“”image_speech_api=“nvcr.io/${NGC_TARGET}/riva-speech:${riva_ngc_image_version}”image_init_speech=“nvcr.io/${NGC_TARGET}/riva-speech:${riva_ngc_image_version}-servicemaker”riva_daemon_speech=“riva-speech”
if [[ $riva_target_gpu_family != “tegra” ]]; then
riva_daemon_client=“riva-client”
fiHI @samfarfarThanks for your interest in Riva,Quick check, are you runningFor your config you need to use the Datacenter versionAlso your card RTX 2060 is won’t be very suitable for Riva workload,Riva is supported on any NVIDIA Volta or later GPU (NVIDIA Turing and NVIDIA Ampere GPU architecture) for development purposes. Care must be taken to not exceed the memory available when selecting models to deploy. 16+ GB VRAM is recommended.https://docs.nvidia.com/deeplearning/riva/user-guide/docs/support-matrix.htmlThanksI am running the Datacenter version already however It still refuses to load the models from the riva server. Also although it is not very suitable my GPU should still work correct?Powered by Discourse, best viewed with JavaScript enabled"
582,my-eos80d-turns-on-and-then-off-when-launching-gazeredirect,"Hi! i have an RTX 3070 TI , and been using gaze redirect with no problems in a previous windows install , so i know it works on my setup. But after reinstalling windows gaze redirect stopped working, it opens the camera (canon 80D) but inmmediatly closes it and shows black screen with some kind of “glitch” line … like its not being able to encode the stream. This is my cmd output:
O] [DSH]  Already initialized
[INFO] [DSH]  Requested to perform initializing sequence
[INFO] [DSH]  Already initialized
[INFO] [DSH]  Requested to perform initializing sequence
[INFO] [DSH]  Already initialized
[INFO] [DSH]  On Thread Create
[INFO] [DSH]  Request to start core exe
[INFO] [DSH]  Core reported already running – Verifying
[INFO] [DSH]  Verified core exe started
[INFO] [DSH]  Request core to start streaming
[ERR ] [DSH]  start stop invalid core response
[INFO] [DSH]  Fill buffer started
[INFO] [DSH]  Initializing image blit data
[INFO] [DSH]  jpeg exif orientation property not found
[INFO] [DSH]  image size acquired successfully. src: 1920x1080
[INFO] [DSH]  image scaler initialized successfully. dst: 1280x720
[INFO] [DSH]  Flip/Rotation Options: 16
[INFO] [DSH]  flip rotator initialized successfully
[INFO] [DSH]  Successfully copied image pixels
[INFO] [DSH]  Verify core stream readiness before the first frame (sh)
[INFO] [UIB]  Request to wait for core output stream readiness
[INFO] [UIB]  Core output stream is ready
[INFO] [DSH]  Core is running but streaming is stopped – check for launch init and restart streaming
[INFO] [DSH]  Request to wait for core launch init
[INFO] [DSH]  Core launch init is done
[INFO] [DSH]  Restart streaming
[INFO] [DSH]  Request core to start streaming
[INFO] [UIB]  Request to wait for core output stream readiness
[INFO] [UIB]  Core output stream is ready
[INFO] [DSH]  Request query surfParticularry in the line:
[INFO] [DSH]  Core is running but streaming is stopped – check for launch init and restart streamingi think its warning me about the problem but i dont know whats different in this installation of windows that causes this.
My GPU drivers are updated to the latest according to Geeforce Experience
The camera works fine with eye redirect via Nvidia Broadcast, but now automatically shuts down on gaze redirect (MAXINE)I have installed FFMPEG and OpenCVEDIT: My run.bat file its this:SETLOCAL
SET PATH=%PATH%;…..\samples\external\opencv\bin;…..\bin;
SET NVAR_MODEL_DIR=…..\bin\models
GazeRedirect.exe --split_screen_view=trueVideo of the problem:Google Drive file.Any ideas? Thx in advanceEDIT: Found the problem, nothing related to anything i was thinking. Turns that Canon Webcam Utiliy PRO, was causing this … just installed previous version of Canon WebCamUtility and works perfect!Powered by Discourse, best viewed with JavaScript enabled"
583,fail-riva-client-offline-when-try-to-enable-diarization,"Hardware - GPU 2080ti
Hardware - CPU i9-10900X 32GB
Operating System ubuntu 20.04
Riva Version 2.8.1
config:
service_enabled_asr=true
service_enabled_nlp=false
service_enabled_tts=falseHi! inside container riva_client try to enable diarization and show this error.root@workstation:/opt/riva# riva_asr_client --audio_file wav/es-US_sample.wav --language-code=es-US --speaker_diarization=trueI0201 00:07:51.942627 194 riva_asr_client.cc:462] Using Insecure Server Credentials
Loading eval dataset…
filename: /opt/riva/wav/es-US_sample.wav
Done loading 1 files
RPC failed: Error: Unavailable diarizer model requested given these parameters: pipeline_type=diarizer; type=offline;
Done processing 1 responses
Some requests failed to complete properly, not printing performance stats
or
root@workstation:/opt/riva# python3 examples/transcribe_file_offline.py --input-file wav/es-US_sample.wav --language-code=es-US --speaker-diarizationError: Unavailable diarizer model requested given these parameters: pipeline_type=diarizer; type=offline;thanks.I forgot make this:Hi @ddelca87Thanks for your interest in RivaThanks for your update
Do you still face the problem after uncommenting the rmir_diarizer_offlineIf yes, request to kindly share the config.sh used with usThanksYes work fine, only uncomment this rmir_diarizer_offline.ThanksHi, I faced the same problem. However, after eneabiling rmir_diarizer_offline_v2.10.0 I get a time out when running riva_start.sh.Hi @razvan.porojanCan you share with us the config.sh used and riva_init complete log outputThanksHi Rvinobha,I attached the requested info. I am using a g4dn.xlarge machine on AWS. With this machine and this AMI (machine image) I make it start once. So it is very weird why it times out.Thank you,Razvanriva_logs.zip (4.2 KB)Powered by Discourse, best viewed with JavaScript enabled"
584,scaling-and-letter-boxing-a-packed-rgb-texture-for-inference-with-tensor-rt,"I’m looking for advise for a CUDA kernel that can scale/normalise packed RGB data into a planar output buffer. When I have packed RGB (uint8_t) data in host memory what would be a good solution to process this packed RGB data using a CUDA kernel, in such a way that the output is written into a planar buffer after I’ve performed normalisation of the input values?I was thinking to use a cudaTextureObject_t and make use of hardware based interpolation. The packed input buffer can be of any resolution, e.g. 1280 x 720 and I need to scale this packed RGB data into an output buffer of 640 x 640 where each color is stored in a separate plane.So in short, I’ve a 1280 x 720 RGB uint8_t buffer which I want to convert into a 640 x 640 float, normalized bufer.One of the challenges I found is that when I use a texture object I can only sample 1, 2, and 4 channels using the tex2D<>() function, and my input data has 3 channels. What are aproaches to work around this?Hi,Sorry for the delayed response.
Maybe instead of trying to sample all three channels at once, you can split the RGB buffer into three separate texture objects, each containing a single channel. This allows you to sample each channel individually using the tex2D<>() function.Please refer to the CUDA programming guide, in case it helps you.The programming guide to the CUDA model and interface.If you need further assistance, please reach out to the CUDA programming related forum to get better help.Thank you.Powered by Discourse, best viewed with JavaScript enabled"
585,riva-speech-container-failed-to-start,"Hardware - GPU: Tesla P40
Hardware - CPU: Intel(R) Xeon(R) Gold 5118 @ 2.30GHz
Operating System: Linux 4.14.49 x86_64 GNU/Linux
Driver Version: 460.73.01
CUDA Version: 11.2
Riva Version: v2.9.0When run the command bash riva_start.sh, riva-speech container exit abnormlly. and run the command docker logs xxxx(riva-speech’s containerId), following error message printed:
cudaError-t 209 : “no kernel image is available for execution on the device” returned from ‘cudaGetLastError()’ in fileexternal/cu-feat-extr/src/cudafeat/feature-online-batched-spectral-cuda.cc line 122
cudaError-t 209 : “no kernel image is available for execution on the device” returned from ‘cudaGetLastError()’ in fileexternal/cu-feat-extr/src/cudafeat/feature-online-batched-spectral-cuda.cc line 149
cudaError-t 209 : “no kernel image is available for execution on the device” returned from ‘cudaGetLastError()’ in fileexternal/cu-feat-extr/src/cudafeat/feature-online-batched-spectral-cuda.cc line 199
cudaError-t 209 : “no kernel image is available for execution on the device” returned from ‘cudaGetLastError()’ in fileexternal/cu-feat-extr/src/cudafeat/feature-online-batched-spectral-cuda.cc line 257[logging.cc:43] 6: The engine plan file is generated on an incompatible device, expecting compute 6.1 got compute 7.5, please rebuild.
[logging.cc:43] 4: [runtime.cpp::deserializeCudaEngine::66] Error Code 4: Internal Error (Engine deserialization failed.)
[tensorrt.cc:5665] TRITONBAKEND_ModelInstanceFinalize: delete instance state
[tensorrt.cc:5604] TRITONBAKEND_ModelFinalize: delete model state
[tensorrt.cc:5627] TRITONBAKEND_ModelInstanceInitialize: riva-trt-confomer-e-S-a-offLne-an-streanino-off1ine A (GPU device 0)
failed to load ‘riva-trt-conformer-en-US-asr-streamino-am-streaming’ version 1: Internal: unable to create TensorRT engineAnd following 3 models failed to load: riva-trt-conformer-en-US-asr-offline-am-streaming-offline, riva-trt-contormer-en-US-asr-streaming-am-streaming-am-streaming and riva-trt-riva-punctuation-en-US-nn-bert-base-uncased.Could you please help to check the reason?Hi @174362510Thanks for your interest in RivaThanks for sharing the logs,Apologies, we guess that your card Tesla P40 is of Pascal Architecture, we need Volta or higher for RivaRiva is supported on any NVIDIA Volta or later GPU (NVIDIA Turing and NVIDIA Ampere GPU architecture) for development purposes. Care must be taken to not exceed the memory available when selecting models to deploy. 16+ GB VRAM is recommended.Reference
https://docs.nvidia.com/deeplearning/riva/user-guide/docs/support-matrix.html#server-hardwareThanksokay, got it, thank you very much for your help.This topic was automatically closed 14 days after the last reply. New replies are no longer allowed.Powered by Discourse, best viewed with JavaScript enabled"
586,parameter-check-failed-at-runtime-api-executioncontext-cpp-520-condition-mengine-hasimplicitbatchdimension,"Here is the full error messageMy project based on Deepstream-yolo repo with commit SHA - 68f762d GitHub - marcoslucianops/DeepStream-Yolo at 68f762d5bdeae7ac3458529bfe6fed72714336ca
I generated YOLOv4 int8 engine model and use repo TensorRT-For-YOLO-Series/trt.py at main · Linaom1214/TensorRT-For-YOLO-Series · GitHub to evaluate engine model, but I got an errorThis error is related with TensorRT. I searched for but not found solutions. I am using Tensort 8.5.2.2 by Docker
How to fix this? ThanksHi,Please share with minimal issue repro ONNX model for better debugging.Thank you.I also encountered similar situations Problem with：
Username: $oauthtoken
Password:
Error response from daemon: login attempt to https://nvcr.io/v2/ failed with status: 502 Bad GatewayHi @AlanChenI have informed the team that manages the site.
Please stand by as we work to resolve this.Thanks for your patience.Tom@spolisetty
Thanks, it seem that I used implicit quantization with batch size = 1. When running inference, I used execute_async_v2() and it did not work.Hi,Are you still facing this issue.
Please share the minimal issue repro ONNX model for better debugging.Thank you.I solved problem by using execute_async()@spolisetty
Method execute_async() work, but output of engine model is seem not correct.
I debug my inference code and recognize that output values is very small. Output has length 4, predictions[3] is class index of my model (from 0 to 79), but 4.2e-42 is obtained. This values seem allocated values from system.Please share the minimal issue repro ONNX model, scripts/sample data and output logs for better debugging.Hi,Are you still facing the above issue?
If you still face the issue, please open a new post with complete error logs and minimal issue repro (ONNX model and scripts) for better debugging.Thank you.Thanks. It wasn’t solved. I used different commit and can run inference.This topic was automatically closed 14 days after the last reply. New replies are no longer allowed.Powered by Discourse, best viewed with JavaScript enabled"
587,a-plan-of-operation-graph-convolution-biasadd-relu-cannot-be-finalized,"cudnn_cbr_sample.cpp (24.2 KB)
Makefile (6.5 KB)Attachment is a demo code of an operation graph “convolution+biasadd+relu”. It has some error in finalizing the plan. But I have no idea about the mitigations through the cudnn log.Can anyone help?Hardware: NVIDIA GeForce RTX 3060
CUDA Version: 11.8
CUDNN Version: 8.6.0.163-1+cuda11.8
OS: Ubuntu 18.04Hi  @zuxian ,
Can you please help us with the detailed error logs to debug it further.ThanksHi @AakankshaS ,Here is the log with CUDNN_LOGINFO_DBG, CUDNN_LOGWARN_DBG, CUDNN_LOGERR_DBG opened:log.txt (124.3 KB)The error logging is:Please take a look. Thanks.Hi @AakankshaS . Is there any update?Hi @zuxian ,
We are checking on this. Please allow us some time.
Thank you for your patience.@AakankshaS i was also trying to do something similar to this and was wondering if there are updates on thisCUDNN Version: 8@zuxian did you ever solve this issue?Not yet. @ryantong@AakankshaS Any update on this?cudnn_cbr_sample.cpp (24.2 KB)Finally, I got this pattern run-through. Some changes might be highlighted:@ryantong  FYI.Also, the validated platform is updated:niubilityThis topic was automatically closed 14 days after the last reply. New replies are no longer allowed.Powered by Discourse, best viewed with JavaScript enabled"
588,nsight-error-target-application-exited-before-profiling-started,"Hi,I’m a new user to CUDA and NSight system. I tried to profile some sample code in some text book but I got this error from Nsight.
ApplicationExitedBeforeProfilingStarted (1104) {
RuntimeError (120) {
OriginalExceptionClass: N5boost10wrapexceptIN11QuadDCommon16RuntimeExceptionEEE
OriginalFile: /build/agent/work/323cb361ab84164c/QuadD/Host/Analysis/Clients/AnalysisHelper/AnalysisStatus.cpp
OriginalLine: 79
OriginalFunction: static QuadDAnalysis::AnalysisHelper::AnalysisStatus::StatusInfo QuadDAnalysis::AnalysisHelper::AnalysisStatus::MakeFromErrorString(QuadDAnalysis::AnalysisHelper::AnalysisStatus::StatusType, QuadDAnalysis::AnalysisHelper::AnalysisStatus::ErrorType, const string&, const DevicePtr&)
ErrorText: The target application exited before profiling started.
}
}
Could some please help me?

Screenshot from 2023-05-27 21-16-502772×1646 301 KB
Hi,Please reach out to the Nsight Systems - NVIDIA Developer Forums to get better help.Thank you.Powered by Discourse, best viewed with JavaScript enabled"
589,what-batch-size-to-use-for-post-training-quantization-int8-calibration,"Reading over the TRT 8.6 docs, I see the following statement:image1343×560 140 KBWhat does this mean in practice? Above it says 500 images are sufficient for calibration. In theory, setting the batch size as large as possible would mean that I use 1 batch of 500 images. That doesn’t seem right to me, so what is a practical batch size to use for a sample size of 500 images?Hi @cyrus.behr,The advice is to use as large a batch as possible.
How many images comprise this batch will depend on the model and the resources it requires vs the resources available on your machine. In other words, if your GPU has enough resources to process 500 images in a single batch, we advise that you use 500 images.It’s important to understand that 500 is an empirical value that may not be large enough for certain models, data sets and tasks. You might want to try using more images for calibration if the accuracy is low. You can examine the dynamic ranges TensorRT computes for each tensor when using progressively larger calibration sets and make sure they stabilize.
During calibration TensorRT collects statistics of the dynamic range of each intermediate activation tensor. To provide a good estimate of the range, the calibration dataset needs to be a good representation of the ""real’ dataset (images that will be inputs during deployment).Powered by Discourse, best viewed with JavaScript enabled"
590,how-to-implement-a-custom-plugin-equivalent-to-tf-tile-in-python-api,"I tried to convert LPRNET to Tensorrt engine. However some layers such as tf.tile are not supported.
I know that such blocks must be implemented using the API (for me Python API). And have seen the samples, but I am completely confused and not sure where to start.Any suggestions for:Hi @fardo54,
I believe, the below link should be able to help you.This Samples Support Guide provides an overview of all the supported NVIDIA TensorRT 8.4.3 samples included on GitHub and in the product package. The TensorRT samples specifically help in areas such as recommenders, machine comprehension, character...Thanks!Dear @AakankshaS,Thank you for your reply.I have already seen this page and the sample codes, but unfortunately I was not able to figure it out.
Some provided samples (in Python) have used the equivalent blocks available in Python API for the unsupported blocks; I could not find some sample code with Python implementation of the unsupported layers from scratch.
Some other sample codes have implemented the custom plugins in CUDA which I am not familiar with…any suggestion?Thank you in advanceHi @fardo54,
You can refer to the below linkTensorRT is an SDK for high performance, deep learning inference. It includes a deep learning inference optimizer and a runtime that delivers low latency and high throughput for deep learning…Thanks!Thanks again @AakankshaS   🙏
I will take a look at that.Hi @fardo54 have you solve this? If yes, would you mind to share? thanksPowered by Discourse, best viewed with JavaScript enabled"
591,how-do-i-do-an-inference-of-a-tensorrt-plan-model-utilizing-python,"Whenever we try to do any inference with our model it fails. Something between the lines of allocating buffers or streams fails.TensorRT Version:
nvcr.io/nvidia/tensorrt:22.05-py3
GPU Type:  RTX 3090
Nvidia Driver Version:  5.15
NVIDIA-SMI 515.86.01    Driver Version: 515.86.01    CUDA Version: 11.7
CUDA Version:
CUDNN Version:
Operating System + Version:  ubuntu 2004
Python Version (if applicable):  3.9
TensorFlow Version (if applicable):
PyTorch Version (if applicable):
Baremetal or Container (if container which image + tag):Please attach or include links to any models, data, files, or scripts necessary to reproduce your issue. (Github repo, Google Drive, Dropbox, etc.)Please include:Hi @mahmoud_saad ,
Can you plesae share more details with us,
like the error logs, onnx model and script to reproduce the case, so that we can assist better.ThanksPowered by Discourse, best viewed with JavaScript enabled"
592,multi-gpu-training-using-mig,"Hi all,I recently got access to a GPU cluster that uses the MIG technology to slice larger GPU cards.
Now Iam wondering if there is any way to use several slices for multi-GPU training (e.g. Pytorch’s nn.DistributedDataParallel)?
For non MIG devices one can specify several GPUs using CUDA_VISIBLE_DEVICES=0,1,2… . MIG also supports CUDA_VISIBLE_DEVICES, but that does only work for a single GPU.Thanks in advance,MPowered by Discourse, best viewed with JavaScript enabled"
593,unknownerror-failed-to-get-convolution-algorithm-this-is-probably-because-cudnn-failed-to-initialize-so-try-looking-to-see-if-a-warning-log-message-was-printed-above-op-conv2d,"command used for package installation :It installed :tensorboard 2.0.0 pyhb38c66f_1
tensorflow 2.0.0 gpu_py37h57d29ca_0
tensorflow-base 2.0.0 gpu_py37h390e234_0
tensorflow-estimator 2.0.0 pyh2649769_0
tensorflow-gpu 2.0.0 h0d30ee6_0 anaconda
cudatoolkit 10.0.130 0
cudnn 7.6.5 cuda10.0_0
keras-applications 1.0.8 py_0
keras-base 2.2.4 py37_0
keras-gpu 2.2.4 0 anaconda
keras-preprocessing 1.1.0 py_1code :Traceback :Hi,Please check if below suggested solutions works:
https://github.com/tensorflow/tensorflow/issues/24828#issuecomment-471950922ThanksHi… Thank you for the solution…Powered by Discourse, best viewed with JavaScript enabled"
594,multiple-gpu-error,"was doing some testing with multiple gpus and i get this error "" Error: 2 UNKNOWN: in ensemble ‘citrinet-1024-en-US-asr-streaming’, audio_signal: failed to perform CUDA copy: an illegal memory access was encountered""Hi @ryeinThanks for your interest in Riva,the error seems to be related to GPURequest to kindly share details aboutMoreover the issue seems more related towards GPU Memory, So when running the riva  server can we kindly track the memory usage using nvidia-smiThanksHI @ryeinOne more kind inputs requiredCan you kindly share the CUDA version used at your end,ThanksWhen I was looking at the memory
GPU 1 has the highest used at around 10-11 Gigs and the others have 3gigs used.  Memory never seems to be an issue.  I think is just has to do with some error as when running this setup with only 1 card it works fine.Topology:
GPU0    GPU1    GPU2    GPU3    GPU4    GPU5    GPU6    GPU7    NIC0    CPU Affinity    NUMA Affinity
GPU0     X      NV2     NV1     NV2     NV1     SYS     SYS     SYS     SYS     0-13,28-41      0
GPU1    NV2      X      NV2     NV1     SYS     NV1     SYS     SYS     SYS     0-13,28-41      0
GPU2    NV1     NV2      X      NV1     SYS     SYS     NV2     SYS     SYS     0-13,28-41      0
GPU3    NV2     NV1     NV1      X      SYS     SYS     SYS     NV2     SYS     0-13,28-41      0
GPU4    NV1     SYS     SYS     SYS      X      NV2     NV1     NV2     PHB     14-27,42-55     1
GPU5    SYS     NV1     SYS     SYS     NV2      X      NV2     NV1     PHB     14-27,42-55     1
GPU6    SYS     SYS     NV2     SYS     NV1     NV2      X      NV1     PHB     14-27,42-55     1
GPU7    SYS     SYS     SYS     NV2     NV2     NV1     NV1      X      PHB     14-27,42-55     1
NIC0    SYS     SYS     SYS     SYS     PHB     PHB     PHB     PHB      XHi @ryeinThanks for sharing the detailsCan you try running riva with CUDA 11.8  instead of 12ThanksI was trying to get into it more deeply to install a different version and nvcc reports something different from nvidia-sminvcc: NVIDIA (R) Cuda compiler driver
Copyright (c) 2005-2021 NVIDIA Corporation
Built on Thu_Nov_18_09:45:30_PST_2021
Cuda compilation tools, release 11.5, V11.5.119
Build cuda_11.5.r11.5/compiler.30672275_0NVIDIA-SMI 525.105.17   Driver Version: 525.105.17   CUDA Version: 12.0     |This is the host systemi installed 11.8 directly from the nvidia website and still got the erorr.Hi @ryeinThanks for checking this out,I will escalate the issue with the internal team and provide updateThanksPowered by Discourse, best viewed with JavaScript enabled"
595,cannot-run-onnxruntime-inference,"Hello ,
I can’t run my script od onnxrumtime inference and get ““illegal instruction (core dumped”” error , this is my code
import onnxruntime as ort
import time
import librosa
import kaldi_native_fbank as knf
import numpy as np
import mutagen
from mutagen.wave import WAVEdef compute_feat(filename):
sample_rate = 16000
samples, _ = librosa.load(filename, sr=sample_rate)
opts = knf.FbankOptions()
opts.frame_opts.dither = 0
opts.frame_opts.snip_edges = False
opts.frame_opts.samp_freq = sample_rate
opts.mel_opts.num_bins = 80def audio_duration(length):
hours = length // 3600  # calculate in hours
length %= 3600
mins = length // 60  # calculate in minutes
length %= 60
seconds = length  # calculate in seconds
total_duration = hours * 3600 + mins * 60 + seconds
return float(total_duration)def main():
filename = “citrinet/sample1.wav”
audio_filepath = filenameif name == ‘main’:
main()Hi @mohandhassan91Thanks for your interest in Riva,Apologies, quick check, is the problem related to Rivacan you share details about the Riva Model you are using,If this is Nemo related query, please let me knowThanksPowered by Discourse, best viewed with JavaScript enabled"
