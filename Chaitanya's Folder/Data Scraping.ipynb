{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e13bddec",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "import time\n",
    "import pandas as pd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "55792f00",
   "metadata": {},
   "outputs": [],
   "source": [
    "main_url = 'https://forums.developer.nvidia.com'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f8fcc958",
   "metadata": {},
   "outputs": [],
   "source": [
    "page = requests.get(main_url)\n",
    "soup = BeautifulSoup(page.content, 'html.parser')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "60ec2886",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/\n",
      "/c/community-information/83\n",
      "/c/regional/611\n",
      "/c/blogs-events/283\n",
      "/c/agx-autonomous-machines/55\n",
      "/c/autonomous-vehicles/517\n",
      "/c/ai-data-science/86\n",
      "/c/accelerated-computing/5\n",
      "/c/healthcare/149\n",
      "/c/gaming-and-visualization-technologies/192\n",
      "/c/developer-tools/106\n",
      "/c/gpu-graphics/145\n",
      "/c/omniverse/300\n",
      "/c/infrastructure/369\n",
      "/c/physics-simulation/442\n",
      "/c/nvidia-virtual-gpu-forums/328\n",
      "/\n",
      "/categories\n",
      "/guidelines\n",
      "https://www.nvidia.com/en-us/about-nvidia/legal-info/\n",
      "https://www.nvidia.com/en-us/about-nvidia/privacy-policy/\n",
      "https://www.discourse.org\n",
      "https://www.nvidia.com/en-us/about-nvidia/legal-info/\n",
      "https://developer.nvidia.com/legal/terms\n",
      "https://www.nvidia.com/en-us/about-nvidia/privacy-policy/\n",
      "https://developer.nvidia.com/contact\n",
      "https://www.nvidia.com/en-us/about-nvidia/privacy-policy/\n"
     ]
    }
   ],
   "source": [
    "all_links = soup.find_all(\"a\", href=True)\n",
    "\n",
    "# Extract and print the URLs of all links\n",
    "for link in all_links:\n",
    "    print(link['href'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b647fd8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "page_links_forum = []\n",
    "for link in all_links:\n",
    "    if str(link['href']).startswith('https:') == False:\n",
    "        page_links_forum.append(main_url+link['href'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "90dd765a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['https://forums.developer.nvidia.com/',\n",
       " 'https://forums.developer.nvidia.com/c/community-information/83',\n",
       " 'https://forums.developer.nvidia.com/c/regional/611',\n",
       " 'https://forums.developer.nvidia.com/c/blogs-events/283',\n",
       " 'https://forums.developer.nvidia.com/c/agx-autonomous-machines/55',\n",
       " 'https://forums.developer.nvidia.com/c/autonomous-vehicles/517',\n",
       " 'https://forums.developer.nvidia.com/c/ai-data-science/86',\n",
       " 'https://forums.developer.nvidia.com/c/accelerated-computing/5',\n",
       " 'https://forums.developer.nvidia.com/c/healthcare/149',\n",
       " 'https://forums.developer.nvidia.com/c/gaming-and-visualization-technologies/192',\n",
       " 'https://forums.developer.nvidia.com/c/developer-tools/106',\n",
       " 'https://forums.developer.nvidia.com/c/gpu-graphics/145',\n",
       " 'https://forums.developer.nvidia.com/c/omniverse/300',\n",
       " 'https://forums.developer.nvidia.com/c/infrastructure/369',\n",
       " 'https://forums.developer.nvidia.com/c/physics-simulation/442',\n",
       " 'https://forums.developer.nvidia.com/c/nvidia-virtual-gpu-forums/328',\n",
       " 'https://forums.developer.nvidia.com/',\n",
       " 'https://forums.developer.nvidia.com/categories',\n",
       " 'https://forums.developer.nvidia.com/guidelines']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "page_links_forum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ce1a6e3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "page_links_forum = page_links_forum[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fd91734e",
   "metadata": {},
   "outputs": [],
   "source": [
    "page_links_forum_names = [str(x).split('/')[-2] for x in page_links_forum]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "686b97ae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['community-information',\n",
       " 'regional',\n",
       " 'blogs-events',\n",
       " 'agx-autonomous-machines',\n",
       " 'autonomous-vehicles',\n",
       " 'ai-data-science',\n",
       " 'accelerated-computing',\n",
       " 'healthcare',\n",
       " 'gaming-and-visualization-technologies',\n",
       " 'developer-tools',\n",
       " 'gpu-graphics',\n",
       " 'omniverse',\n",
       " 'infrastructure',\n",
       " 'physics-simulation',\n",
       " 'nvidia-virtual-gpu-forums',\n",
       " 'forums.developer.nvidia.com',\n",
       " 'forums.developer.nvidia.com',\n",
       " 'forums.developer.nvidia.com']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "page_links_forum_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "cc1fee4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/\n",
      "/c/infrastructure/369\n",
      "/c/infrastructure/nvidia-bright-cluster-manager/596\n",
      "/c/infrastructure/cumulus-linux/425\n",
      "/c/infrastructure/bluefield/511\n",
      "/c/infrastructure/software-and-drivers/522\n",
      "/c/infrastructure/switches-and-gateways/523\n",
      "/c/infrastructure/adapters-and-cables/524\n",
      "/c/infrastructure/doca/370\n",
      "https://forums.developer.nvidia.com/t/where-are-the-mellanox-support-articles/208396\n",
      "/c/infrastructure/369\n",
      "https://forums.developer.nvidia.com/u/TomNVIDIA\n",
      "https://forums.developer.nvidia.com/t/package-digest-query-fails-for-mft-rpm-file-if-run-on-aarch64-but-not-x86-64/244051\n",
      "/c/infrastructure/software-and-drivers/mellanox-ofed/532\n",
      "https://forums.developer.nvidia.com/u/user31900\n",
      "https://forums.developer.nvidia.com/u/samerka\n",
      "https://forums.developer.nvidia.com/t/get-implemented-infiniband-specification-version-of-nic/262316\n",
      "/c/infrastructure/software-and-drivers/mellanox-ofed/532\n",
      "https://forums.developer.nvidia.com/tag/rdma-and-roce\n",
      "https://forums.developer.nvidia.com/u/k.bodi2\n",
      "https://forums.developer.nvidia.com/u/dwaxman\n",
      "https://forums.developer.nvidia.com/t/netq-free-trial-no-email-received-titled-netq-access-link/262308\n",
      "/c/infrastructure/software-and-drivers/netq/434\n",
      "https://forums.developer.nvidia.com/u/jon.mcnamara\n",
      "https://forums.developer.nvidia.com/t/gpudirect-rdma-difference-between-ibv-reg-mr-and-ibv-reg-dmabuf-mr/262313\n",
      "/c/infrastructure/software-and-drivers/mellanox-ofed/532\n",
      "https://forums.developer.nvidia.com/tag/rdma-and-roce\n",
      "https://forums.developer.nvidia.com/u/k.bodi2\n",
      "https://forums.developer.nvidia.com/t/when-run-ib-write-bw-with-2g-msg-mlx5-reports-0x68-why/262168\n",
      "/c/infrastructure/adapters-and-cables/infiniband-vpi-adapter-cards/541\n",
      "https://forums.developer.nvidia.com/u/511252461\n",
      "https://forums.developer.nvidia.com/u/chenh1\n",
      "https://forums.developer.nvidia.com/t/crypto-enabled-disabled-blue-field2/259919\n",
      "/c/infrastructure/bluefield/511\n",
      "https://forums.developer.nvidia.com/u/cxinyic\n",
      "https://forums.developer.nvidia.com/u/chenh1\n",
      "https://forums.developer.nvidia.com/u/strandeangil\n",
      "https://forums.developer.nvidia.com/u/riajsiaajaloanioanx\n",
      "https://forums.developer.nvidia.com/t/cannot-access-to-smartnic-after-modifying-ovs-bridge-configurations-on-bluefield/261591\n",
      "/c/infrastructure/bluefield/511\n",
      "https://forums.developer.nvidia.com/u/hxp0308\n",
      "https://forums.developer.nvidia.com/u/yavtuk\n",
      "https://forums.developer.nvidia.com/u/shim1\n",
      "https://forums.developer.nvidia.com/t/connectx-4-5-6-configuring-disable-vlan-stripping-in-windows/261039\n",
      "/c/infrastructure/adapters-and-cables/ethernet-adapter-cards/542\n",
      "https://forums.developer.nvidia.com/u/robert.hable\n",
      "https://forums.developer.nvidia.com/u/xiaofengl\n",
      "https://forums.developer.nvidia.com/t/performance-degradation-when-using-connectx-6-dx-to-transfer-udp-multicast-jumbo-frames/262039\n",
      "/c/infrastructure/software-and-drivers/mellanox-ofed/532\n",
      "https://forums.developer.nvidia.com/u/chuckm\n",
      "https://forums.developer.nvidia.com/u/spruitt\n",
      "https://forums.developer.nvidia.com/t/non-standard-connectx-6-dx-firmware/255477\n",
      "/c/infrastructure/adapters-and-cables/ethernet-adapter-cards/542\n",
      "https://forums.developer.nvidia.com/u/shepard.siegel\n",
      "https://forums.developer.nvidia.com/u/dwaxman\n",
      "https://forums.developer.nvidia.com/u/nikola.borisof\n",
      "https://forums.developer.nvidia.com/t/sx6018-please-help/207825\n",
      "/c/infrastructure/switches-and-gateways/infiniband-vpi-switch-systems/538\n",
      "https://forums.developer.nvidia.com/u/relbe0579\n",
      "https://forums.developer.nvidia.com/u/eddies\n",
      "https://forums.developer.nvidia.com/u/bruce10\n",
      "https://forums.developer.nvidia.com/u/heinz1\n",
      "https://forums.developer.nvidia.com/u/softlution2\n",
      "https://forums.developer.nvidia.com/t/quadro-rtx-6000-bar-1-size-for-rdma/218629\n",
      "/c/infrastructure/software-and-drivers/rdma-software-for-gpu/537\n",
      "https://forums.developer.nvidia.com/u/julien.claisse\n",
      "https://forums.developer.nvidia.com/u/k.bodi2\n",
      "https://forums.developer.nvidia.com/t/hgx-h100-ib-hdr-200-is-it-possible/262090\n",
      "/c/infrastructure/adapters-and-cables/infiniband-vpi-adapter-cards/541\n",
      "https://forums.developer.nvidia.com/u/bayonetx\n",
      "https://forums.developer.nvidia.com/u/MvB\n",
      "https://forums.developer.nvidia.com/t/intermittent-connection-timed-out-while-setting-rc-qp-to-rtr/262005\n",
      "/c/infrastructure/adapters-and-cables/infiniband-vpi-adapter-cards/541\n",
      "https://forums.developer.nvidia.com/u/arka.sw1988\n",
      "https://forums.developer.nvidia.com/t/difference-between-device-memory-programming-and-nvidia-peerdirect/260829\n",
      "/c/infrastructure/software-and-drivers/mellanox-ofed/532\n",
      "https://forums.developer.nvidia.com/tag/mellanox-ofed\n",
      "https://forums.developer.nvidia.com/u/k.bodi2\n",
      "https://forums.developer.nvidia.com/u/ipavis\n",
      "https://forums.developer.nvidia.com/t/nvidia-persistenced-failed-to-query-nvidia-devices/261841\n",
      "/c/infrastructure/software-and-drivers/application-accelerator-software/534\n",
      "https://forums.developer.nvidia.com/tag/ubuntu\n",
      "https://forums.developer.nvidia.com/tag/cuda\n",
      "https://forums.developer.nvidia.com/tag/kernel\n",
      "https://forums.developer.nvidia.com/u/fossotaj\n",
      "https://forums.developer.nvidia.com/u/haitaos\n",
      "https://forums.developer.nvidia.com/t/mig-support-in-bcm-enablement-failed-and-clients-auto-reset/261787\n",
      "/c/infrastructure/nvidia-bright-cluster-manager/596\n",
      "https://forums.developer.nvidia.com/u/karanveersingh5623\n",
      "https://forums.developer.nvidia.com/t/why-does-the-device-name-of-connectx-6-dx-depend-on-the-os/261676\n",
      "/c/infrastructure/software-and-drivers/network-management-products/535\n",
      "https://forums.developer.nvidia.com/tag/mlnx_ofed\n",
      "https://forums.developer.nvidia.com/tag/mellanox-ofed\n",
      "https://forums.developer.nvidia.com/tag/networking\n",
      "https://forums.developer.nvidia.com/u/bk-2\n",
      "https://forums.developer.nvidia.com/u/zhangsuo\n",
      "https://forums.developer.nvidia.com/t/installing-ubuntu22-04-on-bluefield-2-dpu/261156\n",
      "/c/infrastructure/software-and-drivers/mellanox-ofed/532\n",
      "https://forums.developer.nvidia.com/u/cerotyki\n",
      "https://forums.developer.nvidia.com/u/ssimcoejr\n",
      "https://forums.developer.nvidia.com/t/bad-p-key-how-to-run-down-this-issue/260660\n",
      "/c/infrastructure/switches-and-gateways/infiniband-vpi-switch-systems/538\n",
      "https://forums.developer.nvidia.com/u/gmckee\n",
      "https://forums.developer.nvidia.com/u/ipavis\n",
      "https://forums.developer.nvidia.com/t/opensm-discovering-same-port-over-and-over/261584\n",
      "/c/infrastructure/software-and-drivers/mellanox-ofed/532\n",
      "https://forums.developer.nvidia.com/u/t.roth\n",
      "https://forums.developer.nvidia.com/u/zhangsuo\n",
      "https://forums.developer.nvidia.com/t/kernel-crash-issue-after-installing-mlnx-ofed-23-04-1-1-3-0-driver/261695\n",
      "/c/infrastructure/software-and-drivers/soc-and-smartnic/530\n",
      "https://forums.developer.nvidia.com/u/kyoon\n",
      "https://forums.developer.nvidia.com/u/hyungkwangc\n",
      "https://forums.developer.nvidia.com/t/connect-x6-dx-best-firmware-for-vmware-sriov/261085\n",
      "/c/infrastructure/adapters-and-cables/ethernet-adapter-cards/542\n",
      "https://forums.developer.nvidia.com/u/a.piasz\n",
      "https://forums.developer.nvidia.com/u/hyungkwangc\n",
      "https://forums.developer.nvidia.com/t/enable-web-gui-on-my-mellanox-sn2010-switches/261710\n",
      "/c/infrastructure/cumulus-linux/system-configuration-management/429\n",
      "https://forums.developer.nvidia.com/u/jon.mcnamara\n",
      "https://forums.developer.nvidia.com/u/attilla\n",
      "https://forums.developer.nvidia.com/t/number-of-hairpin-queues/261685\n",
      "/c/infrastructure/bluefield/511\n",
      "https://forums.developer.nvidia.com/u/yavtuk\n",
      "https://forums.developer.nvidia.com/t/configuring-connectx-5-pfc-without-using-vlan/261149\n",
      "/c/infrastructure/adapters-and-cables/ethernet-adapter-cards/542\n",
      "https://forums.developer.nvidia.com/u/jounghoolee\n",
      "https://forums.developer.nvidia.com/u/sribhargavid\n",
      "https://forums.developer.nvidia.com/t/about-doca-flow/255021\n",
      "/c/infrastructure/bluefield/511\n",
      "https://forums.developer.nvidia.com/u/1036780073\n",
      "https://forums.developer.nvidia.com/u/longranw\n",
      "https://forums.developer.nvidia.com/t/use-cuda-dmabuf-is-not-supported-on-this-gpu/260105\n",
      "/c/infrastructure/software-and-drivers/rdma-software-for-gpu/537\n",
      "https://forums.developer.nvidia.com/u/k.bodi2\n",
      "https://forums.developer.nvidia.com/u/Levei_Luo\n",
      "https://forums.developer.nvidia.com/t/installation-error-cannot-create-dev-rshim0-boot/261268\n",
      "/c/infrastructure/bluefield/511\n",
      "https://forums.developer.nvidia.com/u/TinkerFrank\n",
      "https://forums.developer.nvidia.com/u/namrata1\n",
      "/c/infrastructure/369?page=1\n",
      "/\n",
      "/categories\n",
      "/guidelines\n",
      "https://www.nvidia.com/en-us/about-nvidia/legal-info/\n",
      "https://www.nvidia.com/en-us/about-nvidia/privacy-policy/\n",
      "https://www.discourse.org\n",
      "https://www.nvidia.com/en-us/about-nvidia/legal-info/\n",
      "https://developer.nvidia.com/legal/terms\n",
      "https://www.nvidia.com/en-us/about-nvidia/privacy-policy/\n",
      "https://developer.nvidia.com/contact\n",
      "https://www.nvidia.com/en-us/about-nvidia/privacy-policy/\n"
     ]
    }
   ],
   "source": [
    "url = page_links_forum[12]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "page = requests.get(url)\n",
    "soup = BeautifulSoup(page.content, 'html.parser')\n",
    "temp = soup.find_all(\"a\", href=True)\n",
    "\n",
    "for link in temp:\n",
    "    print(link['href'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "1fc3303f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from selenium import webdriver\n",
    "\n",
    "# Remove the headless option to run the browser in non-headless mode\n",
    "driver = webdriver.Chrome()\n",
    "\n",
    "driver.get(url)\n",
    "\n",
    "# Get the initial page height\n",
    "prev_height = driver.execute_script(\"return Math.max( document.body.scrollHeight, document.body.offsetHeight, document.documentElement.clientHeight, document.documentElement.scrollHeight, document.documentElement.offsetHeight );\")\n",
    "\n",
    "# Scroll step by step and add a delay between each scroll to make it visible\n",
    "scroll_step = 50 # Set the scroll step in pixels\n",
    "while True:\n",
    "    driver.execute_script(\"window.scrollTo(0, {});\".format(prev_height + scroll_step))\n",
    "    time.sleep(3)  # Add a delay of 0.5 seconds between each scroll to make it visible\n",
    "\n",
    "    # Get the current page height after scrolling\n",
    "    curr_height = driver.execute_script(\"return Math.max( document.body.scrollHeight, document.body.offsetHeight, document.documentElement.clientHeight, document.documentElement.scrollHeight, document.documentElement.offsetHeight );\")\n",
    "\n",
    "    # Check if we have reached the end of the page\n",
    "    if curr_height == prev_height:\n",
    "        break\n",
    "\n",
    "    prev_height = curr_height  # Update the previous height for the next iteration\n",
    "\n",
    "# Get the page source after scrolling\n",
    "page_content = driver.page_source\n",
    "\n",
    "# Close the WebDriver\n",
    "driver.quit()\n",
    "\n",
    "\n",
    "soup = BeautifulSoup(page_content, \"html.parser\")\n",
    "\n",
    "\n",
    "\n",
    "forum_links = []\n",
    "\n",
    "# Now you can extract the desired data from the soup object\n",
    "# For example, to extract all the links on the page, you can use the following code\n",
    "all_links = soup.find_all(\"a\", href=True)\n",
    "for link in all_links:\n",
    "    forum_links.append(link['href'])\n",
    "\n",
    "    if str(link['href']).startswith('https:'):\n",
    "        forum_links.append(str(link['href']))\n",
    "    elif str(link['href']).startswith('/t/'):\n",
    "        forum_links.append(str(main_url)+str(link['href']))\n",
    "    \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "00952626",
   "metadata": {},
   "outputs": [],
   "source": [
    "forum_links = []\n",
    "\n",
    "\n",
    "for link in all_links:\n",
    "    \n",
    "    \n",
    "    \n",
    "    if str(link['href']).startswith('https:'):\n",
    "        forum_links.append(str(link['href']))\n",
    "    elif str(link['href']).startswith('/t/'):\n",
    "#         print(str(main_url) + str(link['href']))\n",
    "        forum_links.append(str(main_url)+str(link['href']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "90f69b75",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['https://forums.developer.nvidia.com/t/application-offloading/182665/8',\n",
       " 'https://forums.developer.nvidia.com/t/radius-aaa-configuration-questions/206299/3',\n",
       " 'https://forums.developer.nvidia.com/t/ubuntu-18-04-lts-kernel-4-15-0-140-generic-5-2-and-5-1-drivers-fail-to-install/206142',\n",
       " 'https://forums.developer.nvidia.com/t/blue-field-dpu2-not-coming-up/205067',\n",
       " 'https://forums.developer.nvidia.com/t/connecting-to-mlnx-os-via-ssh-and-public-key-authentication/255066/4',\n",
       " 'https://forums.developer.nvidia.com/t/does-the-switch-support-mac-vlan-functionality/252228',\n",
       " 'https://forums.developer.nvidia.com/t/slow-iperf-speed-4gbit-between-2-mellanox-connectx-3-vpi-cards-with-40gps-link/206211/3',\n",
       " 'https://forums.developer.nvidia.com/t/100g-connectx-5-slow-speed-between-mac-to-proxmox-truenas-vm/244161/6',\n",
       " 'https://forums.developer.nvidia.com/t/mellonax-loopback-test/240030',\n",
       " 'https://forums.developer.nvidia.com/t/whenever-the-machine-restarts-the-single-port-bidirectional-test-rate-will-decrease-by-15-or-an-error-will-be-reported/249307/5',\n",
       " 'https://forums.developer.nvidia.com/t/issues-with-assigning-64-pkeys-for-each-server-in-cluster/250311',\n",
       " 'https://forums.developer.nvidia.com/t/mtu-size-on-sx6036/206178',\n",
       " 'https://forums.developer.nvidia.com/t/mellanox-sn3420-100gb-ports-amber-led-after-update-to-culumus-linux-ver-5-4/249629/12',\n",
       " 'https://forums.developer.nvidia.com/t/unable-to-identify-interface-for-packet-capture-on-the-mellanox-switch/212379',\n",
       " 'https://forums.developer.nvidia.com/t/need-to-upgrade-nclu-command-line-select-local-file-from-switch-image-new-version/205923',\n",
       " 'https://forums.developer.nvidia.com/t/doca-gpu-packet-processing-example/253692/3',\n",
       " 'https://forums.developer.nvidia.com/t/expanding-the-menus-in-documentation/181147/1',\n",
       " 'https://forums.developer.nvidia.com/t/ips-example-application-failure-argument-handler/255308',\n",
       " 'https://forums.developer.nvidia.com/t/rdma-not-working-with-connectx-6/205913/3',\n",
       " 'https://forums.developer.nvidia.com/t/network-access-in-the-embedded-mode/184356',\n",
       " 'https://forums.developer.nvidia.com/t/mig-support-in-bcm-enablement-failed-and-clients-auto-reset/261787/3',\n",
       " 'https://forums.developer.nvidia.com/t/cx5-25g-cards-not-linking-up-with-cisco-aci-randomly/245383',\n",
       " 'https://forums.developer.nvidia.com/t/ip-bgp-community-support/206125/2',\n",
       " 'https://forums.developer.nvidia.com/t/msx6012-mlag/206184/9',\n",
       " 'https://forums.developer.nvidia.com/t/representors-on-bluefield2/229407/3',\n",
       " 'https://forums.developer.nvidia.com/t/help-with-error-write-counter-to-semaphore-operation-not-permitted-on-linux/215719/4',\n",
       " 'https://forums.developer.nvidia.com/t/ive-a-connectx6-mcx653106a-hda-ax-in-server-and-when-i-plugged-in-an-active-cable-it-gets-temperature-warning-message-high-temperature-on-sensors-with-bit-set-0-1-i-have-updated-the-connectx6-to-the-latest-firmware-20-29-2002-but-it-doesnt-help/206177',\n",
       " 'https://forums.developer.nvidia.com/t/how-to-enable-connectx-5-nic-ipsec-offload-capability/206072/3',\n",
       " 'https://forums.developer.nvidia.com/t/how-to-upgrade-sn2410-to-cumulus-4-4-version/221192/2',\n",
       " 'https://forums.developer.nvidia.com/t/how-to-flash-oem-firmware-for-connectx-4-lx/206112',\n",
       " 'https://forums.developer.nvidia.com/t/howto-get-connectx-4-uefi-ib-working-with-dhcp/205922/3',\n",
       " 'https://forums.developer.nvidia.com/t/cumulus-linux-gui/198045',\n",
       " 'https://forums.developer.nvidia.com/t/are-there-any-limit-values-when-configuring-ovs-kernel-hardware-on-connectx-6-lx/261545',\n",
       " 'https://forums.developer.nvidia.com/t/fail-to-start-l2fwd-nv/206011/5',\n",
       " 'https://forums.developer.nvidia.com/t/onie-static-ip-configuration/181150',\n",
       " 'https://forums.developer.nvidia.com/t/install-doca-on-bluefield2-faild/255946/1',\n",
       " 'https://forums.developer.nvidia.com/t/connectx-3-cards-ejectable-hot-plug/230347/2',\n",
       " 'https://forums.developer.nvidia.com/t/hello-everyone-i-have-a-question-regarding-the-programmable-pipeline-for-new-network-flows-feature-of-connectx-6-smartnic-i-want-to-know-how-this-feature-can-be-manipulated/205950/2',\n",
       " 'https://forums.developer.nvidia.com/t/missing-full-offload-parameter-for-full-ipsec-offload-on-connectx-6-dx/205997',\n",
       " 'https://forums.developer.nvidia.com/t/error-in-the-log-msn2410/206111/2',\n",
       " 'https://forums.developer.nvidia.com/t/how-to-reset-mlnx-qos-settings/206273',\n",
       " 'https://forums.developer.nvidia.com/t/difference-between-device-memory-programming-and-nvidia-peerdirect/260829',\n",
       " 'https://forums.developer.nvidia.com/t/mst-start-for-bf2-dpu-fails/239923/1',\n",
       " 'https://forums.developer.nvidia.com/t/firmware-expansion-roms-and-driver-for-connectx-4-lx-on-system-with-uefi-bios/206160/2',\n",
       " 'https://forums.developer.nvidia.com/t/ibportstate-disable-port-and-unrecoverable/206261',\n",
       " 'https://forums.developer.nvidia.com/t/where-is-infiniband-sdk-for-connectx-5-on-windows-system/206031/2',\n",
       " 'https://forums.developer.nvidia.com/t/hi-mellanox-team-how-do-we-connect-sfp28-port-of-server-dual-port25gbps-lan-card-to-qsfp28-port-mellanox-switch-p-n-msn2100-bb2f/206171',\n",
       " 'https://forums.developer.nvidia.com/t/rping-maximum-size/224296/1',\n",
       " 'https://forums.developer.nvidia.com/t/mellanox-nic-didnt-link-with-qsfp-cable/206224/2',\n",
       " 'https://forums.developer.nvidia.com/t/setpci-unable-to-change-the-pcie-maximum-read-request-and-payload-values/248884/7',\n",
       " 'https://forums.developer.nvidia.com/t/connectx-5-rocev2-reliable-connection-ack-vlan-issue/239811/5',\n",
       " 'https://forums.developer.nvidia.com/t/nvue-transition-from-nclu/241297/8',\n",
       " 'https://forums.developer.nvidia.com/t/driver-problem-with-carp-multicast-on-freebsd/206188',\n",
       " 'https://forums.developer.nvidia.com/t/sn2100/209662/2',\n",
       " 'https://forums.developer.nvidia.com/t/both-connectx-5-vpi-adapter-and-mcx4121a-acat-connectx-4-lx-en-adapter-one-machine/215548/1',\n",
       " 'https://forums.developer.nvidia.com/t/will-the-trx-10gsfp-sr-mlx-plugged-into-a-qxg-10g2sf-cx4-work-in-a-ubiquiti-environment/205900/2',\n",
       " 'https://forums.developer.nvidia.com/t/nvue-transition-from-nclu/241297',\n",
       " 'https://forums.developer.nvidia.com/t/release-notes-for-nvidia-bright-cluster-manager-9-1-16/244638/2',\n",
       " 'https://forums.developer.nvidia.com/t/as-described-in-datasheet-for-connectx-6-there-are-8-pfs-and-up-to-1k-vfs-per-port-however-the-parameter-num-of-vfs-can-only-be-set-to-127-at-max-and-there-are-two-physical-ports-so-how-can-we-configure-connectx-6-with-8-pfs-and-1k-vfs/206061/6',\n",
       " 'https://forums.developer.nvidia.com/t/where-to-get-regex-version-5-8-for-bluefield-2/217295/1',\n",
       " 'https://forums.developer.nvidia.com/t/dcqcn-config-with-cx6/205947',\n",
       " 'https://forums.developer.nvidia.com/u/zhangsuo',\n",
       " 'https://forums.developer.nvidia.com/t/mellanox-technologies-mt2892-ethernet-mode/256005/2',\n",
       " 'https://forums.developer.nvidia.com/t/how-to-set-rss-in-dpdk-18-11-about-mlx5/215927/1',\n",
       " 'https://forums.developer.nvidia.com/t/bus-error-core-dump-on-bluefield-2/186781',\n",
       " 'https://forums.developer.nvidia.com/t/is-bluefield-smartnic-l3-cache-divide-into-some-slices/236147/7',\n",
       " 'https://forums.developer.nvidia.com/t/does-this-connect-x-3-pro-card-3b00-support-infiniband/206305',\n",
       " 'https://discord.com/invite/XWQNJDNuaC',\n",
       " 'https://forums.developer.nvidia.com/t/gpudirect-rdma-at-the-ibverbs-level/206260',\n",
       " 'https://forums.developer.nvidia.com/t/mlx4-core-communication-channel-command-0x5-op-0x24-timed-out/205979/2',\n",
       " 'https://forums.developer.nvidia.com/t/release-notes-for-nvidia-bright-cluster-manager-8-1-27/255394',\n",
       " 'https://forums.developer.nvidia.com/t/centos-8-stream-centos-9-stream/215053/3',\n",
       " 'https://forums.developer.nvidia.com/t/c76-with-4-18-0-348-23-1-1-ga8e8b87-el7-1-x86-64-install-mlnx-ofed-linux-5-4-1-0-3-0-faied/235079/3',\n",
       " 'https://forums.developer.nvidia.com/u/haitaos',\n",
       " 'https://forums.developer.nvidia.com/t/gid-table-limit/252815',\n",
       " 'https://forums.developer.nvidia.com/t/jetson-agx-connextx-6-dx-dpdk-performance-issue/257882/5',\n",
       " 'https://forums.developer.nvidia.com/t/how-to-limit-fan-rpm-on-sn2010/242723/4',\n",
       " 'https://forums.developer.nvidia.com/t/need-onyx-3-8-2204/218190/1',\n",
       " 'https://forums.developer.nvidia.com/t/about-vma-support-with-connectx-6-vpi-hcas/210906',\n",
       " 'https://forums.developer.nvidia.com/t/vsan-rdma-question/238306',\n",
       " 'https://forums.developer.nvidia.com/t/how-to-setup-and-deploy-sharp-enabled-network-in-k8s-docker-environment/218926/1',\n",
       " 'https://forums.developer.nvidia.com/t/nvmeof-target-offload-setup-on-connect-x/247770/2',\n",
       " 'https://forums.developer.nvidia.com/t/connectx-4-speed-issue-esxi6-7/212428/1',\n",
       " 'https://forums.developer.nvidia.com/t/application-recognition-signature-files/178577',\n",
       " 'https://forums.developer.nvidia.com/t/any-way-to-get-real-time-power-consumption-of-bluefield2-smartnic-card/254762/4',\n",
       " 'https://forums.developer.nvidia.com/t/release-notes-for-nvidia-bright-cluster-manager-8-1-27/255394/2',\n",
       " 'https://forums.developer.nvidia.com/u/dwaxman',\n",
       " 'https://forums.developer.nvidia.com/t/openvswitch-ofed-verbs-device-not-found/235035',\n",
       " 'https://forums.developer.nvidia.com/t/hi-how-do-i-change-name-management-ip-and-description-of-sx1024-switch/205879/3',\n",
       " 'https://forums.developer.nvidia.com/t/what-is-the-main-difference-between-connectx-4-and-connectx-5ex/206040/3',\n",
       " 'https://forums.developer.nvidia.com/t/what-is-the-caused-to-make-single-port-50gbps-only-on-connectx-6-dx-nic/233239/3',\n",
       " 'https://forums.developer.nvidia.com/t/mcx354a-qcbtb-fails-to-detect-transceiver/222467',\n",
       " 'https://forums.developer.nvidia.com/t/operation-ib-wr-reg-mr-fails-with-ib-wc-mw-bind-err/239411',\n",
       " 'https://forums.developer.nvidia.com/u/chenh1',\n",
       " 'https://forums.developer.nvidia.com/t/mlx5-1-switch-infiniband-to-ethernet/205930/5',\n",
       " 'https://forums.developer.nvidia.com/t/problems-with-jumbo-frames-in-a-sn2010/206104',\n",
       " 'https://forums.developer.nvidia.com/t/latest-kernel-officially-supported-by-mlnx-ofed-lts-4-9-4-0-8-0/205972',\n",
       " 'https://forums.developer.nvidia.com/t/activating-wake-on-lan-for-connectx-3-mcx311a-xcat/205883/3',\n",
       " 'https://forums.developer.nvidia.com/t/release-notes-for-nvidia-bright-cluster-manager-9-2-8/238409',\n",
       " 'https://forums.developer.nvidia.com/t/sn2100-mlag-problem/237653/3',\n",
       " 'https://forums.developer.nvidia.com/t/trying-to-get-to-the-admin-gui-on-a-new-sn2010-i-can-ssh-but-not-web-gui/206127/2',\n",
       " 'https://forums.developer.nvidia.com/t/mlx5-linux-counters-vs-mlx5-ethtool-counters/258450/2',\n",
       " 'https://forums.developer.nvidia.com/t/performance-test-with-rocev2/241928/4',\n",
       " 'https://forums.developer.nvidia.com/t/docker-image-build-disconnects-oob-network/209470',\n",
       " 'https://forums.developer.nvidia.com/t/couldnt-find-vendor-err-list-and-hard-to-debug/245052/4',\n",
       " 'https://forums.developer.nvidia.com/t/mlx5-core-cqe-error-on-kernel-log-vendor-syndrome-0xf9/254383',\n",
       " 'https://forums.developer.nvidia.com/t/problem-about-ofed-installation-on-uos20-1050e/254995/1',\n",
       " 'https://forums.developer.nvidia.com/t/replace-connectx-3-fdr10-adapter-with-a-connectx-5-edr-adapter/206186',\n",
       " 'https://forums.developer.nvidia.com/t/create-monitor-failed-on-pipe-using-doca-switch/254086/3',\n",
       " 'https://forums.developer.nvidia.com/t/as-specified-on-spec-sheet-connectx-6-family-adapters-support-up-to-1000-vf-per-port-max-up-to-1k-vf-per-port-is-it-true-for-all-cards-i-m-targetting-to-buy-nvidia-connectx-6-mcx623102an-adat-or-nvidia-connectx-6-mcx623106an-cdat-thank-you/206190',\n",
       " 'https://forums.developer.nvidia.com/t/mellanox-driver-bugcheck-0x50/245328/4',\n",
       " 'https://forums.developer.nvidia.com/t/connectx-6-dx-testpmd-no-probed-ethernet-devices/245116/6',\n",
       " 'https://forums.developer.nvidia.com/t/mlnx-ofed-drivers-for-mt27520-connectx-3-pro-for-debian-11-bullseye/213505/3',\n",
       " 'https://forums.developer.nvidia.com/t/nftables-iptables-tc-and-changing-source-ip/208886',\n",
       " 'https://forums.developer.nvidia.com/t/mlnx5-nic-testpmd-tx-pp-wqe-index-ignore-feature-is-required-for-packet-pacing/206246',\n",
       " 'https://forums.developer.nvidia.com/t/mlx5-core-poll-health-raise-an-error-devices-health-compromised-reached-miss-count/237664',\n",
       " 'https://forums.developer.nvidia.com/t/hardware-vdpa-offloading/256850/3',\n",
       " 'https://forums.developer.nvidia.com/t/installing-mellanox-connectx-5-driver-on-ubuntu-but-in-airgapped-enviroment/226615/3',\n",
       " 'https://forums.developer.nvidia.com/t/bridge-over-vfs/206207',\n",
       " 'https://forums.developer.nvidia.com/t/problem-loading-mlx5-core/252295',\n",
       " 'https://forums.developer.nvidia.com/t/security-implications-of-log4j-in-cuda/198004/12',\n",
       " 'https://forums.developer.nvidia.com/t/connectx-5-dpdk-performance-degradation-when-enabling-jumbo-scatter-and-multi-segs/204249/2',\n",
       " 'https://forums.developer.nvidia.com/t/mellanox-connectx-5-en-25gb-dual-port-spf-rdma-does-not-work-properly/246815',\n",
       " 'https://forums.developer.nvidia.com/t/ndivia-peer-mem-error-could-not-insert-nv-peer-mem-invalid-argument/256385',\n",
       " 'https://forums.developer.nvidia.com/t/performance-degradation-with-hairpin-and-asynchronous-api-in-dpdk/257747',\n",
       " 'https://forums.developer.nvidia.com/t/help-debugging-example-p4-programs-on-bluefield-2-cards-in-dell-server-running-ubuntu-20-04/240527',\n",
       " 'https://forums.developer.nvidia.com/t/infiniband-bonding/205985',\n",
       " 'https://forums.developer.nvidia.com/t/no-ping-with-connectx-5-vpi-socket-direct-on-esxi-7-0-u2/210317',\n",
       " 'https://forums.developer.nvidia.com/t/creating-kubernetes-cluster-on-bcm-exception-version-of-the-local-path-provisioner-0-0-23-is-too-new/249582/25',\n",
       " 'https://forums.developer.nvidia.com/t/connectx-4-25gbps-speed-issue/212501',\n",
       " 'https://forums.developer.nvidia.com/t/nvidia-mib/252006/2',\n",
       " 'https://forums.developer.nvidia.com/t/bluefield-2-0-2-installation-fails/256748',\n",
       " 'https://forums.developer.nvidia.com/t/ibportstate-disable-port-and-unrecoverable/206261/3',\n",
       " 'https://forums.developer.nvidia.com/t/can-i-install-cuda-directly-on-my-bluefield-2/255341',\n",
       " 'https://forums.developer.nvidia.com/t/can-i-safely-ignore-those-windows-warnings/205893',\n",
       " 'https://forums.developer.nvidia.com/t/how-to-enable-sx6012-running-eth-single-switch-profile-with-vlan-filtering-in-l3-mode/206210/2',\n",
       " 'https://forums.developer.nvidia.com/t/is-it-possible-to-use-connectx-4-5-6-and-an-appropriate-switch-to-create-a-50gbe-or-100gbe-infiniband-between-4-desktop-computers-with-i9-12900k-z690-ddr5-motherboard-and-rtx-a6000-a100-not-sure-if-enough-pcie-lanes-and-if-viable-between-desktops/205904',\n",
       " 'https://forums.developer.nvidia.com/t/how-to-connect-sn-2100-switches-via-vxlan-over-a-campus-network/206237',\n",
       " 'https://forums.developer.nvidia.com/t/how-to-set-rss-hash-calculation-from-inner-layer-not-from-outer-layer-for-tunneling-trafic/206182',\n",
       " 'https://forums.developer.nvidia.com/t/connectx-6-dx-packet-drop-when-enabling-rss-rxqueues/205951',\n",
       " 'https://forums.developer.nvidia.com/t/doca-flow-pipe-acl-and-icmp/259741/6',\n",
       " 'https://forums.developer.nvidia.com/t/ats-pri-and-pasid-capabilities-support-on-connectx-7/254751',\n",
       " 'https://forums.developer.nvidia.com/t/connectx-6-dx-22-37-1014/259628',\n",
       " 'https://forums.developer.nvidia.com/t/inline-ipsec-offload-support-on-crypto-enabled-bluefield-2/233086/3',\n",
       " 'https://forums.developer.nvidia.com/t/can-rdma-be-used-with-lacp/219669/2',\n",
       " 'https://forums.developer.nvidia.com/t/doca-1-1-is-available-now/183643/2',\n",
       " 'https://forums.developer.nvidia.com/t/nvmet-hw-offload-issue/252438/5',\n",
       " 'https://forums.developer.nvidia.com/t/switchdev-not-possible-on-bluefield-2/214729',\n",
       " 'https://forums.developer.nvidia.com/t/cc-mgr-not-present-in-all-lts-4-9-versions-of-mlnx-ofed/213914/3',\n",
       " 'https://forums.developer.nvidia.com/t/gpudirect-rdma-on-x86-linux-pc-driver-build-issue/239924/5',\n",
       " 'https://forums.developer.nvidia.com/t/how-do-i-change-the-mac-address-for-a-mcx4131a-gcat/206245',\n",
       " 'https://forums.developer.nvidia.com/t/installing-doca-on-bluefield-dpu/183790',\n",
       " 'https://forums.developer.nvidia.com/t/performance-interference-of-two-regexes-when-running-regex-accelerators-on-bluefield-2/213672/1',\n",
       " 'https://forums.developer.nvidia.com/t/mlnx-ofed-not-getting-installed-on-ubuntu-20-04-with-latest-kernel/213366',\n",
       " 'https://forums.developer.nvidia.com/t/sn2010-firmware-when-moving-from-onyx-to-cumulus/242868',\n",
       " 'https://forums.developer.nvidia.com/t/behavior-of-mellanox-nic/250356/4',\n",
       " 'https://forums.developer.nvidia.com/t/doca-sdk-libaries-and-drivers-depends-linunx-headers-bluefield-5-4-0-1042-41-but-5-4-0-1044-43-is-to-be-installled/226320/1',\n",
       " 'https://forums.developer.nvidia.com/t/any-mellanox-1g-baset-switch-series/217936/2',\n",
       " 'https://forums.developer.nvidia.com/tag/mellanox-ofed',\n",
       " 'https://forums.developer.nvidia.com/t/sx6012-igmp-snooping-warning/206209/2',\n",
       " 'https://forums.developer.nvidia.com/t/cx-4-ethernet-cx416a-rhel7-inbox-cannot-get-roce-to-initialize-anyone-have-a-good-how-to/257540/2',\n",
       " 'https://forums.developer.nvidia.com/t/a-issue-when-compiling-dpdk-on-dpu/221918',\n",
       " 'https://forums.developer.nvidia.com/t/link-problem-between-sfp28-and-sfp/248557',\n",
       " 'https://forums.developer.nvidia.com/t/how-to-configure-tos-in-rss-rule-using-ethtool/228801',\n",
       " 'https://forums.developer.nvidia.com/t/hwmon-monitoring-for-mlx3-and-mlx4/205943',\n",
       " 'https://forums.developer.nvidia.com/t/onie-static-ip-configuration/181150/1',\n",
       " 'https://forums.developer.nvidia.com/t/bluefield-2-doca-flow-sample-wont-run-on-dpu/257664',\n",
       " 'https://forums.developer.nvidia.com/t/is5030-and-sx6036-questions-about-licenses/206131/2',\n",
       " 'https://forums.developer.nvidia.com/t/doca-examples-url-filter/186476',\n",
       " 'https://forums.developer.nvidia.com/t/trying-to-run-nvme-over-ib-client-installed-ofed-5-1-2-5-8-0-on-rhel-8-3-kernel-4-18-0-240-el8-x86-64-using-mlnxofedinstall-with-nvmf-add-kernel-support-command-failed-to-load-nvme-rdma-driver-getting-a-bunch-of-22-errors-in-dmesg/206231/2',\n",
       " 'https://forums.developer.nvidia.com/t/network-interface-renaming-between-mofed-4-9-3-1-5-0-and-modef-5-4/206049/3',\n",
       " 'https://forums.developer.nvidia.com/t/mlx4-core-communication-channel-command-0x5-op-0x24-timed-out/205979',\n",
       " 'https://forums.developer.nvidia.com/t/innova2-flex-app-not-finding-connectx-device/234827/2',\n",
       " 'https://forums.developer.nvidia.com/t/package-digest-query-fails-for-mft-rpm-file-if-run-on-aarch64-but-not-x86-64/244051/8',\n",
       " 'https://forums.developer.nvidia.com/t/installing-doca-on-bluefield-dpu/183790/4',\n",
       " 'https://forums.developer.nvidia.com/t/release-notes-for-nvidia-bright-cluster-manager-9-2-5/229699/2',\n",
       " 'https://forums.developer.nvidia.com/t/unable-to-use-mellanox-cx5-vf-in-my-dpdk-application/209382/3',\n",
       " 'https://forums.developer.nvidia.com/t/mellanox-mcx516a-ccat-connectx-5-support-enabling-or-disabling-function-for-rx-port/253106',\n",
       " 'https://forums.developer.nvidia.com/t/what-is-eswitch-inline-mode-and-how-can-i-use-it-to-get-performance-boost/205982/2',\n",
       " 'https://forums.developer.nvidia.com/t/doca-secure-channel-failed-to-open-comm-channel/233697',\n",
       " 'https://forums.developer.nvidia.com/t/gtpu-decap-support-in-simple-forward-implementation/217649/2',\n",
       " 'https://forums.developer.nvidia.com/t/error-code-16-at-startup-on-mcx516-gcat-once-in-a-while/206254',\n",
       " 'https://forums.developer.nvidia.com/t/looking-for-data-on-the-455a-ecats-heatsink/238415',\n",
       " 'https://forums.developer.nvidia.com/t/issue-using-the-media-receiver-to-display-2110-20-video/244660',\n",
       " 'https://forums.developer.nvidia.com/t/cloning-microsd-card-to-usb/195628/1',\n",
       " 'https://forums.developer.nvidia.com/t/based-on-connectx-6-what-is-the-different-of-virtio-acceleration-through-vf-relay-software-hardware-vdpa-and-virtio-acceleration-through-hardware-vdpa-which-is-in-documents-of-mlnx-en-documentation-rev-5-3-1-0-0-1-07-09-2021-pdf/206062',\n",
       " 'https://forums.developer.nvidia.com/t/i-cannot-use-sdkmanager-to-install-doca/259461/13',\n",
       " 'https://forums.developer.nvidia.com/t/non-standard-connectx-6-dx-firmware/255477',\n",
       " 'https://forums.developer.nvidia.com/t/md5-switch-image/219524',\n",
       " 'https://forums.developer.nvidia.com/t/are-there-registers-or-any-other-options-that-can-be-set-which-will-affect-performance/206243',\n",
       " 'https://forums.developer.nvidia.com/t/the-meaning-of-flow-control-update-watchdog-timer-expired-messages/206255',\n",
       " 'https://forums.developer.nvidia.com/t/how-to-enable-sx6012-running-eth-single-switch-profile-with-vlan-filtering-in-l3-mode/206210',\n",
       " 'https://forums.developer.nvidia.com/t/kernel-crash-issue-after-installing-mlnx-ofed-23-04-1-1-3-0-driver/261695',\n",
       " 'https://forums.developer.nvidia.com/t/vivado-board-files-for-innova-2-flex-cards/254356/4',\n",
       " 'https://forums.developer.nvidia.com/t/hgx-h100-ib-hdr-200-is-it-possible/262090',\n",
       " 'https://forums.developer.nvidia.com/t/doca-flow-how-to-match-icmp/258371/3',\n",
       " 'https://forums.developer.nvidia.com/t/how-does-the-mlx5-driver-use-the-iova-generated-by-iommu-as-dma-address/249794',\n",
       " 'https://forums.developer.nvidia.com/t/i-have-the-performance-issue-of-connectx-4-lx-on-ubuntu-20-04lts-kernel-5-4-0-84-generic-could-you-please-help-us-for-fix-the-issue/205998',\n",
       " 'https://forums.developer.nvidia.com/t/release-notes-for-nvidia-bright-cluster-manager-9-2-13/260080/1',\n",
       " 'https://forums.developer.nvidia.com/t/setpci-unable-to-change-the-pcie-maximum-read-request-and-payload-values/248884',\n",
       " 'https://forums.developer.nvidia.com/t/getting-started-with-connectx-5-100gb-s-adapter-for-windows-2019-server/250461',\n",
       " 'https://forums.developer.nvidia.com/t/i-have-a-problem-with-ubuntu-14-04-6-lts-and-a-connectx-4-ethernet-card-weve-tried-with-various-mlnx-ofed-linux-4-and-5-and-we-cannot-see-the-ports-can-you-help-me-please/206108',\n",
       " 'https://forums.developer.nvidia.com/t/mellanox-flex2-smart-nic-unable-to-set-guid-and-mac/206253',\n",
       " 'https://forums.developer.nvidia.com/t/ipoib-enhanced-mode-issue/242444',\n",
       " 'https://forums.developer.nvidia.com/t/linux-soft-lockup-in-mellanox-driver/206064/2',\n",
       " 'https://forums.developer.nvidia.com/t/cx-6-nof-offload-parameter-configuration/255187/2',\n",
       " 'https://forums.developer.nvidia.com/t/crypto-enabled-disabled-blue-field2/259919/6',\n",
       " 'https://forums.developer.nvidia.com/t/release-notes-for-nvidia-bright-cluster-manager-9-0-20/248751/2',\n",
       " 'https://forums.developer.nvidia.com/t/mellanox-download-center-offline/215428/1',\n",
       " 'https://forums.developer.nvidia.com/t/hwmon-monitoring-for-mlx3-and-mlx4/205943/3',\n",
       " 'https://forums.developer.nvidia.com/t/setting-cx4-lx-adapter-in-mode-switchdev/243213',\n",
       " 'https://forums.developer.nvidia.com/t/rdma-in-windows-11/213257/3',\n",
       " 'https://forums.developer.nvidia.com/t/sha-offload-dpu-error/245375/1',\n",
       " 'https://forums.developer.nvidia.com/t/why-does-sn2410-show-higher-speeds-available-than-label-suggests/206015/3',\n",
       " 'https://forums.developer.nvidia.com/t/getting-started-with-doca-for-connectx-6-dx/246109/2',\n",
       " 'https://forums.developer.nvidia.com/t/sft-init-failed/243229/3',\n",
       " 'https://forums.developer.nvidia.com/t/connectx-6dx-maximum-number-of-tls-ipsec-offloads/206054/2',\n",
       " 'https://forums.developer.nvidia.com/t/how-to-acl-log/239031',\n",
       " 'https://forums.developer.nvidia.com/t/driver-v4-9-5-1-0-module-verification-failed-signature-and-or-required-key-missing-tainting-kernel/227206',\n",
       " 'https://forums.developer.nvidia.com/t/migration-using-rdma-with-mellanox-connectx3-pro-en/254550/1',\n",
       " 'https://forums.developer.nvidia.com/t/connectx-5-and-raw-ethernet-bw-in-perftest-suite-basic-config-question/246468',\n",
       " 'https://forums.developer.nvidia.com/t/implementation-of-fec-and-digital-fiber-nonlinear-distortion-mitigation/205989/2',\n",
       " 'https://forums.developer.nvidia.com/t/nvidia-connectx6-smartnic/204298/1',\n",
       " 'https://forums.developer.nvidia.com/t/missing-directory-with-sr-iov-on-connectx-4-infinband-with-centos8-and-ofed-5-1-2-5-8-0/206159',\n",
       " 'https://forums.developer.nvidia.com/t/innova-2-flex-open-bundle-download/206252',\n",
       " 'https://forums.developer.nvidia.com/t/innova-2-flex-open-bundle-download/206252/4',\n",
       " 'https://forums.developer.nvidia.com/t/problem-running-doca-reference-application-dns-filter-segmentation-fault/236529/1',\n",
       " 'https://forums.developer.nvidia.com/t/mst-sdk-api/254534/9',\n",
       " 'https://forums.developer.nvidia.com/t/issue-with-vlan-pids-vpid/210059/3',\n",
       " 'https://forums.developer.nvidia.com/t/driver-for-mellanox-technologies-mt25408a0-fcc-qi-connectx/208685',\n",
       " 'https://forums.developer.nvidia.com/t/questions-about-backup-configuration-switch-sn2010-in-rconfig/206055/4',\n",
       " 'https://forums.developer.nvidia.com/t/mellonox-switch-mellonx-performance-issues/209622',\n",
       " 'https://forums.developer.nvidia.com/t/enabling-the-http-rest-api/180227/3',\n",
       " 'https://forums.developer.nvidia.com/t/unable-to-verify-integrity-of-bluefield-os-image/253007',\n",
       " 'https://forums.developer.nvidia.com/t/flashed-wrong-firmware-mhrh2a-xsr/206058',\n",
       " 'https://forums.developer.nvidia.com/t/the-device-does-not-seem-to-be-present-delaying-initialization-need-ifup-device-manually/206238/2',\n",
       " 'https://forums.developer.nvidia.com/t/ucx-error-with-driver-5-1-2-5-8-on-rhel-7-9/206236/4',\n",
       " 'https://forums.developer.nvidia.com/t/gtp-tunnel-cant-be-decap/255095/1',\n",
       " 'https://forums.developer.nvidia.com/t/how-to-improve-the-performance-of-flow-steering-on-connectx-5/205984/4',\n",
       " 'https://forums.developer.nvidia.com/t/sn2010-connectx-4lx-mlag-rdma-with-hyper-v-cable-recommendations-with-this-configuration/224454',\n",
       " 'https://forums.developer.nvidia.com/t/infiniband-mtu-value-cannot-be-changed/238028/5',\n",
       " 'https://forums.developer.nvidia.com/t/firmware-24-30-1004/241843/1',\n",
       " 'https://forums.developer.nvidia.com/t/global-pause-issues-increasing-tx-pause-ctrl-phy-with-multiple-queue-pairs/206009',\n",
       " 'https://forums.developer.nvidia.com/t/ib-card-ports-are-down-or-polling/210428',\n",
       " 'https://forums.developer.nvidia.com/t/dual-port-100g-on-pci3-0-bus-bandwidth/218815/3',\n",
       " 'https://forums.developer.nvidia.com/t/my-iblinkinfo-dont-work-on-host/205887',\n",
       " 'https://forums.developer.nvidia.com/t/mst-sdk-api/254534',\n",
       " 'https://forums.developer.nvidia.com/t/why-performance-of-winof-2-in-multi-threads-is-very-poor/251560/3',\n",
       " 'https://forums.developer.nvidia.com/t/degrade-throughout-when-one-of-vls-in-congested-status/227137',\n",
       " 'https://forums.developer.nvidia.com/t/sx6036-switchs-and-hdr-cables/204276/1',\n",
       " 'https://forums.developer.nvidia.com/t/becn-marking-doesnt-work/206110',\n",
       " 'https://forums.developer.nvidia.com/t/i-cant-install-mlnx-ofex-linux-5-2-2-2-0-0-4-15-0-20-generic/206174',\n",
       " 'https://forums.developer.nvidia.com/t/url-filter-cannot-recompile/177586/4',\n",
       " 'https://forums.developer.nvidia.com/t/sudo-apt-install-doca-tools-fails/259627',\n",
       " 'https://nam11.safelinks.protection.outlook.com/?url=https%3A%2F%2Fsupport.brightcomputing.com%2Fmanuals&data=05%7C01%7Ctkearsley%40nvidia.com%7Ca4dd4a1bc66f40ffec8c08da4219a55c%7C43083d15727340c1b7db39efd9ccc17a%7C0%7C0%7C637894975169167463%7CUnknown%7CTWFpbGZsb3d8eyJWIjoiMC4wLjAwMDAiLCJQIjoiV2luMzIiLCJBTiI6Ik1haWwiLCJXVCI6Mn0%3D%7C3000%7C%7C%7C&sdata=2ZQ7zP3htw1sO6ffp%2BpcfE30GWWrme1wpUw2olzN2iE%3D&reserved=0',\n",
       " 'https://forums.developer.nvidia.com/t/about-the-performance-bottleneck-of-qm8790-switcher/250169',\n",
       " 'https://forums.developer.nvidia.com/t/app-shield-agent-attestation-failure/234803/3',\n",
       " 'https://forums.developer.nvidia.com/t/cumulus-radius-configuration/244781',\n",
       " 'https://forums.developer.nvidia.com/t/operation-ib-wr-reg-mr-fails-with-ib-wc-mw-bind-err/239411/3',\n",
       " 'https://forums.developer.nvidia.com/t/gpudirect-in-pci-passthrough-configuration/197211',\n",
       " 'https://forums.developer.nvidia.com/t/bluefield-2-dpu-handling-only-500mbps-traffic/252773',\n",
       " 'https://forums.developer.nvidia.com/t/cumulus-switchd-and-32-routes-in-software/236490',\n",
       " 'https://forums.developer.nvidia.com/t/how-to-forward-broadcast-subnet-or-global-between-vlans/217670/1',\n",
       " 'https://forums.developer.nvidia.com/t/skyway-appliance/246457',\n",
       " 'https://forums.developer.nvidia.com/t/gpu-has-fallen-off-the-bus-requires-your-serious-attention/247859/4',\n",
       " 'https://forums.developer.nvidia.com/t/measure-the-performance-of-the-accelerators-on-bluefield-1-card/210568/4',\n",
       " 'https://forums.developer.nvidia.com/t/how-to-use-non-root-to-start-testpmd-of-dpdk-17-11-with-mlx5-driver/222668/5',\n",
       " 'https://forums.developer.nvidia.com/t/install-doca-on-bluefield-2-failed/203565/3',\n",
       " 'https://forums.developer.nvidia.com/t/lacp-on-mellanox-mlnx-os-msb7800-infiniband-switch/240747',\n",
       " 'https://forums.developer.nvidia.com/t/the-problems-of-connected-or-datagram-in-ipoib/227911/1',\n",
       " 'https://forums.developer.nvidia.com/t/where-to-get-mlx5-ifc-h-and-device-h-for-windows-10-compile/236420',\n",
       " 'https://forums.developer.nvidia.com/t/using-ethernet-and-rdma-simultaneously/206257/2',\n",
       " 'https://forums.developer.nvidia.com/t/connectx6-mlx5-kernel-driver-strange-behavior/219495',\n",
       " 'https://forums.developer.nvidia.com/t/file-scan-example-set-max-number-of-matches/201177',\n",
       " 'https://forums.developer.nvidia.com/t/ofed-5-6-install-pl-dracut-failures-on-rockylinux-8-5/213947',\n",
       " 'https://forums.developer.nvidia.com/t/sn2000-series-max-tc-rules/206097',\n",
       " 'https://forums.developer.nvidia.com/t/connectx-5-bit-rate-expected-with-winsock-send-and-recv-versus-roce/215788',\n",
       " 'https://forums.developer.nvidia.com/t/help-finding-the-latest-drivers-for-my-connectx4lx-card-on-lenovo/257167',\n",
       " 'https://forums.developer.nvidia.com/t/how-to-mitigate-the-degradation-of-mellanox-connectx-5-forwarding-performance-when-multi-segs-is-activated/254105',\n",
       " 'https://forums.developer.nvidia.com/t/hairpin-error-in-dpdk-testpmd-with-connectx-4-lx/221317/1',\n",
       " 'https://forums.developer.nvidia.com/u/namrata1',\n",
       " 'https://forums.developer.nvidia.com/t/help-finding-the-latest-drivers-for-my-connect-x-4-card/205908/2',\n",
       " 'https://forums.developer.nvidia.com/t/mellanox-flex2-smart-nic-unable-to-set-guid-and-mac/206253/3',\n",
       " 'https://forums.developer.nvidia.com/t/app-may-influence-ovs-offload/200064/1',\n",
       " 'https://forums.developer.nvidia.com/t/how-to-reduce-fans-rpm-on-cumulus-linux-switch-sn2410/201035',\n",
       " 'https://forums.developer.nvidia.com/t/error-while-installing-mofed-for-centos-8-3/206239/5',\n",
       " 'https://forums.developer.nvidia.com/t/i-need-help-with-using-infiniband-in-my-data-center/206059',\n",
       " 'https://forums.developer.nvidia.com/t/rshim-shows-another-backend-already-attached-and-tmfifo-net0-cannot-be-found/206004',\n",
       " 'https://forums.developer.nvidia.com/t/does-connectx-6-dx-and-lx-support-ats-and-pri/247273/4',\n",
       " 'https://forums.developer.nvidia.com/t/port-goes-down-when-added-to-mlag-portchannel-sn2010m/258351',\n",
       " 'https://forums.developer.nvidia.com/t/dell-r620-or-r630-compatibility-with-connectx-3-infiniband-mcx354a-fcbt/206044/3',\n",
       " 'https://forums.developer.nvidia.com/t/mlx5-missing-registers-bar-error-code-19/253354',\n",
       " 'https://forums.developer.nvidia.com/t/firmware-24-30-1004/241843',\n",
       " 'https://forums.developer.nvidia.com/t/receive-side-scaling-rss-interrupts-missing/206048/1',\n",
       " 'https://forums.developer.nvidia.com/t/unable-to-change-mode-to-legacy-mode-devlink-fails-on-one-port-hangs-on-the-other/206148',\n",
       " 'https://forums.developer.nvidia.com/t/connectx-increase-rx-buffer-miss-counter-with-max-buffer-size/206028/2',\n",
       " 'https://forums.developer.nvidia.com/t/mlnx5-nic-testpmd-tx-pp-wqe-index-ignore-feature-is-required-for-packet-pacing/206246/2',\n",
       " 'https://forums.developer.nvidia.com/t/whats-the-deal-with-infiniband-connected-mode/209664',\n",
       " 'https://forums.developer.nvidia.com/t/configuring-mellanox-hardware-for-vpi-operation-is-now-available-on-mellanox-com/208120/3',\n",
       " 'https://forums.developer.nvidia.com/t/vlan-sub-interfaces/204597',\n",
       " 'https://forums.developer.nvidia.com/t/performance-of-ipoib-with-200gbps-adapter/206487',\n",
       " 'https://forums.developer.nvidia.com/t/need-guidance-for-infrastructure-design-for-my-homelab/219839',\n",
       " 'https://forums.developer.nvidia.com/t/mellanox-mcx516a-bdat-does-it-support-xdp-and-can-we-direct-connect-it-to-an-adapter-with-qsfp/206056',\n",
       " 'https://forums.developer.nvidia.com/t/mlxconfig-does-not-install/211629/3',\n",
       " 'https://forums.developer.nvidia.com/t/error-during-burning-cc-image/236826/5',\n",
       " 'https://forums.developer.nvidia.com/t/failed-to-run-ipsec-application-full-offload-failed/218606/1',\n",
       " 'https://forums.developer.nvidia.com/t/trouble-in-file-tcpdump-upload-in-mellanox-switch/212182',\n",
       " 'https://forums.developer.nvidia.com/t/bug-9-2-cm-image-create-swimage-a-x86-64-d-ubuntu2204-bootstrap-fails/260529',\n",
       " 'https://forums.developer.nvidia.com/t/ovs-not-up-in-embedded-mode-of-bluefield-2/221866/1',\n",
       " 'https://forums.developer.nvidia.com/t/any-virtualization-platform-for-infiniband-vmware-kvm-or/208984/3',\n",
       " 'https://forums.developer.nvidia.com/t/how-to-evaluate-the-eye-information-of-mellanox-connectx-4-network-card/206063/3',\n",
       " 'https://forums.developer.nvidia.com/t/oracle-linux-6-9-install-driver-happened-fault-failed-to-build-rshim1-8-rpm-how-can-i-resolve-it/253086',\n",
       " 'https://forums.developer.nvidia.com/t/innova-2-flex-connectx-5-ethernet-to-fpga-direct-communication/236059/2',\n",
       " 'https://forums.developer.nvidia.com/t/dmesg-flooded-with-ofed-related-warnings/205977',\n",
       " 'https://forums.developer.nvidia.com/t/can-the-msn2700-cs2f-switch-be-used-as-a-dhcp-server-please-ask-for-relevant-configurations/252938',\n",
       " 'https://forums.developer.nvidia.com/t/connectx6dx-rte-flow-rss-performance-drop-on-mixed-traffic/216114',\n",
       " 'https://forums.developer.nvidia.com/t/sr-iov-pps-limiting/260140/4',\n",
       " 'https://forums.developer.nvidia.com/t/how-to-find-the-maximum-number-of-rx-queues-for-a-nic-connectx-5/206163/5',\n",
       " 'https://forums.developer.nvidia.com/t/performance-drop-of-the-regex-accelerator-on-bluefield-2/211311',\n",
       " 'https://forums.developer.nvidia.com/t/can-we-use-dpdk-in-cumulus-linux/181146',\n",
       " 'https://forums.developer.nvidia.com/t/download-enterprise-mib-files/210549/2',\n",
       " 'https://forums.developer.nvidia.com/t/the-raw-throughput-of-bluefield-1-cannot-reach-the-line-rate/210572',\n",
       " 'https://forums.developer.nvidia.com/t/vf-lag-offload/225798',\n",
       " 'https://forums.developer.nvidia.com/t/spectrum1-and-recursive-route-lookups/238042/2',\n",
       " 'https://forums.developer.nvidia.com/t/the-speed-and-bandwidth-of-200g-mellanox-network-card-card-model-cx6141105a-connextx-6-200gbe-are-less-than-200g/205956/3',\n",
       " 'https://forums.developer.nvidia.com/t/how-to-use-adaptive-routing-in-ib-subnet/206066/2',\n",
       " 'https://forums.developer.nvidia.com/t/the-mlxup-4-22-1-binary-and-checksum-mismatching/246416',\n",
       " 'https://forums.developer.nvidia.com/t/mlnx-ofed-drivers-for-mt27520-connectx-3-pro-for-debian-11-bullseye/213505',\n",
       " 'https://forums.developer.nvidia.com/t/connectx-7-and-vsphere-8-0-beta/229827',\n",
       " 'https://forums.developer.nvidia.com/t/rtnetlink-error-on-loading-xdp-program-to-connectx-5/206228/3',\n",
       " 'https://forums.developer.nvidia.com/t/vpi-configuration-issue-on-sx6036g-proxy-arp-does-not-come-up/205960/3',\n",
       " 'https://forums.developer.nvidia.com/t/mellanox-nic-real-time-performance-monitoring/230389',\n",
       " 'https://forums.developer.nvidia.com/t/what-is-the-difference-between-vnf-and-switch-in-pipe-mode/245986/4',\n",
       " 'https://forums.developer.nvidia.com/t/network-operator-in-rke2-cluster-for-gpudirect-workloads/257924/10',\n",
       " 'https://forums.developer.nvidia.com/t/severe-kernel-memory-leak-in-rpcrdma-both-in-centos-and-mellanox-drivers/204150',\n",
       " 'https://forums.developer.nvidia.com/t/control-the-flow-of-roce-v2-by-flow-programming/200070',\n",
       " 'https://forums.developer.nvidia.com/t/how-to-connect-a-qsfp56-nic-to-a-qsfp28-switch/206075/3',\n",
       " 'https://forums.developer.nvidia.com/t/rdma-is-not-activated-on-my-mellanox-connectx-2-card/253875',\n",
       " 'https://forums.developer.nvidia.com/t/doca-sample-compile-error/247190',\n",
       " 'https://forums.developer.nvidia.com/t/connectx-3-on-ubuntu-20-04/206201',\n",
       " 'https://forums.developer.nvidia.com/t/my-bluefield-smartnic-does-not-work-in-separated-mode-to-the-best-of-my-knowledge-it-should-work-as-a-simple-nic-in-this-mode-but-no-packet-is-transmitted-received/206175',\n",
       " 'https://forums.developer.nvidia.com/t/question-about-virtual-rdma-support-in-containers/238674/3',\n",
       " 'https://forums.developer.nvidia.com/t/connectx6-dpdk-dpdk-testpmd-receive-tcp-udp-mixed-flow-performance-is-very-low/205903',\n",
       " 'https://forums.developer.nvidia.com/u/relbe0579',\n",
       " 'https://forums.developer.nvidia.com/t/representors-on-bluefield2/229407',\n",
       " 'https://forums.developer.nvidia.com/t/connect-sx6036-with-another-sx6036-standalone-to-expand/214222/3',\n",
       " 'https://forums.developer.nvidia.com/t/how-to-fix-the-hca-self-test-fail-error-counter-check-on-ca-0-hca/237140',\n",
       " 'https://forums.developer.nvidia.com/t/obtaining-and-building-linux-kernel-source-for-doca-1-0/175916/13',\n",
       " 'https://forums.developer.nvidia.com/t/does-bluefiled-2-support-ibv-atomic-glob/249340',\n",
       " 'https://forums.developer.nvidia.com/t/performance-drop-of-the-regex-accelerator-on-bluefield-2/211311/6',\n",
       " 'https://www.nvidia.com/en-us/about-nvidia/privacy-policy/',\n",
       " 'https://forums.developer.nvidia.com/u/ipavis',\n",
       " 'https://forums.developer.nvidia.com/t/vlan-tagging-capture-using-wireshark/236306',\n",
       " 'https://forums.developer.nvidia.com/t/connect-between-cx-6-and-infiniscale-switch/205953',\n",
       " 'https://forums.developer.nvidia.com/t/significant-drop-in-vf-throughput/230090/2',\n",
       " 'https://forums.developer.nvidia.com/t/is-there-a-mailing-list-to-get-software-security-updates/205891/2',\n",
       " 'https://forums.developer.nvidia.com/t/how-to-save-settings-on-connectx-6-dx-en-pcie-mcx623106an-cdat/221836',\n",
       " 'https://forums.developer.nvidia.com/t/release-notes-for-nvidia-bright-cluster-manager-9-1-15/237792/2',\n",
       " 'https://forums.developer.nvidia.com/t/issue-of-vma-snd-flags-dummy/246710',\n",
       " 'https://forums.developer.nvidia.com/t/a-doubt-about-mkpkc-tool/215500',\n",
       " 'https://forums.developer.nvidia.com/t/problem-with-dma-on-multicore/248333/1',\n",
       " 'https://forums.developer.nvidia.com/t/mellanox-mcx516a-ccat-connectx-5-support-enabling-or-disabling-function-for-rx-port/253106/2',\n",
       " 'https://forums.developer.nvidia.com/t/mellanox-connectx-5-physical-state-polling/206128/6',\n",
       " 'https://forums.developer.nvidia.com/t/switch-hpe-3180-fm-with-cumulus-os/227384/3',\n",
       " 'https://forums.developer.nvidia.com/t/teaming-dual-25gbe-connectx-6-lx-cards-50gbps-speed/206022',\n",
       " 'https://forums.developer.nvidia.com/t/connectx-6-nat-hardware-offloading-in-ubuntu-os/261491',\n",
       " 'https://forums.developer.nvidia.com/t/drivers-fail-pkcs-7-signature-not-signed-with-a-trusted-key/206206/8',\n",
       " 'https://forums.developer.nvidia.com/t/do-nvidia-have-any-connectx-6-cards-with-sma-for-pps-on-the-market/206078/3',\n",
       " 'https://forums.developer.nvidia.com/t/problem-loading-onie-after-resetting-sn2100/212747/2',\n",
       " 'https://forums.developer.nvidia.com/t/failed-to-get-crypto-operational-regis/258580/3',\n",
       " 'https://forums.developer.nvidia.com/t/did-anyone-observed-a-sudden-abrupt-jump-in-performance-while-working-with-dpdk-rxonly-mode-from-11-queues-to-12-queues-in-mellanox-connectx-5-100-gbe-nic/206164/2',\n",
       " 'https://forums.developer.nvidia.com/t/enable-web-gui-on-my-mellanox-sn2010-switches/261710',\n",
       " 'https://forums.developer.nvidia.com/t/mellanox-connectx-6-hdr100-qsfp56-ethernet-connection/246775/9',\n",
       " 'https://forums.developer.nvidia.com/t/as-specified-on-spec-sheet-connectx-6-family-adapters-support-up-to-1000-vf-per-port-max-up-to-1k-vf-per-port-is-it-true-for-all-cards-i-m-targetting-to-buy-nvidia-connectx-6-mcx623102an-adat-or-nvidia-connectx-6-mcx623106an-cdat-thank-you/206190/2',\n",
       " 'https://forums.developer.nvidia.com/t/how-to-configure-hierarchical-qos-offloading-in-connectx-6-nic/206109',\n",
       " 'https://forums.developer.nvidia.com/t/fix-build-drivers-for-4-9-6-0-6-4-18-0-425-10-1-el8-7-x86-64-in-rhel-8-7-failing-because-of-brp-mangle-shebangs/243106',\n",
       " 'https://forums.developer.nvidia.com/t/sharp-error-in-sharp-connect-tree/209506',\n",
       " 'https://forums.developer.nvidia.com/t/hpcx-mpi-runtime-error/230866',\n",
       " 'https://forums.developer.nvidia.com/t/vivado-board-files-for-innova-2-flex-cards/254356',\n",
       " 'https://forums.developer.nvidia.com/t/sn2010-dont-seem-to-be-able-to-link-at-25g/206038',\n",
       " 'https://forums.developer.nvidia.com/t/newer-dhcp-relay-rfc-rfc5107-not-isc-dhcp-relay/257602/2',\n",
       " 'https://forums.developer.nvidia.com/t/mounting-cifs-shares-with-mlnx-ofa-kernel/206050/2',\n",
       " 'https://forums.developer.nvidia.com/t/ubuntu-20-04-ip-over-ib-setup/206458',\n",
       " 'https://forums.developer.nvidia.com/t/mellanox-crypto-drivers-not-working-with-dpdk/244615/5',\n",
       " 'https://forums.developer.nvidia.com/u/gmckee',\n",
       " 'https://forums.developer.nvidia.com/t/ax-xdp-zero-copy-support-for-mlx4/231616',\n",
       " 'https://forums.developer.nvidia.com/t/is-ha-subent-manager-needed-on-small-fabrics-40-host-ports/257290/3',\n",
       " 'https://forums.developer.nvidia.com/t/mellanox-mt4099-cannot-send-files-to-maximum-bandwidth-throughput-in-rdma-applications/211177',\n",
       " 'https://forums.developer.nvidia.com/t/mlx5-net-failed-to-allocate-tx-devx-uar-bf-nc/215820/2',\n",
       " 'https://forums.developer.nvidia.com/t/sft-init-failed/243229',\n",
       " 'https://forums.developer.nvidia.com/t/sn2000-series-latency/206100',\n",
       " 'https://forums.developer.nvidia.com/t/flowtable-nat-hardware-offload-on-connectx-5-cards/255957',\n",
       " 'https://forums.developer.nvidia.com/t/mcx354a-wont-do-56gbps/206200/2',\n",
       " 'https://forums.developer.nvidia.com/t/url-filter-cannot-recompile/177586',\n",
       " 'https://forums.developer.nvidia.com/t/how-to-enable-auto-negotiation-of-connectx-3-mt27500-family/205942',\n",
       " 'https://forums.developer.nvidia.com/t/running-rxpbench-using-sfs/253084',\n",
       " 'https://forums.developer.nvidia.com/t/connectx-6-lx-scheduled-sending-only-sending-25-packets/205898',\n",
       " 'https://forums.developer.nvidia.com/t/openibd-service-is-failed-on-a-particular-node/206187/3',\n",
       " 'https://forums.developer.nvidia.com/t/connectx-5-rocev2-reliable-connection-ack-vlan-issue/239811',\n",
       " 'https://forums.developer.nvidia.com/t/migration-using-rdma-with-mellanox-connectx3-pro-en/254550',\n",
       " 'https://forums.developer.nvidia.com/t/url-filtering-app-it-shows-the-failure-when-executing-commit-database-command/199501',\n",
       " 'https://forums.developer.nvidia.com/t/is-burning-a-rom-image-required-for-connectx5-to-pxe-boot-https-www-mellanox-com-related-docs-prod-software-mellanox-preboot-drivers-user-manual-v4-0-pdf/205955/3',\n",
       " 'https://forums.developer.nvidia.com/t/cannot-access-to-smartnic-after-modifying-ovs-bridge-configurations-on-bluefield/261591',\n",
       " 'https://forums.developer.nvidia.com/t/loopback-timestamping-errors/210985',\n",
       " 'https://forums.developer.nvidia.com/t/ibv-device-open-fails-no-space-left-on-device/215223',\n",
       " 'https://forums.developer.nvidia.com/t/packet-generator-on-windows-for-connectx5-100gbe-connected-to-custom-hardware/206071/6',\n",
       " 'https://forums.developer.nvidia.com/t/mellanox-soft-roce-for-windows-10/240969/2',\n",
       " 'https://forums.developer.nvidia.com/t/when-the-port-is-down-then-up-the-ptp-clock-is-synchronized-by-the-system-clock/260505/4',\n",
       " 'https://forums.developer.nvidia.com/u/karanveersingh5623',\n",
       " 'https://forums.developer.nvidia.com/u/fossotaj',\n",
       " 'https://forums.developer.nvidia.com/t/vf-cant-set-trust-on/255619/1',\n",
       " 'https://forums.developer.nvidia.com/t/url-filter-system-design-ovs/202727/1',\n",
       " 'https://forums.developer.nvidia.com/t/mlx5-1-switch-infiniband-to-ethernet/205930',\n",
       " 'https://forums.developer.nvidia.com/t/can-i-still-use-a-output-mirror-port-as-a-management-connection/206005/2',\n",
       " 'https://forums.developer.nvidia.com/t/cant-find-the-firmware-of-mt26428-mt-0d20110009/229988',\n",
       " 'https://forums.developer.nvidia.com/t/does-nvidia-msn2100-support-lossless-network-priority-based-flow-control-explicit-congestion-notification/223021/1',\n",
       " 'https://forums.developer.nvidia.com/t/nvdia-quadro-k2000-is-not-compatible-with-windows-10-and-windows-11/247369',\n",
       " 'https://forums.developer.nvidia.com/t/installing-mlnx-ofed/261330',\n",
       " 'https://forums.developer.nvidia.com/t/mlag-lacp-rate-mismatch-for-linux-host/206088/6',\n",
       " 'https://forums.developer.nvidia.com/t/hi-i-have-msx1012b-2bfs-and-it-has-current-version-of-mlnx-os-at-3-4-2008-when-i-go-to-upgrade-options-and-select-current-software-and-target-software-it-doesnt-show-any-files-to-upgrade-how-can-i-upgrade-this-managed-switch-to-latest-software/205920/4',\n",
       " 'https://forums.developer.nvidia.com/t/how-to-install-driver-in-coreos/207001/6',\n",
       " 'https://forums.developer.nvidia.com/t/how-do-a-loop-back-test-with-single-card-installed/210070/4',\n",
       " 'https://forums.developer.nvidia.com/t/with-cx-5-nic-setup-kernel-nvmeof-following-standard-steps-nvmet-report-error-when-we-try-run-nvme-connect/231500/1',\n",
       " 'https://forums.developer.nvidia.com/t/infiniband-nic-can-not-be-active-on-bluefied2/189382',\n",
       " 'https://forums.developer.nvidia.com/t/rdma-programming-on-windows/231478/4',\n",
       " 'https://forums.developer.nvidia.com/t/we-need-the-guide-on-how-to-accelerate-https-with-dpu-bluefield-2-on-host-server/194333',\n",
       " 'https://forums.developer.nvidia.com/t/mcx623106an-cdat-card-and-mcx623436an-cdab-card-i2c-registers-details/232405/4',\n",
       " 'https://forums.developer.nvidia.com/t/read-the-sonic-202012-user-guide/182139',\n",
       " 'https://forums.developer.nvidia.com/t/help-finding-the-latest-drivers-for-my-connectx4lx-card-on-lenovo/257167/5',\n",
       " 'https://forums.developer.nvidia.com/t/ibdump-with-connect-5-failed-to-set-port-sniffer1-command-interface-bad-param/206448',\n",
       " 'https://forums.developer.nvidia.com/t/no-link-w-eth-mode-in-w10-pro-on-connectx-2-dual-qsfp-vpi-ddr-20gbps-infiniband-ib-eth-mhrh2a-xsr/221473/2',\n",
       " 'https://forums.developer.nvidia.com/t/dpdk-regex-problem-on-bluefield/241957/5',\n",
       " 'https://forums.developer.nvidia.com/t/how-to-make-incoming-traffic-go-outside-without-any-dma/253258/2',\n",
       " 'https://forums.developer.nvidia.com/t/dpdk-21-11-coexist-capability-of-pmd-with-kernel-network-interfaces/242508',\n",
       " 'https://forums.developer.nvidia.com/t/why-does-the-device-name-of-connectx-6-dx-depend-on-the-os/261676/3',\n",
       " 'https://forums.developer.nvidia.com/t/bluefield-2-dpu-handling-only-500mbps-traffic/252773/3',\n",
       " 'https://forums.developer.nvidia.com/u/softlution2',\n",
       " 'https://forums.developer.nvidia.com/t/how-to-setup-and-deploy-sharp-enabled-network-in-k8s-docker-environment/218926',\n",
       " 'https://forums.developer.nvidia.com/t/unknown-psid-mt-0000000493/241125',\n",
       " 'https://forums.developer.nvidia.com/t/smb-direct-support/221382/1',\n",
       " 'https://forums.developer.nvidia.com/t/how-is-the-lid-assigned/206068/4',\n",
       " 'https://forums.developer.nvidia.com/t/any-news-about-bf3/231670',\n",
       " 'https://forums.developer.nvidia.com/t/how-to-disable-nic-multi-queue-in-linux/206027/2',\n",
       " 'https://forums.developer.nvidia.com/t/where-to-get-mlx5-ifc-h-and-device-h-for-windows-10-compile/206018',\n",
       " 'https://forums.developer.nvidia.com/t/how-to-measure-rdma-performance-with-mlxndperf-exe-windows/205986/2',\n",
       " 'https://forums.developer.nvidia.com/t/release-notes-for-nvidia-bright-cluster-manager-9-1-14/230127',\n",
       " 'https://forums.developer.nvidia.com/t/port-goes-down-when-added-to-mlag-portchannel-sn2010m/258351/2',\n",
       " 'https://forums.developer.nvidia.com/t/ovn-ovs-gateway-offload-using-connectx-6/234279/3',\n",
       " 'https://forums.developer.nvidia.com/t/release-notes-for-nvidia-bright-cluster-manager-8-2-30/254561/2',\n",
       " 'https://forums.developer.nvidia.com/t/inconsistent-udp-multicast-transmission-performance-on-windows/226619/2',\n",
       " 'https://forums.developer.nvidia.com/t/on-what-switches-is-sonic-supported/182142',\n",
       " 'https://forums.developer.nvidia.com/t/nvidia-mib/252006',\n",
       " 'https://forums.developer.nvidia.com/t/cannot-access-to-smartnic-after-modifying-ovs-bridge-configurations-on-bluefield/261591/5',\n",
       " 'https://forums.developer.nvidia.com/t/how-to-reset-uefi-bootloader-password/218837',\n",
       " 'https://forums.developer.nvidia.com/t/we-met-an-issue-create-qp-0x500-op-mod-0x0-failed-when-using-mlx5-poll-mode-driver/205941',\n",
       " 'https://forums.developer.nvidia.com/t/connectx-7-and-vsphere-8-0-beta/229827/4',\n",
       " 'https://forums.developer.nvidia.com/t/does-nvidia-msn2100-support-lossless-network-priority-based-flow-control-explicit-congestion-notification/223021',\n",
       " 'https://forums.developer.nvidia.com/t/ibv-reg-mr-hang/241916',\n",
       " 'https://forums.developer.nvidia.com/t/how-can-i-get-the-statistics-of-dcqcn-in-cx5-cards/206994/7',\n",
       " 'https://forums.developer.nvidia.com/t/cumulus-linux-os/227394',\n",
       " 'https://forums.developer.nvidia.com/t/configuring-mellanox-hardware-for-vpi-operation-is-now-available-on-mellanox-com/208120',\n",
       " 'https://forums.developer.nvidia.com/t/release-notes-for-nvidia-bright-cluster-manager-9-0-20/248751',\n",
       " 'https://forums.developer.nvidia.com/t/unable-to-ssh-into-bluefield-after-separated-host-mode/219128',\n",
       " 'https://forums.developer.nvidia.com/t/gbswitch001-error-timeout-after-2500000-s-and-3-retries/258085/1',\n",
       " 'https://forums.developer.nvidia.com/t/how-to-do-remote-firmware-updates-on-windows/206070',\n",
       " 'https://forums.developer.nvidia.com/t/ipsec-encryption-across-dark-fibre-with-two-connectx-6-dx-en/226859',\n",
       " 'https://forums.developer.nvidia.com/t/nfs-rdma-on-centos-7-small-files-corruption-by-memory-leak/205886',\n",
       " 'https://forums.developer.nvidia.com/t/ubuntu-20-04-2-mlnx-ofed-installation-troubles/205929',\n",
       " 'https://forums.developer.nvidia.com/t/nvme-tcp-via-mellanox-nic/208361',\n",
       " 'https://forums.developer.nvidia.com/t/maximum-mtu-on-sn2410-running-sonic/219383',\n",
       " 'https://forums.developer.nvidia.com/t/vma-usage/239956/4',\n",
       " 'https://forums.developer.nvidia.com/t/when-run-ib-write-bw-with-2g-msg-mlx5-reports-0x68-why/262168/2',\n",
       " 'https://forums.developer.nvidia.com/t/release-notes-for-nvidia-bright-cluster-manager-8-2-29/237196',\n",
       " 'https://forums.developer.nvidia.com/t/failed-to-get-crypto-operational-regis/258580',\n",
       " 'https://forums.developer.nvidia.com/t/forward-to-sft-ipv4-udp-failed-error-no-pmd-support-for-sft/195144',\n",
       " 'https://forums.developer.nvidia.com/t/rdma-is-not-activated-on-my-mellanox-connectx-2-card/253875/3',\n",
       " 'https://forums.developer.nvidia.com/t/i-cant-install-mlnx-ofex-linux-5-2-2-2-0-0-4-15-0-20-generic/206174/3',\n",
       " 'https://forums.developer.nvidia.com/t/install-ufm-on-rocky-linux-8-5/229394/3',\n",
       " 'https://forums.developer.nvidia.com/u/longranw',\n",
       " 'https://forums.developer.nvidia.com/t/nvme-driver-hang/249435/3',\n",
       " 'https://forums.developer.nvidia.com/t/gpudirect-in-pci-passthrough-configuration/197211/7',\n",
       " 'https://forums.developer.nvidia.com/t/ipsec-full-offload-connectx-6-dx-upstream-linux/251677',\n",
       " 'https://forums.developer.nvidia.com/t/can-not-run-test-regex-on-host/210500',\n",
       " 'https://forums.developer.nvidia.com/t/how-push-full-100gbps-port/227993/7',\n",
       " 'https://forums.developer.nvidia.com/t/where-to-get-mlx5-ifc-h-and-device-h-for-windows-10-compile/206018/1',\n",
       " 'https://forums.developer.nvidia.com/t/i-am-using-hpcx-and-am-having-trouble-using-fortran-with-the-precompiled-fortran-mpi-mod-any-attempt-to-import-the-module-file-mpi-mod-produces-an-error-is-there-a-way-to-use-fortran-with-newer-or-different-i-e-ifort-compilers/205935',\n",
       " 'https://forums.developer.nvidia.com/t/issues-with-a-static-mac-on-vxlan-access-port/223934/1',\n",
       " 'https://forums.developer.nvidia.com/t/need-onyx-3-8-2204/218190',\n",
       " 'https://forums.developer.nvidia.com/t/automating-cpu-pinning-for-offload-worker-queue-in-hwol-conntrack-offload-environment/258678',\n",
       " 'https://forums.developer.nvidia.com/t/we-have-a-sn2100-switch-with-p-n-sn2100-bb2f-we-want-to-connect-with-switch-with-mellanox-25gbps-lan-card-25gbe-2-port-sfp28-network-adapter-mellanox-cx4/206170/2',\n",
       " 'https://forums.developer.nvidia.com/t/hello-everyone-i-have-a-question-regarding-the-programmable-pipeline-for-new-network-flows-feature-of-connectx-6-smartnic-i-want-to-know-how-this-feature-can-be-manipulated/205950',\n",
       " 'https://forums.developer.nvidia.com/t/cifs-error-alma-linux-8-7/260780',\n",
       " 'https://forums.developer.nvidia.com/t/number-of-svlans-and-cvlans-on-connect-x-5-connect-x-6/251684',\n",
       " 'https://forums.developer.nvidia.com/t/failed-to-run-applications-on-host-side-error-failed-to-start-port-failed-init-port-0/216072/6',\n",
       " 'https://forums.developer.nvidia.com/t/number-of-svlans-and-cvlans-on-connect-x-5-connect-x-6/251684/2',\n",
       " 'https://forums.developer.nvidia.com/t/fix-build-drivers-for-4-9-6-0-6-4-18-0-425-10-1-el8-7-x86-64-in-rhel-8-7-failing-because-of-brp-mangle-shebangs/243106/2',\n",
       " 'https://forums.developer.nvidia.com/t/is-there-any-document-to-explain-how-to-understand-the-meaning-of-every-devx-message-field/255706/2',\n",
       " 'https://forums.developer.nvidia.com/t/opensm-failure-after-reboot-stuck-on-port-initialization/206144',\n",
       " 'https://forums.developer.nvidia.com/t/does-connectx-6-dx-card-support-tls-offloading-with-aes256-and-tls-1-3/246725',\n",
       " 'https://forums.developer.nvidia.com/u/heinz1',\n",
       " 'https://forums.developer.nvidia.com/t/softroce-on-1-or-2-linux-ubuntu-workstations-without-mellanox-nic/253873',\n",
       " 'https://forums.developer.nvidia.com/t/rdma-latency-stability-and-pcie-version-recognize/251349/3',\n",
       " 'https://forums.developer.nvidia.com/t/unable-to-create-send-cq-of-size-5080-on-mlx4-0-cannot-allocate-memory/205973',\n",
       " 'https://forums.developer.nvidia.com/t/ets-features-are-not-supported-on-your-system/205909',\n",
       " 'https://forums.developer.nvidia.com/t/ips-app-not-receiving-response-data-from-host/210609/2',\n",
       " 'https://forums.developer.nvidia.com/t/connectx-6-dx-crypto-and-secure-boot-xdp-hardware-offload-not-possible-because-of-ipsec/206198/2',\n",
       " 'https://forums.developer.nvidia.com/t/is-there-a-trial-version-of-netq-available/182140',\n",
       " 'https://forums.developer.nvidia.com/t/dell-r620-or-r630-compatibility-with-connectx-3-infiniband-mcx354a-fcbt/206044',\n",
       " 'https://forums.developer.nvidia.com/tag/kernel',\n",
       " 'https://forums.developer.nvidia.com/t/mcx4121a-acat-pnic-disappearing-affecting-multiple-systems-running-esxi-7-0u1/206076',\n",
       " 'https://forums.developer.nvidia.com/t/how-to-remove-a-memory-range-added-with-doca-mmap-populate/224092',\n",
       " 'https://forums.developer.nvidia.com/t/driver-not-loading-on-host/236265/4',\n",
       " 'https://forums.developer.nvidia.com/t/ofed-5-8-on-rhel-rocky-8-8/255017/5',\n",
       " 'https://forums.developer.nvidia.com/t/where-should-i-get-the-documentation-about-the-new-layer-api-on-connectx-6-dx/233747',\n",
       " 'https://forums.developer.nvidia.com/t/help-identifying-card/209206',\n",
       " 'https://forums.developer.nvidia.com/t/find-the-mib-file-of-sonic-sn3700c/258966/2',\n",
       " 'https://forums.developer.nvidia.com/t/virtualized-development-environment-for-users-without-a-dpu/196023/1',\n",
       " 'https://forums.developer.nvidia.com/t/support-for-kernel-5-10/206067',\n",
       " 'https://forums.developer.nvidia.com/t/where-are-instructions-on-how-to-use-kubernetes-with-sr-iov/206256/3',\n",
       " 'https://forums.developer.nvidia.com/t/no-rx-with-vlan-filter-offload/230994',\n",
       " 'https://forums.developer.nvidia.com/t/obtaining-a-dpu-to-develop-and-test-doca-applications/174725/1',\n",
       " 'https://forums.developer.nvidia.com/t/can-i-deploy-doca-on-a-arm-host-server/205348',\n",
       " 'https://forums.developer.nvidia.com/t/bluefield-not-reachable-from-the-host-after-installing-cuda-on-the-bluefield/241880',\n",
       " 'https://forums.developer.nvidia.com/t/bonding-teaming-over-multiple-adapters/205993/4',\n",
       " 'https://forums.developer.nvidia.com/t/which-cable-to-connect-connectx-7-with-broadcom-p2100g/247233/2',\n",
       " 'https://forums.developer.nvidia.com/t/how-many-lines-of-code-does-physx-consist-of/208920',\n",
       " 'https://forums.developer.nvidia.com/t/what-is-the-difference-between-dpi-compiler-and-rxp-compiler/209628/2',\n",
       " 'https://forums.developer.nvidia.com/t/sniff-roce-traffic-using-tcpdump/206086',\n",
       " 'https://forums.developer.nvidia.com/t/connectx-3-permission-denied-when-reducing-flexboot-menu-timeout/233656/2',\n",
       " 'https://forums.developer.nvidia.com/t/sockets-and-packet-generation-with-doca/260095/1',\n",
       " 'https://forums.developer.nvidia.com/t/can-i-use-the-infiniband-switch-as-a-daisy-chain/210183',\n",
       " 'https://forums.developer.nvidia.com/t/how-to-simulate-a-dpu/240974/1',\n",
       " 'https://forums.developer.nvidia.com/t/how-can-i-obtain-the-real-rate-limit-of-qp/234328/5',\n",
       " 'https://forums.developer.nvidia.com/t/doca-sample-compile-error/247190/5',\n",
       " 'https://forums.developer.nvidia.com/t/hardware-vdpa-offloading/256850',\n",
       " 'https://forums.developer.nvidia.com/t/high-availability-of-vf-based-roce-protocol/238702',\n",
       " 'https://forums.developer.nvidia.com/t/connectx4-on-freebsd11-4/205961/3',\n",
       " 'https://forums.developer.nvidia.com/t/cannot-create-vfs-on-one-pf-but-can-create-on-another/208561',\n",
       " 'https://forums.developer.nvidia.com/t/how-to-install-ofed-umd-in-docker-and-ofed-kmd-only-in-host-for-ofed-4-9/235570',\n",
       " 'https://forums.developer.nvidia.com/t/installed-mellnox-winof2-but-powershell-modukles-not-available-on-install/205911',\n",
       " 'https://forums.developer.nvidia.com/t/how-to-find-and-set-the-maximum-some-of-parametrs-for-a-nic-connectx-5-with-exaple/258154/6',\n",
       " 'https://forums.developer.nvidia.com/t/10-25-gbps-mellanox-nic-with-linux-nat-offload-support/205959',\n",
       " 'https://forums.developer.nvidia.com/t/connectx-vpi-operating-system-compatibility/205990',\n",
       " 'https://forums.developer.nvidia.com/t/ats-and-pri-support-on-connectx-6-dx-en/210233/6',\n",
       " 'https://forums.developer.nvidia.com/t/download-enterprise-mib-files/210549',\n",
       " 'https://forums.developer.nvidia.com/t/no-ping-with-connectx-5-vpi-socket-direct-on-esxi-7-0-u2/210317/3',\n",
       " 'https://forums.developer.nvidia.com/t/can-we-use-dpdk-in-cumulus-linux/181146/1',\n",
       " 'https://forums.developer.nvidia.com/t/nat-with-cumulus/208416',\n",
       " 'https://forums.developer.nvidia.com/t/obtaining-a-dpu-to-develop-and-test-doca-applications/174725',\n",
       " 'https://forums.developer.nvidia.com/t/degraded-performance-with-drivers-version-5-vs-version-4/206221',\n",
       " 'https://forums.developer.nvidia.com/t/cumulus-linux-bridge-vlan-configuration-question/247613',\n",
       " 'https://forums.developer.nvidia.com/t/cumulus-linux-lacp-modes/247605/3',\n",
       " 'https://forums.developer.nvidia.com/t/mt25408a0-fcc-qi-connectx-failed-to-identify-the-device-can-not-create-signaturemanager/220350',\n",
       " 'https://forums.developer.nvidia.com/t/98gb-s-rdma-but-slow-speeds-in-win-11-with-100g-network/254958/3',\n",
       " 'https://forums.developer.nvidia.com/t/installing-mlnx-ofed-linux-5-6-2-0-9-0-rhel7-9-x86-64-on-centos-7-9-5-with-kernel-version-3-10-0-1160-66-1-el7-x86-64/219178/3',\n",
       " 'https://forums.developer.nvidia.com/t/installing-ofed-on-centos-stream-8-0/206169',\n",
       " 'https://forums.developer.nvidia.com/t/im-getting-low-throughput-with-mpls-tagged-packets-is-there-something-that-can-be-tuned-to-improve-performance/206278/4',\n",
       " 'https://forums.developer.nvidia.com/t/what-is-the-theoretical-throughput-with-single-port-100gbe-mellanox-nic/254037/3',\n",
       " 'https://forums.developer.nvidia.com/t/osm-sa-mad-ctrl-unbind-err-1a11-no-previous-bind-error-messages-are-logged-suddenly/206232',\n",
       " 'https://forums.developer.nvidia.com/t/how-to-disable-the-connectx-on-mnv303611a-edlt/254148',\n",
       " 'https://forums.developer.nvidia.com/t/how-does-the-mlx5-driver-use-the-iova-generated-by-iommu-as-dma-address/249785',\n",
       " 'https://forums.developer.nvidia.com/t/we-want-to-install-mlnx-ofed-linux-4-7-3-2-9-0-ubuntu18-04-x86-64-tgz-on-ubuntu-18-04-5-but-cannot-install-ok-mlx5-is-there-anyone-know-this-issue-and-how-to-fix-it-we-want-to-use-verbs-exp-functions/206082/2',\n",
       " 'https://forums.developer.nvidia.com/t/oracle-linux-6-9-install-driver-happened-fault-failed-to-build-rshim1-8-rpm-how-can-i-resolve-it/253086/5',\n",
       " 'https://forums.developer.nvidia.com/t/how-to-use-connectx-6-dx-data-at-rest-feature/214115',\n",
       " 'https://forums.developer.nvidia.com/t/how-to-set-compiled-kernel-modules-as-persistant-like-lustre-client/231056/11',\n",
       " 'https://forums.developer.nvidia.com/t/connectx-3-in-windows-10-disable-jumbo-packets/205882',\n",
       " 'https://forums.developer.nvidia.com/t/mellanox-connectx-4-lx-made-in-india-legit/254160/5',\n",
       " 'https://forums.developer.nvidia.com/t/adding-nodes-to-jobqueus-slurm/260340',\n",
       " 'https://forums.developer.nvidia.com/t/ibverbs-on-windows/231574',\n",
       " 'https://forums.developer.nvidia.com/t/wake-on-lan-wol-mcx311a-xcat-not-supported/214479/8',\n",
       " 'https://forums.developer.nvidia.com/t/failed-to-run-ipsec-application-on-mbf2h332a-aenot-bluefield-2-p-series-dpu-card/218875',\n",
       " 'https://forums.developer.nvidia.com/t/what-is-the-thermal-design-power-tdp-for-the-mellanox-connectx-4-dual-port-vpi-100-gbps-infiniband-cards/205928/3',\n",
       " 'https://forums.developer.nvidia.com/t/cumulus-linux-gui/198045/1',\n",
       " 'https://forums.developer.nvidia.com/t/how-does-the-mlx5-driver-use-the-iova-generated-by-iommu-as-dma-address/249452/5',\n",
       " 'https://forums.developer.nvidia.com/t/mlx5-core-bad-op-in-xdpsq/254906/3',\n",
       " 'https://forums.developer.nvidia.com/t/connectx6-socket-direct-and-vmware/205995/2',\n",
       " 'https://forums.developer.nvidia.com/t/release-notes-for-nvidia-bright-cluster-manager-9-2-9/243479/2',\n",
       " 'https://forums.developer.nvidia.com/t/what-is-the-root-cause-of-packet-loss-in-the-processing-flow-of-the-function-mlx5-rx-err-handle-dpdk-mlx5-pmd/242198',\n",
       " 'https://forums.developer.nvidia.com/t/dmesg-flooded-with-ofed-related-warnings/205977/2',\n",
       " 'https://forums.developer.nvidia.com/t/problem-with-ptp-in-a-port-channel-on-mellanox-sn2010-sn2100-switches/249029',\n",
       " 'https://forums.developer.nvidia.com/t/mlx5-pcie-event-detected-insufficient-power-on-the-pcie-slot-27w/249240/4',\n",
       " 'https://forums.developer.nvidia.com/t/cannot-run-rxpbench-on-host/235322/2',\n",
       " 'https://forums.developer.nvidia.com/t/application-will-only-function-with-2-ports-num-of-ports-0/209201',\n",
       " 'https://forums.developer.nvidia.com/t/install-doca/184986',\n",
       " 'https://forums.developer.nvidia.com/t/url-filter-system-design-ovs/202727',\n",
       " 'https://forums.developer.nvidia.com/t/msn3700c-bios-password-protected/206234',\n",
       " 'https://forums.developer.nvidia.com/t/100gbase-sr2-supported-pcie-ethernet-adapter/211102',\n",
       " 'https://forums.developer.nvidia.com/t/release-notes-for-nvidia-bright-cluster-manager-9-2-6/232331',\n",
       " 'https://forums.developer.nvidia.com/t/connection-aborted-in-secure-channel-application/232413/1',\n",
       " 'https://forums.developer.nvidia.com/t/cumulus-and-debian-updates/211450',\n",
       " 'https://forums.developer.nvidia.com/t/modifying-making-custom-ubuntu-image-backups/205160/2',\n",
       " 'https://forums.developer.nvidia.com/t/rtnetlink-error-on-loading-xdp-program-to-connectx-5/206228',\n",
       " 'https://forums.developer.nvidia.com/t/connectx-7-ethernet-linux-support/229698/6',\n",
       " 'https://forums.developer.nvidia.com/t/getting-slow-speeds-on-connectx-4-lx/259993',\n",
       " 'https://forums.developer.nvidia.com/t/about-create-srq-by-devx-command/252690',\n",
       " 'https://forums.developer.nvidia.com/t/mlx5-ib-problem-whilst-installing-mlnx-ofed-kernel-dkms/206161',\n",
       " 'https://forums.developer.nvidia.com/t/where-are-the-mellanox-support-articles/208396/2',\n",
       " 'https://forums.developer.nvidia.com/t/hello-there-is-there-a-possibility-to-tag-a-mirror-port-with-a-vlan-id-like-to-combine-several-mirror-sessions-onto-one-port-but-would-liek-to-be-able-to-identify-the-traffic-from-which-port-it-came-thank-you-for-your-help-thomas/206006/2',\n",
       " 'https://forums.developer.nvidia.com/t/how-to-configure-tos-in-rss-rule-using-ethtool/228801/1',\n",
       " 'https://forums.developer.nvidia.com/t/bad-mad-status/260657/3',\n",
       " 'https://forums.developer.nvidia.com/t/read-the-sonic-202012-user-guide/182139/4',\n",
       " 'https://forums.developer.nvidia.com/t/unable-to-enable-decompression-in-rx/246714',\n",
       " 'https://forums.developer.nvidia.com/u/attilla',\n",
       " 'https://forums.developer.nvidia.com/t/mellanox-ofed-drivers-for-ubuntu-20-04/212507',\n",
       " 'https://forums.developer.nvidia.com/t/app-shield-doesnt-work/242023/1',\n",
       " 'https://forums.developer.nvidia.com/t/can-i-install-cuda-directly-on-my-bluefield-2/255341/1',\n",
       " 'https://forums.developer.nvidia.com/t/connectx5-rdma-cm-event-addr-error-error-110/219333/1',\n",
       " 'https://forums.developer.nvidia.com/t/rdma-host-device-performance-during-external-network-communications/193379',\n",
       " 'https://forums.developer.nvidia.com/t/connectx-4-5-6-configuring-disable-vlan-stripping-in-windows/261039',\n",
       " 'https://forums.developer.nvidia.com/t/im-experiencing-a-performance-issue-on-connectx5-ex-cards-device-id-0x1019-in-the-form-of-a-limit-of-the-packet-rate-to-around-6mpps-with-production-internet-traffic/205902/3',\n",
       " 'https://forums.developer.nvidia.com/t/in-switch-multiple-entry-showing-for-lldp-output-for-single-interface-which-is-having-mellanox-mlx5-interface-also-mac-address-showing-instead-of-host-name-of-neighbor/206060/2',\n",
       " 'https://forums.developer.nvidia.com/t/mpi-only-using-1-port-on-dual-port-ib-nic/228715/3',\n",
       " 'https://forums.developer.nvidia.com/t/on-what-switches-is-sonic-supported/182142/1',\n",
       " 'https://forums.developer.nvidia.com/t/trying-to-put-a-bluefield-2-into-bluefield-x-mode-with-power-cycle/241492/3',\n",
       " 'https://forums.developer.nvidia.com/t/doca-1-0-is-available-now/174730/1',\n",
       " 'https://forums.developer.nvidia.com/t/building-openmpi-with-ucx-general-advice/205921',\n",
       " 'https://forums.developer.nvidia.com/t/rdma-performance-improvement-with-vma-offload/206213',\n",
       " 'https://forums.developer.nvidia.com/t/nvidia-gpudirect-on-bluefield2/217939/2',\n",
       " 'https://forums.developer.nvidia.com/t/hi-how-to-recompile-openvswitch-and-mlnx-dpdk-with-debuginfo-in-mlnx-ofed-version-5-4-1-0-3-0-on-centos7-6/206041/2',\n",
       " 'https://forums.developer.nvidia.com/t/wol-wake-on-lan-for-mellanox-connectx-4-lx-mcx4121a-acat/256054/8',\n",
       " 'https://forums.developer.nvidia.com/t/hi-i-am-trying-to-download-updated-drivers-for-my-switches-and-adapters-but-the-web-page-does-not-work-correctly/206192/3',\n",
       " 'https://forums.developer.nvidia.com/t/connectx-5-cards-network-interface-disappeared-and-openibd-daemon-failed-on-several-nodes/257946',\n",
       " 'https://forums.developer.nvidia.com/t/connectx-5-rss-maps-same-hash-to-different-cores/215582/5',\n",
       " 'https://forums.developer.nvidia.com/u/cerotyki',\n",
       " 'https://forums.developer.nvidia.com/u/TinkerFrank',\n",
       " 'https://forums.developer.nvidia.com/t/connextx5-tc-flower-offload-between-ports-on-dual-port-nic/232890',\n",
       " 'https://forums.developer.nvidia.com/t/skyway-appliance/246457/5',\n",
       " 'https://forums.developer.nvidia.com/t/doca-example/243096/2',\n",
       " 'https://forums.developer.nvidia.com/t/mellanox-nic-didnt-link-with-qsfp-cable/206224',\n",
       " 'https://forums.developer.nvidia.com/t/nvidia-mellanox-mcx512a-acat-connectx-5-en-network-interface-card/252913/7',\n",
       " 'https://forums.developer.nvidia.com/t/verbs-exp-h-no-such-file-or-directory/206300',\n",
       " 'https://forums.developer.nvidia.com/t/can-the-mcx515a-ccat-which-has-1-qsfp-port-be-setup-in-a-loopback-mode/236676',\n",
       " 'https://forums.developer.nvidia.com/t/mlx5-crypto-not-enough-capabilities-to-support-crypto-operations-maybe-old-fw-ofed-version/235977/5',\n",
       " 'https://forums.developer.nvidia.com/t/cant-connect-to-new-sn2010-via-console-cable/208858',\n",
       " 'https://forums.developer.nvidia.com/t/is-it-possible-to-split-100g-sr4-fiber-in-4-25g-virtual-devices/206090/2',\n",
       " 'https://forums.developer.nvidia.com/t/another-backend-already-attached/190192/1',\n",
       " 'https://forums.developer.nvidia.com/t/ib-write-bw-does-not-go-beyond-20g-s-when-using-average-packet-size-512/206002/4',\n",
       " 'https://forums.developer.nvidia.com/t/link-problem-between-sfp28-and-sfp/248557/2',\n",
       " 'https://forums.developer.nvidia.com/t/what-is-the-difference-between-a-vf-and-sf/198835',\n",
       " 'https://forums.developer.nvidia.com/t/mellanox-onyx-inter-vrf-routing-possible/205925',\n",
       " 'https://forums.developer.nvidia.com/t/sx6036-switchs-and-hdr-cables/204276',\n",
       " 'https://forums.developer.nvidia.com/t/can-the-interleaved-memory-regions-umr-provided-by-the-mlnx-ofed-mlx5dv-library-be-used-for-raw-packet-qps/205937/1',\n",
       " 'https://forums.developer.nvidia.com/t/ipsec-openvpn-on-cumulus-linux/206051/2',\n",
       " 'https://forums.developer.nvidia.com/t/connectx-3-cx311a-xcat-for-win11-crashed/205962',\n",
       " 'https://forums.developer.nvidia.com/t/ipsec-full-offload-connectx-6-dx-upstream-linux/251677/2',\n",
       " 'https://forums.developer.nvidia.com/t/write-result-is-incorrect-with-nvmeof-target-offloading/230175',\n",
       " 'https://forums.developer.nvidia.com/t/mellanox-connect-x-6-dx-virtio-and-vdpa-with-proxmox-kvm/206096',\n",
       " 'https://forums.developer.nvidia.com/t/how-to-aggregate-cx5-vfs-bandwidth-in-kvm/206269',\n",
       " 'https://forums.developer.nvidia.com/t/fw-upgrade-for-connectx4-lx-gigabyte-lom/205996',\n",
       " 'https://forums.developer.nvidia.com/u/strandeangil',\n",
       " 'https://forums.developer.nvidia.com/t/dpdk-start-app-mlx5-common-failed-to-create-tis-using-devx/205944/3',\n",
       " 'https://forums.developer.nvidia.com/t/ipoib-enhanced-mode-issue/242444/3',\n",
       " 'https://forums.developer.nvidia.com/t/can-the-mcx515a-ccat-which-has-1-qsfp-port-be-setup-in-a-loopback-mode/236676/3',\n",
       " 'https://forums.developer.nvidia.com/t/doca-sample-app-error/239419',\n",
       " 'https://forums.developer.nvidia.com/t/how-to-develop-applications-with-nvidia-bluefield-dpu-and-nvidia-doca-libraries/215347/2',\n",
       " 'https://forums.developer.nvidia.com/t/my-problem-is-that-there-is-a-problem-when-i-compile-the-ibdump-source-file-and-run-the-ibdump-executable-file-the-ib-network-adapter-cannot-be-captured-after-modifying-the-ibdump-source-file-as-prompted-it-still-cannot-be-used-ibdump-of-the-driver-c/205954/2',\n",
       " 'https://forums.developer.nvidia.com/t/mellanox-connectx-2-mnpa19-xtr-driver-for-windows-10/213234',\n",
       " 'https://forums.developer.nvidia.com/t/how-to-collect-device-proprietary-counters-in-linux/237383/2',\n",
       " 'https://forums.developer.nvidia.com/t/sn2010m-switches-high-pause-discards-packets/234326/12',\n",
       " 'https://forums.developer.nvidia.com/t/dpdk-rte-flow-is-degrading-performance-when-testing-on-connect-x5-100g-en-100g/206892',\n",
       " 'https://forums.developer.nvidia.com/t/mlx5-vdpa-disagrees-about-version-error-after-upgrading-mlnx-ofed-driver/259247',\n",
       " 'https://forums.developer.nvidia.com/t/about-doca-flow/255021',\n",
       " 'https://forums.developer.nvidia.com/t/examine-sfp-qsfp-model-and-optical-quality/220656',\n",
       " 'https://forums.developer.nvidia.com/t/mofed-5-2-installer-fails-after-ubuntu-kernel-updated-from-5-4-0-62-to-5-4-0-65/206208/3',\n",
       " 'https://forums.developer.nvidia.com/t/connectx-4-lx-very-slow-on-windows-10-same-speed-as-an-intel-1gb-using-rj45/206218',\n",
       " 'https://forums.developer.nvidia.com/t/switch-hpe-3180-fm-with-cumulus-os/227384',\n",
       " 'https://forums.developer.nvidia.com/t/connectx6-dpdk-dpdk-testpmd-receive-tcp-udp-mixed-flow-performance-is-very-low/205903/3',\n",
       " 'https://forums.developer.nvidia.com/t/typeerror-servo-object-does-not-support-item-assignment/248337/1',\n",
       " 'https://forums.developer.nvidia.com/t/write-result-is-incorrect-with-nvmeof-target-offloading/230175/4',\n",
       " 'https://forums.developer.nvidia.com/t/fail-to-run-doca-flow-samples-on-dpu/260949/2',\n",
       " 'https://forums.developer.nvidia.com/t/max-clock-info-update-nsec-field-in-mlx5dv-h-seems-in-us-rather-then-ns-unit/253100',\n",
       " 'https://forums.developer.nvidia.com/t/sx6012-igmp-snooping-warning/206209',\n",
       " 'https://forums.developer.nvidia.com/t/sniffer-tool-that-captures-high-speed-roce-traffic-for-linux/206130',\n",
       " 'https://forums.developer.nvidia.com/t/dpdk-testpmd-no-packets-are-exchanged/256812/2',\n",
       " 'https://forums.developer.nvidia.com/t/can-i-create-flow-matching-rules-for-ethernet-layers-above-l4-using-mlnx-ofed/205963',\n",
       " 'https://forums.developer.nvidia.com/t/connect-linux-ptpd-to-connectx-4-port/224130',\n",
       " 'https://forums.developer.nvidia.com/t/configure-connectx-with-the-mlxreg-mcra-utility/256706',\n",
       " 'https://forums.developer.nvidia.com/t/switching-between-separated-and-embedded-mode/184230/4',\n",
       " 'https://forums.developer.nvidia.com/t/lro-on-hairpin-queue/257963',\n",
       " 'https://forums.developer.nvidia.com/t/ovs-cannot-detect-the-bluefield-interface-but-the-bluefield-card-works-correctly/212885/9',\n",
       " 'https://forums.developer.nvidia.com/t/connectx-5-rss-maps-same-hash-to-different-cores/215582',\n",
       " 'https://forums.developer.nvidia.com/t/nvidia-native-esxi-drivers-management-tool-nmlxcli-v1-17-14-2-are-not-downloadable/256829',\n",
       " 'https://forums.developer.nvidia.com/t/how-to-connect-hdr-switch-to-older-adapter/206176/2',\n",
       " 'https://forums.developer.nvidia.com/t/sn2100-with-sonic/251273/4',\n",
       " 'https://forums.developer.nvidia.com/t/how-to-do-l2-east-west-traffic-with-ipl-and-mlag/206263',\n",
       " 'https://forums.developer.nvidia.com/t/result-of-write-is-incorrect-with-connectx-6-nvmeof-target-offloading/230444/4',\n",
       " 'https://forums.developer.nvidia.com/t/ptp-on-sriov-vf-connect-x-6-dx/240097/2',\n",
       " 'https://forums.developer.nvidia.com/t/mlx5muxtool-is-lacp-possible-on-windows/232592',\n",
       " 'https://forums.developer.nvidia.com/u/spruitt',\n",
       " 'https://forums.developer.nvidia.com/t/disable-roce-icrc-validation/239825',\n",
       " 'https://forums.developer.nvidia.com/t/developer-download-nvidia-com-connect-problem/188070/1',\n",
       " 'https://forums.developer.nvidia.com/u/shepard.siegel',\n",
       " 'https://forums.developer.nvidia.com/t/unable-to-get-mellanox-connectx-5-to-connect-to-network/218972',\n",
       " 'https://forums.developer.nvidia.com/t/resizable-bar-configuration/260028',\n",
       " 'https://forums.developer.nvidia.com/t/mlx-completion-with-error/257118/3',\n",
       " 'https://forums.developer.nvidia.com/t/opensm-failure-after-reboot-stuck-on-port-initialization/206144/4',\n",
       " 'https://forums.developer.nvidia.com/t/connectx-4-speed-issue-esxi6-7/212428',\n",
       " 'https://forums.developer.nvidia.com/t/bf2-dpu-shows-unclaimed/204687/3',\n",
       " 'https://forums.developer.nvidia.com/t/nvidia-doca-east-west-overlay-encryption-reference-application/193732',\n",
       " 'https://forums.developer.nvidia.com/t/driver-v4-9-5-1-0-module-verification-failed-signature-and-or-required-key-missing-tainting-kernel/227206/7',\n",
       " 'https://forums.developer.nvidia.com/t/failed-to-burn-an-image-onto-innova-2-flex-fpga/254396/17',\n",
       " 'https://forums.developer.nvidia.com/t/maximum-number-of-tc-flow-offloads-in-bluefield-ii/205938',\n",
       " 'https://forums.developer.nvidia.com/t/after-switch-migration-mlag-keeps-showing-peering-how-should-i-fix-it/254852',\n",
       " 'https://forums.developer.nvidia.com/t/cannot-upgrade-l4t/216189/9',\n",
       " 'https://forums.developer.nvidia.com/t/openibd-service-is-failed-on-a-particular-node/206187',\n",
       " 'https://forums.developer.nvidia.com/t/how-to-connect-infiniband-qsfp28-switch-with-rj45/239241/3',\n",
       " 'https://forums.developer.nvidia.com/t/mlnx-qos-enables-pfc-and-the-pfc-function-is-automatically-disabled-after-a-period-of-time/231773/5',\n",
       " 'https://forums.developer.nvidia.com/t/install-doca/184986/3',\n",
       " 'https://forums.developer.nvidia.com/t/how-to-reset-factory-defaults-switch-ib-sb7800/206089/2',\n",
       " 'https://forums.developer.nvidia.com/t/how-can-i-calculate-rss-hash-value-from-c-code-to-reassemble-packet-and-send-them-to-the-same-process-packet-that-have-the-same-ip-port-arrive-to-the-same-queue-if-some-packet-are-ip-fragmented-packet-arrive-to-different-process-it-is-a-problem/205924',\n",
       " 'https://forums.developer.nvidia.com/t/release-notes-for-nvidia-bright-cluster-manager-9-1-16/244638',\n",
       " 'https://forums.developer.nvidia.com/t/error-the-current-mlnx-ofed-linux-is-intended-for-rhel7-2/209702',\n",
       " 'https://forums.developer.nvidia.com/t/no-rx-with-vlan-filter-offload/230994/4',\n",
       " 'https://forums.developer.nvidia.com/t/nvidia-persistenced-failed-to-query-nvidia-devices/261841',\n",
       " 'https://forums.developer.nvidia.com/t/cable-or-opensm-problem/205885/2',\n",
       " 'https://forums.developer.nvidia.com/t/ndr-switch-support-for-cx-6-cards/240346',\n",
       " 'https://forums.developer.nvidia.com/t/firmware-expansion-roms-and-driver-for-connectx-4-lx-on-system-with-uefi-bios/206160',\n",
       " 'https://forums.developer.nvidia.com/t/sx6018-please-help/207825',\n",
       " 'https://forums.developer.nvidia.com/t/security-updates-and-apt-mark-auto/208865/2',\n",
       " 'https://forums.developer.nvidia.com/t/release-notes-for-nvidia-bright-cluster-manager-9-2-12/256515',\n",
       " 'https://forums.developer.nvidia.com/t/mcx545b-ccun-installed-lspci-ethtool-do-not-display-any-info-about-the-device/205895/2',\n",
       " 'https://forums.developer.nvidia.com/t/hi-how-do-i-change-name-management-ip-and-description-of-sx1024-switch/205879',\n",
       " 'https://forums.developer.nvidia.com/t/ibv-reg-mr-hang/241916/4',\n",
       " 'https://forums.developer.nvidia.com/t/doca-examples/183047/17',\n",
       " 'https://forums.developer.nvidia.com/t/nfs-rdma-on-centos-7-small-files-corruption-by-memory-leak/205886/4',\n",
       " 'https://forums.developer.nvidia.com/t/apt-get-update-fails-in-bf2/239864',\n",
       " 'https://forums.developer.nvidia.com/t/latest-bluefield-2-os-with-doca/175712',\n",
       " 'https://forums.developer.nvidia.com/t/hgx-h100-ib-hdr-200-is-it-possible/262090/2',\n",
       " 'https://forums.developer.nvidia.com/t/performance-degradation-when-using-connectx-6-dx-to-transfer-udp-multicast-jumbo-frames/262039/2',\n",
       " 'https://forums.developer.nvidia.com/t/we-need-key-and-key-index-to-burn-fuse-for-secure-boot/193888/1',\n",
       " 'https://forums.developer.nvidia.com/t/issues-with-assigning-64-pkeys-for-each-server-in-cluster/250311/10',\n",
       " 'https://forums.developer.nvidia.com/t/does-bluefiled-2-support-ibv-atomic-glob/249340/7',\n",
       " 'https://forums.developer.nvidia.com/t/mlnx-qos-enables-pfc-and-the-pfc-function-is-automatically-disabled-after-a-period-of-time/231773',\n",
       " 'https://forums.developer.nvidia.com/t/mellanox-connect-x5-rdma-debugging/229991/5',\n",
       " 'https://forums.developer.nvidia.com/t/connectx-4-lx-very-slow-on-windows-10-same-speed-as-an-intel-1gb-using-rj45/206218/8',\n",
       " 'https://forums.developer.nvidia.com/t/behavior-of-mellanox-nic/250356',\n",
       " 'https://forums.developer.nvidia.com/t/ndivia-peer-mem-error-could-not-insert-nv-peer-mem-invalid-argument/256385/3',\n",
       " 'https://forums.developer.nvidia.com/t/magp-number-of-switch/254940/3',\n",
       " 'https://forums.developer.nvidia.com/t/is-it-possible-to-use-sriov-on-bf2-not-on-the-host/252625',\n",
       " 'https://forums.developer.nvidia.com/t/whats-the-proper-memory-region-access-flags-for-gpudirect-rdma/254251',\n",
       " 'https://forums.developer.nvidia.com/t/cc-mgr-not-present-in-all-lts-4-9-versions-of-mlnx-ofed/213914',\n",
       " 'https://forums.developer.nvidia.com/t/i-need-to-upgrade-the-firmware-in-a-mellanox-card-mt27800-they-are-installed-in-a-esxi-running-vpshere-6-7-u3-i-tried-to-install-the-mft-tool-but-i-have-this-error-l-could-not-find-a-trusted-signer-certificate-has-expired/206636/5',\n",
       " 'https://forums.developer.nvidia.com/t/performance-degradation-issue-when-applying-asap2-and-conntrack-offload-in-openstack-and-ovn-environment/256485',\n",
       " 'https://forums.developer.nvidia.com/t/mlx5-ib-issue-while-installing-mlnx-ofed-linux-5-1-0-6-6-0-to-ubuntu-16-04-lts-with-hwe-kernel/206155',\n",
       " 'https://forums.developer.nvidia.com/t/can-rdma-be-used-with-lacp/219669',\n",
       " 'https://forums.developer.nvidia.com/t/mellanox-sn2410-issues/205978/2',\n",
       " 'https://forums.developer.nvidia.com/t/delete-cl-support-files/181153/1',\n",
       " 'https://forums.developer.nvidia.com/t/doca-flow-inspector-service-cant-start-properly-on-bf-2/256442/2',\n",
       " 'https://forums.developer.nvidia.com/t/sr-iov-pps-limiting/260140',\n",
       " 'https://forums.developer.nvidia.com/t/roce-b-w-into-bf2-device-in-separated-mode/238801/3',\n",
       " 'https://forums.developer.nvidia.com/t/using-ethernet-and-rdma-simultaneously/206257',\n",
       " 'https://forums.developer.nvidia.com/t/infiniband-partitioning-in-rocev2/233417',\n",
       " 'https://forums.developer.nvidia.com/t/vlan-sub-interfaces/204597/7',\n",
       " 'https://forums.developer.nvidia.com/t/how-to-use-mellanox-connectx-4-lx/206074/3',\n",
       " 'https://forums.developer.nvidia.com/t/mellanox-crypto-drivers-not-working-with-dpdk/244615',\n",
       " 'https://forums.developer.nvidia.com/t/issue-using-the-media-receiver-to-display-2110-20-video/244660/3',\n",
       " 'https://forums.developer.nvidia.com/t/mellanox-sn2010-mlag-configure-issue/206045',\n",
       " 'https://forums.developer.nvidia.com/t/what-is-the-root-cause-of-packet-loss-in-the-processing-flow-of-the-function-mlx5-rx-err-handle-dpdk-mlx5-pmd/242198/3',\n",
       " 'https://forums.developer.nvidia.com/t/find-the-mib-file-of-sonic-sn3700c/258966',\n",
       " 'https://forums.developer.nvidia.com/t/mlxconfig-for-switches-in-non-default-ib-subnet/252105',\n",
       " 'https://forums.developer.nvidia.com/t/dgx-a100-when-8-ib-network-cards-use-ib-write-bw-to-test-the-bandwidth-at-the-same-time-the-rate-decreases-which-is-not-expected/255091/4',\n",
       " 'https://forums.developer.nvidia.com/t/release-notes-for-nvidia-bright-cluster-manager-9-0-19/238410',\n",
       " 'https://forums.developer.nvidia.com/t/ib-card-ports-are-down-or-polling/210428/3',\n",
       " 'https://forums.developer.nvidia.com/t/whats-the-modulator-chip-of-the-mma1t00-hs-200g-ib-transceiver/257321/3',\n",
       " 'https://forums.developer.nvidia.com/t/firmware-for-sb7800-edr-switch-ib-2/205949/3',\n",
       " 'https://forums.developer.nvidia.com/t/mellanox-neo-performance-provider-fails-to-start-in-version-2-7-20-also-in-2-7-10-how-to-fix/206046',\n",
       " 'https://forums.developer.nvidia.com/t/uneven-load-softirq/228216/4',\n",
       " 'https://forums.developer.nvidia.com/t/what-is-the-procedure-to-update-the-driver-for-mlx5-core/206194/2',\n",
       " 'https://forums.developer.nvidia.com/t/do-nic-virtual-functions-support-pxe-boot/210050/3',\n",
       " 'https://forums.developer.nvidia.com/t/arm-cortex-a78ae-schedulling/249599/1',\n",
       " 'https://forums.developer.nvidia.com/t/mellanox-connectx-4-driver-update-on-synology/251143',\n",
       " 'https://forums.developer.nvidia.com/t/howto-get-connectx-4-uefi-ib-working-with-dhcp/205922',\n",
       " 'https://forums.developer.nvidia.com/t/mlag-ipl-link/206139/2',\n",
       " 'https://forums.developer.nvidia.com/t/is-burning-a-rom-image-required-for-connectx5-to-pxe-boot-https-www-mellanox-com-related-docs-prod-software-mellanox-preboot-drivers-user-manual-v4-0-pdf/205955',\n",
       " 'https://forums.developer.nvidia.com/u/jounghoolee',\n",
       " 'https://forums.developer.nvidia.com/t/problem-with-enabling-gtp-flex-parser/252611/2',\n",
       " 'https://forums.developer.nvidia.com/t/vpi-gateway-license-for-sx6036-details/233204/2',\n",
       " 'https://forums.developer.nvidia.com/t/connection-between-two-infiniband-ports/207229',\n",
       " 'https://forums.developer.nvidia.com/t/is-mcx512a-acat-compatible-with-aspm/235957',\n",
       " 'https://forums.developer.nvidia.com/t/dpi-sample-applications/174732/6',\n",
       " 'https://forums.developer.nvidia.com/t/hello-everyone-i-have-one-ethernet-controller-0200-mellanox-technologies-mt27520-family-connectx-3-pro-15b3-1007-i-am-unable-to-bring-the-interface-up-i-have-four-other-machines-and-all-are-working-and-connecting-the-the-same-switch/206181',\n",
       " 'https://forums.developer.nvidia.com/t/release-notes-for-nvidia-bright-cluster-manager-9-2-6a/232536/2',\n",
       " 'https://forums.developer.nvidia.com/t/i-cannot-use-sdkmanager-to-install-doca/259461',\n",
       " 'https://forums.developer.nvidia.com/t/rivermax-sdk-example-code-run-failed/255548/6',\n",
       " 'https://forums.developer.nvidia.com/t/monitoring-app-issues-writes-after-get-log-failure/233419',\n",
       " 'https://forums.developer.nvidia.com/t/dpdk-pktgen-and-debian-11-with-connectx5-nic/206008/2',\n",
       " 'https://forums.developer.nvidia.com/t/sn2100m-virtuozzo-virtuozzo-storage-sds/206193',\n",
       " 'https://forums.developer.nvidia.com/t/sn2100m-virtuozzo-virtuozzo-storage-sds/206193/2',\n",
       " 'https://forums.developer.nvidia.com/t/cant-install-mlnx-ofed-kernel-dkms-mlnx-en-5-6-1-0-3-5-debian11-2-x86-64-in-proxmox-7-1/215055/3',\n",
       " 'https://forums.developer.nvidia.com/t/how-to-set-traffic-class-using-ibverbs/205966',\n",
       " 'https://forums.developer.nvidia.com/t/ethernet-controller-mellanox-technologies-device-1019-not-working-with-kernel-version-4-19-97-00189-g5dd402a-it-was-working-fine-with-kernel-3-14-25/206000',\n",
       " 'https://forums.developer.nvidia.com/t/mellanox-driver-reports-infiniband-mlx5-0-create-qp-pid-101966-create-qp-type-2-failed-message-at-higher-client-scale/206132',\n",
       " 'https://forums.developer.nvidia.com/t/nv-shield-pro-2019-connection-issues/247379/4',\n",
       " 'https://forums.developer.nvidia.com/t/lacp-on-mellanox-mlnx-os-msb7800-infiniband-switch/240747/7',\n",
       " 'https://forums.developer.nvidia.com/t/i-cant-install-mlnx-ofed-linux-5-2-2-2-0-0-4-15-0-139-generic-and-mlnx-ofed-linux-5-2-2-2-0-0-4-15-0-137-generic/206165',\n",
       " 'https://forums.developer.nvidia.com/t/does-the-switch-support-mac-vlan-functionality/252228/7',\n",
       " 'https://forums.developer.nvidia.com/t/sn2010m-switches-high-pause-discards-packets/234326',\n",
       " 'https://forums.developer.nvidia.com/t/rdma-programming-on-windows/231478',\n",
       " 'https://forums.developer.nvidia.com/t/hpc-x-for-mlnx-ofed-4-9-lts/219176/1',\n",
       " 'https://forums.developer.nvidia.com/t/got-local-protection-error-when-doing-ibv-wr-send/258886/4',\n",
       " 'https://forums.developer.nvidia.com/t/mlx5-ib-and-mlx5-core-cannot-load-in-mlnx-ofed-5-9-kernel-5-15/243181',\n",
       " 'https://forums.developer.nvidia.com/t/sn2010-basic-qdisc-with-hw-offload/246503/3',\n",
       " 'https://forums.developer.nvidia.com/t/vl15-dropped-portrcvswitchrelayeroors/248173',\n",
       " 'https://forums.developer.nvidia.com/t/hi-how-to-recompile-openvswitch-and-mlnx-dpdk-with-debuginfo-in-mlnx-ofed-version-5-4-1-0-3-0-on-centos7-6/206041',\n",
       " 'https://forums.developer.nvidia.com/t/using-simple-forward-vnf-but-gtp-support-is-not-enabled/260311/4',\n",
       " 'https://forums.developer.nvidia.com/t/storm-control-with-switchdev/241573/2',\n",
       " 'https://forums.developer.nvidia.com/t/creating-kubernetes-cluster-on-bcm-exception-version-of-the-local-path-provisioner-0-0-23-is-too-new/249582',\n",
       " 'https://forums.developer.nvidia.com/t/vxlan-woes/225978/1',\n",
       " 'https://forums.developer.nvidia.com/t/do-nvidia-have-any-connectx-6-cards-with-sma-for-pps-on-the-market/206078',\n",
       " 'https://forums.developer.nvidia.com/t/innova-2-flex-connectx-5-ethernet-to-fpga-direct-communication/236059',\n",
       " 'https://forums.developer.nvidia.com/t/mlx5dv-devx-obj-create-fails/258347',\n",
       " 'https://forums.developer.nvidia.com/t/ipl-both-switches-show-as-master/253892',\n",
       " 'https://forums.developer.nvidia.com/t/how-to-configure-a-redundant-ethernet-switch-setup-with-2-6036g-and-2-cisco-sg500x/205933',\n",
       " 'https://forums.developer.nvidia.com/t/what-switches-and-connectors-are-supported-in-cumulus-linux/181155/1',\n",
       " 'https://forums.developer.nvidia.com/t/mlx5-core-000000-0-poll-health-pid-0-devices-health-compromised/238770',\n",
       " 'https://forums.developer.nvidia.com/t/bluefield-2-image/191286/2',\n",
       " 'https://forums.developer.nvidia.com/t/does-the-mellanox-rnic-send-hardware-prefetch-instructions-to-the-memory-region/221087',\n",
       " 'https://forums.developer.nvidia.com/t/what-is-the-difference-between-mfs1s00-hxxxe-and-mfs1s00-hxxe-ll/218486',\n",
       " 'https://forums.developer.nvidia.com/t/bad-input-length-0x50-when-creating-dci-qp/258477',\n",
       " 'https://forums.developer.nvidia.com/t/tls-retransmission-issue-with-hw-offloading/259183',\n",
       " 'https://forums.developer.nvidia.com/t/rdma-without-switch-is-it-even-possible/206864/3',\n",
       " 'https://forums.developer.nvidia.com/t/infiniband-mtu-value-cannot-be-changed/238028',\n",
       " 'https://forums.developer.nvidia.com/t/how-to-create-a-n-w-interface-of-type-mlx5e-rep/255006/3',\n",
       " 'https://forums.developer.nvidia.com/t/mcx4121a-xcat-z590/208696',\n",
       " 'https://forums.developer.nvidia.com/t/opensm-and-rate-10/252777',\n",
       " 'https://forums.developer.nvidia.com/t/lro-on-dpdk-hairpin-queue/257982',\n",
       " 'https://forums.developer.nvidia.com/t/release-notes-for-nvidia-bright-cluster-manager-8-2-29/237196/2',\n",
       " 'https://forums.developer.nvidia.com/t/kernel-panic-seen-while-centos-8-3-linux-boot-with-the-adapter-mellanox-connectx-5-mcx516a-cdat/206133',\n",
       " 'https://forums.developer.nvidia.com/t/e-burning-encrypted-image-on-non-encrypted-device-is-not-allowed/229826',\n",
       " 'https://forums.developer.nvidia.com/t/innova2-flex-image-not-loading/206951',\n",
       " 'https://forums.developer.nvidia.com/t/updating-synchronizing-firmware-on-a-6036-and-6036g-switch/205927',\n",
       " 'https://forums.developer.nvidia.com/t/cumulus-vx-bgp-sessions-unstable-in-vrf-with-vlan-subinterfaces/204868',\n",
       " 'https://forums.developer.nvidia.com/t/rsocket-rsendto-fail/241920/2',\n",
       " 'https://forums.developer.nvidia.com/t/how-to-solve-the-problem-that-buffers-commands-are-not-supported-on-your-system/206085',\n",
       " 'https://www.nvidia.com/en-us/events/siggraph/?nvid=nv-int-unbr-710451#cid=sigg23_nv-int-unbr_en-us',\n",
       " 'https://forums.developer.nvidia.com/t/release-notes-for-nvidia-bright-cluster-manager-8-1-26/233587/2',\n",
       " 'https://forums.developer.nvidia.com/t/sn3700-running-sonic-onie/219162/1',\n",
       " 'https://forums.developer.nvidia.com/t/trying-to-run-nvme-over-ib-client-installed-ofed-5-1-2-5-8-0-on-rhel-8-3-kernel-4-18-0-240-el8-x86-64-using-mlnxofedinstall-with-nvmf-add-kernel-support-command-failed-to-load-nvme-rdma-driver-getting-a-bunch-of-22-errors-in-dmesg/206231',\n",
       " 'https://forums.developer.nvidia.com/t/disable-ddio-for-a-nic/206084',\n",
       " 'https://forums.developer.nvidia.com/t/sn2010-connectx-4lx-mlag-rdma-with-hyper-v-cable-recommendations-with-this-configuration/224454/1',\n",
       " 'https://forums.developer.nvidia.com/t/unable-to-use-bluefields-pka-engine-on-bluefield-2-smartnic-p-n-mbf2h516a-ceeot/205957/2',\n",
       " 'https://forums.developer.nvidia.com/t/mount-error-cifs-filesystem-not-supported-by-the-system/206019',\n",
       " 'https://forums.developer.nvidia.com/u/nikola.borisof',\n",
       " 'https://forums.developer.nvidia.com/t/mlx5-core-poll-health-raise-an-error-devices-health-compromised-reached-miss-count/237664/7',\n",
       " 'https://forums.developer.nvidia.com/t/cx-6-nof-offload-parameter-configuration/255187',\n",
       " 'https://forums.developer.nvidia.com/t/vgt-and-vlan-connectivity-on-dpdk19-11/233414/3',\n",
       " 'https://forums.developer.nvidia.com/t/connectx-3-one-way-traffic-to-switch-over-sfp-dac-10gbps-cx311a/254475',\n",
       " 'https://forums.developer.nvidia.com/t/where-are-instructions-on-how-to-use-kubernetes-with-sr-iov/206256',\n",
       " 'https://forums.developer.nvidia.com/t/modifying-making-custom-ubuntu-image-backups/205160',\n",
       " 'https://forums.developer.nvidia.com/t/running-a-dhcp-server-on-leaf-switches-configured-with-mlag/181152',\n",
       " 'https://forums.developer.nvidia.com/t/snmp-ifindex/206225',\n",
       " 'https://forums.developer.nvidia.com/t/listen-to-the-latest-kernel-of-truth-podcast-season-3-episode-2/181145',\n",
       " 'https://forums.developer.nvidia.com/t/vpi-gateway-license-for-sx6036-details/233204',\n",
       " 'https://forums.developer.nvidia.com/t/expanding-the-menus-in-documentation/181147',\n",
       " 'https://forums.developer.nvidia.com/t/install-doca-2-0-2-ubuntu2204-on-bluefield-2-failed/257630',\n",
       " 'https://forums.developer.nvidia.com/t/is-mlag-vip-required-to-form-an-sn2010-cluster/253747/4',\n",
       " 'https://forums.developer.nvidia.com/t/mlnxofedinstall-with-option-add-kernel-support-generate-unsigned-module/206103/2',\n",
       " 'https://forums.developer.nvidia.com/t/why-ibv-poll-cq-always-have-about-2-seconds-delay-before-available/206114',\n",
       " 'https://forums.developer.nvidia.com/t/add-almalinux-os/206113',\n",
       " 'https://forums.developer.nvidia.com/t/sn3700-running-sonic-onie/219162',\n",
       " 'https://forums.developer.nvidia.com/u/samerka',\n",
       " 'https://forums.developer.nvidia.com/t/about-the-performance-bottleneck-of-qm8790-switcher/250169/2',\n",
       " 'https://forums.developer.nvidia.com/t/what-is-the-procedure-to-update-the-driver-for-mlx5-core/206194',\n",
       " 'https://forums.developer.nvidia.com/t/pci-e-bus-errors-with-connectx-3-and-asus-x-99e-ws/207845/6',\n",
       " 'https://forums.developer.nvidia.com/t/how-to-acl-log/239031/2',\n",
       " 'https://forums.developer.nvidia.com/t/dgx-a100-when-8-ib-network-cards-use-ib-write-bw-to-test-the-bandwidth-at-the-same-time-the-rate-decreases-which-is-not-expected/255091',\n",
       " 'https://forums.developer.nvidia.com/t/nvidia-gtc-21-access-technical-training-and-sessions-built-for-developers/191530/3',\n",
       " 'https://forums.developer.nvidia.com/tag/networking',\n",
       " 'https://forums.developer.nvidia.com/t/how-to-limit-fan-rpm-on-sn2010/242723',\n",
       " 'https://forums.developer.nvidia.com/t/bluefield-2-es-platform-software/219598',\n",
       " 'https://forums.developer.nvidia.com/t/virtual-function-setup-failed/243211/3',\n",
       " 'https://forums.developer.nvidia.com/t/using-libvma-with-gpudirect-rdma/239854/3',\n",
       " 'https://forums.developer.nvidia.com/t/how-to-set-traffic-class-using-ibverbs/205966/2',\n",
       " 'https://forums.developer.nvidia.com/t/sx6036-with-vlans-pfc-not-working/206043',\n",
       " 'https://forums.developer.nvidia.com/t/can-i-closed-icrc-computing/206212',\n",
       " 'https://forums.developer.nvidia.com/t/random-delays-on-two-back-to-back-connected-systems-with-connectx-5-2x100g-cards/206024',\n",
       " 'https://forums.developer.nvidia.com/t/gid-table-limit/252815/2',\n",
       " 'https://forums.developer.nvidia.com/t/computex-nvidia-and-arm-s-data-center-ambitions-enabled-by-taiwan/179994/1',\n",
       " 'https://forums.developer.nvidia.com/t/python-udp-packets-not-showing-on-orin-board/256544/2',\n",
       " 'https://forums.developer.nvidia.com/t/a-doubt-about-mkpkc-tool/215500/6',\n",
       " 'https://forums.developer.nvidia.com/t/windows-failed-to-initialise-mellanox-mcx354a-fcbt/206167',\n",
       " 'https://forums.developer.nvidia.com/t/mellanox-sn2010-mlag-configure-issue/206045/4',\n",
       " 'https://forums.developer.nvidia.com/t/how-to-connect-sn-2100-switches-via-vxlan-over-a-campus-network/206237/2',\n",
       " 'https://forums.developer.nvidia.com/t/set-up-doca-apsh-system/202394',\n",
       " 'https://forums.developer.nvidia.com/t/how-to-install-ofed-4-6-1-on-centos-7-8/206248/3',\n",
       " 'https://forums.developer.nvidia.com/t/mlnx-ofed-linux-5-7-1-mlx5-2-create-qp-pid-19774-create-qp-type-2-failed/232369',\n",
       " 'https://forums.developer.nvidia.com/t/rhel9-support/223328/2',\n",
       " 'https://forums.developer.nvidia.com/t/i-have-2-rdma-nics-installed-on-this-server-ideally-two-netdevs-per-port-for-mlx5-0-example-ens224-ens225-each-with-their-own-mac-addresses-but-they-both-show-up-under-a-single-ens224/206220/2',\n",
       " 'https://forums.developer.nvidia.com/t/loopback-timestamping-errors/210985/4',\n",
       " 'https://forums.developer.nvidia.com/t/random-delays-on-two-back-to-back-connected-systems-with-connectx-5-2x100g-cards/206024/4',\n",
       " 'https://forums.developer.nvidia.com/t/ptp-on-sriov-vf-connect-x-6-dx/240097',\n",
       " 'https://forums.developer.nvidia.com/t/doca-sdk-documentation/174726/10',\n",
       " 'https://forums.developer.nvidia.com/t/rshim-shows-another-backend-already-attached-and-tmfifo-net0-cannot-be-found/206004/4',\n",
       " 'https://forums.developer.nvidia.com/t/bright-cluster-manager-simulator/235009/4',\n",
       " 'https://forums.developer.nvidia.com/t/does-pcie-prefetch-memory-impact-the-throughput/232630',\n",
       " 'https://forums.developer.nvidia.com/t/would-running-suse-linux-on-host-be-ok/241493',\n",
       " 'https://forums.developer.nvidia.com/t/how-do-i-run-performance-tests-between-a-windows-box-with-winof-2-and-a-linux-box-with-mlnx-ofed-linux-5-1-2/206162',\n",
       " 'https://forums.developer.nvidia.com/t/no-conneciton-between-host-and-dpu-in-embedded-mode/205038',\n",
       " 'https://forums.developer.nvidia.com/t/mellanox-connectx-2-mnpa19-xtr-driver-for-windows-10/213234/3',\n",
       " 'https://forums.developer.nvidia.com/t/i-configured-vdpa-and-dpdk-bonding-active-backup-mode-on-connectx-6-dx-slave-0-is-normal-but-failed-to-create-flow-on-slave-1-mlnx-ofed-version-5-4-1-0-3-0/206042',\n",
       " 'https://forums.developer.nvidia.com/t/lag-linux-bonding/206320',\n",
       " 'https://forums.developer.nvidia.com/t/cant-use-rdma-on-azure-nc24rs-v3/206525',\n",
       " 'https://forums.developer.nvidia.com/t/missing-directory-with-sr-iov-on-connectx-4-infinband-with-centos8-and-ofed-5-1-2-5-8-0/206159/4',\n",
       " 'https://forums.developer.nvidia.com/t/register-now-the-next-nvidia-dpu-virtual-hackathon/192597/1',\n",
       " 'https://forums.developer.nvidia.com/t/mlx5-core-will-cause-a-leak-of-a-command-resource/226808/4',\n",
       " 'https://forums.developer.nvidia.com/t/forward-to-sft-ipv4-udp-failed-error-no-pmd-support-for-sft/195144/3',\n",
       " 'https://forums.developer.nvidia.com/t/relationship-between-ovs-offloading-and-fdb/258945',\n",
       " 'https://forums.developer.nvidia.com/t/eal-initialization-failed/204507',\n",
       " 'https://forums.developer.nvidia.com/t/install-doca-on-bluefield2-faild/255946',\n",
       " 'https://forums.developer.nvidia.com/t/use-roce-with-ovs-hardware-offload/255536/4',\n",
       " 'https://forums.developer.nvidia.com/t/mcx354a-qcbtb-fails-to-detect-transceiver/222467/4',\n",
       " 'https://forums.developer.nvidia.com/t/blue-field-dpu2-not-coming-up/205067/4',\n",
       " 'https://forums.developer.nvidia.com/t/running-rxpbench-using-sfs/253084/3',\n",
       " 'https://forums.developer.nvidia.com/t/netdev-watchdog-eth0-mlx5-core-transmit-queue-0-timed-out-kernel-47075-368840-watchdog-bug-soft-lockup-cpu-64-stuck-for-22s-ksoftirqd-64-333/206094/2',\n",
       " 'https://forums.developer.nvidia.com/t/interconnecting-qsfp28-and-qsfp/206029',\n",
       " 'https://forums.developer.nvidia.com/t/kernel-build-for-sn2010-switchdev-failed-to-register-thermal-zone/253590/2',\n",
       " 'https://forums.developer.nvidia.com/t/ufm-ha-reinstall-how-to-save-configuration/209334',\n",
       " 'https://forums.developer.nvidia.com/t/does-pcie-prefetch-memory-impact-the-throughput/232630/2',\n",
       " 'https://nam11.safelinks.protection.outlook.com/?url=https%3A%2F%2Ftraining.brightcomputing.com%2F&data=05%7C01%7Ctkearsley%40nvidia.com%7Ca4dd4a1bc66f40ffec8c08da4219a55c%7C43083d15727340c1b7db39efd9ccc17a%7C0%7C0%7C637894975169167463%7CUnknown%7CTWFpbGZsb3d8eyJWIjoiMC4wLjAwMDAiLCJQIjoiV2luMzIiLCJBTiI6Ik1haWwiLCJXVCI6Mn0%3D%7C3000%7C%7C%7C&sdata=k59DyAkcCrKHycrOKti7Otb9ihCKjJNnF6bIN%2BpalzM%3D&reserved=0',\n",
       " 'https://forums.developer.nvidia.com/t/dpdk-on-doca/185145/1',\n",
       " 'https://forums.developer.nvidia.com/t/connectx-5-edr-connect-x6-hdr-with-sx6036-fdr-switch-incompatibility/212647/6',\n",
       " 'https://forums.developer.nvidia.com/t/connectx6-dpdk-dpdk-testpmd-receive-error-len-error-checksum-udp-packet-performance-is-very-low/205915/11',\n",
       " 'https://forums.developer.nvidia.com/t/vpi-configuration-issue-on-sx6036g-proxy-arp-does-not-come-up/205960',\n",
       " 'https://forums.developer.nvidia.com/t/how-to-connect-a-qsfp56-nic-to-a-qsfp28-switch/206075',\n",
       " 'https://forums.developer.nvidia.com/t/is-possible-to-plug-qsfp28-100g-direct-cable-in-first-port-of-connext5-en-dual-ports-100g-and-qsfp-40g-direct-cable-in-second-port-of-same-nic/206168',\n",
       " 'https://forums.developer.nvidia.com/t/vgt-and-vlan-connectivity-on-dpdk19-11/233414',\n",
       " 'https://forums.developer.nvidia.com/t/will-the-trx-10gsfp-sr-mlx-plugged-into-a-qxg-10g2sf-cx4-work-in-a-ubiquiti-environment/205900',\n",
       " 'https://forums.developer.nvidia.com/t/hi-everyone-is-there-is-a-way-to-get-the-model-number-in-the-cli-the-same-way-we-can-get-it-from-the-web-ui-e-g-mellanox-mlnx-os-sx1036-management-console-how-do-i-get-sx1036-from-cli-please/205987/3',\n",
       " 'https://forums.developer.nvidia.com/t/how-to-use-mmap-dev-mem-with-gpio-port-jetson-xavier-nx/237742/1',\n",
       " 'https://forums.developer.nvidia.com/t/docker-pull-error/245396',\n",
       " 'https://forums.developer.nvidia.com/t/doca-secure-channel-failed-to-open-comm-channel/233697/2',\n",
       " 'https://forums.developer.nvidia.com/t/rss-doesnt-work-for-mellanox-connectx-6-dx/205940/3',\n",
       " 'https://forums.developer.nvidia.com/t/i-need-to-know-how-to-make-rss-work-for-srv6/206219',\n",
       " 'https://forums.developer.nvidia.com/t/why-doesnt-my-mellanox-mcx311a-xcat-network-card-sit-correctly-on-my-supermicro-x10srh-cf-motherboard-pci-slot/206217',\n",
       " 'https://forums.developer.nvidia.com/t/100g-inifiband-system-random-disconnect/212461',\n",
       " 'https://forums.developer.nvidia.com/t/connectx-6-dx-missing-the-rdma/257361/3',\n",
       " 'https://forums.developer.nvidia.com/t/hi-i-have-msx1012b-2bfs-and-it-has-current-version-of-mlnx-os-at-3-4-2008-when-i-go-to-upgrade-options-and-select-current-software-and-target-software-it-doesnt-show-any-files-to-upgrade-how-can-i-upgrade-this-managed-switch-to-latest-software/205920',\n",
       " 'https://forums.developer.nvidia.com/t/rdma-infiniband-cannot-open-hosts-iberror-discovery-failed-port-state-down/217679',\n",
       " 'https://forums.developer.nvidia.com/t/kvm-in-bluefield-linux/215803',\n",
       " 'https://forums.developer.nvidia.com/t/network-interface-renaming-between-mofed-4-9-3-1-5-0-and-modef-5-4/206049',\n",
       " 'https://forums.developer.nvidia.com/t/how-many-udp-flooding-packets-will-be-scanned-and-blocked-by-dpu-ips-rule-in-bludfield2-per-core/244544',\n",
       " 'https://forums.developer.nvidia.com/t/about-the-bright-cluster-manager-user-forum/216255/7',\n",
       " 'https://forums.developer.nvidia.com/t/tcpdump-for-roce/238037/4',\n",
       " 'https://forums.developer.nvidia.com/t/mellanox-nic-real-time-performance-monitoring/230389/2',\n",
       " 'https://forums.developer.nvidia.com/t/mellanox-ofed-5-5-1-0-3-2-send-bandwidth-improves-when-registered-memory-is-aligned-to-system-page-size-4k-how/213355/2',\n",
       " 'https://forums.developer.nvidia.com/t/about-ndi-interface-support-in-winof/213769/3',\n",
       " 'https://forums.developer.nvidia.com/t/my-connectx-6-dx-nics-cant-work-consistently/233392/4',\n",
       " 'https://forums.developer.nvidia.com/t/cannot-ping-from-routed-port-to-device-on-vlan/213261',\n",
       " 'https://forums.developer.nvidia.com/t/about-mellanox-driver-issue-ofed-and-en/258105/3',\n",
       " 'https://forums.developer.nvidia.com/t/best-drivers-on-ubuntu-22-04-for-rtx-2070-mobile-max-q/236747',\n",
       " 'https://forums.developer.nvidia.com/t/performance-issue-with-inbox-drivers/205884',\n",
       " 'https://forums.developer.nvidia.com/t/select-host-queues-from-the-nic/257669',\n",
       " 'https://forums.developer.nvidia.com/t/is-oracle-linux-still-supported/205919',\n",
       " 'https://forums.developer.nvidia.com/t/performance-counters-for-accelerators/247086',\n",
       " 'https://forums.developer.nvidia.com/t/getting-started-with-connectx-5-100gb-s-adapter-for-windows-2019-server/250461/4',\n",
       " 'https://forums.developer.nvidia.com/t/connect-x4-lx-pxe-does-not-appear-in-dell-poweredge-uefi-boot-options/206124/4',\n",
       " 'https://forums.developer.nvidia.com/t/about-mcx75310aas-heat-transceivers/257224/8',\n",
       " 'https://forums.developer.nvidia.com/t/deep-convolutional-generative-adversarial-network-example-from-manual/247104',\n",
       " 'https://forums.developer.nvidia.com/t/too-much-irq-on-only-one-queue-with-vma/206047',\n",
       " 'https://forums.developer.nvidia.com/t/connectx6-socket-direct-and-vmware/205995',\n",
       " 'https://forums.developer.nvidia.com/t/rocev2-is-disabled-on-connectx-5-but-the-nic-still-captures-rocev2-packets/206023/5',\n",
       " 'https://forums.developer.nvidia.com/t/when-gpu-is-installed-memory-pegs-at-100-on-excel-scrolling/216244',\n",
       " 'https://forums.developer.nvidia.com/t/welcome-mellanox-customers/208277',\n",
       " 'https://forums.developer.nvidia.com/t/does-bluefield-v1-smartnic-support-doca/255369',\n",
       " 'https://forums.developer.nvidia.com/t/installing-ubuntu-18-04-on-bluefield-2-failed/205049',\n",
       " 'https://forums.developer.nvidia.com/t/dvorak-keyboard/210934/12',\n",
       " 'https://forums.developer.nvidia.com/t/error-with-configuring-ovs-dpdk-on-bluefiled-2/256030',\n",
       " 'https://forums.developer.nvidia.com/t/doca/243765/1',\n",
       " 'https://forums.developer.nvidia.com/t/do-all-adapter-cards-specially-mcx515a-gcat-support-all-cables-and-sfp-listed-in-mellanox-connectx-5-firmware-release-notes/205899',\n",
       " 'https://forums.developer.nvidia.com/t/application-examples/183049',\n",
       " 'https://forums.developer.nvidia.com/t/what-is-the-correct-bluefield-2-l3-cache-size/217414/1',\n",
       " 'https://forums.developer.nvidia.com/t/dual-port-connectx-5-with-network-bandwidth-issue/242762',\n",
       " 'https://forums.developer.nvidia.com/t/connect-x6-dx-best-firmware-for-vmware-sriov/261085',\n",
       " 'https://forums.developer.nvidia.com/u/kyoon',\n",
       " 'https://forums.developer.nvidia.com/t/problem-with-dma-on-multicore/248333',\n",
       " 'https://forums.developer.nvidia.com/t/opensm-was-dead/213531',\n",
       " 'https://forums.developer.nvidia.com/t/how-to-memcpy-in-linux-driver-directly-to-gpu/227957/5',\n",
       " 'https://forums.developer.nvidia.com/t/mt28908-family-connectx-6-wqe-dump-wq-size-1024-wq-cur-size-0-wqe-index-0x63-len-128/206102',\n",
       " 'https://forums.developer.nvidia.com/t/counter-to-measure-pcie-limitations/249809/2',\n",
       " 'https://forums.developer.nvidia.com/t/offload-infiniband-application-to-bluefield2-dpu/189369/5',\n",
       " 'https://forums.developer.nvidia.com/t/how-to-deal-with-dma-on-the-host-to-access-dpus-memory/221441/3',\n",
       " 'https://forums.developer.nvidia.com/t/connectx-4-roce-speed-less-than-expected/259437/8',\n",
       " 'https://forums.developer.nvidia.com/t/east-west-overlay-encryption-with-ipsec-example/174733',\n",
       " 'https://forums.developer.nvidia.com/u/bayonetx',\n",
       " 'https://forums.developer.nvidia.com/t/nvidia-container-cli-initialization-error-load-library-failed-libnvidia-ml-so-1/237759',\n",
       " 'https://forums.developer.nvidia.com/t/negotiation-failure-connecting-to-sonic-switch/257548/2',\n",
       " 'https://forums.developer.nvidia.com/t/how-to-drop-packets-in-the-switch/206235',\n",
       " 'https://forums.developer.nvidia.com/t/how-to-set-rss-hash-calculation-from-inner-layer-not-from-outer-layer-for-tunneling-trafic/206182/4',\n",
       " 'https://forums.developer.nvidia.com/t/connectx-6-dx-sriov-esxi-trust-mode/260192',\n",
       " 'https://forums.developer.nvidia.com/t/vrrp-mac-received-packet-with-own-address-as-source-address/243021/2',\n",
       " 'https://forums.developer.nvidia.com/t/flow-management-features-on-windows-dpdk/209904/3',\n",
       " 'https://forums.developer.nvidia.com/t/is-there-a-command-that-will-tell-us-why-one-of-the-ports-on-our-516-ccat-adapter-will-not-link-to-a-switch-or-how-to-detect-if-there-is-an-error-on-the-port-or-card/205981/2',\n",
       " 'https://forums.developer.nvidia.com/t/mcx4121a-acat-safe-mode-enable/206053/2',\n",
       " 'https://forums.developer.nvidia.com/t/unable-to-use-bluefields-pka-engine-on-bluefield-2-smartnic-p-n-mbf2h516a-ceeot/205957',\n",
       " 'https://forums.developer.nvidia.com/t/factory-reset-problem-on-sx1036-switch/206284',\n",
       " 'https://forums.developer.nvidia.com/t/missing-commands-in-nclu/240276',\n",
       " 'https://forums.developer.nvidia.com/t/how-to-convert-a-cumulus-os-switch-to-run-mlnx-os/206878',\n",
       " 'https://forums.developer.nvidia.com/t/mellanox-technologies-mt2892-ethernet-mode/256005',\n",
       " 'https://forums.developer.nvidia.com/t/snmp-ifindex/206225/2',\n",
       " 'https://forums.developer.nvidia.com/t/compute-nodes-provisioning-failed-installer-unreachable/229203',\n",
       " ...]"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "forum_links = list(set(forum_links))\n",
    "forum_links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "cfc24bab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2652\n"
     ]
    }
   ],
   "source": [
    "print(len(forum_links))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "eeca2108",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://forums.developer.nvidia.com/t/4-9-ofed-support-for-rhel9/241158\n"
     ]
    }
   ],
   "source": [
    "print(forum_links[-3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "8eb3bf41",
   "metadata": {},
   "outputs": [],
   "source": [
    "def contains_numbers(s):\n",
    "    return  s.isdigit()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "fa298ed0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# i = 0\n",
    "# for link in forum_links:\n",
    "#     if str(link).startswith('https://forums.developer.nvidia.com/t/'):\n",
    "#         print(link)\n",
    "#         query_name = list(str(link).split('/'))[-2]\n",
    "#         if (contains_numbers(list(str(link).split('/'))[-2])):\n",
    "#             query_name = list(str(link).split('/'))[-3]\n",
    "#         else:\n",
    "#             query_name = list(str(link).split('/'))[-2]\n",
    "#         i = i+1\n",
    "#         print(i)    \n",
    "#         print(query_name)\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bfb0bcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "forum_data = {}\n",
    "\n",
    "\n",
    "    \n",
    "for link in forum_links:\n",
    "    if str(link).startswith('https://forums.developer.nvidia.com/t/'):\n",
    "        \n",
    "        query_name = None\n",
    "        if (contains_numbers(list(str(link).split('/'))[-2])):\n",
    "            query_name = list(str(link).split('/'))[-3]\n",
    "        else:\n",
    "            query_name = list(str(link).split('/'))[-2]\n",
    "            \n",
    "            \n",
    "        \n",
    "        url = str(link)\n",
    "        response = requests.get(url)\n",
    "        soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "        paragraphs = soup.find_all(\"p\")\n",
    "\n",
    "\n",
    "        total_text = ''\n",
    "        for paragraph in paragraphs:\n",
    "            text = paragraph.get_text()\n",
    "            total_text = total_text + text\n",
    "\n",
    "        forum_data[query_name] = total_text\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "        \n",
    "    \n",
    "        \n",
    "            \n",
    "            \n",
    "       \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c089448c",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(forum_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "fb806699",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'tesla-p4-opengl-not-supported': 'Hi,\\nI need to run a windows application that requires OpenGL.\\nOut of the box on a server equiped with a Tesla P4, it’s not working.\\nI think this is because the driver is in TCC mode but switching to WDDM is not possible :C:.…>nvidia-smi.exe -fdm 0\\nUnable to set driver model for GPU 00000000:37:00.0: Not Supported\\nTreating as warning and moving on.\\nAll done.Is there a way to use OpenGL on a server equiped with a Tesla P4 ?\\nThanks for your helpSystem Information :==============NVSMI LOG==============Timestamp                           : Thu Apr 18 11:24:49 2019\\nDriver Version                      : 419.69\\nCUDA Version                        : 10.1Attached GPUs                       : 1\\nGPU 00000000:37:00.0\\nProduct Name                    : Tesla P4\\nProduct Brand                   : Tesla\\nDisplay Mode                    : Enabled\\nDisplay Active                  : Disabled\\nPersistence Mode                : N/A\\nAccounting Mode                 : Disabled\\nAccounting Mode Buffer Size     : 4000\\nDriver Model\\nCurrent                     : TCC\\nPending                     : TCC\\nSerial Number                   : 0324318063608\\nGPU UUID                        : GPU-d82d66ce-c5a6-b6a0-7d88-79e5fcaabd4c\\nMinor Number                    : N/A\\nVBIOS Version                   : 86.04.8C.00.10\\nMultiGPU Board                  : No\\nBoard ID                        : 0x3700\\nGPU Part Number                 : 900-2G414-0300-000\\nInforom Version\\nImage Version               : G414.0200.00.03\\nOEM Object                  : 1.1\\nECC Object                  : 4.1\\nPower Management Object     : N/A\\nGPU Operation Mode\\nCurrent                     : N/A\\nPending                     : N/A\\nGPU Virtualization Mode\\nVirtualization mode         : None\\nIBMNPU\\nRelaxed Ordering Mode       : N/A\\n…/…I believe that I am seeing a similar issue, have you made any progress or have any notes that you can share?Powered by Discourse, best viewed with JavaScript enabled',\n",
       " 'dl-comparisons-of-1080ti-and-rtx-20xx-s': 'I found a 1080ti (used) but cheap enough that’s in the same ballpark as getting a 2070 non s and ~350euros cheaper than a new 2080. I have seen a lot of benchmarks putting 1080ti around 2070 and 2080. My question is, do these account for the increased memory capacity of 1080ti? The 1080ti has ~130% the memory capacity of the other two so the question is. Should I go with the 1080ti or a 2070? I am using both RNNs and CNNs.[url]https://xvideos.onl/[/url] https://xnxx.onl/ [url]https://chaturbate.onl/[/url]Powered by Discourse, best viewed with JavaScript enabled',\n",
       " 'power-down-sequence-of-agx-xavier-som': 'HelloI’m trying to develop a custom carrier board to the AGX Xavier SoM.I’m looking for a detailed power down sequence on both the evaluation board and the Xavier spec.The power down signal that is created on the MCU and initiate the power down sequence is missing from the signal diagram (Power Button ?).On Power up sequence, It is not clear what is the pulse width of Power Button see figure1 attached\\nfigure11165×610 169 KB\\nThe attached signal diagram is timeless figure2  – it is not clear what is the time interval between each segmentsThere are some discrepancies between both evaluation board and the Xavier spec e.g. on Figure 8. Power Down Sequence (Controlled Case) of Jetson_AGX_Xavier_Series_OEM_Product_Design_Guide.pdf one can see the MODULE_POWER_ON is de-asserted early, right after SYS_RESET_N is de-asserted, also VDDIN_PWR_BAD_N is de-asserted right after SYS_RESET_N is de-asserted, then SYS_VIN_HV and SYS_VIN_MV are slowly decreased. see figure3\\nfigure3924×582 184 KB\\nHowever this same power down sequence is dipicted in a different way on 2.3.2 Power Down signal diagram on JetsonXavierSOMDataSheet_v0 9.pdfIn 2.3.2 one can see the MODULE_POWER_ON is de-asserted very late, apparently after Jetson Xavier Module Power. Jetson Xavier Module Power is missing from the first signal diagram.VDDIN_PWR_BAD_N , SYS_VIN_HV and SYS_VIN_MV remain high and are not affected from the power down.\\nSee figure3 and compare to figure2Regards,Yaniv.Powered by Discourse, best viewed with JavaScript enabled',\n",
       " 'physx-3-3-2-fluid-simulation-hagen-ndash-poiseuille-equation': 'In physx 3.3.2, can we simulate fluid distributed as Hagen-Poiseuille equation? and How?Powered by Discourse, best viewed with JavaScript enabled',\n",
       " 'new-release-of-opengl-driver-and-how-to-test-it-against-application-which-uses-opengl': 'Hi !We have quite many different issues with opengl drivers - some problems were on our side, some on opengl drivers - but I was wondering whether it’s possible for us to provide you some simple / complex test application, so you could test each open gl driver release against our application.We could have it fully automated (e.g. you will see only 0-100% pass rate) - so you don’t need to bother much with manual checking.If it’s possible - I would prefer to chat about this over e-mail on subject.Powered by Discourse, best viewed with JavaScript enabled',\n",
       " 'alpha-blending-to-pre-or-not-to-pre-blog-discussion': 'This thread is for questions and discussions related to John McDonald’s Alpha Blending: To Pre or Not To Pre blog post.Hello,The WebGL Test at the end of the article fails to appear on multiple browsers, presumably because the source image request returns a 404 not found:GET https://developer.nvidia.com/sites/default/files/akamai/gamedev/images/2x1-rg-pre-webgl.png 404 ()\\nGET https://developer.nvidia.com/sites/default/files/akamai/gamedev/images/1x1-rg-pre-webgl.png 404 ()\\nGET https://developer.nvidia.com/sites/default/files/akamai/gamedev/images/2x1-rg-post-webgl.png 404 ()\\nGET https://developer.nvidia.com/sites/default/files/akamai/gamedev/images/mandrill-512x512.png 404 ()This information was gathered using Inspect element on Chrome v60 (latest stable) on Windows 10 64-bitIt would be great if this simple error could be fixed as these articles are a great resource to help colleagues and students learn these concepts, and are a representation of Nvidia as a leader in the industry.ThanksPowered by Discourse, best viewed with JavaScript enabled',\n",
       " 'raw-pointer-buffer-access-detected': 'Hi all,\\nI am getting an exceptionIt seems to be related to some buffer I am accessing but I am not able to debug it after going through my code and print calls. If I set a usageReport as in the samples provided I get this WARNING:Raw pointer buffer access detectedThis is something I did not get in previous tests, so it has to be related to some changes I have done in the buffers.\\nHow can I debug this problem? Is there any way to know what is that raw pointer access?\\nThanks a lotThe illegal address thing can be about anything, including bugs in OptiX.If you suspect this to related to your own buffer accesses, check if all your variables and structure offsets are adhering to the CUDA clignment rules. I posted links an recommendations yesterday in this thread: [url]https://devtalk.nvidia.com/default/topic/1037798/optix/optix-time-for-launch/[/url]Note that pointer arithmetic on OptiX buffers in device code is not allowed.\\nAlways use operator to access a single element of a buffer.\\nSee this post: [url]https://devtalk.nvidia.com/default/topic/1004680/?comment=5130885[/url]When reporting such issues pleas always include the following system configuration information:\\nOS version, installed GPU(s), display driver version, OptiX version (major.minor.micro), CUDA toolkit version, host compiler version.Hi again,the problem occurs if I use a rtPrintf in my closesHitProgram. Otherwise, everything seems to work OK. The Raw pointer access warning still appears, but results seem to be correct and I do not get the exception.My system configuration is:\\nUbuntu 18.04,\\ngcc version 7.3.0 (Ubuntu 7.3.0-16ubuntu3)\\nTarget: x86_64-linux-gnu\\nCUDA Version 9.2.148I also get the same problem on a Windows 10 machine\\nwith Visual Studio Entreprise 2017, version 15.4.5\\nCuda version 9.1Thanks againNeither CUDA 9.1 nor 9.2 are officially supported by the OptiX 5.1.0 release.\\nIf possible try CUDA 9.0.\\nThere have been some incompatibilities reported with different MSVS 2017 versions, but that should also come with the MSVS 2015 tool chain included.If that doesn’t help we would need a minimal reproduce which shows the error.“OptiX Version:[5.0.1]”\\nPlease upgrade to OptiX 5.1.0.I have “solved” this problem. I do not really know the exact reason. I just want to post it here, just in case someone has some related issue. And to improve my knowledge if an explanation is provided.The problem seems to be in an atomicExch to the buffer. That is, I declare this buffer (which is correctly created and initialized as I have checked)Afterwards, in the closest hit program  I do (some lines are missing, but all the variables used are correctly declared and assigned, including that reflections-1 which is never less than zero)Then I get the “WARNING: Raw pointer buffer access detected” and apparently random illegal address expections, depending on my use of rtPrintfIf I just replace the above buffer declaration with two different ones asand then doEverything is Ok and the “WARNING: Raw pointer buffer access detected” disappears. I  have checked the alignment rules and so on. I do not know if I am inadvertently doing pointer arithmetic.Powered by Discourse, best viewed with JavaScript enabled',\n",
       " 'oculus-touch-a-b-button-capacitive-touch-handling-through-cloudxr': 'Hi, I was wondering when CloudXR would add support for A,B / X,Y button touch events, they appear not to be currently:typedef enum\\n{\\ncxrButton_System,\\ncxrButton_ApplicationMenu,} cxrButtonId;thanksPowered by Discourse, best viewed with JavaScript enabled',\n",
       " 'optix7-3-geforce-driver-466-63-win64-nvoptix-dll-not-found': 'Hi,Just getting setup with the Optix7.3 SDK on Win10. Have attempted to run optixHello.exe but it fails on optixInit() apprently because there is no nvoptix.dll on my system, despite having just installed 466.63-desktop-win10-64bit-international-whql.exe.Is nvoptix.dll missing from 466.63-desktop-win10-64bit-international-whql?Cheers,\\nGregHi Greg,I don’t think the dll is missing from the installer. I checked the contents of that installer by opening it in 7zip, and I can see the packaged version nvoptix.dl_.Will you check your C:\\\\Windows\\\\System32\\\\DriverStore folder to see if nvoptix.dll was copied in there, and maybe whether you have multiple versions? Also, dumb question, but have you rebooted since installing the new driver?–\\nDavid.Yes. I had rebooted :)I found nvoptix.dll in C:\\\\Windows\\\\System32\\\\DriverStore (thanks) and have copied it manually into C:\\\\Windows\\\\System32\\\\, but now I’m getting optixInit returned\\tOPTIX_ERROR_UNSUPPORTED_ABI_VERSION (7801). I’m on 19043.985 which is Win 10 Pro 21H1. Any suggestions?The last time I saw that error, it was a case of an old DLL version getting picked up from previous driver installs. Do you have multiple copies of the DLL, or remnants of older drivers? Check also for nvrtum64.dll. EVGA RTX 2080Ti Black IRAY OptiX 7.1.0 error (SOLVED) - #2 by CarstenWaechter–\\nDavid.Brilliant! Thanks. I should’ve been more careful with which ‘nvoptix.dll’ I manually copied out of the DriverStore. The version of that DLL was 7.2.0, so I found the newer one in a more recent sub-folder in DriverStore and now it works. Thanks for your help sorting that out!Excellent! Glad you got it sorted. I think you shouldn’t need to copy the dll anywhere, that your system should be finding it in DriverStore automatically, so there might still be a little cleanup that could fix the underlying issue. I would hope you don’t need to copy any dlls after future driver installs, that should not be necessary.–\\nDavid.Interesting. Not sure if it’s relevant, but I used “Custom Install” (not “Express”) and accepted the defaults (I think). Checking the Sys Info using the NVidia Control Panel showed the old driver version after the install+reboot. So I re-installed using the Express install and rebooted and the ‘nvoptix.dll’ still wasn’t in System32, but the Sys Info showed the correct driver version.That makes sense, and maybe there is some kind of problem with the custom install. Normally, you shouldn’t have a copy of nvoptix.dll directly in the C:\\\\Windows\\\\System32\\\\ folder, but there should be one somewhere under the DriverStore sub-folder.This might be some kind of conflict with either older driver styles or older OptiX distribution, but if you can get it working with the express install process without having to manually copy any DLLs, that sounds workable.–\\nDavid.Powered by Discourse, best viewed with JavaScript enabled',\n",
       " 'vulkan-interop-for-video-codec-sdk-as-seen-in-the-appmotionestimationvkcuda': \"Hello,I am using Vulkan for rendering an offscreen image, which I would like to encode using the Video Codec SDK. I was following the AppMotionEstimationVkCuda example as a reference on how to supply the images into the Video Codec SDK. Are there any other publicly available examples of such interop, other than the example mentioned? My problem are the external semaphores in Vulkan. I am also presenting the offsceen image on screen for me to be able to see the scene while encoding it and this is where the problems with semaphores begin. This requires the external semaphore to be used as a signal semaphore when the rendering finishes and as a wait semaphore in the presentation queue submit. The problem is that it seems that the Vulkan SDK validation layers are not really complete when it comes to external semaphores or my usage of them was not expected. Validation reports that my external semaphore can not be signaled, which is due to the nature of state tracking of the validation layer, which has conditions to only properly track internal semaphores, but does not hesitate to report error also for the external semaphores, even though the signal tracking is not done for them. My Vulkan app was working fine without any validation errors, until I started to perform the vkGetSemaphoreWin32HandleKHR call. Are there any users with an experience with these issues?You can look at how mpv handles this via libplacebo. You’ll need to do a fair amount of reading, as libplacebo wraps the semaphore usage, but it’s the only complete real world usage of interop that I’m aware of.Reusable library for GPU-accelerated image/video processing primitives and shaders, as well a batteries-included, extensible, high-quality rendering pipeline (similar to mpv's vo_gpu). Supports Vulkan, OpenGL, Metal (via MoltenVK) and...Hi.\\nIn addition to what ‘langdalepl’ suggested, you can also take a look at\\nGitHub - NVIDIA/cuda-samples: Samples for CUDA Developers which demonstrates features in CUDA Toolkit\\nThere are a few samples that demonstrate how to use Vulkan CUDA interop e.g. simpleVulkan, simpleVulkanMMAP, vulkanImageCUDA.\\nThose samples along with the sample from NVIDIA Video Codec SDK should help you.If you still run into any issues, please provide a minimal source code reproducer to be able to help.Thanks.Powered by Discourse, best viewed with JavaScript enabled\",\n",
       " 'sdk-8-2-16-appdecd3d-failed-to-load-in-vs-15-9-6': 'AppDecGL\\nAppDecImageProvider\\nAppDecMultiInput\\nAppDecEnc\\nAppEncLowLatency\\nAppTrans\\nAppTransOnToNare failing, too.any ideas?deletingandin the .vcxproj file allows me to load the projects in VS 15.9.6Hi,Could you provide us with additional information:Thanks,\\nRyan ParkPowered by Discourse, best viewed with JavaScript enabled',\n",
       " 'windows-8-wrl-physx': 'Is there a set of libs/DLL’s for this now? If not, is there a way I can get the source to compile it?I am part of the “Registered Developer Program” but I’m not sure if that’s enough to gain access to all the source. There isn’t much source in the 3.2.2 release.Thanks,\\nJerryI would like to add that there is a 3.2.1 Win8arm download but that might not solve my problem either.I’m trying to build incorporate PhysX in the Windows Store “DirectX 3D Shooting Game Sample”. Additionally, I’m using VS2012 and at link time the following errors are reported:1>PhysX3Extensions.lib(ExtDefaultErrorCallback.obj) : error LNK2038: mismatch detected for ‘_MSC_VER’: value ‘1600’ doesn’t match value ‘1700’ in BasicLoader.obj1>PhysX3Extensions.lib(ExtDefaultErrorCallback.obj) : error LNK2038: mismatch detected for ‘_ITERATOR_DEBUG_LEVEL’: value ‘0’ doesn’t match value ‘2’ in BasicLoader.obj1>libcmt.lib(sprintf.obj) : error LNK2005: _sprintf_s already defined in MSVCRTD.lib(MSVCR110D.dll)Has anyone else encountered these errors with PhysX before? Does anyone know how to resolve them?Thanks,\\nJerryAny help with this?Additionally, I’m using VS2012\\nAfaik, Visual Studio 2012 is not supported currently.is there a way I can get the source to compile it?\\nSource access is fee basedThank you for the response. FYI - I can build and link with VS2012 for WIN32/WIN64. But when I try to do so for Windows Store I seem to have the problems above.I have a similar problem.Using Win7 though / VS2010 ultimate.I am using the PhysX SDK in my own static lib project.\\nThis lib contains oa. a GFX renderer, PhysX support, wiimote support, (de)serialisation, etc…The generated static lib I then use in a Qt application (exe) project.\\nIn release everything compiles and runs fine.In DEBUG mode, however, the static lib compiles fine, but the linker chokes on trying to link the static lib to the app.IE. A heap of these errors pops up during linking:2>PhysX3Extensions.lib(ExtDefaultCpuDispatcher.obj) : error LNK2038: mismatch detected for ‘_ITERATOR_DEBUG_LEVEL’: value ‘0’ doesn’t match value ‘2’ in qrc_archivr.obj…2>PhysX3Vehicle.lib(VehicleUtils.obj) : error LNK2038: mismatch detected for ‘_ITERATOR_DEBUG_LEVEL’: value ‘0’ doesn’t match value ‘2’ in qrc_archivr.obj…As far as my research showed me so far, this error means I am trying to use a release built lib (value ‘0’ - the PhysX LIBS) in a debug build (value ‘2’ - the Qt app).  I do not find any DEBUG libs in the PhysX SDK however.\\nThere are only Checked/profile versions for some of the physX libs.\\nWhat are those?\\nLinking to those ‘checked’ or ‘profile’ libs gives same result… :-SAnyone already bumped on the same problem/ solved it perhaps?\\nIdeally, I would like to be able to run the entire app with ITERATOR_DEBUG_LEVEL set to 2 off course :-)\\nAny help would be greatly appreciated!Powered by Discourse, best viewed with JavaScript enabled',\n",
       " 'dlss-black-pixel-artifacts-on-transparent-materials': 'I’m developing a game using DLSS 2/3 and Unreal Engine 4.26. I’ve noticed that these black pixelated artifacts show up on most transparent textures like skyboxes and fog. Only shows up when DLSS is enabled.Is there any info about this and how to avoid this issue? I couldn’t find this issue on Google. It looks like a pattern of some sort.Hi @kabassmusic, welcome to the NVIDIA developer forums!Very weird effect indeed. Did you try different presets for DLSS or change some of the console command parameters as stated in the Quickstart Guide and see if that changes something?So far I haven’t seen this kind of problem, but I will ask around if someone finds this familiar.It seems to happen on all presets. I’m going to do some isolated tests outside of my main project with the demo project to see if I can somehow replicate it.I think I figured it out.One of my materials has a DitherTemporalAA.\\nWhen DLSS is enabled, it enhances that node and makes it more noticeable.Yep, makes sense. Any form of AA especially temporal can easily be amplified.Good to hear you could find a root cause!This topic was automatically closed 14 days after the last reply. New replies are no longer allowed.Powered by Discourse, best viewed with JavaScript enabled',\n",
       " 'sharing-the-same-cuda-context-between-renderer-optix-denoiser-and-nvencoder': 'Hi there!Is it considered a good practice?I run Optix denoiser in the default cuda stream but it may run in a different stream.a) Is there a recommendation in what order Optix denoiser and NVENCODER should be created when Optix denoiser uses default and non-default cuda stream?b) What stream does NVENCODER run in? Is it cuda default stream or could it be any stream?c) Is any synchronization required between Optix denoiser and NVENCODER creation when they use the same or different cuda context and the same or different cuda streams? If so, what synchronization API (cudaDeviceSynchronize or cuCtxSynchronize) should be used and when?More context for my use case can be found on this post: nvEncDestroyEncodercall hangs when nvencoder and Optix denoiser use the same CUDA contextI’ve already answered in the OptiX forum to the best of my ability. The questions are asking about how the NVENCODER API works, which I have no expertise in. I sent @petr.mpp over here to find out. Please assign someone on the NVENCODER team to answer the questions as best they can. I glanced at the docs, and the NVENCODER Programming Guide suggests in multiple places that people should create a new “floating” context for encoder work, which is not something OptiX necessarily recommends by default, so the first part of the question is whether those recommendations imply that NVENCODER users should try to limit whatever else they do in the same context. My guess is that nobody knows exactly what the interaction is between OptiX denoiser and NVENCODER is, but I’m certain that best practices include doing the initialization for each API serially and not try to use one of them while the other one is initializing. That is the problem @petr.mpp had initially, and synchronizing around initialization seems to resolve the problem. The user just wanted to better understand this workaround so they can be confident in the fix, and to make sure there aren’t better ways to resolve it.–\\nDavid.Thank you David for the reply!Hello @petr.mpp !I am out of my depth here so I will contact our NVENC guys and see if they can add some additional information to what David explained.Thanks!Powered by Discourse, best viewed with JavaScript enabled',\n",
       " 'some-basic-query': 'can you please tell me how to find following\\n1.)  GPU memory (qty, utilization, bandwidth)\\n2.) tensor cores utilization/frequency/memory bandwith, etc.\\n3.) Video Encoding / Decoding Accelerator utilizationI am using jetson xavier nx and nvidia-smi not workingPowered by Discourse, best viewed with JavaScript enabled',\n",
       " 'define-a-polynomial-surface': 'I have a polynomial surface that was fitted by the measurement data. Is it possible to use this surface in the Optix? I only see the sphere and plane in the example code.\\nThanks!There is no out-of-the-box solution, but since you can define ray intersection programs for arbitrary geometries, you can also write one for polynomial surfaces. You will also need a boundary program for the polynomial surfaces.\\nOnce you have both, you can use OptiX with your polynomial surfaces.Thanks for your quick response. I am new for the CUDA and Optix. But I am really interested in this technology.\\nIf I can define this polynomial surface, do you know if the Optix calculate the intersection analytically or iteratively?Thanks a lot!You must provide the algorithm for the intersection routine. Have a look at the SDK examples, it contains an example on how to intersect a triangle mesh. This code can also be found online, e.g. at https://github.com/nvpro-samples/gl_optix_composite/blob/master/shaders/optix_triangle_mesh.cuSo you need to either implement an analytical or an iterative solution which determines if a given ray intersects your surface.Powered by Discourse, best viewed with JavaScript enabled',\n",
       " '3-3-2-doubts-about-rigiddynamic-controller': 'Hello,First, what i want to do ~Second, before start, should mention about I am not good with physics (this is my  starting point), so i keep trying to understand how it works.I was testing the Controller module and i dont know whats the correct way to apply gravity forces, this should be posible by using the method move, this one supposed to be ok, but, for example why i must manage it manually, if the rigidbody is automatically applied, is there a different way to apply it without doing so obfuscated?And about the RigidDynamic module, is a good option at the moment, because it applies the gravity and blah. but, i am not able to lock the capsule axis rotations, except for the one which i should rotate the character/player( up-vector / Y ).Can someone explain me what im doing wrong, or if im on the bad way to do that?~ i do not paste code because dont want a code-solution, wanna understand.Hi,See the ‘Sample bridges’ scene in the PhysX samples. The character controller is kinematic, not fully dynamic, because fully dynamic character controllers suffer from too many artifacts where the designer loses control over the character’s motion.  Most games use a kinematic character controller based on raycasts or other scene query collision detection with the world.Hi again,\\nI can not figure out where are the applied forces in the character at whole ‘bridges project’, there are to many things at that project which make it a bit dificult to track like Render, generic actor creations, plus many other modules.The character controller is kinematic, not dynamic, so no, adding forces and torques won’t do much as far as I know.Powered by Discourse, best viewed with JavaScript enabled',\n",
       " 'help-needed-nvgs-not-switching-to-nvidia-graphics-in-legion-pro-5i-gen-8-intel-16-with-rtx-4050': 'Hi EveryoneI’m facing an issue with my gaming laptop , specifically with the NVGS not switching to the NVIDIA graphics card. I’ve been trying to troubleshoot this problem on my own, but I haven’t been successful so far. I was hoping someone in this community could help me out.Details of my laptop:Model: Legion Pro 5i Gen 8 Intel (16\")\\nGraphics Card: NVIDIA RTX™ 4050Issue:\\nThe problem I’m experiencing is that the NVGS (NVIDIA Graphics Switch) is not automatically switching to the dedicated NVIDIA graphics card when running games or other graphic-intensive applications. It seems like my laptop is relying on the integrated Intel graphics instead.Steps I’ve taken:\\nHere are the steps I’ve tried to resolve the issue, but none of them have worked:Checked the NVIDIA Control Panel settings to ensure that the preferred graphics processor is set to “High-performance NVIDIA processor.”\\nUpdated the graphics drivers to the latest version provided by the manufacturer.\\nRestarted the laptop multiple times.\\nVerified that the power settings are not set to “Power Saving” mode.Despite these attempts, the laptop continues to use the integrated graphics, resulting in lower performance and reduced gaming experience.If anyone has encountered a similar issue or knows of a potential solution, I would greatly appreciate your assistance. Your suggestions, tips, or workarounds would be invaluable.Thank you in advance for your help!Hi there @charleskaren732 and welcome to the NVIDIA developer forums.Generally speaking, this is more of an end-user, GeForce related issue and more likely connected to Lenovo.But since you are already here:But first of all you should install the latest Lenovo BIOS version, chipset drivers and other software updates Lenovo offers in their support section for your Laptop model.\\nThen run all the latest Windows updates as well.If it still does not use the NVIDIA GPU, it is time to contact Lenovo support and return the device.Hope that helps!Powered by Discourse, best viewed with JavaScript enabled',\n",
       " 'problem-with-cooking-trianglemesh-physx-3-2': 'Hi! I using a PhysX 3.2 and Visual Studio 2010. I have a problem with with loading and cooking triangle mesh from STL file.\\nThe STL file looks likefacet normal ni nj nk\\nouter loop\\nvertex v1x v1y v1z\\nvertex v2x v2y v2z\\nvertex v3x v3y v3z\\nendloop\\nendfacetSo far I got vector of vertexes (PxVec3). Now I’m trying to set up a mesh descriptor. The problem is that I don’t how to set up meshDesc properly. The program creates file called “TriMeshCooked.bin” but the file is empty (0 bytes). Maybe I have to create some vector of (PxU32) indices and pass it to MeshDesc ? But my question is how to create it? Is there any algorythm?here is part of code from PhysX 3.2 SDK GuidePxTriangleMeshDesc meshDesc;\\nmeshDesc.points.count           = nbVerts;\\nmeshDesc.points.stride          = sizeof(PxVec3);\\nmeshDesc.points.data            = verts;meshDesc.triangles.count        = triCount;\\nmeshDesc.triangles.stride       = 3*sizeof(PxU32);\\nmeshDesc.triangles.data         = indices32;PxToolkit::MemoryOutputStream writeBuffer;\\nbool status = cooking.cookTriangleMesh(meshDesc, writeBuffer);\\nif(!status)\\nreturn NULL;PxToolkit::MemoryInputData readBuffer(writeBuffer.getData(), writeBuffer.getSize());\\nreturn physics.createTriangleMesh(readBuffer);And my code.Hi these are my declarations:Hope it helps.Where is the data getting loaded into pxTriVertex prior to cooking ?Powered by Discourse, best viewed with JavaScript enabled',\n",
       " 'dual-nvidias-win10-x64-error-code-43-non-functional': 'Hello,REMARK: Move the topic if placed incorrectly, pls.Has ANYBODY in NVIDIA got an idea why cant be two nvidias installed in on desktop?\\nMSI970ag43 + gtx760 + gtx660 + Win10x64 = problems\\nNo way to make both cards installed correctly/fully, no way to install eg different driver versions\\nfor each of them, whats going on?!\\nError 43 at best.Anybody/anything?cheers,Powered by Discourse, best viewed with JavaScript enabled',\n",
       " 'optixtrace-point-of-intersection-and-recursion-depth-limit': 'To implement light bouncing from object, I need the point at which this closest_hit took place, ie. the value of t such that hitPoint = rayOrigin + t*rayDirection.\\nI can obtain rayOrigin and rayDirection from optixGetWorldRayOrigin/Direction() functions but not sure about the t.\\nFrom reading the reference manual, optixGetRayTime() will not accomplish this.\\nSince recursion is involved in the emulation of  the light bouncing around, there needs to be a way to limit its depth: Is there a built in OptiX mechanism for this or the payload of optixTrace() is used (or maybe a slicker way)?\\nFor custom geometries one can do things in intersection shader but for triangles, which use the built in intersection shader, things seem more limited.HI there, you can get the “t” by using optixGetRayTmax(). https://raytracing-docs.nvidia.com/optix7/guide/index.html#curves#curves-and-the-hit-programNote that if you’re using triangles, you can also get the hit point without using “t”. You could use optixGetTriangleBarycentrics() in conjunction with optixGetTriangleVertexData(). This is a little more bandwidth, but the reason to do it this way is if you need better accuracy. Using origin + t * direction can lose precision if t gets very large.optixGetRayTime() is referring to motion blur time, so is used for a different purpose than reconstructing the hit point.You can limit recursion depth on your own, even with triangles, several different ways. If you are calling optixTrace() recursively, you can put a depth value in your payload and track the depth and prevent recursive calls when it’s above a threshold. The more common way is to handle ray depth iteratively in your raygen program, and there you can loop over depth and stop when you’ve exceeded your limit. The benefit of iterating in raygen is you avoid using extra stack memory to handle recursion. See the SDK sample called optixPathTracer for a complete demonstration of iterative depth handling for path tracing in a raygen program.–\\nDavid.Please also look at the sticky posts in this sub-forum which contain additional links to documentation and examples showing this functionality and much more.Powered by Discourse, best viewed with JavaScript enabled',\n",
       " 'calling-function-nvmediaimagegetbits-takes-too-much-time': 'Hi,\\nWhen calling function NvMediaImageGetBits, it takes about 10 milliseconds of time. Is there anyother function can replace the function NvMediaImageGetBits? Which interface can be used to read data from GPU to CPU? Is there any sample?Thanks!Hi Jungang,\\nWould you please descript your use case for calling function NvMediaImageGetBits?Thanks a million,\\nSamHi Sam,\\nThank you for your reply!\\nOur other modules need to use camera data, so I need to read camera data from GPU to CPU and transfer them to other modules.Thanks!Powered by Discourse, best viewed with JavaScript enabled',\n",
       " 'sync-card-2-rtx-6000-ada': 'Hello folks, Does Sync card 2 supports RTX 6000 ADA cards or only A6000? ThanksHello @ronp2 and welcome to the NVIDIA developer forums!According to Quadro Sync pages only up to Ampere generation is supported, meaning RTX A6000.Information on when or if there will be support with this card for Ada generation architecture is not available at this time.Thanks!Powered by Discourse, best viewed with JavaScript enabled',\n",
       " 'nvidiaopenglrdp-exe-uninstall-more-flexibility': 'I really like the tool it allows my company to use our graphicscard performance per RDP. But I’d like more flexibility.How can I uninstall it?\\nWhat are the plans for this will it get updated from time to time?I’d like more insights on if it is installed on a computer or not.\\nAlso a silent install would be great.Thanks for your help!Regarding uninstalling, see my answer here:Regarding silent install, see my answer here:Powered by Discourse, best viewed with JavaScript enabled',\n",
       " 'dropping-contacts-in-solver-on-heightfield-convex-contact': 'Running PhysX 3.3.4 I am getting this error from Checked builds:\\nDropping contacts in solver because we exceeded limit of 32 friction patches.The relevant part of the system (i.e. the only source of contacts to my knowledge) is between a convex mesh dynamic body which is roughly box shaped 0.6m x 4m and a heightfield (0.13m vertex spacing) whose heights change dynamically (This can cause penetration but from what I can see, not enough to result in violent depenetration). Occurrence of this warning appears to coincide with my object falling through the heightfield as if it wasn’t there. I would like to treat the circumstance causing the warning as the true problem and the object falling through the heightfield as a symptom of this problem. I would like suggestions of strategies to avoid this warning. Here are some ideas I’ve come up with:Disable friction. I don’t actually require friction between the body and heightfield so in theory I don’t need the friction patches at all. However I’ve disabled friction and still get this error. I believe these patches are generated regardless and are possibly relevant when resolving contacts even in the absence of friction.Reduce the resolution of my heightfield. I have experimented with this and it appears to improve the situation (i.e. make the warning occur less frequently). Unfortunately I am unable to change the resolution of my source data so I have to do some sort of downsampling. This can result in a loss of locality of height changes. The greater issue though is that this solution does not appear to completely resolve the issue.Change the geometry of the rigid body. It’s possible that a simpler geometry like a box or capsule would generate fewer distinct friction patches. I haven’t experimented with this yet as I would have to compromise on fidelity of geometry but it might be an acceptable option.Change the hardcoded limit of 32 contacts to something larger. I haven’t investigated this at all and in general I’m disinclined to modify the PhysX source code. I suspect the 32 bit limit is in order to store contact masks in a 32 bit integer which would make making it larger a significant undertaking.Upgrade to PhysX 3.4. I have no specific reason to think this would improve the situation but it’s a variable I could tweak.Replace the contact with a PxVehicle like strategy which generates a bunch of soft contacts outside the PhysX tick. I think this may actually be the best solution but it would be a lot of work so I’m hoping to avoid it.Divide the geometry of my rigid body into smaller shapes. It appears that the 32 contact limit applies per pair of contacting shapes so If I have a larger number of smaller shapes then each one should have fewer contacts. I worry that this may result in reduced performance due to redundant division of shapes and a consequent increase in contact count.I’m interested in feedback on any of these ideas or suggestions for alternative ways to avoid this warning.Powered by Discourse, best viewed with JavaScript enabled',\n",
       " 'exception-multimanifold-custom-simulation-on-unrealengine-using-immediate-apis': 'I’m developing a game and I need to customize the execution of physics simulation, to doing this I implemented a class that each tick executes the simulation using immediate mode APIs.I’m developing under UnrealEngine where is created a scene that drive all game simulation. Each tick is execute the the function PxScene->simulate and I’ve replaced this with a function created by me that should simulate the physics using immediate mode.As starting point to create the custom simulate function, I took the snippet written by you, and after some customization to be able to run it under Unreal I get the code ready to be compiled.The problem is that when I start the game I get this error:The file where this error is thrown is: PhysX_3.4\\\\Source\\\\GeomUtils\\\\src\\\\pcm\\\\GuPCMContactConvexMesh.cpp at line 124.I’ve debugger the code but I cannot understand what is causing this exception, here is a picture of debugger\\nhttps://www.dropbox.com/s/ar6wwerzrwmnk6d/Untitled.png?dl=0\\nExternal ImageInto my function the exception starts from here:and the function GenerateContacts is:where physx::immediate::PxGenerateContacts is called.If you check the debug the on the picture on tab Watch (Point 4)\\nyou can see the problem could be caused by two parameters nNumManifolds and mNumTotalContact.So who creates the parameter multiManifold? For sure the problem is on the code I’ve written, but I need to understand where this variable is created.Please help me to get out from this problem, and If you need more information / code please let me know.Thanks in advance.There was an issue with meshes that was fixed in January. Depending on which version of UE4 you have, you may have the bug still. The fix will definitely be contained in UE4.16, which is available in pre-release form on GitHub now.In NpImmediateMode.cpp, ~line 681:if it looks like this::if (cache.isMultiManifold())\\n{\\nmultiManifold.fromBuffer(reinterpret_cast<PxU8*>(&cache.getMultipleManifold()));\\ncache.setManifold(&multiManifold);\\n}it should instead look like this::if (cache.isMultiManifold())\\n{\\nmultiManifold.fromBuffer(reinterpret_cast<PxU8*>(&cache.getMultipleManifold()));\\n}\\nelse\\n{\\nmultiManifold.initialize();\\n}\\ncache.setMultiManifold(&multiManifold);Assuming your code matches the latter, then there should be no circumstances in which your manifold can be null. You are responsible for allocating memory that the multi-manifold references from your PxCacheAllocator.Hope this helpsKierI don’t know how to say you Thanks for this answer!!!Yes, my code is look like the old version. So I’m downloading 4.16 to check if the problem is solved.Thanks you alot!! I’ll update you soon. Thanks!Perfect this problem is resolved!! Thanks you!Powered by Discourse, best viewed with JavaScript enabled',\n",
       " 'graph-nodes-in-optix': 'This seems to be a major discussion point over here, with debates about local v.s. world space, and acceleration structures, and the reasons for possible design decision.  ex// anything in world space skips the needs for a transform. And the better I understand the underline design decisions, the better I can work with the API.Currently, we have a world or scene root that is a rtGroup.  It is a sort of object container, along with an acceleration structure for spatial sorting, it references individual objects composed of (in parent → child order)rtTransform        ( spatial orientation)\\nrtGeometryGroup    ( allows connection to acceleration structure)\\nrtGeometryInstance ( links geometry to material)\\nrtGeometry         ( actual object to render)While what I’ve seen most commonly is… a scene root, if any.  Then a set of instances (hold material, transform) which point to geometry (all local space, including acceleration structure in local space).  Although an instance can have different material, not having a transform would make it of little use, I imagine.I just wanted to gather some thoughts on the way the Graph nodes are set up, and what may be some of the advantageous.That’s solely depends on the requirements you have. There are important performance differences to consider though.OptiX renders fastest with a flat hierarchy.\\nIf you need to manipulate whole groups, adding Transform nodes saves most of the acceleration structure rebuild needed instead of updating the geometry required otherwise. For any affine animations Transforms and a Bvh acceleration structure at the root with “refit” should be the fastest way.\\nIf your application’s scene graph has the possibility of very deep transform hierarchies, a naive translation to OptiX nodes will not be the fastest. Mind there can be factors in rendering performance between a complicated hierarchy with many small geometries and a fully optimized scene.You might want to read some of our Developer Technology Professional Visualization team’s presentations on techniques for fast scene graph traversal resp. the validation and traversal of a scene tree built from that.\\nLinks here (more on GTC 2015 next week): [url]https://devtalk.nvidia.com/default/topic/777618/scenix/announcing-nvpro-pipeline-a-research-rendering-pipeline/[/url]The other level of local vs. world coordinate space decision is in the closest_hit program implementation. It’s also your choice if you want to transform world space hits into local space and calculate BSDFs there or if you do calculations in world space. Some things are simpler in local space. It’s just a matter of the number of dot-products you require.“more on GTC 2015 next week”  I’ll be there!  And thanks for the link.Personally,\\nI prefer to do everything in local space.  Although, I’d assume it would be possible to start with a world space accel structure and using a single matrix back transform to local and forward transform to a new location at once. (like what is common in bone skinning)Agreed on “very deep transform hierarchies”   We save it for the cpu.   If you want a hierarchical scene graph of objects local to other objects, Keept it, do the transforms on cpu (walk the tree and calc final matricies), link each tree node to a single layer deep ( a list ) object in optix, then pass that final matrix into it.So, in summary, we would like to haveGeometry (local space) with an accel structure (local space)and instances thereof, which allow placing geometry in multiple locations, each with its own material.With that in mind,  does this make sense?  Or is there a simpler set up.rtGroup  ← world or scene root that is a container for everything else.Instances… consisting of:   ( this is the part where it gets a bit more complicated)\\nrtTransform        ( spatial orientation)\\nrtGeometryGroup    ( allows connection to acceleration structure, gets it from our geo object)\\nrtGeometryInstance ( links geometry to material)Geometry … which can be instanced, our geo object\\nrtGeometry ( actual object to render)Right, that’s the usual structure for a scene with instances.\\nSee OptiX Programming Guide chapter 3.5.6. Shared Acceleration Structures, Figure 4 for an example how that looks for a scene which has two transformed instances of the same GeometryGroup.Note that Groups and GeometryGroups hold or share the acceleration structure, not the Geometry or GeometryInstance nodes.and instances thereof, which allow placing geometry in multiple locations, each with its own material.“Own materials” won’t work with model instancing via Transform nodes like that.\\nThe Material is assigned to the GeometryInstance which is part of the sub-tree under the GeometryGroup under the Transform. Means the Material in that is the same for any Transform having this sub-tree as child.You would need to create individual sub-trees with different Material nodes to be able to use different materials on instanced model geometry. It doesn’t matter if the Material is actually using a different closest_hit implementation or just different material parameters.\\nThat will become costly for bigger scenes. Build times scale linearly with the number of nodes in the scene.\\nWhat you definitely should do is sharing the acceleration structure among all GeometryGroups holding identical geometry to save a lot of memory.The Material is assigned to the GeometryInstance .  Yup, that is what I meant.  We actually just use a single material (for our purposes) with each instance having a set of params used by that material, such as specularity, transparency, diffuse color.Powered by Discourse, best viewed with JavaScript enabled',\n",
       " 'good-a-job-as-they-care-you-know-ill': 'Good a job as they care you know I’ll and I’ve been involved after one million for a dozen years now a I can’t help number people I knew you were cheating on my hands you know um and those in them eventually got caught in fact everything we’re staying at the time that I I’d RageDNA new with gene got caught some point so I yeah I just I you know you-know-who are this year’s absolutely no doubt about it but is not nearly as prevalent as most people think I’ll but virtually everyone was in as hoot themselves in the .\\nhttp://supertestoboostsfacts.com/ragedna/Powered by Discourse, best viewed with JavaScript enabled',\n",
       " 'how-to-get-started': 'Upon recommendation from Solidworks Visualize Forum, I downloaded the MDL and Exchange. I use Rhino 7 , VRAY, Solidworks2020, and Visualize2020. I have no idea how to access these materials or use them in the applications. Where is the documentation, or instructions on how to use them, modify, create, etc?Kindly,ThomasWe are currently working on a documentation for how to use vMaterials with various rendering applications. We will post these instructions on our vMaterials webpage soon.Any news on this? I used to use an old version (1.6 I think, the one with a .mat included).\\nGot 2.0 now and all I get is ‘failed to load errors’ trying to import the ,mdl’sUsing 3dsmax’22, IrayIn Iray for Rhino on Windows, a mdl folder is automatically created in the “Documents” folder and the vmaterials installer also places a copy of the vmaterials files there. Correctly adding the path of this directory to the Iray settings and restarting Rhino should hopefully add the materials to the library. Perhaps the workflow is similar for 3dsmax.\\n\\nvrmaterials_installation_rhino-iray800×538 156 KB\\nPS link to the webpage for the Rhino Iray plugin: http://irayrhino.com/Dear vve\\nthe new version of vMaterials 2.0 does not use .mat (like a material library).\\nYou can import mdl files as you did, but you have to define where you installed the library.Below is an image of where and what to do.Now it should work without failing errors. Please tell me about your result.\\nimage1278×1655 112 KB\\nPowered by Discourse, best viewed with JavaScript enabled',\n",
       " 'any-hit-program': 'I am new to OPTIX.\\nI want to use it for ray tracing on triangular mesh.\\nEach triangle in the mesh is opaque (i.e. the ray is perfectly reflected).\\nAlso I want to allow up to 6 secondary rays (i.e. bounces).\\nFor this case does the any hit program is mandatory or the closest hit program suffice?You do not need an anyhit program for a ray type which only needs to handle the closest hits of opaque objects.\\nThe default anyhit is a no-operation which means you automatically get the closest hit when that ray type traverses the scene.More explanations here:\\n[url]http://raytracing-docs.nvidia.com/optix/index.html[/url]\\n[url]http://raytracing-docs.nvidia.com/optix/guide/index.html#programs#closest-hit[/url]\\n[url]http://raytracing-docs.nvidia.com/optix/guide/index.html#programs#any-hit[/url]As OptiX beginner please have a look at the OptiX introduction presentation and example sources.\\nLinks here: [url]https://devtalk.nvidia.com/default/topic/998546/optix/optix-advanced-samples-on-github/[/url]\\nThe fourth example already does what you’re asking for and it’s getting better from there.Powered by Discourse, best viewed with JavaScript enabled',\n",
       " 'nvv4l2h264enc-emit-i-frames': 'Hello,\\nOn the nvv4l2h264enc gstreamer plugin, is there a way for the gstreamer plugin/pipeline to emit a signal when an I frame is generated by the encoder. I looked thru the properties exposed by this plugin and nothing seems to correlate to this.Failing this, is there some way to setup the encoder so that the size of the frames would correlate (strongly) with an I frameThanks\\nVictorHi,the nvv4l2h264enc itself does not emit any signal for NAL units, so you can evaluate the NAL units into the GStreamer application or creating a custom GStreamer Element that triggers a signal for each I frame.You can control IDR interval using iframeinterval property into nvv4l2h264enc (See below validation).Regards\\nAngelPowered by Discourse, best viewed with JavaScript enabled',\n",
       " 'vk-nv-raytracing-cannot-create-bottom-level-as-with-two-kinds-of-geometries': 'I try filling BLAS creation info with the following:But when i request the memory requirements for the scratch buffer i get zero.\\nIf i change geometryCount and pGeometries in VkAccelerationStructureInfoNV and point to only one of the two geometries then i get the right memory requirements.I have an RTX 2070 GPU and the OS is Windows 10.\\nDriver version: 417.35Thanks in advance.I don’t think you are allowed to mix triangles and AABBs in the same bottom level acceleration structure.Did you ever manage to get your application working with AABBs and procedural geometry?VkAccelerationStructureInfoNV contains the member pGeometries and the procedural geometry is another instance of VkGeometryNV. I have not seen any prohibition in the Vulkan spec. Maybe i missed something or it is not clear enough. Either way yes i managed to get an app with AABBs and procedural geometry working.\\nYou can find it here: https://github.com/georgeouzou/vk_expThanks for the github link. It helps to see another implementation and a different approach when debugging mine.I know the bottom level acceleration structure API accepts both (and the C signature technically doesn’t prevent you from providing triangles and AABB geometries at the same time). But it does smell fishy with respect to hit groups that can only be specified at the top level AS and their role with hit group shaders.It feels like the specification needs clarification. It’s not quite clear to me also if it’s valid to have a hit shader group that handles both triangles and AABBs at the same time.Given that in practice the HitAttribute used to communicate between the intersection and closest-hit shaders are quite different between triangles (vec2 barrycentrics) and procedurals (whatever you want), it feel like putting the triangles and AABBs in separate bottom level acceleration structures is the right approach.@gouz, i have run into a “zero memory requirement” error too, but in a different area of building AS. RTX reports a zero update memory scratch size for compressed AS. Have you tried to conservatively guess the correct size of the memory and then let it build? That did the trick for my problem, that is you can in fact update compressed AS.Regards@gouz just tested it myself, and it does build, but does not show, so i guess this is a double bug.@GPSnoopy you cannot specify both in the same geometry, but you should be able to have multiple geometries that mix triangles and aabbs in the same AS. The hit group in the top level is an offset into the SBT, and the traceNV in the shader supplies the stride for geometries. So you can in fact have different hit groups for different geometries. In fact that is pretty standard if you consider baking static models with a bunch of different diffuse textures into one AS. Each geometry would represent one “texture group” and have different SBT entries that contain the index into a texture array.Regards@virtual_storm Thanks i will try this again, working around the “zero memory requirement”\\nI tried to get it working with multiple geometries and using the offsets in traceNV as you mention but i could not create the combined BLAS.It is clear that it is impossible to create mixed-type BLAS if you look at type definitions in vulkan header. VkGeometryTypeNV is not a bit mask. It has only two possible values, which occupies only the same single bit: VK_GEOMETRY_TYPE_TRIANGLES_NV = 0 and VK_GEOMETRY_TYPE_AABBS_NV = 1. You cannot combine them using bitwise OR when fill VkGeometryNV.geometryType with value. There is no variant data type in C, so VkGeometryDataNV contains both structures, one of them is the only active – I think this is the reason.Powered by Discourse, best viewed with JavaScript enabled',\n",
       " 'which-video-card-is-better-for-adobe-premiere-1080-1080ti-or-2060': 'which video card is better for adobe premiere 1080 1080ti or 2060what you do exactly with adobe premiere ? give some examples .1080 faster by 20-30% than 2060\\n1080Ti faster by 25-30% than 1080\\n1080Ti faster by upto 50% than 2060this is raw core performance , means not sure if you see much extra performance in adobe premiere2060 have faster DDR6 ramМне для рендеринга видео 4k что лучше 1080 или 2060?Реально какой мошнее для ранеринга видео ?Powered by Discourse, best viewed with JavaScript enabled',\n",
       " 'problems-with-setmaxtracedepth': 'I’m porting an application from OptiX 5.1.0 to OptiX 6.0.0 and running into stack overflow errors with setMaxTraceDepth. If I disable RTX and use the old setStackSize function, it works. When I enable RTX and use setMaxTraceDepth/setMaxCallableProgramDepth, I keep getting stack overflows. At the moment I’m debugging with a minimal test case that casts a single ray per thread, and then may cast a single shadow ray from the closest hit program (though code to iteratively cast more rays is still present). The code does not currently use callable programs.\\nI’ve tried numerous values from 0 to 31 for setMaxTraceDepth, and several combinations with setMaxCallableProgramDepth, but I keep getting stack overflow exceptions. If I use 32 for setMaxTraceDepth, I get the following error: “Encountered a rtcore error: m_exports->rtcPipelineCreate( context, pipelineOptions, compileOptions, modules, moduleCount, pipeline ) returned (1): Invalid value)”.\\nLooking at the usage report, I see the direct and continuation stack sizes for the various optix programs. I would think that the total stack size should be at least rayGenContinuationStack + closestHitContinuationStack * MaxTraceDepth. Which would make the total stack larger than my directly-specified non-RTX stack.\\nAttempting to troubleshoot this has raised some questions:\\n1.) What conditions can generate a stack overflow exception? Is it only thrown when the program’s stack usage exceeds the allocated stack size, or could other conditions like requesting too much stack generate the exception?\\n2.) Are the various stack sizes computed from the optix programs used with the max trace depth to create a single total stack size which can be meaningfully compared with the legacy stack size?\\n3.) What does the rtcPipelineCreate error thrown when calling setMaxTraceDepth(32) signify?\\n4.) Does optix code running in RTX mode use the stack the same way code running in non-RTX mode does?\\n5.) Do you have any idea why the stack overflow exception keeps occurring regardless of the max trace depth? This code uses large ray payloads and requires a substantial stack, but it’s an iterative path tracer with no real recursion, and in non-RTX mode it stays within stack bounds and runs fine.ThanksCUDA 10.0, OptiX 6.0.0\\nQuadro P2000 (laptop), driver version 419.17\\nWindows 10, Visual Studio 20153.) What does the rtcPipelineCreate error thrown when calling setMaxTraceDepth(32) signify?It seems to be the max allowed value. see:\\nhttps://devtalk.nvidia.com/default/topic/1047924/optix/-solved-rtcore-link-error-after-45-animation-frames/post/5317965/#5317965@bdr,m1 is right; the maximum allowed value for setMaxTraceDepth() is 31. Same goes for setMaxCallableProgramDepth(). The default depth is 5 for both types. The invalid value error you’re getting is referring to 32 being out of bounds, but I think we could probably improve that error message.It sounds like the default values should not be causing a stack overflow exception for you. Unless you have some hidden recursion or someone snuck secret callables into your code, based on your description, you should be able to turn down the values for both depths from the default. Would you be able to easily strip your code down to something minimal that causes the problem and share it with us?To answer your other questions:1- Requesting too much stack should not throw an exception, it will just be disallowed with the cryptic error message you’re already getting.2- Yes, the depth values in RTX mode are used to create an internal stack size in bytes that could be compared with the old way. We were trying to make it more intuitive and simpler since it’s often hard to know how big your stack frame needs to be in bytes.3- Answered above.4- RTX mode stack usage is not the same as the old code.5- I don’t know yet why you’re getting the exception, it could be a bug in stack size computation on our end. A very large payload could be a problem. If you can help us reproduce the problem, we will track it down.–\\nDavid.I was able to reproduce the problem by modifying the SDK’s optixPathTracer example to increase the stack usage. Other than enabling RTX and some error printing, the main modification is adding a 2048 float array to PerRayData_pathtrace.\\nI can send the zipped source if you’d like.Awesome. Yes, please. I’ll try the modification manually, but it’d be great to have your copy for reference anyway.–\\nDavid.Powered by Discourse, best viewed with JavaScript enabled',\n",
       " 'dynamic-changing-of-frame-rate-in-nvenc': 'Hello everyone,\\nDoes the nvEncodeAPI support dynamic changing of frame rate? I mean, I want to change the frame rate in the encoding process.I have checked the API, but found no such function or paramters.What about NvEncReconfigureEncoder() ?\\nI think that frame rate (frameRateNum/frameRateDen) is used only for estimating average frame rate input to NVENC to achieve rate control (eg. quality and size that is computed from “*BitRate” and “frameRateNum/frameRateDen”) (rcParams - rateControlMode (CBR, VBR, CostQP …), averageBitRate, maxBitRate, targetQuality). NVENC encode frame as fast as possible and do not add PTS/DTS to frame (only elementary stream).\\nSame as NVDEC decode frame as fast as possible and you must queue decoded frame before display to match PTS/DTS timing ([url]https://devtalk.nvidia.com/default/topic/1022976/video-technologies/nvdec-frame-display-rate/[/url]).mcerveny’s answer is correct. The frame rate has no meaning in NVENC other than deciding rate control parameters.Powered by Discourse, best viewed with JavaScript enabled',\n",
       " 'optix-denoiser': \"I am trying to denoiser to my project, it said 'OptiX Error: ‘Unknown error (Details: Function\\n“_rtCommandListFinalize” caught exception: Could not load denoiser library.)’ when finalize, anyone knows why, thanks.Inside your executable installation, do you have the optix_denoiser.51.dll and cudnn64_7.dll next to the optix.51.dll?Thanks again Detlef, yes, it is because of lacking dll files.:)Hi Detlef, I’m having a similar problem on linux (ubuntu 18.04 LTS).optix_advanced_samples/optixIntroduction/optixIntro_10 is throwing:“Unknown error (Details: Function “RTresult _rtCommandListFinalize(RTcommandlist)” caught exception: Could not load denoiser library.”on m_commandListDenoiser->finalize();I should also say that I modified optix_advanced_samples/CMake/FindOptiX.cmake:set(OptiX_INSTALL_DIR $ENV{OptiX_INSTALL_DIR} CACHE PATH “Path to OptiX installed location.”)So I assume my error is because I haven’t linked liboptix_denoiser.so and libcudnn.so correctly. Any hints on how to fix this?GPU RTX 2080, OptiX 5.1.1RTX 2080 isn’t supported by OptiX yet. There is a similar post not that far back about the denoiser failing on a 2080. Personally I can’t even get the ray tracing working on my 2080.As I suspected, it does seem to have been a simple case of not linking my libraries correctly. I added lib64 to LD_LIBRARY_PATH in my .bashrc file so that it reads:And I no longer get the “Could not load denoiser library” error. Instead, I now get:Backtrace:\\n(0) () +0x711f17\\n(1) () +0x70e701\\n(2) () +0x42b69e\\n(3) () +0x42a00b\\n(4) () +0x3f8214\\n(5) () +0x1aaa1c\\n(6) rtCommandListExecute() +0x1d6\\n(7) optix::CommandListObj::execute() +0x9d\\n(8) Application::render() +0x91\\n(9) main() +0x61b\\n(10) __libc_start_main() +0xe7\\n(11) _start() +0x2aBacktrace:\\n(0) () +0x711f17\\n(1) () +0x70e701\\n(2) () +0x70e549\\n(3) () +0x3c0e8a\\n(4) () +0x1c9541\\n(5) () +0x17a69b\\n(6) rtContextLaunch2D() +0x2b9\\n(7) Application::render() +0x224\\n(8) main() +0x61b\\n(9) __libc_start_main() +0xe7\\n(10) _start() +0x2a================================================================================I gather the second error is when I try and hand the failed denoiser buffer to render buffer.I also get the same -40 error when I try to run the optixDenoiser example.I’ve yet to have a dig around for what this error code refers to, so perhaps this is the 2080 error you were talking about papaboo? I haven’t had any trouble running some of the other examples with my 2080.Please have a look into the OptiX 5.1.1 release notes:[i]Known Issues The OptiX 5.1.0 version supported it, but with a comparably slow fallback because the cuDNN version used was not containing the required features for the fast path on Turing and the one in OptiX 5.1.1 does neither. The next major OptiX version will address this.Thanks for that Detlef.I’ll focus on other aspects of OptiX until the next release then.Powered by Discourse, best viewed with JavaScript enabled\",\n",
       " 'zed-camera-gpu-usage': 'We are using a ZED Camera with Jetson TX1, and the GPU usage is maxing out with a minimal configuration of the ZED Camera. Is there any way we could add-on to the GPU of the TX1 externally or with another TX1, so as to be able to use the combined GPU power?Powered by Discourse, best viewed with JavaScript enabled',\n",
       " 'how-do-i-enable-txaa-within-my-game-engine': 'I’ve been searching around for a code sample of how to enable TXAA and couldn’t find anything. I am running a Nvidia Geforce GTX 680 card. Inside my program I wrote to file what the multiple sample qualities and level the device could have. So, I’m using sample count of 8 and quality level of 32. I still don’t know if that would be TXAA…Would anyone like to point me in the right direction? Is it a library or is it done within a shader file? Thanks!i know this aint the answer your looking for, but for now you could just force it on Nvidia control panel… ok i did some poking around in google and i think this link might interest you, its add on for forcing AA in most gamesDownload  injectSMAA v1.2   Description   adds \"Subpixel Morphological Antialiasing\" to an application  is based on \"injectFXAA\" (written by...\\nlooks like its similar to sweetfx if you have ever used that before.Powered by Discourse, best viewed with JavaScript enabled',\n",
       " 'nx-csi-camera-video-save': 'I have a CSI camera and mount on Xavier NX. My cpu is full with other program and now I want to save video with image from CSI camera. Is there any way can meet my needs? I don’t want to use cpu to do this task, I want to know if thre is a way can record video from hardware which not through cpu?Powered by Discourse, best viewed with JavaScript enabled',\n",
       " 'nvenclockbitstream-returning-wrong-frame-type-for-interlaced': 'I’m trying to use NVEnc to encode interlaced video. My product makes decisions at the muxer stage based on frame type, so I’m passing the frame type from the encoder down the chain to the muxer. This works fine for progressive video, as all the frame types returned by the encoder match the actual frame types. But when I encode interlaced video the very first frame returned is marked as a P frame even though according to the NALs it’s an IDR. Is this a bug? Or is there some setting I’m missing that will cause it to return the proper frame type in the struct? I’d hate to have to parse every frame myself just to determine the proper type.Looking at the NALs closer I see that you guys use a IDR for the top field and P for the bottom field, but the frame type in nvEncLockBitstream is always matched to the bottom field. I need to know when it’s an IDR frame because I use that info up stream to know which frames contain an SPS/PPS. I don’t want to have to parse the NALs of every frame looking for SPS/PPS.Any way we could get a separate variable in the nvEncLockBitstream for top and bottom fields?Powered by Discourse, best viewed with JavaScript enabled',\n",
       " 'memory-corruption-with-ccd': 'PhysX version : 3.3.1My test case is a little lengthy to post so I’ll try to describe as best what is happening. The scene is composed of several actors 1 triangle mesh, 5 planes, several box shapes and several sphere shapes. All actors have CCD enabled ( small scene so I’m not worried about CCD overhead ). The sphere shaped represent projectiles which are emitted on a regular basis. If the sphere shaped collide with any other object in the scene the app can cleaning shutdown without throwing a memory corruption assertion (debug build ). However, if the 2 spheres collide, then an assertion is thrown at shutdown due to an invalid pointer being deleted. If CCD is not enabled, there is no assertion.NOTE: I have implemented an allocator that allocated aligned memory using _aligned_malloc/_aligned_free. I’ve also tried a manual alignment scheme, both with the same result. To further convince myself that the issue is not with the allocation, I added allocation tracking in the debug build and the memory address being sent to allocator to be freed is not present in the list of allocated memory.Been going through the documentation wrt to CCD, memory allocation and nothing has jumped out at me yet. Also I’m using a single simulation thread in my application.Powered by Discourse, best viewed with JavaScript enabled',\n",
       " 'crash-physx-3-3-1': 'I have a crash in the following scenario:Crash in fetchResults() with: “Access violation reading location 0x00000000.”\\nTried to LockWrite/UnlockWrite during section 7 - the same problem.I’m using PhysX 3.3.1, Windows 7 x64(but my application is x86)Many thanks for the repro.  This is indeed a bug.  We’re working on fixing this for 3.3.3.The problem is caused by the combination of joint->setActors and actor->release while the simulation is running.  A workaround would be to defer any one of those calls to after fetchResults.Thanks,GordonPowered by Discourse, best viewed with JavaScript enabled',\n",
       " 'how-to-measure-bandwidth-from-pinned-host-memory-to-device-memory-on-aws-a100-p4d-24xlarge': 'I want to measure the bandwidth from pinned host memory to device memory on NVIDIA A100. On AWS p4d.24xlarge machine, 8 NVIDIA A100 with PCIe 4.0x16 is supported, so the ideal bandwidth should be 31.5GB/s. But I only get the result of about 13GB/s (from pinned host) by running the below code on NVIDIA developer blog.code-samples/profile.cu at master · NVIDIA-developer-blog/code-samples · GitHub .Is there any problem with this code, or any other reason why the speed cannot reach the ideal 31.5GB/S?Hi there @15801191730 and welcome to the NVIDIA developer forums!I am definitely not the expert on AWS capabilities so a question to start with: Does AWS guarantee full PCIe x16 support for all 8 GPUs? That would block 128 lanes, but if that AWS instance still uses Intel 8275CL they only seem to support 48 each, giving 96 total?I might be mistaken in my assessment here, but that would be worth checking I suppose.Still PCIe x8 should yield around 2GB/s more than what you measured, I am not sure how much that difference can be attributed to overhead. But then again, those code samples are 7 years old.If this does not help you answer your question then I think the people in the DGX user forum might be better suited to help.And of course the CUDA team, who might be able to provide a more up to date example of code to measure pinned host memory bandwidth.You can move your topic to those forum categories if you want.Thanks!Powered by Discourse, best viewed with JavaScript enabled',\n",
       " 'how-much-will-upgrade-video-card-count-in-image-processing': 'Hello,I have a Dell Precision 490, with two 3Ghz dual Xeon processors, 16GB of Ram.The current video card is nVidia Quadro 3450, PCI Express x16 1.0 graphics card slot;I would like to know if my workstation allows for the video card upgrade to:nVidia Quadro K2000 or nvidia Quadro K4000First of all I would like to know if the Dell Precision 490 system specs (I have a large power source) permit it.Also, my PCI Express x16 1.0 graphics card slot should allow for a PCI Express x16 2.0 video card to fit the slot but downgrade the speed a little, as far as I have read. I guess I could replace cards with my own hands…\\nSo this first question is actually more for the people at Dell…Also, second question:I would like to know other people’s opinion if this upgrade is worth while…Mainly I will use my system for heavy large file editing in Photoshop (CS6& CC) and intensive 3D work in Autodesk Maya(2011&2014) and Vue Infinite.Will one of the above mentioned video cards make a considerable difference in speed while I process photography(usual curves,saturation,etc and plugins such as Nik Software and Topaz Sharpener) \\\\ and display 3D graphic\\\\s in the viewports? I know that in 3D rendering the processors&RAM are used, not the video card. In viewport display, video card is used but how much improvement would come with an upgrade? Perhaps all the textures would show faster :)When will the CUDA cores be used and is the new video card architecture important for this kind of work? Will OpenGl display benefit from this? ( I believe both Photoshop & Maya use OpenGl for display but will it work better?)But sometimes I will also work with HD video files with Adobe Premiere CS6&CC and Adobe After Effects. Here as far as I know the new video card will make very very much difference… but how much?I would appreciate advice on having this upgrade…Thank you,DanielIn the thread list I see this topic already has 3 replies but I can’t see any of them listed!\\nWhy is that?That was deleted SPAM. The forum software doesn’t show it, but the counter stays.Powered by Discourse, best viewed with JavaScript enabled',\n",
       " 'is-there-nvdecode-support-h264-bs': 'I am studying “NVDecode” to write a video player using D3D11.\\nAnd one branch is input raw H264 bs to decode.\\nAccording to “NVDEC_VideoDecoder_API_ProgGuide.pdf”, chapter 3.1,\\nUse cuvidCreateVideoSource(), it return error value\"300\",\\nDoes cuvidCreateVideoSource() doesn’t support raw H264 BS?Shouwld I call cuvidCreateDecoder directly when raw bs input?ThanksH.264 decoding is supported. What is the meaning of raw bitstream? Have you tried decoding the same bitstream using other decoders? Can you share your code and the bitstream?Thanks for reply and apology for my late answer.\\nI user NvDecodeD3D11 of Video_Codec_SDK_8.0.14 to test decode.\\nUse parameter:-i “Framerp264.264” -o “out.yuv”.to decode Framerp264.264, which is 264 raw bitstream.\\nBut NvDecodeD3D11 cannot work. It quit.\\nDo you have run similiar test unit?Powered by Discourse, best viewed with JavaScript enabled',\n",
       " 'topic': ';\\n&-/=qPowered by Discourse, best viewed with JavaScript enabled',\n",
       " 'nvidia-driver-510-not-loaded-on-ubuntu-20-04-after-the-update-second-monitor-also-not-working': 'HelloI have a problem with my NVIDIA DRIVER 510 installed on UBUNTU 20.04. After the update/actualisation and then reboot, my second monitor did not work and nvidia was not loaded, I have already tried to reinstall several times. But nvidia still not loaded. Security boot is disabled in BIOS.\\nDual Boot machine with WIN 10 64bit. Dell Precission 3450. Driver 510 is recommended.I tryed:nvidia-smi\\nresponse:\\nNVIDIA-SMI has failed because it couldn’t communicate with the NVIDIA driver. Make sure that the latest NVIDIA driver is installed and running.nvidia-settings\\nresponse:\\nERROR: NVIDIA driver is not loadedERROR: Unable to load info from any available system(nvidia-settings:17488): GLib-GObject-CRITICAL **: 12:12:35.095: g_object_unref: assertion ‘G_IS_OBJECT (object)’ failed\\n** Message: 12:12:35.098: PRIME: Requires offloading\\n** Message: 12:12:35.098: PRIME: is it supported? yes\\n** Message: 12:12:35.124: PRIME: Usage: /usr/bin/prime-select nvidia|intel|on-demand|query\\n** Message: 12:12:35.124: PRIME: on-demand mode: “1”\\n** Message: 12:12:35.124: PRIME: is “on-demand” mode supported? yesgrep nvidia /etc/modprobe.d/* /lib/modprobe.d/*\\nresponse:\\n/etc/modprobe.d/blacklist-framebuffer.conf:blacklist nvidiafb\\n/lib/modprobe.d/nvidia-kms.conf:# This file was generated by nvidia-prime\\n/lib/modprobe.d/nvidia-kms.conf:# options nvidia-drm modeset=1and…lspci -v | grep VGA\\nresponse:\\n00:02.0 VGA compatible controller: Intel Corporation Device 9bc5 (rev 05) (prog-if 00 [VGA controller])\\n01:00.0 VGA compatible controller: NVIDIA Corporation GP107GL [Quadro P1000] (rev a1) (prog-if 00 [VGA controller])Thank you very much for any help.please find attached nvidia report bug:\\nnvidia-bug-report.log.gz (3.8 MB)I have exactly the same problem after the updates to Kubuntu 20.04 LTS yesterday.\\nI have also tried reinstalling, uninstalling, switching to the non-Nvidia driver and back.  I have no blacklisted files in modprobe.d.(base) ian@MSI:~$ nvidia-settingsERROR: NVIDIA driver is not loadedERROR: Unable to load info from any available system(nvidia-settings:10480): GLib-GObject-CRITICAL **: 18:13:20.645: g_object_unref: assertion ‘G_IS_OBJECT (object)’ failed\\n** Message: 18:13:20.647: PRIME: No offloading required. Abort\\n** Message: 18:13:20.647: PRIME: is it supported? no\\n(base) ian@MSI:~$ sudo nvidia-bug-report.shnvidia-bug-report.log.gz (81.9 KB)Welcome @ian.jefferson and @blahusiak to the NVIDIA developer forums!Thank you for already sharing your bug report log files, we will have a look at them.As Ian pointed out the Graphics/Linux forums are a great resource for solutions to similar issues like these.Quite often Ubuntu does distribution updates that introduce incompatibilities with the recommended drivers certified as part of the distribution. In those cases the safest bet is either to roll-back for example the kernel, if it was changed, or look for an updated NVIDIA driver on our driver download pages.Hello Thank you for your help.I tryed this one (links bellow)- to update liquorix:resp.Liqourix Kernel is a free, open-source general-purpose Linux Kernel alternative to the stock kernel with Ubuntu 20.04 LTS Focal Fossa. It features customAnd after the reboot it works!  NVIDIA is loaded and both monitor work!\\nscreen808×476 151 KB\\nThank you very much :)That is great to hear!\\nThanks for letting us know.This topic was automatically closed 14 days after the last reply. New replies are no longer allowed.Powered by Discourse, best viewed with JavaScript enabled',\n",
       " 'caliper-support-in-physx': 'Hi everyone,We have a requirement to support caliper that ‘steers along with wheels’ but ‘not move with wheels’. So to support this we are fetching PhysX supplied wheel transformation matrix → decompose matrix to determine roll, pitch, yaw → Use it accordingly to apply it to calipers. But unfortunately after certain angles of rotation it seems to be broken?Hence just wondering if there is any support for caliper through PhysX? If not can you please throw a light about how to handle this efficiently?Best Regards,\\nSuryaPowered by Discourse, best viewed with JavaScript enabled',\n",
       " 'optix-documentation-available-online': 'With OptiX 3.5 we stepped up the quality of our documentation. It’s more detailed, more accurate, and more browsable, since we use Doxygen for all of the reference manual. Also, OptiX is part of Nvidia’s new GameWorks program. As such, our documentation is hosted online at http://docs.nvidia.com. We’ll continue to improve it, but we’re excited about the progress we’ve already made.The reference manual main page is here: http://docs.nvidia.com/gameworks/content/gameworkslibrary/optix/optixapireference/_opti_x_chapters.htmlPowered by Discourse, best viewed with JavaScript enabled',\n",
       " 'nvidia-geforce-382-53-whql-with-vulkan-support': 'http://www.geforce.com/whats-new/articles/dirt-4-game-ready-driverVulkan support now included in mainline Windows drivers, no need to use beta developer driver anymore.Thank you very much!Do you know when, or if, linux will get a 364.* vulkan beta driver? Xorg 1.18 on Ubuntu 16.04 does not support the 355 series drivers.Yes please a Vulkan driver compatible with Xorg ABI 20 would be very nice.Do you have an ETA for Vulkan support in mainline Linux drivers ?vulkan driver does not work on gtx 770 windows 7 64 bit get error 16 when I run the vulkan installer.can anyone help me fix it? thank you .Powered by Discourse, best viewed with JavaScript enabled',\n",
       " 'creating-multiple-typed-views-from-the-backbuffer': 'Hi all,I want to create sRGB and regular views (both render targets and shader resources) from the swap chain’s backbuffer, although I suspect there are no legal ways of doing it.Thing is, you cannot create a swap chain with a typeless backbuffer format (which should be a prerequisite for creating multiple typed views), but, I am able to create a swapchain with a R8G8B8A8_UNORM_SRGB backbuffer and, from that, to create R8G8B8A8_UNORM views on my venerable GTX260 even though that’s illegal under the DirectX standard (it errors under the debug runtime).So I’d like to know whether this “feature” is reliable enough to ship a product with or just a happy glitch of my particular vendor/card/driver combination.CheersPowered by Discourse, best viewed with JavaScript enabled',\n",
       " 'unable-to-use-hevc-main10-profile-for-direct3d11-video-decoding': 'I’m trying to create a ID3D11VideoDecoder for decoding HEVC Main10 profile with the Direct3D11 video API.\\nThe problem I’m facing is the following:I have also tried to loop through resolutions starting from 1x1 to 2000x2000 - none of them worked.\\nI started playing around a bit and noticed that when I use DXGI_FORMAT_P010 or DXGI_FORMAT_420_OPAQUE as the OutputFormat I receive a config count > 0. However in my application I need NV12 as the OutputFormat. To me this looks like a driver bug.I have attached a little code snippet that illustrates the problem:My system specs:Windows 10 Enterprise 64-bit (10.0, Build 14393) (14393.rs1_release.161220-1747)\\nNVIDIA GeForce GTX 960, 4 GB\\nDriver: 378.66Powered by Discourse, best viewed with JavaScript enabled',\n",
       " 'vulkan-rtx-example': \"Is there a good Linux or multiplatform Vulkan RTX example?\\nI found this NVIDIA Vulkan Ray Tracing Tutorial | NVIDIA Developer but it has several errors. The worst is that textures are not loaded.\\nThanks.The example run on Windows fine, no texture missing. i think you need do some modify to make it work on Linux.I aslo learn vulkan RTX from other two example :examples/nv_ray_tracing_basic\\nexamples/nv_ray_tracing_reflections\\nexamples/nv_ray_tracing_shadows[url]https://github.com/SaschaWillems/Vulkan[/url]you can get full game with assets form here:\\n[url]https://www.nvidia.com/en-us/geforce/campaigns/quake-II-rtx/[/url]\\nand source code form github:\\n[url]https://github.com/NVIDIA/Q2RTX[/url]Two projects both support build on Linux and Windows.I’m going to do some advertisement for my own project:Implementation of Peter Shirley's Ray Tracing In One Weekend book using Vulkan and NVIDIA's RTX extension. - GitHub - GPSnoopy/RayTracingInVulkan: Implementation of Peter Shirley's Ray ...SachaWillems examples came after I finished mine, so I cannot comment on their quality (I assume they’re quite good, given the repo history).Otherwise yes, for something that NVIDIA is pushing so hard, its own examples are quite bad.Cool project GPSnoopy.Powered by Discourse, best viewed with JavaScript enabled\",\n",
       " 'glisenabled-gl-uniform-buffer-unified-nv-gl-invalid-enum': 'This seems to be a bug in the NVIDIA Graphics Driver (tested on latest 472.47 Win10 drivers), and should be a trivial fix for one of the NVIDIA OpenGL driver developers.PROBLEM: Calling glIsEnabled( GL_UNIFORM_BUFFER_UNIFIED_NV ) results in a GL_INVALID_ENUM errorAccording to:this should be valid.By contrast, the other 3 NV bindless enables can be queried successfully via glIsEnabled() without tripping a GL error.  So it appears support for GL_UNIFORM_BUFFER_UNIFIED_NV was just never added.Hello @Dark_Photon!Thank you for pointing this out. Did you try if this is only a problem of glIsEnabled() reporting GL_INVALID_ENUM or is it also not possible to use the extension?I will also check with our internal experts.Thanks for the reply, @MarkusHoHo.Did you try if this is only a problem of glIsEnabled() reporting GL_INVALID_ENUM or is it also not possible to use the extension?The core of the extension definitely works properly.  We’ve been using that successfully here for > 1 year for accessing UBOs in shaders.  i.e.:The problem seems to be limited to glIsEnabled() queries for GL_UNIFORM_BUFFER_UNIFIED_NV specifically.  I found this problem by adding some debugging code that “distrusts” the engine’s GL state tracking and queries GL directly to verify the currently-active values in the context.Thank you for the additional details!\\nI was able to reproduce the behavior also on the newest driver. But I am pretty sure it is not a real bug but rather an oversight in the original specification text. I think glIsEnabled should not have been mentioned there in the first place. From a functionality standpoint it does not make sense, since it is not a feature that can be enabled/disabled and there is a difference to EnableState/DisableState. And querying for Extension availability is not done this way either, but rather by using glGetStringi().Nevertheless I brought this to the attention of our engineering team to decide how this should best be handled.Thank you for the additional details!\\nI was able to reproduce the behavior also on the newest driver.Ok, good.  Thanks for testing it.But I am pretty sure it is not a real bug but rather an oversight in the original specification text.\\nI think glIsEnabled should not have been mentioned there in the first place.\\nFrom a functionality standpoint it does not make sense, since it is not a feature that can be enabled/disabledActually, yes it is a feature that can be enabled/disabled.  Just as the other 3 bindless enables can:as defined in these OpenGL extensions:What I meant was the distinction between the Server side glEnable() and the Client side glEnableClientState().Since UBO handling is defined as a Client side state it seems unlikely that you should be able to query it as a server side capability.What I meant was the distinction between the Server side glEnable() and the Client side glEnableClientState().Since UBO handling is defined as a Client side state it seems unlikely that you should be able to query it as a server side capability.Hi @MarkusHoHo.  I understand what you’re saying.  However, this is in conflict with every OpenGL extension spec I’ve found that calls out how to query the current state of client state enables, including NVIDIA’s.It is very clear that OpenGL supports querying client-side or server-side enable states glIsEnabled().For example, search for EnableClientState in the following:Also related to this, see Issue (9) in EXT_direct_state_access.Thank you for pointing this out!Clearly my OpenGL is a bit rusty.As I said, I already filed a bug and will try to get clarification.Great – thanks!Powered by Discourse, best viewed with JavaScript enabled',\n",
       " 'possible-to-access-an-instances-transformation-matrix-from-a-ray-generation-program': 'In my ray generation program, I would like to sample points on a geometric instance so that I can launch rays from it in my ray generation program.  But the problem is I only know the coordinates in object space and I need world space coordinates for a ray generation program.So for example, let’s say you had thousands of rectangular light sources in a scene and want to launch rays from them (note that the lights need to be instances too since I want to intersect rays with them).  I don’t want to have to pass the four vertices for each instance via a buffer or variable because that will waste a ton of memory. I already have set the transformation matrix for each instance, so that gives all the information I need.  Ideally, I would like to query the transformation matrix for each instance in my ray generation program so I can use it to transform sampled points into world coordinates.  However rtTransform* is not supported within ray generation programs for obvious reasons.  Is there any other way to get an instance’s transformation matrix from within a ray generation program?  The only way I can think of would be to launch a ray that I know will hit only the instance, then sample the rectangle from the hit program where I can use rtTransform*, but this would be very computationally wasteful.The only way I can think of is to pass the extra buffer of data about instances, as you described above.  Yes, it duplicates memory that is also stored somewhere in the OptiX scene, but is it enough memory to matter for your app?  Can you compress the description of the geometry instances somewhat?  In the rectangular light example, you could probably describe a light using less than 12 full floats, for example.I’m also curious: is the cost of transferring the buffer of lights/instances to the device significant, compared to the time it takes to run the OptiX programs?  What if you generated all rays on the host and passed them to the device, how does that time compare to the tracing time?-DylanPowered by Discourse, best viewed with JavaScript enabled',\n",
       " 'minor-linker-error-fresh-download-of-physx3-3-with-possible-fix': 'Fresh download from github (060816) of PhysX3.3, using Visual Studio 2015 with Update 2.I chose debug x64 and built. The only glitch was a complaint that libcmt.lib conflicts with other libraries, use /NODEFAULTLIB.Well, that and the fact that lots of complains about force inline not inlining, and since the project was set to treat warnings as errors, the debug build won’t link until that option is set to leave warnings as warnings.I think the issue is that the project(s) include nvToolsExt64_1.lib, which appears to be built with Multi-Threaded (/MT), even in the debug build.I switched the projects in the solution for debug/x64 to /MT, and the complaint disappears, but then a new one appeared.It complains that _CrtDebugReport and _CrtSetReportModel are not found. That makes some sense because now I’m not building with a debug CRT.So, I simply commented out the two lines (and associated code) in PsAssert.cpp (in foundation/source), and that solves that part.It may not be the solution someone else prefers, because I believe you can ignore the linker warning with reasonable results in a debug build.This would mean that release builds would have to be /MT, but that’s a good choice anyway.Powered by Discourse, best viewed with JavaScript enabled',\n",
       " 'wexler-tab-7t': 'Hello!\\nI am very interested in one question.The company “Wexler” says the source code of the software is not laid out in relation to their non-disclosure agreement with “Nvidia”. Is this true, and why it’s done, because “Google” laid out source “Nexus 7”, as well as on “Linux for Tegra” laid out the source for the kernel? This tablet, because of its low price, has become very interesting for developers.\\nIn advance I thank.\\nP.S. Sorry for my bad EnglishPowered by Discourse, best viewed with JavaScript enabled',\n",
       " 'install-optix-on-linux': 'Hello How to install optix on lnux mint?Hi! If this may help, I wrote down my instructions for Installing OptiX on Ubuntu 22.04 LTS here, bottom you will find the Ingo Wald’s blog that helped me a lot.hello thank you for your return. I downloaded the optix file but I don’t know where to install it.Hi @nibutic, on Linux you can install OptiX anywhere you want. (The “install” is just unzipping a zipped archive, there are no system files to install.) Having a sub-folder in your home directory for SDKs, or for code development, would be a common choice. The only thing that matters is that when you write OptiX applications, you’ll have to point your build process to the location where you put your OptiX header files.–\\nDavid.Hello Thank you for your reply NibuticPowered by Discourse, best viewed with JavaScript enabled',\n",
       " 'even-for-simple-glsl-spir-v-shaders-im-getting-error-invalid-vertex-program-header-qu': 'All of my GLSL vertex and fragment shaders compile to SPIR-V just fine using glslangValidator.exe, and they run just fine on another system I have with an ATI GPU, but when I try to create a Vulkan pipeline with them on my GTX 960 (driver version 368.81), I get “error: invalid vertex program header” for some of my shaders.I’ve tried changing the version, the extensions (including enabling GL_KHR_vulkan_glsl), and a number of other things, but I get that error every time for this simple shader. Can anyone else spot what’s wrong with this vertex shader?Thanks,\\nSeanI have not tested this, and the error might be somewhere else, but this shader violates the limits of the NVIDIA driver:On my GTX 970 the limit for maxUniformBufferRange is 64kB and i am pretty certain its the same for the GTX 960. Each GUIData is 48 byte, each TextData is 32 byte. So with your array sizes of 10k for GUIBuffer and TextBuffer, they are 480kB and 320kB, clearly above 64kB. AMD cards have a 4GB limit on this range. The minimum guaranteed limit is 16kB.Use a storage buffer instead of a uniform buffer, which have a minimum guaranteed limit of 128MB. On the GTX 970 the actual limit is 2GB for me. This also allows you to not hardcode a limit on the range, like this:The actual valid range is then determined at runtime based on the buffer range you bound in the descriptor set. This allows you to change the bound buffer size on demand without changes to the shader.RegardsThanks, I did a quick test by dropping the max instance counts, and it seems to have gotten me past that error (on to newer and more exciting errors). I will take a look at the storage buffer when I get a chance.My next error is a memory access violation in vkCreateGraphicsPipelines (trying to read address 0x54) for a different shader program that uses vertex+geometry+fragment stages. That one also works fine on AMD hardware, and the debug callback and validator layers aren’t giving me any clues. Any tips to try besides pulling out pieces until it compiles and runs, then trying to put them back in one piece at a time?Thanks again,\\nSeanWell, without the code i can’t really help you. I don’t think the validation layers have extended shader validation yet. Since you ran into the limits problem once, and again it works on AMD, that’s the first one i would look at. In OpenGL, the driver would simply replace or emulate or rearrange what you did to make it work, for example in your first shader replace the constant buffers with storage buffers under the hood. Vulkan doesn’t, you get what you write, so you really need to know all the limits. Ad-hoc doesn’t really work in Vulkan. Of course it could be a driver bug, but to verify that means to check all the limits first. Seeing that you used 3 descriptor sets for the last shader, maxBoundDescriptorSets is the first one i would check. It’s 8 on newer NVIDIA drivers (it was 4 at the beginning), and 32 on AMD.RegardsThanks again. I was just looking for tips/suggestions, and I believe you’ve given me what I need to track the problem down. I got the storage buffers working last night, and I managed to get everything else working except for that one shader, so I’m almost there.I’ve been tinkering with OpenGL (part-time) since I purchased an nVidia Riva 128 in the late 90’s, so I’m a bit old-school. ;-) I was stuck on the fixed-function pipeline until I started working on this article: http://http.developer.nvidia.com/GPUGems2/gpugems2_chapter16.htmlAs a tinkerer, I probably shouldn’t be messing with Vulkan, but the rendering calls in OpenGL were always too slow for what I wanted to do.Thanks again for all your help.Oh no, you can definitely tinker with Vulkan, it just requires some minimal framework with lots of utility/builders around it. With the direct C interface most calls requires dozens of lines of boilerplate. Sure even then it misses the OpenGL convenience of just changing something last minute right before the drawcall, but with the right abstraction setting up things like vertex layout, descriptor layout, render pass and pipeline state just take a couple lines (if you format it pretty). And the rendering later doesn’t take much more. Convenience functions are key.In fact i found that once you embrace the Vulkan style, it’s easier to program with than OpenGL. With OpenGL you can mix and match all types of calls all the time, with Vulkan there is a more strict order to when you can do what. But that naturally makes Vulkan more linear to program with. I especially find the pipeline state objects, descriptor sets and the (also available in new OpenGL) permanent mapped buffers a great simplification.As for drawcalls being slow: They are not anymore on the CPU! From my own code i can tell you that recording 10k drawcalls (seperate drawcalls, not instancing) with changing pipelines is as fast as 1ms on one core on a relatively old and slow CPU. Submitting them is as fast as 25-50µs! Combined with recording in multiple threads, instancing, indirect draw and command buffer resubmission, you can easily max your GPU with more drawcalls than it can handle.RegardsOh, I know. I really do like the additional control and performance it gives, and when I say “shouldn’t”, it doesn’t mean “wouldn’t”. I just don’t have as much spare time to tinker as I used to, and Vulkan requires a good deal of extra time to get comfortable. It might be a while before I consider it to be “easier” than OpenGL. ;-)If you’re interested at all, the full source is currently on: GitHub - sponeil/VKSandbox: My sandbox for playing with the Vulkan API. (it has a Visual Studio 2015 project file). It includes some third-party libraries (e.g. zlib, libzip, sqlite3, libjpeg, libpng, code from glslangValidator), but all of the code/shaders I wrote myself are public domain, so anyone is free to take and tweak any useful parts they want and leave the rest.If you do take a look at it, comments/criticism are always welcome. I don’t like everything about the wrapper classes I have. I ported them from an older wrapper library I wrote for OpenGL, but I feel like I’m still just getting my feet wet with Vulkan. I do like the ShaderTechnique, which allows me to define fx-style shader files that allow me to change both shader code and pipeline states (e.g. blending, depth, culling) in the fx files, combine several shader programs into the same FX file, include headers and glsl files, and so on.Based on your comments, you probably have a better Vulkan library in place already, but you still might get some ideas from it. If not, someone else reading this might. The only test app I have in that project implements one flavor of spherical clip-maps for dynamic LOD planets I’ve been kicking around. (The colors are randomly-generated plates for a plate tectonic implementation I hope to write in shaders to generate more realistic planetary terrain).Thanks again for all your help.Powered by Discourse, best viewed with JavaScript enabled',\n",
       " 'p3000-nvenc-nvdec-capabilities': 'Hi,I cannot find any formal information about P3000’s NVENC/NVDEC capabilities.Is P3000 based on GP104 or GP106?P3000 supports unlimited number of NVENC encoding sessions, doesn’t it?Does P3000 support VP8 decoder?thanks.Powered by Discourse, best viewed with JavaScript enabled',\n",
       " 'gl-extension-gl-nvx-gpu-memory-info-is-missing': 'I was installing different versions of CUDA recently and I lately found that GL_NVX_gpu_memory_info is not available suddenly. I uninstalled all the recent CUDA installations that I made and as well re-installed NVIDIA graphics driver but still the same issue. I use NVIDIA Quadro P1000 gpu with driver 452.57. Currently the glGetString(GL_EXTENSIONS); returns the below string. How to get back this extension? Are is this removed recently as it was an experimental one?GL_3DFX_texture_compression_FXT1 GL_AMD_depth_clamp_separate GL_AMD_vertex_shader_layer GL_AMD_vertex_shader_viewport_index GL_ARB_ES2_compatibility GL_ARB_ES3_1_compatibility GL_ARB_ES3_compatibility GL_ARB_arrays_of_arrays GL_ARB_base_instance GL_ARB_bindless_texture GL_ARB_blend_func_extended GL_ARB_buffer_storage GL_ARB_cl_event GL_ARB_clear_buffer_object GL_ARB_clear_texture GL_ARB_clip_control GL_ARB_color_buffer_float GL_ARB_compatibility GL_ARB_compressed_texture_pixel_storage GL_ARB_compute_shader GL_ARB_conditional_render_inverted GL_ARB_conservative_depth GL_ARB_copy_buffer GL_ARB_copy_image GL_ARB_cull_distance GL_ARB_debug_output GL_ARB_depth_buffer_float GL_ARB_depth_clamp GL_ARB_depth_texture GL_ARB_derivative_control GL_ARB_direct_state_access GL_ARB_draw_buffers GL_ARB_draw_buffers_blend GL_ARB_draw_elements_base_vertex GL_ARB_draw_indirect GL_ARB_draw_instanced GL_ARB_enhanced_layouts GL_ARB_explicit_attrib_location GL_ARB_explicit_uniform_location GL_ARB_fragment_coord_conventions GL_ARB_fragment_layer_viewport GL_ARB_fragment_program GL_ARB_fragment_program_shadow GL_ARB_fragment_shader GL_ARB_fragment_shader_interlock GL_ARB_framebuffer_no_attachments GL_ARB_framebuffer_object GL_ARB_framebuffer_sRGB GL_ARB_geometry_shader4 GL_ARB_get_program_binary GL_ARB_get_texture_sub_image GL_ARB_gpu_shader5 GL_ARB_gpu_shader_fp64 GL_ARB_half_float_pixel GL_ARB_half_float_vertex GL_ARB_indirect_parameters GL_ARB_instanced_arrays GL_ARB_internalformat_query GL_ARB_internalformat_query2 GL_ARB_invalidate_subdata GL_ARB_map_buffer_alignment GL_ARB_map_buffer_range GL_ARB_multi_bind GL_ARB_multi_draw_indirect GL_ARB_multisample GL_ARB_multitexture GL_ARB_occlusion_query GL_ARB_occlusion_query2 GL_ARB_pixel_buffer_object GL_ARB_point_parameters GL_ARB_point_sprite GL_ARB_polygon_offset_clamp GL_ARB_post_depth_coverage GL_ARB_program_interface_query GL_ARB_provoking_vertex GL_ARB_query_buffer_object GL_ARB_robust_buffer_access_behavior GL_ARB_robustness GL_ARB_robustness_isolation GL_ARB_sample_shading GL_ARB_sampler_objects GL_ARB_seamless_cube_map GL_ARB_seamless_cubemap_per_texture GL_ARB_separate_shader_objects GL_ARB_shader_atomic_counter_ops GL_ARB_shader_atomic_counters GL_ARB_shader_bit_encoding GL_ARB_shader_draw_parameters GL_ARB_shader_group_vote GL_ARB_shader_image_load_store GL_ARB_shader_image_size GL_ARB_shader_objects GL_ARB_shader_precision GL_ARB_shader_stencil_export GL_ARB_shader_storage_buffer_object GL_ARB_shader_subroutine GL_ARB_shader_texture_image_samples GL_ARB_shading_language_100 GL_ARB_shading_language_420pack GL_ARB_shading_language_packing GL_ARB_shadow GL_ARB_stencil_texturing GL_ARB_sync GL_ARB_tessellation_shader GL_ARB_texture_barrier GL_ARB_texture_border_clamp GL_ARB_texture_buffer_object GL_ARB_texture_buffer_object_rgb32 GL_ARB_texture_buffer_range GL_ARB_texture_compression GL_ARB_texture_compression_bptc GL_ARB_texture_compression_rgtc GL_ARB_texture_cube_map GL_ARB_texture_cube_map_array GL_ARB_texture_env_add GL_ARB_texture_env_combine GL_ARB_texture_env_crossbar GL_ARB_texture_env_dot3 GL_ARB_texture_float GL_ARB_texture_gather GL_ARB_texture_mirror_clamp_to_edge GL_ARB_texture_mirrored_repeat GL_ARB_texture_multisample GL_ARB_texture_non_power_of_two GL_ARB_texture_query_levels GL_ARB_texture_query_lod GL_ARB_texture_rectangle GL_ARB_texture_rg GL_ARB_texture_rgb10_a2ui GL_ARB_texture_stencil8 GL_ARB_texture_storage GL_ARB_texture_storage_multisample GL_ARB_texture_swizzle GL_ARB_texture_view GL_ARB_timer_query GL_ARB_transform_feedback2 GL_ARB_transform_feedback3 GL_ARB_transform_feedback_instanced GL_ARB_transpose_matrix GL_ARB_uniform_buffer_object GL_ARB_vertex_array_bgra GL_ARB_vertex_array_object GL_ARB_vertex_attrib_64bit GL_ARB_vertex_attrib_binding GL_ARB_vertex_buffer_object GL_ARB_vertex_program GL_ARB_vertex_shader GL_ARB_vertex_type_10f_11f_11f_rev GL_ARB_vertex_type_2_10_10_10_rev GL_ARB_viewport_array GL_ARB_window_pos GL_ATI_separate_stencil GL_EXT_abgr GL_EXT_bgra GL_EXT_blend_color GL_EXT_blend_equation_separate GL_EXT_blend_func_separate GL_EXT_blend_minmax GL_EXT_blend_subtract GL_EXT_clip_volume_hint GL_EXT_compiled_vertex_array GL_EXT_direct_state_access GL_EXT_draw_buffers2 GL_EXT_draw_range_elements GL_EXT_fog_coord GL_EXT_framebuffer_blit GL_EXT_framebuffer_multisample GL_EXT_framebuffer_object GL_EXT_geometry_shader4 GL_EXT_gpu_program_parameters GL_EXT_gpu_shader4 GL_EXT_multi_draw_arrays GL_EXT_packed_depth_stencil GL_EXT_packed_float GL_EXT_packed_pixels GL_EXT_polygon_offset_clamp GL_EXT_rescale_normal GL_EXT_secondary_color GL_EXT_separate_specular_color GL_EXT_shader_framebuffer_fetch GL_EXT_shader_integer_mix GL_EXT_shadow_funcs GL_EXT_stencil_two_side GL_EXT_stencil_wrap GL_EXT_texture3D GL_EXT_texture_array GL_EXT_texture_compression_s3tc GL_EXT_texture_edge_clamp GL_EXT_texture_env_add GL_EXT_texture_env_combine GL_EXT_texture_filter_anisotropic GL_EXT_texture_integer GL_EXT_texture_lod_bias GL_EXT_texture_rectangle GL_EXT_texture_sRGB GL_EXT_texture_sRGB_decode GL_EXT_texture_shared_exponent GL_EXT_texture_snorm GL_EXT_texture_storage GL_EXT_texture_swizzle GL_EXT_timer_query GL_EXT_transform_feedback GL_IBM_texture_mirrored_repeat GL_INTEL_conservative_rasterization GL_INTEL_fragment_shader_ordering GL_INTEL_framebuffer_CMAA GL_INTEL_map_texture GL_INTEL_multi_rate_fragment_shader GL_INTEL_performance_query GL_KHR_blend_equation_advanced GL_KHR_blend_equation_advanced_coherent GL_KHR_context_flush_control GL_KHR_debug GL_KHR_texture_compression_astc_hdr GL_KHR_texture_compression_astc_ldr GL_NV_blend_square GL_NV_conditional_render GL_NV_primitive_restart GL_NV_texgen_reflection GL_SGIS_generate_mipmap GL_SGIS_texture_edge_clamp GL_SGIS_texture_lod GL_SUN_multi_draw_arrays GL_WIN_swap_hint WGL_EXT_swap_controlPowered by Discourse, best viewed with JavaScript enabled',\n",
       " 'issues-using-the-nvdemux-from-custom-gstreamer-element': 'Please provide complete information as applicable to your setup.•\\tHardware Platform (Jetson / GPU) Jetson TX2\\n•\\tDeepStream Version 5.0\\n•\\tJetPack Version (valid for Jetson only)\\n•\\tTensorRT Version\\n•\\tNVIDIA GPU Driver Version (valid for GPU only)\\n•\\tIssue Type( questions, new requirements, bugs) bugs/ or questions\\n•\\tHow to reproduce the issue ? (This is for bugs. Including which sample app is using, the configuration files content, the command line used and other details for reproducing)\\n•\\tRequirement details( This is for new requirement. Including the module name-for which plugin or for which sample application, the function description)Hello,We did create a custom gstreamer element which is derrived from the nvarguscamerasrc element.\\nSome features are added and we created an own stream mux in it which is used by our application.\\nCurrently we want to input our muxed stream with four video streams in it to de nvdemux.\\nBut when we try this, we get a SIGSEGV crash in the nvdemux element.\\nWe ran it both as pipeline command as from .c program (see attachment).Things we already verified were:\\n-Checked that the batch and frame meta are filled in correctly by our custom element\\n-Locking the batch meta when modifying itWould it be possible to get the source code of the nvdemux or debug symbols for it?\\nSo that we can get the issue and fix it.Best regards,\\nFloris van DrunenBlockquote\\n“===== NVMEDIA: NVENC =====\\nNvMMLiteBlockCreate : Block : BlockType = 4\\nOFParserListModules: module list: /proc/device-tree/tegra-camera-platform/modules/module0\\nOFParserListModules: module list: /proc/device-tree/tegra-camera-platform/modules/module1\\nOFParserListModules: module list: /proc/device-tree/tegra-camera-platform/modules/module2\\nOFParserListModules: module list: /proc/device-tree/tegra-camera-platform/modules/module3\\nNvPclHwGetModuleList: WARNING: Could not map module to ISP config string\\nNvPclHwGetModuleList: No module data found\\nNvPclHwGetModuleList: WARNING: Could not map module to ISP config string\\nNvPclHwGetModuleList: No module data found\\nNvPclHwGetModuleList: WARNING: Could not map module to ISP config string\\nNvPclHwGetModuleList: No module data found\\nNvPclHwGetModuleList: WARNING: Could not map module to ISP config string\\nNvPclHwGetModuleList: No module data found\\nOFParserGetVirtualDevice: NVIDIA Camera virtual enumerator not found in proc device-tree\\n---- imager: Found override file [/var/nvidia/nvcam/settings/camera_overrides.isp]. ----\\nCAM: serial no file already exists, skips storing againLSC: LSC surface is not based on full res!\\n---- imager: Found override file [/var/nvidia/nvcam/settings/camera_overrides.isp]. ----\\nCAM: serial no file already exists, skips storing againLSC: LSC surface is not based on full res!\\n---- imager: Found override file [/var/nvidia/nvcam/settings/camera_overrides.isp]. ----\\nCAM: serial no file already exists, skips storing againLSC: LSC surface is not based on full res!\\n---- imager: Found override file [/var/nvidia/nvcam/settings/camera_overrides.isp]. ----\\nCAM: serial no file already exists, skips storing againLSC: LSC surface is not based on full res!\\nCAM: serial no file already exists, skips storing againLSC: LSC surface is not based on full res!\\nCaught SIGSEGV\\nCAM: serial no file already exists, skips storing againLSC: LSC surface is not based on full res!\\nCAM: serial no file already exists, skips storing againLSC: LSC surface is not based on full res!\\nCAM: serial no file already exists, skips storing againLSC: LSC surface is not based on full res!\\nGST_ARGUS: Creating output stream\\n#0  0x0000007f9406de28 in __GI___poll (fds=0x5584b58440, nfds=547945427848, timeout=) at …/sysdeps/unix/sysv/linux/poll.c:41\\n#1  0x0000007f9417af58 in  () at /usr/lib/aarch64-linux-gnu/libglib-2.0.so.0\\n#2  0x0000005584a65200 in  ()\\nSpinning.  Please run ‘gdb gst-launch-1.0 9498’ to continue debugging, Ctrl-C to quit, or Ctrl-\\\\ to dump core.”\\n\"Blockquote\\n#include <gst/gst.h>\\n#include <glib.h>\\n#include <stdio.h>\\n#include “gstnvdsmeta.h”\\n/* The muxer output resolution must be set if the input streams will be of\\ndifferent resolution. The muxer will scale all the input frames to this\\nresolution. /\\n#define MUXER_OUTPUT_WIDTH 1920\\n#define MUXER_OUTPUT_HEIGHT 1080\\n/ Muxer batch formation timeout, for e.g. 40 millisec. Should ideally be set\\nbased on the fastest source’s framerate. */\\n#define MUXER_BATCH_TIMEOUT_USEC 40000\\nint\\nmain (int argc, char *argv)\\n{\\nGMainLoop *loop = NULL;\\nGstElement *pipeline = NULL, *source = NULL, *nvstreamdemux = NULL, sink = NULL;\\nGstPad srcpad_0=NULL, sinkpad_0=NULL;\\n/ Standard GStreamer initialization /\\ngst_init (&argc, &argv);\\nloop = g_main_loop_new (NULL, FALSE);\\n/ Create gstreamer elements /\\n/ Create Pipeline element that will form a connection of other elements /\\npipeline = gst_pipeline_new (“arguscamerasrc-pipeline”);\\n/ Source element for reading from the file /\\nsource = gst_element_factory_make (“arguscamerasrc”, “camera-source”);\\ng_object_set (G_OBJECT (source), “num-sensors”, 1, NULL);\\n/ Demux /\\nnvstreamdemux = gst_element_factory_make (“nvstreamdemux”, “demux”);\\nsink = gst_element_factory_make (“fakesink”, “fakesink”);\\nif (!source || !nvstreamdemux || !sink) {\\ng_printerr (“One element could not be created. Exiting.\\\\n”);\\nreturn -1;\\n}\\n/ Set up the pipeline /\\n/ we add all elements into the pipeline /\\ngst_bin_add_many (GST_BIN (pipeline), source, nvstreamdemux, sink, NULL);\\nif (!gst_element_link_many (source, nvstreamdemux, NULL)) {\\ng_printerr (“Elements could not be linked: 1. Exiting.\\\\n”);\\nreturn -1;\\n}\\n/ Link the Demuxer and sink via request pads/\\nsrcpad_0 = gst_element_get_request_pad(nvstreamdemux, “src_0”);\\nsinkpad_0 = gst_element_get_static_pad(sink, “sink”);\\nif (gst_pad_link(srcpad_0, sinkpad_0) != GST_PAD_LINK_OK) {\\ng_printerr(“Failed to link: srcpad_0 to sinkpad_0!\\\\n”);\\nreturn -1;\\n}\\ngst_object_unref (srcpad_0);\\ngst_object_unref (sinkpad_0);\\n/ Set the pipeline to “playing” state /\\ng_print (“Now playing: %s\\\\n”, argv[1]);\\ngst_element_set_state (pipeline, GST_STATE_PLAYING);\\n/ Wait till pipeline encounters an error or EOS /\\ng_print (“Running…\\\\n”);\\ng_main_loop_run (loop);\\n/ Out of the main loop, clean up nicely */\\ng_print (“Returned, stopping playback\\\\n”);\\ngst_element_set_state (pipeline, GST_STATE_NULL);\\ng_print (“Deleting pipeline\\\\n”);\\ngst_object_unref (GST_OBJECT (pipeline));\\ng_main_loop_unref (loop);\\nreturn 0;\\n}We have made a bit more progress on this crash by testing with cuda-gdb. That gives us the following stack trace:With GST_DEBUG=8, I found the following differences:Working version with nvarguscamerasrc:Custom version:Looks like right after locking the miniobject, the nvstreamdemux is doing something that makes it crash.As mentioned before, access to the source code or some pointers on what we are doing wrong would be much appreciated!Powered by Discourse, best viewed with JavaScript enabled',\n",
       " 'optix-5': 'OptiX 5 looks awesome from what details have come out so far.Is there a planned release date? Is it known what version of Cuda and VS it will support?October → CUDA 9.0 RC, from what I can tell.Expect no answers on schedules or features of any unreleased product.\\n[url]LMGTFY - Let Me Google That For YouThank you - so October, or November (says tomshardware). Knowing that the release isn’t imminent is useful information for our upgrade plans.Sorry to ask the question, I did manage to contain my enthusiasm for a month…No problem. ;-) It’s just that we simply can’t comment on unreleased products.Let’s try reading official marketing information again:\\n[url]http://nvidianews.nvidia.com/news/nvidia-supercharges-rendering-performance-with-ai[/url]\\n[url]https://blogs.nvidia.com/blog/2017/07/31/nvidia-research-brings-ai-to-computer-graphics/[/url]Any more news regarding a realease date ?\\nSince a few weeks the new 5.0 are featured on the optix site and if you click Get Optix, it´s still 4.1.1. Kinda misleading.Hi David, OptiX 5.0 is now available for download.Powered by Discourse, best viewed with JavaScript enabled',\n",
       " 'vehicle-wheels-spinning-in-place-chassis-jitter-with-identical-snippet-code-physx-3-3-3': 'Sorry for the long title, but I wanted to sum up my problem in it.Basically I copied the Vehicle4W snippet exactly into my code and the vehicle doesn’t move and just jitters.This is my code: #include \"PhysicsLogicSystem.h\"#include <PxPhysicsAPI.h>#include \"extensions - Pastebin.com\\nAnd my “simplified” version of the original snippet: // This code contains NVIDIA Confidential Information and is disclosed to you/ - Pastebin.comThe code is a bit long so I posted the PasteBin links above.I also made 2 simulation recordings with PVD and uploaded them to my Google Drive along with a video recording and my full “project” source.The code is absolutely the same, the only thing that could be different would be the stepping, but it’s still a 1/60 step (the relevant stepping code is in LogicGameState which gets called from the threading setup in Tutorial06_Multithreading.cpp)Help and pointers are much appreciated.My first guess would be that the wheel isn’t getting any friction or that the car has a collision volume touching the ground.Powered by Discourse, best viewed with JavaScript enabled',\n",
       " 'program-stopped-working-after-installed-new-nvidia-64-bit-linux-driver-v310-19': 'I just installed a new video card (GTX 680) in my computer (64-bit ubuntu linux v12.04) and installed the latest nvidia driver (v310.19) and updated my GLEW files to v190. I updated my GL and GLX include files from the nvidia driver, and updated my GLEW include and source files from the new GLEW files.Now my 3D engine does not start up. BTW, it did run on the new graphics card before I updated the drivers and GLEW (I am about 99% sure of that).The problem is, the OpenGL function glGenVertexArrays(1, &vaoid) always returns a value of zero no matter how many times I call the function, and no matter how many VAO identifiers I ask the function to generate.Does anyone know what this problem is… or might be?I cut and paste the section of code where the problem occurs. Note that I just added all those glGetError() calls to debug what’s happening. Note that ALL of the glGetError() calls return zero, indicating no errors. However, it seems to me that glGetVertexArrays() returning a value of zero IS an error (at least on its part).Note that it returns valid values/names for the IBO and VBO (values == 1 and 2).Note that this error is happening the first time glGetVertexArrays() is being called after my program starts up. Also note that the same problem happens whether I compile the program into a 32-bit executable or a 64-bit executable (both of which ran before).My current code is OpenGL v3.20 level. I upgraded my GPU to update my engine to OpenGL and GLSL v4.30 level.Hmmmm. During startup I print out a whole pile of xwindows, GLX and GL values, and I notice the following string prints out for glGetString(GL_VERSION):2.1.2 NVIDIA 313.09What is the 2.1.2 supposed to be? The version of OpenGL? If so, that appears to be a version before the VAO was supported. If so, is there some new call that’s now required to specify what version of OpenGL my program intends to access… and maybe it defaults to v2.12 if nothing is specified? (Of course, I don’t think there ever was a version 2.12 of OpenGL).Also, what is the 313.09 supposed to be? The driver was supposed to be 310.19 on the nvidia website (and that is still the version on their website today, so I don’t think v313.09 even exists yet).It appears the problem happens because my code does not receive an appropriate OpenGL context.  Previously it was requesting an OpenGL v3.20 context and receiving it.  Now when the code requests an OpenGL v3.20 context, it receives an OpenGL v2.12 context (which I guess is the same as an OpenGL v2.10 context).Why would the new driver not give me anything higher than the default OpenGL v2.10 context?  Perhaps something in the order of the function calls in my code is not compatible with some change in the new nvidia v310.19 drivers?Make any sense, anyone?I’ll post a separate question asking about this OpenGL context issue.  The current message title just doesn’t make sense for that.Powered by Discourse, best viewed with JavaScript enabled',\n",
       " 'are-the-advanced-examples-supported-by-osx': 'Hi,\\nI recently downloaded the advanced examples, but came to a few errors during make on OSX 10.11.\\nAt first the sutil seems to be calling the methods from OpenGL 2.x API which is deprecated and causing errors on my machine.\\nI tried to work around this by substituting the sutil.cpp and sutil.h files with those from the SDK. No more API methods errors occurred. But a new one showed saying a linking error regarding to the OptiX lib and x86_64 architecture.\\nI wonder if the advanced examples are not currently intended to run on on OSX. Or is there something else that I should do to build it on my machine?No, the advanced samples are not currently supported on OSX.  If there’s enough interest we could probably get them to work.I’ve figure it out. Thank you. Awesome sample programs!Cool, so did you get the advanced samples to build on OSX?  If the changes were fairly trivial, feel free to post them, or send me a diff on github or @ nvidia.com.Powered by Discourse, best viewed with JavaScript enabled',\n",
       " 'problem-with-prismatic-joint-limits-and-joint-force-feedback': 'I’ve produced a problem with a prismatic joint that just ignores it’s limits. Anyone have any idea what might be going on here? Linked is the pvd recording.https://www4.ibackup.com/qmanager/servlet/share?key=luodo95363Also is there or has there ever been a way to determine the amount of force generated by a constraint. I’m trying to figure out the tension in a rope made of stiff constraints.Hi,I’m afraid I cannot connect to that website, there is some problem with certificates, or unsafe Java, or something.From the User Guide:Force ReportingThe joint may be configured to report each frame the force that was required to hold it together. To enable this behavior, set the joint’s reporting flag. The force may then be retrieved after simulation with a call to getForce():joint->setConstraintFlag(PxConstraintFlag::eREPORTING)\\n…scene->fetchResults(…)\\njoint->getgetConstraint().getForce(force, torque);\\nThe force is resolved at the origin of actor1’s joint frame.Note that this force is only updated while the joint’s actors are awake.I have been experiencing a similar issue with the prismatic joint. The JointLimitPair is only valid for some world space ‘x’ direction. Regardless of the local pose of the joint, or the pose of the corresponding actors.I created this gif where two are connected by a prismatic joint with a non-zero spring value. The two shapes rebound back only at first. Once I apply a torque to the one RigidBody the maximum distance between them continues to grow. When they are vertical they will never rebound.Here is the relevant code:*I’m using the PhysX 3.2.3 version. I will try with an older version to see if it is only a recent issue.I am having the same problem and so does the user in this thread [url]https://devtalk.nvidia.com/default/topic/772673[/url].The limits seem to always be related to the global x-axis no matter what the global axis the prismatic joint works on is. If actors are constrained to global x-axis limits work fine, but if actors are constrained to another axis the limits will still be on the global x-axis. The actors will then be limited in motion so that the projected position of the actor onto the global x-axis will never go outside the limits.I believe this is a bug and it seems to have been around a while. Does anyone know if this problem has been acknowledged by Nvidia?Hi, → If actors are constrained to global x-axis limits work fine, but if actors are constrained to another axis the limits will still be on the global x-axis.I suspect that the problem is coming from when the joint is first set up. Could you post a snippet to reproduce this problem?  Also, please try it using PhysX-3.3.2 if you haven’t already.Thanks,\\nMikeHi Mike,I just tried with 3.3.2 and have the same issue. In my tests I am using code similar to code in post 3 in this thread. I have no simple isolated example.  I am setting the local pose in joint for actors to change the axis to constrain to. Do you know if there is a simple sample example with a PrismaticJoint that I could modify?Just to be clear. The axis that actors are constrained to is as expected, only the limits are the problem.Could you post a PVD capture?Here is a PVD capture of a simple example. It contains 4 actors. One is fixed in space and the other three are connected to it with prismatic joints. The joints have been set up to contrain the movement of the actors around the x-axis, the y-axis and the axis (1,-1,0) respectively. Gravity has been set to 1 -1 0 to pull actors in both x and y.Results are:[url]https://dl.dropboxusercontent.com/u/1669806/slider_limits_problem.pvd[/url]OK, thanks, looking into it.Indeed, this is a bug. It will be fixed in the next release, PhysX-3.3.3. Also, I’ll post a source code fix to the problem as soon as it becomes available. All binary SDK distros include the source to ‘PhysX Extensions’, where the problem lies.Here is the fix. Go to ./Source/PhysXExtensions/src/ExtPrismaticJointSolverPrep.cpp, find the function:…\\n{…\\nif(limitEnabled && !limitIsLocked)\\n{\\nPxVec3 axis = cA2w.rotate(PxVec3(1.f,0,0));\\nPxReal ordinate = axis.dot(bOriginInA);Change that last line to:\\nPxReal ordinate = bOriginInA.x;Thanks! That works!Awesome. This fix will be added to PhysX-3.3.3.Powered by Discourse, best viewed with JavaScript enabled',\n",
       " 'physx-3-3-vs-2013': 'Hello there! I’m trying to integrate physx with my game engine but i cant even compile it because im getting linker errors :when i change Runtime Library to MTdWhere can i get good libraries compatible with newest visual studio?HiLooks like you’re mixing something… Check if you use the release libs for the release configuration (same with debug). Furthermore, I suggest to use CMake to create your Visual Studio project file, then the MSC_VER error should also disappear. Or check in your project properties, Configuration Properties->General->Platform Toolset.I also use Physx with VS2013, and it works.CheersHi againI see there’s a new version, did you use 3.3.2? If you use 3.3.1, you only have one binary directory which works.CheersI’m having the same problem with VS2013 + Physx 3.3.2 builds.\\nThe actual problem for me is linking with PhysX3ExtensionsDEBUG.lib or other variants of it. I guess these are static libraries that cause the problem.Errors like:Did you manage to solve it ?You may have to rebuild the static libraries using a different flavor of CRT to match your application.  The source for the extensions, vehicle controller, character controller, etc., are included in the binary distro for this reasonHi MikeThanks for this info. I rebuilt the libraries, but I don’t find PxTask. This one is also required. Where are the project and source files for this?Thanks,\\nRobUgh. I think you may need a source distro in order to rebuild PxTask. Please inquire at physXlicensing@nvidia.com, refer to this thread, and I’ll see what I can do.Powered by Discourse, best viewed with JavaScript enabled',\n",
       " 'bug-report': 'Adobe application reported “NVIDIA has identified a bug in your driver which might cause random crashes in your Adobe application.” tried reinstalling the latest driver still the issue persists.Hello,Welcome to the NVIDIA Developer Forum. You posted in the Modulas forum, General Driver issues belong in the Drivers category. I will move it over for you.Powered by Discourse, best viewed with JavaScript enabled',\n",
       " 'id3d12device-createdescriptorheap-driver-bug': 'Hello,\\nI’m developing a Direct3D 12 application and the method ID3D12Device::CreateDescriptorHeap triggers an access violation when creating more than one descriptor heap of type D3D12_DESCRIPTOR_HEAP_TYPE_RTV or D3D12_DESCRIPTOR_HEAP_TYPE_DSV.\\nThe error message looks like this:\\nException thrown at 0x00007FF8BA5C1548 (nvwgf2umx.dll) in App.exe: 0xC0000005: Access violation reading location 0x000000809D10B050.\\nEdit: I have a GeForce GTX 970 with the latest driver installed (364.72) on Windows 10 64-bitHi,\\nI have the same setup and I have no problems creating multiple RTV and DSV heaps. Is it the second call to CreateDescriptorHeap() that triggers the access violation? Did you check the debug layer for any warnings or errors?\\nAlso, if you think you’re experiencing a driver bug, you could try running DX12 on the WARP device.Hi,\\nsorry for the late reply, I though I would have received an email if someone replied.\\nThe debug layer was active and didn’t issue any warning or error. As you suggested, I tried running on the WARP device and the call succeeded but the debug layer started to complain: “OMSetRenderTargets: Specified descriptor handle ptr=0x0000002B723A23C0 points to a descriptor heap type that is CPU write only, so reading it is invalid. pDepthStencilDescriptor is the issue”. Then, googling the error I found this thread on gamedev.net: [D3D12] ClearUnorderedAccessViewFloat fails - Graphics and GPU Programming - GameDev.net where I discovered that a RTV or DSV descriptor heap cannot be, obviously, shader visible like I was incorrectly trying to create it.\\nBut I would expect a debug layer warning with a E_FAIL return value and not an access violation. So, there is a driver bug, right?\\nThank you very much for your help.That is very strange, indeed. If I try creating a shader-visible RTV heap I get the same results as you:I have the exact same setup as you (Win10 x64 / GTX 970 / 364.72), so I don’t know which component is failing us here, but I agree with you that CreateDescriptorHeap() should just detect invalid combinations of arguments and return the correct HRESULT.PS: There’s a checkbox to receive email notifications right above the Reply button at the bottom of the page.Thank you very much for your response. This is clearly a bug that we should report, but I don’t know how to do it.PS: I’m replying very late again and I apologize, again.Powered by Discourse, best viewed with JavaScript enabled',\n",
       " 'how-to-use-video-codec-sdk-for-linux': 'hello , I am trying to decode RTSP stream with the help of Ffmpeg based on GPU.\\nAnd the operation system is ubuntu. I have downloaded the SDK and found the description for the NvDecodeD3D9 is quiet few.\\nAny blogs or suggestion about this would be appreciated!decode rtsp stream with ffmpeg on Titan X,Ubuntu16.04Please refer to our decoder programming guide for more details about how to use NVDECODEAPI to program the decoder. Let us know if there is anything specific that is not clear.Thanks,\\nRyan ParkPowered by Discourse, best viewed with JavaScript enabled',\n",
       " 'development-of-tire-contact-model-on-soft-soil': 'Hi,  we are working on the support of soft soil modeling in PhysX and function we are looking at is  PxVehicleComputeTireForce (Vehicles — NVIDIA PhysX SDK 3.4.0 Documentation).I have looked into teh example of the implementation.  However, in order to models oft soil, we need to know the terrain type and tire perpetration  into the  ground. How could we get it?Thanks for attention,Powered by Discourse, best viewed with JavaScript enabled',\n",
       " 'optix7-optixhello': 'Good morning,I am trying to compile the optix7 sample ‘optixHello’ and am getting the following errors:\\nptxas /tmp/tmpxft_00009e0b_00000000-5_draw_solid_color.ptx, line 27; error   : Label expected for argument 0 of instruction ‘call’\\nptxas /tmp/tmpxft_00009e0b_00000000-5_draw_solid_color.ptx, line 27; fatal   : Call target not recognized\\nptxas fatal   : Ptx assembly aborted due to errorsCan anyone point me in the direction of what could be causing this?Thank you in advanced for any help/hints.Would you please add the usual system configuration information:\\nOS version, installed GPU(s), display driver version (mandatory), OptiX version (major.minor.micro), CUDA toolkit version used to generate the OptiX input PTX code, host compiler version.I’m assuming OptiX 7.1.0 and CUDA 10.x or CUDA 11.0.\\n(I would not use CUDA 11.1 with OptiX 7.1.0 because that predates CUDA 11.1.)If you say the problem happens during OptiX SDK 7.x optixHello compile time, there is likely something wrong with your build environment.\\nThere shouldn’t be any PTX assembler invoked during the compilation process from CUDA device code source in *.h and *.cu files to *.ptx OptiX input source modules.\\nThe PTX assembler from the CUDA driver is invoked at application runtime after OptiX built the final kernels from  your input PTX code.Please make sure you use the OptiX SDK top-level CMakeLists.txt inside the SDK sub-folder when configuring the solution. The one next to the INSTALL-LINUX.txt and INSTALL-WIN.txt files which explain that.\\nDo not use the CMakeLists.txt files inside the individual examples’ sub-directories as root for the CMake configuration. Those will miss the necessary settings done in the root level CMakeLists.txt file.@droettger, thank you for the reply.Maybe I should back up. Can you point me to a good beginning code using Optix that can be easily built with Optix 7.0 using CUDA 9.0 ?Thanks again.The OptiX SDK examples should also build with CUDA 9.0 when using the right CMakeLists.txt as root.The OptiX Release Notes lists the supported CUDA versions.\\nBelow are the relevant excerpts from the OptiX SDK 7.1.0 release notes.Development Environment Requirements (for compiling with OptiX)\\n● CUDA Toolkit 7, 8, 9, 10, 11\\nThe OptiX 7.1.0 prebuilt samples on Windows have been built with CUDA 11, but any specified toolkit should work when compiling PTX for OptiX. OptiX uses the CUDA device API, but the CUDA runtime API objects can be cast to device API objects.For running OptiX applications, the display driver needs to support the CUDA version a specific OptiX version has been built with.Graphics Driver:\\n● OptiX 7.1.0 requires that you install a r450+ driver.\\n● Windows 7/8.1/10 64-bit; Linux RHEL 4.8+ or Ubuntu 10.10+ 64-bitFor OptiX 7.0.0 it’s the following:\\nGraphics Driver:\\n● OptiX 7.0.0 requires that you install the 435.80 driver on Windows or the 435.12 Driver for linux… Note\\nOptiX dll from the SDK are no longer needed since the symbols are loaded from the driver.\\n● Windows 7/8.1/10 64-bit; Linux RHEL 4.8+ or Ubuntu 10.10+ 64-bit\\nCUDA Toolkit\\n● It is not required to have any CUDA toolkit installed to be able to run OptiX-based applications.\\nDevelopment Environment Requirements (for compiling with OptiX)\\n● CUDA Toolkit 7, 8, 9, 10\\nOptiX 7.0.0 has been built with CUDA 10.1, but any specified toolkit should work when compiling PTX for OptiX.\\nOptiX uses the CUDA device API, but the CUDA runtime API objects can be cast to device API objects.The examples inside the SDK are beginner level examples and try to show individual features of the OptiX API.\\nThere isn’t a simpler example than optixHello which isn’t even shooting a single ray.For additional OptiX 7 examples from beginner to advanced level please have a look into the resources linked in the following sticky posts at the top of the OptiX sub-forum:\\nhttps://forums.developer.nvidia.com/t/optix-7-1-release/139962\\nhttps://forums.developer.nvidia.com/t/optix-advanced-samples-on-github/48410/4Mind that OptiX 7 only supports Maxwell and newer GPU architectures.\\nCUDA 11.0 dropped support for SM 3.0 and SM 3.2 (Kepler) and deprecated SM 3.5, SM 3.7 (Kepler) and SM 5.0 (first Maxwell). That should all not be an issue with CUDA 9 or 10.\\nThe OptiX 7.1 examples build their PTX code with SM 6.0 (Pascal) to avoid the CUDA 11 deprecation warnings.\\nWhen targeting Maxwell GPUs, you’d need to set the SM version back to 5.0.\\nhttps://forums.developer.nvidia.com/t/optix-7-1-issue-with-running-samples-on-a-maxwell-card/140118/2Powered by Discourse, best viewed with JavaScript enabled',\n",
       " 'not-enabling-debug-output-makes-uploading-meshes-extremely-slow': 'We are developping a game that often creates meshes at runtime. We’ve been having a curious performance degradation whenever the following lines were NOT run during initialization of our renderer:We run them only if we enable some debug flag in the game engine, and in that case uploading our meshes takes about 100 microseconds. But if we don’t, uploading meshes always takes 15 milliseconds, no matter how big they are (tested from 1 to 512 tris), and this makes the game experience a lot worse.The lines that take that long are the following:In particular, the first call to glGenBuffers is the slowest. The next ones are fast. But if I add the extension, all calls are fast.This happens on Windows 10 64 bits GLES3, with a GeForce GTX 1060 6GB/PCIe/SSE2.\\nThere were reports of this when using GLES2 as well.\\nIs it a driver issue or is there something we forgot to do with the OpenGL API?More details about the issue: Enabling verbose output makes `add_surface_from_arrays` 100 times faster · Issue #52801 · godotengine/godot · GitHubWe run them only if we enable some debug flag in the game engine, and in that case uploading our meshes takes about 100 microseconds.But if we don’t, uploading meshes always takes 15 milliseconds, no matter how big they are (tested from 1 to 512 tris), …One minor correction: “queuing” the upload takes 100 microseconds, or 15 milliseconds.  Just looking at CPU timings, you don’t know when the GL driver actually performs the actual underlying work you’ve queued.Also, I wouldn’t expect dynamically creating buffer objects on-the-fly to be a fast operation you should put in your main rendering loop.  This may very likely trigger an implicit sync (where the CPU-waits-on-the-GPU/back-end-driver to catch up to a certain point in the command queue).  That said…Is it a driver issue or is there something we forgot to do with the OpenGL API?I would suggest looking at this in Nsight Systems.  With it, you can tell how much GL driver queue ahead you’re getting at any given point.  Be sure to use NVTX or KHR_debug markup (particularly NVTX Ranges or KHR_debug DebugGroups, respectively), so you can tell exactly where you are on both the CPU and (if desired) the GPU timelines.I don’t know for sure, but my guess is that in the “debug flag” case, you’re not getting much queue-ahead (GPU and back-end driver aren’t that far behind the CPU).  Whereas in the “normal” case, you’re getting nearly 1 60Hz frame of queue-ahead (just less than 16.6 msec) with VSync enabled.So if buffer creation triggers an implicit sync, then the 1st buffer object creation basically stalls your CPU draw thread queuing the work until the GPU / back-end driver catches up to a specific point in the command queue and then unblocks your CPU draw thread.  That’s going to take no time at all in the “debug flag” case (since the CPU isn’t that far ahead), but it’ll take nearly 1 60Hz frame in the “normal case” (since it’s queuing about 1 frame ahead).  This is more likely if your app and/or the driver is limiting queue-ahead to 1 frame (explicitly or implicitly).Anyway, if this is what’s going on, you’ll be able to see this in Nsight Systems.NV driver configs and GL usage issues that relate to if/how much driver queue-ahead you get with NVIDIA’s OpenGL driver include (but aren’t limited to): Full screen+focused vs. windowed, Low Latency Mode / Max Prerendered Frames, Triple Buffering Y/N, Threaded optimization, Monitor Technology, num GPUs, etc.  Of course, max queue-ahead can be thwarted by your app implicitly (via ops triggering implicit sync) or explicitly (by using glFinish() or Fence/Sync operation).  You may be hitting the former.One minor correction: “queuing” the upload takes 100 microseconds, or 15 milliseconds. Just looking at CPU timings, you don’t know when the GL driver actually performs the actual underlying work you’ve queued.I’m aware of this, and I also see that when the next calls are very fast in comparison, without any relation to how complex they are (detail in linked thread). However I did not expect a 15ms-long wait for calling into OpenGL the first time, it’s so long and systematic that it looked like something is wrong even for main-thread rendering. In fact even if I don’t do mesh upload, I can see the “stall” shifts to the first OpenGL call within the engine’s draw() routine. Perhaps that’s why other people had problems too and were able to “solve” their different issues by doing the “debug extension” workaround, in their cases it made perceived performance better.\\nSo it may very well be OpenGL waiting for a sync point. So I thought it’s either a wrong use, or it’s just… normal, and I have to deal with it somehow (not many options though).One use case that this causes problem with, is the fact I upload many meshes very fast, which means I’d like to be able to measure dynamically how much can be uploaded for a given time. If it’s too much, it might error/overflow/lag, if it’s too few, updates will feel too slow as resources are underused.Also, I wouldn’t expect dynamically creating buffer objects on-the-fly to be a fast operation you should put in your main rendering loop.I wish I could do it in another thread. Godot 3 has multithreaded renderer support so that rendering may happen in another thread, however the way it is implemented still stalls the main thread. I’m told OpenGL cannot be used from multiple threads so I can’t do this from another thread of my own. I also got reports of multithreaded graphics support having bugs in Godot 3, so I’ve not focused on it much. The devs plan to improve it in Godot 4, mainly with Vulkan, so likely more control on that.max queue-ahead can be thwarted by your app implicitly (via ops triggering implicit sync) or explicitly (by using glFinish() or Fence/Sync operation). You may be hitting the former.The only glFinish I could find in Godot is done at the end of rendering under some conditions, and is not called during my test. I could not find any use of glFenceSync.I would suggest looking at this in Nsight Systems. With it, you can tell how much GL driver queue ahead you’re getting at any given point. Be sure to use NVTX or KHR_debug markup (particularly NVTX Ranges or KHR_debug DebugGroups, respectively), so you can tell exactly where you are on both the CPU and (if desired) the GPU timelines.I installed NSight Systems, included and added an NVTX range around the region where the buffers are created in Godot’s renderer. Unfortunately when I run from NSight it seems to crash before even getting there. No rendering window opens, and the process just stops after a few seconds. If I run the same executable outside NSight it runs fine.Hi Marc.I’m aware of thisOk, cool.  I figured you did.  But didn’t want to assume.However I did not expect a 15ms-long wait for calling into OpenGL the first time, it’s so long and systematic that it looked like something is wrong even for main-thread rendering.In fact even if I don’t do mesh upload, I can see the “stall” shifts to the first OpenGL call within the engine’s draw() routine. Perhaps that’s why other people had problems too and were able to “solve” their different issues by doing the “debug extension” workaround, in their cases it made perceived performance better.That makes sense.So it may very well be OpenGL waiting for a sync point. So I thought it’s either a wrong use, or it’s just… normal, and I have to deal with it somehow (not many options though).That’s my guess.  I’ve tripped over similar implicit sync issues before in NVIDIA’s GL driver (different cause, but similar result), nailed them down, and resolved them.  It just takes a little patience.  And in hindsight, it makes sense why implicit sync is needed there.  I find that the more you “think like the driver” (what would I have to do here if I was the driver dev supporing this GL call sequence), know about the Vulkan method of doing it, and follow NVIDIA’s performance recommendations (OpenGL like Vulkan is a good start), the less you’ll trip over these implicit sync points.  But it still happens.  This is OpenGL, afterall (very deep driver with lots of voodoo magic going on in the abstraction).As to options, I’ll mention a few below, but first…I would suggest looking at this in Nsight Systems. With it, you can tell how much GL driver queue ahead you’re getting at any given point.Unfortunately when I run from NSight it seems to crash before even getting there.I would suggest posting info on your problem in the Nsight Systems forum for the OS you’re hitting this on.  The NVIDIA folks there have been very helpful to me in the past diagnosing and fixing similar issues (similarly on the Nsight Graphics forum).  The more specific details you can provide on what error is produced and what you are doing in your app/engine that seems to trigger this problem, the more likely it is that they’ll be able to assist and fix the problem you’re hitting.Toward that end, I’d first suggest that you:If that gives you a config that works well in Nsight Systems, great!  You can repeat this, gradually flipping on the knobs to determine what GL usage is likely triggering this problem.  It could be the component causing Nsight Systems issues is irrelevant to your implicit sync repro, in which case you could potentially defer this crash investigation until later.If you decide not to defer though, this procedure will give you specific info to provide to NVIDIA on what triggers the crash.  And if it doesn’t give you a runnable config, then you’ve just carved away a huge amount of code that’s off-the-table, and the issue is in the remainder.When I first started using Nsight Graphics and later Nsight Systems, I hit/tracked/avoided a few similar crash issues.  These stemmed from my app’s extensive use of the GL compatibility API and NVIDIA GL extensions, whereas the NVIDIA apps focus support on the GL core profile with a selected subset of extensions.  When in my app’s “Nsight profiling mode”, I disable use of a few extensions not well supported by the tools (e.g. for Nsight Graphics).  In particularly related to profiling tool crashes, I disable all use of NVIDIA’s awesome Bindless Graphics APIs (bindless buffer object access) when in “Nsight profiling mode”, falling back to the old bound buffers method of submitting batches.  This was important for getting stability in Nsight Graphics, as it did not support use of this extension.  Note that bindless texture use is fine.  And falling back from CSAA to MSAA textures for render targets was, though not required, useful for getting render target preview working.So it may very well be OpenGL waiting for a sync point. So I thought it’s either a wrong use, or it’s just… normal, and I have to deal with it somehow (not many options though).As far as that goes…  This is your baby, so you can deal with it how you want.  But if it were my problem to fix I’d:It sounds like you’ve done part of #1, but your mention of the stall shifting (to the 1st GL call in draw() when skipping the mesh buffer create+upload) suggests there might be more calls in this sequence, or other separate triggers of implicit sync.Anyway, if you determine that it’s the buffer creation that’s triggering implicit sync (very possible), then I’d suggest reading up on:methods.  That is:NVIDIA’s GL driver provides multiple methods to stream CPU data into and GPU render from these buffer objects efficiently, without implicit synchronization.Good luck!Powered by Discourse, best viewed with JavaScript enabled',\n",
       " 'error-code-3-subcode-2-open-gl-problem-rtx-4080': 'Hi,\\nI bought a new graphics card. RTX 4080 model. I am using 3d softwares (3ds max and Maya) both softwares I get the following error. I have no problems when I install rtx 3060 in current configuration.Unable to recover from a kernel exception. The application must close.\\nError code: 3 (subcode 2)andFaulting application name: maya.exe, version: 18.4.0.7413, time stamp: 0x5b513223\\nFaulting module name: nvoglv64.dll, version: 31.0.15.2802, time stamp: 0x63a3de6b\\nException code: 0xc0000409\\nFault offset: 0x0000000001859e25\\nFaulting process id: 0x0x4B8C\\nFaulting application start time: 0x0x1D926010CC17B99\\nFaulting application path: C:\\\\Program Files\\\\Autodesk\\\\Maya2018\\\\bin\\\\maya.exe\\nFaulting module path: C:\\\\WINDOWS\\\\System32\\\\DriverStore\\\\FileRepository\\\\nvmdig.inf_amd64_2f8b15057bd04fc7\\\\nvoglv64.dll\\nReport Id: da4e023d-84c7-43a6-bab8-c5a45f07c85a\\n*Faulting package full name: *\\nFaulting package-relative application ID:My Pc Spec:\\n|Processor|13th Gen Intel(R) Core™ i7-13700K   3.40 GHz|\\n|Installed RAM|64.0 GB (63.7 GB usable)|\\n|Product ID|00330-80000-00000-AA630|\\n|System type|64-bit operating system, x64-based processor||Edition|Windows 11 Pro|\\n|Version|22H2|\\n|Installed on|\\u200e1/\\u200e11/\\u200e2023|\\n|OS build|22621.1105|\\n|Experience|Windows Feature Experience Pack 1000.22638.1000.0|I tried this way but the same errors continue.\\nNVIDIA control panel that the Power management mode is set to “Prefer maximum performance”. or\\nGlobal settings High-performance Nvidia processor.Driver version: 528.02 DCHThank you\\nSerhatHi there @serhatgumus and welcome to the NVIDIA developer forums.In general these kind of issues are better discussed in either the GeForce forums or with customer support as stated in the error message you quoted.But I can give some suggestion on how to fix your problem.The Exception code means that there are mismatching configurations or libraries responsible for this, so the best bet is to do a complete clean re-install of both the NVIDIA drivers as well as your Maya installation.If that does not help, maybe try using a previous NVIDIA driver version that supports RTX40 series. Or exchange the Game Ready driver by the Studio driver.I hope this helps!I tried game driver and studio driver.\\nI tried old version (rtx 4080 not many versions).\\nI did a fresh install of windows.\\nI tried clean install.\\nI do not have this problem in a software I use, I am getting the same error even though I use 2023 version in 3ds max.\\nIs the problem software or a new card I just bought?\\nI actually tried a lot of things before posting here. I couldn’t find a solution.\\nHow can i solve it?Did you try driver versions 522.25 or 522.30? There was one regression with Maya/OpenGL introduced after those drivers, but that was fixed already with 527.38. So I doubt it is connected. Still it might be worth checking.If it still crashes, can you please attach the log and dmp file that the Maya error reporter gives you?Actually, I thought I could find a solution here, but I did not understand that you repeatedly asked what I wrote.\\nI guess I’ll have to continue with the technical service.It’s not just about “Maya”. I have the same problem in 3ds Max. I did a Super Position test today. and here the program crashed, the test ended incorrectly. I think the problem is hardware.\\n1363×934 175 KB\\nHello again!I do understand that it is not just about Maya. Why I mentioned Maya is because it gives us a good error report and failure dump where we could look into.I still think this is rather a software issue than a GPU hardware issue.The error message points to a broken installation of the OpenGL driver. That driver is part of the default NVIDIA driver installation on Windows. If there is a problem, than the complete driver package was not installed correctly.What I do not understand is that you say that everything works with a RTX 3060 but not with an RTX 4080. That is the only indicator that the GPU Hardware could be defect.Powered by Discourse, best viewed with JavaScript enabled',\n",
       " 'optixaccelrelocationinfo-data': 'Hi,I just wanted  more of an understanding of the data stored in OptixAccelRelocationInfo.TIAThe OptixAccelRelocationInfo is an opaque object as said inside the API documentation:The data inside it cannot be used for anything else than checking if an acceleration structure built on one device is compatible with another device using optixAccelCheckRelocationCompatibility and to relocate an AS using  optixAccelRelocate after copying it to a different device address (which must have the required alignment of OPTIX_ACCEL_BUFFER_BYTE_ALIGNMENT).Described here:\\nhttps://raytracing-docs.nvidia.com/optix7/guide/index.html#acceleration_structures#relocationHere is an example where I’m using it to check if geometry acceleration structures can be shared with CUDA peer-to-peer among devices, but that’s more for demonstration purposes since devices connected via NVLINK in that case are always matching the GPU architecture. It helped preventing a bug in a heterogenous GPU setup though.Getting the relocation info:\\nhttps://github.com/NVIDIA/OptiX_Apps/blob/master/apps/nvlink_shared/src/Device.cpp#L1285\\nChecking if it’s compatible:\\nhttps://github.com/NVIDIA/OptiX_Apps/blob/master/apps/nvlink_shared/src/Device.cpp#L1307What I’m trying to find out is would it be possible to share a geometry acceleration structure with 2 Optix contexts.\\nFor example, say I had 2 GPU’s with the same architecture.\\nBoth have their own Optix contexts.\\nIs it possible to copy the GAS from GPU1 to GPU2 and then call optixAccelRelocate using GPU2’s Optix context and GPU1’s OptixAccelRelocationInfo.\\nWould this be valid or OptixAccelRelocationInfo is specific to GPU1’s Optix context?That is not really “sharing” since each device has its own copy afterwards, but yes, that is exactly one use case.This check is also exactly what I’m doing in the two code lines I posted above, just that I don’t copy and relocate the GAS because I share it across the NVLINK bridge. In all other cases I simply build the GAS on each device.So if you take an OptixAccelRelocationInfo from a GAS on device A and if optixAccelCheckRelocationCompatibility with that on device B passes, you can copy the data from A to B (pointer needs to be aligned to OPTIX_ACCEL_BUFFER_BYTE_ALIGNMENT), and because that resides at a different 64-bit address afterwards,  you need to call optixAccelRelocate on device B with the OptixAccelRelocationInfo from A and the new CUdeviceptr to be able to use the copied GAS on device B with the new returned traversable handle.Mind that this relocation always must be done if you copy the AS somewhere else than it had been built to initially, even on the same GPU.Check the API reference on the optixAccelRelocate call which explains that:\\nhttps://raytracing-docs.nvidia.com/optix7/api/html/group__optix__host__api__acceleration__structures.html#gac31b24064e0caf8fd81202c8621036b3In the end this all works because these are just 64-bit CUdeviceptr and the CUDA allocations are all distinct because of the Unified Virtual Address (UVA) space under 64-bit systems.The question is if this is worth it. The GAS are built on the GPU and you could also build the AS in parallel on both devices and have the same result. Just saying.That makes sense.\\nWould it be possible to then share a GAS on 2 different Optix contexts on the same GPU.\\nSay, passing a handle to the GAS from context A to context B via CUDAIPC. But then to create a OptixTraversableHandle on the context B would we need to call optixAccelRelocate or is there another way to create a OptixTraversableHandle to the GAS on context A?I have no experience with CUDA inter-process communication. That’s only available under Linux.Excerpt from the CUDA Programming Guide on IPC:Using this API, an application can get the IPC handle for a given device memory pointer using cudaIpcGetMemHandle(), pass it to another process using standard IPC mechanisms (e.g., interprocess shared memory or files), and use cudaIpcOpenMemHandle() to retrieve a device pointer from the IPC handle that is a valid pointer within this other process. Event handles can be shared using similar entry points.This would make me nervous about the CUdeviceptr in both processes, and rightfully so.\\nReading the cudaIpcOpenMemHandle() manual it says:No guarantees are made about the address returned in *devPtr. In particular, multiple processes may not receive the same address for the same handle.If it’s not the same address it wouldn’t be usable without relocation and optixAccelRelocate() works in-place, which in turn would break the original data in the source process, so that would require a copy anyways.What would be the real-world use case requiring this?I actually got it working by sending an IPC handle of the GAS from one process to another. Then I just did a cast from the GAS buffer to a OptixTraversableHandle on the second application and that seemed to work and I was able to ray trace against it. So does that mean that’s all that’s needed to convert a GAS pointer into a OptixTraversableHandle?As for our use case… we have an application that simulates sonar and we ray trace under a dynamic ocean which is a GAS that updates every frame.\\nAs for the 2nd application, we want to visualise the dynamic ocean and the environment to see what is going on inside the sonar simulation.I know this can all be done easily inside a single application but there are benefits for us to keep it all modular in different applications.Be careful, since this means you’re using memory from one process that is owned by another process. This opens up some potentially tough questions, like ensuring that the process creating the memory always outlives the 2nd process using the memory. You will also be responsible to ensure that the 2nd process always has the security rights to access the memory. If you decided to start up some kind of server for the first process under a different user account, you might suddenly find everything breaks. Trying to share memory between processes could easily make it much less modular. (But I fully understand the desire to not duplicate memory unnecessarily!)And take special note of what Detlef said about cudaIpcOpenMemHandle():“No guarantees” means what it says, even if it appears to work right now. You should not count on the pointer being the same, otherwise it will break at some unexpected time later.Then I just did a cast from the GAS buffer to a OptixTraversableHandle on the second application and that seemed to work and I was able to ray trace against it. So does that mean that’s all that’s needed to convert a GAS pointer into a OptixTraversableHandle?No, you need to be using optixConvertPointerToTraversableHandle(). In general, casting the GAS handle to a pointer or vice-versa will crash, they are not the same value. BTW, we do not guarantee anything about the OptixTraversableHandle’s relationship to the pointer. While there may be some correspondence in the implementation right now, that may not be true on different GPUs or in the future, so don’t assume a working shortcut will continue working.–\\nDavid.I looked at optixConvertPointerToTraversableHandle() but I didn’t know what OptixTraversableType to pass in for a GAS. Would passing in OPTIX_TRAVERSABLE_TYPE_STATIC_TRANSFORM work for a GAS?I’m just wanting to explore if all this is possible as it would be nice not to duplicate memory. Otherwise it’s all probably easier doing this in one process.Oh, you’re right, you can’t use optixConvertPointerToTraversableHandle() to convert a GAS buffer into a handle. Sorry about that. Casting a pointer definitely work work though, so I think there is no sanctioned way to share GAS buffer pointers across processes. We can discuss it internally as a feature request, if it’s critical and there are no other options. It is definitely complicated though, not as simple as sharing a pointer, so I wouldn’t recommend waiting for it.One modular, multi-process architecture to consider would be a client-server type of system, where your server handles generic rendering requests in a multi-threaded, multi-stream way. This way you can have a single process “own” all the BVHs, and share memory whenever possible, but still serve different applications at the same time.–\\nDavid.Powered by Discourse, best viewed with JavaScript enabled',\n",
       " 'video-codec-calling-cuvidparsevideodata-not-triggering-callbacks-for-a-specific-rtsp-source': 'I have an application that works on files and rtsp based on the sample provided.I recently came across an RTSP stream where calling cuvidParseVideoData does not trigger callbacks.\\nWhat can I do to find out what is happening? I tried ulErrorThreshold = 100. No luck.VLC works on this source. It is H264.Turns out I have to manually inject the “extradata” from AvCodecContext with\\ncuvidParseVideoData.Powered by Discourse, best viewed with JavaScript enabled',\n",
       " 'pxjointangularlimitpair-invalid-limit-equivalence': 'The issue I am seeing is in version 3.3.4 but looking at the documentation for 3.4 it looks like it’s probably still applicable.I have configured a PXD6Joint which is locked in all axes except twist and is limited in the twist axis. I would like my joint to be able to twist from 0.8 radians to 3.6 radians. According to the documentation for PxJointAngularLimitPair my limits can go all the way from -2pi to 2pi but my lower must be less than my upper so I set lower=0.8 and upper=3.6. This works perfectly at the lower limit but as I rotate towards the upper limit my system explodes when my twist angle hits exactly pi. I haven’t totally stripped down my system to the simplest possible case but I believe I’ve removed all joints connecting the two actors except my twist joint.Next I observe that 0.8 radians is the same angle as -5.5 radians and 3.6 radians is the same as -2.7 radians so I change my limits to lower=-5.5 and upper=-2.7. This still satisfies the -2pi to 2pi range and lower is still less than upper. As far as I can see this should give the exact same behavior as before but this setup explodes immediately.My question then are:The documentation is incorrect here - the limits must be specified in range (-Pi,Pi) rathber than the documented limits.Releases > 3.4 will support the (-2Pi,2Pi) range but if you are on 3.4 or earlier then the supported range is actually (-Pi,Pi)Thanks for your reply. That’s very useful to know.So how would I express that I would like my joint to range from 0.8 to 3.6? Should I set the limits to Lower=0.8, Upper=-2.6? Is the bit about the lower being required to be less than the upper accurate?If not then presumably I should adjust the local constraint frames such that the twist range is easier to work with?thanks againYes, you will need to adjust the local frames so that your min and max sit inside the range (-Pi,Pi).Cheers,Gordongreat. I can do that. Thanks for the assistance Gordon.Powered by Discourse, best viewed with JavaScript enabled',\n",
       " 'isolating-gpu-or-needing-to-create-a-new-operating-system-to-obtain-neural-network-compute-power': 'I just have a question about isolating my GPU.i need to isolate it so i can utilize it fully.\\nis locking - How to prevent two CUDA programs from interfering - Stack Overflow that i found in the developer.nividia forums (nvidia-smi utility) work for NVIDIA GeForce GTX 1050 Ti with Max-Q Design?I just need to know if it’s really the right way or should create a new Operating System on a usb-stick to be able to utilize fully.I also need to know how it can happen programmatically in c-lang and asm-lang.also is there a Driver Development Kit (DDK) or something i use to create a driver for my OS?thanks and regards,\\n-NasserHello @rownasser, welcome to the NVIDIA developer forums!nvidia-smiwill work on any NVIDIA GPU, including Mobile versions, if you installed the respective driver. And yes, the tool allows to control GPU device affinity among many other things.But to better answer your questions it would help if you could describe in a bit more detail what you want to achieve when you say “i need to isolate it so i can utilize it fully.” Compute workloads? Deep Learning? Rendering? Gaming? Video editing?Before that it is difficult to point you to the correct SDKs. For example CUDA allows you to programmatically select GPU devices to be used. But I don#t know if you even want to use CUDA.On the driver side there is no public DDK. Some of the Linux kernel modules are open source, but we don’t supply sources or tools to create your own drivers.Compute workloads  and deep learning…I know of CUDA, OpenCL and Vulkan. Just checked out CUDA for a bit but not the other two… i saw the CUDA C++ it had some driver API or something… for c or c++ I don’t remember which…And so no programmatic(APIs) ways for nvidia-smi? Also is it unrecomended to run it from a script or c-lang system command by any chance?Also i presume CUDA can have asm? Also c instead of c++?:)\\n-NasserFor compute I would recommend CUDA (see also our dedicated forums for a lot more information). It does allow programmatically controlling the GPU. In terms of disabling the render functionality of the GPU for the operating system, that needs to happen on the OS side since it involves at least restarting the Display Manager, if not a rebootI am not certain if this can be achieved by code, but probably by scripting.Anyways, CUDA can be programmed with a diverse set of languages, check out the CUDA C++ Programming Guide for more detail.ok i’ll check it out…Powered by Discourse, best viewed with JavaScript enabled',\n",
       " 'is-the-protocol-support-for-nvlink-complete-for-the-3090-graphics-card': 'What are the differences between V100 and 3090 connected using an nvlink bridge? Does the 3090 with nvlink support NVLINK UMA.I am using two 3090 graphics cards for neural network training. I want to enable one GPU to obtain training data through another GPU. I would like to know if two 3090 graphics cards with nvlink allow me to develop such a GPU data sharing frameworkPowered by Discourse, best viewed with JavaScript enabled',\n",
       " 'looking-for-vulkan-ray-tracing-tutorials-all-features-in-depth': 'I have not found ANY tutorial that explains in depth how to implement all the RTX/Turing functionalities.\\nFor instance, I have found code examples of shadows, reflections, ray intersection shader… But not well documented and no tutorial about it.\\nAll the tutorials I have found only explain a very basic approach that simply mimics a basic rasterizer (Already found these, don’t waste time to send the links).My project is a game engine built from scratch with a ray-tracing-only renderer, and I need to implement full raytracing shadows, reflections and intersection shaders.Also, DLSS and Denoising would be great… I have not found anything on these, not even sample code.If anyone knows about a way to learn these in depth or want to coach me directly, it will be very appreciated.Thank you.(Also, I would pay $$$ for good coaching on these topics)It depends how comfortable you are with ray tracing already. I have found that once you’ve read some reference material (in my case I read the first two books of Ray Tracing In One Weekend, GitHub - RayTracing/raytracing.github.io: Main Web Site (Online Books)), then the NVIDIA VKRT API concepts are relatively easy to understand.The hard bits are the Vulkan boilerplate code and the weird “virtual table dispatch” used to map different object groups to different sub-shaders.Hi GPSnoopy, Funny enough I was already learning by your own source code in RayTracingInVulkan, and yes I have done all the *In One Weekend tutorials too.Your project is very inspiring to learn from and I have advanced a lot in my understanding of NVidia’s extension and Ray Tracing in general, but I am still missing on DLSS and Denoising.Also, I was actually trying to find out how to contact you without any success, and now here you are !I would like to have a discussion with you (or coaching) for taking the right decisions for my project which is a very, VERY ambitious real-time game engine based on Vulkan and Ray Tracing.The project is a full scale multiplayer space game engine where you can walk on the surface of trillions of realistic sized procedural planets, travel between stars (realistic distances), and realistic space physics.Are you interested in talking about it ?My engine currently in development:\\nhttp://Vulkan4D.comThe game’s Unity prototype I made last year:Galaxy4D is a Realistic, Full-Scale Multiplayer SpaceSimHi ZOlivier,Impressive Galaxy4D video, I like the space video as well. Sounds quite involved.If you’ve read the ray tracing book series and have seen the RayTracingInVulkan source code, then you likely know as much as I do already. This was a hobby project for me.Denoising is a subject I’m not familiar with, hence why I’ve not implemented it. I’d suggest trawling the internet for articles and demos to learn more about it. The Q2 RTX source code is probably a good example of a production quality real-time implementation (GitHub - NVIDIA/Q2RTX: NVIDIA’s implementation of RTX ray-tracing in Quake II). You can even try emailing Peter Shirley (the author of the aforementionned books) for pointers on the subject, he’s quite open on social medias.DLSS is a proprietary implementation by NVIDIA, and the AI model needs to be trained by NVIDIA itself on their own compute platform. IMHO this is unlikely to succeed because it’s so closed. Even their Q2 RTX did not use DLSS.Hope this helps.Just saw NVIDIA has got a few videos on this topic in one of their YouTube channel. For example:Conquering Noisy Images in Ray Tracing with Next Event EstimationReal Time Path Tracing and Denoising in Quake II RTXUsing Path Tracing: Quake 2 on VulkanQuake 2 on VulkanSome of these videos are building on top of the experience gained from the previous implementation. So I’ll let you re-arrange them in the correct order.I have gone through potentially all tutorials + all official documentation related to ray tracing in vulkan.However, there is one very simple thing that I cannot figure out.In every tutorial and documentation they keep saying that instead of having many BLAS, we should provide less BLAS, each with multiple geometries, each geometry with their own hit shader in the binding table.But nowhere do they tell how to specify the shader offset from binding table for individual geometries… it seems to be only on a per-instance basis in the TLAS… or maybe the trick would be multiple instances with different geometries by specifying a geometry index in each instance but this also seems to be impossible…So as I understand it, one can only have no more than one hit shader group per BLAS for the main rays…But that is not what the documentation is saying…Am I missing something here ?Thanks a lot for the help.Powered by Discourse, best viewed with JavaScript enabled',\n",
       " 'vk-khx-external-memory-capabilities-missing-on-gtx-780-ti': 'I only see the VK_NV_external_memory_capabilitiesSearched out on this forum and found vulkaninfo from GTX 760 that had the KHX extension as well.There’s software that requires the VK_KHX extension. I’m unable to start it because the KHX extension is not present.I know this extension is already KHR and it’s going to be available in the next driver release, but why do I have this problem in the first place?Powered by Discourse, best viewed with JavaScript enabled',\n",
       " 'returned-error-4-nv-of-err-invalid-ptr-in-function-nvidiaopticalflowimpl': 'Hi, every one.  When I run the “nvof =cv2.cuda_NvidiaOpticalFlow_1_0.create(frame1.shape[1], frame1.shape[0], 5, False, False, False, 1)”, I meet the problem below:\\ncv2.error: OpenCV(4.4.0-pre) /data00/wangqing/nvidiaSDK/opencv_contrib/modules/cudaoptflow/src/nvidiaOpticalFlow.cpp:380: error: (-27:Null pointer) GetAPI()->nvCreateOpticalFlowCuda(m_cuContext, &m_hOF) returned error 4:NV_OF_ERR_INVALID_PTR in function ‘NvidiaOpticalFlowImpl’I really do not konw how to fix it. Can anyone help me? Thanks in advance!GPU: 4 1080Ti\\nOpencv: 4.4.0-pre\\nOptical_Flow_SDK_1.1.10Hi.GPU: 4 1080TiI assume this is ‘GeForce GTX 1080 Ti’, which is based on Pascal family GPU.\\nNVIDIA Optical Flow requires Turing or above family GPU.Thanks.Powered by Discourse, best viewed with JavaScript enabled',\n",
       " 'to-get-optix-version': 'Hello,how can I get the information, if the graphic card supports OptiX, and if so, which version?Thanks.You could do something like the CUDA device query example and get the Streaming Multiprocessor (SM) versions of the installed devices. Sample3 inside the OptiX SDK does something similar, enumerating the devices before creating a context.\\nThe OptiX release notes document contains information about which SM versions are supported.\\nFor single GPUs all SM versions from 1.0 to 5.2 are supported by OptiX 3.6.3, but there are some features which require at least SM 2.0, for example multi-GPU and some acceleration structure builders on the GPU.\\nBecause of that I would recommend to test if the installed GPU to use with OptiX has at least SM 2.0 to be on the safe side.Can I add to this: is it also sensible to check for a maximum SM version?Suppose that in 2 years there’s a new generation of NVIDIA cards, which is not supported by an old OptiX version (such as was the case with Maxwell and 3.6). The application would then crash.How can I add a reliable check in the sense of “Your graphics card is too new”?True, that happened with Kepler and Maxwell. Forward compatibility is something which hopefully improves with future OptiX versions.\\nYou could inform the user that the OptiX version your program uses is only officially supporting a known set of SM versions but I would not make this an end condition to enable the user to test if the application works after all.I had the same question, as InformatikSuperheld.\\nAnd add to this: for example we use OptiX version 3.0.0 just now.\\nIf the graphic card is “too new” , the application would be crash.\\nIf the graphic card is “too old” (supports only version 2.6), application would be crash too.\\nI see no possibility to avoid crash at all.\\nAre there any possibility to get compatibility status (if OptiX version and the graphic card are compatible)  without start OptiX-application (“from outside”)?Without start OptiX application? You mean before the user starts the app, can they know whether it’s compatible? Yes. The list of supported devices for each OptiX version is in the release notes, and you can pass this info to your users.Or did you mean at application runtime, before creating the OptiX context? This is also possible by querying the compute capability of the cards and not choosing the cards that are too new. Program your application to know which cards are too new, based on the release notes.Powered by Discourse, best viewed with JavaScript enabled',\n",
       " 'screen-problem-please-help-me': 'i do not know if this is the right place on this forum to discribe my problem.\\nmy english is not that good :-(\\ni have a problem to put my large images or wallpapers on my scream, always ( no matter how large the images are). the black beams ( i do not know if beams is the good word ) are always on the left and right side of my screen.how do i get the black beams under and above my screen please ???\\nmy nivdea; gtx force 750dutch:op mijn scherm bevinden de zwarte balken zich steeds aan de linker en rechterkant.\\nmaar hoe krijg ik de zwarte balken in godsnaam boven en onderaan mijn scherm???\\nhet wil mij maar niet lukken !!!Powered by Discourse, best viewed with JavaScript enabled',\n",
       " 'simulate-does-not-return': 'Hello, I am using PhysX version 3.3.1 and I have the problem that when I call Simulate() during my update loop, the method never returns.Any idea how that could happen ?My code is pretty simple and my scene consists of a heightfield and a couple of rigid dynamic actors. Nothing too fancy.I saw someone had the same problem on the “stackoverflow” forums, here\\n[url]c++ - PhysX - simulate() never ends if GPU used - Stack OverflowBut the solution proposed by the user is just to revert to a previous version of physX, I thought I better ask here before jumping to conclusions.Regards\\nChrysPJust to clarify, the thread I mention above is similar not exactly the same . I am not trying to do anything with CUDA or particles etc. Also my application doesn’t freeze. It just seems to not ever get out of the “simulate” method.I am attaching the Physx Visual debugger and i can see my scene and all objects in it but they don’t move (because of simulate not progressing i guess).Just in case someone is interested, here is my code , pretty much all take from the first sample in the SDK.this initializes all the physics stuffthen I initialize my sceneand here is where i update the scenemy code gets stuck on “mpPxScene->simulate(stepSize);”but in the VDO i can see my scene just nothing moves.Is there a reason you are locking the scene? considering I don’t think you are doing concurrent access to PhysX…you are right , I just did that because i saw the examples doing it.\\nInitially I wasn’t doing it and this isn’t causing the issue (I can remove the lines and it still happens).I did find out that by removing my heightfield from the scene the simulate now isn’t blocked anymore.\\nSo maybe my heightfield is too big or something like that.My dynamic actors are still not falling from the sky though. So I probably still have something missing somewhere. :( I’ll try read up on the heightfield issue. (odd that it shows correctly in the VDO though).Okay so turns out things actually were moving just really slow.\\nI think my heightfields definitely need some re-thinking.Powered by Discourse, best viewed with JavaScript enabled',\n",
       " 'unable-to-install-nvidia-drivers-for-3090-on-ubuntu-20-04': 'Hello,I have a 3090 installed on a machine with Ubuntu 20.04. I have been trying to install Nvidia drivers (both manually, using the .run file, and through “Software and Updates”), but cannot get the drivers to work.This is the my kernel version : 5.15.0-53-generic\\nI have tried 520, 515 and 510 versions (open-kernel and metapackage versions), as well as 515.76 .run file from the official drivers website.\\nWith 515,520 open-kernel versions - the system outputs this when running nvidia-smi :NVIDIA-SMI has failed because it couldn’t communicate with the NVIDIA driver. Make sure that the latest NVIDIA driver is installed and running.And with the other non open-kernel versions the system boots to a black screen.I have tried several things, including the solution from this thread : Cannot get nvidia driver (520, 515, 515-open, or 510) working in Ubuntu 22.10 , but to no avail.I have blacklisted Noveau and all the other steps mentioned in the thread as well, but none of them work. (I have not yet disabled Secure Boot, though)What process should I follow to install nvidia-drivers 515 or above on my system?Here is a bit more additional information.Currently, I have the 515 open-kernel drivers installed and it boots up correctly, but upon running nvidia-smi I get this error; this happens with 520 open-kernel version as well. :Unable to determine the device handle for GPU 0000:01:00.0: Not FoundThere is no /etc/modprobe.d/nvidia-graphics-drivers-kms.conf on my system.This is the contents of /lib/modprobe.d/nvidia-kms.conf :options nvidia-drm modeset=1The output of nvidia-settings :ERROR: A query to find an object was unsuccessful\\nERROR: Unable to load info from any available system\\n(nvidia-settings:9774): GLib-GObject-CRITICAL **: 10:06:17.428: g_object_unref: assertion ‘G_IS_OBJECT (object)’ failed\\n** Message: 10:06:17.430: PRIME: Requires offloading\\n** Message: 10:06:17.430: PRIME: is it supported? yes\\n** Message: 10:06:17.447: PRIME: Usage: /usr/bin/prime-select nvidia|intel|on-demand|query\\n** Message: 10:06:17.447: PRIME: on-demand mode: “1”\\n** Message: 10:06:17.447: PRIME: is “on-demand” mode supported? yesThe output of dmesg | grep nvidia :Any help would be appreciated!I am attaching the nvidia-bug-report and nvidia installer logs here :\\nnvidia-installer.log (42.5 KB)\\nnvidia-bug-report.log (3.9 MB)I have recently purged my system and tried to reinstall the drivers. This is the procedure I followed :I rebooted the machine and the GUI seems to be broken, this is what I can see on boot :\\n\\nScreenshot (383)3491×1868 277 KB\\n(Please note that this happened with 520 and 510 non open-kernel versions as well)I have ssh open on the machine, so I can remotely ssh in and use the terminal.\\nnvidia-smi runs properly, this is the output :Unfortunately, any operation I run on the GPU leaves the machine hanging, whether i run it from base or through an ngc docker. (even though torch,tensorflow etc can see that cuda and gpu is available)\\nFor eg, a simple torch.rand(1).to(“cuda”) runs indefinitely.I am attaching the nvidia-bug-report and installer logs here :\\nnvidia-bug-report.log (2.3 MB)\\nnvidia-installer.log (42.5 KB)Thank you!Powered by Discourse, best viewed with JavaScript enabled',\n",
       " 'is-cuvideoctxlock-redundant': 'My understanding from the runtime/driver documentation regarding cuda contexts is that they can exist on multiple threads and can safely be push popped without any synchronization/locks.  However the documentation inside nvcuvid.h appears to disagreeit sounds like this was a way to get around the single context per thread limitation (pre CUDA 4.0 I think) but should be unecessary now is that correct?If so would it also be unecessary to use this with CUVIDDECODECREATEINFO::vidLockI am asking because this lock is still used inside the samples (NvDecoder.cpp:279).Hi @cudawarped and welcome to the NVIDIA developer forums!After some consideration I think the CUDA category will be better suited to answer your question on this.I’ll keep on watching the thread in case I am mistaken and we need to pull in specific video expertise.Thanks!Moved it back as requested, let’s see we can find both video and CUDA experts to comment!Is there any update? Can Nvidia codec developers make a comment? There is a bunch of threading related questions on the forum that have been left unanswered for years.Is the practice of using lock outdated post CUDA 4.0? What is the recommended way of doing multithreaded encoding and decoding with the codec API?Powered by Discourse, best viewed with JavaScript enabled',\n",
       " 'agx-orin-rtc-not-working-after-rebooting-it-is-starting-with-21-april-2022': 'Hi Team ,\\nI am using Orin AGX dev kit module and version Jetson_Linux_R35.1.0_aarch64 /kernel-5.10. In this RTC not working. Mounted proper 3V cell .  Every reboot it is starting with 21 April 2022 . Attached logs for some command response to help guiding us.====================================\\nnvidia@nvidia-desktop:~$ sudo i2cdump -y 0 0x50\\nNo size specified (using byte-data access)\\n0  1  2  3  4  5  6  7  8  9  a  b  c  d  e  f    0123456789abcdef\\n00: 02 00 fe 00 00 00 00 00 00 00 00 00 00 00 00 00    ?.?..\\n10: 00 00 00 0a 36 39 39 2d 31 33 37 30 31 2d 30 30    …?699-13701-00\\n20: 30 30 2d 35 30 30 20 48 2e 30 00 00 00 00 00 00    00-500 H.0…\\n30: 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00    …\\n40: 00 00 00 00 6e 83 78 2d b0 48 31 34 32 31 31 32    …n?x-?H142112\\n50: 32 31 30 39 36 37 33 00 00 00 00 00 00 00 00 00    2109673…\\n60: 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00    …\\n70: 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00    …\\n80: 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00    …\\n90: 00 00 00 00 00 00 4e 56 43 42 00 00 4d 31 00 00    …NVCB…M1…\\na0: 00 00 00 00 00 00 00 00 00 00 00 00 6e 83 78 2d    …n?x-\\nb0: b0 48 0a 00 00 00 00 00 00 00 00 00 00 00 00 00    ?H?..\\nc0: 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00    …\\nd0: 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00    …\\ne0: 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00    …\\nf0: 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 85    …?\\nnvidia@nvidia-desktop:~$\\nnvidia@nvidia-desktop:~$ ls -al /dev/rtc*\\nlrwxrwxrwx 1 root root      4 Apr 21 18:24 /dev/rtc → rtc0\\ncrw------- 1 root root 250, 0 Apr 21 18:24 /dev/rtc0\\ncrw------- 1 root root 250, 1 Apr 21 18:24 /dev/rtc1\\nnvidia@nvidia-desktop:~$ sudo timedatectl set-time ‘2022-12-17 13:41’\\nnvidia@nvidia-desktop:~$ date\\nSaturday 17 December 2022 01:41:03 PM IST\\nnvidia@nvidia-desktop:~$ sudo hwclock -r -f /dev/rtc1\\n[sudo] password for nvidia:\\n1970-01-01 05:45:51.629109+05:30\\nnvidia@nvidia-desktop:~$ sudo hwclock -r -f /dev/rtc0\\n2022-12-17 13:41:27.440697+05:30\\nnvidia@nvidia-desktop:~$ timedatectl set-ntp 0\\n==== AUTHENTICATING FOR org.freedesktop.timedate1.set-ntp ===\\nAuthentication is required to control whether network time synchronization shall be enabled.\\nAuthenticating as: nvidia, (nvidia)\\nPassword:\\n==== AUTHENTICATION COMPLETE ===\\nnvidia@nvidia-desktop:~$ date\\nSaturday 17 December 2022 01:42:05 PM IST\\nnvidia@nvidia-desktop:~$ i2cget -f -y 4 0x3c 0x01\\n0x81\\nnvidia@nvidia-desktop:~$ hwclock --verbose\\nhwclock from util-linux 2.34\\nSystem Time: 1671264842.705772\\nTrying to open: /dev/rtc0\\nNo usable clock interface found.\\nhwclock: Cannot access the Hardware Clock via any known method.\\nnvidia@nvidia-desktop:~$ cat /lib/udev/rules.d/50-udev-default.rulesACTION==“remove”, ENV{REMOVE_CMD}!=“”, RUN+=“$env{REMOVE_CMD}”\\nACTION==“remove”, GOTO=“default_end”SUBSYSTEM==“virtio-ports”, KERNEL==“vport*”, ATTR{name}==“?*”, SYMLINK+=“virtio-ports/$attr{name}”RTC_cmd_logs.txt (3.1 KB)Powered by Discourse, best viewed with JavaScript enabled',\n",
       " 'hammersley-sampling': 'I’m wondering how people use Hammersley sampling commercially (I think UE4 is using it also). AFAIK, it’s patented by NVIDIA. It’s a QMC method. The situation is a bit funny, since I doubt NVIDIA actually wants to stall the game industry. But still, are you feeling safe to use it in your shaders?Powered by Discourse, best viewed with JavaScript enabled',\n",
       " 'strange-intersection-issue': 'I’ve encountered a strange intersection issue in a renderer I’m working on. In the attached image is the front and back faces of the tall box from inside a cornell box. You can see that the back face is “in front of” the front face.\\n1020×1018 1.72 KB\\nIf I switch from Trbvh to NoAccel, then I get the same result, unless I switch the order of the two faces in the indices array (I’m using the triangle_mesh.cu from the samples). I’m kinda stumped by this. It’s obviously something I’m doing wrong, but I don’t understand how this would happen in the first place so not sure what to do to debug!There is not enough information to say what’s going wrong.This result shouldn’t even happen for coplanar faces. One guess would be that something isn’t normalized correctly. Maybe note that the anyhit and closesthit program domains operate in different coordinate spaces (object vs. world).\\nThe triangle_mesh intersection routine has some special mode to refine the hit point. I would recommend to start simpler.There is a more straight forward indexed triangle intersection routine and a function which creates a cube created from 12 triangles inside my OptiX Introduction examples for comparison. Links to the presentation and source code here:\\n[url]https://devtalk.nvidia.com/default/topic/998546/optix/optix-advanced-samples-on-github/[/url]When reporting any OptiX related issues please always provide at least these system configuration details:\\nOS version, installed GPU(s), display driver version, OptiX version (major.minor.micro), host compiler version, CUDA toolkit used to generate the PTX code.Yes sorry Detlef there wasn’t much info there. Was hoping you’d say, “oh well if that happens you’re obviously doing X wrong” :)I get the same results across the following configurations:It’s the same regardless of whether I use the refining intersection, the non-refining variant, or a simple one I copied from the github repo you link:Adding some printfs I get the following, which seems fine at first glance (note I switched from a perspective camera to a simple orthographic just to rule out the perspective matrix doing something weird):Note that prim 2 and prim 3 are never considered, even though their bounding boxes show they’re clearly closer to the ray origin than 0 and 1. Then if I switch the order of the prims so 2 and 3 come first in the index buffer:Here’s an OAC, if that helps: https://drive.google.com/open?id=1PFjP7bvpa3z-oWUks-ttb0ve40kJEUmOWhat should the expected output in the result_buffer look like with that trace?\\nI see a dark yellow rectangle on black background with either NoAccel or Trbvh.\\n(Windows 10, OptiX 5.1.0 DLL, 411.63 display drivers, Quadro P6000.)What does your anyhit program look like in CUDA source?Because when omitting the anyhit program, the result gets dark magenta which should be the front plane according to your image above.\\nThat could be an instance of a known OptiX bug related to scaled transformed transparent objects (using rtIgnoreIntersection).On the system configuration, what is your host compiler and esp. CUDA compiler version?\\nThe driver branch alone doesn’t help. The exact version numbers are required to be sure to reproduce your setup in case this is platform specific. I’m not able to test MacOS or Linux.I would not set variables like the scene_root object on the ray generation program scope but on the context global scope. Actually I have never set any variables on program scope in my renderers.Hi Detlef, now the problem is obvious thanks to you - I’d assigned both a closest hit and an any hit program to the material, which obviously makes no sense, by virtue of a copy-paste error from previous code. I wonder - is there any use for having both assigned to the same ray type, or might this be better considered to be an error upon validation (in order to save people like me from their own stupidity)?“I would not set variables like the scene_root object on the ray generation program scope but on the context global scope. Actually I have never set any variables on program scope in my renderers.”This is interesting - the docs seem to warn against this:Variables with definitions in multiple scopes are said to be dynamic and may incur a performance penalty. Dynamic variables are therefore best used sparingly.but on closer inspection it seems like it’s defining the same variable in multiple scopes that’s slow, so is defining a variable exactly once in some inherited scope ok then?The assignment of closesthit and anyhit programs to raytypes depends solely on the kind of behaviour you want to implement.For example opaque materials only need a closesthit program on the radiance ray and a very simple anyhit program on the shadow ray, which simply indicates that the visibility test failed and calls rtTerminateRay.\\nThough for cutout opacity materials you need to have the same closesthit program on the radiance ray again, but an anyhit program which evaluates the cutout opacity condition and calls rtIgnoreIntersection for holes. Similar for the anyhit program on the shadow ray.My OptiX Introduction presentation and source code examples explain exactly that case on slides 28 and 38.\\nLinks here: [url]https://devtalk.nvidia.com/default/topic/998546/optix/optix-advanced-samples-on-github/[/url]Code in example optixIntro_07 showing that:\\n[url]https://github.com/nvpro-samples/optix_advanced_samples/blob/master/src/optixIntroduction/optixIntro_07/shaders/anyhit.cu[/url]Defining esp. the scene root node object at a program scope doesn’t make much sense because you need that in all rtTrace calls.For a simple path tracer there is normally one rtTrace call inside the ray generation program handling all ray path segments and one inside the closesthit program for the shadow ray doing the visibility test for the direct lighting.\\nMeans you would need to define it on all program objects which need it in your case, and that is not necessary when you simply define it at the context scope.The variable scope lookup order allows to do something like default parameters which can be overwritten by the same variables at a higher scope. That is the case which is more expensive.\\nI recommend to not do that at all but always declare variables in a way which reduces the number of nodes and variables.For example, in my introduction renderer I do not even declare parameter variables at the material scope anymore, because that allowed to use only two materials in the whole renderer and their parameters are controlled by a single index on the GeometryInstance.That’s possible because I have only one closesthit program and three anyhit programs. BSDFs are handled by calling into fixed functions implemented as buffers of bindless callable programs which do the sampling and evaluation. (OK, I have a separate closesthit program for the area light, but just because that made the examples more introductory.)This can get a lot more sophisticated if the material configuration itself is done in another bindless callable program. I’m building these per unique shader from a shader network on the host at runtime to produce efficient code. Input parameters are not baked to be able to reuse shaders for different material instances and esp. to keep the OptiX kernel size small.Thanks Detlef, that makes sense. Seems like another use of the any hit program could be filtering out same-prim intersections if there’s a way of identifying which geometry you’re evaluating as well as which prim.Out of curiosity, what is it that makes the definition of multiple scopes expensive?Yes, self-intersection avoidance is possible in an anyhit program.The geometric primitive ID is known inside the intersection program and can be handled like any other attribute.\\nYou can also store a unique triangle ID into the triangle index. For that I use uint4 triangle indices and put it into the .w component.GeometryInstances can hold variables so you can identify them with another ID which is accessible inside the intersection program as well and can be handled as another attribute.If you only have triangles, all you need as attributes would be the two barycentric coordinates and these two IDs. It’s highly recommended to calculate the necessary attributes deferred inside the anyhit and closesthit program as needed for performance reasons.What does not work is to identify an instanced sub-tree because Transform and GeometryGroup nodes cannot hold variables.\\nBeing able to uniquely identify instances is a long standing OptiX feature request.But for self-intersection avoidance alone it’s not necessary to identify a hit geometric primitive uniquely. It’s enough to detect that it was not the same as the ray started from, and that can be done by evaluating the transform at the primitive you started from against the transform of the hit primitive if both IDs are the same.\\nHow you do that is your choice. It’s normally enough to transform some arbitrary non-null component point and compare the results.For performance reasons I would not recommend that method. Although that allows to hit both coplanar faces, which doesn’t help you much because the order is still traversal dependent, it’s really a lot slower than offsetting the origin to be off the surface.Some related posts:\\n[url]https://devtalk.nvidia.com/default/topic/913414/?comment=4792621[/url]\\n[url]https://devtalk.nvidia.com/default/topic/997269/?comment=5098975[/url]\\n[url]https://devtalk.nvidia.com/default/topic/1025193/?comment=5214700[/url]\\n[url]https://devtalk.nvidia.com/default/topic/930666/?comment=5133996[/url]Defining identical variables in multiple scopes along the variable scope search order simply increases the necessary records and overhead to identify the correct values in OptiX.Powered by Discourse, best viewed with JavaScript enabled',\n",
       " 'dynamic-convex-level-geometry': 'Hello!I’m working on an indie project, and we are trying out PhysX for our physics and collision systems.Our level geometry is built from a great number of convex meshes, that the user can modify in real time, however each mesh has the limitation that it has to be a valid convex polyhedron. They are just about the same thing as brushes from the Quake era, however we don’t use a BSP tree.Each polyhedron is limited to 8 vertices, and 12 faces(They start as cubes, and each face can be triangulated once). So each mesh is very simplistic, however we have a lot of them. They will always be static and non-modifiable while the physics system is actually doing simulation however.I’m doing some simple tests right now and my first approach was to simply generate a convex shape and a static rigid body for each “brush”, add them to the scene and let PhysX deal with it.The level geometry can get quite dense in certain places, where the artist decided to go a bit crazy with the detailing.As opposed to trying to guess a number of brushes, I’ve taken a screenshot from the visual debugger showing the complexity of a fairly full-featured room: Imgur: The magic of the InternetWe’re basically looking at 5-15 times that, for a level.The question is if this is a decent approach or if I’ve completely missed some other way of doing things that would be preferable? In terms of polygon counts we’re not breaking any records obviously, however I’m worried about the sheer count of separate static meshes.Any insight from someone with more experience would be helpful.You might want to experiment with PxAggregate.Large shape counts can put pressure on the broadphase. A good way of helping with broadphase performance is to create a PxAggregate and then add multiple actors to it.  The caveat is that we only allow 128 shapes per PxAggregate.You have a few options with aggregates. You can add a single rigid body, attach 128 shapes and then add the rigid body to its own aggregate. Alternatively, you could make 128 rigid bodies each with a single shape and then add all the actors to the aggregate.  There’s probably not much performance difference but there will be a memory difference between these two approaches. My instinct is that the biggest difference will be how easy it is for you to implement code to add and remove shapes.  Instinctively, it seems easier if you have one shape per actor. That comes at the cost of a larger memory footprint.There is no guarantee that aggregates will give better performance.  We’ve found they typically give a good boost but there are some scenarios where they make little difference. If they don’t help and you’re worried about memory consumption then you might want to look at just attaching multiple shapes to static actors.  There is no limit to the number of shapes supported on a single actor so you could go to the extreme of having a single static actor and attaching all your shapes to it.Thank you!I will try the approaches you suggested.Powered by Discourse, best viewed with JavaScript enabled',\n",
       " 'is-optix-3-9-compatible-with-cuda-toolkit-8-0': 'Hi,I have a Fermi based Quadro card and wanted to test some stuff with Optix. It seems like Optix 3.9 is last release supporting those cards and i am curious whether i can use that in conjunction with Cuda 8.0 release.Thanks…No, CUDA Toolkit 7.5 is the most recent one which works with OptiX 3.9.1.\\nAlways check the OptiX Release Notes before setting up a development environment for any version.Thanks for the clarification Detlef. I did check the release notes, but it was not clear to me whether that is possible.BestPowered by Discourse, best viewed with JavaScript enabled',\n",
       " 'unreal-engine-4-with-vr-works-360-video': 'I was wondering if its possible to use VR Works 360 Video with UE4 so I can record stereoscopic movies?\\nAt the moment I use Ansel with a macroscript but its not the most reliable thing.Any ideas? ThanksI would suggest to use Surreal Capture software (https://www.surrealcapture.com) to capture 360 video from UE4 directly without much effort.Powered by Discourse, best viewed with JavaScript enabled',\n",
       " 'cloudxr-server-app-dont-import-cloudxrserveroptions-txt-file': 'Dear CloudXR team\\nI try to install CloudXR in my PC and try to enable audio receive feature in server side but fail.\\nI check the log and find receive audio feature is disable event I add -ra in CloudXRServerOptions.txt.\\nit looks like the CloudXR service don’t import the CloudXRServiceOptions.txt to change its setting.\\nattach file it the log and screen capture which in my PC.\\ncan you help to check this problem?regards\\nCloudXR Server - SteamVR Log 2022-05-24 10.45.34.txt (2.1 KB)\\n\\n16533628945631406×940 88.4 KB\\n@nomo.hsu\\nOn the CloudXR server side Windows settings, can you confirm that the Microphone is not disabled in “Control Panel” → “Privacy”\\nThen on the lower left panel, select “Microphone”Please enable these settings:\\n“Allow access to the microphone on this device”\\nand\\n“Allow apps to access your microphone”That should enable CloudXR server to receive audio from your device.JeremyHi Jeremy\\nYes, I check the privacy of Microphone, all are enable for all device, but this issue still exist,\\nBTW, in my another PC the audio is work and I can see below message in the log, so I doubt maybe the issue PC can’t open the CloudXRServerOptions.txt to read parameters, do you have any idea?work log:\\n[13:05:10.450] Read server options file at C:\\\\Users\\\\XRSPACE\\\\AppData\\\\Local\\\\NVIDIA\\\\CloudXR/CloudXRServerOptions.txtCan’t similar log in fail log.\\nCloudXR Server - SteamVR Log 2022-05-20 13.05.10.txt (28.5 KB)Did you also enable the audio flag on the client as well using -sa?\\nhttps://docs.nvidia.com/cloudxr-sdk/usr_guide/cmd_line_options.html#client-command-line-optionsAlso – I sometimes accidently double append the .txt in Windows files.  Can you verify your file name is not CloudXRServerOptions.txt.txt?HI SIR\\nYES, I set the -sa parameter in CloudXRServerOptions.txt which local in User/“Username”/AppData/Local/NVIDIA/CloudXR folder, but the txt file don’t be read, I have no idea about it.regards\\nNomo Hsu‘failing’ log should have a similar line, but would say something like:\\nWARNING: Failed to parse options file at {path}Ensure that path is where you placed your file, and that the current user has access to that path (it should, otherwise we wouldn’t be able to write logs or anything).Hi Sir\\nthanks for your support, I fixed this issue, in my PC, the default folder parameter is hide the file extension but I am not award, so the option file I create is CloudXRServerOptions.txt.txt, double extension cause cloud server can’t recognizing.\\nregardsThanks for letting us know it works.Yes, the ability to hide file extensions has caused issues on the PC for decades!This topic was automatically closed 14 days after the last reply. New replies are no longer allowed.Powered by Discourse, best viewed with JavaScript enabled',\n",
       " 'will-blast-work-with-xbox-playstation-and-amd': 'Just starting to look at branch with blast plugin.Will it work with projects targeting consoles/ non-nvidia gpu PC too?Thanks for any feedbackYes, it will.kstorey, that’s great to know Blast is available for the PS4. The GitHub page says to contact NVidia to get the Blast libraries for the PS4 (or updated source code).  Where do I contact? (Yes, I do have a PS4 development account).Drop gameworkslicensing@nvidia.com an email and somebody should be able to help you.Thanks a lot, I have emailed gameworkslicensing@nvidia.com.Powered by Discourse, best viewed with JavaScript enabled',\n",
       " 'setting-gl-texture-max-level-to-0-fires-gl-invalid-operation': 'With an old driver version of Nvidia, whenever I try to setgl3.glTexParameteri(GL3.GL_TEXTURE_RECTANGLE, GL3.GL_TEXTURE_MAX_LEVEL, 0);I get the gl error GL_INVALID_OPERATION 1282…Looking at the man pageGL_INVALID_OPERATION is generated if the effective target is either GL_TEXTURE_2D_MULTISAMPLE or GL_TEXTURE_2D_MULTISAMPLE_ARRAY, and pname GL_TEXTURE_BASE_LEVEL is set to a value other than zero.GL_INVALID_OPERATION is generated by glTextureParameter if texture is not the name of an existing texture object.GL_INVALID_OPERATION is generated if the effective target is GL_TEXTURE_RECTANGLE and pname GL_TEXTURE_BASE_LEVEL is set to any value other than zero.I and II case have nothing to do since my target is GL_TEXTURE_RECTANGLE and I am using glTexParameteri instead of glTextureParameter.\\nIII case is interesting, but I do set the GL_TEXTURE_BASE_LEVEL to 0 just before, as following:gl3.glTexParameteri(GL3.GL_TEXTURE_RECTANGLE, GL3.GL_TEXTURE_BASE_LEVEL, 0);\\ngl3.glTexParameteri(GL3.GL_TEXTURE_RECTANGLE, GL3.GL_TEXTURE_MAX_LEVEL, 0);So neither in this case I should get the GL_INVALID_OPERATION, but I do…why?Powered by Discourse, best viewed with JavaScript enabled',\n",
       " 'image-accumulaiton-in-path-tracer-example': 'I’m having problems saving images from the pathTracer example that match the quality of the interactive window display after a few seconds. I’d like to save images after going through the render loop for like 5 seconds but without an interactive window. Just launching multiple sub frames and saving afterwards didn’t do the trick. Is there some kind of accumulation inside of the gl display? I guess this is an easy one, but i’d appreciate any help on where to look for this!Have you read through the optixPathTracer.cpp code?That offers a command line option --file | -f <filename>  File for image output and that sets the std::string outfile variable which is then used inside the main loop to select between interactive progressive accumulation with display to the window and single shot rendering with saving of the image data to the specified image.So if you want that saved image to be progressively refined the same way as the interactive display, you would need to replace the single launchSubframe() with a similar loop used inside the interactive display which updates the launch params with the proper subframe_index (untested code):Background Information:The accumulation happens only inside the ray generation program.\\nInside the extern void __raygen__rg() this line does the accumulation:\\naccum_color = lerp( accum_color_prev, accum_color, a );\\nand the two lines after that store the result into the float4 accum_buffer and as uchar4 into the frame_buffer:Now if you search the host code for these two buffers, you see that the float4 accum_buffer is a device buffer allocated with cudaMalloc() inside void initLaunchParams( PathTracerState& state ) and on each resize of the window in handleResize().The uchar4 frame_buffer which is used for display, is wrapped by the sutil::CUDAOutputBuffer<uchar4>& output_buffer instead. to make that work with and without OpenGL interop.The place where the uchar4 frame_buffer ( == output_buffer) is transferred into the OpenGL texture for display is inside the function displaySubframe() and there inside the  gl_display.display() function.Now if you look at what that does in sutil\\\\GLDisplay.cpp, the glTexImage2D() calls in there are transferring the image data to the OpenGL texture image.\\nSo you would need to work your way backwards from there to store the same image.This whole example is written with interactive OpenGL display in mind.\\nIf you want an offscreen process which does the same, you would need to replace basically the whole OpenGL part of that example application framework and only launch as many sub-frames as you like and and save the result into a image.Thanks for the help! I actually forgot to augment the subframe_index in the params… I knew it had to be something stupid i forgot.This topic was automatically closed 14 days after the last reply. New replies are no longer allowed.Powered by Discourse, best viewed with JavaScript enabled',\n",
       " 'vulkan-pendant-to-quadbuffered-stereo': 'Hi folks,I’ve been looking into stereo rendering using Vulkan, as I’m trying to run some tests on a Quadro M6000. The topic seems to be hardly touched so far, since I’m unable to find any samples or how-to’s online, also the Vulkan documentation is very sparse when it comes to the stereo. I posted on stackoverflow and got forwarded to Nvidia for a vendor-specific implementation of how to render to the buffers for left/right eye. I don’t know where to start and how to accomplish that, in OpenGL there was a pretty straightforward way by enabling GL_STEREO and then rendering to the LEFT/RIGHT variants of front or back buffers, but how would I do this using Vulkan? Maybe there are some samples I missed, that can get me started? Also, is there really no hardware agnostic way (maybe “yet”) to do this, would a solution really be specific to Nvidia drivers?Thanks!Hey iko79,unfortunately, there doesn’t seem to be something equivalent for Vulkan (yet), you can however work around the problem by using Nvidia’s NV_draw_vulkan_image extension (https://www.khronos.org/registry/OpenGL/extensions/NV/NV_draw_vulkan_image.txt) to draw a Vulkan-generated image into an existing OpenGL context. jherico has an example app how to use it at Vulkan/glinterop.cpp at cpp · jherico/Vulkan · GitHubExtending usage to quad-buffered stereo then is easy, the workflow is essentiallyThe semaphore there is important to make sure OpenGL and Vulkan render in-step.Powered by Discourse, best viewed with JavaScript enabled',\n",
       " 'gl-line-strip-crashes-on-quadro': 'Hi,I’m experiencing crashing drivers (311.35 on Quadro 5000, Win7/x64) when calling GL_LINE_STRIP.\\nI have a simple graph class which paints a signal. It works on Geforce-class cards, but on Quadro’s it crashes. If I change (pseudo-code):glBegin(GL_LINE_STRIP); for(i=0;i<10;i++)glVertex2f(x,y); glEnd();into:glBegin(GL_LINES); for(i=0;i<10;i++){ glVertex2f(x,y); glVertex2f(x,y); } glEnd();then everything is ok.Is anybody else experiencing this?RuudPowered by Discourse, best viewed with JavaScript enabled',\n",
       " 'pxpreprocessor-h1-error-size-of-array-pxcompiletimeassert-dummy-is-negative': 'This is with API version 3.2.0. Has anyone else seen this error on any system? I am seeing this on ubuntu 12.10. Including the configure.log if that helps.\\nThanks for any ideas,\\n-nuunHi,This compile time error is triggered when your compiler’s packing setup is incompatible with the SDK.\\nPlease try compiling with ‘-malign-double’ (See comment above the breaking line in PxPreprocessor.h)Powered by Discourse, best viewed with JavaScript enabled',\n",
       " 'installing-android-platform-tools-26-0-1-failed': 'I tried to fix the codeworks nvidia installation error. But I came across such a mistake and can not do anything about it. Google translator.I also would like to know about it. I hope someone will give reply updates.Powered by Discourse, best viewed with JavaScript enabled',\n",
       " 'when-will-the-nvidia-web-drivers-be-released-for-macos-mojave-10-14': 'Hello Nvidia Developers. Can someone please give me information or an update regarding the release of the Nvidia web driver for macOS Mojave 10.14? I am planing on upgrading 12 computers that have GTX 1080 GPU’s. I need this for my business. There are many features in Mojave that i would like to utilize. A time frame would be greatly appreciated.This is so important to my workflow and business. thanks in advance Developers!first Driver after a major release will take some time.they will have to do a lot of work to get a good codebase. i think there are a lot of changes with Metal etc.Some infos about the status will be very nicePlease tell us something.Thanks a lot.Adalberto from ItalyYes please. Also waiting for it!ThanksIm in a pretty similar situation as xtremepcgamer, an estimated release date would be appreciated.I’d love to find a way to get notified when the drivers are released. It seems kind of painful to have to perform a Google search every day to find out of the drivers have been released. Is there an official place to sign up to get notified when Mojave drivers are released?I’m on a Mac Pro 5.1 with a GTX 1080i card running the Adobe Creative Suite and a couple of other CUDA apps. On the side, I run Xplane, which I understand has a Metal development release in progress, but that’s not work related.Hello, I cannot provide a date for the driver release but I will notify this thread if and when they are available.Please stay tuned.\\nThanks for your patience,TomThanks Tom. We’ll be waiting!@ThomasK@Nvidia Thank you for your input, anything will help and i assure you your reply and updates are welcomed with my full gratitude. I am sure everyone on the post is happy you are going to update us. Thank you Thomas K.THANK YOU Nvidia for supporting Mac in general!.  Completely destroying Apple’s bureaucratic proprietary everything BS.  You guys are a great company by choosing not to beef and taking the higher road and releasing drivers regardless of the unfair stance taken by apple in recent years…  I do also greatly look forward to these drivers being released.  Hackintosh for 6+ years.  Been running Nvidia most of itI, too, am really happy to see Nvidia still release mac drivers.  I look forward to the Mojave release so I can run the Octane renderer.Thx!As far as developers are concerned, first Mojave beta was released in June this year, and that is not only to beta-test macOS itself, but to give plenty of time for third party developers to update their software. So, it has been more than 4 months now - it surely cannot be all that complicated for such a huge company as Nvidia is to make their drivers compatible with Metal2 APIs?Anyway, subscribing to this topic :)Please stay tuned for 6 or 9 weeks until I come again with another postThanks for your patiencePlease stay tuned for 6 or 9 weeks until I come again with another postThanks for your patienceMy ow my are you optimistic. :) Replace weeks with months and probably you will be closer.I am sure Nvidia did work during the Beta period. They just didn’t focus on the drivers for Mac as much.  Now there’s the churning long wait for them to release new drivers and when they will be ready Mac OS 15.0 will be around the corner.If you plan on using CUDA - go to Windows and PC, you’re on a safer path that way.I don’t use CUDA, but I do need eGPU for Video and Photo editing. As such, I went for the lower HW with AMD and it works OOB smoother than Nvidia ever worked (even though they have better HW) — low quality drivers impact a lot.I still have 2 more machines with Nvidia cards eGPUs and since I am having a good experience with AMD, I think I will be switching those as well and get rid of this embarrassing and I would dare say shameful way of delivery of tools for the green product.Expensive cards collect dust or run on previous generation OS for more than half year. Something is wrong with this picture or probably it’s just me…I feel the same.Even I should thank that Nvidia still develop Mac Os drivers, I also regret they take so long releasing them. As said above, beta program of Mac os is to give time to developers adjust drivers and software to be ready on releaee date.I’m also considering a swith to AMD on my next builds.Please stay tuned for 6 or 9 weeks until I come again with another postThanks for your patienceMy ow my are you optimistic. :) Replace weeks with months and probably you will be closer.I am sure Nvidia did work during the Beta period. They just didn’t focus on the drivers for Mac as much.  Now there’s the churning long wait for them to release new drivers and when they will be ready Mac OS 15.0 will be around the corner.If you plan on using CUDA - go to Windows and PC, you’re on a safer path that way.I don’t use CUDA, but I do need eGPU for Video and Photo editing. As such, I went for the lower HW with AMD and it works OOB smoother than Nvidia ever worked (even though they have better HW) —Just wondering how such a big company like Nvidia can’t do job faster with thier all engineersSomething is wrong ; 4 month is so long time and we should wait more\\nCuda is important with 3D modeling they should solve this issue as soon as possible with MojaveNote: Nvidia is working with Apple to get Mojave support.https://devtalk.nvidia.com/default/topic/1042279/?comment=5286813Following. My experience with NVIDIA has been excellent with my cMP and hope to continue it soon w/ Mojave…Some kind of time frame would be nice. We talking days, weeks or months?Powered by Discourse, best viewed with JavaScript enabled',\n",
       " 'find-programs-using-the-gpu-locking-set-mosaic-command': 'I am trying to create a program that uses the NVAPI interface to enable/disable warp.A problem i am running into when trying to set a new resoloution (with overlap) that some program locks the driver and prohibits changing the resoloution.The setmosaic commandline tool checks this, and throws an error with the programs locking the driver.Is there a way to do this check through the NVAPI?I found the call NvAPI_EnumAppStatistics, but i can’t find any documentation on this call.or is the sourcecode of the setmosaic.exe program available somewhere?Hi s.geusSlides 19 to 21 ( Slide 1 (gputechconf.com))  will give you some hints in terms of how the configuremosaic.exe app works.The NVAPI calls you I think you are looking for are NvAPI_QueryNonMigratableApps which will return a list of applications that are currently running on the GPU that need to be stopped prior to changing the mode on the driver.Unfortunately, that call is only part of the NDA version of NVAPI.  If you don’t have access to the NDA version you will need to contact your NVIDIA rep to get an NDA in place.:D:Powered by Discourse, best viewed with JavaScript enabled',\n",
       " 'cuda-vray-gpu-next-3-1': 'Hi i have a problem whith a new Vray Gpu Next 3.1.\\nWhen run rendering i have this error\\n\" checking cuda driver version installed driver version is newer than the latest verified one \"\\nhelp pleasePowered by Discourse, best viewed with JavaScript enabled',\n",
       " 'carrier-board-does-not-power-up': 'I’m a hardware engineer working in robotics and AI feilds.\\nI designed the customized carrier board(Nano) according to schematics of Nvidia (P3449–B01), their was minor changes in circuits in the customized carrier board, some connectors change their placement acording to our project constains, board Dim chaned too.All the voltages values and voltages sequennces are checked and are ok, the problem is we can’t power up the board, can’t power up the HDMI(so can’t connect the board to monitor and making a debuggibg for it), other main functions does not works to , like USB, wi, etherent… it seems that we missing something in our EEPROM configuration (which in our customized carrier board exatly like in NVIDIA schematics) , how can I configure the EEPROM in order to power up my board?? do I need external device for EEPROM configurations? if yes how can I make the debugging without powering up the HDMI for debugging?, please I need help in this issue in order to powering up my carrier board and test the functions(MIPI, USB, Etherent , wifi, HDMI, etc…)\\nmy mail :Nassim@nanovel.co.ilThanks for help.Powered by Discourse, best viewed with JavaScript enabled',\n",
       " 'build-optix-with-my-makefile': 'Hi,I want to build projects with Optix.\\nFor example, if I compile SDK sample1 without the original cmake,$ nvcc -arch=sm_20 -ptx -m64 draw_color.cu -o draw_color.ptx -I${DIR_OPTIX}/include\\n$ nvcc -arch=sm_20 -c sample3.c -o sample3.o -I. -I${DIR_OPTIX}/include\\n$ nvcc -arch=sm_20 -c sutil.c -o sutil.o -I. -I${DIR_OPTIX}/include\\n$ nvcc -arch=sm_20 sutil.o sample3.o -o sample3 -L${DIR_OPTIX}/lib64 -L/usr/local/cuda-6.0/lib64/ -lcudart -loptix\\n$ ./sample3\\n./sample3: error while loading shared libraries: liboptix.so.1: cannot open shared object file: No such file or directoryWhat can I do?on linux? Make sure your LD_LIBRARY_PATH  environment variable contains  the directory where liboptix.so.1 lives, probably ${DIR_OPTIX}/lib64on linux? Make sure your LD_LIBRARY_PATH  environment variable contains  the directory where liboptix.so.1 lives, probably ${DIR_OPTIX}/lib64on linux? Make sure your LD_LIBRARY_PATH  environment variable contains  the directory where liboptix.so.1 lives, probably ${DIR_OPTIX}/lib64Thanks a lot! It works now.\\nBy the way, I used CUDPP but forgot to change LD_LIBRARY_PATH. But that didn’t bring any error?Edit: just found CUDPP uses .aPowered by Discourse, best viewed with JavaScript enabled',\n",
       " 'can-we-rename-the-common-folder-name-which-is-included-with-videocodecsdk': 'I am trying to rename the “common” folder with my specific name, i have changed the directory path and able to compile but at run time cuModuleLoadDataEx() function failed and return error:-“CUDA_ERROR_INVALID_CONTEXT”.If i will change it with older one(common),the cuModuleLoadDataEx() function calls successfully,Pls help if anyone has knowledge about it.Which sample are you building? It should surely be possible to change the name. You just have to find all the occurrences of references to common. That’s why I ask which sample you are building.I am working on NvDecodeD3D11 sample that is given in video codec sdk, Already i have find all the occurrences to common and renamed it with my new folder name ,but at run time cuModuleLoadDataEx() function failed, i want to know there is something with “common” folder that is hardbound in NVDEC APIs.I changed my name to common2 and everything worked fine. You’ll have to use the debugger to find out where you went wrong. I had to change the VC directories include path, some C++ file properties, and all the hard-coded SDK search paths. It looks like you may have missed the path for the PTX file.Powered by Discourse, best viewed with JavaScript enabled',\n",
       " 'deploy-nvidia-opengl-rdp': 'Hello,\\nWe have several workstation labs with NVIDIA GeForce and Windows 19.09. We need to provide them with remote access by RDP to use applications such as Marvelous Designer, Mari, DJV, etc…\\nWe have found an application “nvidiaopenglrdp.exe” in this same forum, which gives us the option to enable OpenGl acceleration via RDP.\\nThe problem is that we have a considerable number of computers and we do not find how we can make the deployment of this software in a massive way.\\nWe have tried with MSI converters, or with command line, but we have not succeeded.\\nCan you help us?\\nThank you very much!The executable is displaying EULA on launch, and a message that has to be dismissed afterwards, so you would need a way to dismiss those two dialogs automatically to get anywhere.The best option would be if NVIDIA provided an updated tool with command line support to run it in silent mode with logging.Failing that, you could find a tool that can simulate user clicks such as AutoIT, or you could perform DLL redirection to a custom user32.dll which overrides MessageBoxW so it does nothing, and DialogBoxParamW so it will always return 1.Powered by Discourse, best viewed with JavaScript enabled',\n",
       " 'bug-report-gl-4-6-beta-driver-compiler-does-not-recognize-shader-group-vote-functions-without-suffi': 'In Windows driver 384.94, the GLSL compiler does not recognize function names allInvocations and anyInvocation, which are now core features of GLSL. allInvocationsARB and anyInvocationARB work as usual.As I realized only now, the newest driver (highest version number) is not the OpenGL 4.6 beta driver. Oh well …Powered by Discourse, best viewed with JavaScript enabled',\n",
       " '522-25-driver-black-screen-crash': 'Ever since i updated to this driver version, i get a crash every day. Reading reddit/other sources suggest this problem exists since 512.+. Is there a way to submit crash logs or info that this issue is known to nvidia?I was on 472.84 before the update and never had a crash since 3 days ago.I am having the same issue.Purchased a new Asus TUF 3090 Ti OC last week,  and after getting everything installed and updates working through windows 11, I get black screens of death after about 10 minutes in some selected games.For instance, Red Dead Redemption 2, Max Payne 3, Apex Legends have all given me the black screen. But I can play games like Team Fortress 2, Starcraft 2, Overwatch 2 with no issues for well over an hour as per my testing.At this point I am not sure if it’s a driver issue or a faulty card as I have gotten it so recently.Before the driver update, I was able to play RDR2 for over an hour without any hiccups, so there may be my answer, however I am also unsure of which driver was installed before I updated with GeForce Experience.Hello @astraar1990 and @rzo, welcome to the NVIDIA developer forums.If you see issues with certain drivers, I would highly recommend using the “Send Feedback” option to let the correct support channels within NVIDIA know about your issue. The advantage of this is that your system details as well as further debug information is automatically shared with our engineers.We sadly cannot provide individual installation support within these forums.As to community support by other users with similar issues I recommend checking out our dedicated GeForce forums. The developer forums here are mainly aimed at support for our professional SDKs and tools.I hope this helps.Thank you for your answer. I was searching for the old feedback forums and could not find it. This “send feedback” button seems to be only in gforce experience, which i never install. So maybe i should give it a try.Thanks anywaysPowered by Discourse, best viewed with JavaScript enabled',\n",
       " 'branch-divergence': 'As the number of times the rays get bounced around is different for different points in the scene, there must be significant gpu branch divergence, hence poor performance. Is there a study on this; what can be done about it?There are different sources of divergence, so it depends on the scene & renderer whether divergence will be a problem. One source of divergence is ray traversal, intersection, and any-hit shaders. When rays in a warp are traversing different depths of an acceleration structure, divergence happens. If the rays are going through parts of the scene with very different geometric density, divergence appears.Another source of divergence is shading materials in the scene. If each ray in a warp hits a surface that uses a unique material shader, the cost will be significantly higher than if all the rays hit the same material.Yes there are studies on this topic, and yes there are things you can do to improve situations that have significant divergence. For traversal, using the RTX hardware traversal and intersection for all of the scene geometry is a major way to both improve performance and cut down divergence. Avoiding any-hit programs and custom intersection programs when possible is another way to improve performance and potentially improve divergence. (It’s not always possible to avoid any-hit programs or custom intersectors, and I don’t want to stigmatize the use of valid necessary features, but any-hit and intersection programs do interrupt hardware traversal to execute your code on the SMs, and so there’s significant overhead.)For materials, some people try to use an “ubershader” - a single shader program that can handle many kinds of material properties. Others sometimes use a “wavefront” architecture that separates tracing work and shading work into separate passes, where the shading work can be sorted and scheduled in batches by material. A wavefront architecture also allows you consolidate ray tracing work at every step of path depth, so the kernels get smaller as you go, and warps stay compacted with active work on all threads.When programming for GPUs, simply porting a large CPU program into an equally large GPU kernel is generally not a good approach. Due to SIMT execution model on GPUs, divergence in control flow carries substantial performance penalties, as does high...–\\nDavid.Powered by Discourse, best viewed with JavaScript enabled',\n",
       " 'ffmpeg-and-nvenc-legal-question-for-use-in-commerical-product': 'Dear NVIDIA and Community,FFmpeg license lists NVENC as not being compatible with the GPL license:\"The NVENC library, while its header file is licensed under the compatible MIT license, requires a proprietary binary blob at run time, and is deemed to be incompatible with the GPL. \"What do I need to do as a software developer of a commercial product in order to use the NVENC via FFmpeg in commandline tool fashion?So to be clear, no source code of FFmpeg or NVENC is being integrated or modified, my software does not dynamically link or use code of either FFmpeg or NVENC SDK.I am simply calling FFmpeg as a commandline tool with the NVENC codec like so:ffmpeg -i output.mp4 -c:v h264_nvenc -profile:v high -preset slow -b:v 12000k compressed.mp4Thanks in advance for any info on this!That text is out of date. ffmpeg can be compiled with nvenc support without --enable-nonfree.Thank you for letting me know @philipl !Powered by Discourse, best viewed with JavaScript enabled',\n",
       " 'dx11-debug-layer-fails-with-corrupted-multithreading-when-encoding-using-asynchronous-method': 'Following the guidelines in the documentation, and due to wanting to get the absolute minimum latency out of the system as possible, I moved to asynchronous mode on Windows 10.When encoding and then waiting for the completion event and locking output buffers on the same thread, my code works fine.When I move the code which waits on the completion event and then calls nvEncLockBitstream and nvEncUnlockBitstream, I was getting unpredictable crashes in nvwgf2umx.dll. I turned on the debug runtime, and the following message was shown:This usually occurs while within the call to nvencLockBitstream, and other times within the call to nvEncUnlockBitstream. This occurs on the secondary non-render thread, as per the documentation. I’ve verified that the main thread is not attempting to access the input resource that was being processed by the encoder.The callstack of the call into nvencLockBitstream is as follows:A few years ago, a similar thread was brought up on this forum, and the solution was to move back to a single thread. DirectX 11 and asynchronous encoding I wonder if asynchronous receipt of the encoded packets is actually not supported? Do we have to use CUDA perhaps? It’s really not clear, and there don’t seem to be any examples of this approach.I reworked my application to convert all DX11 objects into CUDA objects for consumption by the NVENC library.After this work, the stability issues went away.It seems as if DX11 encoder devices do not work when attempting to read the locked bitstream on another thread.Powered by Discourse, best viewed with JavaScript enabled',\n",
       " 'nvencdxgioutputduplicationsample-using-yuv420-instead-of-nv12-causes-error': 'I have the nvEncDXGIOutputDuplicationSample sample from NVIDIA’s GitHub. What I want to do seems like it should be relatively simple. I want to change the color conversion to convert to YUV420/I420 instead of NV12.I changed this line from NV_ENC_BUFFER_FORMAT_NV12 to NV_ENC_BUFFER_FORMAT_IYUV, and then addedto this switch blockUnfortunately, when I try and run the code now, I get Error 0x80070057 E_INVALIDARG in CreateVideoProcessorOutputView at this linegithub.\\ncom/NVIDIA/video-sdk-samples/blob/master/nvEncDXGIOutputDuplicationSample/Preproc.cpp#L141Any ideas why? The modifications I made were so small so I can’t imagine why it would be causing a problem.For D3D12 video, RTX 3070 only supports encoding NV12 format.\\nIt’s probably just not supported on hardware level to use DXGI_FORMAT_420_OPAQUE directly, you must convert your textures to NV12 manually.sorry, I want to change the color conversion to convert to YUV 444 instead of NV12.I also  face the same problem at T4 card。can you help meDXGI_FORMAT GetD3D11Format(NV_ENC_BUFFER_FORMAT eBufferFormat)\\n{\\nswitch (eBufferFormat)\\n{\\ncase NV_ENC_BUFFER_FORMAT_NV12:\\nreturn DXGI_FORMAT_NV12;\\ncase NV_ENC_BUFFER_FORMAT_ARGB:\\nreturn DXGI_FORMAT_B8G8R8A8_UNORM;\\ncase NV_ENC_BUFFER_FORMAT_YUV444:\\nreturn DXGI_FORMAT_AYUV;\\ndefault:\\nreturn DXGI_FORMAT_UNKNOWN;\\n}\\n}\\t\\nHRESULT InitEnc()\\n{\\nif (!pEnc)\\n{\\nDWORD w = bNoVPBlt ? pDDAWrapper->getWidth() : encWidth;\\nDWORD h = bNoVPBlt ? pDDAWrapper->getHeight() : encHeight;\\nNV_ENC_BUFFER_FORMAT fmt;\\nfmt = bNoVPBlt ? NV_ENC_BUFFER_FORMAT_ARGB : NV_ENC_BUFFER_FORMAT_YUV444;Unfortunately, when I try and run the code now, I get Error 0x80070057 E_INVALIDARG in CreateVideoProcessorOutputView at this linegithub.\\ncom/NVIDIA/video-sdk-samples/blob/master/nvEncDXGIOutputDuplicationSample/Preproc.cpp#L141Any ideas why? The modifications I made were so small so I can’t imagine why it would be causing a problem.T4 can support YUV444 h264,but I can’t test it okHi fengliang191,YUV444 is a completely different color format, it has no “compression” as compared NV12/YUV420, so I would be surprised if this would work out of the box.Regarding your error message please try to debug the issue in your development environment first to see which argument is invalid and if it has anything to do with the color formats.T4 has YUV444 encode capabilities, yes, that is correct.Beyond that I am afraid I am unable to help here.Powered by Discourse, best viewed with JavaScript enabled',\n",
       " 'vertex-light-baking': 'Hi, in this slides I can read “Vertex Light Baking: Available publicly. Just ask us”So, this is my request: Where can I download infos? :-)Thx+1Powered by Discourse, best viewed with JavaScript enabled',\n",
       " 'optix-exception-code-traversal-invalid-hit-sbt-root-cause': 'Hey,\\nin my OptiX7.6-based pathtracer (using the OptiX Apps architecture)  I again got validation error OPTIX_EXCEPTION_CODE_TRAVERSAL_INVALID_HIT_SBT, but this time only on some materials, while most opaque and MDL materials work fine. Tested with separate settings on different launches.\\nTo me it seems to be a problem somewhere in a volume material, but when I remove another material, the volume material at least does not cause the validation error.\\nI tried to increase MaxTraversalDepth (from 3 to 5) and pipeline_link_options.maxTraceDepth (from 3 to 5) without any success. In all cases I use 3 ray types: radiance, occlusion and scattering\\nThe object uses in-built triangles, no motion-blur, no re-fitting, no cutout.From the API Reference I found optixGetExceptionInvalidSbtOffset() and added it into the exception program but no output from there shows up.  There is only one global “Exception program record”. Or do I need to add another exception program for the hit groups? I found no example wihtin the SDK.I’ve seen SBT problem when using multiple GAS objects. - #2 by droettger and I use the index as described there:while for test I even use pg_null for scattering. pg_null is built using optixProgramGroupCreate with all parameters zero.In another (closed) case I got this validation error, when a pipeline setting related to curves was invalid. In the current test there are no curves; All objects are out of inbuilt-triangles.My System:\\nOptiX 7.6.0 SDK\\nCUDA 11.8\\nGTX 1050 2GB\\nWin10PRO 64bit (version 22H2; build 19045.2846)\\n8GB RAM\\ndevice driver: 531.79\\nVS2019 v16.11.26\\nMDL SDK 2020.1.2\\nWindows SDK 10.0.19041.0Hard to tell what is going on from that description.When you say you got a validation error, do you mean you enabled OptiX ’ validation mode https://github.com/NVIDIA/OptiX_Apps/blob/master/apps/MDL_renderer/src/Device.cpp#L286The validation error did not report any additional information which SBT index it complained about?What is the maximum traversal depth and maximum trace depth actually needed inside your scene?\\nAlways use the minimum values for both to reduce the required stack size.Did you calculate the stack size explicitly? (That is obligatory when using callables!)There is only one exception record per pipeline.\\nAre you saying you do not catch that exception inside your exception program?\\nIf not, what happens with validation mode disabled?If the problem moves or disappears when changing the number of materials strip down the scene to the smallest reproducing content.Check all values inside the OptiX instances and Shader Binding Table for correctness (sbtOffset, SBT record headers, number of entries inside all SBT records, their stride, their alignment).Try isolating which optixTrace call (which ray type) is responsible for the invalid SBT index error.\\nMy OptiX applications are not using three ray types, so you’re actually implementing a different device program architecture which might be the culprit if you’re having issues with volume materials.Check all optixTrace calls for correct arguments.\\nUse the formula in chapter 7.3 of the OptiX Programming Guide to check the effective SBT index by printing out all values from the instance and optixTrace call which affect the final SBT index, calculate that yourself, and print it before all optixTrace calls. Check if any of them is out of bounds.Try newer display drivers.Try newer OptiX SDK versions.Thank you for your answer.I think I found the problem, but some issues still remain.yes, I enabled OptiX validation mode:\\nOptixDeviceContextOptions.validationMode = OPTIX_DEVICE_CONTEXT_VALIDATION_MODE_ALL;no sbt index reported, here the original message:\\n[ 2][       ERROR]: Validation mode caught builtin exception OPTIX_EXCEPTION_CODE_TRAVERSAL_INVALID_HIT_SBT\\nError recording resource event on user stream (CUDA error string: unspecified launch failure, CUDA error code: 719)printf correctly compiled into the PTX, but when launching this simple program, no output occurs:(Also no Validation Exception, although validation mode is ON)when running without validation mode rendering works without crash;the volume objects did not show up correctly; but that is solved now:\\nvolume_scattering20232446×578 389 KBobivously there was a  “local IAS” setup, which is needed for motion blur and for volume scattering:Generally  index * ray_count works for the other cases, but the local IAS is one level under the IAS.\\nI’ve set it to zero now. (optix_instances.sbtOffset = 0;) Now the validation error is gone.The “local IAS” function worked properly up to driver 512.15 OptiX 7.4 giving the correct output (latest screenshot from May 2022), although it had that invalid SBT index.\\nBut when I now run the exectuables from that time, the old version on the newer driver, it does not render anything at all anymore.However, the local IAS is used to start traverval on multi-scattering volumes only against the volume object itself (not the whole scene).I have set maxTraversalDepth = 3;\\nto allow IAS+MT+GASes\\nor IAS+IAS+GASes (volumes)\\nor  IAS+GASes (no motion blur)For MDL materials the validation error OPTIX_EXCEPTION_CODE_CALLABLE_PARAMETER_MISMATCH occured then, but also solved nowMorphing again also works fine for MDL material objects.source code for the function deleted in this post now; since it seems to be solvedprintf correctly compiled into the PTX, but when launching this simple program, no output occurs:That’s a defect inside the R530 drivers which is fixed in R535 drivers.\\nPlease read this thread and the link to the more general debugging topic in there.\\nhttps://forums.developer.nvidia.com/t/printf-not-working-in-optix-kernel/246586obivously there was a “local IAS” setup, which is needed for motion blur and for volume scattering:Generally index * ray_count works for the other cases, but the local IAS is one level under the IAS.\\nI’ve set it to zero now. (optix_instances.sbtOffset = 0;) Now the validation error is gone.So you’re saying that you’re using multi-level acceleration structures.Please study the instance and SBT setup described inside the OptiX Programming Guide chapter 7.3 covering different cases and esp. 7.3.5 Example SBT for a scene\\nThat explains which sbtOffset needs to be used for the instances.\\nThis also shows GAS with more than one build-input and more than one SBT record for one of the build inputs.\\nThat the instance1 in that example starts at sbtOffset 6 is what I meant with prefix sum in your previous thread about GAS memory consumption.Mind that the instance offset of the bottom-most instance is affecting the final SBT index selecting the hit record. If you used sbtOffset = index * ray_count; for the local IAS and index is the number of instances in your top-level IAS, and your SBT only contains as many hit records (times ray types) as there are top-level instances (which wasn’t shown inside your code excerpts), then that is obviously accessing the SBT out of bounds.I’m not using an SBT hit record per instance in my later OptiX 7 examples (rtigo10 and MDL_renderer) anymore to keep the SBT small. Instead I use one SBT hit record per material shader in rtigo10 and and only five hit records inside the MDL_renderer, four variants from the product of (no emission, emission) x (no cutout, cutout)closesthit and anyhit programs and one for cubic B-spline curves. (The no-emission ones are for performance optimization.)\\nAll additional data is accessed via the user-defined instanceId field. That indexes into an array of GeometryInstanceData structures which define the necessary IDs for the material, light, and object, and the device pointers to the vertex attributes and primitive indices.\\nMeans the instance sbtOffset only select the hit record, the instanceId picks the material and light callables and their parameters and the geometry information used inside the hit programs.\\nhttps://github.com/NVIDIA/OptiX_Apps/blob/master/apps/MDL_renderer/src/Device.cpp#L1589\\nThe optixTrace arguments switch just the ray type, as usual.Thank you very much for your answer.\\nI’ve seen the threads related to printf before, but I thought that issue was only related to CUDA 12, not to the driver. OK, now I know.I have always one build-input in any IAS and so this prefix sum for one instance is the number of ray types:  (index * ray_count). Since there is no second “local” instance, the sbtOffset is 0 for the “local IAS”. I simply somewhow missed, that the “local IAS” is on a lower hierarchy level.\\nThanks again for the clarification.Your “keeping the SBT small” architecture approach is brilliant, when it comes to SBT clarity and obviously also speed advantages when rebuilding such smaller SBT’s.\\nFor calculating motion differences I have already a global SubsetDefinition struct buffer, but only for face indices and a current/previous vertex buffer, I’m simply for now adding the material id buffer into it, then I have something similar to your “GeometryInstanceData” array. Yet I still re-use an subset id passed through the sbt-record, changing that needs more refactoring.\\nI didn’t know, that the instanceId field is user-defined. I thought that must exactly fit the index value within the SBT.\\nCurrently for now I will keep the implementation as is, since its working ;)\\nOnly adding a face-based (instead of subset-based) material id; But I think I’ll also try to apply your new architecture in my renderer in the future.Right, there are three useful things inside the OptixInstance which can be used for different tricks.\\nThe sbtOffset affects the SBT hit record used via that SBT index formula directly, the instanceId can be anything you need, and there is also the zero-based instance index which comes from the order in which the instances have been put into its IAS.\\nCheck all the optixGetInstance* device functions, here optixGetInstanceId and optixGetInstanceIndex.in the OptiX SDK curve.h functions (for example optixGetCatmullRomVertexData()) is shown to take a “gasSbtIndex” parameter,\\nobtained from optixGetSbtGASIndex()\\nwhen I now would only put one curve material into the SBT, which then would be used by more than one instance in the IAS, how would I get the associated vertex data, if using your archtitecture with the instanceId?\\nSo for curves it seems all instances need to be still present within the SBT, right?So for curves it seems all instances need to be still present within the SBT, right?No, not in my MDL_renderer architecture.when I now would only put one curve material into the SBT, which then would be used by more than one instance in the IAS, how would I get the associated vertex data, if using your archtitecture with the instanceId ?First, I’m not using OPTIX_BUILD_FLAG_ALLOW_RANDOM_VERTEX_ACCESS because that makes the acceleration structure bigger and slower, which means I cannot call any of the functions fetching vertex data from the GAS (optixGetTriangleVertexData, optixGetLinearCurveVertexData, optixGetQuadraticBSplineVertexData, optixGetCubicBSplineVertexData, optixGetCatmullRomVertexData, optixGetSphereData).Instead I store all vertex attributes and indices into device memory (where it’s needed for the AS build anyway) and access it via the instanceId by indexing into an array of a small custom GeometryInstanceData structure as described above with the link to its definition:The ids in that struct allow assigning different materials to different instances of the same GAS. In that case the attributes and indices pointers of that GAS are reused, so there is only one GAS built for instanced geometries and that’s assigned to all instances’ traversableHandle field using that GAS, as usual.These GeometryInstanceData structures are accessed inside my closest hit programs like this:\\nGeometryInstanceData theData = sysData.geometryInstanceData[optixGetInstanceId()];\\nand because triangles and curves use different closesthit programs, they know how to interpret the data behind the generic attributes and indices pointers accordingly, for triangles:\\nhttps://github.com/NVIDIA/OptiX_Apps/blob/master/apps/MDL_renderer/shaders/hit.cu#L100\\nand for cubic B-spline curves:\\nhttps://github.com/NVIDIA/OptiX_Apps/blob/master/apps/MDL_renderer/shaders/hit.cu#L1307That way materials and geometries are decoupled and the SBT only needs to store hit records per material shader.If you look at the OptiX SDK examples, you also need to follow where the optixGetCatmullRomVertexData function is used there and that shows that it calls optixGetSbtGASIndex to get the gasSbtIndex inside functions called by the closesthit programs and that should always return the GAS index of the currently intersected primitive. That is usually zero when there is only one build inputThen if you read the chapter about the differences between curves, spheres, and triangles you’ll find that curves cannot have SBT index buffer, means no different SBT records per primitive, so only curve GAS with more than one build input can have gasSbtIndex values greater than 0.\\nThat doesn’t happen in my MDL_renderer implementation. Is that a use case in your renderer implementation?I don’t use instancing at all, cause bone+shapes animation would be much slower, when not pre-calculated. And normally I never needed instancing of an exact identical mesh.\\nSo for me I go with:\\nconst unsigned int instanceId = valid_subset ? subset_id : gi.getLight_ID();\\nwhich simply reduces the complexity to an object subset array and a light definition array. (emissive objects are simply subsets, which only are seen as subsets (e.g. triangle-objects) during intersection and seen as lights for light sampling). Only subsets have materials, which are indexed from the subset data itself, saving the need of another buffer.\\nHowever, I think I could use instancing of the same GAS even this way, cause it then would simply have another entry in the IAS, with a different transform an a different instanceId linking to a “virtual subset” redirecting to an exisiting subset for geometry data.\\nCurrently I got the light sources which use sphere/ellipsoid/box/cylinder geometry working with the new instance id referencing.So when sharing the same material in one SBT entry also works for a curve type (I normally only use catmull rom), in the future, I’ll change the vertex data fetching for curves to your way of doing it.only curve GAS with more than one build input can have gasSbtIndex values greater than 0.\\nThat doesn’t happen in my MDL_renderer implementation. Is that a use case in your renderer implementation?No, I always only use one build input anywhere. So I can use gasSbtIndex == 0 there always avoiding the call to  optixGetSbtGASIndex. as long as I still yet use optixGetCatmullRomVertexData  Thank you for pointing that out!After implementing your idea for curves, from my older post I recognized @dhart’s answer, where he pointed out, that keeping that vertex buffer causes the data to be twice in memory. So using the vertex functions have a real advantage related to the memory usage.\\nEspecially when realizing that the thickness data is stored in a separate buffer, which also needs to be present, keeping all that buffers additionally is costly.\\nHowever, when using temporal denoiser I still need both buffers to calculate the difference between current curve hitpoint and previous hitpoint; but in case the temporal denoiser is not used, calling the vertex functions saves that memory.Note you can still use optixGetCubicBSplineVertexData() for the current frame’s data (i.e. the data used to build your BVH), and only refer to your own vertex buffer for the previous frame’s data, if you want. The advantage of doing that is a memory savings, at the cost of having to access the data two different ways. By memory savings, I’m referring to whether you choose to delete your current frame’s vertex buffer after building the BVH, and then in that case you will need to use the OptiX vertex data function.So the only difference would be the OPTIX_BUILD_FLAG_ALLOW_RANDOM_VERTEX_ACCESS  flag, about you saidI’m not using OPTIX_BUILD_FLAG_ALLOW_RANDOM_VERTEX_ACCESS because that makes the acceleration structure bigger and slowerIs there a general assumption how much bigger they are? Would that about compensate the vertex / thickness / index data?not using the vertex functions give me a slightly better speed, but in my test (no temporal denoiser) I get exactly the same memory sizes:not using OPTIX_BUILD_FLAG_ALLOW_RANDOM_VERTEX_ACCESS flag:\\n(using vertex / index / thickness buffers directly for calculating normals)using OPTIX_BUILD_FLAG_ALLOW_RANDOM_VERTEX_ACCESS flag:\\n(using optix vertex functions for calculating normals).when using the temporal denoiser (incl flow vector calculation for cuves, albedo and normals):frame times may be compromised by other applications.I did these tests several times and noticed, that in some early tests I had a buildflag mismatch: OPTIX_BUILD_FLAG_ALLOW_RANDOM_VERTEX_ACCESS was not set in the OptixBuiltinISOptions.buildFlags, but is was set in OptixAccelBuildOptions.buildFlags; no valdiation error occured, no different output.all still on driver 531.79since the temporal denoiser will be in use nearly always, I simply use the direct access without the vertex functions.Is there a general assumption how much bigger they are? Would that about compensate the vertex / thickness / index data?I have no measurements about how that affects curve primitives. It’s interesting that adding the OPTIX_BUILD_FLAG_ALLOW_RANDOM_VERTEX_ACCESS doesn’t change the compacted curve GAS size on your system configuration.That was more a comment about built-in triangles where that is also affected by the underlying hardware. RTX GPUs will show differences for those. Your Pascal GPU will behave differently for built-in triangles.I’m also especially not using OPTIX_BUILD_FLAG_ALLOW_RANDOM_VERTEX_ACCESS and OPTIX_BUILD_FLAG_PREFER_FAST_TRACE flags in my multi-GPU examples when sharing GAS across NVLINK bridges because the GAS size affects the performance more then.Note that the geometry acceleration structure size is mainly affected by the different AS optimization settings.\\nThat also includes curve primitives: https://raytracing-docs.nvidia.com/optix7/guide/index.html#curves#splitting-curve-segments\\nYou’re using OPTIX_BUILD_FLAG_PREFER_FAST_TRACE which usually results in bigger AS sizes.The same advice for AS comparisons as in this post apply for each different GPU architecture:\\nhttps://forums.developer.nvidia.com/t/build-time-and-bvh-size-for-different-primitive-types/251639/7I have finally fully implemented your new architecture using only one material entry per actually used material  in the SBT and then indexing into them from IAS entries.\\nI removed OPTIX_BUILD_FLAG_ALLOW_RANDOM_VERTEX_ACCESS to have speed improvements for built-in triangles.\\nAll works nowThank you very much!This topic was automatically closed 14 days after the last reply. New replies are no longer allowed.Powered by Discourse, best viewed with JavaScript enabled',\n",
       " 'catmull-rom-normals': 'in the optixHair sample (OptiX 7.4) there is no normal calculaton shown for catmull rom,\\nso instead of “optixGetCubicBSplineVertexData” I found “optixGetCatmullRomVertexData”, but\\nhow to define CatmullRomSegment  interpolator( controlPoints )  ?thank you.Analogously to the Cubic normal computation (normalCubic() in optixHair.cu), you’d initialize the CubicInterpolator using a linear transformation to convert the Catrom control points into those for Cubic interpolation. Add the following to the  struct CubicInterpolator in curve.h:And for the actual normal computation addin optixHair.cu.In other words, you’re reusing the existing cubic interpolator and transform the Catmul-Rom control points into the form the interpolator needs using a matrix multiplication the same way it’s done for the cubic b-spline control points.I tested the other matrix shown in this post:\\nThat one works better for me.Ah, right. Good you found David’s post. Those are the right weights for the current SDK. The code I showed is from the upcoming release in which we’re also changing the CubicInterpolator to use plain polynomial evaluation. Sorry for the confusion.This topic was automatically closed 14 days after the last reply. New replies are no longer allowed.Powered by Discourse, best viewed with JavaScript enabled',\n",
       " 'calculating-mipmap-lod': 'In a piece of shading code I wish to know the mipmap level being sampled from a texture.\\nAt my disposal I have a dPdx and dPdy. To determine the mipmap level I could sample from a texture with a unique colour at each mipmap level.\\nHowever, this would come at the cost of another texture sample and I’d like to know if a manual calculation would be possible and faster.\\nIs there any documentation on how rtTex2DGrad determines the final mipmap level sampled?I’m working with OptiX 3.9.1 and CUDA 7.5.Suppose you did find a way to predict the mip level.  Would you use that information to do your own custom texture filtering or for some other purpose?  Are you perhaps trying to load textures dynamically?  I’m curious about the high level goal, if you don’t mind sharing.  Maybe there’s another way to accomplish it.  Feel free to email one of us or optix-help.Powered by Discourse, best viewed with JavaScript enabled',\n",
       " 'rendertargetarrayindex-in-d3d12-view-instance-location-mismatched-with-effective-sv-rendertargetarrayindex': 'Hello, I met some trouble while working with view instancing in d3d12.Create PSO with D3D12_VIEW_INSTANCING_DESC, such as belowand set render target array that has 4 slices.Each slice has rendered, but the order is not correct.I’ve checked if a variable of SV_ViewID is equal to another variable of SV_RenderTargetArrayIndex, and those are not equal some cases.If ViewInstanceCount is 3 and Locations are {0, 0}, {0, 1}, and {0, 2}, then SV_RenderTargetArrayIndex is each given as 0, 2, 1.Also I tried with both SV_ViewportArrayIndex and SV_RenderTargetArrayIndex,The result of SV_RenderTargetArrayIndex is still wiered, but the SV_ViewportArrayIndex is equal with SV_ViewID as I set.So I set the RenderTargetArrayIndex such as below, and finally get an increasing order of RenderTargetArrayIndex.However, can’t find proper description why I should code such that.Note that.\\nWhen I create PSO with fixed RenderTargetArrayIndex=0 for each pViewInstanceLocations[0~3] to set SV_RenderTargetArrayIndex dynamically in pre-rasterizer shaders, it works well as I want.Powered by Discourse, best viewed with JavaScript enabled',\n",
       " 'custom-resolution': 'I am developing a custom app for an installation project which will displayed on a massive wall made of multiple screens, the screen will act as one display, the resolution is 8320x2160 which is higher than the current top card 1080Ti tops out at 7680x4320, but i know that people have run these cards to drive three 4k screens, and i am wondering if i can get some advise on the best approach to this problem?Powered by Discourse, best viewed with JavaScript enabled',\n",
       " 'nvidia-tegra-2-t20-how-to-use-all-memory-need-help': 'Hello. I have Dell Streak 7 with Tegra 2 T20. In this platform uses 512Mb RAM and 16Gb FLASH.\\nI resolder RAM and FLASH chips and now inside 1Gb RAM, 64Gb FLASH. FLASH visilble fully, no problem with read/write. RAM visible only 512Mb(New chip fully compatable with old). I tried to use other versions of OS, RAM not changed. I read many material on this theme. I think that it blocked on kernel(or bootloader) level. In spec of Tegra 2 support up to 1 Gb. May be anyone can help me? May be need to update bootloader?  If need more info I can give it.\\nThanks for any help.I just know some moments.\\nodmdata - in my tablet 0x8b0c0011. For use 1 Gb RAM it need to change to 0xbb0c0011. If I want to 256VRAM it need to change to 0xbc0c0011. But else need to change bootloader.bin and Steak.bct. May be anybody can help with it? I can’t trust that no body know how it is?Hello again. I read many docs about kernel, boot config table and other. But I’m not programmer. I think that need to edit *.bct file and/or kernel. Please give me the direction what I must to edit. Which file to change?Powered by Discourse, best viewed with JavaScript enabled',\n",
       " 'noisy-behavior-after-camera-eye-and-vfov-adjustments': 'I’m using Optix-SDK-5.1.1-linux64.When looking at the optixTutorial demo, I noticed that if I moved the camera_eye back and changed the vfov so that the field of view should be roughly the same, the further away I got, the noisier the image was.Is there some kind of global world extents that I’m violating? Or perhaps something else I may be missing?My example takes tutorial3 and compares against the demo as is vs a camera_eye that is:\\n2x, 5x, 10x, 50x, 100x, 1,000x, and 10,000x times further away from the look_at position.The noise becomes visible at about 50x, and it really bad at 10,000x.\\n\\ncamera_eye_behavior.jpg1352×1000 309 KB\\n\\ncamera_eye_behavior.jpg1352×1000 312 KB\\nThis is outside my area of expertise. However, based on what you describe about the effect and my general knowledge of fixed-point and floating-point computation for graphical environments, this would appear to be a consequence of limited granularity in the underlying data representation. Accumulated error (e.g. from rounding or truncation) turns into visible noise when one reaches the precision limits of the underlying data formats.You could confirm or refute this hypothesis by switching the underlying data representation to double precision, use of which would make the artifacts disappear if my hunch is correct (they would re-appear when even more extreme viewpoints are used, however). No idea whether Optix offers such an option.I may have run into z-fighting. The final images look better when I switch to a scene_epsilon of 1e-1 instead of 1e-4.[url]https://devtalk.nvidia.com/default/topic/913414/optix/how-can-optix-solve-the-problem-of-z-fighting-/[/url]You may wish to ask optix questions on the optix forum.What Robert Crovella said.I was not aware that Optix, or raytracing in general, makes use of classical z-buffering. Z-fighting is just one example of what happens when one reaches the precision limits of a given data representation.The underlying numerical issue is typically that when adding a small delta to a number large in magnitude, the result either shows no change, or the change appears somewhat random because this delta change is approximately equal to the numeric format’s resolution (i.e. is at the rounding limit). The banding in your picture is indicative of such granularity/rounding effects. Using a larger scene_epsilon presumably results in bigger delta changes (away from the resolution limit).Thanks for looking into it. My apologies for posting in the wrong forum.Moved to OptiX forum.\\nThis is not really z-fighting from coplanar faces. This is shadow acne from self-intersections with the primitive the shadow ray started from.\\nFind more details in the thread you linked to above.Powered by Discourse, best viewed with JavaScript enabled',\n",
       " 'removechild-cause-a-memory-viollation-if-i-dont-call-launch-every-frame': 'Hello.I have a problem when work with D3D11 interop in that situation:Rendering on D3D only (dont call optix launch)Remember, I dont call optix launch, only D3D calls, but execute all operations in OptiX structs, Add geometry, material, accel etc. In theory all structs and graphs of OptiX are OK.When I change the rendering mode to OptiX, the optix launch are invoked and cause memory violation.But, if I execute exactly same proccess calling the launch everyframe all works fine. Why?I have a lot of time about this and dont have success.ThanksI found the problem.In this scenario where I add a geometry and soon remove, without calling the launch between the operations. The optix should try to build the graphs using Geometry which was removed in the next operation. Basically the sequence of operations Add> Remove-> Launch does not work. With the sequence Add-> Launch-> Remove-> Launch everything works fine.Any idea how to solve this problem without this call between Add-> Remove?My primary idea would accumulate operations when I’m in D3D mode and validate / perform before calling the launch.Done.Work’s perfectly now. Now a can change between tech targets with no problems.Powered by Discourse, best viewed with JavaScript enabled',\n",
       " 'how-to-inquire-about-the-current-frame-rate': 'Hi guys,How can I know the current frame rate while the game is running (I’m using Physx in UE4).\\nI want this value to calculate the current frame or next frame velocities (displacements rather, to be accurate). So if there some function that calculates those then it will be perfect as well.Thanks for reading.Powered by Discourse, best viewed with JavaScript enabled',\n",
       " 'different-results-for-rtx-and-non-rtx-hardware': 'I have a very simple case that shoots just one ray at a single triangle.  The results vary ever so slightly depending on the hardware compute capability where the application is run.  Both machines have identical Optix 7.0 libraries installed and are running the same executable with compilation limited to just compute_60,sm_60.  Here are the results in which the hardware type, 3 triangle vertices, then launch and hit info are printed.  There is a slight difference in the hit distance, t:GeForce RTX 2080 Ti, compute capability = 75\\nv0   1.99999988e-01 -1.99999988e-01  1.99999988e-01\\nv1   0.00000000e+00  0.00000000e+00  1.99999988e-01\\nv2   0.00000000e+00  0.00000000e+00 -1.99999988e-01\\norigin:  2.09999993e-01 -9.99999940e-02  9.99999940e-02\\ndir: -1.00000000e+00  0.00000000e+00  0.00000000e+00\\nt =  1.09999992e-01  <---- DIFFERENCE HERE\\nu =  2.49999985e-01\\nv =  2.49999985e-01GeForce GTX 1060, compute capability = 61\\nv0   1.99999988e-01 -1.99999988e-01  1.99999988e-01\\nv1   0.00000000e+00  0.00000000e+00  1.99999988e-01\\nv2   0.00000000e+00  0.00000000e+00 -1.99999988e-01\\norigin:  2.09999993e-01 -9.99999940e-02  9.99999940e-02\\ndir: -1.00000000e+00  0.00000000e+00  0.00000000e+00\\nt =  1.09999999e-01 <---- DIFFERENCE HERE\\nu =  2.49999985e-01\\nv =  2.49999985e-01When I shoot a denser set of rays, then there can be slight differences observed in the resulting  t, u, and/or v values – or no differences at all.  It seems as if there is a variation in the least significant bit of the result.By any chance, is there a way to make these ray trace results identical?  Thanks.Hi there,The main difference here is the use of RTX hardware, not really the compute capability. The triangle intersection on the 2080Ti is done in hardware, and on the GTX 1060 it’s software. This means the OptiX built-in triangle intersection can never be executing identical instructions between these 2 GPUs, and there is no way to request identical execution unless you want to avoid using the built-in OptiX primitives. Due to the differences between software and hardware implementation, it is expected that order of operations, intermediate results, and rounding will be affected, so a 1 ULP discrepancy in the result is not surprising.If you wish to prioritize matching results between RTX and non-RTX GPUs over performance, the way to guarantee matching results is to avoid the hardware triangle intersector by writing your own triangle intersection program. This way you will have complete control over the execution on any GPU. But be aware that you will sacrifice a lot of performance compared to the RTX hardware triangle intersection.–\\nDavid.Powered by Discourse, best viewed with JavaScript enabled',\n",
       " 'rtxgi-albedo': 'Hello,I was integrating RTXGI into our project and the results I got differed quite a lot from the sample included with the SDK.I tracked down the difference stemming from how we treat the albedo (baseColor) texture versus how the sample does it. And now I’m a bit confused.My understanding is that in GLTF with pbrMetallicRoughness the albedo texture is supposed to be encoded with sRGB, from the spec:\\n\"baseColorTexture: The base color texture. The first three components (RGB) **MUST** be encoded with the sRGB transfer function.\"From what I can see the sample does not perform any conversion of the albedo read from the texture from sRGB to Linear, is this correct?\\nIf so, what would be the reason for this?Powered by Discourse, best viewed with JavaScript enabled',\n",
       " 'ffmpeg-video-encode-add-artefacts-on-very-dark-scenes-near-to-black': 'Hi,I have many videos ripped from blu ray (h.264).I don’t have sufficient space to store all videos and I like to re-encode all video with hevc codec (h.265) and keep similar quality (not exactly the some) but with a good compromise.I have tried this command:ffmpeg.exe -hide_banner -hwaccel nvdec -hwaccel_device 0 -vsync 0 -i \"title00.mkv\" -c:s copy -an  -c copy -c:v:0 hevc_nvenc -profile:v main10 -pix_fmt p010le  -rc-lookahead 32 -spatial-aq 1 -aq-strength 12  -cq 30 -qmin 1 -qmax 30 -b:v:0 10M -maxrate:v:0 20M -gpu 0  title00_nvidia_no_audio.mkvthe quality is excellent and is very close to original, but on the very dark scenes (near to black) there are visible artefacts.To solve the problem I have tried this changes without get any improvement:-cq 25 -qmin 1 -qmax 25-spatial-aq 1 -aq-strength 4\\nFor some strange reason seem that if I decrease the -cq from 30 to 25 the artefacts are more visible.I general I’am satasfied about the quality result but before start the encoding I like to understand if there is a way to reduce/remove these artefacts on very dark scenes.Any tips ?Thanks !Powered by Discourse, best viewed with JavaScript enabled',\n",
       " 'impossible-downlad-iray-for-maya': 'When trying to download iray for maya 2017 it starts the downlaod but then stops every time and says download error. Even when trying to restart the download it stops again. Tried it in a different computer and happened the same. Checked the internet connection by downloading another thing and it downloaded perfectly.Powered by Discourse, best viewed with JavaScript enabled',\n",
       " 'nvfbc-on-geforce': 'If Steam can use nvFBC on GeForce why can I not develop my own program that uses nvFBC on GeForce?Anyone there?Why does nobody want to answer my question?NVFBC is a licensed product and not supported on GeForce for all users. Please see https://developer.nvidia.com/capture-sdk for list of supported GPUs.Please read my question again.nvFBC works on GeForce with in home streaming in Steam so why can I not use it in my own program with GeForce?!How can I use nvFBC on GeForce products in my program?As Nvidia states in previous posts it is not allowed to use nvFBC/nvIFR API and nvENC API is also crippled to 2 parallel streams per system with customer grade GeForce cards (and lowend <*2000 Quadros cards). And yes, Nvidia probably use this forbidden API with it’s own products (Nvidia GameStream). My recommendations:I’ve figured out how they enable access to NVFBC on consumer cards and I am able to use the it as I wish. You just have to put in the leg work to figuring out how Steam did it (they got a special key from NVIDIA)Yup, I am fully aware of this, but by doing so you are breaching the NVIDIA Capture API License, which is the crux of the issue. I am the author of Looking Glass (https://looking-glass.hostfission.com) where we would like to use NvFBC on consumer cards and have been trying to find a way to legally allow this for 2 years now.If its legal why not post it here?I need to validate the legality of my claim. I will have to run my method by NVIDIA to see if they approve or it’s release.progress?progress?How do I use the magic key with NvFBC ToSys, pPrivateData only exists in NvFBCCreateParams which is only used in the NvFBC CUDA sample of the Capture SDK?Check the SDK documentation and/or header files for low level C function (NvFBC.h) and/or look to C++ opensource code in examples - nvFBCCreateHandle()/NVFBC_CREATE_HANDLE_PARAMS (linux) or NvFBC_CreateEx()/NvFBCCreateParams (windows) (check code https://codesearch.isocpp.org/actcd19/main/l/looking-glass/looking-glass_0+a12-2/host/Capture/NvFBC.cpp). (for IANAL discussion see also https://github.com/gnif/LookingGlass/issues/188).I can provide my code for retrieving the privateData UUID from a program that is not under NVIDIA’s EULA however they may remove my forum post. Please standby I will post a GitHub link FridayHere is my code https://github.com/trevor403/get-priv-dataI also released a codebase using GeForce Experience GitHub - trevor403/get-priv-data-gfe: Retrieve NvFBC Private Data (UUID) from a GeForce Experience installationPowered by Discourse, best viewed with JavaScript enabled',\n",
       " 'how-to-install-physx-in-ms-vs-2015': 'How to install Physx in ms vs 2015?\\nSorry for my English!Powered by Discourse, best viewed with JavaScript enabled',\n",
       " 'dynamic-linkage-performance-loss-after-adding-more-class-implementations': 'Hello.\\nI found weird behaviour on GeForce 760 GTX card. When using dynamic linkage at some point adding another class implementation causes performance loss even though this class is not used during tests.(This class instance was not assigned to interface variable, but the fact that it could be done during application running caused performance loss). I did not found such dependency on AMD cards. Is it possible that NVIDIA does not support DX11 dynamic linkage mechanisms properly?Powered by Discourse, best viewed with JavaScript enabled',\n",
       " 'writes-after-the-first-512mb-of-a-structured-buffer-are-ignored': 'Hi,I have created a structured buffer of ~720MB, but it looks like that any write done after the first 512MB (in a compute shader) is ignored. When I read the values I only get zeros.Is there an API/hardware limitation that I am not aware? I am running this test on a 1070.ThanksPowered by Discourse, best viewed with JavaScript enabled',\n",
       " 'generating-ptx-code-using-optix-7-4-and-cmake': \"Good afternoon,I am trying to generate some PTX code from CUDA code for raygeneration program and all goes will until I try to call optixTrace in the code. When I do, I get the following PTX output error:\\nptxas ptxcode.ptx, line 30; error   : Call to '_optix_get_launch_index_x' requires call prototypeMy defined CMakeLists.txt file uses the arch=sm_60 with CXX standard 17, calling the embedd_optix.cmake CMake macro in the tools/ directory follows:The following is the cmake macro embbed_optix_shader in the tools/ directory that generates the PTX code during compilation:I am hoping it is something simple in the above code that is causing the issue - a flag maybe? A line I have neglected to include perhaps?The CUDA code being compiled via the above cmake macro follows:Thank you to anyone with any hints or ideas as to what I am doing wrong.Hey @picard1969,Is part of the problem that ptxas is being executed? If you’re trying to generate PTX code, then perhaps allowing ptxas to run is going one step too far?BTW, I doubt you want optix_stubs.h in your device side code, since it only defines host-side stubs.I assume that your SDK samples, like optixPathTracer, build & run without error? (This would rule out version problems of any tools involved, since the path tracer’s raygen program is roughly the same structure as yours.)–\\nDavid.Thank you for the fast response @dhartI will remove the optix_stubs.h since it is pointless in device side code.I really only want to generate the PTX code, not necessarily execute it. Do you have any suggestions as the optimal way to do this from within cmake - something like a cmake macro or is that overkill ?Thanks again.My cmake fu isn’t great. ;) I’m guessing the main issue here is piggy-backing on a cmake build pipeline that’s just doing slightly more than you really want? One option perhaps is to define another custom command for your nvcc step, instead of defining your PTX as a ‘target’. That way you might have a bit more direct control over the steps & inputs & outputs.–\\nDavid.target_compile_options(${output_var}_lib PRIVATE --generate-line-info -use_fast_math -arch=sm_60 --keep)If these are all your NVCC command line options, you’re missing the --ptx flag which is telling the compiler to only translate from *.cu to *.ptx code. You’ve most likely compiled to cubins. Simply look at your *.ptx output files in a text editor. If that is not human readable PTX assembly text, you’re doing it wrong.In my OptiX applications I’m using a custom build rule for each *.cu file which is implemented here. That predates the newer native CMake support for CUDA which I haven’t looked at.\\nNote the commented message instruction in line 45 which will print the actual NVCC command line to the CMake output window. Compare that to the command lines you’re using.\\nhttps://github.com/NVIDIA/OptiX_Apps/blob/master/3rdparty/CMake/nvcuda_compile_ptx.cmake\\nUsage is shown here:\\nhttps://github.com/NVIDIA/OptiX_Apps/blob/master/apps/nvlink_shared/CMakeLists.txt#L168We had that discussion before: https://forums.developer.nvidia.com/t/simple-ptx-shader-optix-7/165303(Mind that payload and attribute registers are of type unsigned int. For cleaner code, use __uint_as_float and __float_as_uint for the reinterpret casts. The next OptiX SDK release is going to correct that in its examples.)duh. You are correct, we have had the discussion before. I apologize.Thanks,Powered by Discourse, best viewed with JavaScript enabled\",\n",
       " 'warping-and-blending-18-unique-displays-is-it-possible': 'Greetings, I am currently working on a large projection mapping installation in Tbilisi, Georgia.One of the main center pieces of the building that our team is currently working on is a 25 meter long tunnel that is warped and blended via MadMapper software and using spliters though the installation that we are planning now will not work with this sort of setup.We are planning a real time project using Azure Kinect for full body tracking, we need to warp and blend 18 projectors in the tunnel without the use of spliters and preferably have a unique audio/video signal for each projector, since each projector has its own speakers we could use it to produce unique sound zones.What solutions are there to output 18 unique signals, since I think that even 4 GPUs would not provide enough ports, and will Nvidia Warp & Blend be able to handle mapping a project like this? The project itself is in Unity.Thanks in advance.Powered by Discourse, best viewed with JavaScript enabled',\n",
       " 'nvidia-shield-tablet-insufficient-memory-error': 'I was reinstalling the 1.2.1 update to my Shield Tablet, which I have done several times before, and I received an error.\\nWhen I input “fastboot flash system system.img” , I get FAILED (remote: Insufficient memory.)  I have a 16GB WiFi only US Tablet.  The only thing that is different than the other times I did this process, is that I recently had Lollipop update on the tablet.  Please help…this is the scary part…\\nim stuck on the same process.\\nLTE USA nvidia tablet.\\nneeded to downgrade my firmware since im having cellular problems with lolipop.\\ntried turning on my device. failed. stuck on bootloopIt is SCARY… I had to contact Nvidia and do an RMA… They tried to tell me that because I tried to downgrade the unit that I voided the warranty.  Which is bogus, it was thier image… they finally agrred to send me another unit… A REFURBISHED one, but it works… I  had my unit for only 3 months…I noticed the smaller 639mb (Factory) 648mb (59) can install.  the larger system images error out.I have this same problem with a Shield Portable.  I can only flash the two original .img files Factory and 59.  Any other update errors out when attempting to flash the system.img I get an error remote: Insufficient Memory.Attempting to let the system run the updates from Settings/About etc… it can attempt to update to version 103 but it will not complete the install and I end up with the new boot screen and it locks up.Is this error limited to some specific original machines?I don’t mind running an older build but I need GamePad Mapper support.  I don’t thing the “Factory” or version 59 include it.Powered by Discourse, best viewed with JavaScript enabled',\n",
       " 'reducing-nvenc-memory-usage': 'Hello! My team has been using the reconfigure NVENC API and found it to work well, but one thing that’s confused us is the VRAM usage of an NVENC session that has not encoded anything. Specifically, we find that setting maxEncodeWidth=2880 and maxEncodeHeight=1800, without encoding any frames, takes 130mB of VRAM. We’d like to lower this, while still having the ability to begin encoding quickly, instead of only starting the encoder when we know we need to start encoding. We’re using OpenGL at the moment, but maybe CUDA could help?We’ve gone through all the usual suspects:Is there a way to do most of the latency heavy NVENC startup routine without using lots of VRAM?Alternatively, is there a way to save the NVENC VRAM state in system memory? Maybe this is possible with CUDA?Powered by Discourse, best viewed with JavaScript enabled',\n",
       " 'quadro-p2000-p1000': 'can the Quadro P2000 or P1000 run in a open ended x4 or x8 slot ?Powered by Discourse, best viewed with JavaScript enabled',\n",
       " 'difference-between-ray-types-in-optix-prime': 'Hi, I have started to use Optix Prime for just some weeks, and I have no problems compiling it and using it, but I have some optimization concerns, and the Optix Programming Guide does not go into many details regarding Optix Prime. First of all, I’m using it to further improve my raytracer, and what I need over all is speed (that’s why I chose to use Prime).My question is, how exactly is the ray type RTP_BUFFER_FORMAT_RAY_ORIGIN_DIRECTION different from RTP_BUFFER_FORMAT_RAY_ORIGIN_TMIN_DIRECTION_TMAX?-I understand that the main difference is the size and that transferring a huge amount of rays would be slightly slower using the “t_min_t_max”, just because there are 8 more bytes per ray to be transferred. But is the “just-origin_direction” version test rays from t_min = 0 to t_max = HUGE_VALUE?-The simplePrime sample uses RTP_BUFFER_FORMAT_RAY_ORIGIN_TMIN_DIRECTION_TMAX with t_min=0, t_max=1e34f fixed values, so wouldn’t it be better to just use ray type RTP_BUFFER_FORMAT_RAY_ORIGIN_DIRECTION instead?-In what kind of scenario would it be useful to specify the t_min, t_max values?Thanks!You got that right.First two questions:\\nYes, the OptiX API Reference Guide also outlines the exact structure in the enum RTPbufferformat section.\\nYes, it’s just that the Ray::format defined in simplePrimeCommon.h is used in more than one exampleIn what kind of scenario would it be useful to specify the t_min, t_max values?For example:\\nt_min to avoid self intersections.\\nt_max when sampling shadow rays to positional light sources or ambient occlusion rays with limited range.Powered by Discourse, best viewed with JavaScript enabled',\n",
       " '3ds-max-2018': 'I’m not a programmer.  I’m an artist.  I apologize for sounding total n00b.  I saw on the PhysX part of the site that precompiled binaries are no longer available.How can I make PhysX work with 3DS Max 2018 without a precompiled plugin I can download and install?Will one not be available at all because of recent Autodesk changes?Am I just too high?Thank y’all very much.I’m still seeing it in the Download Center https://developer.nvidia.com/gameworksdownload#?dn=physx-apex-sdk-1-3-0\\nphysX2018.png1331×332 35.8 KB\\nPowered by Discourse, best viewed with JavaScript enabled',\n",
       " 'will-the-oculus-quest-2-touch-controllers-be-supported-as-a-cxrcontrollertype': 'We have a CloudXr client that works with Quest 1 and 2, but the only cxrControllerType option for Quest is cxrControllerType_OculusTouch, which results in the black Quest 1 controllers in SteamVR (instead of the white Quest 2 controllers).Will more models of the touch controllers be supported as cxrControllerTypes in later version of CloudXR? Is there a potential workaround for showing the white touch controllers in SteamVR in the meantime?@GJones-NVIDIA-XR-Team Are you able to provide some details on my question regarding additional cxrControllerTypes?There are plans to overhaul the controller interfaces in a future release.  but at the moment there’s a limited set of controllers we map to on the server, and when we brought up quest 2 I guess we had no immediate need to expand it to be a distinct controller.One change we’re looking at is splitting out the visual from the functional, which would suit this case well.But this is all still in planning, I have no ETA at this time.(And unfortunately I don’t have any workaround to show the white touch controllers… Well… aside from messing with files on the steamvr server so that touch 1 controllers have the touch 2 graphics – that might work, if you don’t care about quest 1…)It’s great to hear that you’re using CloudXR with your Oculus Quest and exploring all the amazing VR experiences. As for your question, it’s possible that more models of touch controllers will be supported in future versions of CloudXR, but it’s best to check with their support team to confirm. In the meantime, one workaround could be to manually map the white Quest 2 controllers to the black Quest 1 controller profile within SteamVR. This can be a bit of a hassle, but it should allow you to use the white controllers with CloudXR. And if you’re looking for more advice and resources on VR and troubleshooting, you should check out Multitechverse. It’s an online community dedicated to all things VR, offering great advice and support for VR enthusiasts. Best of luck, and happy VR gaming!Powered by Discourse, best viewed with JavaScript enabled',\n",
       " 'optix-5-1-1-precompiled-samples-on-geforce-rtx-3060-do-not-work': 'Good morning, I’ve changed computer and I’m having problems porting my program to Optix 6.5 as the selectors have been removed and I’m having problems with shadow calculation.I wanted to try with Optix 5.1 but if I use the NVidia examples they work on a GTX 1660 Super (desktop) but not on a GeForce RTX 3060 Laptop.If I run “optixDeviceQuery” (Optix 5.1.1) I get`OptiX 5.1.1\\nNumber of Devices = 1Device 0 (0000:01:00.0): NVIDIA GeForce RTX 3060 Laptop GPU\\nCompute Support: 8 6\\nTotal Memory: 6442450944 bytes\\nClock Rate: 1425000 kilohertz\\nMax. Threads per Block: 1024\\nSM Count: 30\\nExecution Timeout Enabled: 1\\nMax. HW Texture Count: 1048576\\nTCC driver enabled: 0\\nCUDA Device Ordinal: 0Constructing a context…\\nCreated with 1 device(s)\\nSupports 2147483647 simultaneous textures\\nFree memory:\\nDevice 0: 5407899648 bytes`while instead if I run “optixConsole” I getOptiX error: Unknown error (Details: Function \"_rtBufferCreate\" caught exception: Encountered a rtcore error: m_exports->rtcDeviceContextCreateForCUDA( context, devctx ) returned (2): Invalid device context)Do I have any hope of using Optix on my new PC? (I don’t want to port the APP to Optix 7.x it would be too expensive…)Grazie\\nADOptiX 5 is not supported on Ampere GPUs. It’s too old for that.\\nThe OptiX core implementation moved into the display driver with OptiX 6 to handle that going forward.\\nAnswered before here: https://forums.developer.nvidia.com/t/optix-5-1-1-crashes-on-a100-gpu-on-attempt-to-create-buffer/178530I have some old code with Optix5.1. We upgraded some systems to RTXs. I do plan to migrate to Optix6.x/7.x in near future. Meanwhile is there a way to programmatically find if Optix5.1 is not compatible on current device, so I can error appropriately instead of crashing?\\nThank youI believe you should be able to call rtContextCreate() and check whether it returns RT_SUCCESS. Otherwise, if it returns RT_ERROR_NO_DEVICE, then you could stop and display an error message to the user.–\\nDavid.Hi DavidOptix5.1 was fine on our GTXs. But terminates on RTXs. rtpContextCreate was successful, but crashes on rtpModelUpdate.\\nOptix6.5 works fine, but before we migrate to 6.5 or above, I just need to handle this nicely.Here is the crash I get running Optix5.1’s sample primeSimple on RTX./primeSimple\\nUsing cuda context\\nError at <</root/sw/wsapps/raytracing/rtsdk/rel5.1/samples_sdk/primeSimple/primeSimple.cpp(158): Function “RTPresult _rtpModelUpdate(RTPmodel, unsigned int)” caught exception: Encountered a CUDA error: radix_sort_temp_size → cub::DeviceRadixSort::SortPairs returned (8): invalid device function’ (999)Unfortunately the bad news is that OptiX Prime is not compatible with Ampere GPUs on recent display drivers even in the OptiX SDK 6.5.0 release.\\nhttps://forums.developer.nvidia.com/t/optix-6-5-prime-samples-fail-with-rtx-3080/177078That API has been removed from OptiX 7 SDKs because is does not make use of the RTX ray tracing hardware units.\\nhttps://forums.developer.nvidia.com/t/optix-and-optix-prime/161308The only feasible future proof solution would be to port the OptiX Prime application over to the OptiX 7 API and that should actually not be too difficult because of the limited features the OptiX Prime API offered.\\nThe OptiX SDKs contain an example named optixRaycasting for quite some time now (even in OptiX 6 versions) which demonstrates the below things.OptiX Prime applications would only handle the ray-triangle intersection part with it, in an also limited acceleration structure hierarchy (one instance level over triangle geometry). Everything around that, means ray generation and shading calculations, would happen outside of that, usually in native CUDA kernels. That part can be completely reused when just implementing the ray-triangle intersection with OptiX 7 instead.OptiX Prime only supported a completely flat hierarchy (triangle geometry only) or a single-level hierarchy (instances of triangle geometry) which are fully hardware accelerated cases in OptiX 7 on RTX boards (e.g. look for OPTIX_TRAVERSABLE_GRAPH_FLAG_ALLOW_SINGLE_LEVEL_INSTANCING) inside the OptiX 7 docs.Building the same kind of acceleration structure and defining geometric primitives would need to be changed inside the host code. On the device side, the ray generation program takes your ray query data and shoots the rays. There would only need to be one closest hit program because all that does is returning hit results from triangles. A miss program wouldn’t be needed since that could be covered by the default initialization of the hit result (negative t_hit to indicate miss).The benefit of using the OptiX 7 API would be full RTX hardware acceleration of the BVH traversal and ray-triangle intersection, and additionally you could handle other primitive types, have a more flexible scene hierarchy, fully custom ray query and hit result data, and some more options.Once that is working, using the whole ray pipeline by also moving the ray generation and shading calculations into OptiX 7 device code would allow to increase the performance even more.Meanwhile is there a way to programmatically find if Optix5.1 is not compatible on current device, so I can error appropriately instead of crashing?You could query the CUDA device properties with the CUDA Runtime API resp. device attributes with the CUDA driver API for the streaming multi-processor version and reject too new architectures.\\nhttps://github.com/NVIDIA/OptiX_Apps/blob/master/apps/intro_runtime/src/Application.cpp#L541\\nhttps://github.com/NVIDIA/OptiX_Apps/blob/master/apps/intro_driver/src/Application.cpp#L593Thank you. Pretty detailed explanation. I am looking forward to migrate to Optix7.x.Meanwhile to check if I can run Optix5.1 on a current machine. The 2 links you provided leads to same computeCapability majors on our GTX and RTX systems. Should I not be checking what is the max computeCapability that an Optix supports and compare again the device’s? Is that possible or am I missing the point.The issue is that OptiX doesn’t really specify a maximum compute capability which it supports, only a minimum which is Maxwell GPUs since OptiX 6.0.0.\\nEach OptiX SDK version’s release notes list the supported GPU architectures at the time it was released and that doesn’t include the ones shipped later.\\nThat OptiX Prime stopped working on Ampere was actually unexpected and due to discontinued support of some specific CUDA instructions still used inside these old acceleration structure builder kernels. That code is part of that old SDK, so there was also no way to solve this with a driver update like it’s possible with the higher-level OptiX API implementation living inside the display drivers since OptiX 6. Note that OptiX 7 is a header-only API.Anyway, I thought you were interested in rejecting specific GPU devices before doing anything with OptiX and when using OptiX Prime you would most likely have a CUDA initialization code part inside your application already after which you could call either of these queries to determine the GPU architecture and if it’s Ampere or higher, exit gracefully instead of crashing on the incompatible OptiX Prime acceleration structure builder.Here is a nice list of what GPU has what compute capability: https://en.wikipedia.org/wiki/CUDAThe 2 links you provided leads to same computeCapability majors on our GTX and RTX systems.For which GPUs?\\nI meant to use both the major and minor compute capability to distinguish the GPUs.\\nYour GTX 1660 should be SM 7.5 (Turing) and an RTX 3060 should be SM 8.6 (Ampere).\\nIs that not the case and the one with SM 7.x is also not working?Got it. You are right, I just need to reject running Optix5.1 on specific devices for time being, I could just do deviceComputeCapabilityMajor < 7.\\nSo instead of hardcoding, I was wondering if there was a suitable variable, so I could do\\ndeviceComputeCapabilityMajor < optix.maxComputeCapabilityMajor.But I have what I need and thanks for all the help. I look forward to Optix7.Powered by Discourse, best viewed with JavaScript enabled',\n",
       " 'opticalflow-sdk-wont-build': 'I am using OpticalFlow SDK on linux.\\nMy system satisfied all requirements. However,several error came out:collect2: error: ld returned 1 exit status\\nmake[2]: *** [AppOFCuda/CMakeFiles/AppOFCuda.dir/build.make:349: AppOFCuda/AppOFCuda] Error 1\\nmake[1]: *** [CMakeFiles/Makefile2:116: AppOFCuda/CMakeFiles/AppOFCuda.dir/all] Error 2\\nmake: *** [Makefile:136: all] Error 2How should I solve them?\\nThanks!Hi,\\nCan you share entire build log?Thanks.I have figured it out. Anaconda could mess up with libraries, therefore,  libraries should be manually assigned in the cmakelist.This topic was automatically closed 60 days after the last reply. New replies are no longer allowed.Powered by Discourse, best viewed with JavaScript enabled',\n",
       " 'correct-configuration-for-decoding-7-video-pipelines': 'Hi. I need to decode 7 FULL HD/60fps pipelines. In my first attempt i used one 2060 SUPER and it turns out that there is no enough power to decode 7 pipelines, it worked just with 6. So i added second 2060 SUPER to my machine. There is my current nvidia-smi:So i tried to decode with new configuration but already at 5 pipelines decode started to delay. This is nvidia-smi when i decode 5 pipelines:\\nBut it’s very strange because when i used just one  graphic cadr it was utilized at 88% percents when i decoded 6 pipelines.\\nAnd what is more strange it’s what they do during decode. One of them just decode something (look below) and second just copying something…\\n\\n11681×835 27.9 KB\\n\\n\\n21683×942 19.2 KB\\nAnd one more thing. When i start to decode first pipelines, one of GPU disables it’s coolers.To decode i use nvdec module from GStreamer, which uses nvidia sdk codec inside.So i have a few questions:\\n1.How to configurate my GPUs correctly? i want to split pipelines between them\\n2. Maybe this happens because the module does not correctly manage several devices?What about configuration of my pc:\\nGPUs: 2x 2060 SUPER\\nmotherboard: TUF x299 MARK 2\\nCPU: i9 10900X\\nPower Supply: 1100WPowered by Discourse, best viewed with JavaScript enabled',\n",
       " 'opengl-cuda-interoperation': 'Hi.\\nLast day I try compiling a basic code found in Internet by it generate many errors, here is the listing of bugs.Error\\t1\\terror LNK2019: unresolved external symbol __imp__cutCheckCmdLineFlag@12 referenced in function _main\\t\\nError\\t2\\terror LNK2019: unresolved external symbol __imp__cutResetTimer@4 referenced in function “void __cdecl computeFPS(void)” (?computeFPS@@YAXXZ)\\tcudaPart1.cu.obj\\tCuda2\\nError\\t3\\terror LNK2019: unresolved external symbol __imp__cutGetAverageTimerValue@4 referenced in function “void __cdecl computeFPS(void)” (?computeFPS@@YAXXZ)\\tcudaPart1.cu.obj\\tCuda2\\nError\\t4\\terror LNK2019: unresolved external symbol __imp__glewIsSupported referenced in function “enum CUTBoolean __cdecl initGL(int,char * *)” (?initGL@@YA?AW4CUTBoolean@@HPAPAD@Z)\\tcudaPart1.cu.obj\\tCuda2\\nError\\t5\\terror LNK2019: unresolved external symbol __imp__glewInit referenced in function “enum CUTBoolean __cdecl initGL(int,char * *)” (?initGL@@YA?AW4CUTBoolean@@HPAPAD@Z)\\tcudaPart1.cu.obj\\tCuda2\\nError\\t6\\terror LNK2019: unresolved external symbol “void __cdecl motion(int,int)” (?motion@@YAXHH@Z) referenced in function “enum CUTBoolean __cdecl initGL(int,char * *)” (?initGL@@YA?AW4CUTBoolean@@HPAPAD@Z)\\tcudaPart1.cu.obj\\tCuda2\\nError\\t7\\terror LNK2019: unresolved external symbol __imp__cutCreateTimer@4 referenced in function “enum CUTBoolean __cdecl runTest(int,char * *)” (?runTest@@YA?AW4CUTBoolean@@HPAPAD@Z)\\tcudaPart1.cu.obj\\tCuda2\\nError\\t8\\terror LNK2019: unresolved external symbol __imp__cutGetCmdLineArgumenti@16 referenced in function “void __cdecl cutilDeviceInit(int,char * *)” (?cutilDeviceInit@@YAXHPAPAD@Z)\\tcudaPart1.cu.obj\\tCuda2\\nError\\t9\\terror LNK2019: unresolved external symbol _launch_kernel referenced in function “void __cdecl runCuda(struct cudaGraphicsResource * *)” (?runCuda@@YAXPAPAUcudaGraphicsResource@@@Z)\\tcudaPart1.cu.obj\\tCuda2\\nError\\t10\\terror LNK2019: unresolved external symbol __imp__cutStopTimer@4 referenced in function\\nError\\t14\\tfatal error LNK1120: 13 unresolved externals\\tC:\\\\Users\\\\dev.cuda\\\\Documents\\\\Visual Studio 2008\\\\Projects\\\\Cuda2\\\\Debug\\\\Cuda2.exe\\tCuda2I searched into forum but any kind of reponse helped me.\\nThanks to think in my problem.Did you link your application against the required libraries?Powered by Discourse, best viewed with JavaScript enabled',\n",
       " 'nvidia-h100-sxm5-driver-support': 'Hi Experts:To H100 SXM5 GPGPU , driver download website shows it only support Nvidia H100 PCIe on Linux and Windows, do you know if this driver can support H100 SXM5 also, or where to find SXM5’s driver?\\nThanks!Powered by Discourse, best viewed with JavaScript enabled',\n",
       " 'full-screen-and-sli-support': 'Am I right in thinking that the VK_KHR_display is for full screen support?\\nIs this a feature you plan to support in the future?Also, are you planning to add support for explicit SLI control, like there is in DX12?\\nIs this exposed by having multiple Queue Families?How do I tell which Graphics cards have a display attached?I have tested this with the 364.51 drivers. I am not in a great hurry for these features, just planning out what is needed to add Vulkan support to my Graphics engine.Efficient GPU to GPU resource sharing is currently not exposed as part of Vulkan 1.0 but it something that Khronos is going to look into the future.Regards,MathiasJust a note to support the notion that it would be great to have information on this sooner rather than later to plan ahead.  As multi-GPU definitely has a brighter future for all given VR/upgrade-value etc.  Also I am not sure how things like nvlink may effect things in the future if it appears on x86.  All of which I am sure nvidia expects.Hopefully we get a great multi-GPU upgrade to the Vulkan spec soon that covers it all then :)Powered by Discourse, best viewed with JavaScript enabled',\n",
       " 'problem-creating-joint-between-two-objects-in-collision': 'Hello,When I create a PxFixedJoint between two PxRigidBodies A and B that are colliding, B moves to its new anchored position but with a crazy rotation.I could upload a minimal code if you need. I already did it; and clearly separating A and B solves the issue.My current solution is not… “sexy”: put both A and B in the same CollisionGroup, wait a cycle, do create the PxFixedJoint.Do you have a better solution?(I tried setLinearVelocity(null), setAngularVelocity(null), clearForce(), clearTorque(), …)LokfryAfter testing my minimal code, it clearly seems a bug in PhysX’s code itself.When I create my fixed joint, I also set the collision off between A and B.“joint->setConstraintFlag( PxConstraintFlag::eCOLLISION_ENABLED, false );”But it seems this function breaks something in simulation filter data; just re-assigning the same filter data does resolve the bug.My workaround:When assigning:I could investigate, find, solve this bug (it seems the internal buffer in ScbShape is corrupted) instead of using my workaround, and send a pull request on your github project. However I am not sure you are active (many pull requests are still waiting)LokfryThis definitely doesn’t seem right. It also seems very strange because, what you’re suggesting is that a data member in the shape class is being stomped when you set a flag on the constraint, which should not have any influence on the shape.Could you confirm which version of PhysX you are using? I can try to make a repro locally but I don’t have a lot of confidence that I’ll be able to repro this issue.You mention “the internal buffer in ScbShape is corrupted”. Does that mean you are overlapping setting these values with simulation so that it’s hitting the buffering cases?Hi, thanks for your reply!First I am not sure at all about my “the internal buffer in ScbShape is corrupted”. I didn’t dig into physx’s code, I don’t know how it works.Because the simulation filter data where exactly the same before and after the call to “setConstraintFlag” (I displayed each bit), I wanted to know where these data were stored and I found the mentioned buffer in ScbShape. And this buffer seems shared. So I did a quick supposition that someone was misreading/mis-writing into this buffer.I just uploaded my minimal code project if you need:Minimal code for debugging a project using PhysX. Contribute to Schebb/bug_mcjointcoll development by creating an account on GitHub.\\n(just have to change the link to PhysX in the CMakeLists.txt)\\nThere are also two screenshots showing what happens when the workaround is on/off.My version is: PhysX 3.3(.4)\\nIt comes from the ‘master’ branch, last commit is:I did the following modifications in order to compile:My compiler (clang++) was complaining about:By the way,We tried to make a repro out of this and failed to reproduce the issue. We tried both PhysX 3.3 and 3.4 and, in both cases, there was no failure. We attempted to repro on Windows 10 64-bit using VS2015 x64. We didn’t include your rendering code but everything else was a straight copy.What’s strange is that PxConstraintFlag::eCOLLISION_ENABLED is disabled by default so there should not have been a need to disable this flag to make the pair stop colliding.Hi,\\nI see that you use clang to compile the PhysX SDK, is that correct?\\nCould you please try these clang flags for compilation to see if it fixes the issue you see:\\n-std=c++11 -fno-rtti -fno-exceptions -ffunction-sections -fdata-sections -Werror -ferror-limit=0 -Wall -Wextra -fstrict-aliasing -Wstrict-aliasing=2 -Weverything -Wno-documentation-deprecated-sync -Wno-documentation-unknown-command -Wno-float-equal -Wno-padded -Wno-weak-vtables -Wno-cast-align -Wno-conversion -Wno-missing-noreturn -Wno-missing-variable-declarations -Wno-shift-sign-overflow -Wno-covered-switch-default -Wno-exit-time-destructors -Wno-global-constructors -Wno-missing-prototypes -Wno-unreachable-code -Wno-unused-macros -Wno-unused-member-function -Wno-used-but-marked-unused -Wno-weak-template-vtables -Wno-deprecated -Wno-non-virtual-dtor -Wno-invalid-noreturn -Wno-return-type-c-linkage -Wno-reserved-id-macro -Wno-c++98-compat-pedantic -Wno-unused-local-typedef -Wno-old-style-cast -Wno-newline-eof -Wno-unused-private-field -Wno-undefined-func-template -Wno-format-nonliteral -Wno-implicit-fallthrough -Wno-undefined-reinterpret-cast -Wno-disabled-macro-expansionThese should work with clang 3.9 hopefully, except the compilation issue you see, that has been already fixed internally, but we did not release 3.3.5 yet unfortunately. We will try to release 3.3.5 asap.Currently 3.3 does not support clang compilation, we have clang support with 3.4 and these settings seems to work for it. Please could you give it a try?\\nThanks,\\nregards\\nAlesYes, I did use clang for the PhysXSDK too.\\n (EDIT: no PhysX SDK was compiled with g++, and my project with clang – my bad!)Okay I will able to test this in approx 2 hours. If it doesn’t work I will test it also on Win 10 64-bit - VS2015 (just to confirm it’s clang).Many thanks!Hi,What’s strange is that PxConstraintFlag::eCOLLISION_ENABLED is disabled by default so there should not have been a need to disable this flag to make the pair stop colliding.Ok, true. I didn’t know it was the default behavior. Last time, when reducing the code, I ended up to surround this setConstraintFlag() with my getFilterData/setFilterData – it fixed my problem – I wrongly supposed it was because of this.I commented this line and the behavior is the same: same problem without my dirty ‘fix’.(Thus I still need these dirty get/setFilterData to make it work :s)Then I did some tests:…so that they are not added to the mentioned ones (+ it seems this is mainly about optimisation flags)\\nand added the new ones as follow:\\n(MakeFile)But more interesting:In all cases it failed (with the exact same behavior – see screenshots) and I need this little tricks get/setFilterData…Since it’s working on your side, I am a bit lost. Now I tested it on MSVC 2015, I see only few differences with your test:For the graphics part, I need to check again. I already found some errors (the biggest one was default rotation quat(0, 0, 0, 1) instead of quat(1, 0, 0, 0)) but nothing has resolved the issue. I will be available again next week, if you want I test something.Thanks,\\nLokfryPowered by Discourse, best viewed with JavaScript enabled',\n",
       " 'draw-visualize-rays-in-optix-prime': 'Hello,I would like to visualize the ray paths in my OptiX Prime program. Is there any way to give the rays some color to be able to draw them into the rendered ppm file?Thank you!Visualize the ray paths as lines in your ray traced image?Doing that with ray tracing doesn’t make much sense. You would need the rays as some geometry primitives in a separate scene, and ray trace that with a special ray-line intersection routine, and merge that with depth testing to the original rendered image.The simpler approach would be to render them with the exact same projection with a rasterizer. Look at the OptiX SDK collision example which does something like that for the visibility test rays in the RenderLinesOfSight() routine. (That whole example renders with OpenGL, just the visibility tests are ray traced.)\\nCorrect depth testing against your ray traced image would be an additional challenge.Doing that visualization for more than a few (secondary) rays at a time in a path traced image will result in serious clutter.In a pinch it can also be useful to dump the ray of interest to a plain text OBJ file as a skinny triangle or some other simple shape.  Then you can load the ray file and possibly the scene geometry into a 3d viewer like Meshlab or Maya, without having to write your own viewer.Hi,@Detlef, I am using Optix 5.0.0 SDK. I could not find the collision example.May I know if this is still included in the samples program ?@dlacewell, Any hint to dump the ray of interests to record its history so that it can be exported as vectors or line paths ?Thanks\\nKusyou can find that sample in OptiX 3.9.0 SDK:\\n\\\\OptiX SDK 3.9.0\\\\SDK\\\\collision\\\\CollisionGL.cpp\\n[url]https://developer.nvidia.com/designworks/optix/downloads/legacy[/url]Powered by Discourse, best viewed with JavaScript enabled',\n",
       " 'usb-driver-for-shield': 'I got a NVIDIA SHIELD and trying to deploy my application with TADP 2.0 but it can’t find SHIELD as a device due to missing driver.Does anyone know where I can find a driver for SHIELD?I couldn’t find any drivers for Windows either so I installed Ubuntu 13.04.Android devices don’t require any drivers in Linux.I installed TADP 2.0 for linux and it’s working flawlessly with SHIELD. Let me know if you get stuck setting it up. Happy coding!We’re working on getting the Shield USB driver for Windows out ASAP.The Shield USB driver for Windows will be bundled as part of the new TADP 2.0r5 release to be announced shortly (we’re days away!)Please keep checking https://developer.nvidia.com/tegra-resources for the release of 2.0r5.That’s great news! Looking forward to it…It still doesn’t work (with drivers from NVPACK 2.0r5)\\nWell, not for my SHIELD anyway — android_winusb.inf doesn’t even contain entry for hardware (USB\\\\VID_0955&PID_7102&REV_0232)What’s even more interesting, I can’t find any ‘USB debug’ settings on device itself — the whole ‘Development Setting’ section missing from UI.Kernel version is 3.4.10-g5dec698 which is not latest and updating may resolve some issues, I believe. But system update tells ‘your system is up to date’.Have you tried the trick of going to “settings” then “about tablet” or “about phone” and tapping on the build number seven times?Google’s been hiding developer mode from casual users, even on the Nexus models.This is something they introduced in Jelly Bean AFAIK.Hey,\\nI just can use any of Android USB composite driver. From another android devices (such as Samsung and LG) and it worked nicely. I can adb and MTP my SHIELD.to higeneko and all the others, i have GOOD NEWS!!! same as all of you guys i have the same problems. i have an old galaxy nexus and the shield tablet. “both unrooted” i don’t wanna root my phone fearing it would delete the cache data of the game. even the tablet coz i might end up destroying it or unable function. after 3 days of extensive reseach and tons of apps forums testing etc. i made it. so heres what i did.i went to helium app. it requires pc and both the devices installed.\\nthe android phone is easy coz it has the adb req in the helium website.\\nthe tricky part is the tablet. as it detects on the pc but only as files.\\ni found this website where u can extract the adb for the shield tablet.http://www.andromods.com/download-files/nvidia-shield-tablet-drivers-usb-adb-fastboot.htmljust follow the instructions and install the adb on your pc.then. back up the games and data cache on ur android device using helium sync and back up.\\nin my case i used google drive. and using my google+ acct. then i sync and restore it in the tablet.walla have fun playing all ur games on hd in your shield tablet.IMPORTANT NOTE! to be able to sync in google drive or cloud u have to buy helium premium 5$\\nbut note that only do this when u have successfully installed the ADB on ur pc 1st.CHHERS guys have fun!Hi WarDAddy!\\nThanks for this information, it is really helpful, I really appreciate it…Powered by Discourse, best viewed with JavaScript enabled',\n",
       " 'is-h-264-avc-multiple-reference-mode-dwmaxnumrefframes-1-supported-in-low-latency-hq': 'Hi alli have a problem in setting multiple :Apparently, NvEnc disables activation of multiple reference mode. Maybe because the low latency?Hi,This is an expected behavior as NVENC supports only one reference frame for P-frame up to pascal generation GPUs.Thanks,\\nRyan ParkPowered by Discourse, best viewed with JavaScript enabled',\n",
       " 'enabling-supersampling': 'Hello,I’m trying to enable supersampling for each fragment, however, I’m unsure how. I have read https://www.opengl.org/registry/specs/ARB/texture_multisample.txt but I couldn’t find an explanation. At the moment I accomplish it with a terrible hack, nameley by making a sample position query in the fragement shader program:Without that the supersampling only occurs at the edges, having both GL_SAMPLE_SHADING_ARB and GL_MULTISAMPLE enabled.I hope someone can help me. Thanks.Powered by Discourse, best viewed with JavaScript enabled',\n",
       " 'how-can-i-get-gpu-chip-thermal-resistance-for-simulation': 'How can I get GPU thermal resistance or where can find this information.\\nI need to set 2R(RJc、RJb)to improve simulation accuracy\\nGeforce card industry\\nThank youPowered by Discourse, best viewed with JavaScript enabled',\n",
       " 'optix-debugging-and-profiling-tools': 'We are excited to announce that Nsight Compute and cuda-gdb now have support for debugging and profiling OptiX applications! These tools will work for both OptiX 6 and OptiX 7 projects.The following tools now support OptiX programs:The latest version of Nsight Compute with OptiX support is available as part of the CUDA Toolkit, or directly here: https://developer.nvidia.com/nsight-computecuda-gdb is part of the latest CUDA Toolkit: https://developer.nvidia.com/cuda-gdbUsing these tools with OptiX requires a 435 driver or newer. Official Drivers | NVIDIAWe will soon be posting a video demonstration of profiling an OptiX application, so stay tuned.Note If you have questions, please feel free to start a new thread. I’m locking this thread only to save a spot for updates and announcements, not to limit questions. There is also a forum dedicated to Nsight Compute issues here: https://devtalk.nvidia.com/default/board/327/nsight-compute-–\\nDavid.Powered by Discourse, best viewed with JavaScript enabled',\n",
       " 'booting-error-after-update-of-geforce-3060-driver-to-515-48-in-ubuntu-20': '\\n20230516_1849321920×1440 119 KB\\n\\nI am getting loading initial ramdisk error after updating drivers for Geforce 3060 from 495.23 to 515.48 in ubuntu 20. I have to update the drivers for carla simulator and unreal engine and now I am stuck.Please help  me!!\\nThanks in advanceHello @ms.sona.12, welcome to the NVIDIA developer forums.Can you switch to a different terminal using CTRL+ALT+F1 or any other Function key from F1 through F6.There you will find console terminals where you can log on to the machine and first of all backup your important data.\\n(I suppose the comment Issues booting up Ubuntu OS with Nvidia GeForce RTX 3060 - #3 by ms.sona.12 was from you and referring to this post here?)Then you can change your boot mode in grub to not boot into the GUI but console mode only. You can find a lot of tutorials online on how to do that. Then no NVIDIA kernel modules are loaded that cause issues with the installation process… Then you should do a fresh install of the new driver. Which meansAfter that let’s see if there is still an issue.I hope this helps.console modeno I am not able to switch to console modes as far as I can understand. I can access grub menu and go to advanced recovery menu.\\nHow to remove all NVIDIA drivers I don’t understand. And I am also skeptical that if I remove all driver it might affect the system as system has dedicated GPU .\\nCan you please guide me how to purge all existing NVIDIA drivers from grub menu itself?Thanks for reply. I have already invested more than 20 hours to get rid of this error.I would be grateful if u can help me out.If you can go to advanced recovery then you should also be able to open a shell, correct? A normal terminal or console. If you get there then you can also use that. Otherwise I recommend reading up about how to disable booting into the GUI by default, for example on askubuntu.Once you have a shell/terminal/console open, start by making sure no nvidia modules are loadedIf there are still modules loaded, unload them with sudo modprobe -r and the name of the module. You might need to do it in certain order. Then uninstall the nvidia driver for example withReboot and get back into a shell. Now install the NVIDIA driver you want to install. Make sure to follow the instructions of the installer exactly! If you have secure boot enabled you MUST follow the correct authentication process, otherwise the kernel module will not be loaded.After the installation you can re-enable the boot to GUI.\\n20230517_1755111920×1440 301 KB\\n\\nI am getting this screen.  Do I need to set anything as shown or I can go ahead with reboot ?Thanks for reply.Can you first check the output ofDid you install the NVIDIA driver using dpkg? That would also explain the problems especially if the old driver was installed using a different method. It is recommended to use either the driver package provided with Ubuntu (you can check that with nvidia-detector or use the GUI software manager, look for 3rd party applications). Or you download the .runfile from NVIDIA and use the built-in installer.\\n20230518_0934421920×1440 228 KB\\n\\nI have not installed anything. This is the output of the command.Powered by Discourse, best viewed with JavaScript enabled',\n",
       " 'ubuntu-22-lts-not-able-to-extend-screens-from-usb-docking-station': 'ubuntu 22 LTS not able to extend screens from USB docking stationnvidia-bug-report.log.gz (550.9 KB)\\nadding the nvidia bug reportHello @kirani and welcome to the NVIDIA developer forums.You need to provide a little bit more detail than this.What kind of System are you running, meaning what GPU, CPU, Laptop(?) etc.?Most importantly, what kind of USB docking station? Not every docking station replicates internal GPU signals.Thanks!Hello,i am running Linux Ubuntu 22.04 LTS on an ASUS TUF Dash F15 laptop with GeForce RTX 3060 Mobile\\nthe docking station is a Trendnet. It actually works on a different ASUS TUF ( older version) laptop of a colleague of mine.\\nSame docking works on a Lenovo laptop using Ubuntu.\\nthe Same docking station was ok on WIndowsPlease find attached the nvidia bug report too.Thank youREgards,\\nKarimnvidia-bug-report.log.gz (551 KB)Your Docking station seems to be the type that uses Internal Display Hardware to act as an additional Display output.I found the lsusboutput in the log you supplied:That means the NVIDIA GPU is independent from this display adapter.A few things you could check:Some built-in GPUs in laptops can use Thunderbolt to send  display signals. That means if the specification of your Laptop explicitly states that it supports “DisplayPort through Thunderbolt”, you can attach a USB-C to DisplayPort adapter and use it as an additional Display output. There are some Docking stations that do support that as well, but they have to explicitly list that in their specifications as well.I hope this helps!hello,\\nthanks for your reply.\\nthis is working now, for some reason i have disabled the secure boot and checked all your recommendations as well using the same docking stationThank youGreat to hear it!Probably the kernel module needed for the docking station was not authenticated with Ubuntu and prevented it from being loaded. That can happen with the NVIDIA driver as well, but I didn’t see any indication of that issue in the logs.Anyway, good that it works for you now!Thank you !This topic was automatically closed 14 days after the last reply. New replies are no longer allowed.Powered by Discourse, best viewed with JavaScript enabled',\n",
       " 'nvidia-texture-tools-exporter-standalone-use-effects-in-cli-batch': 'Is there a way to use the effects presented in the exporter ui, in the CLI?--help shows no effects and I have thousands of images that would greatly benefit from the effect stackPowered by Discourse, best viewed with JavaScript enabled',\n",
       " 'controlador-host-compativel-com-usb-xhci-codigo-10': 'Eu comprei uma EVGA RTX2060, mas após a instalação ficou um erro no dispositivo controlador host compatível com usb xhci, eu ja fiz de tudo, drive nvidia, chipset, ta tudo certo, mas eu estou achando que tem algo haver com o drive da GPU porque o dispositivo com problema esta no mesmo baramento da GPU, que é, Barramento PCI 7, dispositivo 0, função 2. Os 2 dispositivos estão no mesmo barramento Barramento PCI 7.Hi @rodrigoes288, this is the NVIDIA developer forum. We mostly help with and talk about the different tools and SDKs NVIDIA offers to developers.From your description (after automatic translation to English) I gather that you have an end user issue with your graphics card. For that I suggest you either contact the manufacturer support or look into our GeForce forums.Thanks!This topic was automatically closed after 5 days. New replies are no longer allowed.Powered by Discourse, best viewed with JavaScript enabled',\n",
       " 'mpi-and-optixprime': 'I successfully create a program using optixPrime with a combination of:I now want to MPi-ize my code. There are 3 reasons for that.-A) I want to speed up my computation using more core for part 2) , using all the core available on a machine\\n-B) I want to speed up my computation across multiple machines\\n-C) I want to be able to use more memoryAnd, I am aware that optixPrime detect the number of core on your local machine and launch the corresponding number of threads.\\nI am considering 2 approaches:Approach 1)Approach 2) I would rather go with approach 2) to minimize communication but approach 1) might be more interesting for memory reasons (question C)So, finally, here are my questions: I would like to know:A) If I am launching several instance of optixPrime on the same machine, is there a way to limit the number of cores launched by an instance of optixPrime ?  (Approach 2) )\\nEdit: What I want to do is to limit the number of instance to 1 thread so avoid launching N*N thread of optix Prime, N being the number of cores (In the case I launch my application with mpirun -np N myApplication)\\nB) If I want to use MPi accross multiple machines, do I need to use the same GPU ? or is using the same optixPrime version as well as the same cuda drivers is enough ?\\nC) If I am launching several instance of optixPrime on my machine, is this possible to share some of the geometry structure to minimize my memory overhead. (I looks very unlikely, but maybe that is the case ?)    Some an additional information: I do not want to go to optix for now (and use full CUDA) because of the memory limitation inerrant to the GPU. I foresee that if I do a GPU implementation, the only way I can fit all my data is in a 4Go Gpu or more and as a significant part of that data needs to be written by all the threads (and there are no way I can separate the domain since everything is interconnected), I would need to use atomic operation all the time which will - according to what I remember - annihilate all the advantage of using the GPU.\\nIn the case I use a CPU implementation, each thread will have enough memory to be run and I can perform the necessary post-treatment to addition the sampled data coming from all my threads.\\nI could consider to use that approach on GPU as well, but I foresee a requirement of 100Mo of data to be written per thread, so it will limit quite a lot the number of “thread” I would launch on a GPU if I did not want to use atomic operation. (I foresee at least 2 Go or more of overhead due to the geometry and the geometry structure)\\nIf my understanding is not correct, please correct me.Powered by Discourse, best viewed with JavaScript enabled',\n",
       " 'limit-keyframe-size': 'In my application, I’m streaming 4K video over a lossy transport protocol. Normally this works well, but keyframes end up having such a large bitrate that frequently parts of the compressed video are dropped while being sent to the receiver, resulting in visible corruption.I’ve set both averageBitRate and maxBitRate to my target bitrate, but when the IDR is created there is still a very large spike in birate, far in excess of what I had requested. Is there any setting to tell the encoder that I want to limit the size of the keyframe?Powered by Discourse, best viewed with JavaScript enabled',\n",
       " 'brutforce-password-with-gpu': 'Hello, I have a question, a test was conducted on the Internet that the RTX 3090 picks up 4 billion passwords per second, how many passwords the RTX 5000 and Tesla V100 will pick up in due time, I’ll ask you to find the answer to the question that torments me, thank you.Powered by Discourse, best viewed with JavaScript enabled',\n",
       " 'optix-7-visual-profiling-with-timeline': 'Hi,is it correct that Visual Profiler does not show OptiX 7 kernels? And will never support?Can Nsight Systems show a visual timeline of both CUDA and OptiX 7 kernels? If yes, how to do it?If neither Visual Profiler or Nsight Systems can show this timeline, can Nsight Graphics be used to show it? The application is a console application with only CUDA and OptiX 7 calls (no windows).I was able to find and measure raygen* kernels from the application using Nsight Compute but it seems there is no way to visualize the kernels on a timeline?Hi mikoro,I believe that Nsight Systems has been updated to work with OptiX, but Visual Profiler has not. The other tools that have OptiX support are Nsight Compute and Nsight VSE.We are updating OptiX to show your OptiX 7 kernels in your Nsight Systems output, but that isn’t out in a public driver yet. It’s coming soon.In the mean time, you can use NVTX to mark your own OptiX kernels, and let you see them in Night Systems mixed with your CUDA kernels. Adding NVTX markers is quite simple, you just wrap your launch call with the markers. Here’s a code snippet I’ve used before to show my OptiX launch:In Nsight Systems, make sure you enable collection of NVTX data when you profile.–\\nDavid.Powered by Discourse, best viewed with JavaScript enabled',\n",
       " 'physx-3-2-4-vehicles-how-to-interpret-values-from-pxvehiclewheelsdyndata-gettirelatslip-and-pxv': 'Hello,1)In order to generate slipping sounds, I need to know when a vehicle actually slip.\\nI would need a threshold applied on these functions, but I don’t know how to compute it.\\nIn other words, I need to transform their analog values into booleans (ie slipping or not)2)I noticed that when a vehicle slips, the brake is triggered, how can I disable this behaviour please ?Thanks in advance.Hi,There are two types of tire slip:  lateral and longitudinal.  If you are using 3.3 then these are reported in PxWheelQueryResult::lateralSlip and PxWheelQueryResult::longitudinalSlip.  In earlier versions there are accessors to return these values per wheel.The vehicle sdk decomposes the rigid body velocity at each wheel contact point into forward and lateral directions tire directions. This produces two speeds lateralSpeed and LongitudinalSpeed in the contact plane.  The longitdunal slip is the difference between the speed of the spinning wheel at the contact point and LongitudinalSpeed .  The normalised longitudinal slip is a more useful propertyWhen the wheel is locked by the brake (wheelRotationSpeed=0) we have longSlip = -1, when the car is freely rolling (wheelRotationSpeed*wheelRadius = vLongitudinal) we have longSlip = 0. and when the wheel is spinning faster than the car we have longSlip in range (0, infinity).  There is special logic in the code to avoid divide-by-zero but that doesn’t really affect it too much.The lateral slip isWhen the tire has zero lateral speed we have zero lateral slip, while the lateral slip grows with the ratio of lateral and longitudinal speeds in range (-infinity, infinity).As you can see, there isn’t a threshold at which any tire could be said to be “slipping” because it is a continuous process.  The timestep also has a profound affect on the slip values, making it hard to set a specific threshold.   I would recommend setting up the telemetry graphs so that you can drive your car around your map.  This should give you an idea of the kinds of slip values being generated by your car in your application.  An alternative would be to experiment with threshold values and tune them until you hear audio effects as you expect them.The brake is definitely not triggered when tires start slipping.  Could you please explain a bit more what you are experiencing?Thanks,GordonThank you very much for your detailed explanation about lateral et longitudinal slips. It’s really helpful.Indeed, the brake is not triggered when tire slip, it’s a bug in my implementation (where the same keys can accelerate or brake, according to the vehicle move. (For example up pressed and moving forward=accelerate, and down pressed and moving forward=brake) )Thanks.Powered by Discourse, best viewed with JavaScript enabled',\n",
       " 'saving-and-loading-optix-buffers': 'Is there any way I can easily save a buffer’s data onto a file in my hard drive that is not a PNG, and then load it back up into my code? I’d rather not use any additional library for PNG parsingThanks in advanceI don’t see the problem. You map your output or input_output buffer and have a host pointer to its memory. You can write that to disk any way you like.\\nIf you need it as an image format to open it in other applications and it’s an RGB8 or RGBA8 format, the TGA file format offers a simple uncompressed binary representation.Powered by Discourse, best viewed with JavaScript enabled',\n",
       " 'how-to-switch-intersection-program-inside-kernel': 'I’m using optix to bake some texture data for a mesh.  Therefore, I ray trace through each texel, intersect against the 2D texture geometry.  Then pass the interpolated 3D position/normal to the “ClosestHit” program.  In the closest hit program, I need to cast shadow rays.  However, these shadow rays need to be cast against the 3D geometry (not the previous 2D texture geometry).  Is there a way to specify a different intersection program be used in a call to rtTrace?If it is not possible, I guess a workaround would be to create two copies of optix::Geometry, but with the different intersection programs (and also two rtObject nodes).  Then specify the rtObject with the 3D mesh intersections for the shadow ray.Wait, why exactly do you need to “intersect against a 2D texture geometry” inside the scene at all?When baking information into something per texel, the standard approach would be to explicitly calculate the origin and direction for the primary rays from the baked texture’s mapping information onto some 3D geometry surface. Since that generates the primary rays, that’s done inside the ray generation program and gathers the resulting incoming information from these rays and outputs that per texel, which gives your launch dimension. Do that progressively and you can gather as much detail as required.Is there a way to specify a different intersection program be used in a call to rtTrace?No. The intersection and bounding box program are per Geometry. You normally have one intersection and bounding box program per geometric primitive type (e.g. triangle, sphere, etc.).If you want to have different behaviours for the hit primitives, you can handle that via different materials holding different any hit and closest hit programs per ray type.\\nTo make things invisible to some rays and not others, you can use the any hit program for example.\\nThe OptiX Introduction examples explain that for cutout opacity:\\nhttps://devtalk.nvidia.com/default/topic/998546/optix/optix-advanced-samples-on-github/Wait, why exactly do you need to “intersect against a 2D texture geometry” inside the scene at all?When baking information into something per texel, the standard approach would be to explicitly calculate the origin and direction for the primary rays from the baked texture’s mapping information onto some 3D geometry surface.Well my idea was to ray cast through each texel, find the 2D texture triangle intersection barycentric coordinates, and use that to interpolate the 3D position/normal, and continue processing in 3D from there.Otherwise, when processing each texel, how would you determine the triangle it belongs to?Actually this discussion helped me:https://devtalk.nvidia.com/default/topic/1029449/optix/baking-to-texture/Looks like I should precompute the texel to 3d data ahead of time as a preprocessing step.Well, you could use OptiX to precompute the mapping from uv space to 3d space if you like; I don’t see why that wouldn’t work.  It’s probably overkill for a pre-process, but as you said, you could place all the uv coordinates of the triangles into an acceleration structure in OptiX, then shoot rays through the texels to find the triangle id and barycentric coordinate for each texel center.  The “per texel, per triangle” algorithm described in the link above is the simpler linear version of this.I would offset and start the rays behind the uv plane, e.g. if you use (X=u, Y=v, 0) for the 2d vertices, then I would start the rays with some Z coordinate like (-1), not 0.  Otherwise you would be doing a point query on a 2d triangle, and that might stretch the robustness of the acceleration structure.This would be a completely different acceleration structure, different bounding box program, and different intersection program, versus the ones you would use later when computing final color for that texel.Powered by Discourse, best viewed with JavaScript enabled',\n",
       " 'gsync-for-linux': 'Does anyone know when Gsync will be supported by the Nvidia Linux driver?I have my monitor and a photodiode, and would be happy to beta test to make sure it works.Powered by Discourse, best viewed with JavaScript enabled',\n",
       " 'cuda-visualization-without-opengl': 'Hello,I am CUDA beginner and this is my situation at my University:\\nI am using a cluster of 16 CUDA enabled nvidia cards, that are behind a head node. This head node does not have any dedicated graphics card to see any of the visualizations (for now I am trying the SDK samples). I can run the normal CUDA code and everything is fine except that there is no way for the visualizations.I was wondering whether there is a way to save the CUDA visualization as a video etc? all suggestions are welcome.Thank youHow do you generate your output without tying in DISPLAY? just curious.Powered by Discourse, best viewed with JavaScript enabled',\n",
       " 'multiple-ray-generation-points-camera': 'Is it possible to run ray tracing from multiple locations for the same object simultaneouslyHi @EvanYL, yes it is possible. You can decide in your raygen program how to map your launch index into a ray.This means it is very easy to (for example) render an output image consisting of image tiles, where you choose a different camera based on which tile the launch index is in.Or, sometimes people will pass in a 1D buffer of rays to the OptiX kernel, and every single ray might have a different origin. You could look at the optixRaycasting sample in the SDK for an example of this.You have complete control over how rays and camera views are generated and organized, to allow for workflows like tiled rendering or texture baking or stereo or any number of scenarios.–\\nDavid.Is there an upper limit of a total number of ray can be processed at the same time? Is it relate to the model of the GPU? Is there a simple estimate equation to calculate like the X number of RT core can process Y number of ray (no matter it is from 1 or multiple origin)Please have a look into this thread and follow the links in there: https://forums.developer.nvidia.com/t/optix-7-0-0-memory-error/180777In theory you can have a as many optixTrace() call invocations inside your ray tracing kernel as you want when staying under the maximum launch dimension.\\nBut you will compete against the operating system for the GPU resources.\\nIf you are running on a GPU dedicated to compute tasks only (Tesla Compute Cluster (TCC) driver mode), there is no OS timeout like under the Windows Display Driver Model (WDDM since Vista, it got better in Windows 10 with WDDM2).The RT cores on the top end Turing GPUs could already handle over 11 GRays/second. With complex scenes or shaders you will usually run into memory bandwidth limits first.It’s usually the simplest approach to partition work into reasonably sized chunks and launch less work more often instead of using some super expensive kernel which does it all.\\nThere are plenty of options in computer graphics to split work, e.g. into individual samples in a progressive Monte Carlo algorithm, into tiles of an image, into an image per light source etc.Powered by Discourse, best viewed with JavaScript enabled',\n",
       " 'wgljoinswapgroupnv': 'I am having a problem with wglJoinSwapGroupNV. When set my app to join a swap group (group 1), I get the application to hang. The application is hanging on the swap buffer. I do the join swap group after I have created the window, and have valid hdc. I have demo app that uses wglJoinSwapGroupNV, and it seems to run fine. Right now I am creating between 1-3 windows each with 1 OpenGL context, it does not seem to matter if I use 1 or 3 windows, same result.I am using this on a machine with 2 Quadro K5000 and a sync board.If any one has any ideas on what could cause the application to hang on the swap buffer, it would be appreciated.I’m having the same issue running my opengl application with three K6000 cards + quadro sync. There are three fullscreen windows on three monitors, each with its own opengl context. Behavior is the same you have described. I’m using GLFW 3.0.4 to create windows, my system is Windows Server 2012 R2 (64bit), R380.84 driver.Please, have you found solution for to this problem?I revive this old thread as I came by the same issue in a naive GLFW test application.I believe this NVidia presentation has the answer : Display for each window in a separate threadIf the swapBuffers calls are done in sequence, in the same thread, this will cause a deadlock.When looking at the extension specification, we can see thatAll of the following must be satisfied before a buffer swap for a window\\ncan take place:andA window is ready when all of the following are true:A group is ready when the following is true:So, if the code is, in a single thread, like the following :then, the first glfwSwapBuffers will never return because the swap group is not ready, and will never be because “buffer swap command has not been issued” for window2.\\n2022.06.02 - NVidia info on WGL Swap Groups1356×625 147 KB\\nThe issue I ran into was related to timeBeginPeriod, basically we found the graphics driver was super sensitive to it. It would hang, and sometimes deadlock. Basically we figured out you just can’t set it all…maybe that has changed with the latest and greatest driver…In our case we had the swapbuffers in separate processes, and actually separate machines too.At the end of the day, we ended up not using this anyways…basically we had an application with a high level of optical flow, where having all 16 displays hang for 1 dropped frame was way worse than losing blending for 1 frame.\\ntimeBeginPeriodPowered by Discourse, best viewed with JavaScript enabled',\n",
       " 'gpu-bootleneck-transcoding-live-video': 'Hello people,I’m using a NVIDIA P2000 and FFMPEG for transcoding .\\nI whant to reach 20 TV Channels (live) transcoded to 3 ABR profiles each (360p,480p,720p)\\nI PULL video from Digital TV HeadEnd (mpegts over UDP).\\nActually i have 14 Channels working OK and when i try to start transcoding #15 some channels begins to broken his video quallity\\nI have doubts about what is considered “GPU Bottleneck”I read some DOCS, like this: http://on-demand.gputechconf.com/gtc/2017/presentation/s7111-abhijit-patait-nvidia-video-technologies.pdf\\n, in page 35 (ANALYZING PERFORMANCE BOTTLENECKS) i understand that 90%, 99% is considered GPU BottleneckSo, in my scenery i have 65% for Encoding and %70 for Decoding (measured with “nvidia-smi dmon”)This is what i have in my transcoder box:FFMPEG downloaded and compiled from NVIDIA\\nffmpeg version N-93005-gd92f06eb66 Copyright (c) 2000-2019 the FFmpeg developers\\nbuilt with gcc 7 (Ubuntu 7.3.0-27ubuntu1~18.04)Drivers downloaded and compiled from NVIDIA\\nfilename:       /lib/modules/4.15.0-43-generic/updates/dkms/nvidia.ko\\nalias:          char-major-195-*\\nversion:        410.79\\nsupported:      external\\nlicense:        NVIDIA\\nsrcversion:     1283EC37DF82D5A8A902589filename:       /lib/modules/4.15.0-43-generic/updates/dkms/nvidia-drm.ko\\nversion:        410.79\\nsupported:      external\\nlicense:        MIT\\nsrcversion:     68386DCC84A5B791B6EEC80This is my FFMPEG command:I missed something???..must i agregate some filter for best GPU performance???Thanks to all !!!Powered by Discourse, best viewed with JavaScript enabled',\n",
       " 'gldrawcommandsnv-and-uniform-buffers': 'Hi,\\nI’m using the NV Command List extension and have some questions about the use of uniform buffers together with glDrawCommandsNV. According to the spec. it says (about glDrawCommandsNV) that:\\n“The current binding state of vertex, element and uniform buffers will not be effective but must be set via commands within the buffer, other state will however be inherited from the current OpenGL context.”\\nAs I read it, I then have to supply anything related to the uniform buffers (bindless/address) in the command/token buffer? I have done that and it works great!\\nNow, I’ve also tried to skip adding anything related to uniform buffers in the command/token buffer and instead used a “regular” OpenGL approach for the uniform buffers (just glBindBufferBase(GL_UNIFORM_BUFFER,…), NO glEnableClientState(GL_UNIFORM_BUFFER_UNIFIED_NV), and NO layout(commandBindableNV) uniform in the shader) and that seems to also work well. My question is now, is this going to be consistent behaviour? Or did I just get lucky because there are some fallback mechanisms in the driver or something?\\nThe reason I’m asking is that I want to re-use the same command/token buffer when rendering depth textures for shadows, but my shader for doing that has different content/size/number of uniform buffers in that case, so if I must supply uniform buffer information through the command/token buffer I will have to create a separate command/token buffer for that case. I know that I can probably do some kung-fu to make re-use possible anyway, but I wanted to fully understand the behaviour.\\nSo, to summarize: Can I use a “regular” OpenGL approach for uniform buffers (no command/token/bindless/address) together with glDrawCommandsNV? I know that the spec. says that default-block uniforms works, but I’m not sure about uniform buffers. It does work, but can I rely on it?\\nAlso, just to clarify, this is how I typically use the uniform buffer(s) it in the shader:\\nstruct UBOUniformData\\n{\\nmat4 viewMatrix;\\nmat4 viewProjMatrix;\\nvec4 lightDirWS;\\n};layout(std140, binding=1) uniform UBOUniformDataBuffer {\\nUBOUniformData uboUniforms;\\n};Thanks,\\nMikaelHi Mikael,it indeed will work as you found out already. Basically glDrawCommands* triggers a relatively heavy validation in the drawcall, more than regular drawcalls since we don’t know what state was changed by the tokens on the GPU. That’s why it’s faster to do the binds in the tokens, rather than on the driver side.you can also use different offsets to trigger different bindings{ubobindingsA,ubobindingsB,drawcalls…}Then you can draw with 2 ranges by passing different offsets & sizes: first range is using the appropriate ubobindings section, second the drawcalls.Christoph,\\nThanks for your quick response!Ok, so then I should actually read the spec (for glDrawCommandsNV) as “If the client states for VERTEX_ATTRIB_ARRAY_UNIFIED_NV, ELEMENT_ARRAY_UNIFIED_NV and UNIFORM_BUFFER_UNIFIED_NV are enabled, the current binding state of vertex, element and uniform buffers will not be effective…\"? And this is not something that just happens to work in the current drivers? This is the behaviour we can also expect from future drivers?About the heavy validation cost, that is per glDrawCommandsNV call, right? So if I have very few glDrawCommandsNV calls per frame I would not really see any major performance difference between using “regular” OpenGL UBO binds and tokenized? Or is there any other difference in cost “during” the individual draw calls from the token buffer?Great suggestion also with the offset/sizes stuff, wasn’t aware of that!Anyway, I would also like to thank you for this extension and the great work you’ve done with “CAD scene rendering techniques” and occlusion culling. I’m using it here with huge CAD-models (50-150 million triangles) that also happen to have a lot of occlusion and the performance is really great!/MikaelOk, I see now that my above ”modification” to the spec was not correct…I did some tests and it appears that there is flexibility in uniform buffers in that we can use “regular” OpenGL binds (no need to place in token buffer).\\nHowever, vertex attrib arrays must be added to token buffer / use bindless.So, things are a bit more clear now!I’m still interested to hear more about the “validation costs”, though :)/MikaelYes, the CPU validation occurs only per glDrawCommands*NV and is more expensive than a regular glDrawArrays/Elements etc.\\nGlad things are working out well.Powered by Discourse, best viewed with JavaScript enabled',\n",
       " 'lwjgl-glfw-api-unavailable-error-linux': 'nvidia-bug-report.log.gz (321.7 KB)Hi, hope this is the right forum to ask.\\nI would like to learn lwjgl but can not start the openGl hello world, but starting glxgears via terminal works\\n“[LWJGL] GLFW_API_UNAVAILABLE error\\nDescription : GLX: No GLXFBConfigs returned”any idea how to fix the glx problem?I’m not an Linux expert(yet) and not an graphics expert, so sorry for the silly questionsmy  system is Linux mint 20.3 Cinamon\\ngraphics card 2060 8gb(non super)I’m not sure what the problem is, but I know that lwjgl examples starts if I uninstall the nvidia drivers and it falls back to mesa,( but the performance is really bad)\\nso could be something with the driversI asked also 5 days ago in stackoverflow with 0 answers after 5 daysno ideas what could be wrong?\\n~$ sudo glxinfo | grep “GLX version”\\nGLX version: 1.4fixed.\\nroot cause was broken IntelliJ installation via flatpak\\nand not driver issuePowered by Discourse, best viewed with JavaScript enabled',\n",
       " 'cannot-get-gl-context-v2-10-after-installing-64-bit-linux-driver-v310-19': 'My 3D engine program was running with OpenGL v3.20 context before I installed the latest nvidia driver on my 64-bit ubuntu v12.04 computer (nvidia driver 310.19, which is still shown as current as of today).  After I installed the new driver, my program stopped working and the first error generated by my program was glGenVertexArrays() returning a value of zero (and no error).At first I thought the problem was with VAOs, but after inspecting the printout my program generates in the console window, I noticed it appears to say my application is no longer receiving the OpenGL v3.20 compatibility context it requests.  With my new GTX680 and v310.19 driver, it should be able to get OpenGL contexts up to v4.30 as I understand things.So the question probably is, why is my program not getting an OpenGL v3.20 context?Since we seem to have narrowed down the problem to OpenGL context and not VAO functions, I create this new, appropriately named threat to continue this conversation.I have always had problems initializing the combination of xlib, GLX, GLEW, OpenGL.  I have never been able to sufficiently grasp everything necessary about xlib, GLX, GLEW and OpenGL and thus fully understand what is necessary, much less appropriate.I guess the best way to attempt to resolve this is to explain exactly what I am doing now.  Here goes:#1:  Before anything significant happens in my program, it executes the ig_graphics_initialize() function, which only calls old-style GLX functions and creates, then destroys an old-style GLX context and window, then initialize GLEW so all newer GLX functions become visible.#2:  Later, after everything is initialized, the program creates a default window with newer GLX functions (and should-be v3.20 compatibility context).  It was creating this OpenGL v3.20 compatibility context before I updated my nvidia GPU to GTX680 and nvidia driver to v310.19 (64-bit version).Next I’ll be more specific and show the code in the ig_graphics_initialize() function, and then the ig_window_create() function.  I suspect that something “minor” changed that obsoletes my old approach, and now requires something be done in a different order, or with different functions or arguments.  But what do I know?  Not enough, obviously.I’ll appreciate any tips or comments anyone might have, especially from xlib, GLX, GLEW, OpenGL initialization gurus.NOTE:  You can skip the first ~130 lines below to get to the meat of the function (where real xlib, GLX, GLEW stuff happens)There is a bug in GLEW. It uses glGetString(GL_EXTENSIONS) and GL_EXTENSIONS is deprecated. Seems it’s been around since 2010 without anyone actually fixing it. Possibly a fix will be in GLEW 1.9.2 (although I’m not holding my breath).http://sourceforge.net/p/glew/bugs/120/\\nhttp://sourceforge.net/p/glew/bugs/174/GLEW Does Not Work with an OpenGL 3 or 4 Core ProfileYou can try and patch it yourself or look at using another extension loader like gl3w or glee (OpenGL 3.0 only).Ah, just shoot me!  I hate when this kind of nonsense happens.  Maybe/Probably my fault.In my IDE, I had directory “/usr/lib” specified as a linker search path… probably from before they switched over to /usr/lib/i386-linux-gnu and /usr/lib/x64-linux-gnu a year or so ago. I had both specified, but “/usr/lib” was first in the list (which might matter).So presumably what was happening was one or another of the libraries my 3D engine was linking to was an older version in the “/usr/lib” directory, while a newer version exists in the “/usr/lib/i386-linux-gnu” and “/usr/lib/x64-linux-gnu” directories.It also seems to me like the codeblocks IDE (or perhaps the GNU tools/linker) has gotten “smarter” (maybe too smart) about what directories to search for shared libraries.  I forgot to put any linker library names or search directories in my test program, yet somehow it linked to xlib and GL (neither of which is that standard a library, nothing like the C standard libraries or something).Anyhow, that was the problem all along. Gads, can’t believe how much BS I went through where no problems in my programs existed.Probably the new nvidia drivers put their shared libraries in the /usr/lib/i386-linux-gnu and /usr/lib/x64-linux-gnu directories… while older versions of the shared libraries got put in /usr/lib directly (or something along those lines). Otherwise, who freaking knows.Nice to have it solved, but I hate when I waste a day or three on something as stupid as this. Sigh.Powered by Discourse, best viewed with JavaScript enabled',\n",
       " 'camera-disconnection-connection-handling': 'How to handle if the camera disconnects while receiving video and reconnects after some time?\\nShould this be done in the driver or in the application?hello all dear,\\nThere is no one to answer?!when the camera is disconnected the board return timeout, when connected, we must be stop the stream and then start the stream. how to handle that without stop and then start stream?Powered by Discourse, best viewed with JavaScript enabled',\n",
       " 'boolean-operations-in-physx-truncated-an-object-with-a-plane': \"(Sorry for the machine translation).\\nHello. Subject subject following. I want to create a free program for tracking the 3D printing process by the neural network. The task of the neural network is elementary, and I hope to find help in the relevant sections of the NVidia community. The neural network will only have to select the printer 3D carriage and guides in the image, with the printer carriage, and remove them from the subsequent processing.\\nI will describe a message process:What you need in this section - I need to know how to perform logical operations on the models on the table (truncated with a plane from above) in the PhysX. The engine is going to use Unreal Engine, if of course you can free, as the program does free.\\nThe enthusiast himself, the inventor, have patents and patent inventions, when it is necessary to implement something, then I write a code, and this task I solve in parallel with the setting of video surveillance on the family farm, decided that if there I adjust (need forced - climate changes and delivers problems, they need to be monitored), then mood and for the printer.I 'm grateful for the help in advance.\\nIf I need to seek help on the subject, of clipping the object, to other forums - please give a link.I take it I need to use APEX PhysX?\\nProbably there can be logical operations on the object…Powered by Discourse, best viewed with JavaScript enabled\",\n",
       " 'closed-caption': 'I am new to use nvidia sdk.\\nI am using video codec sdk 9.0.20 for encoding the h264 video using nvidea gpu. Can you please advice me how to add closed caption in AppEncuda project. I am working on widows platform.Thank You,\\nDhrumil ShethHi Dhrumil,\\nI suggest you to look at FFmpeg for such a use case. FFmpeg has NVIDIA hw accelerated video decode/encode supported. Video Codec SDK 9.0.20 has documentation about how to use FFmpeg with NVIDIA GPU.Thanks.Thanks for reply.\\nCan you provide the link for sample ffmpeg code with nvidea scale_npp.Thanks.Powered by Discourse, best viewed with JavaScript enabled',\n",
       " 'are-there-plans-for-ffv1-hardware-encoding-and-decoding-support': 'Hi,I’m curious whether FFV1 hardware encoding and decoding is on the roadmap – it’s a very popular preservation codec in public archives because it’s lossless and openly licensed (FFV1 - Wikipedia), though I haven’t seen any fixed-function implementations despite its maturity at this point. In my professional life it is by far the most common codec that I encode to which is not currently supported by NVENC (in fact, virtually the only one), and it is often used for very, very large files that a hardware implementation would be very useful for. Compared to AV1 encoding, I believe it should be far easier to support, and I think there would be widespread interest in such a project at e.g. No Time To Wait and other venues.Thanks!Hi,I would really interested by this as well. Had you the opportunity to study the algorithm? If yes, do you think it can take advantage of parallel computation?Powered by Discourse, best viewed with JavaScript enabled',\n",
       " 'building-walds-optix-7-turtorial-centos-7': \"I have had little luck getting Ingo Wald’s turorial demos to build using cmake under Scientific Linux/CentOS 7.Here’s a quick list the the problems I’ve hit so far.1) Download: link on https://gitlab.com/ingowald/optix7course is badSays: git clone Ingo Wald / optix7course · GitLab\\nShould be: git clone Ingo Wald / optix7course · GitLab2) CMAKE: I have CUDA 10.2 installed in /usr/local/cuda, and the OptiX 7 headers is /usr/local/optix/include\\n\\nmkdir build\\ncd build\\nccmake3 …Brings up the GNU curses interface.  I press [c] to configure and get:CMake Error at /usr/share/cmake3/Modules/FindPackageHandleStandardArgs.cmake:137 (message):\\nCould NOT find CUDA (missing: CUDA_INCLUDE_DIRS CUDA_CUDART_LIBRARY) (found\\nversion “10.2”)\\nCall Stack (most recent call first):\\n/usr/share/cmake3/Modules/FindPackageHandleStandardArgs.cmake:378 (_FPHSA_FAILURE_MESSAGE)\\n/usr/share/cmake3/Modules/FindCUDA.cmake:1092 (find_package_handle_standard_args)\\ncommon/gdt/cmake/configure_optix.cmake:23 (find_package)\\nCMakeLists.txt:32 (include)So, I try instead:ccmake3 … -DCUDA_CUDART_LIBRARY=/usr/local/cuda/lib64/libcudart.so  -DCUDA_TOOLKIT_ROOT_DIR=/usr/local/cuda -DOptiX_INSTALL_DIR=/usr/local/optix -DOptiX_INCLUDE=/usr/local/optix/includeThis time pressing [c]onfigure (twice!?!) gives the option to [g]enerate.  I do that and ccmake3 exits.3) Make (and undefined constants)I typemakeand getScanning dependencies of target gdt\\n[  1%] Building CXX object common/gdt/CMakeFiles/gdt.dir/gdt/gdt.cpp.o\\n[  2%] Linking CXX static library …/…/libgdt.a\\n[  2%] Built target gdt\\nScanning dependencies of target ex01_helloOptix\\n[  3%] Building CXX object example01_helloOptix/CMakeFiles/ex01_helloOptix.dir/main.cpp.o\\nIn file included from /home/squeen/Downloads/NVIDIA/optix/optix7course/example01_helloOptix/main.cpp:18:0:\\n/home/squeen/Downloads/NVIDIA/optix/optix7course/common/gdt/gdt/gdt.h:142:19: error: ‘uint32_t’ does not name a type\\ninline both uint32_t divRoundUp(uint32_t a, uint32_t b) { return (a+b-1)/b; }\\n^\\n/home/squeen/Downloads/NVIDIA/optix/optix7course/common/gdt/gdt/gdt.h:144:19: error: ‘uint64_t’ does not name a type\\ninline both uint64_t divRoundUp(uint64_t a, uint64_t b) { return (a+b-1)/b; }\\n^\\n/home/squeen/Downloads/NVIDIA/optix/optix7course/common/gdt/gdt/gdt.h: In function ‘double gdt::getCurrentTime()’:\\n/home/squeen/Downloads/NVIDIA/optix/optix7course/common/gdt/gdt/gdt.h:218:41: error: ‘nullptr’ was not declared in this scope\\nstruct timeval tp; gettimeofday(&tp,nullptr);\\n^\\nmake[2]: *** [example01_helloOptix/CMakeFiles/ex01_helloOptix.dir/main.cpp.o] Error 1\\nmake[1]: *** [example01_helloOptix/CMakeFiles/ex01_helloOptix.dir/all] Error 2\\nmake: *** [all] Error 24) HACK#1:  Using standard integer data types.I change the ‘uint32_t’ in optix7course/common/gdt/gdt/gdt.h to the standard “unsigned” type and similiarly the ‘uint64_t’ “unsigned long” types (is this Windows baggage?) and I am able to get a bit further…Scanning dependencies of target ex01_helloOptix\\n[  3%] Building CXX object example01_helloOptix/CMakeFiles/ex01_helloOptix.dir/main.cpp.o\\nIn file included from /home/squeen/Downloads/NVIDIA/optix/optix7course/example01_helloOptix/main.cpp:18:0:\\n/home/squeen/Downloads/NVIDIA/optix/optix7course/common/gdt/gdt/gdt.h: In function ‘double gdt::getCurrentTime()’:\\n/home/squeen/Downloads/NVIDIA/optix/optix7course/common/gdt/gdt/gdt.h:218:41: error: ‘nullptr’ was not declared in this scope\\nstruct timeval tp; gettimeofday(&tp,nullptr);\\n^\\nmake[2]: *** [example01_helloOptix/CMakeFiles/ex01_helloOptix.dir/main.cpp.o] Error 1\\nmake[1]: *** [example01_helloOptix/CMakeFiles/ex01_helloOptix.dir/all] Error 2\\nmake: *** [all] Error 25) HACK#2:  nullptr becomes the C standard NULL\\n\\nFixing line 218 I meet with some success:make\\n[  1%] Building CXX object common/gdt/CMakeFiles/gdt.dir/gdt/gdt.cpp.o\\n[  2%] Linking CXX static library …/…/libgdt.a\\n[  2%] Built target gdt\\n[  3%] Building CXX object example01_helloOptix/CMakeFiles/ex01_helloOptix.dir/main.cpp.o\\n[  5%] Linking CXX executable …/ex01_helloOptix\\n/usr/bin/ld: CMakeFiles/ex01_helloOptix.dir/main.cpp.o: undefined reference to symbol ‘dlsym@@GLIBC_2.2.5’\\n//usr/lib64/libdl.so.2: error adding symbols: DSO missing from command line\\ncollect2: error: ld returned 1 exit status\\nmake[2]: *** [ex01_helloOptix] Error 1\\nmake[1]: *** [example01_helloOptix/CMakeFiles/ex01_helloOptix.dir/all] Error 2\\nmake: *** [all] Error 26) Adding -ldl to CMAKEI triedccmake3 … -DCUDA_CUDART_LIBRARY=/usr/local/cuda/lib64/libcudart.so  -DCUDA_TOOLKIT_ROOT_DIR=/usr/local/cuda -DOptiX_INSTALL_DIR=/usr/local/optix -DOptiX_INCLUDE=/usr/local/optix/include -DBIN2C=/usr/local/cuda/bin/bin2c -DCMAKE_CXX_FLAGS=-ldlNow the make error became…Scanning dependencies of target gdt\\n[  1%] Building CXX object common/gdt/CMakeFiles/gdt.dir/gdt/gdt.cpp.o\\n[  2%] Linking CXX static library …/…/libgdt.a\\n[  2%] Built target gdt\\nScanning dependencies of target ex01_helloOptix\\n[  3%] Building CXX object example01_helloOptix/CMakeFiles/ex01_helloOptix.dir/main.cpp.o\\n[  5%] Linking CXX executable …/ex01_helloOptix\\n[  5%] Built target ex01_helloOptix\\n[  6%] Building NVCC ptx file example02_pipelineAndRayGen/cuda_compile_ptx_1_generated_devicePrograms.cu.ptx\\nIn file included from /usr/local/optix/include/optix_device.h:43:0,\\nfrom /home/squeen/Downloads/NVIDIA/optix/optix7course/example02_pipelineAndRayGen/devicePrograms.cu:17:\\n/usr/local/optix/include/optix_7_device.h:36:2: error: #error Device code for OptiX requires at least C++11. Consider adding “–std c++11” to the nvcc command-line.\\n#error Device code for OptiX requires at least C++11. Consider adding “–std c++11” to the nvcc command-line.\\n^\\nIn file included from /usr/include/c++/4.8.2/type_traits:35:0,\\nfrom /home/squeen/Downloads/NVIDIA/optix/optix7course/common/gdt/gdt/math/vec/functors.h:19,\\nfrom /home/squeen/Downloads/NVIDIA/optix/optix7course/common/gdt/gdt/math/vec.h:378,\\nfrom /home/squeen/Downloads/NVIDIA/optix/optix7course/example02_pipelineAndRayGen/LaunchParams.h:19,\\nfrom /home/squeen/Downloads/NVIDIA/optix/optix7course/example02_pipelineAndRayGen/devicePrograms.cu:19:\\n/usr/include/c++/4.8.2/bits/c++0x_warning.h:32:2: error: #error This file requires compiler and library support for the ISO C++ 2011 standard. This support is currently experimental, and must be enabled with the -std=c++11 or -std=gnu++11 compiler options.\\n#error This file requires compiler and library support for the \\n^\\nCMake Error at cuda_compile_ptx_1_generated_devicePrograms.cu.ptx.Release.cmake:211 (message):\\nError generating\\n/home/squeen/Downloads/NVIDIA/optix/optix7course/build/example02_pipelineAndRayGen/cuda_compile_ptx_1_generated_devicePrograms.cu.ptxmake[2]: *** [example02_pipelineAndRayGen/cuda_compile_ptx_1_generated_devicePrograms.cu.ptx] Error 1\\nmake[1]: *** [example02_pipelineAndRayGen/CMakeFiles/ex02_pipelineAndRayGen.dir/all] Error 2\\nmake: *** [all] Error 27) Adding -std=c++11 to nvccTried again with a new CMake command…ccmake3 … -DCUDA_CUDART_LIBRARY=/usr/local/cuda/lib64/libcudart.so  -DCUDA_TOOLKIT_ROOT_DIR=/usr/local/cuda -DOptiX_INSTALL_DIR=/usr/local/optix -DOptiX_INCLUDE=/usr/local/optix/include -DBIN2C=/usr/local/cuda/bin/bin2c -DCMAKE_CXX_FLAGS=-ldl -DCUDA_NVCC_FLAGS=-std=c++11‘make’ produces…Scanning dependencies of target gdt\\n[  1%] Building CXX object common/gdt/CMakeFiles/gdt.dir/gdt/gdt.cpp.o\\n[  2%] Linking CXX static library …/…/libgdt.a\\n[  2%] Built target gdt\\nScanning dependencies of target ex01_helloOptix\\n[  3%] Building CXX object example01_helloOptix/CMakeFiles/ex01_helloOptix.dir/main.cpp.o\\n[  5%] Linking CXX executable …/ex01_helloOptix\\n[  5%] Built target ex01_helloOptix\\n[  6%] Building NVCC ptx file example02_pipelineAndRayGen/cuda_compile_ptx_1_generated_devicePrograms.cu.ptx\\n/home/squeen/Downloads/NVIDIA/optix/optix7course/example02_pipelineAndRayGen/devicePrograms.cu(28): warning: extern declaration of the entity optixLaunchParams is treated as a static definition[  7%] compiling (and embedding ptx from) devicePrograms.cu\\nScanning dependencies of target ex02_pipelineAndRayGen\\n[  8%] Building C object example02_pipelineAndRayGen/CMakeFiles/ex02_pipelineAndRayGen.dir/cuda_compile_ptx_1_generated_devicePrograms.cu.ptx_embedded.c.o\\n[ 10%] Building CXX object example02_pipelineAndRayGen/CMakeFiles/ex02_pipelineAndRayGen.dir/SampleRenderer.cpp.o\\nIn file included from /usr/include/c++/4.8.2/type_traits:35:0,\\nfrom /home/squeen/Downloads/NVIDIA/optix/optix7course/common/gdt/gdt/math/vec/functors.h:19,\\nfrom /home/squeen/Downloads/NVIDIA/optix/optix7course/common/gdt/gdt/math/vec.h:378,\\nfrom /home/squeen/Downloads/NVIDIA/optix/optix7course/example02_pipelineAndRayGen/LaunchParams.h:19,\\nfrom /home/squeen/Downloads/NVIDIA/optix/optix7course/example02_pipelineAndRayGen/SampleRenderer.h:21,\\nfrom /home/squeen/Downloads/NVIDIA/optix/optix7course/example02_pipelineAndRayGen/SampleRenderer.cpp:17:\\n/usr/include/c++/4.8.2/bits/c++0x_warning.h:32:2: error: #error This file requires compiler and library support for the ISO C++ 2011 standard. This support is currently experimental, and must be enabled with the -std=c++11 or -std=gnu++11 compiler options.\\n#error This file requires compiler and library support for the \\n^\\nIn file included from /home/squeen/Downloads/NVIDIA/optix/optix7course/example02_pipelineAndRayGen/SampleRenderer.h:20:0,\\nfrom /home/squeen/Downloads/NVIDIA/optix/optix7course/example02_pipelineAndRayGen/SampleRenderer.cpp:17:\\n/home/squeen/Downloads/NVIDIA/optix/optix7course/example02_pipelineAndRayGen/CUDABuffer.h:81:28: warning: non-static data member initializers only available with -std=c++11 or -std=gnu++11 [enabled by default]\\nsize_t sizeInBytes { 0 };\\n^\\n/home/squeen/Downloads/NVIDIA/optix/optix7course/example02_pipelineAndRayGen/CUDABuffer.h:82:28: warning: non-static data member initializers only available with -std=c++11 or -std=gnu++11 [enabled by default]\\nvoid  *d_ptr { nullptr };\\n^\\n/home/squeen/Downloads/NVIDIA/optix/optix7course/example02_pipelineAndRayGen/CUDABuffer.h:81:24: warning: extended initializer lists only available with -std=c++11 or -std=gnu++11 [enabled by default]\\nsize_t sizeInBytes { 0 };\\n^\\n/home/squeen/Downloads/NVIDIA/optix/optix7course/example02_pipelineAndRayGen/CUDABuffer.h:81:28: warning: extended initializer lists only available with -std=c++11 or -std=gnu++11 [enabled by default]\\nsize_t sizeInBytes { 0 };\\n^\\n/home/squeen/Downloads/NVIDIA/optix/optix7course/example02_pipelineAndRayGen/CUDABuffer.h:82:18: warning: extended initializer lists only available with -std=c++11 or -std=gnu++11 [enabled by default]\\nvoid  d_ptr { nullptr };\\n^\\n/home/squeen/Downloads/NVIDIA/optix/optix7course/example02_pipelineAndRayGen/CUDABuffer.h:82:20: error: ‘nullptr’ was not declared in this scope\\nvoid  d_ptr { nullptr };\\n^\\n/home/squeen/Downloads/NVIDIA/optix/optix7course/example02_pipelineAndRayGen/CUDABuffer.h:82:28: warning: extended initializer lists only available with -std=c++11 or -std=gnu++11 [enabled by default]\\nvoid  d_ptr { nullptr };\\n^\\n/home/squeen/Downloads/NVIDIA/optix/optix7course/example02_pipelineAndRayGen/CUDABuffer.h:82:28: error: cannot convert ‘’ to 'void’ in initialization\\nIn file included from /home/squeen/Downloads/NVIDIA/optix/optix7course/example02_pipelineAndRayGen/CUDABuffer.h:19:0,\\nfrom /home/squeen/Downloads/NVIDIA/optix/optix7course/example02_pipelineAndRayGen/SampleRenderer.h:20,\\nfrom /home/squeen/Downloads/NVIDIA/optix/optix7course/example02_pipelineAndRayGen/SampleRenderer.cpp:17:\\n/home/squeen/Downloads/NVIDIA/optix/optix7course/example02_pipelineAndRayGen/CUDABuffer.h: In member function ‘void osc::CUDABuffer::alloc(size_t)’:\\n/home/squeen/Downloads/NVIDIA/optix/optix7course/example02_pipelineAndRayGen/optix7.h:33:15: error: ‘runtime_error’ is not a member of ‘std’\\nthrow std::runtime_error(txt.str());                            \\n^\\n/home/squeen/Downloads/NVIDIA/optix/optix7course/example02_pipelineAndRayGen/CUDABuffer.h:45:7: note: in expansion of macro ‘CUDA_CHECK’\\nCUDA_CHECK(Malloc( (void)&d_ptr, sizeInBytes));\\n^\\n/home/squeen/Downloads/NVIDIA/optix/optix7course/example02_pipelineAndRayGen/CUDABuffer.h: In member function ‘void osc::CUDABuffer::free()’:…this goes one for hundreds of additional errors…8) HACK #3: To Hell with Cmake!I created a good old fashioned GNU Makefile (below) and was able to make an executable of example01!Ran it and got:ex01\\n#osc: initializing optix…\\n#osc: found 1 CUDA devices\\n#osc: successfully initialized optix… yay!\\n#osc: done. clean exit.Yippee! but…Example02 won’t build.building: example02_pipelineAndRayGen\\ncd example02_pipelineAndRayGen;g++ -std=c++11 -Wall -I. -I… -I…/common/gdt -I…/common -I/usr/local/cuda/include -I/usr/local/optix/include -o ex02 main.cpp -L.  -L… -L/lib64/ -L/usr/local/cuda/lib64 -lrt -lm -ldl -lcudartIn file included from LaunchParams.h:19:0,\\nfrom SampleRenderer.h:21,\\nfrom main.cpp:17:\\n…/common/gdt/gdt/math/vec.h:356:1: warning: multi-line comment [-Wcomment]\\n//#define _define_vec_types(T,t)    \\n^\\nIn file included from CUDABuffer.h:19:0,\\nfrom SampleRenderer.h:20,\\nfrom main.cpp:17:\\nCUDABuffer.h: In member function ‘void osc::CUDABuffer::alloc(size_t)’:\\noptix7.h:33:15: error: ‘runtime_error’ is not a member of ‘std’\\nthrow std::runtime_error(txt.str());                            \\n^\\nCUDABuffer.h:45:7: note: in expansion of macro ‘CUDA_CHECK’\\nCUDA_CHECK(Malloc( (void**)&d_ptr, sizeInBytes));\\n^\\nCUDABuffer.h: In member function ‘void osc::CUDABuffer::free()’:\\noptix7.h:33:15: error: ‘runtime_error’ is not a member of ‘std’\\nthrow std::runtime_error(txt.str());                            \\n^\\nCUDABuffer.h:51:7: note: in expansion of macro ‘CUDA_CHECK’\\nCUDA_CHECK(Free(d_ptr));\\n^\\nCUDABuffer.h: In member function ‘void osc::CUDABuffer::upload(const T*, size_t)’:\\noptix7.h:33:15: error: ‘runtime_error’ is not a member of ‘std’\\nthrow std::runtime_error(txt.str());                            \\n^\\nCUDABuffer.h:68:7: note: in expansion of macro ‘CUDA_CHECK’\\nCUDA_CHECK(Memcpy(d_ptr, (void )t,\\n^\\nCUDABuffer.h: In member function 'void osc::CUDABuffer::download(T, size_t)':\\noptix7.h:33:15: error: ‘runtime_error’ is not a member of ‘std’\\nthrow std::runtime_error(txt.str());                            \\n^\\nCUDABuffer.h:77:7: note: in expansion of macro ‘CUDA_CHECK’\\nCUDA_CHECK(Memcpy((void *)t, d_ptr,\\n^\\nmake: *** [ex02] Error 1More C++ mangled garabge — ever considered a nice, simple, little C-based tutorial with no custom data types, invented classes, utility libraries, etc. — just core language C99 and C CUDA?  Pedantic, yes…but kind to the new users too.Ouch!Got a bit further…Make again (with GNU Makefile above), now I get…building: example02_pipelineAndRayGen\\ncd example02_pipelineAndRayGen; g++ -std=c++11 -Wall -I. -I… -I…/common/gdt -I…/common -I/usr/local/cuda/include -I/usr/local/optix/include -o ex02 main.cpp -L.  -L… -L/lib64/ -L/usr/local/cuda/lib64 -lrt -lm -ldl -lcudart\\nIn file included from LaunchParams.h:19:0,\\nfrom SampleRenderer.h:21,\\nfrom main.cpp:17:\\n…/common/gdt/gdt/math/vec.h:356:1: warning: multi-line comment [-Wcomment]\\n//#define _define_vec_types(T,t)    \\n^\\n/tmp/ccB9cy4V.o: In function optixInitWithHandle': main.cpp:(.text+0x10): undefined reference to g_optixFunctionTable’\\nmain.cpp:(.text+0x1b): undefined reference to g_optixFunctionTable' main.cpp:(.text+0xaa): undefined reference to g_optixFunctionTable’\\n/tmp/ccB9cy4V.o: In function main': main.cpp:(.text+0x4a00): undefined reference to osc::SampleRenderer::SampleRenderer()’\\nmain.cpp:(.text+0x4a3b): undefined reference to osc::SampleRenderer::resize(gdt::vec_t<int, 2> const&)' main.cpp:(.text+0x4a4a): undefined reference to osc::SampleRenderer::render()’\\nmain.cpp:(.text+0x4aa2): undefined reference to `osc::SampleRenderer::downloadPixels(unsigned int*)’\\ncollect2: error: ld returned 1 exit status\\nmake: *** [ex02] Error 110) Fixed Makefile to include SampleRendeer.cppThe build command now looks like thisg++ -std=c++11 -Wall -I. -I… -I…/common/gdt -I…/common -I/usr/local/cuda/include -I/usr/local/optix/include -o ex02 main.cpp SampleRenderer.cpp -L.  -L… -L/lib64/ -L/usr/local/cuda/lib64 -lrt -lm -ldl -lcudartNow the only warnings/error isIn file included from LaunchParams.h:19:0,\\nfrom SampleRenderer.h:21,\\nfrom main.cpp:17:\\n…/common/gdt/gdt/math/vec.h:356:1: warning: multi-line comment [-Wcomment]\\n//#define _define_vec_types(T,t)    \\n^\\nIn file included from LaunchParams.h:19:0,\\nfrom SampleRenderer.h:21,\\nfrom SampleRenderer.cpp:17:\\n…/common/gdt/gdt/math/vec.h:356:1: warning: multi-line comment [-Wcomment]\\n//#define _define_vec_types(T,t)    \\n^\\nSampleRenderer.cpp: In member function ‘void osc::SampleRenderer::buildSBT()’:\\nSampleRenderer.cpp:303:35: warning: comparison between signed and unsigned integer expressions [-Wsign-compare]\\nfor (int i=0;i<raygenPGs.size();i++) {\\n^\\nSampleRenderer.cpp:316:33: warning: comparison between signed and unsigned integer expressions [-Wsign-compare]\\nfor (int i=0;i<missPGs.size();i++) {\\n^\\n/tmp/cc6Su9i8.o: In function optixInitWithHandle': SampleRenderer.cpp:(.text+0x0): multiple definition of optixInitWithHandle’\\n/tmp/cc3HnWt9.o:main.cpp:(.text+0x0): first defined here\\n/tmp/cc6Su9i8.o: In function optixInit': SampleRenderer.cpp:(.text+0xcb): multiple definition of optixInit’\\n/tmp/cc3HnWt9.o:main.cpp:(.text+0xcb): first defined here\\n/tmp/cc6Su9i8.o: In function osc::SampleRenderer::createContext()': SampleRenderer.cpp:(.text+0x80a): undefined reference to cuCtxGetCurrent’\\n/tmp/cc6Su9i8.o: In function osc::SampleRenderer::createModule()': SampleRenderer.cpp:(.text+0xa9b): undefined reference to embedded_ptx_code’\\ncollect2: error: ld returned 1 exit status\\nmake: *** [ex02] Error 1Seems like it might have something to do with this line in SampleRender.cpp (line#18-19)Hey, Squeen,Thx for reporting this - I’ll look into that right away. I do admit I’ve never tested CentOS, but am still a bit surprised because for most of my projects I do, and have rarely seen any (let alone that many) between CentOS on one side, and Ubuntu on the other (in particular, not recognizing uint32_t sounds weird).As said, I’ll look into it right away (in fact, just setting up a Cent7 machine for that). One question, though: You said “CentOS/Scientific” - would “CentOS 7 with latest updates” do the trick?We use Scientific Linux (7.7), but it’s essential CentOS 7.7, (which are both RHEL 7.7 with small tweaks).Thanks for looking at this.Another issue I am reminded of (with regard to the optix_stub.h header) is the following — in order to get a pure (-pedantic) C99 app to build the following changed was necessary to optixInit.NOTE: This dlsym casting is recommended by OpenGroup.org — dlsymBlocked out by the “#if 0” is what exists in the current OptiX 7 release (optix_stubs.h)Sorry; took a bit longer to set up a full developer box with centos 7, optix, cuda, etc, than expected, but the final fix was actually not too bad: The main issue was that the default gcc version on CentOS 7 is older than on Ubuntu, and was a bit more picky wrt things like C+±11 features.Anyway - latest version successfully builds on both CentOS 7.7 and Ubuntu 18.04. Haven’t tested U19 or Windows, yet, but will do so right away.Thanks again for reporting this; I’ll actually add the new CentOS machine as a gitlab runner as well, now.PS: Any further issues with this project feel free to (also?) post an issue on github/gitlab as well, that’s more certain to reach me. Thx again!I should respond to this publicly by saying that Ingo’s fixes have solved the build issue for me using CentOS 7.7.I did (as he suggests) add the following to my .bashrc file:then it’s just “cmake3 …” and “make” from the build directory, as he states on the Gitlab website.Many thanks to Ingo.  These tutorials have been invaluable in adding OptiX 7 capability to our in-house scene graph library—the should be fleshed out with some explanatory prose in an official PDF “Getting Started Guide” for the software.  They are well thought out and superior (IMO) to the other SDK examples that come with the OptiX download.  The key features are the logic progression (in complexity) and the minimization of dependencies (e.g. no global SDK “helper” functions that you have to hunt down and unravel).  Well done!Powered by Discourse, best viewed with JavaScript enabled\",\n",
       " 'optical-flow-newb-questions': 'I’m by no means an NVidia expert. I inherited some code to process incoming captured video frames, L/R stereo pairs, and all the video frames are stuffed into cv::cuda::GpuMats. Now, I need to calculate depth disparity maps from the L/R matched pairs. All of the cuda code that does image processing is referenced using my cv::cuda::Stream m_CudaStream. When I try creating the OF (optical flow) components at the start of my processing thread, it needs a CUcontext to pass to nvCreateOpticalFlowCuda. if I create this context by using cuDeviceGet(), followed by cuCtxCreate( &context, 0, cuDevice[0] ), it works find and OF initializes correctly, but all my cuda image processing code starts to fail. guessing that it because I created another context, after leaving the OF init routines, I called (just a guess), ::cuCtxPopCurrent( … ) and what do you know, my cuda code worked again. I don’t know what it even MEANS to swap in and out a cuda context.let’s suppose I can only have one context going at a time… I use the OF context to initialize the OF stuff. then I pop that context and get back my old, default one. I do some image processing on the L/R buffers. Now it’s time to calculate the depth disparity map.At this point, I have a pair of cv::cuda::GpuMats and I need to massage them into these very strange OFGPUBuffer things. My GpuMats are special - I’ve mapped them to DirectX textures. I don’t want to create the OFGPUBuffer and process those through my image processing routines through the cuda routines. I’m happier using GpuMats and at the end, performing a device-side-blit into the OFGPUBuffers, then calling the OF stuff to execute(). The trouble is, how do I perform that blit?Obviously, the below code won’t work. but any suggestions how to make it work?\\nKeep in mind I will probably have to swap contexts again to get the OF stuff to execute. Can somebody help explain this to me?Powered by Discourse, best viewed with JavaScript enabled',\n",
       " 'v-ray-performance-on-gpu': 'With 3ds Max 2018 and the V-ray 3.60, I ran a simple rendering job where the frame size is HD (1920x1080). On a Sandybridge 8-core cpu (with hyperthreading it is 16) which is rather an old CPU, the redering took 16 mintes.I also, ran that on a PNY Quadro M2000 and I checked that the GPU is fully loaded. It took 13 minutes to finish the job!So, does that comparison make sense?actually have the same question. So there are a lot commercial renderfarm [url]https://rentrender.com/gpu-render-farms-list/[/url] and CPU time cost more than CPU time. But when ask them about difference all says something like that “All depends on … 1000000 detail”Assuming that this is a single GPU installed in the system, there is a load placed on the GPU as a display device as well.\\nPlease reference this information Documentation - Chaos Help\\nThere are options for hybrid mode and optimizing your workflow to improve render performance.\\nThanksThe Benchmark tool:Test free the render performance of your hardware with V-Ray® Benchmark. See how fast V-Ray renders on your machine using our free standalone app.To compare results with others:\\nhttp://benchmark.chaosgroup.com/cpuTop of the line NVIDIA GPU benchmarks:V-Ray GPU Benchmarks on Top-of the-Line NVIDIA GPUsI ran the official V-ray benchmark on both CPU and GPU.CPU:\\nXeon 8-core (16 threads)\\nSandybridge 2660 (v2)\\nPrice is $190 https://www.newegg.com/Product/Product.aspx?Item=9SIAC3K5202140RAM:\\n1 DIMM\\n16 GB\\nDDR3GPU:\\nPNY M2000 (maxwell)\\n700 cores\\nGDDR5\\nPrice is $400 Amazon.comMB:\\nTyan 7059  Barebones FT77AB7059 B7059F77AV6RNow what was the result?![b]  CPU time: 3 minutes and 11 seconds. See the screen shot at Pasteboard - Uploaded Image.\\nGPU time: 5 minutes and 21 seconds. See the screen shot at Pasteboard - Uploaded Image.[/b]\\nIs there any idea about the bad result of GPU? It is really strange.please review the GPU and CPU configurations that others have posted with similar benchmark runtimes here …\\nhttp://benchmark.chaosgroup.com/gpu/details?hw=Intel(R)%20Xeon(R)%20CPU%20E5-2630%20v4%20%40%202.20GHz%20x20%2C%20Quadro%20M2000%204096MB&id=2383So that actually confirms what I achieved.\\nAn important question now is that why a $400 gpu defeated from a $200 cpu while the cpu is rather old and the application is actually GPU friendly.\\nI have another experience with Abaqus (engineering software) where the M2000 runtime was about 25 minutes while using 4 cores of the cpu had 10 minutes.May I ask In what circumstance, using M2000 is beneficial?Hi,Mansoor from Nvidea Tech support chat has sent me here with my issue.\\nFor some reason my graphics crash and I get artefacts all over my screen when using Vray for CPU based rendering. I find this bizarre as the GPU is only under 3-4% load. What is causing this and how do we fix it?Ive tried all of Chaos Group suggestions. Ive spent £800 on upgrading my system. Ive had my system fully diagnosed and every piece of hardware tested. The repair shop fixed it by installing an older version of windows 10. I installed the latest drivers and my software. The issue is there again. This only happens when I do CPU rendering with Vray ADV 3.6. GPU rendering is fine. GPU simulation is fine with realflow.Powered by Discourse, best viewed with JavaScript enabled',\n",
       " 'vkenumeratephysicaldevices-returns-1-vkphysicaldevice-with-3-1080s-installed': 'Hello,I’m having trouble using Vulkan with 2 of my 3 GPUs. I’ve got three 1080 cards installed:\\n±----------------------------------------------------------------------------+\\n| NVIDIA-SMI 381.22                 Driver Version: 381.22                    |\\n|-------------------------------±---------------------±---------------------+\\n| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\\n| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\\n|===============================+======================+======================|\\n|   0  GeForce GTX 1080    Off  | 0000:03:00.0     Off |                  N/A |\\n| 26%   37C    P0    42W / 180W |      0MiB /  8114MiB |      0%      Default |\\n±------------------------------±---------------------±---------------------+\\n|   1  GeForce GTX 1080    Off  | 0000:04:00.0     Off |                  N/A |\\n| 27%   43C    P0    42W / 180W |      0MiB /  8114MiB |      0%      Default |\\n±------------------------------±---------------------±---------------------+\\n|   2  GeForce GTX 1080    Off  | 0000:81:00.0     Off |                  N/A |\\n|  0%   40C    P0    39W / 180W |      0MiB /  8114MiB |      2%      Default |\\n±------------------------------±---------------------±---------------------+However I receive only 1 physical device during the query (Pardon my rust code):\\nlet mut len = 0;\\nassert_eq!(vk::SUCCESS, self.vk.EnumeratePhysicalDevices(self.id, &mut len, ptr::null_mut()));\\nlet mut result = Vec::with_capacity(len as usize);\\nresult.set_len(len as usize);\\nassert_eq!(vk::SUCCESS, self.vk.EnumeratePhysicalDevices(self.id, &mut len, result.as_mut_ptr()));\\nprintln!(“{} physical GPU device(s) installed”, len);This always reports 1 device installed. I’m running Ubuntu 16.04 server (w/o X) on Vulkan 1.0.46. I’m aware that I won’t be able to use device sharing unless I explicitly asked for the extensions to load, I just want to target a particular GPU and run code on it without sharing any resources. I am not loading any extensions.FWIW Here is my entire install procedure (minus my app) after a brand new Ubuntu install:\\nsudo -i\\necho blacklist nouveau | tee -a /etc/modprobe.d/blacklist-nouveau.conf\\necho blacklist lbm-nouveau | tee -a /etc/modprobe.d/blacklist-nouveau.conf\\necho options nouveau modeset=0 | tee -a /etc/modprobe.d/blacklist-nouveau.conf\\necho alias nouveau off | tee -a /etc/modprobe.d/blacklist-nouveau.conf\\necho alias lbm-nouveau off | tee -a /etc/modprobe.d/blacklist-nouveau.conf\\necho options nouveau modeset=0 | tee -a /etc/modprobe.d/nouveau-kms.conf\\nupdate-initramfs -u\\napt install -y build-essential linux-image-extra-virtual module-init-tools\\ncurl -L -o /tmp/NVIDIA.run http://us.download.nvidia.com/XFree86/Linux-x86_64/381.22/NVIDIA-Linux-x86_64-381.22.run\\nsh /tmp/NVIDIA.run --accept-license --no-questions --ui=none\\ncd /tmp\\ncurl -L -o /tmp/Vulkan.run https://vulkan.lunarg.com/sdk/download/1.0.46.0/linux/vulkansdk-linux-x86_64-1.0.46.0.run\\nsh ./Vulkan.run\\ncd /tmp/VulkanSDK/1.0.46.0/x86_64/lib\\n(tar cf - . | (cd /usr/lib; tar xf -))\\nrm /usr/lib/libVkLayer*Vulkan API Version: 1.0.46INFO: [loader] Code 0 : Found ICD manifest file /etc/vulkan/icd.d/nvidia_icd.json, version “1.0.0”Instance Extensions\\tcount = 12\\nVK_KHR_surface                      : extension revision 25\\nVK_KHR_xcb_surface                  : extension revision  6\\nVK_KHR_xlib_surface                 : extension revision  6\\nVK_KHR_display                      : extension revision 21\\nVK_EXT_debug_report                 : extension revision  5\\nVK_KHR_get_physical_device_properties2: extension revision  1\\nVK_KHX_device_group_creation        : extension revision  1\\nVK_KHX_external_memory_capabilities : extension revision  1\\nVK_KHX_external_semaphore_capabilities: extension revision  1\\nVK_EXT_display_surface_counter      : extension revision  1\\nVK_EXT_direct_mode_display          : extension revision  1\\nVK_EXT_acquire_xlib_display         : extension revision  1‘DISPLAY’ environment variable not set… Exiting!John, Thanks for reporting this issue. I have raised this issue with our engineering team.Wen Su,Thank you. I should note that I still see this issue with the 384.47 beta driver.We have fixed this issue in upcoming r384_00 driver.Hi Sandip,I can confirm this is fixed for me in the 384.59 driver. Thanks!Powered by Discourse, best viewed with JavaScript enabled',\n",
       " 'vehicle-raycasts-seems-to-affect-other-vehicles': 'Hi, i’ve been noticing a strange behavior in my racing game.I have multiple vehicles, and when one of the vehicles turns upside down, or go up to high, or get out of the scene (falls from defined ground, for example), it seems to affect other cars raycasts, by making them to sink into the ground, and their stand only in the ground supported by the body of the vehicle itself, not the wheels… it looks like, if a vehicle wheels fails to get a valid raycast (don’t find ground), somehow affects other cars wheels.Not sure why this happens… i’m using physx 3.3.0.I’m doing the raycasts (PxVehicleSuspensionRaycasts(…)), followed by PxVehicleUpdates(…), on every step.I’ve taken too much time to figure this out myself, but simply can’t understand what’s happening…Any help would be appreciated!Thanks!It sounds like the raycasts for each vehicle stop returning a hit with the ground when one of them falls out of the world or goes upside down.  This makes me think that they are all sharing the same raycast results.  Is this plausible?  For example, if your track was perfectly flat it might not be immediately clear that they are all colliding sharing the same raycast result because the result would be correct for all vehicles.  Is your track a flat plane?Thanks,GordonHi Gordon, thanks for the reply. I’ve followed that lead just after noticing this… the track is not all flat.I’ve done numerous tests (also visual ones) that show me that they are not sharing the same raycast results. I found a workaround for this - set an extremely high value for the suspension spring. This keeps affected vehicles above the road, instead of sink into the ground, and drivable, but it’s not the perfect scenario… this also shows that affected vehicle have hits with the ground, contrary to vehicle that may be upside down or out of scene.Any other thought that may occur to you?Thanks!\\nCarlos V.I can’t think of anything obvious.  It’s worth doing some quick experiements, though, to help diagonose the problem.  The first would be to update each vehicle individually rather than pass an array of vehicles to PxVehicleUpdates.Let’s say you have an array of N vehicles and you currently do the following:PxVehicleSuspensionRaycasts(batchQuery, N, vehicleArray,  etc etc);\\nPxVehicleUpdates(params, other params, N, vehicleArray);You could do the following instead:for(int i = 0; i < N; i++)\\n{\\nPxVehicleSuspensionRaycasts(batchQuery, 1, &vehicleArray[i],  etc etc);\\nPxVehicleUpdates(params, other params, i, &vehicleArray[i]);\\n}orfor(int i = 0; i < N; i++)\\n{\\nPxVehicleSuspensionRaycasts(batchQuery, 1, &vehicleArray[i],  etc etc);\\n}\\nfor(int i = 0; i < N; i++)\\n{\\nPxVehicleUpdates(params, other params, i, &vehicleArray[i], etc etc);\\n}It would be interesting to see the results with different update orders.Thanks,GordonI’m actually raycasting and updating one at a time, like one of yours suggestions. I already tested also raycasting and updating all vehicles at once, with no luck…I’ve tried just now your last suggestion, with no luck too :(Just weird stuff going on here ;)Thanks!The next step would be to use pvd to visualise the raycasts.  This will let you see the extent and direction of every suspension raycast, along wtih the hit triangle, position and normal.  My guess is that the error is in the raycast rather than the vehicle update.GordonHi Gordon, i will test it like that… thanks for the suggestion!Carlos V.Hi Gordon,I’ve never used PVD before, so i went looking for info about it… unfortunaly i think it will be necessary Visual Studio and Windows to use it… well, i just use Linux so i guess this is not an option for me right now.Could you lead me to how to get the raycasts information so that i can print them out myself?Thank You!Carlos V.Hi,Pretty much everything of relevance is available for inspection in PxWheelQueryResult immediately after callling PxVehicleUpdates.The extents of the most recent raycast are stored in these member variables:PxVec3 suspLineStart;\\nPxVec3 suspLineDir;\\nPxReal suspLineLength;The results of the most recent raycast are stored here:PxVec3 tireContactPoint;\\nPxVec3 tireContactNormal;Hope this helps,GordonDear Gordon, Dear Carlos,\\nI have exactly the same issue in my current game.\\nthe issue occur only with a lot of vehicles and never with only one.\\nI think i have investigated more deeper than you. This is the result of my last investigations:\\nThe raycast results seems to be ok, and the code in process Suspire Wheels seems to work fine. The issue seems to be located in susp limit constraint.\\nI strongly suspect an bug on the management of Px1 Constraint when we have a lot of constraints ( we can have a maximun of 4 constraint per vehicles, and the static const PxU32 MAX_CONSTRAINTS = 12 look a lot optimistic).\\nearly in the thread carlos write : “I found a workaround for this - set an extremely high value for the suspension spring.”\\nWith this workaround, the susp limit constraint is never needed, and the bug don’t occur.\\nLike Carlos, I need a real bugfix, i can’t keep hardest springs in my game.\\nGordon, have you any idea ?Regards,\\nDavid AllozaHi,\\nI put here the solution:This bug is fixed on last PhysX version.If you use an old version, to fix this issue you must remove the line:[i]data->mSuspLimitData.mActiveFlags = false;in file : PxVehicleSuspLimitConstraintShader.hregards,\\nDavid Alloza.Edit…Edit…Powered by Discourse, best viewed with JavaScript enabled',\n",
       " 'find-edges-of-a-polygon': 'Good afternoon,Is there a way to get the edges of a certain polygon based on its vertexes - or any other information for that matter?Thanks in advancePowered by Discourse, best viewed with JavaScript enabled',\n",
       " 'using-a-fixed-joint-to-simulate-crane': 'Hi all,\\nI’m trying to simulate a crane using fixed joints. The user can move the hook of the crane, and when the hook is near the load I move the load in the right position then I crate the joint.\\nThis joint could be broke and recreated many times by pressing a button, but when the joint is recreated again its setup doesn’t match a fixed joint anymore. It behavies like a distance or a spherical joint causing the load rotation along an horizontal plane.\\nI don’t know why the load is moving like this, have you any suggestions?Thanks, AndreaPowered by Discourse, best viewed with JavaScript enabled',\n",
       " 'how-to-use-render-target-in-directx9': 'I want to use .fx effect file that use off screen render target.The effect file have 2 passess . Do i need to create the off screen render target manually at my application? or does the scripting create the needed render target? Can you please provide me small demo application that uses effect file with 2 passess and use rendertarget .Powered by Discourse, best viewed with JavaScript enabled',\n",
       " 'resizebuffer': 'Hi, I studied hard about optiX.I have questions while studying and I want to ask a question.I have modified the optiX tutorial sample basically to perform the calculations I want to do, and the calculations are progressing more smoothly than I thought.Before this topic, I didn’t mentioned about what I do and how I modified the code.I’have study about radiative heat transfer, view factor using optiX ray tracing.\\nSo, I calculated the viewfactor using classic raytracing and got the accuracy of the old papers.\\nBut the error is as big as I thought. We think that it is necessary to carry out an extended analysis through detailed division.I am getting the projection of the required object based on the hit area, and doing the recalculation by expanding only that part.Partially there is a problem, but my problem is divided into mathematical problem and coding. What I would like to ask about this topic is the size of the buffer.here’s resize code I used:\\n1. Read the buffer and calculate hit area of object2. ResizeBuffer3. related variables, structuresThis code works well based on generally calculated values.\\nBut sometimes it does not work. At that time, it involves the following error.OptiX Error: ‘Unknown error (Details: Function “_rtContextLaunch2D” caught exception: Encountered a CUDA error: cudaDriver().CuEventSynchronize( m_event ) returned (719): Launch failed)’The problem is that sometimes it does not work. I am wondering what steps I need to take to overcome this problem.“Launch failed” is a generic CUDA error. The provided information is not sufficient to tell what’s going wrong.Please always list the following system configuration information when asking about OptiX isuses:\\nOS version, installed GPU(s), VRAM amount, display driver version, OptiX major.minor.micro version, CUDA toolkit version used to generate the input PTX, host compiler version.Your rtBufferUnmap() is at the wrong place!\\nIf sub == true, your ARS() function resizes the buffer you have currently mapped.\\nThat’s illegal and should fail with RT_ERROR_ALREADY_MAPPED if you reach that code.That wouldn’t have happened if you map() and unmap() buffers for the shortest possible time.\\nMeans move the rtBufferGetSize() and all temporary host allocations out of the map/unmap block.\\nThe buffer only needs to be mapped directly before the for-loops and can be unmapped immediately afterwards, before the ARS() call.Note that after resizing a buffer in OptiX, the data inside it is undefined and for input buffers you need to upload it again.Other things:I don’t expect any of the above changes to solve your issue though, well, maybe the incorrect resize location since sub == true, but that should have resulted in an error during the resize and invalid accesses or out of bounds exceptions in the device code.Thanks for the advice.\\nI am checking my problem with the advice you gave me.Inspired by your advice, I think I will study harder.In fact, there are a few more important issues.It is part of box_closest_hit_radiance () of tutorial. :This is sometimes seen as the ‘ffnormal’ of another surface normal if the hit point penetrates through the opaque surface of the object.It also shows a similar case when the hit point is located at the boundary of the geometry. Of course, this problem seems naturally arises, not only optiX’s.\\n\\xa0\\nI work with people but I feel always study and work lonely.\\nI am happy that there is someone who can ask something. Thank you.Sure, coplanar faces cannot be resolved with a single ray test.\\nDepending on the floating point precision and traversal order it’s either one or the other.\\nThat’s the same effect as depth bleeding in a rasterizer.Here’s one possible solution:\\n[url]https://devtalk.nvidia.com/default/topic/930666/radiation-physics-problems[/url]The Raytracing Gems Book [url]http://www.realtimerendering.com/raytracinggems/[/url] contains an article about self-intersection avoidance, but that won’t help with coplanar faces either.\\nThe better solution would be to not have any coplanar faces in the scene, but let all geometries be a bounding surface between two volumes.thank you. This is what I am looking for and need. I do not know if this can solve the problem I have now, but I think it will solve many of the fundamental questions I had.Powered by Discourse, best viewed with JavaScript enabled',\n",
       " 'mdl-sdk-binary-releases-and-versioning': \"Hi!There have been MDL SDK releases on GitHub in January and last December. The release notes always speak of ABI compatibility with a binary release of the same name. However, when I head to https://developer.nvidia.com/mdl-sdk, I only find releases dating back October '21 and older. Is this intended?Secondly, I’m a bit confused when it comes to the versioning shown on the web page.\\nMDL SDK 2021.0.2 - October 2021\\nMDL SDK 2021.0.1 - September 2021\\nMDL SDK 2021.1.0 - June 2020\\nMDL SDK 2020.1.2 - December 2020I think the third entry should be MDL SDK 2021.0 which was released in June 2021.Hi pdelg!\\nwe currently have some trouble signing the binary release which is why it is missing for the last version. we hope to resolve this within a few days and will release a minor update both on Github and binary.the December release was indeed skipped. If this is urgent i could make it available, but it was only a minor update.\\nI will check the version number and get it corrected where necessary! Thanks for the catch!cu\\nJanThanks for the update @jjordan!@jjordan, I see that there is a new binary release available. Great, thanks!Regarding the versioning: “MDL SDK 2021.1.0 (June 2020)” has been correctly adjusted to “MDL SDK 2021.0 (June 2020)”. But the year should still be 2021, as can be seen from this GitHub release: Release MDL SDK 2021 (344800.2052): 01 Jun 2021 · NVIDIA/MDL-SDK · GitHubSecondly, the October release version number has been changed from “2021.0.2” (correct) to “2012.0.2” (incorrect).And the March release “2012.1.1” should be “2021.1.1”.Sorry for that, the web team is a bit under pressure. I will file bugs for that. Thanks for reporting!\\nYes, we have a new version. And we actually will upload yet another version 2021.1.2 which mainly adds the missing Mac M1 build.\\nThanks pdelg!Powered by Discourse, best viewed with JavaScript enabled\",\n",
       " 'cxr-function-crashing-call-with-a-null-pointer-dereferenced-cloudxr-ver-3-0-sdk': 'Working in Unity I’ve managed to get the right native android EGL pointers with the library I wrote, as far as I can tell, but now it’s crashing on the cxr function call with a ‘null pointer dereferenced’ error message.\\nWe don’t have source for the underlying function call itself, so I’m not sure what exactly is causing it to spin out. Open to ideas re: debugging- Stack trace attached here\\nstack_trace.txt (3.6 KB)I don’t know much about Unity CS->native bridging, but if you send me a snippet of your descriptor setup and call into native, I can take a quick look and see if anything jumps out.cxrCreateReceiver itself is fairly lightweight.  For the most part, it does a ton of sanity checking of the receiverDesc + deviceDesc (and callbacks), then allocates an internal object and returns it as a void*.  I don’t see many places we’d be dereferencing a null pointer, but if we were getting a ** instead of a * for instance that would obviously blow up. :)The other possibility might be the shareContext.  But I don’t think we use it until later calls.Hi Here is a stripped back version of the wrapper code that just tries to call the function giving us problemsHere is a stripped back version of the wrapper code that just tries to call the function giving us problems\\nCloudXRWrapperThin.cs (14.8 KB)a few quick thoughts.otherwise best guess is one of the pointers isn’t getting marshalled properly (like one is a ** and the other is a *…).Alternative approach is to develop your own shim around cloudxr, and do all the interesting bits in your own C++ code/plugin.  So the surface exposed in C# becomes minimalist.Thanks will try it out and get back to youPowered by Discourse, best viewed with JavaScript enabled',\n",
       " 'reportsliceoffsets': 'How to use reportSliceOffsets in NVENC encoding? enableEncodeAsync  must be false?Powered by Discourse, best viewed with JavaScript enabled',\n",
       " 'cant-find-nvidia-linux-x86-64-525-60-13-run': 'The open kernel github page has had the 525.60.13 release out for several days now. But the documentation implies that one must have the 525.60.13 proprietary release installed before installing the open kernel release.  I can only find NVIDIA-Linux-x86_64-525.60.11.run, so I am presuming that installing the open kernel 525.60.13 release will cause  problems.  Is this correct?Was this answered somewhere else?Same problem here…Powered by Discourse, best viewed with JavaScript enabled',\n",
       " 'drawing-in-one-draw-call-vertex-buffer-with-different-programs': 'I am trying to figure out how to implement the following concept with modern OpenGL API (4.3)I have a vertex buffer which has a regular vertex array.But I need half of its data to process with ShaderProgram A, and another half - with ShaderProgram B.Currently what I do is creating two different VAOs with vertex attribute pointers pointing to related parts of the vertex array.But in this case I must issue 2 draw calls -one per VAO.Can I do it with a single draw call?P.S: I thought of primitive restart,but AFAIK it doesn’t switch shaders on each restart.Powered by Discourse, best viewed with JavaScript enabled',\n",
       " 'how-to-detect-tegra-device-from-system-info-under-android-jellybean-4-3': 'Up until now we’ve been following the sample code “nv_syscaps” which opens “proc/config.gz” and looks for the line:CONFIG_ARCH_TEGRA=yHowever (so far on the Nexus 7) with stock Android 4.3 update installed, proc/config.gz no longer exists (the kernel seems to have been built without that option enabled to have that file output). Curious if there might be some other way (from the cpuinfo or otherwise) to detect that the device has a Tegra SoC without having to initialize a GL context and look at the “RENDERER” string (which does contain Tegra)?For example, if I do a “cat proc/cpuinfo” inside an adb shell on the Nexus 7, I see a couple things I am not sure of what they mean (and I can’t find info about them), “CPU Variant” and “CPU Part” - are those by chance Tegra-specific identifiers that could be used when proc/config.gz is not found?Beyond that I could resort to special-casing the Nexus 7 as a “whitelist” device in our Tegra detection code, but that would break if other devices start removing config.gz as well from their stock kernels.Has anyone at nVidia or otherwise run into broken Tegra detection code with the stock 4.3 update yet (specifically on the Nexus 7 so far but not necessarily limited to that device) and if so, how did you address it?TIAFWIW - i fixed this by adding a fallback case to call eglInitialize() and then make a call to glGetString(GL_RENDERER) and checking the resulting string for “Tegra”that seemed to do the trick if “config.gz” could not be found. it looks like eglInitialize() is reentrant and is not refcounted so looks like it’s ok to call that ahead of the actual EGL init code.By “eglInitialize()”, do you mean “go through the entire EGL initialization sequence”?Because when I tried just eglInitialize(), glGetString() crashed with “no context!”Powered by Discourse, best viewed with JavaScript enabled',\n",
       " 'vrworks-360-video-sdk': 'when will this sdk be available to download?Is this a lie？Powered by Discourse, best viewed with JavaScript enabled',\n",
       " 'openexr-not-found-in-cmakelists': 'Dear all,I want to run the optix samples from optix SDK 7.4 and in the INSTALL-LINUX.txt, it says run ccmake. Then I got the warning as:CMake Warning at lib/ImageReader/CMakeLists.txt:39 (message):\\nOpenEXR not found (see OpenEXR_ROOT).  Will use procedural texture in\\noptixDemandTexture.It is a warning but I cannot generate makefiles then. So I have installed openexr using command:\\nsudo apt-get install openexr\\nsudo apt-get install libopenexr–devBut still it cannot find the root dir of the openexr. What should I configure for the path of OpenEXR?I have only found OpenEXR header files in my /usr/include. I have tried to export OpenEXR_ROOT= /usr/include/OpenEXR. But still I cannot generate make file. And I am using Ubuntu 20.04 LTS. If any other information needed, I will add then:)Many thanks in advance!Best regards,\\nLongHi @xueyun.long,I’ve tried building the OptiX 7.4 SDK samples on Ubuntu 20.04, and I see the OpenEXR warning, but cmake still produces a “Makefile” anyway. If you start with ccmake and get the warning, you just need to [c]onfigure again one more time, and then the [g]enerate option should appear and become available. Have you tried that?–\\nDavid.Hi David,oh I have tried and thats true! Thank you very much!!\\nWish you a nice week!LongFor posterity, if you do want to compile with OpenEXR support, I think the right answer is what you already tried. For me it works when I set OpenEXR_ROOT=/usr/include/OpenEXR, after installing libopenexr-dev.I swear I tried that path 2 days ago just like you did, and saw the same behavior - cmake error. But today when I create a clean new build/ folder and use that path it works. So maybe we both had a bad cmake cache state or something. It might be the case that we needed to clear our cmake cache and/or start with a fresh build/ folder. Just making a note of this in case anyone else hits the same problem.–\\nDavid.Yes, that is the same situation for me. I have created a new build folder and then the path works. Thank you very much!This topic was automatically closed 14 days after the last reply. New replies are no longer allowed.Powered by Discourse, best viewed with JavaScript enabled',\n",
       " 'enquire-about-hdcp-srm-error': 'Hi there,I’m writing to enquire about HDCP SRM error.Our company is developing Windows Protected Media Path(PMP) player and plug-ins.Our application purpose is apply a Output Protection using NVIDIA Geforce GTX 650 Ti Graphic cards. (for use with HDCP)PMP player require to MFPROTECTIONATTRIBUTE_HDCP_SRM when we applied HDCP.We input “About DCP | Digital Content Protection” HDCP.SRM sample data.\\n[Sample SRM Data]\\n80 00 00 01 00 00 00 2B D2 48 9E 49 D0 57 AE 31\\n5B 1A BC E0 0E 4F 6B 92 A6 BA 03 3B 98 CC ED 4A\\n97 8F 5D D2 27 29 25 19 A5 D5 F0 5D 5E 56 3D 0EBut we had received error message “0xC026251E ERROR_GRAPHICS_OPM_DRIVER_INTERNAL_ERROR”What is HDCP_SRM??? which forwarding a data? Sample SRM? or the other data?Powered by Discourse, best viewed with JavaScript enabled',\n",
       " 'debug-fragment-shader-on-gtx-980m': 'I have an opentk 4 application.  My machine is an MSI with a GTX 980m.    I’m having trouble with textures.   Can NSight help me debug a simple fragment shader?   My texture is a color gradient.  And I’m using this single line of code in my fragment shader to pick a color.  Is there a way to debug this?  If I run NSight via Extensions->NSight-> Start CUDA Debugging (Legacy) it disconnects immediately upon startup.Powered by Discourse, best viewed with JavaScript enabled',\n",
       " 'opt-out-of-variable-refresh-rate-g-sync-support': 'Normaly (no variable refresh rate display), you can request an Immediate present interval, and the video card will display your frame as fast as possible without waiting.  This causes tearing.\\nIn D3D9, this was done by using D3DPRESENT_INTERVAL_IMMEDIATE, and drawing with tearing is also possible in other graphics APIs (OpenGL, D3D12, etc)But if you have a variable refresh rate display (such as G-Sync), the meaning of “presenting immediately” changes.  The driver interprets each present call as if it is a full complete frame, and wants to wait for the display hardware to be ready for another frame.Is it possible to get the “Immediate” behavior where there is zero delay with tearing,  and not the “variable-refresh rate” behavior?Powered by Discourse, best viewed with JavaScript enabled',\n",
       " 'wgl-nv-dx-interop-with-windows-mixed-reality': 'I’m trying to port an existing OpenGL application to run on a Windows Mixed Reality platform, using WGL_NV_DX_interop.  Working up through the various examples step by step, things have gone well.However, I’ve hit a wall when I finally try to render into a WMR back buffer (via Windows::Graphics::Holographic::HolographicCamera).  When I call wglDXRegisterObjectNV() for the back buffer, I get the debug message “GL_INVALID_OPERATION error generated. Invalid dimensions.”The reason for this seems to be that the back buffer object is actually a 2-layer texture (for stereo rendering).  This cannot change, since for WMR the back buffer is owned and controlled by the system.Is there a switch somewhere that I can flip to enable interop support for 2-layer textures?If not, what would be the possibility of getting an update for WGL_NV_DX_interop that addresses this scenario?  Even if it’s limited to only supporting 2-layer textures it would be fine I think.Or should the support already be there and I’m probably doing something wrong?  I know you can’t tell what I’m doing without sample code, but it’s a whole Universal Windows Platform project.Any help appreciated!More info…I find an updated interop spec (v2) kronos.org:https://www.khronos.org/registry/OpenGL/extensions/NV/WGL_NV_DX_interop2.txtThat document specifies in the wgl.objtypes table that TEXTURE_2D_ARRAY should be supported.However, on the NVidia site, the version 1 spec is posted (exaclty matching khronos v1):http://developer.download.nvidia.com/opengl/specs/WGL_NV_DX_interop.txtWhich version do the NVidia drivers support (GeForce GTX 980 Ti, v391.01)?Could you please share the examples of the codes that are running openGL in windows mixed reality ?\\nI am struggling to run opengl on HLPowered by Discourse, best viewed with JavaScript enabled',\n",
       " 'quatro-p2200-display-card-error-unable-to-load-info-from-any-available-system': 'Dear Sir,we are experiencing the following problem with our setup using the Quatro P2200 display card:Operating System: Oracle solaris 10 5.10\\nmain board:d3358-A1X\\nkernel:SUNos 5.10 Generic_150401-13\\nmodel:Fujitsu Celsius R940Monitor1: FUJITSU Display B24W-7 LED\\nMonitor2: FUJITSU Display B24W-7 LED\\nMonitor3: Turbo-X Android TV TXV-AU6580SMT 65\" 4Κ Ultra HDSetup: We are trying to change the resolution of the 3rd monitor to 4K, but we can’t since every time we run nvidia-settings we get back from the system: “ERROR: Unable to load info from any available system”we have installed driver version SOLARIS DISPLAY DRIVER – X64/X86Version: 440.82\\nRelease Date: 2020.4.7\\nOperating System: Solaris x86/x64\\nLanguage: English (US)\\nFile Size: 98.29 MB\\nbut we are unable to get the resolution on the monitor mentioned above beyond 1920x1080\\nAlso we have verified that the monitor is working fine, because we have make it work in 4k with another pc that is identical but have older graphic card.Powered by Discourse, best viewed with JavaScript enabled',\n",
       " 'cant-find-the-vk-nv-ray-tracing-extension-on-my-rtx2080': 'Hello, as of last week VK_NV_ray_tracing has stopped appearing as a device extension for my rtx2080 despite having tried both updating and downgrading my gpu drivers. Currently I’m using the latest gpu driver and Vulkan sdk.I’m not completely sure if this is just a Vulkan issue or whether both DX12 and Optix are also broken but (and this may just be incompetency on my part) I wasn’t able to get ray tracing to work in UE4 or the DX12 ray tracing samples https://github.com/Microsoft/DirectX-Graphics-Samples/tree/master/Samples/Desktop/D3D12RaytracingAlthough when a friend tried running Vulkan Info with his rtx2070, he also wasn’t able to see VK_NV_ray_tracing in the device extensions list so that makes it seem like it’s probably a software issue? Either way what’s the best approach in this situation? Is my gpu broken and I should rma it or is this a software issue? If it wasn’t for my friend’s 2070 also not showing up I’d be pretty sure it’s a hardware issue. Anyway I’ve already tried contacting nvidia support and they just pointed me here after telling me to do the usual “update/downgrade everything”.Hopefully this can be resolved soon!Quick update, I got DXR to work so it can’t be my gpu. Which I guess means that vulakn’s lacking rtx support right now. Any chance someone could confirm this and who needs to be bothered so that vulkan gets rtx support again?have you tried the latest Nvidia driver to check the Vulkan RTX support？Yes but that wasn’t the issue. I should have posted once I got it working. Basically I was using CLion and had to use the Visual Studio compiler or it wouldn’t work and since CLion’s debugger isn’t compatible with Visual Studio I switched it to MinGW and broke it… So pretty much it broke because I was an idiot and I fixed it by pure accident and got it working. Although Vulkan info still doesn’t show the ray tracing extension so I’m not sure why that’s the case but hey at least my code works!Powered by Discourse, best viewed with JavaScript enabled',\n",
       " 'opengl-4-4-very-slow-opengl-1-1-very-fast-performance-problem-quadro-k4200-k2000': 'Are there any known issues with the Quadro K4200/2000 card running on Dell Precision Workstations (dual 8 core E5-2630 Xeon and single 6 core Xeons), WHQL 354.56 driver on Windows 10 that would cause the performance of OpenGL 4.4 to be much slower than the same identical data, displayed on OpenGL 1.1 on the same hardware?  A similar problem has been observed on the GeForce 740M with WHQL 361.43 driver.OpenGL 1.1 using the old, deprecated, client arrays is extremely fast.  First update is in the range of 0.32 sec with subsequent updates on the same data in 0.04-0.09 sec.  The code processes all data the same way on each update and does not cache any display data.  With OpenGL 1.1, the video card appears to be caching triangles in the GPU after the first update. OpenGL 4.4 performance appears gated and is 5-10 times slower, typically OpenGL 4.4 display times are in the 0.93-1.25 sec per update, each update takes about the same amount of time in OGL 4.4.The same data run on the HP Envy notebook (i7 4900HQ) with a GeForce 740M card with the 361.43 driver Win 10, using OGL 4.4 is 2-3x faster than the Quadro K4200M card at between 0.46-0.62 sec.  Similarly the update time on the HP notebook using OGL 1.1 is much faster.  After the first update the update time is in the 0.04-0.12 sec range.Our application (x64) has just been ported from OpenGL 1.0 forward to a dual system using OpenGL 1.1 for legacy cards and OpenGL 4.4 for newer cards that can support persistent, coherent, pinned memory mapping through glMapBufferRange.  Where possible on non-OpenGL calls, the 2 code paths are sharing commmon code.  The 2 code paths are sharing common code for building buffers of data to display.  The 2 versions of OpenGL very carefully follow separate paths where necessary to set up the correct OpenGL state for each version.  No deprecated functions are being called in the OGL 4.4 code path.  Simple, smooth shading with a single directional light and no material properties are being used.Unfortunately, nsight 5.0 doesn’t work on this app - it seems to have the same stability issues other users were reporting in November.\\nThe Visual Studio 2015 profiler results are inconclusive and vary strangely depending on how the profiler is run (sampling, instrumented, run from exe project or one of the app’s dlls).Both paths use the ARB create context, the compatability profile, and the same pixel format.  Everything displays correctly and consistently on both versions.Build Tools:  Application Built with Visual Studion 2015 Update 1 - x64 build, MFC app with CodeJock, mostly single threaded at this point.Systems have plenty of RAM 16GB to 32 GB - OS and app use ~6GB when running.Screen Resolution: 1920x1080 (both workstations and HP notebook).  Workstation has dual monitors, only displaying on one monitor.Function Wrangler is a small C++ object created from the libEpoxy OGL 4.5 header files which were extracted from the Khronos definition.  The Epoxy dll had build issues on Windows.  The app is being careful to make a specific function pointer table per context, etc.  Seems to work fine.ARB Profile:   Compatability (runs a bit faster than the Core 4.4 profile)\\nPixel Format:  10 (Quadro), Full hardware acceleration, Double buffered, RGBA, 24 bit depth, 8 bits each R,G,B,A, 8 bit stencil buffer\\nDisplay Type:  GL_TRIANGLES (fill, shaded)\\nPolygon Mode:  GL_FRONT_AND_BACK, GL_FILL\\nDraw Buffer :  Back\\nSwap Call:     SwapBuffers\\nDraw Call:\\n- OGL 1.1 glDrawElements with a simple sequential index\\n- OGL 4.4 glDrawArrays passing pointer to pinned memory subbuffer.  Have also tried glDrawElements with OGL 4.4 - same performance as glDrawArrays.\\nData Buffer Size:\\n- OGL 1.1 a maximum of 1026 vertices or 342 triangles per glDrawElements call.\\n- OGL 4.4 have tried a range of sizes from the 1026 size to 32769 bigger is somewhat better.Lighting:  Single directional light with diffuse, ambient terms only.Vertex Data:  Double precision vertices, normal and texture data and 4 bytes of RGBA loaded per vertex and converted to float (same data as for OGL 1.1 path)\\nShader: Have tried both 2 stage (vertex, fragment with math on vertex) and 3 stage (vertex, geometry, fragement with triangle math in geometry shader)\\nHave also tried the simplest possible pass thru shader which made little or no difference in the OGL 4.4 performance.\\nShaders are loaded at startup and compiled and linked once.\\nFor lighting 2 4x4 float matrices and 4 float light parameters are loaded as uniforms.  The pass through shader loads one matrix.Persistent GPU Buffer:\\n- Triple buffer approach, shown as the best method on various web sites and examples\\n- Memory allocated one time and held for duration of the run.\\n- glFenceSync used as recommend with appropriate wait when changing buffers.\\n- Keeping data in local buffers and doing a single memcpy to pinned  memory for the GPU to avoid many small transfers and possible performance issues that might cause.  memcpy is done just before the glDrawArrays call.Data Size:  130,000 - 260,000 triangles is typical load for these tests.OpenGL 4.4 Code snippet on buffer creation (one time only):typedef struct\\n{\\nGLdouble\\tgldVertex3D[3];\\nGLdouble\\tgldNormal3D[3];\\nGLdouble\\tgldTexCoord2d[2];\\nGLubyte\\t\\tglubColorRGBA[4];}GLDrawGeometry;I appreciate any insight or advice on what could be causing this performance slowdown on the Quadro K4200 card on Windows 10 x64.  And why OpenGL 1.1 is faster on both the Quadro and the GeForce cards.Thanks,Elaine AcreeNever mind.  Fixed it.  Cast data to float just prior to calling glDrawArrays.\\nOpenGL 1.1 was taking the same data and apparently casting to float far more efficiently than OpenGL 4.4.  And far from a 50% performance hit, it was more like 5-10X performance hit depending on the card and driver combination.  It’s now doing wheelies in dynamics -).Powered by Discourse, best viewed with JavaScript enabled',\n",
       " 'optimus-did-a-wrong-choice': 'Greeting,First let me introduce myself, my name is Linda MacGill and I am a VFX Supervisor at BitBox, Ltd.We have a large amount of bug-reports (about thousands) from our notebook-players about wrong device being chosen by the NVIDIA® Optimus™ technology. The Optimus changes device to the integrated one instead of discrete adaptor. We have “NvOptimusEnablement” in our code. And the issue occurs on the default setting of the profile at the Control Panel.\\nSure, the customizing the profile fixes problem, but technically we can’t work with every customer to show how to set profile with a control panel. Many of them usually ignore the readme and trobuleshooting boards.\\nHow can we fix this issue from the “inside”?Most common reports from:\\nGeForce 710M\\nGeForce 670MXv\\nGeForce 650\\nGeForce GTX 880M\\nAll above 300+OS:\\nwin 7, win 8, win 8.1P.S: Furthermore, I consider that Carthage must be destroyedThanks for attention, Linda MacGill\\nVFX Supervisor,\\nBitBox Ltd.Powered by Discourse, best viewed with JavaScript enabled',\n",
       " 'update-to-optix-5-1-all-rays-enter-only-the-miss-program': 'Hi,\\nRecently, I’ve updated the OptiX version from 5.0.1 to 5.1.The programs compile and run, but the problem is that all rays land in the “miss” program. I didn’t change anything in the old code which was working before. So my question is if anyone has any intel on breaking changes that happened during the transition of the OptiX versions. I have read the release notes and didn’t find anything that would cause such a behavior.I’ve checked then launch_index in the camera program, also implemented the new setUsageReportCallback function to get all the info from OptiX and everything looks fine - no warnings or errors. Geometry is present and rays are being shot.Also, might be interesting:\\nOS: OpenSuse 42.3 Linux\\nGPU: Nvidia M5000M (8GB)\\nDriver: 396.24\\nCUDA: 9.0.176.4Edit: Messages printed with rtPrintf are reaching only from the ray_generation program. The rtPrintf calls inside mesh_bounds and mesh_intersect never show up! I renamed only the functions in the Cuda file and OptiX crashed accordingly, then I’ve set the new name in the Program creation and it was found again - so I suppose they are called but rtPrintf doesn’t work :/…\\nThe setUsageReportCallback reports:\\nLevel: 2, Tag: INFO, Message: Program cache HIT  : mesh_intersect\\nLevel: 2, Tag: INFO, Message: Program cache HIT  : mesh_boundsSo if anyone has an idea what might cause this problem I would be grateful for such information.First, can you build the set of SDK samples that ship with OptiX and check that optixMeshViewer and other samples work on the same system?  Just to rule out any compatibility issues with the driver.Then try loading geometry in your application that you know works, e.g., the cow mesh from the SDK, or a single triangle, or a unit sphere.  Often a disappearing mesh is caused by nan/inf vertices that break the BVH builder.I should have mentioned that I’ve built all the 5.1 samples and they were all working perfectly.I will try the cow as you mentioned but isn’t there a way to get info on a broken BVH from OptiX? I would expect some errors/exceptions from OptiX if nan/inf happens. Possible I missed to check a return value on an OptiX function - maybe you can point me to function names I could check? I’ve set setUsageReportCallback to 3 but it’s probably not the right place to look at.Any luck with the cow?A nan/inf vertex position could possibly mess up the bounding boxes for the acceleration structure, but it would still be “valid” as far as memory accesses, etc.  This would not be caught as an exception or error.  I’m shooting in the dark a little bit here – this could turn out to be something else entirely that is making geometry disappear.I will have some time by the end of this week to test the cow. Thanks a lot for your help!Powered by Discourse, best viewed with JavaScript enabled',\n",
       " 'how-to-make-nvidia-gt220-use-irq-instead-of-polling-to-detect-vertical-sync-when-vertical-sync-mode': 'Does NVidia GT220 use polling or IRQ to detect Vertical Sync when Vertical Sync mode is turned On?\\nIt seems to me, that it uses polling because turning On Vertical Sync mode causes 100% load of one thread of Intel i3 processor.\\nIf I am right, then how to make NVidia GT220 use IRQ instead of polling to detect Vertical Sync when Vertical Sync mode is turned On?Powered by Discourse, best viewed with JavaScript enabled',\n",
       " 'question-about-destruction': 'I’m trying use PhysX3.2 to simulate simple destruction.\\nThe debris was exported from Max.The shape use boxshape simply.But as the shape is not precisely,the debris shape often overlap with each other at the first time.If a small box overlaped with a large box, the smaller one will create a large speed,it looks not comfortable.Maybe because some debrises are part under terrain at first time, i don’t sure which results this appearance.\\nIs there any way to solve this question? Or could I set a max linear speed?\\nThank you very much!Sorry,It look like because models are scaled larger.\\nBut the movement looks much different from scale 1.0.\\nIs there any other properties should scale except the shape size?This sounds strange because it runs well on my side. Can you send me your sample? My email is juma@nvidia.com.This sounds strange because it runs well on my side. Can you send me your sample? My email is juma@nvidia.com.Sorry,it’s my stupid fault.The initialize position not scaled…Powered by Discourse, best viewed with JavaScript enabled',\n",
       " 'issue-of-using-nv-enc-buffer-format-yuv444-as-input-buffer-format': 'Hi,I first decompose the input image into YUV format and arrange it into “YYYYUUUUUVVVV”, then rearrange the image as the input of the encoder (also set the input format as NV_ENC_BUFFER_FORMAT_YUV444), and then encode the input data.But, in the CreatEncoder function NVENC_API_CALL (m_nvenc nvEncInitializeEncoder (m_hEncoder, & m_initializeParams));Prompt NVENCException!Tracking errors found that if chromaFormatIDC is set to 1, there is no exception, and chromaFormatIDC=3 in YUV444 generates an exception.Does anyone know how to fix this mistake?Thank you very much!Powered by Discourse, best viewed with JavaScript enabled',\n",
       " 'cannot-load-libnvidia-encode-so-1': 'I want  accelerated ffmpeg encoding with “h264_nvenc” codec.ffmpeg -i video.mp4 -vcodec h264_nvenc -crf 25 -acodec copy Output.mp4The above code works fine in “nvcr.io/nvidia/tensorflow:19.10-py3”,\\nbut in “nvcr.io/nvidia/cuda:10.1-cudnn7-devel-ubuntu18.04” it gives the following error message.\\nIf i change h264_nvenc to default encoder the code works fine.Why is this error occurring?\\nPlease tell me how to fix the error!Reproduce the problem situationThis is my nvidia-smi result.Hi.\\nIt looks like ffmpeg used is built with higher version of NVIDIA Video Codec SDK, but the driver you using is an older version.\\nThis error message comes from ffmpeg code nvenc_print_driver_requirement():libavcodec/nvenc.c\\nYou can refer to Using_FFmpeg_with_NVIDIA_GPU_Hardware_Acceleration.pdf from latest 9.1 SDK to know more about how to compile and use ffmpeg or you can switch to an older version of ffmpeg.Thanks.Powered by Discourse, best viewed with JavaScript enabled',\n",
       " 'where-is-nvidia-flow-1-1': 'This page is saying Flow 1.1 was released at GDC this year with new featureshttps://developer.nvidia.com/nvidia-flowHowever I cannot find any download pages or publications related to the release, does anyone know what happened to it?Thankssame questions…\\ninterested in new Vulkan/Linux support!\\nseems more Vulkan Gameworks enabled lbis are coming like Waveworks 2.0…\\nhope by Siggraph some libs get posted…Thank you for the interest in Flow Vulkan/Linux support!GitHub release was delayed a bit. Stayed tuned for a release within the next month.Hi,\\nthanks for offering an ETA…\\nwill keep and eye for it!So 7 months since “stayed tuned for a release within the next month”.\\nGithub still has 1.0.1.\\nWhat’s the deal ?Hi Chris,we had a high priority project come up that we had to staff; this is leading to the delay you are seeing on Flow development.  I cannot promise you any delivery dates right now, but rest assured that we are eager to get back to Flow development as soon as our team’s other deliverables permit it.Cheers,AdamPowered by Discourse, best viewed with JavaScript enabled',\n",
       " 'context-recompilation-callable-programs': 'Hi,\\nIn trying to optimize my program, I have narrowed down my bottlenecks to context recompilations.\\nIn the specific case in question, I call many successive launches, while switching callable programs in between calls.\\nI seem to recall in some thread and/or the programming guide that the whole point of such callable programs is that their switching does not incur recompilation, so I guess I am using them wrong somewhere.My flow is as follows:Since I preloaded the progs into the context and am just attaching them to the variable, does anyone have any insights why I am getting these expensive recompiles all the time?Thanks!Changing a single bound callable program at context scope requires a recompile because the resulting mega-kernel is different. Only the programs which can actually be reached from an entry point will be compiled into the  kernel at launch. It’s not enough that you created the program objects.Instantaneous switching between context wide callable programs can be achieved with buffers of bindless callable program IDs which build a function table you can index into to select the current function.\\nThat way the programs are all present inside the kernel and you only need to change the function table index variable between launches.Thanks Detlef.\\nThat makes perfect sense.\\nIf I understand correctly, the downside of this approach would be that the programs lose the scope of their caller? And I can manually overcome this by passing any needed attribute as a parameter?Correct. Bindless callable programs only have the context and the program itself as scope.\\nYou can also not call any rtTrace or rtTransform calls in them because those need the transform hierarchy which is not existent in the context and the program scopes.I’m heavily using bindless callable programs in my ray tracers because they actually help to reduce the kernel size by reusing fixed function code. I can handle arbitrary lens shaders with a single ray generation program and only need a single closest hit and any hit program because all materials and lights are configured, sampled, and evaluated via bindless callable programs.Thanks a lot.\\nI’ll give bindless a try then.Reporting back:\\nConverting my callable programs (~10) to bindless was a real pain, but the results are definitely worth it! By cutting the recompilations, I’ve cut down my flow (which includes 4 separate tracings) from 2.5s to 400ms!U may think of bound programs as inline functions, they will have different results according to not just their paramenters but different context environment witch is changing all the way. So different context environment need to recompile the kernel(bound programs).Bindless programs U may think of them as normal functions, same paramenters same result, nothing more. they need no recompiling.That is not correct and misleading. When OptiX needs to recompile kernels does not depend on the type of callable programs, but on the presence of code at launch calls, as I said above.So much thanks for correcting my misunderstanding. I really need such book or article to further learning. any suggestions will be appreciated.Hi z_Spark,The paper below gives me an overview of Optix, and I think it is helpful to understand how Optix works:1184.49 KBHIH,YashizPowered by Discourse, best viewed with JavaScript enabled',\n",
       " 'memory-grows-unless-calling-collide-advance-fetchresults': 'We don’t call the collide(), advance() or fetchResults() functions. We just create static and rigid actors and perform scene queries. In this case, memory usage continues to grow, even when calling flushSimulation(). Is there anything that can be done to reclaim this memory?Thanks.Hi,\\nwhat configuration of PhysX are you using? Does this happen even with release libraries? It might be possible that PVD does capture scene query data.Regards,\\nAlesI’m using PhysX 3.4. It happens in both debug and release, both with and without enabling PVD. I can see things like the transform cache continue to grow in size over time.What you could try is to attach shapes with only PxShapeFlag::eSCENE_QUERY_SHAPE. Then calling simulate/fetchResults should be cheap to call and the memory should get released. Additionally while calling the simulate/fetchResults a new scene query tree is build over time if objects move in your scene otherwise the current tree for the dynamic scene will become unbalanced over time.Additionally you can also disable the simulation on PxActor and removing those actors from simulation completely.Powered by Discourse, best viewed with JavaScript enabled',\n",
       " 'physx-repx-problem': 'Hey guys,I’m trying to upgrade from PhysX 3.2 to PhysX 3.4 and everything has gone smoothly, except for this problem I’m having with RepX. I have a .RepX file that I want to create a collection from, and then use that collection as a reference to which I can create multiple instances of. I’m able to create the collection, but I don’t know how to create “instances” of this collection.Typically what I would do (in PhysX 3.2) is:instantiateCollection(*mCollection, *mPhysics->PhysXSDK, *mPhysics->Cooking, mStringTable, itemAdder);but in PhysX 3.4 this is no longer a function.Much Appreciated,ConnorAnybody?Hi,There is a significant change to serialization from PhysX 3.2 to PhysX 3.4. Please take a look at our PhysX Guide (Serialization section) for the latest update.[url]https://github.com/NVIDIAGameWorks/PhysX-3.4/blob/master/PhysX_3.4/Documentation/PhysXGuide.chm[/url]Powered by Discourse, best viewed with JavaScript enabled',\n",
       " 'dlss-comparison-video': 'hello i am ozan from og games entertainment we have a big game we are working on right now and we are using nvidia technologies in this game we are planning to hold a showcase for our game in december and we would like to put a dlss comparison video in this showcase also we will place your logo on the showcase as we use your technologies dlss comparison Is there anything we should know about the video and would you mind if we use your logo, thank youHello @oggamesofficials and welcome to the NVIDIA developer forums!I am no legal expert, so the following is not official advice.With respect to using NVIDIA technology there is no problem in doing so as long as it is freely available tech for example as part of Unreal Engine or as an SDK you can access through our developer portal.If you make use of DLSS and want to showcase the advantages of it, you can do so as long as you are not making incorrect claims.In that light, after looking at your web page, I would suggest to you to be more cautious in your pictures and wording. “Many thanks to all the companies that provide their technologies to us” is only correct if your are working directly with representatives of those companies. Is that the case? “Providing” implies direct support, but my guess would be that NVIDIA does not directly support your game studio at this time.You can list which technologies you are using of course, but claiming support of those companies might backfire.Similarly the game Logos which depict UE, Epic and PS5. Usually this implies that you already have contracts with Epic and Sony to use their distribution platforms.You might want to look into how to publish your game on one of the platforms that have Indie studio support like Steam Direct. And NVIDIA also as a program to showcase exceptional games by independent developers.I hope that helps!we already filled this form but we didn’t get any feedback. we are really want to work with Nvidia. how can we contact you? is there any e-mail address for this.We are eagerly waiting for your reply. thanks for suggestions by the way.Hi again,You can imagine with the huge number of independent game developers out there that there are always a lot of applications to our programs. That means it can take a while until you get feedback from NVIDIA and only a very small selection of the best content do in the end get someone from NVIDIA to directly work with them.On the logo topic I wanted to add that I asked around and you should NOT use any official company logos on any publication before you have cleared that with the companies in question. I understand that can be difficult, but for example in case of a DLSS comparison video you can also DM me when your game and the video are done and I will try to get someone to review it. But I cannot promise anything, as I said, the teams are usually extremely busy.Thanks!Powered by Discourse, best viewed with JavaScript enabled',\n",
       " 'is-there-some-appropriate-method-in-physx-for-actors-activation-deactivation': 'Hi,For activation/deactivation of object’ collision, we need it to be an instant operation.\\nI thought PxScene::addActor/removeActor can be used but its cost can not be ignored.\\nThen I thought maybe I can use PxShapeFlag.I wonder that if there is some better choice for it.Thanks for any advise.Powered by Discourse, best viewed with JavaScript enabled',\n",
       " 'capturing-displaying-and-getting-a-raw-frame-matrix-to-work-from-with-gstreamer': 'I am trying to capture video in a    Jetson_nano with a Raspberry pi 2 camera, using python and gstreamer. I am new to gstreamer and did not find examples for this application, capturing a frame , display it, and getting a raw array of the frame  to work with it.\\nI tried initially to record the frame and I used :gst-launch-1.0 -vvv videotestsrc ! tee name=t ! queue ! x264enc ! mp4mux ! filesink location=xyz1.mp4but it only worked with videotestsrc, i need to use nvarguscamerasrc, but didn´t workHow can I do that?\\nHow can I get a raw matrix (in GBR or RBG) to work with?\\nI need help…\\nThankfully…Powered by Discourse, best viewed with JavaScript enabled',\n",
       " 'changing-z-buffer-value': 'To begin with, I would like to apologize for my bad English, and also for asking for help on this forum, not being a developer. I ask for help here, because I could not be helped by any other forum.I need to change the value of the Z-buffer (depth buffer) to 24 or 32 bits on the GTX760 graphics card, since I have the following problem: during the movement, I see flickering of textures in almost any game, to be more precise - flicker of textures of thin objects (fences, poles, etc.). Also I see flickering of the joints of textures.In YouTube I found a video on which someone showed a similar problem, and it shows it well. Here it is:No anti-aliasing settings and other settings do not solve this problem to me. After a long search, I finally found a few articles about similar problems and it was said that the problem is precisely the incorrect bit depth of this Z-buffer. Actually, the question is, how can I change it? In standard programs from Nvidia similar settings are not providedI understand that this forum is for questions of a different kind, so I apologize once again for having asked this question here, but I simply have nowhere else to seek help except this place…HiIt’s a general issue with lots of games.\\nPlease Nvidia, find a solution, this is giving me headaches.Powered by Discourse, best viewed with JavaScript enabled',\n",
       " 'error-running-optixpathtracer-7-1-0-sample-sometimes': '[ 4][       KNOBS]: All knobs on default.[ 4][  DISK CACHE]: Opened database: “C:\\\\Users\\\\0\\\\AppData\\\\Local\\\\NVIDIA\\\\OptixCache\\\\cache7.db”\\n[ 4][  DISK CACHE]:     Cache data size: “23.7 MiB”\\n[ 4][   DISKCACHE]: Cache hit for key: ptx-22283-key8c9a27d547cd8cd731f06a1f9c7f3be5-sm_75-rtc1-drv451.67\\n[ 4][COMPILE FEEDBACK]: Info: Pipeline has 1 module(s), 4 entry function(s), 2 trace call(s), 0 continuation callable call(s), 0 direct callable call(s), 26 basic block(s) in entry functions, 686 instruction(s) in entry functions, 7 non-entry function(s), 42 basic block(s) in non-entry functions, 616 instruction(s) in non-entry functionsGL error Invalid operation at D:\\\\programs\\\\OptiX\\\\OptiX SDK 7.1.0\\\\SDK\\\\sutil\\\\sutil.cpp(344): glClearColor( 0.212f, 0.271f, 0.31f, 1.0f )Caught exception: GL error Invalid operation at D:\\\\programs\\\\OptiX\\\\OptiX SDK 7.1.0\\\\SDK\\\\sutil\\\\sutil.cpp(344): glClearColor( 0.212f, 0.271f, 0.31f, 1.0f )And I find that if I don’t define DO_GL_CHECK in Exception.h, which means not doing GL_CHECK, everything works well…Powered by Discourse, best viewed with JavaScript enabled',\n",
       " 'request-for-extension-more-double-precision-functions-in-glsl': 'OpenGL spec only defines functions such as addition, multiplication, divison, sqrt and a few comparisons and clamp etc for double-precision. However, Nvidia hardware implements many more double-precision functions (such as trigonometric functions) and these are accessible from CUDA and OpenCL. I was wondering if there is a vendor extension that allows double-precision trigonometric and trascendental functions in GLSL. Particularly interested in using these in GL 4.3 compute shaders.If there is no such extension, please consider this a request for a new extension :)Powered by Discourse, best viewed with JavaScript enabled',\n",
       " 'desired-deterministic-movment-of-pxcontroller-time-stepping-or-something-similiar': '( Yes, I am finally back, I had much to do, sorry community :D)PhysX 3.3Hello,I want to move my PxBoxController when he hits a special sphere shape along a “path”.\\nThe idea: Move the PxBoxController via the move(velocity,deltaTime,…) function, with\\na precalculated velocity.\\nIt should be simple calculated but the results must be determinstic and shouldn´t vary.\\n(The part of collision etc works perfect, the only problem is the mDeltaTime calculation of force.)I know that my game is frame independent, and combining physics with apply forces to a kinematic object ( like the PxController) cant handle it right.\\nAppling forces to dynamic objects works perfect, all I need is to add the force.But for kinematic objects (maybe espacially the PxController) cant handle it right, so I multipled\\nmDeltaTime for a much better result when the fps are changing, but all I get are different positions.The goal is when I touch a special sphere ( from above) , I want to jump over it.\\nThis must be deterministic, when its not, its possible to “cheat” because the player could jump\\nfar or less away than it should.\\n(E.g. when he change windows or when he had a slow computer :D)\\nI dont know how to explain it better, but maybe this pic could help you:External MediaHere are the “important” code snippets:Again: The problem is that the player is “jumping” always to a new position, and not to a\\nposition which is always the same, mostly the changes are 0 - 1 units (in global position) or\\nwhen the delta time is very very very high, he could jump more than 10+ units away from the desired\\nposition. (I cant tell what the desired position is, - its calculated thru the gravity and jump force,- but this should alway be the same result.)\\nE.g: The PxController should move 20 units in his “face” direction, but he also moves up (jump) and falls down ( gravity) .I also tried adding force for a little while (frame independent!) but this also leads to vary results.My question is: How could I archieve my jumping movement which is always at changing frames deterministic and gives me the right result?!I guess one way is to call from the “hidden” PxActor inside PxController the\\nsetKinematicTarget(…) for position the PxController, but this shouldnt be the right way,\\nwe lost information like the possible collisionFlags while he is moving. ( See PxController::move())I want to get any collision information which could happen while\\nthe PxController is “jumping over” it, and I dont want to use a separate movement update for it.\\nBut maybe this could be a solution to the problem, but again, I dont want to use it unless\\nthere is a efficent and easy way via the “force” calculation.Hi,thank you for the link - i´ve read it and googled it, and found a better way:\\nRK4 should be better than TCV - look at this for a basic implementation:Introduction Hi, I’m Glenn Fiedler and welcome to Game Physics.\\nIf you have ever wondered how the physics simulation in a computer game works then this series of articles will explain it for you. I assume you are proficient with C++ and have a basic...But your sample code:\\nvelocity *= mDeltaTime * (mDeltaTime / mPrevFrameDeltaTime);Wont work corrctly, maybe its the way I calculate the mDelta time -\\nits only updated when the “main” loop are done, since I use a semi-fixed timestep\\n( for reasons, look at Fix Your Timestep! | Gaffer On Games )Should be the “real” mDeltaTime calculated when the physics are updating?\\nLike, I calculate the “real” mDeltaTime direct inside the\\nwhile (mAccumulator >= mStepSize) loop to get the real mDeltaTime and mPrevFrameDeltaTime from\\nthe physic update?I´ve tested it with the “real” physic delta time, and the “main game loop” delta time (which\\nincludes every possible delay…) but I wont get the same result.Do you have a hint or something else?UPDATE:I hope the TCV is usabe for changing acceleration (like gravity…).\\nOne of the questions are:\\nIs it okay to pass the (untouched) mDeltaTime for the elapsed (falling) time?\\nLike I did it here : mJumpTime += mDeltaTime*10;So - the other question is how do I calculate the mDeltaTime?\\nFor the whole update loop?\\n(PhysX and the whole updates etc are single threaded, so I dont have\\na multithreaded game - no asynchron “run overs” are possible.)One other thing is:\\nThe “untouched” mDeltaTime is passed to the PxController::move(…) - so this wouldn change it?Maybe I understood it wrong, but I guess I only need to multiply my mVelocity with the TCV -\\nwhich dont work ( results are almost the same with the “untocuhed” mDeltaTime).Still I get an incredible movement when the game “freezes” or run slow - huge new velocity are result\\nand the PxController moves this distance.— The nice thing is that this behavior is only on the PxController (maybe each kinematic object)-\\nthe dynamic objects are easy to handle, just add the desired force and you are done.Hi,look at the code sample in the first post, its fixed timestep but I call it semi fixed, because\\nI cut it at some point, as a workaround for the “spiral of death” which is possible while using accumulated time.It makes no sense for me to update the controller again, before I call simulate()while (true)\\n{\\n…\\t\\nmObjectManager->update(); //Here is the PxController going to update\\nmPhysic->update();\\n…\\n}But nevermind, I´m doing it.So, I update my PxController in the loop before the physic update, and like I said, it doesnt change anything and make no sense (but the PxController moves faster, and almost anything blow up).I even testet it with different mDelta, from the “whole game update delta time” and with the\\n“physic deltaTime” - they lead to mixed results which is unusable like the “usally”\\ndeltatime calculation for movement.Hmm…I´ve implemented the TCV as a test case:\\nTry it and you will see that TCV wont work correctly when mDeltaTime is changing.This is a 1:1 port from the excel file created by Jonathan “lonesock” Dummer,\\nwhich was the first link in your post. I´ve found it somewhere on google.\\n(Of course, there is no graph…, but the algorithm is the same)So I hope there is a solution to this problem.And I dont want to use an infinity unefficently workaround way…\\n(Like creating a hidden actor which is an untouchable shape which moves like the\\nPxController should, and I set the hidden actor position as my PxController position…)Or other weird solutions :DHi,maybe its worth to say that the PxController is a PxExtension - so its independent from\\nthe PxScene::simulate() and fetch() - it still can be updated via PxController::move() and\\nit collides with all PxShape´s even when the physic wasn´t updated.\\n(The PxScene is unimportant the PxController works independent from the PxScene…)So - I guess this brings a new way to think about the solution.Hi,so, I dont know if this is the right way, but I have found a possible and simpel solution:\\nThe mistake was mJumpTime += mDeltaTime*10; ← When I didnt pass the mDeltaTime, just only a constant,\\neverything works very well, the results are deterministic even with greatest delay or no fps something else, it works like I want it. Okay, maybe its not the right way to do it, but this is perfect for me.Maybe its the right way …\\n(or not, because its linear, but this is the elapsed time, which should be linear, but its dependend how often the update() is called…)\\nbut I pass the mDeltaTime to the PxController which do some stuff to make it right :D (or maybe not :D)Powered by Discourse, best viewed with JavaScript enabled',\n",
       " 'poco-notfoundexception-thrown-in-driver-version-531-18': 'I installed latest driver (531.18) through GeForce Experience. I develop graphics applications using DirectX 11. I am using Visual Studio 17.5.1. When calling D3D11CreateDevice using a non-null IDXGIAdapter, found by adapter enumeration, and requesting D3D_FEATURE_LEVEL_11_1, a POCO exception is thrown. The call stack shows I am in the NVIDIA driver (nvwgf2umx.dll) and it is calling a function in MessageBus.dll. If I continue execution I get a second POCO exception. Continuing execution, my applications run but on exit when I release the ID3D11Device object, another POCO exception occurs. My code has been working for many years, so this exception was unexpected. Is the use of POCO new to drivers starting with 531.18 ? I am running Windows 11 (latest) with an NVIDIA GeForce RTX 2080 SUPER.Same exception here.  This is with an RTX 4080 16GB.  My Visual Studio version is 2022 however.  I also have an AMD CPU on this box (3900X) if that’s of any relevance.  Windows 11 also.Am now going to try the studio drivers.EDIT: Current studio drivers are of course fine being an earlier iteration (528.49).Hello to both of you @deberly and @RobinsonUK, welcome back to the NVIDIA developer forums.POCO has been part of the drivers for a while now, this sounds more like a possible regression to me, just not clear where.@deberly would you mind sharing a call stack of the first occurrence of the exception if that is possible?Ideally i would ask for a minimal repro app, but maybe engineering can repro already based on this information.Thanks!One addendum, can you rule out that this was related to some other update that coincided? Like Windows or VS updates?Thanks!I will make an attempt to write a minimal repro. However, here is a link to a screen capture with the call stack and the code window. The mFlags input to D3D11CreateDevice is 0. POCONotFoundExceptionI do not believe I will be able to rollback the Windows or Visual Studio updates on my main dev machine. I have another machine on which I can install the 531.18 driver. That machine has Windows 10 and not the latest Visual Studio. I will report the results with the minimal repro.A simple repro is provided by the project in this zip file: POCONotFoundException.zipI am running withWindows 11 Pro 22H2 OS build 22621.1265. I am using MSVS 2022 version 17.5.1. The project configuration has “Windows SDK Version: 10.0 (latest installed version)”, which on my machine is 10.0.22621.0.The MSVS 2022 default Exception Settings, C++ Exceptions, is to have “<All C++ Exceptions not in this list>” unchecked. In order for the exception dialogs to pop up, you need to check this item. I have comments in the code indicating where the exceptions occur.I have run into the same issue with a visual studio 2019 calling D3D12createDevice  throws the poco::NotFoundExceptionThank you @codehammer and also welcome to the NVIDIA developer forums!All the details are forwarded, if I get status updates from engineering, I will share if I receive public information.Thanks!Exception thrown at 0x00007FFA0BC24B59 in DirectX11Test.exe: Microsoft C++ exception: Poco::NotFoundException at memory location 0x000000A28A2F15A0531.61 st’ll not running on win10 VS2019 x64I’m working on a project that involves using the D3D11CreateDevice function, but I’m running into an issue. When I call the function, I get a Poco::NotFoundException. I’m not sure what’s causing this exception, and I’d appreciate any help or suggestions on how to resolve it.For what it’s worth, compiling and running the Godot game engine using VS2022 results in these errors also. I created an issue on the Godot GH tracker before I realized it seemed more driver-related than Godot related.You can find more details here: Godot always throws Poco::NotFoundException · Issue #76707 · godotengine/godot · GitHubPoco::NotFoundException in VS 2022. Its not godot problem.The Poco exception is still thrown as of NVIDIA GeForce driver 532.03. I suppose a fix for this is low priority. For other folks, here is the workaround when using Microsoft Visual Studio. I should have thought of this previously…This assumes you have checked the box in MSVS Exception Settings window that says “<All C++ Exceptions not in this list>”. If you do not have this checked, set a breakpoint at a line of code before the call to D3D11CreateDevice and run to the breakpoint. Then check the aforementioned box. Continue the execution and an exception dialog is launched when the call is made to D3D11CreateDevice. That dialog has a box checked “Break when this exception type is thrown”. Uncheck the box. You will see a new item in the Exception Settings window under the “C++ Exceptions” that says “Poco::NotFoundException”. It is unchecked, which means the debugger will not stop execution for that particular exception.This exception is still thrown in the latest driver 535.98 (release date: 2023.5.30).It is not related to Godot engine, because it also happens in other engines - in my case in Ab3d.DXEngine (DirectX 11 rendering engine for .Net).I am using Windows 10 with NVIDIA 3080 Ti.This should be easy to reproduce. I do not understand how this is not fixed after 3 months!Having the same exception thrown on teardown instead, with VulkanI created an account just to complain about this error as well. In driver version 535.98 (the latest NVIDIA driver as of this post) this same exception appears with my GeForce RTX 4060 Laptop GPU. I even get this error in Microsoft’s own DirectX 12 sample programs! No errors appear if I choose to create the device with Intel’s integrated Xe GPU instead as an adapter in the same DirectX 12 program. This is entirely the fault of NVIDIA’s own drivers. I am immensely disappointed. NVIDIA has done nothing for over three months and multiple driver updates to fix such a prominent issue.Thank you @qvindicator for joining the NVIDIA developer community, hopefully you find more reasons to be part of it.Also to @abenedik a warm welcome.I am aware this has been going on for a long time, but I hope you understand that I cannot share very much of the ongoing debugging process. I can only stress again that there are people working on this, trying to find a fix.Thanks!I use the visual studio 2019( Version 16.11.27) and my  RTX 2026 with driver 536.23 , when I debug following code,  the D3D12CeateDevice with throw the same exception.Anyone found the solution to this? do you just revert back to the drivers before this started happening?  I am getting Poco::NotFoundException at memory location when callingAnd then\\nThrowIfFailed(D3D12SerializeRootSignature(&descRootSignature, D3D_ROOT_SIGNATURE_VERSION_1, pSignature.GetAddressOf(), pError.GetAddressOf()));\\nthrows the error Poco::NotFoundException. It works in debug, but not in release.@cpyburn78 I installed 528.49 and no longer see the problem. I’m hoping they get this fixed for newer drivers soon.Powered by Discourse, best viewed with JavaScript enabled',\n",
       " 'there-are-singularities-in-the-optical-flow-generated-by-nvidia-optical-flow-sdk': 'Hi all,I am totally a freshman as NVIDIA API user.I am confused to discover that there are some singularities in the optical flow generated by NVIDIA Optical Flow SDK, like at the top of attachment “grove3_nvidia_flows.png”.\\nFor comparison, groud truth is uploaded as “grove3_ground_truth.png”.I just compiled the Optical_Flow_SDK_1.0.13 and run the command on 2080Ti:Did I do something wrong?Many thanks in advance for your consideration.\\n\\ngrove3_ground_truth.png880×320 57.1 KB\\n\\n\\ngrove3_nvidia_flows.png880×320 18.2 KB\\nI noticed the same thing.  I ran tests a couple months ago with sample images that I translated by various amounts (both integer and fractional translations using texture lookups with linear filtering) up to 128 in X and Y.  The errors correlate to the “speed” (“slow” has fewer and smaller than “medium”, which in turn has fewer and smaller than “fast”).  Larger displacements generate more and larger errors and errors along the left edge as well.Any insights from the nvidia devs would be welcome.And I see the same issues with the 3.0 SDK on both the RTX 2080 and 3060.Powered by Discourse, best viewed with JavaScript enabled',\n",
       " 'vulkan-support-missing-from-freebsd-driver': 'Hi,From what I have seen, the FreeBSD driver is delivered without the vulkan libraries/ICDs on versions that should support it. This feature was advertized as a cross platform support and is sorely missing from my setups as vulkan-only software begin to be pushed.Can we expect this situation to be resolved, I understood that in the past the FreeBSD project undertook many changes that would allow the NVidia driver to use the same codebase as the Linux one, it feels also frustrating when other solutions begin to have decent support, like it was for OpenCL…Regards,Powered by Discourse, best viewed with JavaScript enabled',\n",
       " 'forcing-v-sync-on': 'For various reasons, we would like to NOT allow V-Sync to be disabled (i.e. always force V-Sync). It looks like despite setting the correct “presentation interval”, some of our users are playing with the setting off and getting tearing artifacts etc.Is there a way to force the setting ON? Or will the driver/control panel always be able to override us?Thanks!Francoisim not sure if your saying people are changing it from NVcontrolpanel. if that is you could restrict the other user from accessing the controlpanel and force vsync always on from the admin account.Powered by Discourse, best viewed with JavaScript enabled',\n",
       " 'issue-with-inline-raytracing-committedtrianglefrontface': 'Hi, im having an issue when i use the call to RayQuery::CommittedTriangleFrontFace in a inline raytracing context all the other calls to RayQuery::Committed*** functions return 0.\\nThe same code executes fine and calls to RayQuery::Committed*** return the right values when RayQuery::CommittedTriangleFrontFace is not called.\\nIs anybody having this same issue or knows what could be wrong?Im running on:Here goes a sample code snippet:\\nRayDesc ray;\\nray.Origin = origin;\\nray.Direction = direction;\\nray.TMin = 0.01;\\nray.TMax = 1000.0;RayQuery<RAY_FLAG_NONE> query;\\nquery.TraceRayInline(bvh, 0, 0xFF, ray);\\nquery.Proceed();if (query.CommittedStatus() == COMMITTED_TRIANGLE_HIT)\\n{\\n//const bool isFrontFace = query.CommittedTriangleFrontFace(); // uncomment this makes all the other calls to return zero\\nconst uint primitiveIdx = query.CommittedPrimitiveIndex();\\nconst uint geometryIdx = query.CommittedGeometryIndex();\\nconst float2 barycentrics = query.CommittedTriangleBarycentrics();\\n… code …\\n}Thanks\\nNunoLooks like the underlying issue has recently been detected and picked up.When using inline raytrace, after calling proceed on a ray query, using the call… to RayQuery::CommittedTriangleFrontFace makes all the other calls to RayQuery::Committed*** return zeros or invalid. Not using the call to RayQuery::CommittedTriangleFrontFace makes the other calls return meaningful values.The issue results from not using the Proceed call return value.\\nPossible workaround:instead ofPowered by Discourse, best viewed with JavaScript enabled',\n",
       " 'some-questions-about-foveated-rendering': 'I just wanted to ask some questions about VR and Foveated Rendering.\\nFirstly, what fps do you need to make VR stable and useable, because I have seen many websites saying that no VR setup reaching below 90fps is suitable? Is this actually true or is this just a way for companies like oculus to support various other GPU and CPU producers? I normally use graphics software running real time Ray/Path tracing at around 25-30fps and it feels completely stable, and I do realise that the main fps decreasing factor for VR is having to render two viewpoints simultaneously. Would 25-30fps be useable, obviously not from a gaming POV, but from more of a immersive visual experience? So what I mean is not the kind of VR where you are constantly having to duck turn and jump around shooting things, but instead the kind of graphics visually display based VR experience. So would 30fps be suitable or not?\\nSecondly, with foveated rendering, can you decrease the fps of the visuals outside of the periphery? Would you be able to render all of the display around the eye at 60-70fps whilst rendering everything else at around 25, or can you only decrease resolution not fps?\\nWhat I am mainly trying to achieve is photorealism within VR. I am looking into Omni verse software, and am seeing very high end realism. I put this post in the VR section because the majority of the questions are based around that.\\nI have access to two high end 3080 PC’s, and my question is, could you use one PC and one 3080 to render each eye instead of splitting rendering load between a single GPU? I am looking for the kind of photorealism shown in the Omni verse Marbles At Night demo. I understand that a 3090 is significantly more powerful than a 3080, however because I have access to two 3080’s, I decided that I could use VR to render the image(s) due to the implementation of eye tracking and foveated rendering. Because foveated rendering roughly decreases resolution to 1/3 of the average, could I get fps stable enough to visualise it in VR properly without breaking immersion or having very bad motion sickness? The Marbles Demo was run at 1440p on a 3090, so could I get it to run at 4k for periphery, then minimum (whilst still accurate for foveated render) for the rest of the display, at around 30fps or higher?\\nJust a few questions, thxI may have some thoughts for your first and second query. (First) although there is no standard fps for VR headsets, modern VR are constantly improving the fps because higher frames lead to better immersion. I actually do not remember the paper title, it is from MIT/Stanford, they suggest 1800 Hz is the appropriate refresh rate for VR to match with real-world visualization. In that sense, 90+ fps has become a standard.  (second) You cannot have two different frame rate in a single scene (fovea, and periphery) because it will lead you to jittering effect, and the flickering will also increase. Do not forget, due to dense rod distribution, the peripheral vision is more sensitive to motion.Powered by Discourse, best viewed with JavaScript enabled',\n",
       " 'if-there-are-mixing-yuv-image-sample-code-in-cuda': 'I have found scale yuv image sample code in nvidia_video_sdk(CNvEncoderLowLatency::ScaleNV12Image),if there are mixing yuv image sample code?I need to mixing 1080p with 320p image in CUDA,thanksWhat’s the meaning of “mixing”?Powered by Discourse, best viewed with JavaScript enabled',\n",
       " 'geforce-driver-bug-on-dx11-causing-flickering-and-other-problems': 'I’ve reported this problem few years ago, when you still had another website:\\nhttp://nvidia-submit.custhelp.com/app/account/questions/detail/i_id/768361\\nAnd I’m disappointed that the problem still hasn’t been fixed.There’s a problem on the GeForce on DX11, causing flickering and other issues.I’m attaching the app for you to test:Dropbox is a free service that lets you bring your photos, docs, and videos anywhere and share them easily. Never email yourself a file again!If you run on GeForce 650m GT you will see significant flickering.\\nIf you run on any non GeForce GPU (Intel or ATI) you will see it’s working fine.If I slightly modify the shader, to move the “Color[0]” constant to the Vertex Shader, then the problem goes away.Expected results:\\n\\n\\nWhat I’m getting:\\n\\n\\n\\n\\n\\n\\n\\n\\nApplication code:Shaders:This is causing significant problems in the engine and the game that I’m developing.\\nThank you for your help.Bump.This works OK on Intel/ATI, but not on GeForce.Thank you for your help.I tested your DX11 code on my Geforce GTX 1050 (driver 388.59)  and it works ok. No flickering.Thanks for the reply.I’ve tested on a friend’s laptop with 1050m and works fine too.However on 650m GT I’m having flickering, updated drivers today (390.77).I remember that when I submitted the problem to the older Nvidia website, I’ve got confirmation from Nvidia employee that they were able to reproduce the problem on their machine as well. Perhaps they’ve used 650m like me.DxDiag:Microsoft Graphics Hybrid: Supported\\nDxDiag Version: 10.00.16299.0015 64bit UnicodeDirect3D:    0/4 (retail)\\nDirectDraw:  0/4 (retail)\\nDirectInput: 0/5 (retail)\\nDirectMusic: 0/5 (retail)\\nDirectPlay:  0/9 (retail)\\nDirectSound: 0/5 (retail)\\nDirectShow:  0/6 (retail)Device Problem Code: No Problem\\nDriver Problem Code: Unknown\\nDisplay Memory: 1792 MB\\nDedicated Memory: 32 MB\\nShared Memory: 1760 MB\\nCurrent Mode: 1920 x 1080 (32 bit) (60Hz)\\nHDR Support: Not Supported\\nDisplay Topology: Internal\\nDisplay Color Space: DXGI_COLOR_SPACE_RGB_FULL_G22_NONE_P709\\nColor Primaries: Red(0.629406,0.350109), Green(0.349133,0.614758), Blue(0.154797,0.112805), White Point(0.313977,0.329602)\\nDisplay Luminance: Min Luminance = 0.500000, Max Luminance = 270.000000, MaxFullFrameLuminance = 270.000000\\nMonitor Name: Generic PnP Monitor\\nMonitor Model: unknown\\nMonitor Id: LGD0323\\nNative Mode: 1920 x 1080(p) (59.975Hz)\\nOutput Type: Internal\\nMonitor Advanced Color Capabilities: None\\nDisplay Pixel Format: DISPLAYCONFIG_PIXELFORMAT_32BPP\\nDriver Name: igdumdim64.dll,igd10iumd64.dll,igd10iumd64.dll\\nDriver File Version: 10.18.0010.4653 (English)\\nDriver Version: 10.18.10.4653\\nDDI Version: 11.2\\nFeature Levels: 11_0,10_1,10_0,9_3,9_2,9_1\\nDriver Model: WDDM 1.3\\nGraphics Preemption: DMA\\nCompute Preemption: Thread group\\nMiracast: Supported\\nHybrid Graphics GPU: Integrated\\nPower P-states: Not Supported\\nDriver Attributes: Final Retail\\nDriver Date/Size: 07-Apr-17 13:00:00, 11158160 bytes\\nWHQL Logo’d: n/a\\nWHQL Date Stamp: n/a\\nDevice Identifier: {D7B78E66-4226-11CF-DE61-AD35B4C2C735}\\nVendor ID: 0x8086\\nDevice ID: 0x0166\\nSubSys ID: 0x15A71043\\nRevision ID: 0x0009\\nDriver Strong Name: oem65.inf:5f63e5341859ec8c:iIVBM_w10:10.18.10.4653:pci\\\\ven_8086&dev_0166\\nRank Of Driver: 00D12001\\nVideo Accel: ModeMPEG2_A ModeMPEG2_C ModeWMV9_C ModeVC1_C\\nDXVA2 Modes: DXVA2_ModeMPEG2_VLD  DXVA2_ModeMPEG2_IDCT  DXVA2_ModeVC1_D2010  DXVA2_ModeWMV9_IDCT  DXVA2_ModeVC1_IDCT  DXVA2_ModeH264_VLD_NoFGT\\nDeinterlace Caps: {BF752EF6-8CC4-457A-BE1B-08BD1CAEEE9F}: Format(In/Out)=(YUY2,YUY2) Frames(Prev/Fwd/Back)=(0,0,1) Caps=VideoProcess_YUV2RGB VideoProcess_StretchX VideoProcess_StretchY VideoProcess_AlphaBlend DeinterlaceTech_EdgeFiltering\\n{335AA36E-7884-43A4-9C91-7F87FAF3E37E}: Format(In/Out)=(YUY2,YUY2) Frames(Prev/Fwd/Back)=(0,0,0) Caps=VideoProcess_YUV2RGB VideoProcess_StretchX VideoProcess_StretchY VideoProcess_AlphaBlend DeinterlaceTech_BOBVerticalStretch\\n{5A54A0C9-C7EC-4BD9-8EDE-F3C75DC4393B}: Format(In/Out)=(YUY2,YUY2) Frames(Prev/Fwd/Back)=(0,0,0) Caps=VideoProcess_YUV2RGB VideoProcess_StretchX VideoProcess_StretchY VideoProcess_AlphaBlend\\n{BF752EF6-8CC4-457A-BE1B-08BD1CAEEE9F}: Format(In/Out)=(UYVY,YUY2) Frames(Prev/Fwd/Back)=(0,0,1) Caps=VideoProcess_YUV2RGB VideoProcess_StretchX VideoProcess_StretchY VideoProcess_AlphaBlend DeinterlaceTech_EdgeFiltering\\n{335AA36E-7884-43A4-9C91-7F87FAF3E37E}: Format(In/Out)=(UYVY,YUY2) Frames(Prev/Fwd/Back)=(0,0,0) Caps=VideoProcess_YUV2RGB VideoProcess_StretchX VideoProcess_StretchY VideoProcess_AlphaBlend DeinterlaceTech_BOBVerticalStretch\\n{5A54A0C9-C7EC-4BD9-8EDE-F3C75DC4393B}: Format(In/Out)=(UYVY,YUY2) Frames(Prev/Fwd/Back)=(0,0,0) Caps=VideoProcess_YUV2RGB VideoProcess_StretchX VideoProcess_StretchY VideoProcess_AlphaBlend\\n{BF752EF6-8CC4-457A-BE1B-08BD1CAEEE9F}: Format(In/Out)=(YV12,YUY2) Frames(Prev/Fwd/Back)=(0,0,1) Caps=VideoProcess_YUV2RGB VideoProcess_StretchX VideoProcess_StretchY VideoProcess_AlphaBlend DeinterlaceTech_EdgeFiltering\\n{335AA36E-7884-43A4-9C91-7F87FAF3E37E}: Format(In/Out)=(YV12,YUY2) Frames(Prev/Fwd/Back)=(0,0,0) Caps=VideoProcess_YUV2RGB VideoProcess_StretchX VideoProcess_StretchY VideoProcess_AlphaBlend DeinterlaceTech_BOBVerticalStretch\\n{5A54A0C9-C7EC-4BD9-8EDE-F3C75DC4393B}: Format(In/Out)=(YV12,YUY2) Frames(Prev/Fwd/Back)=(0,0,0) Caps=VideoProcess_YUV2RGB VideoProcess_StretchX VideoProcess_StretchY VideoProcess_AlphaBlend\\n{BF752EF6-8CC4-457A-BE1B-08BD1CAEEE9F}: Format(In/Out)=(NV12,YUY2) Frames(Prev/Fwd/Back)=(0,0,1) Caps=VideoProcess_YUV2RGB VideoProcess_StretchX VideoProcess_StretchY VideoProcess_AlphaBlend DeinterlaceTech_EdgeFiltering\\n{335AA36E-7884-43A4-9C91-7F87FAF3E37E}: Format(In/Out)=(NV12,YUY2) Frames(Prev/Fwd/Back)=(0,0,0) Caps=VideoProcess_YUV2RGB VideoProcess_StretchX VideoProcess_StretchY VideoProcess_AlphaBlend DeinterlaceTech_BOBVerticalStretch\\n{5A54A0C9-C7EC-4BD9-8EDE-F3C75DC4393B}: Format(In/Out)=(NV12,YUY2) Frames(Prev/Fwd/Back)=(0,0,0) Caps=VideoProcess_YUV2RGB VideoProcess_StretchX VideoProcess_StretchY VideoProcess_AlphaBlend\\n{BF752EF6-8CC4-457A-BE1B-08BD1CAEEE9F}: Format(In/Out)=(IMC1,YUY2) Frames(Prev/Fwd/Back)=(0,0,1) Caps=VideoProcess_YUV2RGB VideoProcess_StretchX VideoProcess_StretchY VideoProcess_AlphaBlend DeinterlaceTech_EdgeFiltering\\n{335AA36E-7884-43A4-9C91-7F87FAF3E37E}: Format(In/Out)=(IMC1,YUY2) Frames(Prev/Fwd/Back)=(0,0,0) Caps=VideoProcess_YUV2RGB VideoProcess_StretchX VideoProcess_StretchY VideoProcess_AlphaBlend DeinterlaceTech_BOBVerticalStretch\\n{5A54A0C9-C7EC-4BD9-8EDE-F3C75DC4393B}: Format(In/Out)=(IMC1,YUY2) Frames(Prev/Fwd/Back)=(0,0,0) Caps=VideoProcess_YUV2RGB VideoProcess_StretchX VideoProcess_StretchY VideoProcess_AlphaBlend\\n{BF752EF6-8CC4-457A-BE1B-08BD1CAEEE9F}: Format(In/Out)=(IMC2,YUY2) Frames(Prev/Fwd/Back)=(0,0,1) Caps=VideoProcess_YUV2RGB VideoProcess_StretchX VideoProcess_StretchY VideoProcess_AlphaBlend DeinterlaceTech_EdgeFiltering\\n{335AA36E-7884-43A4-9C91-7F87FAF3E37E}: Format(In/Out)=(IMC2,YUY2) Frames(Prev/Fwd/Back)=(0,0,0) Caps=VideoProcess_YUV2RGB VideoProcess_StretchX VideoProcess_StretchY VideoProcess_AlphaBlend DeinterlaceTech_BOBVerticalStretch\\n{5A54A0C9-C7EC-4BD9-8EDE-F3C75DC4393B}: Format(In/Out)=(IMC2,YUY2) Frames(Prev/Fwd/Back)=(0,0,0) Caps=VideoProcess_YUV2RGB VideoProcess_StretchX VideoProcess_StretchY VideoProcess_AlphaBlend\\n{BF752EF6-8CC4-457A-BE1B-08BD1CAEEE9F}: Format(In/Out)=(IMC3,YUY2) Frames(Prev/Fwd/Back)=(0,0,1) Caps=VideoProcess_YUV2RGB VideoProcess_StretchX VideoProcess_StretchY VideoProcess_AlphaBlend DeinterlaceTech_EdgeFiltering\\n{335AA36E-7884-43A4-9C91-7F87FAF3E37E}: Format(In/Out)=(IMC3,YUY2) Frames(Prev/Fwd/Back)=(0,0,0) Caps=VideoProcess_YUV2RGB VideoProcess_StretchX VideoProcess_StretchY VideoProcess_AlphaBlend DeinterlaceTech_BOBVerticalStretch\\n{5A54A0C9-C7EC-4BD9-8EDE-F3C75DC4393B}: Format(In/Out)=(IMC3,YUY2) Frames(Prev/Fwd/Back)=(0,0,0) Caps=VideoProcess_YUV2RGB VideoProcess_StretchX VideoProcess_StretchY VideoProcess_AlphaBlend\\n{BF752EF6-8CC4-457A-BE1B-08BD1CAEEE9F}: Format(In/Out)=(IMC4,YUY2) Frames(Prev/Fwd/Back)=(0,0,1) Caps=VideoProcess_YUV2RGB VideoProcess_StretchX VideoProcess_StretchY VideoProcess_AlphaBlend DeinterlaceTech_EdgeFiltering\\n{335AA36E-7884-43A4-9C91-7F87FAF3E37E}: Format(In/Out)=(IMC4,YUY2) Frames(Prev/Fwd/Back)=(0,0,0) Caps=VideoProcess_YUV2RGB VideoProcess_StretchX VideoProcess_StretchY VideoProcess_AlphaBlend DeinterlaceTech_BOBVerticalStretch\\n{5A54A0C9-C7EC-4BD9-8EDE-F3C75DC4393B}: Format(In/Out)=(IMC4,YUY2) Frames(Prev/Fwd/Back)=(0,0,0) Caps=VideoProcess_YUV2RGB VideoProcess_StretchX VideoProcess_StretchY VideoProcess_AlphaBlend\\nD3D9 Overlay: Supported\\nDXVA-HD: Supported\\nDDraw Status: Enabled\\nD3D Status: Enabled\\nAGP Status: Enabled\\nMPO MaxPlanes: 1\\nMPO Caps: Not Supported\\nMPO Stretch: Not Supported\\nMPO Media Hints: Not Supported\\nMPO Formats: Not Supported\\nPanelFitter Caps: Not Supported\\nPanelFitter Stretch: Not SupportedDevice Problem Code: No Problem\\nDriver Problem Code: Unknown\\nDisplay Memory: 6046 MB\\nDedicated Memory: 2007 MB\\nShared Memory: 4038 MB\\nCurrent Mode: Unknown\\nHDR Support: Unknown\\nDisplay Topology: Unknown\\nDisplay Color Space: Unknown\\nColor Primaries: Unknown\\nDisplay Luminance: Unknown\\nDriver Name: C:\\\\WINDOWS\\\\System32\\\\DriverStore\\\\FileRepository\\\\nvami.inf_amd64_80865528b013df3a\\\\nvldumdx.dll,C:\\\\WINDOWS\\\\System32\\\\DriverStore\\\\FileRepository\\\\nvami.inf_amd64_80865528b013df3a\\\\nvldumdx.dll,C:\\\\WINDOWS\\\\System32\\\\DriverStore\\\\FileRepository\\\\nvami.inf_amd64_80865528b013df3a\\\\nvldumdx.dll,C:\\\\WINDOWS\\\\System32\\\\DriverStore\\\\FileRepository\\\\nvami.inf_amd64_80865528b013df3a\\\\nvldumdx.dll\\nDriver File Version: 23.21.0013.9077 (English)\\nDriver Version: 23.21.13.9077\\nDDI Version: 12\\nFeature Levels: 11_0,10_1,10_0,9_3,9_2,9_1\\nDriver Model: WDDM 2.3\\nGraphics Preemption: DMA\\nCompute Preemption: DMA\\nMiracast: Not Supported by Graphics driver\\nHybrid Graphics GPU: Discrete\\nPower P-states: Not Supported\\nDriver Attributes: Final Retail\\nDriver Date/Size: 23-Jan-18 13:00:00, 931584 bytes\\nWHQL Logo’d: n/a\\nWHQL Date Stamp: n/a\\nDevice Identifier: Unknown\\nVendor ID: 0x10DE\\nDevice ID: 0x0FD1\\nSubSys ID: 0x15A71043\\nRevision ID: 0x00A1\\nDriver Strong Name: oem72.inf:0f066de368ef4c06:Section050:23.21.13.9077:pci\\\\ven_10de&dev_0fd1&subsys_15a71043\\nRank Of Driver: 00D10001\\nVideo Accel: Unknown\\nDXVA2 Modes: DXVA2_ModeMPEG2_VLD  DXVA2_ModeVC1_D2010  DXVA2_ModeVC1_VLD  DXVA2_ModeH264_VLD_Stereo_Progressive_NoFGT  DXVA2_ModeH264_VLD_Stereo_NoFGT  DXVA2_ModeH264_VLD_NoFGT  DXVA2_ModeHEVC_VLD_Main  DXVA2_ModeMPEG4pt2_VLD_Simple  DXVA2_ModeMPEG4pt2_VLD_AdvSimple_NoGMC\\nDeinterlace Caps: n/a\\nD3D9 Overlay: Unknown\\nDXVA-HD: Unknown\\nDDraw Status: Enabled\\nD3D Status: Enabled\\nAGP Status: Enabled\\nMPO MaxPlanes: 0\\nMPO Caps: Not Supported\\nMPO Stretch: Not Supported\\nMPO Media Hints: Not Supported\\nMPO Formats: Not Supported\\nPanelFitter Caps: Not Supported\\nPanelFitter Stretch: Not SupportedDefault Sound Playback: Yes\\nDefault Voice Playback: Yes\\nHardware ID: HDAUDIO\\\\FUNC_01&VEN_10EC&DEV_0663&SUBSYS_104315A7&REV_1000\\nManufacturer ID: 1\\nProduct ID: 100\\nType: WDM\\nDriver Name: RTKVHD64.sys\\nDriver Version: 6.00.0001.7535 (English)\\nDriver Attributes: Final Retail\\nWHQL Logo’d: n/a\\nDate and Size: 16-Jun-15 00:00:00, 4504320 bytes\\nOther Files:\\nDriver Provider: Realtek Semiconductor Corp.\\nHW Accel Level: Basic\\nCap Flags: 0x0\\nMin/Max Sample Rate: 0, 0\\nStatic/Strm HW Mix Bufs: 0, 0\\nStatic/Strm HW 3D Bufs: 0, 0\\nHW Memory: 0\\nVoice Management: No\\nEAX™ 2.0 Listen/Src: No, No\\nI3DL2™ Listen/Src: No, No\\nSensaura™ ZoomFX™: NoDefault Sound Capture: Yes\\nDefault Voice Capture: Yes\\nDriver Name: RTKVHD64.sys\\nDriver Version: 6.00.0001.7535 (English)\\nDriver Attributes: Final Retail\\nDate and Size: 6/24/2015 23:57:00, 4504320 bytes\\nCap Flags: 0x0\\nFormat Flags: 0x0BusReportedDeviceDesc: USB2.0 HD UVC WebCam\\nParent: USB\\\\VID_04F2&PID_B36E\\\\200901010001\\nDriverProblemDesc: n/a\\nUpperFilters: n/a\\nLowerFilters: n/a\\nStack: \\\\Driver\\\\ksthunk,\\\\Driver\\\\usbvideo,\\\\Driver\\\\ACPI,\\\\Driver\\\\usbccgp\\nContainerCategory: n/aVendor/Product ID: n/a\\nFF Driver: n/aVendor/Product ID: n/a\\nFF Driver: n/aVendor/Product ID: 0x0EEF, 0x7910\\nFF Driver: n/aVendor/Product ID: 0x1038, 0x170E\\nFF Driver: n/aVendor/Product ID: 0x0EEF, 0x7910\\nFF Driver: n/aVendor/Product ID: 0x1038, 0x170E\\nFF Driver: n/aVendor/Product ID: n/a\\nFF Driver: n/aVendor/Product ID: 0x046D, 0xC326\\nFF Driver: n/aVendor/Product ID: 0x1038, 0x170E\\nFF Driver: n/aVendor/Product ID: 0x046D, 0xC326\\nFF Driver: n/aPoll w/ Interrupt: NoFree Space: 6.5 GB\\nTotal Space: 188.1 GB\\nFile System: NTFS\\nModel: Intel Raid 0 VolumeFree Space: 10.7 GB\\nTotal Space: 260.8 GB\\nFile System: NTFS\\nModel: Intel Raid 0 VolumeDevice ID: PCI\\\\VEN_8086&DEV_1E12&SUBSYS_15A71043&REV_C4\\\\3&11583659&0&E1\\nDriver: C:\\\\WINDOWS\\\\system32\\\\DRIVERS\\\\pci.sys, 10.00.16299.0248 (English), 2/10/2018 19:06:11, 362904 bytesDevice ID: PCI\\\\VEN_8086&DEV_0151&SUBSYS_15A71043&REV_09\\\\3&11583659&0&08\\nDriver: C:\\\\WINDOWS\\\\system32\\\\DRIVERS\\\\pci.sys, 10.00.16299.0248 (English), 2/10/2018 19:06:11, 362904 bytesDevice ID: PCI\\\\VEN_1969&DEV_1083&SUBSYS_15A71043&REV_C0\\\\4&D5DB309&0&00E3\\nDriver: C:\\\\WINDOWS\\\\system32\\\\DRIVERS\\\\L1C63x64.sys, 2.01.0000.0016 (English), 9/30/2017 02:41:02, 121344 bytesDevice ID: PCI\\\\VEN_10DE&DEV_0FD1&SUBSYS_15A71043&REV_A1\\\\4&1B5D39FA&0&0008\\nDriver: C:\\\\WINDOWS\\\\system32\\\\DRIVERS\\\\NVIDIA Corporation\\\\Drs\\\\dbInstaller.exe, 23.21.0013.9077 (English), 1/24/2018 13:23:45, 464880 bytes\\nDriver: C:\\\\WINDOWS\\\\system32\\\\DRIVERS\\\\NVIDIA Corporation\\\\Drs\\\\nvdrsdb.bin, 1/24/2018 13:23:45, 1473020 bytes\\nDriver: C:\\\\WINDOWS\\\\System32\\\\DriverStore\\\\FileRepository\\\\nvami.inf_amd64_80865528b013df3a\\\\NvContainerSetup.exe, 1.00.0007.0000 (English), 1/24/2018 13:23:45, 4181928 bytes\\nDriver: C:\\\\WINDOWS\\\\System32\\\\DriverStore\\\\FileRepository\\\\nvami.inf_amd64_80865528b013df3a\\\\NvCplSetupInt.exe, 1.00.0007.0000 (English), 1/24/2018 13:23:45, 101308928 bytes\\nDriver: C:\\\\Program Files (x86)\\\\NVIDIA Corporation\\\\coprocmanager\\\\detoured.dll, 2.01.0000.0224 (English), 1/24/2018 13:23:45, 19952 bytes\\nDriver: C:\\\\Program Files (x86)\\\\NVIDIA Corporation\\\\coprocmanager\\\\nvd3d9wrap.dll, 23.21.0013.9077 (English), 1/24/2018 13:23:45, 219800 bytes\\nDriver: C:\\\\Program Files (x86)\\\\NVIDIA Corporation\\\\coprocmanager\\\\nvdxgiwrap.dll, 23.21.0013.9077 (English), 1/24/2018 13:23:45, 144592 bytes\\nDriver: C:\\\\Program Files\\\\NVIDIA Corporation\\\\coprocmanager\\\\detoured.dll, 2.01.0000.0224 (English), 1/24/2018 13:23:45, 19760 bytes\\nDriver: C:\\\\Program Files\\\\NVIDIA Corporation\\\\coprocmanager\\\\nvd3d9wrapx.dll, 23.21.0013.9077 (English), 1/24/2018 13:23:45, 262536 bytes\\nDriver: C:\\\\Program Files\\\\NVIDIA Corporation\\\\coprocmanager\\\\nvdxgiwrapx.dll, 23.21.0013.9077 (English), 1/24/2018 13:23:45, 171712 bytes\\nDriver: C:\\\\Program Files\\\\NVIDIA Corporation\\\\license.txt, 8/22/2017 14:01:55, 27203 bytes\\nDriver: C:\\\\Program Files\\\\NVIDIA Corporation\\\\NVSMI\\\\MCU.exe, 1.01.5204.20580 (English), 1/24/2018 13:23:45, 849392 bytes\\nDriver: C:\\\\Program Files\\\\NVIDIA Corporation\\\\NVSMI\\\\nvdebugdump.exe, 6.14.0013.9077 (English), 1/24/2018 13:23:45, 419312 bytes\\nDriver: C:\\\\Program Files\\\\NVIDIA Corporation\\\\NVSMI\\\\nvidia-smi.1.pdf, 1/24/2018 13:23:45, 78094 bytes\\nDriver: C:\\\\Program Files\\\\NVIDIA Corporation\\\\NVSMI\\\\nvidia-smi.exe, 8.17.0013.9077 (English), 1/24/2018 13:23:45, 509936 bytes\\nDriver: C:\\\\Program Files\\\\NVIDIA Corporation\\\\NVSMI\\\\nvml.dll, 8.17.0013.9077 (English), 1/24/2018 13:23:45, 918088 bytes\\nDriver: C:\\\\Program Files\\\\NVIDIA Corporation\\\\OpenCL\\\\OpenCL.dll, 2.00.0004.0000 (English), 1/24/2018 13:23:45, 438768 bytes\\nDriver: C:\\\\Program Files\\\\NVIDIA Corporation\\\\OpenCL\\\\OpenCL64.dll, 2.00.0004.0000 (English), 1/24/2018 13:23:45, 532040 bytes\\nDriver: C:\\\\WINDOWS\\\\System32\\\\DriverStore\\\\FileRepository\\\\nvami.inf_amd64_80865528b013df3a\\\\nvlddmkm.sys, 23.21.0013.9077 (English), 1/25/2018 10:41:38, 17493824 bytes\\nDriver: C:\\\\WINDOWS\\\\System32\\\\DriverStore\\\\FileRepository\\\\nvami.inf_amd64_80865528b013df3a\\\\nvpciflt.sys, 23.21.0013.9077 (English), 1/25/2018 10:42:14, 48072 bytes\\nDriver: C:\\\\WINDOWS\\\\System32\\\\DriverStore\\\\FileRepository\\\\nvami.inf_amd64_80865528b013df3a\\\\nv-vk64.json, 1/24/2018 13:23:45, 669 bytes\\nDriver: C:\\\\WINDOWS\\\\System32\\\\DriverStore\\\\FileRepository\\\\nvami.inf_amd64_80865528b013df3a\\\\nvd3dumx.dll, 23.21.0013.9077 (English), 1/24/2018 13:23:45, 18969472 bytes\\nDriver: C:\\\\WINDOWS\\\\System32\\\\DriverStore\\\\FileRepository\\\\nvami.inf_amd64_80865528b013df3a\\\\nvd3dumx_cfg.dll, 23.21.0013.9077 (English), 1/24/2018 13:23:45, 19358800 bytes\\nDriver: C:\\\\WINDOWS\\\\System32\\\\DriverStore\\\\FileRepository\\\\nvami.inf_amd64_80865528b013df3a\\\\nvinitx.dll, 23.21.0013.9077 (English), 1/24/2018 13:23:45, 197992 bytes\\nDriver: C:\\\\WINDOWS\\\\System32\\\\DriverStore\\\\FileRepository\\\\nvami.inf_amd64_80865528b013df3a\\\\nvldumdx.dll, 23.21.0013.9077 (English), 1/24/2018 13:23:45, 931584 bytes\\nDriver: C:\\\\WINDOWS\\\\System32\\\\DriverStore\\\\FileRepository\\\\nvami.inf_amd64_80865528b013df3a\\\\nvoglshim64.dll, 23.21.0013.9077 (English), 1/24/2018 13:23:45, 173432 bytes\\nDriver: C:\\\\WINDOWS\\\\System32\\\\DriverStore\\\\FileRepository\\\\nvami.inf_amd64_80865528b013df3a\\\\nvoglv64.dll, 23.21.0013.9077 (English), 1/24/2018 13:23:45, 35404592 bytes\\nDriver: C:\\\\WINDOWS\\\\System32\\\\DriverStore\\\\FileRepository\\\\nvami.inf_amd64_80865528b013df3a\\\\nvumdshimx.dll, 23.21.0013.9077 (English), 1/24/2018 13:23:45, 577056 bytes\\nDriver: C:\\\\WINDOWS\\\\System32\\\\DriverStore\\\\FileRepository\\\\nvami.inf_amd64_80865528b013df3a\\\\nvwgf2umx.dll, 23.21.0013.9077 (English), 1/24/2018 13:23:45, 28909824 bytes\\nDriver: C:\\\\WINDOWS\\\\System32\\\\DriverStore\\\\FileRepository\\\\nvami.inf_amd64_80865528b013df3a\\\\nvwgf2umx_cfg.dll, 23.21.0013.9077 (English), 1/24/2018 13:23:45, 29740528 bytes\\nDriver: C:\\\\WINDOWS\\\\system32\\\\NvFBC64.dll, 6.14.0013.9077 (English), 1/24/2018 13:23:45, 1126888 bytes\\nDriver: C:\\\\WINDOWS\\\\system32\\\\NvIFR64.dll, 6.14.0013.9077 (English), 1/24/2018 13:23:45, 988464 bytes\\nDriver: C:\\\\WINDOWS\\\\system32\\\\NvIFROpenGL.dll, 23.21.0013.9077 (English), 1/24/2018 13:23:45, 616240 bytes\\nDriver: C:\\\\WINDOWS\\\\system32\\\\nvEncMFTH264.dll, 23.21.0013.9077 (English), 1/24/2018 13:23:45, 1325384 bytes\\nDriver: C:\\\\WINDOWS\\\\system32\\\\nvEncodeAPI64.dll, 23.21.0013.9077 (English), 1/24/2018 13:23:45, 795928 bytes\\nDriver: C:\\\\WINDOWS\\\\system32\\\\nvapi64.dll, 23.21.0013.9077 (English), 1/24/2018 13:23:45, 4580832 bytes\\nDriver: C:\\\\WINDOWS\\\\system32\\\\nvcompiler.dll, 23.21.0013.9077 (English), 1/24/2018 13:23:45, 40269808 bytes\\nDriver: C:\\\\WINDOWS\\\\system32\\\\nvcuda.dll, 23.21.0013.9077 (English), 1/24/2018 13:23:45, 12843496 bytes\\nDriver: C:\\\\WINDOWS\\\\system32\\\\nvcuvid.dll, 7.17.0013.9077 (English), 1/24/2018 13:23:45, 4308976 bytes\\nDriver: C:\\\\WINDOWS\\\\system32\\\\nvfatbinaryLoader.dll, 23.21.0013.9077 (English), 1/24/2018 13:23:45, 1134768 bytes\\nDriver: C:\\\\WINDOWS\\\\system32\\\\nvinfo.pb, 1/24/2018 13:23:45, 48407 bytes\\nDriver: C:\\\\WINDOWS\\\\system32\\\\nvopencl.dll, 23.21.0013.9077 (English), 1/24/2018 13:23:45, 19796336 bytes\\nDriver: C:\\\\WINDOWS\\\\system32\\\\nvptxJitCompiler.dll, 23.21.0013.9077 (English), 1/24/2018 13:23:45, 13444552 bytes\\nDriver: C:\\\\WINDOWS\\\\System32\\\\DriverStore\\\\FileRepository\\\\nvami.inf_amd64_80865528b013df3a\\\\nv-vk32.json, 1/24/2018 13:23:45, 669 bytes\\nDriver: C:\\\\WINDOWS\\\\System32\\\\DriverStore\\\\FileRepository\\\\nvami.inf_amd64_80865528b013df3a\\\\nvd3dum.dll, 23.21.0013.9077 (English), 1/24/2018 13:23:45, 15591424 bytes\\nDriver: C:\\\\WINDOWS\\\\System32\\\\DriverStore\\\\FileRepository\\\\nvami.inf_amd64_80865528b013df3a\\\\nvd3dum_cfg.dll, 23.21.0013.9077 (English), 1/24/2018 13:23:45, 15912784 bytes\\nDriver: C:\\\\WINDOWS\\\\System32\\\\DriverStore\\\\FileRepository\\\\nvami.inf_amd64_80865528b013df3a\\\\nvinit.dll, 23.21.0013.9077 (English), 1/24/2018 13:23:45, 171896 bytes\\nDriver: C:\\\\WINDOWS\\\\System32\\\\DriverStore\\\\FileRepository\\\\nvami.inf_amd64_80865528b013df3a\\\\nvldumd.dll, 23.21.0013.9077 (English), 1/24/2018 13:23:45, 774304 bytes\\nDriver: C:\\\\WINDOWS\\\\System32\\\\DriverStore\\\\FileRepository\\\\nvami.inf_amd64_80865528b013df3a\\\\nvoglshim32.dll, 23.21.0013.9077 (English), 1/24/2018 13:23:45, 145616 bytes\\nDriver: C:\\\\WINDOWS\\\\System32\\\\DriverStore\\\\FileRepository\\\\nvami.inf_amd64_80865528b013df3a\\\\nvoglv32.dll, 23.21.0013.9077 (English), 1/24/2018 13:23:45, 27975152 bytes\\nDriver: C:\\\\WINDOWS\\\\System32\\\\DriverStore\\\\FileRepository\\\\nvami.inf_amd64_80865528b013df3a\\\\nvumdshim.dll, 23.21.0013.9077 (English), 1/24/2018 13:23:45, 495352 bytes\\nDriver: C:\\\\WINDOWS\\\\System32\\\\DriverStore\\\\FileRepository\\\\nvami.inf_amd64_80865528b013df3a\\\\nvwgf2um.dll, 23.21.0013.9077 (English), 1/24/2018 13:23:45, 24473608 bytes\\nDriver: C:\\\\WINDOWS\\\\System32\\\\DriverStore\\\\FileRepository\\\\nvami.inf_amd64_80865528b013df3a\\\\nvwgf2um_cfg.dll, 23.21.0013.9077 (English), 1/24/2018 13:23:45, 25371952 bytes\\nDriver: C:\\\\WINDOWS\\\\SysWow64\\\\NvFBC.dll, 6.14.0013.9077 (English), 1/24/2018 13:23:45, 1054704 bytes\\nDriver: C:\\\\WINDOWS\\\\SysWow64\\\\NvIFR.dll, 6.14.0013.9077 (English), 1/24/2018 13:23:45, 939832 bytes\\nDriver: C:\\\\WINDOWS\\\\SysWow64\\\\NvIFROpenGL.dll, 23.21.0013.9077 (English), 1/24/2018 13:23:45, 506864 bytes\\nDriver: C:\\\\WINDOWS\\\\SysWow64\\\\nvEncMFTH264.dll, 23.21.0013.9077 (English), 1/24/2018 13:23:45, 1043128 bytes\\nDriver: C:\\\\WINDOWS\\\\SysWow64\\\\nvEncodeAPI.dll, 23.21.0013.9077 (English), 1/24/2018 13:23:45, 635248 bytes\\nDriver: C:\\\\WINDOWS\\\\SysWow64\\\\nvapi.dll, 23.21.0013.9077 (English), 1/24/2018 13:23:45, 3894304 bytes\\nDriver: C:\\\\WINDOWS\\\\SysWow64\\\\nvcompiler.dll, 23.21.0013.9077 (English), 1/24/2018 13:23:45, 35180016 bytes\\nDriver: C:\\\\WINDOWS\\\\SysWow64\\\\nvcuda.dll, 23.21.0013.9077 (English), 1/24/2018 13:23:45, 10900248 bytes\\nDriver: C:\\\\WINDOWS\\\\SysWow64\\\\nvcuvid.dll, 7.17.0013.9077 (English), 1/24/2018 13:23:45, 3709424 bytes\\nDriver: C:\\\\WINDOWS\\\\SysWow64\\\\nvfatbinaryLoader.dll, 23.21.0013.9077 (English), 1/24/2018 13:23:45, 885680 bytes\\nDriver: C:\\\\WINDOWS\\\\SysWow64\\\\nvopencl.dll, 23.21.0013.9077 (English), 1/24/2018 13:23:45, 16449872 bytes\\nDriver: C:\\\\WINDOWS\\\\SysWow64\\\\nvptxJitCompiler.dll, 23.21.0013.9077 (English), 1/24/2018 13:23:45, 11026080 bytes\\nDriver: C:\\\\WINDOWS\\\\System32\\\\DriverStore\\\\FileRepository\\\\nvami.inf_amd64_80865528b013df3a\\\\VulkanRT-Installer.exe, 1.00.0065.0000 (English), 1/24/2018 13:23:45, 979288 bytes\\nDriver: C:\\\\WINDOWS\\\\system32\\\\nvdispco6439077.dll, 2.00.0049.0004 (English), 1/24/2018 13:23:45, 1976120 bytes\\nDriver: C:\\\\WINDOWS\\\\system32\\\\nvdispgenco6439077.dll, 2.00.0026.0002 (English), 1/24/2018 13:23:45, 1673616 bytesDevice ID: PCI\\\\VEN_8086&DEV_0154&SUBSYS_15A71043&REV_09\\\\3&11583659&0&00\\nDriver: n/aDevice ID: PCI\\\\VEN_8086&DEV_1E22&SUBSYS_15A71043&REV_04\\\\3&11583659&0&FB\\nDriver: n/aDevice ID: PCI\\\\VEN_8086&DEV_088E&SUBSYS_40608086&REV_24\\\\4&20A32E22&0&00E1\\nDriver: C:\\\\WINDOWS\\\\system32\\\\DRIVERS\\\\NETwew01.sys, 15.18.0000.0001 (English), 5/4/2015 16:54:36, 3354384 bytes\\nDriver: C:\\\\WINDOWS\\\\system32\\\\DRIVERS\\\\Netwfw01.dat, 5/4/2015 16:53:00, 8108584 bytesDevice ID: PCI\\\\VEN_8086&DEV_1E10&SUBSYS_15A71043&REV_C4\\\\3&11583659&0&E0\\nDriver: C:\\\\WINDOWS\\\\system32\\\\DRIVERS\\\\pci.sys, 10.00.16299.0248 (English), 2/10/2018 19:06:11, 362904 bytesDevice ID: PCI\\\\VEN_8086&DEV_282A&SUBSYS_15A71043&REV_04\\\\3&11583659&0&FA\\nDriver: C:\\\\WINDOWS\\\\system32\\\\DRIVERS\\\\iaStorA.sys, 15.02.0000.1020 (English), 9/20/2016 11:04:30, 795640 bytesDevice ID: PCI\\\\VEN_8086&DEV_1E3A&SUBSYS_15A71043&REV_04\\\\3&11583659&0&B0\\nDriver: C:\\\\WINDOWS\\\\system32\\\\DRIVERS\\\\HECIx64.sys, 8.01.0000.1263 (English), 7/3/2012 12:16:02, 62784 bytesDevice ID: PCI\\\\VEN_8086&DEV_1E31&SUBSYS_15A71043&REV_04\\\\3&11583659&0&A0\\nDriver: C:\\\\WINDOWS\\\\system32\\\\DRIVERS\\\\USBXHCI.SYS, 10.00.16299.0125 (English), 12/8/2017 12:24:06, 437144 bytesDevice ID: PCI\\\\VEN_8086&DEV_0166&SUBSYS_15A71043&REV_09\\\\3&11583659&0&10\\nDriver: C:\\\\WINDOWS\\\\system32\\\\DRIVERS\\\\igdkmd64.sys, 10.18.0010.4653 (English), 5/18/2017 15:11:34, 3811816 bytes\\nDriver: C:\\\\WINDOWS\\\\system32\\\\igd10iumd64.dll, 10.18.0010.4653 (English), 5/18/2017 15:11:32, 12442968 bytes\\nDriver: C:\\\\WINDOWS\\\\system32\\\\igdusc64.dll, 10.18.0010.4653 (English), 5/18/2017 15:11:36, 4710224 bytes\\nDriver: C:\\\\WINDOWS\\\\system32\\\\igdmd64.dll, 10.18.0010.4653 (English), 5/18/2017 15:11:34, 480584 bytes\\nDriver: C:\\\\WINDOWS\\\\system32\\\\igfxcmrt64.dll, 3.00.0000.1284 (English), 5/18/2017 15:11:36, 209640 bytes\\nDriver: C:\\\\WINDOWS\\\\system32\\\\igfx11cmrt64.dll, 3.00.0000.1284 (English), 5/18/2017 15:11:36, 202240 bytes\\nDriver: C:\\\\WINDOWS\\\\system32\\\\igfxcmjit64.dll, 3.00.0000.1284 (English), 5/18/2017 15:11:36, 2044416 bytes\\nDriver: C:\\\\WINDOWS\\\\system32\\\\IccLibDll_x64.dll, 5/18/2017 15:11:32, 111616 bytes\\nDriver: C:\\\\WINDOWS\\\\system32\\\\igdde64.dll, 5/18/2017 15:11:34, 238592 bytes\\nDriver: C:\\\\WINDOWS\\\\SysWow64\\\\igdde32.dll, 5/18/2017 15:11:34, 200192 bytes\\nDriver: C:\\\\WINDOWS\\\\system32\\\\iglhxs64.vp, 5/18/2017 15:11:36, 2582 bytes\\nDriver: C:\\\\WINDOWS\\\\system32\\\\iglhxo64.vp, 5/18/2017 15:11:36, 44025 bytes\\nDriver: C:\\\\WINDOWS\\\\system32\\\\iglhxc64.vp, 5/18/2017 15:11:36, 43494 bytes\\nDriver: C:\\\\WINDOWS\\\\system32\\\\iglhxg64.vp, 5/18/2017 15:11:36, 43256 bytes\\nDriver: C:\\\\WINDOWS\\\\system32\\\\iglhxo64_dev.vp, 5/18/2017 15:11:36, 42079 bytes\\nDriver: C:\\\\WINDOWS\\\\system32\\\\iglhxc64_dev.vp, 5/18/2017 15:11:36, 43816 bytes\\nDriver: C:\\\\WINDOWS\\\\system32\\\\iglhxg64_dev.vp, 5/18/2017 15:11:36, 43298 bytes\\nDriver: C:\\\\WINDOWS\\\\system32\\\\iglhxa64.vp, 5/18/2017 15:11:36, 1125 bytes\\nDriver: C:\\\\WINDOWS\\\\system32\\\\iglhxa64.cpa, 5/18/2017 15:11:36, 2813952 bytes\\nDriver: C:\\\\WINDOWS\\\\system32\\\\iglhcp64.dll, 9.00.0020.9000 (English), 5/18/2017 15:11:36, 240424 bytes\\nDriver: C:\\\\WINDOWS\\\\system32\\\\iglhsip64.dll, 9.00.0020.9000 (English), 5/18/2017 15:11:36, 1174824 bytes\\nDriver: C:\\\\WINDOWS\\\\SysWow64\\\\igdusc32.dll, 10.18.0010.4653 (English), 5/18/2017 15:11:36, 3733488 bytes\\nDriver: C:\\\\WINDOWS\\\\SysWow64\\\\igdmd32.dll, 10.18.0010.4653 (English), 5/18/2017 15:11:34, 390920 bytes\\nDriver: C:\\\\WINDOWS\\\\SysWow64\\\\igd10iumd32.dll, 10.18.0010.4653 (English), 5/18/2017 15:11:32, 12007928 bytes\\nDriver: C:\\\\WINDOWS\\\\SysWow64\\\\igdumdim32.dll, 10.18.0010.4653 (English), 5/18/2017 15:11:36, 10676400 bytes\\nDriver: C:\\\\WINDOWS\\\\SysWow64\\\\igdail32.dll, 5/18/2017 15:11:32, 161280 bytes\\nDriver: C:\\\\WINDOWS\\\\SysWow64\\\\iglhcp32.dll, 9.00.0020.9000 (English), 5/18/2017 15:11:36, 204840 bytes\\nDriver: C:\\\\WINDOWS\\\\SysWow64\\\\iglhsip32.dll, 9.00.0020.9000 (English), 5/18/2017 15:11:36, 1170632 bytes\\nDriver: C:\\\\WINDOWS\\\\SysWow64\\\\IntelCpHeciSvc.exe, 9.00.0020.9000 (English), 5/18/2017 15:17:36, 280696 bytes\\nDriver: C:\\\\WINDOWS\\\\SysWow64\\\\igfxcmrt32.dll, 3.00.0000.1284 (English), 5/18/2017 15:11:36, 179592 bytes\\nDriver: C:\\\\WINDOWS\\\\SysWow64\\\\igfx11cmrt32.dll, 3.00.0000.1284 (English), 5/18/2017 15:11:36, 172544 bytes\\nDriver: C:\\\\WINDOWS\\\\SysWow64\\\\igfxcmjit32.dll, 3.00.0000.1284 (English), 5/18/2017 15:11:36, 1775616 bytes\\nDriver: C:\\\\WINDOWS\\\\system32\\\\difx64.exe, 1.04.0003.0000 (English), 5/18/2017 15:17:46, 156280 bytes\\nDriver: C:\\\\WINDOWS\\\\system32\\\\igfxDH.dll, 6.15.0010.4653 (English), 5/18/2017 15:11:36, 689664 bytes\\nDriver: C:\\\\WINDOWS\\\\system32\\\\igfxDHLib.dll, 1.00.0000.0000 (Invariant Language), 5/18/2017 15:11:36, 77312 bytes\\nDriver: C:\\\\WINDOWS\\\\system32\\\\igfxDHLibv2_0.dll, 1.00.0000.0000 (Invariant Language), 5/18/2017 15:11:36, 87040 bytes\\nDriver: C:\\\\WINDOWS\\\\system32\\\\igfxDI.dll, 6.15.0010.4653 (English), 5/18/2017 15:11:36, 302080 bytes\\nDriver: C:\\\\WINDOWS\\\\system32\\\\igfxDILib.dll, 1.00.0000.0000 (Invariant Language), 5/18/2017 15:11:36, 28160 bytes\\nDriver: C:\\\\WINDOWS\\\\system32\\\\igfxDILibv2_0.dll, 1.00.0000.0000 (Invariant Language), 5/18/2017 15:11:36, 28160 bytes\\nDriver: C:\\\\WINDOWS\\\\system32\\\\igfxLHM.dll, 6.15.0010.4653 (English), 5/18/2017 15:11:36, 269824 bytes\\nDriver: C:\\\\WINDOWS\\\\system32\\\\igfxLHMLib.dll, 1.00.0000.0000 (Invariant Language), 5/18/2017 15:11:36, 22528 bytes\\nDriver: C:\\\\WINDOWS\\\\system32\\\\igfxLHMLibv2_0.dll, 1.00.0000.0000 (Invariant Language), 5/18/2017 15:11:36, 22528 bytes\\nDriver: C:\\\\WINDOWS\\\\system32\\\\igfxEM.exe, 6.15.0010.4653 (English), 5/18/2017 15:17:48, 530552 bytes\\nDriver: C:\\\\WINDOWS\\\\system32\\\\igfxEMLib.dll, 1.00.0000.0000 (Invariant Language), 5/18/2017 15:11:36, 27648 bytes\\nDriver: C:\\\\WINDOWS\\\\system32\\\\igfxEMLibv2_0.dll, 1.00.0000.0000 (Invariant Language), 5/18/2017 15:11:36, 27648 bytes\\nDriver: C:\\\\WINDOWS\\\\system32\\\\GfxUIEx.exe, 6.15.0010.4653 (English), 5/18/2017 15:17:28, 959608 bytes\\nDriver: C:\\\\WINDOWS\\\\system32\\\\Gfxv4_0.exe, 8.15.0010.4653 (English), 5/18/2017 15:17:34, 4382840 bytes\\nDriver: C:\\\\WINDOWS\\\\system32\\\\Gfxv4_0.exe.config, 5/18/2017 15:11:30, 889 bytes\\nDriver: C:\\\\WINDOWS\\\\system32\\\\Gfxv2_0.exe, 8.15.0010.4653 (English), 5/18/2017 15:17:30, 4379256 bytes\\nDriver: C:\\\\WINDOWS\\\\system32\\\\Gfxv2_0.exe.config, 5/18/2017 15:11:30, 895 bytes\\nDriver: C:\\\\WINDOWS\\\\system32\\\\MetroIntelGenericUIFramework.dll, 1.00.0000.0000 (English), 5/18/2017 15:11:38, 626688 bytes\\nDriver: C:\\\\WINDOWS\\\\system32\\\\igfxCUIServicePS.dll, 5/18/2017 15:11:36, 103936 bytes\\nDriver: C:\\\\WINDOWS\\\\system32\\\\igfxCUIService.exe, 6.15.0010.4653 (English), 5/18/2017 15:17:48, 319096 bytes\\nDriver: C:\\\\WINDOWS\\\\system32\\\\igfxCPL.cpl, 5/18/2017 15:11:36, 272896 bytes\\nDriver: C:\\\\WINDOWS\\\\system32\\\\igfxTray.exe, 6.15.0010.4653 (English), 5/18/2017 15:17:54, 372856 bytes\\nDriver: C:\\\\WINDOWS\\\\system32\\\\igfxDTCM.dll, 6.15.0010.4653 (English), 5/18/2017 15:11:36, 226816 bytes\\nDriver: C:\\\\WINDOWS\\\\system32\\\\igfxHK.exe, 6.15.0010.4653 (English), 5/18/2017 15:17:52, 247416 bytes\\nDriver: C:\\\\WINDOWS\\\\system32\\\\igfxOSP.dll, 6.15.0010.4653 (English), 5/18/2017 15:11:36, 389120 bytes\\nDriver: C:\\\\WINDOWS\\\\system32\\\\DPTopologyApp.exe, 8.15.0010.4653 (English), 5/18/2017 15:17:22, 545912 bytes\\nDriver: C:\\\\WINDOWS\\\\system32\\\\DPTopologyApp.exe.config, 5/18/2017 15:11:30, 889 bytes\\nDriver: C:\\\\WINDOWS\\\\system32\\\\CustomModeApp.exe, 8.15.0010.4653 (English), 5/18/2017 15:17:16, 399992 bytes\\nDriver: C:\\\\WINDOWS\\\\system32\\\\CustomModeApp.exe.config, 5/18/2017 15:11:30, 889 bytes\\nDriver: C:\\\\WINDOWS\\\\system32\\\\DPTopologyAppv2_0.exe, 8.15.0010.4653 (English), 5/18/2017 15:17:24, 545400 bytes\\nDriver: C:\\\\WINDOWS\\\\system32\\\\DPTopologyAppv2_0.exe.config, 5/18/2017 15:11:30, 895 bytes\\nDriver: C:\\\\WINDOWS\\\\system32\\\\CustomModeAppv2_0.exe, 8.15.0010.4653 (English), 5/18/2017 15:17:18, 399480 bytes\\nDriver: C:\\\\WINDOWS\\\\system32\\\\CustomModeAppv2_0.exe.config, 5/18/2017 15:11:30, 895 bytes\\nDriver: C:\\\\WINDOWS\\\\system32\\\\igfxext.exe, 6.15.0010.4653 (English), 5/18/2017 15:17:58, 195192 bytes\\nDriver: C:\\\\WINDOWS\\\\system32\\\\igfxexps.dll, 6.15.0010.4653 (English), 5/18/2017 15:11:36, 49928 bytes\\nDriver: C:\\\\WINDOWS\\\\SysWow64\\\\igfxexps32.dll, 6.15.0010.4653 (English), 5/18/2017 15:11:36, 48128 bytes\\nDriver: C:\\\\WINDOWS\\\\system32\\\\resARA.cui, 5/18/2017 15:11:42, 165460 bytes\\nDriver: C:\\\\WINDOWS\\\\system32\\\\resCHS.cui, 5/18/2017 15:11:42, 149524 bytes\\nDriver: C:\\\\WINDOWS\\\\system32\\\\resCHT.cui, 5/18/2017 15:11:42, 150404 bytes\\nDriver: C:\\\\WINDOWS\\\\system32\\\\resCSY.cui, 5/18/2017 15:11:42, 156628 bytes\\nDriver: C:\\\\WINDOWS\\\\system32\\\\resDAN.cui, 5/18/2017 15:11:42, 153508 bytes\\nDriver: C:\\\\WINDOWS\\\\system32\\\\resDEU.cui, 5/18/2017 15:11:42, 158388 bytes\\nDriver: C:\\\\WINDOWS\\\\system32\\\\resELL.cui, 5/18/2017 15:11:42, 184036 bytes\\nDriver: C:\\\\WINDOWS\\\\system32\\\\resENU.cui, 5/18/2017 15:11:42, 152164 bytes\\nDriver: C:\\\\WINDOWS\\\\system32\\\\resESN.cui, 5/18/2017 15:11:42, 158052 bytes\\nDriver: C:\\\\WINDOWS\\\\system32\\\\resFIN.cui, 5/18/2017 15:11:42, 155972 bytes\\nDriver: C:\\\\WINDOWS\\\\system32\\\\resFRA.cui, 5/18/2017 15:11:42, 160196 bytes\\nDriver: C:\\\\WINDOWS\\\\system32\\\\resHEB.cui, 5/18/2017 15:11:42, 164884 bytes\\nDriver: C:\\\\WINDOWS\\\\system32\\\\resHRV.cui, 5/18/2017 15:11:42, 155540 bytes\\nDriver: C:\\\\WINDOWS\\\\system32\\\\resHUN.cui, 5/18/2017 15:11:42, 160260 bytes\\nDriver: C:\\\\WINDOWS\\\\system32\\\\resITA.cui, 5/18/2017 15:11:42, 158356 bytes\\nDriver: C:\\\\WINDOWS\\\\system32\\\\resJPN.cui, 5/18/2017 15:11:42, 164948 bytes\\nDriver: C:\\\\WINDOWS\\\\system32\\\\resKOR.cui, 5/18/2017 15:11:42, 158532 bytes\\nDriver: C:\\\\WINDOWS\\\\system32\\\\resNLD.cui, 5/18/2017 15:11:42, 157332 bytes\\nDriver: C:\\\\WINDOWS\\\\system32\\\\resNOR.cui, 5/18/2017 15:11:42, 154004 bytes\\nDriver: C:\\\\WINDOWS\\\\system32\\\\resPLK.cui, 5/18/2017 15:11:42, 157652 bytes\\nDriver: C:\\\\WINDOWS\\\\system32\\\\resPTB.cui, 5/18/2017 15:11:42, 156708 bytes\\nDriver: C:\\\\WINDOWS\\\\system32\\\\resPTG.cui, 5/18/2017 15:11:42, 156420 bytes\\nDriver: C:\\\\WINDOWS\\\\system32\\\\resROM.cui, 5/18/2017 15:11:42, 158148 bytes\\nDriver: C:\\\\WINDOWS\\\\system32\\\\resRUS.cui, 5/18/2017 15:11:42, 179828 bytes\\nDriver: C:\\\\WINDOWS\\\\system32\\\\resSKY.cui, 5/18/2017 15:11:42, 157492 bytes\\nDriver: C:\\\\WINDOWS\\\\system32\\\\resSLV.cui, 5/18/2017 15:11:42, 154964 bytes\\nDriver: C:\\\\WINDOWS\\\\system32\\\\resSVE.cui, 5/18/2017 15:11:44, 155124 bytes\\nDriver: C:\\\\WINDOWS\\\\system32\\\\resTHA.cui, 5/18/2017 15:11:44, 191476 bytes\\nDriver: C:\\\\WINDOWS\\\\system32\\\\resTRK.cui, 5/18/2017 15:11:44, 156596 bytes\\nDriver: C:\\\\WINDOWS\\\\system32\\\\ig7icd64.dll, 10.18.0010.4653 (English), 5/18/2017 15:11:32, 8530944 bytes\\nDriver: C:\\\\WINDOWS\\\\SysWow64\\\\ig7icd32.dll, 10.18.0010.4653 (English), 5/18/2017 15:11:32, 6518272 bytes\\nDriver: C:\\\\WINDOWS\\\\system32\\\\igdumdim64.dll, 10.18.0010.4653 (English), 5/18/2017 15:11:36, 11158160 bytes\\nDriver: C:\\\\WINDOWS\\\\system32\\\\igdail64.dll, 5/18/2017 15:11:34, 179712 bytes\\nDriver: C:\\\\Program Files (x86)\\\\Common Files\\\\Intel\\\\OpenCL\\\\llvm_release_license.txt, 5/18/2017 15:11:40, 1981 bytes\\nDriver: C:\\\\Program Files (x86)\\\\Common Files\\\\Intel\\\\OpenCL\\\\readme.txt, 5/18/2017 15:11:42, 9788 bytes\\nDriver: C:\\\\Program Files (x86)\\\\Common Files\\\\Intel\\\\OpenCL\\\\version.ini, 5/18/2017 15:11:42, 32 bytes\\nDriver: C:\\\\WINDOWS\\\\SysWow64\\\\Intel_OpenCL_ICD32.dll, 1.02.0011.0000 (English), 5/18/2017 15:11:38, 77824 bytes\\nDriver: C:\\\\WINDOWS\\\\SysWow64\\\\IntelOpenCL32.dll, 10.18.0010.4653 (English), 5/18/2017 15:11:36, 304128 bytes\\nDriver: C:\\\\Program Files (x86)\\\\Common Files\\\\Intel\\\\OpenCL\\\\bin\\\\x86\\\\task_executor32.dll, 3.00.0001.10891 (English), 5/18/2017 15:11:42, 225280 bytes\\nDriver: C:\\\\Program Files (x86)\\\\Common Files\\\\Intel\\\\OpenCL\\\\bin\\\\x86\\\\OclCpuBackend32.dll, 3.00.0001.10891 (English), 5/18/2017 15:11:40, 6776320 bytes\\nDriver: C:\\\\Program Files (x86)\\\\Common Files\\\\Intel\\\\OpenCL\\\\bin\\\\x86\\\\intelocl32.dll, 3.00.0001.10891 (English), 5/18/2017 15:11:40, 644096 bytes\\nDriver: C:\\\\Program Files (x86)\\\\Common Files\\\\Intel\\\\OpenCL\\\\bin\\\\x86\\\\cpu_device32.dll, 3.00.0001.10891 (English), 5/18/2017 15:11:40, 326144 bytes\\nDriver: C:\\\\Program Files (x86)\\\\Common Files\\\\Intel\\\\OpenCL\\\\bin\\\\x86\\\\clbltfnn8.rtl, 5/18/2017 15:11:40, 2986764 bytes\\nDriver: C:\\\\Program Files (x86)\\\\Common Files\\\\Intel\\\\OpenCL\\\\bin\\\\x86\\\\clbltfnn8_img_cbk.o, 5/18/2017 15:11:40, 240244 bytes\\nDriver: C:\\\\Program Files (x86)\\\\Common Files\\\\Intel\\\\OpenCL\\\\bin\\\\x86\\\\clbltfnn8_img_cbk.rtl, 5/18/2017 15:11:40, 375380 bytes\\nDriver: C:\\\\Program Files (x86)\\\\Common Files\\\\Intel\\\\OpenCL\\\\bin\\\\x86\\\\clbltfng9.rtl, 5/18/2017 15:11:40, 2930216 bytes\\nDriver: C:\\\\Program Files (x86)\\\\Common Files\\\\Intel\\\\OpenCL\\\\bin\\\\x86\\\\clbltfng9_img_cbk.o, 5/18/2017 15:11:40, 234396 bytes\\nDriver: C:\\\\Program Files (x86)\\\\Common Files\\\\Intel\\\\OpenCL\\\\bin\\\\x86\\\\clbltfng9_img_cbk.rtl, 5/18/2017 15:11:40, 374552 bytes\\nDriver: C:\\\\Program Files (x86)\\\\Common Files\\\\Intel\\\\OpenCL\\\\bin\\\\x86\\\\clbltfns9.rtl, 5/18/2017 15:11:40, 2408580 bytes\\nDriver: C:\\\\Program Files (x86)\\\\Common Files\\\\Intel\\\\OpenCL\\\\bin\\\\x86\\\\clbltfns9_img_cbk.o, 5/18/2017 15:11:40, 197272 bytes\\nDriver: C:\\\\Program Files (x86)\\\\Common Files\\\\Intel\\\\OpenCL\\\\bin\\\\x86\\\\clbltfns9_img_cbk.rtl, 5/18/2017 15:11:40, 329372 bytes\\nDriver: C:\\\\Program Files (x86)\\\\Common Files\\\\Intel\\\\OpenCL\\\\bin\\\\x86\\\\clang_compiler32.dll, 3.00.0001.10891 (English), 5/18/2017 15:11:38, 14906880 bytes\\nDriver: C:\\\\Program Files (x86)\\\\Common Files\\\\Intel\\\\OpenCL\\\\bin\\\\x86__ocl_svml_n8.dll, 2.00.0000.0000 (English), 5/18/2017 15:11:42, 5911552 bytes\\nDriver: C:\\\\Program Files (x86)\\\\Common Files\\\\Intel\\\\OpenCL\\\\bin\\\\x86__ocl_svml_g9.dll, 2.00.0000.0000 (English), 5/18/2017 15:11:42, 5182464 bytes\\nDriver: C:\\\\Program Files (x86)\\\\Common Files\\\\Intel\\\\OpenCL\\\\bin\\\\x86__ocl_svml_s9.dll, 2.00.0000.0000 (English), 5/18/2017 15:11:42, 4695040 bytes\\nDriver: C:\\\\Program Files (x86)\\\\Common Files\\\\Intel\\\\OpenCL\\\\bin\\\\x86\\\\tbb\\\\tbbmalloc.dll, 4.00.2012.0408 (), 5/18/2017 15:11:42, 183232 bytes\\nDriver: C:\\\\Program Files (x86)\\\\Common Files\\\\Intel\\\\OpenCL\\\\bin\\\\x86\\\\tbb\\\\tbb.dll, 4.00.2012.0408 (), 5/18/2017 15:11:42, 350656 bytes\\nDriver: C:\\\\WINDOWS\\\\system32\\\\Intel_OpenCL_ICD64.dll, 1.02.0011.0000 (English), 5/18/2017 15:11:38, 81408 bytes\\nDriver: C:\\\\WINDOWS\\\\system32\\\\IntelOpenCL64.dll, 10.18.0010.4653 (English), 5/18/2017 15:11:36, 394240 bytes\\nDriver: C:\\\\Program Files (x86)\\\\Common Files\\\\Intel\\\\OpenCL\\\\bin\\\\x64\\\\task_executor64.dll, 3.00.0001.10891 (English), 5/18/2017 15:11:42, 263680 bytes\\nDriver: C:\\\\Program Files (x86)\\\\Common Files\\\\Intel\\\\OpenCL\\\\bin\\\\x64\\\\OclCpuBackend64.dll, 3.00.0001.10891 (English), 5/18/2017 15:11:42, 9179136 bytes\\nDriver: C:\\\\Program Files (x86)\\\\Common Files\\\\Intel\\\\OpenCL\\\\bin\\\\x64\\\\intelocl64.dll, 3.00.0001.10891 (English), 5/18/2017 15:11:40, 814080 bytes\\nDriver: C:\\\\Program Files (x86)\\\\Common Files\\\\Intel\\\\OpenCL\\\\bin\\\\x64\\\\cpu_device64.dll, 3.00.0001.10891 (English), 5/18/2017 15:11:40, 390144 bytes\\nDriver: C:\\\\Program Files (x86)\\\\Common Files\\\\Intel\\\\OpenCL\\\\bin\\\\x64\\\\clbltfnh8.rtl, 5/18/2017 15:11:40, 2941464 bytes\\nDriver: C:\\\\Program Files (x86)\\\\Common Files\\\\Intel\\\\OpenCL\\\\bin\\\\x64\\\\clbltfnh8_img_cbk.o, 5/18/2017 15:11:40, 295744 bytes\\nDriver: C:\\\\Program Files (x86)\\\\Common Files\\\\Intel\\\\OpenCL\\\\bin\\\\x64\\\\clbltfnh8_img_cbk.rtl, 5/18/2017 15:11:40, 388464 bytes\\nDriver: C:\\\\Program Files (x86)\\\\Common Files\\\\Intel\\\\OpenCL\\\\bin\\\\x64\\\\clbltfne9.rtl, 5/18/2017 15:11:40, 2884992 bytes\\nDriver: C:\\\\Program Files (x86)\\\\Common Files\\\\Intel\\\\OpenCL\\\\bin\\\\x64\\\\clbltfne9_img_cbk.o, 5/18/2017 15:11:40, 284840 bytes\\nDriver: C:\\\\Program Files (x86)\\\\Common Files\\\\Intel\\\\OpenCL\\\\bin\\\\x64\\\\clbltfne9_img_cbk.rtl, 5/18/2017 15:11:40, 387724 bytes\\nDriver: C:\\\\Program Files (x86)\\\\Common Files\\\\Intel\\\\OpenCL\\\\bin\\\\x64\\\\clbltfnl9.rtl, 5/18/2017 15:11:40, 2365892 bytes\\nDriver: C:\\\\Program Files (x86)\\\\Common Files\\\\Intel\\\\OpenCL\\\\bin\\\\x64\\\\clbltfnl9_img_cbk.o, 5/18/2017 15:11:40, 228328 bytes\\nDriver: C:\\\\Program Files (x86)\\\\Common Files\\\\Intel\\\\OpenCL\\\\bin\\\\x64\\\\clbltfnl9_img_cbk.rtl, 5/18/2017 15:11:40, 342372 bytes\\nDriver: C:\\\\Program Files (x86)\\\\Common Files\\\\Intel\\\\OpenCL\\\\bin\\\\x64\\\\clang_compiler64.dll, 3.00.0001.10891 (English), 5/18/2017 15:11:40, 19593728 bytes\\nDriver: C:\\\\Program Files (x86)\\\\Common Files\\\\Intel\\\\OpenCL\\\\bin\\\\x64__ocl_svml_h8.dll, 2.00.0000.0000 (English), 5/18/2017 15:11:42, 6108160 bytes\\nDriver: C:\\\\Program Files (x86)\\\\Common Files\\\\Intel\\\\OpenCL\\\\bin\\\\x64__ocl_svml_e9.dll, 2.00.0000.0000 (English), 5/18/2017 15:11:42, 5529600 bytes\\nDriver: C:\\\\Program Files (x86)\\\\Common Files\\\\Intel\\\\OpenCL\\\\bin\\\\x64__ocl_svml_l9.dll, 2.00.0000.0000 (English), 5/18/2017 15:11:42, 5076480 bytes\\nDriver: C:\\\\Program Files (x86)\\\\Common Files\\\\Intel\\\\OpenCL\\\\bin\\\\x64\\\\tbb\\\\tbbmalloc.dll, 4.00.2012.0408 (), 5/18/2017 15:11:42, 215488 bytes\\nDriver: C:\\\\Program Files (x86)\\\\Common Files\\\\Intel\\\\OpenCL\\\\bin\\\\x64\\\\tbb\\\\tbb.dll, 4.00.2012.0408 (), 5/18/2017 15:11:42, 425920 bytes\\nDriver: C:\\\\WINDOWS\\\\SysWow64\\\\IntelOpenCL32.dll, 10.18.0010.4653 (English), 5/18/2017 15:11:36, 304128 bytes\\nDriver: C:\\\\WINDOWS\\\\SysWow64\\\\igdbcl32.dll, 10.18.0010.4653 (English), 5/18/2017 15:11:34, 338944 bytes\\nDriver: C:\\\\WINDOWS\\\\SysWow64\\\\igdrcl32.dll, 10.18.0010.4653 (English), 5/18/2017 15:11:34, 1803264 bytes\\nDriver: C:\\\\WINDOWS\\\\SysWow64\\\\igdfcl32.dll, 10.18.0010.4653 (English), 5/18/2017 15:11:34, 17854976 bytes\\nDriver: C:\\\\WINDOWS\\\\system32\\\\IntelOpenCL64.dll, 10.18.0010.4653 (English), 5/18/2017 15:11:36, 394240 bytes\\nDriver: C:\\\\WINDOWS\\\\system32\\\\igdbcl64.dll, 10.18.0010.4653 (English), 5/18/2017 15:11:34, 383488 bytes\\nDriver: C:\\\\WINDOWS\\\\system32\\\\igdrcl64.dll, 10.18.0010.4653 (English), 5/18/2017 15:11:34, 2003968 bytes\\nDriver: C:\\\\WINDOWS\\\\system32\\\\igdfcl64.dll, 10.18.0010.4653 (English), 5/18/2017 15:11:34, 22922752 bytes\\nDriver: C:\\\\Program Files\\\\Intel\\\\Media SDK\\\\libmfxhw32.dll, 5.15.0005.0029 (English), 5/18/2017 15:11:38, 8401656 bytes\\nDriver: C:\\\\Program Files\\\\Intel\\\\Media SDK\\\\mfxplugin32_hw.dll, 3.11.0005.0020 (English), 5/18/2017 15:11:38, 599288 bytes\\nDriver: C:\\\\Program Files\\\\Intel\\\\Media SDK\\\\mfx_mft_h264ve_32.dll, 6.16.0007.0004 (English), 5/18/2017 15:11:38, 811008 bytes\\nDriver: C:\\\\Program Files\\\\Intel\\\\Media SDK\\\\mfx_mft_mjpgvd_32.dll, 6.16.0007.0004 (English), 5/18/2017 15:11:38, 753664 bytes\\nDriver: C:\\\\Program Files\\\\Intel\\\\Media SDK\\\\he_32.vp, 5/18/2017 15:11:30, 28553 bytes\\nDriver: C:\\\\Program Files\\\\Intel\\\\Media SDK\\\\c_32.cpa, 5/18/2017 15:11:30, 846855 bytes\\nDriver: C:\\\\Program Files\\\\Intel\\\\Media SDK\\\\cpa_32.vp, 5/18/2017 15:11:30, 993 bytes\\nDriver: C:\\\\Program Files\\\\Intel\\\\Media SDK\\\\dev_32.vp, 5/18/2017 15:11:30, 21523 bytes\\nDriver: C:\\\\Program Files\\\\Intel\\\\Media SDK\\\\mj_32.vp, 5/18/2017 15:11:38, 26961 bytes\\nDriver: C:\\\\Program Files\\\\Intel\\\\Media SDK\\\\libmfxhw64.dll, 5.15.0005.0029 (English), 5/18/2017 15:11:38, 9309432 bytes\\nDriver: C:\\\\Program Files\\\\Intel\\\\Media SDK\\\\mfxplugin64_hw.dll, 3.11.0005.0020 (English), 5/18/2017 15:11:38, 696056 bytes\\nDriver: C:\\\\Program Files\\\\Intel\\\\Media SDK\\\\mfx_mft_h264ve_64.dll, 6.16.0007.0004 (English), 5/18/2017 15:11:38, 1003520 bytes\\nDriver: C:\\\\Program Files\\\\Intel\\\\Media SDK\\\\mfx_mft_mjpgvd_64.dll, 6.16.0007.0004 (English), 5/18/2017 15:11:38, 932864 bytes\\nDriver: C:\\\\Program Files\\\\Intel\\\\Media SDK\\\\he_64.vp, 5/18/2017 15:11:30, 6533 bytes\\nDriver: C:\\\\Program Files\\\\Intel\\\\Media SDK\\\\c_64.cpa, 5/18/2017 15:11:30, 1519616 bytes\\nDriver: C:\\\\Program Files\\\\Intel\\\\Media SDK\\\\cpa_64.vp, 5/18/2017 15:11:30, 993 bytes\\nDriver: C:\\\\Program Files\\\\Intel\\\\Media SDK\\\\dev_64.vp, 5/18/2017 15:11:30, 21523 bytes\\nDriver: C:\\\\Program Files\\\\Intel\\\\Media SDK\\\\mj_64.vp, 5/18/2017 15:11:38, 6321 bytes\\nDriver: C:\\\\WINDOWS\\\\system32\\\\IntelWiDiMCUMD64.dll, 4.05.0072.0000 (English), 5/18/2017 15:11:36, 150496 bytes\\nDriver: C:\\\\WINDOWS\\\\system32\\\\IntelWiDiSecureSourceFilter64.dll, 4.05.0072.0000 (English), 5/18/2017 15:11:36, 1478624 bytes\\nDriver: C:\\\\WINDOWS\\\\system32\\\\IntelWiDiAAC64.dll, 4.05.0072.0000 (English), 5/18/2017 15:11:36, 4032992 bytes\\nDriver: C:\\\\WINDOWS\\\\system32\\\\IntelWiDiMux64.dll, 4.05.0072.0000 (English), 5/18/2017 15:11:36, 625632 bytes\\nDriver: C:\\\\WINDOWS\\\\system32\\\\IntelWiDiDDEAgent64.dll, 4.05.0072.0000 (English), 5/18/2017 15:11:36, 199648 bytes\\nDriver: C:\\\\WINDOWS\\\\system32\\\\IntelWiDiAudioFilter64.dll, 4.05.0072.0000 (English), 5/18/2017 15:11:36, 668128 bytes\\nDriver: C:\\\\WINDOWS\\\\system32\\\\IntelWiDiUtils64.dll, 4.05.0072.0000 (English), 5/18/2017 15:11:36, 232416 bytes\\nDriver: C:\\\\WINDOWS\\\\system32\\\\IntelWiDiLogServer64.dll, 4.05.0072.0000 (English), 5/18/2017 15:11:36, 116192 bytes\\nDriver: C:\\\\WINDOWS\\\\system32\\\\IntelWiDiWinNextAgent64.dll, 4.05.0072.0000 (English), 5/18/2017 15:11:38, 881120 bytes\\nDriver: C:\\\\WINDOWS\\\\system32\\\\IntelWiDiSilenceFilter64.dll, 4.05.0072.0000 (English), 5/18/2017 15:11:36, 366560 bytes\\nDriver: C:\\\\WINDOWS\\\\system32\\\\IntelWiDiVAD64.exe, 4.05.0072.0000 (English), 5/18/2017 15:17:42, 2497568 bytes\\nDriver: C:\\\\WINDOWS\\\\system32\\\\IntelWiDiUMS64.exe, 4.05.0072.0000 (English), 5/18/2017 15:17:40, 433784 bytes\\nDriver: C:\\\\WINDOWS\\\\system32\\\\igfxCoIn_v4653.dll, 1.03.0018.0000 (English), 5/18/2017 15:11:36, 212480 bytes\\nDriver: C:\\\\WINDOWS\\\\system32\\\\DisplayAudiox64.cab, 5/18/2017 15:11:30, 316245 bytesDevice ID: PCI\\\\VEN_8086&DEV_1E20&SUBSYS_15A71043&REV_04\\\\3&11583659&0&D8\\nDriver: C:\\\\WINDOWS\\\\system32\\\\DRIVERS\\\\hdaudbus.sys, 10.00.16299.0015 (English), 9/30/2017 02:40:59, 86016 bytes\\nDriver: C:\\\\WINDOWS\\\\system32\\\\DRIVERS\\\\drmk.sys, 10.00.16299.0015 (English), 9/30/2017 02:40:59, 96768 bytes\\nDriver: C:\\\\WINDOWS\\\\system32\\\\DRIVERS\\\\portcls.sys, 10.00.16299.0015 (English), 9/30/2017 02:40:59, 379392 bytesDevice ID: PCI\\\\VEN_8086&DEV_1E16&SUBSYS_15A71043&REV_C4\\\\3&11583659&0&E3\\nDriver: C:\\\\WINDOWS\\\\system32\\\\DRIVERS\\\\pci.sys, 10.00.16299.0248 (English), 2/10/2018 19:06:11, 362904 bytesDevice ID: PCI\\\\VEN_8086&DEV_1E26&SUBSYS_15A71043&REV_04\\\\3&11583659&0&E8\\nDriver: C:\\\\WINDOWS\\\\system32\\\\drivers\\\\usbehci.sys, 10.00.16299.0015 (English), 9/30/2017 02:41:08, 95640 bytes\\nDriver: C:\\\\WINDOWS\\\\system32\\\\drivers\\\\usbport.sys, 10.00.16299.0015 (English), 9/30/2017 02:41:08, 454040 bytes\\nDriver: C:\\\\WINDOWS\\\\system32\\\\drivers\\\\usbhub.sys, 10.00.16299.0015 (English), 9/30/2017 02:41:08, 513944 bytesDevice ID: PCI\\\\VEN_8086&DEV_1E57&SUBSYS_15A71043&REV_04\\\\3&11583659&0&F8\\nDriver: C:\\\\WINDOWS\\\\system32\\\\DRIVERS\\\\msisadrv.sys, 10.00.16299.0015 (English), 9/30/2017 02:41:03, 18840 bytesDevice ID: PCI\\\\VEN_8086&DEV_1E2D&SUBSYS_15A71043&REV_04\\\\3&11583659&0&D0\\nDriver: C:\\\\WINDOWS\\\\system32\\\\drivers\\\\usbehci.sys, 10.00.16299.0015 (English), 9/30/2017 02:41:08, 95640 bytes\\nDriver: C:\\\\WINDOWS\\\\system32\\\\drivers\\\\usbport.sys, 10.00.16299.0015 (English), 9/30/2017 02:41:08, 454040 bytes\\nDriver: C:\\\\WINDOWS\\\\system32\\\\drivers\\\\usbhub.sys, 10.00.16299.0015 (English), 9/30/2017 02:41:08, 513944 bytesDirectShow Filters:\\nWMAudio Decoder DMO,0x00800800,1,1,WMADMOD.DLL,10.00.16299.0015\\nWMAPro over S/PDIF DMO,0x00600800,1,1,WMADMOD.DLL,10.00.16299.0015\\nWMSpeech Decoder DMO,0x00600800,1,1,WMSPDMOD.DLL,10.00.16299.0015\\nMP3 Decoder DMO,0x00600800,1,1,mp3dmod.dll,10.00.16299.0015\\nMpeg4s Decoder DMO,0x00800001,1,1,mp4sdecd.dll,10.00.16299.0015\\nWMV Screen decoder DMO,0x00600800,1,1,wmvsdecd.dll,10.00.16299.0015\\nWMVideo Decoder DMO,0x00800001,1,1,wmvdecod.dll,10.00.16299.0015\\nMpeg43 Decoder DMO,0x00800001,1,1,mp43decd.dll,10.00.16299.0015\\nMpeg4 Decoder DMO,0x00800001,1,1,mpg4decd.dll,10.00.16299.0015\\nWD Secure Source Filter,0x00200000,0,1,WDSecureSourceFilter.dll,3.05.0040.0000\\nDV Muxer,0x00400000,0,0,qdv.dll,10.00.16299.0015\\nColor Space Converter,0x00400001,1,1,quartz.dll,10.00.16299.0015\\nWM ASF Reader,0x00400000,0,0,qasf.dll,12.00.16299.0015\\nAVI Splitter,0x00600000,1,1,quartz.dll,10.00.16299.0015\\nVGA 16 Color Ditherer,0x00400000,1,1,quartz.dll,10.00.16299.0015\\nSBE2MediaTypeProfile,0x00200000,0,0,sbe.dll,10.00.16299.0015\\nMicrosoft DTV-DVD Video Decoder,0x005fffff,2,4,msmpeg2vdec.dll,10.00.16299.0248\\nDS Video Buffer Filter,0x00200000,1,1,DSBuffer_Video.ax,3.05.0040.0000\\nAC3 Parser Filter,0x00600000,1,1,mpg2splt.ax,10.00.16299.0015\\nStreamBufferSink,0x00200000,0,0,sbe.dll,10.00.16299.0015\\nMJPEG Decompressor,0x00600000,1,1,quartz.dll,10.00.16299.0015\\nMPEG-I Stream Splitter,0x00600000,1,2,quartz.dll,10.00.16299.0015\\nSAMI (CC) Parser,0x00400000,1,1,quartz.dll,10.00.16299.0015\\nVBI Codec,0x00600000,1,4,VBICodec.ax,10.00.16299.0015\\nMPEG-2 Splitter,0x005fffff,1,0,mpg2splt.ax,10.00.16299.0015\\nClosed Captions Analysis Filter,0x00200000,2,5,cca.dll,10.00.16299.0015\\nSBE2FileScan,0x00200000,0,0,sbe.dll,10.00.16299.0015\\nMicrosoft MPEG-2 Video Encoder,0x00200000,1,1,msmpeg2enc.dll,10.00.16299.0015\\nInternal Script Command Renderer,0x00800001,1,0,quartz.dll,10.00.16299.0015\\nMPEG Audio Decoder,0x03680001,1,1,quartz.dll,10.00.16299.0015\\nTechSmith File Source,0x00400000,0,1,CamtasiaFilters.dll,8.01.0002.1327\\nTechSmith SWF Writer,0x00200000,2,0,CamtasiaFilters.dll,8.01.0002.1327\\nTechSmith WMFSDK Writer,0x00200000,1,0,CamtasiaFilters.dll,8.01.0002.1327\\nTechSmith Simple PIP,0x00200000,0,0,CamtasiaFilters.dll,8.01.0002.1327\\nImageSource,0x00200000,0,0,CamtasiaFilters.dll,8.01.0002.1327\\nTitleSource,0x00200000,0,0,CamtasiaFilters.dll,8.01.0002.1327\\nTechSmith Time Adjust,0x00200000,1,1,CamtasiaFilters.dll,8.01.0002.1327\\nTechSmith Splitter Filter,0x00200000,1,1,CamtasiaFilters.dll,8.01.0002.1327\\nTechSmith Frame Skip Filter,0x00200000,1,1,CamtasiaFilters.dll,8.01.0002.1327\\nTechSmith Perf Skip Filter,0x00200000,1,1,CamtasiaFilters.dll,8.01.0002.1327\\nTechSmith ZoomPIP Filter,0x00200000,2,1,CamtasiaFilters.dll,8.01.0002.1327\\nTechSmith PushVMR Source,0x00200000,0,1,CamtasiaFilters.dll,8.01.0002.1327\\nTechSmith PushBitmap Source,0x00200000,0,2,CamtasiaFilters.dll,8.01.0002.1327\\nTechSmith PushBitmap Source,0x00200000,0,2,CamtasiaFilters.dll,8.01.0002.1327\\nTechSmith SimplePushBitmap Source,0x00200000,0,2,CamtasiaFilters.dll,8.01.0002.1327\\nTechSmith Wave Dest,0x00200000,0,0,CamtasiaFilters.dll,8.01.0002.1327\\nTechSmith Overlay,0x00200000,1,1,CamtasiaFilters.dll,8.01.0002.1327\\nTechSmith Wave Buffer,0x00200000,1,0,CamtasiaFilters.dll,8.01.0002.1327\\nTechSmith ForceColor 8,0x00200000,0,0,CamtasiaFilters.dll,8.01.0002.1327\\nTechSmith ForceColor 555,0x00200000,0,0,CamtasiaFilters.dll,8.01.0002.1327\\nTechSmith ForceColor 565,0x00200000,0,0,CamtasiaFilters.dll,8.01.0002.1327\\nTechSmith ForceColor 24,0x00200000,0,0,CamtasiaFilters.dll,8.01.0002.1327\\nTechSmith ForceColor 32,0x00200000,0,0,CamtasiaFilters.dll,8.01.0002.1327\\nTechSmith Force Color32A,0x00200000,0,0,CamtasiaFilters.dll,8.01.0002.1327\\nSSFileWriter,0x00200000,1,0,CamtasiaFilters.dll,8.01.0002.1327\\nTechSmith Frame Rate Tuner,0x00200000,1,1,CamtasiaFilters.dll,8.01.0002.1327\\nTechSmith Camera Adjust,0x00200000,1,1,CamtasiaFilters.dll,8.01.0002.1327\\nTechSmith Sound Effects Filter,0x00200000,1,1,CamtasiaFilters.dll,8.01.0002.1327\\nTechSmith Flv Key Frame Setter,0x00200000,1,1,CamtasiaFilters.dll,8.01.0002.1327\\nTechSmith Floating Point Wave Filter,0x003fffff,1,1,CamtasiaFilters.dll,8.01.0002.1327\\nDV Splitter,0x00600000,1,2,qdv.dll,10.00.16299.0015\\nVideo Mixing Renderer 9,0x00200000,1,0,quartz.dll,10.00.16299.0015\\nMicrosoft MPEG-2 Encoder,0x00200000,2,1,msmpeg2enc.dll,10.00.16299.0015\\nACM Wrapper,0x00600000,1,1,quartz.dll,10.00.16299.0015\\nVideo Renderer,0x00800001,1,0,quartz.dll,10.00.16299.0015\\nMPEG-2 Video Stream Analyzer,0x00200000,0,0,sbe.dll,10.00.16299.0015\\nLine 21 Decoder,0x00600000,1,1,\\nVideo Port Manager,0x00600000,2,1,quartz.dll,10.00.16299.0015\\nVideo Renderer,0x00400000,1,0,quartz.dll,10.00.16299.0015\\nVPS Decoder,0x00200000,0,0,WSTPager.ax,10.00.16299.0015\\nIntel® AAC encoder,0x00200000,1,1,intelaac.dll,\\nWM ASF Writer,0x00400000,0,0,qasf.dll,12.00.16299.0015\\nVBI Surface Allocator,0x00600000,1,1,vbisurf.ax,\\nFile writer,0x00200000,1,0,qcap.dll,10.00.16299.0015\\nIntel® Mux Renderer,0x00200000,2,0,intelmux.dll,3.05.0040.0000\\nDVD Navigator,0x00200000,0,3,qdvd.dll,10.00.16299.0015\\nOverlay Mixer2,0x00200000,1,1,\\nAVI Draw,0x00600064,9,1,quartz.dll,10.00.16299.0015\\nTechSmith Grabber Sample Filter,0x00200000,1,1,CamtasiaFilters.dll,8.01.0002.1327\\nMicrosoft MPEG-2 Audio Encoder,0x00200000,1,1,msmpeg2enc.dll,10.00.16299.0015\\nWST Pager,0x00200000,1,1,WSTPager.ax,10.00.16299.0015\\nMPEG-2 Demultiplexer,0x00600000,1,1,mpg2splt.ax,10.00.16299.0015\\nDV Video Decoder,0x00800000,1,1,qdv.dll,10.00.16299.0015\\nSampleGrabber,0x00200000,1,1,qedit.dll,10.00.16299.0015\\nNull Renderer,0x00200000,1,0,qedit.dll,10.00.16299.0015\\nMPEG-2 Sections and Tables,0x005fffff,1,0,Mpeg2Data.ax,10.00.16299.0015\\nMicrosoft AC3 Encoder,0x00200000,1,1,msac3enc.dll,10.00.16299.0015\\nStreamBufferSource,0x00200000,0,0,sbe.dll,10.00.16299.0015\\nSmart Tee,0x00200000,1,2,qcap.dll,10.00.16299.0015\\nOverlay Mixer,0x00200000,0,0,\\nAVI Decompressor,0x00600000,1,1,quartz.dll,10.00.16299.0015\\nWD Audio Filter,0x00200000,0,1,WDAudioFilter.dll,3.05.0040.0000\\nAVI/WAV File Source,0x00400000,0,2,quartz.dll,10.00.16299.0015\\nWave Parser,0x00400000,1,1,quartz.dll,10.00.16299.0015\\nMIDI Parser,0x00400000,1,1,quartz.dll,10.00.16299.0015\\nMulti-file Parser,0x00400000,1,1,quartz.dll,10.00.16299.0015\\nFile stream renderer,0x00400000,1,1,quartz.dll,10.00.16299.0015\\nMicrosoft DTV-DVD Audio Decoder,0x005fffff,1,1,msmpeg2adec.dll,10.00.16299.0015\\nStreamBufferSink2,0x00200000,0,0,sbe.dll,10.00.16299.0015\\nAVI Mux,0x00200000,1,0,qcap.dll,10.00.16299.0015\\nLine 21 Decoder 2,0x00600002,1,1,quartz.dll,10.00.16299.0015\\nFile Source (Async.),0x00400000,0,1,quartz.dll,10.00.16299.0015\\nFile Source (URL),0x00400000,0,1,quartz.dll,10.00.16299.0015\\nWDSource Filter,0x00200000,0,1,WDSourceFilter.dll,3.05.0040.0000\\nWD Silence Filter,0x00200000,0,1,WDSilenceFilter.dll,\\nInfinite Pin Tee Filter,0x00200000,1,1,qcap.dll,10.00.16299.0015\\nEnhanced Video Renderer,0x00200000,1,0,evr.dll,10.00.16299.0248\\nIntel®WiDi H264 encoder,0x00200000,1,1,h264HWEnc.dll,\\nBDA MPEG2 Transport Information Filter,0x00200000,2,0,psisrndr.ax,10.00.16299.0015\\nMPEG Video Decoder,0x40000001,1,1,quartz.dll,10.00.16299.0015WDM Streaming Tee/Splitter Devices:\\nTee/Sink-to-Sink Converter,0x00200000,1,1,ksproxy.ax,10.00.16299.0015Video Compressors:\\nWMVideo8 Encoder DMO,0x00600800,1,1,wmvxencd.dll,10.00.16299.0248\\nWMVideo9 Encoder DMO,0x00600800,1,1,wmvencod.dll,10.00.16299.0015\\nMSScreen 9 encoder DMO,0x00600800,1,1,wmvsencd.dll,10.00.16299.0248\\nDV Video Encoder,0x00200000,0,0,qdv.dll,10.00.16299.0015\\nMJPEG Compressor,0x00200000,0,0,quartz.dll,10.00.16299.0015Audio Compressors:\\nWM Speech Encoder DMO,0x00600800,1,1,WMSPDMOE.DLL,10.00.16299.0015\\nWMAudio Encoder DMO,0x00600800,1,1,WMADMOE.DLL,10.00.16299.0015\\nIMA ADPCM,0x00200000,1,1,quartz.dll,10.00.16299.0015\\nPCM,0x00200000,1,1,quartz.dll,10.00.16299.0015\\nMicrosoft ADPCM,0x00200000,1,1,quartz.dll,10.00.16299.0015\\nGSM 6.10,0x00200000,1,1,quartz.dll,10.00.16299.0015\\nCCITT A-Law,0x00200000,1,1,quartz.dll,10.00.16299.0015\\nCCITT u-Law,0x00200000,1,1,quartz.dll,10.00.16299.0015\\nMPEG Layer-3,0x00200000,1,1,quartz.dll,10.00.16299.0015Audio Capture Sources:\\nMicrophone (Realtek High Definition Audio),0x00200000,0,0,qcap.dll,10.00.16299.0015PBDA CP Filters:\\nPBDA DTFilter,0x00600000,1,1,CPFilters.dll,10.00.16299.0192\\nPBDA ETFilter,0x00200000,0,0,CPFilters.dll,10.00.16299.0192\\nPBDA PTFilter,0x00200000,0,0,CPFilters.dll,10.00.16299.0192Midi Renderers:\\nDefault MidiOut Device,0x00800000,1,0,quartz.dll,10.00.16299.0015\\nMicrosoft GS Wavetable Synth,0x00200000,1,0,quartz.dll,10.00.16299.0015WDM Streaming Capture Devices:\\nUSB2.0 HD UVC WebCam,0x00200000,1,2,ksproxy.ax,10.00.16299.0015\\nRealtek HD Audio Mic input,0x00200000,1,1,ksproxy.ax,10.00.16299.0015\\nRealtek HD Audio Stereo input,0x00200000,1,1,ksproxy.ax,10.00.16299.0015WDM Streaming Rendering Devices:\\nRealtek HD Audio output,0x00200000,1,1,ksproxy.ax,10.00.16299.0015BDA Network Providers:\\nMicrosoft ATSC Network Provider,0x00200000,0,1,MSDvbNP.ax,10.00.16299.0015\\nMicrosoft DVBC Network Provider,0x00200000,0,1,MSDvbNP.ax,10.00.16299.0015\\nMicrosoft DVBS Network Provider,0x00200000,0,1,MSDvbNP.ax,10.00.16299.0015\\nMicrosoft DVBT Network Provider,0x00200000,0,1,MSDvbNP.ax,10.00.16299.0015\\nMicrosoft Network Provider,0x00200000,0,1,MSNP.ax,10.00.16299.0015Video Capture Sources:\\nUSB2.0 HD UVC WebCam,0x00200000,1,2,ksproxy.ax,10.00.16299.0015Multi-Instance Capable VBI Codecs:\\nVBI Codec,0x00600000,1,4,VBICodec.ax,10.00.16299.0015BDA Transport Information Renderers:\\nBDA MPEG2 Transport Information Filter,0x00600000,2,0,psisrndr.ax,10.00.16299.0015\\nMPEG-2 Sections and Tables,0x00600000,1,0,Mpeg2Data.ax,10.00.16299.0015BDA CP/CA Filters:\\nDecrypt/Tag,0x00600000,1,1,EncDec.dll,10.00.16299.0192\\nEncrypt/Tag,0x00200000,0,0,EncDec.dll,10.00.16299.0192\\nPTFilter,0x00200000,0,0,EncDec.dll,10.00.16299.0192\\nXDS Codec,0x00200000,0,0,EncDec.dll,10.00.16299.0192WDM Streaming Communication Transforms:\\nTee/Sink-to-Sink Converter,0x00200000,1,1,ksproxy.ax,10.00.16299.0015Audio Renderers:\\nSpeakers (Realtek High Definition Audio),0x00200000,1,0,quartz.dll,10.00.16299.0015\\nDefault DirectSound Device,0x00800000,1,0,quartz.dll,10.00.16299.0015\\nDefault WaveOut Device,0x00200000,1,0,quartz.dll,10.00.16299.0015\\nDirectSound: Speakers (Realtek High Definition Audio),0x00200000,1,0,quartz.dll,10.00.16299.0015So I’m having this exact same problem with my openGL game.\\nYour demo is buggy on the same machines my game is buggy on.I posted about it here:\\n[url]What do nvldumdx.dll & nvd3dumx.dll do? - OpenGL - NVIDIA Developer ForumsIt’s obviously not related to those dll’s, but in my case when I record and replay with apitrace I get the correct results.What I forgot to mention in my post was that this same code used to work 2 years ago on windows 7.Powered by Discourse, best viewed with JavaScript enabled',\n",
       " 'gllinkprogram-error-using-subroutines': 'I am currently developing a shader library which utilizes subroutines. Compiling the shaders works fine, although linking them outputs this error:Internal error: assembly compile error for fragment shader at offset 7617:I then get the internal assembly text:I have tested this on an ATI 7970 and it works fine.My setup is W7 with a GTX 690 using the 340.52 driver.I forgot to post the GLSL code used to generate the error.Powered by Discourse, best viewed with JavaScript enabled',\n",
       " 'failed-to-create-cloudxr-receiver-error-16-a-parameter-is-invalid-cxrerror-parameter-invalid': 'Hello Guys,I’m randomly getting below error :\\nFailed to create CloudXR receiver. Error 16, A parameter is invalid [cxrError_Parameter_Invalid].How to fix this issue? Please suggest some solution.Thanks,we’d need to see the code and structures leading up to the create call.Powered by Discourse, best viewed with JavaScript enabled',\n",
       " 'double-precision-trigonometric-functions': 'Hi all,I’m having a code that is doing a lot of computations in double precision. It worked fine with OptiX 2.5. But as I’m trying to migrate the project to OptiX 3.5 and CUDA toolkit 5.5, I’m getting the following runtime error:Details: Function “_rtProgramCreateFromPTXFile” caught exception: defs/uses not defined for PTX instruction (Try --use_fast_math):  madc.hi, [1310866])I traced the error down to calls of the cos- and sin-functions for double variables. If I compile the project for compute capability 1.3 (-arch sm_13) and use fast math (–use_fast_math) everything works fine. I think I read somewhere, that compiling for this compute capability automatically treats all doubles as floats.Am I right that usage of cos-calls for double precision are not allowed in OptiX 3.5 anymore?\\nIs there anything I can do about it?Hi all,with the help of the OptiX team I was able to locate the problem. It seems that CUDA toolkits starting from 4.2 produce ptx code, that is incompatible with OptiX, for a call to double precision trigonometric functions. So my solution is to use OptiX 3.5 with CUDA 4.0. This seems to work for me.Powered by Discourse, best viewed with JavaScript enabled',\n",
       " 'implement-new-operating-system-riscos-on-nvidia-jetson-nano': 'We plan to adopt an existing operating system RISCOS (which was the original operating system for ARM CPU’s) to a new hardware platform. Therefore we investigate the possibility to implement it on the NVIDIA Jetson family as they have high performance GPU’s.\\nMain issue is here how we can implement the video drivers. Can we get support by NVIDIA, Can we get source files (C++ or ARM Assember) to implement the low level functions like video decompression, shading functions etc.?Powered by Discourse, best viewed with JavaScript enabled',\n",
       " 'crash-from-cg-call-when-called-from-multiple-threads-geforce-gtx-750-ti': 'I am working on an image display application in which I am using cg shaders from multiple threads. It is a Windows application and each thread has its own HGLRC. But at times the cg calls like cgGLIsProfileSupported gets crashed when simultaneously accessed from different threads. The call stack of the crash is as from WinDbg is as below. For both thread the crash point and call stack is similarIt is observed that the issue occurs only for GeForce GTX 750 TI card. The issue is not observed for other cards like GTX 1050 or lower cards like GTX 640. Have anybody observed similar issues and any suggestions/solutions for the problem?The Cg Toolkit is a legacy NVIDIA toolkit no longer under active development or support.\\nhttps://developer.nvidia.com/cg-toolkit\\nI think nVidia will not fix this problem.Powered by Discourse, best viewed with JavaScript enabled',\n",
       " 'cuda-on-shield-tv-with-android-8-0': 'Hi,We have an Android app using CUDA. Previously, we could successfully run it on the Shield TV. At the moment, we can only successfully run it on the Shield K1 tablet.When we try to run the app on the Shield TV, we get cudaErrorInsufficientDriver.We’re using the CUDA for Android Toolkit 7.0.When targeting the Shield TV, we compile our CUDA code for the Maxwell GPU, as per the Makefile below:We embed the libcudart_static.a library into the APK.The Shield K1 tablet is running Android 7.0 and the Shield TV is running Android 8.0.We have these questions please:Powered by Discourse, best viewed with JavaScript enabled',\n",
       " 'flex-unity-beta-updates': 'Flex has been released on the Unity Asset Store as beta. There’s a lot I’d like to do to improve. Many features and integrations on the C# end. Is there a place to suggest these changes / throw up some pull requests?Powered by Discourse, best viewed with JavaScript enabled',\n",
       " 'nvidia-driver-440-freeze-after-resume-from-hibernation': 'I am running ubuntu 18.04.4 LTS, kernel 5.3.0-45-generic, using the package nvidia-driver-440, GTX1050ti, and after resume from hibernation the system graphics hangs and am not even able to kill Xorg.This problem happens after all hibernation, not instantly after wake up but after a bit of usage, generally when triggering some animation of the DEI would be happy if anyone could tell me of any workaround too because after freezing killall -9 Xorg doesn’t do much and any other attempt to restart Xorg. During this freeze switching tty also doesn’t work. Audio, network ( i could ssh to the machine), everything else is still running.I meet the same situation.\\nMy gpu is 1060Ti.\\nAfter resume from hibernate and wait a while, screen freezes, just mouse pointer can moves. I can remote-ssh into system.\\nThis is nvidia bug report.\\nnvidia-bug-report.log.gz (356.3 KB)This is my syslog.\\nsyslog-j-1 (190.7 KB)I have experienced the same issue.Powered by Discourse, best viewed with JavaScript enabled',\n",
       " 'unable-to-get-opencv-2-4-8-2-tegra-sdk-tutorial-4-cuda-sample-to-work': 'I’m trying to use Android Studio to import the tutorial-4-cuda project that is packaged with the OpenCV-2.4.8.2-Tegra-sdk, however I keep getting an undefined reference error involving the call to cv::gpu::FAST_GPU.  I know that this project works on the Shield tablet, which is what I am developing for, because I am able to push the .apk to the tablet.  However, I develop using Android Studio (it is the supported IDE by Android) and there is no documentation from NVIDIA on how to use the Opencv samples for Tegra within the Android Studio IDE.  Please help…Powered by Discourse, best viewed with JavaScript enabled',\n",
       " 'light-linked-list-broken-on-geforce-cards': 'Hello,\\nI developed the Light Linked List algorithm and I am currently running it flawlessly on consoles and PCs equipped with Radeons.\\nHowever I tried to run it on two new machines one equipped with the GeForce GTX670, the other with a GeForce GTX980 and I am getting terrible artifacts.\\nI tried debugging my app with the latest NVIDIA NSight but the driver crashes when I attempt to capture a frame.\\nHere’s a link to a light-weight LLL demo:Contribute to JavaCoolDude/Advanced-Lighting development by creating an account on GitHub.\\nI would appreciate some help solving this problem,\\nThank you,\\nAbdulI suspect the issues that I am seeing are related to the implementation of InterlockedExchange function in a pixel shader.\\nBtw you can reload the shaders at runtime by pressing R.\\nAbdulI changed the implementation and I no longer suspect the atomic functions: it’s almost as if some light shells primitives are not rasterized or something…\\n[url]http://i57.tinypic.com/359wpcw.png[/url]Well through sheer grinding I found out what the issue was:\\nI am passing a unique light index in the 0-255 range from the vertex shader to the pixel shader. The light index is the same for all vertices in a given draw call.\\nApparently when my index value makes to the pixel shader sometimes it “loses” precision and drops down to I-1, so instead of let’s say index “5” I end up getting 4.xxx which when converted to an uint turns into “4”.\\nMy quick fix was to nudge the index in the vertex shader:output.fLightIndex  = instance.m_LightIndex + 0.5f;AbdulOr use an unsigned int data type and “flat” interpolation modifier to keep it intact.\\nFrom 2010: [url]Passing integer from vertex to fragment shader? - OpenGL - Khronos ForumsHey Detlef,\\nThank you for the input, since I am using Directx and HLSL I ended up using an unsigned int and the “nointerpolation” flag:struct VS_OUTPUT_LIGHT\\n{\\nfloat4               vPosition    : SV_POSITION;\\nnointerpolation uint iLightIndex  : TEXCOORD0;\\n};AbdulPowered by Discourse, best viewed with JavaScript enabled',\n",
       " '16bit-texture-object-in-sutil-7-sdk': 'in OptiX 7.0.0 SDK project “sutil_7_sdk” file Scene.cpp\\nin Scene::addImage() in line 476is also used for 16bit textures.From the CUDA docs I found:\\ncudaCreateTextureObject() creates the texture object from the resource description … “cudaResourceDesc”.And in Scene::addSampler() that resource description uses then also that cuda array (built with the uchar4 channel description) on a 16bit texture…\\nIs there an internal conversion? Or is this intended?so for 16bit textures I would expect :Or I am wrong?Yes, that looks incorrect inside the examples.\\nI’ll file a bug report to have it fixed.Powered by Discourse, best viewed with JavaScript enabled',\n",
       " 'slowdown-problem-on-multiple-screens-nvs-810': 'Hello community NVIDIAWe are developing an 8x2 VideoWall (16 monitors with 1920x1080 resolution) using mosaic technology with the following resources. Our purpose is to show network video sources using the h264 and h264 codecs, spread it across the panel, be able to show it on a monitor with 1920x1080 resolution or use 4 monitors to obtain 4k resolution.We currently have these resources for its development.*** Nvidia Quadro NVS 810 (in total there are 2 graphics cards with 8 outputs each, they are cards with 2 integrated GPUs each)**\\n*** Driver version: 515.65.01**\\n*** OS: RedHat8**\\n*** Desktop: Xfce**\\n*** Window system X**Well, the problem we have is that when we start to fill the video monitors, it starts to slow down a lot. We have verified that it is not a CPU load error or a bandwidth error, since it happens to us with both network video and local video. I think that the failure may come from some limitation of the NVS810 card that we are using.I have read in an NVIDIA user guide that this type of graphics is intended for static displays or slow motion video.DU-05620-001_v13 | April 5, 2017\\nDigital Signage is designed to be viewed from a distance and typically has slow moving video or static images. This type of application is ideal for NVIDIA’s NVS 810, Quadro K1200 or P1000 GPUs, which supports 4 displays per card. The NVS 810, Quadro K1200, or Quadro P1000 can be used in smaller workstations.I have also read in different forums, problems very similar to ours using the same hardware.I wanted to know if this type of behavior can be normal using the NVS810 graphics and if so, what type or model of graphics is best suited for our wallDisplay?Greetings and thank you very much.Hi,There will be a definite limit to the number of video streams that can be decoded and displayed and likewise the NVS 810 is limited in decode performance relative to other GPUs.A useful resource to estimate decode performance is our Video CODEC SDK webpage: NVIDIA VIDEO CODEC SDK | NVIDIA DeveloperIf you scroll down there are performance charts showing the decode performance in terms of # of HD streams. The chart uses our datacenter GPUs, however, this can be used to estimate the performance of equivalent desktop GPU and as an example the T4 would be similar to the RTX 4000.For the kind of installation you describe we would probably recommend something like  two RTX 4000 cards (or even higher) instead of the NVS 810 because the offer significantly more performance and can support decoding the required number of streams. The RTX 4000 also support Quadro Sync II which would allow you to synchronise all the display outputs like the NVS 810 offers.I don’t know if you have system constraints in terms of available PCIe x16 slots since clearly two RTX 4000s would consume 2 PCIe x16 slots along with an additional slot for the Quadro Sync II card.You also might be interested in a GTC session we created describing how you could “Build a Super Resolution Video Compositor”: How to Create a Super Resolution Compositor that Scales to 32 Displays | NVIDIA On-DemandIt has a Windows focus however the principles also apply on Linux, even if some of the API specifics are different.Thanks,\\nIanHi,Thank you very much for the response to our query.We have decided to change our current NVS 810 for another one with higher performance,\\nThe answer raises new questions.Looking at the RTX 4000, I see that it only has 3 display port connectors\\nTo support 16 monitors, can we use 2 of the same card?Wouldn’t each miniDP output be connected to a single monitor?Finally, I have seen that the consumption can be around 125W and we do not know if we would go a little fair. I wanted to tell you about the T1000 graph which we had studied for some time.\\nDo you think that card has enough resources for the use we are looking for?thank you very much again.Miguel Ángel.Powered by Discourse, best viewed with JavaScript enabled',\n",
       " 'getting-started-easy-one-shot-code-gen-for-df-evaluators': 'Hi MDL developers,first of all, thanks for your work and for finally making the SDK publicly available! Looks like the MDL really got it all right and could solve many of the interoperability issues that exist with heterogeneous tool pipelines and PBR today.The SDK is quite impressive too (nice lean ABI layer, BTW), and I’m currently reading into the docs / trying to get on top of things, so I hope you don’t mind my (at this point possibly naive) questions.It seems the following recipe would suffice to get some basic rendering and running (only considering the surface BSDF for simplicity of discussion):Yet API-wise the whole thing feels somewhat off-beat, but it also seems needlessly complex to write code against several (in case of supporting CUDA / GLSL) additional APIs for compilation, where I already have compilation abstracted by the MDL SDK…Questions:Hi,Given that I must admit that I’m not fully understanding the details of the ‘hack’ your proposing, I might not be exactly answering your questions - but I hope I can still help you here.Implementing BSDF support is currently the biggest hurdle to get MDL support running in a renderer. Right now we don’t offer a turnkey solution for that, i.e. the compiler cannot generate ready-to-use BSDF code for you yet. Instead you need to implement all BSDF support inside you renderer, which means you basically need to do the following:Quite soon, however, we will offer functionality to compile surface.scattering into ready-to-use BSDF evaluation and importance sampling functions (potentially only for the PTX backend initially). This should make the integration into a physically based renderer much simpler. But even with that, I think implementing your own BSDF machinery can be beneficial since it allows to tightly integrate everything with the small details and peculiarities each renderer has.Like Matthias said the hacky solution is not entirely clear to us, we might need some more context. Maybe some comments regarding OSL:OSL without implementing custom closures cannot faithfully represent MDL’s BSDF. OSL provides many closures that match MDL BSDF (like GGX or a henyey greenstein phase function for volumetric rendering) but BSDF layering can only approximately be implemented in OSL. It would need ideally custom closures (which OSL imho begs for and many renderers like Arnold provide) For a start an approximation might be enough though.right now we have only class vs instance compilation. We work on an option to customize which parameters to expose after compilation but i can not confirm whether it will be part of the next release. It has pretty high priority though.Thanks for your replies!OSL? Wait… Sorry, I’m switching between the two ITM and and both have three letters. I meant MDL, of course! :DOSL without implementing custom closures cannot faithfully represent MDL’s BSDF\\n[…] custom closures which OSL imho begs forYes, no dependence on ‘L’ for measured curve / Schlick. Also: No ‘eta’ or ‘other_ior’ input anywhere for Fresnel vs. anything but air.If I wanted to compile MDL to OSL (despite the limitations), I guess it’d be much easier emitting source code while traversing the expression trees produced by the SDK…“native” is a way to use non mdl functions inside an MDL material. its mostly for debugging purposes.Ah! Mostly? Will it generate calls to external code you can link against via LLVM and PTX APIs (like the PTX backend’s texture call mode “direct_call”)?We work on an option to customize which parameters to expose after compilation but i can not confirm whether it will be part of the next release.\\nIt has pretty high priority though.Quite soon, however, we will offer functionality to compile surface.scattering into ready-to-use BSDF evaluation and importance sampling functions\\n(potentially only for the PTX backend initially). This should make the integration into a physically based renderer much simpler. But even with that,Good news!I think implementing your own BSDF machinery can be beneficial since it allows to tightly integrate everything with the small details and peculiarities\\neach renderer has.Yes, I agree for the integration into a full-blown, existing renderer, yet that’s not the only possible use case out there:I know, “MDL is not supposed to be a shading language”, yet it seems perfectly well-suited to express sampling strategy and distribution functions as you might have in your renderer, preprocessing stage, or whatever tool (say, something new with MDL as its only material representation). MDL is already portable between CPU & GPU, so it would only seem natural to let the user apply it how it suits hir needs. Please make the SDK allow it. Reference implementations for the DFs would be super useful for both direct integration, conformance testing of alternatives and tightening of the spec.Like Matthias said the hacky solution is not entirely clear to usI was talking about programmatically transforming a material into something crude like thisif we’re fine with point lights or brute force, or, with importance sampling might look somewhat like this (I guess this case would need a smarter code transform to split up the probability cake):Note that I’m not seriously considering something like this. I was hoping you’d point me to less crazy realms that let me exploit the MDL-SDK as a (more or less) general purpose compilation and linking framework. Seems like linking currently has to happen via LLVM and/or CUDA APIs.If you feed in the missing state through the material interface, (i.e. view-direction, lightsources, environment) MDL is in principle powerfull enough to implement necessary evaluation, ignoring shadows or complex light transport.\\nI am hestitant to reccomend something like this, even if it could work. It would be quite some work that, after an initial start, would be wasted.“native” generates calls to external code. If you can implement  functionality  outside of MDL that you want to call, what is the big step to writing the whole BSDF eval outside and just call MDL for the texturing functions?The compiler does work outside of the MDL material evaluation. in iray we for example use it also to allow programming custom environment functions. So if you just look for a compiler infrastructure that generates code for multiple targets, that should work. You could probably devisean interface for the bsdf and implement the eval and sample functions as MDL functions, compile to your target architectue and call those mdl functions from your renderer as needed.Instead of the full bsdf machinery, another way would be to implement the ue4 material model only for a start and use distilling to simplify any material to ue4. for the paraeters you could call the compiled MDL code like its shown in the samples shipped with the SDK.\\nIn many cases the differences will be small/0 and once we ship the the bsdf evaluation sample it would be a smaller step to switch to this.Seems like linking currently has to happen via LLVM and/or CUDA APIs\\nLLVM is our universal backend driving everything from I86 CPU to CUDA to potentially everything an LLVM backend exists for. The current samples shiped illustrate calling an MDL function from CPU code. The bsdf sample matthias talked about will eventually work on cpu, but there is additional work to adapt it to windows and linux calling conventions.Powered by Discourse, best viewed with JavaScript enabled',\n",
       " 'lgpl-ffmpeg-and-nvenc-in-a-closed-source-commercial-application': 'We are using the NVENC as part of FFmpeg in a closed source  commercial project. We are compiling FFmpeg using LGPL 2.1 and releasing the FFmpeg source code in compliance with that license. Is the NVENC code that is compiled into the FFmpeg binaries compatible with LGPL 2.1? Do we need to release the NVENC source code? If not, how do you suggest going about using FFmpeg and NVENC in a closed source commercial application?I am not a laywer, etc, but we wrote the ffmpeg code that interfaces with nvenc so that turning it on did not stop your ffmpeg build from being lgpl compliant (it does not requrie the non-free flag).The basic theory here is that nvenc itself is covered by the system library exception (the libraries come with the driver) and the header files that are bundled with ffmpeg are MIT licensed, and so are lgpl compatible.So as long as you are following lgpl 2.1 in your use of ffmpeg, turning on nvenc support does not change anything.Powered by Discourse, best viewed with JavaScript enabled',\n",
       " 'nvidia-t4-power-consumption': 'Dear Sir/ Madam, recently we launched a PC project with building in a T4 and Windows 10 platform. We designed the power supply based on 70 watt power consumption specification of T4. But one of our customers fed back that they lost the T4 when burning in their AI model to maximum. And they found the T4 could run over 90 watt as the system at busiest moment. If this is possible to run over the 70 watt specification, please how is the technical limitation of T4? And is there any method to limit this to avoid the hardware broken?Powered by Discourse, best viewed with JavaScript enabled',\n",
       " 'bug-report-crash-managing-shared-resources-in-multithreaded-app-using-opengl-4-5-on-geforce-g': 'Looks like a bug.\\nIn my application I use two different threads (and two shared contexts) to upload and render data.The app sometimes crashes (it seems when the workload is higher than average) when the main thread renders (glMultiDrawArraysIndirect), using buffers (the problem is with GL_DRAW_INDIRECT_BUFFER, I suppose) uploaded in another thread.(Exception thrown at 0x000001CD7121F302 in viewer.exe: 0xC0000005: Access violation reading location 0x0000000000000100)if I use debug mode there are no GL errors.some simplified pseudocode:\\nWorking thread:\\n{\\n…\\nuploadContext->makeCurrent();\\nglCreateBuffers(1, &tile.geometryBuf);\\nglNamedBufferStorage(tile.geometryBuf, geometryBuffer.size(), geometryBuffer.constData(), 0);glCreateBuffers(1, &tile.indirectBuffer);\\nglNamedBufferStorage(tile.indirectBuffer, indirectBuffer.size(), indirectBuffer.constData(), 0);glCreateBuffers(1, &tile.settingsBuffer);//);\\nglNamedBufferStorage(tile.settingsBuffer, settingsBuffer.size(), settingsBuffer.constData(), 0);//need this for syncronization between threads\\ntile.sync = glFenceSync(GL_SYNC_GPU_COMMANDS_COMPLETE, 0);\\nglFlush();emit tileReady(tile); //send a signal to the main thread\\n}Main thread:\\n{\\n…\\nrenderingContext->makeCurrent();\\n///sync\\nGLenum waitReturn = glClientWaitSync(tile.sync, 0, 0);if (waitReturn != GL_ALREADY_SIGNALED && waitReturn != GL_CONDITION_SATISFIED)\\nreturn;\\nglDeleteSync(tile.sync);//binding should validate the buffer in this context according to specs (but seems that it doesn’t)\\nglBindBuffer(GL_DRAW_INDIRECT_BUFFER, tile.indirectBuffer);//vao is defined during initialization\\n//now only change arrayVertexBuffer\\nglVertexArrayVertexBuffer(vao, 0, tile.geometryBuf, 0, 2 * sizeof(float));\\nglBindVertexArray(vao);glBindBufferBase(GL_SHADER_STORAGE_BUFFER, 0, tile.settingsBuffer);\\nglMultiDrawArraysIndirect(GL_TRIANGLES, 0, tile.counts, 0);\\n…\\n}\\nSo it crashes sometimes if like this.It does NOT crash if:Config:\\nWindows 10 Pro, 64 bit\\nGeForce GTX 1060\\ndriver version 416.81Powered by Discourse, best viewed with JavaScript enabled',\n",
       " 'cleanup-fail-with-invalid-context-error': 'Hi. I’m running into issues trying to clean up when exiting application. Invoking rtContextDestroy causes error code 1280, with the following error text:\\nInvalid context (Details: Function “RTresult _rtContextDestroy(RTcontext)” caught exception: Encountered a CUDA error: cudaDriver().CuCtxGetCurrent( &context ) returned (4): DeinitializedI’ve already confirmed that the context is created only once throughout the lifetime of the application, and it’s not destroyed multiple times.\\nI tried running rtDeviceGetDeviceCount when exiting, just out of curiosity, and it segfaults.\\nIf I destroy the context earlier in the application (instead of upon exit), everything seems fine.Anybody have any idea what might be happening?I wish I had an example code to post, but for some reason I can’t reproduce the same problem with smaller applications or with any of SDK examples.\\nI’m using Ubuntu 16.04, Optix 5.1, display driver 396.45 if that helps.I noticed (via lsof) that when the application is exiting, libcudart.so had already been unloaded before rtContextDestroy is invoked. Could that have anything to do with it? If so, what would cause it to unload cudart before context is destroyed?Not sure what could be destroying the CUDA context earlier.  Is this a plugin environment or do you have full control over the application?  The primary CUDA context, which OptiX uses, is shared by everything in the current process.  Is it possible that another thread (or plugin) in the same process is cleaning up first?Powered by Discourse, best viewed with JavaScript enabled',\n",
       " 'cloudxr-3-2-eye-processing-thread-shutdown': 'It seems that CloudXR 3.2 has higher chance to have problems upon launch.There are occasions where I got connected to the server but cannot see anything from the screen.In log, normal logs look like:When the problem occus, logs look like:Both created a receiver connected to the server, but the later’s Eye0 processing thread shutdown for unknown reasons and is not starting Eye1 processing thread. Thus I cannot latch frame successfully since no eye threads are up.I’d probably recommend using -v to generate verbose logs, and then in a failing case quit the app, go find the client and server logs for that run, and post them for further review.I can’t say that I’ve personally ever seen this.  Any chance you are using RDP to the server in the failing case?  That is known to impact capture.  Other than that, maybe logs will point us towards potential issue.We get an issue similar to this, it might be correlated. If you take your headset off and put it on again, sometimes it shows a black screen and the logs show something failed during the re-creation of the swap chain textures inside the CloudXR library. We had something similar in our older 3.0 builds as well but the repro was seemingly random. If you enter / leave the play tracking space multiple times (random number of times), the app would simply crash during the resume function when it’s recreating the swap chains. Overall 3.2 seems actually less stable than 3.0 for us and we’re hoping if there are any hotfixes coming soon then we’ll be able to adopt 3.2 for the higher refresh rates and other features.Again, not something I’ve seen generally here.  client+server+logcat logs right after a crash would be helpful to try to understand the cause.  Explicit repro steps would also be useful, since if we can get it to happen here, we can more directly debug.There’s still some lifecycle work needed on both Focus and Quest.  Focus in a sense hard-pauses your process, so you have limited opportunity to clean up and prep for later resume.  The lifecycles are also slightly different across vendors, let alone can change in subtle ways across versions of the OS.  And what happens to the app when you ‘go home’ is also quite different between the platforms.  This all makes it difficult to have a one-size-fits-all solution.But I think we’re getting there.  3.2 actually for me (having worked on the lifecycle problems) is WAY better for the most part.  There are still some issue to root out, that is for sure.The more logs and details you can provide, the better the chance we can investigate and fix whatever the problem is.Thank you. I took me some time to get back to this problem.\\nI’m running on an Android 12 device and Android 12 doesn’t give WRITE_EXTERNAL_STORAGE permission so it’s a bit awkward for me to have CloudXR logs at this moment.\\nIs there anyway to output CloudXR logs via logcat?Ok, so I just went a bit deeper into this problem, and got myself into a very awkward situation…Phenomenon:\\nWhen this happens (processing thread shutdown), the connection is established and the server graphics will respond to my pose correctly. But there’s nothing I can see in my glasses, and it keeps that way. Latch frame won’t succeed.I was running my client with Nreal’s glasses on an OnePlus 9 with Android 12. Thus I cannot authorize “android.permission.WRITE_EXTERNAL_STORAGE”, and cannot obtain any logs from verbose, local events, stream events, or qos generated by CloudXR SDK itself, as it only writes into /sdcard/CloudXR/logs, which is prohibited by Android 12.So I found myself another phone with Android 10, then I found I had to lower target SDK and related versions to 29 to authorize the correct permissions. When I can finally generate these logs from my client, I can no longer reproduce this problem… My client is really robust on Android 10 somehow…Intermediate Conclusion:\\nIt looks like this problem has something to do with Android 12 at least.\\nIt would be helpful if CloudXR logs can be generated at custom locations, such as inside the application’s data directory.Thanks.Powered by Discourse, best viewed with JavaScript enabled',\n",
       " 'does-cloudxr-require-a-dedicated-gpu': 'Hi,We recently purchased an a40 GPU. However, plan to use it with multiple customer virtual machines. I am wondering if i can still use a VM with high v-ram dedicated from the GPU as a CloudXR server, or if we will need an entire new GPU dedicated for the server.Any notes are much appreciated!The number of users that can be supported by a single, virtualized graphics card is largely dependent on the VR application. We know that for typical encoding workloads a single card Turing or Ampere generation card can support a maximum of two users for HMDs with specs similar to those of the Quest 2.Powered by Discourse, best viewed with JavaScript enabled',\n",
       " 'nvidia-cut-dp-performance-on-new-gpus-as-p6000': 'Nvidia cut DP performance on new Gpus as P6000, so now people who are doing cad work are gonna have to\\nswitch to amd? Or DP is not such a big deal? How to know if software can make use of second gpu or accelerator card? How to know how much is softwares importance of DP?Powered by Discourse, best viewed with JavaScript enabled',\n",
       " 'download-older-vmaterials': 'Since the MDL is an executable file, it seems like there are compatibility issues between when an update to the MDL is released and when a version of software (i.e. SOLIDWORKS) is released.Most customers don’t upgrade to the latest versions of rendering software, unless it’s forced upon them like updates on a phone or OS.Where can a customer download older versions of the MDL if they haven’t updated, or can’t update, their rendering software?We currently provide vMaterials 1.7 and vMaterials 2.0 installers. The 1.7 release is backwards compatible with the prior vMaterials versions and will work just fine.Powered by Discourse, best viewed with JavaScript enabled',\n",
       " 'gl-ovr-multiview-performance-on-rtx3000': 'I am trying to evaluate the ‘GL_OVR_Multiview\\n’ extension on a rtx3000I wrote a small program that render points arranged in a cube to the left side and the right side of a window.I do it by using two methods:\\nThe first one - Rendering the cube to the left side and then rendering it to the right side (sequential).\\nThe seconds method – Rendering the points only once while using the ‘GL_OVR_Multiview’ extension for sending the drawing to both sides of the window.In both methods I measure the average rendering time over some frames.\\n(Just to clear things up, I render without waiting to the vertical sync, swap interval = 0(\\nTo my surprise, I get a quite similar rendering time for both methods.\\nI would expect (according to NVIDIA papers on the subject) that the rendering time in the second method would be reduced by at least 30% comparing to the\\nrendering time of the first method.Are my conclusions correct? Did I do something wrong in my program? Wrong board setup or missing setup?Your help we be highly appreciated,YossiSource.zip (6.9 KB)\\nShaders.zip (1.7 KB)Powered by Discourse, best viewed with JavaScript enabled',\n",
       " 'optix-on-tx1': 'Is OptiX available for Jetson TX1 or TX2 or are there any near-term plans for supporting this architecture?  We would really like to use GPU-accelerated ray tracing on the TX1, and we have already built our application to use OptiX on x86_64 systems.We don’t currently build or release OptiX on ARM.  I’m curious about the use case for this though.We generate radar and optical signatures in near real time that are used for processing on an embedded system on a platform with low power usage.  Accelerated ray tracing is required for the rapid generation of the signatures.  We have already written code to use OptiX for this, and it would be best if we could just use it on the ARM architecture instead of finding another accelerated ray tracing package.I recommend sending mail to optix-help@nvidia.com with as many details as you can – both technical and business context for ARM support in OptiX.  If you have a really compelling case, this request should probably go to the OptiX product manager for consideration.I’m also interested to know if we could move our GPU-accelerated ray tracing components over to the Jetson Nano for light-field transport calculations in our augmented reality devices.We would be quite interested in this as well, and was surprised it was not supported. We use this for projection mapping solutions and it would be useful to be able to use ARM devices. I’ll send an email.We too are using optix for sensor calculations, would be ideal to have it on arm architecture, would save a lot of time . instead of re-implementing ray tracing based logic in CUDAI noticed that on the OptiX Downloads page, it now says that ARM support is coming soon.  Do you know an approximate timeframe for this, and if this includes binaries that can be used on the TX2?  We need to implement ray-tracing on the TX2 within the next few months, and I would prefer to use OptiX.We are also eagerly waiting, would really like to see the ARM Support especially JETSON based devices (nano , tx1 , tx2 …)The only thing I can say for sure is that OptiX on ARM is actively being worked on. We do hope to release some time in the next few months, if all goes well. It’s not quite ready today, but as soon as it is ready we will post here in the forum, so stay tuned.–\\nDavid.@dhart Thanks for the update.  Don’t want to miss this when it’s announced.@dhart , Thank you for the update. :)We look forward to seeing this support!Thanks for the update, do you know if OptiX on ARM will be released soon?  We are getting to the point in our development in which we will need GPU accelerated ray tracing on the TX2, and we will need to use a different solution if OptiX is not available.It looks like OptiX on the TX2 is still not available.  Do you know what other options are available?  We’re thinking of potentially using the Vulkan ray tracing API (not sure if the TX2 GPU supports it) or OpenGL Compute Shaders, thanks.This would be fantastic. Any update on the progress, is support expected in the next version of OptiX?Finally… ARM support is now live! NVIDIA OptiX™ Downloads | NVIDIA Developer–\\nDavid.Thanks!Amazing, thanks!Powered by Discourse, best viewed with JavaScript enabled',\n",
       " 'help-me-get-the-pix-disassembly-dll': 'yo how to get the pix disassembly dll, i sent multiple submissions but i got nothing\\n(hints: i don’t work at any company, just a lone learner)\\nthank youPowered by Discourse, best viewed with JavaScript enabled',\n",
       " 'a-problem-with-gstream-to-collect-video-stream-in-real-time': 'Hi!\\nI use a Raspberry PI camera(IMX 219) to capture the video. Now I want to save the real time video stream in the jetson nano with gstream.The command is following:\\ngst-launch-1.0 v4l2src ! video/x-raw-yuv,format=(fourcc)YUY2,width=320,height=240 ! xvimagesinkBut the error information as following:\\n0.10-style raw video caps are being created. Should be video/x-raw,format=(string)… now.\\nWARNING:erroneous pipeline:could not parse caps “video/x-raw-yuv,format=(fourcc)YUY2,width=320,height=240”how to solve it?\\nThanks ver much!Powered by Discourse, best viewed with JavaScript enabled',\n",
       " 'how-to-get-mosaic-topology-gpu-display-order-and-position-by-code': 'I Try to  use python and c6++ to retrieve info of desktop screen.\\nBut can not get the info of MOSAIC TOPOLOGY GPU DISPLAY POSITION. IS THERE ANY FUNCTION OR METHOD TO GET THAT INFO?Hi carlosstarw,Thanks for the question.You can use NVAPI to get information about the MOSIAC topology.The  function:NvAPI_Mosaic_EnumDisplayGridswill return the topology information for the Display grid.Is there anyway to use NVAPI from python or c++?Is there anyway to use NVAPI from python or c++? in windows?\\nI test with python but still can not connect to NVAPIOr can we use ConfigureMosaic to retrieve NvAPI_Mosaic_EnumDisplayGrids data?configureMosaic.exe listconfigcmdThis will give you the current MOSAIC setup.NVAPI should work with c++ - I don’t know of it working with python.configureMosaic.exe set rows=1 cols=1 width=1920, height=1080, freq=60, rr=60.000000out=0,0 rotate=0  nextgrid rows=1 cols=1 width=1920, height=1200, freq=59, rr=60.000000out=0,1 rotate=0  nextgrid rows=1 cols=1 width=1920, height=1200, freq=59, rr=60.000000out=0,2 rotate=0I can have the mosaic info. like this. but I can not know actually position of display. how can I get relation between mosaic info. and Display info\\nDisplay info example.\\nDisplay 1 out=0,1 Geometry Position  = 0, 1200, Display 1 out=0,2 Geometry Position  = 1920, 1200How to have the mosaic info. and monitor ID.\\nUse listconfigcmd only can have mosaic setup but can not have monitor ID.is that correct to get the correspondence relation between the configuremosaic.exe listconfigcmd and configuremosaic.exe query current?configureMosaic.exe set rows=1 cols=1 width=1920, height=1080, freq=60, rr=60.000000out=0,0 rotate=0  nextgrid rows=1 cols=1 width=1920, height=1200, freq=59, rr=60.000000out=0,1 rotate=0  nextgrid rows=1 cols=1 width=1920, height=1200, freq=59, rr=60.000000out=0,2 rotate=0Hi carlosstarw,I am not sure I understand your question.  Have you tried runningconfiguremosaic.exe listconfigcmdCan you share the output from your machine and what is missing with respect to what you are looking for.thanks\\n-D-Powered by Discourse, best viewed with JavaScript enabled',\n",
       " 'imp-pxgetfoundation-linker-error': 'I am using PhysX-3.3.0 (17045688).When linking my program, I get the error(non-resolved external symbol)I can’t find a .cpp file containing PxGetFoundation.The same with PxCreateFoundation.SOLVED: Add PhysX3Common_x64.lib to the link list.Powered by Discourse, best viewed with JavaScript enabled',\n",
       " 'kinematic-char-controller-and-vehicle-issue': 'Hello,When the vehicle is put into drive mode and another player walks up to the vehicle, he/she can push it around like it weighs nothing. They can also jump on it and cause it to be pushed up and fly all around. This is caused by the infinite mass kinematic character controller.My question is, how did other people solve this and what is the right direction for me to go in?Currently, to avoid the problem, I put a box obstacle around the vehicle. However, this is not a long term fix. Can someone point me in the right direction?Thanks,\\nChrisHi,I dont use the vehicle extension and I really dont know anything about it.\\nBut maybe you could get a onContactModify -callback. [You need to specifiy this]Within that callback, you can ignore the collisions, or set them with an offset “higher”. (Not good)\\nOr better to set the force to zero if its possible.\\nI dont know if this is a good way to solve it, or if it even works, but this is the first thing I tought when I read this post.But I guess that a obstacle box is a much better solution that the onContactModify - dirty fix.Maybe you could set some PxFilterFlags for the vehicle, so the PxController cant “collide” with it.-\\nAnd you do some fancy sweeps or AABB checks against it - but I think the obstacle is the better solution.Why do you think that obstacles are bad for it?What is when you set the vehicle to a kinematic actor while he is not moving?Again, I dont know the best solution for it or even if this is possible.Powered by Discourse, best viewed with JavaScript enabled',\n",
       " 'driver-issue': 'Report:\\n(NOTE: the problem is solved for me for now; but maybe the solution is a speed decrease)since driver 431.36 (I updated from 419.67 on Jul 11th) my pathtracer application (using OptiX) freezed a lot of times (which it never did before this way).\\nThe application has a heavy kernel load (I tried to implement @c_schied’s  great ASVGF filter [url]https://cg.ivd.kit.edu/atf.php[/url]   [url]https://cg.ivd.kit.edu/publications/2018/adaptive_temporal_filtering/a_svgf.zip[/url]).\\nThere might be still some bugs in my code, but it never freezed the app.\\nAlso the CUDA kernels copying the output buffer to a DirectX 11 Texture were always working before.But after installing driver 431.36  sometimes the app freezed (independent from Denoiser On/Off).\\nOnly ending the process through task manager stopped it. CPU core has 100% but GPU has 0%.\\nSometimes even some visual artefacts occurred.Then I tried to use report level 3. With success. Then no freeze occured !\\nAnd instead of that I added some screenings through “OutputDebugStringA”; again no freezes.My current solution: calling  cudaDeviceSynchronize();  before each kernel launch.\\nHowever, this maybe inefficient, but the freeze completely disappeared !!!The kernels+buffers are very complex, so I cannot provide a simple reproducer.System: OptiX 6.0.0 SDK   CUDA 10.0    GTX 1050 2GB    Win10PRO 64bit (version 1809)  device driver: 431.36   VS2017/VS2019 (toolkit v140 of VS2015)\\nASVGF_Test.jpg2216×1030 649 KB\\n\\nASVGF_Test.jpg2216×1030 649 KB\\nHi @m1,It will be really difficult to determine the exact cause without being able to reproduce, but let’s talk through a few possibilities.Based on what you described, I’m assuming you’re using some CUDA interop here, and possibly allowing some concurrent kernel runs, and/or buffer copies concurrently with CUDA kernels? I’m glad that cudaDeviceSynchronize() is a viable workaround for now. I don’t need to know how the kernels work, but can you describe how your kernels and buffer copies interact with each other? Were you already using cudaDeviceSynchronize() anywhere, or doing anything else to sync your CUDA stream(s) after rendering but before starting the buffer copy? If your buffer copy were to start before your kernel was done rendering, and you copied uninitialized data to the host (and/or to a DX buffer), can you imagine any reasons your code might not work properly? Are you able to investigate the timing difference before and after adding cudaDeviceSynchronize()?Seeing visual artifacts would be one expected result of starting a buffer copy before the OptiX launch kernel is done rendering, but that typically won’t cause hanging, so it sounds like something more serious. The 100% CPU load while hanging might be a strong clue, you might try using a debugger to break while that’s happening and see if you can pin down which module is hanging – you won’t get OptiX symbols, but you might be able to see whether the code is hanging inside a particular DLL. Even watching it hang with Sysinternals’ procexp might offer some extra clues.The most recent driver does have some internal changes that perhaps could be affecting you if you’re using multiple CUDA streams. I don’t know of anything in OptiX that would cause a complete hang, and we haven’t had any other reports of that, but I am taking your report seriously and I’d like to figure out a way to verify which side of the API line your hang is caused by, and start the process of getting it fixed if it’s OptiX. I can imagine a scenario where your app perhaps wasn’t synchronizing correctly but still worked in the old driver, and the new driver could be revealing a latent already-existing problem. This could also be an issue with DirectX interop. But reviewing, inspecting and verifying that your synchronization is correct could help narrow things down.Would you be willing to draw a diagram of the inputs and outputs of each kernel and buffer copy identifying when each of them start and stop and what data each of them depends on?–\\nDavid.Hi David, thanx for your answer.yes, for the output pixel I use CUDA interop (mapping DirectX textures to CUDA);\\ncudaDeviceSynchronize() was always already used before and after that interop.I tried to compare launch times (using the exact camera orientation), but from what I see, there the speed seems not to decrease as I first thought. I see no artefacts in this test; but the app hangs.I sent a diagram with a list all kernels, all buffers and their interactions in a private message.m1Powered by Discourse, best viewed with JavaScript enabled',\n",
       " 'formulation-of-the-temporal-gauss-seidel-tgs-solver': 'Is there any documentation (paper or technical document) about this new introduced TGS solver in PhysX 4.0?We haven’t yet but we should publish papers on it in the near future.Powered by Discourse, best viewed with JavaScript enabled',\n",
       " 'direct-compute-samples-file-not-found': 'The links are not working correctly.Powered by Discourse, best viewed with JavaScript enabled',\n",
       " 'advice-for-generation-of-diffracted-rays': 'I am planning on moving some simulation code from a CPU only raytracer to optix - I am an optix newbie, but very familiar with RT concepts and programming. Part of the simulation involves generating diffracted rays when a ray hits a “sharp” edge ( it is up to the application to define what is a sharp edge- in an RT engine we use lines with a radius  - aka ‘hair’ ). When a ray hits an edge up to , say, 256 new rays in a cone shape can be generated from that hit .  This can be done in a recursive algorithm, or a multi-pass approach where we save the rays to be cast at diffraction each level. Recommendations? A diffraction level of  about 5 is a practical limit as the ray loses energy each time - after 5 diffractions the ray is typically ‘all worn out’…Powered by Discourse, best viewed with JavaScript enabled',\n",
       " '4-26-1-unreal-engine-rtx-github': 'When will the UE 4.26.1 version of UE RTX be available on githhub to compile or is there another way to do itPowered by Discourse, best viewed with JavaScript enabled',\n",
       " 'ubuntu-not-working': 'I have created an Ubuntu server, but after ready the server when I try to open that, so their server gives me an error 356. I don’t know what this error I have google is, and nothing found from there is. Please help me with this. I trying to found a solution from the last two hours :/Powered by Discourse, best viewed with JavaScript enabled',\n",
       " 'optix-3-5-release': 'I received an e-mail today about NVIDIA Game works. In the list there is a link to request access to OptiX 3.5.Am I wrong or the 3.5 version of OptiX will only be avaiable for GameWorks requests?Sorry for this post, it can be deleted. I saw the post about the oficial 3.5 release.Powered by Discourse, best viewed with JavaScript enabled',\n",
       " 'bug-using-continue-in-shaders-minimum-example-code-included': 'Using the following GLSL Fragment Shader renders strange behaviour. The second continue is never executed - even if b is GL_TRUE. (This example is a reduced version of a bigger shader, that has some code inbetween both continues.)Driver Version 335.23; GeForce GTX 660; Windows 8.1 64 bitsFor completeness, I use the following vertex shader:and this C++ code (using SDL2 and glLoadGen) to execute the program:Same case with me.\\nAfter update driver 337.88, this problem is encounted.Could you please verify if the problem is still present in the available newer beta driver 340.43?\\n[url]Official Advanced Driver Search | NVIDIA\\nIf yes, I’ll let the OpenGL driver team know. Thanks.Same with me. Еncountered in 340.52 and in OpenGL 4.5 340.65 beta drivers.I reported this issue with a bug tracking ticket way back in January of 2014.\\nIt began with the driver version released that late Dec/13 or Jan/14When I inquired about the fix a few times they just said maybe the next driver. never fixed.I worked around this issue with an if statement and moved-on …I was and am kind of stunned really, that they appear not to care.Powered by Discourse, best viewed with JavaScript enabled',\n",
       " 'rtaccelerationgetdata-in-optix-4-0': 'Hi.I am beginner using Optix. I want to avoid building de BHV every time I load the model, by cache acceleration structure alonside the model, as suggested in this presentation (page 19)2.72 MBThem,\\nThe idea is to use rtAccelerationGetData the first time the tree is build, and rtAccelerationSetData the following times, since the scene geometry is fixed (there is no animation or moving object).The problem is rtAccelerationGetData always return 320 bytes header and it´s seems there is no tree information. Checking the manual I found rtAccelerationGetData was deprecated in optix 4.0.Which function or alternative scheme should I use?Thanks,The rtAccelerationGetData/SetData functions are from a time when building a BVH took  many seconds. That is no longer the case. Now you should just use the default “Trbvh” builder type, which builds in parallel on the device.Thanks for your quick response.I´m loading a scene made of 500.000 triangles, and it takes about 20 / 30 seconds to start. But, as you said, I could verify that the TRBVH builder is extremelly fast, so the bottle neck must be in other place.( I´m loading several textures and compute LOD mipmaping also).\\nThanks again!Yes, the Trbvh builder should handle 500k triangles in < 1 second for most hardware.You might try timing the first OptiX launch (you can use sutil::currentTime() if nothing else). The first launch includes a JIT compile step, a BVH build step, and the frame render.  That should at least rule out an OptiX overhead vs. other things like building mip levels.Powered by Discourse, best viewed with JavaScript enabled',\n",
       " 'posture-analysis-application-using-jetson-nano': 'A program OpenPose based for posture analysis.Program provide to patience’s movement until the right position and save the new outline of body and angle values using Jetson Nano.The server.py  can be used on any Developer Kit. And at least one camera must be integrated to Kit.The client.py is for your personal computer, you can remote here all of operations on Kit.Introduction Video\\n113511234-89bd8080-9567-11eb-8f77-f8bfb8b6c1551657×800 109 KB\\nHi ysfzkn58,You can post your Jetson Nano project at Latest Jetson & Embedded Systems/Jetson Projects topics - NVIDIA Developer ForumsHi kayccc, thank you for information.Powered by Discourse, best viewed with JavaScript enabled',\n",
       " 'setvisualizationcullingbox-not-working': 'How does setVisualizationCullingBox work in PhysX?  I tried setting a box to cull my debug lines, but it seems to have no effect.Powered by Discourse, best viewed with JavaScript enabled',\n",
       " 'optix-direct-callable-program-examples': 'I want to use direct_callable program in my application, but failed to find any examples. It seems that the direct callable program should be named with “direct_callable” prefix, and it should be a “global” function, with no return value. Is it right?Forgot to say that my application is based on optix7.Same request.I still confused with the use of direct/indirect callable. Is there any example about that? (OptiX 7.0)I have an example showing that and I’m just waiting for a decision where to publish it.Two of them are ports of this OptiX Introduction example from OptiX 5 to OptiX 7 and work just the same.\\nThe renderer architecture is exactly the same.\\nhttps://github.com/nvpro-samples/optix_advanced_samples/tree/master/src/optixIntroduction/optixIntro_07Here are some excerpts of that.Let’s say you have sample() and eval() functions implemented for each basic BSDF.\\nThis would be the simplest one, a specular reflection:Now assume that code is inside the PTX file name ./appname_core/bxdf_specular.ptx relative to the executable.\\nThen the host code to load it into an OptixModule, create program descriptions, program groups and the necessary shader binding table records is this:Now inside the device code you can call any direct callable inside the SBT callablesRecord with the function optixDirectCall().Hope that helps.Thanks, it works.It’s a good example to show how to use direct callable programs as a replacement of virtual functions in path tracing application. Can I know when will the examples be published?Hi, thanks for this example. I tried to include it in my codebase but it fails when calling optixProgramGroupCreate(…). The reported error is:\\nCOMPILE ERROR: \"__direct_callable__texture\" not found in programDescriptions[5].callables.moduleDCI tried to investigate the problem, and the only thing that I could think of is that the ptx that I generated using nvcc does not contain the definition of my function, but only the ones specified as __global__.Here is the command I used to generate the ptx (I omitted some include folders for clarity):The direct callable is defined as follows (inspired by your example):And the program group description is initialized like this:I also tried to define the function as __global__, which worked but prevented me from returning any value from it or even return values through pointer arguments (which I would have been okay with). I really don’t know what I’m missing.\\nAny help would be appreciated!try to add\\n--relocatable-device-code=true\\nin the “nvcc” command line compile optionsIt worked! Thanks a lot! I tried reading the compile options for nvcc but didn’t really understand most of them…Note that OptiX 7 applications mentioned above are published in the meantime.\\nFind the link here: https://forums.developer.nvidia.com/t/optix-advanced-samples-on-github/48410/4\\nThose are using direct callables extensively.The --relocatable-device-code=true (-rdc) option is required since CUDA 8.0 because OptiX callable programs are functions which don’t appear as calls inside the PTX module and the CUDA compiler will optimize them away as dead code otherwise.Powered by Discourse, best viewed with JavaScript enabled',\n",
       " 'uvec4-gl-position-as-streamoutput-member': 'Hi.\\nI’m converting all my (DX) code to OpenGL.\\nSome of my vertex shaders write to a uint4 (uvec4) StreamOutput buffer (TransformFeedback).\\nusing any “user” output name runs fine on NVidia & ATI driver.\\nusing gl_Position (SV_POSITION) does not work on NVidia (runs on ATI).Is it an issue in the NVidia driver or something prohibited (gl_Position MUST be float4 ???)\\nby the standard (means that ATI driver is too much permissive ?)Did you check the info log with glGet[Shader|Program]InfoLog() after compilation of the shader and link of the program for any warnings or errors?What #version and #extension directives have you specified at the beginning of your shader?The GLSL specs only mention built-in variable gl_Position as vec4. I don’t know offhand if any extension changes that. If your custom output name works, that’s the more general and modern method anyway.thanks for your very fast answer :)there is no error during link [glGetProgramiv(GL_LINK_STATUS) => ok]\\nI’ll give a try to get the InfoLog even on Link valid status to see if there are warnings.\\n(is there a way to enable “warning as error” during shader compilation ?)I prefix the shader with “#version 420 core”Yes, it runs fine without using gl_Position.\\nBut, I spend 2 weeks converting roughtly my whole DirectX code to OpenGL\\nthen spend 4 weeks (not finished) tracking “magic” issues like this one.\\nAnd … generaly speaking, I trying to have a list of “howto” & “hownotto”\\nwith explanations (not “magic”)\\nis there a way to enable “warning as error” during shader compilation <<Yes, that was possible with the NVEmulate tool. (I haven’t used it in a long time.)\\n[url]https://developer.nvidia.com/nvemulate[/url]You might also want to check out Nsight if you spend a lot of time with debugging graphics programs.\\n[url]http://www.nvidia.com/object/nsight.html[/url]Hi :)I add in my code some lines to retrieve the InfoLog from Shader (compile) & Program (link) even if there is no error.\\nOn NVidia hardware, I get nothing (I assume there is no warning) (using a uvec4 as type for a StreamOutput value in a VS, named “toto” or “gl_Position”)\\nOn ATI hardware, I get this warning :Outputs that are integers should be qualified with the interpolation qualifier “flat”\\nwhich seems to me rather … surprising because outputs are outputs so I don’t undestand the need to qualify interpolation mode (as input, it’s clear, but as output, I don’t understand)\\nthanks for the informations.I will try nvemulate. It seems very interesting to be able to take a look at the ASM code of a shader (I’m using nearly every day GPU Shader analyzer from ATI for this feature, it would be really great to have such a tool (GUI able to open & edit a text file, then compile with some options and see the ASM result) for NVidia hardware)To “debug” DX code I’m using GPA (Intel) which is very convenient.I tried nsight some weeks ago (to debug my DX → OpenGL conversion) but, maybe I didn’t try enough, I didn’t achieve to get the informations I need to debug with it … I’ll try more ;)Powered by Discourse, best viewed with JavaScript enabled',\n",
       " 'badvalue-while-running-randomfog': 'I am trying to run /usr/local/cuda/extras/demo_suite/randomFog from CUDA demo suite over ssh -X ... connection with CUDA 10.  I get an error:What am I missing?  I installed openGL with sudo apt-get update && sudo apt-get install -y libglu1-mesa freeglut3 on a new V100 DGX-Station with Ubuntu 16.04.  The same for other programs nbody and oceanFFT from the demo suite.Running a GPU-accelerated OpenGL app over a remote connection requires more than just x-forwarding.This SO answer gives an idea of what is involved:[url]ubuntu - How to run CUDA/OpenGL interop (particle) sample from a remote machine - Stack OverflowPowered by Discourse, best viewed with JavaScript enabled',\n",
       " 'hdcp-noncompliant-original-hmd': 'Now, I’m trying to run the direct_mode_dx program using the original HMD.\\nHowever, in order to operate the direct_mode_dx program, HMD must support HDCP, but the original HMD used is not compatible with HDCP and can not be operated.\\nIs there any way to operate the direct_mode_dx program even when it is not compatible with HDCP?Powered by Discourse, best viewed with JavaScript enabled',\n",
       " 'linear-shape-cast-for-vehicle-wheels': 'I’m currently using PhysX 3.3 to simulate a dirtbike. I’m having a problem with the current wheel functionality (ray cast) and was curious if anyone had already solved my problem or had any tips.Currently my vehicle is using the PxVehicleWheelData/PxVehicleTireData/PxVehicleSuspensionData for its wheel setups. By default it seems that the wheels/suspension data work off a single ray cast. This works fine for driving around on flat terrain but when we have a lot of variance it doesn’t work so well.[url]http://imgur.com/RydvJ99[/url]On the left side of the drawing you can see my current behavior. The wheel doesn’t know it has collided with a box until its ray hits it. This causes the wheel to clip through the box, then when the ray cast hits it, the wheel pops up on top.I would prefer the right side of the drawing, where the wheel does a spherical cast and responds as soon as the front of the wheel hits the box. I have read posts where people use d6 joints to solve this problem, however none of them describe how they accomplished it. I want to make sure my solution affects suspension properly and can be driven by the vehicles engine data.Thanks in advance!Wheel sweeps have already been implemented for 3.4 but we don’t have a release date for that yet.  In addition to sweeps, wheel rigid body contacts will have the option to be filtered by normal and position.  The idea is to reject rigid body contact points near the bottom of the wheel because they will be handled by the sweep or raycast.  Rigid body contact points at the front of the wheel, however, could be accepted to allow the wheel to selectively respond to the environment.  It will all be explained and demonstrated with code snippets.The workaround in the short-term is to add some extra collision geometry at the front and/or side and/or rear of the wheel.  The key thing is to make sure that the extra geometry is high enough off the ground to avoid it interfering with the suspension system.  This solution has been adopted by many customers with good success.I put together a quick diagram to show you how it might work[url]http://gyazo.com/33b80158daf245336eed644d7478a3ca[/url]I hope this solves your problem.Thanks,GordonThanks Gordon,I gave that a shot, it improves the effect for running into an object such as a wall, but when rolling on and off an object with a sharp edge it has some abnormal behavior (imagine the wheel rolling on/off the edge of a box).I’m looking forward to the new release with sweeps.-JacobIs the problem now that you are picking up contact normals facing downwards from faces on the extra geometry?  This has a simple solution using the sdk contact modification feature.It is not too hard to set up contact modification so that a callback fires every time a contact on a selected object is reported .  SnippetContactModification in 3.3. is a good introduction to this topic.  The idea would then be to reject contacts involving the “extra geometry” based on the contact normal.  I’m guessing you want to keep contacts that point along the forward direction of the car but reject contacts that are pointing downwards relative to the car’s forward direction.GordonPowered by Discourse, best viewed with JavaScript enabled',\n",
       " 'physx-3-how-to-run-it-on-ios': 'Hi everybody,If I understand correctly, iOS is included in supported platforms of PhysX. But on official site where you can download PhysX SDK there are archives only for PC, Linux, OSX, Android and win8arm. So where can I find PhysX SDK for iOS(if it really does exist) or how should I run other builds on iOS?Sorry if my question is not correct. I’m not iOS developer myself.Thanks in advance.So where can I find PhysX SDK for iOS\\nI believe you should send that question to PhysXlicensing@nvidia.comHi.I didnt find any information about native support of PhysX for iOS.\\nThere are only information about Android SDK.Maybe there will be in PhysX 3.3 - (closed beta is “outside”)\\nBut I dont think so. They would say something about it.PhysX is avaiable for OSX, but … not for iOS I guess.Of course, UDK and Unity have iOS support - including a version of PhysX.\\nMaybe you take a look these game engines.I didnt find any information about native support of PhysX for iOS\\niOS as supported platform is listed in Release Notes, for example.It just resides outside “free” SDKs, similar to console versions.That’s why it is better to ask NVIDIA licensing directly.Ups:)I did a fast search for it - but didnt found it.\\nAsked google - and found strange things but nothing important.\\nOkay. Sorry for that!\\nBut I didnt find a page where the iOS is marked to have a native PhysX SDK to public.In the release notes (from PhysX SDK) there is a hint - okay.Of course, its better to ask Nvidia directly for it.I’ve sent an email to NVidia. I’m waiting for response.Well, this is NVidia’s answer:“The iOS version of PhysX is not free.”this is NVidia’s answer\\nIt is certanly a bad idea to publish pricing info on public forum - companies don’t like thatOk , sorry)Powered by Discourse, best viewed with JavaScript enabled',\n",
       " 'no-collisions-between-rigids-that-are-part-of-a-joint-why': 'Hello!When I create a joint (D6 joint) between two rigid dynamic actors, I can’t have these two rigid to collide anymore.Why? And how can I change it?Thanks!MaxPowered by Discourse, best viewed with JavaScript enabled',\n",
       " 'crash-in-vkcreateinstance': 'Hi, our support team was solving a crash with one of our customers. The problem was narrowed to Vulkan driver problem.Details:\\nI am getting crash in vkCreateInstance() on Win10 system and Quadro 2000. Surely, there is no Vulkan support on Fermi generation, but all I expect is no crash as our software can safely continue running without Vulkan. So, I would either expect that there would be no vulkan-1.dll in the system, or that the system refuses to create Vulkan Instance, or (preferably) that it will return zero devices, or something similar. Instead, I am getting application crash inside Vulkan driver - yes, there is vulkan driver installed on this system - installed by the latest driver installer for this graphics card.vkvia output of the problematic system:\\nvkvia.html (19.6 KB)Steps to reproduce:Note that the crash does not happen with apiVersion VK_API_VERSION_1_1 and it does not happen if I replace VK_KHR_get_physical_device_properties2 with some arbitrary string.To further analyze the problem, I created simple application:\\nmain.cpp (2.0 KB)\\nexecutable and 8 batch files for various options:\\nvulkanInstanceCrash.zip (25.8 KB)In “results” subfolder, you can see v10-noExtension.txt that shows the correct failure in enumeratePhysicalDevices():Creating instance (apiVersion: 0x400000, extensionName: < nullptr >)…\\nInstance created.\\nFailed because of Vulkan exception: vk::Instance::enumeratePhysicalDevices: ErrorInitializationFailedI believe that error is correct although returning just zero devices would be better (?).In v10-PhysicalDeviceProperties2Extension.txt, you can see only single line not followed by “Instance created” text because of the crash inside vkCreateInstance():Creating instance (apiVersion: 0x400000, extensionName: VK_KHR_get_physical_device_properties2)…This is the problematic case that would be nice to have fixed by the correct Vulkan error instead of application crash.The rest of txt files shows various Vulkan error codes returned on this Vulkan-free system. Hopefully, the error codes are right and according to doc. Anyway, no crash is all that I am asking :-) .JohnThank you for reporting the bug. I have reviewed the development timeline. In the vkvia.html it reports a driver version 21.21.13.7783. Quadro Desktop/Quadro Notebook Driver Release 375 | R375 U11 (377.83) | Windows 10 64-bit | NVIDIA shows it was released on 2018.1.18. Vulkan 1.1 driver was first release on March 7th, 2018 - Windows 389.10, Linux 387.42.05 based on https://developer.nvidia.com/vulkan-driver. Unfortunately we cannot port a new Vulkan version back into an older branch because all the new features and its dependencies.Please, do not port Vulkan 1.1 to Quadro 2000, or, actually I do not care. I would rather see Vulkan 1.0 fixed. That is the point when it crashes.If I may suggest the solution: Just do not initialize Vulkan support in the driver as Quadro 2000 is GF106GL (Fermi). I do not think you support Vulkan on Fermi generation.Powered by Discourse, best viewed with JavaScript enabled',\n",
       " 'how-the-nvencs-performance-of-quadro-p2000-compared-to-quadro-k2200': 'We are using NVENC in a big realtime streaming solution where having more than two encoding sessions.\\nIn Session count limitation for NVENC (No Maxwell GPUs with 2+ NEVENC sessions?) - GPU-Accelerated Libraries - NVIDIA Developer Forums , Eric_Young says: If you want Maxwell GPUs with the best NVENC performance, K2200 and M6000 are the recommended products.M6000 is too expensive, the price of P2000 is about the same as K2200.In https://developer.nvidia.com/nvidia-video-codec-sdk, the nvenc’s performacnce of P2000 is bether than K2200.Does anyone verified in reality that Quadro K2200/P2000, which one should we buy?Does anyone verified in reality that Quadro K2200/P2000, which one should we buy?I recommend P2000 (but check support issues like passthrough in virtualization environment (see https://devtalk.nvidia.com/default/topic/992447)). The Pascal generation (P2000) has also HEVC encoder (not supported by Maxwell Gen 1 (K2200)) (see https://developer.nvidia.com/video-encode-decode-gpu-support-matrix). P2000 has also more RAM (5GB vs. 4GB) (see memory allocation problems with multiple encoder streams https://gridforums.nvidia.com/default/topic/1310/#4789).\\nMy K2200 has max core clock 1124 Mhz (reported by nvidia-smi) and my P2000 has max core clock 1721 (reported by nvidia-smi (after “nvidia-smi -ac 3504,1721”)) and if you read “NVENC_Application_Note.pdf” you get expected performance metrics (but reality depends on many factors):\\n1280×554 284 KB\\nThanks very much. Do you have any idea to resolve the issue of Quadro P2000 passthrough in virtualization environment?Hi mcerveny,\\nWhat tool did you use to verify the results from Nvidia (number of encoded frames)\\nI want to use it to benchmark M4000 and compare with P4000 with non 16/9 ratio streams.\\nThanks in advance,Powered by Discourse, best viewed with JavaScript enabled',\n",
       " 'cudabindtexturetoarray-deprecated': 'Can someone please tell me how can I replace cudabindtexturetoarray which is now deprecated I don’t see an alternative in the latest CUDA docs. Want to map an OpenGL texture and use it in a CUDA kernel.Hi,\\ndid you already found a solution?\\nI’m looking also for this topic but found nothing than this post.Yes I found it. I hope it helps.Yes!\\nThank you!Powered by Discourse, best viewed with JavaScript enabled',\n",
       " 'vkcmdimagecopy-not-working-after-driver-update': 'Updated to the latest drivers and using vkCmdImageCopy doesn’t seem to work for me anymore.  Validation layer does not flag any issues with how I’m using Vulkan.    Using nsight to do a frame grab and checking the resources - it appears the source image is correct but only a black image is in the destination image after a copy.  HW is a GTX 1060 w/ 466.27.  Not sure what the previous driver version was where the code worked as intended.  Any ideas appreciated!Powered by Discourse, best viewed with JavaScript enabled',\n",
       " 'vrworks-360-video-sdk-1-1-monostitching-issue': 'when I just change the value of output_video_options pipeline “stereo” to “mono” in stitcher_spec.xml,nvstitchCreateStitcher() function returns \"bad parameter\"value(retval ::NVSTITCH_ERROR_BAD_PARAMETER) in nvstitch_sample project.previous version didn’t make any problem on this change.So I tried to find cause, comparing difference between current and previous.but I couldn’t find it.  Are there what I have to modify??Hi,Changing the pipeline parameter to mono is not enough to run mono stitching, as the output panorama aspect ratio is different between the two options. When changing between mono and stereo pipelines, panorama height needs to be updated too. For mono, it should be half of the width.Let us know if this resolves the issue.Thanks,Ryan ParkThanks a lot Park. I resolved it by your assistance. It works well, just changing panorama height.Powered by Discourse, best viewed with JavaScript enabled',\n",
       " 'video-codec-sdk-9': 'We are still waiting for release SDK 9 for Turing, it is already more than 3 months from release of Turing and nothing happends, is there some release date?I also joined early access program without any response from Nvidia.Yeah I signed up too but didn’t hear anything. Ah well.Apparently the early access is “by invitation only” even though the site allows you to apply. I’m usually a staunch supporter of nVidia but here I think they are letting down the community. I have been coding for CUVID/NVDec for many years and provided much useful feedback to nVidia over the years (DGAVCDecNV Development Dialog with Nvidia), but I’m inexplicably not eligible for this early access. It’s a slap in the face. And not even bothering to reply to people who apply? Disgraceful.I just applied as well, but if Donald Graft couldn’t get access to it, then I shouldn’t have bothered.Video Codec SDK 9 ready drivers are there, 418.30 BETA driversWho cares? It’s the SDK 9 that we need and want. What good is driver support for it if we can’t get it?nVidia has rudely ignored our applications for access, which they solicited (!), and our forum administrator Ryan has gone absent when we need him the most.Video Codec SDK 9 ready drivers are there, 418.30 BETA driversThat is for Linux only, Windows version is not released yet.NVIDIA Video Codec SDK 9.0A new release of the Video Codec SDK will be available in Q1 2019.\\nFor more information and early access sign-up, refer to NVIDIA VIDEO CODEC SDK | NVIDIA DeveloperThe main features available in this release are listed below:This is the worst soft-launch of a physical product ever.NVIDIA has launched hardware in September 2018 and it may work as advertised in April 2019.Video SDK 9 is now available!So let me get this straight – Turing is slower than the 2nd Gen Maxwell and Pascal? Like between 1.4x and 4.2x slower?That is, if I am reading the table 3 (NVENC performance) correctly?Basically, since RTX 2080 Ti (TU102) has one NVENC while 1080 Ti (GP102) has two, the latest and greatest (and most expensive so far) will be two times slower than previous generation, at least when looking at high-quality dual pass preset for H.264?The footnote 1 in the table here:Find the related video encoding and decoding support for all NVIDIA GPU products.Is telling me that one Turing NVENC is the same as two Pascal NVENC because of improvements in performance and quality.And yet the table 3 from NVENC application note in SDK 9.0 says that Quadro RTX 8000 is pushing 306 FPS ON ONE NVENC engine, while Quadro P2000, which is not even in the same price range or directly comparable, is pushing 432 FPS (1.4x faster) also with ONE NVENC engine?!?Not to mention that you don’t give numbers from Quadro P5000 or P6000 which have TWO NVENC engines and would thus push at least 864 FPS (2.8x faster!) if not more?!?And if you consider that GP100 or GV100 have 3 NVENC engines the way this is presented really turns into an outright insult to our intellect.I understand that NVIDIA is trying to make a good all-around product and that some sacrifices had to be made to make room for RT and Tensor cores, but you absolutely should not go below current highest performance with your next generation product while jacking up the prices and claiming they are, in fact, faster without real evidence to support that claim. Or you do what you did, but then you can’t fault people for not buying your arguments and your products.What a joke has NVIDIA became as of late, shame on you guys.Video SDK 9 is now available!…and wasn’t it nice of Nvidia to announce this to all those people who registered for “early access”…!Latest FFmpeg Zeranoe Windows builds now include SDK9 and the new “b_ref_mode” flag.Although whenever I set b_ref_mode to middle using the latest Zeranoe windows build, I get a huge output of “invalid DTS: xxxxx Invalid PTS: xxxxx in output stream 0:0, replacing by guess” messages…We understand that the Video Codec SDK 9.0 release was a bit delayed and we appreciate your patience. We will try to improve the gap between the hardware and the SDK release in the future.About early-access notification, as you now know, we decided to directly do a public release of SDK 9.0 and that’s why many of you did not get any responses from us despite applying for early access. We also have had some staffing issues in monitoring the forums and so Ryan or NVIDIA staff have not been very active here for past few weeks.However, rest assured that we do monitor these forums. Even if you don’t get a response on each query, we are taking every feedback/question and internalizing it to prioritize on the engineering roadmap.Thank you.Thx for answer, i understand that there was alotof work with next gen GPU, I have one very important question. Quality of Turing NVENC is very good, but can we expect any significant performance improvments on current RTX implementation of NVENC? This question is important for us, because we are designig video encoding systems with our partners based on Nvidia GPU for almost 3 years.[never mind]Hello Thunderm,As you may be aware, performance is dependent on quality. In Turing, the major focus has been on improving NVENC encoding quality. At higher quality, the loss of performance is expected (compared to older GPUs). At higher quality, any further NVENC performance improvements are unlikely with Turing GPU.Hello Thunderm,As you may be aware, performance is dependent on quality. In Turing, the major focus has been on improving NVENC encoding quality. At higher quality, the loss of performance is expected (compared to older GPUs). At higher quality, any further NVENC performance improvements are unlikely with Turing GPU.May I ask if there is difference on performance between Tesla T4 and Quadro RTX 5000 ?SDK 9.0.18 → 9.0.209.0.20 deletes expression about TU117 in “NVENC_Application_Note.pdf” and “NVENC_VideoEncoder_API_ProgGuide.pdf”.GEFORCE GTX 1650 (TU117)\\nhttps://www.nvidia.com/en-gb/geforce/graphics-cards/gtx-1650/ → NVIDIA Encoder (NVENC)  Yes b[/b]Powered by Discourse, best viewed with JavaScript enabled',\n",
       " 'plea-for-help-nvflash-tegra-note': 'Nvidia can I get some help please.Whilst tinkering with my Tegra Note 7 (TN7) to learn about developing I saved one of the configuration files on the device with a typo.  The unit now will not completely boot.  It must be failing on driver initiation.  The problem is the boot doesn’t get to a stage to enable the ADB service, and considering my TN7 was early release with a plain jane boot partition without any button sequence to strap into recovery, I’m stuck with a software bricked Tegra Note 7.  Now one would think I could get nvflash and format the APP/system partition and force the unit to boot into recovery, but no, nvflash simply does not want to act correctly with the Tegra Note 7.Can a Nvidia software technician make a utility to force a device into recovery mode by means of nvflash apx mode, or release a proper working nvflash executable, or tell us what is going wrong?I’ve tried various combinations, like using partition numbers instead of ‘APP’ descriptor, and various boot.img.Nothing happens. I pull the usb and;$300 software bricked Nvidia Tegra Note 7 sitting here for a month, and currently ZERO help from NVidia.EDIT: The problems I experience were due to an early model tegra note which had a basic bootloader with no fastboot support or was a factor reject, considering I had other manufacturing problems.With a proper bootloader it should be near impossible to brick a device, irrespective of APX/NVflash being a lower level repair mechanism, so my plight may have been specialised.I would like to inform viewers that NVidia made good my problems, so much so that I am now a devoted Nvidia customer, Yes that means I will only buy Nvidia products.  That’s a turn around hey!I have similar problem as below. Do you mind teach how to solve this problem. Thanks a lotNvflash 3.09.1700 started\\nchip uid from BR is: 0x600000015c3e1040240000000d038640\\nminiloader download failed (response read failed)\\nrcm version 0X0Powered by Discourse, best viewed with JavaScript enabled',\n",
       " 'tire-shader-example': 'Hi, are there any examples using a custom tire shader?http://docs.nvidia.com/gameworks/content/gameworkslibrary/physx/guide/Manual/Vehicles.html#tire-shadersThanks\\n-KenThe default tire function PxVehicleComputeTireForceDefault is itself a shader so that might provide the best example.  It is implemented in PxVehicleUpdate.cpp.Cheers,GordonPerfect, thanks!Powered by Discourse, best viewed with JavaScript enabled',\n",
       " 't1000-tcc-mode': 'Can T1000 card be put into TCC mode instead of WDDM? Thank youPowered by Discourse, best viewed with JavaScript enabled',\n",
       " 'xorg-vram-leak-because-of-qt-opengl-application': 'Hello board,I am working on a complex Qt/OpenGL Application.\\nXorg starts leaking in VRAM when i’m using the application and never release the memory, until I restart X of course.The version of Xorg does not matter, tested a few.\\nThe version of the driver does not matter, as long as it’s nvidia, tested 340, 384, 390\\nThe linux distribution does not matter, tested Ubuntu 16.04, 18.04, fedora\\nThe de does not matter, tested Unity, Gnome-shell, Xfce, Lxde + Compton, Openbox + compton\\nThe compositor used does not matter, but the leak disappear without a compositor.\\nI did not test Wayland.Do you know what could cause this behavior ?\\nCould this be a nvidia driver bug ?\\nIf not, what could, in our code create this behavior ?The issue was resolved thanks to an intense debugging session.This was a Qt issue.\\nThe leak was caused by NULL parenting the parent of the windowContainer containing the QVTKOpenGLWindow just before deletion.This code was here before when we used a QOpenGLWidget and it caused no issue. In any case, NULL parenting a widget before deletion is useless so removing the line resolve the issue.This leak shouldn’t happen though, even in this situation, so I have opened a Qt issue to report it.\\n[url][QTBUG-69429] NULL parenting a parent of a window container containing a QOpenGLWindow make Xorg leaks VRAM - Qt Bug TrackerPowered by Discourse, best viewed with JavaScript enabled',\n",
       " 'bug-physx-3-3x-rotation-breaks-overlapmultiple-returns-only-1-possible-hit': 'Hi,I get sometimes a deterministic “bug”:I know overlapMultiple is deprecated, but maybe this “bug” is in other PhysX versions too.The “bug” is that you only get 1 hit on your overlapMultiple(…) as result, which cant be because I have many other shapes which touch it. With normlized values I get the right count of hits.This is the value which breaks the overlapMultiple(…) (not normalized)\\nw: 0.50009 x: -0.50000 y: -0.50000 z: 0.50009\\n(So I guess there should be somewhere marked that only normalized values are valid)But after I normalizing:\\nw: 0.50004 x: -4.99996 y: -4.99996 z: 0.50004\\nWith this new value, the overlapMultiple(…) works again.Should there an assert that you are using a rotation which isnt normalized?Without normalizing, everything seems to works perfectly, but only sometimes it doesnt.\\n(After playing around I found this value above - I forgot to normalize my rotation calculation)I have another question: When will be PhysX 3.3 (non beta) released?\\nOr maybe another new version?Its almost 5 months ago where Nvidia released the PhysX 3.3 beta.( I know you had much to do, but … we would like to see something more in this SDK…)When will be PhysX 3.3 (non beta) released?\\nIt was released yesterday through RDP.Hi,thank you. After clicking thru the sometimes confusing websites of PhysX, I finally download it.I didnt read the news before.(And I like this great news :D)Powered by Discourse, best viewed with JavaScript enabled',\n",
       " 'nvidia-vxgi-and-rtx-hardware': 'Just a general question regarding VXGI and the new RTX cards. Will VXGI benefit from the RT cores of the new RTX lineup to increase performance, or will we need to use RTX specific tools to take advantage of them? I am developing a game in UE4 with VXGI, and if it will improve performance by using RTX, that is even better. VXGI 2.0 is already a massive improvement. Thanks!Powered by Discourse, best viewed with JavaScript enabled',\n",
       " 'bug-report-nvidia-geforce-drivers-cause-idxgiswapchain4-present1-to-block-when-it-should-not': 'Hello, after dozens of hours of extensive testing, I believe I have found a bug in NVIDIA GeForce drivers.(Although it might seem to be a bug in Windows or Direct X, it is not, because their D3D_DRIVER_TYPE_WARP CPU emulation device does not show this issue.)In a 64-bit Windows 10 Desktop program written in C++, I have created DXGI swapchain code that never blocks the calling thread.The above call must never block the calling thread. It works (meaning it does not block and there is no screen tearing), but only if I use NVIDIA GeForce driver version 460.89 or earlier OR if I use D3D_DRIVER_TYPE_WARP.In other words, if I use driver 461.09 or later, or if I use the non-emulated device D3D_DRIVER_TYPE_HARDWARE, the call starts blocking for ~14 ms on each and every call.I even tried with the DXGI_PRESENT_RESTART flag – no change.\\nI also tried the DXGI_PRESENT_DO_NOT_WAIT flag, which did cause the Present1(0, DXGI_PRESENT_DO_NOT_WAIT ) call to become non-blocking, but then it kept returning DXGI_ERROR_WAS_STILL_DRAWING for ~14 ms, effectively blocking the thread again.This is not a Windows Store app, not a UWP, not a WPF app - it is just a classic 64-bit Desktop app.Simplified paraphrased repro steps (cannot post a verbatim copy of our project files due to legal reasons, sorry). Also I’ve deleted all error/exception handling for clarity, etc.Observed while running:GeForce driver settings:\\nIn NVIDIA Control Panel, our program does not have any per-program settings, and I have double checked that that this relevant global setting is set to “Use the 3D application setting”: ‘3D Settings’ > ‘Manage 3D settings’ > ‘Vertical sync’.\\nIf I set that setting to ‘Off’ with the latest driver, I get the correct non-blocking behavior, but this introduces unwanted screen tearing. If I set it to ‘On’ with the latest driver, I get blocking behavior but with no screen tearing, which is unwanted as well. And I will repeat that we can achieve the desired state (non-blocking behavior without any screen tearing either by using GeForce driver 460.89 or earlier OR by using D3D_DRIVER_TYPE_WARP).Tested NVIDIA GeForce drivers:\\nThese work correctly (non-blocking behavior):\\n457.51-desktop-win10-64bit-international-whql\\n460.89-desktop-win10-64bit-international-nsd-dch-whql\\nThese do NOT work correctly (blocking behavior):\\n461.09-desktop-win10-64bit-international-dch-whql\\n461.40-desktop-win10-64bit-international-nsd-dch-whql.exe\\n461.92-desktop-win10-64bit-international-nsd-whql\\n472.12-desktop-win10-win11-64bit-international-whql\\n472.84-desktop-win10-win11-64bit-international-nsd-whql\\n522.30-desktop-win10-win11-64bit-international-nsd-dch-whql\\n526.47-desktop-win10-win11-64bit-international-dch-whql\\n526.98-desktop-win10-win11-64bit-international-dch-whql\\n526.98-desktop-win10-win11-64bit-international-nsd-dch-whql\\n531.61-desktop-win10-win11-64bit-international-nsd-dch-whqlProgram features:Some potentially relevant preprocessor definitions in VS 2022 project settings:\\nX64_BUILD\\n_WINDOWS\\nNTDDI_VERSION=NTDDI_WIN10_RS4\\n_WIN32_WINNT=_WIN32_WINNT_WIN10\\nWINVER=_WIN32_WINNT_WIN10\\nDIRECTINPUT_VERSION=0x0800\\n_CRT_SECURE_NO_WARNINGS\\n_ALLOW_RTCc_IN_STLPotentially relevant linker inputs:\\nd3d11.lib\\nd2d1.lib\\ndxgi.lib\\ndwrite.lib\\ndsound.lib\\ndinput8.lib\\ndxguid.lib\\nodbc32.lib\\nodbccp32.lib\\nwinmm.lib\\nkernel32.lib\\nuser32.lib\\navrt.lib\\nPowrProf.libI used the DX debug layer to see if any issues are reported (there were indeed a couple at the beginning of the development, but I’ve fixed all since). So very briefly, I used:\\nDXGI_CREATE_FACTORY_DEBUG\\nD3D11_CREATE_DEVICE_DEBUG\\nD2D1_DEBUG_LEVEL_INFORMATIONThank you for looking into this.Powered by Discourse, best viewed with JavaScript enabled',\n",
       " 'cuvidparsevideodata-returns-succeed-but-there-are-no-frame': 'I try to develop streaming program,I have 4k videos that 5 fps and 20 seconds, I encode frames with cuda encoder successfully, i send encoded data over rtp successfully but when i try to decode that data, i got 0 frame from cuvidParseVideoData . Should i do something before the cuvidParseVideoData for example use ffmpeg demuxer ?RecvDec.cppNvDecoder.cppMy data always starts with 0x00 0x00 0x00 0x01. This is start frame tag. cuvidParseVideoData always returns 0 and m_nDecodedFrame is 0.Powered by Discourse, best viewed with JavaScript enabled',\n",
       " 'vulkan-1-2-support-with-driver-445-75': 'With driver version 442.82 my MX150 reports Vulkan 1.2 support, but when I install the newer 445.75 its gone.\\nWhy has the support for Vulkan 1.2 been removed and when will it be readded?Vulkan 1.2 is currently only included in beta release drivers. 445.75 is a general release driver, meaning it comes with Vulkan 1.1.As of writing, the latest beta driver for Windows is 442.82 (the one you listed in your post).Presumably, general release drivers will start shipping with Vulkan 1.2 once it has been sufficiently tested and deemed stable enough. For now, you’ll just need to stick to beta drivers.You can keep an eye on the latest beta driver releases here.Powered by Discourse, best viewed with JavaScript enabled',\n",
       " 'multi-joint-articulated-suspension-problems': 'I am using PhysX SDK version 2.8.4 with the Torque 3D game engine in my work to create an articulated suspension system. I’m using multiple joints because as I’ve seen thus far there is nothing that allows me to use one to create steering, wheel rotation, and a suspension spring simultaneously.The following is the code I’m using for the front wheels in the also following video. The rear wheels are essentially just not using one of the joints and the additional actor. By my understanding, since I’ve locked all directions for the joints for the front wheel, they should remain completely rigid. This happens when I lock them for the rear wheels. However, as you can plainly see in the video, they are not remaining locked at all. I can only assume this has something to do with the additional actor, but I’ve been testing numerous configurations and have made no improvements which is why I’m asking for assistance.This is the link to the video:http://vimeo.com/57153475My code is the remainder of this post, thank you in advance.Nothing? I was at least hoping for a “Hmmm that isn’t even interesting” comment.Hi,Sorry for the lack of attention! For some reason I cannot get your video to play. However, looking at your code, I would think that you would want to use the twist axis for the wheel roll DOF. I’ll ask one of our vehicle experts to comment.Thanks,\\nMikeLooks like you already have the rear wheels working but the front wheels are rotating on a wrong axis. Have you tried creating the front wheels based on the rear wheels, but enabling just 1 more DoF for steering? Thanks.Powered by Discourse, best viewed with JavaScript enabled',\n",
       " 'nvs-810-on-16-displays': 'Hello,I need a little help. It was my understanding that I could use (2) NVS 810 cards in one computer to drive 16 displays for a 4 x 4 wall. I finally got the wall built and the displays hooked up (16 Vizio 65\" 4k tvs) but I’m only seeing 8 displays. I’m using Display Port to HDMI adaptors that are all the same. The displays that show up are from both cards, so I know both cards are working. Can someone please tell me what I might be missing?Thanks for any help or suggestions you can give me.JoshPowered by Discourse, best viewed with JavaScript enabled',\n",
       " 'dynamic-hardware-tessellation-basics': '(Discussion topic coming soon :) )Powered by Discourse, best viewed with JavaScript enabled',\n",
       " 'geforce-second-gpu-usage': 'I have a PC with 2 Geforce GTX 660 GPUs. I want to use both GPUs for rendering of each frame. (similar to the old SFR SLI).Directx allows me to use the second GPU via adapter enumurator. But in OpenGL i cannot found how to use second card without SLI. Regardless of the GPU usage, the second GPU is never used for rendering. We have tried opening several GPU load applications such as Furmark.We have also tried monitor enumerator function in Windows API and forced the context creation to be in the second monitor which is connected to the second GPU. Yet, the rendering is still done in the first GPU.NVIDIA slides suggest using the GPU-affinity function, but this functionality is only available in Quadro graphics cards.How can I use the second card for rendering in OpenGL?Powered by Discourse, best viewed with JavaScript enabled',\n",
       " 'does-nvfbc-support-t4-gpus': 'I noticed in the release notes that it doesn’t say.Are they just old?Hi suhailx0nyo,\\nLinux NvFBC on Ubuntu 18.04 platform that you are using is supported on T4 GPUs.\\nFor Windows NvFBC, deprecation notice and details are available at\\nhttps://developer.nvidia.com/capture-sdkThanks.Hi mandar_godse,\\nI  am trying to get a code sample from the Nvidia Capture SDK 7.1.6 work properly on Ubuntu 18.04 platform using T4 GUPs:  https://developer.nvidia.com/capture-sdk.This is the code sample:\\nCapture_Linux_v7.1.6/NvFBC/samples/NvFBCHwEnc$ ./NvFBCHwEnc\\nApplication version: 4\\nNvFBC API version: 1.6Version mismatch between NvFBC and the X driver interface.So what does it mean by  the “Version mismatch between NvFBC and the X driver interface” ? I am confused about it .\\nAs you see, the used SDK version is 7.1 and my driver version is 470.103.01 which should be matched with each other.Hi.\\nNVIDIA Capture SDK 7.1.6 you refer is very old, latest version available on https://developer.nvidia.com/capture-sdk is NVIDIA Capture SDK 8.0.8Note that, ToHwEnc interface is no longer supported. README from the latest SDK mentions,The old NVFBC_CAPTURE_TO_HW_ENCODER interface is retired. An application\\nrequesting that capture interface will get the NVFBC_ERR_UNSUPPORTED error.If an application wishes to capture and encode, it is recommended\\nto capture, using NvFBC, to system memory or video memory,\\nand then pass that captured memory to the encoder using the Video\\nSDK’s NVENC API. This approach provides finer encode control than is\\navailable with ToHwEnc.\\nSee: NVIDIA VIDEO CODEC SDK | NVIDIA DeveloperHi mandar_godse,\\nFirstly,  thanks for your kindly reply.\\nHowever,  the problem remains the same using NVIDA Capture SDK 8.0.8.\\nBelow is the code sample:rocky@rocky-Super-Server:~/tmp/Capture_sdk_nvidia/Capture_Linux_v8.0.8/NvFBC/samples/NvFBCCUDAAsync$ ./NvFBCCUDAAsync\\nApplication version: 4\\nNvFBC API version: 1.8Version mismatch between NvFBC and the X driver interfaceI’m wondering what factors cause the error message “Version mismatch between NvFBC and the X driver interface”.“Version mismatch between NvFBC and the X driver interface”\\nThis indicates NvFBC (libnvidia-fbc.so) and NVIDIA X driver are incompatible and driver may not be installed correctly. Can you remove and reinstall NVIDIA driver to ensure these libs are correctly installed from the same driver package?Powered by Discourse, best viewed with JavaScript enabled',\n",
       " 'rtx-4000-usb-c-to-dvi': 'Hello all.  I have  an RTX 4000 that i need to drive a projector with 4 DVI inputs using MOSAIC.  The 3 standard display ports are working great with a DP to DVI adapter.  The usb-c is giving me problems.  I purchased an active usb-c to DP adapter (startech brand i think) and from there i used a DP to DVI.  That gives me a signal, but it is very noisy.  I am hoping its just the adapter i purchased.  Any suggestions for a different adapter that is known to work?Hi Bowdendr,What resolution are you trying to set?   NVIDIA doesn’t have any recommended connectors to go from ubs-c to DVI and we don’t test mixing adaptors.Having said that I came across this as a usb-c to DVI adaptor - USB-C to DVI-D Adapter | Accell (accellww.com) .  I don’t have any personal experience of this adaptor - so this is not an endorsement, but hopefully, a single adaptor should give a better experience than chaining adaptors together.DPowered by Discourse, best viewed with JavaScript enabled',\n",
       " 'driver-for-gtx260-video-cards-will-not-install-for-ubuntu-22-04': 'I have tried several different ways to attempt to install the driver for the gtx 260 nvidia card on Ubuntu 22.04. I have three gtx 260’s in SLI set up. However, there is an error every time that takes place at different parts of the installation. Several times the install even went to 100% only to find that after I restarted the cpu, an error in the install had occurred.\\nThe Ubuntu software installer will not bring up anything for software for nvidia for me and I have always had to download the driver from nvidia and run the install through the terminal to get the install to work at all. Is this new version of Ubuntu going to require me to upgrade my video cards or is there something else that perhaps I am not doing correctly?Hi there @william.barnes, welcome to the NVIDIA developer forums!GTX 260 is from around 2008, correct? The latest official driver supporting this on 64bit Linux would be 340.108. Which in turn is from 2019.To be honest I think it might be time to upgrade. There might have been kernel upgrades in the meantime that are not being addressed anymore by this driver.I am sorry if this is not helpful.Powered by Discourse, best viewed with JavaScript enabled',\n",
       " 'optix-and-performance-counter-reports-in-nsight-compute': 'Hello,I’m trying to see if I could get some performance counter metrics running an RTcore-accelerated OptiX application, specifically memory operations and L2 metrics.\\nBut I’m unsure if the perf counter reports I get from the Nsight Compute tool reflects the memory accesses done from the RTcore hardware, or just the CUDA kernels run in the SMs.\\nCould anyone provide a clarification on this, or maybe pointers to any documentations?For what I’ve done so far, I profiled the optixPathTracer example app in the SDK using Nsight Compute and could see two active kernels being profiled:  the ray generation kernel (__raygen__rg_...) and something called NVIDIA Internal. From the Nsight VSE documentation the latter seems to be one of the OptiX kernels invisible to the user.\\nWhat I’m looking for specifically is the part where the RTcore hardware does the acceleration structure traversal - namely the optixTrace call. Can I assume that the NVIDIA Internal part is where that happens? The NVIDIA Internal kernel seems to be running for a much shorter time than the ray generation kernel, and that makes me confused about where the actual traversal is happening.Thanks in advance for any help.Please have a look into this thread:\\nhttps://forums.developer.nvidia.com/t/is-there-a-way-to-measure-rt-core-util/168089I’m not aware that memory traffic in Nsight Compute reports would be partitioned into SM and RT core usage.The __raygen__ is just one of the functions of the whole kernel and you should be able to see other OptiX device program domains inside Nsight like the __closesthit__ functions inside your raytracing kernel.Anything reported as “internal” is either the explicit acceleration structure build, which is a completely different kernel, or internal functions inside the raytracing kernel which are not exposed.Hello Detlef,Thanks for your help! I have looked at the thread you linked before and ensured that I enabled all the line info and profile with a RelWithDebInfo configuration in CMake.Now that I look at the thread count, the __raygen__ kernel closely matches the total number of pixels (1600x900) whereas the internal one only has 40, so your explanation makes more sense that the internal kernel concerns with accel structure builds or some other things. It’s weird that I don’t see other kernels like __closesthit__ and __miss__ as separate reports, but they are visible in the source line view of the __raygen__ report. I guess that’s because __raygen__ is at the top of the call stack.I’m not aware that memory traffic in Nsight Compute reports would be partitioned into SM and RT core usage.Does that mean that the memory traffic report at least somehow reflects the RT core usage, albeit mixed with SM’s?Thanks!Does that mean that the memory traffic report at least somehow reflects the RT core usage, albeit mixed with SM’s?That’s right. The memory stats report the memory system usage, regardless of which part of the processor is requesting memory I/O.It’s weird that I don’t see other kernels like __closesthit__ and __miss__ as separate reports, but they are visible in the source line view of the __raygen__ report. I guess that’s because __raygen__ is at the top of the call stack.Yes, that’s more or less right. For what it’s worth, __closesthit__ and __miss__ are not kernels per se, they are just functions called as part of a kernel execution. For that matter, __raygen__ is not a kernel either. For an OptiX launch, the raygen program is the entry point for the kernel, and raygen is where you can request traversal and calls to the closesthit and miss programs via the optixTrace() function. Because raygen is always present in an OptiX kernel launch, we decided to use the name of the compiled raygen program as the kernel name for profiling purposes. In some older versions, the name contained megakernel (which is a reference to the fact that all your OptiX programs are compiled into a single kernel).–\\nDavid.For what it’s worth, __closesthit__ and __miss__ are not kernels per se, they are just functions called as part of a kernel execution.I appreciate the clarification, this was an important detail to correct for my understanding.Because raygen is always present in an OptiX kernel launch, we decided to use the name of the compiled raygen program as the kernel name for profiling purposes.Ah, that was why the kernel name in the reports had weird prefixes like _0x..._ss_0 and the like! That also explains the name “mega kernel” being mentioned in some of the performance study papers I looked at that targeted older OptiX versions.Thanks for the helpful explanations, Detlef and David!Powered by Discourse, best viewed with JavaScript enabled',\n",
       " 'low-fps-after-driver-update-linux': 'Hello everyone,Two weeks ago I updated Nvidia driver and found very low FPS in two games that worked perfectly before - Civilization VI via Steam and WoW 3.3.5a via wine. I tried to reinstall games, Steam, wine, the whole OS (linux mint 18.3), I tried different versions of linux kernel, wine and also drivers (384.111, 390.48 and 396.18) - problem was everywhere. Running WoW through console has shown openGL problems:Same warnings received also from other shaders.Trying to clarify the problems’ source, I used “+wgl” debug symbols and got huge log, where I found this:When I saw it, I looked for installed openGL extensions and found few ‘GL_NV_vertex_program’ extensions of different versions, highest was 3. Here is a full list: https://pastebin.com/H17Uqdiy\\nOpenGL version - 4.6.0, GLX server and client version - 1.4Any thoughts about how I can fix the issue?Powered by Discourse, best viewed with JavaScript enabled',\n",
       " 'capturing-d3d11-with-nvenc': 'Suppose one would like to capture frames from d3d11 device and encode with h264 codec.\\nI am able to initialize the encoder with my Device (The one used by the app) by passing it to  nvEncOpenEncodeSessionEx, but then i am unable to register this device as a resource.\\nShould one pass a new, empty instance of a device to nvEncOpenEncodeSessionEx ?\\nIf so, what the encoder using it for?Powered by Discourse, best viewed with JavaScript enabled',\n",
       " 'flex-importing-obj-file': 'Hello Folks,I would like to add a different rope (.OBJ file) model into the rope component scene.How can I do that?best!Powered by Discourse, best viewed with JavaScript enabled',\n",
       " 'shutting-down-nvidia-share': 'The project I intend to work on requires installing another software package (Meson), as well as having the NVIDIA CUDA compiler installed.  When I try to install it, it refuses to install while NVIDIA Share is running.  How can I shut down all of the NVIDIA Share processes and keep them shut down long enough to do this installation?   I’m already using Visual Studio 2019 and 64-bit Windows 10.  When I try End task under Task Manager, the processes disappear for a second or two, and then appear again.Hopefully this link will help: Disabling the GeForce Experience Share In-game OverlayThanks.  That worked.Powered by Discourse, best viewed with JavaScript enabled',\n",
       " 'rtx-founders-edition-fan-speeds-in-driver': 'Without any driver, the cards have a much lower idle fan speed.\\nWith the drivers installed fan speed doesn’t go below 41% / 1500rpm.\\nPlease make the threshold lower for idle. The noise is too audible. Especially when working with low to none GPU demand.Powered by Discourse, best viewed with JavaScript enabled',\n",
       " 'basic-driver-from-custom-operating-system-os': 'The Members From Aura-Team and Myself, are developing a custom operating system we have been in development for a few years now and we are looking into adding more advanced graphics, now we have a basic ELF loader for DLL, and some EXE files (com is also supported), but we need information regarding system calls that must be made for a basic driver to work, and some listed structure. Thank you.[note: I am not a business professional, I am just a team member looking for better and faster graphics.]Powered by Discourse, best viewed with JavaScript enabled',\n",
       " 'cloudxr-some-computers-give-extended-black-screen': 'that’s all-- no rhyme or reason to why it happens on some computers but not others. SteamVR doesn’t seem to be recognizing CloudXR is trying to make a connection on those computers.noticed that when I get that extended black screen I see this in the log:Usually such things are related to firewall/network settings.  But the log snippet above clearly shows it cannot bind a network port.  That seems very odd, as if there was some permission issue with the network adapter, or a security app that is locking down ports.I’m also curious if the ports are already in use. That range is pretty popular for any videoconferencing service (Teams, Zoom, Skype, etc).  I’d try running a series netstats to hunt down if this is what’s going on:  Scan Open Ports in Windows: A Quick GuideI figured out the problem was due to using the 3.0 sample app with computers where 3.1 was installed and/or where the exe installation did not open all the needed ports.Great, thanks for letting us know!Yes, we’re hoping to have much better notification of exiting due to version differences in another release.This topic was automatically closed 14 days after the last reply. New replies are no longer allowed.Powered by Discourse, best viewed with JavaScript enabled',\n",
       " 'vs-2013-version': \"I recently moved to a new development environment, and I am now using VS 2013. Would there be any chance that we could get a VS 2013 build for OptiX/OptiX Prime?Have a look at[url][Beginner's Question] Portable Visual Studio Project - OptiX - NVIDIA Developer ForumsHave a look athttps://devtalk.nvidia.com/default/topic/742396/optix/-beginners-question-portable-visual-studio-project-/post/4243645/#4243645Hmm, sorry, I didn’t see that! Luckily, it is no longer an issue. I installed the v110 platform tools and have resolved the issue with mismatched versions of MSVC.Powered by Discourse, best viewed with JavaScript enabled\",\n",
       " '8-channel-4k-hevc-decoder': 'Hello,Any suggestions on a suitable NVIDIA PCIe card which is capable of to 8 channels 4K HEVC decoding at 60fps?NVIDIA cards are primarily used for compute intensive tasks. However, I am looking for a card which can only perform HEVC decoding.Is it possible to use CUDA cores to do HEVC / H.264 decoding in addition to NVDEC core?Thanks,\\nSubbaraoHi,\\nFor a double confirmation, are you looking for a PC with NVIDIA graphics card to do 8 4Kp60 decoding? Or a embedded system like Jetson Xavier?Hello,I am looking for an NVIDIA PCIe card capable of decoding 8 channel 4Kp60 and not a PC or embedded system.Thanks,\\nSubbaraoHi,\\nPlease check\\n[url]https://developer.nvidia.com/nvidia-video-codec-sdk#NVDECFeatures[/url]\\nhttps://developer.nvidia.com/video-encode-decode-gpu-support-matrix… and also check NVDEC_Application_Note.pdf !The answer also depends on input bitrate and output transfers (PCI/DMA, you need up to 16GB/s!). You can estimate minimum required card:DaneLLL, mcerveny,Thanks for the suggestions. I went through the docs and especially NVDEC_Application_Note.pdf. I have tried compiling the ffmpeg and tried to decode a 5MP(5Mbps 8bit YUV 4:2:0 H.264) video file as given in the following webpage of Nvidia.https://developer.nvidia.com/ffmpegUnfortunately, I did not get more than 24fps. The command I used was as follows:ffmpeg debug messages are pasted below for reference. Any suggestions what could be wrong here ?We could achieve the decoding FPS as in application notes. Major bottleneck is to transfer the decoded raw video to host CPU memory. I have updated our latest findings in following thread.[url]https://devtalk.nvidia.com/default/topic/987460/gpu-accelerated-libraries/nvdec-cuda-nvenc-speed-comparison/post/5343483/?offset=10#5344656[/url]Check memory throughput with “CUDA-Z” or CUDA samples “bandwidthTest” ([url]https://docs.nvidia.com/cuda/cuda-samples/index.html#bandwidth-test[/url]). Try page-locked/pinned memory. Check results on Internet (it depends on used card and PCIe generation) but all under 13 GiB/s.Powered by Discourse, best viewed with JavaScript enabled',\n",
       " 'sbt-theoretical-quesions': 'Hello all,I have a couple of questions regarding the SBT in OptiX which may be somewhat theoretical in nature I hope someone has an answer to.struct CHData {\\nfloat3 result;\\nint cnt;\\n};\\nstruct AHData {\\nfloat3 result;\\n}\\ntemplate\\nstruct SbtRecord {\\n__align__( OPTIX_SBT_RECORD_ALIGNMENT )\\nchar header[OPTIX_SBT_RECORD_HEADER_SIZE];\\nT *data;\\n};\\ntypedef SbtRecord SbtCH;\\ntypedef SbtRecord SbtAH;SbtCH ch_sbt;\\nch_sbt.data.result = { 0.0f, 0.0f, 0.0f };\\nch_sbt.data.cnt = 1;SbtAH ah_sbt;\\nah_sbt.data.result = {0.0f, 0.0f, 0.0f);How could I combine SbtAH and SbtCH as part of hit group record when building SBT? Or is this even possible?Thank you in advance for any help.Make sure you do not mix up the terminology.The Shader Binding Table (SBT) contains SBT Records which define the connection between your scene data and shader invocations for any rays.Say I have a __anyhit__ shader that will not even use an SBT, is it still required that I create a SBT for it? A sort of “dummy” SBT.\\nHow can I combine multiple SBT as part of a hit group that contain different numbers of elements? For example, if I have an SBT defined for a __closesthit__ program and another SBT for __anyhit__ whereby both are built using the same underlying struct but have different number of elements.Anyhit shaders are not an isolated thing but part of a hit record entry which consists of intersection, anyhit and closesthit shaders. Means there cannot be differently sized SBT record struct assigned to anyhit and closesthit shaders for the same hit record in an SBT.(Though not all three shaders are required, e.g. a hit record assigned to built-in triangle geometry doesn’t have a user defined intersection shader. Or opaque geometry without cutout opacity does not need an anyhit program. Visibility rays on such opaque objects do not need either anyhit or closesthit programs but can handle the test with a miss shader.\\nAll unused shaders have nullptr values for the module and entryFunctionName in their resp.  OptixProgramGroupDesc field.)The type of the SBT record structure must be unique for each individual of the five OptixShaderBindingTable record field since that defines the stride of the SBT Records in that array.\\nMeans even if you do not source your SbtRecord struct T data field with optixGetSbtDataPointer() on device side, the space for that needs to be present in the arrays of SbtRecord.\\nHow you interpret the pointer returned by optixGetSbtDataPointer() is your decision. The data field will be the same for closesthit and anyhit though.\\n(You should also make sure you can never source uninitialized data from there.)Is an SBT record data field required for every program (shader) you write?No. You can have different SBT Record structures in the five different OptixShaderBindingTable record fields.\\nYou need to have room for SBT records in the SBT for all entries to make sure the SBT index calculation in this formula works for each ray! And for all SBT records which can actually be called, there must be the correct header field in the SBT record because it identifies the shaders to call, even if it’s a null shader.For example my applications have no data on non-hit records, because ray generation and miss shaders can source their data directly from the single launch parameters structure.\\nhttps://github.com/NVIDIA/OptiX_Apps/blob/master/apps/nvlink_shared/inc/Device.h#L183Have a look at the very simple optixTriangle SDK example which shows something like that as well.  (No data on the raygen and hit record and a color on the miss record.)Thank you @droettger for the response.There is a lot of information to take in - truly the SBT must be the most confusing part of OptiX.So, if I want to store the SbtCH and SbtAH into a CUdeviceptr hitRecord (as part of a single hit group record) I can’t do that as the code snippet shows?Sorry if this is a dumb statement/question but I am trying to learn.Thanks againYes, the SBT is a flexible thing.Oh, I didn’t realize that you actually missed the template typename  and arguments in the code and data is not a pointer but the struct itself.\\nIt should have looked like this:but as explained, differentiating this for closesthit and anyhit programs doesn’t make sense because they are both in the same SBT hit group records.Means simply combine the two which means you can use the CHData as data and if only the closesthit program touches the cnt field, then so be it.and then you can allocate a buffer of these on the device getting a CUdeviceptr, copy the necessary data from host to device into the header and data fields, and assign the CUdeviceptr to the OptixShaderBindingTable hitRecord field, set the stride and count correctly, and that’s it.(All the green text in the posts are links. Follow them for more information.)Thank you @droettger for the information, much appreciated.So if I had multiple hit “types”, say instead of just closest hit or any hit I also have an __intersection__it might be best to combine them into a single type of SBT to store in a CUdeviceptr and let DEVICE side code (shader) define how to use the SBT.Thanks again and sorry for the questions on a holiday weekend.No holiday on my side of the Atlantic. :-)So if I had multiple hit “types”, say instead of just closest hit or any hit I also have an __intersection__ it might be best to combine them into a single type of SBT to store in a CUdeviceptr and let DEVICE side code (shader) define how to use the SBT.That’s the whole point. Intersection, anyhit, and closesthit programs together form one OptixProgramHitGroup.There are these five OptixShaderBindingTable record array fields, namely:\\nraygenRecord (single entry),\\nexceptionRecord (single entry),\\nmissRecordBase (one per ray type),\\nhitgroupRecordBase (as many as you need to express your desired mapping of scene graph elements to hit groups times ray types),\\ncallablesRecordBase (one per direct or continuation callable).Means the hitgroupRecordBase holds a CUdeviceptr to an array of SBT records with hitgroupRecordCount many elements and hitgroupRecordStrideInBytes stride per element which is a multiple of OPTIX_SBT_RECORD_ALIGNMENT (16 bytes) and at least OPTIX_SBT_RECORD_HEADER_SIZE (32 bytes) big.That means all your SBT hit records must fit into that stride, which means the biggest hit record defines that whole array layout. Think of it as a union of structures.Now OptiX doesn’t care how you interpret the pointer to the data field in an SBT record returned by optixGetSbtDataPointer() means each hit group record can interpret that differently if you like.\\nSince the SBT record header always has the same size, the pointer to the data field is always at the same offset in the SBT record structure.\\nMeans if you really have different hit record structures for different OptixProgramHitGroup (== different combinations of intersection+anyhit+closesthit programs) then you can do that by carefully filling in the resp. data fields and correctly interpreting the returned optixGetSbtDataPointer().\\nMind that doing this requires matching anyhit and closesthit program behavior.Even when having different geometric primitives, I have not required different hit record structures in my applications.\\nE.g. lets say when using built-in triangles and built-in curves. I store the vertex attributes and indices array pointers in the hit group record, plus some index or pointer to the material parameters. So while the closesthit programs for these work vastly different, the hit record had exactly the same structure. Keeping it simple.But if, for example, the material parameters should be part of the SBT hit record, then that would be similar to your case with rather different parameters (data structure)  for triangle and curve (hair) shaders. Again, the bigger one defines the hit record stride.That means all your SBT hit records must fit into that stride, which means the biggest hit record defines that whole array layout. Think of it as a union of structures.It makes perfect sense thinking of it as a union of structs :). You define how the data variable is employed.But if, for example, the material parameters should be part of the SBT hit record, then that would be similar to your case with rather different parameters ( data structure) for triangle and curve (hair) shaders. Again, the bigger one defines the hit record stride.So even if I have different parameters for different hit (e.g. an intersection versus a closest hit) the larger one defines the record stride - I get it now.Thank you a million times. If I am correct, I see it clearly now.So even if I have different parameters for different hit (e.g. an intersection versus a closest hit) the larger one defines the record stride - I get it now.Again, please drop that idea of having different hit record data structures for the intersection, anyhit, and closesthit programs.\\nThese three belong together into a single OptixProgramGroup and consequently use the exact same hit record data structure, of which not all programs necessarily need to read all fields.For example, the intersection program might only need to read the primitive indices and vertex positions to calculate the intersection, while the closesthit program reads these as well to calculate the final shading attributes, but normally also reads additional data in that hit record like material parameters for example, which are irrelevant for the intersection.Only different combinations of those three program types IS+AH+CH can have different hit records, like in my example, a hit record for built-in triangle geometry (which do not have a user defined intersection program) and a different hit record for curve primitives (which have a pre-defined intersection program you need to put into the OptixProgramGroup).Means the union (== maximum size of all SBT hit record types used in a pipeline) of these two completely different sets of IS+AH+CH programs defines the stride of the hit record groups array in the hitgroupRecordBase of the OptixShaderBindingTable. That’s all.Got it. It is all very clear to me now.I appreciate your patience with my understanding. Thank you again for all you assistance.No problem. The shader binding table is a central topic which must be understood or nothing will work.Note that when I talked about reading the vertex attributes and indices data from an SBT hit record above, I was thinking of an SBT with a hit record per OptixInstance.\\nWithout an instance acceleration structure (IAS), means only using a single geometry acceleration structure (GAS) as the scene, things will be a lot less flexible and the SBT hit records can naturally be used to store the required data.I use an IAS->GAS scene structure in my OptiX 7 examples and an SBT entry per instance because that allowed changing shaders (SBT hit record header) per instance without updating the IAS because nothing changed in the OptixInstances.\\nThis is kind of wasteful when having few materials and many instances (e.g. millions).\\nI would recommend implementing that differently today to simplify the SBT handling.There is another elegant method to index into the SBT for the material and hold the per instance data separately.\\nThe SBT hit records only need to contain the 32 byte header information which selects the material programs defining the material behavior (bi-directional scattering distribution function, BSDF), but none of the input parameters.\\nWhich material is used can be directly selected with the OptixInstance sbtOffset value.All other data required to define an OptixInstance’s geometry topology (vertex attributes and indices) and any other data like material parameters can be stored in separate device memory arrays and uniquely indexed by the user defined  OptixInstance instanceId field which can be read inside the device code with optixGetInstanceId() which is available in IS, AH, and CH programs.\\nNote that there is also the optixGetInstanceIndex() which returns the zero based index inside an IAS. Means when the scene is using a single IAS as the root (implies OPTIX_TRAVERSABLE_GRAPH_FLAG_ALLOW_SINGLE_LEVEL_INSTANCING)  that would be another unique per instance index which could be used to access custom data in device memory.Now, changing material parameters of such an instance would only need to update the material parameter buffer of that instance.\\n(That’s normally done by copying data from host to device with a CUDA host functions, but that could even be done in a kernel on the device since it’s just some device memory pointer. Just in case you’d ever need to animate a huge number of material parameters programmatically.)Switching the instance’s material shader (BSDF)  would need to set a different sbtOffset and update the IAS with optixAccelBuild(). Though that’s  a pretty fast operation.That would provide the smallest possible shader binding table while having all remaining flexibilty offered by OptixInstances.Think of this example:\\nA material library has three different shaders for wood: matte, oiled, coated.\\nBut there are many different kinds of wood types (textures) in that material library and each can be used with the matte, oiled, and coated wood shaders.\\nMeans you would only need three hit records in the SBT for the different shaders, but can have arbitrary many material instances with different input parameters when indexing these via the OptixInstance instanceId.The design of the SBT boils down to the available inputs for the SBT index calculation formula and what an application requires.\\nhttps://raytracing-docs.nvidia.com/optix7/guide/index.html#shader_binding_table#acceleration-structures\\nIt’s your choice how to architect that in the application. The SBT is very flexible to allow handling of different application requirements, and we haven’t even talked about multiple SBT entries per GAS.Thanks for the information @droettgerThere is another elegant method to index into the SBT for the material and hold the per instance data separately.\\nThe SBT hit records only need to contain the 32 byte header information which selects the material programs defining the material behavior (bi-directional scattering distribution function, BSDF), but none of the input parameters.\\nWhich material is used can be directly selected with the OptixInstance sbtOffset value.\\nAll other data required to define an OptixInstance’s geometry topology (vertex attributes and indices) and any other data like material parameters can be stored in separate device memory arrays and uniquely indexed by the user defined OptixInstance instanceId field which can be read inside the device code with optixGetInstanceId() which is available in IS, AH, and CH programs.This smallest possible shader binding table architecture is now shown inside the rtigo10 example added to the OptiX 7 Advanced Examples:\\nhttps://forums.developer.nvidia.com/t/optix-advanced-samples-on-github/48410/8Powered by Discourse, best viewed with JavaScript enabled',\n",
       " '52min-video-on-setup-up-cloudxr-aws-with-quest': 'Hi there, I made a video a year ago on setting up Cloud XR.I’m relying on AWS and the Quest. This is quite introductory as I don’t customize much, e.g the Quest client is the base one. The video description has timestamp so that you can skip the introduction (assuming you are already familiar with the goal of Cloud XR) or jump wherever you need.AWS setup starting with AMI selection 10:00, IP push on CloudXRLaunchOptions.txt to the Quest via adb 16:00, RDP connection 21:00, Steam setup 27:00, CloudXR server setup 30:00, RDP display leading to green/black screen problem 33:00, 1st render...Feel free to share feedback on what was incorrect, unclear or if you have any suggestion.Hope it helps,\\nFabienPowered by Discourse, best viewed with JavaScript enabled',\n",
       " 'optix-and-underlying-cuda-blocks-threads': 'Can I somehow change (or at least check out) the amount of underlying cuda threads and blocks used in the optix-based application. I need this to compare performance of existing OptiX app in different parallelism scenarios, to measure (and to demonstrate it accordingly) the gain in speed of adding more cuda threads.\\nCan anyone help me with this?Actually i have the same question, can we have a size control of block and grid in optix?This troubles me a lotOr could anyone just tell me it’s impossible in optix?Powered by Discourse, best viewed with JavaScript enabled',\n",
       " 'nvidia-smi-has-failed-because-it-couldnt-communicate-with-the-nvidia-driver-make-sure-that-the-latest-nvidia-driver-is-installed-and-running': 'Hi, I mount an NVIDIA GeForce GTX 165 on my XPS 15 9500, dual booted with Ubuntu 18.04 and Windows 11. After updating nvidia-drivers to latest version (510), I startert experiencing screen problems, such as screen cannot lock or, after forcing screen locking with Super+L, if I enter Password on Log Screen  it seems like I started a new session, with all running application closed.$ uname -a\\nLinux andrea-XPS-15-9500 5.4.0-90-generic #101~18.04.1-Ubuntu SMP Fri Oct 22 09:25:04 UTC 2021 x86_64 x86_64 x86_64 GNU/Linux$ ubuntu-drivers devices\\nWARNING:root:_pkg_get_support nvidia-driver-510: package has invalid Support PBheader, cannot determine support level\\n== /sys/devices/pci0000:00/0000:00:01.0/0000:01:00.0 ==\\nmodalias : pci:v000010DEd00001F95sv00001028sd0000097Dbc03sc02i00\\nvendor   : NVIDIA Corporation\\ndriver   : nvidia-driver-470 - third-party non-free\\ndriver   : nvidia-driver-450-server - distro non-free\\ndriver   : nvidia-driver-470-server - distro non-free\\ndriver   : nvidia-driver-510 - third-party non-free recommended\\ndriver   : xserver-xorg-video-nouveau - distro free builtinThe graphic card seems not to be recognized by VGA:$ lspci -v | grep VGA\\n00:02.0 VGA compatible controller: Intel Corporation Device 9bc4 (rev 05) (prog-if 00 [VGA controller])nvidia-smi\\nNVIDIA-SMI has failed because it couldn’t communicate with the NVIDIA driver. Make sure that the latest NVIDIA driver is installed and running.\\nnvidia-bug-report.log.gz (161.2 KB)I also report my nvidia-bug-report.logPowered by Discourse, best viewed with JavaScript enabled',\n",
       " '2-gpus-2-processes-how-to-copy-textures-from-one-to-the-other-through-nvlink': 'Is it possible to do this and how to achieve it?I have 2 processes ProcessA ( internal software) Process B (Unreal) 2 nvidia quadro gpus with and nvlink, lets call them GPUA and GPUB.I want to reserve a gpu for each process. So ProcessA on GPUA and ProcessB on GPUB and be able to copy directx textures from ProcessA to ProcessB through the nvlink. Is this possible?I know that ion directx12 there is explicit multi GPUs and that there is 2 modes, linked gpus and unlinked gpus.Solution 1\\nI am pretty sure that enabling SLI in the nvidia control panel will put the 2 adapters in linked mode and the 2 gpus will appear as 1 adapter ( IDXGIFactory1::EnumAdapters1). Which will make it hard or impossible to force each process to stay/use only 1 of the gpu cards.Solution 2\\nI can also do it using unlinked adapters, I just have to deal with 2 devices. The thing is I don’t know how to make sure that when I copy resources from ProcessA ( GPUA) to ProcessB (GPUB) that it will use the nvlink.Anybody have info on if any of the solution are possible and how? Or any other suggestion?Thanks a lot!Powered by Discourse, best viewed with JavaScript enabled',\n",
       " 'windows-wont-boot-with-two-graphics-card-installed': 'Here is my system info:Windows 10 pro 64 - ver 1809 - build 17763.195i7-8700Asus Prime Z390-A motherboard64GB (4x16) Ripjaws V series DDR4 2666 CL152 x ADATA XPG SX6000Pro 512GB PCIe Gen 3x4 M.2 2280 NVMe drives (RAID 1)PNY Quadro P2000 graphics card (with latest BIOS updated from PNY)PNY Quadro P400 graphics card(All drives and BIOS are updated and current)If I only have one card (either the P2000 or P400) installed in either the x16_1 or x16_2 slot, the system boots and runs fine. Once I add the second card, Windows will try to boot, pausing up on the spinning dots a few times, and then give me a blue screen stating I have no bootable drive. If I go back into the BIOS, my NVMe (RAID 1) drives are gone. But, if I shut down, remove one card and power the system back on without entering BIOS, everything boots fine. If I do enter the BIOS, my drives show and everything is back to normal.Is this a GPU issue or a software issue?Powered by Discourse, best viewed with JavaScript enabled',\n",
       " 'how-to-add-dynamic-logo-and-subtitle-to-the-stream': 'Hi, How to add  dynamic logo and subtitle to the stream when transcoding?Powered by Discourse, best viewed with JavaScript enabled',\n",
       " 'test-tools-for-developing-a-cloudxr-client': 'Hi Sir,\\nAre there any test tools available for developing a CloudXR client to check if all functions are working properly?Thanks!!Powered by Discourse, best viewed with JavaScript enabled',\n",
       " 'make-nvidia-drivers-use-the-gpu-instead-of-the-igpu-for-0-a-d': 'Hi,I work for the FLOSS ( Free Libre and Open Source)  game 0 A.D. (play0ad.com) and I noticed that on most laptops it uses the integrated GPU instead of the dedicated one, which leads to poor performance, and sometimes crashes with buggy intel drivers. Is there any good reason for it?Also it would be nice to have it in the GeForce experience window.We have a significant audience more than 300k last year according to our statistics Download StatisticsSo it would be great if this could be fixed.To be clear I know we can tell our users to do that manually, but it’d be better if it was automatic…Found a solution for Windows. (Note that it only works if the user hasn’t forced the GPU)Hi @stanislas.dolcini , nice to read you again!Thank you very much for sharing that information. As you can imagine for Linux this is more involved, but in general if the user installs the correct NVIDIA drivers and follows installation instructions correctly, the NVIDIA GPU should be prioritized for GL usage. Any GL app or game will use the same renderer that is also running the X screen for your GLX context. If that is not the case you could use PRIME to override this.For Vulkan the situation is a bit different. You can explicitly pick the device (e.g. the discrete GPU) for your render queue and if you use that same device for your swapchain, the output will go through that device.Powered by Discourse, best viewed with JavaScript enabled',\n",
       " 'looking-for-hbao-ultra-built-for-d3d12': 'Hi,\\nWe had added HBAO+ ultra to UE4 four year ago.\\nNobody in our team doesn’t know where that integration sources from, including myself… I am not the one who worked on it at that time.Our HBAO+ ultra integration is available only for D3D11, now we need to make it working for D3D12.I manged to find some sources from https://github.com/NvPhysX/UnrealEngine,\\nbut anything I could find provides only (single pass) HBAO+, so if I apply that to our engine, even D3D11 mode cannot support HBAO+ ultra.Where can I find sources (libs, dlls, and hopefully code too) for HBAO ultra built for D3D12?Any help would be appreciated.DJ SongAs additional note, I know that the most recent HBAO integration is provided by VXGI branch, but built binaries are available only up to VXGI-4.18, not VXGI2 branch.Powered by Discourse, best viewed with JavaScript enabled',\n",
       " 'about-supported-mesh-format': 'Recently I am studying ‘optixMeshViewer’ of tutorial sample.\\nI was doubtful about studying it.\\nCan Optix read only .obj and .ply formats?\\nIf I follow certain rules, can Optix read any format?OptiX is a high-level ray casting SDK. It doesn’t know about any file formats at all, neither 3D geometry, nor images.You as a developer are responsible to provide any type of geometry data to OptiX, which you then handle in your OptiX device programs as needed, by implementing the necessary bounding box, intersection resp. attribute programs.The OptiX SDK examples only demonstrate that with some simple scene formats.\\nThere is no need to use any of the SDK example code in own OptiX applications.\\nMeans you’re not at all limited by what the SDK examples show. They are meant to be small and simple.If you looked at the OptiX Advanced Samples on github, the OptiX Introduction examples in there are generating simple geometry objects from triangles like a plane, box, sphere, or torus at runtime.\\n[url]https://devtalk.nvidia.com/default/topic/998546/optix/optix-advanced-samples-on-github/[/url]If you learned how to setup geometry and materials with OptiX node objects yourself, you’d be able to handle any model file format you can write a loader for.Of course, in tutorial, I’ve seen how to use the .obj file format to determine geometry, and I can do geometry configuration.\\nI was just wondering about scanning Optix meshes and finding scanOBJ and scanPLY, and I asked them to recommend samples that were applied in other formats.Thank you for helping anything. :)Powered by Discourse, best viewed with JavaScript enabled',\n",
       " 'load-an-obj-in-the-pathtracer-example': 'In the path tracer example in the SDK I want to add an OBJ file to trace, so I went over to the loadGeometry() function, and right after the last parallelogram creation, I added this code blocknote that “gis” is a “GeometryInstance” vector.When I run it, the display window opens, and immediately closes and I get the following exceptions:If I comment out the modified code block, it works fine.How can I load an OBJ file to the tracer? Do I need to add something in the shaders / RT_PROGRAMs side?\\nThank in advance!P.S. I know that the loadMesh function takes care of the material, but since the program doesn’t work, I tried to set a material just like it’s shown for all the other GeometryInstances, as demonstrated in the code block above.These host-side exceptions should come with descriptive error messages that should tell you more about what’s going on.  Maybe try running the app from a cmd shell if you can’t find it in VS.Found what you were asking for, the error description is:OptiX Error: ‘Type mismatch (Details: Function “_rtContextLaunch2D” caught exception: Variable “lights” assigned type Buffer(1d, 60 byte element).  Should be Buffer(1d, 32 byte element).)’Not quite sure what it means by that…the “lights” variable is added to the context in the following way:And the “ParallelogramLight” struct is as followsMixing and matching code from multiple OptiX SDK examples is not that easy because many of them have different material implementations which need to be adjusted.In this case the “lights” on device side from the Phong shader used inside the optixMeshViewer is a BasicLight, but the “lights” variable in the optixPathTracer is a ParallelogramLight, which results in this validation error because the element size of the buffer is not matching the rtBuffer declarations in all your device code.If you’re new to OptiX, I’d recommend you watch the OptiX introduction presentation and work through the open source examples.\\nLinks here: [url]https://devtalk.nvidia.com/default/topic/998546/optix/optix-advanced-samples-on-github/[/url]That explains how to setup triangle geometries step-by-step which helps to get arbitrary models loaded into some OptiX application.\\nIt also explains how to implement an elegant architecture for a uni-directional path tracer with OptiX which supports more BSDFs which can be easily extended.\\nIt’s probably more rewarding to add a custom OBJ geometry loader to that than to the optixPathTracer example.And now that you know what the error is, the message from the exception in retrospect is pretty descriptive, right?Yeah thank you for that! Although I understand the error, still don’t know how to fix it just from looking at it :DI’ll take Detlef Roettger’s advice and go through the advanced tutorials!Thank youSo I see the examples use Optix 5, which is built on CUDA 9.\\nI have a GTX 1060 on my laptop, and I when I tried to install CUDA 9, my computer crashed bad to the point where I had to format it and reinstall windows… So I reverted back to CUDA 8, and therefore using OptiX 4.1.1My question being, is there any way I can compile OptiX 5 apps with my GPU? Or maybe I made a mistake while installing CUDA 9 last time? It’s just too much of a risk to try to install it againThis is getting off topic, but my advice on installing CUDA is to NOT install the driver that comes with the toolkit.  Install and manage the driver separately.  That makes the problem a little more separable.OptiX 5 versions require a display driver with CUDA 9.0 support.If you install the CUDA 9.0 toolkit via the custom option where you should disable all display driver components first, and afterwards install the newest available display driver for your system, you should be fine.\\n(That there is a display driver inside the CUDA toolkit at all is one of my pet peeves. It’s outdated as soon as there is a released public driver with support for that CUDA version.)You do not necessarily need to use the CUDA 9.0 toolkit to compile CUDA to PTX source code for OptiX 5.1.0, but I would highly recommend that.\\nUsing CUDA 8.0 for that would be fine as well.\\nHave a look into the OptiX release notes document which lists multiple CUDA toolkit versions.It’s more a matter of which host compiler is compatible to what CUDA toolkit for that choice and that in turn is listed inside the CUDA toolkit CUDA_Installation_Guide_.pdf documents in the resp.  installation.\\nBut it’s not always obvious what works together: [url]https://devtalk.nvidia.com/default/topic/1036401/?comment=5265110[/url]Powered by Discourse, best viewed with JavaScript enabled',\n",
       " 'hairworks-stand-share-mesh-vertices-with-other-lines-but-thats-not-true-pictures-attached': 'Hey i have an strange error i get everytime this error, but there are no Sharing Mesh Vertices.External Mediaso and there are \" the Duplicated\" but thats not True, Just one Guide on One Vertice.External MediaCan Anyone help me pls?Powered by Discourse, best viewed with JavaScript enabled',\n",
       " 'possible-bug-in-opencl-on-gtx-1080': 'Hi,I believe I may have encountered a bug in the CUDA OpenCL stack. A trivial 2 line program generates different outputs when ran on 4 different devices. I have included a self-contained script to reproduce this here:[url]https://pastebin.com/B0LcbGca[/url]Apologies I’m not sure where to post this. Please advise on somewhere more suitable.Cheers,\\nChrisFollowing up, more test cases:https://pastebin.com/SBexknsS\\nhttps://pastebin.com/CrG0X7D0\\nhttps://pastebin.com/Q7GXhUY8\\nhttps://pastebin.com/dkhY5hHrCheers,\\nChrisTwo self-contained C programs which segfault during compilation:http://paste.ubuntu.com/25037746/\\nhttp://paste.ubuntu.com/25037747/Cheers,\\nChrisChris\\nThanks, for bug report for OpenCLPowered by Discourse, best viewed with JavaScript enabled',\n",
       " 'vulkan-driver-memory-leak': 'Hello.While going through some Vulkan tutorials available online, I got an idea to check some examples with memory profiler. So I got Intel Inspector 2017 (student) and it found memory leaks in every single Vulkan application, even the most simple it can be, vkEnumerateInstanceExtensionProperties or the ones included in LunarG SDK.There is always nvoglv32.dll file involved, marked as Kernel resource leak…See attached screenshots with different GeForce drivers. There are less leaks in older versions, however older than 368.81 does not support my GTX1060 so I can’t check them:372.70 (Latest)\\nhttps://app.box.com/s/nizf4zg5sl3qxbg13wrqxn7u43zvtytk372.54\\nhttps://app.box.com/s/gcaehfxe8elxkmwmvinmccak6ofjkrqp368.81\\nhttps://app.box.com/s/23bm1zhpx1rb21qcvvnfq45iqr6ux6roMy PC: i7-6700K, MSI GTX1060, Win10 64bit LunarG SDK 1.0.26 (also tested on 1.0.24)I can also confirm this, but I’m on Antergos Linux.My Github: Vulkan Triangle example.\\nhttps://github.com/leestripp/vulkan_triangleMy PC:  Antergos Linux 64-bit, Gnome 3.20.2.\\nIntel® Core™ i5-4690K CPU @ 3.50GHz × 4\\n31.4 GiB ran\\nGeForce GTX 970/PCIe/SSE2\\nNvidia Driver : 370.28\\nLunarG SDK 1.0.26 : validation layers onPowered by Discourse, best viewed with JavaScript enabled',\n",
       " 'driver-crash-when-using-too-many-optix-objects': 'Hi, I obtain a driver crash (the screen becomes black and the driver reload itself) if I use too many optix objects (see below)The issue is independent from CUDA version (tried with CUDA 6.5 and 7.5)The issue is independent from driver type (same behaviour on QUADRO K2100M and GeForce 460M)The issue is inside the optix 32 bit library (optix 3.5.1 works, 3.6.3 crashes, 3.7.0 crashes and 3.8.0 crashes). Have not tried with version 3.6.0 becouse the 32bit SDK contains a 64 bit library…I’m using windows 7 64 bit on both machines. I’ using a dynamic loaded library with delphi (32 bit) since optix 2.0 so I cannot give you a visual-studio code sample (sorry)I use the 32 bit version of Optix and I cannot use/try the 64 bit versionThe issue happens in a deterministic way if I use more then 400 objetcs (mixed) of this typeSelectors, Groups, GometryGroups, Geometry, etcThe exact limit is near 400 but depends onto the combination of the objets aboveFor example I get NO crash with19 x Selectors\\n41 x Groups\\n85 x Transforms\\n85 x Geometry Groups\\n85 x Geometry instances\\n85 x Geometry\\nSum = 400For example I get a crash with19 x Selectors\\n42 x Groups\\n86 x Transforms\\n86 x Geometry Groups\\n86 x Geometry instances\\n86 x Geometry\\nSum = 405For example I get NO crash with17 x Selectors\\n39 x Groups\\n86 x Transforms\\n86 x Geometry Groups\\n86 x Geometry instances\\n86 x Geometry\\nSum = 400For example I get a crash with18 x Selectors\\n41 x Groups\\n92 x Transforms\\n92 x Geometry Groups\\n92 x Geometry instances\\n92 x Geometry\\nSum = 427The limit is near the value 400 (Sum=399,400,401)From my point of view is a “short buffer” somewhere into Optix Library (32 bit)The crash happens when a lunch the render (Optix gets a “CUDA exception” and the driver restarts within 1~2 seconds)The issue is independent of the acceleration structures types that I use. Happens with “NoAccel” or with others combinations.This is my analysis.Please help becouse Optix 3.5.1, which always works (NO CRASHES) with at least twice the objects (> 900), does not work with my brand new Quadro (Details: Function “_rtContextLaunch2D” caught exception: Encountered a CUDA error: cuGLGetDevices() returned (304): Unknown, |3801520|)Thank youRDIt sounds like you are experiencing a Windows Timeout Detection and Recovery (TDR) error. See this post for details.Tomorrow morning (here in Italy is 19:30) i will set the environment variable “OPTIX_API_CAPTURE” and i will try to create the minimum set of calls that create the crash.One question: when I’m ready with the “dump” which is the e-mail address to use to send you the data?THXHi, I have created the minimum set of “boxes” that crash Optix (>3.6.X)Its a set of simple boxex created with 12 triangles each (the issue is indipendent of the mesh complexity)Here is what I get if add one more box to te image:Details: Function “_rtContextLaunch2D” caught exception: Encountered a CUDA error: result returned (700): Unknown, |6619204|All details (image and traces) sent to optix-help@nvidia.comByeUnfortunately that mail got stuck and is possibly wiped in the meantime by our e-mail spam servers because it had a *.zip attachment which are blocked by default.\\nCould you please resend the trace but rename the attachment extension to *.zi_ to let it pass through?\\nThanks and sorry for the inconvenience.Unfortunately that mail got stuck and is possibly wiped in the meantime by our e-mail spam servers because it had a *.zip attachment which are blocked by default.\\nCould you please resend the trace but rename the attachment extension to *.zi_ to let it pass through?\\nThanks and sorry for the inconvenience.OK, filename changed to *.zi_ two times (recursive)THXHi, were you able to reproduce the issue with my data?THXPowered by Discourse, best viewed with JavaScript enabled',\n",
       " '255-00-26-installation-fails-on-linux': 'At the moment I can’t seem to install the beta vulkan driver. Here is the nvidia-installer.logPastebin.com is the number one paste tool since 2002. Pastebin is a website where you can store text online for a set period of time.What version of Linux and what compiler? I’m able to install the driver on CentOS 7 with GCC 4.8.5.$ /bin/cc --version\\ncc (GCC) 4.8.5 20150623 (Red Hat 4.8.5-4)\\nCopyright (C) 2015 Free Software Foundation, Inc.$ lsb_release -d\\nDescription:\\tCentOS Linux release 7.2.1511 (Core)Looks like you’re using Linux >= 4.3 which isn’t supported. It’s rather trivial to patch the driver to get it to build and work.Just apply this patch to kernel/nvidia/nv-procfs.cIf you’re using a newer kernel your distro probably also ships xorg-server 1.18 which also isn’t supported by this branch so you will have to downgrade that to 1.17.Using current Vulkan driver [*] is not yet supported in ubuntu 16.04? It uses kernel 4.4\\nIs Vulkan yet supported Proprietary GPU Drivers PPA [**]. It has version 361[*] https://developer.nvidia.com/linux64bit\\nNVIDIA-Linux-x86_64-355.00.26.run[**] Proprietary GPU Drivers : “Graphics Drivers” teamIt only builds with the patch I provided on Linux >= 4.3. That’s nothing special. NVIDIA has good support for the latest stable kernel with the official driver, but this driver was based on an older branch. Every 1-2 years it might occur once that bleeding edge distros like Arch Linux ship the latest stable kernel that isn’t officially supported by a new release of the driver so they just ship it with a patch.Update: Ubuntu updated Vulkan PPA to support nvidia-355-vulkan\\nhttps://launchpad.net/~canonical-x/+archive/ubuntu/vulkan?field.series_filter=xenialtrying now (doing install on USB key)Linux 4.3, Xorg 1.18, gcc 4.9.3, Void Linux.If this branch doesn’t support xorg 1.18 then I’m just going to have to wait, downgrading is a hassle I don’t want to deal with.Powered by Discourse, best viewed with JavaScript enabled',\n",
       " 'nv-enc-err-unsupported-param': 'Hello,On a P4000 card, I am not able to encode HEVC at YUV4:4:4 at 8 bits per channel.  This should be a supported format according to documentation.  Can someone help? I am on a Windows 10 64 bit machineThanks!VijayHere is more info:Driver version: 391.33\\nVideo BIOS version: 86.04.56.00.0B\\nPart number: G410 0501Hi kamarshi,Could you provide us with some more additional information:Thanks,\\nRyan ParkPowered by Discourse, best viewed with JavaScript enabled',\n",
       " 'resolved-to-be-deleted': '–resolved, to be deleted–Powered by Discourse, best viewed with JavaScript enabled',\n",
       " 'how-can-we-install-a-new-app-in-the-android-mobile': 'How can we install a new app in the android mobile as I am having a doubt regarding this information how to install so please I need a clarification regarding a new app in any mobile especially from android\\n[url]http://www.cbse10thresults-2019.xyz/2019/12/cbse-10th-results-2020.html[/url]Powered by Discourse, best viewed with JavaScript enabled',\n",
       " 'pxd6joint-problem': 'Hello. My problem is that I can not properly restrict rotation PxD6Jointt, I make the car and can not configure correctly restrictions for the front wheels and the rotation of the steering.Hello, did you solve your problem ?Are you sure you set the correct flags for limits ?I had a problem with limits not holding correctly, there are 2 things that I made that helped:Powered by Discourse, best viewed with JavaScript enabled',\n",
       " 'creating-eglimagekhr-image-from-render-buffer': 'Hello,I have a jetson xavier nx platform and currently trying to create EGLImageKHR image from render buffer on a qml application to move a qml application user interface from one device to another device. I tried to use GLES/gl2.h and GLES/gl2ext.h api instead of QOpenGLFunctions.I succesfully created EGLImageKHR image from EGL_GL_RENDERBUFFER_KHR and just wanted to check the image if it is correct or not before sending it to another device . When I try to create some png image from EGLImageKHR image I see incorrect png as in the attachment.Also I added implementation to create EGLImageKHR image and png creation part from the image.wl_egl.h (947 Bytes) wl_egl.cpp (9.7 KB) main.qml (3.1 KB) main.cpp (3.2 KB) Could you please give me some idea to see what I am missing ?RegardsPowered by Discourse, best viewed with JavaScript enabled',\n",
       " 'weird-behavior-of-atomicadd-in-optix': 'Optix is said to be per-ray model, so in examples __raygen__rg() function calls are nicely isolated in their memory access by the pixel position.\\nI’m trying to implement some global statistics. I changed the image buffer into stats buffer; the access is now overlapping, so I clean the buffer on creation and use atomicAdd(float*, float) calls.\\nThis leads to weird results, though: I get NaNs - that is, until I try checking the incoming values explicitly using isnan(), in which case it magically starts working fine. Looks like some kind of a race condition. I there some obvious mistake that I’m making?How exactly are you clearing that stats buffer on the device?\\nWith some explicit kernel launch or one of the cuMemset calls (recommended) or some host to device memcopy ?\\nhttps://docs.nvidia.com/cuda/cuda-driver-api/group__CUDA__MEM.html#group__CUDA__MEM_1g983e8d8759acd1b64326317481fbf132Are you using an asynchronous call for that?\\nIs that using the same CUDA stream as the following optixLaunch?In that case the following optixLaunch should find the initialized values inside the device buffer and the first atomicAdd should always get the defined clear result.This should usually work. I’ve done this before for scattered color accumulations.\\nhttps://forums.developer.nvidia.com/t/best-strategy-for-splatting-image-for-bidir/111000/2It will obviously not work if buffer entries are cleared in the same optixLaunch call which do the atomicAdd because the single ray-programming model doesn’t allow any assumptions about neighboring launch indices.Some more information would be required to determine what could have gone wrong.What is your system configuration?\\nOS version, installed GPU(s), VRAM amount, display driver version, OptiX (major.minor.micro) version, CUDA toolkit version (major.minor) used to generate the input PTX, host compiler version.How did you allocate that stats buffer?\\nWhere does it reside (device, pinned memory, etc.)\\nAre there multiple GPUs involved?Maybe just post the exact code excerpts which create and initialize the buffer and all device code accessing the buffer.I do cudaMemset(m_device_pixels, 0, size) before calling optixLaunchUbuntu 22.04, 2070, Driver Version: 520.56.06    CUDA Version: 11.8What looks especially weird is that I tried checking the values, and when the check is active, the problem disappears, so it looks more like a race.That’s not enough information to be able to help further.\\nI understand that there might be some race condition. I just can’t say why that could happen, yet, and that’s why I’m asking all these things.Please answer all my questions (including the OptiX version number) or provide a minimal and complete reproducer in failing state, best by changing one of the OptiX SDK examples.Is the optixLaunch using a different stream than the default?\\nDoes the behavior change if you put a synchronization call between the cudaMemset and the optixLaunch?\\nhttps://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#explicit-synchronizationMy deepest apologies. The strange behavior was caused by my miscalculation that caused a local buffer overrun - it caused the weird results. Thanks again for your swift feedback!Powered by Discourse, best viewed with JavaScript enabled',\n",
       " 'progressive-refining-in-sdk-sample-6': 'Good day,I would like to try implementing progressive refining (as is done in the SDK Whitted example) into Sample 6 of the SDK. How should I go about doing this? It looks like the spheres in Sample 6 are not 3D models, but drawn with code. Sample 6 makes use of a 3D model. Does this change how I should approach this?So far it seems that I will have to rewrite the pinhole camera cuda file and make only minor changes to the cpp source files. Do you agree?Basically I’m just trying to wrap my head around how to approach this.ThanksMikeYes, you could use the ray generation code (adaptive_pinhole) in sample6 and copy the corresponding code that sets up the random number generator seeds in the host code.Note that in general, the camera code doesn’t care what sort of geometry is present in the scene.  A ray intersection is a ray intersection regardless if it came from an implicit surface evaluation, a triangle from a mesh, or some other procedurally generated geometry.I can use coarseTrace with my code, but the jittered_trace function does not work yet. The following line of code seems to be the problem:The error I get when using jittered_trace is:terminate called after throwing an instance of ‘optix::Exception’\\nwhat():  Unknown error (Details: Function “RTresult _rtContextLaunch2D(RTcontext_api*, unsigned int, RTsize, RTsize)” caught exception: Encountered a CUDA error: Kernel launch returned (700): Launch failed, [6619200])\\nAborted (core dumped)I am using the functions from adaptive_pinhole.cu and have copied the other code as you suggested. I have the following questions:Fixed the problem. It was specific to how I handled things in my RTPROGRAM.Powered by Discourse, best viewed with JavaScript enabled',\n",
       " 'resolved-rttrace-from-bindless-callable-programs': 'I’m trying to implement my material shaders in bindless callable programs per Detlef’s suggestion. According to the OptiX 6.0.0 release notes, it now supports “rtTrace from bindless callable programs”, which you couldn’t do in the past. However, I haven’t found any documentation or examples of how to do this.Currently, as long as I don’t call rtTrace in my bindless program, everything works fine. I can make ray and ray payload data structures, pass attributes by value, etc. However, as soon as I put rtTrace into the program, I get strange behavior:Here’s the closest hit code:The environment is RTX 8000, Windows driver 418.81, nvcc version 10.0.130I’ll send a stack trace to the help email.Yes, this not going to work like that without additional changes and there is no example inside the OptiX SDK, yet.OptiX cannot always detect automatically if bindless callable programs along a hierarchy of calls contain an rtTrace call which would need additional internal instrumentation to be able to call rtTrace.For that OptiX added a call site instrumentation which allows to tell OptiX which bindless callable program IDs are potentially calling which others.This works automatically when holding a bindless callable program ID directly in an rtDeclareVariable.\\nIt also works automatically when using buffers of bindless callable program IDs.\\nAll other cases need additional call site instrumentation on device side and some host side configuration.First, you need to use rtMarkedCallableProgramId instead of rtCallableProgramId when calling a bindless callable program ID with an rtTrace inside.Please look into the optix_device.h headers for more information on rtMarkedCallableProgramId.That rtMarkedCallableProgramId allows to define a call site via a constant string which can be used on the host side inside the newly added function rtProgramCallsiteSetPotentialCallees which allows to specify which bindless callable program IDs are potentially being called from specific rtMarkedCallableProgramId locations inside the device code.\\nThis allows OptiX to instrument the hierarchy of calls with the necessary information to be able to call an rtTrace.So your code should look something like this:That said, I would not use that mechanism when I can avoid it.\\nIf you can make the bindless callable programs only calculate information which can be used after the return inside the closest hit program to do the necessary rtTrace with these information, that would speed up the bindless callable programs. My OptiX introduction examples do it this way.Thank you for the quick reply, Detlef.I now get a compiler error:This occurs on the line:Sorry, wrong brackets.I’ve marked this resolved, but I’ll add a note as I found this rather tricky. It’s important that the method signature be an exact match, including all const qualifiers. Otherwise, the cudaDriver().CuEventSynchronize( m_event ) error may occur at any place where the code branches (usually at if statements or rtTrace calls).In my working solution, I have the following:It’s important that the method signature be an exact match, including all const qualifiers.That would be always required for all bindless callable program signatures.Really, I still recommend to avoid this functionality if you can. It’s meant for special material system implementations and should only be used if there is no other implementation possible.\\nThis functionality doesn’t come for free inside the compilation step and at runtime.Powered by Discourse, best viewed with JavaScript enabled',\n",
       " 'the-gpu-is-always-lost-when-i-run-tensorflow-program': 'Hi, I use Ubuntu 16.04.4 LTS, with Intel(R) Xeon(R) CPU, 256G Mem, 4 TITAN X GPU.\\nHowever, when I run TensorFlow program on the machine, the GPU always get lost.nvidia-bug-report.log.gz (209 KB)\\nnvidia-bug-report.log (596 KB)I posted it in the wrong forum section, anyone know how to delete this post?Powered by Discourse, best viewed with JavaScript enabled',\n",
       " 'any-update-about-malign-double-issue': 'Hi,\\nis there any update about -malign-double issue which is needed for gcc ?\\nThis flag is dangerous from what I read.\\nThanksPowered by Discourse, best viewed with JavaScript enabled',\n",
       " 'identifier-cudadevicesynchronize-is-undefined-in-device-code': 'I got my code running whith the errors like:\\nmain_array.cu(404): error: calling a host function(“cudaDeviceSynchronize”) from a device function(“ParameterIdentifyCOA::cal_y”) is not allowedmain_array.cu(404): error: identifier “cudaDeviceSynchronize” is undefined in device code\\nbut according to definition: __host__\\u200b__device__\\u200b[cudaError_t] cudaDeviceSynchronize ( void ), cudaDeviceSynchronize  is host and device function\\n.\\nThe CUDA version is CUDA 12.0 / Nvidia rtx 3090\\ng++ (GCC) 4.8.5 20150623 (Red Hat 4.8.5-44)The code runs normally under:\\nThe CUDA version is CUDA 11.8/ Nvidia rtx 3080ti\\ng++ (GCC) 9.3.1 20200408 (Red Hat 9.3.1-2)The system running is: CentOS7Powered by Discourse, best viewed with JavaScript enabled',\n",
       " 'closest-hit-program-any-hit-program-and-ray-type': 'Often, when we use Optix to conduct raytracing program, we will do as follow.\\n1.Usually, we need 2 types of ray, the radiance ray and shadow ray, so the RayTypeCount is 2. And then, the closest hit program relates to radiance ray, while the any hit program relates to shadow ray, must we observe the regulation? if so, why should we specify ray type when setting ClosestHitProgram and AnyHitProgram? If not, which means, we can do like that:In this case, what does it mean? does it mean that 0 represents shadow ray and 1 represents radiance ray? I am a little confused about the design.\\nMoreover, if I just want use one type of ray, for example, the shadow ray, then the ray type count should be 1.3.[code]\\nray = make_Ray(origin,dir,0,1.e-4f,RT_DEFAULT_MAX);\\nrtTrace(top_object,ray,ray_load);\\nIs it right to do like this?Usually, we need 2 types of ray, the radiance ray and shadow ray, so the RayTypeCount is 2. And then, the closest hit program relates to radiance ray, while the any hit program relates to shadow ray, must we observe the regulation?No, you can have any number of ray types and any number of closest hit and any hit programs. For instance you can do this:if so, why should we specify ray type when setting ClosestHitProgram and AnyHitProgram? If not, which means, we can do like that:In this case, what does it mean? does it mean that 0 represents shadow ray and 1 represents radiance ray?It depends on what ray 0 and ray 1 are. There is no rule that says that 0 must be a radiance ray or that 1 must be a shadow ray. What your code says is that you have assigned a closest hit program to the ray that you have called type 1, and you have assigned an any hit program to the ray that you have called type 0.Moreover, if I just want use one type of ray, for example, the shadow ray, then the ray type count should be 1.Is it right to do like this?Yes, that will work.@nljones, Thank you very much.\\nYou are right, it is really helpful.Powered by Discourse, best viewed with JavaScript enabled',\n",
       " 'apparently-an-unexplicable-error': 'I’m using OptiX in a personal project and I decided to add environment mapping just like the Quick Start guide does. So, here is the miss programWhen I attach this as my miss program I get the following errorPlaying with this program I found that if I comment the line where v is calculated, the program runs without errors.I’m stuck in this for a day and still not able to find a solution. Is there a way to check where the problem is?One observation is that when I run the tutorial example in the OptiX samples it works fine, however this is still a mistery to me since the code above is not even accessing the envmap texture.I don’t know if it is sad or not when this happens but right after I post this question I found the problem. Comparing my CMake file with optix samples’ one I found that I wasn’t supplying the –use-fast-math flag. After I set this flags the program works fine.Anyway, is this supposed to happen?That’s most likely an issue during program stitching. Hard to say without a reproducer.Wild guesses:If this doesn’t help it might make sense to send a reproducer in failing state to OptiX-Help to be able to file a bugreport. In that case the following information should be provided:\\nOperating system, 32/64-bit, graphics board, driver version, OptiX version, CUDA Toolkit version, targeted SM version, PTX bitness, (basically the nvcc command line).Just in case, my spherical environment mapping miss program looks like this.Edit: Damn, I had this answer open for too long. :-)\\nThe sin() probably generated a double instead of a float instruction and OptiX could have failed to handle it.Detlef,Thanks for your reply.If you build the tutorial yourself, does it work?\\nA: YESIf the error is in line 8 above, what about sinf()?\\nA: Any trigonometrical function cause the errorDo you have --use_fast_math on your nvcc command line?\\nA: Now I have.Could you try different SM versions as PTX target? (-arch=sm_20 etc.)\\nA: I’m using sm_13. I have a GTX570 but if I use sm_20 I get the following error:Question: do I have to set --use-fast-math in order to OptiX work properly?The question about the sinf() vs sin() was about the float vs. double version.\\nIf --use_fast_math fixes the issue, it could have been a double instruction inside the PTX which shouldn’t happen with use_fast_math which will result in float instructions throughout the code.\\nIf you look at the generated PTX you should be able to see the difference.To the version error, that would be the PTX instruction set version.\\nWhich CUDA toolkit version did you use?\\nI’d recommend 5.0. CUDA 5.5 is not yet supported. (See the OptiX release notes.)I’ll leave the final question for an OptiX core developer.\\nI haven’t used OptiX without the --use_fast_math so far, and I’m explicitly not using any doubles in my OptiX programs. But if you used CUDA 5.5 this could be other root causes.You shouldn’t need to use fast math to get optix to work.Which version of CUDA are you using to generate the input PTX?  OptiX doesn’t support CUDA 5.5 yet (CUDA 5.0 should generate PTX 3.1).Can you try with CUDA 5.0 and see if you still have a problem?I know that OptiX doesn’t officially support CUDA 5.5 but I compile all sdk samples with CUDA 5.5 and they all work fine since I’m using sm_13 as my target architecture.Unfortunately I can’t switch to CUDA 5.0 right now because I’m developing under Windows OS and I’m using some C++11 features that only VS2012 support. I have plans to support other systems but for now it is working fine.Since using sm_13 is working for my development purposes, I’m going to keep developing until an official CUDA 5.5 support from OptiX to be released.Thank you all for the time.Right now even if you manage to compile OptiX samples with CUDA 5.5, there’s no official support and you might encounter undefined behavior during execution.Good news are that OptiX CUDA 5.5 support is incoming. And so are VS2012 plus other interesting updates so you won’t have to wait for long.Until now everything is working fine but I’m look forward for a new version of OptiX;Powered by Discourse, best viewed with JavaScript enabled',\n",
       " 'capture-and-migrate-gfe-and-nvidia-driver-settings-to-multiple-systems': 'Hello!I work in Esports and am responsible for windows deployment in our studio here. I use MDT for deploying windows images and I am trying to automate absolute everything so I can build a production ready system from a task sequence (fully automated) from ISO to show ready.I am trying to capture Nvidia driver settings and GFE settings from a machine so I can add into the process above. I can’t seem to locate the files required to do so, however.Would any of you happen to know which files I need to grab and copy to make this happen?There are a lot of files in the C:\\\\ProgramData\\\\Nvidia and C:\\\\Users%USERNAME%\\\\AppData\\\\Local\\\\Nvidia folders and I have been unsuccessful with my testing so far.I would imagine its pretty straightforward once you know the exact file. Hopefully, someone here does.Any help is appreciated, Nvidia support guy sent me here as he had no clue hehePowered by Discourse, best viewed with JavaScript enabled',\n",
       " 'is-it-possible-to-disable-collision-response-but-keep-have-the-oncontact': 'Hi,\\nIs it possible to disable collision response for everything but still have the onContact event ?\\nThanksSure. In your filter shader, request contact notification but not collision response. For example, raising eDETECT_DISCRETE_CONTACT and whatever notification flags you want but not raising eSOLVE_CONTACTPowered by Discourse, best viewed with JavaScript enabled',\n",
       " 'how-to-configure-stun-server': 'I’m trying to run CloudXR in AWS Wavelenght where incoming UDP traffic blocked. Hence my NAT punch through is not working. I was wondering how i can configure my own STUN server within cloudXR? I didnt see anything described in the docs: Command-Line Options — NVIDIA CloudXR SDK documentationLog files:Powered by Discourse, best viewed with JavaScript enabled',\n",
       " 'quadro-m4000-vertical-line-in-between-2-moisac-displays': 'Hi all,Driver version: R440 U4 (441.66)Output resolution settings as follows:DP output 1 - 1920 x 1080 50Hz\\nDP output 2 - 4096 x 2160 50HzMoisac Display:\\nDP output 3 - 2560 x 1440 50Hz\\nDP output 4 - 2560 x 1440 50HzThere is a visible faint slight opaque vertical line between the 2 mosaic screens.Enclosed kindly take a look at the photos of the issues i’m facing for your kind reference:\\n(Uploaded to Imgur.com)Album:Kindly zoom in to see the details of the problem.Thanks and cheers~Hi ranvier,Curious as to what display you are connecting to with the mosaic display.thanks\\nDPowered by Discourse, best viewed with JavaScript enabled',\n",
       " 'graphics-driver-installation-failed-gtx-680-and-windows-8-1': 'Hi,I’m having a problem installing the beta Vulkan driver on my pc with GTX 680 and Windows 8.1 .At the end of the installation:\\nGraphics Driver - Failed.I already tried the clean install option.Is the GTX 680 compatible, right?Thanks.GTX 680 should be supported. Sorry that the driver didn’t install out-of-the-box.Possible causes :You uninstalled the previous driver and did not reboot before installing the new driverYour installation was intercepted by Windows (or Windows Update) installing a driver at the same time, which can happen when Windows is setup to automatically install driversIf you were not able to install the driver after a reboot, could you please collect the installer logs, as detailed here…http://nvidia.custhelp.com/app/answers/detail/a_id/3581/~/driver-installation-failed\\nhttp://nvidia.custhelp.com/app/answers/detail/a_id/3171and then submit via:https://developer.nvidia.com/content/nvidia-dev-support-submission-formThanks,MathiasPowered by Discourse, best viewed with JavaScript enabled',\n",
       " 'algorithm-for-ray-aabb-test-behind-the-acceleration-structures': 'Does OptiX use robust ray-AABB intersection test in the acceleration structures?UPD:To be more specific, I get different results for my launches, depending on the coordinates of the vertices in the triangle mesh and the position of the ray source. Namely, the result is almost unusable  if the geometry scaled 100 times up.AFAIK, there are three sources of precision loss in my rendered.While I have full control over the first two items, the third one seems to be hidden. And I need to know that to estimate the effects of scene geometry scaling and to find the acceptable range.For example, how does the algorithm relate to the described here http://jcgt.org/published/0002/02/02/paper.pdf?Powered by Discourse, best viewed with JavaScript enabled',\n",
       " 'h264-encode-yuv44-issue': 'Hello,I am using Nvidia Video Codec to encode YUV4:4:4 to H.264 .I compiled Samples/NvEncoder and it generate an executable file “NvEncoder”\\nThen I use following command:\\n./NvEncoder -i …/test.yuv -o test.264 -inputFormat 1 -size 1280 720\\nwhich produce encoded h264 file and “NvEncoder” has following output:Encoding input           : “…/test.yuv”\\noutput          : “test.264”\\ncodec           : “H264”\\nsize            : 1280x720\\nbitrate         : 5000000 bits/sec\\nvbvMaxBitrate   : 0 bits/sec\\nvbvSize         : 0 bits\\nfps             : 30 frames/sec\\nrcMode          : CONSTQP\\ngoplength       : INFINITE GOP\\nB frames        : 0\\nQP              : 28\\nInput Format      : YUV 444\\npreset          : DEFAULT\\nPicture Structure      : Frame Mode\\ndevicetype      : CUDAIssue is the “test.264” file has unknown profile_idc in its sps.\\nIt gives \" 00 00 00 01 67 F4 00 1F\"\\nF4 is the profile_idc = 244\\nThere is no definition of 244 in H264 Recommendation66:Baseline\\n77:Main profile\\n88:Extended profile\\n144:  High 4:4:4 profile\\n122:  High 4:2:2 profile\\n110:  High 10 profile\\n100:  High profileI’m actually not sure what happened with my encoder.244 is a legal value. The definition of SPS data in the spec has:if( profile_idc = = 100 | | profile_idc = = 110 | |\\nprofile_idc = = 122 | | profile_idc = = 244 | | profile_idc = = 44 | |\\nprofile_idc = = 83 | | profile_idc = = 86 | | profile_idc = = 118 | |\\nprofile_idc = = 128 )And later, for example:Conformance of a bitstream to the High 4:4:4 Intra profile is indicated by constraint_set3_flag being equal to 1 with\\nprofile_idc equal to 244. Decoders conforming to the High 4:4:4 Intra profile at a specific level shall be capable of\\ndecoding all bitstreams in which all of the following conditions are true:\\n– profile_idc is equal to 44, 100, 110, 122, or 244,\\n– constraint_set3_flag is equal to 1,\\n– level_idc represents a level less than or equal to the specified level.There are other cases with 244. Search for “244” in the spec.Powered by Discourse, best viewed with JavaScript enabled',\n",
       " 'newbie-question-images-at-60-hz-on-quadro-cx': 'Not sure if this is the right forum for this question…I have a set of images that I would like to display at 60 Hz on my display - stop motion animation.  I have a Dell T7400 Precision workstation running Windows 8 with a Nvidia Quadro CX card.  What is the simplest way to get this done?Powered by Discourse, best viewed with JavaScript enabled',\n",
       " 'double-precision-in-optix': 'Does OptiX support double precision rays (origin, direction, etc.)? If not, can it be adapted to do so?\\nI am trying to figure out if I can use OptiX as the basis for a physics (i.e. not graphical) ray tracer which requires double precision.\\nThanks,\\nCInteresting question. When I started using OptiX it was definitely not possible (or at least I could not figure out how to do it). I bypassed the problem by defining a double precision origin and direction in the per ray data. As I did not use any of the built in acceleration structures, I could do all the tracing on this per ray data. I did never set the origin and direction in optix ray structure.If you’re interested you can check out the code I produced under itom / MacroSim / wiki / Home — Bitbucket . If it suites your needs you’re welcome to use it.regardsflorianThanks superbetonio. I actually need the acceleration structures since I will have up to 100 million rays and/or one million “instances” per run. And I need the run done in about 1.5 seconds.Hmm, I see. You could still do the intersection and hit calculations based on double precision variables in the per ray data structures. You could then try to keep the built in ray.direction and ray.origin up to date with the double precision values. This could work if it is sufficient to do your acceleration structure calculations in single precision.That might work.  I will have to do some test to see how far off course the rays go when using single precision.Powered by Discourse, best viewed with JavaScript enabled',\n",
       " 'nvdec-post-decode-performance-issue': 'Hi,\\nI’m trying to integrate hw decoding capabilities into my video player, so I’m using Cuda 10.1 and the latest Video Codec SDK.\\nCurrently I’m able to decode h264/h265 video stream, but I’m not satisfied by the cpu performance.My test case is a matrix of 40 indipendent video player, and each instance plays the same live stream: H264 704*576 @ 15 FPS.In that situation, the CPU usage climbs up to 90% of my i7-8750H.As far as I understood, there’s nothing wrong with the decoding procedure. In fact, if I comment out the “Picture display” stage (NV12 CUDA_MEMCPY2D/Nv12ToColor32), the CPU usage drops to 12/15%.Any ideas? Thank you!Please can someone give help on my request?Hi fcetrini,\\nWe are trying to replicate this behavior internally. Can you provide more details, OS, GPU used, driver version etc. Have you tried profiling an application to see any specific API/function call consuming higher CPU cycles?For future reference, this issue is tracked internally as 200614071.Thanks.Hi mandar_godse, thank you for your reply.My dev machine: i7-8750H/GeForce GTX 1050/16GB ram\\nI’ve got 445.87 driver at the moment, tried older before with same results.Let me specify better my “Picture display” stage:These are my results (40 instances of H264 704*576 @ 15 FPS stream):Is there something I’m doing sooo wrong?\\nThanks.cuMemAlloc + Nv12ToColor32 + cuMemcpy2D to mapped arrayAre you actually allocating new buffers per frame, and then using the synchronous copy functions? You should probably avoid doing either, and use async operations on a dedicated CUDA stream (created with CU_STREAM_NON_BLOCKING), with only a single GPU to CPU sync point via explicit cuEventCreate(…, CU_EVENT_BLOCKING_SYNC) → cuEventRecord → cuEventSynchronize after the last async copy operation.Furthermore, ensure you have allocated your CUDA context with CU_CTX_SCHED_BLOCKING_SYNC, the default of CU_CTX_SCHED_AUTO (resolving to CU_CTX_SCHED_SPIN) yields horrible performance for any scenario where you have more streams than CPU cores.Finally, there is an issue with cuGraphicsMapResources, it’s unfortunately introducing an undesirable sync point between 3D and CUDA. You won’t notice that issue in terms of CPU utilization though, just stalled 3D context. You said your video players are independent, but using cuGraphicsMapResources to map resources in batch can give a huge win when displaying streams in parallel.Hi Ext3h, thanks for your reply.Yes my video players are independent, each instance got its own rendering pipeline, sharing only the same CudaContext (as documentation suggests). And indeed, the CudaContext is created with CU_CTX_SCHED_BLOCKING_SYNC flag.That said, can you please explain better your suggestion on using a dedicated cuda stream?\\nI’ve never used it before…is there some docs/example on the argument?About your last point, what do you mean with “cuGraphicsSubResourceGetMappedArray to map resources in batch”?Thank you again!Hi again, I’m doing some modifications to my code, I think I’m on the right track.In my previous code, I was allocating new buffers per frame, copying nv12, converting nv12->rgb and finally copying the rgb into my back buffer: it was a total mess!Instead, now I’m creating a single nv12 texture on the rendering side, and that texture I register with cuGraphicsD3D11RegisterResource. After decode/cuvidMapVideoFrame, I’m trying a direct copy of the decoded frame to the nv12 directx texture, and then let the pixel shader do its job.What I’m not succeeding is to copy the ENTIRE nv12 to the d3d11 texture array in a single shot.\\nThis is the code:cuGraphicsMapResources(1, &this->cuGraphicResource, this->cuStream);\\nCUarray dstArray;\\ncuGraphicsSubResourceGetMappedArray(&dstArray, this->cuGraphicResource, 0, 0);\\nCUDA_MEMCPY2D m = { 0 };\\nm.srcMemoryType = CU_MEMORYTYPE_DEVICE;\\nm.srcDevice = dpSrcFrame;\\nm.srcPitch = nSrcPitch;\\nm.dstMemoryType = CU_MEMORYTYPE_ARRAY;\\nm.dstArray = dstArray;\\nm.dstY = 0;\\nm.WidthInBytes = m_nWidth;\\nm.Height = m_nLumaHeight;That way I’m obviously obtaining a greenish frame on screen, because I’m copying only the luminance plane, and the cpu drops dramaticaly.\\nI’ve tried to play with the CUDA_MEMCPY2D parameters, without success.Any help? Thank you.Powered by Discourse, best viewed with JavaScript enabled',\n",
       " 'vk-nv-ray-tracing-motion-blur-on-geforce-rtx-2070-with-471-69-beta-drivers': 'Hello there,\\nI am try to get motionblur support into a Vulkan pathtracer. I saw there is since very recently the “VK_NV_ray_tracing_motion_blur” extension. I installed the Beta drivers and newest Vulkan version. As far as i understood my RTX 2070 card is supported for this extension but if i try to use it Vulkan reports me on the usage of :contextInfo.addDeviceExtension(VK_NV_RAY_TRACING_MOTION_BLUR_EXTENSION_NAME, false,\\n&rtPipelineFeature);Could NOT locate mandatory extension ‘VK_NV_ray_tracing_motion_blur’I would be very happy for any hint.Best,\\nBerndJudging from following link it seems to be Ampere exclusive:Vulkan hardware capability database.Thank you very much for your help.Powered by Discourse, best viewed with JavaScript enabled',\n",
       " 'the-client-device-descriptor-does-not-match-the-preconfigured-descriptor-on-the-server': 'Dear CloudXR team,I am already upgrade to 3.2.\\nthen , told me,\\n05-26 14:56:56.370  4608  4716 V CloudXR : Streamer state stopped. (0: 260), reason: 0x80030023\\n05-26 14:56:56.370  4608  4710 V CloudXR : Eye0 processing thread shutdown.\\n05-26 14:56:56.370  4608  4711 V CloudXR : Eye1 processing thread shutdown.\\n05-26 14:56:56.370  4608  4716 V CloudXR : Stopping the client because of descriptor mismatch - the client device descriptor does not match the preconfigured descriptor on the server. (8).\\n05-26 14:56:56.370  4608  4716 D CloudXR_Jni: [CloudXR]updateClientState state:cxrClientState_ConnectionAttemptFailed, reason:\\n05-26 14:56:56.372  4608  4712 V CloudXR : Worker thread shutdown.can you help to check this problem?Hi there,\\nThank you for trying out CloudXR 3.2. I am wondering if you have either created a pre-configured device on the server or you have tried connecting from two different devices to the same CloudXR server,?More details about pre-configuring for a specfic client device are available here: NVIDIA CloudXR Server — NVIDIA CloudXR SDK documentationIf you have two different client devices, for example, an Oculus Quest 2 and and HTC Vive Focus 3 you need to restart the CloudXR server between sessions.If you have two different client devices, for example, an Oculus Quest 2 and and HTC Vive Focus 3 you need to restart the CloudXR server between sessions.got it，thxPowered by Discourse, best viewed with JavaScript enabled',\n",
       " 'optix-mfc-wizard': 'I have downloaded Optix 4.0, I am working with CUDA 9.0, My problem is that I dont have an OPTIX wizard when I select a new project in Visual STudio 2015.It is also informed that samples of Optix Sdk are successfully running.Is there a wizard to create a MFC application supporting OptiX?Any help would be appreciated.\\nThanksPlease note that OptiX 4.1.1 is the latest available version at this time.\\nIf you’re using a 4.0.x version it’s recommended to update to the latest 4.1.1 version.CUDA 9.0 is not officially supported by any OptiX 4 version. It’s recommended to use the CUDA 8.0 toolkit with those.As far as I know there is no MFC Wizard included since some time. But that shouldn’t deter you from using OptiX.\\nAll you need would be the OptiX library and include folder and the CUDA Visual Studio integration to be able to translate your OptiX CUDA device code for the different program domains to PTX source code.\\nYou could also add a custom build step which calls the CUDA compiler nvcc with the proper command line options.\\nThose command line options normally contain machine=64, compute_30, use_fast_math, relocatable-device-code=true and most importantly ptx to just generate PTX source code and no -g or -G options for debug code.Hi, Detlef Roettger,\\nThanks for your guidance.Is there any specification about how to create a MFC or CUDA runtime application using Optix?Thanks.I’m not sure what exactly you’re asking for. What’s your experience level with MFC?MFC is just another framework to write applications. If you know how to write an MFC application from scratch, and as a bonus use some graphics API like OpenGL for rendering, there shouldn’t be a problem to add OptiX specific code to such an application.\\nIn the end all you need to do is to react on the various window messages and handle them as you like. Everything else is framework agnostic.Building CUDA applications with Visual Studio is explained inside the CUDA documentation.\\n[url]Installation Guide Windows :: CUDA Toolkit Documentation\\n[url]Quick Start Guide :: CUDA Toolkit DocumentationFor OptiX the *.cu compilation settings are different because you only translate *.cu to *.ptx files and not actual binary code and you normally don’t need any of the CUDA runtimes.I haven’t used MFC in years, but to get a working MFC project you just click something together inside the Visual Studio IDE as you need it and then add your customized code which actually does something useful.\\nTo add OptiX you would need to add the necessary OptiX and CUDA include paths to the compiler options, add the optix.1.lib to the linker inputs, and if you have the CUDA Visual Studio integration installed, make sure all *.cu CUDA source files which implement the OptiX domain specific programs are translated to PTX source code given the nvcc command line options described earlier.\\nOr again, write a small batch file with the proper nvvc command line (which can be tested and used independently of the project) and add that as custom build step.Detlef Roettger, Thanks for your reply.\\n(1)“To add OptiX you would need to add the necessary OptiX and CUDA include paths to the compiler options, add the optix.1.lib to the linker inputs,”\\nI am familiar with MFC, but I do not know Which paths and input libraries are necessary in order to use CUDA and OptiX. Is there any introduction about this in detailed? I know I can learn it from the samples.\\n(2) “and if you have the CUDA Visual Studio integration installed, make sure all *.cu CUDA source files which implement the OptiX domain specific programs are translated to PTX source code given the nvcc command line options described earlier.”\\nI have CUDA Visual Studio integration installed, but I don’t understand this statement. Do you mean, “You could also add a custom build step which calls the CUDA compiler nvcc with the proper command line options.Those command line options normally contain machine=64, compute_30, use_fast_math, relocatable-device-code=true and most importantly ptx to just generate PTX source code and no -g or -G options for debug code.”\\nBut how can I  do this?\\n(3)” Or again, write a small batch file with the proper nvvc command line (which can be tested and used independently of the project) and add that as custom build step.”\\nCan you give me some document links about this?Thanks for your continuous guidance.\\nBest regards.Because the samples in SDK 4.1.1 are build through command line. for example, The optiXHello sample, draw_color.cu is build by custom build and all the build configurations is done by written files.\\nIt is too difficult to follow it for me.I want to get a sample using visual studio IDE. Is it available and can it get by web?Thanks very much.Powered by Discourse, best viewed with JavaScript enabled',\n",
       " 'gpu-throttling': 'Hello,Just to confirm what I’m seeing on a Tesla T4 card, is this GPU throttling due to a 85° temperature limit?I found the answer myself, yes it is:Powered by Discourse, best viewed with JavaScript enabled',\n",
       " 'just-started-using-cloudxr-2-1-yesterday-on-iphone-but-running-into-tracking-issues-like-others-here-have-mentioned': 'Is it worth a shot to continue troubleshooting on 2.1 or should I just wait for CloudXR 3.0 since it seems like the tracking issues on iOS will be addressed in the new release?\\nThanks!P.S. can we get a release date for 3.0 soon? :)3.0 was released this morning.  Hopefully you’ll like the improved iOS sample.Powered by Discourse, best viewed with JavaScript enabled',\n",
       " 'optix-raise-error-when-finding-child-node-of-transform': 'I’m currently tring to add some animation to some actor, so I followed the instruction as the optixDynamicGeometry. As I update geometry’s position, I have to set dirty to all of the Graph node(the same structure as optixDynamicGeometry)\\nExternal Mediabut when I travel to geometry group node, the optix raise ‘Invalid value’ error\\nExternal MediaAny reply are appreciated!Hi, I don’t see an obvious issue in the code you’ve posted, would you be able to share a complete sample that reproduces the issue?Also let me know which OS & driver you’re using.–\\nDavid.Hi, I don’t see an obvious issue in the code you’ve posted, would you be able to share a complete sample that reproduces the issue?Also let me know which OS & driver you’re using.–\\nDavid.Thank you for your reply, here is the cpp file that related to this error, this is rebuild from the GitHub project optixPathTracer.\\nAnd I am currently using RTX2070  on Windows 10 1903 with driver version 419.67.\\noptixPathTracer.cpp (30.5 KB)\\nplyFactory.cpp (3.46 KB)\\nplyFactory.h (989 Bytes)\\nsceneLoader.cpp (13.6 KB)\\nsceneLoader.h (2.74 KB)Powered by Discourse, best viewed with JavaScript enabled',\n",
       " 'appdecd3d': 'I am trying to get some of the Video SDKs samples to work.  I am trying to try out the nvDec functionality outputting directly to the display via DirectX.  The goal is to obtain a video stream and do all decoding and rendering with Nvidia hardware.  I have an Nvidia Quadro P620 graphics card with Direct X 12 installed.  My Direct X driver has compatibility with DirectX 11.I have built and try to run the AppDecD3D sample application.  I am setting the Direct 3D option to use Direct 3D 11.  I have an h264 movie I am trying to decode and render.  The sample application reports out that it decodes the movie and shows corresponding stats that are correct, however the display window is just completely black.  The display does go away after the movie time has lapsed, so it thinks it is rendering.Any suggestions on what to check to correct the display output?Powered by Discourse, best viewed with JavaScript enabled',\n",
       " 'in-which-library-nv12tobgra32-function-located': 'I’m trying to build my own project based on the sample projects, the problematic part is that i can not figure out where this function implementation is located. tried reverse engineering it but no success…\\nIf someone can point me to which lib i should include it would be great.Sorry if this is a bit of a stupid question…Hi liron,Please look up the … \\\\Samples\\\\Utils. The function is implemented thorough a Cuda Kernel(ColorSpace.cu) present in same location.Thanks,\\nRyan ParkHi Ryan,I’m a bit confused, still new to cuda sorry, I’m using the Nvidia sdk samples, not the ones in the cuda directory. As I understand the later are no longer supported and the idea is to use the sdk ones as they allow a more friendly wrapper.So to make it clear, am I suppose to use the cuda examples or the Nvidia sdk ones.\\nIn case of the later, what lib file do I need to use in my case, or in that case I also need to use the current file?Thanks lironHi,We are referring you to the samples/Cuda kernels in Video Codec SDK only. And do keep using the Video Codec SDK.As we mentioned please look up in … Samples\\\\Utils folder inside the Video Codec SDK package. The function(Nv12ToBgra32) is implemented in ColorSpace.cu. The function is called from multiple samples inside the SDK. As a reference please look up …\\\\Samples\\\\AppDecode\\\\AppDecD3D\\\\AppDecD3D.cpp which calls the function “Nv12ToBgra32”, that illustrates as how to use the function.As a side note, some of the functionalities (like color space conversion, Resize etc.) are implemented using Cuda and those Cuda kernels are included inside the Video Codec SDK. The Cuda kernels are included as helper functionalities/utilities to help augment the decode/encode pipeline.Let us know if you need more clarification.Powered by Discourse, best viewed with JavaScript enabled',\n",
       " 'correct-install-setup-process-for-nvidia-cuda-oceanfft-sample-w-opengl-cufft-centos-7': 'Running centos 7 w gtx 980m device w nvidia 355.11 driver\\nInstalled cuda toolkit 7.5\\nWhe i try to build oceanfft cuda-opengl-cufft sample getting link error ld cannot find libGL.so\\nNon-GL samples build and run fineAlso curiously the glxinfo tool runs fine\\nand if I do ldd /usr/bin/glxinfo it shows\\nlibGL.so.1Tried installing freeglut3, latest glew tarball (get same error when I try to build that),\\nAlso Mesa GL and Glu development libraries, but still not fixed.Want to start over and make sure Im getting all required packages/dependencies in correct order.\\nWhat is the right way? Cant find complete instructions in nvidia toolkitThx.Well this post fixed my error: the libGL.so symlink was dangling, repointed to libGL.so.1 which points to libGL.so.355.11https://devtalk.nvidia.com/default/topic/928174/opengl/-usr-bin-ld-cannot-find-lgl-collect2-error-ld-returned-1-exit-status/However, pretty sure I installed a bunch of stuff I don’t really need while trying to fix this.\\nSo I’d still appreciate a clear explaniation of what the recommended minimum development setup configuration is to support CUDA with OpenGL integration.(i.e. is installing freeglut3 and/or glew-1-13.0 redundant? what about the mesa GL devel libraries?)New to Linux and CUDA/OpenGL development, so everything is confusing. Thanks.Powered by Discourse, best viewed with JavaScript enabled',\n",
       " 'nvencgetencodepresetconfig-nv-enc-err-invalid-device': 'I am attempting to create as simple of an encoder program to get it working and build on later, but I can not seem to get the encoder to init correctly.The calls to NVIdia APIs are as follows:nvEncGetEncodePresetConfig fails with the error NV_ENC_ERR_INVALID_DEVICE, and I am not sure why that is.Here is the code that sets up the parameters that are passed to nvEncGetEncodePresetConfig:The methods print and nvEncCall are just helpers, nvEncCall is just a giant switch that prints the error and terminates.As far as I can tell I am doing this right, at least from what I am reading in the docs. NV_ENC_ERR_INVALID_DEVICE should never be returned by nvEncGetEncodePresetConfig according to them as well, which I find quite strange.The entirety of the code that I am using for this, a giant block because I am going for “just works” right now.The thing I do not understand in the docs is what a “floating CUDA context” even is, I tried to look that up and came up empty handed.You are using a profile where you should be using a preset GUID. Took me ages to solve the same issue.Hi,\\nI am having the same problem. But in my case\\nconst GUID guidPreset = NV_ENC_PRESET_DEFAULT_GUID;   // OK\\n// const GUID guidPreset = NV_ENC_PRESET_P3_GUID;    Error 4\\nI’m using api 11.0.10.\\nEven the sample is using NV_ENC_PRESET_P3_GUID . Did I miss some settings?Solved: I am using nvEncGetEncodePresetConfig for NV_ENC_PRESET_P3_GUID, which is incorrect. I should use nvEncGetEncodePresetConfigEx for PRESET_PxIn the SDK sample, calling nvEncGetEncodePresetConfig is first executed without checking errors, and then afterwards another call to nvEncGetEncodePresetConfigEx is executed. The first call is unnecessary and incorrect, and probably misleading. I suggest refactor that part or did I understood incorrectly? @rarzumanyanI left a comment at gitlab regarding the part. To the commit 84155d04af40dbc259099639be8b96577e851b32Powered by Discourse, best viewed with JavaScript enabled',\n",
       " 'attribute-offset-not-found-optix-6': 'I recently tried upgrading OptiX to version 6 (cannot upgrade to 7 yet as it will require a substantial re-write), and I’m getting a new error that I’ve never seen before. Specifically, I tried OptiX 6.0 and 6.5. The following error is thrown when running the launch kernel:OptiX Error: Unknown error (Details: Function “RTresult _rtContextLaunch3D(RTcontext, unsigned int, RTsize, RTsize, RTsize)” caught exception: Assertion failed: “offset != offsets.end() : Attribute offset not found”, file: , line: 1141)For OptiX 3, 4, and 5, I don’t get this error and my benchmarks validate with the correct answers. I was thinking perhaps it might have to do with setting the primitive offset for a geometric object. I had not been explicitly setting that before, but I tried making sure to explicitly set the primitive object to 0 for each geometry type, but that made no difference. Also, this tends to happen when the number of primitives/rays is ‘sufficiently’ large, as my very simple benchmark tests with only a few primitives passes without error.I’m wondering if you have any suggestions for how to start debugging this? Running with cuda-gdb does not provide any additional information.Please always provide the following system configuration information when asking about OptiX issues:\\nOS version, installed GPU(s), VRAM amount, display driver version, OptiX (major.minor.micro) version, CUDA toolkit version (major.minor) used to generate the input PTX, host compiler version.Using cuda-gdb implies a Linux OS.Also, this tends to happen when the number of primitives/rays is ‘sufficiently’ large, as my very simple benchmark tests with only a few primitives passes without error.If you are saying that the failure depends on the width, height, depth argument values of your 3D launch, then an important information in your question would have been the values where it still worked and the values where it started to fail.\\nWith the given information so far, it would be possible that you exceeded the maximum launch size limit in the newer versions.\\nIn that case please read this post: https://forums.developer.nvidia.com/t/3d-optixlaunch-to-accommodate-multiple-viewpoints/160421/2If you say that is related to the scene size in number of primitives, the same information about those sizes would be interesting.Some general information about what the application does to understand the dependency between number of primitves and number of rays you mentioned would also be helpful.An assertion inside OptiX normally indicates an issue inside OptiX itself. There might be seldom cases where this is an application error and OptiX isn’t reporting it correctly.\\nIn that case the first thing to try is upgrading the display drivers to a newer version if available because the OptiX core implementation lives inside the driver since OptiX 6.\\nIf that is not helping, we’d need a minimal, complete reproducer project in failing state.Normally correct OptiX host and device code from older versions should work, except when using Selector nodes which have been removed, and there are some cases where OptiX 6.5 got pickier about some device code constructs (e.g. pairing of rtPotentialIntersection and rtReportIntersection calls) but those get the correct error messages.Other than that, if you’re running the same old code from OptiX 3/4/5 with OptiX 6.5 (please do not use OptiX 6.0 anymore), that would mean not using the built-in triangle primitives, which will not use the hardware ray-triangle intersection on RTX boards, which is the crucial part for better performance.\\nhttps://raytracing-docs.nvidia.com/optix6/guide_6_5/index.html#host#geometrytrianglesI don’t think this “Attribute” error has something to do with intersection attributes. Also because your application is running in some cases, there is most likely not a systematic error with those, but note that OptiX 6 changed the intersection attribute handling due to the added built-in triangle primitives by adding attribute programs:\\nhttps://raytracing-docs.nvidia.com/optix6/guide_6_5/index.html#programs#semantics(In OptiX 7, intersection attributes are a set of at most 8 registers and surface attribute calculation needs to happen explicitly inside the hit programs.)Powered by Discourse, best viewed with JavaScript enabled',\n",
       " 'nvenc-hevc-with-full-range-colors': 'I’m currently evaluating the video encoder od SDK 7.0 on a GTX1060.\\nAs an input I use the testsrc generated by ffmpeg:I convert it to YUV444 before, of course.\\nFinally, I use such command to encode:I want to do a lossless encoding, so I need to check that the decoded video has same color than the input, which isn’t the case.\\nIMHO it’s because the encoder consider the YCbCr to be in TV range, where Y is in [0…255], but Cb and Cr are scaled to [16…235].\\nI found in thethere are parameters that could do the full range job:So in the Sample NvHWEncoder.cpp provided with the SDK, I added these lines at line 896.Unfortunately it doesn’t produce a full range color video as I expected. Any idea why?Seeing core NVENC SDK developers suddenly become active today…I am bumping this topic up for their attentionHi.Sorry for a slow response here. This thread was referenced in some other discussion, so I thought I should respond here.For doing lossless encoding, you should use the Lossless preset.NVENC does not care whether the input is full range or limited range. Full-range/limited-range is a RGB to YUV (or vice versa) color conversion issue. It is a job of client application to ensure that end-to-end color space conversion logic is maintained in a consistent manner. To do that, the client application needs to know what CSC matrix was used in converting from RGB to YUV, fill the VUI params correctly, and the decoder needs to honor the information given in the VUI params to decode and perform the color conversion correctly.If the specified input surface format is ABGR or ARGB, full range is assumed inside the NVENCODEAPI for doing the CSC.Thanks.“Unfortunately it doesn’t produce a full range color video as I expected”And how did you check for this? You need to also tag mkv or mp4 container with full range. In case of mp4 you should also write a mp4 nclc chunk with -movflags +write_colrYCbCr to be in TV range, where Y is in [0…255], but Cb and Cr are scaled to [16…235].TV range is [1 … 254] in all three components. It always was. Also Chroma is not scaled between 16 and 235, but between 16 and 240. Wow.Powered by Discourse, best viewed with JavaScript enabled',\n",
       " 'how-to-use-nvidia-cuda-extensions-with-opengl': 'Hi,\\nI am newbie to NvidiaCuda programming, I have created Cpu based openGL application it contains some GL_POYGON rendering functionalities in that in trying to implement CUDA. But In my existing application i included     #include <GL/freeglut.h> headerfile. for implementing CUDA also i included following header files  with in same application\\n//#include “NvAppBase/NvFramerateCounter.h”\\n//#include “NvAppBase/NvInputTransformer.h”\\n//#include “NvAssetLoader/NvAssetLoader.h”\\n//#include “NvGLUtils/NvGLSLProgram.h”\\n//#include “NvGLUtils/NvImage.h”\\n//#include “NvUI/NvTweakBar.h”\\n//#include “NV/NvLogs.h”After including that headers file. i getting following error that are:\\n1.IntelliSense: identifier “PFNGLVERTEXARRAYVERTEXOFFSETEXTPROC” is undefined\\n2.IntelliSense: identifier “PFNGLVERTEXARRAYSECONDARYCOLOROFFSETEXTPROC” is undefined\\n3.IntelliSense: identifier “PFNGLGETVERTEXARRAYPOINTERI_VEXTPROC” is undefinedlike that  i getting totally 91 errors.Please help me to resolve this issue. My aim i have to use both freeglut methods and also Nvidia cuda file methods.Thanks,\\nKirubhaPowered by Discourse, best viewed with JavaScript enabled',\n",
       " 'picking-problem-in-optix7-1': 'I’m hitting an intermittent picking bug in a scene with multiple geometry instances. In the enclosed screenshot, the Pick origin and Pick direction debug info come from inside the kernel and show that the same ray is being generated but occasionally a different object is reported from optixGetInstanceIndex() inside the closest hit program.Any ideas what might cause this behavior?Windows 10 Home\\nVisual Studio Community 2019 version 16.7.2\\nGeForce RTX 2070 SUPER\\nDriver version 452.06\\nOptiX 7.1\\nCUDA 11.0\\nwrong_pick1166×778 318 KB\\nIs anything changing between the picking operations?Maybe floating point inaccuracies?\\nDump the origin and direction as unsigned int bit-fields to see if they are really identical.Maybe highlight the picking pixel and the two instances to see their spatial relationship.\\nDump the intersection distance as well.\\nHitting a coplanar face or some geometry along the silhouette would be susceptible to LSB differences.Thanks for the suggestions. Turned out to be a stupid error on my end and it’s working fine now.BTW, I recently discovered shocker-0x15’s well designed and robust OptiX7_Utility projectOptiX 7 Lightweight Wrapper Library. Contribute to shocker-0x15/OptiX_Utility development by creating an account on GitHub.He’s done a great job of taking much of the pain out of developing OptiX7 apps, especially for those of us had no CUDA programming experience prior to OptiX7’s arrivalI made a simple WIP demo app using the his library here.Demo app using OptiX7_Utility. Contribute to Hurleyworks/Ketone development by creating an account on GitHub.Powered by Discourse, best viewed with JavaScript enabled',\n",
       " 'problem-when-compiling-rt-callable-program-with-reference-parameters': 'Hi, I’m porting my project from Optix3.0.1+CUDA4.0 to Optix3.5.1+CUDA5.5, however my program cannot compile properly now.I build the project using the Optix Wizard in VS2012. If my Callable Program has reference parameters, e.g.Then the compiler will issue a warning: Pointer parameters must be inlined, so overriding noinline attribute on evalBSDF.\\nWhen I run the program, Optix will complain that it cannot find that Callable Program(because it’s inlined).I didn’t have this problem with Optix3.0.1+CUDA4.0, can somebody tell me how to solve this? Thanks!So I will admit that I’m not 100% sure on how callable programs work because I don’t use them often. But, your error message gives some key information.Passing by reference requires a function to be inlined.Take a look at the include/optix_device.h file in the SDK. RT_CALLABLE_PROGRAM is defined as device noinline. This is why there is a problem.You can fix this by passing by value instead (even though passing by const ref is typically a good idea in most applications). Or, you can change your function to be inlined (replace RT_CALLABLE_PROGRAM with device inline) and making sure it’s included in the kernel that calls it. Note that you shouldn’t “create program” with the context on the host when doing this.What’s your OS, bitness, exact CUDA build version, driver version, and installed GPUs?\\nFor which SM architecture are you building the PTX?\\nOr better, what’s the exact nvcc command line when that happens?Powered by Discourse, best viewed with JavaScript enabled',\n",
       " 'cant-add-new-comments-on-bug-site': 'I get this AJAX error in both chrome and firefox for several days straight now.Hi @SeanB,Thanks for bringing this to our attention. I have the team looking into this now.Best,\\nTomHi @SeanB,Are you trying to send a simple comment or sending a huge amount of text? Can you supply a HAR file for troubleshooting? You can PM me the file if you wish.When troubleshooting complex\\xa0issues, it is sometimes necessary for our customer service team to obtain additional information about the network requests that are generated\\xa0in your browser while an ...Thanks,\\nTomIs this working on your side? It’s still down for me.Powered by Discourse, best viewed with JavaScript enabled',\n",
       " 'differences-to-5-2': 'I’ve got a built 5.2 version that I’m using but I’m wondering what the RTX branch really does. Is it just the plugins which I can presumably recompile for 5.2? I’m using it for VR and omniverse so I really need the high end stuff with Lumen and hardware RT. Would I be better off switching to the RTX branch?Hi there @androidsilvereye, nice to read you again.The RTX branch is more than just the plugins. It more or less reflects the latest in ray-tracing related development happening at NVIDIA. When we work on new features or SDKs for developers, they will very quickly also be added to our branch of UE to share the “goodness”.Another difference is that we do not necessarily discontinue already integrated features, unless they break beyond repair. RTXGI is an example of this. Lumen is a slightly different approach to solve the same challenge of GI, so our dedicated RTXGI plugin in UE was not updated for the latest UE version. But it does live on inside the RTX Branch.On the other hand our engineers maintaining the branch are not always on-par with the latest HEAD of the EPIC main-line branch of UE.It really is a question of prioritization for you as the developer, what you need more. The cutting edge EPIC version of UE or the latest and greatest in NVIDIA RT features.I hope that explained it a bit.Powered by Discourse, best viewed with JavaScript enabled',\n",
       " 'nvenc-on-rtx-2080': 'I would like to ask the encoder engine of the RTX 2080. I did the comparison test. In the same parameters, the RTX 2080 Encoder occupancy rate is twice that of 1070Ti. Is it because the RTX 2080 has only one encoder eigine?It looks like this is true, yes. The new consumer RTX cards would appear to only have one NVENC.Yes, RTX 2070, 2080, 2080 TI has only one NVENC engine, there is no support matrix for RTX models, so nobody knows if Quadro RTX 5000/6000 will have 1,2 or 3 engines…Powered by Discourse, best viewed with JavaScript enabled',\n",
       " 'i-hava-something-wiht-cmakelists-on-liunx': 'my CMakelists :cmake_minimum_required(VERSION 3.10 FATAL_ERROR)project(baking_ao)set(OPTIX_DIR …/NVIDIA-OptiX-SDK-6.5.0-linux64/include)set(CUDA_DIR …/…/…/usr/local/cuda-10.2/include)include_directories(${CUDA_DIR})include_directories(${OPTIX_DIR})aux_source_directory(./ SRC_MAIN)#link_directories(…/NVIDIA-OptiX-SDK-6.5.0-linux64/lib64)#set(optix optixu optix_prime)find_package(CUDA QUIET REQUIRED)add_executable(baking_ao ${SRC_MAIN})but when i compile the projecti get a lot of error like this[build] /home/s/baking_ao/…/NVIDIA-OptiX-SDK-6.5.0-linux64/include/optix_prime/optix_primepp.h: In member function ‘void optix::prime::QueryObj::setCudaStream(optix::cudaStream_t)’:\\n[build] /home/s/baking_ao/…/NVIDIA-OptiX-SDK-6.5.0-linux64/include/optix_prime/optix_primepp.h:682:48: error: cannot convert ‘optix::cudaStream_t {aka optix::CUstream_st*}’ to ‘cudaStream_t {aka CUstream_st*}’ for argument ‘2’ to ‘RTPresult rtpQuerySetCudaStream(RTPquery, cudaStream_t)’\\n[build]        CHK(rtpQuerySetCudaStream(m_query, stream));\\n[build]                                                 ^** this is content in optix6.5 file ,how can i resovle this problem . **and my device:\\nlinux : ubuntu 18.04\\ngpu:rtx 2060\\noptix : 6.5\\ncuda version : 10.02\\ncmake :3.10.2\\ndriver version: 440.82The error message describes that there is a namespace mismatch between optix::cudaStream_t and cudaStream_t.But cudaStream_t is defined as typedef struct CUstream_st *cudaStream_t; in optix_prime.h outside of namespaces and it’s also included outside of namespace optix inside the OptiX headers.\\nI’m not sure why the member function inline void QueryObj::setCudaStream( cudaStream_t stream ) is putting an optix:: namespace in front of the  cudaStream_t argument type then. That sounds incorrect by the compiler.Possible fixes:Other than that, what exactly are you’re trying to build there? Is that the (deprecated) OptiX Prime ambient occlusion baking example at https://github.com/nvpro-samples/optix_prime_baking or something based on that?Please note that OptiX Prime is not using the RT cores on RTX boards, means OptiX Prime based applications are not going to run at the possible maximum speed on your RTX 2060.\\nOptiX Prime is discontinued with OptiX 7 already.  I would recommend to use the OptiX 7 API for these kinds of algorithms today.yes,I have run successfully on Window,but when i use the same code in linux, i have lots of bug, except this problem,also like float3 error,i think is maybe system problem,I first tried to solve this problem.I have tried using OptiX 7 API，but is too difficult to me.thinks,and my english is not well,please understand.bucause all error are when i use #include -something lib-,so i think my CMakeLIst maybe hava some mistake.I fear I cannot help with CMake issues under Linux if the same is working under Windows. I’m not a Linux user.Maybe try using the same CMake version first.I’m using a different order of find_package and adding include directories from the found SDKs.\\nCheck that the hard-coded paths you’re using are actually matching what CMake found.Do the OptiX DK 6.5.0 “prime” examples compile on your Linux system?\\nIf yes, that would indicate the issue is not about your installed SDKs.Then it would be a matter of finding out what the different compiler settings are between the OptiX SDK “prime” examples and your project.I’ll try.thinks.hava a good day.I have another question，when i use prime on OptiX SDK 6.5,and my CUDA version is 10.2.\\nAre there some compatibility issues that can be caused by the OptiX version being too different from the CUDA version.I tried running primeInstancing in OptiX6.5 prime under window, but it failed.I have another question，when i use prime on OptiX SDK 6.5,and my CUDA version is 10.2.\\nAre there some compatibility issues that can be caused by the OptiX version being too different from the CUDA version.OptiX Prime is not running any of your CUDA code. The ray queries are hard-coded inside OptiX Prime and not programmable. All it needs is a display driver supporting the CUDA version used inside that OptiX version which is specified inside the respective OptiX version’s release notes.Again, I wouldn’t recommend to use OptiX Prime on RTX boards at all. It should be faster to use OptiX (and esp. OptiX 7) and implementing such ray queries on top of that.\\nThat will use the RT cores and provide a lot more flexibility for the scene graph, geometric primitives, hit result format, and some more possibilities.I tried running primeInstancing in OptiX6.5 prime under window, but it failed.Define “it failed”.\\nThe program is not interactive and only renders a *.ppm image to disc and exits.sorry，is my problem i modify  source code，i‘ll try OptiX 7.thinks.Have a look at the optixRaycasting example inside the OptiX SDK 7.0.0 which demonstrates that.\\n(That example exists since OptiX SDK 5.)think you.Powered by Discourse, best viewed with JavaScript enabled',\n",
       " 'bsod-when-closing-apps-all-apps-which-uses-nvidia-graphics-card': '\\nPXL_20220921_0038301551920×2560 166 KB\\nHello, I have a laptop msi gl65 9sfk, with a RTX 2070 gpu in it. My laptop always crashes when closing games, or benchmark applications. Without overclocking, temperatures range from 78°C to 85°C. This occurrence seems to be on Windows only, I installed gnu/linux on my machine and installed the nvidia graphics card driver, then tried to play the game. It didn’t cause any problems. I’ve tried every method on the internet couldn’t fix it. what do I have to do?Powered by Discourse, best viewed with JavaScript enabled',\n",
       " 'any-working-examples-of-optixprime-using-rtp-buffer-type-cuda-linear': 'Having seen recommendations that performance will be better when using RTP_CONTEXT_TYPE_CUDA and RTP_BUFFER_TYPE_CUDA_LINEAR, I decided to try it out.   But I only get crashes, and the SDK/prime*/ examples were of no help because they are all CPU/HOST based.  See complete code below that involves just 7 triangles and the crash that results.  Any ideas what I am doing wrong?  Thanks.#include <cuda_runtime.h>\\n#include <optix_prime/optix_primepp.h>int main(int argc, const char *argv)\\n{\\nint3 tri[7] = {make_int3(0, 1, 2),\\nmake_int3(0, 2, 3),\\nmake_int3(4, 0, 3),\\nmake_int3(0, 4, 5),\\nmake_int3(0, 5, 1),\\nmake_int3(3, 2, 6),\\nmake_int3(1, 5, 6),\\n};float3 vert[7] = {make_float3(0, 0.127, 0),\\nmake_float3(0.127, 0.127, 0),\\nmake_float3(0.127, 0, 0),\\nmake_float3(0, 0, 0),\\nmake_float3(0, 0.127, 0.127),\\nmake_float3(0.127, 0.127, 0.127),\\nmake_float3(0.127, 0, 0.127),\\n};RTPcontexttype contextType = RTP_CONTEXT_TYPE_CUDA;\\noptix::prime::Context context = optix::prime::Context::create(contextType);\\noptix::prime::Model model = context->createModel();int numTri = 7;\\nint numVert = 7;void *cudaTri = NULL;\\ncudaMalloc((void **) &cudaTri, sizeof(int3) * numTri);\\ncudaMemcpy(cudaTri, &tri[0], sizeof(int3) * numTri, cudaMemcpyHostToDevice);void *cudaVert = NULL;\\ncudaMalloc((void **) &cudaVert, sizeof(float3) * numVert);\\ncudaMemcpy(cudaVert, &vert[0], sizeof(float3) * numVert, cudaMemcpyHostToDevice);model->setTriangles(numTri, RTP_BUFFER_TYPE_CUDA_LINEAR, cudaTri,\\nnumVert, RTP_BUFFER_TYPE_CUDA_LINEAR, cudaVert);\\nmodel->update(0);\\nreturn 0;\\n}libc++abi.dylib: terminating with uncaught exception of type optix::prime::Exception: Function “RTPresult _rtpModelUpdate(RTPmodel, unsigned int)” caught exception: Encountered a CUDA error: cudaEventSynchronize( m_eventEnd ) returned (4): unspecified launch failure\\nAbortCould you please provide more information on your system configuration?\\nOS version, installed GPU(s), display driver version, OptiX version, CUDA toolkit version (in case you used that).“But I only get crashes, and the SDK/prime*/ examples were of no help because they are all CPU/HOST based.”\\nOnly by default, but there are command line options inside the primeSimplePP.cpp main() function which allow to change its operation.I believe the crash is because of using RTP_BUFFER_TYPE_CUDA_LINEAR in the setTriangles() call.  I looked closer at primeSimplePP.cpp, and it hard codes its setTriangles() call to always use RTP_BUFFER_TYPE_HOST.  Therefore, I will do the same in my code.  Thanks.Powered by Discourse, best viewed with JavaScript enabled',\n",
       " 'nvidia-driver-crashes-when-compiling-nested-subroutines': 'Hi,I have a question regarding GLSL and the composition of subroutines. What I mean by the composition is something like:Current drivers simply crash given this piece of code when linking program for the fragment stage. The specification doesn’t mention this usage and this use-case seems to be logical for me. Can anybody give me an advice, whether it is bug in the driver, or whether I use subroutines in a bad way?I am developing on a Linux platform, the driver version is 331.20Powered by Discourse, best viewed with JavaScript enabled',\n",
       " 'performance-transform-or-not': 'I noticed in the programming guide that for best performance, the graph should be kept as shallow as possible with only one GeometryGroup.  I have many repeated structures in my scene, and I’d like the define the primitive once then use transform nodes to translate/rotate it instead of making individual primitives for each repeat.  Since a transform node only can have GeometryGroup or Group as its children, I have as many GeometryGroups as transforms, each with its own (but identical) primitive.  Will I take a big performance hit by doing this?It’s all a balancing act, so we can’t comment on exactly which would be better in your particular application.  If you can avoid the Transform nodes, then that generally performs better.  However if replication of the data increases your memory load or creates other issues in your application (namely with the additional copies or the cost to flatten the data) then the cost of using a Transform node could be worth it.  In practice we’ve found Transform nodes to be useful for instancing.The object of the advice in the programming guide wasn’t to suggest never using Transforms, but it was to inform developers of the potential dangers of create extremely deep graphs and to consider where things such as Transform nodes are actually needed.Powered by Discourse, best viewed with JavaScript enabled',\n",
       " 'gstreamer-nvenc-encoding-from-decklinkvideosrc': 'Hello- On my jetson nano I can use the following command to capture video from a camera and encode to an mp4 file:gst-launch-1.0 nvv4l2camerasrc num-buffers=180 ! ‘video/x-raw(memory:NVMM),height=1920,width=1080,framerate=(fraction)60/1’ ! nvvidconv ! nvv4l2h264enc ! filesink location=rp1.mp4I would like to do something similar but on an x86_64 desktop with a decklink video card. I’ve tried:gst-launch-1.0 -v decklinkvideosrc device-number=1 ! ‘video/x-raw, width=3840, height=2160, pixel-aspect-ratio=1/1, interlace-mode=progressive, framerate=60/1, format=UYVY, colorimetry=bt2020, chroma-site=mpeg2’ !\\n! nvvidconv ! nvv4l2h264enc ! filesink location=rp1.mp4But I get this:WARNING: erroneous pipeline: could not link decklinkvideosrc0 to nvvideoconvert0, neither element can handle caps video/x-raw, width=(int)3840, height=(int)2160, pixel-aspect-ratio=(fraction)1/1, interlace-mode=(string)progressive, framerate=(fraction)60/1, format=(string)UYVY, colorimetry=(string)bt2020, chroma-site=(string)mpeg2Also, Ideally I’d like the frames to go directly into NVMM (presumably through GPUDirect)-- but I’m not sure decklinkvideosrc would support that… is that possible?Thoughts Appreciated…Thanks, RogerI have the  same problem.\\nHow did you solve it?Powered by Discourse, best viewed with JavaScript enabled',\n",
       " 'register-new-file-type-association-for-snapdragon-alexnet-android-studio-example': 'I’m trying to set up the Neural Processing Engine using the example found in https://developer.qualcomm.com/software/snapdragon-neural-processing-engine/getting-started. I’ve reached the end of the tutorial (Build the Example Android APP) and, at a particular point in the Gradle build, I am requested to provide a file type association for the raw_alexnet.zip.flat file. Does anyone know what association I should provide?The Gradle console output is:Executing tasks: [:app:assembleDebug]Configuration on demand is an incubating feature.\\nConfiguration ‘compile’ in project ‘:app’ is deprecated. Use ‘implementation’ instead.\\nConfiguration ‘testCompile’ in project ‘:app’ is deprecated. Use ‘testImplementation’ instead.\\n:app:preBuild UP-TO-DATE\\n:app:preDebugBuild UP-TO-DATE\\n:app:compileDebugAidl UP-TO-DATE\\n:app:compileDebugRenderscript UP-TO-DATE\\n:app:checkDebugManifest UP-TO-DATE\\n:app:generateDebugBuildConfig UP-TO-DATE\\n:app:prepareLintJar UP-TO-DATE\\n:app:generateDebugResValues UP-TO-DATE\\n:app:generateDebugResources UP-TO-DATE\\n:app:mergeDebugResources\\n:app:createDebugCompatibleScreenManifests UP-TO-DATE\\n:app:processDebugManifest UP-TO-DATE\\n:app:splitsDiscoveryTaskDebug UP-TO-DATE\\n:app:processDebugResources\\n/home/dave/snpe-sdk/examples/android/image-classifiers/app/build/intermediates/res/merged/debug/raw_alexnet.zip.flat: error: failed to read data meta data.\\nerror: failed parsing overlays.Failed to execute aapt\\ncom.android.ide.common.process.ProcessException: Failed to execute aapt\\nat com.android.builder.core.AndroidBuilder.processResources(AndroidBuilder.java:796)\\nat com.android.build.gradle.tasks.ProcessAndroidResources.invokeAaptForSplit(ProcessAndroidResources.java:551)\\nat com.android.build.gradle.tasks.ProcessAndroidResources.doFullTaskAction(ProcessAndroidResources.java:285)\\nat com.android.build.gradle.internal.tasks.IncrementalTask.taskAction(IncrementalTask.java:109)\\nat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\\nat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\\nat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\\nat java.lang.reflect.Method.invoke(Method.java:498)\\nat org.gradle.internal.reflect.JavaMethod.invoke(JavaMethod.java:73)\\nat org.gradle.api.internal.project.taskfactory.DefaultTaskClassInfoStore$IncrementalTaskAction.doExecute(DefaultTaskClassInfoStore.java:173)\\nat org.gradle.api.internal.project.taskfactory.DefaultTaskClassInfoStore$StandardTaskAction.execute(DefaultTaskClassInfoStore.java:134)\\nat org.gradle.api.internal.project.taskfactory.DefaultTaskClassInfoStore$StandardTaskAction.execute(DefaultTaskClassInfoStore.java:121)\\nat org.gradle.api.internal.tasks.execution.ExecuteActionsTaskExecuter$1.run(ExecuteActionsTaskExecuter.java:122)\\nat org.gradle.internal.progress.DefaultBuildOperationExecutor$RunnableBuildOperationWorker.execute(DefaultBuildOperationExecutor.java:336)\\nat org.gradle.internal.progress.DefaultBuildOperationExecutor$RunnableBuildOperationWorker.execute(DefaultBuildOperationExecutor.java:328)\\nat org.gradle.internal.progress.DefaultBuildOperationExecutor.execute(DefaultBuildOperationExecutor.java:197)\\nat org.gradle.internal.progress.DefaultBuildOperationExecutor.run(DefaultBuildOperationExecutor.java:107)\\nat org.gradle.api.internal.tasks.execution.ExecuteActionsTaskExecuter.executeAction(ExecuteActionsTaskExecuter.java:111)\\nat org.gradle.api.internal.tasks.execution.ExecuteActionsTaskExecuter.executeActions(ExecuteActionsTaskExecuter.java:92)\\nat org.gradle.api.internal.tasks.execution.ExecuteActionsTaskExecuter.execute(ExecuteActionsTaskExecuter.java:70)\\nat org.gradle.api.internal.tasks.execution.SkipUpToDateTaskExecuter.execute(SkipUpToDateTaskExecuter.java:63)\\nat org.gradle.api.internal.tasks.execution.ResolveTaskOutputCachingStateExecuter.execute(ResolveTaskOutputCachingStateExecuter.java:54)\\nat org.gradle.api.internal.tasks.execution.ValidatingTaskExecuter.execute(ValidatingTaskExecuter.java:58)\\nat org.gradle.api.internal.tasks.execution.SkipEmptySourceFilesTaskExecuter.execute(SkipEmptySourceFilesTaskExecuter.java:88)\\nat org.gradle.api.internal.tasks.execution.ResolveTaskArtifactStateTaskExecuter.execute(ResolveTaskArtifactStateTaskExecuter.java:52)\\nat org.gradle.api.internal.tasks.execution.SkipTaskWithNoActionsExecuter.execute(SkipTaskWithNoActionsExecuter.java:52)\\nat org.gradle.api.internal.tasks.execution.SkipOnlyIfTaskExecuter.execute(SkipOnlyIfTaskExecuter.java:54)\\nat org.gradle.api.internal.tasks.execution.ExecuteAtMostOnceTaskExecuter.execute(ExecuteAtMostOnceTaskExecuter.java:43)\\nat org.gradle.api.internal.tasks.execution.CatchExceptionTaskExecuter.execute(CatchExceptionTaskExecuter.java:34)\\nat org.gradle.execution.taskgraph.DefaultTaskGraphExecuter$EventFiringTaskWorker$1.run(DefaultTaskGraphExecuter.java:248)\\nat org.gradle.internal.progress.DefaultBuildOperationExecutor$RunnableBuildOperationWorker.execute(DefaultBuildOperationExecutor.java:336)\\nat org.gradle.internal.progress.DefaultBuildOperationExecutor$RunnableBuildOperationWorker.execute(DefaultBuildOperationExecutor.java:328)\\nat org.gradle.internal.progress.DefaultBuildOperationExecutor.execute(DefaultBuildOperationExecutor.java:197)\\nat org.gradle.internal.progress.DefaultBuildOperationExecutor.run(DefaultBuildOperationExecutor.java:107)\\nat org.gradle.execution.taskgraph.DefaultTaskGraphExecuter$EventFiringTaskWorker.execute(DefaultTaskGraphExecuter.java:241)\\nat org.gradle.execution.taskgraph.DefaultTaskGraphExecuter$EventFiringTaskWorker.execute(DefaultTaskGraphExecuter.java:230)\\nat org.gradle.execution.taskgraph.DefaultTaskPlanExecutor$TaskExecutorWorker.processTask(DefaultTaskPlanExecutor.java:124)\\nat org.gradle.execution.taskgraph.DefaultTaskPlanExecutor$TaskExecutorWorker.access$200(DefaultTaskPlanExecutor.java:80)\\nat org.gradle.execution.taskgraph.DefaultTaskPlanExecutor$TaskExecutorWorker$1.execute(DefaultTaskPlanExecutor.java:105)\\nat org.gradle.execution.taskgraph.DefaultTaskPlanExecutor$TaskExecutorWorker$1.execute(DefaultTaskPlanExecutor.java:99)\\nat org.gradle.execution.taskgraph.DefaultTaskExecutionPlan.execute(DefaultTaskExecutionPlan.java:625)\\nat org.gradle.execution.taskgraph.DefaultTaskExecutionPlan.executeWithTask(DefaultTaskExecutionPlan.java:580)\\nat org.gradle.execution.taskgraph.DefaultTaskPlanExecutor$TaskExecutorWorker.run(DefaultTaskPlanExecutor.java:99)\\nat org.gradle.internal.concurrent.ExecutorPolicy$CatchAndRecordFailures.onExecute(ExecutorPolicy.java:63)\\nat org.gradle.internal.concurrent.ManagedExecutorImpl$1.run(ManagedExecutorImpl.java:46)\\nat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\\nat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\\nat org.gradle.internal.concurrent.ThreadFactoryImpl$ManagedThreadRunnable.run(ThreadFactoryImpl.java:55)\\nat java.lang.Thread.run(Thread.java:745)\\nCaused by: java.util.concurrent.ExecutionException: java.util.concurrent.ExecutionException: com.android.tools.aapt2.Aapt2Exception: AAPT2 error: check logs for details\\nat com.google.common.util.concurrent.AbstractFuture.getDoneValue(AbstractFuture.java:503)\\nat com.google.common.util.concurrent.AbstractFuture.get(AbstractFuture.java:482)\\nat com.google.common.util.concurrent.AbstractFuture$TrustedFuture.get(AbstractFuture.java:79)\\nat com.android.builder.core.AndroidBuilder.processResources(AndroidBuilder.java:794)\\n… 48 more\\nCaused by: java.util.concurrent.ExecutionException: com.android.tools.aapt2.Aapt2Exception: AAPT2 error: check logs for details\\nat com.google.common.util.concurrent.AbstractFuture.getDoneValue(AbstractFuture.java:503)\\nat com.google.common.util.concurrent.AbstractFuture.get(AbstractFuture.java:462)\\nat com.google.common.util.concurrent.AbstractFuture$TrustedFuture.get(AbstractFuture.java:79)\\nat com.android.builder.internal.aapt.v2.QueueableAapt2.lambda$makeValidatedPackage$1(QueueableAapt2.java:179)\\nat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\\nat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\\n… 1 more\\nCaused by: com.android.tools.aapt2.Aapt2Exception: AAPT2 error: check logs for details\\nat com.android.builder.png.AaptProcess$NotifierProcessOutput.handleOutput(AaptProcess.java:463)\\nat com.android.builder.png.AaptProcess$NotifierProcessOutput.err(AaptProcess.java:415)\\nat com.android.builder.png.AaptProcess$ProcessOutputFacade.err(AaptProcess.java:332)\\nat com.android.utils.GrabProcessOutput$1.run(GrabProcessOutput.java:104)FAILEDFAILURE: Build failed with an exception.Failed to execute aaptTry:\\nRun with --stacktrace option to get the stack trace. Run with --info or --debug option to get more log output.Get more help at https://help.gradle.orgBUILD FAILED in 5s12 actionable tasks: 2 executed, 10 up-to-datePowered by Discourse, best viewed with JavaScript enabled',\n",
       " 'fragmentation-with-physx-in-3ds-max-version-2-86-00811-20500': 'Hey Folks,i guess that probably a lot of you know the Rayfire Tool for 3ds max and what it can do in case of fragmentation. As this Rayfire Tool largely depends on Nvidia’s PhysX Plugin, i just want to ask one question:How is it possible to do fragmentation with Nvidia’s PhysX Plugin for 3ds max without the need to rely on 3rd Party Software?I find no tutorials - beside the one for the PhysX Lab Tool which i will give a shot in a sec. I however need those tools inside my 3d Application. Im no Game Developer - im a CG Artist and want to free me from the tons of expensive tools out there that just depend on what you may probably already get with the PhysX Tools from Nvidia.Would be great if someone has an answer for me.Thanks in advance\\nSteffenis it possible to do fragmentation with Nvidia’s PhysX Plugin for 3ds max without the need to rely on 3rd Party Software?It was promised that at some point PhysXLab functionality will be added to PhysX plug-ins.\\nMeanwhile, you can use some free tools like Voronoi Fracture script.Hi Reticula,i thought that this is true at the moment :-( Still had no chance to have a look at the PhysXLab, but i know Fracture Voronoi - i used this in the past quite often, but it does not allow to fracture at impact (you have to pre-fracture) and it also does not allow to do secondary frags - you would have to create those from existing frags manually. So in the meantime i think i will stick to Rayfire again until Nvidia integrated some parts into the PhysX DCC Plugin.Thanks for the answer anyway and Happy New Year! :-)CheersPlease! Where do I get physx plugin for max 2010?Thanks in advance!!  :)Powered by Discourse, best viewed with JavaScript enabled',\n",
       " 'i-am-unable-to-gain-access-to-nvidia-game-developer-portal': 'I tried signing in to the NVIDIA Game Developer portal through this link\\nBut it shows that \" Your account, myemail@gmail.com , does not have access to NVIDIA Game Developer Portal.\"\\nHow can I sign up then, I need to setup & register my game for Nvidia Geforce Now and other stuff\\n\\nimage869×729 123 KB\\nWe understand the team managing that portal has contacted you.\\nI am marking this as solved - if that is not the case please let us know. ThanksHi,\\nYes the team has contacted me, they are helping me out.\\nThanks.This topic was automatically closed 14 days after the last reply. New replies are no longer allowed.Powered by Discourse, best viewed with JavaScript enabled',\n",
       " 'cuvidparsevideodata-when-do-i-know-provided-data-is-corrupt': 'Sometimes data that I pass to cuvidParseVideoData are corrupt. Because, for instance, over the network a frame or two got lost and the decoder is supposed to decode frame n data without having first decompressed frames n-1, n-2, … How do I detect if the provided data is incorrect/corrupt?\\nFFmpeg on such occasions returns an error upon call to avcodec_receive_framePowered by Discourse, best viewed with JavaScript enabled',\n",
       " 'nvidia-smi-report-wrong-gpu-related-frequency-and-power-info': 'I’m using  GeForce RTX 3070 Ti Laptop GPU on windows11 with integrated graphics card. Recently, when i try to get system info using command nvidia-smi --query-gpu=index,timestamp,power.draw,clocks.sm,clocks.mem,clocks.gr --format=csv -l 1, the reports are always: [No data], 210 MHz, 7000 MHz, 210 MHz. At the same time my gpu core freq sticks on 1.4GHz, memory freq 7GHz. (showd in GeForce Experience)However, when I open GeForce Experience, the power info turns to be corrected, while the frequency infos are still wrong. And after a short while, my gpu core freq drops to 210MHz, and memory freq to 405MHz.During all test above, my gpu load is 0 with 160MB memory reserved for hardware.I had no idea what was going on. It’s my fault? And how can I fix it?Hello @zqaptx, welcome to the NVIDIA developer forums.What is the output of a simple nvidia-smi? The fact that you don’t see the GPU index (should be 0) and no timestamp would indicate that the driver might not be installed correctly.What CPU do you use and what OS?If you are using Linux, please make sure to follow this post:Thanks!Oh, Sorry. The total output is:\\n0, 2023/05/08 18:56:17.715, [No data], 210 MHz, 7000 MHz, 210 MHzI have reinstalled my driver several times (using GeForce Experience, or official installation files).My OS is Windows 11 Home 22621.1635.\\nCPU is 12th Gen Intel(R) Core™ i9-12900H.The nvidia-smi output looks quite normal to me, it might just be that something on the system side interferes with the GFE process reading HW information. It does not work exactly like nvidia-smi does.You could double-check with another tool like HWInfo or Afterburner to see if the values are consistent. If the GPU is otherwise working correctly I would not worry about slightly different values in GFE.Thanks for your advice. I used to inspect the GPU infomation using LibreHardwareMonitor. It shows that my GPU core frequency keeps to be 1.4GHz, and memory frequency keeps to be 7GHz all the time. And only a short while after I open GFE, the frequence drop to 210MHz and 405MHz. If GFE is closed, frequence will increase back to 1.4G and 7G again.I think my gpu is working uncorrectly, but i’m not sure.Actually, GFE works correctly on my PC, but nvidia-smi a little strange. All the frequency information reported by nvidia-smi is always constant.Hi again,good news first, I think your GPU is fine, I see the same effect on one of my systems and the GPU is completely ok.\\nIt might actually be a real issue with nvidia-smi and I will contact engineering to check if this is the case.Thanks for bringing this up!Thanks again.I’m new to deep learning with nvidia gpu, and curious about what happened on my gpu. My gpu works excellently before I train a deep learning model and open my computer the next day( I haven’t checked frequency using nvidia-smi before). And I don’t know what happened.I’m glad to help, and I’m looking forward to knowing the reason.I received information from engineering and this effect you see with nvidia-smi is a known issue and should be addressed by an upcoming driver update.Sadly though I cannot say which driver version will contain this change.Thanks!Powered by Discourse, best viewed with JavaScript enabled',\n",
       " 'glfw-vsync-swapbuffer-related-nvoglv64-crash': 'Developing for GLFW and I switched from a monitor to a TV, the TV has a very noticeable display latency. The crash has never happened when using a monitor. My app crashes from nvoglv64.dll access violation reading location -1.\\nGTX1070 driver version 25.21.14.1722\\nMSVC2015It happens with glfwSwapBuffers enabled inconsistently. It happens approximately on the 17th frame, if it doesnt occur soon then it doesn’t occur at all, it happens about 1/3 of every launch.\\nVsync glfwSwapInterval(1) causes the crash to happen more often than glfwSwapInterval(0), glfwSwapInterval(2) makes the crash very rare.I have commented out all my rendering code in loop other than swapbuffer and it still happens.\\nIt does not happen when I don’t call my timer. The timer uses std::sleep_for and attempts to maintain a framerate of 30fps, matching that of the display. The timer is invoked after swapbuffer.nvoglv64.dll!00000000756ba7af()\\tUnknown\\nnvoglv64.dll!00000000757138f4()\\tUnknown\\nnvoglv64.dll!0000000075715881()\\tUnknown\\nnvoglv64.dll!0000000075718d8c()\\tUnknown\\nnvoglv64.dll!00000000757aa21e()\\tUnknown\\nnvoglv64.dll!00000000757a7786()\\tUnknown\\nnvoglv64.dll!00000000757aa564()\\tUnknown\\nnvoglv64.dll!00000000757a9837()\\tUnknown\\nnvoglv64.dll!00000000757aaa6b()\\tUnknown\\nnvoglv64.dll!00000000757c1ea0()\\tUnknown\\nnvoglv64.dll!00000000756575ce()\\tUnknown\\nnvoglv64.dll!00000000757838e2()\\tUnknown\\nnvoglv64.dll!0000000075743df2()\\tUnknown\\nnvoglv64.dll!00000000757634f7()\\tUnknown\\nnvoglv64.dll!000000007576309c()\\tUnknown\\nnvoglv64.dll!0000000075668816()\\tUnknown\\nnvoglv64.dll!0000000075668985()\\tUnknown\\nnvoglv64.dll!000000007566876e()\\tUnknown\\nnvoglv64.dll!0000000075668266()\\tUnknown\\nnvoglv64.dll!000000007565159c()\\tUnknown\\nnvoglv64.dll!000000007566aa0a()\\tUnknown\\nnvoglv64.dll!00000000753e8dc1()\\tUnknown\\nnvoglv64.dll!00000000753e8c20()\\tUnknown\\nnvoglv64.dll!000000007565c8db()\\tUnknown\\nkernel32.dll!00007ffa4e9b3034()\\tUnknown\\nntdll.dll!00007ffa4f993691()\\tUnknownPowered by Discourse, best viewed with JavaScript enabled',\n",
       " 'nvencoder-error': 'Hi,\\nWhen I encode a yuv file which resolution is 1088x16, the founction\"nvEncInitializeEncoder(m_hEncoder, &m_stCreateEncodeParams);\"return NV_ENC_ERR_INVALID_PARAM. But if I encode a yuv file which resolution is 1088x32, it is success, so does the NvEncode has a minimum resolution which is supported,and if the resolution is smaller than the minimum resolution, the encoder will be wrong?Thank you for your reply!Hi luckyguo,NVENC engine cannot support encoding below a certain resolution. If your frame resolution is less than that the initialization will fail. The minimum supported height is 32, and the failure you are seeing if the minimum height is 16 is expected.Thanks,\\nRyan ParkPowered by Discourse, best viewed with JavaScript enabled',\n",
       " 'how-to-evaluate-the-as-build-time-in-optix': 'Hi\\nI want to know the build time of the AS I used.\\nIs there any built-in method that I can use?ThanksThe only way I know of is timinig the tracing 0 (=“zero”) rays.First call rtContextCompile(0,0), then time the duration of the call to rtContextLaunch1D(context,0,0).Powered by Discourse, best viewed with JavaScript enabled',\n",
       " 'gt-640-no-v-sync': 'No v-sync while playing videos on VLC - GT 640Powered by Discourse, best viewed with JavaScript enabled',\n",
       " 'optix-aabb-for-optix-7': 'Good morning,I am interested in older code I found online at: Optix-PathTracer/quad_intersect.cu at master · knightcrawler25/Optix-PathTracer · GitHubIn particular the RT_Program void bounds(int, float result[6]) function line 87 (the optix::Aabb call. Is there a corresponding call like this in Optix 7 ?Thanks,Nope, there are no bounding box programs inside OptiX 7 anymore.There are only built-in geometric primitives like triangles and curves (OptiX 7.1.0) which build their own axis aligned bounding boxes (AABB) and custom primitives for which you need to provide the pre-calculated AABBs.Means you would normally calculate these AABBs either on the host or much faster (recommended!) with a native CUDA kernel doing the same calculation as that former bounding box program and give the CUdeviceptr with the results to the OptixBuildInputCustomPrimitiveArray aabbBuffers. (Mind the plural because motion blur requires a set of AABBs per motion key.)Explained here, see Listing 5.3:\\nhttps://raytracing-docs.nvidia.com/optix7/guide/index.html#acceleration_structures#acceleration-structures\\nhttps://raytracing-docs.nvidia.com/optix7/api/html/struct_optix_build_input_custom_primitive_array.htmlPlease search the OptiX SDK 7.2.0 source code for “customPrimitiveArray” and you’ll find various examples showing that for very simple cases, like in optixWhitted where that happens on the host inside the sphere_bound() and parallelogram_bound() functions.Thanks @droettger. I will give a look at the OptiX SDK 7.2.0 for “customPrimitiveArray” for some simple examples.It’s also always a good idea to search this OptiX developer forum for the topic you’re interested in.\\nThere are multiple threads dealing with custom primitives already.You also need to have an intersection program for these custom primitives and the reporting looks a little different in the OptiX 7 API.\\nFor example you need to define how many of the maximum 8 attribute registers your pipeline is using if it’s more than the default 2 for the triangle barycentrics.\\nhttps://raytracing-docs.nvidia.com/optix7/guide/index.html#device_side_functions#reporting-intersections-and-attribute-accessYou should also tell the pipeline which geometric primitive types are actually used inside the code. This is mandatory for curves. See listing 8.1 here.\\nhttps://raytracing-docs.nvidia.com/optix7/guide/index.html#curves#differences-between-curves-and-trianglesGiven that the registers defined by optixSetPayload_0 … _7 and optixSetAttribute_0 … _7 do not overlap, is it possible to pass up to 16 32b values from one OptiX program to another?Whoa, wait a minute, these are completely different things.Attribute registers are only written inside intersection programs by optixReportIntersection().\\nThere is no optixSetAttribute call, only optixGetAttribute calls.\\nhttps://raytracing-docs.nvidia.com/optix7/guide/index.html#device_side_functions#device-side-functionsFor questions about how to report intersections, go through these search results for optixReportIntersection()Read these recent threads instead if you need a bigger per ray payload than fits into 8 32-bit registers.\\nhttps://forums.developer.nvidia.com/t/global-payload/159415\\nhttps://forums.developer.nvidia.com/t/optic-7-passing-multiple-ray-data-to-closesthit-program/160005Thank you @droettger for the fast response. What you’re saying makes perfect sense.What I wrote before was pretty dumb. I was kind of hoping for a simple solution - I will think things through better next time before I embarrass myself with another dumb question.Thank you for the links. Much appreciated.No problem.In case you write custom intersection programs, make sure you only produce the minimal amount of attributes you need to calculate the final surface attributes of the hit, because the intersection program is the most often called program and it needs to be as efficient as possible.It’s faster to calculate the dependent hit data deferred once you’ve reached the closest hit (or more often when reaching anyhit program).The minimum is two attribute registers to have the barycentric coordinates beta and gamma for built-in triangles covered. Built-in curve primitives only use one attribute register.Is there any information regarding VBO (from OpenGL/CG) for Optix 7? An array of floats such as float *vboArray and number of vertices as int vertices and offset as int vboOffset maybe stored a s single struct and copied from HOST to DEVICE? Or would one optimally use a SBT?Thanks again for any help/hints.If you mean how you would normally access the vertex attribute data of which you have been using the vertex position field to build the geometry acceleration structure (GAS), then accessing the position and additional vertex attributes like tangents, normals, texture coordinates etc. and the geometric primitive topology via indices inside the device programs, then yes, the SBT data is the place to store pointers to that.\\nThat data can be retrieved with the optixGetSbtDataPointer() device function.Example code here:\\nDeclaring an SBT data structure for the hit groups\\nSetting the data inside the SBT data on the host\\nUsing optixGetSbtDataPointer() to retrieve the dataVertex positions could also be read from the GAS themselves, which requires the\\nOPTIX_BUILD_FLAG_ALLOW_RANDOM_VERTEX_ACCESS to be set.If you mean how to do resource sharing of an OpenGL Vertex Buffer Object in OptiX 7, that would be done with OpenGL-CUDA interop.\\nIf the buffer resource is created by the NVIDIA OpenGL implementation, you can access the data via the cuGraphicsGLRegisterBuffer() function (or the resp. CUDA runtime API function).That’s the same for Pixel Buffer Objects for which OpenGL interop is shown here for example. Search that whole file for m_cudaGraphicsResource\\nhttps://github.com/NVIDIA/OptiX_Apps/blob/master/apps/rtigo3/src/DeviceSingleGPU.cpp#L64That could get a little involved if you need that for many VBOs.Though note that OpenGL and CUDA have different vector type alignment restrictions!\\nWhile it’s fine to use a tightly packed structure with, for example, { float3 position; float2 texcoord; } under OpenGL, using the same data under CUDA with the same vector types will not work because float2 needs to be 8-bytes aligned and it isn’t in that structure when it comes from OpenGL.\\nYou would need to interpret that data as individual floats and load it manually into the respective vector types on CUDA side.Thanks for the fast response @droettger.Unfortunately, it looks like the latter is what I am looking for - OpenGL Vertex Buffer Object in OptiX 7. This is from legacy code that is at least 7 years old and I really appreciate all the information and links - I apologize for hitting the forum so much. It looks like the old code (shader side anyway) just declared a rtBuffer<float> vbobuff at the top of the intersect program.The question is how you set that buffer on host side.  You cannot simply name it “vbo” and that’s it.\\nThere must be quite some code on the host side which would get the device pointer from OpenGL and set it inside the old OptiX buffer variable.\\nThe same steps would need to happen in OptiX 7, just explicitly using the cited CUDA interop functions.If that was using rtBufferCreateFromGLBO in your old code, then all that mapping and unmapping happened behind your back inside the old API. That resource management is now your responsibility in OptiX 7.The major hassle would be to map that shared buffer resource and upload the pointers to the proper locations.\\nSomething like this, here for a PBO: https://github.com/NVIDIA/OptiX_Apps/blob/master/apps/rtigo3/src/DeviceSingleGPU.cpp#L168Now the question is, what happens when switching between OpenGL and OptiX rendering back and forth, because I do not know what happens when using the resource in OpenGL while CUDA has it mapped. You should try that first. Hopefully OpenGL doesn’t care.\\nIf that needs to be unmapped in CUDA to become usable in OpenGL, then you would need to update all VBO pointers on OptiX side everytime you switched APIs because the cuGraphicsResourceGetMappedPointer() function could return a different virtual pointer every time you call it per its documentation. (It normally doesn’t, but that’s implementation dependent behaviour which is always undefined.)Anyway you have enough example code to figure out the necessary steps.\\nVertex-BufferObjects and Pixel-BufferObjects are just linear memory. There should be no problem to get CUdeviceptr to them.\\n(BTW, FrameBuffer-Objects (note the different dash location!) are something completely different.)You are correct. There is quite a bit of code on HOST side and it is going to be a long haul just going through all the files. I am currently working the shaders, of which there are many.Once again, thank you @droettger  for walking me through much of the OptiX 7 code as I try and correlate it to what exists as OptiX 5/6. You have been invaluable in this process.Powered by Discourse, best viewed with JavaScript enabled',\n",
       " 'optix-7-and-ptx-interop': 'Hi,I have some external PTX modules, I’d like to use them as direct callables, so that these external work can be used in OptiX. By using optixModuleCreateFromPTX and entryFunctionNameDC, I can register functions to the pipeline, that’s great.However, I don’t know how to assign the variables of these PTX modules. With cuda, I can use cuModuleGetGlobal. Unfortunately, there seems no way to get cuModule from OptiXModule.As OptiX 7 is very cuda like, are there any way to assign the variables of external PTX modules?Example:In this case, I don’t know how to set diffuseColor and diffuseRoughness.ThanksThe ideal thing to do is put those global variables inside your launch parameters in constant memory instead. Do you have anything preventing that? You would declare them in your module’s code the same way they’re declared in any OptiX program.–\\nDavid.Hi David,Thank you for the answer.It would be tricky for me to convert those global variables to constant memory, because those PTX modules are generated by third parties. In fact the function name is also a problem, I changed that by replacing the name with prefix direct_callable. The example is a simplest case for the remaining problem (the global variable).Are there any non-ideal approaches to tackle this issue?Ah, I see, that’s a tough case. Are you able to negotiate any changes or interface to the 3rd party PTX, or is this code you can’t realistically talk to the authors about at all?There isn’t a way to register and write to variables on the host side for OptiX modules, so here is one possible option that I speculate might work, with the caveat that I haven’t tried it. I hope it’s helpful and that I’m not sending you down a rabbit hole.Probably the easiest of the very ugly solutions would be to setup a separate “setter” launch of size 1 to write all your globals from host memory into device memory, before you run the “render” launch.The setter launch would need it’s own separate raygen program, with the global vars linked as extern, that does all the copying of data from launch params to your external 3rd party global vars. (Or, if you’d rather, you can use your own cuda buffer instead of launch params.)You’ll need a separate SBT for the setter launch that runs your setter raygen program.For it all to work, the setter code and the render code needs to be compiled together in the same pipeline. So, you want 1 pipeline, 2 SBTs, 2 raygen programs, and 2 sets of launch params.I am imagining that if this works, you could abstract it into a host side function that takes a list of global var names and sizes, and then writes & compiles the setter raygen program on the fly, and then runs the synchronous setter launch, all in one go.–\\nDavid.Incidentally, I heard that function parameters do not require the direct_callable tag, only the function names do. Our examples do that by convention mostly for clarity, but maybe it’ll save you some work if you don’t have to rename the parameters.–\\nDavid.It works. Thank you David.Powered by Discourse, best viewed with JavaScript enabled',\n",
       " 'optix-denoiser-is-broken-after-recent-driver-updates': 'Hi everyone.\\nI’m shipping a lightmapper equipped with OptiX Denoiser 5, 6 and 7. They all worked well (apart from 5 not running on 3XXX cards, which is, I guess, by design), but after recent driver updates I started getting many complaints about 6 and 7 producing some weird patterns instead of the denoising. 5 seems unaffected (apparently because it keeps its data in a huge local DLL, not in the driver).\\nPatterns look like this:\\nimage1084×978 149 KB\\n\\nimage1280×846 63.7 KB\\nFrom what I learned, it definitely happens on driver version 471.41 and it definitely doesn’t happen on 460.79.\\nAre there any changes that can be related? Are pre-7.3. denoisers unsupported by the driver and should I use 7.3 now?Hi @Mr_F,I’m not immediately aware of such issues, but I am forwarding your report to the denoising team for comment.Can you share the raw inputs to the denoiser for one of these examples? (Meaning the HDR or LDR beauty layer, along with any other data being passed, such as albedo, normals, AOVs, etc.) It would also help if you could collect the parameter inputs as well, and the results of any denoising prep calls, for example the modelKind, sizes, average color and/or hdrIntensity, scratch sizes, image sizes, etc… Are you using tiling, temporal denoising, or AOVs?–\\nDavid.Hi @dhart ,Here is the input (half-float DDS): https://drive.google.com/file/d/12dq2c8Shs8GtSzhSLLPqG70xE7Fc9zJ3/view?usp=sharingWhen using OptiX 7.2, what I do is:\\nInternally convert half to float via CUDA (the conversion itself is unaffected, looks good)\\noptixInit\\noptixDeviceContextCreate\\ncreateOptixImage with OPTIX_PIXEL_FORMAT_FLOAT4 (twice, for input and ouput)\\noptixDenoiserCreate with OPTIX_DENOISER_INPUT_RGB\\noptixDenoiserSetModel with OPTIX_DENOISER_MODEL_KIND_HDR\\noptixDenoiserComputeMemoryResources\\noptixDenoiserSetup\\noptixDenoiserInvokeThe data passed is a color lightmap, there is no additional data. Not using tiling/temporal, it’s pretty simple.The result is what’s seen in the first post, and it worked in the past, weird.Can fetch avg. intensity and other prep data in a few days (not at my PC atm).Also reproduced it on an RTX 3090 with studio driver v471.68.Okay, thank you for the repro data. Just so you know what to expect, our denoiser engineer is out of the office for a couple of weeks so we might not get this properly looked at until they’re back, but we will use your data to try to reproduce here, and if so file & fix the issue.You answered a couple of questions I had but didn’t ask yet. :) A couple of other questions I have are whether the results change if you use larger or smaller resolutions, for example if you scaled your inputs to one quarter resolution, does the bug still occur? Also it might be worth checking what you get if you use the optixDenoiser SDK sample on your input images - are the same artifacts still visible in that case? (This is the first thing I’ll try myself in the next day or so.)–\\nDavid.Internally convert half to float via CUDA (the conversion itself is unaffected, looks good)Please note that this is not necessary for the denoiser. It actually prefers half data input.\\nMeans your denoising algorithm should be faster overall when keeping the data in half format because that halves the memory size and therefore required memory bandwidth and saves the conversion timeAnother thing to try is switching from the HDR denoiser to the AOV denoiser with just the noisy beauty image.\\nThe programming guide explains the differences (use of optixDenoiserComputeAverageColor function and hdrAverageColor field).\\nhttps://raytracing-docs.nvidia.com/optix7/guide/index.html#ai_denoiser#nvidia-ai-denoiserwhether the results change if you use larger or smaller resolutions, for example if you scaled your inputs to one quarter resolution, does the bug still occur?Interestingly, it seems like the issue doesn’t occur even with 1/2 size. The problem occurs at 4096x4096 and higher, but doesn’t occur with 2048x2048.Technically I can split the image into 2048x2048 tiles as a workaround, but it would be nice if it worked out of the box or at least failed with some error code. My implementation attempts splitting the image if the denoiser fails.Hey this is good to know, I’m adding it to the report, thanks! Glad you have a workaround for the time being, but I agree it’s not great to require a tiling fallback since it takes longer. As long as you have all the output & scratch memory allocated, and there aren’t any OptiX or CUDA status errors before you denoise, then I don’t think it should be failing, and we agree that if there is a reason for the failure, it should be returned as an error code.Hey I didn’t quite understand from the original image snippets what exactly the expected output should look like versus what the problem looks like. I haven’t inspected or converted the dds file either. (BTW what tools can deal with dds files?) I was wondering if it would be possible to post a jpg version of an input image, along with the denoised output with and without the problem (once large with the bug, and once with tiling enabled without the bug)? If they’re all cropped and exposed identically, it might be easier/quicker for the team to get a sense of the problem and expected output.–\\nDavid.BTW what tools can deal with dds files?Personally I just treat them as raw pixel buffers with tiny headers. Height and width start at byte offset 12, and the pixel data is at offset 128 (when using non-DX10-style header). Modern versions of Visual Studio also can open/convert DDS. Photoshop can read it using Nvidia/Intel plugins. So all my tools tend to store images in dds as they’re super easy to construct (if you don’t need all features) and still open in conventional tools.post a jpg version of an input imageMaybe an .exr or an .hdr, as we’re testing HDR denoising?Okay, thanks for the DDS info. Yes EXR or HDR would be totally fine. I would still be nice to have exposed versions as well for reference, if it’s not too much trouble, since the expected exposure & gamma can sometimes get lost in the shuffle when passing around hdr formats.–\\nDavid.Attaching exr and jpg: https://drive.google.com/file/d/1yTfNQKIheeVJS8d6XZBx9l_UrDKHwTOf/view?usp=sharing…although it seems to be independent of content, just high resolution is the problem. And I’m checking every CUDA/OptiX call for !=0, they all look good…Thanks! So this is the input image, correct? I was hoping to also see two versions of the output image, one with the bug and one without. Is that possible?–\\nDavid.Yes.\\nOK, here is an actual input/output pair of EXR+JPG images:\\nhttps://drive.google.com/file/d/1HEjNR9ArSV4-8V6FteJK9_s3IJiG-QRW/view?usp=sharingHi @Mr_F, thanks again for the input data! The denoiser team was able to look at your data and reproduce the problem. There is a real bug here you’ve identified which is going to get fixed in an upcoming driver, however it may take a couple of months to fix and then percolate through our QA and release process.In the mean time, you are still able to use tiling to work around the issue, right? Hopefully you can continue to do that, and that the tiling option is not terribly slow – we would expect that the overheads of tiling at the scale of 4k+ images will not be very large compared to untiled denoising (and hopefully the memory consumption is reduced as a side-benefit).  Another option the team identified is you could use the OptiX 7 denoiser, and enable the AOV mode, which uses a different convolution kernel that is not subject to the issue you bumped into. If you have memory to spare, it sounds like with the OptiX 6 denoiser there is also an option to increase the memory limit high enough to trigger the denoiser to enable automatic tiling. That memory limit should be somewhere in the neighborhood of 200 MB.If you were already thinking of moving to OptiX 7 at some point, maybe this is a decent excuse to try it. If that’s not reasonable at the moment, we are expecting the fix for this issue to appear in drivers numbered 495 and higher, a couple of months from now. Apologies for the bump in the road, and thank you kindly for reporting it and sharing repro data. Let me know if you have any questions.–\\nDavid.Thanks for confirming! Workaround performance is OK and I’ll continue using it for now then.Sounds good. So one last tidbit, you are manually splitting your image into tiles, is that correct? Just in case this applies to you, one thing to be aware of is that correct tiling with the denoiser needs to have an overlap region between tiles, otherwise you might get seam artifacts at the tile edges. Currently the overlap region is 64 pixels, but the OptiX 7 API has explicit calls to determine the overlap size should it change in the future, and the OptiX 6 internal tiling handles this overlap region for you.If your tiles are just separate light maps whose borders are also the tile edges, then you don’t really need to worry about any overlap regions. But if your tile seam goes through the middle of one or more of your light maps or other inputs to the denoiser, then we recommend incorporating the overlap regions into your image splitting & re-merging code.–\\nDavid.@Mr_F, an update - the denoiser team should have the fix for this in the next driver release, expected some time next week.–\\nDavid.Powered by Discourse, best viewed with JavaScript enabled',\n",
       " 'video-codec-sdk-7-0-released': 'The Video Codec SDK includes a complete set of high-performance tools, samples and documentation for hardware accelerated video encode and decode on Windows and Linux.What’s new in Video Codec SDK 7.0* These features require Pascal generation GPUs.Download Video Codec SDK 7.0 and learn more at https://developer.nvidia.com/nvidia-video-codec-sdkHello,We’re interested in integrating real time video recording & playback into our application.  Our video resolution is 5120 by 3840 with 12-bit pixel depth.  We will be using a GeForce 1080.  The NVENC API with HEVC encoding appears to be a good fit.  However the SDK does not appear to support deep color decoding (10 or 12-bit depth).  Will HEVC deep color decoding be supported in the near future (next 4-5 months)?Maximizing image detail is critical (at 30 fps) for our application.  We are also applying transformations such as tone mapping that stretch regions of the color space.  So deep color support is also important.  Any advice on how to use the Hardware decoder would be greatly appreciated.Unfortunately we cannot publicly commit on any future capabilities however I hope you have noticed that we have updated the information on our Video Codec SDK website and our Pascal GPUs now offer limited 10-bit and 12-bit capabilities. Please take a look at the capabilities of Pascal GPUs in the Encode and Decode tables below.Encode capabilities\\n[url]https://developer.nvidia.com/nvidia-video-codec-sdk#NVENCFeatures[/url]Decode capabilities\\n[url]https://developer.nvidia.com/nvidia-video-codec-sdk#NVDECFeatures[/url]Powered by Discourse, best viewed with JavaScript enabled',\n",
       " 'where-can-i-find-physx-sdk-3-3-2': \"i notice that the open source PhysX at Github 's original version is 3.3.3, and the PxCudaInteropMode of PxCudaContextManager no longer supports D3D9_INTEROP, but my renderer is D3D9 and i need a PhysX SDK 3.3.2.i can’t find a binary PhysX SDK 3.3.2 either,where can i find one? thanks.Powered by Discourse, best viewed with JavaScript enabled\",\n",
       " 'possible-driver-bug': 'I hit on a problem when trying to bake irradiance maps for IBL; after talking about it on a discord one of the members created this reduced example of the problem:correct output:Actual output:os: archlinux driver: nvidia-470xx (470.161.03)Tested on a 3050, 1060 and 780tiPowered by Discourse, best viewed with JavaScript enabled',\n",
       " 'second-office-graphics-card-for-monitor-control-alongside-rtx2060-for-machine-learning': 'Hello everybody,I am using an RTX2060 Super for trainings of a Tensorflow model for object recognition. My monitor environment has gradually expanded. This means that there are currently two full HD and one widescreen monitor (2560×1080) attached to the graphics card. Therefore, the three monitors occupy graphics memory that I lack for object recognition.My consideration:Installing a second graphics card with as little TDP as possible, which only takes care of the output on the monitors. I’ve already thought about a T400, but I’m not sure whether there will be driver problems, as the T400 and the RTX 2060 officially have different drivers. I haven’t found anything about it on the net, not even whether the T400 runs with a GeForce GRD.What tips do you have for me? Or should I rather combine it with an AMD office card?Side question: Very rarely do I ever gamble on the computer. Would it work to connect the 2060 to another port on the “big” monitor and switch the input if necessary? Does the monitor then also occupy graphics memory when the input is not active? That would be easier than having to switch cables when necessary.I am very grateful for any tips and suggestions!RegardsPowered by Discourse, best viewed with JavaScript enabled',\n",
       " 'how-many-nvdec-sessions-are-supported': 'There is a table here ( https://developer.nvidia.com/nvidia-video-codec-sdk#NVDECFeatures ) that tells how many ENCODER sessions are supported. But it does not tell how many DECODER sessions are supported on different platforms. Does anyone know? Is there any way to figure this out? I understand that this might depend on frame size - but then again maybe (like the encoder) it does not… There is simply no information available anywhere as near as I can tell.Related to that is there anything I can do to maximize the number of sessions on a given hardware platform?(I write video surveillance software supporting large numbers of cameras so this is an important topic for me!)Powered by Discourse, best viewed with JavaScript enabled',\n",
       " 'thread-count-is-being-ignored-optix-prime-4-0-2': 'Summary:\\nAfter setting the thread count for an Optix Prime CPU context, the call to QueryExecute ignores this parameter and uses far more threads than were specified.How to duplicate this behavior:\\nThis can be duplicated by editing two lines in the primeSimple application that ships with the SDK.Environment:Question:\\nIs this behavior a bug, or have I clearly misunderstood the purpose of rtpContextSetCpuThreads?Thanks for the report.  This does indeed look like a bug on our end – I was able to reproduce.  We will look into it.This bug still exists in OptiX Prime version 4.1.1.  Do you know if there are any plans to fix this in OptiX 5.0?  I would like to benchmark running OptiX Prime with a single CPU thread, but it can’t be forced to do that with this problem.The documentation for rtpContextSetCpuThreads() states that by default one ray tracing thread is created per CPU core.  This normally appears to be the case, but when I run a OpenMPI program that calls OptiX it seems to use a maximum of 2 cores, even though 32 are available.  If I run the same program outside of mpirun, it appears to use a maximum of 32 cores.What function does OptiX use on Linux to programmatically determine the number of available cores?This bug is being tracked internally, but no promises on when it will be fixed.I PM’d you some info about how Prime determines number of available cores on Linux.Stepping back a second, it sounds like you might be intending to use Prime as a pure CPU raytracer, given that the machine specs above don’t include a GPU.  Prime is first and foremost a GPU ray tracer.  That’s where most of our engineering effort goes.  I would expect your benchmarks to reflect this.We are using OptiX for ray tracing in order to enable the generation of optical and radar signatures.  As part of this processing, we perform the ray tracing on GPUs (and fallback to CPUs when one is not available) and then perform additional processing on CPUs.  We are capturing processing timelines for the typical case when a GPU is available, and we would also like to capture processing timelines for the case when the ray tracing happens on a CPU.The machine specs in the initial post were not for us, they were for the original poster (AndrewHardin).Thank you for your PM.  By the way, I realized that OpenMPI was forcing only a certain number of threads to be available to each process that it spawns based on the mpirun options, so that behavior had nothing to do with OptiX.Ah right, you weren’t the original poster, my mistake!Powered by Discourse, best viewed with JavaScript enabled',\n",
       " 'how-do-i-know-if-i-am-enrolled-in-the-developer-program': 'Hi, How do I know if I am enrolled in the Developer Program? Do I get a confirmation email etc. Thank you-JoelHi Joel,You should have received a welcome email. The fact that you are logged into the forums, proves that you are in the program. Can you please check your junk/spam folders for the welcome email?Thanks,\\nTomHi Tom,I did not receive a welcome email. Can you pls confirm that I’m enrolled in the Developer Program and resend me the welcome email. I have no idea what this program is about.Thank you,\\nJoelHi Joel,You can check https://developer.nvidia.com/developer-program for program details. While logged in, click on the “head and shoulders” icon to your profile. In there is a “My programs” tab that will show what you’re a member of.Hi Tom,Thank you for the information. This is what I found. So what is the NVIDIA Developer program? I don’t have any information about it. Please send me the welcome info email and other information about the program.\\nimage.png964×357 14.6 KB\\nThank you,\\nJoelThe welcome email would have gone out back when you joined three months ago. Unfortunately, I cannot tell the system to send another email. The NVIDIA Developer Program is described in detail at https://developer.nvidia.com/developer-program.Powered by Discourse, best viewed with JavaScript enabled',\n",
       " 'currect-way-to-resize-frame-using-cuda': 'Hi, i am using ffmpeg + cuda support to decode video. i was wondering if i can send the compressed frame to the GPU and get back a resized decompress frame.\\nthe flow i am using is\\navcodec_send_packet\\nand then\\navcodec_receive_frame  (here i want to receive an already resized frame to minimize transfer from the gpu memry to cpu memory when i use “av_hwframe_transfer_data”)what is the best way to do the resizing inside the GPU memory and save CPU resources on big transfers.thanksPowered by Discourse, best viewed with JavaScript enabled',\n",
       " 'mipmapping-in-optix': 'Interface for mipmapping has been present in Optix since v1.0, and the doc states that it will be implemented at some point.I was wondering is it known when will this happen?I need mipmapping for rendering, it’s not very urgent but the lack of it produces a lot of aliasing.Would it be worth the trouble implementing it manualy, performance being what it is in ray tracing? I assume that the overhead would be quite big compared to the hardware solution that will come in future releases of optix.You would need to implement ray differentials in your renderer for that anyway, so you could start doing that today and look at the example inside the OptiX SDK called rayDifferentials which shows one way to implement mipmapping manually.Powered by Discourse, best viewed with JavaScript enabled',\n",
       " 'wraps-in-ray-gen-and-how-data-is-initially-stored-in-the-memory-hierarchy': 'Hi! I had two questions regarding how the various optix programs are actually executed on the underlying GPU.First, can I assume that each element launched through optixLaunch will be a separate thread running on an SM? If so, can I assume that the first 32 rays obtained through optixGetLaunchIndex() in the ray gen program will be in the first warp, and the next 32 rays obtained through optixGetLaunchIndex() will be in the second warp, and so on?Second, what memory are used to store the launch parameters, and what memory are used to store the ray payload? I feel that OptiX abstracts the memory hierarchy away from the programmers and so optimizing for memory efficiency requires a bit of guessing where the data resides. Is there a reference that explicitly states what data stays in what memory?Hey good questions.So regarding optixLaunch(), every invocation of your raygen program will be a separate thread. When you specify the width and height of your 2D OptiX launch, you can expect the number of threads to be width * height. Your call to optixGetLaunchIndex() gives you an index that identifies the thread.https://raytracing-docs.nvidia.com/optix7/guide/index.html#device_side_functions#launch-indexNote the last sentence in that section: “program execution of neighboring launch indices is not necessarily done within the same warp or block, so the application must not rely on the locality of launch indices.”So the answer to the first part of your first question is “yes”, and to the second part of the first question, “no”. You should not assume that the first 32 rays are grouped together into the first warp. It’s hard to define what “first” means, and threads and warps in general do not execute in sequential order. OptiX automatically structures a 2D launch into tiles for efficiency, so your sequential thread ids will usually not be in scan-line order. On top of that, OptiX reserves the right to move threads during execution: “For efficiency and coherence, the NVIDIA OptiX 7 runtime—unlike CUDA kernels—allows the execution of one task, such as a single ray, to be moved at any point in time to a different lane, warp or streaming multiprocessor (SM). (See section “Kernel Focus” in the CUDA Toolkit Documentation.) Consequently, applications cannot use shared memory, synchronization, barriers, or other SM-thread-specific programming constructs in their programs supplied to OptiX.” https://raytracing-docs.nvidia.com/optix7/guide/index.html#introduction#overviewSome additional reading on raygen and threads here: https://raytracing-docs.nvidia.com/optix7/guide/index.html#ray_generation_launches#ray-generation-launchesLaunch parameters are in device memory. Currently launch params are put into constant (read-only) memory for efficiency, and the launch params buffer is limited to a maximum size of 64KB.Payload values are generally compiled into registers. If you need more space for a payload than the limited number of payload slots, you can put a pointer to memory in the payload. That usually comes with the associated indirection and memory access costs.The Programming Guide does mention both of these in different sections, but it’s easier for me to find them quickly when I know what I’m looking for. It’s true that OptiX is abstracting this away a bit, but for good reason - in general OptiX is putting these things in the most efficient-to-access place possible.Launch Params in constant mem: https://raytracing-docs.nvidia.com/optix7/guide/index.html#program_pipeline_creation#7054Payloads in registers: https://raytracing-docs.nvidia.com/optix7/guide/index.html#device_side_functions#trace–\\nDavid.Thanks! So if we can’t rely on the fact that threads with neighboring indices are in the same warp, do you have any suggestions how we, programmers, could help minimize control-flow divergence/improve locality? I get that the optix runtime will do some sort of ray grouping/bundling to reduce control-flow divergence and maximize locality, as in any ray-tracing engine, but does that mean programmers can’t really help here?I am asking this because empirically I find that if I manually reorder the ray indices in a locality-friendly way, I get much higher performance that using a random order. So I guess that suggests that the run-time system has its limits in doing ray scheduling and programmers might help, but if programmers can’t rely on certain deterministic facts, rigorous optimizations become harder.This is also a good question.How are you thinking of reordering the ray indices? Do you mean camera rays, or reflection rays? How do you find the locality-friendly ordering?If by reordering, you are talking about raygen, then OptiX is not stopping you from re-ordering your thread’s work at the beginning of raygen, you can map your launch index to camera rays any way you like. You just can’t currently re-order threads during a launch once they’re in progress.And just to be pedantic but careful - threads and rays in general are not 1:1. Raygen is called once per thread, while closest-hit is called at most once per ray. So reordering rays and reordering threads are fairly different activities.For hit shader coherence, there are some things you can do as a programmer. At a high level, a couple of common basic ray tracing architectures are uber-shader and wavefront. Both of these options afford you chances to reduce divergence and improve locality. With an uber-shader, the design of your material system and available materials has a big impact on how much divergence occurs in hit programs. If you can combine common sub-sections in your materials, or reduce the total number of materials, you can reduce divergence. With a wavefront approach, you might stop your launch after each path segment of a path tracer, and then sort your launch indices by material in order to regain coherence, then start a new launch to shade and trace further into the scene.–\\nDavid.I meant camera/primary rays. Basically I map launch index to camera rays such as camera rays with neighboring indices are also spatially close. I am doing something weird so camera ray with neighboring indices are not automatically in a scan line order. What I found is that manually ensuring that rays with neighboring indices are spatially close significantly improves performance.I think I get that “threads and rays in general are not 1:1”, but not sure how to understand your explanation. Are CH, AH, IS programs executed within the same thread that executes the corresponding raygen program? Also, if in, say CH, program I call another optixTrace, that secondary ray will not spawn a new thread, right?Thanks for the suggestions on shader coherence. I am aware of the general techniques. Do you know of a good CUDA implementation of sorting launch indices according to materials?I meant camera/primary rays. Basically I map launch index to camera rays such as camera rays with neighboring indices are also spatially close. I am doing something weird so camera ray with neighboring indices are not automatically in a scan line order. What I found is that manually ensuring that rays with neighboring indices are spatially close significantly improves performance.Ah, okay, so yes you can continue to do that. Just be aware that OptiX is already doing it too, for exactly the same reason. The default right now is 8x4 tiles. If you have a different arrangement you need, you might have to undo the OptiX mapping. We can help with that, or it may be fairly easy to reverse engineer. I haven’t thought about this carefully, but maybe it would work to setup your own camera mapping to intentionally put your threads in scanline order, knowing that OptiX will then tile them.I think I get that “threads and rays in general are not 1:1”, but not sure how to understand your explanation. Are CH, AH, IS programs executed within the same thread that executes the corresponding raygen program? Also, if in, say CH, program I call another optixTrace, that secondary ray will not spawn a new thread, right?Sorry I didn’t clarify. Rays are only spawned via optixTrace() calls. The ray traversal and any associated programs attached to the ray are called within the scope of the same thread as the raygen thread that spawned the ray(s). There is never a case where new threads are spawned. So if you only trace a single primary ray in raygen, then your rays and threads are 1:1. If you trace a path of several ray segments, or trace recursively in CH, or super-sample your pixel, then you’ll have multiple rays per thread.Do you know of a good CUDA implementation of sorting launch indices according to materials?I would start with Thrust or Cub and see if one of those gives you everything you need. Thrust is higher level and more generic. Cub’s a little lower level and more CUDA specific.Thrust :: CUDA Toolkit DocumentationCUB :: CUDA Toolkit Documentation–\\nDavid.Thanks for your response, David.The default right now is 8x4 tilesAm I correct in understanding that if I launch a 2D grid then the first 8x4 indices will be group together and the next 8x4 indices will be in the second group, and so on? And when I call optixLaunch() if I get 31, that’s the last element in the first tile (4th element in the 8th row in my launch grid), not the 32nd element in the first row in my launch grid, right?Does it mean that each 8x4 tile is a warp? I can’t help but to relate a tile to a warp because 8x4 = 32. My guess is that initially they will be in the same warp, but later the runtime might decide to regroup threads into new warps for efficiency reasons?Also, what happens to a 1D or a 3D launch grid? Any tiling being done there?Yes, the 8x4 tiles are grouping threads into warps, the intent is to bundle the primary rays closer together so they’re more likely to traverse the same parts of the scene and less likely to hit different geometries & materials. This is assuming that you’re mapping your launch index into a camera ray by using a straightforward linear mapping of launch indices to camera space (u,v) coordinates, for example, such that a given launch_index.y value corresponds to a scan-line of pixels. And yes, OptiX reserves the right to regroup the threads.Great question, I think the 1D launches are not tiling, and I’m not sure about 3D, I would have to check to make sure. I do think it’s instructive to figure out how to see the sequential thread ordering on-screen, I’d recommend going through that exercise. (Hint: it’s slightly harder than it sounds because you don’t get a scalar thread id when your launch index is 2D or 3D, and so the sequential mapping can happen transparently. In the past I started by returning a solid identifiable color from raygen for only a few far-apart groups of 32 threads.) This would help you verify and visualize your primary ray coherence, and if needed it might help you design any modification you need.–\\nDavid.Thank you. Is there a reference/citation for 8x4 tiling and that a tile is a warp (initially) ?At the moment the tiling scheme is not mentioned in the programming guide mainly because there are no programming choices to make; there is no API and it happens transparently. Also, we might change it if we find higher performance alternatives, so for that reason and all the other reasons we’ve discussed, it’s best to stick to the ‘single-ray programming model’ and not rely on the shape of a warp or make any assumptions about what two neighboring threads are doing, or even that they’re neighbors as far as the GPU is concerned.We have talked about the tiling openly here on the forum and directly with some customers, but as far as I know, nobody has ended up needing to change it or remap it or work around it and I’m not aware of any cases of it leading to slower performance. If you do end up needing to do something different, we’d love to hear about it.–\\nDavid.Following up on this thread on memory hierarchy, in traditional CUDA the compiler would try to store local variables in registers so that we can iterate over them without going to the global memory. In OptiX, if each ray in the IS program needs to update a small amount of shared data, seems like the only way to do that is to pass the shared data as the ray payload? Then my understanding is that that limits the shared data to be 8 32-bit values, which will be stored in registers, and for larger data we would have to store them in global memory and pass pointers as ray payload, correct?Any suggestions as to shared data across rays in the IS program? Thanks!For IS (intersection) programs, you have payload values to communicate inputs to the intersector (via registers, usually), and you have attribute values to output data from the intersector (via registers, usually). Attributes are set in the optixReportIntersection() call, and retrieved via optixGetAttribute_n() where n is [0…7]. You can write to payload slots, but you might want to prefer using attributes (it’s cleaner, and depth order hits are handled for you when reading attributes via closest-hit).For accessing memory, there are several ways to pass pointers into IS programs. You can pass buffer pointers via the payload, or you can access your SBT entry directly in the IS program, or you can access launch params.What kind of shared data do you want to update in intersection? And what does ‘shared’ mean exactly? Shared with all rays in the same thread (e.g. all rays cast in raygen), or shared across all thread in the launch, or some other granularity?Keep in mind if you want to update shared data based on hits and not misses, that can (and possibly should) be done in closest-hit or any-hit, since IS programs are called more frequently than hit programs.–\\nDavid.Oh sorry I should be more clear. I am talking about sharing data across different IS program invocations of the same camera ray. That is, there is a piece of shared data that all the intersected bounding boxes need to read and write, and that shared data is about 200KB for each camera ray.Powered by Discourse, best viewed with JavaScript enabled',\n",
       " 'how-can-i-use-custream-in-cuvidmapvideoframe': 'hi,i hope someone can help me.\\nI am using NvDecoder to decode multiple videos.follow the sample i have finished my job.\\nbut now i want to improve performance.\\nwhen i saw  the following codesNow  i  get a  mistake :\\n‘cuvidMapVideoFrame(m_hDecoder, pDispInfo->picture_index, &dpSrcFrame, &nSrcPitch, &videoProcessingParameters) returned error 400’\\nwhat i can do to solve this problem, cuvidMapVideoFrame can not use CUstream?Powered by Discourse, best viewed with JavaScript enabled',\n",
       " 'denosier-crash-on-2080-in-optix-5-1-1': 'hi. I just download optix 5.1.1 and run the optixDenoiser sample will crash.\\nDLDenoiser run method failed with error -40\\nmy machine is 2080. previous 5.1 works fine.\\nthanks\\nYangOn the download page “Improvements to 5.1.1” there isPlease note that the OptiX 5.1.1 SDK does not support TuringLooks like your card is not supported :(On the download page “Improvements to 5.1.1” there isPlease note that the OptiX 5.1.1 SDK does not support TuringLooks like your card is not supported :(I saw that and thinking it’s just doesn’t support RTX as 5.1 didn’t expect the crash.\\n:`(I supposed that the release note was referring to the new RT hardware. So, 5.1.1 does not work at all with Turing cards?I supposed that the release note was referring to the new RT hardware. So, 5.1.1 does not work at all with Turing cards?Only the commandlist execution will crash. I can still see the image before denoise.Powered by Discourse, best viewed with JavaScript enabled',\n",
       " '4-quadro-rtx-4000-for-unreal-engine-4-simulations-games-and-nvidia-mosaic': 'Hi,I wonder if it’s possible to play a simulation (game) created with Unreal Engine 4 using 4 Quadro RTX 4000, and how good it is. The reason why I need to use 4 cards is that I am going to connect 14 projectors to them (2x7 grid). the total resolution would be 8960x1600 because each projector resolution is 1280x800. Now because RTX 4000 does not support NVLink, I guess only one card will do the work and the rest is just used for display right? I’m looking to buy the right card currently but has a very hard time finding benchmark results on the internet. I’m fine if the game just runs in 24 fps, as long as it’s possible to run it on that resolution, with all the features (nvidia mosaic, quadro sync).What about 4x Quadro P4000 with SLI?Powered by Discourse, best viewed with JavaScript enabled',\n",
       " 'opengl-texture-to-optix-sampler-error': 'Running this line of code:Produces this error:\\n+\\t\\te\\t{m_message=“Invalid value (Details: Function “_rtTextureSamplerCreateFromGLImage” caught exception: GL error: invalid enumerant\\n, [10420290])” m_error_code=RT_ERROR_INVALID_VALUE }\\toptix::Exception &\\nCaught by:Any idea why this happens? I’m running the same texture code block from the Texture Interpolation sample from OptiX:Please make sure that there is no error stuck in the OpenGL state from previous OpenGL calls.\\nAdd a glGetError() before that call and see if it already contains a GL_ERROR_INVALID_ERROR at that time. If yes, find the root cause by adding more glGetError() calls in your application and fix it.To clarify, building and running the original OptiX samples which use this functionality work without error on your system?\\nIf yes, you have a reference implementation you can step through in the debugger and compare against your own code line by line to find the difference.You set filtering modes differently in OpenGL and OptiX, try nearest or linear in both.I would not use read mode RT_TEXTURE_READ_NORMALIZED_FLOAT on floating point data, RT_TEXTURE_READ_ELEMENT_TYPE makes more sense there. Probably doesn’t matter for float types though.I get an “GL_INVALID_ENUM” error from glGetError(); right after glewInit();By catching it the previous problem with:Is resolved. But OptiX still has an issue with this code. When I run without setting:Everything is fine. When I do run this line, I get a very distorted image, which doesn’t get updated.I guess I’m just misusing the sampler in OptiX. My .cu shader kernel looks like this:Even without using the texture sampler in the .cu, the program fails…I can’t seem to fix the GL error, but it doesn’t seem to matter anyway: http://stackoverflow.com/questions/10857335/opengl-glgeterror-returns-invalid-enum-after-call-to-glewinitDo I have to do anything else in OptiX to use a texture sampler? I set it in C++ and declare it in my .cu shader.Powered by Discourse, best viewed with JavaScript enabled',\n",
       " 'physx-3-3-3-19456754-on-android-undefined-reference-to-nvtx': 'I have built PhysX 3.3.3 from the github source for Android9 and it produced serveral versions of the libs, for example:\\nlibPhysX3.a, libPhysX3CHECKED.a, libPhysX3DEBUG.a, and libPhysX3PROFILE.aThe untagged .a files work great, but if I instead try to link to the CHECKED, DEBUG, or PROFILE files, then I get the following linker errors:\\n./…/…/Common/src/CmNvToolsExtProfiler.h:120: error: undefined reference to ‘nvtxRangePushEx’\\n./…/…/Common/src/CmNvToolsExtProfiler.h:125: error: undefined reference to ‘nvtxRangePop’\\n./…/…/Common/src/CmNvToolsExtProfiler.h:134: error: undefined reference to ‘nvtxRangeStartEx’\\n./…/…/Common/src/CmNvToolsExtProfiler.h:139: error: undefined reference to ‘nvtxRangeEnd’\\n./…/…/Common/src/CmNvToolsExtProfiler.h:204: error: undefined reference to ‘nvtxRangePushEx’\\n./…/…/Common/src/CmNvToolsExtProfiler.h:210: error: undefined reference to ‘nvtxRangePop’\\n./…/…/Common/src/CmNvToolsExtProfiler.h:141: error: undefined reference to ‘nvtxRangePushEx’\\n./…/…/Common/src/CmNvToolsExtProfiler.h:146: error: undefined reference to ‘nvtxRangePop’\\n./…/…/Common/src/CmNvToolsExtProfiler.h:129: error: undefined reference to ‘nvtxMarkEx’Is this nvtx library available somewhere for Android so that I can run with these .a files? I’m having a crash in PhysX at the moment, and I’d like to see if the CHECKED or DEBUG version gives more insight into what’s going on.Thanks!Nevermind, I found it in this location:\\nPhysXSDK\\\\externals\\\\nvToolsExt\\\\1\\\\lib\\\\armv7\\\\libnvToolsExt.aLeaving this here in case anyone else runs into the same problem.Powered by Discourse, best viewed with JavaScript enabled',\n",
       " 'using-optix-to-determine-travel-time-for-a-ray-travelling-through-a-material': 'Hi,I am looking to apply the ray tracing in OptiX to accurately determine the travel time through different materials.The field I am involved with is sound, so transmission speeds will need to be defined (in m/s) and refraction from one material to the next is the primary process affecting the passage of the ray between source and receiver.Is this possible with OptiX? I looked through the documentation briefly and I couldn’t see any references to the transmission speed of the ray.ThanksJamesOptiX is a general purpose GPU ray-casting SDK. It’s not a renderer and has no notion of material properties at all. All behavior depending on some material data is implemented by you as the developer.\\nIf you know how to implement some simulation by shooting rays, then it should also be possible to implement that with OptiXRays in OptiX are not traveling at any “speed”. Fundamentally they just determine which geometric primitive you intersect at what distance, or if you missed. Everything happening on such a hit or miss event is defined by your implementation of the respective GPU device programs called by OptiX for each case.Since you can determine how far a ray traveled inside a medium with a ray tracer easily, you can also determine the time it would have taken to travel along that distance inside whatever material and track that information along each ray path. That is all yours to calculate.\\nThe same is true for calculating the reflections on or refraction through geometric surfaces separating volumes with different properties.\\nYou could also do ray marching and step in discrete time intervals and calculate the ray distance depending on the surrounding medium until you hit something. (Not recommended unless you’re dealing with heterogeneous mediums.)Simulations of time it takes to travel from a transmitter to a receiver have been implemented with ray tracing in OptiX before\\nNVIDIA actually has a product based on that:https://news.developer.nvidia.com/vrworks-audio-dials-up-the-immersion-with-rtx-acceleration/Here are two related posts explaining ideas how to go about that with some Monte Carlo integration.\\n(Maybe disregard my idea of tracking the cone angle. That should even work without, by considering the full hemisphere above a hit point.):\\nhttps://devtalk.nvidia.com/default/topic/1032454/optix/sphere-intersection-with-ray-distance-dependent-radius/post/5255713\\nRelated post: https://devtalk.nvidia.com/default/topic/1036110/optix/wireless-channel-characterization-using-optix-as-a-ray-tracing-engine/post/5272000Thank you for a prompt reply, Detlef.Your post clarifies the extent that OptiX ray-traces. I am looking for a developed solution for which I can modify for my purposes.I looked into NVIDIAs audio ray tracer yesterday and thought it would be viable for my situation. I tried to email (vraudio-support@nvidia.com) to clarify the limitations of the current implementation but the email bounced with a “Your message can’t be delivered because delivery to this address is restricted” error. Any idea why this is happening?JamesProbably an e-mail address setup glitch. I’ve asked what’s up with that.And you already found the devtalk forum for it, which would have been my next link.Powered by Discourse, best viewed with JavaScript enabled',\n",
       " 'physx-3-3-0-samples-and-cuda-dependency': 'Previously in PhysX 3.2.2 the samples were able to be run without errors. Now with the 3.3.0 upgrade, which by the way is a huge step forward ( despite a few issues I’ve been having ) there is a hard dependency on the CUDA runtime being available. Missing nvcuda.dll. I do not have a Nvidia GPU hence the error. May I point out that it is only the samples that have this dependency, the SDK works fine.The sdk works fine. Just the sample is depending from CUDA.Correct, that was in the last line of my previous post.Sorry. You affirm and don’t ask.…I agree. So the original post should have been why is there a CUDA dependency for the PhysX 3.3.0 samples. Sometimes what I have in my head and what I type don’t correlate.Another strange thing is that the Debug DLLs from SDK are depending from Checked DLLs.Powered by Discourse, best viewed with JavaScript enabled',\n",
       " 'take-full-advantage-of-cuda-core-and-rt-core': 'optixLaunch is called in host code to invoke a 1D, 2D or 3D array of threads on the device and invokes ray generation programs for each thread. Are these threads supported by CUDA core? If so, how many threads can be started in parallel to make full use of the cuda core?\\nWhen the ray generation program invokes optixTrace, other programs are invoked to execute traversal. How many rays can traverse in parallel to make full use of the cuda core (for custom primitive) and RT core?optixLaunch is called in host code to invoke a 1D, 2D or 3D array of threads on the device and invokes ray generation programs for each thread. Are these threads supported by CUDA core?Yes, OptiX is using CUDA.https://raytracing-docs.nvidia.com/optix7/guide/index.html#introduction#overviewIf you read through the OptiX 7 Programming Guide and example source code, you’ll see that all resource management inside your host application is done with native CUDA host calls, either using the CUDA Runtime API or the CUDA Driver API.\\nOptiX is using a single ray programming model and does all scheduling internally, so some of the native CUDA features like shared memory access and warp synchronization instructions are not allowed.Then all device code you implement for the different program domains (raygen, exception, closest hit, any hit, intersection, miss, direct or continuation callables) is written in CUDA C++ device code.All these device programs run on the Streaming Multiprocessors (SM) of the GPU.\\nOn RTX GPUs there are also RT cores which handle traversal through the acceleration structures and the ray-triangle intersection calculation in hardware.\\nThen there are Tensor cores which are used for the OptiX Denoiser.If so, how many threads can be started in parallel to make full use of the cuda core?The OptiX launch dimension has a limit of 2^30.https://raytracing-docs.nvidia.com/optix7/guide/index.html#limits#limitsWhen the ray generation program invokes optixTrace , other programs are invoked to execute traversal.\\nHow many rays can traverse in parallel to make full use of the cuda core (for custom primitive) and RT core?That is completely abstracted by OptiX. It will launch as many threads from the given launch dimension in parallel as there are resources available on the underlying GPU.Note that it depends on the underlying GPU how many rays are required to saturate a modern GPU architecture.\\nMeaning there is a minimum number threads required to make best use of the GPU hardware which is directly related to how many CUDA cores a specific GPU has, and how many registers are used inside a device program. Then there are other factors like the memory bandwidth, cache sizes, etc. It’s complicated.You’ll find more information about this inside the CUDA Programming Guide:\\nhttps://docs.nvidia.com/cuda/cuda-c-programming-guide/index.htmlPlease use the search field in the top right of this forum to find more detailed information when you’re still having questions after reading the OptiX Programming Guide.\\nFor example this thread:\\nhttps://forums.developer.nvidia.com/t/a-few-questions-about-bvh-traversal-engine-and-triangle-intersection-engine/233412Powered by Discourse, best viewed with JavaScript enabled',\n",
       " 'pxheightfield-documentation-and-samples-issues-and-clarifications': 'I’m attempting to use a heightfield as the collision shape for my procedurally generated terrain, as documentation suggested this would be the most appropriate shape for my case.However, I’ve yet to find any samples of heightfields in the SDK or online, for that matter. In the documentation, a very concise example is given, but neither documentation nor the samples is clear on the order of translation via the local pose and scaling via the heightfield parameters. Whether the patch of samples is centered on the shape’s pose or if it begins at the shape’s pose and extends in the local X and Z axes is also unclear.Additionally, while trying to discover this my self through experimentation, the scene seems to have no debug points, lines, or triangles. I haven’t been able to establish any form of collision with my heightfield, even with default values and sample heights of 0 with scales of 1.0 in all axes. I have set values for all the relevant visualization parameters I could find on the scene, actor, and shape:Other actors seem to collide with each other and, even without setting their own visualization actor and shape flags, increase the number of debug visualization data in the scene’s render buffer, but the height field does not.I’m at a loss for further debugging at the moment - might there be suggestions as to how to proceed with debugging this situation?Additional details: I have no errors or warnings in the VS2012 Output window, though I know that it is working. I’ve tried using both the DEBUG and CHECKED configurations. I’ve also confirmed that the actors/shapes that are not colliding with terrain are able to collide with each other. I’ve tried with gravity disabled, and with simulation disabled or enabled on the rigid static actor, with no change in behavior.Update: I’d been under the impression that the PhysX Visual Debugger required a source license, but it seems not. However, the installer immediately fails with the following message:“NVIDIA PhysX Visual Debugger requires .NET Framework 3.0. Setup has encountered a problem. Setup will now exit”This is not the case, however. I’m unable to install PVD, but .NET framework 3.0 is absolutely installed and functional.Further attempts to resolve the issue have been futile. I’ve confirmed that the registry settings typically used to detect .NET Framework 3.0 are present according to this official documentation ([url]http://support.microsoft.com/kb/318785[/url]) - .NET Framework 3.0 Service Pack 2 is installed. Furthermore, the directory %SYSTEMROOT%\\\\Microsoft.NET\\\\Framework64\\\\v3.0\\\\ has contents that look appropriate. The 32-bit version of that path also has contents that look appropriate. I’ve sent a bug report for this situation. I have tried multiple downloads and even multiple versions of the PVD installer with no success.Finally, success.I’m not sure why the .NET Framework worked fine for all other applications, including my own development, but a full uninstall and reinstall of all .NET Frameworks resolved the issue.It does seem like the height field’s corner is at the shape’s origin, for what that is worth to anyone who seeks this information some day. Also, the scaling passed when creating the heightfield applies to the values from the heightfield samples before application of the shape’s transform to the heightfield.Powered by Discourse, best viewed with JavaScript enabled',\n",
       " 'compile-error-phinode-should-have-one-entry-for-each-predecessor-of-its-parent-basic-block': 'I’m working on an OptiX project and on cuda file compilation I get this error:Specs:\\nWindows 8.1 x64, Cuda 6, Optix 3.6.0. Driver 334.88, Visual Studio 2012\\nThe file was compiled as ptx, compiler flags -gencode=arch=compute_20,code=\"sm_20,compute_20\"\\nSame code compiles using Cuda 5.5, Optix 3.5.1 and works.I have identified that it happens due to an inlined function the function updateMisTermsOnScatter() in the sample below. It was there first, the issue appeared when I changed how bsdfFactor on line 3 is computed, e.g. using a function call (not inlined). If I remove inline directive from updateMisTermsOnScatter() it compiles using Cuda 6 as well.Could anyone explain meaning of the error? Possible causes? Best practices to avoid something like that?\\nFrom searching online it seems it LLVM stuff, but I don’t pretty much anything about that.I tried to reproduce this with a small sample in another project, but couldn’t. I don’t really have time currently to disassemble code piece by piece to pinpoint what exactly is the key problem in this case, so I guess I’ll use Cuda 5.5 for now.I have not seen an error message like this before, but it seems to indicate some sort of internal compiler error, basically a constraint violation or an assertion failure.I would suggest filing a bug, using the bug reporting form linked from the registered developer website. Please attach a self-contained repro code (small code is better, but large code can be dealt with as logn as you rpovide precise build instructions). The compiler team may have a suggestion for a work-around once the issue has been root-caused. If you are restricted from sharing your actual source code, there may be other ways to repor, like providing one of the files that contain the intermediate representation. The compiler team can advise what is possible in this regard.Thank you for your help.Moving to the OptiX forum.Please try if using forceinline instead of inline helps. The latter is just a hint to the compiler but OptiX requires all functions to be inlined. Only callable programs are real calls.\\nI’ve seen CUDA compilers stopping to inline code with increasing number of function parameters.I’m always using a “#define RT_FUNCTION forceinline device” for any function in OptiX programs to match the look of the code to the RT_PROGRAM and RT_CALLABLE_PROGRAM ones.@njuffa as I mentioned in the post I don’t have time currently to make small self-contained repro project, I tried to make one, but I couldn’t reproduce it there. I could do that in 2 weeks. If you wish to look at this bug sooner, I could push code to github (I’m extending an open source renderer).@Detlef forceinline didn’t help.It’s weird. Today it doesn’t compile also when inline is removed from updateMisTermsOnScatter() which worked yesterday. Not sure how it did yesterday.OptiX requires all functions to be inlined. Only callable programs are real calls.What’s “not real” about function calls in Optix?\\nIf Cuda compiler doesn’t inline some function, the program JIT compilation will fail at runtime?I have one ~100 line function that isn’t inlined and it does work. I’d think that Cuda compiler won’t try to inline that.If you look into your compiled PTX code you give to OptiX and that already contains a “call” instruction, I would expect the context->compile() step to fail. Only if you use callable programs OptiX will generate call instructions in the kernel.OptiX takes the input PTX apart and puts it back together. The resulting PTX sent to the CUDA driver can be rather different. This paper explains that in chapter 6: [url]http://graphics.cs.williams.edu/papers/OptiXSIGGRAPH10/Parker10OptiX.pdf[/url]Actually I got carried away by the OptiX connection. If the compiler error already occurs at the nvcc compilation step and CUDA 5.5 works and CUDA 6.0 doesn’t, getting the reproducing *.cu file would be nice to investigate that.Powered by Discourse, best viewed with JavaScript enabled',\n",
       " 'optix-advanced-samples-glass-etc-noisy-when-rotating': 'Hi, sorry if this has been addressed - simple searching didn’t uncover.I’m new to Optix, and just built the ‘advanced optix samples’ on GitHub.When I run any of them on my PC (Win10, i7-8700K, GTX1060), the scene gets ‘noisy’ (like a digital photograph with super high ISO) when I rotate it. After rotating, it takes 2-3 seconds for the noise to slowly disappear.Is this expected behavior with Optix? Is it due to my GPU being only ‘acceptable’ rather than ‘top of the line’?Or is this something I shouldn’t be seeing? How can I mitigate it?If this is not expected behavior, I can try to record a video of it and post it here.Thanks,\\n-Eric.That is the high frequency noise from a Monte Carlo path tracer and completely normal.\\nIt’s not related to OptiX but the global illumination light transport algorithm implemented in those examples.\\n[url]https://en.wikipedia.org/wiki/Path_tracing[/url]If you work through the individual OptiX Introduction examples there, you’ll see how that is done step by step. The first three examples in there don’t do Monte Carlo path tracing.Thanks - I’ll be working through the samples very soon. I assumed it was normal and conceptually makes sense why it is there. But it also seemed to be ‘unspoken’ - at least in my google searches. And I don’t mind asking ‘dumb’ questions. :)Thanks again.Powered by Discourse, best viewed with JavaScript enabled',\n",
       " 'nvidia-optical-flow-sdk-turing-motion-field-runs-in-black-border-areas': 'I am currently testing the new NVIDIA Optical Flow SDK for Turing GPUs in terms of quality and speed.\\nI am using the DirectX11 Sample application.\\nThe flow is parameterization in mode ‘slow’, and the ‘temporal hints’ are disabled.Principelly. the performance of the Optical Flow algorithm looks quite good, quality and speed wise.But, I have one sequence (images with black ‘overscan’ areas on the left and right border), where the calculation motion field ‘runs’ in the black border areas (instead of being zero there).With ‘running’, I mean that in these areas the flow vectors have a high magnitude.\\nThat is of course a problem, as it affects negatively the warped image, and also all calculations derived from the motion field (e.g., when you are using the motion-field for ‘stabilizing’ an image sequence)Of course big homogenous areas (like the black borders) are always a problem for an optical flow algorithm, but every reasonable regularized optical flow algorithm will tend in this areas to the ‘zero motion’Is there anything in terms of parametrization I can do to resolve this issue ? Is there sort of ‘regularization’ paramter (like in TV-L1) which I can tune in order to control the smoothness of the motionfield ? Is that a known issue  ?Related question: Is there an ETA for the SDK version 1.1 ? The current version of the SDK is 1.0.Thank you for the feedback.This is a known issue. Unfortunately, there is no parameter that can be tuned at the API level to fix this.The issue is being addressed in the next SDK version 1.1.Sounds good ! Thanks in advance.Powered by Discourse, best viewed with JavaScript enabled',\n",
       " 'does-cloudxr-2-1-work-with-that-latest-grd-471-41': 'No, the most recent driver supported by CloudXR 2.1 is 471.11.  We suggest upgrading to CloudXR 3.0 which supports the latest drivers.Powered by Discourse, best viewed with JavaScript enabled',\n",
       " 'debug-libraries-not-working-in-3-2-4': 'I’m using 3.2.4 32 bit and VS2012.  I can link and execute my code using the none debug libs but when I try to use the debug versions I get a variety of link errors. The only one which doesn’t generate errors is: PhysX3DEBUG_x86.lib.  For example if I use PhysX3CommonDebug_x86.lib I get this error:1>PhysX3Extensions.lib(ExtExtensions.obj) : error LNK2019: unresolved external symbol “__declspec(dllimport) unsigned int __cdecl physx::Cm::addToStringTable(class physx::shdfnd::Array<char,class physx::shdfnd::ReflectionAllocator > &,char const *)” (_imp?addToStringTable@Cm@physx@@YAIAAV?$Array@DV?$ReflectionAllocator@D@shdfnd@physx@@@shdfnd@2@PBD@Z) referenced in function _PxDumpMetaDataI don’t see this error if I link the release version of the lib.  PhysX3Extensions.lib as similar pproblems.I’m trying to get the visual debugger to work and currently: NULL == g_Physics->getPvdConnectionManager()returns null.  I assume that’s because the libs are release and not debug?Hi,you need to compile the PhysXExtension with VS 2012 in the debug version.\\nWhen you are using PhysXVehicle or PhysXCharacterKinematic libs,\\nyou also need to compile them with VS 2012 in the debug version\\nand link against the new debug libs.The SLN is somewhere in the folder, I dont use PhysX 3.2.4 because I switched to 3.3.x,\\nmaybe the folder-tree are similiar.I found the SLN in PhysX\\\\Source\\\\compiler\\\\vc10win32.Powered by Discourse, best viewed with JavaScript enabled',\n",
       " 'memory-leak-physx-3-3-1': 'My memory tracking system detects memory leak during the following scenario:Interesting observation: if the number of shapes is less than 5, everything is normal.I’m using PhysX 3.3.1Many thanks for your repro.  This is indeed a bug.We are working on a fix for 3.3.2.In the meantime, a workaround might be to call PxRigidDynamic::release() after fetchResults.Thanks,Gordonjust small question is there any ETA on the 3.3.2 release? (due to the fix)Powered by Discourse, best viewed with JavaScript enabled',\n",
       " 'denoiser-denoiser-initialization-fails': 'Hi,\\nI have a customer which use a GeForce GTX660 with drivers 452.06 . With this setup, the following happens:Of course; I don’t reproduce this behavior on any other computer/setup. Can you help me understand and fix this ? What can I test ?Regards,\\nColin ChargyA GeForce GTX 660 board is based on the Kepler GPU architecture.\\nKepler GPUs are not supported anymore since OptiX SDK 6.0.0.Please refer to the OptiX Release Notes for more information about supported GPUs, OSes, and CUDA toolkits.\\n(Link always directly below the OptiX SDK download button.)OK, thanks!\\nMaybe a specific return code would help a lot (unsupported hardware) instead of a generic error.\\nRegards,\\nColin ChargyAs a workaround I would query the CUDA streaming multi-processor version and reject devices below the minimum supported version.CUDE Runtime API:\\nhttps://github.com/NVIDIA/OptiX_Apps/blob/master/apps/intro_runtime/src/Application.cpp#L515CUDA Driver API:\\nhttps://github.com/NVIDIA/OptiX_Apps/blob/master/apps/rtigo3/src/Device.cpp#L350\\nCU_DEVICE_ATTRIBUTE_COMPUTE_CAPABILITY_MAJOR  and CU_DEVICE_ATTRIBUTE_COMPUTE_CAPABILITY_MINOROK thanks for your fast feedback!\\nRegards,\\nColin ChargyPowered by Discourse, best viewed with JavaScript enabled',\n",
       " 'can-i-connect-two-rtx-4000-sff-cards-is-sli-nvlink-supported': 'I have brought two Nvidia RTX 4000 ada SFF Chips to work on a compute-intensive project. Is it possible to connect both to work as a single unit or something near, I have heard that both SLI and NVLink are stopped for RTX 4000 series, Is there any alternative or is there any way to connect to the mother board? Also Is it possible to chip in two cards as both are quite big in size?Hi, no, this is not possible. IIRC 4000 claass GPUs never supported NVlink. SLi was an outdated, Scanoutbuffer-only connection between 2 GPUs, NVlink is a much faster, and generally usable link between 2 GPUs. we feature that on our compute focussd highest end chips, and used to on the high end gfx chips like RTX6000/8000 or RTX A6000. but no more on RTX 6000 ada. And explicitly not on your RTX 4000SFF Ada.\\nIt would not make 2 GPUs just look like one, but only allow for fast transfers of any data between 2 GPUs, so could help overcome limits of when a single GPUs framebuffer was too small for the job.\\nBut there always is PCIe, and its gotten faster since we invented SLi as well as NVlink. so you can always copy data between 2 GPUs via PCIe, gen4 with the RTX 4000SFF ada. If you are not limited by the size of framebuffer of a single GPU (in which case going for a single bigger GPU would have been the way to go really), you should seek to parallelize your compute job, so both GPUs can contribute to the compute, with full dataset in each framebuffer, rather than thinking NVlink would make it ‘a single GPU’.\\nHope this clarifies your questions? regards\\n-FrankPowered by Discourse, best viewed with JavaScript enabled',\n",
       " 'aabb-primitives-in-blas-getting-secretly-merged': 'So, I just initialized a BLAS with a list of 9 AABBs, each with size = 0.5. I always reportIntersectionKHR in the intersection shader, and in the closest-hit shader I output gl_PrimitiveID on the screen.\\nimage1222×1056 4.67 KB\\nNow, I increase the size of the AABBs to 0.8.\\nimage1362×1032 7.09 KB\\nAs shown in the image, two of my AABBs are merged together, and they share the same gl_PrimitiveID. If I move the second AABB a little bit, the two boxes get separated.Now, I do understand that the Vulkan Specification states that “In the case of AABB geometries, implementations may increase their size in an acceleration structure in order to mitigate precision issues. This may result in false positive intersections being reported to the application.”However, I do expect to receive accurate gl_PrimitiveID information in the intersection shader so that I can use that to index into my list of custom geometries and perform intersection tests.Are there any workarounds to avoid this issue?I ran into the same “issue” and had to do my own ray / AABB intersection test to eliminate false positives\\nThis tutorial on “implicit” objects helps in doing that : vk_raytracing_tutorial_KHR/intersection.png at master · nvpro-samples/vk_raytracing_tutorial_KHR · GitHubSee :The DirectX Raytracing (DXR) Functional Spec is more explicit about AABB handling:\\n“Implementations may replace the AABBs provided as input to an acceleration structure build with more or fewer AABBs (or other representation), with the only guarantee that the locations in space enclosed by the input AABBs are included in the acceleration structure.”See DirectX Raytracing (DXR) Functional Spec | DirectX-SpecsThanks for your response! Although it’s reasonable that some AABBs will be larger in size, when I put in an array of 9 AABBs, I do expect to see 9 primitives in the scene with 9 distinct gl_PrimitiveID.If the implementation is just going to merge some AABBs together, what would happen is that the intersection shader for some AABBs will be called with a gl_PrimitiveID inconsistent with what it actually is. I wouldn’t be able to do custom ray / AABB intersection test if I don’t even know which AABB it is.Turns out the merged primitive is still there. The driver just calls the intersection shader twice for the same merged AABB with different gl_PrimitiveID.This behavior needs to be in the Vulkan Specs. The Vulkan Specs did mention that the AABBs could be bigger, but it didn’t mention that the primitive can be merged and the intersection shader would be called twice.Powered by Discourse, best viewed with JavaScript enabled',\n",
       " 'rtxgi-is-great-rtxdi-is-junk-thats-not-a-real-thing': 'Built the RTX unreal build, GI and DI, loaded a scene and put in just a few hundred lights, got this.\\n1335×554 884 KB\\n\\nPerformance absolutely scaled with light countWhat, exactly, is this supposed to do if not this, exactly?0/10 would not use again, avoid seller, scam.Hello again,it is nice to see your interest in NVIDIA RTX technologies. I would kindly like to ask you to maybe spend a bit more time with the subject and the available SDKs, READMEs, Demos and tutorials before giving such a harsh judgement.There are a lot of games already on the market that are using both these features successfully and with great performance.The main use of RTXDI is as an external SDK and game specific adjustments, since it has to be tightly integrated with the render-engine of choice. Beside this RTXDI is also still in Beta for the Unreal 5 RTX branch.In your assessment above I would wish for a bit more information on your setup. Which UE version is this? Is it a release package of the project or debug? Is it the in-editor game render or standalone? What GPU are you using, which NVIDIA driver version, which compiler, etc. This makes it really hard to try and help you identfy why you might be seeing this low performance.Thank you!Hi Markus,Sorry I was really frustrated with the seeming lack of ability to communicate with Nvidia of this stuff. Seems like its just a github dump and a ‘you figure it out’. I did actually get this working in the end, so I wanna apologise for that. Its fairly heavy, though, but seems to work pretty well.Running on a 1080ti with latest drivers.One thing, if I may be so bold after being such a jerk, my understanding from the demos is that the tech will allow emissive surfaces to actually cast light. You can actually have light bulbs as geometry and do away with point lights in unreal. Is that correct? I haven’t been able to get that to work.Also its the latest Unreal 4 branch, .7 I think.Also, a lot of the other ray tech can reduce screen percentage of rays cast. Can RTXDI do this also?Hi again!No hard feelings, I rather see our technology facing heated emotions and productive discussions than facing indifference (I really need some emoticons here). And you are right, our github offerings can seem daunting and lacking in documentation. That is why we have the Devzone for more learning resources.All our raytracing solutions are based on the assumption that you are using an RTX generation GPU, which only started with the RTX20 series. So I am surprised you got this working on a 1080ti at all, since that generation only received limted support for RTX. But since they lack the dedicated RTX cores, significantly lower performance is expected.I am not one of the experts in RTXDI which means I cannot answer your question on emissive surfaces right away, but I will reach out to some people and see if I can get you a reply or a link to some example. The documentation does state that you can create mesh and triangle based lights, so i think the answer is yes, but I could not find an example so far either.Also, a lot of the other ray tech can reduce screen percentage of rays cast. Can RTXDI do this also?Do you mean some visibility/occlusion checks or light culling?  Part of that is implicit in the algorithm, but depending on the integration you will retain any functionality in visibility handling based on your original render engine. I am not aware that you can limit the screen space area where ray tracing is applied, which would not make much sense in any case.Can you point me to some example of “the other ray tech” you refer to here?Thanks!Thanks for the reply, and that would all be much appreciated!Im not 100% sure how the DI algorithm actually works so I might be talking nonsense, but ray reflections for instance can be reeled in a little bit with something like r.raytracing.reflection.screenpercentage=[number] console command. (That might not be 100% word for word correct I dont have it opened right now)This significantly improves performance without a particularly noticeable visual downgrade that with the help of art style is quite acceptable for the 1080ti. You cant go below 50% however without a very strange jumping or jittering visual bugThe same trick can be used with shadows.I was wondering if there is a similar optimisation to be found with RTXDIRegardless, yes please do let me know about geometry/texture emission because as you say the documentation mentions it and so do your demos, very proud of millions of emissive polys, so I assume unreal must also be able to do this. It might be some difference in rendering or whatnot, though.Hey any update on this?Hi captain_keys_343,in fact there s, yes. For one thing it seems that emissive triangles are not yet supported on UE4. The people I asked are very certain it has been listed as a “possible future feature”, but it might not have been completely clear in all our supplied demos or tutorials.Beside that one of the engineers will join the discussion on this soon, depending a bit on work priorities.Thanks!Please thank them for their time for me!Will do.Thank you!Hi captain_keys_343,I’m the maintainer for the RTXDI support in our UE branches. Markus tracked me down to try to help you out.First, on the emissive surfaces, that is a general feature of RTXDI, as you mentioned. However, the implementation in the SDK has a few requirements that UE doesn’t meet. This forces us to use a different approach, and it has been in the backlog for a while. Unfortunately, it has been behind other priorities. The way UE materials drive emission is the key complicating factor here. Additionally, we know that using emissive surfaces as primary lights is a non-trivial impact on the art pipeline, so we tend to prioritize things someone can just turn on higher. We try to be clear about the fact that emissive surfaces are not yet supported in communications around RTXDI support in UE.Now, on the performance side, I think I might need to see your project to understand better. There are a few things that could be the issue here:The last item is both a challenge and a strength of RTXDI. Traditionally, you have pretty limited ability to scale shadowed light costs. RTXDI allows for a much more continuous tradeoff of quality versus performance. However, picking the right balance of settings becomes more complex.On the scalability question of reduced resolution, RTXDI doesn’t really work well with straight screen percentage techniques like reflections or AO. You’ll notice that the ray tracing shadows in Unreal also lack this, because it just doesn’t work that well. The baseline RTXDI SDK offers checkerboard scaling, and we intend to enable this in UE as well. We’ve just been focused on other issues, but it is something I’d like to complete soon. There are some complications in the way UE dithers and checkerboards other effects that complicates this. We’ve encountered a few other case where temporal effects can interact in a “beat” pattern that causes flashing, so there is some additional validation effort required here.Anyway, thanks for reaching out. We really want to improve this feature as we feel it offers a lot to those that want to scale up to more complex lighting environments, and I’d happily take more of your concerns.-EvanThanks for your time, Evan!Okay so for #2 you mentioned that point lights should be pure point lights for best performance? With no source radius?Otherwise, yeah I was wondering because Unreals emissive shader is a bit different if all that would be the case.Its interesting that you feel that using emissive textures would be harder on artists in terms of scene lighting. I would find the absolute opposite. But I understand its not exactly like just tapping a checkbox haha.More settings for RTXDI would be very welcomed. The overhead is totally worth the trade, even so, as you say. I was just hoping there might be a quick hidden little switch that might have got a few extra ms, its really overoptimising at this point, but its good to know this stuff ahead of time so if I need it later I have that knowledge in the toolbelt.I really appreciate your time!To be clear, pure point lights are more efficient without RTXDI. With RTXDI, they are little if any faster. So, if you’re comparing point lights in normal UE vs RTXDI, you’ll see less advantage to RTXDI. Where you’d possibly skip the area to save cost in normal UE, RTXDI will allow you to set a source radius for virtually zero cost.The art challenge is a subtle one. If you are authoring straight for RTXDI, it is not really a challenge. However, if you’re authoring to run across a lot of HW, you typically will have proxy lights surrounded by emissive meshes. For this case, RTXDI with mesh lighting will end up doubling your lighting, if it samples both the mesh and the proxy. My goal has been to allow people to load scenes that they already have, and have them ‘just work’ even if not faster. For instance, you can turn on RTXDI in the old Infiltrator scene, and it’ll run with something like 700-1500 shadowing lights with RTXDI enabled.FWIW, I don’t think we’ve added any “new” settings for RTXDI other than the scalability options. Those actually drive the existing options. (r.RayTracing.SampledLighting.XXX)The top ones for getting some quick perf (with possibly lower fidelity) are:Keep in mind they will only help if you aren’t software bound.-EvanUhm, Evan.Okay so i’ve got an RTXGI volume attached to a player, set to infinite scroll, works great… unless I change verticalityAny stairs cause probes to momentarily, like even just a few frames, decide they should sleep, which seems to purge them of lighting information, and they then have to rebuild it.Basically, all staircases flicker black awfully for no reason.Is there a fix for this or a way to disable this behavior?-BenI’m sorry, I’m an expert on RTXDI, and while I understand the generalities of RTXGI, I’m not an expert. I’m passing your question to our experts on the UE RTXGI plugin.Hi @captain_keys_343,I’m the lead for RTXGI. Evan pointed me to your question, so I’ll try to help.First, I’m sorry you are having troubles with RTXGI in UE. At the moment, the RTXGI UE4 plugin has some issues, one of which you’ve run into. If you are using the binary plugin (from the marketplace) it is not possible to disable probe classification - the feature that decides when probes should sleep or be active. Something to try: disable probe relocation (this is an option on the DDGIVolume in the binary plugin). Relocation may be causing probes to reposition in a way that triggers classification to do undesired things.If that doesn’t help, you can switch to the source code version of the plugin and disable classification by changing the code. I know this isn’t an ideal solution (particularly if you are not a programmer) and we’ll be working to improve the plugin’s options in the future.Hope that helps,\\nAdamP.S. Nice Halo: CE reference in your user name!Thanks, thats awesome you noticed. I’ve had this email since 2001. My only annoyance is that its actually Captain Keyes, with the extra e. Sad haha.I’m using the RTX build github branch. Probe relocation wasn’t it, good suggestion though. I even tried moving the volume in blueprints only once a second, rather than having it attached to anything. It just hates moving vertically I guess.This tech is really fantastic generally, you and your team are doing really, really impressive stuff. So thanks for all that hard work, im blown away by the promise of all this.That said haha,. yeah thats definitely… suboptimal. I guess its pretty state of the art but these issues are pretty significant.The ability to disable that behavior, or get the probes more temporally stable with their shutdown would be really useful. Pretty rough to get a usecase out of this at this stage, I have to say :(Okay, two more thoughts:From:To:and then compile the engine. Probe classification will be disabled.Place another DDGIVolume in the scene that covers the problem area (stairs). Volumes are sorted by lighting priority, then probe density, and the volume with the lowest priority value and/or highest probe density is chosen. So, either set the new volume’s lighting priority to a lower value than the lighting priority value of the player’s ISV or give the placed volume a higher density of probes than the player’s ISV.With this setup, the stairs will not receive indirect light from the player’s ISV, so the classification problems will not cause visual issues.Not ideal, I know. But, a workaround for now.Yeah im using multiple volumes as gi lods basically, haha, so I can try flip the bool if that doesnt totally break multiple volumes, you seem to be saying it will.The 2nd option is a good one, but the world is dynamic and generated from tiles or other instructions, so I cant precalculate anything. ugh, such pain hahah.Its awesome you came back with that stuff, really, thank you for trying!Okay, since your world’s geometry isn’t static or predictable that does make it tougher to use the second solution. Swapping the bool won’t break multiple volumes, it will just disable classification globally. This means all probes (in all volumes) will trace rays, which will be more costly. Disabling classification on the ISVs should do the trick for your use case though, with the tradeoff of a higher performance cost for visual consistency.Hope that gets you unblocked!Powered by Discourse, best viewed with JavaScript enabled',\n",
       " 'draw-pbo-into-the-screen-performance': 'Hi,I’m currently using a PBO to draw into (from a CUDA kernel).Once done I draw it the following way, but it use 20% of my CPU !!!I’m looking for a more efficient way to render my PBO ?I have also try the following, but nothing appear :Does someone have an idea to solve this problem ?ThxThe way with the texture should be the fast path.\\nPossible issues in your code:I use something like this to display a tonemapped HDR buffer:Thanks a lot,I have try your code (without the glUseProgram…) and have the same effect :-(BTW:I still not facing why it does not work !ThanksI have finally a “white” zone… which should correspond to my “Quad”. Bit it is “white” and does not contains the colors of the PBO.Any idea ?In your initialization code you did not bind the texture before setting its TexParameters. Those are per texture object and you have left your texture object on default filter modes which have a min-filter of nearest_mipmap_linear, so need mipmaps to work and if you don’t provide any, the texture image is incomplete and the texture unit it’s bound to will be switched off.Should have been:Great,I have forgot to bind my texture just after creation ! Stupid bug :-PWhy do you use a shader to render the texture ?Just because my code was taken from a program rendering a high dynamic range image into an RGBA32F texture which needed to be tone-mapped for the final display and that’s done in the GLSL shader as a post-process. (I didn’t want to introduce errors when posting changed code without running it.)Ahhh, I seeThanks for your answer…So, this approach works but still use 25% of my CPU :-(In fact I render from an OpenCL kernel, on multiples GPUs. Then I would like to average each image (one image per GPU) and display it. Even without “averaging” the CPU is used a lot !I have try several approaches:But no way… this continue to use 25% of my CPU :-(I have just see the following thread :Hello. When I seperate render in different thread with VSync enabled this thread consumes 100% CPU core while SwapBuffers waiting for sync. Everything works just fine with VSync disabled or in single thread (0 - 1% CPU). Is this way it meant to be or...Is it possible that the CPU use is due to the fact I “swap” in another thread ?Nobody has an idea to help me ?Thanks a lotVery good question, added to my subscriptions.Powered by Discourse, best viewed with JavaScript enabled',\n",
       " 'long-long-black-screen-then-randomly-works': 'Started encountering a new problem with CloudXR. Instead of instantly connecting to a machine via Meta Quest, it will hang on the black screen or loading circle for anywhere between 3 and 15 minutes, then all of the sudden it will connect. The log doesn’t give any useful data… just that it was unable to establish a connection, then all of the sudden it was. Any thoughts on what could be causing this would be much appreciated.that sounds awfully strange, multiple watchdogs should kick in and drop the connection after maybe 20-30s.Would need to see client and server logs of a run, maybe they give a hint what’s happening under the hood.Powered by Discourse, best viewed with JavaScript enabled',\n",
       " 'exceptions-when-using-occlusion-rays': 'Hi,I’m trying to integrate Optix into our tools pipeline to calculate per vertex AO.\\nI’m using Optix 4.0 with Cuda 7.5.However, I’m running into a couple of issues.The most serious is that Optix throws an exception when I use any decent number of rays (>4).\\nI can usually get more rays by increasing the context stack size from the default 5120.\\nThe exception I get is:Pretty much the same shader in the samples (sample6 set in AO mode) can sustain a greater number of occlusion rays. I have pushed it up to 1024 (32x32) in the sample without any crashes.\\nNote that sample6 uses a much smaller stack size (1180 I think).The second problem is that rays are going through highly tessellated walls in my test mesh, resulting in pseudo random patterns of blue dots in the walls (I say pseudo random because they look random but are always in the same place for a given camera view).I know this isn’t much information to go on, but hopefully these problems are known and you can just point me at what I am doing wrong…Thanks!MartinI managed to get this (mostly) working.\\nI had used a NoAccel acceleration structure for a 10000 vertex mesh.\\nPutting a MedianBvh accelerator on it allowed me to go to at least 256 occlusion rays.The blue dots persist, though, where my rays are going through the geometry.I would recommend to crosscheck with the stable OptiX 3.9.0 as well.Rays missing triangles can simply happen due to the non-watertight default triangle intersection routines.\\nIn that case your blue dots would appear on the edges of triangles.If it’s not that, you might have some self-intersection problem. Hard to say without a reproducer.If you’re under Windows you might also not want to exceed the number of rays per launch to get over the WDDM timeout of 2 seconds for kernel drivers which will lead to a display driver restart due to Windows timeout detection and recovery (TDR).In case that is happening, there isn’t really a need to shoot many ambient occlusion rays per launch.\\nA progressive implementation approach for ambient occlusion is really simple. I have one in about 100-150 lines of CUDA code overall. I can post that when I’m back in the office.Looks like this when putting raygeneration, closesthit and anyhit and some of the headers into one file.\\nNot tested this way, I recommend to separate them again.Launch this multiple times with increasing sys_IterationIndex starting at 0 to get an arbitrarily nice ambient occlusion result:Mind that above is just rendering with ambient occlusion.\\nIf you want to bake it, you’d just need to change the raygeneration program to start from uniformly distributed sample positions on your geometry and shoot ambient occlusion rays into the upper hemisphere.\\nThat is, move the code from the closesthit program into the raygeneration program and use only one raytype, the ambient occlusion (shadow) ray with the simple anyhit program. Can’t get any simpler than that.When using the shading normal to define the upper hemisphere, make sure that the rays do not penetrate the geometry by checking against the geometric normal as well. This will happen with coarse geometry and smoothed shading normals or bump maps.What you do with the results is your responsibility.\\nhttps://github.com/nvpro-samples/optix_prime_baking contains some vertex baking algorithms.Thanks Detlef,For reference, what I am doing is this>For each face in the primitive I assign a 16x16 block of virtual pixels (virtual because they never get rendered out). The u,v co-ordinates in the upper left of this block are the barycentric co-ords. I mirror them for the bottom right.For each pixel I shoot out 16 occlusion rays. I use a Hammersley distribution onto the hemisphere cosine function to get the ray direction. I also use the pixel index (0-255) to set up another Hammersley. I then combine these to get a jitter per pixel.Once the pixel is evaluated, I use atomicAdd to sum the results back to the triangle vertices, using the barycentric factors as weights.A simple post shader then normalizes the AO to a unit weight.For stability, I had to limit my launches to 512x512, (ie 32x32 triangles).As for the blue dots, I think this was because the triangles were axis aligned and coincident with the edge of the bounds for the mesh.I’ve not seen them on a more realistic mesh.MartinOk, a progressive method wouldn’t run into the timeout limits with the 16 occlusion rays. You could shoot one or more occlusion rays each launch and have as many launches as you like for arbitrarily good AO quality, or what’s left of it when storing it per vertex in the end.Or you use an NVIDIA Tesla board as dedicated compute GPU which wouldn’t be affected by timeouts. The limit would only be the memory. No OpenGL or D3D interop then though, it’s running a different driver model (TCC).When doing this progressively, the resulting vertex color could also be calculated with a separate ray generation entry point once after all AO results per sample point have been calculated. It’s basically just some CUDA kernel when not calling rtTrace. That would eliminate the atomicAdd which should improve performance and wouldn’t have worked on multi-GPU. That algorithm would gather instead of scatter.I would also recommend a uniform sampling across the whole model, taking the triangle area into account. Your method might miss occlusions depending on the relative size of the triangles.Depending on the complexity of your scene it might even be possible to generate the whole ambient occlusion results for all triangles in the scene at once with that approach.\\nYou need at least one sample point per triangle (the github example uses three), the rest can be sampled uniformly with a number of sample points you define for the whole scene.Using a low discrepancy sampler like you do with Hammersley will work better for the sample point and ray direction sampling than the LCG in my example code.Powered by Discourse, best viewed with JavaScript enabled',\n",
       " 'unity-shader-issue': 'Ok so I’m creating a grid based map mesh and coloring the vertexes based on what terrain each tile will be; then a texture array is used to map textures to those colors. (As i understand the process) so the issue is the small quad area inbetween 4 tiles. I’m attempting to blend the 4 surrounding colors, the splat map should be doing a channel on R,G,B,A and each of those values carries a value of 1 for the vertext/splat mapping.I wind up with this:https://imgur.com/a/ybS7ZvT It displays 4 textures from the texture array, but the one spot I try to blend 4 colors refuses to show anything but a single texture.Here’s the shader code I’ve worked up to this point. Am I just missing something simple or is this a more complex problem?Powered by Discourse, best viewed with JavaScript enabled',\n",
       " 'mingw-libraries': 'Hello,I tried to compile PhysX with MinGW compiler on Windows.\\nThe problem is that the ABI of MSVC (the compiler of the libraries) is incompatible with MinGW.But I really want to use MinGW as compiler.So can anyone help me e.g. with custom library files or anything else?-JanneHi,Why are you opposed to using Visual Studio?  Are you compelled to use MinGW for some reason? I’ll put in a request to consider support for MinGW in future releases, but I don’t know how much traction it will get, because MinGW is not widely used by commercial game developers.Hey,PhysX is a cross platform library, creating cross platform projects would be easier with a cross platform compiler like MinGW/GCC. It would also allow us to use cross platform development environments for example Code::Blocks. I would appreciate it if you would provide MinGW build as well in your release.VáclavHey,MinGW would be very great because I often use the compiler in command line and the syntax of MinGW is much easier. As Václav  said, MinGW is very useful for cross-platform development and I think that the trend of cross-plaform development will rise.-JanneHello,Even though MinGW might be less commonly used in “commercial environments”, it’s also worth noting that there are numerous instances where MinGW’s compiler is more efficient in terms of compile time code checking and as a result helping with bug fixing and development - and it is more easily accessible, as it is completely free, and the programs involved require less resources to use.Overall, being limited to Visual Studio has its own drawbacks, and supporting different environments can make PhysX more accessible to more people in general.-ZhHey,in future releasesIs there a estimated date for this?-JanneMy final decidition:\\nI’m going to start integrating the Physics in two weeks. If I can use PhysX, I will use it.\\nOtherwise, there are enough Physics engines in this World.Otherwise, there are enough Physics engines in this World\\nUltimatum ! That will show them :)Sorry. I was very angry and if I think about it, take your time and do it perfect.\\nSorry again…Hi,I apologize that you have become angry with our processes, but I’m afraid that it takes time to make changes such as your request, and we have a lot of other work happening right now that cannot be de-prioritized. The next version, 3.3, is in a closed beta right now, and we are anticipating an open beta in the next few weeks. Most of our efforts are focused on stabilizing 3.3, improving performance in critical areas, fixing defects in the stable version 3.2.x, etc.  Regardless of whether MinGW/gcc is useful for multiple platforms, I’m certain that we cannot replace all compilers on all platforms with it, so we can’t make the priority very high compared to other support issues. I will try to come up with a better estimate for delivery, but at this point I would say we won’t support it within the next 3 months.Thanks,\\nMikePowered by Discourse, best viewed with JavaScript enabled',\n",
       " 'cloudxr-3-0-crash-in-apppaused-method-quest-2-is-this-one-of-the-bugs-that-was-fixed-in-3-1-or-3-1-1': 'Hi Nvidia, when I leave the play area and re-enter it multiple times in CloudXR 3.0 on Quest 2, triggering AppPaused / AppResumed, after a while (an indeterminate number of times), the app crashes inside AppPaused.In the logcat I see these lines in the CloudXR library:V/CloudXR: Failed nvstSetRuntimeParam: State transition to State-2 is invalid for FrameNum 303.\\nV/CloudXR: Eye1 processing thread shutdown.V/CloudXR: Failed nvstSetRuntimeParam: State transition to State-2 is invalid for FrameNum 303.\\nV/CloudXR: Eye0 processing thread shutdown.…\\nA/libc: Fatal signal 11 (SIGSEGV), code 1 (SEGV_MAPERR), fault addr 0x1000f in tid 18075 (main), pid 18036I’m just curious if this is one of the stability fixes you already solved in 3.1 or 3.1.1.Thanks!Hello Belakampis1,Can I get a bit more information: Can you clarify very precisely what ‘leave the play area’ means.  Exact steps.   And does it happen within SOME maximum count, some period of time: one hour, over the course of one day, over the course of many days?Thanks,\\nGregHi Greg, the crash occurs after stepping out of the Guardian boundary, upon re-entering it. Only thing, it doesn’t happen every time, but sometimes may take 5 enter/exits in a row. This is a problem for us. It’s all within a single session and I can make it crash reliably (every time), but only after a seemingly random amount of exiting / entering the play area.However, to avoid wasting anyone’s time, I will first try to repro this in your basic CloudXR 3.1.1 sample for Quest and get back to you. I see a lot of changes to the state machine and it may be resolved already.Powered by Discourse, best viewed with JavaScript enabled',\n",
       " 'is-raygen-function-necessary-to-create-a-module': 'Hello, every one!I am trying to build multiple modules with multiple .ptx files with Optix 7.1, because I want to split my code into several .cu files for better code management.And I noticed that if there is no Preformatted text __raygen__ function in the .cu file (which is compiled into a .ptx file later), optixModuleCreateFromPTX() will return an 7200 error.I tried this because I want to move my __raygen__ function into a separate file. Currently I solve this problem by adding a dummy __raygen__ function into .cu files which don’t have one.So my question is:\\nIs there any better way to avoid the 7200 error rather than create dummy functions?What is the purpose to test if there is a __raygen__ function in .ptx file, while you are able to create multiple modules and choose functions from any of them?Thank you.It’s not exactly a __raygen__ which is required, but yes, OptiX does not handle modules without at least one of the program domains defined inside each module.\\nPlease see this thread for a similar workaround using a unique dummy __direct_callable__\\nhttps://forums.developer.nvidia.com/t/optix-7-equivalent-of-rtprogramcreatefromptxfiles/129321/8I’m also using multiple modules in my examples, but they are grouped into the main program domains, so that there is alway at least one of the semantic programs inside each module:\\nhttps://github.com/NVIDIA/OptiX_Apps/blob/master/apps/rtigo3/src/Device.cpp#L528For performance reasons I do not split regular functions into separate modules which are then linked.\\nOptiX wants code to be inlined as much as possible. Because of that, I define functions used in multiple modules as __forceinline__ __host__ __device__ inside headers.\\nExamples here:\\nhttps://github.com/NVIDIA/OptiX_Apps/blob/master/apps/rtigo3/shaders/vector_math.h\\nhttps://github.com/NVIDIA/OptiX_Apps/blob/master/apps/rtigo3/shaders/shader_common.hOptiX does not handle modules without at least one of the program domains defined inside each module.Sorry but can you explain what is a program domain?but they are grouped into the main program domains,And how to group modules into one domain?With program domains I just mean the OptiX program types in the table of this chapter:\\nhttps://raytracing-docs.nvidia.com/optix7/guide/index.html#program_pipeline_creation#program-inputIn the Device.cpp code link above  you can see that I have raygen, miss, exception, closest hit and any hit programs as well as different direct callable programs in different PTX files. That’s all.\\nNone of these hit your initial issue because there is always at last one of the program types inside each PTX with this code structure.\\nThat partitioning into separate files isn’t necessary and you could also put everything into one source file.\\nIt’s simply a matter of balancing compile times during development and initial startup times during runtime.Since OptiX caches the internally assembled code and CUDA does the same with the final microcode, it’s only the initial startup of the application where the pipeline creation path is slow.\\nYou can see that when enabling the OptixDeviceContextOptions callback at level 4, which will print out the program cache misses and hits.Find more information about both of these topics in this chapter of the programming guide:\\nhttps://raytracing-docs.nvidia.com/optix7/guide/index.html#context#contextThank you so much for your replies.In fact I find my case is kind of complex after I did some experiments .Before the separation, my only .cu file contained every Optix programs like this:\\nhttps://github.com/gzfrozen/EM_tracing/blob/master/src/cuda/devicePrograms.cu#L73Then I tried to separate the .cu file, first step by only pick out the __raygen__ program, separate it into environmentRender.cu and phaseDetection.cu ( rayLaunch.cu is not been used by now) as following github branch:error_case/src/cudaContribute to gzfrozen/EM_tracing development by creating an account on GitHub.However in this case, despite the original big .cu file (environmentRender.cu) still contains ton’s of Optix programs, an extra dummy __raygen__ program is needed or the module can not be created and return this:[ 2][COMPILE FEEDBACK]: COMPILE ERROR: Invalid PTX input: ptx2llvm-module-001: error: Failed to parse input PTX string\\nptx2llvm-module-001, line 9; warning : Unsupported .version 6.5; current version is ‘6.4’\\nptx2llvm-module-001, line 460; fatal   : Parsing error near ‘.version 6.5’: syntax error\\nCannot parse input PTX stringOptix call (optixModuleCreateFromPTX(optixContext, &moduleCompileOptions, &pipelineCompileOptions, ptx.c_str(), ptx.size(), log, &sizeof_log, &m)) failed with code 7200 (line 377)Magical things happened after I tried further separation into 3 .cu files environmentRender.cu closestHit.cu rayLaunch.cu in this branch:phase_distribution/src/cudaContribute to gzfrozen/EM_tracing development by creating an account on GitHub.Suddenly the dummy __raygen__ is no longer needed and can be safely commented out.So I guess there might be a bug in module compiling?Anyway, thank you again for giving me the advice to separate .cu files. I think I will try to separate the .cu files by program types like this example you have shown me:\\nhttps://github.com/NVIDIA/OptiX_Apps/blob/master/apps/rtigo3/src/Device.cpp#L528Powered by Discourse, best viewed with JavaScript enabled',\n",
       " 'vkgetdeviceprocaddr-and-vkdestroysurfacekhr': 'I don’t know why but for some reason vkGetDeviceProcAddr returns NULL only for vkDestroySurfaceKHR. Is this intended? It feels like a bug because the vkGetInstanceProcAddr returns non-NULL and the other functions from KHR_surface extension are there. I’m using a GeForce 780 GTX with the driver version 368.39 and Windows 10 64bits. Thanks in advance for any info.vkDestroySurfaceKHR is an instance-level function and vkGetInstanceProcAddr should be used for its retrieval.From Valid Usage for vkGetDeviceProcAddr (https://www.khronos.org/registry/vulkan/specs/1.0-wsi_extensions/xhtml/vkspec.html#vkGetDeviceProcAddr):pName must be the name of a supported command that has a first parameter of type VkDevice, VkQueue or VkCommandBuffer, either in the core API or an enabled extensionI’ve read that in the spec but I was confused because the only function I couldn’t get the addr was vkDestroySurfaceKHR and also I was checking this library (GitHub - vallentin/vkel: Simple Dynamic Vulkan Extension Loader) which seems to load every function using vkGetDeviceProcAddr at some time(with vkelDeviceInit at vkel.c).So either vkGetDeviceProcAddr is returning the address for functions it shouldn’t or it should load vkDestroySurfaceKHR aswell.Powered by Discourse, best viewed with JavaScript enabled',\n",
       " 'simulation-stability-at-different-scale': 'Hello,\\nI have a problem dealing with different scales of the same scene.My first scene is configured with a ragdoll of approximately 2m tall, falling from 20m high. Works well and is quite stable once on the ground, as seen in this video:Dropbox is a free service that lets you bring your photos, docs, and videos anywhere and share them easily. Never email yourself a file again!Second test is the same scene, but scaled in centimeters (the ragdoll is 200 units tall, the gravity is -980). The falling parts is ok for me (little bit different, but oK), the falling speed seems Ok (both hit the ground at the same approximate time), but once on the ground, the ragdoll is totally unstable, as seen here:Dropbox is a free service that lets you bring your photos, docs, and videos anywhere and share them easily. Never email yourself a file again!I tried configuring the PxTolerancesScale.length to 100 and the PxTolerancesScale.speed to 10*100, but it didn’t change my result that much, and it’s still unstable (the previous video is with these settings).I saw in the documentation that my bounceThreshold should also be changed.\\nBut the only bounceThreshold I found about in the documentation was the one in PxJointLimitParameters, and when I tried to change it, I realized the default value was zero (and so is also zero in my first video).Is there anything I missed to change units of a simulation ? SnippetVehicleScale and SnippetToleranceScale do not seem to do anything more than I did…Well, I went deeper into this, and I think I get the solution. It seems the problem was a mix of several things:It’s now working quite fine, I hope this subject can help someone someday if they face the same problem :pPowered by Discourse, best viewed with JavaScript enabled',\n",
       " 'optix-4-1-1-released': 'Please visit [url]https://developer.nvidia.com/optix[/url] for more information and the download link for the new OptiX 4.1.1 and previous releases.Improvements to 4.1.1If you’re setting up a new development environment for OptiX, please find the list of prerequisites and supported combinations of host compilers and CUDA Toolkits inside the OptiX Release Notes on the download site.Powered by Discourse, best viewed with JavaScript enabled',\n",
       " 'quadro-m2000m-nvenc-session-limit': 'I have not been able to find any information about NVENC session limit of mobile workstation cards. I am about to purchase several of them, but I need to know if there is a session limit for M2000M cards or not when using NVENC. Could anyone shine some light on this?M2000M has no limit on encode sessions.What are the encode limits session for :P400\\nP600\\nP1000\\nP2000??Impossible to find this information and everybody asking for…ThanksAll you mentioned except P2000 are limited to 2 sessions.A general rule of thumb is that all Quadro’s below X2000 (X = K, M, P) and all GeForce cards have limited encode sessions.We will document this information (it used to be there, but looks like it got removed in a recent update of the site). Thanks for bringing it to our attention.Iam searching information GPU on Laptop support NVENC but i don’t see anything, I try laptop use GPU : Quadro K2100M; K1100M, Geforce GTX 1050 … but not worrking. Can you sent me information GPU on laptop support NVENC, please ? Thanks.Powered by Discourse, best viewed with JavaScript enabled',\n",
       " 'markov-chain-monte-carlo-post-processing': 'Hi,I have tried the mcmc of the optix examples wit my own obj files. It seems that regardless of how long I leave the simulation running the image remains noisy and does never completely converge to a clear image.I have tried even with tripling the image resolution and then to resize the image to its original size but the results still appear quite noisy.Which possibilities are there to either improve the convergence to a better image or to filter the image afterwards to remove noise?Many thanks,\\nBernhardThe way mcmc works is that it creates a predetermined number of samples.  Running it longer only moves the samples about.If you want a less noisy image, you will need to increase the number of samples.  There are different ways to go about this.  You could change the initial number of samples, you could render the same image multiple times and then combine them (make sure to use different seeds), you could also do what you tried and render a larger image and down sample it.There could also be problems where the chain gets stuck in a local minima.  I’m not sure what to do about that.  You should probably check out the literature.Another issue might be related to tone mapping.  If you have a few bright samples they can blow out a pixel.  A tone mapping pass will help with that.Powered by Discourse, best viewed with JavaScript enabled',\n",
       " 'vulcan-cannot-launch-on-geforce-gt-630': 'I was trying to test Vulkan in The Talos Principle game, but it was still crashing. I wondered if it’s a game error, or something wrong with the Vulkan itself, and it seems like it is that second option.When launching “vulkaninfo”, I get “VK_ERROR_INITIALIZATION_FAILED” error.I was testing it on 356.39 drivers, which are the only one on the computer right now (I made sure it is a clean install, and before installing them, removed all previous versions).What might be the problem?Hi.There are two models of the GT630 that are Fermi based ([url]http://www.geforce.com/hardware/desktop-gpus/geforce-gt-630/specifications[/url]).I don’t know what model do you have but if you have one of that Fermi based (GT 440 rebranded) you don’t have Vulkan support at least for now.Hope that nvidia keep his promise of SIGGRAPH in last August and delivery Vulkan support for Fermi.Powered by Discourse, best viewed with JavaScript enabled',\n",
       " 'using-rtx-acceleration-for-voxel-tracing': 'Hi. I’ve written a simple voxel ray tracer that renders cubes instead of triangles. To do this, I have a bounding box program that’s self explanatory and an intersection program that uses the slabs algorithm to calculate t where the ray intersects the voxel. It performs super well, but obviously it’s a duplicate ray-aabb intersection test and doesn’t run on the RT cores. Does OptiX provide a better way to do this so I can take advantage of the RTX acceleration? Ideally the intersection just uses the results from the hardware’s aabb testing.Thanks!Hi @Lin20,As long as you are using RTX hardware and OptiX 6, you do get RTX acceleration in the sense that BVH traversal is being done in hardware. In order to also accelerate the primitive intersection, the way to do that right now is to use the OptiX triangle API. That would mean meshing your visible voxel walls.Meshing voxels might be feasible and reasonably fast if you have the memory for it and if you can build your triangle mesh in a CUDA kernel. You can estimate the upper bound of memory needed for voxel meshing: if you were to do the easiest thing and mesh each visible voxel individually but take some care to only have 1 wall between voxels and share vertices, then I think you could average just slightly over 1 vertex per voxel, and indices would average just over 6 triangles per voxel. If my napkin math is right, that would be roughly 34+64 = 36 bytes per visible voxel. If needed, you’d be able to do simple math in your closest hit shader to recover which voxel you intersect by looking at the hit point and ray direction.Aside from that, OptiX doesn’t provide a way to return a hit point from the leaf BVH node. You are already doing it probably the most reasonable way that’s available without meshing. The main alternative approach that people use for voxel rendering is a voxel marching approach using pure CUDA.–\\nDavid.Thank you very much for the info! It’s greatly appreciated. Experimenting with meshing, or even some sort of hybrid where a triangle representation of the scene is used for the bounces, might be worth it. How funny, since it’s typically voxels accelerating triangle tracing (eg. VXGI).I do hope that one day OptiX - and other ray tracing APIs for that matter - provide targeted support for more than just triangles. Fingers crossed!Powered by Discourse, best viewed with JavaScript enabled',\n",
       " 'ptx-compilation-overhead': 'Hi,In our application we are having this issue where in ptxjitcompiler and takes a significant chunk of time which slows down the startup of our application. I see some varied observations with regards to platform and the graphics card in use that I am unable to explain or fix.Linux :\\nWe have two system one with RTX 2080TI and one with Titan Xp. On the RTX 2080TI we don’t see any calls to the libnvidiaptxjitcompiler.so. But I do see it on the TitanXp machine. Even when I change the NVCC flags to include the architecture for the Titan Xp machine I do not see any improvement. How is it that this library is not called on the RTX 2080TI. What needs to be done on the TitanXp machine to make sure that I get the same startup performance as RTX 2080TI ? They both are on the same nvidia driver version.Windows :\\nIt is very hard to even find the different libraries that are being called on windows to be honest. I tried Nsight System it does not detail out this information which is why I had to resort to Linux. If you can suggest a tool that can layout the different calls to the libraries would be very helpful (I used an application called FlameGraph in Linux). Back to PTX performance on windows even on RTX 2080TI I get very poor performance. Although it does get better from the second run I guess there is some kind of caching happening, but I don’t see the same performance as I see in Linux RTX 2080TI.Some numbers that we are getting (we start and stop the application to render some amount of frames in this case 25 frames ),\\nLinux 2080 TI takes 0.7s\\nLinux TitianXp takes 5-6s\\nWindows 2080TI takes 5-6s for the first time and comes down to 3s from the second time we launch the same application.\\nIt remains the same even if I try to compile the PTX files to same architecture on which the application is running.I would highly appreciate if someone helps me understand what exactly is going on with the different platforms and different GPU architectures.\\nHow can I get the same performance as I get in the Linux 2080TI system where in the library is never called.On Windows you can use procexp or listdlls from the Microsoft Sysinternals utilities to see which DLLs are loaded. I see some other good answers on Stack Overflow too How do I find out which dlls an executable will load? - Stack OverflowI usually use procexp, but it only shows currently loaded DLLs for a running process. It’s not easy to do if the process runs and exits quickly.Which version of OptiX and which driver are you using?I guess you might be seeing normal OptiX cache behavior. Shader programs are cached on disk after compilation, and the cache is checked before compilation, and loaded from the cache instead of compiling, if the requested shader is already in the cache. If you want to understand compilation behavior, it’s a good idea to delete your cache every time you want to replicate first-time compilation behavior. On Linux, the cache is in /var/tmp/OptixCache_<your_username>/. On Windows, the cache is in a folder with a hash in the name, you can use procexp or sysinternals’ “handle” to find the cache file called “cache7.db” (optix 7) or “cache.db” (optix 6).If compilation time is a problem and you want to reduce it, try reviewing how much inlining you have, that’s often a culprit for slow compilation times. Using callable programs in OptiX is one feature we offer that can help with compilation times, by strategically preventing function inlining.–\\nDavid.Hi David,Thanks for the procexp program it was helpful on Windows.\\nFor this particular test we are using OptiX 6.5 and the driver version is 460.32 on Linux and 461.09 on Windows.\\nWe were able to locate the Cache on both Linux and Windows.\\nI did see a significant difference in the compilation time of the first run when I did delete the OptixCache on both Windows and Linux. Although the time it takes on both the platforms is significantly different.\\nOn Linux the first run takes 7s and it takes 1s from the second run onwards (I get exactly the same performance on the Titan Xp as well after deleting the OptixCache).\\nOn Windows the same program, first run takes 12s and 4s from the second run onwards.\\nBoth are running on the same hardware.\\nWhy do I see this difference ?Compilation is a host-side operation, so the time taken is a function of many things that include your operating system, CPU, available host memory, other running processes, NVIDIA driver version, and any OS-specific differences in both the compiler and the compilation. It’s pretty common in my experience to see differences in compilation time across operating systems.–\\nDavid.Thanks for the inputs, David. We will do some more internal analysis here. We do observe that a few months back on Windows we were getting similar results exactly how Linux is performing at the moment (execution times of 1s). It regressed on both Operating systems. I am glad the we have resolved the delay on Linux i.e it is able to use the Cache properly by resetting the Cache. The same trick does not help on Windows.\\nMuch appreciated !Powered by Discourse, best viewed with JavaScript enabled',\n",
       " 'oculus-quest-2-with-cloudxr-3-1-cant-send-audio-to-server': 'Enable sendAudio parameter be true and try to send microphone voice from client to server, but the CLoudXR client APP can’t work.\\nthe streaming client log show “RtpAudioPlayer  Mic Connection Not Present”\\nis any setting I need enable in server site but I miss?upload log for check\\n1.txt (1.5 MB)\\nStreamer Client Log 2022-04-20 12.23.22.txt (157.1 KB)Yes, the server options file needs to contain -ra (I think, check the new documentation on audio in the 3.2 release).  Otherwise it will not set up the receiving end for client audio connection.Powered by Discourse, best viewed with JavaScript enabled',\n",
       " 'physx-ray-casting-precision': 'Hi,I’ve been using PhysX for doing ray-casting onto triangle meshes. In my application I need to find all intersections of the ray with the triangle mesh (regardless of whether it’s inside/outside or how many hits there are).I noticed that in some cases when the ray hits near the edge or vertex of a triangle, multiple hits are found in more or less the same location. I looked into the code and it seems PhysX uses the Möller–Trumbore intersection algorithm for calculating the intersection of a ray with a triangle.In the comments it is mentioned that this method uses an epsilon which is influenced by the size of the triangle in question. So the larger the triangle, the less precise the method becomes. So what is happening is that the ray is finding intersections with multiple triangles where I would expect a single hit.Is there any way to modify the epsilon somehow so that I only get a single hit in these cases? Making the epsilon a fixed value would also solve the issue. I could merge hits within a certain epsilon from each other but the way it is now, it is impossible to know which hits to merge as the epsilon could be any value.I am using the PxGeometryQuery::raycast function at the moment but I saw you can also do a scene ray-cast which allows you to specify a PxQueryFilterCallback. Would I be able to solve my issue using the filter callback? If so, can I also use the filter callback in PxGeometryQuery::raycast somehow or do I have to use a scene raycast?I’ve looked into this a bit more and it seems I made some wrong assumptions/misunderstood the code. The epsilon is not relative, but just not the value I was expecting.When I look at the value of mGeomEpsilon it is 0.0128…, is there a way to influence the value of this epsilon? Is it defined via the tolerance scale set in the cooking parameters? If so, how does the tolerance translate into the epsilon?Hi,\\nthere is a large comment about this geometric epsilon in the code:\\n// Derive a good geometric epsilon from local bounds. We must do this before bounds extrusion for heightfields.\\n//\\n// From Charles Bloom:\\n// “Epsilon must be big enough so that the consistency condition abs(D(Hit))\\n// <= Epsilon is satisfied for all queries. You want the smallest epsilon\\n// you can have that meets that constraint. Normal floats have a 24 bit\\n// mantissa. When you do any float addition, you may have round-off error\\n// that makes the result off by roughly 2^-24 * result. Our result is\\n// scaled by the position values. If our world is strictly required to be\\n// in a box of world size W (each coordinate in -W to W), then the maximum\\n// error is 2^-24 * W. Thus Epsilon must be at least >= 2^-24 * W. If\\n// you’re doing coordinate transforms, that may scale your error up by some\\n// amount, so you’ll need a bigger epsilon. In general something like\\n// 2^-22W is reasonable. If you allow scaled transforms, it needs to be\\n// something like 2^-22W*MAX_SCALE.”\\n// PT: TODO: runtime checkings for this\\nPxReal geomEpsilon = 0.0f;\\nfor (PxU32 i = 0; i < 3; i++)\\ngeomEpsilon = PxMax(geomEpsilon, PxMax(PxAbs(localBounds.maximum[i]), PxAbs(localBounds.minimum[i])));\\ngeomEpsilon *= powf(2.0f, -22.0f);Regards,\\nAlesPowered by Discourse, best viewed with JavaScript enabled',\n",
       " 'opengl-version-1-1-with-quadro-when-launching-task-inside-a-service-on-windows-1809': 'I have problem when I’m launching executable inside a service. This program are retrieving the Microsoft “GDI Generic” OpenGL driver, which is version 1.1. But I have Quadro P2000 with version 4.6 and the last NVidia driver.\\nThe basic usage of that case, is when I launch Unit testing program with Teamcity (continuous build system). This unit test executable are executed inside a Service by Teamcity.On Windows 10 version 1803, usually, I have to set/force the NVidia card as the primary/only graphic card (not using the Intel Integrated card) to solved this problem.But since I have update to the last Windows 10, version 1809, and I updated the NVidia driver to the last “dch” driver, the OpenGL version inside the service is always 1.1 … !!Before the W10 update, using driver 419.17 WHQL, it was working with the last W10 v1809. So it seems that there is a way to have it works on W10 with WHQL driver.\\nBut after installing the 419.17 DCH driver, it’s not working anymore, only OpenGL 1.1 inside service (using a account with administrative rights)…And the problem, is that I’m not able to install the original WHQL version!\\nI try lot of method, but I always I have an error. The NVidia driver installer tell me that the driver is not compatible with the current system (but it seems compatible, because it was using just after the W10 update, before I installer the DCH driver).\\nIf I ask to windows to install this specific driver, it finish with an error.One solution, is to reinstall W10 v1803 and keep the WHQL version, but I have computer which have already been updated (W10 and driver), and I want to avoid a full reinstall.Is there a hidden option to be able to force install a WHQL driver on W10 1809 or to be able to used the NVidia driver with OpenGL 4.6 inside a service on W10 with a DCH driver?I hope someone have a solution, I try so multiples tests since two days, install/reinstall of driver, search on google…Hello,\\nI still have the problem with the last version of Windows and NVidia driver.Here the result when I launch a simple test inside a Service and WHQL driver (using gliwinfo.exe):And here the result, on PC with the last nvidia driver, DCH, on Windows 10 >= 1803:As you see, glewinfo is working before the new DCH driver introduce on Windows 10 1803.\\nIs anyone have a some idea how to be able to have the NVidia driver inside a program running under Windows service?(note that NVidia graphic card is force as the primary card inside the BIOS).Hi,\\nAny update on this issue ? I am experimenting a similar one.\\nThank youPowered by Discourse, best viewed with JavaScript enabled',\n",
       " 'bug-huge-performance-drop-when-using-pxvolumecache-physx-3-3-4': 'When I’m try to use PxVolumeCache it results in huge performance drop no matter how much objects are there in scene.For example:\\nI’ve set up test scene with 7000 static and 1000 dynamic objects. Than I’m caching sphere with radius of 5 meters. It has 14 static and 2 objects inside. Caching takes 0.05 ms - it’s ok. Than doing 800 raycasts with PxVolumeCache takes 15 ms. But doing the same raycasts with scene takes just 3 ms! So it results in 5x performance drop.The problem is in NpVolumeCache::multiQuery. When I replace code inside NpVolumeCache::multiQuery with scene raycast - I’m getting expected 3 ms.After some testing I discovered that performance drop is present in case PxVolumeCache has more than 3 objects(no matter how much object are there in scene).P.S. My CPU is Intel i5-4590 3.30 GHz.Powered by Discourse, best viewed with JavaScript enabled',\n",
       " 'vulkan-api-deactivated': 'I’ve trouble with my rtx 2080 super the Vulkan API is “deactivated”.\\nI was trying to use Vulcan API, but the game crashes immediately. I’ve looked with GPU Z and everything is fine, card is not overclocked or changed. I´ve done an undervolt previously but got back to stock configs.\\nI have an ZOTAC GAMING GeForce RTX 2080 SUPER AMP https://www.zotac.com/de/product/graphics_card/zotac-gaming-geforce-rtx-2080-super-amp.\\nEver thing runs fine except when I force it to use Vulkan. I´ve definitely used Vulkan without problems about an year ago with the same Hardware config, without any Problems.\\nYesterday I chatted to the Nvidia Support and over the Period of 2 hours we tried a lot of installs, deinstalls and reinstalls of the graphics driver, but vulkan didn’t come back.\\nThey send me here to maybe get some help.\\nDoes anyone have a clue?\\nGreetings Phillip Fromme\\n388×539 17 KB\\nHi there @fromme.phillip and welcome to the NVIDIA developer forums.Well, if customer help already sent you here, I will refrain from just passing you on to the GeForce forums where these kinds of topics usually go. :-)Let me take a guess, you did the undervolting by using MSI Afterburner? There have been several occasions where Afterburner caused some side-effect that disabled Vulkan capabilities. (Also for AMD GPUs by the way)So even if you did a lot of un- and re-installs, I would suggestIf the problems persist, share the output of vulkaninfo.exe here please (pipe it to a file).Thanks!vulkaninfo.txt (1.0 KB)\\nWARNING: [Loader Message] Code 0 : Layer name GalaxyOverlayVkLayer does not conform to naming standard (Policy #LLP_LAYER_3)\\nWARNING: [Loader Message] Code 0 : Layer name GalaxyOverlayVkLayer_VERBOSE does not conform to naming standard (Policy #LLP_LAYER_3)\\nWARNING: [Loader Message] Code 0 : Layer name GalaxyOverlayVkLayer_DEBUG does not conform to naming standard (Policy #LLP_LAYER_3)\\nWARNING: [Loader Message] Code 0 : Layer VK_LAYER_AGAUEEYE_Overlay uses API version 1.1 which is older than the application specified API version of 1.3. May cause issues.\\nWARNING: [Loader Message] Code 0 : Layer VK_LAYER_OW_OVERLAY uses API version 1.2 which is older than the application specified API version of 1.3. May cause issues.\\nWARNING: [Loader Message] Code 0 : Layer VK_LAYER_OW_OBS_HOOK uses API version 1.2 which is older than the application specified API version of 1.3. May cause issues.\\nERROR: [Loader Message] Code 0 : Failed to open dynamic library “C:\\\\Program Files (x86)\\\\Overwolf\\\\0.223.0.30.\\\\owclient.dll” with error 1114Well, it seems Overwolf has a hook into your Vulkan driver and something there went wrong.vulkaninfoshould not depend on a third party driver. Could be related to some MOD or other tool that is part of your Overwolf setup. And to be honest, the warning with respect to Agaue Eye (yet another third party tool that needs admin rights to hook into the system…) does not sound correct either.I know this sounds like trying to put blame anywhere but our driver, but I expect things to run much more smoothly after removing all the 3rd party software and reinstalling the GPU driver after a clean re-boot.After that it would be Windows re-install time for me.Powered by Discourse, best viewed with JavaScript enabled',\n",
       " 'is-uploading-rtbufferid-as-int-variable-legal': 'Hey.Is it valid to upload an rtBufferId as an int variable on the OptiX context or are rtBufferId’s only valid as elements in an rtBuffer?\\nI ask because the programming guide only mentions buffers of buffer ID’s as a use case and I’m currently debugging a brutal crash in rtContextLaunch2D that seems to be related to my light sources. Those lightsources were recently refactored from being uploaded as an rtBuffer to be part of a scene struct where the buffer ID is uploaded as an int instead.\\nI remember a few years ago where we were discouraged from doing pointer arithmetic on buffers because of paging (I think) and I’m wondering if that’s the same thing for buffer IDs, OptiX needs to know which buffer ID’s are used and can’t if its secretly uploaded as an int.Cheers\\nAsgerHow do you tell OptiX what kind of buffer format is behind that integer buffer ID?I’m using rtBufferID<type, dimension> id; fields in my light definition struct like this:\\n[url]https://github.com/nvpro-samples/optix_advanced_samples/blob/master/src/optixIntroduction/optixIntro_07/shaders/light_definition.h#L60[/url]For buffers of buffers IDs it would be similar.\\n[url]http://raytracing-docs.nvidia.com/optix/guide/index.html#host#buffers-of-buffer-ids[/url]OptiX knows that a buffer is being used as bindless when you call getId() on it on the host.rtBufferId has a constructor that accepts an int as input, so I can do it like thisconst Light& light = rtBufferId<Light, 1>(g_scene.light_buffer_ID)[light_index];I’ll download that sample later today. Thanks! Originally I wanted to make my struct the same way that you did, but rtBufferId wasn’t declared on the CPU, so I declared it as an int instead to be able to use the same struct on hos and device… Do you have a corresponding struct with ints on the host or how do you upload the IDs?I’m just doing this to set the rtBufferId fields inside the LightDefinition structure:\\n[url]optix_advanced_samples/Application.cpp at master · nvpro-samples/optix_advanced_samples · GitHubThe C++ host code knows about the rtBufferId<> templates from optixpp_namespace.h.\\nLook for #define rtBufferId optix::bufferId around line 1717 in OptiX 5.1.0.\\nIn the end it’s just an int underneath.Fantastic. I found the bug in an environment map default constructor that only initialized half its members and would once in a while cause an invalid environment map ID to be uploaded => buffer access hell.I can’t include optixpp_namespace.h host side without getting a cascade of compiler errors that start with\\nerror : variable “RTAPI” has already been defined\\nMust be some order dependent includes. I’ll fix that later.This should just work if you always put #include <optix.h> before any of the other OptiX includes.\\nThat will define RTAPI via the include optix_host.h only for the host compiler.There is no real need to use optix_world.h and you should not include optix_host.h or optix_device.h explicitly.Powered by Discourse, best viewed with JavaScript enabled',\n",
       " 'tachyon-optix-rendering-not-available-in-nvidia-driver-in-ubuntu-20-04-22-04-22-10-through-wsl2': 'Dear developers,\\nI am trying to use VMD (https://www.ks.uiuc.edu/Research/vmd/) software in Ubuntu 20.04LTS/22.04LTS/22.10 through WSL2 installed in windows11.\\nI installed nvidia-cuda-toolkit manually from website.\\nI used command-line option also, apt install nvidia-cuda-toolkit.\\nI built OptixSDK library too.\\nStill, I amgetting following error while executing VMD.\\nimage1916×641 351 KB\\nMay be, somehow, the nividia driver available in WSL2 is not same as it available in original linux (e.g.  ubuntu20.04/22.04/22.10).\\nMay be Optix-related libraries are missing in nvidia driver of wsl2.Please look into this and help me to fix so that tachyon Optix rendering will be possible in VMD.Powered by Discourse, best viewed with JavaScript enabled',\n",
       " 'nvenc-rgb-output-is-looking-washed-out-while-nv12-output-is-fine': 'I’m using d3d11 Nvenc api to encode the output from desktop duplication interface.if i’m using nv12 format (converted using d3d11 video processor), output is looking as expected.but rgb output is little washed out. my guess is the input was in limited range but nvenc assumed it’s fullrange. If so, how do i fix it? By editing SPS/PPS or is there an option in nvenc for me to set? I can post video samples via youtube if requiredin case someone finds this later, It’s my mistake. dxgi video processor i was using to scale the images is doing this… configuring that fixed my issue.Powered by Discourse, best viewed with JavaScript enabled',\n",
       " 'convert-nv12-to-rgb-without-alpha-channel': 'Hi,in the codec sdk, you provide the api to convert nv12 to rgbaBut I don’t want the alpha channelSo I createAnd modify YuvToRgbKernel as followsbut this does not work…I am not familiar with nv12.Any advise?have you solved this problem? I have the same issue with converting NV12 to BGR24.Powered by Discourse, best viewed with JavaScript enabled',\n",
       " 'installation-of-tadp-failed-on-windows-7-and-ubuntu-12-04-64bit': 'I tried installing TADP 3.0r4. On Windows 7 I got the error message similar to a previous post [url]https://devtalk.nvidia.com/default/topic/811800/android-development/tadp-installation-failure-in-both-win7-and-win8/[/url].I’m more interested in the TADP version for Linux 64bit since it has CUDA. On Ubuntu 12.04 64bit when I install it, I got an error saying that the file /.NVIDIA/TADP/007/android-ndk-r10c-linux-x86_64.bin is not found.I checked that location and the file is actually there, with execution permission. However when I manually try to run it in a terminal it still says found is not found. After doing some google search I found this post explaining the issue:\\n[url]64 bit - No such file or directory? But the file exists! - Ask UbuntuBasically it says that running a 32-bit binary on a 64bit platform will fail.I tried what the post said to verify the property of the file by using the “file” command. It shows the following:$ file android-ndk-r10c-linux-x86_64.bin\\nandroid-ndk-r10c-linux-x86_64.bin: ELF 32-bit LSB executable, Intel 80386, version 1 (SYSV), dynamically linked (uses shared libs), for GNU/Linux 2.6.24, BuildID[sha1]=0x13b76de68ec17f30e2e19d37f8b379fc204107af, strippedSo it seems that the file is indeed a 32bit executable. Is this the issue and how do I resolve this?By following one of the answers in the post I linked and run the two commands below I was able to successfully install TADP on Ubuntu 12.04 64bit.sudo apt-get update\\nsudo apt-get install libc6:i386 libncurses5:i386 libstdc++6:i386Powered by Discourse, best viewed with JavaScript enabled',\n",
       " 'cloudxr-tells-me-steamvr-is-not-running': '\\nit is trying to install serverHello Frank,\\nseems like Steam is installed, but is SteamVR also installed? You need to add that separately from Steam itself. You will then see two icons if both are running:\\nRobertyes that is also running on the machine\\ncloud xr error901×885 116 KB\\n\\nhere by the picturtehmm, that’s definitely strange.I’d prove that SteamVR is actually working without CloudXR in the mix yet.  If you have a wireless HMD like the Quest 2, you can use QuestLink to direct connect to your PC (with a LONG usb-C cable!), and see if steamvr and games/apps run fine.  If they do, then I’d need to escalate to someone who understands the installer and verificatios.  If SteamVR does NOT work, then a complete uninstall/reinstall may be needed.  Or you can try ‘repair’.Powered by Discourse, best viewed with JavaScript enabled',\n",
       " 'can-i-use-rtx-2080-super-or-gtx-1660-ti-for-deeplearning': 'hi, Can I use RTX XXXX super and GTX 16XX serise for cuda programing and deep learning ( tensorflow at most)\\nlist:Explore your GPU compute capability and CUDA-enabled products.\\nI can not find them at list,I ask twice support but not helpful at all.thaks for time.Powered by Discourse, best viewed with JavaScript enabled',\n",
       " 'redundant-video-encoding': 'Hi, I’m working on game streaming and try to find redundant video encoding solution which should improve video quality on WiFi with big amount of packet loss. Is there any nvenc function which should help?Thanks in advanceIs there any nvenc function which should help?Yes, for massive streaming. You can create more encoder sessions with different parameters and different scaled input image. But be aware - with customer grade (GTX/RTX) and lowend Quadro only 2 encoder sessions are allowed per whole system [https://developer.nvidia.com/video-encode-decode-gpu-support-matrix#Encoder]. Video streams can be broadcasted and multiplexed using HLS (RFC 8216: HTTP Live Streaming) or similar technologies.Yes, for point-to-point streaming. You can create some feedback in your streaming protocol to change  stream FPS and encoder parameters  (see NvEncReconfigureEncoder() API). For example check some “professional protocols” like “HDX Adaptive Transport”, “Enlightened Data Transport”…If you want to protect low latency streaming over WiFi or any other lossy networks we use → \\nElasticFrameProtocol for framing the NAL, ADTS (in our case) and AUX data.\\nhttps://bitbucket.org/unitxtra/efp/src/master/Then as transport we use SRT or RIST.\\nSRT Example → \\nhttps://bitbucket.org/unitxtra/cppsrtframingexample/src/master/\\nRIST Example → \\nhttps://bitbucket.org/unitxtra/cppristframingexample/src/master/RIST is WIP by the VideoLan guys right now so the build might be broken. There are new commits all the time and the API’s are not yet stable.The above proposed protocols use ARQ and you can set a time-limit meaning you can control the maximum delay you allow for. However a lower delay setting also covers for less loss, you need to set the levels that works for your application./AndersPowered by Discourse, best viewed with JavaScript enabled',\n",
       " 'possible-reasons-why-cuviddecodepicture-may-block-for-20-seconds-when-decoding-hevc': 'Our situation is the following:We transcode 2 HEVC (4K) video streams using ffmpeg and nvidia.Sometimes (a few times a day or in a couple of days) we observe an error - “Circular buffer overrun. To avoid, increase fifo_size URL option. To survive in such case, use overrun_nonfatal option”.Right before the error we observe that the speed of transcoding drops drastically:Pay attention to the second and third lines - it took 21 second (07:50:36 - 07:50:15 = 21 second) to transcode 10 frames (4537369 - 4537359).\\nIf you look at the 5 and 6 lines, you will see that it took 10 seconds to transcode just 1 frame.And it happened all of a sudden - as you can see in the logs, before that there had been 25 hours of successful transcoding and no errors/warnings.I tracked down the culprit - the process is blocked by function cuvidDecodePicture (https://github.com/FFmpeg/FFmpeg/blob/master/libavcodec/cuviddec.c#L339). But I am unable to investigate any further, because I don’t have the sources of this function. Is there by any chance somebody who was faced with the same issue?More details:\\nGraphic card - GTX 1080/GTX 1050 (the error manifests itself no matter which card we’re using)\\nDriver version - 418.56 (we tried drivers 3xx.xx, didn’t help)\\nDocker as runtime environment (we tried nvidia images with versions 9.2-devel-ubuntu16.04 and 10.1-devel-ubuntu18.04 (https://hub.docker.com/r/nvidia/cuda/))\\nAs input we’re using multicast mpegts HEVC video. Here’s an example of ffmpeg command:What’s interesting - is that the problem can’t be reproduced with the same video sample (after I reproduced the problem with multicast stream, I successfully transcoded the same sample (which I’d kept as an mpegts file on file system)).What’s even more interesting, today we reproduced the same error at the same time on two servers, which were transcoding the same streams in parallel.What could cause the problem?Seems it got fixed in the latest stable driver (430.14).There were no hangs in the last several days since I started transcoding with the new driver.Powered by Discourse, best viewed with JavaScript enabled',\n",
       " 'using-cudamalloc-pointers-in-opengl-cuda-kernel-interop': 'I have a parallel reduction kernel that computes an images min and max.  The calling C code cudaMallocs memory space for the min and max reductions and those pointers are used in the CUDA code.  If I execute the kernel from the C program, it works fine.  If I execute the kernel from a program that is getting a PBO from an OpenGL graphics context, the kernel can not access the cudaMalloc’ed pointers.  I get an access violation error.  If I modify the kernel to use globally declared device arrays, it works fine.  Why can I not access the cudaMalloc’ed memory from the kernel when it is called from within an OpenGL context?  I have no problem accessing the PBO.Thanks.I believe that the device may be getting reset when the graphics is initialized so my allocations go away.  I may go back and try to allocate the memory on the first render pass and see if that works.Powered by Discourse, best viewed with JavaScript enabled',\n",
       " 'actors-initial-position-with-joints': 'At simulation start, my actors (with joints) move to match the joint constraint and get their correct positions, it result a kind of scene initialisation movement but it is not really realistic in a real time application. Sometimes the joint stress between two actors is strong if they are not precisly positionned at start and it provide a kind of projection of the actors moving them strongly. How I can start the scene with this actors already well positionned according joint constraint ?thanksI guess the easiest solution is to calculate their most efficient position based on the joint-type your using. The Joint-Constraint solver will always force the bodies to a transformation that applies for that type of joint. So the only way to limit the strange ‘moving’ behaviour is by giving them an initial position as close as possible to the actual needed constraint state.There is a small topic in the documentation about configuring joints for best behaviour:The behavior quality of joints in PhysX is largely determined by the ability of the iterative solver to converge. Better convergence can be achieved simply by increasing the attributes of the PxRigidDynamic which controls the solver iteration count. However, joints can also be configured to produce better convergence.the solver can have difficulty converging well when where a light object is constrained between two heavy objects. Mass ratios of higher than 10 are best avoided in such scenarios.\\nwhen one body is significantly heavier than the other, make the lighter body the second actor in the joint. Similarly, when one of the objects is static or kinematic (or the actor pointer is NULL) make the dynamic bodythe the second actor.\\nA common use for joints is to move objects around in the world. Best results are obtained when the solver has access to the velocity of motion as well as the change in position.if you want a very stiff controller that moves the object to specific position each frame, consider jointing the object to a kinematic actor and use the setKinematicTarget function to move the actor.\\nif you want a more springy controller, use a D6 joint with a drive target to set the desired position and orientation, and control the spring parameters to increase stiffness and damping. In general, acceleration drive is much easier to tune than force drive.The artifact you describe is what I would call ‘settling’.  For example, if you add a stack of blocks to a scene and they are interpenetrating when they are added, the stack will jump or pop when the blocks are first woken up. If the interpenetration is slight, they will settle. The same is true for actors connected by joints, if the positions of the actors and the joint setup are such that the system is not at equilibrium at startup. Usually this is taken care of in a level editor, the objects are tweaked until they are in equilibrium, and then the asset or level is saved out, so that when the assets are loaded into the game at runtime, they are in equilibrium. If you are generating assets procedurally at runtime, you could insert them into a special test scene, let them settle, then serialize them to a collection which will be de-serialized into your main scene.\\n–MikePowered by Discourse, best viewed with JavaScript enabled',\n",
       " 'lost-buffer-bindings': 'Recently we found a strange behavior on some of our computers when executing our programs. Some computers lost their buffer bindings and so the shader programs were not able to write their results back.We were able to identify the problem by writing a small test application in plain C++ that shows that error. See below.\\nIt writes in a simple loop the loop index in a constant buffer and the shader writes the parameter simply back to its output buffer. After a few times (e.g. in the 48th iteration) the shader loses its binding\\nto the output buffer and the previous results won’t be updated again. So the value (in the example ‘48’) will be reported since then.Does anyone have seen such behaviour? It won’t work on laptops with GTX 5xx series, 6xx and 7xx series. The same strange results also on Quadro 2000m.The test code in C++ can be found in the attachment in file GpuDispatchBugTestC++.cpp and screenshots of the results in “Screenshot1-Buggy.jpg” and “Screenshot2-Working.jpg” in the attachment of this post.Thank you for your reply :-)GpuDispatchBugTestC++.cpp (5.12 KB)\\n\\nScreenshot1-Buggy.JPG1007×154 25.4 KB\\n\\n\\nScreenshot2-Working.JPG978×121 24.1 KB\\n\\nScreenshot1-Buggy.JPG1007×154 25.4 KB\\n\\nScreenshot2-Working.JPG978×121 24.1 KB\\nPowered by Discourse, best viewed with JavaScript enabled',\n",
       " 'torque3d': 'How do I go about getting version 2.8.4.6 for Torque3d?ThanksHi,Can you give more explanation of what you want to achieve?Powered by Discourse, best viewed with JavaScript enabled',\n",
       " 'number-of-monitors-and-resolution-calculation': 'I’ve been tasked with building a PC that can support 8 4k monitors. I’ll likely need 2 cards that can support 4 monitors, but some of the specs are unclear. For example, the Quadro P5000 can support 4 monitors, but it doesn’t say at what resolution. Some of the other cards give a maximum resolution but that is probably for 1 monitor. How can I figure this out?Hi jeff_payne,The Quadro datasheets provide information on what resolutions are supported:233.25 KBThanks,\\nRyan ParkYou misunderstood the question. I need to know the maximum resolution for 4 displays at a time. I know that some cards at maximum resolution are limited to 1 or 2 displays even though they support more displays at lower resolution.Hi jeff_payne,The resolution shown on the datasheet is the max resolution supported for 4 displays at a time.Specifically for your use-case, you should go with 2 Quadro P5000s with a dual 8 pin connnector.If you have budgetary constraints, you can also go with 2 Quadro P4000s, that would also support your use-case as well.Thanks,\\nRyan ParkPowered by Discourse, best viewed with JavaScript enabled',\n",
       " 'video-codec-sdk-samples-building-error': 'Hello I am new to Nvidia Video Codec SDK , I downloaded Nvidia Video Codec SDK 12.0 and I am trying to build the samples I run cmake command and its ok but when I run make I get this error:for your information I fixed it in the cmake files of the samples but I am wondering if you forgot to put pthread library in all of your cmakelists files or this is an installation or a dependency problem ?Powered by Discourse, best viewed with JavaScript enabled',\n",
       " 'optix-stack-size': 'Hi,I was wondering whether it is possible to set the stack size per kernel instead of globally per context. Within a single context, some programs would be much more complex than others and the less complex ones would gain significant performance advantages from a smaller stack size. I know it is possible to set the stack size before launching a program but this seems to be triggering a very slow recompile.Thanks,StevenIt is not possible to set the stack size per kernel, and, in general, it is not expected that this would produce a significant performance gain.Hi bwolfers,Thanks for your input - that’s what I thought :(Re performance, yes it makes a difference. I have 2 OptiX entry points, one of them needs 500 bytes stack size and the other one runs comfortably on 300 bytes. Since I can only define one global stack size per context, I have to set the stack size at 500. The smaller kernel takes 22ms when I set the stack size to 500 and just 4ms when the stack size is at 300.Unfortunately I switch between the 2 kernels often and cannot call setStackSize in between programs since it triggers a recompile.It would be extremely helpful if OptiX allowed setting a stack size per entry point. Conceptually this shouldn’t be a problem to implement since each entry point runs completely independent.I would appreciate any input you may have on the issue.Thanks,StevenWe could add a per entry point stack size, and that would solve the recompilations, but there’s another blocker for this to work properly.Currently we use a flag on the CUDA context that tells it to not resize the LMEM after each launch (CU_CTX_LMEM_RESIZE_TO_MAX).  Thus once you called the 500 byte kernel, the LMEM would be resize to 500 for ever more.  There are ways to avoid this, but they present additional performance issues that are still being addressed.  We are currently working to try and remove the need to use this flag, and at that point a discussion of per entry point stack could be made.That being said, I haven’t found any situations where the stack size affected performance as it has your kernel.  If it’s possible, we would like to obtain a trace of your code to investigate this further when we have resolved the issues with LMEM resizing.  Please contact us at optix-help@nvidia.com if you would like to share a trace with us.removedPowered by Discourse, best viewed with JavaScript enabled',\n",
       " 'is-there-a-way-to-map-a-host-cached-vulkan-buffer-to-a-specific-memory-location': 'Vulkan is able to import host memory using  VkImportMemoryHostPointerInfoEXT . I queried the supported memory types for  VK_EXTERNAL_MEMORY_HANDLE_TYPE_HOST_ALLOCATION_BIT_EXT on an RTX 2060  but the only kind of memory that was available for it was coherent, which does not work for my use case. The memory needs to use explicit invalidations/flushes for performance reasons. So really, I don’t want the API to allocate any host-side memory, I just want to tell it the base address that the buffer should upload from/download to. Otherwise I have to use intermediate copies. Using the address returned by vkMapMemory for the host-side work is not desirable for my use-case. Am I doing this the wrong way or is this just not possible?Powered by Discourse, best viewed with JavaScript enabled',\n",
       " 'crash-when-all-exceptions-enabled': 'I just updated to Optix 6.5.0 and am getting odd behavior. Whenever I enable all exceptions, the program crashes during the first call to rtContextLaunch*D. Using rtContextSetUsageReportCallback shows that it is somewhere in the middle of compiling the device programs for my particular driver version when this happens. There is no message printed to the console, but the exit code is 0xFFFFFFFFC0000005. I am enabling exceptions with this code:Environment: Windows 10, OptiX 6.5.0, display driver 436.15, CUDA driver 10.1.0, GeForce GTX 1080 Ti and RTX 8000 installed (using only one at a time).Hi @nljones,Will you see if you can reproduce this on one of the OptiX SDK samples? I tried using samples_sdk/optixPathTracer, adding context->setExceptionEnabled(RT_EXCEPTION_ALL, true) in the createContext() call, immediately after Context::create(). That doesn’t crash on me yet, so I’d like to figure out how to repro this.If the program crashed, then the exit code provided is a generic Windows system exit code. Searching I found that 0xFFFFFFFFC0000005 means EXCEPTION_ACCESS_VIOLATION via Winbase.h, so it doesn’t give us much information.–\\nDavid.Hi @dhart,Thanks for looking into this. I did some digging, and it turns out that only enabling RT_EXCEPTION_BUFFER_ID_INVALID is a problem. The rest of the exception testing doesn’t seem to cause a problem with my program. Note that there aren’t any invalid buffers (or at least, there weren’t under OptiX 6.0, and the program runs smoothly if I don’t test for them), it’s just the fact that I’m testing for invalid buffers that causes the crash.Perhaps there is something up with my shaders that is causing the error. I’m sending a reproducer to the help email, just enabling RT_EXCEPTION_BUFFER_ID_INVALID. For me it crashes at rtContextLaunch2D (which is of course the end of the trace file). Hopefully this will let you observe the same behavior.Powered by Discourse, best viewed with JavaScript enabled',\n",
       " 'ffmpeg-h264-nvenc-interlaced-encode-1080i-do-not-support': 'HiWhen i using ffmpeg process the 1080i input stream and i want to keep the stream as 1080i , I just add this command:\\n-flags +ilme+ildctfull ffmpeg:\\nffmpeg -vsync 0 -hwaccel_device 1 -hwaccel nvdec -hwaccel_output_format cuda ‘-c:v’ ‘h264_cuvid’ ‘-deint’ ‘2’ ‘-drop_second_field’ ‘1’  -i ‘udp://239.49.17.159:4511?localaddr=127.0.0.1&timeout=10000000’ -map ‘0:v:0’ -map ‘0:a:0’ ‘-c:v’ ‘h264_nvenc’ ‘-profile:v’ ‘main’ ‘-preset:v’ ‘hp’ ‘-r’ ‘25’ -g ‘25’ -flags +ilme+ildct ‘-c:a’ ‘libfdk_aac’ ‘-ar’ ‘44100’ ‘-ac’ ‘2’ ‘-b:a’ ‘128k’ -max_muxing_queue_size 1024 ‘-b:v’ ‘1500k’ -bufsize ‘750k’ -metadata service_name=‘CCTV10’ -progress ‘udp://127.0.0.1:1510’ -flush_packets 0 -f mpegts ‘udp://239.56.33.63:8511?localaddr=127.0.0.1&pkt_size=1316’but ffmpeg saidInput #0, mpegts, from ‘udp://239.49.17.159:4511?localaddr=127.0.0.1&timeout=10000000’:\\nDuration: N/A, start: 35972.938533, bitrate: N/A\\nProgram 21\\nMetadata:\\nservice_name    : Zhejiang International\\nservice_provider: Harmonic\\nStream #0:0[0x214]: Video: h264 (Main) ([27][0][0][0] / 0x001B), yuv420p(tv, smpte170m, top first), 704x480 [SAR 10:11 DAR 4:3], 29.97 fps, 59.94 tbr, 90k tbn, 59.94 tbc\\nStream #0:10x314: Audio: mp2 ([4][0][0][0] / 0x0004), 48000 Hz, mono, s16p, 128 kb/s\\nUsing -vsync 0 and -r can produce invalid output files\\nStream mapping:\\nStream #0:0 → #0:0 (h264 (h264_cuvid) → h264 (h264_nvenc))\\nStream #0:1 → #0:1 (mp2 (native) → aac (libfdk_aac))\\nPress [q] to stop, [?] for help\\n[h264_nvenc @ 0x49bf9c0] Interlaced encoding is not supported. Supported level: 0\\n[h264_nvenc @ 0x49bf9c0] Provided device doesn’t support required NVENC features\\nError initializing output stream 0:0 – Error while opening encoder for output stream #0:0 - maybe incorrect parameters such as bit_rate, rate, width or height\\n[libfdk_aac @ 0x49b44c0] 2 frames left in the queue on closingMy GPU is\\n| NVIDIA-SMI 440.33.01    Driver Version: 440.33.01    CUDA Version: 10.2     |\\n|-------------------------------±---------------------±---------------------+\\n| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\\n| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\\n|===============================+======================+======================|\\n|   0  GeForce GTX 1660    Off  | 00000000:01:00.0 Off |                  N/A |\\n| 47%   50C    P0    29W / 120W |   1284MiB /  5944MiB |     15%      Default |ThanksPowered by Discourse, best viewed with JavaScript enabled',\n",
       " 'what-cable-to-use-for-244hz-and-a-rtx3090': 'Hi,What cable to use for a RTX 3090 that does 244hzPowered by Discourse, best viewed with JavaScript enabled',\n",
       " 'difference-in-performance-and-stability-between-different-joint-types': 'I’m trying to get this kind of setup a bit more stable and preferable improve performance:\\n\\nExternal Image\\n\\nBasically it’s a system of 4 constraints, two of them have angular drive and limits enabled and two are “fixed”. As I use UE4 we have only 6Dof joint out of the box, other types of joints are not implemented.Before I switch to using Articulations for this, I just want to be sure that I wouldn’t get better results with a couple of hinge and fixed joints instead of using 6dof.Any thoughts on this?All joints are an extension to PhysX that makes the PhysX constraint system easier to work with. As such most joints have an equivalent D6 joint setup which will have fundamentally the same behavior and performance. An hinge joint is basically the same as a D6 joint with 5 degrees of freedom locked and the twist axis unlocked (or limited/driven) in terms of the way it generates constraints for physx to solve. Similarly a fixed joint is essentially a D6 joint with all 6 degrees of freedom locked.It wasn’t clear to me from the screenshot exactly what you’re trying to set up but keep in mind that articulations are going to work a lot better when you don’t have any cycles in your chain of bodies.All this is just from reading the docs/source so by all means double check what I’ve said for yourself. I could be wrong.Thank you for reply! I didn’t received any notification and stopped checking forums.Most likely articulation would help with stability but I’ve decided to abandon this approach all together as there where other issues with it. Mainly performance, having 8 sets of setup like this, doesn’t scale with more vehicle being added.\\nFor the fairness, I’ll make articulation setup later, it might be useful in other use cases.Powered by Discourse, best viewed with JavaScript enabled',\n",
       " 'optical-flow-2-0-23-fails-to-build-when-used-from-plain-c': 'Not sure if this is the correct category but I can’t find anything specific for optical flow (nor any tags).Anyway, including optical flow from plain C does not work because NV_OF_ROI_RECT is referred without the struct qualifier. Attached trivial patch fixes the problemPowered by Discourse, best viewed with JavaScript enabled',\n",
       " 'any-examples-ideas-on-how-to-do-bump-normal-mapping': 'I want to add additional roughness, bumps to a solid. As well as the albedo material, I have matching materials for roughness, height, and normal map. I saw an 10yr old question about similar, but would like to see something for optix7.  Thanks.Hi there. There’s nothing unique in OptiX 7 that would affect how you do your own normal & shading calculations. If you have an old example that works in OptiX 6, or even in other APIs like OpenGL, it will be quite portable and should work fine in your OptiX 7 closest-hit program too. I don’t know of a bump mapping sample off the top of my head, I did a cursory check of the SDK and Detlef’s Advanced Samples repo and our SDK and didn’t see one. This answer from 2013 still applies and has a nice link to a normal mapping tutorial if you need one: Layered Materials also Bump mapping example please - #2 by CrogIn OptiX 7 you will use the CUDA texture API to load a texture map. There are basic examples of reading CUDA textures in the CUDA samples that come with the CUDA SDK. The rest will be a small amount of vector math that you can do using the vector libraries we provide in the OptiX or CUDA SDKs, or one of your own. Happy to help if you have specific questions or concerns about the OptiX 7 API.–\\nDavid.Powered by Discourse, best viewed with JavaScript enabled',\n",
       " 'rtcontextvalidate-fails-after-rtcontextsetraygenerationprogram-unless-it-was-also-called-before': 'My small ray tracing application has a very weird behaviour / bug. Calling rtContextValidate after setting up everything, either directly or indirectly via rtContextLaunch, results in a RT_ERROR_TYPE_MISMATCH.However, if I add a call to rtContextValidate at any location in the code before calling rtContextSetRayGenerationProgram, the error goes away and the code produces the expected result!I have no idea what could be causing this. According to what I have read in the docs so far, a call to rtContextValidate that did not encounter any problems should never change the outcome of a subsequent call to rtContextValidate, right?Right now, I have only two potential explanations for this: either I did something nasty with buffers or something that causes undefinded behaviour, or there is a bug in OptiX. Unfortunately, I have no idea how to find out what exactly is going on here.Btw: I am using OptiX 4.0.1 with CUDA 8.0 on Windows 10 (VS 2015)The first thing you should try is updating to OptiX 4.0.2 and see if the problem persists.If yes, I would go through all cases inside the OptiX API Reference document and check which calls can trigger the RT_ERROR_TYPE_MISMATCH and see if the code around that is correctly specifying the necessary objects at the proper size and indices.Check with rtContextGetErrorString if there are more information available about the type mismatch error.Maybe add individual validation calls for other objects than the whole context to see if you can pinpoint this problem.Additionally I would enable all exceptions and add an exception program which prints the exception code during debugging to see if the result is really generated correctly or if there are any other potential problems. I’ve posted code doing this on this forum before. You’ll find it with the search option in the top right on this site.\\nNote that compilation and runtime performance is affected in OptiX 4.0.2 when enabling exceptions. Only use that for debugging, never during benchmarks.What GPU(s) do you use?Thanks for the quick reply and helpful tips. I found the issue, which was pretty simple and stupid. I accidentally assigned a uint to an int variable of my intersection program.Apparently, validate only detects this after the ray generation programms have been added. I guess there is some kind of “already validated” flag or something that prevents it from finding this issue if a validate was called previously.The behaviour persists in 4.0.2Declaring a variable like this in my intersection program:and assigning it like thisCauses the issue.I am using a single GTX 970 btwAs far as why OptiX only detects the error when a ray-gen program is attached:The “validate” command looks at the entire context and tries to find everything that is reachable during rendering, then validates that set of things.  If a piece of geometry is not used – nothing traces rays against its GeometryGroup, for example – then the intersection program doesn’t necessarily have to be checked.  OptiX may ignore it entirely.Does that explain the behavior you’re seeing?  Is there any other way to trigger the intersection program other than by shooting a ray from the ray-gen program?Powered by Discourse, best viewed with JavaScript enabled',\n",
       " 'sending-float-array-from-host-to-device': 'Hello,\\nI am trying to send a float[5] array, defined on host side, to the GPU-device. I have already done that with float3 and float4 types. But since float5 is not originally given by Cudas “vector_types”, I was trying to send a float[5] array with the context method “setUserData(…)”. However, when I am trying to access that data on the device side, I get an error of memory adress violation: Process finished with exit code 139 (interrupted by signal 11: SIGSEGV).I am not sure what I have done wrong. Here is an example of how I am trying to send data to the device from the host:On Host side:On Device side in a .cu file:Of course I set “setPrintEnabled” to true in order for the output to work.I would really appreciate some help, since I dont know how to solve that kind of memory access violation.Thanks a lot.You cannot simply send a virtual pointer in host memory to the GPU device. You need to have that data inside some GPU-addressable memory and in your case the easiest method is to copy that onto the device by using an OptiX buffer variable.On the host:On the device:If you’re new to OptiX, please read through these documents:\\nhttp://raytracing-docs.nvidia.com/optix/index.html\\nand watch the OptiX Introduction presentation and work through its example source code:\\nhttps://devtalk.nvidia.com/default/topic/998546/optix/optix-advanced-samples-on-github/Thanks for your fast reply. That worked for me.I need to correct myself.\\nI am still running into the same Issue. I copy-pasted your example to my code and tried to visualize a float of the array withBut I still get the same type of Memory violation.I copied the host code directly to my main function and the device code to the .cu file, where my ray generation takes place (‘Pinhole Camera’).I found the Error. Seems like there is one line missing on your host code:Correct me, if I’m wrong.Right, I just typed that into the reply and forgot to set the variable on the host.\\nA context->validate() in debug mode should have given you some error that a buffer variable was not set instead of a straight crash.\\nAlso maybe try setting a usage report callback function for additional information during debugging. Example code here:\\n[url]https://github.com/nvpro-samples/optix_advanced_samples/blob/master/src/optixIntroduction/optixIntro_10/src/Application.cpp#L489[/url]Powered by Discourse, best viewed with JavaScript enabled',\n",
       " 'cant-download-it-can-you-send-me-a-package-its-been-two-days': '\\nimage|690x10Can&#39;t download it. Can you send me a package? It&#39;s been two days3835×125 7.8 KB\\nSorry, this package can’t be downloaded\\nimage793×104 7.96 KB\\nHello,Can you please provide the links to the pages that are giving you download issues?Thanks,\\nTomGoogle Chrome Help _download_errors&hl=zh-CN&ctx=20Sorry, this is not enough info for our engineers to troubleshoot. We need the actual URLs that are giving you the issue.https://partners.nvidia.com/DocumentDetails/Index?siteID=307058Uploading: image.png… \\nimage1032×518 28.6 KB\\nPowered by Discourse, best viewed with JavaScript enabled',\n",
       " 'cloudxr-ar-virtual-objects-behind-real-objects': 'Hey!We have been testing CloudXR for almost half a year and we like it. We would like to overwrite the client application to expand it with some features (in AR).Is it possible to set which object is visible and which is not? We would like to see in the sample application whether the boxes disappear, if we put our hands in front of them. How and/or where is the camera picture and the rendered frame merged?There is not currently support for client AR depth, nor server depth buffer streaming, both needed to attempt to do a depth merge on the client.  We simply alpha blend the AR stream over the camera feed.Depth support is on the long-term roadmap, but I have no specific details beyond that.This topic was automatically closed 60 days after the last reply. New replies are no longer allowed.Powered by Discourse, best viewed with JavaScript enabled',\n",
       " 'will-optix-be-integrated-to-nvidia-studio-driver': 'Hi,tried to install experimental Blender Optix BuildThis is a build of Blender 2.81 with OptiX support for Cycles on GeForce RTX GP…but doesnt seem to work with latest “Studio Drivers”\\n“Version: 431.86 - Release Date: 4 September 2019”so my Question is:\\nwill future “Studio Drivers” support Optix Rendering for Blenderthank youFor sure. I’ll need to ask around when that is going to happen.The OptiX 7 version used inside Blender requires 435.80 drivers or newer and so far there are only GameReady drivers released from that branch.cool, would be nice - thank you for that info and your super fast reply :-)Looks like that will take some more time (not this month). If you’d want to try Blender with OptiX 7 RTX acceleration before that you could only do that when installing one of the GameReady drivers with versions 436.02 and up until then.just want to stick with Studio Drivers and if everthing goes well Blender 2.81 with Optix should be ready in November so totally fine if there is an Optix “Studio driver” at that point.\\nthank you for clarification.Powered by Discourse, best viewed with JavaScript enabled',\n",
       " 'time-varying-rendering-with-optix': 'Hello,\\nI use Optix to render VTK models via ray-casting.\\nI have a series of separate models/meshes that are part of a time-series.\\nEvery second I want to display the next model in the series.Right now I create a optix::Group and optix::Acceleration for each model separate and each time the currently displayed model changes I update the hooks in the context.\\nThat means: context_[“top_object”]->set(mesh_groups[timestep]);My problem now is the delay this creates. Optix seems to do a ton of work in the background creating a delay of nearly half a second.\\nI want to minimize this delay but have no real idea how to.Is there a way to load and set all necessary object on the graphics card and tell optix to just ignore all parts that are not the currently displayed mesh? Some way without having optix to do some heavy background work each time the displayed mesh changes?Right, changing the scene structure or geometry data requires a rebuild of the acceleration structure (AS) and probably the PTX kernel as well.\\nThe time it takes to update the AS depends on the AS builder you used. If you used Bvh or Sbvh, try Trbvh.“Is there a way to load and set all necessary object on the graphics card and tell optix to just ignore all parts that are not the currently displayed mesh?”Yes. If you’re able to load all meshes at once without blowing out the GPU memory, you can do that and put them under a Selector node which behaves like a Group node, but holds a visit program with which you control which children should be traversed by calling rtIntersectChild(index).Search the OptiX Programming Guide chapter Selector and the OptiX API Reference document for all calls related to rtSelector*().Means this Selector can be programmed to be a DIP-switch (multiple of many children) or a radio button behaviour (one of many children).\\nLatter is what you need for things like a flipbook animation (that’s how we called it in SceniX) or like in car configurators to pick different options (e.g. rims).The implementation of how you want to pick which children to traverse and which not is completely your choice. It’s just another program domain you implement a small CUDA program for.Advanced sidenote: The switching has per-ray granularity which allows to use this for some very interesting things, since you can actually pick different children to traverse depending on the per ray payload, intersection distance, etc. But mind that the ray is in object space in the visit program domain in case you use it to implement some level of detail with this.In your case it would just need a single variable at the Selector node containing the zero-based child index to traverse. See example code below.The switching is basically instantaneous then.Hello,\\nthat actually seems to be what I want.\\nI have build a version of this and the switching does indeed no longer add a delay.\\nBut now somehow the closest_hit program does not get called nearly as often as needed.The snippet shows my creation of the selector node and adding a single child to it.\\nIf I directly set top_object and shadower to the mesh_group_ everything works fine.My visit/selection program is exactly as you have posted.But now instead seeing the full mesh I only see a small number of dark pixels that enable me to roughly guess the object shape is correct, but 90% of where the mesh should be shows just the background color.I tried rtPrintf in the intersection program and with the selector only maybe a hundred prints are made, and with directly the mesh_group many thousands are printed.Is there a obvious error or some step I am missing?I have not altered neither ray generation program nor the intersection program to work with the selector node.EDIT:\\nI have narrowed the problem down:\\nIn my Phong-shader I cast shadow rays and they seem to be the problem.This is the code concerning the shadows. I assume it is that the shadow rays do not properly terminate and therefore the result of the phong shading is somehow undefined. But that’s just a guess.Sounds like some bug.The variable scopes checked by the visit program are the program itself and the selector node.\\nPlease try to put the variable flipbookIndex at the Selector node, which would be required when having multiple of them sharing the same visit program anyway.Other than that, please always list the following system configuration details when reporting problems:\\nOS version, installed GPUs, display driver version, CUDA toolkit version used to compile the PTX code.Sometimes it’s just a matter of upgrading the display driver.\\nI would recommend OptiX 3.9.0 and CUDA Toolkit 7.5.(Optimization note: There is no need for a top_shadower object variable if it’s always the same as the top_object. That’s something originated from the OptiX SDK examples which show that and then spread to all examples because they shared common programs.)BTW, here are some additional threads on on Selectors:\\n[url]https://devtalk.nvidia.com/default/topic/815975/?comment=4478105[/url]\\n[url]https://devtalk.nvidia.com/default/topic/669183/?comment=4080792[/url]My system informations are as follows:Windows 10 Pro, 64-bit\\nGeForce GTX 560\\nDriver version: 365.10 (the newest updated just now)\\nCUDA Toolkit 7.5\\nOptiX version: 3.9.0I have changed the scope of the flipbooxIndex, removed the dedicated top_shadower and updated the drivers.\\nNothing changed in the result.Also I made the ray generation program use the mesh group directly and only the shadow computation use the selector node. Then I made a rtPrintf that told my what the visit program selected.\\nJust estimating by the amount of prints that are done (and all with the correct index), there are enough shadow rays generated to be correct.In the scene there is only a single tube, so there should be no hit of any shadow ray, and putting a rtPrintf in the any_hit_shadow program does indeed not print anything.So the problem could only be within the actual acceleration structure of the first child of the selector, (which works without the selector node!) or that the “does not hit anything” termination of rays goes wrong when using a selector node.I then printed out the variables that are used to make the shadow ray to see if there are any NaNs or something stupid, but the values are looking fine.Printing something directly after the shadow rtTrace call prints something, but far to few prints.So the complete rays get somehow “lost” including the closest_hit program that casts the shadow rays.\\nAnd some rays are terminated normally and have a normal light attenuation of 1.0 in all components in the payload.\\nBy using an if-clause I verified that there is no other case.The remaining question is: Why does the shadow trace with a Selector node not work properly? I think I eliminated most of the error sources.Ok, that would require a reproducer to see if this can be reproduced in-house.Please have a look at his thread which explains in the last post how to do an OptiX API Capture trace (OAC). [url]https://devtalk.nvidia.com/default/topic/803116/?comment=4436953[/url]Use a minimal reproducer to trace that because everything is stored to disk and these traces can get huge.\\nPlease remove all debugging rtPrintf code before tracing.\\nThe trace.oac is a text file with all OptiX C API calls which sometimes helps to identify setup errors.We would need the whole reproducing oac<running_number> folder as archive.\\nWe can shortly discuss how to transfer that in a private message.Powered by Discourse, best viewed with JavaScript enabled',\n",
       " 'passthrough-rtx-6000-ada-to-proxmox-vm-linux-driver-crash-follow-up-better-solution': 'I also got this:NVRM: GPU 0000:01:00.0: RmInitAdapter failed! (0x11:0x45:2525]\\nNVRM: GPU 0000:01:00.0: rm_init_adapter failed, device minor number 0while trying to pass a whole NVIDIA RTX 6000 Ada to an Ubuntu 20.04 VM under Proxmox.But my simple solution was to set the so called display mode to “compute” with the displaymodeselector tool.\\nThen this GPU apparently also works without problems in the VM.I’m writing this because the solution here sends me to the wrong track, apart from the fact that the vGPU User Guide gave me the crucial hint about the tool.I don’t need a license and installation of vGPU video driver on the host and on the guest to passthrough the whole GPU.Why this was never necessary for e.g. RTXA5000, is something I don’t understand.Regards,\\nMackeBenchmark results for a QEMU Standard PC (Q35 + ICH9, 2009) with an AMD EPYC 7551P processor.\\nHi @maier and welcome to the NVIDIA developer forums.Thank you for sharing your workaround and the GeekBench results. They look quite impressive.Best of success with your projects!Powered by Discourse, best viewed with JavaScript enabled',\n",
       " 'unending-memory-allocation-with-trbvh-on-launch-in-optix-3-6-0': 'My scene graph consists of a Trbvh top group, with several thousand heavily instanced triangle meshes under transform nodes, each with an individual Bvh/Bvh acceleration ( most of the meshes have 16-bit indices, which don’t seem to play well with most of the other builder types ).  Total of around 6.7 million triangles.In OptiX 3.5.1, this configuration successfully compiled, launched, and rendered.  After updating from OptiX 3.5.1 to OptiX 3.6.0 and CUDA 5.5 to 6.0, however, this setup stalls on the call to launch while continually allocating memory.  Somewhere around 4.5 gb by the task manager, it hits an exception over failure to allocate memory.Windows 7, 64 bit, GTX 650, Driver 337.88.Will you retry this with 3.6.3? We fixed many allocation + Trbvh errors, probably including this one.And you’ll probably be better off paying the extra 36MB to use 32-bit indices so that you can use Trbvh everywhere. Also you could not specify the vertex buffer name and index buffer name, so that it runs your BoundingBox program. You can then keep your 16-bit indices, while still using Trbvh. You just won’t get as good of splits if you’re splitting AABBs instead of triangles.And the biggest thing, try to have as small a node graph as possible. Flatten as much as you can into a single geometry group.DaveHello!\\nI am using Optix 3.7.0 stable version and I have the same problem with Trbvh. My scene has a selector at top level and 2 geometry groups below it. Each geometry group has Trbvh acceleration builder. One of them has small geometry and there is no problem. Another group contains larger geometry: for example, 5000 models with 400 triangles in each. I noticed that if the number of triangles exceeds ~2.6 million, I get the exception on GTX 980 (4 GB of memory):Invalid context (Details: Function “_rtContextLaunch2D” caught exception: Encountered a CUDA error: [15466853] returned (4): Deinitialized, [15270150])\\nSometimes there is driver crash, not just exception.I also tested it on Quadro 2000 (1 GB memory) and maximum number of triangles decreased proportionally: it was only 650000. All GPU memory is never occupied during the acceleration build, only between 1/4 and 1/2 of it.\\nIt seems like GPU memory fragmentation happens. Is it some inefficiency in builder or is this its maximum possible geometry size? I also tried Sbvh builder and I didn’t face this problem with much larger geometry.The Trbvh builder can use a considerable amount of GPU memory during the build process, that is why it can be built in chunks.Please read the OptiX programming guide on the Trbvh builder in chapter “3.5.2. Builders and Traversers” and the “chunk_size” parameter if you run into memory related limits when building the acceleration structure. Also please note the differences for the OptiX commercial license version.There are bug fixes for the Trbvh builder inside the upcoming OptiX 3.8.0 release which are not inside the beta, yet, but if you experience issues with Trbvh in OptiX 3.6.x or 3.7.0 releases it’s worth to try OptiX 3.8.0 beta and the release once available.Try OptiX 3.8 final release. We fixed several memory-related Trbvh and OptiX Prime bugs. I bet this is fixed.Powered by Discourse, best viewed with JavaScript enabled',\n",
       " 'parallel-programming-in-nexus-7': 'Hello,Can you advise how to take advantage of the quad-core Tegra 3 in Nexus 7?  Are multiple threads in Android the best way using Tegra 3?  If I want to do regular computation on a matrix (an image), SIMD seems a natural approach.  Is there a better way to program SIMD than using threads?Thank you for the help.Y. Lunope - threads are the best way to go on the Nexus7 - cheers!Powered by Discourse, best viewed with JavaScript enabled',\n",
       " 'a-motion-blur-question': 'Hi,What are the advantages to use per vertex motion (Geometry nodes), transform motion(Transform nodes) instead of just averaging final frames?Imaging there are a moving camera or/and moving objects in the scene, by averaging rendered images in a time interval, we could generate motion blur effects automatically. Sure, OptixMotionBlur shows a use case, but I still do not quite understand why they are useful and when I should use that.Maybe I missed something important,Thanks,YashizWhen rendering final images at discrete time steps and then average them, you’ll get visible banding like on the right side of this image: [url]http://images.slideplayer.com/15/4786242/slides/slide_10.jpg[/url]The “cook” example inside the older OptiX SDK examples did that for motion blur and depth of field.\\nYou would have to render many final images to fill the motion this way, which is only feasible if your rendering is really fast.In contrast, a stochastic motion blur implementation with a time per ray is able to sample the time continuously in a progressive algorithm which produces smoother but initially noisy results like any Monte Carlo algorithm.\\nAdditionally it allows for camera shutter effects like these in a single launch:\\n[url]https://www.qualitymag.com/ext/resources/VS/March2014/DEPT101/VS0314-DEPT-101-p2SL.jpg[/url]Powered by Discourse, best viewed with JavaScript enabled',\n",
       " 'dual-depth-peeling': 'Good afternoon,have you tried dual depth peeling with lights? I have version of Bavoil source code in opengl 3.3 and I would like to know, where should I light object. When I had lighted each peeled pass I got very bright result. Should I light it in final shader?Thank you.Computing lighting after depth peeling (eg deferred shading) would mean storing and peeling extra data. Unless your lighting is very expensive it might be simpler to compute during depth peeling, especially if you’re keeping all layers and compositing later.My assumption is the bright result you’re getting might be to do with the way you’re blending each layer. Double check the alpha you write during depth peeling is correct and that your compositing uses the right method. Eg:\\nglBlendFunc(GL_SRC_ALPHA, GL_ONE_MINUS_SRC_ALPHA)\\nfor the back layers and, purely quoting from the paper:\\nglBlendEquation(GL_FUNC_ADD);\\nglBlendFuncSeparate(GL_DST_ALPHA, GL_ONE, GL_ZERO, GL_ONE_MINUS_SRC_ALPHA)\\nfor the front layers.Since you’re using opengl 3.3, have you had a look at some of the single-pass order-independent transparency methods? (Eg. with linked lists or packing using prefix sum/scan).I have peeling algorithm like this.glDisable(GL_DEPTH_TEST);\\nglEnable(GL_BLEND);So I tried glBlendFuncSeparate(GL_DST_ALPHA, GL_ONE, GL_ZERO, GL_ONE_MINUS_SRC_ALPHA) and that threw same result. I think, that this can add so much vertex colors, that it can result to extremly brighted color. Maybe I try change GL_ADD with GL_MAXWithout going into the code in detail, do you have both a front and back temporary buffer for compositing layers in each direction? Eg after peeling a front and back layer in a single pass, each is blended to a separate temporary buffer before finally blending the front temporary with the back (getting the alpha of the front temporary here is important too). At least this was my understanding of dual depth peeling.Also, if you have many layers with a small amount of alpha you might need floating point temporaries. Although this would make the image darker, not brighter.Finally, all I can suggest is add a nice way to debug your intermediate data and create a very simple scene. Something so you can calculate the values of the colours yourself. Starting with opaque geometry really helps here.Powered by Discourse, best viewed with JavaScript enabled',\n",
       " '1034-error-rep-dumped-me-here-for-help': 'So my RTX 2070 isn’t handling 4 encoded output streams like its supposed to. Throws a resource tied up error -1034. Nvidia NVENC hardware accelerated compressor initialization failed, used by another application.Rep said to visit NVIDIA VIDEO CODEC SDK | NVIDIA Developer and download it.Of course there’s 2 things here, video codec sdk and FFmeg. She didn’t know. Also didn’t know what to do with them, just said to ask you all.I’m assuming I need the cuda toolkit and either the visual studio or visual studio code. Not sure which of those. All of this is on a windows 10 box.Any guidance is greatly appreciated!Powered by Discourse, best viewed with JavaScript enabled',\n",
       " 'nvidia-now-developer-here-needs-some-help': 'I was on NVidia Support for hours trying to get help and they pointed me to this forum. I currently own/develop a couple Steam games and I’m trying to open up my game on NVidia Now… And yes I’m a Nvidia Now Game Developer. However all my links they gave me besides the Github SDK repo doesn’t work. I’m stuck in an endless loop of getting no support at all, but I won’t let that deter me. I would love to be able to bring my game to the Nvidia Now platform. Can someone help me get in touch with Nvidia and get some needed help.Just as an FYI… I have been trying to get support on this for months through proper channels… So I’m turning to this forum for helpPowered by Discourse, best viewed with JavaScript enabled',\n",
       " 'optix-7-2-long-shadows': 'Good morning,Sorry if this is a dumb question, but I am looking into shadows using OptiX 7.2 and thanks to some great help from everyone here on the forum I have been very successful generating a simple shadowing technique using a kind of visibility test. However, I would like to know how to implement something like “long shadows” from a single light source (out of view). I think this will be executed with shadow rays, but am unsure.Is there some example code using this concept, just a simple cube/sphere/etc. whereby a long shadow is generated?I apologize in advance if this is a redundant question.Thanks,Hey @picard1969,What does “long shadow” mean? What problem are you anticipating that is different from the shadows you have now?In general, your shadow rays should work regardless of the orientation of your geometry and light. If you’re worried about very narrow grazing angles and/or shadow-ray self-intersection, this article might be of interest to you: A Fast and Robust Method for Avoiding Self-Intersection, by Carsten Wächter and Nikolaus Binder, from Ray Tracing Gems Ray Tracing Gems–\\nDavid.Thank you @dhart for the reply. I apologize for not getting back sooner, I have been on a mini-vacation, i.e., long weekend.Maybe a sample image of what I am looking for could explain better (please see “long_shadows.png”). I am guess this is constructed using something along the lines of a shadow ray, but am not 100% certain.Any hints/help would be greatly appreciated.Thanks\\nHey @picard1969,I see, yes you want to add shadow rays to your app to generate these kinds of shadows.There’s a complete working example of shadow ray setups in multiple OptiX SDK samples. The optixPathTracer sample is setup almost like the image you posted; it has a quad shaped area light source that casts shadows on the floor. The sample also includes indirect bounce lighting, which you can disable if you change the maximum depth to 0 in the raygen program in optixPathTracer.cu:Wherever you see the word “occlusion” in optixPathTracer, that’s referring to shadow rays. Note there are special hit & miss programs for the shadow rays, because they do something different from the primary camera (“radiance”) rays.Take a tour through that code, learn how it works, play with it a little (turn off the shadows, change the shadow color, etc.) and let us know if you have any questions about it.–\\nDavid.Hi @dhart,Thank you for the response and the helpful information. I will give the code a look and see how it goes.Also, I found this: optix7course/example09_shadowRays at master · ingowald/optix7course · GitHub which looks like it has shadow rays as well.Please also have a look at the new examples I just added to the OptiX 7 Advanced Samples showing how to implement different light types.\\nhttps://forums.developer.nvidia.com/t/optix-advanced-samples-on-github/48410/8?u=droettgerThese allow placing different singular and area lights into a scene and it would be simple to mimic the above screenshot with some runtime generated or loaded objects and a single rectangle area light.This topic was automatically closed 14 days after the last reply. New replies are no longer allowed.Powered by Discourse, best viewed with JavaScript enabled',\n",
       " 'using-opengl-bindless-textures-in-optix': 'I’m making a hybrid opengl and optix program.\\nI upload all the buffers and textures into OpengGl, and the use the interop functions to reference them in optix.The issue is, I started using bindless textures for OpenGl, and from that point on I can’t create optix textures with any combination of function calls I tried.These function calls make the textures bindless, and allow me to use an array of texture indexes in my shader.\\nAfter making the texture bindless, trying to make a texture from it ends up in an exception.Is it even possible to do what I’m trying to? The return value of glGetTextureHandleARB is a Gluint64, but I failed to find optix functions which would actually take a 64 bit index.\\nUsing the old handle also doesn’t work, whether I try it before or after making it bindless.So what should I do now?Powered by Discourse, best viewed with JavaScript enabled',\n",
       " 'bug-in-pxquat-toradiansandunitaxis': 'I am running into some puzzling behavior, where a slowly spinning box appears to pause after completing a full rotation. It seems the rotation is still simulated correctly, resulting in a constantly varying global pose PxTransform obtained for the shape. However, the angle of rotation obtained from the q member of this transform, remains at a constant value of 3.14… when the box is on its second rotation. To reproduce:Set a breakpoint after the last line. The angle variable will correctly change at about one radian-per-second, for a while, then remain fixed for a same amount of time, then repeat this behavior periodically.Am I using this feature incorrectly or is this a bug? The reason I need the angle/axis representation is that this is the most convenient way to transform the Modelview matrix in OpenGL.Thanks,\\n-nuunHi,\\nThat is troubling, I’ll try to reproduce it and get back to you ASAP.\\nThanks,\\nMikeGot it. It was indeed a bug, fixed in version 3.2.2. I can’t get 3.2.2 to play nice with the Windows runtime dll (I’ll start a new thread about this if I can’t figure it out), but I can keep using 3.2.0 and modify PxQuat.h as follows:Powered by Discourse, best viewed with JavaScript enabled',\n",
       " 'compile-optix-without-cmake': \"I am able to build and run example code from the SDK. But I want to compile and build without using Cmake and write my own makefile. Is there an example of that? especially how to compile ptx file and link it with rest of the CUDA code.Assuming you’re using Microsoft Visual Studio with the CUDA Visual Studio Integration installed.The CUDA host code doesn’t need special handling. You would just need to set the additional include directories to your CUDA toolkit installation’s include folder and add the necessary import libraries for the CUDA runtime cudart_<version>.lib and CUDA driver API libraries cuda.lib.For the OptiX device code *.cu to *.ptx source translation, there must be no linking happening. That would mean to assemble and link the code to cubins, but you only need PTX source code.Instead you would need to setup the CUDA compilation options inside the CUDA Visual Studio Integration dialog to compile only from *.cu to *.ptx files. Means there needs to be some --ptx option on the NVCC command line.I’m not aware of an example doing that for OptiX with the Microsoft Visual Studio Integration alone but it you’d should be able to get that working when setting all other NVCC compile options properly as well.I’ve prepared a CMake message inside the scripts building the custom compile command per *.cu file here, which allows printing the actually used NVCC command line.\\nhttps://github.com/NVIDIA/OptiX_Apps/blob/master/3rdparty/CMake/nvcuda_compile_ptx.cmake#L45Enabling that and configuring the projects in CMake prints these additional messages (with all my local names replaced with placeholders):\\n<your_cuda_toolkit_path>/bin/nvcc.exe --machine=64 --ptx --gpu-architecture=compute_50;--use_fast_math;--relocatable-device-code=true;--generate-line-info;-Wno-deprecated-gpu-targets;-I<your_optix_sdk_version>/include;-I<your_device_header_code_path> <your_device_source_code_path>/<source_name>.cu -o <your_target_bin_path>/$(ConfigurationName)/<source_name>.ptxIf you fill out the <your_..._path> and the ConfigurationName (Debug|Release) placeholders with the local directories on your development system, that would be basically the NVCC command line you would issue inside a command prompt to compile OptiX *.cu device source code to *.ptx source code (maybe without the semicolons). Means you could put this into a batch file and get the *.ptx that way.\\n(Actually that probably also needs the host compiler executable location, a path to your MSVS’ x64/cl.exe. Check the NVCC manual. That’s implicit when running inside the MSVS IDE.)Now you would just set all CUDA Visual Studio Integration options to exactly the same settings.\\nMind that there is neither the -G nor -g debug option in this command line but this would be the default for Debug targets in MSVS. You should disable the debug flags in all targets for now. OptiX device source code debugging functionality is still work in progress.Maybe have a search for CUDA Visual Studio Integration tutorials. Though most of these will explain how to compile CUDA kernels to cubins, but you’ll get the idea.Thanks for the answer, I am working on a Linux system, not using CUDA Visual Studio. I will try to use the nvcc option you provide to compile the .cu to ptx first.I am working on a Linux system. I wrote the following make files base on(ray_tracing/makefile at master · apc-llc/ray_tracing · GitHub ) for the helloptix example and it compiled but failed when it ran. Can you tell what is wrong with the make file?error message:\\n$ ./optixHello\\n[ 4][ KNOBS]: All knobs on default.[ 4][ DISK CACHE]: Opened database: “/var/tmp/OptixCache_qchen/cache7.db”\\n[ 4][ DISK CACHE]: Cache data size: “16.0 KiB”\\nCaught exception: Couldn’t open source file draw_solid_color.cu.SUFFIXES: .ptxall: optixHelloOPTION = -O3 -std=c++11optixHello: optixHello.o draw_solid_color.ptx\\n$(NVCC) -arch=sm_$(ARCH) $(filter %.o, $^) -o $@ -L$(OPTIX)/build/lib -lglfw -lglad -lsutil_7_sdk -Xlinker -rpath=$(OPTIX)/build/li\\nb\\n-L$(CUDA_SDK)/lib64 -lcurand -lnvrtc -Xlinker -rpath=$(CUDA_SDK)/lib64draw_solid_color.ptx: draw_solid_color.cu\\n$(NVCC) -I$(OPTIX)/include -I$(OPTIX)/SDK $(OPTION) -arch=sm_$(ARCH) -ptx -c $< -o $@optixHello.o: optixHello.cpp\\n$(NVCC) -Xcompiler “–std=c++11” -I$(OPTIX)/include -I$(OPTIX)/build -I$(OPTIX)/SDK -I$(OPTIX)/SDK/support/GLFW/deps\\n$(OPTION) -arch=sm_$(ARCH) -I$(OPTIX)/SDK/sutil -I$(CUDA_SDK)/include -c $< -o $@random: optixHello\\n./$< random $(NUM_SPHERES) $(NUM_LIGHTS) $(WIDTH) $(HEIGHT) $(FILE_NAME)clean:\\nrm -rf optixHello *.o *.ptx $(FILE_NAME)(Disclaimer: I’m not using Linux myself.)The referenced makefile looks reasonable but is for a very old OptiX version and needs adjustments.\\nSome of the folder names in there like the SDK-precompiled-samples don’t exist in OptiX 7 version anymore.\\nOptiX 7 is a header only API and none of the “optix” export libs are required anymore.\\nYou seem to have adjusted that correctly.Then that old OptiX version still supported 32-bit device code which is not possible since OptiX 4.0 anymore.\\nMeans you’re missing the --machine=64 option I have listed inside the NVCC command line options.Then you’re referencing the $(ARCH) variable which is not set inside the makefile but in the makefile.in inside the referenced repository and is set to 30 (first generation Kepler GPUs) which is not supported by current CUDA Toolkit versions.\\nThat should be set to at least 50 to work on all GPU architectures supported by OptiX 7.Again have a look at the command line options I listed before.Note that all OptiX SDK examples rely on the sutil library which needs to be built as well.\\nOwn applications don’t need to use that which would make the makefile simpler.CURAND shouldn’t be required for optixHello.None of that “random” stuff should be inside your makefile because that is specific to the application inside the repository you’ve copied this from.Did you structure the source files the same way with *.cpp and *.cu files inside the same folder?Now, running OptiX applications which load precompiled PTX source files should not report errors about not finding *.cu files unless you’re compiling them at runtime using NVRTC which the referenced makefile is not doing. But you have added some NVRTC option. From the error output, that could be the main problem.If you single step inside the debugger through the compiled program’s debug target, you should be able to determine what code path was throwing the exception.I’m not sure about the other NVCC invocations in that makefile but to translate from *.cu to *.ptx that should look something like this (not tried):You need to be able to build the OptiX SDK examples with CMake at least once to get the sutil library and the SDK examples with their working *.ptx code for comparisons.Check what the output name of the *.ptx file is. The OptiX SDK example expect a specific naming scheme. Please look at the sutil::getInputData() and sutil::sampleInputFilePath() functions to see where and under which name the OptiX SDK examples expect these files.\\nAgain this is something specific to how the OptiX SDK examples are using the sutil library which I cannot recommend in own application frameworks.--machine=64 --use_fast_math --relocatable-device-code=true --generate-line-info -Wno-deprecated-gpu-targets --ptxHi, Thanks for the answer.\\nI found another better example for the experiment. This one does not use any Sutil and openGL library. rtx_compute_samples/optixSaxpy at master · NVIDIA/rtx_compute_samples · GitHub\\nI am able to build and run with Cmake, and able to compile with my own make file. But got error when running it. After some debug, I found the problem is with the ptx file. If I use the ptx file compiled by Cmake and it works.Error: message using Ptx compiled with my own make file\\nOptix Log[4][DISK CACHE]: ‘Opened database: “/var/tmp/OptixCache/cache7.db”’\\nOptix Log[4][DISK CACHE]: ’    Cache data size: “110.1 KiB”’\\nOptix Log[4][DISKCACHE]: ‘Cache miss for key: ptx-1824-key2b4ce8b7bc557e1673b0b65ceb67938a-sm_70-rtc0-drv465.19.01’\\nOptix Log[4][COMPILE FEEDBACK]: ‘’\\nOptix Log[4][DISKCACHE]: ‘Inserted module in cache with key: ptx-1824-key2b4ce8b7bc557e1673b0b65ceb67938a-sm_70-rtc0-drv465.19.01’\\nOptix Log[4][COMPILE FEEDBACK]: 'Info: Module uses 0 payload values.Info: Module uses 0 attribute values. Pipeline configuration: 2 (default).\\nInfo: Entry function “__raygen__saxpy” with semantic type RAYGEN has 0 trace call(s), 0 continuation callable call(s), 0 direct callable call(s), 1 basic block(s), 19 instruction(s)\\nInfo: 0 non-entry function(s) have 0 basic block(s), 0 instruction(s)\\n’\\nOptix Log[3][PIPELINE CREATE]: ‘params variable “params” not found in any module. It might have been optimized away.’\\nOptix Log[4][COMPILE FEEDBACK]: ‘Info: Pipeline has 1 module(s), 1 entry function(s), 0 trace call(s), 0 continuation callable call(s), 0 direct callable call(s), 1 basic block(s) in entry functions, 19 instruction(s) in entry functions, 0 non-entry function(s), 0 basic block(s) in non-entry functions, 0 instruction(s) in non-entry functions\\n’\\noptixSaxpy.cpp:225 CUDA Error: ‘an illegal memory access was encountered’\\nMax error: 2\\nOptix Log[2][ERROR]: ‘Error synching on OptixPipeline event (CUDA error string: an illegal memory access was encountered, CUDA error code: 700)\\nError destroying OptixPipeline event (CUDA error string: an illegal memory access was encountered, CUDA error code: 700)’\\noptixSaxpy.cpp:234 Optix Error: ‘Invalid value’\\nOptix Log[4][DISK CACHE]: ‘Closed database: “/var/tmp/OptixCache/cache7.db”’\\nOptix Log[4][DISK CACHE]: ’    Cache data size: “121.5 KiB”’\\nOptix Log[2][ERROR]: ‘Failed to destroy launch resources (CUDA error string: an illegal memory access was encountered, CUDA error code: 700)’\\noptixSaxpy.cpp:239 Optix Error: ‘Invalid device context’\\noptixSaxpy.cpp:241 CUDA Error: ‘an illegal memory access was encountered’\\noptixSaxpy.cpp:242 CUDA Error: ‘an illegal memory access was encountered’\\noptixSaxpy.cpp:243 CUDA Error: ‘an illegal memory access was encountered’\\noptixSaxpy.cpp:244 CUDA Error: ‘an illegal memory access was encountered’Then I compare two ptx file and found following differences:\\nworking one:\\ncall (%r1), _optix_get_launch_index_x, ();\\nld.const.f32  %f1, [params+4];\\nld.const.u64  %rd1, [params+8];\\ncvta.to.global.u64  %rd2, %rd1;\\nmul.wide.u32  %rd3, %r1, 4;\\nnot working:\\ncall (%r1), _optix_get_launch_index_x, ();\\n$L__tmp0:\\n.loc  1 44 3\\nld.const.f32  %f1, [__nv_static_55__42_tmpxft_00021c6b_00000000_7_kernels_cpp1_ii_396a1715_params+4];\\nld.const.u64  %rd1, [__nv_static_55__42_tmpxft_00021c6b_00000000_7_kernels_cpp1_ii_396a1715_params+8];\\ncvta.to.global.u64  %rd2, %rd1;\\nmul.wide.u32  %rd3, %r1, 4;If I modify the ptx file __nv_static_55__42_tmpxft_00021c6b_00000000_7_kernels_cpp1_ii_396a1715_params to just params and it works. Can you tell what might cause this difference?I add an echo in the Cmake to print the compile command for ptx\\n[ 16%] Building CUDA object optixSaxpy/CMakeFiles/optixSaxpy_kernels.dir/src/kernels.ptx\\n/usr/local/cuda/bin/nvcc -ccbin=/opt/rh/devtoolset-6/root/usr/bin/c++ -forward-unknown-to-host-compiler -I/local/NVIDIA-OptiX-SDK-7.1.0-linux64-x86_64/include -/rtx_compute_samples/.\\n-I/rtx_compute_samples/optixSaxpy/./include -O3 -DNDEBUG -std=c++11\\n-MD -MT optixSaxpy/CMakeFiles/optixSaxpy_kernels.dir/src/kernels.ptx\\n-MF CMakeFiles/optixSaxpy_kernels.dir/src/kernels.ptx.d -x cu -ptx /rtx_compute_samples/optixSaxpy/src/kernels.cu -o CMakeFiles/optixSaxpy_kernels.dir/src/kernels.ptxmy make file\\nkernels.ptx: kernels.cu\\nARCH = 70\\nOPTION = -O3 -DNDEBUG -std=c++11\\n$(NVCC) -I$(OPTIX)/include  $(OPTION) --machine=64 --use_fast_math  -arch=sm_$(ARCH) --relocatable-device-code=true --generate-line-info -Wno-deprecated-gpu-targets --ptx  -c $< -o $@I tried to add MD, MT\\n, but it also does not change the ptx outcome. I hope you tell me how can I compile like the Cmake did, or is there other dependencies I might need to pay attention to?\\nAnd one more question regarding ptx file. is there another way to wrap the ptx file or use other format, so it does not expose the assembly code like this?\\nThanks and regardsWell, I don’t know why the original compile command is not setting the machine=64 option. Maybe that is the default in newer CUDA toolkit versions.\\nThe other differences between the command line options are the --relocatable-device-code=true, --generate-line-info, --use_fast_math, and it’s unclear which architecture the original uses.The --generate-line-info is responsible for the .loc 1 44 3 output which indicates a source code location and is benign.\\nThe --relocatable-device-code=true changes how the compiler produces code. This setting is required to be able to use direct or continuation callables inside OptiX which are just functions which aren’t called inside a module and are therefore eliminated as dead code since CUDA 8.0 if you don’t set that option or the --keep-device-functions option which in turn is not available when using NVRTC for runtime compilation.\\nSince the kernel.cu is not using callables, it’s not strictly necessary here.\\nNonetheless, that should not result in non-functioning code.The --use_fast_math is used to get a lot faster approximations for trigonometric functions, sqrt, and reciprocals.\\nThat is highly recommended for fast ray tracing kernels. Though these compute examples are comping from the high performance computing world where usually more precision is required. Depends on your use case.You never mentioned the system configuration you’re using.\\n(OS version, installed GPU(s), VRAM amount, display driver version, OptiX (major.minor.micro) version, CUDA toolkit version (major.minor) used to generate the input PTX, host compiler version.)\\nSome of that would have been covered when providing the full PTX source instead of small code excerpts.The base streaming multiprocessor target 7.0, your ARCH setting, will only work on Volta and newer GPUs for the OptiX device code.So what happens when you’re using the exact same command line options inside your makefile as the ones you’ve dumped from the the CMake build environment?\\nI see no reason why that shouldn’t result in exactly the same PTX code and that would have been the first thing I would have tested.Note that these compute examples have a root CMakeLists.txt at the top level of the repository folder.\\nhttps://github.com/NVIDIA/rtx_compute_samples/blob/master/CMakeLists.txt\\nAlways use the top-level CMakeLists.txt to configure all examples inside the repository.\\nNote that this is setting additional options, for example C++14.Thanks!\\nTurn out it is caused by --relocatable-device-code=true. Now it makes sense, since the compilation difference is related to how params are referenced in memory (__nv_static_55__42_tmpxft_00021c6b_00000000_7_kernels_cpp1_ii_396a1715_params vs params), and the error message is related to params’ location as well.In this saxpy code, SaxpyParameters are defined in both kernl.cu and .cpp files.\\nstruct SaxpyParameters {\\nint N;\\nfloat a;\\nfloat *x;\\nfloat *y;\\n};If I am not using --relocatable-device-code=true in my future code development, is there a potential risk? Is this error might just relate to this Saxpy code? like you mention it is required to be able to use direct or continuation callables inside OptiX. Should I just use --keep-device-functions?my setting:\\nCuda version: 11.3\\nOptix: 7.1\\nGPU: V100\\nNVIDIA-SMI 465.19.01Thanks!\\nTurn out it is caused by --relocatable-device-code=true. Now it makes sense, since the compilation difference is related to how params are referenced in memory (__nv_static_55__42_tmpxft_00021c6b_00000000_7_kernels_cpp1_ii_396a1715_params vs params), and the error message is related to params’ location as well.In this saxpy code, SaxpyParameters are defined in both kernl.cu and .cpp files.\\nstruct SaxpyParameters {\\nint N;\\nfloat a;\\nfloat *x;\\nfloat *y;\\n};If I am not using --relocatable-device-code=true in my future code development, is there a potential risk? Is this error might just relate to this Saxpy code? like you mention it is required to be able to use direct or continuation callables inside OptiX. Should I just use --keep-device-functions?One more question, is there a way to compile or wrap the ptx file inside the executable? Or does the ptx file have to be outside of the executable?my setting:\\nCuda version: 11.3\\nOptix: 7.1\\nGPU: V100\\nNVIDIA-SMI 465.19.0If you’re only using NVCC to translate your OptiX device code to PTX source, then you could also use --keep-device-functions to prevent the dead code elimination by the compiler.\\nIt’s just that the runtime compiler NVRTC doesn’t support that option.The OptiX Programming Guide explains this as well:\\nhttps://raytracing-docs.nvidia.com/optix7/guide/index.html#program_pipeline_creation#program-inputStill, I have been using --relocatable-device-code=true in all my OptiX examples using callable programs and have never experienced any issue with the generated code. You can see that in my OptiX 7 examples’ CMake scripts.\\nhttps://github.com/NVIDIA/OptiX_Apps/blob/master/apps/intro_runtime/CMakeLists.txt#L132\\nhttps://github.com/NVIDIA/OptiX_Apps/blob/master/3rdparty/CMake/nvcuda_compile_ptx.cmake#L50Means either setting should produce working code. If you say that using the exact same compiler options once without and once with --relocatable-device-code=true generates working resp. non-working code, that would be unexpected.\\nNote that inside the project you’re citing, there is a native CUDA kernel which is compiled and linked, and there is an OptiX device code which is translated to PTX only. All options mentioned above only apply to the OptiX device code. The options for the native CUDA kernel shouldn’t be changed.One more question, is there a way to compile or wrap the ptx file inside the executable? Or does the ptx file have to be outside of the executable?The optixModuleCreateFromPTX function takes a const char pointer to the PTX source data and its length as arguments.\\nMeans you just need to have that pointer in memory, that’s all. How you get it there is completely your responsibility.\\nThe simplest thing is to load the PTX code from a file for faster turnaround times during development.\\nAnother approach often used is the bin2c executable inside the CUDA toolkit to convert any binary file to a hardcoded constant character array variable as C source.Thanks, Is the PTX code device dependent? If I want my RT code can work on different GPU, do I need to generate multiple PTX, and load it accordingly?Is the PTX code device dependent?To a certain degree. The streaming multi-processor (SM) target you chose for your *.cu to *.ptx compilation will define the minimum compatible GPU since using newer SM targets may let NVCC generate instructions not available on older GPUs.do I need to generate multiple PTX, and load it accordingly?That is not necessary and there actually have been cases, where OptiX couldn’t handle some of the newest available instructions in SM targets from CUDA toolkits newer than the one OptiX itself was built with, so please always refer to the OptiX Release Notes for the recommended development environment.If I want my RT code can work on different GPUIf you want to target all GPU architectures supported by OptiX 7 with one set of PTX modules, you should translate your OptiX device code to SM 5.0 which is the first Maxwell GPU generation.\\nList of available SM versions here:\\nhttps://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#features-and-technical-specificationsNote that the SM 5.0 target is deprecated in current CUDA toolkits and will throw deprecation warnings you can suppress with the aforementioned -Wno-deprecated-gpu-targets option inside the command line I posted earlier which also uses SM 5.0 for all of the OptiX device code.Maxwell GPUs are comparably old and slow in the meantime and if you don’t care about Maxwell support you could also use SM 6.0 as minimum target which means Pascal and newer architectures.For compatibility of native CUDA applications using cubins and PTX source, read these chapters of the CUDA programming guide:\\nhttps://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#binary-compatibilityPowered by Discourse, best viewed with JavaScript enabled\",\n",
       " 'flex-random-gravity-bug': 'Hello, I’m using NVIDIA FLEX library for soft body simulation(shape-matching / clusters) and when my program start, i can get, randomly, strange simulation behavior, looks like, gravity become to hard (about twice).  All solver params and update subSteps are the same and not changed anywhere. This bug become randomly: some times simulation work fine (work as how it is configured), but some times, i think, with extra gravity. Periodicity of this is very randomly but I noticed that this “bug” more often occurs on more powerful graphics cards: I tested it at\\nGTX 660: “Bug” will appear about 1 times per 50 tets\\nGTX 980: 1 per 10-20 tests\\nGTX 1060: 50/50\\nI don’t know how to fix it and why/from what/where it comes. I have a suggestion that this may be due with cuda context because i also use some physx library that is is written and based at physX 2.8.3 version and also work at gpu. PhysX initialized before flex. is there a problem with that?\\nWhen i initialize flex there is no errors (i init whit deviceIndex=-1 at flexInit() method).\\nAll solver parametrs, particles, rigids and othes params are the same with bug or not. I use just one solver. At NVIDIA panel PhysX settings set to “auto” and choosed video card. I use FLEX version 1.0.0(32-bit) at 64 bit version Windows 10 pro. Video driver version: 361.43.The problem I have posted several days ago [url]https://devtalk.nvidia.com/default/topic/977901/physx-and-physics-modeling/problems-positioning-rigid-flex-objects-/[/url] could share the same error with the gravity than yours. I implemented a “particle-shape” wich was positioned some meters above the ground. When I start the simulation everything is ok and the shape drops, however depending on the shape position the movement is not right and the particle-shape moves in every direction. I think the problem should be caused by the gravity. I did some tests that prove this error (See my post).I would very much appreciate if someone can give us some hints about how to resolve this problem.Angel.Most likely it is connected with particle velocity. And why there are lines in the flex samples code?:\\n[url]http://i.imgur.com/WY8yENZ.png[/url]\\nThis lines are not working in default, g_warmup = false;\\nCan someone from the flex dev team answer/explain why these lines were in samles code or say about some velocity/gravity strange behavior?Powered by Discourse, best viewed with JavaScript enabled',\n",
       " 'what-are-prerequisites-to-learn-angularjs': 'Atleast a basic knowledge on Javascript and typescript is mandate to learn AngularJSPowered by Discourse, best viewed with JavaScript enabled',\n",
       " 'unclear-about-why-dev-dri-renderd0-is-missing': 'Hi there,I’m trying to help setup a remote visualization protocol on my University’s HPC using VirtualGL and EGL, and the issues seem to be related to a missing file /dev/dri/renderd{number} which as far as I understand should be created by the Direct Rendering Manager when the drivers are installed, but it appears that didn’t happen.The nvidia drivers were successfully installed, so that programs can use the P100 available on the machine, and nvidia-smi shows\\n±----------------------------------------------------------------------------+\\n| NVIDIA-SMI 440.64.00    Driver Version: 440.64.00    CUDA Version: 10.2     |\\n|-------------------------------±---------------------±---------------------+\\n| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\\n| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\\n|===============================+======================+======================|\\n|   0  Tesla P100-PCIE…  On   | 00000000:0B:00.0 Off |                    0 |\\n| N/A   21C    P0    25W / 250W |      0MiB / 16280MiB |      0%      Default |\\n±------------------------------±---------------------±---------------------+±----------------------------------------------------------------------------+\\n| Processes:                                                       GPU Memory |\\n|  GPU       PID   Type   Process name                             Usage      |\\n|=============================================================================|\\n|  No running processes found                                                 |\\n±----------------------------------------------------------------------------+I also see that /dev/dri/card0 exists.Can anyone explain what step needs to be taken to ensure /dev/dri/renderd{number} exists for this card? Thanks!I have reason to believe that the title includes the wrong name for the file also, and that I’m specifically curious about why /dev/dri/renderD128 isn’t in /dev/driPowered by Discourse, best viewed with JavaScript enabled',\n",
       " 'tk1-which-gpios-could-be-wake-up-source-when-system-in-lp0-mode': 'Hi there,\\nTK1 Which GPIOs could be wake up source  when system in LP0 mode\\nI only find out GPIO_x1~x7_AUD + GPIO_W2~w3_AUD ; Total : 9 GPIOs\\nDo these GPIOs could be wake up source when system in LP0 mode?Powered by Discourse, best viewed with JavaScript enabled',\n",
       " 'struct-of-vectors-instead-of-vector-of-structs-in-optix-api': 'They say(*) that one should use struct of vectors instead of vector of structs for 3d-vector representation for better performance.I mean instead of:I should use something like that:There are good reason for that, like float3 is not GPU cache line aligned and so on.But I don’t understand why there is no such interface for nVidia Optix. For example, there is only one ray definition format RTP_BUFFER_FORMAT_RAY_ORIGIN_DIRECTION which is float3:origin float3:direction.I’m using millions of rays: ray generation, ray tracing and ray processing are 98% of my application workload. Optix consume ~33% of my application workload. And it looks like I can significantly speedup my part of code using that “struct of vectors approach”.So, can we get Optix API with struct of vectors instead of vector of structs? Or I’m mistaken and float3 is just good enough?First, there are two APIs inside the OptiX SDK:The define RTP_BUFFER_FORMAT_RAY_ORIGIN_DIRECTION you cited is from OptiX Prime (RTP for Ray Tracing Prime). All these buffer formats are hardcoded in OptiX Prime and the core takes care to load them as efficiently as possible.Separating the individual float components of a float3 vector into different non-interleaved float arrays doesn’t make sense though. That will ruin the memory accesses when gathering the individual floats compared to reading a float3, which are both read the same way but latter is more often in the same cache line.\\nThe cache argument also holds if the two float3 for ray origin and ray direction, which are both needed at the same time to build a ray, lie next to each other.From your question you seem to be using OptiX Prime and spend most of the time generating rays and handling the hit results?\\nYou do all that with CUDA on the GPU?\\nAre you using asynchronous intersection queries?\\nDo you have multiple queries in flight to work in parallel?If all that processing takes too long, maybe it makes sense to use OptiX and leave the parallelization to that. You have control over the ray generation, ray tracing, and hit event processing (closest hit and any hit) in there and the programmable any_hit domain allows ray continuation.\\nMeans you could possibly handle your whole algorithm in a single launch.Also when using the high level OptiX API you can structure your buffers as you like.\\nYou could for example put your attributes into individual buffers, or use one buffer and put them into a structure-of-arrays format.If you suspect that the loading of the data is a performance problem, you could use float4 instead, to get the vectorized load operation. Though because ray tracing structures are memory intense, it makes sense to save memory. Means what works best or at all depends on the scene size and underlying hardware.I’m normally using an array of structures for my vertex data and that was only 2.5% slower than a structure of arrays with the same data in one test, but is much more convenient to work with.\\nI’m also using a single buffer for these only. For OptiX I recommend to reduce the number of buffers because that’s normally faster and needs fewer operator invocations to access the data.When working with OptiX Prime you’re forced to adhere to the built-in data structures for the query and hit buffers. With OptiX you can do what you like.\\nThere is also an optixRaycasting example inside the OptiX SDK which shows how to use OptiX for intersections only similar to OptiX Prime.Yes, you got it right, I’m using Optix Prime. I’m sorry, that I didn’t mention about Prime.Also I have to introduce our project. It is about thermal solution for satellites. To get idea about it please find our poster “GPU Accelerated Spacecraft Thermal Analysis” from GTC GTC 2022: #1 AI ConferenceProject source code can be found here BitbucketSeparating the individual float components of a float3 vector into different non-interleaved float arrays doesn’t make sense though.\\nThat will ruin the memory accesses when gathering the individual floats compared to reading a float3, which are both read the same way but latter is more often in the same cache line.I have to disagree. Code runs in warps of 32 threads and coalescing memory access is supper efficient. 32 threads x 4 bytes per float = 128 bytes cache line (global memory access line) exactly.\\nMay be if there is strong thread divergency, then reading 2xfloat3 is good. And may be ray-tracing of (random) rays is that case.\\nBut there is another code around ray-tracing. In my case ray-tracing time is 33% of pipeline. That is why I want my code work supper-efficiently.The cache argument also holds if the two float3 for ray origin and ray direction, which are both needed at the same time to build a ray, lie next to each other.2xfloat3 is not aligned to 128-bytes cache line. Coalescing memory access is impossible because 24 bytes are not aligned eventually. How all that works with 32 warp threads?\\nIn my case even simple algorithms like filtering rays can’t utilize memory bandwidth more then 50-60%. That is why I suppose that 2xfloat3 is an issue.From your question you seem to be using OptiX Prime and spend most of the time generating rays and handling the hit results?Yes.You do all that with CUDA on the GPU?Yes. Almost everything.Are you using asynchronous intersection queries?No. Fixed synchronous pipeline significantly simplify code and architecture.\\nBut I’m using async queries for scene update.Do you have multiple queries in flight to work in parallel?No. I’m going to implement multiple queries using multiple GPU - just replicating every workflow step to every GPU.If all that processing takes too long, maybe it makes sense to use OptiX and leave the parallelization to that.I moved almost everything to GPU if it is available. So GPU utilization is ~98% and power consumption is about ~70%.You have control over the ray generation, ray tracing, and hit event processing (closest hit and any hit) in there and the programmable any_hit domain allows ray continuation.\\nMeans you could possibly handle your whole algorithm in a single launch.TLDR; It will be too serious vendor lock ; )Is it possible to integrate anything else: conductivity, static heat source (engines, electronics) in that ray-tracing workflow? Actually, I’m not sure that it will by good idea anyway. Consider that application should work on a bare CPU also.\\nConductivity is a graph solving problem. It is pretty efficient on CPU, but I’m not sure that I can implement it on GPU in a right way.Also when using the high level OptiX API you can structure your buffers as you like.\\nYou could for example put your attributes into individual buffers, or use one buffer and put them into a structure-of-arrays format.Sorry, I don’t get that.If you suspect that the loading of the data is a performance problem, you could use float4 instead, to get the vectorized load operation. Though because ray tracing structures are memory intense, it makes sense to save memory. Means what works best or at all depends on the scene size and underlying hardware.Yes, padding to float4 may improve performance. Yet I will need additional step and memory to convert float4 rays into float3 rays for Prime. And this is an issue.\\nAnother problem is padding itself. Best algorithm (with small amount of computing) will be 25% slower because of lost bandwith.I’m normally using an array of structures for my vertex data and that was only 2.5% slower than a structure of arrays with the same data in one test, but is much more convenient to work with.Really? I reverted my “optimized” CUDA code so many times, so I can believe in that. Main lesson I got with CUDA: just write simple and robust code, it will just work efficiently (or eventually work efficiently on new compute capabilities ; )When working with OptiX Prime you’re forced to adhere to the built-in data structures for the query and hit buffers. With OptiX you can do what you like.Sorry, but I don’t believe in magic : )\\nAm I right that Optix works on top of Optix Prime? Then it will perform that data conversion implicitly.Let me summarize:Back from vacation.\\nOk, so you’re using CUDA kernels highly optimized for memory accesses in your code around the actual OptiX Prime ray tracing. That wasn’t apparent to me from the initial post so I was slightly confused about the potential benefit of your structure of float arrays.To 2.)\\nWhat I had been comparing was the order of vertex attributes in my OptiX renderer, not the rays.\\nGoing from structures of arrays of float4 data for the four attributes I’m using (position, tangent, normal, texcoord) to an array of a per-vertex structure with these fields was only a 2.5% different in overall rendering performance in my highly divergent path tracer. Performance is limited mostly by the traversal there.In OptiX Prime the query and hit result formats are hardcoded and there is no way to feed in your structure of arrays into that directly.\\nBut, as said, the high level OptiX API does allow to structure input/output buffers as you like and has multiple program domains (ray generation, closest hit, any hit, miss, etc.) which are fully under your control by implementing the necessary CUDA code!There is an OptiX example named optixRaycasting inside the OptiX SDK which shows how to use OptiX for ray intersection testing only, similar to an OptiX Prime ray wavefront use case, but with all the additional flexibility of the high level OptiX API (including ray continuation via any-hit programs, custom primitive intersections, more flexible scene graph, etc.).\\nThe ray-generation program and buffer layouts in OptiX are freely programmable, which means you can implement a ray generation program which constructs the ray origins and directions from any buffer layout you desire, including your structure of float arrays. The ray-generation program  would just copy the six floats from your structure of arrays before calling rtTrace() with the resulting ray.The ray generation program in OptiX normally has perfect occupancy because all threads are running the same instructions. I would start measuring an OptiX 1D launch for that linear memory layout.The BVH traversal core of OptiX and OptiX Prime is the same when using Trbvh, so the resulting intersection performance can be expected to be in the same ballpark.Hello again!I got it. My initial question is malformed and has almost no sense:Sorry for confusion : (Optix interface looks pretty attractive. But I need to understand several things before.\\nSo there are my current (final?) questions.I. Is Optix based on Optix Prime under the hood?\\nI believe it should. Then there should be implicit conversion: “The ray-generation program would just copy the six floats from your structure of arrays before calling rtTrace() with the resulting ray”.But (in case Optix is based on Optix Prime API) it should perform that conversion in batches using GPU global memory. In that case I can perform such conversion explicitly (in my simulation engine).\\nAnd I can amortize conversion overhead using mini-batches in several threads, CUDA streams, whatever.Another way is on-the-fly conversion.\\nLike in CUB fancy iterators: CUB: cub::TransformInputIterator< ValueType, ConversionOp, InputIteratorT, OffsetT > Class Template Reference\\nBut I can’t find any Prime API for such on-the-fly conversion. I guess this is because it is C-like and does not support any kind of C++ templates or lambdas.II. Is there any chance that Optix Prime API will accept struct of arrays for ray buffers in the future?\\nThen I can get rid of the conversion (memory bandwidth) overhead.\\nAnd it will be a win! : )I.) It’s not based on OptiX Prime, it just shares some code. That’s exactly the idea and advantage of the OptiX high-level API abstraction which allows arbitrary internal implementations without changing the user API, and that happened a lot already when comparing the initial version 1.0 to the current version 5.0.When trying this with the OptiX API, you provide the ray generation program and fully control the input and output buffer layouts and how and where in your application you want to build the ray query buffers.OptiX has a single-ray programming model and abstracts any scheduling and some more CUDA specifics (grid and warps, synchronizations, shared memory, ballot instructions, and the like; see the caveats chapter inside the OptiX Programming Guide). While you do not have all CUDA hardware programming features available due to that abstraction, it makes the OptiX device code programming rather easy because you just use standard CUDA C++ for the individual OptiX program domains’ code.Powered by Discourse, best viewed with JavaScript enabled',\n",
       " 'how-does-xavier-support-hardware-accel-decode-for-cuda10-no-libcuvid-so-found': 'I am compile ffmpeg with cuda 10 and enable cuvid, but it cannot found the libcuvid.so, please help how to support hardware decode?Powered by Discourse, best viewed with JavaScript enabled',\n",
       " 'nvidia-opengl-driver-cannot-be-found': 'Hi,I have update to a Aourus Geforce RTX 2080 Super Waterforce from a GTX 1060 and since that moment I haven’t been able to play X-Plane 11.The description for Event ID 1 from source NVIDIA OpenGL Driver cannot be found. Either the component that raises this event is not installed on your local computer or the installation is corrupted. You can install or repair the component on the local computer.If the event originated on another computer, the display information had to be saved with the event.The following information was included with the event:Unable to recover from a kernel exception. The application must close.Error code: 3 (subcode 7)\\n(pid=11392 tid=12300 x-plane.exe 64bit)Visit NVIDIA Customer Support for more information.The message resource is present but the message was not found in the message tableI’ve reviewed lots of forums, web, etc and I think I’ve tested everything, from installing a clean OS, different driver versions and including TdrDelay in the registry Key.I don’t know what to do. I thought I have bought a very powerful card but the I can’t enjoy one of my favorite hobbies.This is my system summary:[System Summary (J:\\\\PAP01.nfo)]Item\\tValue\\t\\nOS Name\\tMicrosoft Windows 10 Pro\\t\\nVersion\\t10.0.18362 Build 18362\\t\\nOther OS Description \\tNot Available\\t\\nOS Manufacturer\\tMicrosoft Corporation\\t\\nSystem Name\\tPAP01\\t\\nSystem Manufacturer\\tGigabyte Technology Co., Ltd.\\t\\nSystem Model\\tZ390 AORUS MASTER\\t\\nSystem Type\\tx64-based PC\\t\\nSystem SKU\\tDefault string\\t\\nProcessor\\tIntel(R) Core™ i9-9900K CPU @ 3.60GHz, 3600 Mhz, 8 Core(s), 16 Logical Processor(s)\\t\\nBIOS Version/Date\\tAmerican Megatrends Inc. F11c, 12/18/2019\\t\\nSMBIOS Version\\t2.8\\t\\nEmbedded Controller Version\\t255.255\\t\\nBIOS Mode\\tLegacy\\t\\nBaseBoard Manufacturer\\tGigabyte Technology Co., Ltd.\\t\\nBaseBoard Product\\tZ390 AORUS MASTER-CF\\t\\nBaseBoard Version\\tx.x\\t\\nPlatform Role\\tDesktop\\t\\nSecure Boot State\\tUnsupported\\t\\nPCR7 Configuration\\tBinding Not Possible\\t\\nWindows Directory\\tC:\\\\WINDOWS\\t\\nSystem Directory\\tC:\\\\WINDOWS\\\\system32\\t\\nBoot Device\\t\\\\Device\\\\HarddiskVolume7\\t\\nLocale\\tUnited States\\t\\nHardware Abstraction Layer\\tVersion = “10.0.18362.752”\\t\\nUser Name\\tPAP01\\\\root\\t\\nTime Zone\\tRomance Daylight Time\\t\\nInstalled Physical Memory (RAM)\\t32.0 GB\\t\\nTotal Physical Memory\\t31.9 GB\\t\\nAvailable Physical Memory\\t26.2 GB\\t\\nTotal Virtual Memory\\t33.5 GB\\t\\nAvailable Virtual Memory\\t4.73 GB\\t\\nPage File Space\\t1.56 GB\\t\\nPage File\\tC:\\\\pagefile.sys\\t\\nKernel DMA Protection\\tOff\\t\\nVirtualization-based security\\tNot enabled\\t\\nDevice Encryption Support\\tReasons for failed automatic device encryption: TPM is not usable, PCR7 binding is not supported, Hardware Security Test Interface failed and device is not Modern Standby, Un-allowed DMA capable bus/device(s) detected, TPM is not usable\\t\\nHyper-V - VM Monitor Mode Extensions\\tYes\\t\\nHyper-V - Second Level Address Translation Extensions\\tYes\\t\\nHyper-V - Virtualization Enabled in Firmware\\tYes\\t\\nHyper-V - Data Execution Protection\\tYesThanks in advance for your support!RegardsPowered by Discourse, best viewed with JavaScript enabled',\n",
       " 'skipping-frames': 'Hello,I am in need of only decoding and displaying “IFrames” in various content. (reducing decoder load is the goal here rather than merely discarding full frames later in the pipeline)Currently I am using the standard CUvideoparser and hooking into the pfnDecodePicture callback. Basically:The rest of the code is pretty identical to the nvidia sample apps. This appears to work fine for awhile. However, after about ~12 hours the average CPU load for my application increases by about %15-20. Debugging seems to imply nvcuvid.dll threads are responsible for this increase but I’m not entirely certain.Is this because of how I’m using the CUvideoparser? Am I better off using my own parser? The documentation is somewhat vague on the use case of “I don’t want this frame anymore” and how/if the parser carries any direct references to it anymore after calling the callback. I’m not even sure what returning true/false does in the pfnDecodePicture callback. Anyone know if there’s something further I need to do to discard a frame from the parser in this callback?Powered by Discourse, best viewed with JavaScript enabled',\n",
       " 'hairworks': 'Hi!\\nI’m a indie developer and i still very excited with gameworks, especially Hairworks and Flex, i’ve been worked during 2 weeks in hairworks, but no sucess, how can i attach the guide splines on the mesh? I use Unreal Engine for development but when I import APX. the hair dont follow mesh animation. I search in forum but dont see the answer… Hope us help me, it is my last attempt.\\nSorry for my bad english! thx for allHi.Please follow the document.[url]http://docs.nvidia.com/gameworks/content/artisttools/hairworks/HairWorks_tutorialUE4.html#hairworks-tutorialue4[/url]Hi!\\nI am looking for existing hairs contents suitable with hairworks.\\nWhere can i find that?\\nThanks a lot.ps: use inside UNREAL ENGINE -  I don’t want to create myself haits (using Ornatrix for example)Powered by Discourse, best viewed with JavaScript enabled',\n",
       " 'jetson-nx-gpio-config-as-input-but-cant-work': 'Our program defines nx pin 16 as input pin. We set pin mode as falling status and expect that if The pin status changes from high to low, our callback registered will be called .The connection outside the NX is as follows(designed by our hardware engineer):The line(i.e. Trig_IN1) in the red circle designed to connect with nx pin 16.We find a strange phenomenonif Trig_IN1 not connects with nx pin 16, the period siginal is normal, The signal is as followsBut When Trig_IN1 connects with nx pin 16,  the period siginal is abnormal,  The high level become 1.6V not 3.3V, and the falling can’t be detected( we guess 1.6V regard as low)  The signal is as follows,We also do a experiment to change the resistantor in the red rectangle(0.3K, 20 K), it also can’t workIs there something beyond the export, direction, and value writing that needs to be done to use the GPIO pins with the Dev kit?Powered by Discourse, best viewed with JavaScript enabled',\n",
       " 'to-flex-or-not-to-flex-that-is-the-question': 'Hi PhysX Gurus,Well I’m extremely impressed with the Flex demos (https://developer.nvidia.com/flex) I compiled for my box and am very impressed & intrigued by the Flex concept that separates it from PhysX: “that EVERYTHING is a system of particles connected by constraints”… way cool! :-)The two-way coupling that Flex can bring between rigid bodies, soft bodies, clothing and fluid is a huge differentiating factor for the game I’m working on and am drooling at the possibilities…Unfortunately the fly in the ointment is that thus far Flex only runs on recent NVidia hardware. This severely limits the percentage of people who can buy my game to about 20% of the market (excluding those with AMD cards or NVidia cards older than GeForce 7xx series, mainstream laptops, etc)However in the 2+ year-old post at PhysXInfo.com - MEGAGAME.AI, one of the Flex developers mentioned a port to DirectCompute with a possible CPU implementation later on. Both of those are critically needed for Flex to have a chance to make it in general consumer games…Q1: When do you think Flex will have a CPU implementation?Q2: With the planned port of Flex from CUDA to DirectCompute, would Flex then run on AMD cards?Q3: What changes will be made to Flex in PhysX 3.4?Q4: When will PhysX 3.4 be released?Thanks for taking the time! :)Powered by Discourse, best viewed with JavaScript enabled',\n",
       " 'choosepixelformat-describepixelformat-crash-with-2-nvs-510-and-5-monitors': 'I have a user of my OpenGL application which crashes on their system as soon as either ChoosePixelFormat for DescribePixelFormat is called. I’m at a loss on how to approach this (basic troubleshooting done, recent driver, etc).They have two NVS 510 cards (not SLI) and 5 monitors (4 on one card, and one on the other). According to dxdiag, four of the monitors are 1440x900. Two are connected via “Displayport External” and two through DVI. On the other card is a 1920x1200 monitor connected via “Displayport External”.Is there anything special that needs to be done with their system? A different driver or something?Any help is much appreciated, thanks!Powered by Discourse, best viewed with JavaScript enabled',\n",
       " 'no-camera-from-shield-tablet-open-source-binary-driver-release': 'Hi,I recently built an android image by following http://nv-tegra.nvidia.com/gitweb/?p=manifest/android/binary.git;a=blob_plain;f=README;hb=rel-st8-l-r6-partner.I used wx_na_wf-userdebug for my wifi only tablet.But, I found the below error messages from logcat and I am not able to use the camera.Please note that I ran ./extract-nv-bins.sh as explained in the instruction.\\nI think that libscf.so is missing for some reason.Is this intentionally missing from nvidia?Best,\\nSeunghoonI have the same issue. Some camera stuff seems to be included in the build, but when I try to run my app that uses the android camera api, the tablet doesn’t seem to recognize any of the cameras and seems to think there are none. How are they included in the configuration and build?As it seems that no nvidia engineers follow any of the forums and most questions are eventually answered by the person who had the problem, I have figured out what shared libraries are missing and got the camera to work. However, it doesn’t work nearly as well as the stock image. I’m trying to scan barcodes with zxing and the stock image works great and scans super fast. The source provided by nvidia takes several seconds for the barcode to be recognized. This isn’t acceptable for our application.In case anyone is interested the missing files are: libgov_ui.so, libnvcap.so, libnvhdcp.so, libnvhrcv.so, libnvisp_v3.so, libnvmm_camera_v3.so, libnvvicsi_v3.so, libscf.so. These need to be added to /vendor/tegra/core/modules.mk and /vendor/nvidia/tegra/prebuilt/shieldtablet/Android.mk. Some of them were not included in the proprietary binaries and I had to steal them from the stock image and some just weren’t included in the build.And if any nvidia engineers are out there, will you PLEASE respond and make sure nothing else is missing so the camera will work as well as the stock image!!!Powered by Discourse, best viewed with JavaScript enabled',\n",
       " 'how-can-i-indicate-specific-gpu-card-in-nvjpeg': 'I’m examining nvjpeg and curious how i can indicate index of GPU card.\\nI cannot find any clue from document and example source code.Any help?Powered by Discourse, best viewed with JavaScript enabled',\n",
       " 'frame-rate-drop-while-path-tracing': 'Hi,\\nI have a path tracer program which uses Optix 6.5 (similar to the path-tracer example of the SDK). Due to the complexity of the scene a lot of frames (~100000) are necessary to get a decent image (even with denoising). During raytracing the frame-rate is dropping with increasing frame number to one fourth of the original frame rate or even lower.\\nWith GPU-Z I can see that the memory controller load is increasing with increasing frame number. But there is no change in the payload or the data which is saved or transferred to the CPU (at least I’m not aware of it). The raytracer is not using recursion. The slowdown also happens when I run the program in benchmark mode, without displaying the current result.\\nWhen I pause the raytracer to let the GPU cool down, it is proceeding with the same low frame rate after I continue the program. So it does not seem to be a temperature issue.Any idea, why the GPU is becoming slower?\\nAny hints, what one could do to prevent the continous drop in the frame rate?Welcome bvb70.Unfortunately it’s not possible to say what is going on there. This shouldn’t really happen.Could you please provide the following system configuration information:\\nOS version, installed GPU(s), VRAM amount, display driver version, OptiX (major.minor.micro) version, CUDA toolkit version (major.minor) used to generate the input PTX, host compiler version.What exactly do you mean with “the memory controller load is increasing with increasing frame number”.\\nIs more CPU or GPU memory used the more frames you render? That sounds like a memory leak inside the application then.Do you use the OptiX C++ wrappers in your application?\\nDo you change any scene data and forgot to destroy the previous data? The C++ wrappers in the old OptiX API don’t do that automatically.I assume you’re not using the denoiser per frame but only at the very end at that sub-frame count.\\nIf not, does this also happen when not using the denoiser?Could you please try looking at the clock rates of the GPU in frequent intervals while the performance gets slower and when running the second slow iteration?\\nMaybe it got stuck in a low power state.\\nAssuming Windows OS, the NVIDIA SMI tool is normally installed to C:\\\\Program Files\\\\NVIDIA Corporation\\\\NVSMI and  allows to query that with the command line: nvidia-smi.exe --query --display=CLOCK\\nCheck the nvidia-smi manual (PDF) in that folder for many more options.Other than that,  there are multiple path tracer examples I’ve written against OptiX 5.1 (which also build under 6.5) and OptiX 7 versions.\\nWould you be able to verify if these behave correctly to eliminate a system dependency?\\n(When using the old OptiX Introduction examples with MSVS versions 2017 and newer, please set the CUDA_HOST_COMPILER CMake variable manually. The old FindCUDA.cmake is out of date in that repository.)Please find links inside the sticky posts of this sub-forum, e.g. here:\\nhttps://forums.developer.nvidia.com/t/optix-advanced-samples-on-github/48410I would recommend using OptiX 7 versions for new projects if possible.The OS is Windows 10 (Version 10.0.17763.973]\\nI have seen the “drop in frame rate effect” with a Geforce RTX 2080 super, a Geforce RTX 2080Ti and a Quadro RTX 4000.\\nThe Geforce Display Driver is 446.14-desktop-win10-64bit-international-dch-whql.exe\\nIt’s optix 6.5.0 and Cuda 10.1.2\\nand Visual Studio Professional 2017 Version 15.9.25The GPU memory, which the program uses (~3GB, depending on the scene), stays constant. It is the GPU memory controller load, which is increasing. According to the internet it “measures how much of your total memory bandwidth is being used”.I use Optix C++ wrappers. The scene data is not changed. The only thing that changes is the frame number, which is used for the random number generation on the GPU.I’m not using the denoiser during the frame rate test.I’ll do the other tests you proposed and record the GPU clock rate.What is  remarkable so far is that  during the first 16000 frames the frame rate stays more or less constant. After 32000 frames the frame rate is only ~83% of the original frame rate and it is further decreasing (after 65000 frames it is ~73%, after 13100 its 58%…)Thanks. I have not seen such a result, but I also do not render 100,000 iterations\\nThat you need that is either due to a really complex lighting setup, a sub-optimal light transport algorithm for the job, or a bad random number generator, (or any of these together).If this is a display driver issue, there is only the option of trying out newer display drivers first.\\nThere are plenty of newer ones than 446.14.\\nThat one is not even on the list of RTX 2080Ti Windows 10 64-bit drivers anymore:\\nhttps://www.nvidia.com/Download/Find.aspx?lang=en-usI’m speculating wildly, but I can imagine a couple of reasons this could be happening.If your frames are getting more random over time for any reason, then you might be suffering from lower cache coherency (due to e.g. increasing randomness). You can use Nsight Compute to verify whether this is true or not. Capture a launch early on, then capture a launch much later and use the baselines feature to compare them, while perhaps looking at the memory workload analysis.Another possibility is that you’re getting fragmentation in the OptiX 6.5 memory manager. This could be harder to detect, but you could have your program tear down and recreate a new OptiX context every 1000 frames or something like that. If the behavior is resolved, then you can point the finger at OptiX. If that’s the problem, then the easiest/best solution to fixing it would be use use OptiX 7.The only thing that changes is the frame number, which is used for the random number generation on the GPU.I agree with Detlef, this could be a reason for degrading performance. Try adding a large number to your frame seed right from the start and see if the behavior changes.It may be worth taking a step back and asking why it’s taking 100k iterations to get decent results even with denoising. You haven’t mentioned what kind of problems you’re solving or why the scene is complex, but is a number this high really expected? This is the kind of thing I get if I use pure path tracing without next event estimation (meaning don’t sent shadow rays but wait to hit an emitter randomly.) You might be able to find orders of magnitude more speedup by investing in sampling algorithms or  bidirectional light transport algorithms. Solving the slowdown here can only net a total maximum 4x improvement.–\\nDavid.Powered by Discourse, best viewed with JavaScript enabled',\n",
       " 'vrworks-360-2-1-seams-alignment-issue': 'Our company works in VR area. We are shooting videos using 5-cameras or 8-cameras rigs, and stitching them into 360 degrees panoramic video to watch it via Oculus. We found your site with VRWorks360 tools, and found your methods as very interesting and efficient. We downloaded your version 2.1 for Windows and calibrated our shots using your nvcalib_sample.exe. We have managed to generate calib_rig_spec.xml with quality 0.92-0.93. Then we started to apply your application nvss_sample.exe. It aligned our camera frames in a proper way, but at the seams we could see different artifacts, especially if we have moving objects intersecting the seams.We used different options like “–dept_align”, “–mono_eq”, but the most interesting option in such situations is “–seam_offset”. Our idea was to get the stitched image without seams offset (i.e. with the default seams), and some images with some offsets to left and right, and then replace some rectangles around moving objects intersecting a default seam by rectangles located on the same positions from some images stitched with the seam offset.However, we found that your images obtained with seam offset are not aligned to the image stitched with the default seams. In your documents we read thet you could set the offsets before the alignment and during stitching. Obviously, in nvss_sample.exe they were set before the alignment and during the alignment you take into account their positions. But it looks like significantly less useful tool than just moving the seams after the alignments.Please, could you help us with it? Maybe you have an option that was not indicated in ./samples/nvss_sample/README? Or maybe you could get the access to another executable that does exactly the same but with setting the shifted seams after the alignment?Powered by Discourse, best viewed with JavaScript enabled',\n",
       " 'opening-shot-work-in-progress': '1st shot for a personal work in progress.\\nGoal is to use real-time reflected water caustics as my primary light source!\\nFinal piece.\\nPowered by Discourse, best viewed with JavaScript enabled',\n",
       " 'a100-80g-end-of-life': 'When is End of Life and End of Support for A100 80GB and A100 40GBHi, we only just now ramped up production shipments of H100 systems, so A100 will still be around for a while. Pls contact your preferred system vendor for end of life statements on your systems.\\nAFAIK we haven’t even EOL’d A100 (all varaints of them) to our partners, and our parnters will be able to do last time forcecast and order, so should be able to deliver A100 for much longer. For details, like I said, pls contact your partner.\\nEnd of SW support of the product is still many years out…we support typically for at least 3 years after last time buy… most of the time MUCH longer, since with our unified driver, there is no imminent need to remove cards activly from support…\\nhope this help…\\n-FrankPowered by Discourse, best viewed with JavaScript enabled',\n",
       " 'pcie-power-connector': 'My question is want to know depth information how different between 6/8pin.As I know wire that used on the 6pin connector is 3 wire for +12Vdc and 3 for Ground and on 8pin is 3 for +12Vdc and 5 for Ground, So where is excess current come from? It seem like ratio between 12Vdc wire(rail) input and ground is not equal. Add another ground without add any of 12Vdc rail but got twice power?Thanks you :)Powered by Discourse, best viewed with JavaScript enabled',\n",
       " 'scene-queries-without-stepping-the-simulation': 'I have an application where I need to be able to move actors in the scene and then perform overlap queries. Whenever I query the scene the list of actors seems to be outdated - as if the moving actors have not moved since they were created.Since I’m not stepping the simulation (no calls to collide(), fetchCollision(), advance() and fetchResults()), do I need to perform some other actions before querying the scene?How are you moving the actors? If you are using setGlobalPose(…), these changes should be reflected in the next query after you set the pose. If you are using setKinematicTarget, the kinematic doesn’t move until the next simulation step so, if you don’t actually simulate, it will never move. Setting a kinematic target tells the simulation to move the object to this location using velocity in the next simulation frame. However, if you raise the flag PxRigidBodyFlag::eUSE_KINEMATIC_TARGET_FOR_SCENE_QUERIES on the rigid bodies, scene queries should use the kinematic target so queries will behave like the object’s pose was set using setGlobalPose().Thanks Kier!The PxRigidBodyFlag::eUSE_KINEMATIC_TARGET_FOR_SCENE_QUERIES flag is exactly what I was looking for.Powered by Discourse, best viewed with JavaScript enabled',\n",
       " 'future-proof-32-bit-optix-application': 'The problem I faced here have made me think on this topicMany years ago I have created a Delphi Application which cannot be ported to 64 bit (half a million of lines of code and many external 32 DLL which I cannot rebuild (no source code)I use Optix since 2.0 days and I’m very happy. Currently I use 3.5.1 version.During the last 12 month (or so) NVidia has released 3 new Optix versions 3.6, 3.7 and 3.8 with new features. BUT NVidia declared to cease 32 bit supportNo problems with that since I’m quite happy with version 3.5.1The Problem stats with my new PC which has an NVidia Quadro K2100M card inside, when I used my application with the new PC I got an error (“Details: Function “_rtContextLaunch2D” caught exception: Encountered a CUDA error: cuGLGetDevices() returned (304): Unknown, |3801520|”)From what I read the Optix 3.5.1 is not able to detect the card which, I have to say, is not the “cutting edge” of the NVidia processors (Kepler-based cpu)I tried the new library (32 bit build) to solve the incompatibility but I feced with the issue aboveSo I wonder is it possible to “switch off” the check performad by Optix “cuGLGetDevices() returned (304): Unknown” so I can continue to use an Old (and funcional DLL) ?If not, the “32 bit guys” like me will face a serious problem ==> in few year their SW will not works becouse of new unsupported GPU (which of course will appear)For the moment I resumed my old PC, but I not so happy with this solution…Am I wrong with my thought/fears ?RDPowered by Discourse, best viewed with JavaScript enabled',\n",
       " 'remove-kinematic-mode-on-collision': 'Hello,\\nI have a particular case that I have to solve and I’m not sure on the best way to do it.Here is an example of what I want to do.\\nLet’s say I have a big peace of rock running down an hill. It’s a physX rigid dynamic let free (no kinematic flag).\\nI also have characters running down the same hill, in front of the rock.\\nWhat I want to achieve is to make some of my characters fall whenever they are hit by the rock, but depending on some conditions of my game logic.My first approach.\\nThe first way I did it was setting the characters in kinematic mode (PxRigidBodyFlag::eKINEMATIC flag on the character’s rigid bodies). When there was a collision, I would record it thanks to the “onContact” callback, and on the next step in my game engine, I would remove kinematic flag.The problem with this approach is that I’m one frame late to remove the flag: when physX detects the collision, it instantaneously reacts with the kinematic object and bound on my character. When my game engine realize there is a collision (after the physX step), I change remove the kinematic flag, but it’s already too late, the wrong reaction already occurs.\\nI wish I could remove the kinematic flag during the collision callback, but obviously it’s forbidden to change the physics during this part.Another way.\\nThe way I’m thinking of doing it right now is as follow.\\nWhen a collision occurs between rocks and characters,The downside of this method is that I loose 1 frame:But the good point is that when the character have to switch in dynamic mode, the rock smash him with its full force without the strange reaction it used to have.Does anybody see a better way of doing what i want to achieve ?\\nThe way I want to do it, I have to use the CPU filter system to track pairs and solve the contact or not depending on the fact that it was already tracked, so I’m afraid I’ll considerably slow down the physX simulation even if I do the minimum and fastest thing I can do with filters.I’m using physX 3.3.3 from the git repo.Have you considered using a trigger shape instead of using contact modification?Have you considered using a trigger shape instead of using contact modification?Hello,\\nYeah, that was my third option.\\nBut what I am doing is full ragdolls, so I was afraid:I tried my option yesterday, and it seems all fine for now… I still have to test it on a higher diversity of assets.Powered by Discourse, best viewed with JavaScript enabled',\n",
       " 'cuvid-decode-cuvidgetvideoframesurface': 'HI!\\nHow to use the “cuvidGetVideoFrameSurface” Interface?Powered by Discourse, best viewed with JavaScript enabled',\n",
       " 'multimedia-api-libargus-v4l2-xavier-nx': 'Hi all,I am using a 376*376 camera and I want to stream it on my display…The source can be either libargus or v4l2 but I dont see an appropriate code in multimedia api to stream it…Are there any sample applications to do so…If so can you kindly send…The resolutions seems  to be the problem here.Thanks,\\nAshwinHi @ShaneCCC ,Can you please let me know about this?Thanks in Advance!Suppose Argus unable support smaller size sensor less than 640*480, Maybe you can try v4l2 base APP and demosaic by software then display to screen.Hi Shane will try that but can you please explain how to get it through Argus itself…Current working to support smaller size sensor, but due to we don’t have this kind of design may take more time to figure it out.Okay thanks @ShaneCCC ,Can you give a sample v4l2 application to get the mipi data.ThanksYou check with v4l2-ctl to get the raw data like below command.Yeah @ShaneCCC we have the command but a sample c/c++ application would be better as we need to play around with the bayer data.Have a check MMAPI sample code./usr/src/jetson_multimedia_api/samples/v4l2cudaPowered by Discourse, best viewed with JavaScript enabled',\n",
       " 'large-force-reaction-when-object-sliding-over-evenly-lined-up-but-separate-objects': 'PhysX 3.2.1, PC SDK 64 bitHello.We are experiencing an issue that is causing sliding vehicles to bump into the air on the separation between two objects.For example, say we build a road with slabs made from rectangular PhysX box shapes. We slide a bunch of these together making sure they are perfectly aligned and most importantly at the same height so there are no bumps.(See here: External Media for an example with intentional spacing put in to show the slabs)When we use impulses to slide a car over these aligned slabs of road, the car bounces and sometimes has an angular velocity imparted on it at every \"joint\". Leaving a small gap, or intentionally partly overlapping the objects does not fix the issue. The only mitigation we’ve foind is to cut the slabs in the travel direction like so /-------//-------/. We are assuming the PhysX solver is somehow \"seeing\" the joint between the objects even though from the top surface the sliding object should not be affected.Turning CCD on or off does not seem to affect this issue.This issue is especially important to us because we’re using the PhysX SDK for a 3d virtual world, and people expect objects to react to surfaces how they seem them. This is preventing the creation of smooth racetracks, bridges and other surfaces that are friendly to vehicles.I have tried adjusting the RestOffset and ContactOffset and nothing seems to make the issue better. In fact, surprisingly, increasing the rest offset seems to make it worse.Thank you for your help.Hello,is it possible to post a pvd capture of the bug?  With luck this will provide the information required to understand the problem better.Thanks,GordonHi,\\nI’m having a similar issue. Where you able to solve this? If yes, could you please guide me how?I would like to add that I am also seeing this and would also very much like to see a work around. This happens to me in Unity and Unreal, and this single bug has put my game on hold.Even if someone could point me to the area of the code base that controls this would be helpful.From trying to figure what was going on in Unity I determined that PhysX appears to send back multiple collisions when the seem between two objects is reached. I am thinking that one possible fix would be to flag the seems somehow (maybe using UV2 or UV3) so that the PhysX engine can test for the flag, and ignore the collision when it sees the flag.My problem is trying to figure out the code base is a big learning curve, and may be over my head to be honest. A pointer to the right file to lookup would be really helpful.Powered by Discourse, best viewed with JavaScript enabled',\n",
       " 'baking-to-texture': 'Hi,I’m quite new to optix. How one would start it’s journey on writing simple light baking to a texture (separate uv map)?So, instead of pinhole/thin camera, I need camera that’s accepting rays from given object uv space. Is it even possible in optix?Ok, thinking out loud.We can decide how we throw rays using RT_PROGRAM.\\nHowever I’ve no idea how do that from texture uv space for given object.Hi michax,One possibilty is you ray trace your object UV map, instead of your object. You can store your positions in the UV map. Then you can compute your lighting or what ever from hit positions:0, do general scene graph and acceleration structure setup.\\n1, create a Optix buffer A, and store your object UV map with positions (world positions over UV space) into the buffer which could be the same size of your output_buffer.\\n2, in ray generation (your camera function), read from your buffer A, if there is valid data in the buffer, read the position and generate a ray (from your view to that position) Edit: Ray of “view to position” may make your ray blocked, which might not whatyou need. As Detlef said, you could set position as your ray origin.\\n3, the another things should be no difference to a common ray tracer.HIH,YashizI’m quite new to optix. How one would start it’s journey on writing simple light baking to a texture (separate uv map)?\\nSo, instead of pinhole/thin camera, I need camera that’s accepting rays from given object uv space. Is it even possible in optix?Yes, you have the full control about which rays to shoot in OptiX.Though if you’re a beginner with OptiX or ray tracing in general there is a steep learning curve ahead in the following pseudo algorithm description.There are basically only two main things to solve, finding the world positions per texel center (once), and integrating the incoming light, most recommended with a progressive algorithm (many OptiX launch() calls with the texture size).Let’s start simple and assume you have a single texture map which is mapped to some geometry and you want to bake incoming light into the individual texels of that map.\\nMeans you have a grid of texels you want to write to, that’s your output buffer, and your launch size.Then you have some geometry with texture coordinates which map this texture uniquely, means each texel is used only once on that geometry. Let’s say the geometry is built from triangles.If you want to do this perfectly per texel, you would need to calculate where on your geometry each texel center lies in world coordinates. That’s possible because you know the texture size, the triangle geometry, and the texture coordinates on the triangles.\\nThe simplest approach has quadratic complexity: Two loops: per texel, per triangle: If the texel’s uv coordinate is inside the triangle’s texture coordinates, then calculate the world position at the texel’s texture coordinate on the triangle by interpolating the triangle vertex coordinates, break. No duplicates possible!\\nIt should be more efficient to go over all triangles and find the texels they touch. Texels which are not covered would need to be skipped during baking. They would remain black and could later be filled with some color of the surrounding areas to allow filtering of the texture without artifacts.You do this only once in a pre-process. Those world coordinates are your ray origins. You can write these into a buffer you read in your ray generation program to initialize the OptiX Ray.For each ray origin you also need to figure out the face normal at that point on the triangle. That’s the center of the hemisphere into which you need to shoot rays to integrate the incoming light at that texel.\\n(Possible would also be the shading normal here, for potentially smoother results, but then you would need to make sure ray directions over that hemisphere do not penetrate to the actual surface.)Then you write a progressive renderer which integrates the incoming light at that texel with some algorithm shooting many rays into the upper hemisphere over that texel, defined by ray origin and triangle face normal, and accumulate the result into your output buffer by launch index (means per texel).Normally that’s done with ambient occlusion to capture only the shadows because that’s a view independent method. Baking full global illumination isn’t going to look quite right when rendering with that, because many lighting effects are view dependent.There has been a quite lengthy thread on some baking topics before:\\nhttps://devtalk.nvidia.com/default/topic/923772/?comment=4875006Thank you both for such detailed response!All is clear. Let’s see what I’ll be able to do with it. Coding time!Following your suggestion I managed to write my first lightmapper :) There are still some issues to resolve like expanding uv edges (otherwise seams look fugly). But overall I’m supprised how good it looks. For sure new AI denoisers adds a lot and works great with GI lightmapping.Lightmap texture:\\n[url]https://i.imgur.com/gDYiojz.png[/url]Lightmap texture mapped to object (opengl):\\n[url]https://i.imgur.com/2PiNvHo.png[/url]Scene used for baking:\\n[url]https://i.imgur.com/ztWxkTm.png[/url]I’ve one followup question.Currently my ray shoots from world_position + normal, with direction of inverted normal. Is this correct approach? When I was shooting from world_position with normal direction, I got ortho view from each face (which makes sense :))Once again, Thank you very much for help! So far, I really enjoy working with Optix!“Currently my ray shoots from world_position + normal, with direction of inverted normal. Is this correct approach?”No. That would capture incoming light at a point which is not on your triangle and from the wrong directions. That point might even be inside some other geometry and you’d get black when doing that.“When I was shooting from world_position with normal direction, I got ortho view from each face (which makes sense :))”That is a correct direction for one of the hundreds or thousands of rays you would need to shoot per texel to integrate the incoming light! That’s why I said this needs to be a progressive algorithm.The face normals of a triangle are normally defined on their front face, which in turn is defined by the triangle winding. In a right-handed coordinate system with counter-clockwise winding with float3 triangle vertices (v0, v1, v2) that would be\\nfloat3 face_normal = normalize(cross(v1 - v0, v2 - v0));Now together with the world position (== ray origin on the triangle) that face normal defines the center of the upper hemisphere over that point.\\nThat whole hemisphere needs to be integrated to capture the incoming light at that world position on that triangle.\\nThat is done by shooting many rays (hundreds to thousands) from that single ray origin into its upper hemisphere with a cosine weighted distribution of ray directions. Means one of the ray directions can be that face normal. You need some random number generator to sample these directions.Examples calculating these directions can be found inside the OptiX SDK, for example inside the  optixPathTracer. Search for the code calling into cosine_sample_hemisphere, which works in local coordinates and gets transformed into world coordinates with an ortho-normal basis defined by your face normal:Something like this gives you such a direction:You right. My approach only works because diffuse program already does what you pasted (sampling hemisphere for secondary ray).Will change my ray generator and see how it looks after implementing your suggestion.Thanks.Works great! Thanks Detlef.Output image is noisier but also more accurate. In addition, rays have GI color without source object albedo. Which is prefered option for me.Lightmap with dillate filter applied for edges (edge padding):\\n[url]https://i.imgur.com/b4oKoSE.png[/url]And Lightmap on opengl mesh:\\n[url]https://i.imgur.com/f9RqIVR.png[/url]Next steps:Update.I was able to run my lightmapper on google cloud VM with 4 x P100 GPU’s. Sadly, it doesn’t scale well.\\n4 x P100 setup is only 3x times faster then my laptop’s 960M.In nvidia-smi I can see that GPU usage jumps from 0 to 100% across all GPU’s.What could cause the slowdown? I’m not doing any device memory mapping between frames/launches.\\nHowever I’ve quite a lot float3 buffers.I can also notice that CPU is at 14%.Would appreciate any help in profiling potential bottleneck.\\nAs for now, going through performance suggestions from: [url]http://raytracing-docs.nvidia.com/optix/guide/index.html#performance#13001[/url]As an reference, octane renderer performs 24x faster on 4xP100 then my local laptop.Update.After following [url]http://raytracing-docs.nvidia.com/optix/guide/index.html#performance#13001[/url] and limiting amount of buffers (remaining ones moved to float4), post processing on each launch I’ve hit 20x performance (P100) against my laptop (960M).It’s pretty good, but there is still some room for improvement. When I’ve forced optix/cuda to use one tesla p100. One frame took 265ms. So with 4 x P100, one frame should take around 66 ms. Currently it takes 78 ms.I’ve noticed that everything performs much faster in MultiGpu env when I use RT_BUFFER_GPU_LOCAL. However, understandably, for outputBuffer which I use to save lightmap, it causes articafts (black strips over rendered image).[url]https://i.imgur.com/eK5eHKj.png[/url]Is it possible to wait a little bit and get output_buffer in full when it has RT_BUFFER_GPU_LOCAL type?\\nIf yes, then I would get 67ms per frame, which gives me 23x performance against base and almost perfect scallability result of 3,97x for 4 devices.Another topic I need to investigate is progressive launch. For that I probably need to create progressive buffer bound to output buffer, and pool it’s values. Or is there some kind of progressive update callback?Edit.\\nOk, I’ve found some resources on progressive streaming. Could improve performance further.2.98 MBIs it possible to wait a little bit and get output_buffer in full when it has RT_BUFFER_GPU_LOCAL type?No. This has been discussed quite recently:\\nhttps://devtalk.nvidia.com/default/topic/1029361/\\nhttps://devtalk.nvidia.com/default/topic/1024818/When I’ve forced optix/cuda to use one tesla p100. One frame took 265ms. So with 4 x P100, one frame should take around 66 ms. Currently it takes 78 ms.Scaling of multiple GPUs per OptiX context is not linear with the number of GPUs because rendering happens to pinned memory via the PCI-E bus from all of the boards which comes with some overhead. Compare to the scaling ratio on two boards.Another topic I need to investigate is progressive launch.If you do not have access to an NVIDIA VCA system with a matching OptiX server implementation, there is no benefit of using the progressive API inside OptiX. https://devtalk.nvidia.com/default/topic/883964/\\nThe optixProgressiveVCA example demonstrates how the progressive API can be used.post processing on each launchI added a switch to my renderers to not present the individual accumulation steps done per launch, but only the launches in the first half second and then once every second, unless the accumulation is restarted. The main benefit esp for multi-GPU setups is to save the PCI-E bandwidth of texture transfer for the display since there is no OpenGL interop possible then. The renderer is accumulating anyway and that way you can also see some incremental improvements better if they are displayed only every second.I wonder what kind of results would i get from splitting my 1024x1024 output buffer onto separate renders tasks and assign it to separate GPU. When I render image at half of it’s resolution, time goes down lineary.So If I create 4 render task with 512x512 size, I should get linear improvement.PS. 10ms seems like quite a big number for PCI-E bus overhead for relatively small task (1024x1024).I added a switch to my renderers to not present the individual accumulation steps done per launchCould you please give more details about how you do this?\\nHow to make a “switch” to make sure that the output buffers are not present per launch ?Thank you very much,YashizThat’s all on application side and pretty basic stuff.I use a configuration text file with a lot of settings instead of command line parameters to control my renderer setup. For example which devices to use, if OpenGL interop should be used or not, camera settings, etc. One of the options is a boolean flag controling if the output buffer should be displayed on each launch or in some time interval, one second in my case.I use a timer class, a display() function which calls into a render() function and does the final texture blit, renders the GUI, and does a SwapBuffer.The render() function does the OptiX launch() and then checks if the timer has passed beyond a threshold which should trigger an update of the OpenGL texture used to display the resulting image.\\nOnly if the time has passed, the render function will copy the OptiX output buffer into the texture, advances the time threshold at which the next update should happen by one second, and since I have a timer running anyway, prints some performance numbers as well. The display routine then has a new image to work with.Means the read of the output buffer happens only once a second unless anything required a restart of the accumulation (camera moved, material parameters changed, etc.). Then the render routine updates the texture immediately (actually I do that for the first half second to always see some accumulation) and advances the time threshold for the next present again. And so forth.I also have slightly different behaviors for interactive rendering and benchmarking as well.I use this timer implementation from our open-source nvpro-pipeline:\\n[url]https://github.com/nvpro-pipeline/pipeline/blob/master/dp/util/Timer.h[/url]\\n[url]https://github.com/nvpro-pipeline/pipeline/blob/master/dp/util/src/Timer.cpp[/url]Update.I’ve improved performance of path tracer further. However this only increased a gap between single and multi gpu performance.In the end I did simple test, where I set one output buffer and multiple gpu_local or input ones and in ray generator program i just map output buffer to solid color and nothing else. This alone consumes 15ms on 4 GPU setup and 0ms on single GPU.I’ve found a way to improve it. Instead of using float4 as output buffer, I use BYTE4. But this has other limitations as lack of precision, and if I want to have tone mapping, i need to do it before I populate output buffer.So with BYTE4 as output_buffer, filling buffer with solid color on 4 GPU setup takes 3ms now.However, I still believe better job management/orchiestration could fix this issue on optix level. I could simply split my 1024 by 4, do renders separatelly and have almost linear increase in performance on multi GPU setup. Alternative would be to do more in each launch at cost of interactivity, which I don’t need that much and could be throttled up and down.Will check above two options and come back with results.Update.Checked heavier path ray tracing cycles and with 8x8 samples per launch, 4xGPU setup shined, and got almost linear increase. One frame took 2375 ms on one GPU and 607 ms on four. I’m quite happy with that as it’s not that far away from perfect 593 ms.I need to correct my previous statement here.\\nIt’s actually valid to use RT_BUFFER_INPUT_OUTPUT | RT_BUFFER_GPU_LOCAL buffers for accumulation over  multiple GPUs because the OptiX load balancer is assigning static working sets.\\nHow it does that is still abstracted by OptiX to be able to develop different work distributions in the future.\\nJust the final output needs to happen into a separate buffer which can be read on the host.Also see [url]Progressive photon mapping sample with multiple GPUs - OptiX - NVIDIA Developer ForumsYes, I went with the same approach. Combination of RT_BUFFER_INPUT_OUTPUT | RT_BUFFER_GPU_LOCAL for accumulation and slim BYTE4 for output buffer for display. This has a limit of not being able to do post processing outside launch, but I can live with it for now.In future, It would be nice to be able to fetch, even with significant delay results from all gpus when using RT_BUFFER_INPUT_OUTPUT | RT_BUFFER_GPU_LOCAL.Powered by Discourse, best viewed with JavaScript enabled',\n",
       " 'emulating-opengl-texture-matrix': 'Hi, in my Program (Delphi source) I set the Texture Matrix (4x4) herethen I use it into the OpenGL Shader in this mode:How can I do the same in Optix? (best way to pass a matrix form Host to Optix and exact code for matrix-vector multiplication)Thank you very muchADYou could do exactly the same in OptiX by using an optix::Matrix4x4 variable defined in optixu_matrix_namespace.h.E.g. code like this:Please search for other use cases in the OptiX SDK samples source code.Mind that OpenGL fills in matrices in column-major order and multiplies from the left, so watch the transpose flag when filling OptiX matrices from the same data.Wow , nice trick !Where can I find the complete list of attributes that I can specify as third argument of the function “rtDeclareVariable” ? I suspect that the list is long and very usefull…;-)ADPlease read the OptiX Programming Guide chapter 4.1.4 Attribute Variables.The attribute semantic names are user defined!\\nPick anything you like. I chose them to match old Cg shaders.That user defined attribute semantic name is used to link the attributes you generate inside the intersection program with the variables you read inside the closest hit program.\\nThe variable names themselves don’t actually need to match in the two domains, just the type and attribute semantic name. That is all explained in that small chapter.Powered by Discourse, best viewed with JavaScript enabled',\n",
       " 'old-guides-for-optix-7-4': 'Hi, I’m currently developing a project based on optix 7.4 and wondering where I can find the old guides for optix 7.4. It seems that the AI denoiser has several updates and the API for temporal denoising have changed a lot.\\nAlso, can I use temporal denoising without flow?Please have a look into your OptiX SDK 7.4.0 local installation’s doc folder.\\nThat should contain the OptiX Programming Guide and API Reference at the time of the SDK release.Also, can I use temporal denoising without flow?Well, not having flow vectors would mean there is no temporal component in your denoising, so that wouldn’t make much sense.\\nYou can use the AOV denoisers (“kernel prediction mode”) without flow vectors.\\nPlease have a look into the  OptiX SDK’s optixDenoiser example’s  README.TXT and source code for the command line options which control what type of denoising it can configure.If possible it would always be recommended to update to the most recent OptiX SDK.\\nThat requires specific minimum display driver versions which are listed inside the OptiX Release Notes document. Find the link to that directly below the download button of the respective OptiX SDK version on the developer.nvidia.com site.Somewhat related (denoiser, but 7.7 guide), a small mistake:End of section 14.1.4 says this:I believe hdrAverageColor is meant rather than hdrIntensity.Thanks for reporting it. This will be fixed with the next online documentation update.Powered by Discourse, best viewed with JavaScript enabled',\n",
       " 'updating-of-adaptively-split-curve-build-inputs-is-not-supported-fall-back-to-uniform-splitting': 'Hey folks.  I get this information/warning message from OptiX 7.4 when I build an acceleration structure with curves:“ACCEL_BUILDER:  Updating of adaptively split curve build inputs is not supported, fall back to uniform splitting.”Can anyone help me understand what that means?  Is there something I should be doing to prepare curves better for OptiX?Hey Brian,This message is just letting you know that uniform splitting is being used when you build the BVH using OPTIX_BUILD_FLAG_ALLOW_UPDATE. The “update” (aka refit) operation currently requires uniform splitting. If you don’t need ALLOW_UPDATE, then removing this build flag will restore adaptive splitting and get rid of the warning message.The implications of using ALLOW_UPDATE with curves are just that you lose the modest benefits of adaptive splitting, which might mean a little extra memory usage, and/or a little loss of traversal performance, relative to the memory and performance with adaptive splitting. If either of those is something you want to prioritize, you can use OPTIX_BUILD_FLAG_PREFER_FAST_TRACE to get a higher split factor with curves (and faster performance), or OPTIX_BUILD_FLAG_PREFER_FAST_BUILD to get a lower factor (with lower memory requirements). These two OPTIX_BUILD_FLAG_PREFER_FAST_* flags currently usually have a roughly 2x impact on memory and performance, which is considerably larger than the difference between uniform and adaptive splitting in most cases. (And you can use these flags with adaptive splitting too, if you don’t need BVH refits.)–\\nDavid.This topic was automatically closed 14 days after the last reply. New replies are no longer allowed.Powered by Discourse, best viewed with JavaScript enabled',\n",
       " 'actor-and-forces-managment': 'Getting started with PhysX 2 and I was wondering how does one keep track of the actors in the rendercallback. for example is there like an ID associated with each actor or can i some how assign an ID so i can then recall that ID and then apply a particular force to it alone? say i create an actor on the fly and i want particular forces to be applied to just that actor but not the rest of the actors how does one go about doing that?Im using c++, opengl, and visual studioI think i figured it out. I create a struck or class that i can then pass to the actor and then test for when going through each actor in a for loop during render.I think you’ve figured it out already.  The obvious way to do this is to store a pointer to a struct containing the actor id in PxActor::userData.Thanks,GordonPowered by Discourse, best viewed with JavaScript enabled',\n",
       " 'nvidia-driver-not-installing-correctly': 'I am currently working on a Google Cloud environment with a Tesla T4 GPU type.\\nI need to install NVIDIA driver with it (which I did using the .run file). I downloaded the NVIDIA-Linux-x86_64-515.43.04.run file from the NVIDIA website. I also needed the CUDA Toolkit installer which I also downloaded as a .deb file into my Google Cloud instance (cuda-repo-debian11-11-7-local_11.7.0-515.43.04-1_amd64.deb).I followed these instructions to finish installing CUDA.I get some weird errors. For example when I run the nvidia-smi command in the Cloud cl I get this error: NVIDIA-SMI has failed because it couldn’t communicate with the NVIDIA driver. Make sure that the latest NVIDIA driver is installed and running. I am confused since I’m pretty sure I downloaded the latest version.I am a noob in python and GPU related things so I don’t really know what I’m doing. Can someone help me install the NVIDIA driver onto my Google Cloud instance? Thanks!!Powered by Discourse, best viewed with JavaScript enabled',\n",
       " 'using-rtx-2060-super-with-mst-hub': 'Hey Nvidia,I am planning to build a flight simulator, that uses 3 Projectors for the main screen and 2 Displays for instrumentation.\\nI want to use a GIGABYTE GeForce RTX 2060 SUPER Windforce OC 8G (GV-N206SWF2OC-8GD) to power this setup.\\nWhile researching how to connect 5 displays to this card i found out about MST and consider the option to use a MST Hub like this one from Club-3D to connect all 3 Projectors to one DisplayPort connector on the card.\\nIs the use of MST Hubs supported on this card?Thanks for your helpJulianPowered by Discourse, best viewed with JavaScript enabled',\n",
       " 'motion-vectors-filling-area-beneath': 'Hello,\\nI’m trying to use NVEncode in motion estimate mode only. I updated sample app to support pngs and I’m getting quite weird results. What am I missing?\\nI’m calculating motion vectors from 2 pictures of moving circle:and second image (circle moved right by 10px):and I’m getting motion vectors like this (red means non zero motion vector at that point, light green no movement):Somehow motion vectors gets repeated vertically. I tried with video, with other images and I get this kind of picture all the time. In this example red fills until bottom, but in more complex example it fills until next movement is found.The motion vectors from the previous row of MB’s are used as hints by NVENC, and given that the previous row has the motion, it may bias the motion towards that, given that the cost of encoding is same (the region in the bottom rows is identical even after reconstructing the MB).This should not happen on more realistic video frames. Can you share examples of frames where it happens in real video?Thanks for reply. After working with NVEncode for a while I suspected something like that. I found a way to keep only relevant non zero motion vectors. I use motion estimate on computer generated images/videos so there can be single color areas. Probably in real life videos that’s not the case.Powered by Discourse, best viewed with JavaScript enabled',\n",
       " 'csimux-stream-error-in-serial-ab-and-serial-ef-stream': 'Hi Everyone,\\nI am working in Jetson xavier NX board with following specification.\\nLane Configuration : 3 * 4\\nData Resolution : 60k * 1(width * Height) → Custom data\\nSpeed : 900Mbps and 1350Mbps\\nIC1 connected in stream_ab, IC2 connected in stream_cd and Ic3 connected in stream_ef. I can successfully receive data for 900Mbps from 3 ICs. But When am reading data in 1.3Gbps only IC2 can able to receive data successfully and other 2 ICs not reading data and producing CSIMUX_STREAM error.\\nI would be great if someone can help me to debug it . Here I have attached trace log in text file.trace.txt (89.9 KB)Thanks for your timeIssue solved: GLOBAL type:PHY_INTR0 phy:0 error in EMMC xavier Nx module at 1.3Gpbs - #5 by JerryChangPowered by Discourse, best viewed with JavaScript enabled',\n",
       " 'erratic-nvdec-behavior-only-in-fullscreen-opengl': 'Hi - I’m seeing very poor behavior in my application on Windows 10 using NVDEC, but only in fullscreen. My usage pattern is similar to the DecodeGL sample, the primary difference being that I have a secondary thread which owns the CUDA context and a matched secondary GL context which shares textures with the primary. The other difference is I’ve moved off of the deprecated interop APIs and am using cuGraphicsMapResources() and its brethren.My application performs perfectly in windowed mode, and also in fullscreen mode with vsync off. However in fullscreen with vsync on I am seeing very significant stuttering. In particular, the glFinish() call I’m using to synchronize the secondary thread is stalling for as long as 10secs, followed by normal behavior for a couple seconds, followed again by significant stalling. This call follows the glTexSubImage2D() call which updates a texture with the latest NV12->RGBA PBO per the DecodeGL sample’s pattern.An additional clue is that if I put a glFinish() call on the display (primary) thread, everything behaves fairly well, though the performance implications of that call make me want to avoid it. Yet another clue is that if i eliminate any interaction with OpenGL entirely (which of course prevents me from doing anything useful with the NVDEC output) I see similar stalling at the call to cuvidDecodePicture() instead, which can take similarly long (10+ secs).My intuition is that there is some sort of strange interaction between my two GL contexts and the CUDA context all needing to synchronize with one another. I know that the GL driver seems to engage some sort of different mode when fullscreen is enabled, and my hunch is that that mode robs the driver of a key opportunity to synchronize these contexts (which adding the glFinish() to the primary thread seems to restore). I’ve experimented with inserting cudaThreadSynchronize() at a couple of different points but no luck. Any hints would be greatly appreciated as I’ve already spent a fruitless week trying to track this down.Powered by Discourse, best viewed with JavaScript enabled',\n",
       " 'how-to-load-pxcollection-from-file-physx-3-2-0': 'Hi, I have a problem with load a PxCollection.Now I want to load for example “col.bin” file and add collection to scene. How to define void* 128-bytes aligned buffer?Hello,The PxToolkit::FileInputData can’t be used for binary deserialization. The whole collection data needs to be loaded into a block of 128 byte aligned memory before calling PxCollection::deserialize.Here is an example how you could do it:Make sure to not realease the memory containing the collection objects (collectionBuffer) before you have released all the objects in the collection (whenever you don’t need your PhysX objects anymore).Unfortunately I get an error.\\n…\\\\PhysX\\\\src\\\\NpPhysics.cpp (948) : invalid parameter : Buffer contains data with wrong header indicating invalid binary data.Could you check whats in col.bin right after calling c->serialize(stream); ?\\nThe first 4 bytes should contain the bytes corresponding to the chars ‘S’ ‘E’ ‘B’ ‘D’ which is the header of the data expected.I opened file using notepad. First line starts with:At this point I don’t have a good idea what could be going wrong. Would it be possible to create a little repro with source and attach it here?Powered by Discourse, best viewed with JavaScript enabled',\n",
       " 'path-tracing-with-denoiser-crash': 'Howdy,I’m experimenting with denoised path tracing using the basic code from the “optixDenoiser” sample scene. Optix works amazing well but I’m hitting some instability when there’s lots of objects to trace.[url]PathTraceDenoisedCrash - YouTubeThe crash is random and occurs in nvoglv64.dll. I’ve updated to the latest gpu drivers without success.Here’s my system detailsOptiX 5.0.0\\nCUDA v 9.1\\nNumber of Devices = 1\\nDevice: 0 GeForce GTX 960M\\nDriver Version: 390.77\\nCompute Support: 5 0\\nTotal Memory: 4294967296\\nClock Rate: kilohertz 1176000try CUDA 9.0 (or even 8.0) and device driver: 388.59  and TDR enabled (with long delay times)\\nI had many crashs with CUDA 9.1 in combination with OptiX 5.0.0 on that path tracer + denoiser sample (and a deactivated TDR) but since using CUDA 9.0 and driver 388.59 and enabled TDR no more crashs yet.\\nIn release notes [url]https://developer.nvidia.com/designworks/optix/downloads/5.0.0/releasenotes[/url]\\nTake a look at \"Development Environment Requirements \"  and at  “known issues”.\\nNOTE: when using CUDA 8.0 the MDL part may cause trouble again, see your older question [url]Can\\'t compile the MDL samples using VS 2015 - OptiX - NVIDIA Developer ForumsCuda 9.0 with driver 388.59 and TDR delay of 10 works without crashing.Thanks for the help!Powered by Discourse, best viewed with JavaScript enabled',\n",
       " 'optix-denoiser-output-buffer-data-range': 'I’m using the Optix denoiser in our OpenGL render application, as the last step after ray-tracing. The data format we have is 8-bit unsigned char. As stated in Optix 7.3 release note, this format isn’t directly supported by the denoiser, so I’m manually converting the image to float by dividing 255. So the input data range is 0.0 to 1.0.However on retrieving the output buffer, I realize the data range has been enlarged to beyond 1.0. For some test image, the range seems to be 0.0 to 2.0. I need to convert this date back to unsigned char. If there is a way to find out the min and max of the output buffer, then I can simply do this cast the output buffer like this: (unsigned char)(255 * (image_pixels[i]-min)/(max-min)).Can you instruct me how to find out the output buffer’s expected data range? Or is there another way to do this conversion from float to unsigned char on the output buffer?Thanks very much!Can you instruct me how to find out the output buffer’s expected data range?There is no such functionality to calculate the minimum or maximum value inside a color buffer inside the OptiX denoiser.\\nIt has entry point functions calculating the HDR intensity and average color only, which are needed to produce better results especially for very dark or very bright inputs.Which denoiser mode did you use? (LDR, HDR, AOV)\\nhttps://raytracing-docs.nvidia.com/optix7/guide/index.html#ai_denoiser#nvidia-ai-denoiserAs stated in Optix 7.3 release note, this format isn’t directly supported by the denoiser, so I’m manually converting the image to float by dividing 255Right. You mean float 32-bit? It’s recommended to use half 16-bit instead for better performance.Please always provide the following system configuration information when asking about OptiX issues:\\nOS version, installed GPU(s), VRAM amount, display driver version, OptiX (major.minor.micro) version, CUDA toolkit version (major.minor) used to generate the input PTX, host compiler version.OS version: Windows 10\\nGPU: NVIDIA Quadro P4000\\nDriver: 466.11\\nCUDA: 11.3\\nOptix: 7.3Thanks for your answers. I’m so sorry the initial question I asked was actually caused by my mistakes. The input inputs I passed to Optix denoiser were not in 0 to 1 range. Therefore the output was out of the range as well. I only found that out through debugging. But your answer did help me for at the time I didn’t even realize I was using HDR mode when my data is LDR.Now I’ve fixed that problem by using LDR denoiser mode and make sure input data inside 0 to 1 range. I’ve also fixed the data type problem I had. Now we are using floats as input and output. I will look into the half 16-bit option as you suggested.I just ran into a few other questions I hope you can help me again:In LDR mode, does the input noisy image (the beauty layer) needs to be gamma corrected? How about the corresponding albedo image. Does it need to be gamma corrected as well? It would be better for our code if gamma correction can happen after denoising but if doing it the other way is better for denoising then I’ll make the effort to change that.For optixDenoiserComputeMemoryResources() method, I’m only passing in the width and height of the framebuffer. It doesn’t seem to care about the 3rd dimension (i.e. color channels)? In our case, the color channels are 4 (RBGA). So size of the beauty/normal/albedo layer are all calculated as: width * height * 4 * sizeof(float), while this method only takes in width and height. I just want to double check if I’m using it correctly:\\nmemset(&_sizesDenoiser, 0, sizeof(OptixDenoiserSizes));\\nOPTIX_CHECK(_api.optixDenoiserComputeMemoryResources(_denoiser, _width, _height, &_sizesDenoiser));If I choose to use CUDA driver api instead of CUDA runtime api, does that mean our customer will not need to install the CUDA 11.3 toolkit? I understand I still need to install it in my development environment but in production can they go without the toolkit installation required? They only need to have latest GPU driver run Optix denoiser if I use CUDA driver API. Correct?Please read the OptiX Programming Guide chapter I linked to above again. Some of your questions are already answered in that.  I will only answer the remaining ones.How about the corresponding albedo image. Does it need to be gamma corrected as well?That’s a good questions. It’s not mentioned inside the programming guide. That should always be in linear space.It would be better for our code if gamma correction can happen after denoising but if doing it the other way is better for denoising then I’ll make the effort to change that.Then do not use the LDR denoiser, use the HDR denoiser which works in linear color space. There actually isn’t a separate AI network for the LDR denoiser inside OptiX versions for quite some time anyway.\\nPlease have a look into other posts about the OptiX denoiser here:\\nhttps://forums.developer.nvidia.com/search?q=DenoiserRight, that calculates internal scratch memory sizes which depend on the model and options which are part of the denoiser handle created with optixDenoiserCreate and the input dimensions.\\nPlease follow the examples inside the programming guide\\nhttps://raytracing-docs.nvidia.com/optix7/guide/index.html#ai_denoiser#allocating-denoiser-memory\\nand the optixDenoiser SDK example or the open-source examples you find in the sticky posts of this sub-forum.Running OptiX applications does not require the installation of the CUDA Toolkit on the target system!\\nThat is mentioned inside all OptiX SDK Release Notes.If you’re using the CUDA runtime API and link dynamically you need to ship the resp. CUDA version’s runtime DLL with your application. If you’re linking it statically it’s part of the executable.\\nIf you use the CUDA driver API you always link dynamically and the DLL ships with the display driver.You would only need a CUDA development environment (the CUDA toolkit) and host compiler on the target system if you do anything with the CUDA Compiler (NVCC) at runtime, like generating CUDA program code in some material editor and compiling it to PTX input source for OptiX, but that in turn can be handled with the CUDA Runtime Compiler NVRTC which can be shipped as two DLLs (compiler and built-ins). Though that would need the CUDA headers, which implies a CUDA toolkit installation on the target system when compiling things at runtime.Again, no CUDA Toolkit required on the target system whatsoever. OptiX 7 is a header only API and the implementation ships with the display drivers. The input language is PTX you can compile upfront and must be shipped with your application. The necessary DLLs for the CUDA runtime environment, when you use any, also need to be shipped with your application. The customer only needs to have a display driver version installed which supports the OptiX 7 API version you used to build your application.You have clarified so much for me! Thanks!Can I double check my understanding on the normal buffer (as a guided layer besides albedo). If I understand the programming guilde correctly, the normals we pass to Optix should be in [-1.0 to 1.0] range. When we visualize the normals in images, we can choose to either convert or clamp to [0 to 1.0] but not for input to Optix. Correct? (I think I’m doing it wrong at the time, I’m passing the visualized image in [0 to 1.0] to Optix as normal layer. I’ll need to fix it if this is not right.)Yes, as the programming guide chapter 13.11.1 says, the surface normal vector components have values in the range [-1.0, 1.0] in camera space.The explanation after that is just about the following two images inside the programming guide which cannot visualize that correctly because the negative components cannot be shown. The brighter of the two is effectively how a scaled and biased normal map texture would look like. Neither is the correct input for the denoiser.Example code calculating that from the usual pinhole UVW projection vectors can be found in one of my OptiX examples:\\nhttps://github.com/NVIDIA/OptiX_Apps/blob/master/apps/intro_denoiser/shaders/raygeneration.cu#L145The normal buffer might not always help. It should improve the results for highly detailed geometry.\\nThe albedo buffer is the more important channel to improve the denoising result over just the noisy RGB buffer.\\nTry all three combinations of RGB, RGB+albedo, RGB+albedo+normal buffers in your application and pick the one which works best.Fantastic advice! Thanks for the insights! Really appreciate it!Powered by Discourse, best viewed with JavaScript enabled',\n",
       " 'quadro-4000-no-longer-supported-in-premiere-cc': 'Hi,\\nAdobe Premiere CC 2019 reqiure driver updated to work with CUDA, but there is no newer update for Quadro 4000.Hi,That is correct, the last branch to support Quadro 4000 is R375, and that version does not have the latest version of CUDA. If that is required for Adobe Premiere CC 2019, then Quadro 4000 is not supported.Thanks,\\nRyan Parkhow come pages such as https://developer.nvidia.com/vulkan-driver state its for quadro 4000 ?Powered by Discourse, best viewed with JavaScript enabled',\n",
       " 'possible-to-target-shield-only': 'I understand there are a variety of filters to exclude devices on Google Play, so I’m of course wondering if/how it’s possible to make the Shield the only device your game will run on?Limiting my app to a single device might seem fiscally irresponsible, but I am still curious! :DThanks all!-StevenPowered by Discourse, best viewed with JavaScript enabled',\n",
       " 'glcopyimagesubdata': 'Hey all,I currently want to copy a texture with glCopyImageSubData, but I just get an INVALID_OPERATION. Anyone a idea?My test source:One thing I noticed is that texData is an array which can be implicitly cast when passing to glTexImage2D, however, you are passing a reference(&texData) to the array which ends up being interpreted as a (**), you need to either pass texData or &texData[0], to prevent GL trying to read data from an invalid address. Another thing is unless nullptr can be implictly cast to a GLvoid* I would refrain from using nullptr and use 0, if they are equivalent then all is well. Aside from that, everything else looks correct. What GPU and driver version are you using?You are right with texData. But this don’t change anything if I fix it :(. Setting MIN_FILTER/MAG_FILTER and WRAP before, don’t help too :(.I have a GT435M with latest driver (337.50)Another thing to check is to consult the specification to see if the formats being copied are compatible…try with a RGBA texture and see if that works…then try again with the format you listed above. If it works with the RGBA texture and not with your GL_R32I format, then its either a driver bug or the format is not supported by the function( check the spec to verify though ).The spec don’t list any limitation to format as long as source and target are the same. Only if you copy from compressed to uncompressed format exists limitations.I tried it now with default texture format, but it still don’t work:And how I can report a driver bug?glCopyImageSubData can generate invalid operation errors for the following reasons:\\nGL_INVALID_OPERATION\\u200b is generated if the texel size of the uncompressed image is not equal to the block size of the compressed image.\\nGL_INVALID_OPERATION\\u200b is generated if either object is a texture and the texture is not complete.\\nGL_INVALID_OPERATION\\u200b is generated if the source and destination internal formats are not compatible, or if the number of samples do not match.I would expect that the second condition kicks in here.\\nThe default minification filter uses mipmaps, you didn’t provide any and didn’t say which modes you tried before on both textures.I would try the following things:Additionally GL_RGB with the default glPixelStore GL_UNPACK_ALIGNMENT of 4 won’t read texture rows correctly in glTexImage2D in your case.GL_RGB is a legacy internal format. I would recommend to always use internal formats with explicit precision qualifiers. Use GL_RGB8 or better GL_RGBA8 since that’s what it ends up internally in most implementations anyway.Tried it now - and it works now :). Last time I tried it with setting min/mag filter is doesn’t work - this time (exact same code - have a copy of my last test code, it didn’t :/).Working minimal example:Powered by Discourse, best viewed with JavaScript enabled',\n",
       " 'games-crashing-on-linux': 'Hello,I have a a Geforce 1060 3GB graphics card.When I try to play any graphically intensive programs, usually video games, my computer will regularly crash. Visually it will freeze on the last frame. It will play the last 2 seconds of audio on a never ending loop. The only way to get out of this is to perform a hard shutdown by holding the power button for 10 seconds.I have monitored my CPU while running CPU and GPU heat temps and determined that it is not hitting critical levels during crashes. I ran memtest and have determined my ram is in good shape. I suspect it is the drivers.Certain games will crash faster then others. Dying Light will crash regularly after about 10 minutes of play time making the game unplayable. During the 10 minutes however it runs buttery smooth with no drops in frame rates or other issues.Counter Strike Global Offensive will crash roughly three times a match, so every 30 minutes on average, but it’s more like 20 minutes.I was able to tweak some games to crash later. In Planetary Annihilation: Titans I found I was unable to play on the larger maps for longer then 15 minutes, but smaller maps worked just fine. Upon lowering all of my graphical settings I could then play on the larger maps for about 45 minutes before a crash.I am using the 415.25 drivers.Please let me know if there is any more information you will need from me.Hello,I am going to move this over to the GPU Hardware forum for better visibility. I suggest that you also post this in the Geforce forums.[url]https://forums.geforce.com/default/board/25/nvidia-community/[/url]Best,\\nTomPowered by Discourse, best viewed with JavaScript enabled',\n",
       " 'physx-3-2-1-basic-problems-pvd-connection': 'hello everybodyplease help me whit this problemi’am a amateur in using physx & got serveral question about it…i follow nvidia documentation to create the scene engine, but when i type this code VS2010 shows error that this is function ,not a type name…what should i do??? (i’ve underlined it)\\nstatic PxDefaultSimulationFilterShader gDefaultFilterShader;i don’t know where the underlined item comes from?? (where is it??)\\n#ifdef PX_WINDOWS\\nif(!sceneDesc.gpuDispatcher && mCudaContextManager)\\n{\\nsceneDesc.gpuDispatcher = mCudaContextManager->getGpuDispatcher();\\n}\\n#endifmy project is a simple console application and i just want to see what’s happening with physx visual debugger so i tried the documentation code (PVD basic connection)…\\nit works (i mean no error ?) but the pvdConnectionManager (underlined) is NULL !!! (keep in mind that i compile in debug mode and using default libs ex: PhysXVisualDebuggerSDK.lib)if(mPhysics->getPvdConnectionManager() == NULL)\\nreturn;i’ll be glad if you answer these question as soon as possible,i’am really stock!!!thanksHi.\\nFirst of all PVD needs to be running before you try to connect to it. Second thing is this is how I overcame this filter shader thingand make sure you are using these libs. I had to declare them in my code with #pragma\\nBTW: it’s stuck not stock.Powered by Discourse, best viewed with JavaScript enabled',\n",
       " 'optix-6-5': 'I am new to optix. The lab uses a lower version of optix, so optix6.5 is used, but the official example cannot be single-step debugged by VSE after being processed by cmake. I would like to ask how to deal with single-step debugging. VS2015, CUDA10.1, optix6.5, Nsight Minitor2019.1Do you mean single stepping through the OptiX device code?I’m not sure that ever fully worked with OptiX 6 versions and VSE. Maybe also try cuda-gdb.Source code debugging would require the necessary debug information to be present inside the input PTX source code. Meaning you would need to translate the *.cu files to *.ptx files with the --generate-line-info (-lineinfo) and --device_debug (-G) and no optimizations NVCC options.\\nThat’s not even recommended inside the OptiX 6.5.0 Programming Guide:\\nhttps://raytracing-docs.nvidia.com/optix6/guide_6_5/index.html#building#ptx-generation\\nAfter that OptiX would be required to retain that information when rewriting the PTX input source code to the kernels and there have been many occasions where that was not working in the past.Even with the newest OptiX SDK 7.7.0 and display driver releases the device code debugging feature is still in a preview phase and not fully working, yet. Have a look through the other forum threads which discuss device code debugging.It’s possible to use the rtPrintf or the native CUDA printf function (recommended) inside OptiX 6 device programs though, to get information about what’s happening without any need for NVCC debug options.\\nMind that many threads run in parallel and you would need to limit the output to a specific launch index for reasonably precise informationIf you have any chance of using OptiX 7.x versions, that would be highly recommended over the legacy API used in earlier versions.Powered by Discourse, best viewed with JavaScript enabled',\n",
       " 'dual-titanvs-no-video-signal-on-boot': 'Trying to troubleshoot a no video signal issue. Running Ubuntu mate 18.04 LTS on a dual boot Ubuntu-Windows machine. (Asus X399 Extreme mobo)\\nWhen trying to boot this evening was getting no video signal.\\nI have dual Titan Volta GPU’s and tried switching the HDMI cable to the 2nd GPU. No difference.\\nAlso tried clearing CMOS.\\nHas been working fine for a year or so. Never experienced this before.Not getting any error messages on the motherboard on boot.\\nI know my monitor is fine because I can plug my laptop into the 2nd HDMI input. Both GPUs are getting power as I see the fan spinning.Is it possible either a recent software and or driver update has hijacked the video out signal?Is there some keyboard combination I can try on boot?\\nAny feedback welcome! Op\\nIssue resolved.Tried booting from Live USB using Esc and F12 key options.Was about to start disassembling, pulling video cards, etc but tried booting once more.\\nThis time at least was seeing a post error on my mobo.\\nPlugged in my hdmi cable and hooray, was seeing the American Megatrends BIOS screen.After adjusting BIOS and booting into Ubuntu rechecked my video driver settings and could see that the X.org driver was selected instead of Nvidia 415\\nThis could be a clue as to what caused my problem though not clear why this would result in NO video signal at all.\\nAnyone aware of this issue with X.org Nouveau drivers?Powered by Discourse, best viewed with JavaScript enabled',\n",
       " 'nvidia-3080-failing-to-decode-h-264-level-6-0': 'Hello.\\nI’m getting artifacts in multiple video players trying to play 3840x2160@60fps video encoded with High@L6 level. I don’t get them if i switch to software decoding.\\nSo, my question is: what it the maximum H.264 decoding level supported by NVIDIA Ampere (3080, for example)? According to vdpauinfo under ubuntu it’s 5.1 for all profiles. I couldn’t find any new information relevant to Ampere myself.I think these artifacts with high CABAC ref count is just a bug in the driver.Powered by Discourse, best viewed with JavaScript enabled',\n",
       " 'optix-prime-inconsistencies-on-ray-intersection-computation-with-secondary-rays': 'I’ve been having issues with Optix Prime when computing intersections with secondary rays originating from multiple sources. I am computing the start position of these rays from the hit position of the previous ray. The direction of the each ray is towards each point receiver in my scene ( I can have multiple for both ).The issues I am having is that the rays are being inconsistently intersected by my triangle mesh. Sometimes they get all the way through to my receiver, other times they do not even if it appears that through visualization they should. This appears to be occurring at a roughly 50% rate. This even occurs if I place my source and receivers far above the triangle mesh where the triangles should be unlikely to be obstructing their paths. When checking to see if a ray is received at the point source I am simply checking in each hit result for a miss result ( tri_id = -1 ).I have also verified that my rays are normalized ( or as close as possible ).I will note that my triangle coordinates are in the ECEF coordinate frame which tends to have relatively large single precision magnitudes.I am also visualizing the results using a MATLAB script which draws the rays from each source to each hit point, then a secondary ray from each hit position to each point receiver and where it hits (or not). For visualizing the triangles I am using the trimesh function.What could be causing these issues to occur in Optix Prime?Hi Timothy,You should expect that using the barycentric coordinates to compute your hit point will yield higher precision than computing it with start + raydir * t. I recommend trying that first; the conversion back to Cartesian is trivial.Since you’re using single precision floating point in both cases, you will in general never have an exact point on your surface, only a point that is within some error tolerance. Even using the barycentric coordinates will give you a point that due to rounding error will be below your exact surface, likely around 50% of the time.How far below the surface are the hit points you’re currently getting?Certainly using geocentric coordinates for your mesh vertices is not making this problem any easier. That definitely will cause loss of precision in the ray calculations. If you can transform your scene to have an origin that is nearer to your mesh or your camera, you will have a lot more precision to work with, even when using barycentric coordinates to calculate your hit point. If you use geocentric coordinates in meters, the best possible rounding error of the least significant bit is on the order of a quarter meter, and in practice I would expect to see much worse than that. I came up with that number by visiting this float calculator: IEEE-754 Floating Point Converter entering 6371000 (earth’s radius in meters) in the “decimal” field, and then changing the lowest bit in the binary field.Also note that means that the smallest feature of your mesh you can even represent is ~0.5 meters across (2x rounding error). If you’re tracing or rendering human sized objects, you will see a lot of quantization error. You’d want to make sure at this scale that your field of view is at least several kilometers wide in order to not see any artifacts in your geometry, if you have a typical output image size like 1k or 2k pixels across.For what it’s worth, the watertight option is intended to guarantee that rays can’t accidentally sneak through a mesh when rays are close to edges in the mesh. It is related to precision problems in triangle intersection, but is not intended to nor necessarily going to improve the precision of your results.My values are extremely close to the surface of the mesh. I’m not sure on the exact values yet ( I’d have to probably comptute in double precision to figure that out. ) It’s likely that they’re probably within the 0.5m range you specified though.I’m not tracing anything that small though ( at least not yet ). I am tracing DTED level 2 terrain data with a resolution of 30 meters. I could also use lower resolution data as well.Also, I’ve thought about moving to a coordinate system centered at a non-geocentric origin although I’d have to rewrite a large amount of code in our model to do that. The model I am working on is built to simulate radars, so the transmitter and receiver objects can be several hundred kilometers apart with terrain loaded in between. ( say 400km). If I were 400km out then that would give me about 0.03125m of precision. Do you think that would be enough?I also used the barycentric coordinates method and got slightly more rays to be received. It made a difference of about 5-10% or so. It still appears to be limited to about 50% of the rays that hit the terrain, though.If I were 400km out then that would give me about 0.03125m of precision. Do you think that would be enough?I wouldn’t know how much precision is enough for your sim, that’s more of a question of what needs to happen with the output and whether you need to quantify your tolerances and sources of error. I just wanted to help out with some specifics on the limits of single precision floats, since it can be quite surprising how limited they can be when your world has a range of scales that seems reasonable. Modeling artists run into this problem in 3d modeling software all the time and they have rules of thumb for how big their worlds should be. For example, do some Googling on the terms “z-buffer precision” or “logarithmic z-buffer”. You will even find a few articles on planetary and terrain rendering that might help a bit. Even though you’re not using a z-buffer, the floating point precision issues are more or less the same.I also used the barycentric coordinates method and got slightly more rays to be received. It made a difference of about 5-10% or so. It still appears to be limited to about 50% of the rays that hit the terrain, though.I’m not sure I understand, how do you determine whether a ray is received? What does it mean to hit the terrain or not? You’re talking about the hit point being close to or above the surface versus being far away or beneath the surface?If you are testing whether your hit points are below the surface with high precision, you are always going to find ~50% of them below the surface (the other ~50% will be above the surface, not on it). There are (generally speaking) no points exactly on the surface when you use floats, so you need to define points to be on the surface when they are within a reasonable epsilon of distance from the surface, regardless of whether they’re above or below the actual surface. Typically in ray tracing, this epsilon shows up in the form of a positive non-zero t_min value that is used for the next ray, the one reflected or refracted from your hit point. This way when you have a hit point that is slightly below the surface, you avoid intersecting the same surface again when you send a reflected ray.BTW, to see how much the barycentrics improve things over origin + direction * t, measure the average distance of all hit points from the surface, in double precision.I’m not sure I understand, how do you determine whether a ray is received? What does it mean to hit the terrain or not? You’re talking about the hit point being close to or above the surface versus being far away or beneathWhat I mean by a ray being received is after hitting the terrain it doesn’t hit anything else on it’s path back to the receiver from that terrain point from the original transmitted ray. Sorry if that was confusing! The receiver has no geometry other than being a point in space, it is basically generating signal parameters such as relative power, doppler, phase shift, and other factors for each ray and then combining that into a time domain response.I’m not bouncing these rays multiple times when they intercept something, as the received power will be negligible at the distance scales being used. ( EM signals attenuate it a rate of R^2 per bounce ) I may do primary rays as well from the terrain, then spawn more secondary (diffuse) rays if those hit something to the receivers from those intersected points.Typically in ray tracing, this epsilon shows up in the form of a positive non-zero t_min value that is used for the next ray, the one reflected or refracted from your hit point. This way when you have a hit point that is slightly below the surface, you avoid intersecting the same surface again when you send a reflected ray.Now i understand why you can provide t-min values for the rays. I’m fairly new to ray-tracing environments, so I wasn’t sure how to set up an epsilon for the intersections tolerance. I believe that would fix my issues with the rays likely intersecting the same triangle they just left. Using 1 meter here would likely be sufficient for my needs and avoid most of those issues I was noticing with intersections happening in the 0-1m range. I’ll try this tomorrow and let you know if my problem is solved.Thank you for your help!Setting a tolerance of 1.5m-2.0m does indeed appear to have fixed my problem. Thank you for your help again! All of the rays except for a few outliers that I expect to not intersect with their own triangle they were reflected from now do not. This should be good enough for our purposes.Excellent, I’m glad that worked out!It sounds like you might not need anything trickier. Most people get by just fine with a static epsilon, but if you need something better in the future, if you have any more issues with your remaining outliers, there are more options. One possibility you can check is whether rays you send from a hit point are leaving at a grazing angle. You could optionally use a dynamic next t_min value that increases for grazing angle rays. A second option might be to move the hit point from below the surface to the nearest point above the surface that is representable using single precision floats. You’d probably have to do that calculation in double precision, but then you could change your next t_min value to 0. The main reason to get that tricky would be if you ever need strong guarantees or if you have legitimate ray lengths that could ever be less than your epsilon value. If accuracy is more important than performance, there are ways to improve accuracy.Good luck, let us know if there’s anything else. And we’re always interested in seeing the results- if you publish a paper or end up with interesting visualizations from your simulations, and you’re willing to share, we’d love to see them.Powered by Discourse, best viewed with JavaScript enabled',\n",
       " 'seeking-studio-driver-titan-x-maxwell': 'Good day.  I am trying to get my Titan X running to render iRay graphics but cannot find anything but game drivers.  I have studio driver for my 1080Ti but not for my Titan.  It is not possible to render iRay images with a game driver.  Do I just have a useless GPU?Powered by Discourse, best viewed with JavaScript enabled',\n",
       " 'why-i-cant-lauch-the-nv12tobgra32-kernel-function-with-different-streams-parallel': 'I create 2 streams for 2 videos load and display with the sample called AppDecGL.It is shows that the two streams do not work parallel with the analysis of Nsight.What should I do?Hi,What does stream term refer here: cuda stream (CUstream object) or video stream?To run two video streams in parallel, you should create two cuda stream object and should pass one cuda stream object to cuvidMapVideoFrame() and Nv12ToBgra32 kernel for first stream and 2nd cuda stream object to another video stream. Without separate cuda stream, kernels would run on the null cuda stream which would serialize the execution.Let us know if this suggestion helps you.Thanks,\\nRyan ParkPowered by Discourse, best viewed with JavaScript enabled',\n",
       " 'extreme-latency': 'I have the cloud XR server running on a HP g4 with RTX 6000 card. SteamVR 1.16.10. Hololens2 with latest updates. Server connected to Asus RT-AC88U router by cable. H2 using 5Ghz wifi. I start steamvr and then the provided h2 client application. It connects swiftly and I get an image. BUT: latency is extreme. Approaching 1 sec. Temporarily turned off HP remote Boost and firewall with no change in quality. Even turned of firewall in router (no change) Tried to look at the server logs but cannot really make anything from them. Opening device portal for the h2 and looking at 3D-view I can see the extreme lag there as well (the view frustum updates with large delay). If  I stop the CloudXR client on the h2, The 3d view updates as it should. I find it very hard to troubleshoot this. Anyone have any ideas?HiHL2 requires quite a bit of bandwidth. Is there anything else using your router? Also, where abouts are you vs where your router is physically located? (separate room / line of sight / etc ?) Ideally, you should be running WiFi 6, not 5Ghz. I know the documentation says 5Ghz is supported, but 5Ghz is not great with certain XR workloads / devices (depending on the Optics in use).I highly doubt the RTX 6000 is running out of performance (I’m assuming this isn’t virtualised?), but you can check various resources on the CXR Server to see if there are any issues, Windows Task Manager and GPU Profiler ( Releases · JeremyMain/GPUProfiler · GitHub ) will give you some very quick and high level indicative stats to see if there’s anything obvious. After that, you’re into GPU performance modes and encoding latency using nvidia-smi. If all that looks ok, then you’re back in to network latency and throughput. What’s the ping time to your HL2 from the CXR Server?RegardsMGHi, sorry for slow answer, Easter + had to test some more. However just  to make absolutely sure I didn’t have a bad network I bought a new wifi 6 router for extreme gaming (ASUS RT-AX88U) and connect only the H2 and the server. No internet. No other devices. I get the exact same low performance. Firewalls wide open. While it does technically work (it sends frames etc) but the performance is probably around 5 fps with a latency approaching a second.\\nBtw, there seems H2 will not respond to ping at all (https://www.reddit.com/r/HoloLens/comments/k56o8t/send_ping/).\\nDoes it work for you beyond just connecting and sending frames? IE, do you get useful performance?I have a HL2 using my ISP provided router (nothing special run of the mill huewai wireless AC router) and it works just fine. I haven’t tried anything too intensive or an application made specifically for the HL2 but I’ve “played” star wars droid repair bay and that worked just fine. As a sanity check it might be worth seeing if this works: Holographic Remoting Player - Mixed Reality | Microsoft Docs to make sure its not a connection issue?Ok thanks , good to know it at least should work! I have tested using virtual desktop using the server client sw “Mirage” and that gives me excellent performance so it should really not be any issues with my network per se. Anyway, I will test remoting player as well to see where it’s at.FYI: We actually got in contact with NVIDIA and they confirmed the bug and that the H2 + CXR is sketchy atm. Let’s hope to see a new updated release on GTC.  I will drop CXR for the time being as it seems Holographic Remoting might work right now.For now, the use I have is streaming with CXR throught SteamVR an UnityApp, and It’s working fine. I even tried with complex display and no latency occurs, except maybe on the hand tracking, which may come from the Hololens2 native Hand Tracking.Could you please share what kind of router / hub have you been using? I am experiencing a lag too on the standard HL2 client released by NVidiaHello, I’m using a Netgear Orbi Router RBR850, a wifi 6 routerHello, same issue here, with the sample provided by nvidia.Hi,If you are experiencing latency issues with the HoloLens 2 sample, could you provide the QoS logs and ETL Traces for analysis? Information on collecting the data is available in the included Overview Guide, sections “5.3.4 - Additional Logging Features” and “5.4 - ETL Traces”Thanks,\\nWillQuick notice: 2.1 doesn’t work on my HL2 any better then earlier version. Sorry don’t have time to debug cloud XR at this time. MS holographic remoting works excellently so it is not my computer or network.Oh, and 2.1 with the iOS client works fine!Just upping this thread. @wrice  any news?Powered by Discourse, best viewed with JavaScript enabled',\n",
       " 'physx-does-not-start-on-winxp-due-to-enumsystemlocalesex': 'Hello,One of the users of my app, has just attempted to start it on Windows XP, however the app failed to start with the error of\\n“EnumSystemLocalesEx … could not be located in Kernel32.dll”I’ve used Total Commander to look for symbol EnumSystemLocalesEx occurences in the app files, and the only occurrence are in PhysX DLL files.Quick look at EnumSystemLocalesEx on MSDNEnumerates the locales that are either installed on or supported by an operating system.Note\\xa0\\xa0The application should call this function in preference to EnumSystemLocales if designed to run only on Windows\\xa0Vista and later.\\ngives the information that:\\n“Minimum supported client Windows Vista”Which implies that PhysX is currently unavailable on Windows XP.Windows XP is still very popular, and I would recommend supporting it as well.Powered by Discourse, best viewed with JavaScript enabled',\n",
       " 'codeworks-for-android-how-to-install-offline-without-internet-connection': 'I have to install CodeWorks for Android one the offline computer.\\nI read this article and try it. but it didn’t work. (1R7u1)\\n(1R6u1 worked well)\\nhttps://devtalk.nvidia.com/default/topic/876064/android-development/androidworks-installing-without-internet-connection/post/4715521/#4715521How to install 1R7u1 CodeWorks?Hello,Have you read this topic?CodeWorks 1R7 installation wizard is not startinghttps://devtalk.nvidia.com/default/topic/1038138/android-development/codeworks-installation-wizard-is-not-starting/post/5303853/#5303853Powered by Discourse, best viewed with JavaScript enabled',\n",
       " 'physx3gpu-lib-missing': 'I downloaded the 3.2.3 PC SDK zip but when looking for the GPU libraries didn’t find them. Did GPU support for PhysX get moved somewhere else or something?Its dlls are under the Bin folder.Powered by Discourse, best viewed with JavaScript enabled',\n",
       " 'how-to-use-the-video-codec-sdk-in-tx2': 'Hi,I’ve downloaded the sdk file and found that there was a folder called Lib. However, the .so file in Lib is built for x86_64. Now, I want to use the sdk in Tx2. So I have to build the .so file from scratch. I want to know where the source files are ?Thanks in advance.Thanks for your reply. Very helpful !I’ve tried the above method. I can use omxh264dec in GStreamer now. But the other way, multimedia API is a little bit complex. I’ve got another two questions about multimedia API.I cannot use the sample in multimedia API to decode rtsp stream. The sample I use is “02_video_dec_cuda”. The version of L4T is 32.1.  I notice that this api is based on V4L2. so I guess rtsp is not supported by the multimedia API, am I right ?The effectiveness produced by multimedia API is higher than that by omxh264dec, am I right ? If I decode the same avi video file, multimedia API is faster, right ?Regards,Ask again @ https://devtalk.nvidia.com/default/board/188/Powered by Discourse, best viewed with JavaScript enabled',\n",
       " 'unmap-by-deffered-contex-causes-null-pointer-exception-on-nvidia-cards-possible-driver-error': 'Hello.\\nThere is a problem on NVIDIA cards when we call FinishCommandList between Map and Unmap calls on deffered context. Unmap crashes then in 100% cases.All pointers are valid and Map and FinishCommandList succeed. Also this code works perfectly on AMD cards. pTexture during this code is not used anywere in rendering pipline. Also DirectX11 documentation doesn’t says that it is not allowed to have mapped textures when FinishCommndList is called. So in my opinion there is a bug on nvidia drivers side and something which shouldn’t be freed is freed during FinishCommandList.You might want to check the driver command list support on AMD cards, using ID3D11Device::CheckFeatureSupport() with an argument of D3D11_FEATURE_THREADING.  I don’t believe AMD drivers use driver command lists, instead letting the D3D11 API build command lists for the driver, whereas the GeForce drivers do support that D3D11 feature.  That’s probably where the difference in behaviour stems from.The actual problem with the Map/Unmap is I believe related to how deferred command lists treat mapping dynamic resources.  My understanding is deferred maps with discard aren’t actually deferred and occur the same way an immediate map with discard will, creating a temporary resource that is then swapped with the actual resource later.  That way you can map and modify the resource while it’s still being used by the GPU on previous draw commands, without causing a stall.When you call FinishCommandList on a deferred context, I believe it’s resetting all associations with that context, including associations to any temporary aliases of resources you mapped.  FinishCommandList essentially makes a deferred context’s relationship to any calls made prior to FinishCommandList the same as it’s relationship to an entirely different deferred context.  So trying to Unmap a resource after calling FinishCommandList is like trying to Unmap a resource mapped by a different deferred context.So the difference in behaviour you’re seeing is just different responses to an illegal Unmap call.  With an AMD driver it is D3D11 handling command list creation and possibly deals with the mapped resource differently to the nvidia driver’s own command list creation.  It seems D3D11 deals with it more graciously.In short, to my understanding, you should treat command lists generated by the same deferred context the same as you would with command lists generated by seperate deferred contexts.  It’s also my understanding that due to the way mapping with discard works regarding temporary resource aliasing, you should always unmap dynamic resources within the same command list you map them with, or map/unmap with the immediate context directly.More info:API level discussion of map/unmap: [url]Microsoft Docs - Developer tools, technical documentation and coding examples\\nDriver level discussion of map/unmap: [url]Microsoft Docs - Developer tools, technical documentation and coding examplesQuick Edit:It’s also worth mentioning that, because deferred map creates an alias of the resource, it has an immediate memory cost.  The memory that the deferred map allocates may only be freed upon releasing the command list.  That presents a serious memory problem if you’re re-using a commandlist that contains a resource map, so I think it’s best practise to release any mapping commandlist every frame.  If you want to re-use a command list rather than release and re-create, you should move any map calls to the immediate context (or a commandlist you will release after executing).Nevermind, I was thinking about where I’d remembered that from, and cant find any mention.  It is apparently faster to do your mapping on the immediate context though, since in D3D11 command lists (non-driver) the deferred mapped resource data is copied twice (once on the deferred context to a temp location, second time during commandlist execution on the immediate context it’s copied from the temp location to the GPU driver).  I’m not certain but I think the same penalty doesn’t apply on nvidia GPUs since the deferred map can interact directly with the GPU driver.  That probably also explains why your illegal unmap call only causes an exception on nvidia GPUs.hello. Thank you for your response. What I see from what you are writing is that the problem lays in different interpretations what FinishCommandList should do. My understanding of DX documentation is similar to what really happens when you let DX manage command list, because, as you wrote, that is happening on AMD. So in that case FinishCommandList is not resetting associations and it works fine.\\nIn my case this texture was just newly created and it was never associated to any stage of rendering pipeline so it shouldn’t cause any stall.\\nAnyway thanks for answer. After that I knew that it was not treated as a bug and it would work the same in the future, so I made workaround and it’s working.\\nCheersPowered by Discourse, best viewed with JavaScript enabled',\n",
       " 'exception-0xc06d007e-module-not-found-nvcamera32-0x311275': 'Hello, I have a question about crash.Since late October, the “NvCamera32 Module not found” crash has been collected in the crash reporter of the game we are serving.I think it’s a module that has nothing to do with ourThe crash accounts for 20% of the total crash. When does this crash occur?\\nAnd how do I solve it?About Crash\\nExceptionCode : 0xc06d007eCallstack\\nKERNELBASE!RaiseException+0x62\\nNvCamera32!+0x311275Powered by Discourse, best viewed with JavaScript enabled',\n",
       " 'immediate-mode-rendering': 'How can i disable TBR  / Enable Immediate Mode rendering. My GPU is the GTX 1070 Ti & My driver is 441.41Powered by Discourse, best viewed with JavaScript enabled',\n",
       " 'nvidia-gtx-1060-driver-535-not-compatible-with-ubuntu-server-22-04-06': 'nvidia gtx 1060 driver 535 not compatible with ubuntu server 22.04.06535 drivers get installed but on reboot, it failed saying not compatible driver.root@example:~# inxi -G\\nGraphics:\\nDevice-1: NVIDIA GP106 [GeForce GTX 1060 3GB] driver: nouveau v: kernel\\nDisplay: server: X.org 1.20.13 driver: fbdev,nouveau\\nunloaded: modesetting,vesa tty: 80x24\\nMessage: Advanced graphics data unavailable in console for root.How do I resolve this please?I am using  Ubuntu 20.04.6 LTS (GNU/Linux 5.4.0-153-generic x86_64)Powered by Discourse, best viewed with JavaScript enabled',\n",
       " 'actual-server-client-bitrate-is-just-half-of-maximum-bitrate-setting': 'Max bitrate: 40000 → Utilized : 17000~20000\\nMax bitrate : 20000 → Utilized: 10000~12000Is there anyone get same issue?Powered by Discourse, best viewed with JavaScript enabled',\n",
       " 'duplicate-vertex-separated-while-simulation-with-obj-file': 'Hello Team,I am working with simulation project using the openGL with Cuda.\\nI am using the toolkit 7.5 with VS 2010 using the quadro P4000 GPU.I am loading the nonoptimize obj file (2 piece with common edges) and its loading fine, while i am using the simulation part from the cuda code, the common vertex are going separate.In the cuda code creating the block using the face data from the obj file.Please guide me how the obj file will simulate as single piece.I can share obj file and any other information related the code.Powered by Discourse, best viewed with JavaScript enabled',\n",
       " 'nvenc-compatibility-with-quadro-mobile-platforms': 'Hi,I’m looking for information about the compatibility between the NVENC functionalities and the Quadro mobile family (e.g. P1000, P3000, P5000, M620).Regarding NVENC framework supported matrix (https://developer.nvidia.com/video-encode-decode-gpu-support-matrix) and the main NVIDIA documentation (Page Not Found | NVIDIA), no information are given for those mobile Quadro platforms.I’m mainly looking after the maximum number of encoding/decoding streams supported by those mobile Quadro cards.Thanks in advance for your help.–\\nVincePowered by Discourse, best viewed with JavaScript enabled',\n",
       " 'preset-guids-for-nvencodeapi': 'Hello,I’m working on a project, which was using an older video codec SDK, namely v9.1. I upgraded it to v11.1.5 and got some warnings, because the preset GUIDs were deprecated. I went through the header file and found the new ones, but when I tried using them and placed the new GUID in place of the old one I got some errors. In fact the NvEncGetEncodePresetConfig function returnd error code 12 , which corresponds to NV_ENC_ERR_UNSUPPORTED_PARAM.  I tried using NvEncGetEncodePresetConfigEx, which actually worked, but then I encountered another problem. Initialising the encoder with NvEncInitializeEncoder failed, presumably because of the new GUID provided in the NV_ENC_INITIALIZE_PARAMS struct.For more context I am trying to encode a H264 video and I have a NVIDIA GeForce RTX 3060 Ti GPU. I replaced the GUIDs according to the table given in the preset migration guide. I also tried the steps listed in the video codec sdk (which I cannot link, since I’m allowed only one link) with the same result. Strangely, I can see all the GUIDs applicable to my GPU for H264 encoding by calling NvEncGetEncodePresetGUIDs , including the new ones.I’m looking to solve the issue, but I cannot think of further steps, so I would like to know if someone has encountered the same problem or if this is a known bug in this SDK version. Any help would be appreciated.Thanks!Did you figure it out?\\nI am facing the same issue and so far have not been able to find what is causing it.Hello,No I wasn’t able to, I just went back to the old version instead.Thank you for your quick reply!After hours of tinkering I finally managed to get it to work by setting these two parameters :\\nrcParams.multiPass = NV_ENC_MULTI_PASS_DISABLED;\\nrcParams.lowDelayKeyFrameScale = 0;Hopefully this will help anyone in the same situation.Great, thanks for letting me know!This topic was automatically closed 14 days after the last reply. New replies are no longer allowed.Powered by Discourse, best viewed with JavaScript enabled',\n",
       " 'driver-crash': 'I couldn’t find an official “submit bug” spot in the dev zone so I’m posting this here:Driver v335.23 on Win8.1 Pro x64 with Geforce GTX 680Code is running D3D 11.0 feature level 11 in an x64 debug application with the debug device. The crash also occurs in release mode.I get a crash on a thread in the driver in nvwgf2umx.dll:nvwgf2umx.dll!00007ffec13721cf()\\tUnknown\\nnvwgf2umx.dll!00007ffec0f5a554()\\tUnknown\\nnvwgf2umx.dll!00007ffec0f5dd2e()\\tUnknown\\nnvwgf2umx.dll!00007ffec0f5e478()\\tUnknown\\nnvwgf2umx.dll!00007ffec0f5e7de()\\tUnknown\\nnvwgf2umx.dll!00007ffec0f5e96d()\\tUnknown\\nnvwgf2umx.dll!00007ffec0f4c3ae()\\tUnknown\\nnvwgf2umx.dll!00007ffec0e0ec98()\\tUnknown\\nnvwgf2umx.dll!00007ffec0dbe0f4()\\tUnknown\\nnvwgf2umx.dll!00007ffec0da23fc()\\tUnknown\\nnvwgf2umx.dll!00007ffec0da0a3a()\\tUnknown\\nnvwgf2umx.dll!00007ffec0d5f451()\\tUnknown\\nnvwgf2umx.dll!00007ffec0d1b9c5()\\tUnknown\\nnvwgf2umx.dll!00007ffec0a7f3d7()\\tUnknown\\nnvwgf2umx.dll!00007ffec099f881()\\tUnknown\\nnvwgf2umx.dll!00007ffec0aea64e()\\tUnknown\\nnvwgf2umx.dll!00007ffec0af8a4f()\\tUnknown\\nnvwgf2umx.dll!00007ffec0af898f()\\tUnknown\\nnvwgf2umx.dll!00007ffec13702c7()\\tUnknown\\nnvwgf2umx.dll!00007ffec137046e()\\tUnknown\\nkernel32.dll!BaseThreadInitThunk()\\tUnknown\\nntdll.dll!RtlUserThreadStart()\\tUnknownOn the main thread (crash did not occur on this thread):The flags for D3DCompile() are:D3DCOMPILE_ENABLE_STRICTNESS | D3DCOMPILE_PREFER_FLOW_CONTROL | D3DCOMPILE_DEBUG | D3DCOMPILE_SKIP_OPTIMIZATIONFinally what’s even crazier is that this crash seems to be caused by a simple multiplication in my shader. Specifically a shader that works fine:float3 light;\\nfloat ndotl;swith(lightType)\\n{\\ncase X:\\n{\\nlight = blah;\\nndotl = dot(lightDir, normal);\\nlight *= ndotl;\\nbreak;\\n}\\n}Crashes:float3 light;\\nfloat ndotl;swith(lightType)\\n{\\ncase X:\\n{\\nlight = blah;\\nndotl = dot(lightDir, normal);\\nbreak;\\n}\\n}light *= ndotl;I can provide a sample application and the actual shaders upon request. Thanks!Powered by Discourse, best viewed with JavaScript enabled',\n",
       " 'float-to-unorm-conversion': 'Hello.I have shader1 which produces results in range [0.0 … 1.0], which I store in R8 texture. I have shader2 which reads from that texture and does further computation.I am trying to write shader3 which combines shader1 and shader2 without going through the intermediate R8 texture. However, the conversion float32->unorm8->float32 that the GPU does seems to be non-trivial.For example: 64.49/255.0 → 64 and 64.51/255.0 → 65 as I would expect.But 65.49/255.0 → 65 and 65.54 → 65 too. Only at 65.55/255.0 I get 66 in the target r8 texture.Is this behavior documented somewhere? Is there a code snippet that will properly truncate a float value as if it was written to R8 and read back as float?Thank you.\\nAlex.Powered by Discourse, best viewed with JavaScript enabled',\n",
       " 'nvrtx-5-1-crash-conflict-setting-up-a-third-party-primary-spatial-upscaler-or-temporal-upscaler': 'Hey all,Hope you can help. As soon as I hit simulate in my project, I get this error.Assertion failed: !(ViewFamily->GetTemporal UpscalerInterface() != nullptr && ViewFamily->GetPrimarySpatialUpscalerInterface() != nullptr) [File:E:\\\\UnrealEngine-nvrtx-5.1.0-1\\\\Engine\\\\Source\\\\Runtime\\\\Renderer\\\\Pri vate\\\\SceneRendering.cpp] [Line: 4589]\\nConflict setting up a third party primary spatial upscaler or temporal upscaler.I have DLSS enabled, but I’m unsure why it appears to be clashing?Hi there @luke62 and welcome to the NVIDIA developer forums!Look at the assertion and try to trace what object is actually active in either UpscalerInterface. That should point you in the direction of the upscaler you might still have active when trying to use DLSS.It is not uncommon that when you use a project template or similar, that some default setting will activate the standard UE upscalers (like TSR )or AA tools, which will then cause this issue.I recommend checking the UE documentation on these and see if anything in your project directly or implicitly enables one of them.Also there are some great Webinars by one of our UE experts on how to use RTX and DLSS with UE, they also touch some of the common pitfalls.I hope that helps!P.S.: ALso check out our Discord server for even more interaction with the communtiy.Conflict setting up a third party primary spatial upscaler or temporal upscaler.Hey Markus,I managed to fix the crashing by changing Temporal Anti-Aliasing to (TAA) AND additionally changing the Project Setting ‘MSAA Sample Count’ to ‘No MSAA’.Thanks againGreat to hear that!This topic was automatically closed 14 days after the last reply. New replies are no longer allowed.Powered by Discourse, best viewed with JavaScript enabled',\n",
       " 'vkcreateimage-returns-vk-error-out-of-device-memory': 'This repo contains a small Vulkan program that attempts to create a vkImage with the maximum extent as reported by VkImageFormatProperties.maxExtent. vkCreateImage fails with VK_ERROR_OUT_OF_DEVICE_MEMORY. The README in the repo contains information on how to build and run it. (It’s really easy).I ran this on my laptop to get output for both NVIDIA and Intel. Here’s the output:Creating device: NVIDIA RTX A5000 Laptop GPU\\nNVIDIA Driver version: 517.40.0.0\\nVkImageFormatPropeties.maxExtent = { 16384, 16384, 16384 }\\nVkPhysicalDeviceFormatProperties2.properties.limits.maxImageDimension3D = 16384\\nCreating image with extent: { 16384, 16384, 16384 }\\nvkCreateImage returned VK_ERROR_OUT_OF_DEVICE_MEMORYCreating device: Intel(R) UHD Graphics\\nIntel Driver version: 100.533\\nVkImageFormatPropeties.maxExtent = { 16384, 16384, 2048 }\\nVkPhysicalDeviceFormatProperties2.properties.limits.maxImageDimension3D = 2048\\nCreating image with extent: { 16384, 16384, 2048 }\\nvkCreateImage returned VK_SUCCESSNote that on Intel the result is as expected, but on NVIDIA it is not, as creating an image with the maximum extent fails.I added another reproducer to the github repo. This one repeatedly creates and deletes a 1024 x 1024 x1024 vkImage. On my Quadro RTX 8000 (Turing) this fails after ~800 iterations. On the RTX A5000 Laptop GPU and the Intel UHD Graphics GPU, it doesn’t fail. Please take a look at the README there and the reproducers for all the details.I’m working on a 3D MegaTexture system, and would like to create images that are as large as possible, but sparsely resident and sparsely bound.Cheers,\\nFrodePowered by Discourse, best viewed with JavaScript enabled',\n",
       " 'is-physx-2-8-4-source-code-available': 'is physx 2.8.4 source code available for ios, I’ll build a version for arm64. thanks.Powered by Discourse, best viewed with JavaScript enabled',\n",
       " '4-quadro-rtx-5000-sync2-6-2-mosaic-4k-displays': 'Hello.Can I build a system for 4x Nvidia Quadro RTX5000 cards via SYNC2 card for 12x 4k projectors with mosaic? I am going to use ASUS ESC8000 G4 as a platform. As an operating system, use Windows 10. What incompatibility problems can I face? And will these warp & blend sdk be supported in such a topology?Powered by Discourse, best viewed with JavaScript enabled',\n",
       " 'sphere-intersection-list': 'hello all,I have a scene that contain about 30 spheres with other stuffs.know, I need to know the number, the coordinates and the indices of all spheres hidden partially or entirely by each sphere.\\nIs there an easy way to do this in Optix?\\nI am counting to use rtIntersectionDistance with the Ray direction each time to calculate the coordinates of the hit sphere, which should give me a way to determine the index of this sphere.\\nI am not sure about the way to return these indices, in a 3D rtBuffer is a good idea?thxUsing rtintersectionDistance is the good way to calculate the hit point and what you want to do should work.Consider two alternatives:Any good?\\nLPowered by Discourse, best viewed with JavaScript enabled',\n",
       " 'how-can-i-copy-nvencode-input-buffer-to-a-direct3d11-texture': 'I am using nvencode for video encoding. My capture card copies frames to pre allocated input buffers (NV_ENC_INPUT_PTR) using direct dma transfers (GpuDirect). the sequence looks like so:nvEncodeApi->nvEncLockInputBuffer(_encoder, &lockInputBufferParams); // lock input buffer\\nDmaCopy(lockInputBufferParams.bufferDataPtr); // initiate dma transfer\\nnvEncodeApi->nvEncUnlockInputBuffer(_encoder, lockInputBufferParams.inputBuffer);// here I initiate the encoding processThis works fine, but in order to present the captured frame I need to copy it to a texture.Is there a method to issue a copy from the NV_ENC_INPUT_PTR inputBuffer to a Direct3d11 texture once it is unlocked?Note: I know I can use Direct3D texture as an input buffer to nvencode and use Map/Unmap API. But my capture card can’t seems to be able to perform DMA transfers directly to a mapped texture.Hi,You cannot copy NV_ENC_INPUT_PTR inputBuffer to a Direct3d11 texture. The only option here is to somehow feed Direct3D texture as an input buffer to nvencode and have some mechanism to copy the data to the Direct3D texture that you intend to feed to NVENC.Thanks,\\nRyan ParkPowered by Discourse, best viewed with JavaScript enabled',\n",
       " 'more-information-for-sdk-samples': 'The SDK is a mine of interesting things, but sometime the information are very limited (just the code).In particular for this samples: “mcmc_sampler”, “mis_sample” and “path_tracer”,it is possible to have more information on the methods implemented?Some link to the original technical document would be appreciated ;-)Thank you very muchPowered by Discourse, best viewed with JavaScript enabled',\n",
       " 'undefined-symbols-for-architecture-x86-64-physx-3-3-2-xcode': 'Hey I am new to Physx and I can’t seem to get it to compile. It seems like Xcode cant find the libraries but I did link them in.Here is my errors:\\nUndefined symbols for architecture x86_64:\\n“PxCreateFoundation(unsigned int, physx::PxAllocatorCallback&, physx::PxErrorCallback&)”, referenced from:\\nPhysxManager::Init() in PhysxManager.o\\n“PhysxManager::gDefaultErrorCallback”, referenced from:\\nPhysxManager::Init() in PhysxManager.o\\n“PhysxManager::gDefaultAllocatorCallback”, referenced from:\\nPhysxManager::Init() in PhysxManager.o\\nld: symbol(s) not found for architecture x86_64\\nclang: error: linker command failed with exit code 1 (use -v to see invocation)Thank youFigured it out. Forgot linkerflags. haPowered by Discourse, best viewed with JavaScript enabled',\n",
       " 'organizing-gases-iases-for-local-ray-tracing-in-grid-based-scene-subdivision': 'Hi there,I was experimenting with a hybrid algorithm to increase spatial coherency. Conceptually my scene is subdivided into a grid of 3D cells. One requirement of the algorithm is that each cell has a local tracing range, that is, the maximum range in which it can perform path tracing. I define this tracing range to be the content of the cell itself, along with a set of its neighbours (could be 26, 63, 124… neigbours, depending on the range we want to take into account). Now my question is, how do I best organize the acceleration structures for each cell? I do not want a shared acceleration structure that contains my whole scene, because the whole point of the algorithm is to only access the ‘local data’ within the tracing range.One idea was to have a separate acceleration structure per cell that only contains the geometry of that cell. I would then take all ASes of the neighbours in the tracing range and have to test for intersections against each one of them (there are optimizations possible such as skipping the non-facing neighbours but let’s omit that here), saving the closest intersection. An advantage of this approach I believe is that we have no duplicate geometry present in the ASes, each AS just represents the geometry of the cell it belongs to. However, we do have to loop over the ASes and test for intersections against each one of them, which negatively impacts performance (correct me if I am wrong here).Another idea was to create an AS for each group of neighbours in a tracing range. So for a given cell, take its own geometry and the geometry of its neighbours within the tracing range to build the AS. Now we end up with only one AS per cell that we need to trace against, but there is severe redundancy in the geometry that multiple ASes represent. So a disadvantage that I expect here is much greater memory usage.I hope these 2 ideas make it a little bit more clear what I am trying to achieve.Any thoughts or recommendations on this?Thanks in advance.Hi @Chuppa, sounds like an interesting problem!I think you’re right that building a Geometry AS (GAS) per cell and looping over neighbor GASes explicitly would slow down your perf considerably. However, one thing to remember that may open up some options for your is that you can use Instance ASes (IAS) to group GASes however you want. So you do have the option to build 1 GAS per cell, and avoid all duplicate geometry, but also build 1 IAS per cell+neighbors. Then the only thing that’s duplicated is the handles to the neighbor GASes, rather than all the geometry. You can then trace a single ray against the IAS for a given cell+neighbors, and OptiX+RTX takes care of looping over all the neighbors in a much more efficient way than if you try to loop explicitly. So I think you can combine both approaches you describe, and take the best of each. Does that makes sense and sound doable? I think the biggest potential for trickiness here is that depending on what your shading/material system looks like, you might have to be careful with SBT offsets, or you might have to create an SBT for each IAS, which might create some data duplication you weren’t thinking about.All that said, I know you said you don’t want a shared acceleration structure, but I’m curious if you really truly need to physically subdivide your scene? What if you kept the AS for the entire scene, and had an auxiliary grid structure for your cells that keeps only tracing range information? Would it still work for you if you could look up the tracing range before tracing a ray, and then set the ray’s tmax parameter accordingly? This would keep a ray from tracing beyond any given cell’s neighbors, without having to explicitly partition the scene. If your tracing range is constant, then of course you don’t need an explicit grid structure either, but you can still read/write other information per cell before & after tracing a ray, if that’s part of the algorithm.The reasons to consider the single shared AS are that you can still control locality of access via the ray origin and the ray tmax parameter. The ray will not iterate through anything in the scene that it can trivially cull, so if you shoot a ray inside a conceptual cell with a range that stops at the cell’s neighbor bounds, traversal will only access approximately the cell & neighbor contents. Each ray might traverse through a couple of large top-level boxes in the hierarchy, but keep in mind that if you partition your scene explicitly into an AS per cell, it will not eliminate traversal through the top level boxes, it will just move that part of traversal into software, where it might be slower. My suggestion above to use an IAS for the cell+neighbors is an example of this: while it can add more steps in the BVH tree traversal on the hardware side, it replaces a much slower software iteration over the neighbors, and the hardware can do extra BVH culling in parallel and for approximately free from your perspective. So, I recommend re-examining the assumption that you don’t want a single shared AS, you might be able to explore your locality algorithm even with a single AS, and it might even give you more freedom to set a continuous range limit on rays, instead of using a discrete cell size.–\\nDavid.Hey @dhart,Thank you for sharing your insight! The IAS per cell + neighbors approach sounds great, I did not know that was possible but it sounds like it is indeed the best of the 2 worlds that I described above.Concerning the approach using only 1 AS for the entire scene: I think what I am currently doing is exactly what you described there, setting the tmax parameter according to the tracing range of that cell. However, I had my concerns in terms of coherency, which is the reason why I asked this question and wanted to transition to the IAS per cell + neighbors approach.In this project I am experimenting with a memory coherent approach to path tracing. What I really want to achieve here is to make sure that the local data of one cell + it’s neighbors inside the tracing range fits into cache, ultimately leading to a large amount of cache hits. I then want to optimize the cell size depending on the GPU’s cache sizes and so on. For small toy scenes this approach will not/barely make a difference, but my hope is that the method can benefit performance in larger scenes. I am not really aware of the underlying OptiX implementation when it comes to intersection tests, but my concern with one giant AS for the whole scene was that a lot of incoherent memory accesses will still be made, essentially rendering the technique I am trying to implement useless. Do you think that would be the case?Conceptually I agree with what you said, if I just set the tmax parameter to the tracing range, only some top-level large bounding boxes will be intersected with, the intersection itself should still be found within the local tracing range.  But I am not sure what exactly will happen ‘under the hood’. I also believe that for large scenes the size of the AS scales up considerably as well (possibly many levels of the hierarchy need to be traversed before we are in ‘tracing range’), am I right? Given this scenario, do you think the access to those bounding boxes higher up in the hierarchy can lead to incoherent memory accesses? If not, one AS for the whole scene is the easiest to implement and also the most preferable option I think.Thanks in advance!– ChuppaMakes sense to me! Yes I’d still guess that using tmax will help constrain memory access and keep it a bit more coherent. Under the hood, OptiX is not requesting memory for anything culled by a ray, and use of tmax will automatically cull anything further away than tmax. So I would expect using tmax to limit the amount of memory requested for a ray during traversal, and to limit the memory requests to geometry and tree nodes that land within in the [tmin, tmax] range.The good news is that it should be pretty easy to test. Nsight Compute can show you stats about total memory request sizes and L1/L2 cache hit rates. If we’re right about reducing tmax helping your algorithm, then the memory stats and any associated speedup should be visible without too much effort.This may depend entirely on how many rays you launch at a time, and whether a batch of rays are starting nearby each other. If you limit the tmax for all rays, but cast them from every cell in the scene all going random directions, then the kernel overall is still going to request approximately everything in the scene, and the access might still be quite incoherent. This is one reason to start looking at a grid data structure that can process batches of rays that are all near each other. Another consideration is whether all the rays in a warp tend to be spatially local. OptiX tiles your thread indices to keep camera rays together, which usually helps with that first path segment. Secondary rays for AO or global illumination can become incoherent quickly, and this might lead to warp level divergence where trimming tmax or even partitioning the scene might not do that much good.One option in the near future will be use to the upcoming Shader Execution Reordering feature to launch rays sorted by their spatial locality. This doesn’t limit the batch to have nearby rays, but it might help make sure that all rays in a warp are very near each other, which might allow the GPU to coalesce some or all of the memory access in the warp.–\\nDavid.Great! That sounds exactly like what I want to achieve. About the profiling with Nsight Compute, I recently started looking into this. Is it correct that I should profile my overall application with Nsight Systems and look into more detail on the places where possible overhead occurs with Nsight Compute?Thanks again for the help and great tips.– ChuppaThis topic was automatically closed 14 days after the last reply. New replies are no longer allowed.Powered by Discourse, best viewed with JavaScript enabled',\n",
       " 'link-error-with-physx-3-3-1-on-linux': 'I’m currently experiencing linking errors, more specifically undefined reference to __*_finite where * are the usual trig functions cos, sin, asin, acos after an upgrade from 3.3.0. Is it possible to have the documentation updated with the version of glibc and the compiler version used to built the SDK. Is anyone else experiencing this…if so, what was the solution ? Thanks in advance.compiler?\\nflags?\\nverbose compilation output?R u sure u didn’t use some parts of 3.3.0 and some of 3.3.1?Markuscompiler haven’t changed GCC 4.5.2 on Ubuntu ( have been using this since 3.0 of the SDK with no link issues whatsoever ).The compiler is configures to spit out all warning etc…but maybe I’m not understanding the question.I’m sure that nothing has changed in my codebase. Downloaded the 3.3.1 the day it became available, updated my shell script to point to the new path and the link error showed up…changed shell script to point to the 3.3.0 SDK…builds fine.Some preliminary investigation indicates that those function are present/reference in glbic 2.15+, but my distro have 2.13…not 100% positive if this is the issue. However, if this is the case, then the toolchain/enviroment used to build the SDK must have been changed between the 3.3.0 and the 3.3.1.Same problem here with version 3.3.2, glibc 2.15 is needed for physx to work in linux?Powered by Discourse, best viewed with JavaScript enabled',\n",
       " 'loading-data-from-compute-shader': 'Hello, i really need to solve this problem and i hope someone could help me herei just want to load 2 values, set in my compute shader, into an array. I tried something IÂ´ve seen in a sample, like:So my simple cs should set new values for resultsRWStructuredBuffer data : register(u4);[numthreads(1,1,1)]\\nvoid stillSimpleCS( uint3 GroupID : SV_GroupID, uint3 DispatchThreadID : SV_DispatchThreadID, uint3 GroupThreadID : SV_GroupThreadID, uint GroupIndex : SV_GroupIndex )\\n{\\ndata[0] = 2.0;\\ndata[1] = 1.0;\\n}but he seems to fill results with 0. Can anybody tell me how to fix this?Are you creating your “g_pReadBackBuffer” with the correct flags for a staging buffer? e.g.:D3D11_BUFFER_DESC sbDesc;\\nsbDesc.BindFlags            = 0;\\nsbDesc.CPUAccessFlags       = D3D11_CPU_ACCESS_READ;\\nsbDesc.MiscFlags            = D3D11_RESOURCE_MISC_BUFFER_STRUCTURED;\\nsbDesc.Usage                = D3D11_USAGE_STAGING;Powered by Discourse, best viewed with JavaScript enabled',\n",
       " 'nv-command-list-and-binding-state': 'Looking for a little clarification: I’ve got a basic implementation using just DrawCommandsNV from  [url]https://www.khronos.org/registry/OpenGL/extensions/NV/NV_command_list.txt[/url], and I’m noticing that it appears to pick up “binding state” of uniform binding points from the OpenGL context (i.e. outside of the submitted queue), despite the fact that the extension explicitly states that it never will. My questions are:Am I correct in interpreting “binding state” as anything bound via glBindBuffer, glBindBufferBase, glBindBufferRange, glBindVertexBuffer, etc?If so, is NVIDIA following some convention I should be aware of for detecting “binding state?” E.g., is it detected in use of DrawCommandsNV but not DrawCommandsStatesNV? It would be nice to not have to respecify “global uniform buffers” bound with glBindBufferBase for every command buffer I submit. (This is especially awkward when following the pattern of “build a uniform buffer while building the command buffer, then prepend the final uniform buffer to the command buffer.”)I’m testing on a GTX 970 and Windows 10, fwiw.Powered by Discourse, best viewed with JavaScript enabled',\n",
       " 'nvdec-hevc-decoded-pictures-not-in-display-order-anymore': 'Hi,\\nSince driver version 465.89 the callback pfnDisplayPicture doesn’t deliver the decoded frames in display order anymore when using B frames.\\nUp to driver version 462.59 everything works fine.\\nMaybe there had been more versions between, but theses are the only versions I could access to narrow the problem down.\\nThe number of decoding buffers are given according to the (new) documentation of min_num_decode_surfaces given in pfnSequenceCallback and also returned.\\nDoes anyone knows something of this behaviour change, because documentation of pfnDisplayPicture still states “Parser triggers this callback when a frame in display order is ready”?Best regards,\\nOliverOk, found the problem by myself, but if someone else has a similar problem:\\nThe behaviour of the CUVIDSOURCEDATAPACKET.timestamp value changed.\\nUntil driver 46.259 the value was not interpreted or used and just given back when an image was decoded. Now it is use to sort the decoded images before giving them back (don’t know how they did this before exactly).\\nOur implementation just used this value to reference metadata with an index, this worked fine the last years, but now the value has to match the correct timestamp order of the encoded images.This topic was automatically closed 14 days after the last reply. New replies are no longer allowed.Powered by Discourse, best viewed with JavaScript enabled',\n",
       " 'cloudxr-understanding-of-the-number-of-streams-in-transmission-and-in-reception': 'Hi,In the enumeration below we can choose between Generic or XR modetypedef enum\\n{\\ncxrStreamingMode_XR,\\ncxrStreamingMode_Generic\\n} cxrStreamingMode;when using cxrStreamingMode_XR the value of the number of streams is directly changed to 2 (cxrReceiverDesc.numStreams = 2) by the SDK. I would have liked to know if it is an emission stream to send the video from the cameras to the server and a reception stream to retrieve the rendering linked to the real-time engine.does that seem right to you?then in the CloudXR FAQ they say we have 1 stream per eye\\nso that would mean that we have:is my reasoning correct?and if so, would it be possible to change this to a single Stream in reception to avoid using too many spatial streams?Julien.Powered by Discourse, best viewed with JavaScript enabled',\n",
       " 'building-physxpvdsdk-dynamically-not-working': 'I’m trying to get PhysX running on a personal project.calling PxCreatePvd gives the following linker errorNow that’s okay, I’m not linking against PhysXPvdSDK_64.lib.I went back to my PhysXSDK.sln and switched PhysXPvdSDK project from Static library (.lib) → Dynamic Library (.dll). Trying to build the PhysxSDK or link against the .lib that is generated in the personal project results in the following error:It is worth noting that there was no .dll generated and there were no errors if the PhysXPvdSDK project is built individually.I also tried using the PhysXPvdSDK_static_64.lib however that throws the following error.Edit:\\nAlright I switched the PhysXPvdSDK Runtime Library from /MT (default) to /MD and it is linking correctly. I would like it if I could get confirmation that that is correct.Powered by Discourse, best viewed with JavaScript enabled',\n",
       " 'ar-support-for-hololens': 'In the current docs it is explicitly stated that AR is not supported on Hololens 2. Will this change soon or is there a way to enable anyway?  Also, is there any docs on what gestures map to what input?Hi,AR is actualy supported on Hololens2 and CloudXR 2.0.\\nYou have to use the files available in Sample\\\\WindowsMR\\\\HoloLens.The hand tracking is working fine (three 3DoF on translation an 1DoF on rotation, and Trigger_Click)\\nOtherwise, by dinging into code I found this mapping :\\nHOLD = Touchpad_Click\\nSINGLE_TAP = Trigger_Click\\nHAND_MOVED_LEFT = ApplicationMenu\\nHAND_MOVED_RIGHT = System\\nHAND_MOVED_UP = Grip_ClickThanks. That is the code I am already using. Good to hear that it actually supports AR rendering against transparent background and not just fullscreen VR… Have you seen any example of how one should use it and how to configure apps using OpenVR to utilize this? (In unity? In unreal? Or some sample c++ OpenVR project?) Just trying to understand what is possible before I invest too much time in this. In the end I am looking for tracking real objects and placing serverside rendered content in relation to that.  For a first test it would be great to see this working at all by placing server side rendered object on surfaces, just like in the basic Hololens unity projects but having the heavy lifting being done on the server.Hi again,In Unity, you have to hide the Environment and to show only the objects you want to display on the AR Device. Then, the camera must be on SolidColor mode, with black as the Background, and the Far Clipping Planes at arroud 10 meters.However, the issue you’ll facing is that you won’t be able to perform tracking of a real object by the Hololens2 as the app running (CloudXR) won’t get the camera video stream that is used by commun codes…Thanks! I will test this once I get h2 to work a little better with the basics. And I guess I can create a more extensive h2 client app that sends tracking info to the server in some way.Powered by Discourse, best viewed with JavaScript enabled',\n",
       " 'how-to-create-independent-entries': 'Hi,How to make entries independent in OptiX ? “independent” means, OptiX only verifies the variables used by the entry that is launching (e.g., BVH should not be rebuilt if no rtTrace). Correct me if I am wrong, it seems there is no way to do that at the moment.Independent entries could be useful, for example an entry for per-vertex skinning / deforming / morphing. It is more interesting than CUDA kernels for multiple GPUs and with a OptiX ray tracer.A possible workaround which also doesn’t work, is to allow sharing OptiX buffers between different OptiX contexts.Thanks.Right now there is no way to do this.  An upcoming release of optix will allow deferral of JIT compilation for unused entry points, but validation will still occur on all entry points.As for sharing buffers between optix contexts, this is achievable via CUDA interop – you can allocate your memory via CUDA malloc, then hand the memory to both contexts via rtBufferSetDevicePointer.Powered by Discourse, best viewed with JavaScript enabled',\n",
       " 'running-optix-4-0beta-with-cuda-8-0rc-not-supported': 'Hi,\\nI know should be expected but getting error:\\nOptiX Error: Parse error (Details: Function “_rtProgramCreateFromPTXFile” caught exception: C:\\\\ProgramData\\\\NVIDIA Corporation\\\\OptiX SDK 4.0.0\\\\SDKbin2\\\\lib\\\\ptx/glass_generated_pinhole_camera.cu.ptx: error: Failed to parse input PTX string\\nC:\\\\ProgramData\\\\NVIDIA Corporation\\\\OptiX SDK 4.0.0\\\\SDKbin2\\\\lib\\\\ptx/glass_generated_pinhole_camera.cu.ptx, line 9; fatal   : Unsupported .version 5.0; current version is ‘4.3’\\nCannot parse input PTX string\\nfile:C:\\\\u\\\\workspace\\\\goldenrod-win64-build\\\\sw\\\\wsapps\\\\raytracing\\\\rtsdk\\\\playpen\\\\goldenrod\\\\src\\\\Compile\\\\PTX\\\\PTXtoLLVM.cpp, line: 71, file:C:\\\\u\\\\workspace\\\\goldenrod-win64-build\\\\sw\\\\wsapps\\\\raytracing\\\\rtsdk\\\\playpen\\\\goldenrod\\\\src\\\\Compile\\\\PTX\\\\PTXtoLLVM.cpp, line: 71)I assume final Optix 4.0 version will support CUDA 8.0 right?\\nif not how to use a GTX 1080?Just use CUDA 7.5 for now.\\nThe generated PTX code is rewritten by OptiX internally and sent to the CUDA driver for assembly.\\nThat CUDA driver comes with the display driver and will support the Pascal architecture.\\nYour original PTX input doesn’t need to be compiled for the newest Streaming Multiprocessor version.\\nAnything from SM 2.0 to SM 5.2 will do with CUDA 7.5.Powered by Discourse, best viewed with JavaScript enabled',\n",
       " 'send-data-as-buffer-id-vs-variable': 'I was wondering if anyone could tell me what the preferred (in terms of performance)\\ndata representation in Optix programs is.\\nThink of BRDF materials in form of a struct.\\nShould I send all materials in a single buffer and link the indices to the geometry instances,\\nor would it be better to link the materials directly to each geometry instance?\\nI would need to access the material data in multiple programs/launches each cycle if that changes anything.For few variables it’s faster to put them into individual variable declarations because of the additional indirection when reading material parameters from a buffer.I favor the buffer plus index method. In my global illumination path tracer, which concentrates on the simple creation of complex layered materials, I use a context global input buffer with an array of structures to define all material parameters in the scene.\\nThe individual Material node itself only needs a single integer variable to index into that buffer.\\nThe same index is used to present the GUI-side parameters driving these (which is using a different representation of values, e.g. degrees in the GUI vs. radians in the shader.)The performance difference was in the 1-2% range, definitely not worth the hassle of individual variable declarations per Material node.The centralized routine to update any material parameter is just so much more convenient if you have many parameters per material. My current material parameter structure defines 36 different color, scalar, and bindless texture ID parameters and each layered material picks a subset from that as needed.If you animate many parameters at the same time that should also be beneficial.\\nWith a huge number of materials in the scene, animating individual parameters might be more costly due to the map, write, unmap of the whole buffer, though I have not seen any issue of updating individual parameters in the GUI interactively. (So far tried with a maximum of 4096 different materials in a scene.)Thank you very much for sharing your experience!\\nI’ll stick to the buffer+index method in that case.Powered by Discourse, best viewed with JavaScript enabled',\n",
       " 'maya-physx-rigid-body-impulses-or-constraint-motors': 'Hello,Just started to use Physx for a dynamic rigging in Maya instead of Bullet, since its constraints are more stable. But I cannot find a solution for impulse rigid bodies or constraint motors.In a Physx documentation from 2008, there are lines about these things, but in the actual version these features are missing. [url]http://developer.download.nvidia.com/PhysX/tools/dcc-plugins/maya/PhysX_Maya_User_Guide.pdf[/url]Is there any known solution to make a wheel spin for example by keyable force?thanks,\\nMattHello there,We also made this change from Bullet to Maya, was quite easy and straightforward to do for us.For impulse, we are using the addForce method, which can be configured as an impulse:\\nhttps://developer.nvidia.com/sites/default/files/akamai/physx/Manual/RigidDynamics.html\\nhttp://docs.nvidia.com/gameworks/content/gameworkslibrary/physx/apireference/files/classPxRigidBody.htmlFor motors, we are using the drive method of joints:\\nhttp://docs.nvidia.com/gameworks/content/gameworkslibrary/physx/apireference/files/classPxD6Joint.html\\nIt works quite well in my opinion, we have better results than anything we had when we were using Bullet.Hopes it helps ;)\\nYannPowered by Discourse, best viewed with JavaScript enabled',\n",
       " 'cannot-import-materials-to-ue4': 'Hi I am trying to use the Omniverse UE 4 Connector to import nvidia marbles sample scene but each time I try to import the USD file, none of the materials load and all the objects in the scene have a default grey material. Please let me know how to fix this issue so I can start working with the scene.I have the same issue. Any help would be much appreciated.Powered by Discourse, best viewed with JavaScript enabled',\n",
       " 'problems-with-cuda-files-in-clion': 'I have simply downloaded the OptiX SDK (7.1), and have obviously installed CUDA (11). When opening a sample project in CLion, everything runs as it should, however, all the optix libraries are unknown to the IDE. Do you know anything about this?\\nimage1888×656 97.3 KB\\nI haven’t used CLion, but this must be an issue with the editor’s ability to find and parse the header files. I’d recommend checking the CLion forums, and/or their support channels. I did a quick Google search and found others having similar issues, I hope this might lead to something fruitful: https://intellij-support.jetbrains.com/hc/en-us/community/posts/360000060350-How-to-make-CLion-find-header-files-Good luck!–\\nDavid.Powered by Discourse, best viewed with JavaScript enabled',\n",
       " 'tracing-works-as-expected-with-optixbuildaccel-with-only-one-optixbuildinput-but-fails-with-two': 'I used the raycasting example as a base for building a larger set of classes to trace a custom set of rays using OptiX SDK 7.6 on an Ubuntu 22.04 base development image with the latest CUDA 12.0 install and driver 525.105.17 on either an RTX 3050 or GTX 1050. They both work, the former faster of course.I set it up to allow adding multiple meshes, each mesh being triangular. When I add one OptixBuildInput with a single triangle mesh and run optixBuildAccel with a pointer to just that single element, it traces everything very nicely.However if I give it more than one, it still traces the whole geometry (with some strange artifacts), but it spews errors like these:This of course varies from run to run because of asynchronicity, but this is a common error output.While the traced output with one mesh perfectly matches my CPU-only tracer backend, the output with two meshes shows hits which are mostly correct, but sometimes appear displaced from the same thing I get from my CPU-only tracing backend. The tuples being output are the problematic launch indexes. I looked up the error in the source, which seems to be the only place this output is ever mentioned at all. It’s obviously related to the SBT. But both meshes use the same SBT. Shouldn’t this not be a problem?The SBT records all exist and are empty. They’re properly aligned and also have the required header. I don’t reference the SBT records at all from the hit raygen, hit, and miss programs. I realize that there is some implicit data that becomes accessible when a closest hit is encountered, but I only read and write the globally accessible Params structure.I tried creating another SBT setup just to have it there so the errors can’t complain “index 1” isn’t there. This caused memory access error on the GPU and ended up blocking my unit tests from running at all on the GPU.There’s no mention of these messages anywhere on the internet that I can find. I’ve tried everything I can think of at this point. I’m sure it’s something I’m missing about the SBT, but I don’t know what it could be. The programming guide seems to instruct me to do what I’m doing already for multiple OptixBuildInput. Is there any general insight anyone has that  might help understand this complaint about the SBT?Here’s the acceleration structure build. Note that the OptixBuildInput list is stored internally in a map, but at build time I put them into a vector. Whether they’re referenced to the original or not doesn’t seem to matter. They both produce the same result.This is the source that builds the OptixBuildInput.For completeness I’ll include the SBT setup method.Just note that addGeometry adds the geometry and allocates the space in RAM and GPU memory, but does not put actual data points in the structure. Before the above acceleration build function is called another function places all the necessary points in the structure so the geometry is there for the build.This all works very well with only one OptixBuildInput, i.e. just one of the _optixInputs[meshString] given as input.Any insight would be greatly appreciated. I’m at a dead end.I see the discussion in the programming guide for 7.6 that shows how to find the SBT record for the given build input. But it doesn’t seem to indicate what the records correspond to and how much to allocate. I tried to allocate as many SBT records as there are build inputs but this just gave me a strange access violation at runtime that cited a cudaFree call.I tried to allocate as many SBT records as there are build inputs but this just gave me some access violation at runtime.That’s the correct thing to do and should have worked if all allocations are properly sized, aligned and initialized.The OptiX Programming Guide says in chapter 5.1 Primitive Build inputs that “each build input maps to one or more consecutive records in the shader binding table (SBT)”, so in your case each build input maps to its own SBT hit record, not the same.According to the error message invalid hit sbt of 1 at primitive with gas sbt index 1 the renderer tries to access more than one SBT hit record for that GAS.You’d need to allocate _buildInputArray.size() many SBT hit records and initialize each with the same hit group record from optixSbtRecordPackHeader if the sub-meshes should not have different materials.The only example code inside the OptiX SDK 7.7.0 I found which does this is in utils/Scene.cpp.\\nSee line 909: std::vector<OptixBuildInput> buildInputs( num_subMeshes );\\nand line 940: triangle_input.triangleArray.numSbtRecords               = 1;\\nThe SBT is built at the very end of that source and builds hit records per instance per sub-mesh times ray type count (hitgroup_records.size()).If that is not working, a minimal complete reproducer would be required to investigate this.\\nPlease provide the usual system configuration information along with it.\\nOS version, installed GPU(s), VRAM amount, display driver version, OptiX (major.minor.micro) version, CUDA toolkit version (major.minor) used to generate the input PTX, host compiler version.Thanks for the reply. I’m guessing I didn’t set up the OptixBuildInput SBT parameters exactly as they should have been. I’ll have a look at Scene.cpp section you mentioned. I was thrilled to get the whole thing running using a single mesh and was sure I just had a minor setup problem when the second mesh broke the process.I’ll try allocating the two SBT records again and make sure I’m pointing the two meshes at them.Again, much appreciated.I’m just stopping by to give an update on the situation. I realized before when I tried to fix the SBT records setup that I had allocated the space but didn’t copy the same packed header data into the allocated space for both OptixBuildInputs. In short, it works very nicely now. It also doesn’t produce any of the smearing effects that I was seeing before making these fixes. So there must have been some memory getting corrupted and displaced in my viewer.I’m finally starting to get a better feel for how the SBT records work. Thanks again.Nice!Yes, the SBT is a very flexible thing but in the end it boils down to the sbt-index formula in chapter 7.3. of the programming guide.\\nThat gives you all input values with which you can control the final SBT index used for an intersection.\\nWhen using instances over GAS you have additional values to control that. Some care needs to be taken of the instance SBT offsets when using more than one SBT record per GAS which affects the SBT offset of later instances.In my OptiX 7 examples I use different SBT layouts. All GAS use only one SBT entry and the scene hierarchy is always a single level instance (IAS-> GAS), so the additional fields for the SBT index calculation are available.\\nThe earlier examples have an SBT hit record per instance and store geometry attribute data pointers inside the SBT hit record data, the later ones use one SBT hit record per material shader and access the geometry attribute pointers via the user-defined instance ID.\\nThe rtigo10 example shows the smallest possible SBT among these because that doesn’t even need anyhit programs due to a faster shadow ray implementation possible when not supporting cutout opacity, which then doesn’t need SBT hit records for the shadow ray type at all, only a miss program.This topic was automatically closed 14 days after the last reply. New replies are no longer allowed.Powered by Discourse, best viewed with JavaScript enabled',\n",
       " 'bug-report-weight-p-does-not-work-properly-since-nvidia-drivers-387-92-onwards': 'As per topic, the weight-p option fail to work properly with encode process being randomly aborted half-way.\\nRead the bug reports in order;\\nhttps://github.com/rigaya/NVEnc/issues/15\\nhttps://github.com/rigaya/NVEnc/issues/18\\nhttps://github.com/rigaya/NVEnc/issues/34Aside from rigaya, ffmpeg users also report the same issue.Hi JohnLai,Our engineering team needs the exe, command line, and the content from you.Could you provide them for us?Thanks,\\nRyan ParkHi JohnLai,Our engineering team needs the exe, command line, and the content from you.Could you provide them for us?Thanks,\\nRyan ParkYou can get the latest transcoder version 4.02 here.\\nhttps://drive.google.com/drive/folders/0BzA4dIFteM2dS1ZUT1FjTnF3Q0E\\nExtract NVEncC folder out. You only need the NVEncC64.exe and its companion files (avcodec-57.dll, avfilter-6.dll,avformat-57.dll,avutil-55.dll and swresample-2.dll)Err…the command line can be found at NVEnc_3.29: --weightp unfortunately does not work with NVIDIA drivers 390.77 · Issue #34 · rigaya/NVEnc · GitHub\\nHere is the image:\\n\\n979×666 24.3 KB\\nAs for content, let say sharing of full movie is discouraged. It doesn’t occur right away with short sample. One need full length video as the error randomly trigger during bright to dark scene (fade in out) transition.\\nJust try to encode action movie such as marvel or dc superheroes. There are a lot of suitable scene to cause the error.Note: We are talking about transcoding from bluray H264 to HEVC.Aside from Rigaya NvencC64 transcoder, the weight-p error also happen with FFMPEG with error “Failed unlocking input buffer!: generic error (20)”. However, sometimes ffmpeg will continue to ‘encode’, but the output onwards after right fade in/out or screen transition will be corrupt.Hi JohnLai,The SA team cannot reproduce the problem in order for them to identify and fix the issue.I know it may be challenging, but could you provide us the sample source? It would definitely help in speeding up the turnaround process to fix this issue.Thanks,\\nRyan ParkNow that is an issue. I can’t exactly upload almost 50Gb of Thor Ragnarok BD → 1080p. My upload speed is only 10Mbps.drSHLEFF report from the github is trying to convert PASSENGERS(2016)UHD BD → 4Kdmurd report is trying to convert Chinese DVD movie Spring in a Small Town → 480 Spring in a Small Town - WikipediaUnless we are provided content, the SA team cannot proceed. They need a local repro on their end to look into the problem.Is there perhaps a smaller file that one of these users can send directly to us?You can email the content to video-sdk-feedback@nvidia.comThanks,\\nRyan ParkUnless we are provided content, the SA team cannot proceed. They need a local repro on their end to look into the problem.Is there perhaps a smaller file that one of these users can send directly to us?You can email the content to video-sdk-feedback@nvidia.comThanks,\\nRyan ParkHow about the youtube video directly? This black and white film should be to able trigger the error. I hope the team know how to download the youtube content in H264/AVC format…Chinese DVD movie Spring in a Small Town.The type of video shouldn’t matter as it is the fade in/out that causes the error. Hmmm…Hi JohnLai,The SA team cannot reproduce the problem in order for them to identify and fix the issue.I know it may be challenging, but could you provide us the sample source? It would definitely help in speeding up the turnaround process to fix this issue.Thanks,\\nRyan ParkHi rypark,You can download an archive with a 30-second fragment of the original UHD-BD, on which an error occurs with the “–weightp” enabled. I added NVEncC 4.03 and all the necessary files to the archive. You just need to unrar it to any folder and run “sample–weightp.bat” (need to install Avisynth+ 2.60).http://gofile.me/2bfU9/qY2ik2QaxSize: 206Mbrypark, it had been two weeks, any update on the weight-p bug?Powered by Discourse, best viewed with JavaScript enabled',\n",
       " 'how-do-i-create-a-virtual-display': 'Hi there, I want to create a virtual display.\\nMy device is Tesla M10, My OS is CentOS 7.6_64 with GNOME,\\nI want to create an extened screen without connecting a physical display.\\nDoes NVIDIA provide the relevant SDK? How can I do that?Thanks!I have met the same problem, but I did not find the information provided by Nvidia. Does anyone know?I have the exact same problem since at least a few months ago. Is there a solution？Powered by Discourse, best viewed with JavaScript enabled',\n",
       " 'truck-and-trailer-simulation': 'Hi everyone,\\nI need to simulate a truck with an attached trailer. I implemented the truck as a PxVehicleDrive4W. It correctly works, but I can’t make a trailer. Since all the wheels of the trailer are non-driven, I first implement it as a PxVehicleNoDrive with setDriveTorque and setBrakeTorque set to 0 for all the wheels. I then attached the two vehicles by a spherical joint. The problem is the wheels of the trailer can’t freely rotate. The truck can’t push or pull the trailer at all. Wheels only rotate if I set the torque at a value > 0. In this case, the trailer can follow the truck, but the wheels slowly rotate even at rest. Then I tried to implement the trailer as a PxVehicleDriveNW, with all the wheels set as non-driven in the PxVehicleDifferentialNWData structure. Again, the wheels can’t freely rotate.Which is the correct way to implement a trailer?Thanks in advance.Hi,You’ve done everything correct so far.  The problem here is a bug in the vehicle code. The bug is already tracked by the sdk team but not yet fixed or released.  The good news is that there is a simple workaround.  The best news of all is that you’ve already implemented the workaround.I’ll quickly describe the problem.  When a vehicle comes close to rest the tire forces are replaced with velocity constraints.  The target velocities at each wheel decrease each frame so that the vehicle is brought completely to rest over several frames.  The constraints are released as soon as the car receives a non-zero accelerator value.  If both jointed vehicles come to rest then they are both held there by velocity constraints.  When you accelerate the front vehicle it will release its velocity constraints.  The rear vehicle, however, doesn’t know that it should also release its velocity constraints.  As a consequence, it is stubbornly held where it is with constraint forces.You’ve already seen that applying a torque to a towed PxVehicleNoDrive releases its velocity constraints when it is at rest.  This is pretty much the workaround that I would recommend.  Every frame that you set a non-zero accelerator value to your front vehicle also apply a vanishingly small torque to the rear vehicle.Something like this:vehDrive4W->mDriveDynData.setAnalogInput(PxVehicleDrive4WControl::eANALOG_INPUT_ACCEL, myAccel);\\nif(myAccel > 0)\\n{\\nvehNoDrive->setDriveTorque(frontLeftWheel, 1e-5f);\\nvehNoDrive->setDriveTorque(frontRightWheel, 1e-5f);\\n}\\nelse\\n{\\nvehNoDrive->setDriveTorque(frontLeftWheel, 0.0f);\\nvehNoDrive->setDriveTorque(frontRightWheel, 0.0f);\\n}In the above pseudo-code I applied a vanishingly small torque to the front-left and front-right wheel to preserve symmetry.  This probably isn’t necessary because a torque of 1e-5f will be so small that it will have no affect on the car at all.I hope this is acceptable,GordonThe workaround seems to be fine. Thank you very much. Anyway, I look forward to an official bug fix.PaoloPowered by Discourse, best viewed with JavaScript enabled',\n",
       " 'import-obj-to-optix': 'Hi.\\nI want to import a .OBJ (or .POV) that comes from a  .OSM, to Optix 7.2, so I can use raytracing over it. Since its the first time I’m using this technology, I’m following ingowalds tutorials to familiarize with it right now, but I have to do this in the future. I’ve found people who tried it but it was with older versions of optix, such as 5.1 and 4.x, so I feel like I need some guidance on the task since I don’t even know where to start or have any examples.How would you approach this task for CUDA 11.1 and Optix 7.2? I’m using VS 2019.Thanks!Since OptiX is not a renderer but a ray casting SDK, it doesn’t know anything about file formats, and it’s your responsibility to convert any model data (vertex attributes, geometric primitives, and materials) to a format that you define in your OptiX-based renderer.There are many different ways to structure your OptiX render graph contents and even more ways to implement a material system. If you first learn how to do that in OptiX, loading data from model files becomes pretty straightforward.\\nMaterials can still be a pain. It’s of course possible to program a material system which exactly handles the associated MTL material files of an OBJ model.With that said, you basically need some way to load *.obj files into host memory which you can convert into whatever data you require for your renderer implementation using OptiX.\\nFor the fist step older OptiX SDK examples have been using tinyobjloader for that: https://github.com/tinyobjloader/tinyobjloader\\nThe newer OptiX SDK 7 versions are loading the more modern Khronos glTF format instead and use the tinygltf loader for that.Actually Ingo Wald’s SIGGRAPH OptiX 7 course contains at least two examples loading OBJ models, so you can keep studying these.\\n(I would be wary of the vector class used in that code though because that is not GPU friendly enough.https://forums.developer.nvidia.com/t/optix-7-and-msvs-2017-hello-world-on-windows-10/141711/9 )My more advanced OptiX 7 apps are using the ASSIMP library to load triangle data of any format supported by that library. I do not handle the materials or textures of that at all, except for the diffuse color. That would require quite some changes to the material system in there and those examples are meant to show other features like multi-GPU and NVLINK resource sharing.\\nThere is an example scene description file named scene_rtigo3_instances.txt for the rtigo3 and nvlink_shared examples which loads the cow.obj as multiple instances. The scene_rtigo3_models.txt shows how to load a single model.\\nI use a very simple host side scene graph which is then flattened to an OptiX render graph with single-instancing structure which is fully hardware accelerated by RTX boards.Please have a look into all sticky threads of this OptiX sub-forum containing the links to these resources.Thank you! I’ve been able to succesfully import the .obj with tinyobjloader! It seems to be good enough for what I want to do, but thank you for taking the time to reference some alternatives, I’m sure it will be usefull for someone else.Powered by Discourse, best viewed with JavaScript enabled',\n",
       " 'support-of-vk-ext-extended-dynamic-state-on-quadro-drivers': 'Hello,My company is porting its rendering engine from OpenGL to Vulkan. We are currently struggling to port our code using the OpenGL state machine to Vulkan graphics pipelines and we thought the extension VK_EXT_extended_dynamic_state could be very useful.Unfortunately, this extension seems to be not supported by our various Quadro GPUs (RTX4000/5000, M4000/5000, P4000/5000).Could we hope for a support of this extension soon or later or is it reserved to the GeForce product range?Thanks !Powered by Discourse, best viewed with JavaScript enabled',\n",
       " 'crf-for-nvenc': 'Hi,I’m unable to find any mention of CRF in the NvEnc documentation and the forum. There are quite a few speculations to be found online, in relation with ffmpeg (as in “doesn’t seem to be supported”), but nothing really definite nor up-to-date. Can anybody from nvidia please officially comment on this? In case it really isn’t supported, is there a technical reason or will it be added in the future?Thanks!In case anybody got the same question: So I finally learned that CRF is not part of the standard, but a feature specific to x264 and x265 encoders. As such, it is not likely to show up in future releases of NvEnc.In case anybody got the same question: So I finally learned that CRF is not part of the standard, but a feature specific to x264 and x265 encoders. As such, it is not likely to show up in future releases of NvEnc.There is something similar…but you must use initial QP 1, max bitrate for your chosen level (4.1?), VBR rate control and Target Quality.Read NVENC_VideoEncoder_API_ProgGuide.pdf page 14.V_ENC_PARAMS_RC_VBR\\naverageBitRate = maxBitRate (eg HEVC L5.1 38400kbps) as not to set any bitrate constraint.\\ntargetQuality = use value around 27, this will control the level of quality and bitrate. Increase the value to lower the quality and decrease the value for better quality.\\nenableAQ = 1\\nenableInitialRCQP = 1\\ninitialRCQP = 1 = must be at 1.\\nenableLookahead = 1\\nlookaheadDepth = 32 maxIf you are using pascal gpu for HEVC encoding, enableWeightedPrediction = 1 and use high bit depth. 10 bit maybe? Turn off W.P. for H264 encoding.In case if you use staxrip GUI\\n\\n688×2121 318 KB\\n\\nDon’t forget to find and enable Weighted prediction if you have pascal gpu.\\nThat pic is old after all.\\nSo…change the bitrate to 38400 for level 5.1\\nAnd select “NVencc(avcuvid native)” as Decoder.Powered by Discourse, best viewed with JavaScript enabled',\n",
       " 'moving-from-3-3-to-3-4-issue': 'I’m moving from 3.3 to 3.4 and got an issue.\\nTriangle mesh shape became flat. In debug I can see that mesh for PxTriangleMeshGeometry has normal coordinates and bounding box has normal dimensions.\\nLooks like box shapes are creating good. But mesh and convex shapes has issues. Like one coordinate doesn’t have difference in values.debugger picture\\n[url]http://i66.tinypic.com/15z179f.jpg[/url]Are you cooking assets using 3.4 cooking code or are you loading assets that were cooked in 3.3?I’m cooking using 3.4 code.\\nCooking result looks like OK.\\nTriangle mesh [url]http://oi63.tinypic.com/2iqyf01.jpg[/url]Any ideas why this issue appears?Hi,\\nThe mesh looks ok, please what PxMeshScale did you set for the triangle mesh geometry? Note that in 3.4 negative scale is supported.Hi,\\nThe mesh looks ok, please what PxMeshScale did you set for the triangle mesh geometry? Note that in 3.4 negative scale is supported.I don’t set specific scale. So it has default value 1.1.1\\n810×130 7.4 KB\\nCould you please attach PVD capture or even better the faulty mesh serialised in binary/repx format.Have you tried reading the vertex positions back in from the convex mesh and comparing them to the pre-cooked vertices to see exactly what is different?Could you please attach PVD capture or even better the faulty mesh serialised in binary/repx format.pdv captureShare your pictures, send large videos, exchange music or transfer big files. No registration required. Unlimited upload/download speed.\\nbetter visible in wireframe modeI have opened it in PVD and the mesh looks correct to me. Could it not be a rendering issue?\\n[url]http://i68.tinypic.com/29wqoon.png[/url]Thanks a lot. Sorry. My fault.\\nI should know that better to use old debuggers :). When I was working with PhysX 2.x I used debugger from 1.8 because it was light and easy to use…\\nBtw guys from NVidia you have issues in last PVD.Powered by Discourse, best viewed with JavaScript enabled',\n",
       " 'simulating-cloth-with-ragdoll-crashes-maya-and-3ds-max': 'Hi, I’m new on apex clothing and I gave a try to make an apex cloth. Everything works fine, but now that I need the ragdoll to collide the cloth showed me a problem.When I create a Kinematic Ragdoll and play simulate, the software crashes. I already tried with Maya 2012, 2016 and 3DS Max 2016.Some pics below:Problem GalleryStep 1\\n\\n\\n\\nStep 2\\nhttp://imgur.com/YFDtzO8Step 3\\nhttp://imgur.com/qzUQA0hStep 4\\n\\n\\n\\nThank you for attention!Powered by Discourse, best viewed with JavaScript enabled',\n",
       " 'quadro-vs-geforce-for-parallel-encoding': 'We’re trying to test multiple parallel encoding on a single GPU device and seem to hit a limit with GeForce that doesn’t exist with Quadro. When initializing a third encoder on GeForce, the call to NvEncOpenEncodeSessionEx fails with NV_ENC_ERR_OUT_OF_MEMORY. This is regardless of video resolution.So question, is this a true limitation of the “consumer” grade GeForce? Or a bug in our implementation that coincidentally doesn’t trigger on a Quadro device?The Quadro board in question is a M6000 and the GeForce is a Titan Xp. Latest driver 381.22 running on CentOS 7.3Yes, true limitation (in driver).\\nOnly >=x2000 Quadros (and Grid and Tesla) support unlimited encoder sessions/streams (see [url]https://developer.nvidia.com/video-encode-decode-gpu-support-matrix[/url]).Okay thank for the info! Although this is annoying for unit tests running in parallel…Did some tests and it seems that the limitation is per host rather than by device. We have a rig with two Titan Xp and wish to leverage the encoders, I would expect we’d be able to do 4 parallel encoding but we still hit the same restriction. This was tested with the official sample application NvEncoder like so:These first two session run fine, one on each deviceThe third one, second for device 0, aborts with an errorThe limit seems pretty much arbitrary if it’s per host, and a major problem for our requirements.Yes, this behavior is officially defined in “application nodes”. Older SDK (<=5) limits to 2*encoder session per system when there is at least one GeForce card in system (license bug - see [url]https://devtalk.nvidia.com/default/topic/800942/gpu-accelerated-libraries/session-count-limitation-for-nvenc-no-maxwell-gpus-with-2-nevenc-sessions-/post/4764210/#4764210[/url]).Cite from Video SDK 5.0 “application notes”:The current SDK package allows up to two simultaneous encode sessions per system for low-end Quadro and GeForce cards. If the system contains any low-end hardware (even in conjunction with other high-end hardware), only two encoding sessions will be permitted.Cite from Video SDK 6/7/8 “application notes”:The licensing policy is as follows:\\nAs far as NVENC hardware encoding is concerned, NVIDIA GPUs are classified into two categories: “qualified” and “non-qualified”. On qualified GPUs, the number of concurrent encode sessions is limited by available system resources (encoder capacity, system memory, video memory etc.). On non-qualified GPUs, the number of concurrent encode sessions is limited to 2 per system. This limit of 2 concurrent sessions per system applies to the combined number of encoding sessions executed on all non-qualified cards present in the system.\\nFor a complete list of qualified and non-qualified GPUs, refer to NVIDIA VIDEO CODEC SDK | NVIDIA Developer.\\nFor example, on a system with one Quadro K4000 card (which is a qualified GPU) and three GeForce cards (which are non-qualified GPUs), the application can run N simultaneous encode sessions on Quadro K4000 card (where N is defined by the encoder/memory/hardware limitations) and two sessions on all the three GeForce cards combined. Thus, the limit on the number of simultaneous encode sessions for such a system is N + 2.Powered by Discourse, best viewed with JavaScript enabled',\n",
       " 'pxheightfield-problem': 'Hi,\\nI’ve got some problems with PxHeightField. Objects behave unnaturally during contact with heightfield. The collision with heightfield results in object flying away at high velocity. The same simulation is stable when PxPlane is used.\\nI tried to tune various parameters but without visible results. I use PhysX 3.3.4 compiled on Ubuntu 16.04.\\nI prepared compressed version of the application:main.cpp:CMakeLists.txt:The program works well when groundPlane is used. Problems are related to commented heightfield section (groundPlane section should be commented and heightfield section uncommented).\\nWhere should I search for possible errors?\\nThanks!Hi,\\nsimulation becomes stable when framerate is set to 200. It is intuitive for me. I expected more accurate and stable but slower simulation when the value (elapsedTime) passed to PxScene::simulate is smaller.Powered by Discourse, best viewed with JavaScript enabled',\n",
       " 'simple-optix-example': 'Hello,I am beginning to study Optix.  I have downloaded all the latest resources and documentation.  I have started looking over some of the tutorials and examples and I am finding the learning curve rather steep.I am looking for a simple example that would show how to use Optix to intersect a very simple triangle mesh with a ray.  For example, say a flat sheet plate of 2 or more triangles in the z = 0 plane with a ray launched from a given beg point and direction looking into the triangle scene.  Such an example would be isolated from glut displays, pin hole cameras, phong materials, opengl rendering, etc … I don’t understand why all the samples and tutorials need be so complicated with all these other aspects of ray tracing.  I am suggesting at least one simple example showing nothing but bare minimal Optix usage and computations on tracing ray hits over a mesh of triangles (and keep the mesh simple).  The application I have in mind doesn’t need all those other peripheral pieces.  I just need to know what triangle was hit, where the ray hit on the triangle (u,v) coordinates, and perhaps the ray length from beginning of ray to hit point.As a comparison, I just finished studying Embree and I was up and running ray tracing on simple meshes in roughly 4 hours from download time.  I am seeing a much longer delay for Optix.  I do realize GPU computing is far more complicated than CPU computing but I still think there is an opportunity for demonstrating examples on an aspect of ray tracing similar to what I described above.If anyone would like to share such an example I would be grateful.Mind that the OptiX SDK contains two raytracing APIs: OptiX and OptiX Prime.OptiX is a general purpose GPU accelerated ray casting SDK which allows complex scene hierarchies and full programmability in CUDA C++ of the given program domains using a single ray programming model, which allows for custom primitive intersections, recursions, user defined per ray payloads, etc.\\nIt’s GPU ray tracing “to the algorithm”. Yes, the learning curve can be steep depending on how you start looking at it.If you only need to shoot a number of rays and get intersection results similar to Embree, you need to compare it to OptiX Prime which is a low-level intersection API doing only that. Everything else like ray generation, lighting and shading is your responsibility.Please have a look at the OptiX SDK examples with “prime” in the name if you’re only interested in accelerating ray intersections.My recommended approach to start learning OptiX is still this: [url]Tutorials & Webcasts - OptiX - NVIDIA Developer ForumsAh yes.  I see the distinction on the main Optix web page, and I see the “prime” examples you noted.  However, the tutorials and samples do not distinguish the two, or at least I did not see the distinction until you pointed it out.  Never-the-less, I believe Optix Prime is indeed the best place to start for me and I am grateful for your recommendation.  I will start with prime.  Perhaps the future documentation and examples can include a “prime” example discussion for those of us in the field that just need raw ray intersection computational horsepower versus ray trace rendering.Powered by Discourse, best viewed with JavaScript enabled',\n",
       " 'optix-version-constant': 'I’ve just needed to support 2 different versions of Optix and then I found this in versions 3.7.0 and 4.0.1:Guys, you are awesome! How did you manage to change the versioning scheme itself in a new version?? )))))That was intentional to allow two digits micro numbers.\\nTo select the proper decoding, check the major number with the old method first and if it’s greater than 3, use the new format, else use the old format.Powered by Discourse, best viewed with JavaScript enabled',\n",
       " 'pxphysics-object-lost-after-initialization': 'Hello!I just downloaded PhysX3.3 and for some reason my main physics object becomes null as soon as initialization code is finished.Definition:Initialization:Initialization returns true, but as soon as I try to do anything that involves calling the main PxPhysics object (g::pPhysXSDK) - like: creating material, the application crashes with unhandled exception error because  g::pPhysXSDK is 0.So I can use g::pPhysXSDK only inside InitializePhysX(). Meaning I can’t do anything at runtime… I don’t think it’s supposed to be this way.Ignore. Silly mistake.By the way. I am getting some memory leaks from InitializePhysX(). Does anyone know why?This is my cleanup method:It looks like you still need to release the cpuDispatcher.Hope that works out,GordonYes. That did it. Thank you.Powered by Discourse, best viewed with JavaScript enabled',\n",
       " 'does-nvidia-cloudxr-3-0-include-a-sample-client-for-hl2': 'In Jan. 2011 we added a sample client for the Microsoft HoloLens2 (HL2) – v2.0 and v2.1.  This sample client was an attempt at bridging OpenVR applications with the functionality of the HL2.  This bridging, even with user feedback and optimizations, did not result in a sample client that was performant to NVIDIA standards.  As a result we did not include an HL2 sample client in the CloudXR 3.0 release. At the same time we realize the HL2 is a critical display in the realm of enterprise AR and will continue working to create a highly performant CloudXR HL2 sample – watch for news on our HL2 plans on this thread.Powered by Discourse, best viewed with JavaScript enabled',\n",
       " 'geforce-gtx-1050-ti-linux-mint-driver-version-390-116-problems': 'Hi, I’ve wobbles and bad fonts rendering problems.\\nKernel:4.15.0-20-generic\\nNvidia Driver: 390.116\\nOS: Linux Mint 19.1\\nGeForce GTX 1050 Ti \\n\\n\\n \\n\\n\\nHow can I fix it?Powered by Discourse, best viewed with JavaScript enabled',\n",
       " 'vksetdebugutilsobjectnameext-crashes-in-nvoglv64-dll-for-vkintance-and-vkphysicaldevice': 'I’m trying to give names to VkPhysicalDevices and VkInstance for debugging purposes, but I’m encountering a crash while doing so. My situation is similar to that discussed in the topic vkSetDebugUtilsObjectNameEXT() causes a crash when attempting to set a name for VkPhysicalDevice. I’ve followed the instructions detailed there and in this GitHub issue but I still can’t get it to work.\\nI’m using vkGetInstanceProcAddr() to fetch the vkSetDebugUtilsObjectNameEXT() function.The crash occurs two calls deep into the validation layers and about(can vary) three calls deep into nvoglv64.dll into a function pointer which sometimes displays: 0x0000000000000000(), 0x00000000FFFFFFFF(), and depending on the setup(if I set the physical device’s name first or the instance’s) it can display other addresses.My OS is Win10 build 18363.1016, NVIDIA Vulkan Driver version 451.98, LUNARG SDK version 1.2.148.1, GPU GTX970 and Vulkan Info.exe reports instance version 1.2.148Powered by Discourse, best viewed with JavaScript enabled',\n",
       " 'i-have-a-problem': 'excuse me :)this is the first time visiting this great website so excuse me if this topic not in the right placeI have a Problem with game pro evolution soccer 2014 its run on my  Intel card but i have a NVIDIA card also my laptop is acer with NVIDIA Geforce 610 M and this game pro evolution soccer 2014 run by intel please help me on this problem however i have already install the last version of the NVIDIA update  help me and thank you .I have the same problem. I think it’s a driver problem that Nvidia will fix soon with a new update. Is that right?i have the same problem … so what is the solution ?!I have the same problem and i tried a couple of things that didnt work for me but maybe it will for you so try this :Mr. mahmouddxi have already done what you have told and didn’t work :( iam relay need a solutionI have the same problem with my gt540m 1GB. Still not sure if it’s Konami ot Nvidia’s mistake… Fix it soonI hope NVIDIA Fix it Soon !When will fix be available?? :(((you should download this fileModdingWay is the source for Games Mods, Patches, Updates - FIFA 22, FIFA 21, NBA 2K21, PES 2021, Madden 21, FIFA 20, NBA 2K20, FIFA 16, FIFA 15, FIFA 14 - PC, Xbox Series X, PlayStation 5, Xbox One, PlayStation 4 and Wii video games news, reviewsfix 100%\\nwork 100%I have run and config File loader config.exe but could not working. It is not a resolvable.everyone who have nvidia optimus problem solved that problem.\\nopen file loader config.exe…\\nyou should change the resolution to 1366’768…try it.good luck.Still not working…It is able to quite play in resolution 1366x768, but in Setting->Specification->GPU not display nVidia Card but instead of Intel HD Graphic. Nvidia and Konami should fix this problem.[update comment 24-Sep-2013]Now best way to fix it, just download and install PES 2014 PESEdit PATCH 0.1Try this URL [u]http://kickass.to/pes-2014-pesedit-patch-0-1-t7904861.html[/u]Specification can detect your VRAM of Nvidia Card,\\nyou really can play smooth with high graphic setting\\n\\nSPEC.jpg1173×517 177 KB\\nhey guys i  have the same problem, i tried a lot of things like to change settings from Nvidia control panel, or change graphic card mode form bios but it didn’t worked anyway.is there any news how to solve this problem?It works with File Loader 1.0.0.2.\\nJust follow the instructions here: http://www.pes-patch.com/2013/09/pes2014-file-loader-1-0-0-2-by-jenkey1002.htmli have solve the problem by this file download it and put it in the game file it workshttp://www.mediafire.com/download/k8hn5hypwl9myh5/NVIDIA.rartry and tell mei have solve the problem by this file download it and put it in the game file it workshttp://www.mediafire.com/download/k8hn5hypwl9myh5/NVIDIA.rartry and tell meThe patch didn’t help!!\\nIs there any other way round? :/Hello guys , i was bought the game month ago , and gues what ?? I have the same problem.Maybe u know about any solution from konami or nvidia ??I was buy pes in presale , this is very dissapoiting to see what they do with our money.U are buying the game , if u have a laptop and u are using optimus drivers - ITS YOUR PROBLEMI was send a message to konami - i dont have any reply to this timeIts boring :(hey man , i have the same type of card as you and i had the same problem but i just solved it out minutes ago :D first go here Official Drivers | NVIDIA and download the last driver for your graphic card ( GEFORCE 327.23 ) then reboot your computer and check if it’s fixed . if not tell me here and i’ll tell you what to do . :)Powered by Discourse, best viewed with JavaScript enabled',\n",
       " 'vkgetimagesparsememoryrequirements2-returns-large-miptail': 'If i create a sparse residency image with this info:and then call vkGetImageSparseMemoryRequirements2 for that image, i get a count of 1 and the VkSparseImageMemoryRequirements2.memoryRequirements is filled with this infoI would expect imageMipTailSize to be exactly 1 page and imageMipTailFirstLod to be 2 in this example, since the mipmap is 384x256 at level 1.The 1.1.73 spec reads in 29.4.2 (page 866 with all extension):The following image returns even more confusing results. An image withgets me thisThis behaves like the VK_SPARSE_IMAGE_FORMAT_ALIGNED_MIP_SIZE_BIT flag, but my physical device reports residencyAlignedMipSize is false, so that flag cannot be present. While the VK_SPARSE_IMAGE_FORMAT_SINGLE_MIPTAIL_BIT is weird (since the other image did not have it), i do not think it is prohibited, but since info.arrayLayers == 1 is makes no difference.\\nI would expect imageMipTailSize to be, again, exactly 1 page and imageMipTailFirstLod to be 3 in this example, since the mipmap is 960x540 at level 1 and 480x270 at level 2.My system:\\nWindows 7 SP1 x64\\nMSVC 2017 x64 C++ compiler\\nGeForce GTX 970 with Beta-Driver 397.40\\nLunarG Vulkan SDK 1.1.73.0The problem is still present in both driver 398.36 and 397.96.\\nI would really appreciate it if i could get some feedback on it, and i am willing to provide a repro-app if required (even though it seems pretty straight-forward to identify).RegardsPowered by Discourse, best viewed with JavaScript enabled',\n",
       " 'suspicious-memory-consumption-for-rtcontextcreate-rtcontextdestory-pair-in-optix-3-8-0': 'Hello,I am trying to find a memory leak in my code. For test purposes I made a simple program where I create and delete context in an infinite loop. RSS section of ps command on linux is persistently (~1MB per sec) increasing during the run. Could it be some sort of  memory leak there?Optix: NVIDIA-OptiX-SDK-3.8.0-linux64\\nDriver Version: 352.21UPDATE:Also tried it withNVIDIA-OptiX-SDK-3.6.0-linux64 - more or less the same behavior;\\nNVIDIA-OptiX-SDK-3.5.1-PRO-linux64 - RSS also increases, but slower that in 3.6.0 and 3.8.0.Hi,It still leaks in 4.1.1.I also tried using Handle template, it also leaks:Powered by Discourse, best viewed with JavaScript enabled',\n",
       " 'flickering-problem-on-secondary-window-on-multi-gpu-system': 'Hi,I’m the main developer of Black Ink a drawing software using DirectX to render his UI.\\nSome user report some weird flickering on secondary Window of the Application.\\nThis issue isn’t reproducible here ( won’t dont have system with 2 GPU ).\\nThe issue only occurred when the NVidia GPU is selected as the rendering graphic card. when the user selected the Intel HD all is fine.here 2 video showing the problemVideo1\\nVideo2Video captured on a Surface Book 2\\nGPU: GeForce GTX 1060 6Gb\\nWindows: 10 (1909 18363.752)All swapchain are rendered with a SyncInterval of 1 ( which mean the the program wait the VSync ).if anyone had a solution would be great.Powered by Discourse, best viewed with JavaScript enabled',\n",
       " 'buget-card-for-video-software': 'Hi, i am developping a video related software, and never owned an nvidia video card, i wanted to buy one but i don’t know which to choose to fit my criterias :Any suggestion are welcome !Powered by Discourse, best viewed with JavaScript enabled',\n",
       " 'rendering-artifacts-with-2d-plane': 'I’ve been playing around with the new introduction samples and have hit a strange problem.I’m trying to switch at runtime between a raytracer (optixIntro_03) and  pathtracer(optixIntro_04). I’m switching between renderers using 2 entry points and I’m using unmodified versions of the 03 and 04 sample programs except that I made the miss program render blue for the raytracer. I’m sharing the same bounding box and intersection programs. When the render type is changed, I switch miss programs and update each GeometryInstance with the appropriate material before rendering again.Each render type works correctly if it’s the first one used but when I switch from one to another there are rendering artifacts.Here’s a short video\\n[url]OptixRenderSwitching - YouTubeThese artifacts show up only if the ground plane is 2D. When I make the ground a 3D box, switching back and forth between renderers works perfectly.These artifacts are typical self intersection patterns.The sysSceneEpsilon is used to offset the continuation ray’s start interval value t_min in the optixIntro_04 example. That epsilon offset is scene size dependent.\\nIf you run any of the original examples 04 or higher and decrease the scene epsilon factor in the GUI (System → Scene Epsilon) to 0.0 you’ll see them on all geometries.Also note that the default settings of that 04 example is showing ambient occlusion by limiting the maximum path length to 2 and using all white Lambert materials in a white environment.\\nBut it’s actually a full global illumination brute force path tracer, only using just a Lambert material.\\nTry to set the floor plane material number 0 to some other color and then increase the maximum path length (hmm, I shortened the GUI names too much) and you’ll see the proper color bleeding from the floor onto the objects.Now, the optixIntro_03 is doing nothing like that because it is only shooting primary rays.\\nThe GUI is empty except for the Mouse Ratio factor.\\nIt’s also not doing antialiasing through jittering of primary rays’ sub-pixel position, it’s just shooting primary rays through the center of the pixel and is done after one launch.That 03 example is only meant to show how to get Geometry nodes into the scene and connect GeometryInstances with Materials and the closethit program. I would not “combine” that example with any of the following. Not sure what you did there, but you possibly forgot to reset the sysIterationIndex to restart the accumulation.The tutorial examples were meant to show the progression of starting with OptiX from scratch to a very elegant and easy to extend uni-directional path tracer. Each of the examples completely replaces the previous one architecturally until 07.\\n08 and 09 just add motion blur and denoising on top.If you want to integrate the resulting path tracer into an own application I would start with optixIntro_07. Anything before that is just leading to that final architecture.If you want to have a normal vector visualization mode in that path tracer, you would simply add some new BSDF implementation to the buffer of bindless callable program IDs with maybe a “mode” parameter which is selecting some value to return on the sampling function and terminates the ray.\\nThat wouldn’t be a directly lit material, so evaluation would do nothing. Do not set FLAG_DIFFUSE and it will not be evaluated. Use the dummy evaluation program from the specular reflection material which just returns null.\\nIf you let that visualization output into the radiance of the per ray data, it’s actually behaving like an implicit emissve material then.\\nAnyway, it’s some way to handle Arbitrary Output Vectors (AOV, name it bsdf_aov maybe).Some notes on adding geometry dynamically to the scene.\\nThe examples are currently not showing instancing of identical Geometry via Transform nodes as described in the slides 8 to 11 in my GTC 2018 presentation.\\nFor example, in the optixIntro_06 are two spheres. That wouldn’t have been necessary because they have the exact same geometry, scaling is done via the Transform.\\nIf your application can add arbitrary numbers of these basic geometries, it’s highly recommended to reuse the Geometry and Acceleration nodes, if you’re not already doing that.Powered by Discourse, best viewed with JavaScript enabled',\n",
       " 'create-joints-would-crash-when-using-pvd': 'Dear Everybody,I am using vs2012+physics3.3.2+pvd2.01.A simple box falling program works fine with my program, I could see the animation from pvd. However, when I just want to link two objects with a joint(whatever kinds of joints it is). The program builds well, but would crash if the PVD is connected. If I didn’t open the PVD, the program will go through.The error message from the debugger is …..\\\\PhysXVisualDebuggerSDK\\\\PvdDataStream.cpp(504): Assertion failed: success.use breakpoint to debug, the crash seems accured here:\\nPvdError createInstance( const TDataType* inst )\\n{\\nreturn createInstance( getPvdNamespacedNameForType(), inst );\\n}My programm is also quite simple as follows:\\n//\\tCreate Fix Joint\\nPxVec3 pos = PxVec3(0, 5, 3);\\t\\t\\t\\t//\\tposition of static actor\\nPxVec3 posDyn = PxVec3(0, 3, 0);\\t\\t//\\tposition of dynamic actor\\nPxVec3 offset = PxVec3(0,1,0);\\t\\t\\t//\\tOffset of connected actor from joint\\n//creating actors\\nPxRigidActor* staticActor = PxCreateStatic(*gPhysicsSDK, PxTransform(pos), PxSphereGeometry(0.5f), mMaterial);\\nPxRigidDynamic connectedActor = PxCreateDynamic(*gPhysicsSDK, PxTransform(posDyn), PxBoxGeometry(0.5,0.5,2), *mMaterial, 1.0f);\\n//adding actors to scene\\ngScene->addActor(*staticActor);\\ngScene->addActor(connectedActor);\\nPxDistanceJoint distanceJoint = PxDistanceJointCreate(*gPhysicsSDK, staticActor, PxTransform(offset), connectedActor, PxTransform(-offset));Would anybody help me about this problem?ThanksAre there any help?Are there any help?Sorry for the delay, I will try to reproduce the problem and pass it to the engineer responsible for PVD.\\n-MikeI have the exact same problem, using unconnected boxes is fine but as soon as I use any type of joint it gives the same assertion error.using vs2013+ physx 3.3.2 + pvd 2.0100.09.14356I have the same problem, simple scene works (also with pvd), but when I use joint app crash (assertion failed, PvdDataStream.cpp (504) ), without pvd app works normalvs2010, physx 3.3.2, pvd 2.0100.09.14356, my app is 32bitJust tested right now - 3.3.0 work without problem even with PVDI have the same assertion fault with physx 3.3.2 VS2010, and PVD.Addition of joints is also the cause of this error message.Running on windows 7 64Bits and 32Gb of RAM if it is any help.I am also wondering if it affects the profiling tools of PVD? Because I can’t seem to get the thread usage to move even with a 5000 cube stress test.Installing 3.3.0 to test.Any news?I’ve the same issue on Windows10, latest PhysX 3.3.3 and PVD from GIT, Visual Studio 2012, application compiled with x64 debugEDIT: it seems that clicking few times “ignore” on assertion dialog boxes enable to run the program. A problematic joint exists in scene, but is not visualized in PVD.Same issue. Running windows 10, physx 3.3.3 and PVD. Seems when I step into the create joint function it leads me to PvdConnection.cpp which fails during the createInstance function during this checkOption cls( mMetaData.findClass( clsName ) );\\nif ( cls.hasValue() == false ) return false;hasValue returns false.Same issue : PxDistanceJointCreate, Windows 10, physx 3.3.3. VS2015 x64Debug, PVD 3.2016.04.20614672, also with PVD 2.0100.09.14356\\n Debug Assertion Failed in PvdDataStream.cpp Line: 487No problem with PxDistanceJointCreate if PVD not runing.Any help ?Solution is to use PxInitExtensions right after PxCreatePhysics:Forgot to initializeThank You,\\nIlyaPowered by Discourse, best viewed with JavaScript enabled',\n",
       " 'the-rendered-scene-is-again-shown-in-optix-6-0': 'I use OpenGL to create a window interface for an Optix program, the purpose is to achieve selective display between multiple scenes。When I close my first scene through the button and open the second scene, my second scene screen shows white 。I found that my second scene was also running by calculating the rendering time, but it was not displayed on the interface I drawn。\\nWhen I close the first scene via the button, I also log out the buffer, context, and ray_gen_program.\\nWhen I open the second scene, I will recreate the subform I drew，But the second scene shows nothing in the subform。\\nHow should i solve this problem？That is really not enough information to say what’s going on.Sounds like something is not correctly initialized with GLUT, OpenGL or OptiX or the interoperability between OptiX and OpenGL to transfer the rendered image.If you completely destroy all OptiX resources and the GLUT window used for OpenGL display, then the second time should just do the exact same initialization steps again. Note that the OpenGL pixelformat can only be set once per lifetime of a window though.Have you checked all OpenGL calls for errors with glGetError()?\\nDo you have an exception program running in your OptiX pipeline to see if the renderer worked?You should check where the white image comes from.\\nIf you clear the screen, change the clear color to some debug color to see if that is actually from the background or the texture blit.\\nIf you fill in data to the OpenGL texture  manually does the OpenGL display itself work?\\nCan you look at the rendered OptiX image manually to see if that worked?\\n(Map the output buffer, check what’s inside, for example in a memory window of the debugger, unmap the buffer)\\nMaybe let the ray generation program fill in a single color for easier debugging.\\nIf either doesn’t work then that is your first problem.\\nIf both work, then the mechanism you use to transfer the image to the OpenGL texture would be the next thing to debug.thank you for your reply。\\nI created a main window and created its child windows in the main window。The results drawn by Optix are displayed in this sub-window。This is the child window I drew。I create a button in the main window and give feedback through the glutMouseFunc function。This is the main window code that I drewvoid glutInitialize()\\n{\\nglutInitDisplayMode(GLUT_RGB | GLUT_DEPTH);\\nglutInitWindowSize(1080, 720);\\nglutInitWindowPosition(100, 50);\\nMainWindow=glutCreateWindow(“Main Window”);\\nglClearColor(1.0f, 0.0f, 0.0f, 1.0f);\\nglutDisplayFunc(display);\\nglutMouseFunc(mouseFunc);\\ngluOrtho2D(0, 1080, 0, 720);\\nglutMainLoop();\\n}The function of the button is implemented in the mouseFunc function。This is the code of mouseFunc functionvoid mouseFunc(int button,int state,int x,int y)\\n{This is the code of huizong  functionvoid huizong()\\n{\\nif (texture_path.empty())\\n{\\ntexture_path = std::string(sutil::samplesDir()) + “/data”;\\n}\\n#ifndef APPLE\\nglewInit();\\n#endif\\nstd::string out_file;\\n// load the ptx source associated with tutorial number\\ntutorial_ptx = “D:\\\\C#\\\\OptixTest1\\\\x64\\\\Debug\\\\tutorial0.cu.ptx”;\\ncreateContext();\\ncreateGeometry2();\\nsetupCamera();\\nsetupLights();\\ncontext->validate();\\nif (out_file.empty())\\n{\\nglutRun();\\n}\\nelse\\n{\\nupdateCamera();\\ncontext->launch(0, width, height);\\nsutil::displayBufferPPM(out_file.c_str(), getOutputBuffer());\\ndestroyContext();\\t\\n}\\n}This is the code of createGeometry2 function。Judge the objects drawn in each scene through sceneTextvoid createGeometry2()\\n{This is the code of destroyContext functionvoid destroyContext()\\n{\\nif (context)\\n{\\ncontext->destroy();\\ncontext = 0;\\t\\n}\\n}Most of the code of the program is rewritten on the basis of optixTutorial in the Optix6.0 sampleDisplaying the result of Optix drawing on OpenGL is achieved by the following functionvoid glutDisplay()\\n{\\nif (context)\\n{\\nupdateCamera();\\ncontext->launch(0, width, height);\\nsutil::displayBufferGL(buffer);\\n{\\nstatic unsigned frame_count = 0;\\nsutil::displayFps(frame_count++);\\n}\\nglutSwapBuffers();\\n}\\t\\n}glutDIsplay is called through the glutRun functionvoid glutRun()\\n{\\n// Initialize GL state\\nglMatrixMode(GL_PROJECTION);\\nglLoadIdentity();\\nglOrtho(0, 1, 0, 1, -1, 1);\\nglMatrixMode(GL_MODELVIEW);\\nglLoadIdentity();\\nglViewport(0, 0, width, height);\\nglutShowWindow();\\nglutReshapeWindow(width, height);\\n// register glut callbacks\\nglutDisplayFunc(glutDisplay);\\nglutIdleFunc(glutDisplay);\\nglutReshapeFunc(glutResize);\\nglutKeyboardFunc(glutKeyboardPress);\\nglutMouseFunc(glutMousePress);\\nglutMotionFunc(glutMouseMotion);\\nregisterExitHandler();\\n}The glutDIsplay 、 glutRun and destroyContext functions are included in the sample。I have created a single color to be displayed on the screen with rays. When I click it at the beginning, the color I set will appear. When I click the logout button and then click the generate button, the screen is still white.Please don’t post incomplete code excerpts as text. The forum allows attaching files with the respective full source code which would make understanding what else you did easier.As far as I know, each GLUT sub-window holds its own OpenGL context. You assigned a different display function to the sub-window with glutDisplayFunc(display2) without showing what that does.I would not expect that the sutil::displayBufferGL(buffer) helper function knows about different OpenGL contexts.\\nDepending on when and how you created the buffer you want to display, that might not have the correct OpenGL resources assigned, but from another OpenGL context and then textures will be incomplete and not show anything but white with the default color and default modulate mode for textures.So far this looks like you didn’t manage OpenGL correctly.thank you for your reply。\\ntest1.cpp (18.7 KB)\\nsorry，I wanted to upload my entire source program, but I can’t upload it. I can upload my .cpp file. Can you help me find the errors in it?You’re creating the OptiX output buffer with an OpenGL pixel buffer object as backing store (OpenGL interop with OptiX, rendering directly into the PBO) because use_pbo = true.If it works when setting that to false (then the buffer will be copied through the host which is slower) then something is wrong with the OpenGL context handling.That is all yours to figure out then. Maybe download the freeglut sources and build it yourself, then step through all your GLUT code inside the debugger. Put a breakpoint in wglMakeCurrent to see if the same OpenGL context is current when creating the buffer and displaying it. If not, that’s the issue.I’ve not seen GLUT example code calling glutInitDisplayMode() more than once. That defines the OpenGL pixel format and I wouldn’t change that per sub-window. Just use glutInitDisplayMode(GLUT_RGB | GLUT_ALPHA | GLUT_DEPTH | GLUT_DOUBLE) once in main().There wouldn’t actually be a need to destroy the OptiX context when just switching scenes. It would be enough to destroy everything else except for the context and then just rebuild the render graph again.If it’s not the OpenGL context mismatch then I can’t say from that source what else could have gone wrong.Thank you for your reply.I tested my program,I set use_pbo=false,I found it to work.But I set use_pbo=false in the sample optixTutorial, and found that it can also run normally.I do not use the windows API, so I cannot use wglMakeCurrent to test。No context is set for OpenGL, the context used by the program is created by Optix::Context。The configuration I use is：window 10。VS2019。Optix 6.0。Most of the functions I use are freeglut. The buttons are formed by drawing rectangles.Yes, I have seen what your program does.What I said is that if you look into or debug through the freeglut sources, you will find that it is calling wglMakeCurrent internally when using sub-windows. All callback functions per window need to set the current OpenGL context when entering them.Again, if this works when disabling OpenGL interop via the pixel-buffer-object, then the failing case most likely created the OptiX output buffer with a PBO in a different OpenGL context as is active when trying to copy it to the final display texture. Means the names of the these OpenGL objects do not exist in the same context and result in no data transfer and maybe even an OpenGL error.This is not an OptiX issue then. I can’t help with this further. I’m not using GLUT or OptiX 6 anymore.The freeglut 3.2.1 sources contain a demo dedicated to sub-windows. Maybe that helps getting the right structure into your application.BTW, when using OptiX versions below 7, please at least update to the newest available version. I wouldn’t use OptiX 6.0.0 for new developments when there is OptiX 6.5.0 available, or even more future proof, switch to OptiX 7 which uses a completely different and more modern explicit API where resources are managed with native CUDA code.\\nThe SDK examples and other OptiX 7 examples found on this forum inside the sticky posts use GLFW as windowing framework.\\nAlways read the OptiX Release Notes for each version before setting up a development environment. It describes the required minimum display drivers and compatible CUDA toolkits.Thank  you for your reply.\\nIf I can’t debug the old version, I try to create it with the new version。\\nThank you for your help。Powered by Discourse, best viewed with JavaScript enabled',\n",
       " 'optix-4-0-0-bug-assertion-failed-verifyfunction': 'Glad to see the new version! However, I get an error as soon as I try to launch a kernel.The following appears in my command line:After which rtContextGetErrorString produces this message:I’ve sent an OptiX API capture to the help email account.Windows 7, Quadro K4000, driver 362.13, OptiX 4.0.0 beta 1, CUDA Toolkit 7.5.Hi nljones,thanks for testing the beta, and thanks for the bug report! :)\\nWe believe we have a fix which will be included in the next release.Thanks,\\nRalfPowered by Discourse, best viewed with JavaScript enabled',\n",
       " 'wgl-and-nvidia-card': '3Please check if you’re actually running your OpenGL code on the NVIDIA implementation and not on the Intel one. (E.g. glGetString(GL_VENDOR))Might be that P-Buffers are not supported on that implementation, which should have been found earlier when querying for the P-Buffer extension.\\n(P-Buffers should be considered legacy code and replaced with Framebuffer-Objects when possible.)Related post: [url]opengl - glGetString(GL_VENDOR) on hybrid graphics machines - Stack OverflowPowered by Discourse, best viewed with JavaScript enabled',\n",
       " 'optix-6-0-rtx-acceleration-is-supported-on-maxwell-and-newer-gpus': 'The release notes for OptiX 6.0.0 reads as follows:“RTX acceleration is supported on Maxwell and newer GPUs but require Turing GPUs for RT Core acceleration”I’m not sure I understand the part about “Maxwell and newer GPUs”. I thought RTX acceleration wasn’t available on GPUs older than Turing.Yeah, that’s formulated confusingly.What it means is the new “RTX execution strategy” in OptiX, which is just a name for the new core OptiX 6.0.0 implementation. It’s in contrast to the “mega-kernel execution strategy” used so far in all previous OptiX versions.That new RTX execution strategy allows to use the RT cores inside the Turing RTX GPUs for BVH traversal and triangle intersection.\\nOn GPUs without RT cores the bulk of that new code path will still be used for shader compilation, acceleration structure builds, multi-GPU support, etc. but the BVH traversal and triangle intersection routines run on the streaming multi-processors as before.To invoke the hardware triangle intersection routine or a fast built-in routine on boards without RT cores, you’d need to change your OptiX application to use the new GeometryTriangles and attribute programs.\\nThe GeometryTriangles neither have a bounding box nor an intersection program, but they have a new attribute program which allows to fill in your developer defined attribute variables, so that you can use the same any hit and closest hit programs from custom primitives in Geometry nodes as well as from GeometryTriangles.Unfortunately the OptiX Programming Guide is slightly behind on explaining that. The online version hopefully gets updated accordingly soon.\\n[url]http://raytracing-docs.nvidia.com/optix/index.html[/url]\\nThe OptiX API Reference document and of course the headers contain the necessary function explanations and the optixGeometryTriangles example demonstrates the new GeometryTriangles and attribute program usage.Thanks for explaining, Detlef! Just to make sure, I’m doing it as follows:a) Is it correct to do it right after instancing the context, like I did?\\nb) I suppose turning the RTX mode on is recommended whenever possible, correct?\\nc) When using the C++ interface, is there any other way to turn it on besides using the rtGlobalSetAttribute function? I tried to take a look at the programming guide from the SDK folder but I didn’t find anything.\\nd) Since you mentioned the new GeometryTriangles, is it supposed to be faster than, say, the old sutil/triangle_mesh.cu approach?\\ne) Are there any plans to add support to Selectors and visitor programs in the RTX mode in the future?a) No, you should set RT_GLOBAL_ATTRIBUTE_ENABLE_RTX global attribute before calling optix::Context::create();\\n(Also note that there is a new stack size API which allows to set the maximum number of recursions you expect instead of the old byte based stack size which isn’t used anymore then.)b) Yes, that RTX execution strategy should become the default and is also faster for boards without RT cores in many areas.\\nEDIT: There is no need to enable the RTX execution strategy in newer drivers anymore.c) No, since rtGlobalSetAttribute is a function which is not tied to an OptiX object, there is no C++ wrapper for that.\\nSimilar to all rtDevice* functions which you can call before creating a context. Example code here:\\n[url]https://github.com/nvpro-samples/optix_advanced_samples/blob/master/src/optixIntroduction/optixIntro_07/src/Application.cpp#L265[/url]d) Yes, and you must use the new GeometryTriangles to invoke the Turing RT core triangle intersection hardware.\\nGPUs without RT cores can become faster that way as well, because evaluation of the attributes in the attribute program will happen deferred, where it might not do that inside the intersection program(s) and that is the most often called program for custom geometric primitives.e) No, that would affect traversal performance too much. It always did but with hardware traversal that’s uncanny. The workaround is to change the scene hierarchy and rebuild, which also got faster. Also there are a few ray masks which allow to do some visibility and culling methods.d) Yes, and you must use the new GeometryTriangles to invoke the Turing RT core triangle intersection hardware.\\nGPUs without RT cores can become faster that way as well, because evaluation of the attributes in the attribute program will happen deferred, where it might not do that inside the intersection program(s) and that is the most often called program for custom geometric primitives.Regarding deferred evaluation of the attributes. Does that mean that for GPUs without RT cores, the performance boost can only manifest if the processed geometry does not have an “any-hit” program associated with it?To my understanding, in “mega-kernel execution strategy”, attribute evaluation takes place for every successfully intersected triangle. Then if the geometry has an “any-hit” program associated with it, the performed computations are potentially not wasteful. Similarly, in the RTX execution strategy attribute evaluation will take place for every intersected triangle, provided it has an “any-hit” program associated with it. As a result, the performance is the same for both strategies.Conversely,  if there is no “any-hit” program, then in “mega-kernel execution strategy”, attribute evaluation is performed for naught. Whereas RTX strategy will only perform one necessary attribute evaluation, i.e. for the “closest-hit” program. As a result, the RTX strategy shows superior performance.Is that understanding correct? Please correct me if I am wrong, much appreciated.The attribute program will be used inside the anyhit program as well.\\nCalculations of attributes which are not sourced will be removed as dead code.\\nThis is the same for RTX and pre-Turing GPUs.“To my understanding, in “mega-kernel execution strategy”, attribute evaluation takes place for every successfully intersected triangle”Only if OptiX is not able to optimize that automatically, which it did in the past, but not for all cases. That’s why I said it might not always do that\\nIt always will now with the attribute program in the RTX execution strategy, which is the default now.Having an anyhit program will generally run slower than not having one. This is especially true for RTX boards where the traversal and triangle intersection runs in hardware on the RT cores but anyhit programs will run on the streaming multiprocessor.\\nYou’ll need the attributes latest in the closesthit program.There are even more possibilities now. For scenes with only opaque materials (means no cutout opacity) there isn’t actually an anyhit program needed for either the radiance or the shadow ray in OptiX 6 anymore, because that introduced ray flags which are hardware accelerated on RTX GPUs and result in less execution divergence overall. The ray flags allow, for example, to terminate the ray on first hit, which is exactly what an anyhit program for the shadow ray of opaque materials does.\\nThis is just one example how things can be sped up in OptiX 6 for special cases.Please have a look a the GTC Presentation S9768 - New Features in OptiX 6.0 once it becomes available (30 days after the GTC 2019) on:\\n[url]https://on-demand-gtc.gputechconf.com/gtcnew/on-demand-gtc.php[/url]\\nIt shows the execution flow for these things.Powered by Discourse, best viewed with JavaScript enabled',\n",
       " 'varying-variables-in-a-struct': 'I am currently adding layout locations for all my varying variables between shader stages.\\nThe first thing I noticed was a noticeable drop in performance compared to not using layout locations.\\nWhen checking the assembly it looked like layout locations generated more\\nATTRIB fragment_attrib = { fragment.attrib[0…7] };\\ncompared to not using it.\\nThis is the only real difference I could find.Since then I switched everything to an input and output struct between stages and performance improved again.\\nThere is one more thing that looks odd to me though.The following struct uses 8 locations in the fragment stage, when it would perfectly fit into 5 locations for interpolation.So my question is, are varyings not packed when used with layout locations? Do I have to manually pack everything into vec4s and unpack it again in the next stage to maximize performance?Powered by Discourse, best viewed with JavaScript enabled',\n",
       " 'about-the-windows-server-application': 'I downloaded the package 2021-04-28-CloudXR-SDK.zip, in which I found many client-side examples for Windows, Android … but on the server side I didn’t find any application example that shows how to use Cloud XR SDK for streaming to the client.\\nCan anyone help me?Powered by Discourse, best viewed with JavaScript enabled',\n",
       " 'unity-flex-collision-or-trigger-report': 'Hi, I’ve been struggling with this issue for too much time and I couldn’t find a solution… Is there any way to report collision or trigger detection between Flex actors and Unity colliders??many thanks.Powered by Discourse, best viewed with JavaScript enabled',\n",
       " 'additional-trackers': 'In addition to hand trackers, SteamVR supports additional trackers, for example, for full body tracking. Is it possible to transfer tracker position data to CloudXR except for two controllers so that SteamVR starts to recognize them? Or maybe you can pass four controllers instead of two?We don’t have support for additional trackers, but it’s a great topic for exploration.  Thanks.Powered by Discourse, best viewed with JavaScript enabled',\n",
       " 'ship-optix-7-1-0-application-as-exe': 'Hi. I want to ship .exe format application including optix7.1.\\nI made a distinction between the case when the cu file exists and the case when it exists. If you have a cu file, compile it with nvrtc and save ptx. If there is no cu file, read ptx directly. This flow looks normal. I want to know if there are any serious mistakes that I haven’t noticed.Thanks for your help.Hi @tommyecguitar, welcome!Your workflow sounds normal to me too. If there is always a backup ptx file, it should always work. For users that have a cu file, will they need to install the CUDA toolkit to get nvrtc?–\\nDavid.@dhartThank you for your comment.\\nI’m going to output the code as it is now.\\nYou’re right, if users have only cu file, they need the cuda toolkit, but I’m not assuming that ptx is not present in the output application.Thank you.Powered by Discourse, best viewed with JavaScript enabled',\n",
       " 'samplerycbcrconversion-feature-not-working-as-expected': 'Hello;I am developing a video manipulation related application and I’ve encountered a problem when dealing with Vulkan’s SamplerYcbcrConversion feature. The problem is that the sampled texture has only the red component. This problem only occurs on NVidia hardware, so I suspect that it is a driver-related issue. Moreover, validation layers do not complain about anything and I have meticulously followed the Vulkan spec regarding the feature. I am running Debian 10 with the latest drivers and a GTX 980,I do not know how to retrieve logs related to the Vulkan execution of the driver, so if anyone teaches me how to do so, I’ll attach any related error logs. As I have mentioned earlier, VK_LAYER_KHRONOS_validation does not complain, so I have nothing to attach.Moreover, if you know any other application that makes use of this extension, I’ll test it to check if the error is specific to my application. This blog post mentions that MPV uses it, but I have not found evidence of it on the source code of MPV nor its underlying lib’s code.Thank youPowered by Discourse, best viewed with JavaScript enabled',\n",
       " 'what-gpu-is-on-my-geforce-gt-710-board': 'I want to do some GPU code development and I happen to have a GT 710 on my computer.\\nHowever, I cannot find anything online about which GPU chip is on it, or what architecture it is, or other specs.\\nWhat I do know is that it has 3.5 compute capability, and it has 192 CUDA cores, and it has a base clock of 135 MHz and a boost clock of 954 MHz.  The GPU Caps Viewer app tells me nothing more.\\nI believe it is a Kepler architecture, but I’m not sure about that, because when I read a spec for Kepler, it mentions having 15 SMX units, whereas mine only has one (I read the %nsmid register to verify this).Can anyone tell me which GPU device I have, and where I can find the specs for it?\\nI’d like to know the details for low level programming, such as how many registers in the register file, size of shared memory, number of thread blocks that can be active concurrently, etc.\\nThanks.Your GT 710 GPU uses a GK208 chip.Thanks.  By the way, how did you find that out, and could I have found the info myself somehow?Actually, never mind.  I just followed the link in your answer, over to Wikipedia.\\nNOTE to NVIDIA folks:\\nI’d like to say that it would be really nice if the NVIDIA product pages would provide this information.  Such as a comparison of different GEForce cards, or more details about the GT 710 in particular.@michaelrrolle45,Is this what you are looking for?\\n[url]https://developer.nvidia.com/video-encode-decode-gpu-support-matrix[/url]Powered by Discourse, best viewed with JavaScript enabled',\n",
       " 'ue4-flex-multiplayer-is-broken': 'Does Flex not work with multiplayer? I am using Flex 4.16 and none of the demo levels work in the sample flex project on multiplayer. I’ve tested listen and dedicated servers. Since Killing Floor 2 used Flex, there must be a way to get it to work. Any ideas?To be clear, I mean that the game will instantly freeze and become unresponsive when run with 2 players.Powered by Discourse, best viewed with JavaScript enabled',\n",
       " 'cloudxr-android-sdk-on-vive-focus-3': 'Hi everyone,Was playing today with the SDK on Vive Focus 3, manage to run it correctly, but there is a resolution problem (obvious because it is made to run on Focus Plus), you can see the controllers, the SteamVR environment, but with big eye problems :-)Wanted to know where/how I can tweak things to update the code to work on the Vive Focus 3Thanks !Can you send a picture of the problem, along with client and server logs from the run?We’re working with HTC and have not heard of any issues from them on the F3, and given we work on so many different headsets, I’m admittedly at a loss to understand what you are seeing that is wrong.Hi,Thanks for the answer. I will send the logs and screens. But I was worried it wasn’t yet supported, so it is good news ! I followed the guidelines explained there : Wave VR Client (Android) — NVIDIA CloudXR SDK documentationJust a bug in the beginning with the permissions but then it launches.Hi, tried again.First of all, following the “Wave VR Client (Android)” step by step guide, I think the “wave_3.2.0_native_1.zip” reference is obsolete, because this SDK is too old, right ?\\nI’ve tried with this version of the SDK (wave_3.2.0), and it builds ok, run on the focus 3, and give this kind of view if turning my head a bit :\\nimage1920×960 39.6 KB\\nIt is as if the left and right eye where not recalculate when moving the head, still looking in the same direction, and placed behind my physical head.I’ve tried with the Native SDK “wave_4.1.0_native” and does the same.I’m probably missing something somewhere, but don’t know what.In attachment, the server log\\nlogs.zip (37.1 KB)And the VF3 client log\\nclient logs.zip (21.0 KB)Hope it will be useful for youThanks !I’m hoping to get a Focus 3 soon, which will help to investigate this more directly.  But sounds strange that the pose isn’t updating.The server logs I’d need to see are the CXR server logs.  The steamvr logs won’t tell me anything.  However, the client streamer log is sufficient to show you have an issue:So there’s a network connectivity issue here, could be causing problems (I’m not sure which channel that is…).  If you can try without any firewalls involved, that would help isolate.Hi, thanks for the answer.\\nAttached the logs you’ve asked (the one in the appdata folder on the server).CloudXR Server - SteamVR Log 2021-08-04 17.11.00.txt (370.2 KB)\\nStreamer Server Log 2021-08-04 17.11.00.txt (202.5 KB)About the firewall, there is nothing between the headset and the wifi AP and the CloudXR host. Also, it works perfectly with an Oculus Quest 2 and Quest 1, problem is only with Focus 3 (Oculus apk has also been built by myself, I didn’t use the prebuild apk).I’m also wondering if the “Building Wave VR Client” section in the documentation is suitable for Focus 3. Indeed, the doc refer to the wave_3.2.0_native_1.zip SDK from HTC, but is this relevant for the Focus 3 ?\\nIf I try to build using the latest SDK (wave_4.1.0_native), I have compilation errors (some functions has changed names)Thanks for your help !Best regards,NicolasSorry for the delay.  I still haven’t had cycles to investigate, but do have a Focus 3 finally, so will be able to look at this directly at some point.  I’ll let you know when I do.Server logs don’t indicate anything further, but the client logs again were indicating some kind of issue.  Again, won’t know further until I can get around to looking into this on F3 directly.Thanks for confirming.  I’ve implemented the proper check, so this should be addressed in the next release out of the box.Hi there I’ve got the same problem with the Focus3 and current CloudXR SDK 3.0, got it running by fixing the permissions in WaveXR client but the viewing frustums are not correct - if it works for you could you please give me a hint where to do the tweaking?\\nThanks a bunch :-)find the line:\\nheadMatrix = WVR_Inverse(WVR_Convert(framePose));\\nand remove the WVR_Inverse.Note that’s only one potential mod you’ll need to make – the controller mapping is another big piece.  I’ve been working on full F3 support, but ran into a hiccup that’s delayed things a bit.I’ll try, thank you Dave !Hi everybody !\\nIt does now fix the problem, however, I think the FOV is not yet ok.\\nIt seems that everything is bigger, like magnified :-)\\n(the Vive controller is enormous)Is it normal ?Thanks !ok great, so rendering seems good now from the frustums with this fix :-)there are still a few issues though - controller mapping not there yet as you said, also when i’m using it with Omniverse KIT (Tablet XR extension, is there a better way?)  the overall framerate is very low, like 5Hz maybe and its not rendering on full resolution (quarter resolution?)nevertheless thank you for your efforts Dave, much appreciated :-DHi Dave,\\nI also started hooking up my Vive Focus 3 with my CloudXR Server.\\nReading this post it seems like Wave Client is not ready yet supporting Vive Focus 3.\\nSo I like to ask:F3 support should be in the next point release, but I don’t have any timeline yet.  I’ll have to ask around whether we might be able to post just a sample update at some point.There have been some discussions around github, but nothing definitive.Great, thanks Dave!Just to close out this thread, the 3.1 release prioritizes Focus 3 and deprecates Focus Plus.  Should be pretty close parity to the quest 2, though we’ve noticed the Focus has wifi quality/perf issues that quest does not.Let us know if 3.1 is working for all of you!I am seeing the 3.1 release of the headset XR app for the Vive Focus 3 not being able to find the /sdcard/CloudXRLaunchOptions.txt file when the app is ran:12-19 20:06:27.845  6480  6536 E CloudXRWaveNative: Fatal error: No server IP specified to connect to.\\n12-19 20:06:27.845  6480  6536 E CloudXRWaveNative: Unable to initialize CloudXR!After pushing with this command:C:\\\\Users$env:username\\\\AppData\\\\Local\\\\Android\\\\Sdk\\\\platform-tools\\\\adb.exe push C:\\\\Users$env:username\\\\Desktop\\\\CloudXRLaunchOptions.txt /sdcard/CloudXRLaunchOptions.txtI validated the file is indeed in the proper place on the headset:PS C:\\\\Windows\\\\system32> C:\\\\Users$env:username\\\\AppData\\\\Local\\\\Android\\\\Sdk\\\\platform-tools\\\\adb.exe shell\\nkona:/ $ ls\\nls: ./init.zygote64_32.rc: Permission denied\\nls: ./init.rc: Permission denied\\nls: ./init.usb.rc: Permission denied\\nls: ./ueventd.rc: Permission denied\\nls: ./init.zygote32.rc: Permission denied\\nls: ./init: Permission denied\\nls: ./cache: Permission denied\\nls: ./init.environ.rc: Permission denied\\nls: ./init.recovery.qcom.rc: Permission denied\\nls: ./postinstall: Permission denied\\nls: ./init.usb.configfs.rc: Permission denied\\nls: ./init.htc.storage.exfat.rc: Permission denied\\nls: ./metadata: Permission denied\\nacct bin        charger d    debug_ramdisk dev lost+found odm proc    product_services sbin   sdcard2 sys    usbdisk2\\napex bugreports config  data default.prop  etc mnt        oem product res              sdcard storage system vendor\\n1|kona:/ $ ls sdcard\\nAlarms Android CloudXRLaunchOptions.txt DCIM Download Movies Music Notifications Pictures Podcasts RingtonesThe same .txt file works on the Oculus Quest; any ideas as to where this file should go?make sure that your application has storage permission.  that’s the usual cause for that kind of issue.Powered by Discourse, best viewed with JavaScript enabled',\n",
       " 'nvidia-optix-7-error': 'Hii compile Otix_Apps example and whene i run example it show this error0:1(10): error: GLSL 3.30 is not supported. Supported versions are: 1.10, 1.20, 1.30, and 1.00 ESPowered by Discourse, best viewed with JavaScript enabled',\n",
       " 'can-i-have-two-separated-scenes': 'Hi,\\nWe are investigating if we can use PhysX for our project. We need to do simulations in two completely separated worlds (scenes). Each will have its own acotors and its own simulation tick rate. So can we do that using PhysX?\\nAnd if so, will it be possible to use the GPU acceleration for only one of them?Hope someone can answer these questions. Thx.Hello,\\nyou can by using PxPhysics::createScene multiple times. It takes a scene descriptor object, that determines if you want to use CPU or GPU and other solver.https://gameworksdocs.nvidia.com/PhysX/4.1/documentation/physxapi/files/classPxPhysics.htmlPowered by Discourse, best viewed with JavaScript enabled',\n",
       " 'optix-5-1-unkown-error-with-sample-code': 'Hello,\\nI am absolutely new to the Optix framework. I downloaded the latest version 5.1 and used cmake to configure and make compile as mentioned in INSTALL-LINUX.txt (I am using Ubuntu 16.04). Then went on to run some sample executables in  /build/bin, for all of them I am getting same error:If I runthe output is:I have seen similar errors in this forum, most of them complain that they are not able to run sample codes, but the function which throwing the error is different from mine and none of them have a solution.Thanks :)My bad the default driver used for display way not nvidia driver, hence the error.Powered by Discourse, best viewed with JavaScript enabled',\n",
       " 'bilinear-warping-with-nvapi-warp-blend': 'I’ve written a program that calls the warp and blend methods in the NVAPI API, and have been using it successfully to warp and blend projectors on planar surfaces, using quadrilateral ‘perspective’ warping, via NV_GPU_WARPING_VERTICE_FORMAT_TRIANGLESTRIP_XYUVRQ.While these transformations are suitable for most projectors, it looks like a quadrilateral transformation is not sufficient for cases where the projectors shift their principal points (e.g., with tilt-shift lenses).  In these cases, it seems that the lens distortion from the tilt-shift optics makes it impossible to successfully warp projectors into alignment.Being able to apply an arbitrary lattice of warp points could help – e.g., something akin to:\\n[url]https://github.com/paulhoux/Cinder-Warping[/url].That is, instead of use (4) vertices defining the corners of a quadrilateral to define a warp, I’d like to use a grid of perhaps [8x4] vertices.Is there support for this kind of warping and where might I find it?It seems there are additional API methods that require licensing; is this true and is there any place to get more information?Thanks,-KevinIn my post above I’d asked about any differences between public NVAPI and special licensing, presumable with individual ISVs.  I’ve not sure there is a private license for NVAPI, but reading between the lines in the NVAPI documentation seems to suggest there in.Is there any definitive word on public v. licensed NVAPI methods?have u solved this? I am also confused by this.Hi,\\ni’m also trying to define a warp grid with NxM vertices, but i’ve the problem of quad joints described also here:Pixels and polygons and shaders, oh my!\\nIn this article it’s solved with a vertex + pixel shader, how can i use it in my warping Nvidia API ?Thank you.Same as the question.Powered by Discourse, best viewed with JavaScript enabled',\n",
       " 'problem-with-charactor-controller': 'I start to use PhysX one month ago and now I meet a problem with the behaviorCallback of charactor controller.The PhysX version is 3.3.3I create two CCTs, a Capsule and a box, with default desc. And then I reset some property like this:There is moving kinematic box actor in the scene just like SampleBrighe. Its height is 2.0f.Two CCTs can do right thing when I return PxControllerBehaviorFlag::eCCT_CAN_RIDE_ON_OBJECT.The capsule controller can do right thing ( I set its position on the box ) when I return PxControllerBehaviorFlag::eCCT_CAN_RIDE_ON_OBJECT | PxControllerBehaviorFlag::eCCT_SLIDE. But it can not climb on the box again after it falls from the box any more. It shakes at the box edge but never climb on it.Do you know why the capsule controller do something wrong and how to fix it?Thank you for your time!Powered by Discourse, best viewed with JavaScript enabled',\n",
       " 'ubuntu-20-04-nvidia-driver-version-460-xx-detect-hdmi-but-not-display': 'Dear Support\\nWe have a problem with the HDMI display on the Nvidia driver of the ubuntu 20.04 operating system, when I plug in the HDMI cable on the nvidia Quadro p4000 I found the driver but no show on TV Here Nvidia-Bug-report.lognvidia-bug-report.log.gz (137.7 KB)Powered by Discourse, best viewed with JavaScript enabled',\n",
       " 'video-encode-and-decode-gpu-support-matrix': 'Video Encode and Decode GPU Support Matrix | NVIDIA Developer is cool, but we still need https://developer.nvidia.com/video-encode-and-decode-gpu-support-matrix since it has information hat is practically impossible to find elsewhere for older generations like GK110B\\nPlease bring the old page back and place a link on top of it for new version instead of a redirect, or integrate this information into new page, don’t just throw it away.What about adding RTX 3060 and new low-profile RTX A2000? I’m especially waiting for the NVDEC/NVENCs countHi everyoneAccording to the support matrix, the NVIDIA T600 Laptop GPU has HEVC B Frame support.\\n2021-09-20_21-09-041276×148 41.5 KB\\nHowever, the TU117 chipset should not have support for B frames according to Wikipedia and the information I found (also here in the forum).Is the matrix incorrect or have I overlooked something? Thank you in advance.Dears,Can we know the maximum number of parallel encoding streams with resolution like 1080p30 per graphic card board?Like when we see Tesla M10 Tesla M10 GPU Accelerator | NVIDIA, it’s showing “28 H.264 1080p30 streams”, can we have the same for other graphic card board?Thanks.The support matrix has been updated today , please check it out:\\nhttps://developer.nvidia.com/video-encode-decode-gpu-support-matrixWhy ist there no column for H.265 (HEVC) 4:2:2? Is it included in support for 4:4:4?When will A10 and A16 be supported ?  @TomNVIDIAHello @Benutzer799 and welcome to the NVIDIA developer forums!All supported formats are listed in the matrix, so 4:2:2 is not nativley supported. Since there is considerable difference between 4:4:4 (uncompressed) and 4:2:2 you should not assume inclusion here.Hope that helps.Could the RTX 40 series GPUs and the L40 GPU be added to the matrix?Why the RTX 40 Ada Lovelace lineup lacks support for 4:2:2 HEVC formats? Please clarify, as to why we are expected to rely on Intel’s iGPU (Quicksync) for decoding these codecs when NVIDIA’s own dGPU is not equipped to do so. Failures to support the latest codecs from NVIDIA’s end force us to narrow our choices in the CPU department. How long do I have to continue buying outdated 10nm++++ Intel CPUs?Also, the 3080Ti Mobile GPU is incorrectly labelled as ‘GA106’ when it is actually GA103S, so may please amend the corrections.Hello,\\nIs this table up to date? Can you check this issue?\\nThank youI’m trying to better understand columns 5-7 of the decode matrix here. What’s the difference between “# OF CHIPS”, “# OF NVDEC/CHIP”, and “Total # OF NVDEC”?For example in the new RTX Ada/Quadro line, there are some that are 1-4-1, some are 1-2-2, etc. I can’t find the correlation beyond simply “more is better”.\\n\\nNVDEC1154×594 104 KB\\nHi there @bbuckley4444 and welcome to the NIVIDA developer forums!I thought I could answer this immediately, but the for example the “1 4 2” combo I cannot. Ireached out internally to get some clarification.Thanks!Hello again @bbuckley4444,there will shortly be an update to the support matrix tables which will be clearer.And for GeForce and Workstation cards it is safe to go with the “more is better” approach.Thanks!Thanks for the update @MarkusHoHo. Looks like that’s been changed now.I guess this turns into more of a product question now, but how does that look in practice for the total number of decode streams? Is it just double the number of streams if there are 2 vs 1 Total # of NVDEC? (my use case is ingesting 8+ 1080p60 capture card inputs in OBS).The interesting thing is they all say 1 chip now, including on the higher end Ada cards that are usually described as having dual encoder chips. So do the higher end Ada cards just have a single but larger encode/decode chip? I’ve messed around with the dual encoding on my 4090 but am looking at getting an RTX 4000 SFF for a portable rig.Please, could you add jetsons to this matrix?@bbuckley4444 If you choose a GPU with 2 NVDEC instances than, depending on available memory and bandwidth of course, you will theoretically be able to decode double the number of streams. In practice your mileage will vary of course due to other possible overhead.The “Chip” count means the number of physical GPU dies on the Board. Since no consumer cards exist that have more than one GPU die, you will not find anything but 1 there. Of course some server setups have 8 or 16 GPUs, while the old Tesla M10 had 4 for example.But same generation GPUs will also have same generation NVENC/NVDEC chips, no differences.@masip85 I don’t think that would make much sense since Jetson hardware uses different video technology than GPUs in this matrix. For video capabilities it is easier to check the detailed tech specs on the Jetson pages, for example Jetson Orin.I hope this helps.In nvidia specifications webpage , av1 is not present.But here , av1 is specified:NVIDIA Jetson Orin is the most powerful AI computer for energy-efficient autonomous machines. With up to 275 TOPS and up to 8X the performance of NVIDIA® Jetson AGX Xavier™.Here too:59.28 MBIs that correct?Here it works … but is it hardware accelerated?Yes, as far as I understand there is HW accelerated AV1 encoder with AGX Orin (only).\\nSee:\\nhttps://docs.nvidia.com/jetson/archives/r35.1/DeveloperGuide/text/SD/Multimedia/AcceleratedGstreamer.html#gstreamer-1-0-plugin-referenceYou can find this in the NVIDIA download center as well. For example searching for all Jetson Orin variants will list the data sheets which show that Orin Nano supports AV1 envode through software, while Orin NX lists it as supported by the SOC.Powered by Discourse, best viewed with JavaScript enabled',\n",
       " 'solved-rtcore-link-error-after-45-animation-frames': 'my application uses OptiX 6.0.0    CUDA 10.0  Win10PRO 64bit VS2017 (toolset v140 of VS2015) Driver 419.17   GTX 1050 2GBon using setGeometry(): everything (including animated mesh; geometry rebuild; mark dirty) works fine.on using setGeometryTriangles():  as long as the geometry is unaffected, the scene renders fine; but when the animation comes to\\nanimation frame 45 (45x times mesh replacement; destroy; mark dirty)  this exception occurs:OptiX Error: Unknown error (Details: Function “_rtContextLaunch2D” caught exception:\\nEncountered a rtcore error:\\nm_exports->rtcPipelineCreate( context, pipelineOptions, compileOptions, modules, moduleCount, pipepline )\\nreturned (8): Link error)In my code I used:\\ncontext->setMaxTraceDepth( 25 );\\ncontext->setMaxCallableProgramDepth( 25 );when changing to:\\ncontext->setMaxTraceDepth( 40 );\\ncontext->setMaxCallableProgramDepth( 40 );\\nthen this message occurs (before the first frame was rendered):\\n\\nOptiX Error: Unknown error (Details: Function “_rtContextLaunch2D” caught exception:\\nEncountered a rtcore error:\\nm_exports->rtcPipelineCreate( context, pipelineOptions, compileOptions, modules, moduleCount, pipepline )\\nreturned (1): Invalid value)I tried out value 10: then exception on animation frame 46. and link error 8\\nI tried out value 6: then exception on animation frame 46 and link error 8Any ideas what the root for this issue could be?\\nI think its no memory leak, cause all the geometry instances and the geometryTriangles are destroyed with the ->destroy() method and the GPU memory is not completely full.\\nEDIT: it was a memory leak.After setting Acceleration Properties Trbvh correctly, still the same exception occurs, but now on animation frame 51.\\nAlso removing the property settings does not help.UPDATE:\\nSo I tried to use “setGeometry()” instances only (no setGeometryTriangles).But after many more animation frames (not only 50) and many light moves, unfortunately the same exception now also occured there.\\nUnknown reason.I assume your’re still using my OptiX Introduction based path tracer architecture.\\nThat only has an rtTrace depth maximum of two. raygeneration shoots radiance ray, closest hit program shoots shadow ray, that’s it. This requires only setMaxTraceDepth(2);Then, are you even nesting calls to callable programs? If not setMaxCallableProgramDepth(1);I think the maximum value in both these trace depth calls is 31 in OptiX 6.0.0.If it still fails after that stack space reduction, a running reproducer in failing state would be required for analysis. As always, the samller, the better.thank you for your answer.yes, I use still the same architecture.   same exception at same frame on simple materials also on max trace depth 2 and max callable depth 1.I also tried to use a simple solid color in the closesthit.cu using RTX mode + GeometryTriangles:\\nthat works for 75 animated frames until the exception occurs.Then I switched off RTX mode (and so there’s also no usage of GeometryTriangles): Now the animation runs without problems; Also after many animation frames and many manually light moves no exception. The only difference is, that its much slower.I also did a test replacing the geometry within the optixGeometryTriangles SDK example on each frame. That works fine\\nAlso I  setup optixIntro_07 with OptiX 6.0.0 + CUDA 10.0 from your original Introduction Sample again… That also runs without any exception after I inserted the GeometryTriangles replacement handling for a simple sphere.So there seems to be some deeper issue in my app, which is obivoulsy tolerated by the megakernel strategy, but RTX mode doesn’t accept it.  I will try again.I also got very similar error message:\\nUnknown error (Details: Function “_rtContextLaunch2D” caught exception: Encountered a rtcore error: m_exports->rtcPipelineCreate( context, pipelineOptions, compileOptions, modules, moduleCount, pipeline ) returned (8): Link error)The only difference is the returned value, it is 8 instead of 1 here.\\nThe program never launches a kernel because it is a link error in my case.\\nThis error doesn’t occur if I disabled RTX mode.However the program works various input scenes except one scene. Failed scene includes alpha textures.\\nMy program adds any hit program when an input scene includes alpha textures, so I think that the error suggests OptiX fails to link the any hit program.\\nOn the other hand, however I’ve observed that the program works other scenes including alpha textures.\\nIt fails to link something for a specific scene.OptiX appears to cause some memory corruption or some inconsistency in some situtation if RTX mode is on.\\nOr I had a mistake in my program but it happen not to have the error.Thanks @shocker.0x15 for your message! I tested before with the opaque materials only.\\nSo this time I used the “cutout” material for the sphere (which has an “alpha”-similar handling)And on frame 248  (248x times replacement + destroy + mark dirty) the exception was thrown:…\\nReplaceGeometry ‘244’\\nReplaceGeometry ‘245’\\nReplaceGeometry ‘246’\\nReplaceGeometry ‘247’\\nReplaceGeometry ‘248’\\nUnknown error (Details: Function “_rtContextLaunch2D” caught exception: Encountered a rtcore error: m_exports->rtcPipelineCreate( context, pipelineOptions, compileOptions, modules, moduleCount, pipeline ) returned (8): Link error)\\nReplaceGeometry ‘249’\\nUnknown error (Details: Function “_rtContextLaunch2D” caught exception: Assertion failed: “!m_enteredFromAPI : Memory manager already entered from API”, file: , line: 1071)\\nReplaceGeometry ‘250’\\nUnknown error (Details: Function “_rtContextLaunch2D” caught exception: Assertion failed: “!m_enteredFromAPI : Memory manager already entered from API”, file: , line: 1071)\\n…I sent the source code (which can be directly plugged into a sample) for my test by private message to Detlef.Thanks to Detlefs help I was able to find the reason for the exception:\\nit was a memory leak.\\nI did clear the geoInst[ “XXXX_buffer”] variables on replacement, but there was no call to → destroy() on these buffers. Since they are properly destroyed now; not one time the exception happened again. So using RTX mode works fine now.Powered by Discourse, best viewed with JavaScript enabled',\n",
       " 'mdl-sdk-prebuilt-library': 'Hello,\\nCould someone please share how to use the pre-built MDL SDK in CLion or in Visual Studio? I am new to pre-built SDK’s and I’ve been facing some problems in integrating it in my project. There is no clear documentation on the process.\\nThanks in advanceThat should be pretty straightforward.\\nIt’s the same procedure for either the open-source or the pre-built MDL-SDK.All you need is to add your installation’s MDL-SDK\\\\include path to the Additional Include Directories of your project to be able to include the necessary headers inside your application’s source.Everything else you need is loaded dynamically at runtime, like the libmdl_sdk.dll, the nv_openimageio.dll or nv_freeimage.dll, dds.dll, shown inside the MDL SDK examples:\\nhttps://github.com/NVIDIA/MDL-SDK/blob/master/examples/mdl_sdk/shared/utils/mdl.h#L206\\nhttps://github.com/NVIDIA/MDL-SDK/blob/master/examples/mdl_sdk/shared/utils/mdl.h#L325\\nUsed here:\\nhttps://github.com/NVIDIA/MDL-SDK/blob/master/examples/mdl_sdk/shared/utils/mdl.h#L431Copy and adapt those functions to your application framework as needed.An initialization outside the MDL-SDK example framework can be found in one of my OptiX 7 demo applications using the PTX backend to generate material shader code for OptiX 7:\\nhttps://github.com/NVIDIA/OptiX_Apps/blob/master/apps/MDL_renderer/src/Raytracer.cpp#L1571https://raytracing-docs.nvidia.com/mdl/api/mi_neuray_getting_started.htmlshould have all the info.\\nBasically the result of the cmake process is a visual studio solution for the sdk examples. This can also serve as a blueprint for your own solutions.Powered by Discourse, best viewed with JavaScript enabled',\n",
       " 'nv-enc-err-unsupported-device-on-nvencopenencodesessionex': 'After updating driver to 398.36 on one of our test machines(gtx 970, maxwell), nvidia encoding is failed in nvEncOpenEncodeSessionEx with NV_ENC_ERR_UNSUPPORTED_DEVICE.Following code is what I am using for opening nvidia encoding session… Any idea?Hi hmGo,Could you provide the following information:Thanks,\\nRyan ParkHi Ryan,I found the reason…\\nNow I am using triple monitors and two of them were connecting to mother board, not nvidia card.\\nI could resolve it with connecting those monitors to nvidia card.\\nBut here is question… Why it causes this plen when some of monitors are connected to mother board?Which device type are you using here? Is it DX or CUDA?For a moment, assuming that you are using DX device here, it seems that you may be passing a device created on a non-NVIDIA adapter to nvEncOpenEncodeSessionEx. If the device is not created a proper adapter then nvEncOpenEncodeSessionEx API is expected to fail.What we think/guess could be happening here is when you are connecting all your displays to NVIDIA card, then the adapter being enumerated is NVIDIA’s and device is created on NV GPU and hence nvEncOpenEncodeSessionEx started to pass.Thanks,\\nRyan ParkYes I am using DX device…\\nIf so, there is no way to create the device on Nvidia adapter by force no matter what monitors are connected to mother board and graphics card together?\\nI’m also using NvOptimusEnablement api just in case.while doing encoding getting below logs…GPU in use: TITAN RTX\\nunknown file: Failure\\nC++ exception with description \"NvEncoder : m_nvenc.nvEncOpenEncodeSessionEx(&encodeSessionExParams, &hEncoder) returned error 2 .any idea ?Q:\\nNvEncOpenEncodeSessionEx(void* device, NV_ENC_DEVICE_TYPE deviceType) return 2 (NV_ENC_ERR_UNSUPPORTED_DEVICE)A:\\ndoes void* device refer nvidia GPU? (multiply GPU on board such as laptop)tryPowered by Discourse, best viewed with JavaScript enabled',\n",
       " 'video-codec-exception-on-load': 'Hi I’m trying to run the examples from the library on an Xavier AGX.However I encounter an exception on the call:I get the error:The returned error is always a random negativ int.Do you have any idea what might cause this behaviour?Best regards\\nMichaelOk, for everyone who is trying to work with nvidia codec sdk, it is not working with jetson. The right api is the jetson multimedia api Jetson Linux API Reference: Main Page | NVIDIA DocsPowered by Discourse, best viewed with JavaScript enabled',\n",
       " 'how-to-relate-data-to-rgb-code-and-show-over-pointing-when-replay-a-3d-video': 'Regullary we inspect hazardous and confined sites, like tunnels and sinkholes, equipment or materials to asure quality status and/or certificate useful of them, capturing photos and videos, nowadays with drones, finally our quality experts make reports (papers) based on these media, if they were not on the site. Now, there are many solutions for making 3D-video and replay them, but I am looking for interact with this 3D-video adding one or more layers wich shows different values based on RGB codes when user is navigating and point an area or a point.Powered by Discourse, best viewed with JavaScript enabled',\n",
       " 'tesla-m60': 'HiI have been using the Nvenc 6.0 API and using the RGB input formats to skip converting to NV12.  It works great on a Quadro 3100M laptop and a Titan X desktop PC.  However on a Tesla M60 for some reason the same unmodified code runs without errors but only a quarter of the video (on the left side) is visible the other 3/4 of each frame of compressed video is all black.  Anyone else seeing this?  I have tried experimenting with changing the pitch but the same behavior still happens.  This is using a d3d11 device with a texture registered and mapped as input.Can you please tell us which version of the NVIDIA driver you are using and can you please also try the latest driver and see if the issue still occurs?The issue occurs on 356.54.  At the time I had to use the windows 10 driver since the Windows Server 2016 wasnt available.  So the driver that is being used has this file name: 356.54-tesla-desktop-win10-64bit-international-whql.exeI tried to use 376.33-tesla-desktop-winserver2016-international-whql.exe and I get an error now whether or not I use direct RGB or NV12 conversion myself.  The error is:NV_ENC_ERR_UNSUPPORTED_DEVICEI am using a directx 11.0 device.So I had to switch back to the 356.54 driver.The following simple initialization code fails for a Tesla M60 on Windows Server 2016 using driver version 376.33:Thank you trying with the latest driver and sending the code sample.\\nWe are looking into the issue.If you are passing the debug flag, make sure Windows SDK is installed.http://stackoverflow.com/questions/32809169/use-d3d11-debug-layer-with-vs2013-on-windows-10You can either remove the debug flag or install Windows SDK.The test was run in release mode so _DEBUG and DEBUG are not defined.  I have run into the issue the link talks about but that is not what is happening in this case.  That same code works on the same machine with nvidia driver 356.54 but not 376.33.  Also to be clear the failure occurs when calling nvEncOpenEncodeSessionEx.Do you have TCC mode enabled on your GPU?http://http.developer.nvidia.com/ParallelNsight/2.1/Documentation/UserGuide/HTML/Content/Tesla_Compute_Cluster.htmFor driver version 356.54 this is the outout of nvidia-smi.exe:±-----------------------------------------------------+\\n| NVIDIA-SMI 356.54     Driver Version: 356.54         |\\n|-------------------------------±---------------------±---------------------+\\n| GPU  Name            TCC/WDDM | Bus-Id        Disp.A | Volatile Uncorr. ECC |\\n| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\\n|===============================+======================+======================|\\n|   0  Tesla M60          WDDM  | F241:00:00.0     Off |                  Off |\\n| N/A   41C    P8    15W / 150W |    222MiB /  8192MiB |      0%      Default |\\n±------------------------------±---------------------±---------------------+For 376.33:Failed to initialize NVML: Unknown ErrorThis issue has been fixed in 376.84.I ran into the issue mentioned in #3 with the newest drivers.Some info about my setup:I’m using a ID3D11Device* as NV_ENC_OPEN_ENCODE_SESSION_EX_PARAMS::device and calling nvEncOpenEncodeSessionEx() returns NV_ENC_ERR_UNSUPPORTED_DEVICE.Here is a snippet of the initialization code I have:Any ideas what could be wrong here? I tried with older drivers (376.33), but encountered the same issue.@Osurac On your Tesla M60 this works with the newest (376.84) drivers, right?Yes the initialization works on M60 with driver version 376.84.Powered by Discourse, best viewed with JavaScript enabled',\n",
       " 'rtpayload-size-fixed-crashes-when-changing-payload-size': 'I’ve been playing around with some constructive solid geometry recently, and as a result I have had to add a couple of new fields to PerRayData_radiance.This doesn’t seem to work (crashes, graphical artifacts) unless I remove existing fields though - I have to keep the size of this structure constant for some reason.This is the existing structure:struct PerRayData_radiance {\\nfloat3 result;\\nfloat importance;\\nint depth;\\n};It gets referenced in the shader like so:\\nrtDeclareVariable(PerRayData_radiance, prd_radiance, rtPayload, );I was under the impression ray payloads were fully programmable - is there something special I need to do in order to make a large structure work? I’m looking for something like:struct PerRayData_radiance {\\nfloat3 result;\\nfloat importance;\\nint depth;\\nint inside_sphere;\\nint inside_box;\\n};I’ve checked the forums and the documentation, but neither seem to mention any sort of special size required for this struct - in fact the rtPayload is supposed to be customizable. I can’t find any relevant buffer allocations either, and all the SDK examples use the exactly the same structure with no added fields.For my purposes, I can get away with using depth as a bitfield - but I would prefer not to.I’ve had no problem adding an arbitrary number of fields to existing ray payload structures or creating new ray payload structures.Can you make sure that PerRayData_radiance is defined only once in your project? It seems possible that some of your files might still be referring to an old definition with a different size structure.Maybe you get some OptiX exceptions (stack overflow comes to mind), did you enable them?\\nTry raising the OptiX stack size and see if it affects your results.I actually added an entirely new structure PerRayData_csg_radiance, and set it as a ray payload - double definitions shouldn’t be an issue.I can up the stack size, but presumably this should have no effect since the same number of calls are being performed either way? I’ll try it though.I should also mention that for some reason, this issue actually causes an OS crash (not just an Optix one). I can easily reproduce the effect just by trying to set a single field in the structure - a one line difference causes the crash (I can define an arbitrary number of fields, but changing values is what causes issues). Sometimes when it doesn’t crash, I get Optix timeouts.I was having the same problem, when the ray payload contained more than a certain number of fields I would get an unexpected behavior and get a lot of exceptions. I don’t remember exactly the size of fields that I was using which was causing the issues, but as you said, more fields can be added and it won’t cause problems until you start modifying the values of those fields. What I ended up doing was to perform the ray trace (rtTrace) more than once with the same origin and directions but with different ray types containing different payloads. I guess there is a limit on the size of the ray payload that Optix can handle.By the way, the size of the structures of one of the payloads that I am using is larger than the one that you are trying to use and I don’t have any problems with it.\\nThis is how my struct looks like:Some of my ray payloads are 76 bytes, and I have not had a problem. What are you using in terms of OS, bitness, CUDA and OptiX versions, GPUs, etc?Would one of you retry your code that shows the bug with 3.7.0 beta 2? If it still fails, could you make us an OAC trace and send it to OptiX-Help@nvidia.com for us to fix?This is my workable example: from 3.5.1. to 3.7.0 beta 2Probably another reason brings an error. Every time I thought that rtPayload worked wrong I was wrong.\\nMy platforms:\\nNotebook: Intel® Core i7-3630QM @ 2.40GHz, Win 8.1, x64, VS2010, CUDA 6.0/6.5, OptiX 3.6.2./3.7.II, GeForce GT 650M.\\nDesktop: Intel® Core 2 Quad @ 2.50GHz, Win 8.1, x64, VS2010, CUDA 6.0/6.5, OptiX 3.6.2./3.7.II, GeForce GTX 560 Ti.Happy New Year!\\nsudakPowered by Discourse, best viewed with JavaScript enabled',\n",
       " 'nvidia-a2-and-a10-video-encoding-performance-benchmarks-and-performance-recommendations': 'I am looking for some benchmarks/guidance on the A2 and A10 GPUs for H.264 encoding.I need to spec a GPU (or a pair of GPUs - not more than 2) that can handle inline encoding of fourteen1080P video streams. I receive either 10-bit1080i60 or 10-bit1080P30 over SDI, then H.264 encode it.From this T4 benchmark: https://developer.nvidia.com/blog/turing-h264-video-encoding-speed-and-quality/Two T4s should be adequate to provide some head room, but would prefer an ampere card to get the PCIe 4 bandwidth.The A2 does not seem like a direct replacement of the T4 with much lower specs and am skeptical it has the same capacity as the T4.The A10 roughly seems to be about double the performance of the T4. With no benchmarks I would think I could achieve at least the same numbers as two T4s based on the benchmark I linked above. I don’t know if this is a valid conclusion or not.Doing an evaluation with Nvidia is not an option. This application requires many PCI slots and the line of servers that I use do not have a relevant Nvidia certification for any model with an adequate number of PCI slots for any relevant model of GPU, and very few certifications at all for anything smaller than an A30.Does anyone have any comments on the performance of these two cards in this application?Hi there!Are you aware of our Video Codec SDK? As part of the documentation you will find also an overview of raw encode performance compared across different GPU generations. Look for “NVENC Performance” on the NVENC Application Notes.Another bit of information is the GPU NVENC/NVDEC support matrix, where you can see that T4 and A2/A10 have the exact same number of NVENC chips (1) and capabilities, just different Chip generations. So other differences aside, you should still expect the same or even better performance with either A2 or A10.I hope that helps.I was aware of the SDK, but not the documentation you provided - this is fantastic, exactly what I needed. I had searched around a bit but indexing isn’t fantastic.The biggest understanding gap I had was that encoding is totally offloaded on the nvenc and have been busy chasing cuda core count.This topic was automatically closed 14 days after the last reply. New replies are no longer allowed.Powered by Discourse, best viewed with JavaScript enabled',\n",
       " 'use-fruc-to-do-framerate-up-conversion-more-than-2x': 'Hi:The newly published OpticalFlow SDK 4.0 has a FRUC lib that  can complete  frame rate up conversion,but the doc and sample only introduce the method of interpolating one frame(2x framerate up), so how to interpolate more than one frames?I’v tried modify output parameter’s “nTimeStamp” field of “NvOFFRUCProcess” function with same consecutive frames multi times,it looks like working, but I’m not sure it is correct, so can I call NvOFFRUCProcess() with same consecutive frames multi times to interpolate more frames?Thanks for sharing the experiment with the frames. I believe that FRUC doesn’t care about what frames you put there so theoretically you can just pass the same frames forever with increasing timestamps or something. Another thing that comes to my mind is using the newly interpolated frame in the next interpolation as one of the inputs. But this might propagate the interpolation errors further to the novel views.I have done multiple passes of the frame interpolation adding the frames generated by the first pass to the base set of frames. I started with a 240 fps video and split into frames and then interpolated those 3 times.  For the content I have it does a good job. The only issue I have found is that the video generated from the frames flickers, any ideas on that would be appreciated.Hi,\\nCurrent implementation of FRUC library support only 2x framerate in one run.\\nFRUC library is capable to interpolate a frame anywhere between two frames e.g. given two frames at time stamp 0 and 1 you can interpolate frame at any arbitrary time stamp like 0.25, 0.33, 0.75 and so on.\\nFrame rate of more than 2x can be achieved using FRUC library multiple times on same input stream. This solution is not optimal and efficient but will allow you to test and check on your end.Powered by Discourse, best viewed with JavaScript enabled',\n",
       " 'large-ch-function': 'I have a large Closest Hit function(about 400-500 lines of code). It has a lot of condition checks and calculations. I wish to split it up to make it easier to edit the code. I know that I can use other device functions in CH but most of the calculations require the Payload data(which is packed into a couple of integers). What is the best method of keeping the CH small(and simple)?This is in OptiX 7, right?There’s no single best method, and it depends on exactly what your goals are. Keeping things small & simple is sometimes a bit at odds with keeping things fast, so it’s a balancing act.You can use any of the normal C++ abstraction tools at your disposal to factor your CH function; functions, classes, templates, defines, etc… It’s common to break the function into several functions and rely on inlining to make sure you’re not losing any performance. The CUDA compiler normally inlines functions defined in the same file. You can use the forceinline tag if you want to make sure. This means you can make a helper function next to your CH function that takes a RadiancePRD& parameter, for example, and the compiler will expand your function call so that accessing your payload is no different between your CH function and the helper function you call.Take a look at the optixWhitted_exp sample, for example. The getRadiancePRD() function in shading.cu returns a struct. Because it’s inlined, this is less expensive than it looks.The optixWhitted_exp sample has other examples of helper functions as well, see the accompanying “helpers.h” file and take a look at how the functions are used.Please be aware that this Whitted example isn’t really following the absolute best practices for performance, but this is a good example of how to build helper functions to keep your CH program manageable. If your payload is less than 64 bytes (which it is in optixWhitted_exp), the advice for best performance is to use the payload attributes directly (via the optix{Get,Set}Payload_*() functions.) That means don’t copy the payload values into a struct or variables, instead just use the payload functions directly. Using this advice might change how you choose to factor your CH function. You could imagine writing some helper functions or a class to abstract which payload value is in which payload slot, so that you don’t have to use the slot number directly in your code every time you access the payload.The optixPathTracer_exp sample uses packed pointers for the payload, which is best for large payloads that can’t fit into the 8 payload registers.Try to make sure you any conditional checks in your CH function that are constant across your launch are not requesting memory, if you can. Using #defines or constant template parameters is better than a run-time check since the unused code can be elided. If you do need a run-time check, prefer using your constant params and your payload to shared or global memory.I’m mixing code management advice with performance advice, so I hope it helps but I’m not sure I’m answering your question. If this doesn’t give you some ideas, then feel free to get more specific on your situation and what your constraints look like.–\\nDavid.Powered by Discourse, best viewed with JavaScript enabled',\n",
       " 'debug-interfaces-failing-on-laptop': 'When I try to call D3D12GetDebugInterface or CreateDXGIFactory2 with the flag DXGI_CREATE_FACTORY_DEBUG, the calls are failing on my laptop, but not on my desktop. Other calls work fine on the laptop – I just can’t initialize the debug layer. The desktop has an oldish GeForce 650 Ti, and the laptop is a Dell XPS15 laptop (latest gen), which has a GeForce GTX 960M. Both have the latest drivers. Any ideas what could be going wrong?Powered by Discourse, best viewed with JavaScript enabled',\n",
       " 'how-to-access-the-elements-of-a-matrix-from-within-the-gpu-memory': 'I have a GPU matrix “GpuMat d_src” it’s actually holding the original image matrix “cvMat src” received from the CPU.\\nNow I need to to access the element of “GpuMat d_src” through OpenCV.But I cannot do so.Powered by Discourse, best viewed with JavaScript enabled',\n",
       " 'configuration-question': 'Hello,\\nI am currently configuring a Windows workstation with two Quadro P5000 in cooperation with Dell. Their main use will be GPU Rendering (i.e. Octane Render, Iray). As far as I know SLI is not recommended for this purpose. However, I want to create VR applications via Unity as well. Would you recommend a SLI Bridge? If so, is it possible to deactivate SLI for GPU Rendering as a default setting and activate it just for Unity?Thank you very much in advance.The NVIDIA control panel can be used to quickly enable and disable SLI as needed.Powered by Discourse, best viewed with JavaScript enabled',\n",
       " 'executecommandlist-on-deferred-context-fails-on-nvidia': 'Hello. I’m unable to merge command lists from two deferred contexts with ExecuteCommandList.\\nEvery time I get “Access violation reading location 0x00000000.”\\nHere is the sequence of calls:For sure there are no other calls on m_commandlist from other threads and there is now call on immediate context with executecommandlist between these two calls. There is also no difference if both deferred context are in the same or different threads (of course access to m_commandlist is guarded with critical section).\\nWhat could be wrong in this case? Is there a bug here?\\nThanks for any help.Hi Zachar,I reported this issue to Nvidia back in March of this year and was told it might take a little time to fix. I’m actually already chasing up this bug with my contact (for personal interest) but when I hear back I can update this post if it’s still of relevance to you.Adamhello Adam.\\nThank you for your response. I will be grateful for any update on this as this issue makes impossible to use NVIDIA cards and we have to stick to AMD.\\nCheers\\nWojtekPowered by Discourse, best viewed with JavaScript enabled',\n",
       " 'd3d12enableexperimentalfeatures-causes-d3d12createdevice-to-fail': 'I am following Nvidia’s DXR tutorial: D3D12HelloTriangle. I call below code before creating device and it returns trueUUID experimentalFeatures = { D3D12RaytracingPrototype };\\nHRESULT hr = D3D12EnableExperimentalFeatures(1, experimentalFeatures, NULL, NULL);But this causes later for D3D12CreateDevice to fail. I am using Nvidia Geforce GTX 1050 :Card name: NVIDIA GeForce GTX 1050\\nChip type: GeForce GTX 1050\\nDAC type: Integrated RAMDAC\\nDevice Type: Full Device\\nDevice Key: Enum\\\\PCI\\\\VEN_10DE&DEV_1C8D&SUBSYS_12651025&REV_A1\\nDevice Status: 0180200A [DN_DRIVER_LOADED|DN_STARTED|DN_DISABLEABLE|DN_NT_ENUMERATOR|DN_NT_DRIVER]\\nDriver Version: 23.21.13.8873\\nDDI Version: 12\\nFeature Levels: 12_1,12_0,11_1,11_0,10_1,10_0,9_3,9_2,9_1\\nDriver Model: WDDM 2.3You need the 1809 build of windows and turing cards to make it workPowered by Discourse, best viewed with JavaScript enabled',\n",
       " 'performance-profiler': 'Is there a DirectCompute performance profiler anywhere?Powered by Discourse, best viewed with JavaScript enabled',\n",
       " 'nvh265enc-gstreamer-plugin-with-rtx-a4500-or-a5000': 'Hello,I am using the nvh265enc Gstreamer plugin on a host with a Quadro P2000 GPU.\\nNow I would like to use the same plugin on a host with RTX family GPU (A4500 and A5000 in particular), however, it does not seem to be available.\\nWe have “GStreamer Plugin Bad” installed.Is the issue that this plugin is not implemented for RTX cards?PatrickHi patrickogf,How did you determine that it is not available on the RTX A4500 or 5000? As far as sources go the plugin is still part of the GStreamer github.GStreamer ans especially the “bad” plugins are not maintained by NVIDIA, so there is limited infomration from our side if or how one of them are supposed to work.Powered by Discourse, best viewed with JavaScript enabled',\n",
       " 'gpu-max-operating-temp-seems-low-for-laptop-3060': 'Usually when looking at “nvidia-smi.exe -q -d TEMPERATURE” outputs, the GPU Max Operating Temp is within a few degrees of the GPU Slowdown Temp, however for the mobile 3060 (laptop) and possible other mobile cards, I’m noticing the GPU Max Operating Temp is quite low comparatively.For my mobile 3060 the GPU Max Operating Temp is 87C while the GPU Slowdown Temp is 102C, that’s quite a significant difference and I can’t think of a scenario where it beings throttling at 87C and eventually hits 102C. As far as I know the GPU Max Operating Temp is when it starts to throttle clocks through SW and GPU Slowdown Temp is when it throttles via HW.Anyone know why this GPU Max Operating Temp is so low, especially for a laptop GPU? I thought laptop GPUs were able to run hotter, and the high slowdown temp definitely indicates that. I’ve noticed alot of other desktop GPUs have max operating temps in the 90s.Powered by Discourse, best viewed with JavaScript enabled',\n",
       " 'nvidia-optical-flow-vapoursynth-plugin-motion-compensation': 'Greetings everyone, mainly NVOF developers.I represent  the community of digital video processing, using the open-source graph processing framework VapourSynth.\\nSome of the best algorithms we use rely heavily on motion estimation and compensation. We reconstruct current frame from its surrounding to gather information and achieve, for example, purely temporal denoising.For years now we’ve been stuck with a slow and not very reliable block-matching motion estimation. So I and, I believe, the whole community are interested in having precise dense optical flow calculation tool.I made a proof of concept NVOF plugin. It’s open-source and available here:\\nhttps://bitbucket.org/mystery_keeper/vapoursynth-nvofSo far it’s unoptimized, and there might bugs and flaws in my logic. Comments on the code and improvement suggestions are welcome.I wanted to tell my full experience in this post, but as new user, I cannot post more than one link/image. I’ll try to tell more in the comments and outline my main concern here:NVOF cannot be used for motion compensation without a confidence field. And the cost field it produces cannot be used as one.I get impressive results in some frame areas and horrible errors in others. And masking by thresholding the cost field doesn’t really help to discern the pixels with good vectors from the ones with errors. The cost field actually looks weird!I’m very interested in the improvement of NVOF. My plugin can already be used as a testing tool. And I’d be happy to help with testing. The fastest way to reach me is in Discord: Kittyfluff#6935Thank you, and keep up your work.Here is the VapourSynth script I use for testing.nvof-test.vpy (3.3 KB)Here’s a frame, that has both good vectors and errors. It shows, why the cost field doesn’t work as confidence field.\\nnvof-test.vpy - 22851440×1080 168 KB\\nHere’s the flow in a perfectly still sequence!\\nnvof-test.vpy - 121440×1080 259 KB\\nTo debug the visualization, I wrote a Python script that generates a video of a circle moving along an infinity sign trajectory. Here’s the script.infinity-circle.py (1.0 KB)And here’s a very weird flow I get for it.\\ninfinity-circle.vpy - 31440×1440 286 KB\\nNo feedback from the NVOF team, huh?\\nWell, I haven’t been idle and wrote a (very slow) local correlation filter.\\nWith this I can finally find and correct the motion compensation errors.\\nReally need to accelerate it though.\\nMaybe with OpenCL.\\nnvof-test.vpy - 22851920×960 214 KB\\nAnother thing NVOF desperately needs is scene change detection.I’m getting random BSODs when trying to use my filter.\\nDoesn’t seem to be a memory issue.\\nCan’t tell if its a problem in NVOF or in my code.Bitbucketdude, have you tried to improve the optical flow by checking the consistency with Forward and Backward.Finally got time and tried it.\\nIt filtered out some good vectors.\\nIt didn’t filter out some very bad vectors.\\nIt significantly ramped up VRAM and RAM consumption and slowed down my filter.\\nIt’s still unreliable, so I’ll just keep using correlation for now.Also, noticed finally getting no vectors in perfectly still frames.\\nLikely there were improvements in drivers.\\nStill getting false vectors in flat backgrounds with a single moving object.Powered by Discourse, best viewed with JavaScript enabled',\n",
       " 'visual-representation-of-ray-propagation-using-ray-tracing': 'Hi again!As always, thanks a lot for the fast and thorough answer, your explanation makes sense and is understandable even for beginners as myself. I have seen in similar threads that you recommend to work through the SDK samples to achieve better understanding of the engine, which is something I plan to do. If I understand you correctly, you would recommend some extra time for the optixRaycasting sample?Oh also, our plan is to integrate our optix project into a flexible 3D-environment in order to easily create different environments such as an office with the help of programs, eg Unreal Engine or Unity. A post from 2017 mentioned that optix is not compatible with Unreal Engine, is that still the case and if so, are there any other softwares that you would recommend?Thanks!If I understand you correctly, you would recommend some extra time for the optixRaycasting sample?Not at all. That’s definitely not an example I would recommend to look at first.This is special in the way that it’s showing how to do only ray-triangle intersections with the high-level OptiX API.\\nMeans everything else like ray generation and shading calculations happen inside native CUDA code.That is a so called wavefront approach. You put a number of rays you want to shoot into the scene into a buffer, you launch a ray query with the dimension of that buffer, then the OptiX ray generation program reads the rays from that buffer (one per launch index), calls optixTrace with it, and the closest hit and (optional) miss programs report some data back to the per-ray payload, which is then written to the same dimension hit result buffer, which you then evaluate with a native CUDA kernel (outside the OptiX launch), which does the “shading” calculations and then generates potential continuation rays, and repeat that until there are no more rays to be shot.This is effectively what the old discontinued OptiX Prime low-level ray-triangle intersection API did. OptiX Prime is not accelerated on RTX cards, that’s why the optixRaycasting example exists as an alternative, which actually offers more flexibility.Still this wavefront approach has the drawback that it’s very memory access intensive. It’s usually faster to calculate the rays inside the ray generation program than to write to and read from a global memory buffer. The more the GPU can handle in registers the better.\\nAlso this would require a launch for each set of new ray segments in a path tracer. And you would be responsible for handling the scheduling when some rays terminated early.\\nIt’s much faster to iterate over the whole path while staying inside the ray generation program in a single launch and let OptiX handle the scheduling internally.Long story short, I would recommend looking at all other OptiX SDK and open-source examples you find linked in the sticky posts of this sub-forum first, to understand how the whole ray tracing pipeline with raygen, exception, intersection (built-in for triangles (in hardware) and curves), anyhit, closesthit, miss and maybe direct and continuation callables play together. When done correctly, this is going to be the faster solution.Actually read the OptiX 7 Programming Guide first. https://raytracing-docs.nvidia.com/Oh also, our plan is to integrate our optix project into a flexible 3D-environment in order to easily create different environments such as an office with the help of programs, eg Unreal Engine or Unity.\\nA post from 2017 mentioned that optix is not compatible with Unreal Engine, is that still the case and if so, are there any other softwares that you would recommend?The OptiX API knows nothing about application frameworks, windowing systems, scene file formats, UI, controllers, etc.\\nRelated post about what OptiX is and isn’t: https://forums.developer.nvidia.com/t/how-to-develop-user-defined-rendering-in-optix/185374/2It’s your responsibility to build the necessary acceleration structures from whatever geometrical descriptions you have.\\nIt’s also your responsibility to implement everything related to shooting rays and handling potential material behaviors inside the respective domain programs.If or how that is possible inside these game engines you cite is outside my expertise. It’s some years ago since I touched the Unreal engine and that thing is huge. I don’t know how difficult it would be to integrate some simulation module like that into it. Mind that these are using graphics APIs like Direct12/DXR or Vulkan (not sure about Vulkan Raytracing). You would use CUDA and OptiX. Sounds problematic to me.For a start you might want to look at some of the OptiX SDK examples which can load OBJ and (not all) glTF model files .\\nMy more advanced OptiX 7 examples use ASSIMP to load mesh geometry (not points, not lines, not really materials) from any supported file format.What I’m saying is, that it would be simpler to start with some standalone OptiX application which can generate or load some scene data and develop the required algorithms with that first, before trying to delve into full blown game engines which work completely differently and might not even allow what you’re describing.Follow all links in this post. The one to the OptiX 7.2 Release contains links to more examples:\\nhttps://forums.developer.nvidia.com/t/optix-7-3-release/175373\\nhttps://forums.developer.nvidia.com/t/optix-advanced-samples-on-github/48410/6When you have worked through all the samples @droettger suggested to understand the basics of OptiX and have some knowledge of DirectX11 / Unity, here you can try a way to move buffer/texture data (in your case the tracked radio wave hitpoints buffer data)\\nfrom OptiX to Unity or back:\\nNOTE: the posting shows how to move data from Unity to OptiX, you would need the other direction for the results;\\nFrom that found hitpoint data then you simply create the rasterizer primitves to run draw calls for those lines within your scene.With the intention to use Unreal or Unity as the world builder for the simulation, the main problem is that there is no simulation possible without having the scene’s 3D geometry and transformation hierarchy inside OptiX acceleration structures in the first place.I would expect that data to exist in some scene graph representation on the host in any application at one point.CUDA interop would only be required if the data is held in the graphics API’s device buffers and then it depends on the formatting and alignment if that data could be reused inside CUDA/OptiX directly.With all such interop ideas, you need to consider that the memory and lifetime management happens on the game engine’s side since that is the owner of the data. Registering resources and doing some work on them while the game engine might do things asynchronously is deemed to fail. Such mechanisms require intricate knowledge of the game engine’s internals.An independent copy of the data would be more robust but then you could also just save the scene into a loadable file format and get it from there as a start. That’s complicated enough for an OptiX beginner.True, of course the 3D geometry could be loaded twice (as an independent copy), in the game engine and in OptiX;  but for the radio wave propagation simulation, that 3D data I did not intend to suggest that to be registered/mapped.\\nInstead (as I wrote above) for speed register/mapping the hitpoint results written to an OptiX (CUDA) buffer would be faster, than using the CPU memory, because then it directly can be read from the game engine on the device.\\nOf course registering resources can be tricky. But if the resource is only setup for this purpose and you have some atomic (inter)locks or semaphores in the game engine in place, that can work safely. From my experience I only found memory limits could be problematic with registering/mapping. Even registering the resource for each frame and then unregister can work in those cases and might improve speed (when memory is an issue).\\nHowever, its true of course it depends on how experienced you are on DirectX11 and OptiX.\\nI simply wanted to point out that its possible.\\nFor integrating such a complex operation into a flexible 3D-environment it may take some time to get that done. And dependent on the goal a standalone OptiX application can be certainly the easier path to go.Hello again!An update for you:\\nWe decided to use the base of optixWhitted as a template for our project since it contained a lot of physics we wanted from the start, such as fresnel-schlick and beers law. We are playing around with the scene at the moment by creating new spheres and tweaking the placements of things just to get a feel for it.Now we are trying to change/add properties to the rays coming from the light source, how can one do this?. For example we would like to add a new variable “freq” in order to represent signal strength and make the strength of the ray dynamic by something like new_ray_strength = freq * material_attenuation so that the signal is weaker once it has passed through an object depending on both the materials attenuation and the frequency of the ray.Thanks!If you want to track any parameter along the ray, you need to put it into the per-ray payload which you pass on via the optixTrace calls.\\nThe overloads of optixTrace support a limited number of 8 unsigned int payload registers.\\nThese can be filled with any data you like and could, for example, be reinterpreted as float with the CUDA float_as_uint() and uint_as_float() functions.\\nhttps://raytracing-docs.nvidia.com/optix7/guide/index.html#basic_concepts_and_definitions#ray-payload\\nhttps://raytracing-docs.nvidia.com/optix7/guide/index.html#device_side_functions#traceIf you need more per ray payload memory than fits in 8 unsigned int registers, which is the case for most applications, you define your own custom payload structure, instance that inside the ray generation program in local memory, and split the 64-bit pointer to that local structure into two of the available payload registers.The OptiX SDK examples show that. Look for the packPointer and unpackPointer functions inside the example code.\\nAlso look at the optixPathTracer example inside the SDK.My OptiX 7 examples are implementing path tracers and the same mechanism but I aliased the single payload pointer with an uint2 in a union to prevent any shift operations used in the pack/unpack implementation.\\nThe compiler is pretty clever and generates only move instructions in both cases though.\\nFind the implementation of that here. Uses cases can be found inside the raygen and closesthit pogram.\\nhttps://github.com/NVIDIA/OptiX_Apps/blob/master/apps/intro_runtime/shaders/per_ray_data.hNote that CUDA variable types have specific alignment requirements. You can avoid automatic padding by the compiler in your device structures when ordering the field properly.\\nRead these posts:\\nhttps://forums.developer.nvidia.com/t/directx-optix-single-geometry-buffer-or-multiple/39351/3\\nhttps://forums.developer.nvidia.com/t/rtbuffer-indexing/167440/13Hi!For our purpose, do you think that our idea of using and modifying a copy of one of your samples is a good idea and if so, which one can you recommend for us. Or do you think it is better to start from scratch?ThanksLet’s describe how I would approach that.I would have no difficulties taking one of my more advanced OptiX 7 examples (rtigo3 or nvlink_shared) apart and replace the current full global illumination renderer with the required OptiX domain programs doing that receiver/transmitter simulation.The benefit of that would be that you have a ready-to-use CMake based OptiX application framework not using anything from the OptiX 7 SDK except its API headers. You can start from scratch with that, but running.nvlink_shared is the newer application which contains a simple arena allocator which simplifies the CUDA memory handling, though that example is targeted at multi-GPU use cases and could need adjustments to run optimally on single-GPU as well. That’s just a matter of not doing the compositing step when there is only one active device. But you’re not rendering images anyway.\\nThe rtigo3 program shows single- and multi-GPU rendering mechanism with different OpenGL interop modes meant to demonstrate how to program interactive applications.\\nBut both contain a benchmark mode which is completely independent of any windowing or GUI framework. Means it’s also possible to completely remove the OpenGL, GLEW, GLFW, and ImGUI parts to make either run offscreen on any CUDA capable device without display capabilities. (I did that for either of these examples before.)I would keep the application framework as it is, keep everything related to generating simple objects at runtime and loading mesh data from scene file formats supported by ASSIMP into the simple host-side scene graph, and probably start off with the simplest render mode which is single-GPU. Means the system and scene descriptions would work the same way.If there is no need to render any images with the raytracing, a lot of the code can simply go away. For example the whole Rasterizer and OpenGL part.\\nOn the other hand it would be possible to change that to actually rasterize the scene and visualize the result of your ray traced simulation. That’s not too hard given that the scene data is in that simple host-side scene graph which could be traversed by a rasterizer similarly to how the ray tracer traverses it to build the GAS and IAS data once.Then I would implement some code placing the transmitters and receivers definitions into the scene description. That is probably best handled in a separate description file to make it independent of the current scene. Means that could be reloaded without restarting the application, or run as batch process for many different configurations.Then the transmitter/receiver definitions need to be made available via buffer pointers inside the global launch parameters to be able to access them for sampling. Like the CameraDefinition and LightDefinition arrays in the current examples.Then I would re-implement the ray generation and closesthit programs and the transmitter/receiver visibility tests.\\nMind that this means replacing all existing material and camera handling and the rendering of the images as well.\\nI would just need to know how the ray distributions work for the simulation (reflection, refraction, diffraction, attenuation, etc.) and how to store the resulting data.So getting a framework which allows implementing this with my existing OptiX application frameworks would foremost be removal of existing source code to the bare minimum.\\nSome of the things the renderer uses today, like tangents to be able to render anisotropic glossy materials would probably not be required either, so there are a lot of small changes possible.\\nI would also change the shader binding table layout from one entry per instance to one entry per material and use the instance ID for the indirection to the per instance data (vertex attributes and material properties) while the instance sbtOffset selects the resp. closest hit programs. (That is, no more use of direct callable programs for the different materials. That should be faster overall.)So yes, I would think you could start with one of my OptiX 7 examples and change all the things to your liking.\\nTo get a 3D scene into that standalone program would be just a matter of having it in some format which ASSIMP can load, e.g. even OBJ would be a potential start.\\nI think adding such simulation framework directly into any of the game engines you listed would be a lot more complex.\\nThough if you know exactly how to access all scene resources in such a game engine, that would be the next step.\\nIt should be much simpler implementing, debugging, and optimizing the simulation in a standalone application first.Hello!I am not quite sure how to make the transmitter work as I would like. I want to shoot my own custom rays with OptixTrace from a custom coordinate in the scene where the transmitter would be placed and maybe by visualizing the rays with light see the rays being emitted from the custom coordinate. OptixTrace seems to be using the camera somehow so that when I am experimenting with the direction of the rays from my own OptixTrace call it messes up the camera and the scene looks weird. I do not want these rays I am trying to generate from the transmitter to have anything to do with the camera. I think I am also confused about how to set the ray direction correctly within OptixTrace. Could you be more specific regarding how to shoot rays from a transmitter at a custom coordinate within the scene.Thank you!Hi @layser,The ray origin and direction you pass to optixTrace() is completely under your control, and not something OptiX knows about other than the arguments you pass in. OptiX does not have any notion of a “camera” per-se, that is something you can write in order to generate rays. In the OptiX SDK samples, the camera is part of the “sutil” library, not part of the OptiX API. In your own application, you might have your own camera code, or no camera at all and you can just generate unique rays in your raygen program any way you see fit.The idea of cameras and light sources is analogous to the more general idea of receivers and transmitters, respectively. Whether you trace rays that originate from a receiver or a transmitter is up to you; there are simulation algorithms for going in either direction (or both). So you can shoot rays from a transmitter, and they will never have anything to do with a camera unless your code is making some kind of association. In order to generate rays from a transmitter, you would set the ray origin to be at the transmitter’s position, if it’s a point, or perhaps by sampling a point on the transmitter’s surface or volume. You can set the ray direction to be in whatever direction you need- it might be to form an “image” of some sort, or you might be sampling surfaces in your scene, or you might need a certain directional distribution. All of this is your responsibility, and OptiX knows nothing about what your transmitters and receivers look like. OptiX only processes whatever rays you pass in to optixTrace().BTW, this is a separate topic from the above thread. We can continue this discussion here, but please in the future start new threads for new questions. New threads are encouraged and welcome.–\\nDavid.Hello again @dhart,Thank you for your reply, forgot to mention that me and OP are working on the same project together so that is why I posted it here.After your suggestion we started modifying the rtigo3 sample and wrote our own function in the raygen program called “integrator_radiowaves” where we specify origin and direction to what we want, we also pass a custom payload “frequency”, below is an outtake from our function:\\n\\ntrace i vår funk929×157 22.9 KB\\nand here is where we call our function, directly below where the call to the original integrator function is:\\nWe tested our function and played around with the direction of our rays to make sure that our rays triggered the hit programs, which they did, however if we want to do something with our rays in one of the hit programs, for example change color by modifying prd->radiance we cannot see anything change. That is because in the picture above “radiance” gets its value from the integrator function and radiance is later passed to the outputbuffer which renders what we can see as can be seen below right?\\nSo we would like our rays to visually change the scene in addition to what we can already see in the original rtigo3 sample, can we somehow add our own “layer” to the outputbuffer where our changes can be visual or could we maybe create another raygen program and show the result from both?I feel like I did not do a great job of explaining myself so please ask if anything is unclearThanks!or could we maybe create another raygen program and show the result from both?Yes, it doesn’t really make sense to merge the simulation and the rendering of the image together into one ray generation program.\\nI would expect that the image synthesis and the simulation should have decoupled launch dimensions.The radiance integrator in that program is implementing a uni-directional path tracer with direct lighting (next event estimation) and multiple importance sampling between light and BSDFs. The primary rays are mapped to a pinhole camera. That all needs to be changed for your simulation pipeline.What you’re doing with that integrator_radiowaves() is shooting a single hardcoded ray for all launch indices into the scene with the same radiance ray type, so that would use the same SBT entries and calculate something in the existing closest hit programs.\\nSpatially that has nothing to do with the pinhole camera rays’ launch indices of the radiance integrator.\\nSo how would you like the rendered image to change depending on your simulation rays? These two parts do not interact with each other.BTW, if you have the frequency as field inside the PRD, there shouldn’t be a need to have it as additional payload register on the optixTrace call. Since you’re sending the PDR pointer on the first two payload registers anyway, you can access that frequency field in all called programs. At least there is no good reason visible in the code excerpts for doing it.I explained that the simulation algorithm could be derived from the renderer architecture because the transmitter and receiver can be handled like camera projections generating the primary rays and the receivers as lights (or vice versa if that is simpler).\\nThat simulation needs to happen first to be able to include its results inside the rendering in any way, like with the OpenGL rendering in the images of the initial post.\\nHandling that inside the same launch doesn’t make much sense to me.Means you need to create a second OptixPipeline with new ray generation, closest hit and miss programs you need for the simulation. You wouldn’t shoot “radiance” rays either.\\nThat is a path tracer with next event estimation as well, that’s why most of the concepts from the current architecture can be reused. But the ray generation needs to calculate the rays shot from your transmitter and connect them with the receivers and bounce around on surface hits with some distribution matching your simulation behavior. means you need to implement a material system which represents the behavior of your rays at your frequency which mimics the real world behavior. (That’s the most complicated task.)It’s not like these simulation paths would affect the renderer paths, unless you’re adding some geometry for these paths. and that obviously requires to run the simulation completely first. That is a separate progressive Monte Carlo algorithm which requires many OptiX launches to produce the output data.In the end you’d need to store the resulting connecting paths into some data structures which could then be used in a visualization algorithm to show them inside the 3D scene, like by rendering the whole scene with OpenGL and adding the connecting paths as line primitives.\\nThat is not going to happen easily inside that existing path tracer.That could render linear curves if you wanted, but that requires some changes to the host side scene graph and the pipeline to handle curve primitives. I have done that in the past.Again, please don’t try to bolt that simulation onto that renderer code. That are two completely different things.\\nInstead derive the simulation code from the existing algorithmic structure of the renderer pipeline.\\nIt needs to be stripped down to an empty template because all programs, the material handling and the resulting output need to be changed. Means you need a second OptixPipeline and separate optixLaunch calls for the simulation and the renderer.\\nDecide what output you require first, then plan up from there what structures you require to get to that result.however if we want to do something with our rays in one of the hit programs, for example change color by modifying prd->radiance we cannot see anything change.I’m not sure why writing to the payload isn’t working for you. Do you want to post the relevant parts of your hit program that reads the payload pointer and writes to your payload struct? If you write to your payload struct in memory, then the values will be written. I would assume the most likely reason that the change might not appear to be made is if some other part of the code is overwriting the value later. One option is to use printf() before and after optixTrace() to confirm the value changed. Restrict the print to a single pixel to prevent too much output.Note with OptiX 7.4 you might consider passing all your payload values directly, rather than passing the struct pointer. If so, you then write to the payload using optixSetPayload_<n>() functions in your shader program instead of an assignment like prd->radiance = 1.f.In your code sample, is there a reason to pass the local frequency variable separately? It should be available in your hit program as prd->frequency.–\\nDavid.Hi all,\\nit seems that you want to do something similar to what we did. You can take a look at our approach. I mentioned it on a previous post:Our radio propagation simulator can be used as a Unity plugin. As mentioned here, simulation is separated from rendering. The code is using Optix 6.5, though. We are not really interesting in rendering, just simulation, so the plugin is not very efficient. We use Unity mainly to create the 3D scene that it is later loaded to the simulator. It can render the propagation paths (the method should be definitively changed) but it is enough for us because we use it just for debugging. Anyway, all of this may be useful to you to get started in your project.Hello again!We are currently in the process of creating a new pipeline as previously suggested. We have created a new initpipeline function and created our own modules for raygen/closest hit programs etc, the rest of the pipeline is pretty much a copy of the original pipeline for now. For starters we tested the new pipeline to make sure that the right raygen program started, which it did. The problem we are running into right now is that only the last initpipeline call is happening. For example in the picture below, it seems like only the original pipeline is run:\\nWe suspect that this is because we need a new SBT for our new pipeline. Is there something else/more required in order for both pipelines to work?Thanks everyone for your previous answers!Ok, so instead of copying the whole program and changing it to do only the simulation, you’ve duplicated the initPipeline() function and replaced the code in there with your own OptiX programs and build a second OptixPipeline.\\nThat initPipeline() also initializes that shader binding table (SBT) part which are NOT the hit records.\\nThat final part happens later inside the void Device::createHitGroupRecords() function which is called after the scene has been built, because that is when it’s known which instance needs what additional SBT data with the vertex attributes and indices and material and light index.You would need a separate SBT for your new pipeline and add the hit record initialization accordingly.\\nThen you would need to call optixLaunch() with your new OptixPipeline and matching SBT to run the simulation obviously.\\nThat would also need a different launch parameter block with the necessary information about your transmitter and receiver information and the output buffer which should receive your resulting connecting path of the simulation somehow. (That’s a whole different topic.)The SBT I’m using has an entry per instance because that made storing the vertex attribute and index pointers and material index and light index data straightforward.\\nAs said before, today I would recommend to build the SBT with the number of materials and put the per-instance data structs into a separate buffer which is indexed by the instanceId. That would result in a smaller SBT. Maybe something for later.Yes, we duplicated the initPipeline() and made our own version of it.I do not quite understand though why we should rewrite the material system. The one already existing should be sufficient since, in our case, we just want all materials to have the same properties, just different values on the properties. Like permittivity, permeability etc.  And these values later on control how the rays will interact with the objects.We have now created our own sbt which we use when we call optixLaunch() and also created new Raygeneration, closest hit, any hit and miss programs. When we start the optix sample and our rays using OptixTrace() inside our own Raygeneration program does not hit an object, then the sample boots up fine. But when our rays intersects with an object, then it does not boot up and we get the message “illegal instruction”.You mentioned hit records in your previous reply, we suspect this might be the problem. How do you mean we should initialize these? We have added the following code related to hit records in the void Device::createHitGroupRecords() function:\\n\\nScreenshot from 2021-11-19 18-05-20900×59 16.4 KB\\nKind regards!I do not quite understand though why we should rewrite the material system. The one already existing should be sufficient since, in our case, we just want all materials to have the same properties, just different values on the properties. Like permittivity, permeability etc. And these values later on control how the rays will interact with the objects.Well, I was expecting that you’d need to implement some other distributions than the Lambert, Specular and GGX distributions. Those are implementing specific reflection and transmission behaviors with an index of refraction and the renderer handles absorption in homogeneous media.We have added the following code related to hit records in the void Device::createHitGroupRecords() functionThat’s not enough. If you implemented your own OptixPipeline, then you also need to build your own SBT with the program headers you got from your own program groups and everything with optixSbtRecordPackHeader() needs to be duplicated as well.\\nMeans you need to duplicate the whole Device::createHitGroupRecords() function and everything referenced in there to a version which only uses your new program groups and a separate SBT.\\nYou must not use these hit groups record headers in your own SBT:\\nhttps://github.com/NVIDIA/OptiX_Apps/blob/master/apps/rtigo3/src/Device.cpp#L863That’s why I said you should copy the whole application and replace these things, which would be simpler than bolting on a separate pipeline to the existing application. I was describing how to make the whole simulation a separate application.Powered by Discourse, best viewed with JavaScript enabled',\n",
       " 'create-a-module-with-payload-size-less-than-the-max-size-used-in-the-ptx': 'I try to create a module from a ptx which includes many kernels for multiple pipelines. Each pipeline’s requirement for payload/attribute size are different. In this case the current OptiX seem not to allow me to create a module with payload/attribute size smaller than the maximum size used in the ptx even if I don’t use a kernel with the maximum size in the final pipeline.\\nThe error is:\\nError: Requested 4 payload values in optixSetPayload but only 3 are configured in the pipelineIs there any way to avoid this restriction?This is maybe rather a potential request than a question.\\nIf so my request will be:\\nIs it possible to defer the above error evaluation to when linking a pipeline instead of when creating a module?\\nOr is there some reason that that evaluation should be here?\\nIt seems possible to generate multiple pipelines with different payload/attribute size from single DXIL library with DXR (while I’m not sure how correspondence between OptiX’ and DXR’ objects is.)Thanks,Interesting, I wasn’t aware of that check, but I also never put programs with different pipeline compile options into the same module.It makes sense because that prevents including programs using more payload or attribute registers into a pipeline which doesn’t support that number. I’ll check with engineering what’s the exact reason.I don’t think that the code generation for the individual modules with the different compile options will be moved out to the pipeline link step. That would break your control over what happens when in this explicit API.The reasonable workaround and generally recommended approach inside the application would be to separate the code affected by the different pipeline compile options into separate modules.\\nThe individual modules for pipelines with different OptixPipelineCompileOptions will need to call optixModuleCreateFromPTX for each of the settings anyway and then it makes sense to make the CUDA source code as small as possible and not include programs which aren’t used inside a pipeline.Initial module compilation with cold OptiX program cache is expected to be faster when not having many small functions in a huge file. It makes sense to split the modules into such logical partitions alone for that reason.Thanks for the quick reply and some reasoning!Powered by Discourse, best viewed with JavaScript enabled',\n",
       " 'where-can-i-find-libnvoptix-so-1': 'Where can I find libnvoptix.so.1? OptiX SDK 7.6.0 seems to depend on it, but I can’t find libnvoptix.so.1 after running NVIDIA-OptiX-SDK-7.6.0-linux64-x86_64-31894579.sh.Thanks.Hi @yyan2,All files you need to run OptiX are part of the Nvidia display driver. Which version of the driver do you have installed? Note OptiX 7.6 requires a driver numbered 520 or higher.–\\nDavid.I’m using Driver Version 470.141.03.\\nDoes it have libnvoptix.so.1?\\nCan OptiX SDK 6.5.0 work with Driver Version 470.141.03?\\nThanks.For OptiX 7 and higher, all .so /.dll files you need are in the driver. For optiX 6.5 and earlier, there are a couple of .so / .dll files that ship with the SDK that you may need to place in an appropriate spot, or set your environment (e.g. LD_LIBRARY_PATH) so it can be found.You can find the driver requirements for each version of OptiX in the Release Notes on our download page, or packaged with each SDK. OptiX 6.5 requires a driver numbered 435 or newer, so yes 470 will work.–\\nDavid.I can’t find any files with file name libnvoptix* after running NVIDIA-OptiX-SDK-6.5.0-linux64.sh. I was able to follow the instructions in INSTALL-LINUX.txt and build executables such as optixHello. But after the build, I still can’t find libnvoptix.so.1, and optixHello fails because libnvoptix.so.1 cannot be found.Where exactly is libnvoptix.so.1 under the NVIDIA-OptiX-SDK-6.5.0-linux64 directory?Thanks.The file named libnvoptix.so.1 is always in the driver. If your app cannot find it, then it means the app isn’t talking to the driver. Which OS are you on, are you using WSL or docker or something, or a full native desktop linux?The .so file that is packaged with OptiX 6.5 is named something like optix.so.6.5.0 on Linux or optix.6.5.0.dll on Windows.–\\nDavid.Thanks. I used ssh to connect to a Linux machine. After switching to vnc, I don’t encounter the issue with libnvoptix.so.1.Now I am running into a different problem:$ ./optixHello\\nOptiX Error: \\'NVRTC Compilation failed.\\nnvrtc: error: invalid value for --gpu-architecture (-arch)\\n’I guess it is because OptiX SDK 6.5.0 is not officially supporting CUDA 11, as mentioned in this post: Problem with running OptiX 6.5 program. \"invalid value for --gpu-architecture\"In my environment\\n±----------------------------------------------------------------------------+\\n| NVIDIA-SMI 470.141.03   Driver Version: 470.141.03   CUDA Version: 11.4     |\\n|-------------------------------±---------------------±---------------------+I can build OptiX SDK 7.3.0 and run optixHello successfully.Powered by Discourse, best viewed with JavaScript enabled',\n",
       " 'ansel-responding-to-mouse-keyboard-even-when-not-in-focus': 'This is a regression from Studio driver version 462.59 to 471.11 (and continuing to current 472.12).In 462.59, when an Ansel session was started (the UI came up), it would hide the windows cursor and show its own cursor captive to the game window. If you Alt+Tabbed out or used the Windows key to wrestle focus away from the game window, Ansel would hide its own cursor and ignore keybaord and mouse input.In 471.11 and after, when an Ansel session is started, it still hides the windows cursor and shows its own cursor captive to the game window (same as before). However, now when removing focus from the game window, the Ansel cursor stays active and responds to all mouse and keyboard input, as well as the actual windows mouse. This leads to clicks on other monitors or keyboard strokes in other focused windows actually being responded to by the Ansel UI as well.This causes issues for us as we use Ansel to render images in the background while continuing to use the workstation to perform other tasks.This occurs in both UE4.26 and UE4.27 and is repoducible only in the driver versions stated above. Additionally, this is using the original Ansel implementation (in the Program Files\\\\NVIDIA Corporation\\\\Ansel\\\\ folder), since the new Ansel implementation does not allow rendering in non-fullscreen mode, and is unable to perform a background render when the game is not in focus.Hi @cameron.abt , welcome back to the forums!Thank you for bringing this to our attention. Sadly I don’t have an immediate fix for you, but I will bring this up with our developers. If they require more information, I will post here again or send you a direct message.In the meantime, you might want to check out the related community in our dedicated GeForce end-user forums, it is possible someone had the same issue and found a workaround.Cheers,\\nMarkusThanks Markus. I checked through that forum before posting and couldn’t find anything similar. I also tried to post there first, but for some reason the Create button wouldn’t enable after I was putting in the title… EDIT: Ok, turns out the UI isn’t great and the “Title your topic” section is a text input field while I was trying to put the topic title in the body section. I’ll cross post there too.Happy to hear from development though, I expect chances of a solution are higher here.CameronYou are welcome.One quick follow-up if you don’t mind, which version of GeForce Experience are you using?Thanks,\\nMarkusI re-installed Studio driver 462.59 including the GeForce Experience version that came with it, so I’m currently running 3.23.0.74 which is functional. (EDIT: it actually looks like 3.23.0.74 is the most recent version, not the version that would be installed alongside 462.59. I expect when I downgraded from 472.12 back to 462.59, it kept the newer version of GFE and didn’t install the older 3.22.0.32 version)When upgrading to the new (non-functional) drivers, I did a clean install and included the GeForce Experience install with those drivers - I don’t have those GFE versions offhand.Just to make my original post more clear, the Ansel version we’re using has a UI that looks like the attached image.Thanks again,\\nCameron\\nAnsel1282×752 182 KB\\nThanks for the additional picture, I will add that to the issue report.The GFE version did not change as far as I am aware, so that information also helps.MarkusPerfect, thanks. I figured I might as well add a video to show the issue too. This is taken after a clean install of Studio driver version 472.12, installed from the non-DCH version (472.12-desktop-win10-win11-64bit-international-nsd-whql.exe):Shared with DropboxHey Markus,Have you heard anything from the developers? I saw a new studio driver was released (472.39) and was hoping maybe that fixed the issue.Thanks,\\nCameronHi Cameron,I am sorry, I don’t have an update for you yet. But I am following the progress internally and will post here if there are news.Thanks for your patience!\\nMarkusHas this issue been resolved? I’m having the same thing happening to me as we speak.No, unfortunately it’s not fixed. We’ve been using an outdated driver this whole time and I’d love it if there was something else we could try.Hi guys and welcome @cjkyles to the NVIDIA developer forums.Unfortunately I cannot share any good news with you. Since the focus of Ansel support has shifted to GeForce Experience and the In-Game Overlay usage of it, the standalone version of Ansel, the one showing this behavior, had to be deprecated.But on the good side, now that UE5 is out, the Ansel plugin for this version needs to be refreshed, so you should send your feature requests through the GFE Feedback button to the team.If I understood it correctly this is mainly about two things:I can possibly relay that as well.Sorry again that I can’t give you a better solution, but I hope this information is still useful.@MarkusHoHo thank you for the help!Let me share a few notes about our workflow to give you an idea about our use case. We work in archviz and place many (~100) cameras in each project we work on. We use internally developed tools to do lower quality 4K 360 renderings, but rely on Ansel’s high-res tiling capabilities to produce final, 16K 360 and flat renderings, with ray-tracing turned up to 11. Because of this, it can take 15 minutes or so on our hardware to render out a single image.We’ve created a keyboard macro to tab and arrow into the proper mode and settings, start the render, wait for the cancel button to go away, then get out of the Ansel interface. It moves to the next camera automatically, then starts the next one up. We force the game to run in windowed mode at 1280x720 to ensure the macro works properly, the graphics driver doesn’t timeout, and the computer is more generally usable (when not using the macro, as detailed below).When using the macro the issue above isn’t a huge deal since there’s no other interaction. Sometimes we do have to render some images using Ansel manually though, and that’s where it becomes an issue when we let it render in the background and try to work on other tasks in the foreground (since all the clicks and mouse movement are duplicated in the Ansel window.If there is opportunity to get some requirements in front of the GFE Ansel team to work better with our workflow, that would be outstanding. In particular, we’d love to deprecate our use of a macro as it’s a very fragile implementation dependent on the UI layout and even button sizes.It sounds like there’s a different version of Ansel built into GFE now. Does that work the same way as the standalone Ansel version (in terms of plugin integration with UE4)? We use a config file to maintain consistent settings across the standalone version (applied to the registry), where are settings saved for the new version? Perhaps using this version can be the new plan.I’ll use the “Send Feedback” button in GFE to put this in front of that team as well.Thanks again,\\nCameronHello, I think we are friends on the same road. We are also using Ansel to deploy the automatic rendering process, and we also encounter the failure to effectively set the expected mode and automatically start rendering. Can you share your scheme and codeHello and welcome to the NVIDIA developer forums @1317489420 .I just realized I never thanked you for your input @cameron.abt ! I am so sorry!I forwarded the info as well, but overlooked to reply here.Otherwise no further information from our side I am afraid. And regarding your question:Does that work the same way as the standalone Ansel version (in terms of plugin integration with UE4)?It does not, no. There is no readily available plugin connection to UE from GFE.Thanks @MarkusHoHo for keeping us in the loop.Is there any type of API access to Ansel in GFE at all, outside of UE4? If that’s the case I could whip up some middleware to talk between the two for now. My fear is that Ansel is only meant to used by a gamer and a mouse.CameronI was hoping that by now we would have some more news on Ansel, but I have to disappoint, no new announcements or similar are available at this time.As for Ansel APIs, there are none. Ansel is an implementation on top of the nvcamera module, which in turn is not currently exposed through APIs.Powered by Discourse, best viewed with JavaScript enabled',\n",
       " 'distorted-video-with-the-sample-d3d11-encoder-but-the-cuda-encoder-works-fine': 'I downloaded the latest NVIDIA Codec SDK, and compiled all the sample C++ projects in Visual Studio 2015. I also obtained a raw YUV 4:2:0 video file to use as test.When using the D3D9 or D3D11 sample applications to encode the video, I get a strange and completely distorted black and white image. This image rapidly scrolls upwards. The amount of frames outputted by the application also does not match (the original video file is 300 frames, while the application only outputs 112).When using AppEncCuda however, everything works normally and the output is good.I’m running on Windows 10, GPU driver version 430.64. The GPU is an NVIDIA Quadro M2000M.Here are the Powershell commands I used, as well as the outputs of these two commands:Why could this be happening, and how do I fix it?Hi hugo.zink,AppEncD3D11 expects the input to be bgra format. Whereas, AppEncCuda can read multiple input formats,  with the default format as iyuv. I hope this explains the behavior you see.Thanks.Thanks, that explains it! I converted the video to raw bgra using ffmpeg, and the BGRA file works perfectly on the D3D9 and D3D11 encoders now.Powered by Discourse, best viewed with JavaScript enabled',\n",
       " 'matrix4x4-assignment-not-working': 'Hi,\\nI am trying to assign a 4x4 matrix in CUDA code inside my intersection program. The ptx file is successfully created but in host code rtProgramCreateFromPTXFile() fails with exception for this program. I have tried removing the matrix assignment part and it seems to work fine but now I have no way of assigning the matrix elements. setCol() and passing float none of these work. I am using CUDA 6 and Optix 3.6.2.Some actual code would have been helpful.If you do that as default initialization like this:\\nrtDeclareVariable(optix::Matrix4x4, mat44, , ) = optix::Matrix4x4(make_flot4(1,0,0,0), …);\\nthat won’t work with CUDA because this is a class and a module global variable.\\nThere is no CUDA kernel running which could do this initialization via a constructor at the time of the declaration.You must initialize this mat44 variable inside the host application by using the OptiX API calls rtVariableSetMatrix4x4fv() and the like.Inside a program as local variable, there should be no problem to do things like this:I was actually creating a pointer and then assigning via constructor inside my intersection program:\\nMatrix4x4* mat = new Matrix4x4();\\nI changed it to the way you showed above and it seems to work. Thanks.Btw, I do not need the matrix in the host code. so i am creating it locally in my intersection program.Please read the caveats section at the end of the OptiX Programming Guide.\\nDynamic allocations (malloc, free, new, delete) are not supported inside OptiX device code.Powered by Discourse, best viewed with JavaScript enabled',\n",
       " 'error-c1189-error-optixu-math-namespace-h-needs-nominmax-defined-on-windows': 'I met the problem when compiling the optix project,  If I want to solve the problem what should I do? Where and how should I define the NOMINMAX? My Optix version is 3.0.1I suggest you to include OptiX headers in the way they’re supposed to be used (that will make things way easier). If you’re trying to use rtu* APIs just include <optix_world.h> as in the traversal sample shipped with the SDKI suggest you to include OptiX headers in the way they’re supposed to be used (that will make things way easier).What does “in the way they’re supposed to be used” mean? I know optix.h header file is used for C API, and optix_world.h is used for C API and C++ API, I have read the program guide manual chapter 5 building with optix, but I still can solve the problemyou have to undefine the windows.h min and max macros using NOMINMAX.Right Click on your project in Visual Studios, select configuration properties->C/C+±>Preprocessor->Preprocessor DefinitionsThen Add NOMINMAXSorry max I thought you were including optixu_math_namespace.h directly. If you’re using it by first defining windows.h then mdkoa1’s suggestion is correct (thanks md!)Thank you, marknv and mdkoa1, you really help me a lot.I thought you were including optixu_math_namespace.h directly.What does the word “directly” imply? Is there a indirect way to include optixu_math_namespace.h? Although I have solved the problem as mdkoal said, I want to know something .Some headers are just not meant to be included directly into C/C++ programs since they might require other headers before or macros defined.You can include optixu_math_namespace.h in your source files “directly” (e.g. not through including another file).And the error message about NOMINMAX generally needs to be solved not by including a particular header file from optix, but by defining it as a compiler preprosessor definition like mdkoa1 describes.Including a header directly doesn’t always work but that’s true for optixu_math_namespace (I didn’t verify it for that header).Powered by Discourse, best viewed with JavaScript enabled',\n",
       " 'gpu-accelerated-physx-sph-fluids-are-not-working-on-some-machines': 'I’ve got 7 test machines all trying to use our application which includes GPU accelerated SPH particles.The code to setup the cuda context manager (see below) works on some machines, but not others. We’ll get a fatal error if it does not work.Here are the setups we’re using and the test results:CUDA Working? - OS - Graphics Card - Current Driver V - CUDA DLL Version\\nYes - Windows 8.1 (64-bit)    - GTX 670 MX    - 382.05 - 6.14.13.8205 (8.0.0)\\nYes - Windows 10 Pro (64-bit) - GTX 980 x2    - 382.05 - 6.14.13.8205 (8.0.0)\\nYes - Windows 10 ??? (64-bit) - GTX 770 x2    - 376.53 - 6.14.13.7653 (8.0.0)\\nNo  - Windows 10 Home (64-bit)- GTX 770 x2    - 376.53 - \\nNo  - Windows 10 Pro (64-bit) - Quadro K2200  - 382.05 - \\nNo  - Windows 10 Pro (64-bit) - GTX 980 Ti x2 - 382.05 - 6.14.13.8205 (8.0.0)\\nYes - Windows 10 ??? (64-bit) - GTX 1070      - 376.53 - 6.14.13.7653 (8.0.0)What could cause it to work on some machines and not others? Reinstalling video drivers did not do the trick. Are there any troubleshooting steps I should consider?Have you included PhysXDevice.dll (or x64 variant if building x64 config)? If its not present alongside the exe, you would be reliant on using the version installed with the PhysX system software, which can be optionally bypassed during driver installation.In addition, check in nvidia control panel that physx is not forced to cpu. If its forced to cpu,the cudaContextManager creation will return nullAh, adding the PhysXDevice.dll next to the .exe did the trick. Thanks!Powered by Discourse, best viewed with JavaScript enabled',\n",
       " 'call-multiple-closest-hit-programs-on-geometry': 'I am currently writing a simple OptiX program with a few Geometries. One Geometry is 50%transparant and emits some light. The other only emits light. Right now I need a closest hit program for each Geometry. I would however like to make one closest hit program for each Material.Which means: if I would like a Geometry to emit light, I simple add the light emitting material (=closest hit program). If I also want it to be transparant, I add that material too. Now this means that after my rtPotentialIntersection() I need this:However this doesn’t work. The variables changed in the closest hit program on index 0, are not seen in the closest hit program at index 1. So it only executes the second rule.Is there any way in which I can add multiple closest hit programs on a geometry and call them after each other?I would think the best approach would be to define two materials.  Your first closest hit program will be for the material that is 50% transparent and emits light, and your second closest hit program will be for the material that only emits light.  Then assign one material to each geometry, and don’t try to call rtReportIntersection() more than once in your intersection program.The variables changed in the closest hit program on index 0, are not seen in the closest hit program at index 1. So it only executes the second rule.You would need to transfer data between programs via the per-ray payload.You would need to transfer data between programs via the per-ray payload.The payload which I change in closest hit program 0, remains unchanged in closest hit program 1. Like every closest hit program start with a “clean” ray payload.Right now I “solved” the issue by adding an if-else clause in my closest hit program for every material property… Not very ideal though.Powered by Discourse, best viewed with JavaScript enabled',\n",
       " 'nvidia-video-codec-sdk-lossless': 'Hi,Is there someone who know what “lossless H264” mode means in the following matrix : NVIDIA VIDEO CODEC SDK | NVIDIA DeveloperI see in the table that YUV 4:2:0 and YUV 4:4:4 are supported for the H264 codec.\\nHowever what “H.264 (AVCHD) LOSSLESS” means ? Is it a direct RGB support instead of a YUV/NV12 format ?Does it exist any example code which illustrates the use of direct ARGB encoding ?Thanks in advance for your help,See you,\\nVincePretty sure the lossless just means “lossless” encode quality which isn’t usually very useful.Docs about the mode itself are sparse but there’s this in the API:There’s also the presetwhich I’d assume handles the various settings needed.No idea about ARGB input, I would assume you’re gonna need to convert it to YUV for the encoder.Hi VelaK,Thanks for your answer but we are not talking about the same thing.\\nIndeed, there exists a lossless mode as encoder preset but this is not the lossless format I’m targeting.Actually, the nvidia matrix seems to indicate that a lossless encoding format exists. YUV420 and YUV444 both imply a loss of information in 8bits/channel. YUV420 and YUV444 in 10bits/channel are not available for H264 so I’m wondering what is the H264 lossless encoding format.Considering the NVENC_DA-06209-001_v08.pdf file, which can be founded in the doc directory of the nvidia video codec SDK 7, H264 seems to accept ARGB input since Kepler family.I’m a looking for an example with this format because it could be a real lossless encoding format if the encoder does not implicitly convert images to NV12 nor YUV 8bits format.See you,\\nVinceAh, I see. I was looking around for lossless in the api and that’s all I saw.You could try modifying the example projects and change the NV_ENC_BUFFER_FORMAT to NV_ENC_BUFFER_FORMAT_ABGR10 / NV_ENC_BUFFER_FORMAT_ABGR / NV_ENC_BUFFER_FORMAT_ARGB / NV_ENC_BUFFER_FORMAT_ARGB10 and see if it works.AllocateIOBuffers() in the VideoEncoder.cpp example I’d imagine would be the place to start.I have used directly RGB input instead of YUV/NV12 format.\\nFirst you have to change NV_ENC_BUFFER_FORMAT to NV_ENC_BUFFER_FORMAT_ARGB and you have to copy RGB data buffer to EncodeBuffer input surface.Hi,Thank you both for your replies.ssharma01oct, do you find that the resulting H264 file is actually formated in ARGB ?\\nI made the test to and the file is recognized as a YUV420 high predictive format by ffprobe.\\nI suppose that the video framework makes an implicit conversion in order to recover a YUV format…See you,\\nVinceAnother question is, does the hardware decoder support another format other than the YUV420 for H264 ?See you,\\nVinceHello Vince,I am not sure about this if encoder internally converted the ARGB format to YUV. Because i did not make test for this.\\nI just use the ARGB input stream as a input buffer and directly copy (without any conversion)  on EncodeBuffer input surface and get the output in h264 format.If nvEncEncodePicture() internally converting the format ,i can’t say anything about it.H.264 video encoding, as per the standard, only supports YUV encoding. There is no way to directly “encode” RGB using H.264.The input format NV_ENC_BUFFER_FORMAT_ARGB is only supported for convenience. The driver internally converts the data into YUV420 and then encodes it using NVENC.Hi,Ok I see, thanks for your reply.VinceH.264 video encoding, as per the standard, only supports YUV encoding. There is no way to directly “encode” RGB using H.264.The input format NV_ENC_BUFFER_FORMAT_ARGB is only supported for convenience. The driver internally converts the data into YUV420 and then encodes it using NVENC.Actually it converts it internally to NV12.Hi,In our tests, lossless h264 is only truly lossless using yuv444 as input.We’d like to have lossless compression with RGB as input (to aid in testing our pipeline). I’m assuming this doesn’t currently work because as @Abhijit_Patait  mentioned, it internally converts RGB input to yuv420.Would it be possible to allow an internal lossless RGB → YUV (I guess yuv444?) conversion so that the entire pipeline could be lossless? Are there plans to allow for this? If not, can I make a feature request?I too am trying to support lossless encoding in UE4 and am finding it very difficult.I’m using NV_ENC_PRESET_LOSSLESS_HP_GUID, NV_ENC_H264_PROFILE_HIGH_444_GUID, NV_ENC_LEVEL_H264_52, a qpPrimeYZeroTransformBypassFlag of 1, qp of 0, rate control of constqp, a min bit rate of 100Mbit and a max bit rate of 150Mbit to feed WebRTC in chrome. These settings are accepted by nvEnc, but nothing is showing in Chrome. The bit rate rarely goes above 20MbitAny suggestions?Cheers\\nJohnPowered by Discourse, best viewed with JavaScript enabled',\n",
       " 'quadro-p1000-resolutions': 'I am thinking of buying a Quadro P1000 for use with 64b Windows 10 Pro.\\nCan this card support:\\n1 monitor at 2560x1440 and\\n2 monitors at 1680x1050\\nI can’t find anything about using different resolutions on the same card.\\nApologies if this is posted in the wrong place.  Seems like there is no Quadro Support Forum.Hi tfelton,Yes, Quadro P1000 supports both those resolutions.See link to datasheet below for info:239.75 KBHope that helps!,Ryan ParkPowered by Discourse, best viewed with JavaScript enabled',\n",
       " 'screen-of-notebook-turned-off-when-charger-plugged-in-or-outg': 'I am using Asus Tuf Dash F15 with windows 10 64bit OS. The notebook has GeForce RTX3050 graphics card.When I plug the charger in or out the screen turned off for 3-4 seconds. Sometimes the screen never turns on.Refresh rate of the screen changes from 60 (on battery) to 144 (when charging) Hz. Making some research people arguing the problem caused due to change in refresh rate of the screen. However, changing the refresh rate manually is not permanent solution and work only for first try. Then it starts change the rate again.Windows, Asus and Nvidia updates are all up to date.How to fix this problem?Hello,Welcome to the NVIDIA Developer forums! Your post is in the wrong category, this should be in the GPU Hardware category. I will move this to the proper forum. I would also consider posting your issue in the Asus forums.TomHi @iamzain16 ,Did you check if you have the most current BIOS installed?The switch between battery and cord power causes a switch between CPU and GPU graphics. I myself have an ASUS ROG Zephyrus Dual and see the same effect of a blank screen, but only for less than a second. You cannot influence this through Windows settings to default to one or the other. This is controlled on BIOS level.One thing you could check is in ASUS Armoury Crate to see if changes to power and performance settings have an influence on this behaviour.But if the screen does not come on at all after you plug in the power cord, that is very concerning in that it might be a HW defect. That is why I agree with Tom that you should contact ASUS and possibly consider an RMA.Nvidia Updates are all up to date … Also update BIOS and Chipset.Hello @makeupideaz , if you are really a new user, welcome to the forums and please describe your issue in a bit more detail, otherwise everyone i missing a bit of context.But in case this is @iamzain16 just with a second account, please see the end of my last comment. You should try for a repair or replacement by ASUS.Powered by Discourse, best viewed with JavaScript enabled',\n",
       " 'resolved-why-doesnt-positional-tracking-work-in-ios': 'When using the iOS client it seems to me that only rotation is supported. I made a Unity project using the basic (old, deprecated OpenVR plugin) and I get a stream so initially I was very happy that it seemed to work. But after adding features to the client (for image tracking etc) I have come to the conclusion that position changes on the iOs device doesn’t seem to be used/sent to the cloudxr server. I also tested ar_test.exe and some of the shelf VR software but even in these the translational movement of the iOS device seems not to be used. Is this the case or am I missing something? (and btw, I have the issues with shearing of the image when turning to portait that is reported in another thread)This was a case of usage error. Using ImageTrackingConfig for ARKit disables device tracking so keeping the config as it were works. (more info: Apple Developer Documentation)Powered by Discourse, best viewed with JavaScript enabled',\n",
       " 'video-stuttering-when-inserting-idr-frames': 'Greetings,I took AppEncD3D11 example from Video Coder SDK and integrated it into my app.\\nI’m using it to constantly capture screen video. However, I need to store only N last seconds of the video. Once encoded frames count gets too big, I erase some amount of old frames (simple FIFO - I store encoded frames in a cicrular buffer).As far as I understood, to make correct video, later frames should have correct reference frames. So for that I decided to insert IDR frame every second. And it made the job for me - now the video is always successfully decoded.But now I have other issue: the video started stuttering. I tried using GOP instead of manually inserting IDRs, but I’ve got exactly same issue.I wrote code, which generated 300 frames from different shades of red (based on frame index), and encoded 30 FPS video from these frames using original AppEncD3D11 example code. It worked fine and I’ve got what is expected (check file without_idr.h264).But this does not solve my task. So then I encoded 2 additional videos with only next code modifications:From both samples it feels like there are lost frames in the sequence. However, both samples contain the same amount of frames and generated from the same input.\\nCould someone please help me understand why simply inserting IDR frames influence correctness of the video, and how can I fix it?without_idr.h264 - [url]https://drive.google.com/open?id=1RtDw4Qz_2ZV9s1EIgrUUiX8JczV2s0_M[/url]\\nevery_30th_idr.h264 - [url]https://drive.google.com/open?id=1-pc3_quphe4pHYZe7P67V9E68myUbOLA[/url]\\nevery_frame_idr.h264 - [url]https://drive.google.com/open?id=1UYd7HZzYmOpObKUZCyiA-reUcJK4Yanr[/url]I just checked my own post to be sure the videos are uploaded correctly.\\nAnd for some reason they are working fine. Previously I checked them using VLC player. For through Google Drive they are looking absolutelly correct.\\nLooks like the issue was not in the code, but in the testing…Powered by Discourse, best viewed with JavaScript enabled',\n",
       " 'lots-of-the-vulkan-documentation-links-are-broken': 'On this page, specifically the “Vulkan Articles” section.https://developer.nvidia.com/vulkanConfirmed. None of the links in the Vulkan | NVIDIA Developer section work.Powered by Discourse, best viewed with JavaScript enabled',\n",
       " 'performance-of-multiple-pipelines-render-passes-vs-1-pipeline-implementing-render-algorithm': 'Hi, I was wondering what the performance difference is between having multiple pipelines to implement several steps of a rendering algorithm (render passes) vs having less pipelines (maybe even 1 pipeline) implementing these steps sequentially within the same pipeline. I can imagine there being some redundant data between multiple pipelines requiring a bit extra memory, but is there an actual speed difference between the 2 approaches, e.g. how much drawback does the amount of optixLaunch calls give?Hi @Chupp4,Your pipeline determines your total register count, and by extension, occupancy. The pipeline also contains information about your payload and its layout. This is one reason you might choose to take the extra effort to create multiple pipelines, because you might be able to reduce and/or optimize one of them more than the other, so you might be able to get it to run faster than if you put multiple render passes into a single pipeline. When you put multiple launch types into a single pipeline, the compiler will use the more conservative choice between all of them when it needs to figure out how to allocate registers (i.e., all passes in a single pipeline will consume the same number of registers from the hardware perspective, whether they actually use them or not).Other considerations include whether you will compile these pipelines at different rates, and whether you want to link them when any of your render passes changes or recompiles. Having multiple pipelines lets you keep the compilation and link more separated in addition to any performance benefits you might find.If all of your render passes use the same payload & layout, and approximately the same number of registers, then I don’t expect there is any performance advantage to having your passes in multiple pipelines or together in a single pipeline.–\\nDavid.This topic was automatically closed 14 days after the last reply. New replies are no longer allowed.Powered by Discourse, best viewed with JavaScript enabled',\n",
       " 'translate-and-rotate-a-wired-sphere-using-values-from-vbo': 'I’m using OpenCL kernel to calculate some vertices. After calculation, these vertices (x,y,z coordinates) are updated to the OpenCL-OpenGL shared buffer.I am using NVIDIA GrForce 820m.\\nI need to draw a wired sphere and translate it according to the values obtained in this shared buffer.The code is as follows:I tried addingBut they do not work. Also, please let me know how to make use ofwith VBO values. I need to draw a revolving sphere. Something like a moon revolving round the earth. The next position of the moon is obtained by the shared buffer. The sphere needs to be shifted by a position obtained dynamically from the VBO(openCL kernel update). Thanks in advancePowered by Discourse, best viewed with JavaScript enabled',\n",
       " 'geforce-direct-x-10-11-30-bit-color-support': 'Based on [url]Error | NVIDIA I think that I should be able to render 10-bit color using a Geforce card and Direct X (with 10-bit monitor of course). Can anyone confrim if this is true? Or am I miss interpreting the post?Powered by Discourse, best viewed with JavaScript enabled',\n",
       " 'new-opengl-3d-engine-and-ide': 'Open Source atm. Pls delete or inform me to delete if this post is off-topic for this forum, I wasn’t sure?- YouTube < video of Vivid3D Engine + IDE.https://github.com/AntDevUnity/Vivid3D < Github repo to download, open source. Updated several times a day with latest code.Requires Visual Studio 2017 With .Net component installed. Will automatically download several nuget packages. You may need to manually fix the irrKlang reference though.Powered by Discourse, best viewed with JavaScript enabled',\n",
       " 'ghost-collisions': 'HiI have a lot of collisions where no object is. The red bone (rigid static) is a PxTriangleMesh, the blue is a simple box (kinematic rigid dynamic). When I move the box near the bone, from some distance on, I get lots of collisions which are just somewhere.See the image:\\n\\n\\n\\nSorry, couldn’t upload an image here, and this tinypic was the first hit in google :-)What is going on here?I’m using PhysX 3.3.2 on VS2013.Thanks for you helpDid you maybe set a very large contact offset on the mesh and/or box shape (see PxShape::setContactOffset())?Powered by Discourse, best viewed with JavaScript enabled',\n",
       " 'what-is-the-nvenc-performance-of-gtx-1080-comparing-to-m60': 'I know that GTX 1080 can only do at most 2 streams at a time when using NvENC and M60 does not have this limitation. However, how if I just need to encode one stream at a time?How is the encoding performance of GTX1080 comparing to M60 if both of them are just encoding one stream or one video at a time?GTX 1080 = GP104 = “Pascal”, GPU Core clock ~ 1607 (by wikipedia or use GPU-Z)\\nTesla M60 = GM204 = “Second Gen Maxwell”, GPU Core clock ~ 1178 (by wikipedia or use GPU-Z)Download Video SDK package from NVIDIA VIDEO CODEC SDK | NVIDIA Developer then open included “NVENC_Application_Note.pdf” then read pages 7-8 then compute performance metrics yourself.Powered by Discourse, best viewed with JavaScript enabled',\n",
       " 'looking-for-vita-physx-2-8-4-libs': 'Hello,I am looking for VITA PhysX 2.8.4 (or as close as possible version) libs for a port we are doing. Can someone point me in the right direction?Thanks,RobHi,Sorry for the delay. Contact physxlicensing@nvidia.com. VITA is not a supported platform on 2.8.4, but some developers have purchased the source and ported it themselves.–MikePowered by Discourse, best viewed with JavaScript enabled',\n",
       " 'external-gpu-graphic-card-for-mac-mojave': 'I’d like to buy an eGPU/graphic card for my MacBook Pro to use for simple deep learning tasks. My setup is:MacBook Pro (15-inch, 2017)\\nGraphics: Radeon Pro 555 2 GB\\nIntel HD Graphics 630 1536 MB\\nVersion: Mojave 10.14.5I understand for Deep Learning (i.e. use of tensorflow-gpu) this is not currently supported for my Mac. On saying that, I was recommended to purchase the NVIDIA TITAN RTX or NVIDIA Quadro® GV100, but they’re quite pricey at 1000s of euros/dollars a piece. At first, I just want something to play around with, but if this is the only option for the Mac then I may have to invest (or alternatively even a laptop with integrated CUDA supported graphic card).What Nvidia eGPU/graphic card would you recommend for simple i.e. not mega large data sets for DL processing? There seems to be so many models to choose from that it’s not clear what would satisfy my needs. Would a GIGABYTE GeForce® GTX 1050 Ti OC 4GB suffice?Moving to GPU hardware forum.Powered by Discourse, best viewed with JavaScript enabled',\n",
       " 'exploring-optix-sdk-path-tracer-tex2d-causes-crash-at-start-up': 'Hello,I’m exploring OptiX 3.9.0’s SDK with CUDA 7.5. I opened the SDK sample called ‘path_tracer’ and attempted to add textures to it. However, it crashes at start up after adding a few lines of code loading and sampling a texture.The debug log reads some exceptions:\\n… [rethrow] at memory location …\\n… optix::shared::ValidationError at memory location …\\n… optix::Exception at memory location …As far as I understand I have caused a memory leak, but I would not know how.For a minimal working example I added four lines of code, for which I used the sample ‘cook’ as an example.\\nTwo lines are added to path_tracer.cpp. To the top (line 36):And just after setting the floor material in line 303:To path_tracer.cu I added a line to the variable declarations (line 7):And a line sampling the texture to the top of the diffuse() material program (line 174):I am curious to know what my mistake is. Thanks in advance to anyone trying to help me find it.My first guess would be that you’ve attached the texture only to the floor GeometryInstance but changed the closest hit program implementation which is used for all objects. Means only the floor would be able to resolve that texture sampler variable scope, all other GeometryInstances would have an invalid sampler access.If you declare the “floor_texture” sampler variable at the OptiX context scope it’ll be accessible everywhere.\\nI would recommend to move that variable from the GeometryInstance to the Material instead. Then you could build a scene with different materials for the floor and the other objects later.\\nSee the OptiX Programming Guide Chapter 2.3 and 4.1.5 for how the variable scopes are resolved.If you’re using OptiX 3.9.0 right now, it’s worth updating to OptiX 3.9.1.Your first guess was completely correct. Thank you for the explanation and advice.Also I have updated to OptiX 3.9.1. May I ask why it is that you didn’t advice 4.0.0?\\nI tested that too, but the precompiled samples wouldn’t start, reporting that no supported graphics card was found.Because I could be sure that OptiX 3.9.1 would work if you’re running 3.9.0 successfully, and it’s faster and more robust than 3.9.0.\\nI assume you have a Fermi GPU architecture board if 4.0.0 doesn’t work, because that architecture isn’t supported in 4.0 anymore.Powered by Discourse, best viewed with JavaScript enabled',\n",
       " 'fedora-32-workstation-using-nvidia-quadro-p5000-mobile-blank-external-monitor': 'The laptop is Dell Precision 7720. Previously using Fedora 30 with docking and 3 external monitors without any problem. After upgrade to Fedora 32, the external monitor just blank. xrandr error “xrandr: Configure crtc 4 failed” . Already try to change the crtc but the same error return. I’ve been battling with this since April. Any help will be appreciated.Some errors highlight :\\nAug 13 10:18:46 FCW32 /usr/libexec/gdm-x-session[2939]: randr: falling back to unsynchronized pixmap sharing\\nAug 13 10:18:46 FCW32 /usr/libexec/gdm-x-session[2939]: (EE) modeset(G0): failed to set mode: No space left on devicenvidia-bug-file : nvidia-bug-report.log (3.9 MB)After a month battling with this problem, finally I’ve solved the mystery.\\nI had to uninstall everything related with nvidia driver including the conf files from ;reboot the machine, then install the following driver/tools :Install the long live driver from nvidia 450.66 :Get it here - https://forums.developer.nvidia.com/t/linux-solaris-and-freebsd-driver-450-66-long-lived-branch-release/147366Edit the /etc/X11/xorg.conf : (I have 3 external monitor + 1 laptop display)Reboot, and… still not running… :(\\nI had to manually do :Then I got all display running…\\nI’m still figuring out why I had to manually do the modprobe… why don’t the driver do it for me…\\nPlease if somebody have the answer… do point me out…Hi BTM,I have been trying to recreate issue but no luck so far. I just wanted to check with you if issue is reproducible with single monitor connected.Hi @amritsIt’s because of mixed driver between  nvidia-driver* and xorg-x11-drv-nvidia*…\\nI’m now using kernel 5.7.17-200.fc32.x86_64 with only xorg-x11-drv-nvidia* package with bumblebee.\\nIt’s run okay now. But sometimes PRIME display (eDP1) did not shows up… only the external monitor is run.\\nNvidia-Monitor887×920 148 KB\\nIs such problem occurs when you connect only one external display ?Yup, the same…\\n\\nNvidia-Monitor-1797×766 128 KB\\nThis is using only one external monitor. I lose PRIME monitor eDP1…\\ncurrent nvidia-bug-report - nvidia-bug-report.log.gz (1.6 MB)Ok, it looks like sometimes you are loosing the prime monitor and sometimes you are loosing the external monitor.\\nPlease confirm if my understanding is correct.Hi @amrits,\\nYes, I can confirm that. The other thing is, If I’m only using the laptop eDP1 display, I can access Nvidia GPU for some period time. After that, the Nvidia GPU just wouldn’t respond.Hi @amrits,I noticed that from time to time, my Nvidia Quadro P5000 just went dead with error message :Sep 20 21:13:40 fcw32 kernel: pcieport 0000:00:01.0: AER: Root Port link has been reset\\nSep 20 21:13:40 fcw32 kernel: pcieport 0000:00:01.0: AER: device recovery successful\\nSep 20 21:13:40 fcw32 kernel: pcieport 0000:00:01.0: AER: Multiple Uncorrected (Fatal) error received: 0000:01:00.1\\nSep 20 21:13:40 fcw32 kernel: pcieport 0000:00:01.0: AER: PCIe Bus Error: severity=Uncorrected (Fatal), type=Transaction Layer, (Requester ID)\\nSep 20 21:13:40 fcw32 kernel: pcieport 0000:00:01.0: AER:   device [8086:1901] error status/mask=00004000/00000000\\nSep 20 21:13:40 fcw32 kernel: pcieport 0000:00:01.0: AER:    [14] CmpltTO                (First)\\nSep 20 21:13:40 fcw32 kernel: snd_hda_intel 0000:01:00.1: AER: PCIe Bus Error: severity=Uncorrected (Fatal), type=Inaccessible, (Unregistered Agent ID)\\nSep 20 21:13:40 fcw32 kernel: snd_hda_intel 0000:01:00.1: AER:   Error of this Agent is reported first\\nSep 20 21:13:40 fcw32 kernel: nvidia 0000:01:00.0: AER: can’t recover (no error_detected callback)\\nSep 20 21:13:40 fcw32 kernel: snd_hda_intel 0000:01:00.1: AER: can’t recover (no error_detected callback)\\nSep 20 21:13:40 fcw32 kernel: nvidia 0000:01:00.0: can’t change power state from D0 to D3hot (config space inaccessible)\\nSep 20 21:13:41 fcw32 kernel: pcieport 0000:00:01.0: AER: Root Port link has been reset\\nSep 20 21:13:41 fcw32 kernel: pcieport 0000:00:01.0: AER: device recovery successful\\nSep 20 21:13:41 fcw32 kernel: nvidia 0000:01:00.0: AER: can’t recover (no error_detected callback)\\nSep 20 21:13:41 fcw32 kernel: snd_hda_intel 0000:01:00.1: AER: can’t recover (no error_detected callback)can’t do modprobe as the system will freeze…\\nI had to reboot to make it active again…\\nIs it a symptom where the GPU is not well?syslog attached - nvidia-dead.log (84.7 KB)Hi btm,\\nThis looks like to be a different issue than the original one reported.\\nAre you doing any specific steps which create such error messages ?Hi @amrits,The steps is pretty straight forward.\\nRun this command$optirun glxspheres64it runs well…\\nthen kill it by [ctrl] + [c]…\\nthen restart the command… and gives an error message saying that there is no secondary GPU…\\nI’m checking the /var/log/messages and found this error.\\nrestarting the nvidia by$modprobe nvidia-drmmakes my laptop freeze… had to press the power button for 5 seconds to reboot. This has happened mostly without an external monitor attached…\\nI don’t know if this is related to Nvidia driver package or the Fedora 32 OS itself or my GPU is nearly dead…Hi Btm,\\nPlease check with latest released driver if issue is any longer reproducible.Hi @amrits ,I’ve upgraded the Nvidia Installer to version 465.31\\nIt’s solved the problems. I’ve also upgraded the OS to Fedora 34.nvidia-installer:  version 465.31\\nThe NVIDIA Software Installer for Unix/Linux.This program is used to install, upgrade and uninstall The NVIDIA Accelerated Graphics Driver Set for Linux-x86_64.Linux FCW32 5.12.11-300.fc34.x86_64 #1 SMP Wed Jun 16 15:47:58 UTC 2021 x86_64 x86_64 x86_64 GNU/LinuxPowered by Discourse, best viewed with JavaScript enabled',\n",
       " 'swapchain-display-only-image-0': 'I’m trying to find a good way to use swapchain for CAD-like scenarios. Rendering is performed not in a loop frame after frame but only if something is changed.My code is here: Bitbucket See ‘example’ folder for MSVC 2015 project\\nPre-built windows binary can be downloaded from here: Bitbucket | Git solution for teams using JiraI create VK_PRESENT_MODE_FIFO_KHR swapchain with 2 images, basic vsync mode.In this demo app frame is re-rendered on each left mouse button click. Each time with a different color, color and swapchain image are logged to console.And on my NVIDIA GT 740M with 368.22 drivers on Win10 x64 I have 2 problems with this code:Only rendering into swapchain image #0 is visible. Looks like vkQueuePresentKHR with image #1 is ignored by driver.\\nThis ignoring of #1 image occurs also in game-like rendering loop but it’s much less noticeable.If window is left without updates for some time (around 30 seconds) index buffer (in device local memory) gets corrupted\\nExternal ImageIs it me doing something wrong or is it a driver bug?Powered by Discourse, best viewed with JavaScript enabled',\n",
       " 'iray-luminosity-calculation-change-breaks-a-lot-of-assets-in-daz-studio': 'I am posting this here because Iray and OptiX teams are in my understanding dependent and thus work closely together so I hope this will be read by NVIDIA developers and managers who decided to make those changes, rethink their decisions, and provide workarounds.Iray versions after version 334300.9558 have two significant changes which were made in an effort to make the raytracing more physically correct.One of them is thin film calculations and I won’t get into that, because DAZ team already has a partial workaround, though the previous color result cannot be restored completely in more complex scenarios.The other change is about luminosity calculation change and how it now affects emissive surfaces – it is much bigger change and it breaks a lot of existing 3D assets in many stores not just DAZ3D store, not to mention all the scene files already saved using those affected assets.To understand why it is a breaking change, you need to understand a bit of history. Iray was (and in my opinion still is despite the improvements that made use of light portals obsolete) not really great when it comes to interior lighting.To work around that, people using DAZ Studio and Iray used a simple trick – they used something called a Ghost Light Kit. Those are essentially 2D planes with emissive surface and cutout opacity set to a really low value of 0.0000001.Since cutout opacity up to and including the Iray version 334300.9558 wasn’t affecting luminosity, that esentially makes ghost lights invisible to primary rays AND reflections (they are not visible in mirrors and on glossy surfaces) while providing nice diffuse light source of arbitrary size which produces soft light casts no shadows, and which can be placed anywhere in your indoor scene – directly under ceiling, even over the floor or on a wall to get the desired light level and to speed up the render without having to fiddle too much with tonemapping or to waste an exorbitant amount of time setting up dozens of individual point and/or spot lights which don’t even work if sun/sky lighting mode is used while emissive surfaces always do regardless of the scene lighting mode.Anyone who ever saw a gaffer on a movie set can understand the value of such a light source – gaffers go to great lengths to eliminate unwanted shadows and reflections when lighting a movie scene even if that is not how the scene would look naturally.The Iray change causing the problem with ghost lights is that now luminosity is multiplied with cutout opacity and the result is that they no longer emit light until cutout opacity is restored. There is a new advanced Iray node property that allows making an emissive surface invisible to primary rays but they are still visible in reflections and on glossy surfaces so that doesn’t help at all because the main advantage of those ghost lights was that they were invisible and didn’t cast shadows.There are a lot of 3D assets (mainly environments, but also Sci-Fi clothing and weapon props, candle props, fire props) that used emissive surfaces with lowered opacity. An example off the top of my head would be an apartment with lightbulbs in light fixture in every room. They are modelled with glass ball which is transparent and emissive (at a lower level than the filament) and now they don’t emit any light. Another example are windows which have loadable emissive presets for day and night which no longer work because of the luminosity = luminosity x opacity change. Clothing with glowing parts such as armors and spacesuits are also affected as are weapon props, displays, etc.Now I understand the desire to get raytracing as physically correct as possible, however I don’t understand blatant disregard for the consequences of such a change on the 3D asset ecosystem centered around rendering applications using Iray.If it is really impossible to keep the old behavior for backward compatibilty with all the affected 3D assets and existing scenes, I would urge Iray team to at least consider adding a new light source type – an invisible to primary rays and reflections emissive surface with 2D plane geometry which doesn’t cast shadows so we can at least have a built-in replacement for ghost lights going forward.Iray and OptiX teams are in my understanding dependent and thus work closely together so I hope this will be read by NVIDIA developers and managers who decided to make those changes, rethink their decisions, and provide workarounds.Hi @igor.levicki, I need to reiterate that the OptiX forum is just not the place for either DAZ or Iray support. Nobody on the Iray team is monitoring this forum on a regular basis, because this is not a channel for Iray-specific concerns.The Iray team, in turn, is not the place for DAZ support. It really would be best to take up your concerns with the features you want to see in DAZ, as well as with how DAZ is supporting and managing changes to the software it relies on, directly with the DAZ team, so that DAZ is aware of what you need and can request whatever support they need from the Iray team.–\\nDavid.@dhart I am well aware that this is not the place, but as far as I am aware Iray has no dedicated forum for end-user feedback.Of course I did raise my concerns with DAZ first and they did work with NVIDIA on a “workaround” for ghost lights (advanced property node I mentioned) but that is not good enough for the reasons I stated above.DAZ moderators are deleting complaints from the forum. I even emailed Iray program manager Alexander Fuchs directly on November 29th. I still haven’t got any response.DAZ is aware of what we need – we need stuff that worked to keep working so we don’t have to spend hours trying to figure out how to fix broken assets (if they can be fixed at all) and redo all our saved scenes using said assets.DAZ replies are along the lines of “this is an NVIDIA Iray change and we can do nothing more about it than we already did” (which as I explained above isn’t a lot).What I am getting out of all this is that they are washing their hands, and now NVIDIA is washing their hands as well. So we have two companies playing the blame game, while the paying customers of both companies are stuck in the middle as a collateral damage.I hope you can now understand my frustration and the reason I posted this here.Please don’t mistake my comment for lack of sympathy to your issues. I’m sorry to hear that DAZ support is not listening or taking responsibility for managing upstream changes in DAZ Studio. It’s just that this issue is 100% unrelated to OptiX. NVIDIA hasn’t responded, neither my reply nor Mr. Fuchs can be construed as washing our hands. It’s entirely possible your email will be answered in time. Emailing the PM directly during the holidays in Germany was not likely to get a fast response, many people there are still on their holiday break.This seems like an ongoing issue you’ve been having with DAZ for several years now, and that you need them to establish a stronger support channel, that will listen to your requests without deleting them, and that they will forward your requests to the Iray team rather than leave you on your own to do so?–\\nDavid.@dhart I agree with you that issue is 100% unrelated to OptiX, I simply have no other venue to raise my concerns about what Iray team is doing and how it affects some applications that use it.I myself am on a holiday break, but I wasn’t aware that holidays in Germany start at the end of November.Yes this is an ongoing issue, and the reason seems to be simple.As far as I am aware, DAZ is earning all their revenue through commissions from their 3D asset store, and DAZ Studio application itself can be downloaded and used for free just by signing up for a DAZ account.That in theory means DAZ should care a lot whether something in DAZ Studio is broken or not, but in practice it means that DAZ can use the excuse “it’s free, there are no warranties” instead of actually solving complex issues – in other words, they can focus on working on, and supporting, what they think will maintain or increase their revenue stream from the 3D asset store and ignore everything else.One proof of that is that their support flat out refuses to acknowledge any application issues with 3D assets not bought in their own 3D asset store.It doesn’t help that a lot of people using DAZ Studio aren’t professionals and thus seem to have very low expectations when it comes to software support, despite indirectly paying for the application development through their 3D asset purchases.In my opinion, DAZ’s behavior described above can only negatively affect NVIDIA’s brand through association.Thanks for listening! If you are able to, please feel free to pass this feedback internally.Apparently there is now a new solution in Daz for ghost lights (plus it also exposes the brandnew guided sampling of Iray to speed up indoor scenes!): Daz Studio 4.20 | Volumetric Clouds, Smoke, and Fire Effects - Daz 3D Blog@CarstenWaechter Sadly, the “new solution” not only isn’t new, but it is not really solving the main problem which is the lack of a fully invisible emissive light source in Iray.Since I already tested DAZ Studio 4.20 let me tell you what is the “new solution”.One “new solution” which was available since 4.16 beta is a new advanced iray node property which basically says “this surface is invisible to primary rays”. But light emitted by that surface is still visible in reflections so it’s useless, and the reason I say it’s not new is because there was already a “Render Emitter” setting on built-in Iray light sources that did the same thing (hide geometry) with exact same limitations (light visible in reflections).The other “new solution” is to adjust the luminosity on ghost lights to compensate for opacity multiplication change in Iray – since opacity required for a surface to be invisible is 0.0000001, to get the equivalent luminosity of 100 kcd/m^2 from previous Iray version you now need to use 1000000000 kcd/m^2. Because IEEE754 double precision math accumulates errors fast when you add/subtract/multiply/divide very large and very small values (think other light sources in the scene), doing that results in ghost light surface having quite a few fully white dots regardless of how well lit the scene is or what your exposure values are and neither firefly filter nor AI denoising can get rid of them.So, both “solutions” fail to address the problem we all have with Iray – the lack of a truly invisible emissive light source.Ghost Light Kit has solved that problem long time ago – those emissive surface based lights were invisible both to primary rays and in reflections and didn’t cast shadows so you could put them anywhere in a cramped indoors scene to get any visual effect you wanted without them being picked up by the camera. If you ever watched what gaffer does on a movie set, you would know that lighting a movie scene isn’t about accurate simulation of light physics – it’s exactly the opposite (i.e. you want to remove as much unwanted shadows and reflections as possible).We understand that Iray team considered not multiplying the opacity a bug and fixed it, and we are not asking for that change to be fully reversed, but we are sure that a compromise is possible.Simplest solution would be to add an advanced iray node property that can disable the opacity multiplication for a surface (basically disable the bugfix). That way, the new default would be an accurate light simulation like it was supposed to be, but it would allow us to set it on ghost light surfaces so they can continue to work as they did before.Keep in mind that even that solution wouldn’t be completely painless for DAZ customers – we would still have to modify all existing scenes where we used ghost lights to add the new property.Another solution would be to add a new fully invisible light source to Iray, but that would require much more work from NVIDIA to implement, then from DAZ to expose that functionality in their product, and finally from customers to replace all ghost lights with those new lights.However, the worst thing NVIDIA can do is to claim that only real-life light behavior is acceptable in a raytracer. If photographers and gaffers of this world have so many tools at their disposal that allow them to “cheat” light physics in order to realize their artistic vision why should work in a 3D environment be any different?@igor.levicki Are Light Path Expressions fully exposed in Daz?\\nCause this way one could filter out certain interactions of lightsources, which would also result in kinda ghostlights in some cases.@CarstenWaechter They are exposed under Render Settings → Advanced → Canvases.Once you enable Canvases you can select from predefined types – Beauty, Diffuse, Specular, Glossy, Emission, Environment Lighting, Alpha, Normal, Depth, Distance, Material ID, Shadow (Interactive only), Ambient Occlusion (Interactive only), or select LPE as type and enter a formula, but I am not sure to what extent expressions are supported. There is also an alpha checkbox and an option to create node list from selection and assign it to canvas.However, even if LPEs are fully supported, using them as a workaround for ghost lights would just add unnecessary complexity for something that should in my opinion be a built-in Iray feature (and it probably even wouldn’t be the first time a bug got turned into a feature).This opacity trick wasn’t used just for ghost lights – many indoor environments used it with black cutout opacity map for light sources (lightbulbs, lamps, glowing parts of furniture, etc), and many Sci-Fi items also used it for glowing parts.Finding and fixing all that in existing scenes is going to be a massive amount of work for customers even without having to deal with LPE complexity, multiple render passes, and compositing, not to mention that not many of them even know what LPEs are, much less how to use them.On the other hand, adding an advanced iray node property that disables opacity multiplication bugfix (per surface) would allow for writing a simple script which would enumerate all scene nodes, find surfaces with emission color > 0 and cutout opacity < 1, and then add and enable this new property for those surfaces. It would still require some manual checking here and there, but it would be way easier for customers to deal with, and it also wouldn’t require 3D asset vendors to update the assets they are selling.The only question that remains is whether such property could be added without negatively impacting Iray performance and whether Iray team would be willing to consider it.If you have any better ideas on how to solve this problem going forward we’d like to hear them, currently many people are holding back on upgrading because of this and thus cannot benefit from your hard work on bug fixing and implementing cool new features such as VDB and guided sampling.@CarstenWaechterJust so you guys know what Daz 3D forum admininstrator (presumably a Daz 3D employee) is saying about this issue:\\ndazmodpost1335×728 134 KB\\nAre people really supposed to believe Daz 3D developers don’t read NVIDIA Iray changelog before integrating new Iray versions into Daz Studio?Are you guys really OK with Daz 3D blaming NVIDIA for the workflow disruption to their customers, and claiming they are totally powerless to even attempt to work it out with you?I sincerely hope not.Powered by Discourse, best viewed with JavaScript enabled',\n",
       " 'onccdcontactmodify-danger': 'Hello NVidia,I am trying to use the callback onCCDContactModify, and I stumbled upon a problematic crash / corruption that I would like if you can shed some light on.The callback itself provides a list of PxContactModifyPair and a count, and for each one, we get a PxContactSet. Now we can tweak the contact with function such as: void setInvMassScale0(const PxReal scale).\\nHowever, if we look inside this function, it is doing some unsafe memory access like this:When a normal (non-ccd) contact callback occurs, everything is fine, because before calling the callback, this is being calculated:However, in the case of a CCD contact, in file PxsCCD.cpp inside function virtual void runInternal(), the contact point is created directly on the stack:with no way for the reinterpret cast memory offset subtraction above to work properly, which results in a random crash soon after or general memory stack corruption.So far, the only “legal” thing I’ve found that I can do when in a CCD contact callback is this:Can you help me understand my issue?Thanks for reporting. It will be fixed in an upcoming patch to PhysX 3.4Powered by Discourse, best viewed with JavaScript enabled',\n",
       " 'how-to-locate-a-rigid-actor-quickly-in-physx-visual-debugger': 'It’s hard to locate a  rigid actor in PhysX Visual Debugger V3.2016.04.\\nI’m working with ue4 v4.18, I used pvd connect/disconnect command to visualize my game physics scene, my game scene is very large, eg.10km * 10km. PVD shows PxRigidStatic actor and PxDynamic actor in left outline windows successfully, but I cannot locate my character or any other rigidbody quickly in pvd scene window. I have tried to use Camera->LookAtPoint option, but it doesn’t work, I entered a PxRigidStatic globalpose postion in Look at point window, but the camera go some wierd place.\\nAny suggestion would help! Thanks!!!Powered by Discourse, best viewed with JavaScript enabled',\n",
       " 'codeworks-installation-fails-on-ubuntu': 'Hi,To build Apk’s of the sample application from the unreal engine, it needs to install CodeWorks for Android. So building of Apk and deploying on device needs this CodeWorks to be installed.Presently i’m facing issues while installing this CodeWork. It fails after the download of manifest file and indicating an error at the bottom of the component manager. I’m trying to figure out the reason for this error. There is no information regarding the reason for the error.Can someone please help me to understand and resolve what could be the issue.Show following logs in CodeWorks.log in NVPACK folderE: fopen failed, errno = 2\\nE: Can’t write local.cfg\\nE: HTTP response code said errorThanks in Advance.Thanks,\\nHarinath\\n\\nWindowWhileDownloading.png1366×768 131 KB\\n\\n\\nError.png1366×768 104 KB\\nPowered by Discourse, best viewed with JavaScript enabled',\n",
       " 'nvs-810-2nd-gpu-core-screens-out-of-alignment': 'Hi all,We have a setup running 7 screens of one NVS 810, facing some problems which we can’t find a solution for.3 out of 7 screens are out of alignment (please see image here: Dropbox - Error).While we managed to fake fix this somehow by changing the Windows display layout scale - basically cropping massively from the edges - this miss alignment is still visible. The fake fix resets at logout or restart, which leads to someone on-site finding a different scaling percentage to hide this.Even stranger is that this doesn’t happen at the login screen, but only after once we logged in.Another observation is that the 3 screens with the bug are on the 2nd GPU, versus the first 4 screens which work well and are on the 1st GPU. Can this be a Mosaic issue?Windows 10 64bit is up to date, Driver version: 376.84.Any help is highly appreciated,\\nThanks,Hi work6y95b,How are you connecting to the displays?Are you using a native display port, cabled extenders, or converting from display port to HDMI?This type of shift happens typically because display timing sent to display is not native to the display. This can occur when:There may be other reasons for the shift, but please confirm first whether the signal path and the displays are all identical.Hope that helps!Powered by Discourse, best viewed with JavaScript enabled',\n",
       " 'driver-365-19-on-windows-10-x64-with-optix-3-9-cuda-7-5-on-gtx-750-crash': 'The driver 365.19 crashes with CUDA memory errors. The driver 365.10 works great!Is this a known issue in the 365.19 driver?PS: I can’t test OptiX 4.0 Beta because there’s still no DirectX 11 interop.Could you please add some more details about ways to reproduce this and the exact error message you’re seeing?\\nFor example, is this happening with any of the OptiX 3.9.0 SDK pre-built sample executables?Thanks for your reply.It doesn’t happen with the pre-built samples, it happens with my code when I drop I new model on the window.Initial it shows a ground plane and that works fine. When I drop a model (a .obj, .stl, whatever) this happens:create a Geometry\\ncreate a GeometryInstance\\nset Geometry to the GeometryInstance\\nadd the GeometryInstance to a GeometryGroup\\nadd Material to the GeometryInstanceGeometryGroup and the Acceleration (“Trbvh”, “Bvh”) are already created.The errors I get are:Exception thrown at 0x00007FFC0E8A1F28 in ConsoleApplication3.exe: Microsoft C++ exception: optix::shared::CudaError at memory location 0x00000064778FE8B0.\\nException thrown at 0x00007FFC0E8A1F28 in ConsoleApplication3.exe: Microsoft C++ exception: optix::Exception at memory location 0x00000064778FF7F0.\\nUnknown error (Details: Function “_rtContextLaunch2D” caught exception: Encountered a CUDA error: result returned (714): Device stack error (stack error or exceeded stack size limit), [6619204])I have the stack size normally set on 5000, I’ve tried 10000 but it still crash.With previous drivers there was no issue. I’m in the process of simplifying my code and trying to find where things goes wrong.It seems something goes wrong with the buffer created with m_context->createBufferFromD3D11Resource(RT_BUFFER_OUTPUT, pOptiXBuffer);When the program closes it has problems destroying that:Exception thrown at 0x00007FFC0E8A1F28 in ConsoleApplication3.exe: Microsoft C++ exception: optix::shared::CudaError at memory location 0x00000064778FF710.\\nException thrown at 0x00007FFC0E8A1F28 in ConsoleApplication3.exe: Microsoft C++ exception: optix::shared::CudaError at memory location 0x00000064778FF660.\\nException thrown at 0x00007FFC0E8A1F28 in ConsoleApplication3.exe: Microsoft C++ exception: optix::Exception at memory location 0x00000064778FFA10.\\nUnhandled exception at 0x00007FFC0E8A1F28 in ConsoleApplication3.exe: Microsoft C++ exception: optix::Exception at memory location 0x00000064778FFA10.Just brainstorming, perhaps something about DirectX changed in the new driver?I wish I could test DirectX interop with OptiX 4.0.Thanks for the information.Are you just shutting down the OptiX context or are you manually destroying all OptiX resources?\\nDid you unregister the buffer before destroying it?If you can provide a minimal reproducer to reproduce this in-house that would help to investigate it.\\nWe normally need an OptiX API Capture trace in failing state from the application for analysis. The shorter, the better.\\nLink to how to do that here [url]https://devtalk.nvidia.com/default/topic/803116/?comment=4436953[/url]I’m not sure if that alone would capture the DirectX 11 app behavior as well. If this cannot be reproduced with an OAC trace alone, we would need a minimal application reproducing the issue.If everything else is correct and this is a CUDA or DirectX driver regression there is nothing you can do to solve this on your side. The pragmatic approach would be to stay on the driver which worked while the problem is fixed in a future driver based on your reproducer.Powered by Discourse, best viewed with JavaScript enabled',\n",
       " 'is-the-hevc-h265-aq-temporal-aq-spatial-aq-configuration-supported-on-tesla-p4': 'Is the hevc(h265) aq(temporal-aq/spatial-aq) configuration supported on tesla p4?\\nI want help ,please help me !thank you very much~~Powered by Discourse, best viewed with JavaScript enabled',\n",
       " 'life-cycle-assessment-lca-carbon-footprint-of-geforce-3090-rtx': 'Hello,I’m using a GeForce 3090 RTX for AI training, and I want to compare the carbon footprint with the Nvidia-jetson 4g equipment. Is there any Life Cycle Assessment carbon footprint in order to include fabrication, use and the end of the life of the machines ?Thank you by advance,\\nNicolasHi Nicolas,Thank you for the question, it is quite important to keep an eye on this kind of impact. But I am sorry, we do not provide that sort of data you are looking for. Tracking this as a fab-less company across all foundries and OEMs we are working with would be nearly impossible.But we do make sure that the companies which fabricate our chips and other NVIDIA Hardware adhere to our high ethical and ecological standards.Thanks!Powered by Discourse, best viewed with JavaScript enabled',\n",
       " 'instance-acceleration-structure-optix-7-1': 'I am trying to figure out how to build a TLAS properly.\\nUsing the samples that came with OptiX 7.1 and Ingo Wald’s Optix 7 samples, started with a triangle (just the BLAS that holds the geometry) and it works fine (moved the triangle sample of the SDK to Wald’s example framework).\\nNext I introduced a TLAS with one instance (that is the BLAS from before) and using that TLAS in the shader but I am not getting a single hit.\\nWhat am I not doing correctly?Instead of using the return value of the function above, I feed it to the TLAS creation function below and use its output handle in the shaders:For reference the shader code (it works just fine when the OptixTraversableHandle comes from the first function above:}Please have a look at my OptiX 7 applications https://github.com/NVIDIA/OptiX_Apps which are all using IAS with each instance holding a single GAS with triangles.You probably have not changed the OptixPipelineCompileOptions traversableGraphFlags to contain the flag OPTIX_TRAVERSABLE_GRAPH_FLAG_ALLOW_SINGLE_LEVEL_INSTANCING which should be present for IAS->GAS only  scene hierarchies.\\nhttps://github.com/NVIDIA/OptiX_Apps/blob/master/apps/intro_runtime/src/Application.cpp#L1733I’m assuming you set the instanceAcceleratorHandle as the root traversable handle inside the launch parameters.There is also no reason for the OptixAabbs on the instances.Those fields were only used for motion blur and actually have been removed in OptiX 7.2 because OptiX can calculate these itself based on the children.\\nhttps://raytracing-docs.nvidia.com/optix7/api/html/struct_optix_build_input_instance_array.html\\nMeans I would recommend updating to the OptiX SDK 7.2 and matching drivers.There are logger and validation features in OptiX 7.2 which might report that mismatch:\\nhttps://raytracing-docs.nvidia.com/optix7/api/html/struct_optix_device_context_options.htmlI would also always recommend to calculate the pipeline’s required stack space explicitly yourself.\\nThe built-in stack size calculation does not handle all cases, esp. not with direct or continuation callables.\\nIt needs to take the maximum traversable graph depth into account which is 2 with single level instancing:\\nhttps://github.com/NVIDIA/OptiX_Apps/blob/master/apps/intro_runtime/src/Application.cpp#L2018Thanks a lot.\\nOPTIX_TRAVERSABLE_GRAPH_FLAG_ALLOW_SINGLE_LEVEL_INSTANCING and setting\\nmaxTraversableGraphDepth = 2 in optixPipelineSetStackSize() did the job.\\nI will definitely study your applications as well.Powered by Discourse, best viewed with JavaScript enabled',\n",
       " 'wpf-directx9-clickonce': 'Hello,\\nHas anyone else had an issue with newer nvidia cards showing a black video? It’s strange that it only effects nvidia cards, and only the newer ones… even more wierd that videos will play fine when the app is started from its .exe locally. Just not when its started from a click-once link.I’ve ruled out a lot of things already, and am running out of possible causes.Powered by Discourse, best viewed with JavaScript enabled',\n",
       " 'problem-of-ccd-in-3-3-2': 'Hi.I am using the 3.3.2 version.\\nBut I found a problem.CCD is a conflict of Convex and static box.\\nConvex dug into the Meter boxes.So use,Please, help me.Powered by Discourse, best viewed with JavaScript enabled',\n",
       " 'does-physx-sdk-follow-the-physics-law': 'Hi, physX developers.I’m a newer developer with physX SDK and recently encounter an odd problem.\\nWe try add force to slider for simulate the vehicle driving behaviour, and create a simple demo to test the friction how to influence the “vehicle” moving, we set the gravity(0,-10,0), slider mass 1.0, slider shape box dim(1,1,1), static frition 0.5(both shape and world, use CombineMode NX_CM_AVERAGE), we compute the force which just can push the slider moving is f=umg=5N, then we addLocalForceAtLocalPos(5.1, NxVec(0,-1,0)), but we can’t get the result we expected.What’s error with the test? Shall we missed sth? NX_SKIN_WIDTH is 0.0, all other parameter is set to default.\\nAny reply, prompt or clue is appreciative. :)Powered by Discourse, best viewed with JavaScript enabled',\n",
       " 'nvidia-control-panel-settings': 'I’m looking for a way to apply the NVIDIA Control Panel settings to all users on several computers. Specifically I’m trying to set in the NVIDIA Control Panel > Adjust image settings with preview > Use my preference emphasizing: Balance. Default settings is “Let the 3D application decide”. All the computers are on Windows 10 Enterprise 2016 LTSB. Any suggestions or is this not possible?Thank youPowered by Discourse, best viewed with JavaScript enabled',\n",
       " 'how-to-link-and-include-physx-4-0-extensions-library': 'EDIT: Did not know to post questions on the Github repo directly.\\nEDIT 2: Got an answer to this question on the Github repo: https://github.com/NVIDIAGameWorks/PhysX/issues/115Hey there,I am including the PhysX 4.0 SDK into my C++ opengl project, but I cannot use anything from the PhysX Extensions library like PxDefaultErrorCallback or PxDefaultMemoryOutputStream because I need to include the Extension library first. The problem is I cannot find the PhysXExtensions32.lib and its DLL in any folder of the PhysX SDK that I have compiled myself.I could find a static library called PhysXExtensions_static_32.lib, but when linking against this library I get this error:Is there a way I can get the Extension libaray as Multithreaded-Debug-DLL (/MDd) instead of a Multithreaded (/MT)?I link against these libraries:I redistribute these DLL files:Any help would be really appreciated!Powered by Discourse, best viewed with JavaScript enabled',\n",
       " 'blocky-artifacts-in-shadows': 'I am experimenting with hevc_nvenc and I am having trouble getting a high quality image with VBR mode.  I tried giving it a super high bitrate of 100Mbps but this only encodes 29Mbps.  How can I make it use more bits, and are there any other parameters I should try for better quality?  I’m using Tesla T4.  I tried B frame refs instead of weighted prediction but that didn’t make any difference and I believe you can’t use both at the same time.Powered by Discourse, best viewed with JavaScript enabled',\n",
       " 'introducing-pumex-a-vulkan-renderer': 'Cheers,I present you first public release of my Vulkan renderer :https://github.com/pumexx/pumexPumex has few properties that you may find interesting :Feel free to check it out.thank you for sharing this with Vulkan community!Powered by Discourse, best viewed with JavaScript enabled',\n",
       " 'agx-xavier-how-to-find-ethernet-qos-datasheet': 'Hi list,\\nI am looking for the reference manual of Ethernet EOS controller available on Jetson AGX Xavier .\\nI retrieve the reference manual “Technical Reference Manual NVIDIA Xavier Series” Version:1.4p, ID:DP-09253-002 but\\nthere is no register description for Ethernet EOS: there a few references of  “ETHER_QOS_x” but that all…Can you point me out the documentation for the Ethernet controller please?\\nThanks.Powered by Discourse, best viewed with JavaScript enabled',\n",
       " 'shader-disk-cache-bug-with-opengl-4-6-spir-v-specialization': 'It seems that the specialization constant indices and values provided to glSpecilaizeShader do not factor into the hash generation. So only the first specialization is reloaded from the cache even with different specialization constants provided on subsequent calls to glSpecializeShader (also on different program instances, only the first ever passed constants are in use).A test program with two shader programs created from a single spir-v source with different specialization constants illustrates this, with shader cache enabled both shader programs behave identically, with the cache disabled, the expected result is rendered. (A test on an AMD system also renders correctly)Tested on Linux and Windows with the current 430.40 driver.Powered by Discourse, best viewed with JavaScript enabled',\n",
       " 'ogre-3d-conflicts-with-nvidias-opengl-implementation': 'There is a libogre-1.9-dev package installed in my system. Is it similar to the mesa libraries and conflicts with Nvidia’s opengl implementation?Powered by Discourse, best viewed with JavaScript enabled',\n",
       " 'customed-carrier-board-hdmi-dispaly-black': 'on my custom carrier board. but display screen is balck, i use command “xrandr” and “export DISPLAY=:0” , it detects display device. but it is also black. i have uploaded uart log,can you help me ?x_20201228.log (35.7 KB)Powered by Discourse, best viewed with JavaScript enabled',\n",
       " 'delete-joints-on-ragdoll-for-trench-coat': 'I need help. I’m trying to make a physx trench coat using the cloth and ragdoll tools. On the video tutorial it says to delete the extra joints on the ragdoll in the attribute editor but I can not find it. I’m not sure I am in the right fourm section but any help would be great.Thank youSorry for the repost loading troublePowered by Discourse, best viewed with JavaScript enabled',\n",
       " 'glsl-shader-debugging-opengl': 'I read somewhere that NSight will soon support OpenGL GLSL shader debugging.  Is this true?  If so, is it on schedule for Q2?I have a feeling I won’t be able to complete my current project without it.Also does anyone know what the configuration will be?  Will it only be networked, dual cards, or single card, or perhaps emulation?Powered by Discourse, best viewed with JavaScript enabled',\n",
       " 'listview-scrolling-with-mouse-wheel-issue': 'Hello.After scrolling down the list within the listView using mouse wheel OnItemClickListener stops working: OnItemClicked methos is not triggered and listview is scrolled back (without animation) to top of the list.\\nThe bug is present in ListView and ExpandableListView (other were not tested) and can be reprodused only on NVidia Shield.Scrolling with DPAD or with the KeyBoard works good on NVidia Shield.The same app with the same data in ListView works without any bug on other Android Platforms (Phones, Tablets, Android TV).I would really appreciate some help, cause the bug looks very strange and I really can’t figure out what I am missing here.Best Regards,\\nVlad.Powered by Discourse, best viewed with JavaScript enabled',\n",
       " 'trouble-compiling-rt-callable-program-into-ptx-when-upgrading-to-cuda-8': 'I am using Win7, Optix 4.0.2, VS2012. project set to use compute_30,sm_30.My project runs fine with CUDA 7.5  but when I change my project to use CUDA 8.0, none of the ptx files that my project generate have any of the RT_CALLABLE_PROGRAM functions included in them.Can anyone provide any insights to what might cause this or maybe suggest some troubleshooting ideas.CUDA 8.0 does more aggressive dead code elimination and needs the “–relocatable-device-code=true” or “-rdc=true” option to force code generation of OptiX callable programs.[url]Loading callable program from file fails with RT_INVALID_SOURCE - OptiX - NVIDIA Developer ForumsThank you for the answer and the link @Detlef Roettger. It was very helpful!Powered by Discourse, best viewed with JavaScript enabled',\n",
       " 'gpu-pcb': 'So I’ve been interested in manufacturing some gpu pcbs and need an idea. Is it possible for me to get some designs of outdated pcbs like the 210 or something?Powered by Discourse, best viewed with JavaScript enabled',\n",
       " 'unity-flex-create-inclusions': 'Hi,\\nI need to create a Flex object with inside inclusions with different consistance.\\nThere is a way for make this in Unity Flex? Or any trick for create this object.\\nThanks.Powered by Discourse, best viewed with JavaScript enabled',\n",
       " 'sqrt-of-positive-number-is-nan-with-newer-drivers': 'I’m seeing an issue where the sqrt of a large positive number is -nan in some driver versions. The issue can be duplicated by calling the function below from an example in the OptiX SDK (I’m testing in optixPathTracer). The code works correctly in older drivers (tested with 472.84). I assume the issue is the driver’s PTX compilation substituting a single precision sqrt for the double precision version.\\nI generally avoid using doubles with CUDA/OptiX, but a few specific calculations really do need them. I might be able to work around this particular case with some scaling hacks, but is there some other method of invoking double precision sqrt that might dodge this substitution issue?\\nThanksCurrent test configuration:\\nWindows 10 21H2\\ndual Quadro RTX 4000\\n512.15 driver\\nCUDA 11.1 and OptiX 7.3\\nProblem also occurs with CUDA 10.0 and OptiX 6.5The OptiX SDK examples usually translate PTX code with the flag --use_fast_math for performance reasons.\\nThat replaces trigonometric, reciprocal, and square root calculations with approximated instructions.I’m actually not sure if that setting also replaces double with float calculations. I’m not using doubles in OptiX device code because they usually incur a hefty performance hit except on a few workstation GPUs architectures.See the NVCC help (this was 10.1) for what specific flags are affected (mind the --prec-sqrt=false):Please check what your NVCC command line options are exactly.If you’re using --use_fast_math, compare the resulting PTX with and without and look for instructions with approx suffix in the use_fast_math case.\\nEspecially compare the code size of the PTX results. Note that the precise calculation of trigonometric functions can result in a lot bigger code and a huge impact on the runtime performance.\\nCheck if your sqrt instruction is using doubles inside the PTX code.You might want to change everything to the fast math mode except for the sqrt.I double checked the flags, and we don’t use fast math in our application. It’s a scientific application and we accept the performance impact. I don’t think it affects this issue since the behavior is the same between our application and the SDK examples.\\nThe PTX is sqrt.rn.f64My understanding is that OptiX compilation (which involves a PTX → LLVM internal conversion and related optimization passes) involves transformations resembling the effects of --ffast-math in C/C++. In other words, removing this flag in nvcc is not enough, because this only represents a small part of the larger compilation task.It would be very useful to have a OptixModuleCompileOptions::disableFastMath flag in some future API version to prevent undesirable optimizations in a scientific context.It seems rsqrt stays 64-bit in these drivers.\\nx = 1.0 / rsqrt( x ); worksNote that according to the PTX ISA 64 bit rsqrt instruction is always a .approx instruction and always uses software emulation [1]. Maybe that’s plenty of precision, but check to make sure, and also look at the PTX & SASS to see what ultimately happens with the reciprocal as well.Not sure if this is helpful here or not, but wanted to note that you can exercise control and mix fast math and accurate (slower) math if you use the double-underscore device intrinsics explicitly for code you want to force a certain way regardless of what the compiler does.The PTX ISA also lists the error bounds of the .approx versions of math instructions, in case that’s useful. The bounds are sometimes better than people expect, given the name “approx”.[1] PTX ISA: rsqrt“Note that rsqrt.approx.f64 is emulated in software and are relatively slow.”\\n“For PTX ISA version 1.4 and later, the .approx modifier is required [for rsqrt].”–\\nDavid.rsqrt being approximate is noted, and I need to do more testing on the precision impacts. But the answers it returns are much more accurate than -nan.\\nThe issue with sqrt.rn.f64 being apparently treated as single precision persists in the 516.25 driver.Have you checked whether this looks like a CUDA bug or an OptiX bug? Do you get the right 64 bit behavior if you put the code in one of the CUDA SDK samples?Also I’m not yet exactly sure where it’s going wrong. Since 1e40 is just slightly larger than FLOAT_MAX, I’m curious if the problem is a cast to float before the PTX sqrt instruction. That might make sense because the bit pattern would have a 0 (negative) sign bit. Maybe you can inspect the bits before and after more carefully than the %f print, or even experiment and see if the problem is that the sqrt is operating on the lower 32 bits of your double?–\\nDavid.Testing with the CUDA 11.1 examples (specifically simplePrintf and ptxjit), I do see the correct 64 bit results.\\nTesting with 1e36 (in OptiX) produces the correct answer, which seems to imply it’s a downcast to float rather than using half the bits of the double.\\nHere’s the PTX snippet for a bit more context, though it seems the bug is introduced later in compilation.\\nmov.u64 \\t%rd135, 5205425776111082661;\\nst.local.u64 \\t[%rd129], %rd135;\\nld.local.f64 \\t%fd26, [%rd129];\\nsqrt.rn.f64 \\t%fd27, %fd26;I was able to repro this in the OptiX 7.3 SDK with a 515 driver. However, testing in OptiX 7.4 and 7.5 SDKs, I see the correct result for sqrt() with both PTX and OPTIX-IR compilation. So it does appear that this is an OptiX bug, and that it was already fixed along the way somewhere. Sorry about that. Can you try again with OptiX 7.5? FWIW I doubt it matters but I’m also using CUDA 11.7.BTW, we will be seeing about fixing the bug in 7.3 and earlier. Hopefully the workaround can get you past this immediately, but if not, let us know and we’ll try to prioritize accordingly.–\\nDavid.I’m still seeing the -nan with OptiX 7.5. So it looks like the conditions involved in triggering the issue are more complicated than I’d been thinking. Is their any apparent difference in the PTX between your 7.3 and 7.5 cases that might explain the behavior change?Tested with:\\nWindows 10 21H2\\ndual Quadro RTX 4000\\n516.25 driver\\nOptiX 7.5 with CUDA 11.1 and Visual Studio 2019\\nOptiX 7.5 with CUDA 11.7 and Visual Studio 2019\\nOptiX 7.5 with CUDA 11.7 and Visual Studio 2022All my previous tests have been Release builds. The Debug executable for the OptiX 7.5 / CUDA 11.7 / Visual Studio 2022 computes the sqrt correctly. The Release executable is still giving -nanOkay, interesting. This might indeed be more complicated. I’ll try again with 7.5.I did see a couple of cases that were confusing to me and it seemed like I was reproing with 7.5. However, I started using a more careful workflow to repro where I did two things: first, delete my build directory, then run cmake from scratch, and build from scratch. Second, I deleted my optix cache every time as well. This ensures that the executable, the PTX, and the SASS are all recreated from identical conditions, guarantees that I’m not accidentally picking up something from a previous run.Is it worth double-checking your 7.5 results with a clean and total rebuild, or did you already do that?Either way, I have filed a bug report the team will be looking into, and I’ll update the description and repro and priority if the bug has no current fix.–\\nDavid.Should I be paranoid about the OptiX cache now? It has seemed well behaved in the past.\\nRetrying the OptiX 7.5 Release/Debug experiment with new build areas and an empty cache each time produced the same result. Debug works, Release does not.There’s no reason to be paranoid, the cache is very reliable. It’s me who’s less reliable, sometimes I don’t know or don’t keep track of what’s changed, so cleaning cache eliminates the possibility that I made a mistake or forgot something. And, of course, reproducing with a clean cache shows that caching is not the problem. This bug has been picked up and the team is working on it, we can reproduce, and it appears I was wrong about this being tied to the SDK version number - so I did indeed make a mistake somewhere. Thank you for reporting this issue, we’ll get it fixed and released ASAP. We’ll also take Wenzel’s fast math suggestions under advisement.–\\nDavid.A minor update here is that the compiler team has found and fixed the sqrt bug. Thank you for reporting it! It is limited in practice to OptiX programs. What is the importance level of getting this fixed soon, and do you have any acceptable workaround in the mean time? Does your rsqrt() trick work, and/or can you do your double precision via CUDA? Sorry for even asking, we want the fix released ASAP, of course, and normally I wouldn’t ask this, but I’m asking because there are some branch scheduling issues that could make getting this out in the current driver branch difficult, and the next major workstation branch is a couple of months out. If the damage is already done and a month or two of extra delay isn’t very relevant at this point, we could let the fix work it’s way into the next major workstation driver update. But if this continues to impact you then we will try to do the work to get it into 515.–\\nDavid.The rsqrt method seems to be good enough for my purposes, and I’m planning to stick with it for some time to work around the issue in existing drivers. So I’d say the fix can wait for the regular release schedule.\\nThanksPowered by Discourse, best viewed with JavaScript enabled',\n",
       " 'hardware-acceleration-using-dxva2-on-quadro-cards': 'Hello,\\nI don’t know if this is the right place to ask, i will try…\\nI am using DXVA2 on win 10 to accelerate video on the graphic cards, for video HEVC/H.265 videos.With my flow, cards that not support the hardware codec([url]https://developer.nvidia.com/video-encode-decode-gpu-support-matrix[/url]), will not used the hardware acceleration, BUT for some reason for unsupported cards, the “card” is cheating, and return that it’s support the codec.For example, Quadro M1000M which from the supported matrix doesn’t support HEVC, DXVA threat that supporting hardware decoding.\\nWhen testing this with “DXVA Checker” there is same result.I find that there is feature of “PureVideo” that will do hybrid decoding ,[url]https://devtalk.nvidia.com/default/topic/1031720/video-codec-and-optical-flow-sdk/video-sdk-hybrid-decode-hevc/post/5252583/#5252583[/url].so my question is, is there any way to disable this and let my software to only CPU decoding?For Maxwell generation GPUs, the GPU falls back to hybrid decoder implemented using CPU and CUDA when using DXVA decoding. There is currently no way to explicitly disable this.May I know the reason why you would like to disable it?Thanks Patait for the answer.\\nFrom that answer i was worried :), in my software i can control if i want to use CPU decoding or use hardware decoding using DXVA2, my concept was that if the GPU not support the codec, DXVA2 will not return a valid decoder device GUID, and then i will fallback to CPU decoding [url]https://docs.microsoft.com/en-us/windows/win32/api/dxva2api/nf-dxva2api-idirectxvideodecoderservice-getdecoderdeviceguids[/url].\\n(I can say that it’s working for some other cards K600 for example for few HEVC profiles)There is degraded in performance when this hybrid decoding is taking place, it’s better for me to use CPU over it, but i cannot get indication for that as describe above.I Will be happy for some sort of solution for me.AsafCan you check the GPU model and avoid using the hybrid decoder? Fully-accelerated HEVC hardware decoding is supported only starting GPUs GM20x. You can check the device IDs. I understand this is not an ideal solution, but that’s the only possible solution I can suggest at the moment.Thank you.\\nI will take your advise and try to avoid those cards, using a black list.Is there any chance i can talk with nVIDIA driver persons to look if there is a hidden configuration that can disable this (Feature Set E)?Powered by Discourse, best viewed with JavaScript enabled',\n",
       " 'labeled-objects-in-gas': 'I consider using Optix for non-rendering applications. Is there a way to add a label or flag to each object in GAS? So when each ray hits the object, the call back function is able to know which object it hit and return the label/flag in the HIT call back function.Sure. You could even return each individual primitive you hit in a GAS.There are multiple ways to design that.If you use a GAS with multiple shader binding table (SBT) entries, you could store user defined data on the SBT data which gets accessed via optixGetSbtDataPointer() and written to your per ray payload inside the closest hit program.\\nThe OptiX SDK optixPathTracer example uses that method to assign different diffuse and emission colors to the parts in a single GAS.You could also store an additional per primitive attribute which indicates the index you want to return. That needs more memory, but could have different IDs per primitives in one GAS irrespective of the assigned SBT hit record.If you use a render graph hierarchy with a single top level instance acceleration structure (IAS) where each OptixInstance is referencing an individual GAS with one SBT record per GAS, things get more flexible because there is the instance index (the child number), the user defined instance ID (and the instance’s SBT offset) with which you could uniquely identify your GAS.\\nLook at the different OptiX device functions here https://raytracing-docs.nvidia.com/optix7/guide/index.html#device_side_functions#device-side-functions to find how to get the instance index and ID values.Sorry, I am still not quite sure how to do it. Still use the triangle example. Follow are two triangles If I want to know all the rays that hit triangle A. How do I label { -1.0f, 1.0f, 0.0f },{ -1.0f, -1.0f, 0.0f },{  1.0f,  1.0f, 0.0f }, and what to add in the hit program to achieve this label, and then I will output it with payload, so I know which ray hit which triangle.\\nCan you provide me more instruction on how to achieve this?In that simple example all triangles are in a single geometry acceleration structure (GAS) and the triangles in that are already uniquely identified by their primitive index. There is no need for an additional label in that case.To identify which primitive index in a GAS you hit, you call the OptiX device function optixGetPrimitiveIndex() inside the closest hit program. (That link is the same as above.)\\nSearch for that inside the OptiX SDK example code to see how it’s used.That unsigned int primitive index is the zero-based index of your primitives in the same order in which you stored them inside the respective OptiX acceleration structure build input using inside the optixAccelBuild() call for this GAS.When defining triangles you want to make sure that they are all having the same winding. The default winding is counter-clockwise for front faces. Your second triangle is defined clockwise.\\nFor that unit square [-1, 1] I would normally define the two triangles in the following order by walking on the outside edges of the quad in counter-clockwise order starting in the lower left.Thanks, say if I have 4 triangles, and they are belong to two groups, and I only want to know which group the ray hit. In this way, I will need to add a label to the triangle right? How can I do that? can you explain more about this “You could also store an additional per primitive attribute which indicates the index you want to return.” Is there a example for this?Again, there are multiple ways to handle that, which I already explained in the first answer.1.) If you put the triangles into a separate GAS per group, then you would need an instance acceleration structure (IAS) on top of them and there are OptiX device functions in that same Programming Guide chapter linked twice now, which allow querying the child index inside that IAS or the user defined instance ID field at each OptixInstance.\\nMeans that instance index or ID would be your unique “group” identifier.(That instance index or ID plus the primitive index would uniquely identify each triangle in the scene, even when instancing the same geometry multiple times.)There are many OptiX SDK examples which use OptixInstance structures.2.) If you want to store all triangles inside a single GAS, you would need to store a unique per triangle attribute containing your “group” identifier with each triangle.\\nI would usually define indexed triangles, means there is an additional uint3 array with the indices into the vertices pool which define which vertices build one triangle primitive. Means if you wish to assing each triangle to a specific “group” there would need to be another unsigned int array with the same number of elements (triangle count) as the uint3 triangle indices in which you’d store your group identifier.\\n(More advanced: That could also be defined interleaved, like using an uint4 and the .w component is the unique triangle index, which can then be read along with the triangle vertex indices in the hit shaders. Reading an uint4 is faster than an uint3 because there is a specialize vectorized load instruction for that.)This is working the same way as providing any other per triangle data to the OptiX device code. This is not different than accessing triangle indices. Search the OptiX SDK examples for optixGetPrimitiveIndex.\\nIt just needs some device side memory buffer containing your group indices per triangle. You provide that as CUdeviceptr pointer either on the launch parameters or the SBT hit record data and index into that with the primitive index.3.) If you’re using more than one shader binding table hit record for that single GAS where each “group” of triangles has its own SBT hit record entry, you could store the “group” identifier along with the SBT hit record data.\\n(The optixPathTracer example is using this to assign material and light colors.)I would recommend the first method because that has more flexibility for more complex scenes in the future.Please work through OptiX Programming Guide and the other OptiX SDK examples to see how instance acceleration structures with OptiX instances are used.Powered by Discourse, best viewed with JavaScript enabled',\n",
       " 'fastest-way-to-read-optix-buffer': 'Hi.I’m looking for the fastest way to read Optix::Buffer into the Host.buffer->map() takes 1ms on 1280x720 buffer.Please tell me if there are other faster methods.Thanks.1280 * 720 * 4 bytes / 1 ms <==> 3,686,400 bytes / ms <==> 3,686,400,000 bytes / sYou’re reading over the PCI-E bus and depending on the PCI-E generation and number of electrical lanes of that slot in your motherboard and the precision of your measurement, 3.5 GB/s could be an expected result.Find a PCI-E performance table here: [url]https://en.wikipedia.org/wiki/PCI_Express[/url]Powered by Discourse, best viewed with JavaScript enabled',\n",
       " 'nvencmapinputresource-with-nv-enc-err-invalid-param': 'I’m capture screen with Windows.Graphics.Capture,but failed in m_nvenc.nvEncMapInputResource. the origin sample capture with dxgi is ok.what’s the problem???I’m having the same issue.  I have even verified that I am un-mapping the resource correctly (though from another thread) I also made my queue 16 deep.  I am wondering how big is your queue?  (I.e. size of your m_vRegisteredResourcesForReference?EDIT: Same issue even if mapping and un-mapping from the same thread.At least in my case it turns out that I was just encoding the frames in too quick of a succession.  I’m not exactly sure how to make sure I don’t bulldoze the buffer, but when I reduced the incoming frame rate, it stopped crashing.Powered by Discourse, best viewed with JavaScript enabled',\n",
       " 'image-processsing': 'Hello,I would like to know how do we access to the screen when we just want to display an image, whithout using openGL, directX etc…Powered by Discourse, best viewed with JavaScript enabled',\n",
       " 'nvenc-sometimes-hang-when-running-two-simultaneous-encoder-sessions': 'Hi,I am facing an issue when using NVENC SDK to lively encode camera streams.\\nI have two encoder sessions in my system.\\nAnd sometimes, one of the encoder session will hang at start up. It can initialize encoder successfully. But when feeding frames into the encoder, it will hang infinitely and no encoded frames available at output.\\nThis happens occasionally and makes it really hard to debug.\\nI used NVML to get some encoder information out of the system. And it seems like the encoder that hang had utilization of 34% initially and then all 0s.\\nThe two encoder sessions run in parallel in two different threads.\\nNvidia driver version is 384.59\\nPlease let me know if there is anything I should check.Thanks.Hi,Could you try the R396 drivers, R384 drivers are old. Meanwhile, could you provide the following info:Thanks,\\nRyan ParkPowered by Discourse, best viewed with JavaScript enabled',\n",
       " 'ue4-nvphysx-branch-404-error': 'Hi, I am currently developing a game based on UE4 Flex since there is no alternative for soft body physics in UE5 Chaos. I was using the branch from https://github.com/NvPhysX/UnrealEngine/ but for some reason I am not able to access this repos anymore as I am getting a 404 error even though I can access the EpicGame branch of UE successfully . Could you help me reach this repo? Thank you very muchHi @Sephiroth_FF !I am really sorry for this inconvenience. The only thing I can share for now is that there are technical issues with this particular Github repo of NVIDIA since last weekend. We are working on a solution for this, but we don’t know an ETA yet.For now please use your latest cloned or downloaded version of the repo. If you don’t have a local copy please send me a DM and I will check what I can do.I will try to follow up with an update as soon as public access is restored.Thanks!Hi @MarkusHoHo, Thank you so much for your response. I can definitely work with my currently cloned version of Flex. I got worried that since Nvidia has deprecated support for these branches, that they where being taken down permanently. Glad to hear they will be back eventually.While all the other GameWorks tech have had alternatives by mainline UE5.1, Flex is currently the only soft body physics for Unreal with support for advanced skeletal mesh interaction and GPU acceleration. Really wish NVIDIA could make a exception for Flex and port it to 5.1 but, I understand it might not be feasible with all ressources needed for the maintenance of NvRTX branch.Thanks again!Hi folks, just wanted to follow up on this to ask if NVPhysx repo access will be restored?\\nI was relying on this branch for UE engine builds as it’s often updated quicker and sooner than NVRTX, so be good to know if it’ll be coming back.\\nThanks!I am sorry, but I don’t have any good news yet.It seems it is a more complicated than technical issues since GitHub changed some of their service agreements for companies. Sorting through this might take a bit longer I am afraid.Have there been any updates since January for accessing the NVPhysX repo to use with the Unreal Engine? Is there an alternative location that could be considered?Hello @toasticuss ad welcome to the NIVIDA developer forums.Thank you for coming back to us about this.I am afraid that at this time there is no plan to (re-)open a separate PhysX repository beside the one that is part of Omniverse.But the Omniverse one is also more current and has the latest and greatest.I hope that helps!Powered by Discourse, best viewed with JavaScript enabled',\n",
       " 'glx-arb-create-context-profile-is-unavailable-with-x11-forwarding': 'I am trying to launch instant-ngp gui application on remote ubuntu server (18.04) and forward display to local Mac with XQuartz. The application fails to start with the following output:Has anyone encountered the same problem? Any help would be greatly appreciated.+1I also have the same problem, but couldnt find a solution after spending lots of hours. It does not happen on UbuntuPowered by Discourse, best viewed with JavaScript enabled',\n",
       " 'mesh-shader-threads-beyond-wave-thread-count': 'Amplification and mesh shaders are an awesome addition to shader programming.\\nThe leverage of a single CPU call (DispatchMesh) being multi-threaded into a set of amplification threads and each of those being further threaded into a set of mesh shader threads is huge.\\nWhile the total number of threads per shader is limited 128 it appears that their is no way to use more than the hardware supported number of threads per wave.\\nWith Nvidia cards this is currently 32.\\nWhile 32 * 32 is incredible (also difficult to fully utilize) it seems that not being able to push these to a full 128 is leaving out more optimization.First, am I correct that the shaders are limited to the threads per wave?\\nNote that I see SV_DispatchThreadId is unstable beyond the threads per wave.Is the 128 just future hardware support already built in to the software?Any insight on these would helpful.Powered by Discourse, best viewed with JavaScript enabled',\n",
       " 'network-qos-implementation': 'Is there more information for “Missed packets - Forward error correction” and other features of QoS networking ?\\nIs it inserting “XOR-ing” packet after packet set to recover from random lost of one previous packet in packet set (or other proprietary and/or patented technology) (Adaptive Forward Error Correction (AFEC) based Streaming using RTSP and RTP) ?\\nIs the selective acknowledgement (ACK/NACK) and repeating packet used (UDP use was confirmed as transport technology) (or not due to increased latency) ?\\nIs some sort of packet scheduler used like HTB in linux traffic control ?Powered by Discourse, best viewed with JavaScript enabled',\n",
       " 'how-can-i-decode-rtmp-stream-video': 'Hi, I am new to video codec, I am starting to use video sdk, I have tested the decodec sample, it can play local video file,I wonder how can I play a video stream online?Thanks.Powered by Discourse, best viewed with JavaScript enabled',\n",
       " 'system-shut-down-while-running-cuda-using-nvidia-gforce-gtx970': 'I am using “NVIDIA GFORCE GTX970” graphics ,While running my CUDA projects(OpenCV+CUDA),suddenly the system shutdown, but the sample CUDA projects works fine.I have another sytem with GTX640 ,the same CUDA project works fine in that system.Both the system using same CUDA driver(version 7.5) and driver(Version: 370.28) ,I also tried remove and reinstall the drivers ,but still the issue exist.Can anyone suggest any idea to solve this issue?Powered by Discourse, best viewed with JavaScript enabled',\n",
       " 'physx2-8-4-nx-clf-hardware': 'I try to use GPU accerlation,\\nI set \"SDKDesc.flags &= ~NX_SDKF_NO_HARDWARE;\" when I created my SDK and used\\nNxHWVersion hwCheck = gPhysicsSDK->getHWVersion();\\nif (hwCheck == NX_HW_VERSION_NONE)\\n{\\ncout<<\"\\\\nWarning: Unable to find a PhysX card, the cloth will be simulated in software.\\\\n\\\\n\";\\n}\\nto check if GPU is set or not. then I create a cloth fall down because of gravity.\\nI set\\n\"NxClothDesc desc;\\ndesc.flags |= NX_CLF_VISUALIZATION ;\\ndesc.flags |= NX_CLF_BENDING;\\ndesc.flags |= NX_CLF_HARDWARE;\"if I don’t set \"desc.flags |= NX_CLF_HARDWARE;\", Everything is fine. the cloth fall down and stay on the ground, but if I set \"desc.flags |= NX_CLF_HARDWARE;\" it will hit ground then it rebound to very high altitude and disappear. It seems very strange, What happen in my program?You might want to look at the SampleCloth to copy the HW cloth properties. Hooking up to PVD is useful to track what’s going with your cloth.You might want to look at the SampleCloth to copy the HW cloth properties. Hooking up to PVD is useful to track what’s going with your cloth.The HW cloth properties I find from SampleCloth areand set flag NX_CLF_HARDWARE when I create clothAnything missed?Powered by Discourse, best viewed with JavaScript enabled',\n",
       " 'oculus-client-shutdowns-when-the-app-loses-focus': 'Hi,We noticed an issue in the CloudXR Oculus client, when the app loses focus for a couple of seconds (either when the headset goes to sleep, the power button is pressed or the user leaves the guardian area), the app shuts down.After the headset awakes the app is already closed and either the user is back in the lobby or in a black screen with the oculus menu enabled.We noticed the issue happening in v2.0 but it is also present in v3.0.The following logs were generated in both versions by leaving the guardian for 3 to 5 seconds, they also contain logs from logcat.CloudXR logs.zip (57.2 KB)Thanks in advance.Hi,What game/application is running on the server side when you experience the client shutdowns?-WillHi, thanks for the reply.The issue occurs regardless of the executed application, it even happens just with SteamVR.We are building an enterprise solution that enables the upload of your VR catalog to the cloud and execute them on demand via streaming, due to the nature of this product I’m able to test it with multiple applications (tested with at least 6 different applications).Hi,Just to clarify is this a Quest 2 or a Quest 1?-WillHi,It actually happens in both, you can test it by using the sample CloudXR apk for Oculus Quest (any from v2.0 to v3.0) and simply leaving the Guardian area and coming back to it when the client is already streaming from the server.Hi,I was unable to replicate this on the Quest 1. I tested leaving the guardian, sleeping the headset, and simply taking the headset off. I left it in these sleep modes for about 5 minutes and was able to resume from where I left off. Based on your logs, it looks like you are losing network connection to the server when the headset sleeps.In the latest version of the Oculus software, they fixed a “wifi degradation issue”. Some users had been reporting that when their headset went to sleep, they would lose wifi, especially when using a 5ghz wifi network. If you were hitting this issue, I can see that causing your client application issues. Could you try picking up the latest update (v31.0) for your headsets and trying again?-WillHi Will,We are noticing a similar issue, and looking at the Android logs, it appears we are getting a crash in the c++ side of the app when the stream is being shut down in the pause function. This only started for us when we updated to 3.0. The shutdown looks normal in the logs up until the last bit where it’s destroying the receiver:I/VrApi: FPS=72/72,Prd=34ms,Tear=0,Early=0,Stale=0,VSnc=1,Lat=0,Fov=0,CPU4/GPU=1/0,940/305MHz,OC=FF,TA=0/0/0,SP=N/N/N,Mem=1555MHz,Free=2894MB,PLS=0,Temp=26.7C/0.0C,TW=4.14ms,App=2.30ms,GD=0.29ms,CPU&GPU=6.06ms,LCnt=1,GPU%=0.51,CPU%=0.24(W0.26),DSF=1.00\\nI/VrApi: FPS=72/72,Prd=34ms,Tear=0,Early=0,Stale=0,VSnc=1,Lat=0,Fov=0,CPU4/GPU=1/0,940/305MHz,OC=FF,TA=0/0/0,SP=N/N/N,Mem=1555MHz,Free=2890MB,PLS=0,Temp=26.7C/0.0C,TW=4.13ms,App=2.31ms,GD=0.62ms,CPU&GPU=5.87ms,LCnt=1,GPU%=0.54,CPU%=0.30(W0.33),DSF=1.00\\nI/VrApi: FPS=72/72,Prd=34ms,Tear=0,Early=0,Stale=0,VSnc=1,Lat=0,Fov=0,CPU4/GPU=0/0,710/305MHz,OC=FF,TA=0/0/0,SP=N/N/N,Mem=1353MHz,Free=2889MB,PLS=0,Temp=26.7C/0.0C,TW=4.11ms,App=2.32ms,GD=0.31ms,CPU&GPU=5.28ms,LCnt=1,GPU%=0.52,CPU%=0.31(W0.55),DSF=1.00\\nI/VrApi: FPS=72/72,Prd=34ms,Tear=0,Early=0,Stale=0,VSnc=1,Lat=0,Fov=0,CPU4/GPU=0/0,710/305MHz,OC=FF,TA=0/0/0,SP=N/N/N,Mem=1353MHz,Free=2889MB,PLS=0,Temp=26.7C/0.0C,TW=4.11ms,App=2.33ms,GD=0.76ms,CPU&GPU=5.48ms,LCnt=1,GPU%=0.57,CPU%=0.30(W0.39),DSF=1.00\\nI/Telemetry: App memory usage: PSS=285MB DalvikPSS=1 MB\\nI/VrApi: FPS=72/72,Prd=34ms,Tear=0,Early=0,Stale=0,VSnc=1,Lat=0,Fov=0,CPU4/GPU=0/0,710/305MHz,OC=FF,TA=0/0/0,SP=N/N/N,Mem=1353MHz,Free=2891MB,PLS=0,Temp=26.7C/0.0C,TW=4.13ms,App=2.34ms,GD=0.29ms,CPU&GPU=5.74ms,LCnt=1,GPU%=0.51,CPU%=0.28(W0.43),DSF=1.00\\nI/VrApi: FPS=72/72,Prd=34ms,Tear=0,Early=0,Stale=0,VSnc=1,Lat=0,Fov=0,CPU4/GPU=0/0,710/305MHz,OC=FF,TA=0/0/0,SP=N/N/N,Mem=1555MHz,Free=2887MB,PLS=0,Temp=26.7C/0.0C,TW=4.15ms,App=2.30ms,GD=0.62ms,CPU&GPU=5.91ms,LCnt=1,GPU%=0.54,CPU%=0.32(W0.39),DSF=1.00\\nI/VrApi: FPS=72/72,Prd=34ms,Tear=0,Early=0,Stale=0,VSnc=1,Lat=0,Fov=0,CPU4/GPU=0/0,710/305MHz,OC=FF,TA=0/0/0,SP=N/N/N,Mem=1555MHz,Free=2887MB,PLS=0,Temp=26.7C/0.0C,TW=4.16ms,App=2.37ms,GD=0.34ms,CPU&GPU=6.25ms,LCnt=1,GPU%=0.53,CPU%=0.27(W0.32),DSF=1.00\\nV/main: In AppPaused() method\\nV/message: In AppPaused() method\\nE/main: shutting down playbackStream…\\nD/AAudio: AAudioStream_requestStop(s#1) called\\nD/AudioTrack: stop(944): called with 1071364 frames delivered\\nD/: PlayerBase::stop() from IPlayer\\nD/AAudio: AAudioStream_close(s#1) called ---------------\\nV/threaded_app: WindowFocusChanged: 0x70c9b49b80 – 0\\nD/FA: Event not sent since app measurement is disabled\\nV/FA: Using local app measurement service\\nV/FA: Activity paused, time: 281130609\\nV/FA: Local AppMeasurementService is starting up\\nV/FA: Bound to IMeasurementService interface\\nV/FA: Connected to service\\nV/FA: Processing queued up service tasks: 1\\nD/AAudio: AAudioStream_close(s#1) returned 0 ---------\\nE/main: shutting down recordingStream\\nD/AAudio: AAudioStream_requestStop(s#2) called\\nD/: PlayerBase::stop() from IPlayer\\nD/AAudio: AAudioStream_close(s#2) called ---------------\\nD/: PlayerBase::~PlayerBase()\\nD/AAudio: AAudioStream_close(s#2) returned 0 ---------\\nE/main: shutting down receiver\\nV/CloudXR: Closing the CloudXR log.\\nDisplayed 916 frames over 21.34 seconds (42.9 FPS).\\nV/CloudXR: Worker thread shutdown.\\nV/CloudXR: CloudXR destroy receiver\\nInput stream disconnected.\\nAudio Send stream disconnected.\\nV/CloudXR: Eye1 processing thread shutdown.\\nV/CloudXR: Eye0 processing thread shutdown.\\nI/VrApi: FPS=72/72,Prd=34ms,Tear=0,Early=0,Stale=0,VSnc=1,Lat=0,Fov=0,CPU4/GPU=0/0,1171/525MHz,OC=FF,TA=0/0/0,SP=N/N/N,Mem=2092MHz,Free=2883MB,PLS=0,Temp=26.7C/0.0C,TW=4.16ms,App=2.37ms,GD=0.34ms,CPU&GPU=6.25ms,LCnt=1,GPU%=0.53,CPU%=0.27(W0.32),DSF=1.00\\nV/threaded_app: NativeWindowDestroyed: 0x70c9b49b80 – 0x6fe1588010\\nI/VrApi: FPS=72/72,Prd=34ms,Tear=0,Early=0,Stale=0,VSnc=1,Lat=0,Fov=0,CPU4/GPU=0/0,1171/525MHz,OC=FF,TA=0/0/0,SP=N/N/N,Mem=2092MHz,Free=2882MB,PLS=0,Temp=26.7C/0.0C,TW=4.16ms,App=2.37ms,GD=0.34ms,CPU&GPU=6.25ms,LCnt=1,GPU%=0.53,CPU%=0.27(W0.32),DSF=1.00\\nV/CloudXR: Streamer shutdown.\\nV/CloudXR: Mediacodec: Decoder thread done.\\nD/SurfaceUtils: disconnecting from surface 0x6fe1674010, reason disconnectFromSurface\\nV/CloudXR: Mediacodec: Decoder thread done.\\nD/SurfaceUtils: disconnecting from surface 0x6fe16be010, reason disconnectFromSurface\\nI/VrApi: FPS=72/72,Prd=34ms,Tear=0,Early=0,Stale=0,VSnc=1,Lat=0,Fov=0,CPU4/GPU=0/0,1171/525MHz,OC=FF,TA=0/0/0,SP=N/N/N,Mem=1353MHz,Free=2869MB,PLS=0,Temp=26.7C/0.0C,TW=4.16ms,App=2.37ms,GD=0.34ms,CPU&GPU=6.25ms,LCnt=1,GPU%=0.53,CPU%=0.27(W0.32),DSF=1.00\\nA/libc: Fatal signal 11 (SIGSEGV), code 1 (SEGV_MAPERR), fault addr 0x550779d5476cfa in tid 27243 (main), pid 27156 (utosphere.devel)\\nI/VrApi: FPS=72/72,Prd=34ms,Tear=0,Early=0,Stale=0,VSnc=1,Lat=0,Fov=0,CPU4/GPU=0/0,1171/525MHz,OC=FF,TA=0/0/0,SP=N/N/N,Mem=1804MHz,Free=2858MB,PLS=0,Temp=26.7C/0.0C,TW=4.16ms,App=2.37ms,GD=0.34ms,CPU&GPU=6.25ms,LCnt=1,GPU%=0.53,CPU%=0.27(W0.32),DSF=1.00\\nI/VrApi: FPS=72/72,Prd=34ms,Tear=0,Early=0,Stale=0,VSnc=1,Lat=0,Fov=0,CPU4/GPU=0/0,1171/525MHz,OC=FF,TA=0/0/0,SP=N/N/N,Mem=1804MHz,Free=2848MB,PLS=0,Temp=26.7C/0.0C,TW=4.16ms,App=2.37ms,GD=0.34ms,CPU&GPU=6.25ms,LCnt=1,GPU%=0.53,CPU%=0.27(W0.32),DSF=1.00It doesn’t happen every time, so it seems like a race condition in the client shutdown. We are also seeing this with the August hot-fix version. And we’re running v32 of the Quest 2 software.Not sure if this is the same issue, but the behavior is the same to the user. Is there more information I could provide that would be useful?/marcI’m hopeful this will be resolved in the next release.  It is certainly much improved, but as always there might be corner cases we didn’t catch yet.@tegradave Do you believe that the 3.1 release helps this condition?I’ve not seen the crash or original issue.  But there’s a lot of improvements to the oculus sample, a lot of changes to lifecycle handling, so hoping this is better now.  Let me know if you still run into issues and repro steps.Hi everyone, a quick update, turns out that CloudXR has nothing to do with the issue, the real problem was that the Activity where CloudXR is loaded was not running in the default process, we made this change so it could co-exist with a Unity player.I also discovered that any activity that does not run in the default process kills the app as soon as the app loses focus while given activity is in the foreground (i.e., the hmd loses track of the guardian, the hmd is out of the guardian, the hmd goes to sleep or the power button is pressed).yes, we’re looking to improve the current lifecycle handling in a future update.  3.1 does have some improvements, so that pause cases should not cause the entire system to shut down.  note that oculus and wave clients are not necessarily in parity when it comes to lifecycle, as they implement platform support completely differently.Powered by Discourse, best viewed with JavaScript enabled',\n",
       " 'artifacts-due-to-hit-programs-fixed': 'Hi,\\nI am seeing artifacts that appear to be related to the Acceleration structure grid.Here is a pure Opengl render of the mesh:\\nopengl.PNG1818×1787 394 KB\\nHere is the render with OptiX:\\noptix.PNG1922×1788 430 KB\\n \\noptix2.PNG1415×1782 370 KB\\nI’ve checked the winding order of polygons, and confirmed my mesh renders ok with pure opengl.Some triangles appear to be “cut out”. However, upon closer look, some edges of the artifacts are straight and track with the acceleration grid as the camera moves. Therefore I suspect its not a triangle issue.Here is what happens when I do “NoAccel”:\\noptix3_noaccel920×1750 135 KB\\nMy Optix graph is as follows:\\ntop_object → Group → Transform → Geometry Group → Geom Instance → GeometryI am using a OptiX 6.5.0 with driver 452.06Any ideas?Found the problem!It was in my code after all (not surprisingly!)Here is how I was setting the closest & any hit programs:std::string ptx_file = fname + “.ptx”;\\nProgram ch_program = CreateProgramOptix ( ptx_file, cast_prog );\\t\\t\\nProgram ah_program = CreateProgramOptix ( ptx_file, shadow_prog );// ray types\\nomat->setClosestHitProgram ( 0, ch_program );\\t\\t\\nomat->setAnyHitProgram (\\t 0, ah_program );\\t\\t\\t\\nomat->setClosestHitProgram ( 1, ch_program );\\t\\t\\nomat->setAnyHitProgram (     1, ah_program );It wasn’t clear to me from the programming guide if I should set a closest hit and any hit program on each ray type. Once I tried this it worked:std::string ptx_file = fname + “.ptx”;\\nProgram ch_program = CreateProgramOptix ( ptx_file, cast_prog );\\t\\t\\nProgram ah_program = CreateProgramOptix ( ptx_file, shadow_prog );// ray types\\nomat->setClosestHitProgram ( 0, ch_program );\\t\\t\\nomat->setAnyHitProgram (     1, ah_program );I needed to only set either the closest hit or any hit program for each ray type ID.Powered by Discourse, best viewed with JavaScript enabled',\n",
       " 'no-device-found-while-using-nvidia-smi': 'I am facing problem with nvidia driver installation with Ubuntu 20.04.6. I have Geforce RTX 2080Ti. After installation of 530 open kernal driver using recommanded option, nvidia-smi shows no devices found. Please if some one can help with this issue. I am also uploading bug report.\\nnvidia-bug-report.log.gz (100.2 KB)Hello @axay.patel2003, welcome to the NVIDIA developer forums!Please check out in your Log:I highly recommend using the standard drivers for consumer GPUs, unless you really depend on being able to modify and recompile the NVIDIA kernel module.Thanks!Powered by Discourse, best viewed with JavaScript enabled',\n",
       " 'nvsdk-nvencinvalidaterefframes-not-works': 'Hi, Guys\\nIt seems  api nvEncInvalidateRefFrames does not work on my Quadro T1000 card.\\nIt always returns zero with Whatever inputs.Here is my HW&SW circumstance:\\nQuadro T1000\\nWin64+ NVSDK 11.1\\nEncodeing HEVC with UltraLowlatency presetI found a previous post in. this forum\\nIt seems some kind of platform do not support nvEncInvalidateRefFrames.\\nAnyone know in what nvsdk version or platform it works ?Thanks.Powered by Discourse, best viewed with JavaScript enabled',\n",
       " 'appdecgl-app-fails-to-run-on-second-gpu-in-the-system': 'Hello,I am trying to run the AppDecGL sample app on a second GPU (GPU1) in my system. It runs fine on GPU0. Following is the system info.\\nMotherboard : ASUS X99-WS\\n2 Quadro P4000 Cards\\nWindows 10\\nCUDA 10\\nLatest Driver (412)\\nI have configured Mosaic with 1x5 display wall, 4 displays connected to GPU0 and 5th connected to GPU1.\\nWhen I give -gpu 1 in arg list to run the app, it crashes at  ck(cuGraphicsResourceGetMappedPointer(&dpBackBuffer, &nSize, cuResource)); in FramePresenterGL.h in Display() function, with error CUDA_UNKNOWN_ERROR.\\nCan anyone help or show me how to run that app (decode and render) on second GPU (or for that matter any GPU other than GPU0)?ThanksHi ajoshi,\\nDo you still reproduce this issue? If yes, can you help with minimized reproduction steps.\\nIs mosaic configuration required or issue occurs with a single display as well?Powered by Discourse, best viewed with JavaScript enabled',\n",
       " 'decoding-problem-when-feeding-cuvideodecoder-manually': 'Hi,After integrating NVDEC into our video management system, I noticed that almost always, after creating new  CUvideoparser, it skips first frame (intra) without feeding it into decoder. Since this behavior is problematic for our pipeline, I wrote a custom parser for H264 and populated the CUVIDPICPARAMS with its output.\\nThis indeed solved my problem, but the decoded video starts getting artifacts on objects in motion. I compared the contents of CUVIDPICPARAMS filled from my parser and from CUvideoparser and there’s no apparent difference, except maybe picture index and frame index, which I guess have nothing to do with motion prediction decoding.I desperately need an assistance with this issue (or maybe there’s a way to make CUvideoparser not to skip frames)Best regardsI create a parser like this and I don’t lose the first frame.I inject data by NALUs. Each NALU goes like this, where buf points to the NALU and len is its length:You should make sure you are injecting correct data, including SPS/PPS.Well, my code is almost a clone of yours except for I use a different number for decoding surfaces. Tried to play with this param as well, but still the same. It swallows the first I frame without invoking any callback and after following P frame is submitted, it starts spitting out data for the decoder (starting obviously with I frame, submitted in past iteration). Of course, the I frame buffer includes SPS/PPS as well. Very weird behavior, for which I cannot get any clarification from NVIDIA guys.That’s not weird at all. It’s standard re-ordering behavior.But now you’re confusing me because you initially said the parser was losing the first frame. But now you are saying that you don’t get the I frame until you have submitted the P frame. That is normal.Oh well, my bad - incorrect wording. I wonder then what’s the purpose of this behavior and is there any way to change it?It’s a standard process to re-order frames for display. Consider the encode order:I2 B0 B1 P5 B3 B4 P8 B6 B7 …where the numbers are the display numbers, i.e., 0 is displayed first, etc.The rule is that a decoded B frame is displayed immediately and the Is and Ps are delayed, i.e., decoding an I/P outputs the previous I/P. Then the decoder thinks like this:decode I2, don’t output anything\\ndecode B0, output it right away\\ndecode B1, output it right away\\ndecode P5, output I2\\ndecode B3, output it right away\\ndecode B4, output it right away\\ndecode P8, output P5\\n…Thereby the display order is achieved. Similar things happen for IPBBPBBPBB. The initial I won’t be output until the following P is decoded.You can’t change this behavior but you don’t need to. Pace things on the output side, rather than the input side. I could explain that better if I knew exactly what you are trying to do.Yes, it definitely makes sense in case where B frames are present, however the nature of security cameras is that they do not use B frames, at least by default. It’s usually a sequential IPPPIPPPI… stream, which makes me wonder why it still delays the first intra.The decoder does not know that it will not encounter B frames.Given that the decode rate is typically hundreds of frames per second, how would a 1-frame latency adversely affect your application?Of course nothing wrong, in case the pipeline design is robust enough. I wish I had that luxury, but unfortunately we don’t always inherit the perfect codebase, eh? The whole pipiline I’m working with expects the same frame, that was submitted, to show up at the end, so the fact that the decoder introduces a delay drives the whole system crazy. Thanks for the help anyways!deleted – gotta be careful where we cast our pearlsdeleteddeletedCompetent engineers have a COURTESY not to insult each other without knowing all the details, which you obviously do not. Again, I appreciate your help, but your last comment was idiotic in its arrogance.Plonk.Powered by Discourse, best viewed with JavaScript enabled',\n",
       " 'isl79987-collect-video': 'Video capture using ISL79987，How to extract the video capture?\\n，thank you。Powered by Discourse, best viewed with JavaScript enabled',\n",
       " 'setting-up-physx-in-visual-studio-2017': 'Hi there,\\nI am new to using Nvidia Physx but I am getting stuck as I keep getting this error whenever I try and generate a project. What do I need to do?\\n\\nimage972×510 30 KB\\nThanks :)Nvm I figured it out, I needed to add system32 to my user’s environment variables. That’s why ‘where’ wasn’t recognisedPowered by Discourse, best viewed with JavaScript enabled',\n",
       " 'how-to-play-rtsp-stream-with-hardware-accelaration': 'As a developer, I’ve already gone through Multimedia library and I knew there are a lot samples demonstrating how to play H.264 files by hardware VideoDecoder.\\nBut what I want is to play a real-time h.264 stream over RTSP. Is there any clues or where should I start from?\\nNot much favor for GStreamer.\\nThanks!Powered by Discourse, best viewed with JavaScript enabled',\n",
       " 'cxrhmdtrackingflags-hasprojection-does-not-appear-to-work': 'There is a new flag “cxrHmdTrackingFlags_HasProjection” which looks like it’s supposed to allow for updating the field of view values dynamically at runtime.  However, including this flag in the HMD tracking state flags, along with new projection values, does not appear to change the FOV used by the video stream.  Is this a known issue?  Has anyone else reproduced this?We’ve tried using that option while trying to solve the issue below on the Focus 3 sample app but it didn’t work either.Stereoconvergence issue with CXR 3.2 and the Vive Focus 3 - XR – VR/AR/MR / CloudXR (VR and AR Streaming) - NVIDIA Developer ForumsPowered by Discourse, best viewed with JavaScript enabled',\n",
       " 'arb-texture-stencil8-in-fbo': 'Hi, I’m trying to have a FBO with separate depth and stencil buffers.\\nI know that NVidia GPU’s historically only supported packed depth/stencil.\\nHowever I stumbled on the ARB_texture_stencil8 extension and wonder how to use it against a FBO.\\nMy code gives gl error 1159 on glFramebufferTexture2DEXT(GL_FRAMEBUFFER_EXT, GL_STENCIL_ATTACHMENT_EXT, GL_TEXTURE_2D, fboStencilTexture, 0):Powered by Discourse, best viewed with JavaScript enabled',\n",
       " 'mingw-linking-error': 'Is there a way to use PhysX with MinGW?I’m currently trying but I get a linking error:My command line:And finally my code: http://pastebin.com/aL5yBCgjHow do I fix this linking issue? I really don’t want to switch to MSVC.Hi,The PhysX Android(ARM) binaries can be built with MinGW, so it should also work for x86.\\nPxDefaultErrorCallback should be in PhysX3Extensions.Hey,Thank you for your answer.PhysX3Extensions.lib is in search path and nm found the constructor and destructor. The --start-group is a great idea but dosen’t fix this problem.I got another idea: I just wrote an own simple error callback. It compiled! But if I start the program, it crashes with exit code -1073741819.How do I fix this issue?Found the error: the MinGW ABI may be incompatible with the libraries. Moved discussion to [url]MinGW libraries - Physics Modeling (closed) - NVIDIA Developer ForumsPowered by Discourse, best viewed with JavaScript enabled',\n",
       " 'how-to-install-nvidia-driver-in-nxp-lx2160a': 'I want to install a driver of P2200 in NXP LX2160A. So , I expect to get some advice, how to install it? Is there has a driver which support for NXP LX2160A?Hi @pucong.wang,Welcome to the NVIDIA Developer forums! You posted in the Networking category, I believe your issue should be posted in the Drivers category.I will move it over for you.Regards,\\nTom KHi there @pucong.wang!Please visit our Driver download page and on the OS dropdown menu click on “Show all Operating systems”. Then you will find “Linux aarch64” as one selection. That will bring you to the corresponding ARM version of our drivers for Linux operating systems.If you run into issues with the installation, we do also have a dedicated category for Linux related questions.Best of success!Powered by Discourse, best viewed with JavaScript enabled',\n",
       " 'the-animation-in-my-optix7-program-does-not-work-correctly': 'Hello！I created an animation, it can be displayed correctly when I move the camera, but when I don’t move the camera, it can’t be displayed correctly like being interpolated. I checked the code and found that it was caused by subframe_index . When the camera is not moving, a program similar to interpolation is executed. Is there any way to solve this problem? By the way, I am using a path tracing algorithm.Thank you for your help!If you’re using a progressive path tracing algorithm which accumulates the converged final image over multiple sub-frames (samples per pixel), then this progressive rendering will start from the first sub-frame after each change to the view to generate the new final image from scratch.If you’re using an animation and you do not reset the sub-frame index to the first sub-frame but still advance your animation every sub-frame, then your animation will simply accumulate over each other at different sub-frames which generates a kind of stutter blur effect with the animation smeared over the final frame.If you want to have the animation rendered out to converged final frames, then you would need to have two nested loops in your application, one over the animation frames (the full result images) and one over the number of sub-frames you want to accumulate per final image so that each animation frame starts at the first sub-frame and accumulates all samples per pixel you define.If you just want to animate the object every frame, and have it shown as quickly as possible, than it you would need to reset the sub-frame index every time the view or the animation changes. Exactly the same thing as above but not rendering all sub-frames for each animation frame.An example of something like that can be found in one of my OptiX 7 examples which implements linear and scale-rotate-translate (SRT) motion blur.\\nThat has a simple animation moving and rotating two cubes by changing some values in an animation timeline or the camera interactively and accumulates the current animation frame only.\\nLook at the intro_motion_blur example here: https://github.com/NVIDIA/OptiX_Apps\\nThe restartAccumulation() function in there resets the rendered sub-frame index to 0 to start a new image accumulation.  That example will “animate” while you’re changing the current frame in the GUI and converge to a final frame as soon as you’ll stop interacting.Thank you for your answer!If you want to have the animation rendered out to converged final frames, then you would need to have two nested loops in your application, one over the animation frames (the full result images) and one over the number of sub-frames you want to accumulate per final image so that each animation frame starts at the first sub-frame and accumulates all samples per pixel you define.Yes, as you said, I want to have the animation rendered out to converged final frames. But I feel confused about the method as you said, I hope you can explain it in detail, if you can, can you post the example code, it may help me a lot.About the advanced samples, I can’t install it correctly, it failed at installing the 3rdparty library, the generator has already install correctly , but the error like this:Thank you for your answer!If you pulled that github repository you already have the code I mentioned.Search the *.cpp files inside the intro_motion_blur example for the restartAnimation function. It’s called whenever the camera or animation GUI parameters are changed.Also look where the m_iterationIndex, which is set to zero in there, is used in the rest of the code to set the sub-frame index inside the renderer on device side. It’s this variable on the device side in the OptiX launch parameters:\\nhttps://github.com/NVIDIA/OptiX_Apps/blob/master/apps/intro_motion_blur/shaders/system_parameter.h#L65You would need to change your own code to work similarly with whatever your animation frames and rendered sub-frame indices are. Mind that these examples are a little more involved than the simpler OptiX SDK examples.On the 3rdparty.cmake script issue:\\nIt seems you ran the 3rdparty.cmd inside the proper x64 Native Tools Command Prompt for VS2017 or x64 Native Tools Command Prompt for VS2019 because it detected the cl.exe 64-bit architecture.That the 3rdparty.cmake script wasn’t able to detect the MSVS version from the cl.exe version string is most likely due to a different language localization than English.\\nThe regular expression\\nstring(REGEX REPLACE \".*Version (..).(..).*\" \"\\\\\\\\1.\\\\\\\\2\" cl_version ${cl_info_string})\\nlooks for the string \"Version \" and two two-digit numbers separated by some character.\\nThat can fail for non-English localizations of the compiler which have a different string than Version there. (That happened for Spanish in the past.)In that case, just execute the cl.exe inside your x64 Native Command Prompt for VS and look at what version it reports.\\nThere is a table further down in the 3rdparty.cmake  script which maps cl.exe versions to MSVS versions.\\nThen you can either change the regular expression in line 36 to match your language or simply hardcode the VS generator in the if-else blocks at line 62 https://github.com/NVIDIA/OptiX_Apps/blob/master/3rdparty.cmake#L62E.g. if you’re using MSVS 2019, keep these two lines:For the rest of the 3rdparty pre-requisites and the build process, please follow the README.md by the letter.Powered by Discourse, best viewed with JavaScript enabled',\n",
       " 'vrworks-360-rig-calibration-issue': 'Hello,\\nWe want to use VRWorks 360 for stitching frames from four fisheye cameras in mono mode, but can’t pass calibration routine. We used nvcalib_sample application with ours .xml rig description and frames. But calibration process fails with NVCALIB_ERROR_FAILED_CONVERGENCE.\\nDescription of lenses we are using is here http://www.fujifilm.com/products/optical_devices/pdf/cctv/fa/fisheye/fe185c057ha-1.pdf\\nXml description is the following:Can somebody explain which parameter can affect to calibration convergence?I have encountered the same issue.Powered by Discourse, best viewed with JavaScript enabled',\n",
       " 'physx-3-3-4-multithreading-issue-cant-get-a-shapes-scene': 'I’m running PhysX in a mutithreaded environment, so I need to lock the scene when accessing data. This works fine for the scene itself, or an actor by calling PxActor::getScene(). However, when accessing a shape, I need to call PxShape::getActor()->getScene(), which fails because PxShape::getActor() checks to see if the scene is locked before returning the actor. Is there some way around this without caching the scene somewhere? I’d prefer not to have that extra data around.Powered by Discourse, best viewed with JavaScript enabled',\n",
       " 'retiring-swapchain-with-vulkan-on-linux-causes-swapchain-image-acquisition-timeout': 'Passing an existing valid swapchain to the oldSwapchain parameter of VkSwapchainCreateInfoKHR causes a timeout with vkAcquireNextImageKHR.Here is an example self contained program. Resizing the surface is the most common case of needing to recreate the swapchain, so resizing triggers the swapchain resize in this example. Comment out the linked line and the program works perfectly fine with Linux. Compiling and running the same program listed here on Windows works fine without commenting this line.\\nFrom outside observation it looks like internally the driver uses the remaining swapchain images in the previous swapchain before switching to the newly created one. However, it doesn’t seem to actually switch to the new swapchain and therefore times out acquiring the next swapchain image for the new swapchain. I guess this is the case  because it acquires a couple more swapchain images before it hits the timeout.nvidia-bug-report.log.gz (409.2 KB)Bumping since this may have been lost.Powered by Discourse, best viewed with JavaScript enabled',\n",
       " 'nvapi-getsupportedmosaictopologies-and-nvapi-enablecurrentmosaictopology-return-3': 'mosaic api return -3 , Can who anyone give me some helps?Hi,NvAPI_GetSupportedMosaicTopologies and NVAP_EnableCurrentMosaicTopology are legacy APIs only used on Windows XP for NVIDIA QuadroPlex products.To enable MOSIAC on Windows 7 or Windows 10 you will need to use the following APIs:Each call has the parameter NV_MOSAIC_GRID_TOPO which defines the display grid in terms of rows, columns, display ids that make up the GRID and the display resolution.Code snippet to get return the current GRID is:NvU32 get_current_grid(NV_MOSAIC_GRID_TOPO *gridTopo)\\n{\\n//Enumerate display grids\\nNvU32 gridCount;\\nNvAPI_ShortString estring;\\nNvAPI_Status   status;// gridTopo->version = NV_MOSAIC_GRID_TOPO_VER;}Note: If MOSAIC is not enabled when you call this it will return multiple GRIDS each wait a single display.Once you have defined the grid you want to set you can validate if it is supported on your setup:void mosaic_validate_displayGrid(NV_MOSAIC_GRID_TOPO *gridTopo, NvU32 gridcount)\\n{\\nNvAPI_ShortString estring;\\nNvAPI_Status   status;}Finally to enable MOSAIC:Hi, ryparkI have met challenges in use the NVIDIA MOSAIC API to realize the fuctions of “configureMosaic.exe”.\\nCan you give me a demo about this?\\nHope for reply, Thanks!\\nBest Wishs.Could you provide more specific details?Are you asking for help to develop a configureMosaic like app, or asking for help to run “configureMosaic”?Thanks,Ryan ParkYes, I’m asking for help to develop a configureMosaic like app.\\nHope for reply, Thanks!\\nBest Wishs.Powered by Discourse, best viewed with JavaScript enabled',\n",
       " 'unable-to-install-nvidia-driver-on-debian-10': 'Hello\\nI have nvidia GeForce 920MX\\ndebian 10 kde\\nCP intel i5 4288U\\nan error occurred during installationnvidia-installer log file ‘/var/log/nvidia-installer.log’\\ncreation time: date\\ninstaller version: 440.31In File\\nPATH: /usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/binERROR: Installation has failed.  Please see the file ‘/var/log/nvidia-installer.log’ for details.  You may find suggestions on fixing installation\\nproblems in the README available on the Linux driver download page at www.nvidia.com.\\nERROR: Installation has failed.  Please see the file ‘/var/log/nvidia-installer.log’ for details.  You may find suggestions on fixing installation\\nproblems in the README available on the Linux driver download page at www.nvidia.com.Powered by Discourse, best viewed with JavaScript enabled',\n",
       " 'lib-files-missing-from-physx-3-3-master': 'I am trying to port the PhysX3.2 to 3.3 to 3.4 but found that the folder structure is different for PhysX-3.3 (old version) vs PhysX-3.3-master (git hub). The latter also lacks a lot of .lib files PhysX3Common_.lib and\\nPxFoundation_.lib along with dll files PhysX3Cooking_.dll, PhysX3CharacterKinematic_.dll. I am using Cmake and VS11 for linking graphics and openhaptics. Cmake file requires folder structure for these files but with newer master files does it still need?You get CPU source code with PhysX-3.3-master on git. It doesn’t include pre-build binaries apart from those for GPU simulation components. You build the libs/dlls from the source files yourself. Solution files are included in PhysXSDK/Source/compiler folder.Powered by Discourse, best viewed with JavaScript enabled',\n",
       " 'maya-2015-and-3-3-2-keyframe-constraint-on-off-not-working': 'Has anyone been able to get this to work or have a workaround? I’m trying to turn a constraint off at a particular point in the timeline. However it just stays locked at whatever is the first frame of the simullation. I want to start with it on, then turn it off, at say frame 300. But It stays on regardless of the keyframe value.I have the same problem and i’m in maya 2014Is this using the latest Maya plugin? Which version are you using?Powered by Discourse, best viewed with JavaScript enabled',\n",
       " 'nvidia-jetson-module-as-a-window-client': 'Hi,We are considering attaching a portable pc to a tethered headset, and streaming the VR content from the cloud using CXR.We are wondering would there be a possibility of configuring one of the NVIDIA Jetson modules for running the Windows CXR client as we have tested the Windows client on our workstations and successfully streamed VR content to HP Reverb G2/G1. We’ve noticed that the Windows Mixed Reality Portal has several requirements for enabling the WMR headset (HP Reverb G2/G1). Therefore, I’m not sure which module would be suitable for the minimal requirements.Many thanks,ZehaoExtending question:\\nIs there available or when will be available generic ARM client (not for android but for linux clients like RasperryPI, Pine64…) ?\\nOr will the client be opensourced ?Hi,A lightweight end client device is an interesting idea, and something that warrants investigation in the future, however the Jetson runs L4T, and there are no L4T clients available. Regarding the generic ARM clients, there is nothing available for them either.Thanks,\\nWillPowered by Discourse, best viewed with JavaScript enabled',\n",
       " 'official-product-feature-request-portal-for-cloudxr-launched': 'CloudXR has grown considerably and we’re looking to better capture feature requests from everyone. Therefore, we’re excited to be launching a new feedback portal that allows you to submit ideas for CloudXR. Ideas that get submitted will be reviewed by us and help guide the future of the product. We value thoughtful submissions; please be detailed on the idea and include specifics on how it will help you with your work (and even describe your work). As an example, submissions that are written like user story are the most helpful.To submit a product feature request, visit: https://nvidiaxr.ideas.aha.io/Powered by Discourse, best viewed with JavaScript enabled',\n",
       " 'cloudxr-port-setting-on-aws': 'When I setup a CloudXR AMI on AWS, several ports are requested to open by the security group: 48010, 47998-48000, 48005, and 48002 for TCP and UDP.\\nCould I get any support document to justify those ports? So I could provide information for security team to understand the purpose of enabling access over each port.Hi,\\nI was also courios about which ports to open and found this list in the forum. Maybe it helps:\\n|TCP| 47999 |CloudXR Control|\\n|UDP| 47999 |CloudXR Control|\\n|TCP| 48000 |CloudXR Audio|\\n|UDP| 48000 |CloudXR Audio|\\n|TCP| 47998 |CloudXR Video|\\n|UDP| 47998 |CloudXR Video|\\n|TCP| 48005 |CloudXR Video|\\n|UDP| 48005 |CloudXR Video|\\n|TCP| 48002 |CloudXR Microphone|\\n|UDP| 48002 |CloudXR Microphone|\\n|TCP| 48010 |CloudXR RTSP|\\n|UDP| 48010 |CloudXR RTSP|Note from Nvidia documentationAll ports needed are UDP, except RTSP, which is TCP.The RTSP stream is used to establish the connection. Once this is done the individual streams transmit data with RSP.This topic was automatically closed 14 days after the last reply. New replies are no longer allowed.Powered by Discourse, best viewed with JavaScript enabled',\n",
       " 'defining-and-referencing-an-enum-within-a-module-causes-name-lookup-errors': 'Hey,we are just getting started with the mdl sdk and we have some troubles with enums. We’ve created a little repro case that should generate a mdl module that looks somewhat like this:However, what we actually get is this:Note that the return type of the function uses a fully qualified name, as well as the default value for arg0, while the type of arg0 doesn’t.We did some debugger hackery to get this dump, by forcing it in MDL-SDK\\\\src\\\\io\\\\scene\\\\mdl_elements\\\\mdl_elements_module_builder.cpp:2215 so this is not a final output.Unfortunately, we can’t generate the module because the module analysis fails because it can’t find the module ::test::materials. Here are the error messages:C108 ‘test::materials’ is not a package or module name\\nC108 ‘test::materials’ is not a package or module name\\nFailed to analyze created module “::test::materials”.What can we do to fix these errors?We are using VS 17.1 (and 16.11) with C++20 and we built the mdl sdk from current master (commit d6c9a6560265025a30d16fcd9d664f830ab63109).Here is the full repro codeI can also share a complete VS 2022 project with our mdl sdk build if you need it. We didn’t make any changes to the sdk itself tho and the code above is the same.Hi Timo,what a nice and complete problem description :-)\\ni will get a dev on it.Powered by Discourse, best viewed with JavaScript enabled',\n",
       " 'iray': ':) Hey everybody!..I’m new here. And OK I know that this is going to sound like a stupid question. But…Where do we go to download a copy of Iray??..Get back to me please,\\nThanks!\\nStar :)Hi there @blue.starfire and welcome to the NVIDIA developer forums!No stupid questions around here and since Iray is a bit different to other SDKs, it might seem hard to find the exact information how to get access to it.If you check out NVIDIA Iray GPU Rendering | NVIDIA and scroll down a bit you will see that Iray is only available as an integration into 3r party tools or as an SDK integration through our partners.There is no classical “Download” link available.I hope I could help.Powered by Discourse, best viewed with JavaScript enabled',\n",
       " 'some-recent-work-using-optix-7-6': 'I’ve been having a lot of fun lately with a natural light rendering app that I’ve cobbled together from some great open source projects, including shocker-0x15’s OptiX_Utility wrapper and his graphics experiments.OptiX 7 Lightweight Wrapper Library. Contribute to shocker-0x15/OptiX_Utility development by creating an account on GitHub.Sandbox for graphics paper implementation. Contribute to shocker-0x15/GfxExp development by creating an account on GitHub.Here’s a link to the renders:Most of these renders were made just by using Newton Dynamics physics engine to make a pile of stuff and then hunting around to find something worth rendering. They all render around 40-60 fps full screen using a Geforce RTX 3090.Here’s a link to that processThanks again to the OptiX team for making such an amazing tech available to the masses!Powered by Discourse, best viewed with JavaScript enabled',\n",
       " 'ubuntu-error-when-installed-nvidia-415-driver': 'i installed ubuntu 18 with nvidia RTX 2080 of GPU. Then i downloaded and installed the gpu drivers nvidia 415. Then i rebooted it shows this msg only no GUI./dev/vda2: clean, 168760/32735232 files, 4049825/130940416 blockplz help mei installed ubuntu 18 with nvidia RTX 2080 of GPU. Then i downloaded and installed the gpu drivers nvidia 415. Then i rebooted it shows this msg only/dev/vda2: clean, 168760/32735232 files, 4049825/130940416 blockUsing Alt+Ctrl+F1 i can enter the no GUI cmd mode and login.plz help meUsing Alt+Ctrl+F1 i can’t enter currently cmd modePowered by Discourse, best viewed with JavaScript enabled',\n",
       " 'how-to-execute-hostapd-and-hostapd-cli-in-android-terminals': 'Hi all,I always got error. When i execute hostapd command. I was changed everything in my hostapd.conf file but i got error.hostapd_cli wps_pbc Failed to connect to hostapd - wpa_ctrl_open: No such file or directoryroot@sabresd_6dq:/system/etc/wifi # hostapd_cli\\nhostapd_cli v2.5-devel-6.0.1 Copyright (c) 2004-2015, Jouni Malinen and contributorsThis software may be distributed under the terms of the BSD license. See README for more details.Could not connect to hostapd - re-tryingHow to fix this.?Thanks,\\nVinothS,Powered by Discourse, best viewed with JavaScript enabled',\n",
       " 'rtx-remix': 'hi! just 2 questions about RTX Remix.\\n1: if RTX Remix can be used with games that use DirectX 9.0, can you use it with DirectX 9.0 EX?\\n2: will there be support for more DirectX versions?just some questionsHello pantantuna,As you can see on the official RTX Remix landing page, this tool has no official release yet. As soon as that is out, there will also be extensive documentation and FAQ to answer most questions coming up. I already reached out to the group and shared your questions with them.For now all I can say is that it should cover all of the DX9 spec, but there is no sample available or testing done right now for DX9 EX specifically.At the moment Remix supports DX8 and DX9 technology. Support for more versions is still under discussion.I hope this helps!just another question, will rtx remix come out at the same time as portal rtx?also have they tested DX9 EX? (I want to make Sonic Generations RTX)The Remix RTX Portal version will come out this week, you can join the official launch party on Twitch or Steam tomorrow!You can ask there as well about general availability of RTX Remix, but I think that they will answer the same as I that you should sign up for release notification on our dedicated Remix page.On DX9 EX I don’t have any information to share.Thanks.hello, since portal RTX has been released, why hasn’t RTX remix come out yet?just a questionalso, I forgot about asking about rtx remix so I didn’t ask them, can you tell me, please?Oh that is easy to answer. Portal RTX was done in house by our own engineers. It was quite an effort and a way to try out Remix before giving it out. That means now comes the work to polish Remix and guarantee great user experience once it is released.Thanks for your patience!Powered by Discourse, best viewed with JavaScript enabled',\n",
       " 'flex-physx-interop': 'So the manual for Flex says that “it is recommended to use Flex in conjunction with a traditional rigid-body physics engine, such as PhysX”. Looking very briefly through the API reference I see no means of communication between the two, although this is hinted at in at least one NVIDIA presentation.How are Flex and PhysX supposed to work together?Hello there, does Flex has to be installed separately or it comes with the PhysX SDK 3.4?Flex is distributed separately from PhysX. You can now find the latest Flex binaries on the developer downloads section, or by accepting the Gameworks EULA:https://developer.nvidia.com/content/apply-access-nvidia-physx-source-codeAnd going to the Github Gameworks page:NVIDIA Technologies for game and application developers - NVIDIA GameWorksFor an example of how to interface Flex and PhysX I would recommend taking a look at UE4 Flex integration:https://github.com/NvPhysX/UnrealEngine/tree/FleX-4.15.0(Must be a registered UE4 developer)Cheers,\\nMilesPowered by Discourse, best viewed with JavaScript enabled',\n",
       " 'video-decoder-frames-latency-between-first-frame-inserted-and-first-frame-extracted': 'Hello everybody!I’m new to this forum so I hope I’ve submitted my questions into the right place (if not please let me know wich is the correct section).I use HW decoders to decode both h264 and MJPEG streams coming from video surveillance cameras.\\nMy example “acquisition pipe” works the same as the “cudaDecodeD3D9” example, which comes along with the SDK (tipically in C:\\\\ProgramData\\\\NVIDIA Corporation\\\\CUDA Samples\\\\v8.0\\\\3_Imaging\\\\cudaDecodeD3D9), except for the VideoSource object. So we correctly use the VideoParser and the VideoDecoder.Everything is working perfectly with h264 intra and not intra frame streams (gop > 1) and with MJPEG streams.The only thing I’ve noticed is that it takes some frames to receive the first HandlePictureDisplay callback even if my stream gop is equal to 1 (MJPEG or H264 intra). The first frame passed to the parsed and to the decoder is always a key-frame or I-frame even ig gop > 1.For the first frame passed to the parser and to the decoder I set CUvideopacket = CUVID_PKT_TIMESTAMP | CUVID_PKT_DISCONTINUITY, for the further frames to CUVID_PKT_TIMESTAMP, and for the last frame to CUVID_PKT_TIMESTAMP | CUVID_PKT_ENDOFSTREAM. In this scenario I was not able to get the first HandlePictureDisplay before submitting other frames to the decoder.I also tried to set the CUVID_PKT_ENDOFSTREAM flag to the first frame and the first frame comes out before submitting the next one but it only works in some circumstances (sorry but I was not able to identify them).So, I assumed that an entire GOP has to be decoded before receiving the first HandlePictureDisplay.Thank you in advance, I hope someone can enlighten me on this subject.\\nRegards!GiorgioI have the same “problem”. I have live streams as well. I have to put in 5 frames before I get the first out. Is there a way to lower this latency?I actually use ffmpeg for decoding, it has NVDECODE integrated. If i use the ffmpeg software h.264 decoder, it return the first frame right after I pass the first frame. If I use h264_cuvid, it takes 4 extra frames to pass before it returns the first frame.For testing I added a one second sleep between pass a frame (avcodec_send_packet(…)) and getting a decoded frame (frameFinished = avcodec_receive_frame(…)). The results are the same.Thanks in advance.Internally, the hardware is pipelined, and yields maximum performance when handling multiple frames at different stages of the decoding process. I expect that the hardware requires the pipeline to be filled before it will start returning anything, and that this is independent of GOP or I-frames.I would expect that setting ENDOFSTREAM would force it to decode the single frame (I don’t know what problems you saw, but they should be separate and solvable), but I’d also expect the performance to be bad - worse than realtime bad? I don’t know, but noticeable, to be sure.Is there somebody from NVIDIA that can give an defintive answer on this? It will be much appreciated. Thanks in advance.I have the same problem with SDK-9.0.22 that the first 5 frames are buffered. Is there somebody than can explain this?Thanks in advance.Hello.We would suggest to follow below steps to reduce latency during decoding process:We are hoping that #1 and #2 should solve the problem. If it doesn’t, please share the bit-stream with us. For H264, will you be using I frame only stream?Thanks.Hello Mandar!By the way, if I set CUvideopacketflags::CUVID_PKT_ENDOFSTREAM in cuvidParseVideoData(), the decoder output immediately. But it only works with I frame.Hi.\\nCan you share the failing stream with us to analyze? And, also help me understand what use case this is.Thanks.Hi mandar,Sorry for the late reply. Actually, I’m working with a live stream through RTP from an IP camera. I record a video from it and below is ther URL.Thanks.Hi mandar_godse. Do you have any update on this topic?  I have a single frame H264. and decode this frame. setting like your suggest but need to push other data second data (even that NULL data, size 0) to get the first decoded data.Powered by Discourse, best viewed with JavaScript enabled',\n",
       " 'released-video-codec-sdk-7-1-9': 'New release Video Codec 7.1.9 (compatible with Linux drivers 375.95 or newer and Windows drivers 375.20 or newer) is downloadable on nicely updated page [url]https://developer.nvidia.com/nvidia-video-codec-sdk[/url].\\nThe essential GPU Support Matrix page [url]https://developer.nvidia.com/video-encode-decode-gpu-support-matrix[/url] does not work for me (yes, I am logged and joined DesignWorks). Is someone successful ?\\nI still cannot find any correct information which chips or variants of chips have one or two or three encoding engines ! It has major impact on encoder speed (see m60/m10/m6 comparison [url]https://gridforums.nvidia.com/default/topic/823/grid-boards/tesla-m10/[/url]).It works now with added requested encoder count information.Powered by Discourse, best viewed with JavaScript enabled',\n",
       " 'pick-ray-in-optix7': 'I’m trying to generate a pick ray in Optix7.  The actually picking part seems to be working okay but I can’t figure out how to get the meshID and primitiveID back to my application. Here’s the program code.\\n[url]https://gist.github.com/Hurleyworks/a5798887b4e679fb8f0943615e8ed331[/url]When generating a pick ray the error message is  CUDA error on synchronize with error ‘an illegal memory access was encountered’ Lines 74 and 75 are causing the problem but I’m not sure why.\\nparams.pickData[0]= payload.meshID;\\nparams.pickData[1]= payload.primitiveID;extern “C” {\\nconstant PickParams params;\\n}Change the code to not write into the memory you access via the OptixPipelineCompileOptions.pipelineLaunchParamsVariableName, here “params”. That lies in constant memory.\\nInstead put a CUdeviceptr there which points to a buffer with the two results and write to that.The OptixTraversableHandle is a 64-bit value and must lie on an 8-byte aligned address. Same for CUdeviceptr.\\nAll other fields in that PickParams stucture require only a 4-byte aligned addresses.\\nI recommend to always order all device side structures by alignment requirements with the largest first.\\nIf the structure is used in an array, I even manually pad its overall size to the largest alignment.I would make the picked IDs unsigned int because both optixGetPrimitiveIndex() and optixGetInstanceId() return unsigned int values and the payload registers are also unsigned int references, which means there is no need for any integer conversions.\\nYou can use ~0 as miss identifier which is safe in OptiX 7.0.0 because neither OPTIX_DEVICE_PROPERTY_LIMIT_MAX_PRIMITIVES_PER_GAS nor OPTIX_DEVICE_PROPERTY_LIMIT_MAX_INSTANCES_PER_IAS can exceed 4 Gig.Great, I’ve got it working now … except for using -0 as a miss identifier. That comes back as 0 on my machine which of course isn’t useful since it could be a valid instance or primitive index. Using -1 gives something close to the max unsigned integer so that’s easy enough to look for.Thanks for the help!No idea what your problem is with unsigned int ~0. Did you mistype “minus zero”? I wrote “bitwise not zero”.\\nThere should be absolutely no change to the bits if every variable type in the code holding it is unsigned int.Heh, yes I thought you typed minus zero … due to small monitor, old eyes and a tired brain :)I’m having some trouble integrating picking into my projects. My apps are physics based content creators that can have thousands of instances. Users can paint instances so I need to render the scene while generating a steady stream of pick rays when the user is painting.I have tried 2 approachesThe full working code for this is here [url]https://github.com/Hurleyworks/Optix7Sandbox[/url]Here’s a slightly simplied version of the cuda code[url]https://gist.github.com/Hurleyworks/0997f7a4b0247988d7659c90dbc043dc[/url]Any idea about how to make this work?I would make the picking a special case of the rendering.All you’d need is to make the data required for picking available on the per ray payload, have a place where to output them, and distinguish the current launch mode.That could be done with a flag and the picking coordinates in the global launch parameters. Handle both cases from a single ray generation program and combined global parameters instead of PickParams and WhittedParams in your case. (Alignment requirements in the latter is wasting space between members again.)\\nThe closest hit on a primary ray would need to fill in the hit IDs when picking is enabled.There should be no need for a separate ray type either then. That just bloats the number of hit record entries in the Shader Binding Table (SBT). Two ray types should be enough to handle all cases (rendering/picking and visibility check.)The launch dimension would need to be decoupled from the rendering resolution to be able to calculate the single picking ray (launch dimension 1x1) on the full image resolution. (That’ll be required for multi-GPU rendering distribution anyway.)You can also keep picking in a separate ray generation program.\\nFor that you’d simply put both the picking and the Whitted ray generation program entry points into one OptiX pipeline and then create two SBTs where only the ray generation program record is different.Means if you build all your other program domains to handle both rendering and picking, then you don’t have the issue with multiple SBT records for the other program domains. All other CUdeviceptr holding the SBT records are identical then. Dynamic scene changes happen only once.To unify the payload count, you can simply allocate the per ray payload structure inside the ray generation program and split its pointer into the first two payload registers of the optixTrace call. You can get away with using only 2 registers in any arbitrarily complex rendering algorithm that way.Other comments from glancing over the device code:Thanks for the reply! Unfortunately there’s a lot I don’t understand. :)I’ll start with the comments I think I do understand.* There is no need for __miss__pickMiss if you initialize the picking results for a miss event in pick().\\nInstead of uint32_t u0=0, u1=0; use uint32_t u0=~0, u1=~0; and the miss program is unnecessary.\\nThe initialization of payload.meshID = ~0; payload.primitiveID = ~0; is unnecessary because they will be written unconditionally (unless there are exceptions during optixTrace()).Okay, that makes sense. Thanks!Why are you setting OptixVisibilityMask( 1 )? The default mask to hit everything should be OptixVisibilityMask(255).I copied that from the whitted.cu used in the optixMeshView sample so maybe that needs to be changed. I’ve not seen any exception program in the *.cu files. It’s highly recommended to use them for debugging purposes.I couldn’t find any example of how to use make an exception program in either the OpitX Samples or in Ingo Wald’s tutorialsThe launch dimension would need to be decoupled from the rendering resolution to be able to calculate the single picking ray (launch dimension 1x1) on the full image resolution. (That’ll be required for multi-GPU rendering distribution anyway.)I’m not sure how to do that, could you explain that a little more?For that you’d simply put both the picking and the Whitted ray generation program entry points into one OptiX pipeline and then create two SBTs where only the ray generation program record is differentIf I have 2 SBT’s then a I have to make 2 separate calls to optixLaunch(), one for picking and one for rendering, correct?I think that’s enough for now. Thanks for the helpDecoupling the launch dimension from the rendering resolution just means that you define the resolution of your output buffer inside the global system parameters and use that instead of the launch dimensions to calculate the primary rays.If I have 2 SBT’s then a I have to make 2 separate calls to optixLaunch(), one for picking and one for rendering, correct?Yes, if you have different ray generation programs you always need two launches.1.) This example would use only one ray generation program and would still need two launches with different dimensions but has only one pipeline and SBT. (You can do with one as well, see below.)2.) Since you said you need a continuous stream of picking results, it’s really simple to combine picking and rendering into one launch as well. Means one of the launch indices would return picking results on every render.For that you would just need to have an uint2 pickingPixel; in the SystemData and then indicate on the per ray data (PRD) for the primary ray where theLaunchIndex == sysData.pickingPixel that this ray should also fill in the PRD field with the hit IDs.\\nThat needs a depth field on the PRD to identify primary rays (just like above as well).I couldn’t find any example of how to use make an exception program in either the OpitX Samples or in Ingo Wald’s tutorialsThis is what I’m using.Thanks for the excellent help as usual!One question before I try to make this work.// The closesthit programs would be responsible to return the desired picking information on the resp. per ray data fields and end the path if picking is enabled.The problem I see with ending the path in the CH program when picking is enabled is that I’ll never get an updated render while continuous picking and that won’t for my needs. Is there any way to get a render while continuously picking?One piece of good news is that I’ve implemented geometry instancing in my project and am able to pick instances too … that’s something I could never get working in OptiX 6I described two different cases above. I added numbers to them now for clarity.\\nThe second idea with the single launch would of course continue rendering, but would only need to save the hit IDs at the single selected primary ray once.\\nMeans it needs the ray depth or path length to know that it’s a primary ray, and a flag to indicate that this is the picking ray, and the result IDs on the PRD must be initialized for the miss event before the optixTrace() of that primary ray.I went with the 2 launch version and it’s working perfectly. Thanks again for all the help!A couple of things if anybody else is following alongShould be changed to(Detlef: Thanks, is corrected above now.)And when setting the pickingFragment in you app don’t forget that Optix screen space origin is bottom left, not top left … it took me awhile to figure out why my picking code wasn’t working. :Oops, yes, that was dry coding. I’ll correct it.Yes, the launch index (0, 0) of the pinhole camera is the bottom left corner, matching the orientation of OpenGL texture images used for the final display.\\nSee slide 18 here: [url]http://on-demand.gputechconf.com/gtc/2018/presentation/s8518-an-introduction-to-optix.pdf[/url]\\nThat’s solely under the developer’s control. OptiX doesn’t imply a screen coordinate system layout.Powered by Discourse, best viewed with JavaScript enabled',\n",
       " 'glsl-compute-shader-have-a-bug-with-loop-operation': 'first of all, sorry about my short English.i found a unexpected situation when using “for” loop increment term with decreasing.\\nthis situation could reproduce with code like below.the index value “gl_LocalInvocationIndex” is zero because the shader is dispatched by single invocation and single group.\\nso we can predict a result easily.\\nbut result is not matched. oData[1] = 1          oData[1]=0\\n oData[2] = 2          oData[2]=0\\n oData[3] = 3          oData[3]=3\\n oData[4] = 4          oData[4]=4\\n oData[5] = 5          oData[5]=5\\n oData[6] = 6          oData[6]=6\\n oData[7] = 7          oData[7]=7\\n oData[8] = 8          oData[8]=8is this a bug? or my mistake?\\nif it is a bug, what should i doing for resolve it?First of all, for the record, is indexing of oData You written in Your post correct? You know that indexing starts from 0 in most of the C-derived languages?Second question - I can’t deduce the size of oData buffer? Or is it set up in host code?MKFirst of all, for the record, is indexing of oData You written in Your post correct? You know that indexing starts from 0 in most of the C-derived languages?Second question - I can’t deduce the size of oData buffer? Or is it set up in host code?MKthe value of predicted and result oData at index [0] is 0 respectively, just it was omitted. and indexing rule of c-language is common sense. :)the size of shader storage buffer is initialize in host side. in this case, it was allocated 1024 bytes and set by zero before the compute shader dispatched.the link of below is a captured screen of IDE. it could be helpful, if you want to check a result.\\nhttp://nvidia.custhelp.com/ci/fattach/get/141105/1378730288/filename/decrease%20case%20(incorrect).pngand the link of below is indexing with increasing case.\\nhttp://nvidia.custhelp.com/ci/fattach/get/141106/1378730288/filename/increase%20case%20(correct).pngPowered by Discourse, best viewed with JavaScript enabled',\n",
       " 'driver-crash-in-vkqueuepresentkhr-upon-unplugging-external-hdmi-display-on-windows-10': 'Hi,I’ve experienced an NVIDIA driver crash on several Windows-10-powered PCs in an SDL2/Vulkan-based game that my company is developing and have been able to reproduce it almost every time under the following conditions:At this point, depending on your hardware and configuration, different things can happen:Our internal logs showed that the crash/unexpected behavior occurs in the vkQueuePresentKHR function (time logs indeed showed that 2-10 seconds are spent inside this function when the issue occurs), but the latest Vulkan SDK’s validation layers (version 1.1.30) do not output any error message, apart from the vkCreateDevice error message above.We tried to reproduce this issue with an OpenGL context instead of a Vulkan context, and all of our attempts have failed: unplugging the external display device does not cause this issue.\\nWe also tried to reproduce it with a Surface Pro 4 tablet PC (which only has an Intel iGPU), both with Vulkan and OpenGL, but the issue didn’t show up in this case either.All of the former have led me to believe this is an NVIDIA driver bug that is specific to Vulkan (maybe a presentation engine bug?).Also, not sure how relevant this is, but our tests were performed using a Samsung 4K TV as the external display device, and the present mode we used was VK_PRESENT_MODE_FIFO_KHR.Looking forward to your replies!Best,\\nAlexStill getting this issue as of today.I no longer get a black screen on my gaming laptop as I used to, but I now get a BSoD with error PAGE_FAULT_IN_NONPAGED_AREA located in file nvlddmkm.sys (Windows 10 updates have probably changed the way such issues are handled by the OS, which, I believe, is the main reason why a BSoD is now triggered by this driver issue).For some reason, this crash does not occur when my application is run using the Visual Studio (2015, if any relevant) debugger (application behaves correctly, returning VK_ERROR_OUT_OF_DATE_KHR from the vkQueuePresentKHR function, and swap chain recreation works fine), but it does in all other conditions (running using the LLDB debugger, or running without any debugger). I guess the Visual Studio Debugger uses some kind of fault-tolerant heap here?I’ve tried a lot of workarounds (like calling vkDeviceWaitIdle every frame to prevent potential synchronization issues, or using functions from the VK_EXT_full_screen_exclusive device extension), but none of them has changed anything.NVIDIA, please consider taking a look at this issue. This driver crash still occurs with Sascha Willems’ examples, and Sascha Willems’s examples are considered as an almost authoritative reference on how to use Vulkan. Or, at least, please provide a workaround to avoid hitting the buggy code path in the driver.EDIT: just tested some Vulkan free and open-source software engines/games (vkQuake for instance) and they are all affected by this issue :/Vulkan driver team is looking into this issue. If you could provide some additional system information it would be helpful to match your setup.Hi Wen_Su,Thank you for investigating this issue.\\nPlease find the logs you requested in this Dropbox folder.This folder contains :Feel free to request additional logs if necessary.Best,\\nAlexutalex,The crash dump is helpful. We are able to pin point the crash location. Thank you for providing the details. Driver team is working on it now. I hope to bring an update once the bug is resolved.Those are great news, thank you Wen_Su!Looking forward to testing this update!Powered by Discourse, best viewed with JavaScript enabled',\n",
       " 'using-a-infinite-sphere': 'Hi,I want to unify calculation for the implicit light path hitting environment (infinite sphere) to the calculation of ordinary surfaces.\\nI think good solution is defining intersection/bounding box programs for infinite sphere and using code path for the calculation of implicit path hitting ordinary surfaces instead of writing calculation as miss program.However I’m afraid of that using bounding box program returning infinite(-inf to inf or -FLT_MAX to FLT_MAX) bounds affects performance because TRBVH uses LBVH as basis and morton code calculation in LBVH uses bounding box of entire scene.Is this apprehension true for TRBVH implementation in OptiX?Yes, adding a huge bounding box to the Bvh for the infinite sphere will add at least a slight overhead during ray intersection for 1 extra ray-box test.  You could improve on this by calculating the real bounding box of the scene and then creating a “miss” box that is only slightly larger (not inf or flt_max).What is your motivation to unify the closest-hit and miss programs?  Just for code cleanliness, or do you think having the same program will increase coherence?Thanks for the reply.What is your motivation to unify the closest-hit and miss programs?  Just for code cleanliness, or do you think having the same program will increase coherence?Both. I will have an overhead a bit due to abstraction to correctly handle inifinite distance, but I expect unifying code path of implicit light path hitting a light source (ordinary surface light and environment (infinite sphere) light) to improve performance.Yes, adding a huge bounding box to the Bvh for the infinite sphere will add at least a slight overhead during ray intersection for 1 extra ray-box test.This is inevitable and I must accept.I think LBVH first quantizes primitives’ coordinates by dividing space in the entire scene AABB into 2^N cells (for each dimension).\\nIf the entire scene AABB is very very large, almost all primitives belong to the same cell.\\nThis seems to introduce serious performance degradation.This is my concern.\\nShould I use another builder like Bvh?\\n(Can I avoid the performance degradation due to the infinite sphere?)Yes, what about the workaround of making your “infinite” sphere geometry a tight bound for the rest of the scene?  You just need the ray direction, so a sphere of any size larger than that minimal sphere should give the same result.  Unless I’m missing something.As far as how or whether Trbvh does initial binning, I’m not sure I can publicly answer that even if I knew :)That workaround seems good for secondary rays or beyond, but how about primary rays?\\nNow I think a new way like the followingGroup\\n(NoAccel)\\n/ . . . . . . . \\nGeomGroup . . . . . . . .Group\\n. (Inf Sphere) . . . . . . . . (Scene)This increases the layer of scene graph but seems good if an overhead is small.\\nI will try this.This will always test against the Inf Sphere group, even if the ray hits other geometry. I’m curious to see if this is any more efficient in practice than a miss shader.Powered by Discourse, best viewed with JavaScript enabled',\n",
       " 'nvidia-vulkan-samples-and-optimus-based-laptop-gpus': 'Our initial Vulkan samples use Vulkan and OpenGL interop. As such they require to be run on the NVIDIA GPU (and not the integrated GPU).Please refer to sections 2 and 3 of this guide about how to do this:http://docs.nvidia.com/gameworks/content/technologies/desktop/optimus.htm#Enabling_high_perf_rendering_optimusRegards,MathiasОбеспечивая высокую производительность графической системы NVIDIA рендеринга на системах ОптимусЭта функция включается с помощью следующей совокупности методов, перечисленных в порядке убывания их приоритета:Принудительный Режим Рендеринга\\nЩелкните Правой Кнопкой Мыши Контекстное Меню\\nНастройки Профиля Приложения\\nСтатическая Библиотека Привязки\\nГлобальная переменная NvOptimusEnablement (новое в драйвере выпуска 302)\\nГлобальные Настройки Профиля\\nГде это находится? я не понимаю по английски !Hi, I have a GeForce GTX 610 I downloaded the latest version of the Vulcan and I now will not start Dota 2 ! Writes error : Failed to initialize Vulkan. Please make sure your driver and GPU support Vulkan. What should I do?\\nHi, I have a GeForce GTX 610 I downloaded the latest version of the Vulcan and I now will not start Dota 2 ! Writes error : Failed to initialize Vulkan. Please make sure your driver and GPU support Vulkan. What should I do?Maks123134, Vulkan is supported in Kepler and later architecture. I do not think GeForce GTX 610 meets the minimum requirement.Powered by Discourse, best viewed with JavaScript enabled',\n",
       " 'need-help-identifying-missing-components-on-gtx-1080-fe': 'Hi, I was directed to this forum by nvidia support as they were unable to help me out, hopefully there’s an engineer here with access to a schematic or board view!A friend of mine was replacing the thermal paste on his GTX 1080 Founders Edition, and slipped with a screwdriver when re-fitting the cooler, breaking a small transistor and a capacitor from the board, and now the card won’t work. Unfortunately these tiny components have been lost.I have experience repairing motherboards and soldering delicate electronics so my friend has offered to send me the card to repair and keep, to replace my aging GTX 780, but I’m unable to identify the parts that I’ll need to replace, Q501 and C503 in this photo. They are on the back of the board near the power connectors.Q501 is labelled “LP” in this photo of a working 1080 FE, I could guesstimate the value of the ceramic cap, but I’m at a loss when it comes to this transistor.Any assistance would be greatly appreciated.Thanks, AlexHi Alex, same here. My GTX1080 FE is missing the Q501 component. Did you find something?\\nThank you!Just found out:\\nQ501 → RJU003N03 SOT-323C503 → 22uF 16VPowered by Discourse, best viewed with JavaScript enabled',\n",
       " 'how-to-receive-streamed-video-from-icamera2-onto-agx-with-gstreamer': 'I’m trying to receive live stream video from my icamera2 inside my network for the purpose of applying computer vision model to detect objects on my AGX. I couldn’t get my gstreamer to connect. My iCamera was configured to receive MJPEG. I was able to successfully connect to my camera from a Windows laptop using VLC. Interested is knowing if anyone has done this successfully. The gstreamer command I used was:gst-launch-1.0 uridecodebin uri=rtsp://camera ip:8555/test ! nvvidconv ! “video/x-raw(memory:NVMM), width=640, height=360” ! nvegltransform ! nveglglessink window-x=1981 window-y=180Error I got was:\\nUsing winsys: x11\\nPipeline is live and does not need PREROLL …\\nGot context from element ‘eglglessink0’: gst.egl.EGLDisplay=context, display=(GstEGLDisplay)NULL;\\nProgress: (open) Opening Stream\\nProgress: (connect) Connecting to rtsp://camera ip:8555/test\\nProgress: (open) Retrieving server options\\nProgress: (open) Retrieving media info\\nERROR: from element /GstPipeline:pipeline0/GstURIDecodeBin:uridecodebin0/GstRTSPSrc:source: Could not open resource for reading.\\nAdditional debug info:\\ngstrtspsrc.c(5829): gst_rtspsrc_setup_auth (): /GstPipeline:pipeline0/GstURIDecodeBin:uridecodebin0/GstRTSPSrc:source:\\nNo supported authentication protocol was found\\nERROR: pipeline doesn’t want to preroll.\\nSetting pipeline to PAUSED …\\nSetting pipeline to READY …\\nSetting pipeline to NULL …\\nFreeing pipeline …I also tried:\\ngst-launch-1.0 uridecodebin uri=rtsp://yyyy:xxxx@<my camera’s ip>:8555/test ! nvvidconv ! “video/x-raw(memory:NVMM), width=640, height=360” ! nvegltransform ! nveglglessink window-x=1981 window-y=180where yyyy is my userid and xxxx is my passwordand I got:\\nSetting pipeline to PAUSED …Using winsys: x11\\nPipeline is live and does not need PREROLL …\\nGot context from element ‘eglglessink0’: gst.egl.EGLDisplay=context, display=(GstEGLDisplay)NULL;\\nProgress: (open) Opening Stream\\nProgress: (connect) Connecting to rtsp://yyyy:xxxx@camera ip:8555/test\\nProgress: (open) Retrieving server options\\nProgress: (open) Retrieving media info\\nERROR: from element /GstPipeline:pipeline0/GstURIDecodeBin:uridecodebin0/GstRTSPSrc:source: Could not open resource for reading.\\nAdditional debug info:\\ngstrtspsrc.c(5829): gst_rtspsrc_setup_auth (): /GstPipeline:pipeline0/GstURIDecodeBin:uridecodebin0/GstRTSPSrc:source:\\nNo supported authentication protocol was found\\nERROR: pipeline doesn’t want to preroll.\\nSetting pipeline to PAUSED …\\nSetting pipeline to READY …\\nSetting pipeline to NULL …\\nFreeing pipeline …Powered by Discourse, best viewed with JavaScript enabled',\n",
       " 'dx11-dxgi-error-device-hung-since-2080-ti-guess-at-certain-tessellation-stage': 'Hi NV Team,We have problems with our DX11 engine running on NVIDIA’s 2080 TI series which we already integrated in certain customer projects by a large number. On 1080 TI and lower our engine running fine.\\nWe already isolate the problem at certain tessellation stage and nail it down in a Nsight Snapshot (~50mb). There we have a situation where the frame is rendered approx. 10 minutes until we get an “DXGI_ERROR_DEVICE_HUNG”\\nThere is from our point of view no wrongdoing with the D3D11/HLSL API. We really try many, many variations of code to track the crash further down with without success.Is there a way that you take a look at the snapshot? Can I upload the data somewhere on your side? Or By Mail? Upload is not possible due to the size of the compressed archive.Cheers,\\nRenéFurther info:\\nWe could reproduce the problem on a development machine with the same switch of graphics cards.\\nUsing Aftermath we could narrow it down to single render call. We even have two different render paths in that case (DrawInstancedIndirect vs. DrawInstanced) which both crash.\\nEven after stripping down the scene to pretty much nothing except this render call and stripping down the used shader considerably, we still get the same crash.\\nWhat seems to prevent the crash is setting all the EdgeTessFactors in the hull shader to 1.0f, to prevent any tesselation.We then took a C++ Capture with Nsight which can reproduce the crash. It might take a while until the crash happens, usually up to 15 minutes (but we had everything from 10 seconds to 2 hours).We ran the debug version of the capture from within VS2019 with D3D debug layer active and the following arguments:\\nApplication__2020_05_08__12_23_19.exe -automated -wbThe error output:\\nD3D11: Removing Device.\\nD3D11 ERROR: ID3D11Device::RemoveDevice: Device removal has been triggered for the following reason (DXGI_ERROR_DEVICE_HUNG: The Device took an unreasonable amount of time to execute its commands, or the hardware crashed/hung. As a result, the TDR (Timeout Detection and Recovery) mechanism has been triggered. The current Device Context was executing commands when the hang occurred. The application may want to respawn and fallback to less aggressive use of the display hardware). [ EXECUTION ERROR #378: DEVICE_REMOVAL_PROCESS_AT_FAULT]\\nNvda.Replay Error: Present Failed with error 0x887a0005\\nException thrown at 0x00007FF846C3A799 in Application__2020_05_08__12_23_19.exe: Microsoft C++ exception: std::runtime_error at memory location 0x000000AED50FDD38.D3D11: Removing Device.\\nD3D11 ERROR: ID3D11Device::RemoveDevice: Device removal has been triggered for the following reason (DXGI_ERROR_DEVICE_HUNG: The Device took an unreasonable amount of time to execute its commands, or the hardware crashed/hung. As a result, the TDR (Timeout Detection and Recovery) mechanism has been triggered. The current Device Context was executing commands when the hang occurred. The application may want to respawn and fallback to less aggressive use of the display hardware). [ EXECUTION ERROR #378: DEVICE_REMOVAL_PROCESS_AT_FAULT]\\nD3D11: **BREAK** enabled for the previous message, which was: [ ERROR EXECUTION #378: DEVICE_REMOVAL_PROCESS_AT_FAULT ]\\nException thrown at 0x00007FFA304FA799 (KernelBase.dll) in DisiApplication__2020_05_15__17_40_19.exe: 0x0000087A (parameters: 0x0000000000000001, 0x000000F4589895F0, 0x000000F45898B3E0).Thx in advance for any kind of idea how to tackle the problem.Best,\\nRenéPowered by Discourse, best viewed with JavaScript enabled',\n",
       " 'problemas-con-tarjeta-nvidia-1060-se-desabilita-aceleracion-de-direct3d': 'He tenido Problemas con la tarjeta de video que se desabilita al trabajar con after effects, se traba y la tarjeta deja de funcionar, si tengo instalados los drivers mas recientes:en el visor de eventos me arrojo esto:ERROR 1\\nNombre de la aplicación con errores: AfterFX.exe, versión: 16.0.0.235, marca de tiempo: 0x5bb315c5\\nNombre del módulo con errores: nvoglv64.dll, versión: 25.21.14.1701, marca de tiempo: 0x5bedfc9b\\nCódigo de excepción: 0xc0000409\\nDesplazamiento de errores: 0x0000000000ec0499\\nIdentificador del proceso con errores: 0x1b70\\nHora de inicio de la aplicación con errores: 0x01d489a7a2c0f8e4\\nRuta de acceso de la aplicación con errores: C:\\\\Program Files\\\\Adobe\\\\Adobe After Effects CC 2019\\\\Support Files\\\\AfterFX.exe\\nRuta de acceso del módulo con errores: C:\\\\Windows\\\\System32\\\\DriverStore\\\\FileRepository\\\\nvlti.inf_amd64_286452189ba106d8\\\\nvoglv64.dll\\nIdentificador del informe: d79998eb-03a7-42a0-9125-982942058196ERROR 2\\nNo se encuentra la descripción del id. de evento 1 en el origen NVIDIA OpenGL Driver. El componente que provoca este evento no está instalado en el equipo local, o bien la instalación está dañada. Puede instalar o reparar el componente en el equipo local.Si el evento se originó en otro equipo, la información que se va a mostrar tenía que haberse guardado con el evento.Se incluyó la siguiente información con el evento:\\nUnable to communicate with the display driver.  The application must close.Error code: 2\\n(pid=7024 tid=4932 afterfx.exe 64bit)Visit NVIDIA Customer Support for more information.Hello,Can you provide more information about your issue? What Video card, OS and version are you running? I suggest that you should also post this issue in the Adobe After Effects forum.Thanks,\\nTomInstale Windows de NuevoMicrosoft Graphics Hybrid: Supported\\nDxDiag Version: 10.00.17134.0001 64bit UnicodeDevice Problem Code: No Problem\\nDriver Problem Code: Unknown\\nDisplay Memory: 10298 MB\\nDedicated Memory: 128 MB\\nShared Memory: 10170 MB\\nCurrent Mode: 1920 x 1080 (32 bit) (60Hz)\\nHDR Support: Not Supported\\nDisplay Topology: Internal\\nDisplay Color Space: DXGI_COLOR_SPACE_RGB_FULL_G22_NONE_P709\\nColor Primaries: Red(0.590344,0.350109), Green(0.330578,0.555188), Blue(0.153820,0.119641), White Point(0.313977,0.329602)\\nDisplay Luminance: Min Luminance = 0.500000, Max Luminance = 270.000000, MaxFullFrameLuminance = 270.000000\\nMonitor Name: Generic PnP Monitor\\nMonitor Model: unknown\\nMonitor Id: CMN15D3\\nNative Mode: 1920 x 1080(p) (60.008Hz)\\nOutput Type: Internal\\nMonitor Capabilities: HDR Not Supported\\nDisplay Pixel Format: DISPLAYCONFIG_PIXELFORMAT_32BPP\\nAdvanced Color: Not Supported\\nDriver Name: C:\\\\Windows\\\\System32\\\\DriverStore\\\\FileRepository\\\\iigd_dch.inf_amd64_f02a6686365638a8\\\\igdumdim64.dll,C:\\\\Windows\\\\System32\\\\DriverStore\\\\FileRepository\\\\iigd_dch.inf_amd64_f02a6686365638a8\\\\igd10iumd64.dll,C:\\\\Windows\\\\System32\\\\DriverStore\\\\FileRepository\\\\iigd_dch.inf_amd64_f02a6686365638a8\\\\igd10iumd64.dll,C:\\\\Windows\\\\System32\\\\DriverStore\\\\FileRepository\\\\iigd_dch.inf_amd64_f02a6686365638a8\\\\igd12umd64.dll\\nDriver File Version: 25.20.0100.6444 (English)\\nDriver Version: 25.20.100.6444\\nDDI Version: 12\\nFeature Levels: 12_1,12_0,11_1,11_0,10_1,10_0,9_3,9_2,9_1\\nDriver Model: WDDM 2,4\\nGraphics Preemption: Triangle\\nCompute Preemption: Thread\\nMiracast: Supported\\nHybrid Graphics GPU: Integrated\\nPower P-states: Not Supported\\nVirtualization: ParavirtualizationCard name: NVIDIA GeForce GTX 1060\\nManufacturer: NVIDIA\\nChip type: GeForce GTX 1060\\nDAC type: Integrated RAMDAC\\nDevice Type: Full Device\\nDevice Key: Enum\\\\PCI\\\\VEN_10DE&DEV_1C20&SUBSYS_38E117AA&REV_A1\\nDevice Status: 0180200A [DN_DRIVER_LOADED|DN_STARTED|DN_DISABLEABLE|DN_NT_ENUMERATOR|DN_NT_DRIVER]\\nDevice Problem Code: No Problem\\nDriver Problem Code: Unknown\\nDisplay Memory: 16222 MB\\nDedicated Memory: 6052 MB\\nShared Memory: 10170 MB\\nCurrent Mode: Unknown\\nHDR Support: Unknown\\nDisplay Topology: Unknown\\nDisplay Color Space: Unknown\\nColor Primaries: Unknown\\nDisplay Luminance: Unknown\\nDriver Name: C:\\\\Windows\\\\System32\\\\DriverStore\\\\FileRepository\\\\nvlti.inf_amd64_f949ee26cba75def\\\\nvldumdx.dll,C:\\\\Windows\\\\System32\\\\DriverStore\\\\FileRepository\\\\nvlti.inf_amd64_f949ee26cba75def\\\\nvldumdx.dll,C:\\\\Windows\\\\System32\\\\DriverStore\\\\FileRepository\\\\nvlti.inf_amd64_f949ee26cba75def\\\\nvldumdx.dll,C:\\\\Windows\\\\System32\\\\DriverStore\\\\FileRepository\\\\nvlti.inf_amd64_f949ee26cba75def\\\\nvldumdx.dll\\nDriver File Version: 25.21.0014.1721 (English)\\nDriver Version: 25.21.14.1721\\nDDI Version: 12\\nFeature Levels: 12_1,12_0,11_1,11_0,10_1,10_0,9_3,9_2,9_1\\nDriver Model: WDDM 2,4\\nGraphics Preemption: Pixel\\nCompute Preemption: Dispatch\\nMiracast: Not Supported by Graphics driver\\nHybrid Graphics GPU: Discrete\\nPower P-states: Not Supported\\nVirtualization: Paravirtualization\\nBlock List: No Blocks\\nCatalog Attributes: Universal:False Declarative:False\\nDriver Attributes: Final Retail\\nDriver Date/Size: 28/11/2018 06:00:00 p. m., 958616 bytes\\nWHQL Logo’d: Yes\\nWHQL Date Stamp: Unknown\\nDevice Identifier: Unknown\\nVendor ID: 0x10DE\\nDevice ID: 0x1C20\\nSubSys ID: 0x38E117AA\\nRevision ID: 0x00A1\\nDriver Strong Name: oem25.inf:0f066de3a2ed62a8:Section218:25.21.14.1721:pci\\\\ven_10de&dev_1c20&subsys_38e117aa\\nRank Of Driver: 00D10001\\nVideo Accel: Unknown\\nDXVA2 Modes: DXVA2_ModeMPEG2_VLD  DXVA2_ModeVC1_D2010  DXVA2_ModeVC1_VLD  DXVA2_ModeH264_VLD_Stereo_Progressive_NoFGT  DXVA2_ModeH264_VLD_Stereo_NoFGT  DXVA2_ModeH264_VLD_NoFGT  DXVA2_ModeHEVC_VLD_Main  DXVA2_ModeHEVC_VLD_Main10  DXVA2_ModeMPEG4pt2_VLD_Simple  DXVA2_ModeMPEG4pt2_VLD_AdvSimple_NoGMC  DXVA2_ModeVP9_VLD_Profile0\\nDeinterlace Caps: n/a\\nD3D9 Overlay: Unknown\\nDXVA-HD: Unknown\\nDDraw Status: Enabled\\nD3D Status: Enabled\\nAGP Status: Enabled\\nMPO MaxPlanes: 0\\nMPO Caps: Not Supported\\nMPO Stretch: Not Supported\\nMPO Media Hints: Not Supported\\nMPO Formats: Not Supported\\nPanelFitter Caps: Not Supported\\nPanelFitter Stretch: Not Supportedsin Fallaa, funcionando:\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nCon Falla, Despues de que se traba:\\nDespues de que se traba, se desabilita sola la tarjeta nvidia 1060 6 gb para laptop en una lenovo y720 \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nAl ejecutar el dxdiag me da esta informacion:DxDiag…txt (104 KB)I am moving this to the GPU Hardware forum for better visibility.Powered by Discourse, best viewed with JavaScript enabled',\n",
       " 'vertex-based-motion-blur-in-optix-6-0-0-using-geometrytriangles': 'Hi,Yet I successfully implemented motion blur (using motion keys through transforms) for rendering in my engine (a Path Tracer based on the “OptiX Advanced Introduction Samples”), but that only causes motion blur for an entire object.\\nI also want vertex-related motion blur  (so that a bone animation smoothly also produces motion blur based on the relative vertex movings).The previous vertex buffer (related to current frame) is already present on the GPU and can load the associated vertex  (which is related to the current one) from that prevVertex buffer to calculate velocity between them. I can visualize the motion without problems.But in the attribute program in GeometryTriangles it seems not to be possible to directly apply the velocity, since there is no final vertex position; And changing the barycentrics would be invalid. I could pass veloctiy as an attribute to closest hit. That would provide the info for shading only. But I need to update the vertex position.Is there a way to apply velocity in GeometryTriangles on the intersection position somehow on the GPU  (since the buffers are already there) ? The optixParticles sample uses intersection and bounding box programs. That would be a solution, but then I cannot use GeometryTriangles.Is my assumption right about the following (which I want to try):\\nI found rtGeometryTrianglesSetMotionVertices, which can set motion steps. So when there is a current vertex buffer and a previous vertex buffer present and then mixed them both this way:\\nmotionStepCount  := 2   (via rtGeometryTrianglesSetMotionSteps)Would that do the job of creating vertex-based motion blur based on current and previous frame’s vertex buffers ?The documentation says: […]Triangles are linearly interpolated between motion steps.[…]\\nDoes this mean that the value of rtCurrentTime sets also there the “t” value for the vertex motion time position within rtGeometryTrianglesSetMotionRange ?Thank you very much.\\nAny help is appreciated.my system: Win10PRO 64bit VS2019 Community CUDA 10.0 driver 431.36 OptiX 6.0.0    GTX 1050 2GBRight, vertex based motion blur (morphing) is possible directly with OptiX as long as the mesh topology doesn’t change, simply by providing the vertex positions for the different key frames as additional data inside the Geometry\\n[url]https://raytracing-docs.nvidia.com/optix_6_0/guide_6_0/index.html#motion_blur_math#motion-in-geometry-nodes[/url]\\nor GeometryTriangles\\n[url]https://raytracing-docs.nvidia.com/optix_6_0/guide_6_0/index.html#host#3614[/url]The custom Geometry primitives need a special bounding box program which gets called at least per primitive per key frame.\\n[url]https://raytracing-docs.nvidia.com/optix_6_0/guide_6_0/index.html#motion_blur_math#bounding-boxes-for-motion-blur[/url]For GeometryTriangles look for the entry points with “Motion” in the name.\\nAPI Reference Guide: [url]https://raytracing-docs.nvidia.com/optix_6_0/api_6_0/html/group___geometry_triangles.html[/url]\\nIn particular the rtGeometryTrianglesSetMotionVertices you already found:\\n[url]https://raytracing-docs.nvidia.com/optix_6_0/api_6_0/html/group___geometry_triangles.html#gac9c60bfb06bb0ff39c5a84e17e4573f5[/url]OptiX will then interpolate the positions according to the current time and if you have a renderer which already supports SRT motion via transforms, nothing else needs to be changed. It should simply work when providing the additional morphed mesh data per key frames.Thanx for the clarifications. Very much appreciated.I reached a conclusion using setMotionVerticesMultiBuffer which even can take the 2 vertex buffers without mixing them into one.\\nThose buffers are already optix::Buffers on the GPU.\\nWas much easier than I thought. And I already got it work in a small test renderer app.  Great!Powered by Discourse, best viewed with JavaScript enabled',\n",
       " 'forget-about-things-such-as-keys-or-wallet': 'OPTIMIND -zxt trial\\nThe formula fight …\\nForget about things such as keys or wallet\\nmemory loss Optimind\\nVery low power consumption\\nDifficulty concentrating\\nLack of motivation and focus\\nLess able to mentally performance\\nBenefits  OPTIMIND -zxt\\nOPTIMIND workhttp://www.nationalhealthadvisor.com/optimindPowered by Discourse, best viewed with JavaScript enabled',\n",
       " 'flex-unity-softbody-freez-axes': 'Hi, I downloaded Flex plugin for unity, does someone know how to access the position of particles ?\\nI would like to freez the position of my player on Z axis to make a 2D gameplay.Powered by Discourse, best viewed with JavaScript enabled',\n",
       " 'i-want-to-run-the-latelatch-program-with-original-hmd': 'Now I’m trying to run the latelatch_dx program using the original HMD, but I do not know what to do to display the image on the HMD display.\\nFor example, if you have a direct_mode_dx program, you can display by setting EDID etc of HMD, but please let me know what kind of things are necessary in latelatch_dx.Powered by Discourse, best viewed with JavaScript enabled',\n",
       " 'reason-for-deprecation-of-nv-enc-params-rc-2-pass-framesize-cap-mode': 'Can someone from nvidia please explain why NV_ENC_PARAMS_RC_2_PASS_FRAMESIZE_CAP was deprecated?In older documentation, NV_ENC_PARAMS_RC_2_PASS_FRAMESIZE_CAP was suggested in scenarios where low latency and staying below a certain bitrate/framesize is desired. Now that FRAMESIZE_CAP is deprecated, what is the correct rc mode to use now, given the same frame size + latency constraints?I do see that the release notes for 7.1 say “The names of the rate control modes have been\\nmodified to reflect quality and performance.”, but I’m looking for a bit more specific information. Was functionality changed, or were the names simply changed to new names? Is there still an equivalent to NV_ENC_PARAMS_RC_2_PASS_FRAMESIZE_CAP, which keeps the framesizes more consistent? What is that equivalent? etc.Thanks.NV_ENC_PARAMS_RC_2_PASS_FRAMESIZE_CAP is equivalent to NV_ENC_PARAMS_RC_CBR_HQ.The functionality is the same, with some additional quality enhancements and better bitrate conformance.Powered by Discourse, best viewed with JavaScript enabled',\n",
       " 'what-does-rt-error-invalid-value-mean': 'I get above return code when trying to\\nrtBufferGetDevicePointer(\\nbufs.vertices->get(),\\nOCD->oOptixDev,\\n&OCD->pVtx )\\nSo whose VALUE it is referring to?? I did go through the manual there is no more information on the enumeration.\\nI did check bufs.vertices->() passes RTBuffer verify method. And it passes the test.\\nSome help is much appreciated.Hi there, have you checked the results of both rtContextGetErrorString() as well as from rtContextSetUsageReportCallback()?I can see 4 different reasons that an invalid value might be returned:What flags did you use for creating your buffer?If you are in a position to consider moving to OptiX 7 in the future, I’d encourage it. Both CUDA interop and error handling are much improved.–\\nDavid.Here is how I created the bufferOn the device ordinal I did notice some strange behaviour. Some times it gives “6” and other times some other times a very large integer both negative an positive. In any case the run fails with same error.From the following the device seems to be enabled.Here is device info just before calling the getDevice pointer\\nrtDeviceGetAttribute() output\\nNumber of optix devices: 1\\nOptix version: 60600\\nMax threads per block: 1024\\nClock rate: 1113500\\nMultiprocessor count: 20\\nExecution timeout enabled: 0\\nMax hardware texture count: 1048576\\nCompute capability: sm_61\\nTotal memory: 7981694976\\nTCC driver: 0getEnabledDeviceCount = 1\\ngetCPUNumThreads = 1\\ngetUsedHostMemory = 66088\\ngetGPUPagingActive = 0\\ngetGPUPagingForcedOff = 0\\ngetStackSize = 2688\\ngetEntryPointCount = 1\\ngetRayTypeCount = 2\\ngetPrintBufferSize = 0\\ngetVariableCount = 7\\ngetDeviceCount = 1\\ngetDeviceName = Tesla P4\\nMaterials: 1\\nMesh:\\nnum vertices : 281\\nhas normals  : false\\nhas texcoords: false\\nnum triangles: 525\\nnum materials: 1\\nbbox min     : ( -1.5161, -0.035755, -0.035864 )\\nbbox max     : ( 0, 0.0359, 0.035864 )\\nNVRTC Diabled getPtxStringFromFileOk. Now I know the issue is with Device Ordinal. It was my goof.  I should have heeded the warning signs on device ordinal being inconsistent. The device ordinal variable in the call was not set. My sincere apologies for this. But at least the error code possibilities helps others :))Hey glad you got it working! Zero need to apologize, we’re here to help. Just curious, in case we need to add or improve the error messages, did you end up trying either of the error message functions, the error string or the usage report?–\\nDavid.On the Error reporting issue, I had to modify RT_CHECK_ERROR as follows since APIError call does not provide this detail as expected. I did not bother investigating.Now Ii have  no more errors in buffer creations and getDevicePointer calls. They all work with no issues. But the  “context.validate()” fails with theterminate called after throwing an instance of ‘optix::Exception’\\nwhat():  Variable not found (Details: Function “RTresult _rtContextValidate(RTcontext)” caught exception: Variable “Unresolved reference to variable bPixHit from _Z6camerav” not found in scope)It is clear that it is having issue with bPixHit variable\\nOn host side  the following lines had no issue.\\nbuffer = context->createBuffer(RT_BUFFER_OUTPUT,\\nRT_FORMAT_UNSIGNED_INT,\\nwidth, height );\\ncontext[“bPixHit”]-> setBuffer(buffer);In my device side code   “ptracer.cu” which contained RT_PROGRAM camera\\nrtBuffer<unsigned int, 2> bPixHit;         is present.So what is happening here. Does this anything to do with the fact that I have disabled NVRTC. I did make sure that NVIDIA card arch SM_61 that  my instance is using matched with the value specified in CMakefile.Unresolved key word appears to be pointing to some linking process . I did check in the build directory to see what happens to my optix related device code.\\nI do find\\nrat_generated_ptracer.cu.ptx.NVCC-depend , cmake, depend filesHere is “grep” output on the assembly code under ptx directory of lib\\n> ~/build/lib/ptx$ grep bPixHit   rat_generated_ptracer.cu.ptx> .global .align 1 .b8 bPixHit[1];Also here is the header of the assembly that gives the detail on the compiler and architecure\\n/\\n// Generated by NVIDIA NVVM Compiler\\n//\\n// Compiler Build ID: CL-28540450\\n// Cuda compilation tools, release 11.0, V11.0.194\\n// Based on LLVM 3.4svn\\n//.version 7.0\\n.target sm_61Will be great if you can help resolve this. Not sure if I should have opened a new topicPowered by Discourse, best viewed with JavaScript enabled',\n",
       " 'normals-on-bezier': 'Hi,In OptiX 7.7, how do you calculate normals along the OPTIX_PRIMITIVE_TYPE_ROUND_CUBIC_BEZIER?\\n(curve.h in SDK seems not to support this)Thanks!Now I’ve got:It seems to work correctly. Though if there is any better way to get the normal, I’d be happy to adopt it.I’ve asked internally and it’s hopefully making it into curve.h inside the upcoming OptiX SDK release.\\nIt shouldn’t take too long. The R535 drivers for it are already out.\\nOtherwise we can post the code before that, once it’s in.Great! Thanks for checking. Waiting for the next SDK :)Until then, if you add these two functions to the struct CubicInterpolator, that will enable using the surfaceNormal() function the same way as with cubic B-splines.\\nAn example showing that for cubic B-splines is optixHair inside the optixHair.cu normalCubic() function. Just replace initializeFromBSpline() with the new initializeFromBezier() function when using Bezier control points.Thank you! Sounds clear. I’ll put it in the code and let you know how it works.OK, checked, code works fine. Performance is exactly the  same as with my code, so I’ll stay with your solution since it is going to appear in SDK. :)Thanks again!Thanks for checking.Depending on how your curves are specified exactly, there are different options for the surfaceNormal() function template which allow taking quicker code paths if possible.Powered by Discourse, best viewed with JavaScript enabled',\n",
       " 'requesting-ability-to-use-double-float-for-depth-in-drivers': \"Would be nice to have the ability to have a higher precision depth format as in some situations 24bits of precision is not enough causing a lot of Z fighting, especially in Pcsx2 emulation where it has 32bits of precision.Heres a ticket showcasing issue in detailGames that have problems with textures flickering over a model. (Due to not know…ing what's on top for depth)\\nAvoids making duplicate issues.\\n\\n- [ ] Kingdom Hearts 2 Final Mix #3394\\n- [ ] Okami #2008\\n- [ ] Megaman X #1558\\n- [ ] DBZ Budokai Tenkaichi 2 and 3 #544\\n- [ ] Sonic Heroes #2118\\n- [ ] Sonic Riders\\n- [ ] Shox\\n- [ ] NASCAR Heat 2002 (HW)\\n- [ ] Gran Turismo 4 (SW and HW)\\n- [ ] Guitar Hero (HW)\\n- [ ] Midnights Club 3 Dub Edition Remix\\n- [ ] F1 2004\\n- [ ] Persona 4 (HW)\\n- [ ] Tony Hawk Pro Skater 3 (HW)\\n- [ ] Need For Speed - Most Wanted\\n- [ ] Harry Potter and the Order of the Phoenix #2339\\n- [ ] Harry Potter and the Half-Blood Prince\\n- [ ] Harry Potter and the Chamber of Secrets\\n- [ ] Vice City (HW only)\\n- [ ] Indiana Jones and the Staff of Kings (pyramid)\\n- [ ] Fatal Frame series #4493\\n- [ ] Classic British Motor Racing (billboard)\\n- [ ] Kuon (Flickering foot in Intro FMV for yang phase)\\n- [ ] Aggressive Inline Skating (HW only) (Demo from PSmagazine 24)\\n- [ ] Toca Race Driver (HW only) (Demo from PSmagazine 24)\\n- [ ] Commandos Strike Force (Demo from PSMagazine 70) (Zoom-in on the mountain before you move character)\\n- [ ] GTA 3 (Go straight down the road till you see walls with flickering shadows)\\n- [ ] Digimon World Data #6200\\n- [ ] Quake III Revolution #5394\\n- [ ] Onimusha 3 #3609Hello again @yumzion90!I think you need to be a bit more precise with your request. What depth buffer implementation do you mean? OpenGL? DirectX? Vulkan? The API specifies the supported precision, not the GPU driver.Preferably Vulkan, but would also be useful in DX11/12 and OpenGL.  I believe there might be an extension to do so with OpenGL but I’m not sure on the support for that.Any progress yet?, appreciated.🙂, would do the community a real solidHi yumzion90, I think I stated the reply before. The GPU driver already supports double float, that is not the issue. The issue is that the API needs to expose that feature. of course this can be suggested to Microsoft (DX) or Khronos (OpenGL, Vulkan) or Apple (Metal), but this is not something that could be addressed by the driver alone.I am sorry if that is not helpful.I see , guess ill have to open a ticket for vulkan/opengl and directx as wellUpdate:Would be nice to have the ability to have a higher precision depth format as in some situations 24bits of precision is not enough causing a lot of Z fighting, especially in Pcsx2 emulation where it has 32bits of precision.  32bit&64bit would...Powered by Discourse, best viewed with JavaScript enabled\",\n",
       " 'how-to-encode-a-frame-twice-without-change-reference-frame': 'I want to encode a frame with different QPs to observe how different QPs influent the output size. But after encoding once, the reference frame changed automatic. Is there a method to control reference frame unchanged? Thank you!Powered by Discourse, best viewed with JavaScript enabled',\n",
       " 'cant-build-some-of-optix-samples': 'I installed CUDA, OptiX and tried to build and run OptiX samples. Most of the samples(ex. optixTutorial, optixWhitted, etc.) work well. But some samples emit build error. For example, when I build optixRayCasting, I get the following error:I googled about this error, but failed to find useful information. Could you help how to solve this error?(My environment: Windows 10, Visual studio 2017 / CUDA 9.2, Optix SDK 5.1.0)Thanks.Visual Studio 2017 and CUDA 9.2 are not officially supported with OptiX 5.1.0 and there are some incompatibilities among the individual versions.\\nPlease see this thread for more information and potential workarounds.\\n[url]https://devtalk.nvidia.com/default/topic/1036401/?comment=5265672[/url]In general please always refer to the OptiX Release Notes before setting up a development environment for OptiX work.\\nMSVS 2015 and CUDA 9.0 would be the recommended combination for OptiX 5.1.0.Powered by Discourse, best viewed with JavaScript enabled',\n",
       " 'solved-physx-3-3-0-bug-in-multi-material-mesh': 'There is a bug in 3.3.0 that cause my game crash randomly.every time I make a shape with this function:createShape(pTriangleMeshGeometry, &physmaterials[0],numMaterial);it cause random crashes in my game.My game is a racing game and it seems every time an object collide with two different part of mesh with different materials my game crash with no reason.I double checked my code and there is no error in it.unfortunately there is no sample in physx that uses multi material so I can’t be sure about it but I tested my code in 3.3.1 and it works but I have another problem in 3.3.1.You can see it here:I updated my project from 3.3.0 to 3.3.1 and suddenly my project didn’t work as before.I realized physx ignores collision filter in the first frame if the objects bounding volumes overlap when they first added to scene.  My project is a racing game...I found the problem.It seems setting the inverse interia tensor of objects to zero was the cause of random crashesPowered by Discourse, best viewed with JavaScript enabled',\n",
       " 'quick-question-how-to-change-the-default-sm-architecture-in-the-optix-samples': 'Greetings!I have created a project based on the bundled Optix-Samples.I am using some features that need SM architecture greater than 2.1 and i fear that the default is lower when compiling the samples. Could you help me change it ? I have tried and failed so far.P.S. when building the samples is reads “nvcc : warning : The ‘compute_10’ and ‘sm_10’ architectures are deprecated, and may be removed in a future release.”and at run time\\n“blah blah filename : error : Feature ‘Addresses as initial values’ require PTX ISA version 2.1 of later”\\nthanksOptiX ships with a FindCUDA.cmake version which is newer than the one shipped with CMake, but will probably find its way into the official release. The difference is that it supports static linking of the CUDA runtime.(The macro(CUDA_COMPILE_PTX generated_files) contains the main logic for the custom build rule generation.)According to the comments at the top of that FindCUDA.cmake file you should change CUDA_NVCC_FLAGS to add more nvcc command line parameters and that is done in OptiX’ top-level CMakeLists.txt file.Search for this blockand add your desired -arch parameter accordingly.That comment says you can also do that in the CMake GUI after the first configuration was successful which sounds convenient, so try adding -arch sm_21 in the CUDA_NVCC_FLAGS line.\\nYou could add a message to the CMakeLists.txt to dump what CUDA_NVCC_FLAGS is during the following configure step.I was trying that exact solution,among others that seemed possible yesterday, but eventually  it refused to add the “use fast math” directive when i added the -arch … I will try it again more carefully. Thank you very much for your time!Building PTX binaries from CU files is actually VERY simple from a command line. My scripts look like this (Windows):nvcc.exe -O3 -use_fast_math -arch=compute_30 -code=sm_30 -I “C:\\\\ProgramData\\\\NVIDIA Corporation\\\\OptiX SDK 3.6.2\\\\include” -I “C:\\\\Program Files (x86)\\\\Microsoft Visual Studio 10.0\\\\VC\\\\include” -m 64 -ptx INPUTFILE -o OUTPUTFILENAMEYou can change -arch and -code accordingly to your needs (put 21 instead of 30) and you must change the -m 64 option to -m 32 if building a 32 bits executable. You probably just want to remove this (-m) option if you’re not under Windows.I am having the same difficulty as conkal in changing the SM architecture of the OptiX samples using Detlef’s approach. Oddly, it works just fine on my code; it’s only a problem with the samples.Here’s the error I get:And I can see that the -arch flag has been added twice somehow:As far as I can tell, I’m only responsible for the first occurrence of “-arch SM_20” in that string. However, if I remove the first -arch flag using CMake, the code compiles as SM_10.Any ideas?Edit: This is with Windows 7, CUDA 6.0, CMake 3.0.0, and VS 2010.Powered by Discourse, best viewed with JavaScript enabled',\n",
       " 'hit-position-in-any-hit-program': 'Hi everyone,I am building an implementation which needs figure out every intersection ray-geometry. Since in my case all rays never change their directions, an any-hit program might be ideal to handle the calculation. So I wonder if it’s possible to use the “rtIntersectionDistance” in an any hit program to locate the hit position or is there any other corresponding variable. Otherwise, how to determine a hit point in an any-hit program?Thanks in advance.yunsongHiYes, you can use rtIntersectionDistance in any-hit program. But take in account that because of AS, ray traverses not exactly in order with the any-hit. You will have to sort hits if it’s needed. With closest hit program you always get the first intersection along the ray.HiYes, you can use rtIntersectionDistance in any-hit program. But take in account that because of AS, ray traverses not exactly in order with the any-hit. You will have to sort hits if it’s needed. With closest hit program you always get the first intersection along the ray.thanks a lot, qconst.\\nI just find it in programming guide.Now I get another question : For example, I’m tracing 32 rays at the same time. When each ray terminates, a counter as output buffer will +1. Since I need a 32 on output, how to avoid the error that several rays counting at the same time? Cause in this way, the output will be surely inferior to 32. I faced the same problems longtime ago on cuda and totally forgot how to handle it. Do you have any ideas?thanks.You need to ensure a serialized access to your counter variable.\\nYou can do this using atomic operations, e.g. atomicInc().\\nHave a look at the CUDA documentation: Programming Guide :: CUDA Toolkit DocumentationYou need to ensure a serialized access to your counter variable.\\nYou can do this using atomic operations, e.g. atomicInc().\\nHave a look at the CUDA documentation: Programming Guide :: CUDA Toolkit Documentationexactly what i need, thanks!\\nunfortunately my gpu is not powerful enough, i’m considering changing it. :)Powered by Discourse, best viewed with JavaScript enabled',\n",
       " 'gpu-compute-problem-in-compute-shader': 'Hi!\\nI would like to print a number to a texture, the problem is that I’ve imprecision problem, in fact, when I want to display this number :double number = 45896.8579;\\ndrawNumber(ivec2(0, resolution.y-1), 1, vec4(1, 0, 0, 1), number);I got 45896.859375 instead of 45896.8579.I extract the digits of the decimal part like this :double rest = fract(number);\\nif (rest > 0) {\\ndrawPunt(position, nbPixels, color);\\nposition.x += digitSpacing;\\ndo {\\nrest *= 10;\\nint digit = int(rest);\\nrest -= digit;\\ndrawDigit(position, nbPixels, color, digit);\\nposition.x += digitSpacing;\\n} while (rest != 0);\\n}it should work like this :rest = 8.579, digit = 8.\\nrest = 8.579 - 8 = 0.579.\\nrest = 5.79 digit = 5.\\nrest = 5.79 - 5 = 0.79.\\nrest = 7.9, digit = 7 or I get 9!!! So it’s the conversion from double to int which is not correct or it’s rest*10 which is not correct or it’s 5.79 - 5 which is not correct!I think it’s simply imprecision problem because of binary representation of float/double, so I need to correct this imprecision problem but I don’t know how, but anyway it was just for debuging so if it’s approximative it’s ok.Powered by Discourse, best viewed with JavaScript enabled',\n",
       " 'shield-image-download-links-dead': 'Hi, I’ve been trying to download some developer images for my old Shield Pro but all of the links appear to be broken. I can find the list of images here:Get the latest Gameworks software for your game development work.But all of the links there just 404. Are these images still available anywhere?Hello,Welcome to the NVIDIA Developer forums!\\nI am searching for the owner of the Shied downloads page to assist here.Thanks for your patience,\\nTomHi @hunter_kallerPlease try this link: Gameworks Download Center | NVIDIA Developer\\nScroll down to the Shield images. Let me know if you have any issues.Cheers,\\nTomAh, it looks like some of the links there work, it’s just the ones I tried that didn’t (mostly the 7.2.* series; I’m trying to debug a problem with my program that broke on Android 9.0+). It looks like the 7.1* series is working, though, along with some of the newer ones, so that should get me fixed up.Thanks!This topic was automatically closed 14 days after the last reply. New replies are no longer allowed.Powered by Discourse, best viewed with JavaScript enabled',\n",
       " 'driver-issue-with-bindless-textures-pixels-are-flickering-on-the-window': 'Hi! I use 3 shaders, the goal is the check the most opaque fragment over the refractable objects, and then hide refraction if there is an opaque fragment before.The code worked well until I use bindless texturing.\\nI think it’s a driver issue but I post here to be sure it’s it.In this shader the texture is ok :But in this shader, no textures, alpha is always 0.This is really strange because I use the same UBO for the two shaders and the same code to bind to UBOs to all my shaders :I printed values of texIndex and fTexCoords into the textures to see if they are correct and they are correctly affected.\\nI also passed a color the the colour output of the shader, to see if my draw functions are correct and they are correct.So I really don’t know what’s wrong, I tested everything, event to calculate size of all uniforms to set them to a multiple of 16 but nothing seems to solve this problem.So I think it’s a driver issue, everything is ok, excepts the UBO of sampler2D which is empty excepts in my per pixel linked list fragment shader so the UBO is not binbed to all my shaders.Ok it seems I had to bind/unbind buffers with glBindBufferBase because I use different buffers with different rederer but now I’ve this problem. (some pixels are flickering)\\nTest odfaeg1920×1080 387 KB\\nI solved the problem by using one context by FBO, UBO are not shared between context so that was the initial problem but when I use only one context pixels are flickering I thing, it’s a driver issue.Powered by Discourse, best viewed with JavaScript enabled',\n",
       " 'cannot-load-video-codec-sdk-samples-in-vs2017': 'I just installed Visual Studio 2017, the DirectX SDK, CUDA 10.0 tool kit and the Video Codec SDK 9.0.20.When opening the samples solution, many of the projects fail to load:The error message is as follows (translated by me):\\nThe imported project “P:\\\\Applications\\\\Visual Studio 2017\\\\IDE\\\\VC\\\\VCTargets\\\\BuildCustomizations\\\\CUDA 10.0.props was not found”. Please make sure the path in the  declaraion is correct and the file exists on the drive.Hello,I am not an expert on this subject, but this comment from Robert might shed some light on your issue.[url]https://devtalk.nvidia.com/default/topic/933708/cuda-setup-and-installation/compiling-and-setting-up-cuda-libraries-on-windows-10/post/4869753/#4869753[/url]Powered by Discourse, best viewed with JavaScript enabled',\n",
       " 'nvidia-quadro-k420-no-driver-working-for-windows-10-64-bit-version-22h2': 'Hi there,I have 15 years experience in setting up PC and attempted installing the driver for the Quadro K420 PCIe card on a Lenovo M900 tower running Windows 10 64-bit version 22H2. The installer tells it is incompatible with this version of Windows.I tried several versions of the driver for Windows 10 64-bit, the latest one, as well as older ones. I also tried both variants of the driver: the WHQL version and the other one.Other users reported here the exact same problem for Windows 11 version 22H2 on Microsoft’s forum:\\nhttps://answers.microsoft.com/en-us/windows/forum/all/gpu-stopped-working-after-windows-11-22h2-update/a9143904-d38b-4c6b-bbeb-ba8dae5d0064?page=2I also unsuccessfully tried installing the card, by directly launching the setup files in the folder where the installer was unpacked.\\nApplying all Windows updates (on 2022/01/06) didn’t solve the problem.Can we expect soon some fix of this driver for Windows 10 & 11 version 22H2?Thanks a lot.---- Latest version tried —Version:\\tR470 U12 (474.14)  WHQL\\nDate de réalisation:\\t2022.12.20\\nSystème d’exploitation:\\tWindows 10 64-bit, Windows 11\\nLangue:\\tFrançais\\nTaille:\\t559.46 MBVersion:\\tR465 U2 (466.11)\\nDate de réalisation:\\t2021.4.14\\nSystème d’exploitation:\\tWindows 10 64-bit\\nLangue:\\tFrançais\\nTaille:\\t511.92 MBHello there @forums4 and welcome to the NVIDIA developer forums.I am sorry to hear that you are running into installation issues.Did you try a complete clean install of the driver, meaning that all older drivers are purged from the system? Sometimes corrupt driver installations can cause this error message.Nevertheless I took the liberty of opening an internal bug for you to make sure there is no regression. For future reference, as a registered developer you can also open bugs with NVIDIA on your own, check out your Account pages on developer.nvidia.com or go directly to the Bug submission form.That said, Kepler architecture is 9 years old and has been deprecated from new driver development. It will only receive cumulative maintenance and security updates. That means there is no guarantee on an ETA for a possible fix for your issue. I am sorry.Hello again @forums4 !Would it be possible for you to provide us with more information?Hardware ID from the device manager:\\nFrom the device manager right click on the GPU and select properties. Select Details → Hardware IdsInstallation logs:\\nYou can get those if you first manually extract the driver package, then install the driver by running below command:And then share the both the HW-ID and the logs from C:\\\\nvlog folder in this thread.Thank you!It’s definitely annoying when a driver’s not compatible, especially after putting in all that effort trying different versions and updating Windows. I feel you, been there done that! I’d recommend checking out the Microsoft Answers page linked in your post, as there’s probably some good advice from other users who’ve had the same issue. You never know, someone might have cracked the code and can help you out. If not, it might be worth reaching out directly to NVIDIA. In the meantime, you can get more tips on the reddit windows keys legit because it’s a great community.Powered by Discourse, best viewed with JavaScript enabled',\n",
       " 'publish-snapshot-artifacts-to-azure-devops-artifacts': 'I have a Gradle build set up in Azure DevOps, which compiles the code in an Azure DevOps git repository, then publishes the generated JARs (as Maven artifacts) to Azure Artifacts, as explained here. Code in other Azure DevOps git repositories can then reference these components as dependencies. This is fine for formal releases of these components (with unique version numbers), but I also need a way to get this working for in-progress snapshot releases. The problem is that I cannot publish an artifact with the same version number (e.g. 1.2.3-SNAPSHOT) more than once. This seems to be because packages in Azure are immutable.From my understanding, that would mean that Azure Artifacts cannot be used to store in-progress snapshot artifacts. Is that correct?If it is, is there any alternative that still uses Azure DevOps? I can see that I can publish artifacts to Azure Blob Storage, but presumably this is something you have to pay for on top of existing use of Azure Artifacts. I can also see that there’s a number of GitHub Maven plugins for treating a GitHub repo as a Maven repo, but I can’t find anything similar for using an Azure DevOps repo as a place to publish Maven artifacts.Powered by Discourse, best viewed with JavaScript enabled',\n",
       " 'bugreport-writing-to-cuda-surfaceobjects-produces-no-result': 'Hi,My program has the exact same set-up as this (using “bindless” surface objects introduced with Kepler)https://stackoverflow.com/questions/58303950/writing-to-cuda-surface-from-optix-kernelI do understand that if I dispatch 1280x720 optix raygen, it wont actually dispatch a 1280x720 kernel but some group of persistent workgroups that will iterate through ray-packets, and hence more than one physical dispatch thread may end up writing to the Surface.However even if my memory is 100% not coherent I’d still expect the writes to go through, is there some sort of a memory barrier I should issue like in OpenGL ?I’ve tried hand-massaging the PTX before it goes into optix trying out different cache op modes for the sust op, such as .wb, .wt and .cg.I also even put a stream synchronise and an OpenGL memory barrierStill no availIt seems that OptiX is excising the sust instruction from the PTX during its compilationoriginal CUDA Coriginal PTX (%r3 and %r4 contain the launch_index.xy)disassembly (no sust instruction)Hi devsh,Which OptiX version and driver did you try this with? I haven’t actually tried writing to a surface in an OptiX program, it is possible there’s a bug. Do you have a complete & minimal reproducer you could share with us?FWIW, if you launch a 1280x720 optix raygen, we choose the block size, but other than it does equate to a kernel who’s dimension is 1280 * 720 threads. You can verify this in Nsight Compute, for example. The main thing you need to be aware of and careful with is that OptiX programs are not CUDA, even though we’re trying to make it as close as possible. OptiX shaders cannot use shared memory, synchronizations, barriers, or other SM-thread-specific programming constructs in device code.–\\nDavid.I’m using OptiX 7.0.0, latest one.I could try and put some minimal and complete reproducer, but my stuff is always NVRTC JIT compiled, and using OpenGL interop where OpenGL owns the textures and buffers. But i dont think it would be as nice as the code from here to debug for you\\nhttps://stackoverflow.com/questions/58303950/writing-to-cuda-surface-from-optix-kernelI’d much rather you patch the Hello World SDK sample with this guy’s changes (far better repro sample)\\nhttps://stackoverflow.com/questions/58303950/writing-to-cuda-surface-from-optix-kernelFWIW, if you launch a 1280x720 optix raygen, we choose the block size, but other than it does equate to a kernel who’s dimension is 1280 * 720 threads. You can verify this in Nsight Compute, for example.Actually you use persistent threads (common raytracing trick and HPC GPGPU), I can see that you’re launching 288 blocks of 64 invocations on my RTX 2070, this actually turns out to be 8 invocations per “CUDA core” (I have 2304 of those, 36 SMs and Turing can do 64 SIMD in 2 warps of 32)So I’d presume there’s some cooperative CUDA going on or a shared global atomic counter and a circular buffer work-list ;)OptiX shaders cannot use shared memory, synchronizations, barriers, or other SM-thread-specific programming constructs in device code.Yeah, noted, already knew that… but image storage from a kernel is not any of the above, right?I asked around and discovered that we have an open bug report on surface writes, it’s indeed not working correctly in OptiX. I’ll follow up here when it’s fixed. Thanks for the report.–\\nDavid.Hi, I met the same issue during investigation of a related issue :surf2Dread in OptiX kernel.I created a minimal reproducer for this issue.Google Drive file.\\nI created two equivalent kernels, the one is written as OptiX’s raygen, the other is a normal CUDA kernel.\\nThe reproducer creates an array (512x256, float4) and fills all the pixels by red.\\nThe both kernels read the array via surface object and add blue gradient over the red image. If the kernels work as expected, the resulted image should be red to purple gradient.The reproducer is set to use the OptiX kernel by default. The result is completely red image in my environment. On the other hand, CUDA kernel (can be enabled by commenting out USE_OPTIX_KERNEL in test_shared.h) produces the gradient.\\nFor validating purpose, I put a macro to switch the surface object to a plain buffer by commenting out USE_SURFACE_OBJECT. In this case both kernels produce the gradient.Thanks,Environment:\\nWindows 10, 1909\\nNVIDIA Driver: 445.87\\nCUDA 10.1.243\\nOptiX 7.0, installed at the default location.\\nVisual Studio Community 2019, 16.5.4\\nRTX 2070Thanks. I downloaded the reproducer project and filed another bug for investigation.@devsh Just in case you hadn’t seen the message in the thread linked in comment 7, the R450 display drivers supporting OptiX 7.1.0 fixed the surface access for that case.\\nPlease try if you’re getting the expected results with drivers from that branch. Thanks.Powered by Discourse, best viewed with JavaScript enabled',\n",
       " 'cardiax-3n-pro-is-certainly-better-absorbable-and-bio-available-inside-the-body': 'MyPlate on Campus is a health initiative,cardiax 3n professional from the USDA , to induce college and university students thinking and talking concerning healthy eating in their campus community. The goal is to induce students who participate in the initiative to adopt healthy lifestyles that can stay with them during and beyond their school years. A negative body image can hurt girls’ nutrition, inflicting them to skip meals or take diet pills.   >>>>  [url]http://www.supplementsauthority.com/cardiax-3n-pro/[/url]Powered by Discourse, best viewed with JavaScript enabled',\n",
       " 'nvidia-smi': '| NVIDIA-SMI 450.119.03   Driver Version: 450.119.03   CUDA Version: 11.0     |\\n|-------------------------------±---------------------±---------------------+\\n| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\\n| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\\n|                               |                      |               MIG M. |\\n|===============================+======================+======================|\\n|   0  Tesla T4            On   | 00000000:00:1E.0 Off |                    0 |\\n| N/A   76C    P0    71W /  70W |   4065MiB / 15109MiB |    100%      Default |\\n|                               |                      |                  N/A |\\n±------------------------------±---------------------±---------------------+±----------------------------------------------------------------------------+\\n| Processes:                                                                  |\\n|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\\n|        ID   ID                                                   Usage      |\\n|=============================================================================|\\n|    0   N/A  N/A     32136      C   /snap/blender/206/blender        2415MiB |\\n|    0   N/A  N/A     32390      C   /snap/blender/206/blender        1647MiB |\\n±----------------------------------------------------------------------------+This is what I am using. Is there a way to change the type C to type G? as I need to use this for blender, so obv type G is better or type C+G. I am also not sure why GID and CID are N/A and GPU is 0, is my GPU even being used?I can be wrong\\nas I know Blender use CUDA instead of compute (because way too many bugs in OpenGL shaders)\\nand to display Blender use OpenGL 4.5you can click Help-System info in Blender to see your system specs that Blender useI do not know why you want see G+C in the nvidia-smi\\nas I saw in my apps G+C displayed for every GLES3.1 application when it does not use any compute, same as every Vulkan app has C+G when app not use compute… so it may be just “decorative value” and not displaying real application behaviorPowered by Discourse, best viewed with JavaScript enabled',\n",
       " 'xcode-installation-issues': 'Hello,When I tried to build the Optix’s sample code from Xcode (Version 8.3.1) I get the following error:nvcc fatal   : The version (‘80100’) of the host compiler (‘Apple clang’) is not supportedCMake Error at cuda_compile_ptx_generated_phong.cu.ptx.cmake:245 (message):\\nError generating\\n/Developer/OptiX/SDK/buildT/lib/ptx/cuda_compile_ptx_generated_phong.cu.ptxmake: *** [/Developer/OptiX/SDK/buildT/lib/ptx/cuda_compile_ptx_generated_phong.cu.ptx] Error 1\\nCommand /bin/sh failed with exit code 2So nvcc complains about an unsupported host compiler version.\\nThat normally means the CUDA Toolkit you used either doesn’t know of a newer host compiler or doesn’t support an older host compiler anymore.Check which CUDA Toolkit you used and what its documentation says about supported host compilers.\\nThe latter information can be found inside the per OS documents named CUDA_Installation_Guide_.pdf inside the CUDA doc/pdf folderFor example the CUDA 8.0 CUDA_Installation_Guide_Mac.pdf says on page 2 it supports Xcode 7.3 only.\\nMeans your host compiler is too new.Thank you for your fast response.I will re-install Xcode 7.3, and looking forward for support on Xcode 8.3.1.Thankshi Detlef,i’m also trying to install onto a MBP running 10.12,\\nand with Cuda compilation tools, release 8.0, V8.0.61but looking at the system requirements table here\\n[url]CUDA Toolkit Documentation\\nmade me believe it should work with my currently installed XCode 8.3.1?but i continue to get the same error, “The version (‘80100’) of the host compiler (‘Apple clang’) is not supported”are the sys requirements above inaccurate, or am\\ni just reading that table wrong?  do i still need to go back to Xcode 7.3?thanks for your help, RikBut the table you linked to says Xcode 8.2 and Apple LLVM 8.0.0 under Mac OS X 10.12, not Xcode 8.3.1 and LLVM 8.1.0(?) as the error message seems to indicate.Here are the more detailed release notes for Mac OS X linked on that site, saying Xcode 8.2.1 is supported:\\n[url]Release Notes :: CUDA Toolkit DocumentationSorry, I need to pass on Mac OS X questions apart form citing existing documentation. I’m not using OS X myself.thanks Detlef.  yes, i had noticed the differences in minor release numbers, but was trying to be optimistic about the basic Xcode v8.x with CUDA v8.x operability; that the issues now seem to have\\nto depend on these minor variants is discouraging.are there other NVIDIA moderators whose attention i might be able to get on the Mac OS X platform?\\ni’m trying to be prepared for a NVIDIA class ([url]https://www.eiseverywhere.com/ehome/236672[/url]) and i have to believe others will have the same issues.I recently dealt with the same problem. It’s not hard to solve, and you don’t have to revert to Xcode 7.3.You can follow the advice from StackOverflow that I’ve copied here:My addition to these instructions: If you are using CMake, then you can just install the command line tools in step 1 and set the compiler for NVCC to use there instead of doing steps 2 and 3, so you don’t even need to be an administrator on your machine.Does anyone know the date we will have an update, like CUDA 9 or something, that supports the newer LLVM provided with Xcode ? I have Apple LLVM version 8.1.0 (clang-802.0.42) and Cuda compilation tools, release 8.0, V8.0.54 installed.Thank you.Yes, I’m really waiting for a CUDA 8 update that supports Xcode 8.3.3!! Please NVIDIA release it! :-)Waiting for support for Xcode 8.3.3 and OSX 10.12.XHave supported for Xcode 8.3.3 or Xcode9 and OSX 10.12.6? What version CUDA should I download?Powered by Discourse, best viewed with JavaScript enabled',\n",
       " 'video-transcoding-using-multiple-gpus-32-live-streaming-jobs': 'Hello everyone.I’m having problems with ffmpeg transcoding.I have the following configuration:I know that one Quadro M4000 is capable to transcode 8 jobs with following characteristics:Video Input:\\n1080p, 24 fps, h264Vide Output\\n720p, 24 fps, h264\\n480p, 24 fps, h264\\n360p, 24 fps, h264\\n180p, 24 fps, h264Live streaming Input (Ethernet) → Transcoding → Live streaming output (localhost) (Multi resolution)These jobs are live streaming. This means that I need to keep an 1x speed.I’m using the following command:I want to transcode 56 of these jobs (7 GPUs with 8 jobs each) but today I’m able to transcode only 32 jobs (4 GPUs with 8 jobs each). If I launch another job, the speed starts to going down.With 32 jobs, CPU load average is too high but CPU consumption is 20% . RAM bandwidth is loaded at 4% of its\\ncapacity. I’m not writing to SSD.I have run several analysis using VTune and results say that I have problems with Frontend and Backend but I’m not sure how to interpret those results. I think that the nature of the jobs (live streaming) produce cache misses and branches misprediction resulting in stalls. The VTune results also says that CPI is to high (>2.5) and instruction retiring is approximate 15% of clock ticks.This is an image from VTune:\\nhttps://drive.google.com/file/d/1naqotnGl1pr8osi9p5mLcSH5dGoVF6ZS/view?usp=sharingSomebody has a similar configuration? What do you recommend to improve the performance? Do you think a server with two Sockets could improve the performance?This is the output of nvidia-smi topo --matrixProbably your issue is related to my findings here:\\nhttps://devtalk.nvidia.com/default/topic/1049717/video-codec-and-optical-flow-sdk/performance-limit-at-around-2500-fps-/You will need to patch and recompile ffmpeg with the patch I sent and test if it also fixes your issue.Hello malakudi,Thanks for your comment. I’m going to patch ffmpeg and comment the resultsProbably your issue is related to my findings here:\\nhttps://devtalk.nvidia.com/default/topic/1049717/video-codec-and-optical-flow-sdk/performance-limit-at-around-2500-fps-/You will need to patch and recompile ffmpeg with the patch I sent and test if it also fixes your issue.Hello malakudi,I have applied your patch and it is working.\\nI’m transcoding 64 jobs using seven GPUs (RTX 4000: 16 jobs, M4000: 8 jobs each). Have you found an explanation for this patch?If you want, follow up the open ticket at #7674 (ffmpeg with cuvid transcoding after version 3.4.1 work unstable on heavy load CUDA card) – FFmpeg with your user case and confirm the fix I posted works for your case too.My understanding of the code is not enough to explain why this code affects performance and commenting it out brings performance back. I found it by cherry picking commits.Powered by Discourse, best viewed with JavaScript enabled',\n",
       " 'optix-3-6-3-released': 'Hey folks,Most of you are registered developers so you already received mail that we released OptiX 3.6.3. If you didn’t receive this email you need to sign up to be a registered developer. Go to NVIDIA OptiX™ Ray Tracing Engine | NVIDIA Developer to sign up. If you are already a registered developer and need to change your password or username, email OptiX-Help@nvidia.com.3.6.3 was a bug fix release, with the following fixes:•\\tBug fix for Trbvh.\\n•\\tBug fix Commercial release had some warning messages.\\n•\\tBug fix RTP_BUFFER_FORMAT_VERTEX_FLOAT4 wrong ray tracing kernel was selected.\\n•\\tBug fix Overly large library size on Linux.\\n•\\tBug fix Properly dirty buffer contents for buffers accessed via rtBufferGetDevicePtr.\\n•\\tBug fix Buffer dumping of GPU_LOCAL buffers in OAC.\\n•\\tBug fix Result buffer contains proper results when scene is empty in Prime.\\n•\\tBug fix Determine hit by testing t < 0 in Prime samples.\\n•\\tBug fix Properly build Trbvh over multi-Group accelerations with custom BoundingBox programs.\\n•\\tBug fix Trbvh peak GPU memory usage further reduced.\\n•\\tBug fix Double-precision cosine and other transcendentals now work properly, although ray tracing internals are still single-precision.\\n•\\tBug fix NVAPI error message thrown when using RemoteDesktop and only TCC-mode devices.\\n•\\tBug fix In Prime, buffer descriptor offsets for indices and vertices now work properly.\\n•\\tBug fix Buffers of structs that contain Buffer IDs now work.\\n•\\tBug fix Buffer of Buffer IDs corner cases now work.good!how to download?Go to NVIDIA OptiX™ Ray Tracing Engine | NVIDIA Developer to sign up for the registered developer program. We will then email you the info to download it.Powered by Discourse, best viewed with JavaScript enabled',\n",
       " 'optix-for-real-world-ray-tracing': 'Is Optix useful only for rendered scenes or can it be used for real world applications like medical imaging?OptiX is a general purpose high-level ray-casting SDK, not a renderer.\\nIf you have a visualization problem which can be solved by shooting rays, you can most likely implement it using OptiX.For examples about what developers are doing with OptiX, take a look at the GPU Technology Conference presentations by searching for “OptiX” here:\\n[url]https://on-demand-gtc.gputechconf.com/gtcnew/on-demand-gtc.php[/url]If this about the usual medical visualization of density volume grids, there isn’t always the need for a full blown ray tracer which is optimized to find ray - geometry intersections. Some solutions simply involve ray marching which can also be done efficiently with native compute APIs. If there is geometry in the scene as well, that could use a raytracer. It depends on the use case what solution would be the best fit.Are there any resources to understand the difference between ray-tracers and ray marching? Thank you.A simple web search for “medical image visualization with ray marching” turns up enough articles and papers on the first page to explain these methods, even a wiki page.\\n[url]https://en.wikipedia.org/wiki/Volume_ray_casting[/url]I want to study light propagation through a crystal using ray tracing. Can optix work on a real crystal or can it only work on rendered objects?You can use OptiX to simulate light propagation through a physics based model of a crystal. (And it doesn’t need to be light, you can compute any type of radiation that you can simulate and model using your own code, such as sound or heat or gamma rays, etc…) You’ll be responsible for modeling the crystal and it’s properties, and for modeling your light propagation and it’s interaction with the interfaces between media, i.e. crystal vs air. You are not required to render a picture with OptiX, you can design what you want the inputs and outputs of your simulation to look like.–\\nDavid.Powered by Discourse, best viewed with JavaScript enabled',\n",
       " 'better-character-controller': 'Hey,I’ve had a character controller implemented into my engine for a while now using PhysX, and it works decently well, but I’m looking for a better solution. The biggest issue I have is that the character controller doesn’t support cylinders because their math is hard to work with, and I’ve never really liked the way that capsule character controllers gently slide down a 90º drop-off because their bottom is rounded. I also had some issues with collision between the character controller and dynamic objects, and the behavior was inconstant, sometimes not being able to move the dynamic body and sometimes hitting it like a truck.I’ve been playing a lot of Natural Selection 2 lately (great game, go buy it if you haven’t) which also uses PhysX. They have some behind the scenes videos where they show the PhysX visualizer where they show their character controller as a cylinder, which I assume was approximated with a convex mesh… Heres a link if anyone is interested: NS2HD[328] - A Day in the Life of UWE - YouTubeI played around in the game for a little bit and noticed that for some of the other characters in the game they use shapes other than cylinders. For instance one of the characters in the game is almost rhinoceros like and uses a large, definitely not cylindrical or capsule shape.I wanted to revisit my implementation of the character controller and try to make it smoother as well as be a cylinder or another shape. I’ve tried using a rigid body with some modifications and setting the velocity every frame, but I had some very noticeable jittering issues when the controller pushed up against a wall. I also tried to use some sweeping of a convex mesh but I was a bit confused by the hit result’s distance and making sure that the shape didn’t penetrate and get stuck. Any ideas on what I can do to improve my controller?ThanksPowered by Discourse, best viewed with JavaScript enabled',\n",
       " 'fluid-simulation-flex-and-multiplaform': 'Hello everybody,We are a studio working on multiplatform titles. Our next game will be released for next generation, so we have a few questions.\\nNow a days, we have already ported the old plugin for Flex on UnrealEngine 4.15, to the current UE4.25 version. We are prototyping some features for our game, and we love the feeling we are getting with this technology (the performance is awesome too). So in order to continue working project, we would like know what are your thoughts or your roadmap for this technology.In case of not planning to continue working with Flex, we would like to know if some fluid simulation multiplatform system will be added to the new Physx5.0.I’ll appreciate If anybody can let me know another fluid simulation multiplatform better or at least similar than Flex, it would be awesome too.Thanks for your time,Juan M.Montoya,Powered by Discourse, best viewed with JavaScript enabled',\n",
       " 'how-to-influence-auto-select-of-dedicated-graphics-card-for-our-game': 'Hello,\\nI’m developer of the game Imagine Earth.We have a problem with the auto-selection of the dedicated vs. integrated graphics card on laptops.\\nThe NVIDIA System automatically selects the integrated graphics card for our game which results in low FPS. So each player has to manually change this to the dedicated graphics card for our game.Is it possible to influence this auto-selection somehow?Best, MartinIs there no way to influence the default selection for a program?Powered by Discourse, best viewed with JavaScript enabled',\n",
       " 'sharing-texture-by-direct3d9-and-register-using-cuda': 'Hello,Iam trying to share texture using Direct3D9 and register it using cuD3D9RegisterResource(…).So i should pass the sharedhandle parameter through Direct3D9- Createtexrure(…).When i try to pass this parameter as “NULL” it works fine.But I have to share the texture with multiple device and for this I have to pass the sharedhandle parameter.But what happening here is :- if this parameter is passing as sharedhandle thus causes an error “CUDA_ERROR_INVALID_HANDLE”Iam attaching a portion of my code here.HANDLE sharedTextureHandle = NULL;\\nHRESULT hResult =  pChInfo>pD3D9Device>CreateTexture(tVideoFormat.coded_width,tVideoFormat.coded_height, 1,\\nD3DUSAGE_RENDERTARGET /* | D3DUSAGE_NONSECURE*/,D3DFORMAT::D3DFMT_A8R8G8B8,\\nD3DPOOL::D3DPOOL_DEFAULT, &pChInfo->sharedD3DTexture, &sharedTextureHandle);assert(S_OK == hResult);/* get the surface level */\\nhResult = pChInfo->sharedD3DTexture->GetSurfaceLevel(0, &pChInfo->sharedD3DSurface);\\nassert(S_OK == hResult);checkCudaErrors(cuD3D9RegisterResource(&pChInfo->sharedD3DTexture, 0));//error “CUDA_ERROR_INVALID_HANDLE”Is there any way to share the texture with multiple device using Direct3D9 and cuda interoperability.Powered by Discourse, best viewed with JavaScript enabled',\n",
       " 'nvidia-geforce-rtx-4070-ti-support-for-vulkan': 'I am working on seting up a Vulkan project (my operating system is Windows 11). I have an NVIDIA GeForce RTX 4070 ti, and yet the VK_KHR_swapchain Vulkan extension is not supported, despite having installed the Vulkan driver. Given that swap chains are fundamental to actually proceed with any project, something is surely not right here. Can someone help me out?These are the extensions my GPU supports, according to the values returned from vkEnumerateInstanceExtensionProperties:\\nimage593×599 13.4 KBDoes this even look close to the number that should be there?Hello @Jobless_Squirrel and welcome to the NVIDIA developer forums.If that is the complete list, then there is definitely something wrong.What do you mean byhaving installed the Vulkan driverWhen you install the normal Windows NVIDIA graphics driver, Vulkan support is automatically included.Can you run vulkaninfo.exe and check if that even lists the NVIDIA GPU? Feel free to attach the output here.Thanks!Powered by Discourse, best viewed with JavaScript enabled',\n",
       " 'nvidia-surround-detects-4k-instead-of-the-native-1440p-for-multiple-monitors': 'Hi,I was told by Nvidia support to come here and ask for help. I have three BenQ EX3203R’s that are native 1440p monitors being driven by a Nvidia RTX 3080. They detect as native 1440p monitors in Windows 10 with the latest WHQL driver but when I try to use Nvidia Surround, it calculates the three for a total of 11520x2160 instead of 7680x1440. The option to add a resolution is greyed out. I’ve used the Nvidia cleanup tool, latest drivers and to no avail as per Nvidia support’s direction. I have the three monitors connected via DisplayPort but have tried one of them with HDMI which made no difference as others have suggested via Google searches. When searching for this issue, there are other people with 3080’s and 3090’s that are affected. Hope you can help!Here is a thread from other users on the nvidia forums:\\nhttps://www.nvidia.com/en-us/geforce/forums/geforce-graphics-cards/5/423071/nvidia-surround-3090fe-triple-screen-not-working/KenHi!\\nI have had the same problem for a long time. I think it’s a problem on Benq’s side as the driver seems to report the resolution 3840x2160 to windows. I think it can be worked around using CRU (Custom Resolution Tool), but I found a simpler way, at least if you don’t have to switch between triples and VR all the time, in which case it doesn’t work well.If you enter Nvidia control panel and go to Configure Surround → Configure. Select your topology and your screens. Then enable surround with the resolution 11520x2160. The result will look garbled, but when it has finished after a few seconds you will see that the small button on top od the resolution dropdown will enable. If you press that, you will be able to add your resolution, I suppose it is 7680x1440. MAke sure you can see that resolution in the ‘active’ resolutions section. Press OK. Now you can select the correct resolution in the dropdown. Select it and press ‘Apply’. You should now have surround activated with the correct resolution. IT shall also be possible to select the correct refresh rate (144/120 Hz depending on which Freesynd mode the monitor is in).Hope this helps.\\n//JohnThanks John! Your solution works well.Powered by Discourse, best viewed with JavaScript enabled',\n",
       " 'using-nvidia-opencl-drivers-with-an-icd-loader-on-windows': 'I have configured Installable Client Drivers (ICDs) on an Ubuntu-distribution with Intel OpenCL runtime drivers, which was fairly straightforward. These drivers come as * .so-files which can be loaded by specifying their path in an *.icd-file under  /etc/OpenCL/vendors/How would I proceed to specify Nvidia OpenCL-drivers on Windows, and where are these drivers located?I use MINGW64 with and OpenCL-ICD-Loader installed via MSYS2Where is the corresponding directory to add the *.icd -files?Powered by Discourse, best viewed with JavaScript enabled',\n",
       " 'mosaic-and-cloning': 'Hi,I have a 4K 120Hz 3D projector (Christie). This projector uses 4 Full HD inputs to create the 4K image.\\nTo manage this, I use Mosaic under Windows 10 Pro.In order to properly manipulate my applications on this screen, I would have to clone the display to another screen.My system has 2 quadro P6000 (no SLI), one dedicated to the video projector (4 DisplayPort outputs for Mosaic), and the other the secondary display.Currently, it is impossible to clone the Mosaic display to the secondary display.I did a test with 2 quadro P4000 in SLI, but as soon as I create the Mosaic configuration, the option allowing me to clone disappears.What are the solutions that can be used to clone the Mosaic display to a secondary display?Thanks.Powered by Discourse, best viewed with JavaScript enabled',\n",
       " 'application-crash-in-latest-nvidia-driver-while-calling-glclientwaitsync': 'Using the latest driver version 456.38 on Windows using a 1050 I am able to reproduce the crash using this code.Also note that this code works on Linux using the Nvidia driver version 455.23.Is there any way to work around this? Any help would be appriciated.I’ve narrowed the problem down to a shader execution. The following fragment shader, after execution causes the driver to crash at the next client sync point:If I replace the shader with a stub shader that simply samples the cube map array with the passed in uv coords no crash occurs. Again note that all this code works on the Linux Nvidia driver version 455.23.Edit: If I increase the sampleDelta to 0.1 the crash doesn’t occur so this seems to be timing based.Powered by Discourse, best viewed with JavaScript enabled',\n",
       " 'starting-to-work-with-physx': 'Hi\\nI’m new to Physx and want to learn about it to be able to check if we can use it. When reading the documentation it almost immediately starts with coding. But I would like to read first about how PhysX works in more general terms. Which steps are needed. What do you have to upload to PhysX and in which format. If running a timestep, what does it calculate and how is the result presented. Can you do a collision detection for only one component? (what is reported for collisions). EtcIs there any document or turotrial avaialble that does not explain the code but the overall function/operations?ThanksI’ll help you keep this post live, as long as it takes, until a solution presents itself because man, I feel you. It’s a shame that I’ve been looking for stuff for almost 2 weeks and there’s only dated install info on what to use. It’s difficult to find what you’re looking for if the vocabulary isn’t taught. nearly 20 years doing this and after all this time, programmers still don’t know how to talk to artists.My friend and I were able to find this link to the newest Repo to PhsX 4.1.1:NVIDIA PhysX SDK. Contribute to NVIDIAGameWorks/PhysX development by creating an account on GitHub.We downloaded the Repo and behold… more programmer language that needs deciphering:Here’s what I understand… Typically, when normal people install things, we use an *.EXE. There are some in the directory (see attached). However, in the README file, it doesnt state which one of these will work with Max.So if there is no exe, that means we have to compile our own stuff.I found this Python Compiler. My instinct is telling me that this PhysX Repo is a stand alone piece of software that has absolutely nothing to do with obtaining a plugin for Max.OnlineGDB is online IDE with python compiler. Quick and easy way to compile python program online. It supports python3.Hmmm… conflicting info much? I will be posting my frustrations inside the Unreal and Max forums as well.\\xa0Thank you for posting your issue about this. I support you #Mechamania!I might be able to help sort out some of these issues (not affiliated with NVIDIA).I think part of the problem is one of terminology.These are generally different audiences too; this forum is particular suited for (1), the programmers using PhysX as another library and technology (along with OpenGL perhaps), hence the programmer-speak. The Unity forums for example are better suited for (3) and (2) doesn’t really have a single community other than what you find spread across various gaming forums.The GitHub page you found is also independent from the (2) and (3) and only really applicable to (1), that’s why the instructions are all in programmerese rather than plain English.With this in mind, you can start seeing how “getting started with PhysX” could be interpreted in a few ways.The most user-friendly (official) documentation is probably this.And the most programmer-friendly documentation is likely this.Hope it helps!Best,\\nMarcusPowered by Discourse, best viewed with JavaScript enabled',\n",
       " 'addforce-after-create-immediately-not-work': 'I create a dynamic box,and addforce at once,it seems not work.\\nBut if I addforce at the second frame,it works.\\nIs there anything wrong with me?\\nI’m using 3.2.2.\\nThanks!Hi,Did you add the box to the scene while the simulation was running?  If you do this then if you also call addForce while the simulation is running then the force is lost.  This is a bug that we have already addressed for 3.3 but not yet addressed for 3.2.Thanks,GordonPowered by Discourse, best viewed with JavaScript enabled',\n",
       " 'missing-reduced-coordinate-articulation-joint-getforce-pid-controller': 'Dear developers,\\nthe PxJoint has a getConstraint method returning a PxConstraint, that has a getForce method.I am astonished the same is not true for PxArticulationJointReducedCoordinate.Is there any way to detect the applied force from the joint drive for these.Also the drive has only a PD controller (stiffness and damping), however for modeling any real servo there is need for PI(D) control loop. I have been looking over the sources, however the real\\napplication of the drive is so burried I cannot find how I would extend the drive code.Should I ditch the drive and make my own applying the forces externaly with smaller sim step?\\nThe integrated joint drive have advantages with multiple iterations.Powered by Discourse, best viewed with JavaScript enabled',\n",
       " 'how-to-enable-b-frames': 'Hi!In the programming guide at the end there is “Recommended NVENC settings for various use-cases” (Table 1). For some configurations B frames are recommended, but I see no setting where to enable/disable B frames. Where can I do this?JochenHi Jochen,You can enable B frames by NV_ENC_CONFIG:: frameIntervalP. The explanation of this variable is provided in the comment of nvencodeapi.h.If you are using B frames, please also consider enabling other features like LookAhead, B-as-ref etc. which further improves the quality on top of normal B frame feature. These are documented in the programming guide.As a general suggestion, we would also encourage you to look at the nvencodeapi.h along with the programming guide. The encode API header also provides good amount of information/documentation as how to use to API.Thanks,\\nRyan ParkHi Ryan,Thanks for the info. In the code I see frameIntervalP = 0: I, 1: IPP, 2: IBP, 3: IBBP.\\nIs it also possible to have for example IBBBBBP by setting frameIntervalP to 6?Is it required that gopLength is a multiple of the frame interval length? Does it simply repeat the GOP pattern? Does for example frameIntervalP = 1 and gopLength = 9 result in IPPIPPIPP or is it IPPPPPPPP ? And does frameIntervalP = 3 and gopLength = 16 result in IBBPIBBPIBBPIBBP or is it IBBPBBPBBPBBPBBP ?\\nOther possibility would be representing frameIntervalP as regular expression:is itframeIntervalP = 0: I+, 1: (IP)+, 2: (IBP)+, 3: (IBBP)+or (more likely)frameIntervalP = 0: I+, 1: IP+, 2: I(BP)+, 3: I(BBP)+?JochenYes. That’s right. The number of B frames is frameIntervalP-1.No. The GOP length means the interval between 2 Intra frames. Say if gopLength = 30 and frameIntervalP =3 then the pattern will look like IBBPBBPBBP….And one I frame will be introduced after 30 frames and so on. As explained in the encodeapi header frameIntervalP is the GOP pattern and is not related with GOP length.Let us know if you have further questions.Thanks,\\nRyan ParkPowered by Discourse, best viewed with JavaScript enabled',\n",
       " 'state-of-directx-interop': 'Hey.What’s the state of DirectX interop on OptiX 4.1.0? I see that optix::Context still doesn’t expose it and it’s not mentioned in the programming guide, but the C API still contains the relevant functions in optix_d3d11_interop.h. So before I start re-writting my interop layer to use the C API I’d like to know if it even works or if I should wait for 4.2. If it’s not yet 100% done and you’re looking for beta testers, then I’d also happily lend a hand.CheersDirectX interop is not currently supported in OptiX 4+.  Can you tell us what you’re doing with it, and make a good case for bringing it back?  You can send mail to optix-help if you like, or post here.Nuts.I only use OptiX for my sparetime path tracing, where I picked DirectX over OpenGL because I wanted learn DirectX, since I’d only used OpenGL in the past. I guess that choice is going to bite me in the ass now. :)\\nThat’s not really a good case though, so all I can argue is ‘don’t deprecate what works well’ and ‘professional pride’. I hope that it gets reintroduced sometime soon’ish. Until then I guess I can do it myself using CUDA buffers and CUDA-DX interop.\\nYou should deleted the optix_d3d11_interop.h and similar headers then though, if it’s not supported at all. Just to avoid any future confussions.Agreed, we should stop publishing the deprecated headers.Powered by Discourse, best viewed with JavaScript enabled',\n",
       " 'anisotropic-friction-in-physx-3-x': 'Hi guys,Anisotropic friction has been depricated in PhysX 3.2. I’m wondering if there is a way to implement sleigh like behaviour on contacts (when object slides in one direction better than another). Migration guide doesn’t provide any pointers.I know that sleigh can be implemented with help of wheels in the same way as vehicle does but unfortunately that’s not the case because I can’t predict which side of object will touch a surface.Thank you,\\nArtemPowered by Discourse, best viewed with JavaScript enabled',\n",
       " 'can-i-get-some-force-like-friction-by-physx-in-ue4': 'Can I get static friction or dynamic friction during the simulation physics? If there is only one friction, I can calculate the result. But most of the time there will be two frictions and the results cannot be calculated.I can’t find related variable of Force in the source code.I am using UE4.20Powered by Discourse, best viewed with JavaScript enabled',\n",
       " 'actor-with-triangle-mesh-shape-and-collisions': 'Hi,To avoid error, I use the SmapleHelloWorld as base.I create a triangle static actor.\\nI also create a sphere kinematic Actor.\\nI also create a triangle kinematic Actor.I activate collisions between kinematics.I move the kinematic sphere to the triangle static actor and I can detect the collision. (with stKinematicTarget)I move the kinematic triangle actor to collide with the triangle static actor with setGlobalpose and there is not collision (normal) and all is ok.However, if I move the kinematic triangle actor with setKinematicTarget, there is a Debug Assertion failed, the program crash and in the console there is this message:PxcNpBatch.cpp(713): Assertion failed: materialMethodMy Material is a simple:PxMaterial *mat=getPhysics().createMaterial(0.4,0.5,0.5);Is it a bug?ThxI have installed 3.3.3Same error, but How I have the source code, I was checking the file PxcNPBatch\\nThe error now is in line 695.PxcGetMaterialMethod materialMethod = g_GetMaterialMethodTable[g0][g1];I check that my values when I have the kinematic sphere and the static trimesh are (g0,g1): 0 5But, I have the error when I have the kinematic trimesh and the static trimesh which values are (g0,g1): 5 5And g_GetMaterialMethodTable[5][5] is 0So, Collisions between trimesh static and trimesh kinematics are not allowed?Powered by Discourse, best viewed with JavaScript enabled',\n",
       " 'sdk-path-tracer-sample-app': 'HI, I was snooping into SDK samples, in particular the Path Tracer sample.Path Tracing is the main reason of my interest into Optix.The problem is that I’m very interested,but I know very little of Path Tracing (in the past I’ve used only classical raytracing => PovRay)I mean: how to implement the fastest Path Tracer, I can do, with Optix (best practice)I’ve found this lines of code into the file “path_tracer.cpp” (SDK 3.5.1):Sound promising ! I’ve searched, but this variable (“sampling_stategy”) is not used.The question are:Can you please complete the example? …pleaseCan you please add some more Path Tracing examples to SDK? E.g. “Bi Directional Path Tracing”I’ve seen that there is a texture “tv.ppm” under the folder “data” but it is not usedPlease NVidia guys…==> Full “Design Garage” Source Code, would be a good start… ;-)take a look at mis_sample, it contains the mentioned sampling strategies.@bi directional pt\\ni suppose the samples are made to demo the functionality and explain the framework. since the given samples already do that, there is no need for an additional bidir pt example.but anyway, i’m working on a bi dir renderer for an university course and i’ll probably release the source code after the course is finished (in may).…but anyway, i’m working on a bi dir renderer for an university course and i’ll probably release the source code after the course is finished (in may).Thank you for the quick reply.i’ll look forward to hearing your news soon…Please, reply to this topic with the download link when done.Thank you very muchHehe, it’s been over two months, but I finally finished coding and made a blog entry including the source code: Global illumination rendering using path tracing and bidirectional path tracingPowered by Discourse, best viewed with JavaScript enabled',\n",
       " 'gl-arb-texture-gather-problem-on-opengl-3-3-capable-gpu': 'I am currently having trouble compiling fragment shaders that rely on textureGather function enabled via extension in GLSL 3.3. I’ve put together a simple fragment shader that causes this.While I am able to compile this on GTX 760 without any problems, my laptop with NVS 5100M keeps giving me following compile error, even though when I check for extension support it is availableI tried searching for some related information, but couldn’t find any and I am wondering whether someone had any trouble with this before. It looks like a driver bug to me (tried updating drivers, but it didn’t help), but I wanted to hear someone else’ opinion before submitting a bug report.Powered by Discourse, best viewed with JavaScript enabled',\n",
       " 'physx-3-2-2-vehicle-wobbles-around-and-moves-uncontrolled': 'Hi!I’m trying to implement the vehiclesdk into my framework, i’m nearly finished with the vehicle - but it moves uncontrolled around and jitters/wobbles… shaking? I don’t know which word is correct >_<\\nI have no idea from what these behavior comes, i triple checked my vehicle against the vehicle sample from the sdk, i think i have done mostly the same. Maybe somebody has a hint for me or another sample or… something that helps :(Also, it looks like the wheels are very static, it doesn’t looks like the suspension in the sample, but the values are equal… but i call the VehicleSuspension function and the updates function after that…Has someone a hint or a solution?Hi Lunatix!\\nIf you could provide some code to which we could relate to, it would make helping a lot easier. Without knowing the code I would expect some functions being called twice, initialized with wrong values or have wrong values passed.\\nThat’s what I can say for now. If you can get us some samples, then we could help some more.\\nGood luck.I finally figured it out: My chassis where where rotated by the 3d modeller, so i transformed the localpose of the chassis shape (-90°, -90°, 90°) - when i corrected the vertices at loading time and left the shape as Matrix.Identity, the behavior was as it should be :)\\nOnly thing now is, that the wheels are spinning very fast but don’t get much grip…Is this a PxMaterial thing? But in the vehicle sample, the materials of the terrain are as mine on a plane (0.5f, 0.5f, 0.2f). My tires have “0.8f, 0.8f, 0.2f” and the chassis has “0.5f, 0.5f, 0.2f”. Engine has enough power, vehicle mass is around 1400 and the gravity is -9.81f… also, things are falling a bit slowly o_oAny suggestions?So, i figured this out, too.\\nVehicle was to big and i had to correct some values… its almost not perfect now,\\nbut that will be a bit more finetuning.Thanks for trying to help, jaworekplay :)Hi,Sorry for the difficult time with the vehicle controller, we are working on API and tools improvements to make tuning easier in a future version of the SDK.Thanks,\\nMikePowered by Discourse, best viewed with JavaScript enabled',\n",
       " 'internal-compiler-error-minimal-sample-name-a-0001-shouldnt-be-defined-but-i': 'Hello,I’m working on a tool to minimize shader code size after compression. One aspect is to re-use variable names as much as the OpenGl spec allows, for better compression rates, and this triggers an internal compiler bug in recent Nvidia driver.Here is a minimal code snippet that reproduces it:This gives the errors: error C9999: Name “@a-0001” shouldn’t be defined, but is!\\nand: error C1038: declaration of “@a-0001” conflicts with previous declaration at 0(11)The issue seems to be the declaration and initialisation of a local variable (a) with the same name as a global variable, if that global variable has been used in the same scope. This is perfectly fine according to the OpenGL scope rules, the local variable should simply hide the global variable for the rest of the scope.This has happened at least since the notebook driver version before the 364.72 one, and still happens in 368.39. It did not happen in drivers from about one year ago (not certain, I don’t update the drivers on that laptop very often). It also doesn’t happen on AMD cards.Kind regards,\\nSvnThanks for reporting this issue.\\nIt has been identified and fixed in an internal driver version by the compiler team.\\nAt this time I cannot say which public driver release version will pick up the fix.Powered by Discourse, best viewed with JavaScript enabled',\n",
       " 'h-264-encoder-fails-with-nv-enc-err-unsupported-param-with-insider-builds-of-windows': 'We have our software using hardware H.264 encoder with various NVIDIA adapters. Recently we started getting reports from users on unexpected failures, and the are all on insider builds of Windows (e.g. 19033 and higher).At this moment it is hard to tell more details even though we have some minidumps on hands because debug symbols for insider builds are not published. So far we know:If there are any known problem or workaround, please share. I will update as I have more details.Also one of the users reported that he has side by side another video encoding software which seemingly works, so I am guessing it might be something like this https://devtalk.nvidia.com/default/topic/1069890/video-codec-and-optical-flow-sdk/possible-bug-forceidr-to-resend-sps-pps-does-not-work-in-d3d11-encoder-api-but-works-in-cuda/ related, or there is a composition of problems.OK, so for whatever reason NVIDIA Video Encoder API started enumerating 7 new presets (whether these are new yet not announced and documented ones, or it’s a math problem with number of elements in enumeration). When preset details are requested the API responds with a failure as I mentioned above.And yeah I only managed to reproduce the problem once I upgraded a rig to fast ring insider build 19555.Powered by Discourse, best viewed with JavaScript enabled',\n",
       " 'hello-need-help-pls': 'my software\\namd fx 6100\\nnividia gtx 650 2gbhello i have been trying 2 play on my steam account 2 games chivalry medieval warfare,war of the roses with my new and awesome nividia card. only 2 see (physx cpu) and a key locked symbal painted across my screen the whole time i play them. this does not happen with any of my other games and only started recently with these games.as you can imagine this is worrying and confuseing any help or answers would be cool if anyone can help thxFirst of all, this is wrong Forum. Right one is located here\\n[url=http://forums.geforce.com/default/board/48/physx/]http://forums.geforce.com/default/board/48/physx/[/url]Second, I believe you are talking about so called PhysX Visual Indicator.\\nIt can be disabled through “NVIDIA Control Panel” - > “3d Settings” → “Show PhysX Visual Indicator”\\n[url=http://physxinfo.com/wiki/File:PhysX_Visual_Indicator_-_Drivers.png]PhysXInfo.com - MEGAGAME.AIsorry for wrong location i was lost but thank you for replying you were right lol panic over thank you…:)Powered by Discourse, best viewed with JavaScript enabled',\n",
       " 'working-with-large-meshes': 'Hello,I am currently working with very large triangle meshes. Until now, I’ve been using rtuCreateClusteredMeshExt(…) to internally format meshes for optimal paging performance.The problem is that I need to track the id of the object to which an intersected triangle belongs. Currently I’m achieving this using separate material programs for each object in the scene, which are then called from the internal rtuCreateClusteredMeshExt(…) collision routine using the materialIndices argument (thank you for fixing this in Optix 3.0.1 btw!).The problem is that there can be several thousand objects in a scene, and using a seperate material program for each object (where each program almost exactly the same thing) is causing significant warp divergance.Is there another way to do this using rtuCreateClusteredMeshExt(…)? For example, is there a way to directly determine the primId of the intersected triangle found in rtuCreateClusteredMeshExt()?If this is not possible, is there way to structure my mesh such that its corresponding acceleration structure (I’m using Lbvh) uses less memory? I’ve noticed a near order of magnitude difference in acceleration structure size when using my native meshes vs those produced with the clustered mesh routines.Cheers.HiI think it’s impossible now to directly determine the primId of the intersected triangle found in rtuCreateClusteredMeshExt. And using material id is the only workaround for the moment. You can define different materials but the same closest hit program for them, each material will have its own id in a variable. That will decrease divergence, but still it’s waste of material memory.  The right thing would be to add primId attribute into the intersection program of rtuCreateClusteredMeshExt. This will be done in the next OptiX release.In rtuCreateClusteredMeshExt cluster of triangles is a primitive for AS, that’s why AS is smaller. To decrease device memory consumption of AS you can also use BvhCompact traversal.Hi qconst,Thank you for your response. Currently, my solution is exactly what you proposed: using the same closest hit program for each material, and tracking ids using a variable associated with each material. Unfortunately, the performance is not great when I have 1000s different materials (objects) in once scene.Being able to directly access the primId as an attribute would be great. Is there a rough release date planned for the next OptiX version?Also, thank you for the tip to use the BvhCompact traverser. This possibility had slipped my attention.Powered by Discourse, best viewed with JavaScript enabled',\n",
       " 'vulkan-profiling': 'Hi all, I’m looking to profile my Vulkan application on NVidia GPUs as we get horrible performance on them currently.  After looking for a Vulkan profiler for older NVidia GPUs (9xx series), I’m left wondering what I can do other than provide a terrible experience on NVidia cards?That said, are there any profiling tools available?  Are there perhaps some hardware counters which might help me get an idea of what’s going on in my application?Thanks in advance!Kevin Bdo you have tried Nsight connected with visual studio ? For me, it’s good working in performance termI can do “profiling” that tells me when the CPU is waiting on the GPU, but I can’t get access to any metrics on shader occupancy, GPU-side bottlenecks, or anything of the sort.  The software tells me I need a Turing or RTX hardware to access that functionality with Vulkan.  Are you able to get it to work for older GPUs somehow?  My primary dev machine has a 970 GTX in it.No i can’t get anything of that. I think that kind of information is only for dx users.\\nI can get one frame capture, see when gpu do what, and how time it took too cpu for doing this. But the debug names are broken, so Nsight become useless at this time…\\nI hope somebody will teach us how doing thatPowered by Discourse, best viewed with JavaScript enabled',\n",
       " 'physx-3d-studio-max-plugin-3-0-1-exporter': 'Does anyone know why, when exporting spherical joints in a Physx scene as a .repx file from 3d Studio max, creates d6 Universal joints instead of PxSpherical Joints in the xml/repx file?Have you tried the latest 3DS Max plugin (3.0.2) ? Found at https://developer.nvidia.com/gameworksdownloadThis is because those joints are all based on d6 joint in Plug-in.Powered by Discourse, best viewed with JavaScript enabled',\n",
       " 'epic-integrates-nvidia-developer-tools-for-ue4-android-development': 'At GDC 2014, Epic Games released Unreal Engine 4 to the world, releasing all of its leading-edge tools, features and complete C++ source code to the development community to build games for PC, Mac, iOS and Android. Learn more at https://www.unrealengine.com.Epic Games uses NVIDIA Tegra Android Development Pack and NVIDIA Nsight Tegra, Visual Studio Edition as their Android development environment for Windows PCs.If you are or want to be an Unreal Engine 4 developer for Android, Epic Games has included NVIDIA Nsight Tegra, Visual Studio Edition for pain-free native debugging on Windows, empowering Unreal Engine 4 developers with an amazing visual debugging experience. Using NVIDIA Tegra Android Development Pack and Nsight Tegra is an ideal approach to jumpstart developing Unreal Engine 4 games on Android for UE4 licensees and developers with PC and console backgrounds without learning an unfamiliar environment.“NVIDIA’s Tegra Android Development Pack (TADP) and Nsight Tegra Visual Studio Edition 1.5 enable a turnkey Unreal Engine 4 development environment for Android. These tools simplify installation of an Android development environment, and ensure that Visual Studio building and debugging of UE4 and NDK is completely seamless.”\\nEpic Games on NVIDIA Tegra Android Development Pack and Nsight Tegra.Powered by Discourse, best viewed with JavaScript enabled',\n",
       " 'is-it-possible-to-use-rtxgi-ue4-plugin-in-linux': 'Since the RTXGI UE Plugin require the Windows 10 64-bit operating system, but I want to package a game in UE with RTXGI UE Plugin, and run it in Ubuntu, is there any way to make it?  I only have two servers(Ubuntu/Windows Server 2019)，none of them support RTXGI UE Plugin now.\\n\\nimage791×489 24.9 KB\\nHello @hitwxj and welcome to the NVIDIA developer forums!I am sorry I have to disappoint you, but that will not work. You might get around the Windows 10 requirement, but it has to be a 64bit Windows based operating system. Because the other dependency of DirectX Raytracing API, which is a Microsoft Windows API, is essential. You will need DirectX 12 as well to make this work. And so far DX12 does not run on Linux operating systems.I hope this answers your question.Thanks!DirectXThanks for ur reply. BTW,  I can see the windows 2019 server operating system support DirectX API, can I use RTXGI UE4 Plugin in windows 2019 server?In case you manage to install DirectX 12 including the DircetX Raytracing 1.0 API, then it is possible, but not guaranteed.\\nThe only officially supported environment is the one mentioned on the product page, which is Windows 10 64bit.ok, thanks!This topic was automatically closed 14 days after the last reply. New replies are no longer allowed.Powered by Discourse, best viewed with JavaScript enabled',\n",
       " 'timing-rttrace-via-nvapi': 'Hi,\\nI just came across this blog post:\\nhttps://developer.nvidia.com/blog/profiling-dxr-shaders-with-timer-instrumentation/\\nIt is showing timing capabilities per pixel in DXR using Nvidias NVAPI.My question: is this available in general CUDA code too and in Optix especially? This would be a nice debugging/profiling tool. If so, how can it be done? As far as I understood the NVAPI documentation the functions described in the post exist in DXR extensions only so not readily available in CUDA/Optix.Thanks and kind regardsYes, you can simply use the CUDA clock() instruction for that and scale it accordingly.Please find more information and links to example code in this post where I mentioned that blog post as well:\\nhttps://forums.developer.nvidia.com/t/optix-6-5-wireframe-rendering/119619/10Actually the OptiX SDKs before 7.0.0 also had that feature inside the pinhole_camera.cu example. Search for TIME_VIEW.clock()Thank you! I assumed there is something similarly easy but could not find it directly.Powered by Discourse, best viewed with JavaScript enabled',\n",
       " 'mosaic-doesnt-work-properly-with-multiple-touch-displays': 'Dear all,\\nI currently have 2 Philips touch screen displays (1920x1080) connected to a NVIDIA NVS 310 graphics card installed on a Windows 7 (32 bit).\\nWhen I extend the desktop (Win 7 properties) to these monitors, the touch screen inputs working correctly (i.e when I touch the middle screen the touch is there, left is left etc. etc).\\nIf I use NVIDIA mosaic mode and connect the screens into a unique display, the touch screens do not work as expected. Since Windows sees the computer as one widescreen mosaic (3840 x 1080) but for the touch the resolution is only 1920 X 1080, as if the 3840 x 1080 screen has been compressed into a single display of 1920 x 1080. So when I touch the left screen on its left edge it will detect it there, if I touch the right screen on its right edge it will detect it there. But if I drag my finger from one extreme to the other, the pointer has a twice speed that of the finger, and when I’m in the middle of a monitor I’m in the middle of the mosaic. In the follow link you can see a video with the problem:\\nhttps://drive.google.com/file/d/0B80jt2zR1YewS0VpZ0xaejEzeEU/view?usp=sharingThis is my NVIDIA System Information:\\nNVIDIA System Information report created on: 11/28/2014 09:14:08\\nSystem name: SC-00[Display]\\nOperating System:\\tWindows 7 Professional, 32-bit (Service Pack 1)\\nDirectX version:\\t11.0\\nGPU processor:\\t\\tNVS 310\\nDriver version:\\t\\t310.90\\nDirectX support:\\t11.1\\nCUDA Cores:\\t\\t48\\nCore clock:\\t\\t523 MHz\\nShader clock:\\t\\t1046 MHz\\nMemory data rate:\\t1750 MHz\\nMemory interface:\\t64-bit\\nMemory bandwidth:\\t14.00 GB/s\\nTotal available graphics memory:\\t1865 MB\\nDedicated video memory:\\t512 MB DDR3\\nSystem video memory:\\t0 MB\\nShared system memory:\\t1353 MB\\nVideo BIOS version:\\t75.19.57.00.06\\nIRQ:\\t\\t\\t0\\nBus:\\t\\t\\tPCI Express x4 Gen2[Components]nvui.dll\\t\\t8.17.13.1090\\t\\tNVIDIA User Experience Driver Component\\nnvxdsync.exe\\t\\t8.17.13.1090\\t\\tNVIDIA User Experience Driver Component\\nnvxdplcy.dll\\t\\t8.17.13.1090\\t\\tNVIDIA User Experience Driver Component\\nnvxdbat.dll\\t\\t8.17.13.1090\\t\\tNVIDIA User Experience Driver Component\\nnvxdapix.dll\\t\\t8.17.13.1090\\t\\tNVIDIA User Experience Driver Component\\nNVCPL.DLL\\t\\t8.17.13.1090\\t\\tNVIDIA User Experience Driver Component\\nnvCplUIR.dll\\t\\t6.9.850.0\\t\\tNVIDIA Control Panel\\nnvCplUI.exe\\t\\t6.9.850.0\\t\\tNVIDIA Control Panel\\nnvWSSR.dll\\t\\t6.14.13.1090\\t\\tNVIDIA Workstation Server\\nnvWSS.dll\\t\\t6.14.13.1090\\t\\tNVIDIA Workstation Server\\nnvViTvSR.dll\\t\\t6.14.13.1090\\t\\tNVIDIA Video Server\\nnvViTvS.dll\\t\\t6.14.13.1090\\t\\tNVIDIA Video Server\\nnvDispSR.dll\\t\\t6.14.13.1090\\t\\tNVIDIA Display Server\\nNVMCTRAY.DLL\\t\\t8.17.13.1090\\t\\tNVIDIA Media Center Library\\nnvDispS.dll\\t\\t6.14.13.1090\\t\\tNVIDIA Display Server\\nNVCUDA.DLL\\t\\t8.17.13.1090\\t\\tNVIDIA CUDA 5.0.1 driver\\nnvGameSR.dll\\t\\t6.14.13.1090\\t\\tNVIDIA 3D Settings Server\\nnvGameS.dll\\t\\t6.14.13.1090\\t\\tNVIDIA 3D Settings ServerI have the same problem,One display is a touch screen and the other is not. But when I touch the screen the touch mapping goes across the full resolution (3840x1080) as if my touch screen is both displays. so for example: If I am touching pixel 1000x500… the courser is actually touching pixel 2000x500.is there a way to map the touch only to the resolution of the touch screen (1920x1080)?Hey guys I have the same problem presented on the video… Have you ever solved it?Hello,Unfortunately, there is no way of combining multiple touch screens using the standard Windows touch USB driver. Windows assume a single touch interface.It is recommended that you use a touch overlay that covers all x monitors – rather than an overlay per display.  I recommend you speak to the company supplying the overlays to discuss how they operate in a multi-display environment.\\nFor example, just relying on the MS touch driver will not work for this scenario.Powered by Discourse, best viewed with JavaScript enabled',\n",
       " 'possible-to-disable-suspension-on-physx-vehicle-physx-3-3': '(PhysX 3.3)Hi! I’ve been trying to create some stiff wheels, but don’t seem to find anyway to disable the suspension. If I lower the suspension compression and droop very low, the constraint start to generate wierd forces forward(local Z-axis).Everything works fine if I increase the compression and droop, but the handling feels wrong since the wheels are supposed to be stiff.Any ideas? Thanks in advance!I confused with what you said about wheels being stiff… The suspension is what supports the weight of the car, and not the wheels from a physics perspective, the wheels are just the link between the drive surface and the suspension. What do you mean by the handling feels wrong ? I know what you mean in general that it doesn’t feel right, but to be more specific what makes it not feel right…to floaty ? too wobbly ?Exactly, it feels to wobbly and floaty. The thing is a lot of modern vehicles doesn’t have a spring suspension, their drive axle is directly attached to the chassi. (Example: A wheelbarrow)So the problem is that I want the drivetrain, but no the spring suspension. But if I lower the compression and droop of the spring to about 1cm(0.01m) I get wierd behaviours, like acceleration without any force applied.Well ofcourse there is force applied but the acceleration comes from the constraint instead of the vehicle update.I’m not sure that physx vehicles would be the best way to model a wheelbarrow.  The model is pretty much a box on springs with the spring forces used as input to the tire force calculation.  The model is based on the assumption that the spring force dominates over the force generated from the springiness of the tire.  A further assumption is that the tire has a stiffness at least an order of magnitude greater than the suspension spring.  This allows us to ignore the tire because it will be almost static on the timescales of the spring.  These assumptions are fairly standard in vehicle modelling.The obvious thing to try is to shorten the suspension travel (maxDroop and maxCompression) and then add a stiff spring.  The difficulty is that a stiff spring needs short timesteps, especially when the suspension travel is short.  I can’t see this being an efficient or faithful solution.One thing to think about is that a wheelbarrow typically has a soft tire and you might consider a soft spring as a good approximation of a soft tire. If you really want a stiff tire and are trying to simulate a vehicle more complex than a wheelbarrow then doing this with rigid bodies sounds more sensible because there isn’t really any springiness left in the system.Cheers,GordonPowered by Discourse, best viewed with JavaScript enabled',\n",
       " 'tadp-2-0r8-not-able-to-get-the-setup-done': 'i have got the tadp pack with unreal engine ,but its behaving abnormally and i am not able to finish the setup. Please any one from the developer team help me out to get this done. issue  says 106296 kbytes to download and the 342248 already installed still not able to finish the setup .tadp-2.0r8-windows.exe got this with setup of unreal engine 4 and is needed to deploy the game developed in same on android devices.Powered by Discourse, best viewed with JavaScript enabled',\n",
       " 'nppigraphcut-wierd-issue': 'Hi all,I’m using nppiGraphcut_32f8u to compute my graph. In my main, I have a for loop like this that calls an extern void “C” function that does the CUDA/NPP computation like this:where data_buffer is the volume (bigger) array I’m extracting from to ‘temp’.Then in my twoDseg() void function, I have the nppiGraphCut call as follows:which returns GC error = 0 the first iteration of the main for loop, but for the 2nd frame or 2nd iteration, the GC error = -3.The strangest thing is, this works when nppiGraphCut is called twice like this:which returns GC error 1= -3, GC error 2=0. So it means it worked the 2nd time…Can someone help explain or show me why calling nppiGraphCut twice would result in different success?Powered by Discourse, best viewed with JavaScript enabled',\n",
       " 'dlss-unreal-engine-4-27-ghosting-trailing-behind-moving-objects': 'We have integrated the latest DLSS plugin into our Unreal Engine 4.27 project to make use of the improvements to Anti-Aliasing and Performance. However during this integration, we have encountered an issue with moving objects leaving behind ghosts/trails.\\nThe following settings have been validated:Using the Debug overlay, we noticed that we had yellow colored interior offsets when looking at the jitter offset debug. We realized that the default r.TemporalAASamples 8 was not sufficient, so we changed it to r.TemporalAASamples 16. This did not seem to have an effect on the ghosting/trailing we see.We have tried different combinations of disabling DilateMotionVectors, AutoExposure, BasePassForceOutputsVelocity.We have also downloaded and built the NvRTX 4.27.2-4 version of the Unreal Engine from the NvRTX/UnrealEngine Github. We have had no improvement to the ghosting/trailing issue with the custom engine or any of the other settings that were suggested by the documentation provided with the plugin.Is this a known issue of DLSS for Unreal Engine. Does anyone have any suggestions or workarounds that might work to alleviate the issue?Thank you,\\nTravisHello @tclements1 and welcome to the NVIDIA developer forums!It is really unfortunate that you are seeing ghosting in your project with DLSS. To get you started and before the experts chime in, did you see the FAQ for Unreal and DLSS, in particular the question about ghosting? While it is too much to hope that this fixes your problems, it still is worth a shot.Can you please also find out which version the DLSS DLL has that you installed as part of the plugin?Thanks!Hello @MarkusHoHo ,Yes, I have tried the suggestion that was in the FAQ about using r.BasePassForceOutputsVelocity, this did not solve the issue.The version of the DLSS DLL is 2.3.11.0, this DLL comes from the latest plugin for UE4.27 provided from the DLSS Nivida Download page, Get Started | NVIDIA DeveloperThank you,\\nTravis@MarkusHoHo,I wanted to add that I sent an email that contains the same information, plus a few videos to nvrtx-support@nvidia as recommended from Epic Employees on Unreal Developer Network, and DLSS-Support@nvidia.  These emails were sent around November 1st, 2022, when neither of the emails got any replies, I decided to create this forum post.Has there been any movement on one of the experts chiming in?  This is the primary issue we are experiencing when using DLSS, and is what is keeping us from moving forward with the DLSS implementation.Thank you,\\nTravisHi again,Our GTC Fall last week has kept people really busy, so it might take a while for an answer. But now this should pick up again and I will also reach out to engineering again.Thanks for your patience!Setting r.NGX.DLSS.DilateMotionVectors to 0 fixed the ghosting for me.Unfortunately that did not work for me. Ghosting still occurs especially after an actor has stood still for a period of timeThank you for the suggestion, but it did not fix the issue. Before originally posting on the forum, I had tried different combinations of enabling/disabling the following commands with no improvement to the ghosting/trailing.We are having a very similar problem.  We use the latest DLSS plugin with UE5.0.3 and have severe ghosting.  We tried many things without success.  We emailed dlss-support@nvidia.com too, but got no reply.We reproduce this ghosting problem with the simplest scene.  I suspect this is a well known problem at nVidia.  A status from the dev team would be appreciated.Regards,JS\\nimage990×540 30 KB\\nThis issue is mentioned in a test of DLSS by ComputerBase.de for the game Atomic Heart:\\nAtomic Heart im Technik-Test: Nvidia DLSS und etwas AMD FSR im Detail - ComputerBaseDLSS 3.1.1 DLL version by default chooses “preset D” for DLSS Quality, Balanced, and Performance modes for most games, and apparently, this preset still has issues with object ghosting in quite a number of games. It often seems to occur when e.g. NPCs start moving again after standing still for a while. “Preset C” usually doesn’t show this issue.Really hope “preset D” can be made less prone to this issue with future DLSS updates.\\nAnd on a different note, reduced “smearing” for thin object edges (e.g. flying birds) would also be nice.Sorry that it has been rather silent in this thread. There were several sets of optimizations to DLSS over the last couple of months, addressing apart from other things also Ghosting in different scenarios. But it seems to be in the nature of the problem that it is hard to get 100% coverage for all possible combinations of content.Development continues and you can expect improvements in the future. And any and all feedback on quality issues are welcome and will be passed on to the development team.Thanks!Has there been an update on this? I am having pretty bad ghosting/trailing as well.Powered by Discourse, best viewed with JavaScript enabled',\n",
       " 'error-malformed-or-missing-stop-data-with-size-0': 'I deployed the cloudXR service in the cloudError code obtained when accessing locally\\n[14:04:32.414] ERROR: Malformed or missing stop data with size: 0.\\n[14:04:32.415] Streamer state stopped. (0: 0), reason: 0x800E840DThe error code I get when I use my local machine to access the cloudXR service deployed on the cloud\\n[14:47:35.369] ERROR: Malformed or missing stop data with size: 0.\\n[14:47:35.369] Streamer state stopped. (0: 0), reason: 0x800E840D\\n[14:47:47.158] Input stream disconnected.\\n[14:47:47.158] Streamer state error. (1: 0), reason: 0x80085190\\n[14:47:47.158] Connected but streaming failed.CloudXR Client Log 2022-06-13 14.04.31.txt (1.4 KB)\\nCloudXR Client Log 2022-06-13 14.47.34.txt (1.3 KB)\\nStreamer Client Log 2022-06-13 14.40.29.txt (183.2 KB)\\nStreamer Client Log 2022-06-13 14.47.34.txt (201.8 KB)steamVR is deployed with cloudXR, and steamVR cannot be called\\n\\nimage355×561 82 KB\\n\\nimage1202×670 15.2 KB\\nPowered by Discourse, best viewed with JavaScript enabled',\n",
       " 'amd-kicking-nvidias-ass-in-opengl': 'Get your attention? hehe… I thought it might.I’m a huge fan of Janus VR and the problem currently is that we can get much better framerates with an AMD card than we can in any Nvidia cards. For comparison, I can get Janus VR to run at around 200 FPS with a Radeon HD 7770 but with my Nvidia GTX 980 I’m getting around 60 max (yes vsync off).The core of the problem seems to be the OpenGL calls that Janus VR is making. So the question is:Is there a list of known OpenGL calls that are considerable slower on Nvidia architecture than AMD so that we can try to optimize and get users with Nvidia cards better performance?As the last post in the linked thread is suggesting, try getting away from legacy APIs like immediate mode and display lists which are not available in newer OpenGL core versions anymore and port to VBOs.Immediate mode and display lists are highly optimized for workstation applications because they still use a much wider range of OpenGL API entrypints. The consumer drivers are tuned for game like content which are normally using available modern APIs earlier due to much shorter longevity of the software.Shameless plug: If you need an OpenGL renderer backend which really shines with respect to triangle throughput and parameter update performance while using modern OpenGL features you might want to have a look at the “RiX” module in our open source nvpro-pipeline.\\nMore details here: [url]https://devtalk.nvidia.com/default/topic/777618/scenix/announcing-nvpro-pipeline-a-research-rendering-pipeline/[/url]Thank you so much for the reply Detlef! Great info and I’ll replay that.Powered by Discourse, best viewed with JavaScript enabled',\n",
       " 'optix-7-6-download-missing': 'Hi, I noticed that Optix 7.6 is no longer available for download, only 7.7 and 7.5 and older. Is this intended?We updated to Optix 7.7, but ran into the issue that there is no suitable driver for some GPUs (e.g. Tesla V100). So we are thinking about going back to 7.6 for now. Can you tell me the required driver version for Optix 7.6? Also, will there be a 530+ driver for the Tesla V100 in the near future?Thanks.Right, sorry, that the OptiX 7.6.0 download buttons are missing from the older versions’ page is unintentional and is going to be updated.The direct download link actually still works. If you’re logged-in to the NVIDIA developer site, go to the OptiX Legacy Download page with the older versions, then right-click on the OptiX 7.5.0 Windows or Linux download button, copy-paste that link into your browser’s address bar, then change 7.5.0 to 7.6.0, and it’ll download the package you’re looking for.I have OptiX 7.6.0 running on a system with 526.67 drivers, so that means the current drivers from the R525 branch should work. OptiX 7.5.0 needed R515 drivers.The drivers for data center products are on a different release cadence than normal desktop drivers.  The upcoming R535 drivers should support that. I don’t know when exactly that’ll be released, but probably in June. Keep checking the driver download site.This topic was automatically closed 14 days after the last reply. New replies are no longer allowed.Powered by Discourse, best viewed with JavaScript enabled',\n",
       " 'does-the-loadmesh-function-costs-so-much-time': 'Dear Developer:\\nI am dealing with a question about improving the rendering scene’s rate. Afer testing, i find it that My algorithm costs a little time. However, the loadmesh() function of the OptiXMesh class costs so much time. At the same time, the 3D Obj file is very large, too.\\nAbove this question leads to a slow rate of imaging. Do you have a any good idea about this situation? Thanks!The OBJ file format and its accompanying MTL material file format are ASCII only.\\nLoading and parsing the text alone and transferring it into an OptiX scene representation needs time.If you need to load huge scene files it would be more efficient to use some other format with a binary representation on disk (and possibly more suitable scene structure) inside your application.Note that the simple examples inside the OptiX SDK are meant to show how individual features work. That it contains some scene loader is just to have more interesting geometry in a few samples, but none of the optixu or sutil utility code is really required inside an own OptiX application framework.Powered by Discourse, best viewed with JavaScript enabled',\n",
       " 'black-screen-with-optix-6-0-and-quadro-rtx-3000': 'Hi\\nI’m using Optix in Version 6.0\\nmy optix application renders well on Geforce GT and RTX cards.\\nBut on a Quadro RTX 3000 with Driver Version 425.45 i have only a black screen. There are no optix error messages.\\nwhat can i do? install other driver version? compile on an other optix version? or what can cause a black screen?Try installing the newest available driver 441.28 for the RTX 3000 board:\\nhttps://www.nvidia.com/Download/Find.aspx?lang=en-usThat supports OptiX 6.5.0 as well which would be the recommended version when staying on OptiX 6.\\nLong term you should consider porting to OptiX 7, but that uses a completely different host API.Powered by Discourse, best viewed with JavaScript enabled',\n",
       " 'is-there-any-way-to-remove-the-restriction-that-consumer-grade-geforce-cards-are-restricted-to-only': 'Just as title says, can anyone help?\\nMany thanks.Powered by Discourse, best viewed with JavaScript enabled',\n",
       " 'join-the-pimax-developer-ecosystem-on-our-roadshow': 'Dear developers,Pimax is a VR brand which shares the best VR headset with everyone who wants to live in a digital oasis. After announced our next generation ‘ALL-IN-ONE’ device — Pimax Portal. We will begin a roadshow in 20+ sites worldwide.This time, we’re going to provide all the developers the following benefits:Please click the link：Pimax Roadshow Events - Signup and participate as a developer to complete the form if you’re interested in being part of our ecosystem. Hope to see you there!Pimax Team.\\n\\n企业微信截图_16687634884492(4)1280×720 101 KB\\nDear Developers, We are excited to announce Pimax Content Incentive Program for Indie Developers. We have found gifted developers can be found working for small studios or are even developing independently. We have been amazed at the skill and creativity of indie developers who have already contributed to the Pimax store platform, and we want to create the best opportunities possible for you all to succeed. Check the announcement for detailed information: Pimax Developer Registration - Google 表单Powered by Discourse, best viewed with JavaScript enabled',\n",
       " 'flex-platforms-availability': 'Hello,I was wondering if the forthcoming Flex release would be available for Xbox One and or PS4?Hi,We are investigating that, but we have no plans to announce support for those platforms at this time.Hi, I’m just wanting to ask if flex is yet available for Xbox One and PS4?Powered by Discourse, best viewed with JavaScript enabled',\n",
       " 'question-about-nvencencodepicture-function': 'The encoders in the sample project eventually encode them by calling the nvEncEncodePicture function.This function encodes each frame.Is there a function that encodes multiple frames at once?I looked up api reference, but it did not seem to exist.I would appreciate your help.Powered by Discourse, best viewed with JavaScript enabled',\n",
       " 'view-shift-black-screen': 'When we turn or move our head, the view doesn’t move fast enough,\\nit shows a black screen in that area, then loads after 0.5 or 1 second.\\nSame problem on Local network as well as Cloud (AWS)\\nWe are using OCULUS Quest 2 as client.This image may explain the problem more clearly\\n\\nQuest-2-View-Shift-Problem1935×1943 161 KB\\nThis generally only really happens in bad network conditions with a tight FOV and extreme prediction timing.  If frames are coming consitently, we have good prediction, and decent FOV, generally timewarp on device will catch us up to edges but not reach into the ‘black’ area.So, how do we optimize this ?\\nDo we need to make any changes on Hardware ? Software ? Network ? App Design ?Probably best to start a conversation with your primary field engineering contact (which may eventually come back to core engineering), as these things always turn out to be specific to something about the overall combination.  Trying to give a generic answer without specific details about all the above topics just isn’t going to help any.Thanks for your prompt answer, we were hoping to be something that you guys could help with, but given that we are in pre-production phase, we will have to discard this option for now and look at optimizing once the show is finished.Powered by Discourse, best viewed with JavaScript enabled',\n",
       " 'nvidia-drivers-not-working-in-visual-studio-2013-ultimate': 'Hello!I have a notebook with a GeForce video card 630M model, and I’m using the drivers in the latest version (353.30). The problem is that the nvidia drivers are not working with Microsoft Visual Studio (2013 version - Ultimate), and programs are being renderized by the integrated graphics.\\nEven the control panel can not solve the problem.Here as a Screen Shoot of the problem:\\n[url]http://imageshack.com/a/img540/3678/CF7NcX.png[/url]Anybody know some solution?Powered by Discourse, best viewed with JavaScript enabled',\n",
       " 'oculus-quest-2-with-link-registers-vive-controllers': 'On both CloudXR 2.1 and 3.0, my Oculus Quest 2 under link registers correctly in SteamVR on the client computer, but shows up as Vive controllers (and is therefore unusable due to mapping issues) on the Server computer’s SteamVR.While I saw the answer from here:\\nI also see that the Oculus Touch bindings are included by default in the CloudXR Client directory, as well as in the RemoteXRHMD directory (or whatever) of the Server install.Why are the Oculus Touch controller bindings included, but not detected?Powered by Discourse, best viewed with JavaScript enabled',\n",
       " 'ray-contact-points-for-objects': 'Hello.\\nCurrently, we are looking for a way to get all the points that a ray can touch when penetrating, without any reflection or refraction of one object.\\nI’m sorry for the rudimentary question, what kind of method is there?Are you saying you want to gather all intersections along a straight ray?Please follow the links in my answer to this thread for possible implementation methods and caveats to consider:\\n[url]distinguish objects when closest hit program occured - OptiX - NVIDIA Developer ForumsThank you for your reply. I will read the thread.\\nThat’s right about the question.\\nFor example, suppose you have a sphere with a center (1,1,1) radius of 0.5 and a ray fired from (0,0,0) to (1,1,1). I want to get two intersections where the line meets the sphere.Your sphere example is not quite what I explained. I thought you wanted to capture all intersections along a ray inside a full scene where all intersection programs exist already.Your sphere example would be a custom geometric primitive intersection routine. It’s your responsibility to calculate the primitive intersection points inside an intersection program for any custom geometric primitive, like parametric spheres. Only triangles have built-in intersection routines in OptiX 6 and 7.The OptiX SDK contains example code for a ray-sphere intersection routine. Search the SDK sources for *.cu files with “sphere” in the name and you’ll find that intersection calculation.I misread your question because my English reading ability was insufficient.I saw a sample of .\\nIn this sample, is it correct that the registered process (intersect this time) is called when the light beam emitted by <pinhole_camera> hits an object?\\nAlso, when it is called, is it correct that the ray information that was struck is passed in the ray of rtDeclareVariable (optix :: Ray, ray, rtCurrentRay,);I ’m sorry to ask you some basic questions.In this sample, is it correct that the registered process (intersect this time) is called when the light beam emitted by <pinhole_camera> hits an object?Yes, that intersection program is called whenever a ray hits a bounding box for a geometry with that custom intersection program assigned.\\nAlso see this thread:\\nhttps://devtalk.nvidia.com/default/topic/1050973/optix/when-the-rtpotentialintersection-t-return-true-is-t-the-any-hit-or-must-closest-hit/post/5334456\\nhttps://raytracing-docs.nvidia.com/optix6/guide_6_0/index.html#programs#intersectionAlso, when it is called, is it correct that the ray information that was struck is passed in the ray of rtDeclareVariable (optix :: Ray, ray, rtCurrentRay,);Yes, the semantic variable of type rtCurrentRay is accessible inside the intersection program.\\nNote that the coordinate space in that intersection program domain is object space, also in the anyhit program, but it’s in world space inside the raygeneration and closest hit program domain. See Table 5 and 7 here:\\nhttps://raytracing-docs.nvidia.com/optix6/guide_6_0/index.html#programs#internally-provided-semanticsIf you’re new to OptiX, I would recommend to watch my GTC 2018 OptiX Introduction presentation.\\nLinks to that and the examples’ source code here:\\nhttps://devtalk.nvidia.com/default/topic/998546/optix/optix-advanced-samples-on-github/\\nGTC 2018 S8518 - An Introduction to NVIDIA OptiX http://on-demand-gtc.gputechconf.com/gtc-quicklink/4JAjApThen watch the GTC 2019 presentation from David Hart about new features in OptiX 6.0.0.\\nGTC 2019 S9768 - New Features in OptiX 6.0 http://on-demand-gtc.gputechconf.com/gtc-quicklink/4FB3lIf you’re starting a new development, there is also the newest OptiX 7 API.\\nThe device side programs behave mostly the same, but the host side API is very different:\\nhttps://devtalk.nvidia.com/default/topic/1058310/optix/optix-7-0-release/\\nhttps://devtalk.nvidia.com/default/topic/1058577/optix/porting-to-optix-7/post/5368038The landing page for all NVIDIA raytracing docs is here: https://raytracing-docs.nvidia.comThank you for your polite explanation. I was saved.\\nI will study for a while.\\nI might ask again.Powered by Discourse, best viewed with JavaScript enabled',\n",
       " 'physx-3-3-1-loading-obj-and-creating-a-dynamic-actor': 'Hi Guys!We are currently trying to simulate a waterwheel. Particles are running down a feed and fall into the chambers of the wheel, which causes the wheel to rotate.The problem is we are stuck at rotating the wheel. We use the wavefront.cpp from the samples to load the wheel from an obj file. I then tried to use cooking to create a PxTriangleMesh and use this to create a kinematic PxRigidDynamic according to https://developer.nvidia.com/sites/default/files/akamai/physx/Manual/Shapes.html#kinematic-triangle-meshes-planes-heighfieldsThe cooking seems to work, i also get a correct collision with the particles. What i can’t figure out is how to rotate the actor using addForce/addTorque/angularVelocity. The only movement i got was by changing the global pose of the dynamic actor.Is this even the correct approach for my problem? Is a kinematic actor with a triangle mesh enough for the wheel rotation or do i need to work with convexMeshes? (already dismantled the wheel for the convex shapes to work, but still can’t get a rotation to work)I looked through the User’s Guide, the Forum and the Samples, still can’t get the hang of this.Regards,\\nAsterIf you have set up the waterwheel as a kinematic then it can only be rotated/translated with the function PxRigidDynamic::setKinematicTarget.  The alternative is to lower the kinematic flag on the actor and set an angular velocity or torque.  The combination of kinematic flag and angular velocity, however, will definitely not work.For performance reasons, some limitations are placed on triangle meshes and heightfields that are attached to dymamic actors.  Such geometry does participate in scene queries but does not participate in rigid body or particle contact.  I’d recommend using multiple convex shapes to represent the waterwheel geometry.Thanks,GordonPowered by Discourse, best viewed with JavaScript enabled',\n",
       " 'bug-report-nvidia-driver-390-installed-but-not-detected-by-ubuntu': 'i have a bug on using nvidia-390 driver on ubuntu 18.04 on my laptop hp omen ek0019tx with with gtx 1650ti, please solve the bugnvidia-bug-report.log.gz (249.3 KB)Powered by Discourse, best viewed with JavaScript enabled',\n",
       " 'quadro-sync-external-reference': 'Hi,\\nI’m new to this ‘devtalk’ site, so I’m not quite sure if I’m correct here.\\nFor a special purpose I need to synchronize a Quadro M6000 or K5200 connected\\nto a PNY Quadro Sync from an external source.\\nEverything seems to work fine, but unfortunately the Stereo output signal on\\nthe 3-pin Mini-DIN connector on the Quadro graphics board is sometimes in phase,\\nbut sometimes 180’ shifted related to the input signal, after enabling external\\nsync.\\nThe external signal is a 50Hz rectangular signal (3.3V / 50% duty cycle) provided\\non the BNC connector. Everything seems to be OK - as the indicators on the\\nFrameLock section of the ‘nvidia-settings’ tool (I’m using Ubuntu 18.04 LTS)\\ngo green every time.\\nIs this a known issue?\\nIs there a know workaround for getting the same phase of the above mentioned\\nsignals every time?\\nIs the RJ45 connector an option for getting rid of the above mentioned problem?\\nI found a pinout of the RJ45 connector in a document named ‘BD-06576-001_v01’.\\nUnfortunately the explanations are not very sufficient. Not every signal is\\nexplained in detail.\\nBecause I don’t want to send the Quadro Sync board to the ‘happy hunting ground’\\n(if you know what I mean…), I need some more information about that (when to\\nprovide what signals on which pins and when to enable / disable what feature\\nbefore providing the signal(s)).\\nA request to PNY support offered me to forward the inquiry to this forum. I hope\\nI will get some more information here.\\nIn case you need more information, please let me know.Thanks in advanceGeorg GlockPowered by Discourse, best viewed with JavaScript enabled',\n",
       " 'transcoding-using-v100-and-intel-scalable-cpus-with-qsv': 'Hi, I’m not a developer, but run a company dedicated to digital video. I have several questions which I would love to hear opinions on;The new V100 is not listed in the NVENC and NVDEC Support Matrices. Has anyone tried HEVC 1080p encoding on a V100?We need to transcode several hundred channels from MPEG2 1080i to HEVC 1080p or to 720p with 3 representations for each. Has anybody quantified how many HEVC 1080p or 720p encodes can be accomplished per NVENC instance? and how many instances can be run on a V100? I understand from the NVENC matrix that the P100 supports 3 instances, and from the Encode Streaming Performance graph at https://developer.nvidia.com/nvidia-video-codec-sdk#NVENCPerf that a single instance of NVENC could encode 13 HEVC 4K streams on Pascal GPUs. If that is for 4K, would it be correct to interpolate that at 1080p we should be able to achieve 4x13 or 52 streams per NVENC?and following the above, would it be correct to assume that on a TESLA M60 we can run NVENC 4x while on a P100 the limit is 3X?How many instances of NVENC may be run on a V100?Thank you all for your kind replies.Hi Mmorales,I am not part of NVIDIA team, following is based just on my experience.NVENC is hardware block present on GPU and paired with NVDEC block. This is newer technology than CUDA-based encoder. So number of CUDA cores is not a factor in encoding performance. This is awesome because transcoding solutions can use low-end quadro card and not pay for unused cuda cores :)$9428.00 for Tesla P100 SXM2 16GB\\n$ 110.49 for NVIDIA Quadro P400 2 GBI would advise you to use Quadro P400 for transcoding and use 4 cards in one motherboard.GPU is ideal for your task as you can avoid I/O bottleneck in copying raw frames from system to GPU memory and back.MPEG2 > GPU memory > NVDEC > DEINTERLACE > NVENC > system memory > storage/streaming serverNowadays NVENC SDK comes with FFMPEG integration so you are free to do all kinds of benchmarks with single command-line. You can also test expensive cards at nvidia test lab :)Only geforce cards have software limit to 2 NVENC sessions.Kind regards,\\nAlexThank you Alex.I need to transcend several hundred live TV channels. Do you have any idea how many MPEG2 to HEVC 1080p or 720p transcodes can be achieved with one NVENC instance on one of those cards? I also need to run several output representations on each channel for MPEG-DASH.Meant to say transcode, not transcend which was done by my spellchecker. :-)https://developer.nvidia.com/nvenc-application-notehttps://developer.nvidia.com/nvdec-application-noteYou are interested in “Pascal” table column.For H.264 1920x1080 @388fps == 15 streams @25fps\\nFor H.265 1920x1080 @259fps == 10 streams @25fpsThe encoding values should be doubled if 2 encoder exists (see NVENC/chip https://developer.nvidia.com/video-encode-decode-gpu-support-matrix and should be the same for GTX cards with equivalent chips) and all values should be lowered if the chip is intentionally underclocked (20-30% to lower TDP).Powered by Discourse, best viewed with JavaScript enabled',\n",
       " 'nv-command-list-fbo-causes-access-violation': 'Hi,I’m optimizing my app with NV_command_List(win7, 382.05, GeForceGTX670).\\nBut NV_command_list with FBO change causes access violation at glBindFramebuffer after glDrawCommandsStatesNV.\\nIt seems to depend on last fbo in NV_command_list or last bound fbo before glDrawCommandsStatesNV.I modified gl_commandlist_basic sample to repro this issue.\\nThis program makes 2 fbos with same color+depth textures.\\nRegular gl context uses fbo0 and glDrawCommandsStatesNV uses fbo1.Please run this program, then choose draw mode to nvcmdlist buffer.\\nIt will cause access violation at glBindFramebuffer.\\nThe output is\\n“Exception thrown at 0x00000000691FA94F (nvoglv64.dll) in gl_commandlist_basic.exe: 0xC0000005: Access violation reading location 0x0000000000000B00.”I think there are two workarounds.\\n1) binding 0 fbo before glDrawCommandsStatesNV\\n2) enqueue nop with fbo bound just before glDrawCommandsStatesNV as last token\\nYou can test two workarounds if you change WORK_AROUND_TYPE macro in gl_commandlist_basic.cpp header part.Is this behavior intended?\\nAnd I’m sorry if I was wrong.basic-nvcommandlist.zip (9.23 KB)Powered by Discourse, best viewed with JavaScript enabled',\n",
       " 'big-shader-compiling-after-installing-ominverse-connector-for-unreal': 'Hi! I’mUnreal engin user.\\ni installed Omniverse and Unreal connector for Audio to Face yesterday\\ntoday i created new project folders and suddenly more than one thousand Shader compiling message(preparing shaders) is pop up on unreal and it takes so many time(about 1hour)\\nI think it’s about MDL Maaterial compiling\\nis this normal having this long compling time every new project files?and is this necessary? can i avoid it?Hello there @dhsong and welcome to the NVIDIA developer forums!From my own experience I can tell you that whenever I start a new project based on a template I have not used before, Unreal will start compiling a lot of shaders. Just yesterday I started an FPS project from a fresh UE5 install and UE started compiling more than 6500 shaders. It took a while to finish…But usually this only happens once, not every time you open your project or create a new one based on the same template. When you are in your UE project, you might still see “Preparing Shaders” as a message, which will still happen in the background, but that should be a one-time only occurrence per project as well.What might reduce the amount of shaders compiled on project creation is to chose “Scalable” instead of “Maximum” in the initial Quality settings of your template. But I did not try that myself, so your mileage may vary.For more discussions on the topic you are also welcome to join our forum category specifically for the Unreal Engine connector for Omniverse or discuss on the Omniverse Discord server.I hope this helps!This topic was automatically closed 14 days after the last reply. New replies are no longer allowed.Powered by Discourse, best viewed with JavaScript enabled',\n",
       " 'nv-copy-image-for-directx': 'Hi,I was wondering if there is a DX equivalent of OGL’s NV_Copy_Image extension? If not, what is the fastest way to duplicate data across multiple GPU’s?Thanks!Powered by Discourse, best viewed with JavaScript enabled',\n",
       " 'program-dont-work-on-32-bit-windows-8': 'Program worked well on Windows 7 64 bitBut on Windows 8 32 bit it throws Exception at memory location on context->compileP.S. ofcourse, all dll have been replacedI didn’t know that ptx files must be regenerated.Now it works well. Thank you very muchPowered by Discourse, best viewed with JavaScript enabled',\n",
       " 'nvidia-settings-error-nvidia-driver-is-not-loaded-rtx-quadro-4000': 'Hello, I have some issues trying to connect my second monitor.  First of all I had my two monitors connected to my RTX but I havent got the drivers installed, so after install 455 nvidia driver, my second monitor dont show and Nvidia settings didnt work neither 455,450,440 nvidia-versions.Thanks.Powered by Discourse, best viewed with JavaScript enabled',\n",
       " 'cuvidgetdecodercaps-ctx-caps8-failed': 'Hey:\\nWhen I use ffmpeg in rtx3070 hardware decode，I found this error，please help solve it，thx。\\ncuvidGetDecoderCaps(&ctx->caps8) failed\\ncuvidGetDecoderCaps(&ctx->caps10) failed\\ncuvidGetDecoderCaps(&ctx->caps12) failedRTX3070\\ncuda11.2\\ndrvier: 460.39\\nVideo_Codec_Sdk : 11.0Hi,\\nPlease provide exact details to help reproduce this issue like ffmpeg command used, input files, ffmpeg version etc. You can also enable FFmpeg verbose logs to get better idea of this failure.\\nCan you attach nvidia-bug-report as well?Thanks.Powered by Discourse, best viewed with JavaScript enabled',\n",
       " 'debugging-jit-compiled-code-line-by-line': 'I am trying to debug some JIT compiled code that is used by my Optix 7  _raygen__rg() method but the stack trace is not yielding any information finer than the name of the called method.  For example:CUDA Exception: Warp Misaligned Address\\nThe exception was triggered at PC 0x17bf99d0Thread 1 “blort” received signal CUDA_EXCEPTION_6, Warp Misaligned Address.\\n[Switching focus to CUDA kernel 0, grid 48, block (135,0,0), thread (32,0,0), device 0, sm 0, warp 4, lane 0]\\n0x0000000017bf99e0 in ProcessResult ()\\n(cuda-gdb) l\\n1\\tOPTIX/generated/internal: No such file or directory.I provided these options to optixModuleCreateFromPTX():Yet cuda-gdb seems either unaware of the ptx source or unable to find it.  Any ideas?  Thanks.Full symbolic debugging of OptiX programs in cuda-gdb isn’t supported yet, we are working on that. SASS level debugging works, but you have to figure out the correspondence to your source code manually. That can be pretty difficult, I don’t expect that to be a helpful suggestion or viable workaround unless you are extremely comfortable with assembly debugging. Breakpoints sometimes work, so you may be able to use them to help narrow down the region causing your misaligned address exception.Here are a couple of threads with some suggestions for other possible ways to debug:(see the print_pixel macro)–\\nDavid.Powered by Discourse, best viewed with JavaScript enabled',\n",
       " 'message-broker-for-images': 'Hi,As the processing of video takes huge amount of time, I would want to segregate video recording with that of video processing. In this case, I want to have image to be sent to the message broker, and the same be used by processing logic. This way i can avoid any impact on video recording logic. Could you please share with me how to approach this design using gStreamer/multimedia api.Powered by Discourse, best viewed with JavaScript enabled',\n",
       " 'help-connecting-1920x1200-monitor-via-hdmi': 'I was redirected to the developer forum to ask for help - I hope this is the proper place to do so.I’m running Ubuntu 20.04 on my Laptop with GTX 1660 Ti and a 1920x1024 display and connect my Samsung Syncmaster 275t plus (native resolution 1920x1200) via HDMI. However, try as I may, I am only able to get it to connect with 1920x1024, but not with the full resolution supported by my monitor.I tried adding a new configuration via:and then assigning it to HDMI-0 via:but this fails with:X Error of failed request: BadMatch (invalid parameter attributes)\\nMajor opcode of failed request: 140 (RANDR)\\nMinor opcode of failed request: 18 (RRAddOutputMode)\\nSerial number of failed request: 37\\nCurrent serial number in output stream: 38Also trying to follow several instructions on the web to manipulate xorg.conf led nowhere, since no such file was available on my computer. I created and edit one from nvidia-settings, but that seems to be ignored as well.I cannot believe it is impossible to display 1920x1200 through HDMI on GeForce GTX 1660 Ti. Please advise!(Linux-x86_64, NVIDIA Driver Version 440.100, Sever Version 11.0)Powered by Discourse, best viewed with JavaScript enabled',\n",
       " 'intra-refresh-a-way-to-do-it-diagonally-vs-vertically': 'Is there a way to set it so the macroblocks are created on a diagonal instead? I noticed they sweep down vertically.No it’s not possible.Powered by Discourse, best viewed with JavaScript enabled',\n",
       " 'duplicate-collision-points-and-positive-separation': 'Hi allI have a strange issue, I get for example 30 collision points, however, some are duplicates, so it should be only about 15 which are unique. The remaining have the exact same position (not all are the same point, they are for example 5 different duplicate points).And the second issue is, sometimes I get positive separation values. This should be the penetration depth, so what would this mean? Can I just ignore them?I use PhysX 3.3.0 on Visual Studio.Thanks for your help.\\nRobertPowered by Discourse, best viewed with JavaScript enabled',\n",
       " 'nvidia-drivers-dxva-error': 'There is program that can play several videos simultaneously. It uses DirectShow (to extract frames) + DXVA (to process frames: brightness, contrast, hue etc.) + Direct3D 9 (to visualize frames).\\nSo, we are processing video frames with DXVA. And there is a problem with RGB-sources (videos, images).“NVD3DREL:\\nFailed to run Algo 0 of Stream 0.”As far as we understood, it’s nvidia driver error.\\nOn 280.26 drivers version there was no such problem.And this bug appeared and stably reproduced on next drivers versions:\\n285.62\\n295.73\\n301.42\\n306.97\\n310.70We tested it on videocards:\\n8600GT\\n9500GT\\nGT430\\nGT640On Radeon videocards there is no such problem.So, what does this error mean and how to avoid id?Powered by Discourse, best viewed with JavaScript enabled',\n",
       " 'yuv444p-video-decoding-with-ffmpeg-error-cuda-error-not-supported-operation-not-supported': 'Hi,I got following error when using GPU acceleration decoding YUV444p pixel format, h264 codec videos with FFmpeg, the CPU path works fine with the same video. Could anyone give me some insights on this?\\nimage1114×337 39.4 KB\\nThe same codes works for yuv420p videos, so I am imagining the problem is related to YUV444P pixel format. However, as far as I know, NVIDIA GPU does support 8 bits YUV444P, NV12 and so on… Am I missing anything here?It does nor support 444 for AVC. Only for hevc.Same for ffmpeg.exe -hwaccel cuda  -i .\\\\8.mp4 -f null -Also yuv444p8 is not NV12. NV12 is only enough for yuv420p8. Not enough for yuv420p10 or yuv422p8.Powered by Discourse, best viewed with JavaScript enabled',\n",
       " 'problem-when-connecting-a-tv-instead-of-a-monitordr': 'Hello. I ran into a problem when connecting my computer to the TV Sony X900H. Artifacts appear on the TV. I tried to solve this problem by reinstalling windows and as it turned out the problem was in the version of the drivers. Driver version 512.77 and earlier worked correctly. Newer drivers were causing artifacts to appear on the TV. Could you please fix this issue in a future driver release, as I have been reading the forums and people sometimes encounter similar problems when connecting a computer to a TV.\\nTested on windows 10 and 11 with RTX 3070\\nPS: When I use monitor to display the image, everything is fine.\\nIMG_20220722_2015531431920×2560 717 KB\\nHello,Welcome to the NVIDIA Developer forums. I am moving this out of the Networking category to “Drivers” sub-category.I am having the exact same problem with my Sony X90J. Now I am not able to play any new games cause it requires a new version of Nvidia which I can’t use cause it causes this issueStill the same Problem on the X80k and the X90k, exactly as in the picture running a 3080, works fine on the monitor, not on the TV… Shame Nvidia isn’t doing anything about this…Same issue with Sony 43x75kHaving the exact same issue on a Sony X900H. Nvidia, please fix this.Hello everyone and welcome to the NVIDIA developer forums!TV sets and how they advertise their capabilities in EDID and how they implement HDMI are quite often not as standardized as you would hope. That can cause incompatibilities for specific driver and TV firmware combinations. This can even be seen between different devices of the same model.Signal strength can also be a problem as can HDMI switches or AV devices.So the suggestion is to:Our QA is constantly checking our GPUs and drivers against a huge amount of possible EDIDs and actual physical TVs. Still it is impossible to test with every single one that you can buy.But if we do encounter this issue in our testing rest assured that it will get addressed and included in a future driver release.Thanks!Powered by Discourse, best viewed with JavaScript enabled',\n",
       " 'passing-parameters-from-intersection-program-to-closest-hit-program': 'Hi everyone,Let’s say I have some per vertex attributes for my triangle mesh. I would like to get the interpolated value in the closest hit program. I don’t think it’s possible without the intersection program passing some information(primitive ID or the interpolated value directly) to the closest hit program. Can anyone give me some hints on how this can be implemented?Thanks very much,\\nWoodyJust like what you have mentioned, the solution requires you to modify the intersection program.In your intersection program, you have to declare the values that you want to pass as an attribute:And in your material program, you have to declare the attribute that you want to receive from your intersection program:It is similar to how the geometric and shading normals are passed into the material script.Thanks very much. It works now.Powered by Discourse, best viewed with JavaScript enabled',\n",
       " 'there-seems-to-be-a-bug-with-vkcmdcopyimage-in-417-01': 'Images are only partially being copied… mind you I’m doing 3 at a time with barriers/layout-transitions executed for all 3 before and after the copy. Debug layer reporting is clean (no errors.)Here’s a linear trace of one of the destination 3D images on 416.94: \\nExternal Image\\n\\nSame build, same trace on 417.01: \\nExternal Image\\n\\nI can provide a full build if you’d like.Thanks,\\nBaktash.Oh I’m also using a GeForce RTX 2080Ti.I had access to a GeForce 970M GTX that didn’t seem to exhibit the same issue.Also using Vulkan SDK 1.1.92.1Amazing turnaround time… 417.22 fixed it! \\nExternal Image\\n\\nThank you so much for this swift resolution.Cheers,\\nBaktash.Powered by Discourse, best viewed with JavaScript enabled',\n",
       " 'txaa-library': 'Hi,\\nOur company produces rendering engine (targeting NVidia-only HW) for civil flight simulators used for pilots training. The product is sold with nvidia-only hardware solutions. We are looking for an opportunity to aquire TXAA library for use within developed engine.Just wondering which steps should companies` techincal\\\\management stuff follow to apply for TXAA library request. Should i contact local NV representative? Or apply somewhere on NV website?Thanks for your attention.Hello? Wish someday one NVIDIA person kindly explains what to do…I’d be interested too.Powered by Discourse, best viewed with JavaScript enabled',\n",
       " 'cuvidcreatedecoder-failed-with-cuda-error-invalid-value-gt-630-2032x2032-resolution-over-1080p': 'As title, cuvidCreateDecoder failed with CUDA_ERROR_INVALID_VALUE with 4k (3840X2160) video using GT 630. It runs normally with 1080p video. In fact, I’ve tried videos with some resolutions. I found that the resolution below 1080p runs normally, otherwise, it failed with CUDA_ERROR_INVALID_VALUE. I also tried GT 730 and it runs normally with all resolutions. So, I think the decoder of GT 630 might not support resolutions higher than 1080p.Are there any documents or specifications that I can refer to? Or, how do I know further information about the error CUDA_ERROR_INVALID_VALUE?Thanks a lot.Sorry, I updated the Nv decode SDK and found that cuvidGetDecoderCaps can query the ability of decode. I get max decoding width/height of GT 630 is 2032/2032. However, it still failed with CUDA_ERROR_INVALID_VALUE when the resolution of source video is higher than 1080p (e.g. 1920X1440, 1920X1200).Any hint about this?Hi.\\nCan you provide more details related to this failure - which codec, OS and driver version you used.Hi Godse,Sorry for the unclear information.OS: Windows 10\\nCodec: h264\\nDriver version: 23.21.13.9135Another question. Can I use Nv Codec SDK on OS X? I’d like to make a h264 decoder application on OS X. If not, do you know any low-level decode library on OS X?Thanks a lot.Hi.It appears you have a Fermi GPU.\\nAs you correctly found out, cuvidGetDecoderCaps() API can be used to query the decoder capabilities.I want to point out to the snippet in cuviddec.h from 9.0 SDKAlong with width and height, macroblock count also should be within the limit.\\nCan you check that is causing problem for you?Thanks.Another question. Can I use Nv Codec SDK on OS X?\\nVideo Codec SDK is not supported on OS X.Thanks.Powered by Discourse, best viewed with JavaScript enabled',\n",
       " 'why-not-use-the-mass-center-as-chassis-renderactor-center-in-vehicle-sample': 'I find the RigidDynamicActor and Chassis RenderActor use the same coordinate .\\nAlso I find the RigidDynamicActor has a member mBody which use the the mass center as origin.\\nI think the RigidDynamicActor should rotate around the Mass Center not the RenderActor Center.\\nSo why update RigidDynamicActor in this way?The Mass Center is just set below/forward the Actor center\\nI think it’s not very accurate when I implement a vehicle in practice.\\nSo How to compute the accurate Mass Center of Vehicle?A wheel’s CMOffset is vec3(0.9,-0.75,1.5)\\nThe unit of CMOffset is meter ?The vehicle is parameterised relative to the centre of mass of the rigid body rather than to the actor.  This is documented extensively in the guide. There are some advantages to this but there are also some disadvantages.  Whether you think this is a positive or a negative really depends what you are doing and whether you have manufacturer data to hand.There are multiple ways to compute the mass center of the vehicle but most users backwards rather than forwards.  I mean that it makes more sense for games to start with the centre of mass and then work out the parameters that reflect that rather than work forwards.  The reason for this is that the vehicle model is only a model and doesn’t have any knowledge of density distribution or the mass of steering wheels or the foam used in the back seat.  Computing the centre of mass from the vehicle parameters will always be a very rough estimate.  It is far better to start with the kind of handling that you want to have.  Do you want the vehicle to be heavier on its front wheels or its rear wheels?  If so, then by how much?  That will allow you to specify the centre of mass offset relative to the actor center.  You can then compute the wheel centres relative to the centre of mass and use the function PxVehicleComputeSprungMasses to compute the masses of each suspension line.Does this make sense?The vehicle is parameterised relative to the centre of mass of the rigid body rather than to the actor.  This is documented extensively in the guide. There are some advantages to this but there are also some disadvantages.  Whether you think this is a positive or a negative really depends what you are doing and whether you have manufacturer data to hand.There are multiple ways to compute the mass center of the vehicle but most users backwards rather than forwards.  I mean that it makes more sense for games to start with the centre of mass and then work out the parameters that reflect that rather than work forwards.  The reason for this is that the vehicle model is only a model and doesn’t have any knowledge of density distribution or the mass of steering wheels or the foam used in the back seat.  Computing the centre of mass from the vehicle parameters will always be a very rough estimate.  It is far better to start with the kind of handling that you want to have.  Do you want the vehicle to be heavier on its front wheels or its rear wheels?  If so, then by how much?  That will allow you to specify the centre of mass offset relative to the actor center.  You can then compute the wheel centres relative to the centre of mass and use the function PxVehicleComputeSprungMasses to compute the masses of each suspension line.Does this make sense?Thanks! It makes sense.So If I want to simulate a BMW in VR Environment ,I need to get the manufacturer data to compute the mass center ?\\nThe manufacturer data should have density distribution /mass of steering wheels and so on?\\nAlso I should consider the mass and position of passengers.\\nI think it’s still hard to compute the accurate mass center…Computing the mass centre is indeed a very difficult exercise.  The manufacturer data might quote the roll centre [url]https://en.wikipedia.org/wiki/Roll_center[/url] but that isn’t quite the same as the mass centre. I doubt anyone truly knows the mass centre of a vehicle.  It is also a shifting value because adding passengers and luggage will change the mass centre.We might expect that the car is designed to divide the mass fairly evenly between the four suspension lines. Therefore, it is likely that the mass centre lies near the centre point of the four wheels.  The roll centre can then help to estimate the vertical coordinate of the mass centre.  These, however, are all estimates and deductions.  That’s about the best that you can do.  The centre of mass is only the tip of the iceberg.  What is the moment of inertia of a vehicle?  I doubt anyone really knows.  What about tire longitudinal stiffness?  All we can do is estimate and test.The PhysX vehicles SDK is not designed to accurately model specific cars. The intention is to provide behaviour that seems reasonable and plausible.  The intention is also to provide a paramaterised model that allows different behaviours to be easily created just by changing a few numbers. I would expect a developer to start with an estimation of the centre of mass and moment of inertia and then tweak those numbers until they get the behaviour that they want.I did once validate PhysX vehicles against real vehicle data.  Some numbers did end up being educated estimates: centre of mass, moment of inertia, clutch stiffness etc.  Sensible tweaks of the values allowed reasonably good tracking of a real car. Manufacturer data is necessarily incomplete and not always a good fit to a physical model.Computing the mass centre is indeed a very difficult exercise.  The manufacturer data might quote the roll centre [url]https://en.wikipedia.org/wiki/Roll_center[/url] but that isn’t quite the same as the mass centre. I doubt anyone truly knows the mass centre of a vehicle.  It is also a shifting value because adding passengers and luggage will change the mass centre.We might expect that the car is designed to divide the mass fairly evenly between the four suspension lines. Therefore, it is likely that the mass centre lies near the centre point of the four wheels.  The roll centre can then help to estimate the vertical coordinate of the mass centre.  These, however, are all estimates and deductions.  That’s about the best that you can do.  The centre of mass is only the tip of the iceberg.  What is the moment of inertia of a vehicle?  I doubt anyone really knows.  What about tire longitudinal stiffness?  All we can do is estimate and test.The PhysX vehicles SDK is not designed to accurately model specific cars. The intention is to provide behaviour that seems reasonable and plausible.  The intention is also to provide a paramaterised model that allows different behaviours to be easily created just by changing a few numbers. I would expect a developer to start with an estimation of the centre of mass and moment of inertia and then tweak those numbers until they get the behaviour that they want.I did once validate PhysX vehicles against real vehicle data.  Some numbers did end up being educated estimates: centre of mass, moment of inertia, clutch stiffness etc.  Sensible tweaks of the values allowed reasonably good tracking of a real car. Manufacturer data is necessarily incomplete and not always a good fit to a physical model.Computing the mass centre is indeed a very difficult exercise.  The manufacturer data might quote the roll centre https://en.wikipedia.org/wiki/Roll_center but that isn’t quite the same as the mass centre. I doubt anyone truly knows the mass centre of a vehicle.  It is also a shifting value because adding passengers and luggage will change the mass centre.We might expect that the car is designed to divide the mass fairly evenly between the four suspension lines. Therefore, it is likely that the mass centre lies near the centre point of the four wheels.  The roll centre can then help to estimate the vertical coordinate of the mass centre.  These, however, are all estimates and deductions.  That’s about the best that you can do.  The centre of mass is only the tip of the iceberg.  What is the moment of inertia of a vehicle?  I doubt anyone really knows.  What about tire longitudinal stiffness?  All we can do is estimate and test.The PhysX vehicles SDK is not designed to accurately model specific cars. The intention is to provide behaviour that seems reasonable and plausible.  The intention is also to provide a paramaterised model that allows different behaviours to be easily created just by changing a few numbers. I would expect a developer to start with an estimation of the centre of mass and moment of inertia and then tweak those numbers until they get the behaviour that they want.I did once validate PhysX vehicles against real vehicle data.  Some numbers did end up being educated estimates: centre of mass, moment of inertia, clutch stiffness etc.  Sensible tweaks of the values allowed reasonably good tracking of a real car. Manufacturer data is necessarily incomplete and not always a good fit to a physical model.So It’s not really possible to develop a CAE software based on PhysX,\\nif  I want to use it in Vehicle Industry …Computing the mass centre is indeed a very difficult exercise.  The manufacturer data might quote the roll centre https://en.wikipedia.org/wiki/Roll_center but that isn’t quite the same as the mass centre. I doubt anyone truly knows the mass centre of a vehicle.  It is also a shifting value because adding passengers and luggage will change the mass centre.We might expect that the car is designed to divide the mass fairly evenly between the four suspension lines. Therefore, it is likely that the mass centre lies near the centre point of the four wheels.  The roll centre can then help to estimate the vertical coordinate of the mass centre.  These, however, are all estimates and deductions.  That’s about the best that you can do.  The centre of mass is only the tip of the iceberg.  What is the moment of inertia of a vehicle?  I doubt anyone really knows.  What about tire longitudinal stiffness?  All we can do is estimate and test.The PhysX vehicles SDK is not designed to accurately model specific cars. The intention is to provide behaviour that seems reasonable and plausible.  The intention is also to provide a paramaterised model that allows different behaviours to be easily created just by changing a few numbers. I would expect a developer to start with an estimation of the centre of mass and moment of inertia and then tweak those numbers until they get the behaviour that they want.I did once validate PhysX vehicles against real vehicle data.  Some numbers did end up being educated estimates: centre of mass, moment of inertia, clutch stiffness etc.  Sensible tweaks of the values allowed reasonably good tracking of a real car. Manufacturer data is necessarily incomplete and not always a good fit to a physical model.So It’s not really possible to develop a CAE software based on PhysX,\\nif  I want to use it in Vehicle Industry …I would guess that pretty much all vehicle models use either a single rigid body or multiple rigid bodies. They will all need to know the centre of mass.   Even military simulations use vehicles based on rigid bodies.  Additionally, PhysX isn’t too far from CarSimEd, which was also based on rigid body mechanics.  Anything that computes tire or suspension forces will need to apply those forces to something that is almost certainly a rigid body.The core of the model is that there is a rigid body and there are forces arising from tire and suspension forces.  The rigid body’s transform helps compute the forces and the forces tell the rigid body how to move.  I’m guessing that pretty much all vehicle models work on that principle and only differ to the degree of complexity of the model.  Some models have more sophisticated tire models, some model the gears and clutch more accurately, some model the suspension system with multiple bodies coupled with constraints.  PhysX does all of these in a fairly simple way but it shares the core model of a rigid body moving under force.The problem with all simulation is that the model used is always an approximation.  I wouldn’t even say that the centre of mass estimation is the biggest approximation in a vehicle simulation.  Tire forces, for example, are always going to be approximate because it is hard to mesure the regression data for a specific tire on a specific car on a specific road in specific temperature conditions.  Even if we had a great model it would be very difficult to drive that model with accurate data. The suspension model is also prone to error. PhysX uses a simple spring but it would be easy to extend that to a jointed system that more closely mimics reality.  If we did that we would be faced with all sorts of technical problems like how to accurately model a non-linear spring or one that has bi-directional damping. Even measuring the stiffness graph of a non-linear spring is a significant challenge. How would we even measure that data so that it could be plugged into a vehicle simulator?  Finally, there is the timestep of the simulation.  As the timestep reduces the simulation ought to to converge but that requires very, very small timesteps that are computationally expensive.  Making a good timestep choice that balances computational expense with accuracy is fraught with difficulty.  This is all hard.It is always difficult to map the real world to physics modelling parameters.  That is true of all vehicle models as well as PhysX.  I would say that PhysX is a fairly simple model; more sophisticated packages will ask for more and more parameters that might not be readily available to the user.  The best we can do is to improve our estimates by calibrating the simulated motion against real data.Powered by Discourse, best viewed with JavaScript enabled',\n",
       " 'best-pcie-camera-link-video-grabbers-for-jetson-tx2-developer-board': 'Hi.\\nThe goal of this topic is sharing all experiences about pcie camera link frame grabbers that can be used with jetson tx2 developer board,Powered by Discourse, best viewed with JavaScript enabled',\n",
       " 'missing-libvulkan-so-on-shield-tablet-after-update-to-4-2': 'After I did a full OTA 4.2 flash I can’t find libvulkan.so in /system/lib anymore. Also all of my Vulkan sample applications don’t work anymore. Is there something I have to consider before making the update or is it just missing from the full wifi image? If so, maybe someone could provide me with their libvulkan.so from the OTA 4.2 for the original Shield Tablet.ThanksI found out that libvulkan.so now is located in /vendor/lib but if I try to run my written Vulkan code it dies at vkCreateInstance for some reason. I can’t read the return value because the whole application has died before the return of vkCreateInstance.edit: None of the Vulkan applications work anymore, not mine or any of the NVIDIA sample applications.Powered by Discourse, best viewed with JavaScript enabled',\n",
       " 'how-to-use-tesla-k80-in-vm': 'I created a virtual machine using VMware vSphere Web Client. Was passthrough to the video card Tesla K80. I can’t use her resources. Please tell me how to run on it, for example a video file 4K.Powered by Discourse, best viewed with JavaScript enabled',\n",
       " 'mentor-s-needed-for-object-detection-no-idea-what-i-am-doing': 'Hi all. Recovering developer (Retired) looking for something to do.  I would like to work with YOLOv5 and need a lot of handholding.  Been a few years since i did any development and it was mostly in the commercial investment space building data warehouses and general batch processing.  I have an environment set up on my PC that works.  Yolov5/Visual studio/python/pyTorch/Cuda and maybe more.My next step is to learn how to train the system to recognize specific objects i am interested in. Any great tutorials for doing that? I am using some YouTube videos by Ivan Goncharov which have gotten me started, but i might need something more interactive.Thanks for any ideas or help!\\nRobHi @user102826 and welcome to the NVIDIA developer forums!Unless you use out of the box solutions for object detection there is quite a bit to learn before you can run for example some Deep Learning Inference system on you home PC. A good place to start looking for self-paced or guided courses is NVIDIA’s Deep Learning Institute. There you will find content for every Deep Learning topic and lot of great learning resources to get you started.Also Coursera is a great portal to learn fundamentals of AI, some of the courses being free (if you don’t want a certificate), and especially those by Andrew Ng can be recommended.If you don’t want to do the learning part but instead jump right into some example code NVIDIA provides, I would suggest checking out one of our (slightly older, but still valid) Blog posts on Object detection in Python. Don’t worry that this is on Jetson, since it is based on Python you should be able to use this on PC as well with some adjustments. The “Hello AI” github is in general a good source for simple AI examples.I hope this helps!Powered by Discourse, best viewed with JavaScript enabled',\n",
       " 'using-rtsp-input-source-in-video-codec-sdk-examples': 'I am trying to use CUDA assisted video decoding and have run the AppDec decoder example.  However, when I tried to use an external web cam’s RTSP url the examples gave an error at the start.Is there something different needed to interpret this type of video stream?Powered by Discourse, best viewed with JavaScript enabled',\n",
       " 'physx-3-3-cloth-framerate-drop-with-pxclothflag-escene-collision': 'when I enable PxClothFlag::eSCENE_COLLISION for the cloth and start rotating it with setTargetPose() once it gets to a certain speed frame rate drops to a crawl until I destroy the cloth. happens with or without gpu enabled. if no scene collisions are enable everything works fine.anyone else experienced this?Sorry to bump an on old thread but i am getting this exact same problem. Happening with both 3.3.1 and 3.3.2. With PxClothFlag::eSCENE_COLLISION set to true i get“…..\\\\foundation\\\\src\\\\PsFoundation.cpp (268) : abort : User allocator returned NULL.”Which obviously means the default memory allocator is failing to allocate memory.set PxClothFlag::eSCENE_COLLISION to false and all is good. My cloth mesh is tiny, 80 verts.I have also noticed if a disable my terrain the problem goes away too (if i use a smaller terrain the problem goes away also). What ever is happening physx is trying to allocate a massive amount of memory on the first call to scene->simulateMy terrain is only 2048x2048, certainly not that big and if i remove the cloth everything also runs perfectly on this terrain size. Also if i enable PxClothFlag::eSCENE_COLLISION but comment out the call to cloth->lockParticleData(physx::PxDataAccessFlag::eREADABLE) in the render tick function the problem disappears tooAlso no GPU enabled for cloth or PxSceneThe PxCloth implementation in PhysX-3 is designed primarily for the clothing use case. The problem you describe here is that the mechanism for collision detection between the cloth and the terrain can be expensive.  You should disable the collision detection between cloth and static terrain until you actually need it, don’t leave it on all the time. How big is the cloth with respect to the terrain?  The terrain has >4M vertices, how many vertices of terrain are covered by the flag when it drops on the ground?Thanks for the reply mike,The cloth never even touches the terrain at any stage, the cloth is fixed and hanging in the air. Kind of like a flagpole.I will try your suggestion though of disabling collision between the two anyway and see if that fixes it because i actually never need collision between terrain & clothThe collision cost doesn’t occur only when the cloth actually touches the ground. In order to prepare for the case where the cloth might touch the ground, the pipeline performs a scene query on the nearby environment, gathers up the geometry and inserts it into the cloth collision rig for narrow-phase collision testing. Sounds like turning off the collision is the right way to go for you.Just an update to this. I suppressed collisions between terrain and cloth object in the filter shader and everything works fine now with cloth and PxClothFlag::eSCENE_COLLISION=true. Other objects can collide with the cloth just not the terrain.Powered by Discourse, best viewed with JavaScript enabled',\n",
       " 'vmaterials-for-solidworks-visualize': 'Hi,\\nI use Solidworks Visualize 2022 edition and I just downloaded Nvidia vMaterials.\\nI installed application but I not found where the mdl files are in SW Visualize.\\nCould you help me please ?ThanksHallo @be15If Solidworks Visualise 2022 and vMaterials are installed correct. You still have to tell SW where vMaterials are located.\\nPlease use our documentation. Step 3 is where you should be right now.https://raytracing-docs.nvidia.com/vmaterials/solidworks_visualize_vmaterials/index.html#solidworks_visualize#solidworks-visualize-and-vmatJust continue with the following steps and you will be able to work with vMaterials in SW.Best regards\\nMaikHi Maik,Thank you for this answer.The problem is I don’t knew where is the Nvidia Vmaterials folder was installed. Now, i found the folder in my documents and place in the correct place to Solidworks Visualize.Also, I have two other questions :1 - Do you know why it is not possible to see the rendering on littles sample on right on my screen.I only have this problem on Nvidia Vmaterials.On all appearances included In Solidworks Visulize, I have not this.I attached screen sharehttps://we.tl/t-DopuwlQobG2 - Why the number of appearances are not the same between windows folder and Solidworks Visualize (Paint for example) ?Thank youCedricHi @be15 ,\\nAll the thumbnails for the new appearances will be generated automatically. This takes extra computing time and may slow down rendering in the viewport. After a while you should be able to see the thumbnails.\\nThere are a lot of materials So it will take a bit of time.I noticed that you use an older version of vMaterials. Try also 2.1. https://developer.nvidia.com/vmaterialsWhat you see now in SW is only the mdl file. These files contain all the appearances. E.g. Chalk_Paint.mdl contains all chalk paint appearances. SW should show them one by one with their respective thumbnails as in Windows Explorer. So don’t worry about a missing match or a failed installation because materials are missing.Please get back to me if they will not generate.\\nHave a nice weekend.\\nMaikHi Maik,I installed v2.0 because 2.1 is access denied.Also, the materials are not generate since last week. I kept my computer On during the week end but nothing more.Wh ais the difference between AEC and Design files ?https://we.tl/t-UOBXCg7a3ZBestCedricHi @be15 ,\\nI tested 2.1 and the only thing was that I need a sign-in with the mail address. I think you have to join the Developer program to get access.  What happens if you press the Developer button you got on the Access Denied page?About the generation of the thumbnails. Let me contact a colleague who should know better and try the documentation from Solidworks, too. vMaterials is a material library and we provide an installer but how vMaterials is implemented in third-party applications depends on the development of the company that provides the software that you use.The difference between AEC and Design: AEC stands for Architecture Engineering Construction and in this relation, we provided in v1.7 materials for these areas, and for Design, there are all visual product-related materials.  Since 2.0 we rebuilt the material library without the folder structure but we added keywords inside of the mdls. So that the third-party application can implement a search function that makes use of our keyword system, which is more precise and makes sense while vMaterials is uprising the amount of the materials.Best regards\\nMaikPowered by Discourse, best viewed with JavaScript enabled',\n",
       " 'texture-arrays': 'I read an old post that texture arrays are not supported in optix, and we should use bindless texturing.  Is that still true in the latest version? What about 3D volume textures?All supported texture types in CUDA are available via OptiX bindless textures and the OptiX rtTex*(id, …) intrinsics you’ll find in the optix_device.h header.See my answer in your other question about null initializations for links with more details.Powered by Discourse, best viewed with JavaScript enabled',\n",
       " 'human-mesh-model-generation': 'Hi, I would like to generate human-body mesh models (that can be ingested into OptiX for reflective ray tracing) given their postures, say represented as keypoints.  Alternative, if I can find some sample of human body models with different postures, I hand craft their keypoints. Basically, I would like to find some sample pairs of human body mesh models and their postures as keypoints.  I would like a variety of postures, but I do not require specific postures.  The human bodies only need to resemble faceless but proportionally realistic full-body  mannequins.  Can you suggest any tools and/or repositories?I am new to graphics and animations.Thanks,Hi user99334 and thank you for your interested in OptiX.\\n3D modelling and animation tools allow you to do what you’re describing, taking a mesh (of a human form), combining it with a “control skeleton” (your control points) and then the tool deforms the mesh based on the movement/pose of the skeleton. Attaching the skeleton to the mesh is usually referred to as “character rigging”. Web search this will get you to tutorials. As for actual tools you have many options; a free/open source one is Blender.\\nTo get a mesh to work with, you could turn to one of the many websites hosting 3d models. Many of those sites even offer models that are already rigged for animation in file formats for the various modelling tools (including Blender).\\nThe modelling tools allow you to export the meshes you create (of the various poses) to file formats amenable to rendering/visualization in OptiX. The programming examples in the OptiX SDK use a format called GLTF (with the vast majority of tools can export to).Welcome to the NVIDIA developer forums @user99334And thank you Frank for your great suggestions!I would like to add that if you are new to graphics and animation and you decide to work with Blender, another good companion to get you started on Raytracing would be NVIDIA Omniverse, especially Omniverse Create. Depending on what you want to achieve, this could be a bit more beginner friendly.Thanks you both.  Appreciate the suggestions.Powered by Discourse, best viewed with JavaScript enabled',\n",
       " 'stuttering-issue': 'Hi,\\nWe are having Stuttering problem with 2 cards connected to 4 screens.All data has been sent to escalate : Incident: 160821-000138We have raised an escalation to Microsoft , they state the below:\\n\"\\nThe bad news: I can’t tell you how you can fix the problem in software, because it isn’t a problem that appears to have anything to do with your code and design, and it is NOT a Windows bug.Here are some of the bits of things I was looking at via GPUView:Type: Unknown / Multiple (32), Time: 92617409408 (9261,740.9408ms), TaskId: 0, Version: 2\\nStackWalk TimeStamp: 27141775722, Associated Event: 92617409348\\nProcess: Review.exe\\n0 0xFFFFF80002EE59EB ntoskrnl.exe!KeSetEvent+0x81\\n1 0xFFFFF8800FD71057 dxgmms1.sys!VidSchiUnwaitWaitQueuePacket+0xBB\\n2 0xFFFFF8800FD71178 dxgmms1.sys!VidSchiCompleteSignalCommmand+0x114\\n3 0xFFFFF8800FD6D059 dxgmms1.sys!VidSchiProcessCompletedQueuePacketInternal+0x131\\n4 0xFFFFF8800FD6C7DA dxgmms1.sys!VidSchiProcessDpcCompletedPacket+0x3B6\\n5 0xFFFFF8800FD6BE00 dxgmms1.sys!VidSchDdiNotifyDpcWorker+0x198\\n6 0xFFFFF8800FD6BC4C dxgmms1.sys!VidSchDdiNotifyDpc+0x94\\n7 0xFFFFF8800FC721CF dxgkrnl.sys!DxgNotifyDpcCB+0x77\\n8 0xFFFFF8800F0E2C28 nvlddmkm.sys+0xC8C28\\n9 0xFFFFF8800F0E386B nvlddmkm.sys+0xC986B\\n10 0xFFFFF8800F0E36B2 nvlddmkm.sys+0xC96B2\\n11 0xFFFFF8800F141DEF nvlddmkm.sys+0x127DEF\\n12 0xFFFFF80002E96B1C ntoskrnl.exe!KiRetireDpcList+0x1BC\\n13 0xFFFFF80002E8E165 ntoskrnl.exe!KyRetireDpcList+0x5\\n14 0xFFFFF80002E8DF7C ntoskrnl.exe!KiDispatchInterruptContinue\\n15 0xFFFFF80002ED7453 ntoskrnl.exe!KiDpcInterruptBypass+0x13\\n16 0xFFFFF80002E87522 ntoskrnl.exe!KiInterruptDispatch+0x212\\n17 0x00007FFD179C9E80 ???!???\\n18 0x000000080C8A0064 ???!???Thread: 2576\\nType: Unknown / Multiple (32), Time: 92617410393 (9261,741.0393ms), TaskId: 0, Version: 2\\nStackWalk TimeStamp: 27141775877, Associated Event: 92617409895\\n27141798799\\nProcess: Review.exe\\n0 0xFFFFF8800F230AB8 nvlddmkm.sys+0x216AB8\\n1 0xFFFFF8800F232913 nvlddmkm.sys+0x218913\\n2 0xFFFFF8800F42DDC1 nvlddmkm.sys+0x413DC1\\n3 0xFFFFF8800F1BE393 nvlddmkm.sys+0x1A4393\\n4 0xFFFFF8800F1B6301 nvlddmkm.sys+0x19C301\\n5 0xFFFFF8800F10938B nvlddmkm.sys+0xEF38B\\n6 0xFFFFF8800F109417 nvlddmkm.sys+0xEF417\\n7 0xFFFFF8800F0E3744 nvlddmkm.sys+0xC9744\\n8 0xFFFFF8800F141F72 nvlddmkm.sys+0x127F72\\n9 0xFFFFF80002E8747C ntoskrnl.exe!KiInterruptDispatch+0x16C\\n10 0x00007FFD179C9E80 ???!???\\n11 0x0000000804FE006F ???!???Interesting thread stack (how do I turn this into a thread ID?)\\nType: Unknown / Multiple (32), Time: 92621940580 (9262,194.0580ms), TaskId: 0, Version: 2\\nStackWalk TimeStamp: 27143058988, Associated Event: 92621940559\\nProcess: Idle\\n0 0xFFFFF80002EE58AA ntoskrnl.exe!KiReadyThread+0x5\\n1 0xFFFFF80002E96F97 ntoskrnl.exe!KiProcessExpiredTimerList+0x157\\n2 0xFFFFF80002E96DEE ntoskrnl.exe!KiTimerExpiration+0x1BE\\n3 0xFFFFF80002E96BD7 ntoskrnl.exe!KiRetireDpcList+0x277\\n4 0xFFFFF80002E8336A ntoskrnl.exe!KiIdleLoop+0x5A\\n5 0x00007FFD179C9E80 ???!???\\n6 0x000000080C910064 ???!???Thread 48:\\nType: Unknown / Multiple (32), Time: 92618219854 (9261,821.9854ms), TaskId: 0, Version: 2\\nStackWalk TimeStamp: 27142005137, Associated Event: 92618219412\\nProcess: System\\n0 0xFFFFF80002E459CC ntoskrnl.exe!KeDelayExecutionThread+0xE3\\n1 0xFFFFF8800F1BCFCE nvlddmkm.sys+0x1A2FCE\\n2 0xFFFFF8800F4B557E nvlddmkm.sys+0x49B57E\\n3 0xFFFFF8800F51ABE1 nvlddmkm.sys+0x500BE1\\n4 0xFFFFF8800F51A864 nvlddmkm.sys+0x500864\\n5 0xFFFFF8800F510769 nvlddmkm.sys+0x4F6769\\n6 0xFFFFF8800F5326D0 nvlddmkm.sys+0x5186D0\\n7 0xFFFFF8800F52EDFF nvlddmkm.sys+0x514DFF\\n8 0xFFFFF8800F52E71A nvlddmkm.sys+0x51471A\\n9 0xFFFFF8800F52F337 nvlddmkm.sys+0x515337\\n10 0xFFFFF8800F52E9F7 nvlddmkm.sys+0x5149F7\\n11 0xFFFFF8800F52F9EC nvlddmkm.sys+0x5159EC\\n12 0xFFFFF8800F52F823 nvlddmkm.sys+0x515823\\n13 0xFFFFF8800F51146E nvlddmkm.sys+0x4F746E\\n14 0xFFFFF8800F2AA564 nvlddmkm.sys+0x290564\\n15 0xFFFFF8800F39A175 nvlddmkm.sys+0x380175\\n16 0xFFFFF8800F39A3EA nvlddmkm.sys+0x3803EA\\n17 0xFFFFF8800F39AEB0 nvlddmkm.sys+0x380EB0\\n18 0xFFFFF8800F1B7B08 nvlddmkm.sys+0x19DB08\\n19 0xFFFFF80003181F4D ntoskrnl.exe!IopProcessWorkItem+0x3D\\n20 0xFFFFF80002E95A21 ntoskrnl.exe!ExpWorkerThread+0x111\\n21 0xFFFFF80003128CCE ntoskrnl.exe!PspSystemThreadStartup+0x5A\\n22 0xFFFFF80002E7CFE6 ntoskrnl.exe!KxStartSystemThread+0x16\\n23 0x00007FFD179C9E80 ???!???\\n24 0x000000080E51006F ???!???//JBT note the mutex (mutant) which is being held.\\nThread 48:\\nType: Unknown / Multiple (32), Time: 92621961469 (9262,196.1469ms), TaskId: 0, Version: 2\\nStackWalk TimeStamp: 27143064899, Associated Event: 92621961430\\nProcess: System\\n0 0xFFFFF80002EE58AA ntoskrnl.exe!KiReadyThread+0x5\\n1 0xFFFFF80002EC0710 ntoskrnl.exe!KiProcessThreadWaitList+0x60\\n2 0xFFFFF80002E9682A ntoskrnl.exe!KeReleaseMutant+0x2EA\\n3 0xFFFFF8800F21037E nvlddmkm.sys+0x1F637E\\n4 0xFFFFF8800F39AEBE nvlddmkm.sys+0x380EBE\\n5 0xFFFFF8800F1B7B08 nvlddmkm.sys+0x19DB08\\n6 0xFFFFF80003181F4D ntoskrnl.exe!IopProcessWorkItem+0x3D\\n7 0xFFFFF80002E95A21 ntoskrnl.exe!ExpWorkerThread+0x111\\n8 0xFFFFF80003128CCE ntoskrnl.exe!PspSystemThreadStartup+0x5A\\n9 0xFFFFF80002E7CFE6 ntoskrnl.exe!KxStartSystemThread+0x16\\n10 0x00007FFD179C9E80 ???!???\\n11 0x000000080CB70064 ???!???Thread 48 is busy processing stuff via IopProcessWorkItem and the nVidia driver it calls for over 400 ms. That last stack trace shows it is releasing a mutex held by the device driver, in stack frame 2, which is called by stack frame 3, the device driver.\\nThe last thing one of your rendering worker threads is doing is going into the nVidia device driver to handle an interrupt: that’s the second trace shown for thread #2576. Right before that, it’s calling SetEvent, so it’s the driver handling the interrupt on that thread.Thus: it appears from the information I currently have that the nVidia driver is waiting on the same mutex, quite probably shared amongst all installed devices on the system (I would need to spend a bit more time to verify that: I also don’t have nVidia symbols available at this moment). Thus, this is why it would result in your system having all the displays running into this problem.From using Windows Performance Analyzer, while it is stuck in that 400+ ms dead time where you are not getting updated video, one of your 24 cores is maxed out, but the others are waiting with nothing to do: it is processing all the queued up GPU commands and whatever it is doing during that time in the nVidia driver: it’s a real pity you’re not getting good utilization out of that Xeon as a result of the driver.It is unclear to me which version of the nVidia driver this is: please run DXDiag in your System32 folder and upload that data. nVidia and Microsoft have a close working relationship, though we don’t currently have their symbols. I can get this information to nVidia, but also I’d urge you to contact them as well. This counts as a driver bug.While that is going on, I would urge you to go and get graphics cards from an alternate vendor: your application should work fine with other GPUs, at least as far as this goes. Because I don’t know the extent as to what will happen if you get a garbage collection pause from your managed application portion because I don’t have enough information on how that is tied together, I cannot tell you whether or not, or how badly your system would be affected if there’s a long garbage collection pause.nVidia Graphics Cards : Solved by using Driver 348.17, Means that no more than 80ms Stutter (ran for 3 Stability Days)http://www.nvidia.com/download/driverResults.aspx/85608/en-usPowered by Discourse, best viewed with JavaScript enabled',\n",
       " 'address-out-of-bounds-error-help': 'Hi,I’m sometimes running into a CUDA Address out of bounds error. It the same error : ========= Invalid global write of size 4\\n=========     at 0x00001218.The app runs, and exits successfully, but part of the output data is corrupted when viewed (graphics image).Here’s the lo-down on the eGPU in use, on OSX 10.3.6, with a Sonnet eGPU.Detected 1 CUDA Capable device(s)Device 0: “GeForce GTX 1070”\\nCUDA Driver Version / Runtime Version          10.1 / 10.1\\nCUDA Capability Major/Minor version number:    6.1\\nTotal amount of global memory:                 8192 MBytes (8589737984 bytes)\\n(15) Multiprocessors, (128) CUDA Cores/MP:     1920 CUDA Cores\\nGPU Max Clock rate:                            1772 MHz (1.77 GHz)\\nMemory Clock rate:                             4004 Mhz\\nMemory Bus Width:                              256-bit\\nL2 Cache Size:                                 2097152 bytes\\nMaximum Texture Dimension Size (x,y,z)         1D=(131072), 2D=(131072, 65536), 3D=(16384, 16384, 16384)\\nMaximum Layered 1D Texture Size, (num) layers  1D=(32768), 2048 layers\\nMaximum Layered 2D Texture Size, (num) layers  2D=(32768, 32768), 2048 layers\\nTotal amount of constant memory:               65536 bytes\\nTotal amount of shared memory per block:       49152 bytes\\nTotal number of registers available per block: 65536\\nWarp size:                                     32\\nMaximum number of threads per multiprocessor:  2048\\nMaximum number of threads per block:           1024\\nMax dimension size of a thread block (x,y,z): (1024, 1024, 64)\\nMax dimension size of a grid size    (x,y,z): (2147483647, 65535, 65535)\\nMaximum memory pitch:                          2147483647 bytes\\nTexture alignment:                             512 bytes\\nConcurrent copy and kernel execution:          Yes with 2 copy engine(s)\\nRun time limit on kernels:                     Yes\\nIntegrated GPU sharing Host Memory:            No\\nSupport host page-locked memory mapping:       Yes\\nAlignment requirement for Surfaces:            Yes\\nDevice has ECC support:                        Disabled\\nDevice supports Unified Addressing (UVA):      Yes\\nDevice supports Compute Preemption:            Yes\\nSupports Cooperative Kernel Launch:            Yes\\nSupports MultiDevice Co-op Kernel Launch:      Yes\\nDevice PCI Domain ID / Bus ID / location ID:   0 / 188 / 0Here’s running the app through CUDA Memcheck from the OSX terminal with an OK result:$ cuda-memcheck --leak-check full ./resize_textureRunning OpenMP, number of threads : 8\\nB&W - Texture\\nDimension of x blocks of threads : thread_count : 32\\nDimension of y blocks of threads : thread_count : 32\\nNumber of Threads per block: threads.x * threads.y : 1024\\nDimension of x grid in blocks) : blocks.x : 138\\nDimension of y grid in blocks) : blocks.y : 78\\nNumber of Blocks: blocks.x * blocks.y : 10764\\nSaved : lop.tif\\nTime : 2.28168========= LEAK SUMMARY: 0 bytes leaked in 0 allocations\\n========= ERROR SUMMARY: 0 errors(To confirm terminal params, verified / analyzed app through nVidia Visual profiler)Here’s running the app, with errors. Here, the image size is slightly larger than the above example:Error type : unspecified launch failure\\n========= Error: process didn’t terminate successfully\\n=========        The application may have hit an error when dereferencing Unified Memory from the host. Please rerun the application under cuda-gdb or Nsight Eclipse Edition to catch host side errors.\\n========= No CUDA-MEMCHECK results found(To confirm terminal params, verified / analyzed app through nVidia Visual profiler)Here’a another, again with a slightly larger image than the first:Error type : unspecified launch failure\\n========= Error: process didn’t terminate successfully\\n=========        The application may have hit an error when dereferencing Unified Memory from the host. Please rerun the application under cuda-gdb or Nsight Eclipse Edition to catch host side errors.\\n========= No CUDA-MEMCHECK results found(To confirm terminal params, verified / analyzed app through nVidia Visual profiler)Powered by Discourse, best viewed with JavaScript enabled',\n",
       " 'd3d12-raytracing-tier-not-supported': 'Hi,I am using a rtx studio laptor(RTX 3070) with the latest nvidia studio driver(472.12).But when I go ahead and try to check featuresupport on a dx12 device for DXR(CheckFeatureSupport(D3D12_FEATURE_D3D12_OPTIONS5) it fills up the  “D3D12_FEATURE_DATA_D3D12_OPTIONS5” struct but RaytracingTier member is  D3D12_RAYTRACING_TIER_NOT_SUPPORTED.I am using agility sdk for trying out the new dx12 ultimate features and I am successfully getting ```\\nD3D12_SAMPLER_FEEDBACK_TIER_0_9Hi friendandvis, welcome back to the NVIDIA developer forums!I took the liberty of moving the topic into the DirectX category, I think it might get more attention this way.If I understand you correctly you don’t get the expected feature set reported when querying the device capabilities, correct?Can you tell me which Laptop model exactly you are using? Does it possibly have integrated graphics support by the on-board CPU? If so it could be that you first have to enumerate the graphics adapters before querying features and choose the RTX card. The Agility SDK might do that internally and show the max capabilities of your laptop.Let me know if this helps.Markushi Markus,\\nthanks a lot for the reply and moving my topic to the right category.I did double check the adapter  I am using at time of device creation it is the nvidia one and I did a little more digging around and found the issue which was causing tracing tier to return as D3D12_RAYTRACING_TIER_NOT_SUPPORTED,It seems NVIDIA does not supports raytracing on a 32 bit application while sampler feedback is supported.So building my application on x64  mode did the trick and expected raytracing tier(1_1) was returned.thanks again\\nRegards,\\nVishwasGreat to hear that you could resolve the problem and thanks for sharing how to work around it!Best of success in your development.MarkusPowered by Discourse, best viewed with JavaScript enabled',\n",
       " 'cloudxr-valve-index-finger-tracking': 'Hey guys! Newcomer to CloudXR, tinkering around to see what I can do before implementing into any app.\\nOn the PCVR client,  I noticed some apps and games would misposition my hands, lock my hands in the floor, or even refuse to show any hands at all. I suspect this to be due to the missing finger tracking, since most apps will only read the force sensor as grip, trigger and buttons. Is this a known issue?Welcome to the NVIDIA CloudXR club!  Yes, this is a known issue.  CloudXR does not support the SteamVR skeletal input API: SteamVR Skeletal Input · ValveSoftware/openvr Wiki · GitHub.  I will definitely put it on the list of user interests!Thanks,\\nGregPowered by Discourse, best viewed with JavaScript enabled',\n",
       " 'performance-dependence-on-kernel-resolution': 'Hi, experts:\\nI recently do a research, based on an idea that ray tracer’s performance is proportional to the work load. Thus reducing the tracing ray’s number could result in performance’s improvement.\\nHowever, I do some tests in isgReflection example and set doISG as false to remove the influence of Image Space Gathering. The following table shows the ray tracer’s performance depending on the kernel resolution:\\nray-tracing time\\n10241024          7.32ms\\n512512             3.46ms\\n256256             2.31ms\\n128128             2.04ms\\n6464               1.93ms\\n3232               1.85msIt’s calculated by averaging 1000 frames.  It doesn’t scale well and even in small kernel, raytracer pays not small price.I know optix may use some ray coherence things, but I still want to know is there some way to “make” it linear?My system is windows 7, visual studio 2012, gtx 980m. cuda 7 and optix 3.8.\\nAnd optix 3.9 gets the similar results.Easy, you’re not generating enough load on the GPU to keep all streaming multiprocessors busy with these tiny launch sizes and there is a constant software overhead per launch.\\nGo the other way and use bigger sizes and results should be more in line with your expectations.Have a search for “NVIDIA CUDA grids blocks threads explained” to see how much work the GPU can handle in parallel.Powered by Discourse, best viewed with JavaScript enabled',\n",
       " 'v-sync-tearing-problems-windows': 'Dear nvidia guys, take a look at this topicWow,  I thought I’d never experience this issue and now I have it (for the first time in my life actually).  mplayer -vo xv (which is a default) and -vo vdpau both exhibit noticeable video tearing in full screen mode.  mplayer -vo gl doesn’t have...The same same problems exist on windows. What the hell?\\nPeople who make this videocards are blind or what?\\nHow is it possible to make so stupid glitch?\\nThis is insane.Powered by Discourse, best viewed with JavaScript enabled',\n",
       " 'optix-6-5-hanging-turing-gpu-unless-enabling-rtprintf': 'The OptiX code I’m involved in developing causes Turing GPUs (or at least RTX 2060 and 2070) to hang with 100% GPU usage, but only if printing is disabled. If enabling printing, it works fine. It also works fine on Pascal cards, with or without printing enabled.The only rtPrintf statement in our programs is in an exception program, and exceptions are turned off when it hangs. With exceptions and printing turned on, I don’t get any indication of an exception being thrown.The problem seems to depend on the number of if statements (or similar) in the code. Commenting out some of these makes the code run. Of course diverging code has an adverse result on performance, but I wouldn’t expect it to cause the GPU to hang entirely. Unfortunately I’m not able to reduce the branchiness enough without affecting the result, at least not for all our projects. In one of them I’m able to get it to work by rewriting an any hit program for shadow rays so that it doesn’t call rtTerminateRay, but that might cost performance on other architectures and I’m afraid it might break anyway as soon as some other feature is added.Current driver version: 440.82 (also happened in 440.59), OS: OpenSUSE 15. All the OptiX samples run.Is there a known problem with diverging code causing freezes on the Turing architecture? Any known workarounds or ideas on how to troubleshoot it further?The OptiX core implementation resides inside the driver since that OptiX 6.5.0 version, but because you’re already on the most current Linux drivers 440.82, there isn’t anything newer to test.Which CUDA Toolkit version did you use to generate the PTX code?\\nAre you using GeometryTriangles primitives?From your descriptions this sounds very much like a device code compilation issue.\\nThat wouldn’t be possible to investigate further without a minimal complete reproducer in failing state.Note that the code generation can change considerably with exceptions enabled, which supports the compilation problem thesis.rtPrintf was notoriously buggy in some OptiX versions. You could try to use the CUDA native printf instead.\\nThat doesn’t need any of the OptiX print enables. Limiting it to specific launch indices would need to be done manually.With respect to the anyhit program, just in case, note that both the rtTerminateRay() and the rtIgnoreIntersection() functions immediately return. Make sure to update all your payload values before them.Not calling rtTerminateRay() for a visibility test ray can have drastic performance implications depending on the scene.Other things to test would be to check if all rays are valid. Means no NaN or INF values in the origin and directions, all directions normalized and never null-vector, 0.0f <= t_min < t_max.Thanks Detlef for your reply and suggestions!The CUDA version is 10.1.243.\\nWe do not use GeometryTriangles in our current implementation, but triangle meshes with intersection and BB programs.The payload was updated before rtTerminate(). In the case of rtIgnoreIntersection() we don’t want to update it. I will try to reintroduce the rtTerminate() if I can get it to work.\\nI will check the rays - I think I’ve checked that the directions of the primary rays are normalized, but I’m not sure about the origins. I haven’t checked the shadow rays and reflection/refraction rays yet, so I’ll get working on that. Also I’ll try using the CUDA printf.If none of this helps, I hope we can create a minimal reproducer next.We do not use GeometryTriangles in our current implementation, but triangle meshes with intersection and BB programs.Interesting, then you’re actually not using the RT core hardware triangle intersection on the RTX boards but only the hardware BVH traversal. Means there a huge potential to get more performance on the RTX boards.The OptiX Advanced Samples on https://forums.developer.nvidia.com/t/optix-advanced-samples-on-github/48410 were written against OptiX 5.1.0 and would run similarly with OptiX 6.5.0 on RTX boards.But if you say that all other SDK examples work, there is little to do about this from remote.\\nIn any case, this shouldn’t happen when everything else is correct.\\nLong term I would recommend to port to OptiX 7 before changing the existing renderer more.Interesting, then you’re actually not using the RT core hardware triangle intersection on the RTX boards but only the hardware BVH traversal. Means there a huge potential to get more performance on the RTX boards.Thanks, I’m aware of this and I’m sure we’ll want to fix it as soon as time allows. The same goes for porting to OptiX 7. We’ll probably try that before attempting to create a minimal reproducer. Checking the rays didn’t reveal anything strange, but we may need to check our intersection programs as long as we use them.Powered by Discourse, best viewed with JavaScript enabled',\n",
       " 'gl-point-size-max-possible-regression-in-driver-x86-64-304-51-for-gnu-linux': 'With driver NVIDIA-Linux-x86_64-285.05.09 the point size could be really big - as big (in pixels) as the screen in fact. I have shaders relying on this.Now with 304.51 driver,glGetFloatv(GL_POINT_SIZE_MAX, &max_point_size );sets max_point_size to:63.375000Is this intended, or is it a regression?I can verify that it’s also present in 304.60 (latest driver).Have you tried to use higher values to see if that is the actual limitation or if it is something that just went wrong with reading the limitation?I do not use Linux, so I cannot really help you any further … hope someone else comes along.Maybe you should try to post in this forum section instead?Linux DriverGood luck!Powered by Discourse, best viewed with JavaScript enabled',\n",
       " 'physx-ragdoll-issue': 'I used 3ds max build a ragdoll and exported it, then deserialized it in physx, also added animation function to ragdoll. Now some issues appear when I want to do a demo like this. You could use mouse select a bone of the ragdoll and pull it to some places， after that release it.When I add forces to the bone selected, and pull it into air, the bone will jitter or shake inabnormal way.\\nAnother issue is that when I used ball to hit the ragdoll, sometimes the ragdoll would crash. The bones of it would fly all over the scene.\\nAny advice? thanks！If you have a drive fighting to keep two bodies connected while a very large force tries to pull them apart you might get unstable behavior. Have you tried reducing the magnitude of the forces you add?I believe all the default physx joints have a breaking force. If they need to exert a larger force than this to hold the bodies together they instead “break” such that they never add any forces again. I guess it’s possible that hitting your ragdoll with a ball is sufficient to cause some joints to break. Consider increasing the breaking force.I’m not sure how your ragdoll is set up. If it’s using PxArticulation then some of what I’ve described won’t apply since the constraint solving works in a completely different way.Powered by Discourse, best viewed with JavaScript enabled',\n",
       " 'the-illegal-memory-access-error-on-cudamemcpy-function': 'Hello, I am confused now because of an error in my OPTIX code.My code launchs rays using optixLaunch function below.This optixLaunch function figures out and stores information on whether some rays hit any mesh in the device memory.And after optixLaunch function is finished, this information in the device memory is delivered to the host memory by cudaMemcpy function below.This works well for a small number of launchWidth.However, when the launchWidth becomes larger than about 500 million, the cudaMemcpy function show me an error: FATAL ERROR: An CUDA error has occurredcudaErrorIllegalAddress: an illegal memory access was encounteredFurthermore, after optixLaunch function is finished, not only cudaMemcpy function that transfers device memories written by the optixLaunch function, but also any other cudaMemcpy function that transfers memories irrelevant with optixLaunch function show me the same error.It seems like the optixLaunch function destroys the whole device memory.I recognize that the maximum number of launchWidth is 2^30, so I thought 500 million launchWidth should work well.Could you give me any advice on this problem?Thank you.P.S. I am using  RTX A6000 GPU and the device memory is sufficient.Hi @yongwankim,Can you explain a little more about what’s going on? Your OptiX launch is not showing any error, but the cudaMemcpy has an error? If your launch was successful and there were no errors, then this would tend to indicate that the parameters to cudaMemcpy are incorrect, either a bad pointer, or a misaligned pointer, or a bad size. Have you checked if there is a CUDA error immediately after the launch but before calling cudaMemcpy? Have you checked if your host & device pointers were corrupted before calling cudaMemcpy? What is the actual size of the memcpy?I see you are using a buffer of boolean values. How did you allocate and initialize this buffer? Is there a std::vector involved?When you say other cudaMemcpy calls have an error, these calls are in the same CUDA context I assume? Does your CUDA_ERROR_CHECK macro call cudaGetLastError()? Are there any asynchronous activities happening in your CUDA context / stream?–\\nDavid.Hi David,Thanks to you, I double checked which function makes the error.As a result, I have identified that precision problem of sqrt operator in optixLaunch function makes the error.Now this error has been solved.Thank you!Powered by Discourse, best viewed with JavaScript enabled',\n",
       " 'shadows-builders-problems': 'i am confused about my shadows.i use bvh/bvh as builder/traverser, cause this is the only builder making use of the boundingbox program and not building the bounding box over the triangle data its self. it seems that other builders arent able to use the boundingbox program. trbvh is ‘strongly considered for all dataset’, only for triangles?my primitives have 3 vertices and 3 normals and a biquadratic surface.\\nwith these primitives i made a icosahedron, looking like a sphere.\\ni have one light in front of the object. if i put shadows on, i ll get a shadow on the the front of the object faceing the light. i dont like that.\\nbut when i take only the primitives setting up the front of the icosahedron (looking like a half of a sphere), i ll get no shadows on these primitives. so this shadow is no self shadow.\\ni am confused of the behaviour of any hit. my material is the obj_material.cu (with phong.h) from the samples.\\nif i take a epsilon bigger than twice the radius of the icosahedron, there is no shadow at the front.if i take trbvh i have no shadows at the front, but ill get some holes concerning the wrong boundingboxes.\\nif i take noAccel i have the wrong shadow.tried some changes at the anyhit ray creating, but all seems to be set up right. first i thought the any hit ray are maybe sended into the wrong direction, but they arent.\\nwhere could be a mistake? are there known bugs?my fault. during some test i was sending the any hit rays in the wrong direction and forgot to undo this, thats why other tests have not been working.the code i ll get the wrong shadows.i have to check the any hit ray if they are in the tmin and tmaxorwhich way is better or are they the same?the bvh/bvh and the NoAccel builder need these checks. Trbvh/bvh dont. maybe these differences would be interesting in the Programming guide.**edit\\nthe any hit program calls phongShadowed.\\nRT_PROGRAM void any_hit_shadow()\\nwould look better in the code line 1.forget my last post. it kills all shadows. i better should delete it.i have managed to get the trbvh running with different boundingboxes. there seems to be a bug mentioned already here [url]Crash while validating a context - OptiX - NVIDIA Developer Forums\\ni use OptiX 3.6.3\\ni have changed the buffers name from vertex_buffer to vvertex_buffer and vindex_buffer to vvindex_buffer and suddenly trbvh works (without setting the triangle acceleration properties). my project based on Sample6. maybe there are some deeper dependencies, i didnt see. but for me its a bug.but i am disappointed that it didnt fix my shadow problems, which realy look like a bug aswell.\\nthe trbvh brings some more speed.\\n\\nshadowBugLightFromTop.PNG1290×999 42.6 KB\\n\\nshadowBugLightFromTop.PNG1290×999 42.6 KB\\ndoes anyone have an idea how to fix that shadow seen in PNG attached in the last post?\\nthis behaviour is acceleration independent.i have managed to get the trbvh running with different boundingboxes. there seems to be a bug mentioned already here https://devtalk.nvidia.com/default/topic/758617/crash-while-validating-a-context/\\ni use OptiX 3.6.3\\ni have changed the buffers name from vertex_buffer to vvertex_buffer and vindex_buffer to vvindex_buffer and suddenly trbvh works (without setting the triangle acceleration properties). my project based on Sample6. maybe there are some deeper dependencies, i didnt see. but for me its a bug.but i am disappointed that it didnt fix my shadow problems, which realy look like a bug aswell.\\nthe trbvh brings some more speed.Thanks for your solution. It bothers me couple of days, until I see your post. When I changed the vertex_buffer to vvertex_buffer and vindex_buffer to vvindex_buffer, Trbvh works…\\nBut I’m still not clear why this happens. Its a bug for Optix?Check out OptiX 3.8. We’ve fixed bugs with Trbvh and have also cleaned up stuff in our sample infrastructure related to the named vertex and index buffers. Note that the API has default values for the “vertex_buffer_name” and “index_buffer_name” properties of AS builders. So if your buffers have those defaults as names, or if you set those properties to your buffers’ names it will treat your buffers as triangles and build the BVH in a more optimal way for triangles. This is almost always what you want. But if you don’t want it you can always suppress it and build your BVH over AABBs using the bounding box program.I can’t see your shadow picture, but it could be that what’s going on here is due to splitting in the BVH build. Sometimes we can get much tighter bounds on a primitive by bounding it with two or more small boxes than one larger box. In this case the primitive is a child of all of the split boxes. Thus, the same ray can be intersected against the same triangle multiple times. If you’re doing cumulative things in your any hit program the splitting can affect your results. You may see a darker shadow on one side of the bounding box than the other.Your epsilon should be small, nowhere near the size of your scene.Powered by Discourse, best viewed with JavaScript enabled',\n",
       " 'cloudxr-vive-xr-elite-support': 'Dear Development Team,Do you know when Vive XR Elite support is planned for CloudXR 3.2 ?Best regards,MarcPowered by Discourse, best viewed with JavaScript enabled',\n",
       " 'use-mediafoundation-with-d3d11-to-read-video-file-d3dkmtwaitforverticalblankevent-exception': 'I read video file by use mediafoundation(IMFSourceReader), and i create d3d11 device and dxgi manager set to reader.\\nthen decoding every frame in loop.\\nthen I the debug output window print info like “Exception at 0x7ffe85ba4f99, code: 0xe06d7363: c++ exception, flag=0x1 in d3d11 D3DKMTWaitForVerticalBlankEvent.”\\nbut the app work well if i ignore this exception:The exception is only on nvidia gpu(My GPU is 1050 and 1030),I test the sample code in amd and intel GPU ,there are no expection output !!!.Code:\\n#include \\n#include \\n#include \\n#include <atlbase.h>\\n#include <d3d11.h>\\n#include <mfapi.h>\\n#include <mfidl.h>\\n#include <mfreadwrite.h>\\n#include <windows.h>#pragma comment(lib, “d3d11.lib”)\\n#pragma comment(lib, “mf.lib”)\\n#pragma comment(lib, “mfplat.lib”)\\n#pragma comment(lib, “mfreadwrite.lib”)\\n#pragma comment(lib, “mfuuid.lib”)#define ENABLE_HW_ACCELERATION\\n#define ENABLE_HW_DRIVERvoid handle_result(HRESULT hr)\\n{\\nif (SUCCEEDED(hr))\\nreturn;}int main(int argc, char *argv)\\n{\\nQCoreApplication a(argc, argv);\\nhandle_result(CoInitializeEx(nullptr, COINIT_APARTMENTTHREADED | COINIT_DISABLE_OLE1DDE));\\nhandle_result(MFStartup(MF_VERSION));}Powered by Discourse, best viewed with JavaScript enabled',\n",
       " 'launchparameters-containing-an-array-of-dynamic-size-lights-data': 'Hey there, I am trying to pass an array of LightData (LightData*) to my Launch Parameters. My host code initializes an std::vector<LightData> lightData with the corresponding information for each light, and then I pass it to my launch parameters by assigning launchParams.lights = lightData.data(). However, my device code keeps crashing because of an illegal memory access within this LightData array. Before I make the call to OptiXLaunch, I make sure to update the space allocated for the launch parameters on my device by freeing the device_ptr, then reallocating space of size sizeof(launchParams). Could this be because my pipeline’s module is set up by passing the pipelineLaunchParamsVariableName, and at that point it just sees a LightData*  in my launch parameter struct, not knowing how many elements the array will contain? What is the correct way to do this?Kind regards,\\nChuppaHowever, my device code keeps crashing because of an illegal memory access within this LightData array.The launchParams.lights must be a pointer to GPU device memory. You cannot just assign the host side array pointer to that. That is not visible by the GPU.Instead you need two more steps: Allocate the device memory with a CUDA runtime or driver API call to get a CUdeviceptr to a sufficiently large memory block.\\nThen you need to copy your lightData array data from host to device with a CUDA memcpy call.Here is example code doing exactly that:\\nCUDA Runtime API: https://github.com/NVIDIA/OptiX_Apps/blob/master/apps/intro_runtime/src/Application.cpp#L2145\\nCUDA Driver API: https://github.com/NVIDIA/OptiX_Apps/blob/master/apps/intro_driver/src/Application.cpp#L2169There is no “size()” call on these pointers, so you must also store the number of your light elements in another field in your launchParams.\\nThat happens here in my examples:\\nhttps://github.com/NVIDIA/OptiX_Apps/blob/master/apps/intro_driver/shaders/system_parameter.h#L65\\nhttps://github.com/NVIDIA/OptiX_Apps/blob/master/apps/intro_driver/src/Application.cpp#L2185A few lines further down that code, my host side SystemParameter structure is allocated on the device and then copied there:\\nhttps://github.com/NVIDIA/OptiX_Apps/blob/master/apps/intro_driver/src/Application.cpp#L2194\\nThat copy is done everytime anything is changing inside the structure.Then you should also be aware of the CUDA alignment requirements for vectors and arrays.\\nFollow the links in this post: https://forums.developer.nvidia.com/t/rtbuffer-indexing/167440/13This topic was automatically closed 14 days after the last reply. New replies are no longer allowed.Powered by Discourse, best viewed with JavaScript enabled',\n",
       " 'tesla-k40m-gpu-operation-mode': 'Running Ubuntu Server 14.04\\nOutput of nvidia-smi:Require these cards to run OpenGL and latest shaders, which they appear unable to do even though technical documentation mentions they support it.It does not appear that we are able to enable these cards to allow direct rendering, any help would be great.Hello,I am encoutering the same issue with K80 Tesla cards.\\nFollowing this NVidia article (https://devblogs.nvidia.com/parallelforall/interactive-supercomputing-in-situ-visualization-tesla-gpus/), I wanted to try using offline rendering on the K80, as it seems to be supported.While trying to query the gom value:When I try to change the value, I end up with the same errors:Could it come from the fact that I am using the graphics driver, with manual installation, and not the specialized Tesla drivers ? (I am using version 358.16)Thanks in advance for your answers !Actually, on K40 and K80, GOM is “All on” by default, this is why you can not change it.Powered by Discourse, best viewed with JavaScript enabled',\n",
       " 'clearing-srgb-swapchain-images': 'Hi,I’ve created swapchain with VK_FORMAT_R8G8B8A8_SRGB imageFormat. Later I clear it withVkClearColorValue clear_color = { { 0.5f, 0.5f, 0.5f, 1.f } }Here is what spec says (https://www.khronos.org/registry/vulkan/specs/1.0-wsi_extensions/xhtml/vkspec.html#clears-values):\\n“Floating point values are automatically converted to the format of the image, with the clear value being treated as linear if the image is sRGB.”So I’m expecting it to produce rgb(187,187,187) pixels. But with 365.19 drivers I get rgb(127,127,127) just like with VK_FORMAT_R8G8B8A8_UNORM.Does the image view used to clear have SRGB format set?Could you share a code snippet and/or get a repro app?Regards,Mathias SchottIt was my fault.vkGetPhysicalDeviceSurfaceFormatsKHR returns only BGRA formats, and there was an mistake in my format-choosing code. My code selected VK_FORMAT_B8G8R8A8_UNORM then I was trying to get something as close as possible to VK_FORMAT_R8G8B8A8_SRGB. And created swapchain was actually in UNORM format.With VK_FORMAT_B8G8R8A8_SRGB clear work as expected.Glad to hear that you got it to work :)MathiasPowered by Discourse, best viewed with JavaScript enabled',\n",
       " 'centos7-optix-error-a-supported-nvidia-gpu-could-not-be-found': \"I’m trying to setup an OptiX environment under CentOS 7. I’ve got a GeForce RTX 2080 Ti and I installed the Nvidia driver (version 418.56). I also installed Cuda-toolkit-10.1 and OptiX-SKD-6.0.0.When I run the Cuda samples, everything goes well (see code snippet 2), but when I run the OptiX samples I got the error : “OptiX error: A supported NVIDIA GPU could not be found”.I know there is already a thread about this error. But for him the problem came from the driver version, he was under 418.40 and he solved the problem by installing the version 418.56. Me, I am already under the version 418.56, so I guess the problem does not come from the driver.Also, I set the LD_LIBRARY_PATH to the OptiX libraries. And I tried with the CUDA_VISIBLE_DEVICES variable too.Here is the nvidia-smi command output:Here is the “deviceQuery” cuda sample output (only relevant info):Hi ABardoux,I’m not sure what’s wrong, but it looks like you tried to do the right things. Maybe the first thing to do is run one of the samples using strace and see which OptiX .so files it’s trying and failing to load. Also check which ones it does load, perhaps it’s picking one from somewhere you’re not expecting?–\\nDavid.Hi ABardoux,I just noticed that while you have driver version 418.56, you still have NVIDIA-SMI version 418.40, so perhaps you did not completely uninstall the old driver before installing the new driver? I was only able to get things working on Ubuntu after uninstalling all NVIDIA software and starting fresh.Hi,Thanks for your feedback.When I turned my computer on, all the graphical part was down. So I tried to re-install the driver, but I couldn’t because of a Linux header file conflict. I decided to uninstall the driver in order to re-install it from a clean base. It worked. But now I can’t re-install it, because the nvidia kernel modules are already loaded, and I can’t unload them. It seems it’s because the version 418.40 has installed (I don’t understand where it comes from), and everything is working (gnome, Cuda, OptiX).So the problem was probably due to a conflict between the driver v418.56 and NVIDIA-SMi v418.40, has you noticed it nljones, or because there was an error I didn’t notice when I installed the driver version 418.56.Anyway, is there a way to upgrade everything to the version 418.56 (or 418.74)?Thanks,\\nArnaudYou might find Ingo Wald’s notes on driver installation helpful: Installing the latest NVidia Driver, CUDA, and OptiX on Linux/Ubuntu 18.04 – Ingo's BlogSpecifically, he uses run level 3 to install the drivers so that the kernel modules aren’t loaded and can be replaced.–\\nDavid.Powered by Discourse, best viewed with JavaScript enabled\",\n",
       " 'unable-to-initialize-egl': 'HelloI am trying to run PoseCNN algorithm on a RTX 4090 based system with Nvidia Driver 525.85 (installed using run file) using a Docker Container image: Cuda 11.7.0 devel ubuntu 20.04 from docker hub.While running “python3 setup.py install” inside the dockerfile while image creation, the image creation fails:[13/14] RUN cd /deps/PoseCNN/lib/layers && python3 setup.py install     && cd /deps/PoseCNN/lib/utils && python3 setup.py build_ext --inplace     && cd /deps/PoseCNN/ycb_render && python3 setup.py develop && cd …/ && ./build.sh:\\n#0 1.346 No CUDA runtime is found, using CUDA_HOME=‘/usr/local/cuda’\\n#0 1.351 running install\\n#0 1.389 running bdist_egg…\\n…\\n#0 1.414 Traceback (most recent call last):\\n#0 1.414   File “setup.py”, line 8, in \\n#0 1.414     setup(\\n#0 1.414   File “/usr/lib/python3/dist-packages/setuptools/init.py”, line 144, in setup\\n#0 1.414     return distutils.core.setup(**attrs)\\n#0 1.414   File “/usr/lib/python3.8/distutils/core.py”, line 148, in setup\\n#0 1.414     dist.run_commands()\\n#0 1.414   File “/usr/lib/python3.8/distutils/dist.py”, line 966, in run_commands\\n#0 1.414     self.run_command(cmd)\\n#0 1.414   File “/usr/lib/python3.8/distutils/dist.py”, line 985, in run_command\\n#0 1.414     cmd_obj.run()\\n#0 1.414   File “/usr/lib/python3/dist-packages/setuptools/command/install.py”, line 67, in run\\n#0 1.414     self.do_egg_install()\\n#0 1.414   File “/usr/lib/python3/dist-packages/setuptools/command/install.py”, line 109, in do_egg_install\\n#0 1.414     self.run_command(‘bdist_egg’)\\n#0 1.414   File “/usr/lib/python3.8/distutils/cmd.py”, line 313, in run_command\\n#0 1.414     self.distribution.run_command(command)\\n#0 1.414   File “/usr/lib/python3.8/distutils/dist.py”, line 985, in run_command\\n#0 1.414     cmd_obj.run()\\n#0 1.414   File “/usr/lib/python3/dist-packages/setuptools/command/bdist_egg.py”, line 172, in run\\n#0 1.414     cmd = self.call_command(‘install_lib’, warn_dir=0)\\n#0 1.414   File “/usr/lib/python3/dist-packages/setuptools/command/bdist_egg.py”, line 158, in call_command\\n#0 1.414     self.run_command(cmdname)\\n#0 1.414   File “/usr/lib/python3.8/distutils/cmd.py”, line 313, in run_command\\n#0 1.414     self.distribution.run_command(command)\\n#0 1.414   File “/usr/lib/python3.8/distutils/dist.py”, line 985, in run_command\\n#0 1.414     cmd_obj.run()\\n#0 1.414   File “/usr/lib/python3/dist-packages/setuptools/command/install_lib.py”, line 23, in run\\n#0 1.414     self.build()\\n#0 1.414   File “/usr/lib/python3.8/distutils/command/install_lib.py”, line 109, in build\\n#0 1.414     self.run_command(‘build_ext’)\\n#0 1.414   File “/usr/lib/python3.8/distutils/cmd.py”, line 313, in run_command\\n#0 1.415     self.distribution.run_command(command)\\n#0 1.415   File “/usr/lib/python3.8/distutils/dist.py”, line 985, in run_command\\n#0 1.415     cmd_obj.run()\\n#0 1.415   File “/usr/lib/python3/dist-packages/setuptools/command/build_ext.py”, line 87, in run\\n#0 1.415     _build_ext.run(self)\\n#0 1.415   File “/usr/local/lib/python3.8/dist-packages/Cython/Distutils/old_build_ext.py”, line 186, in run\\n#0 1.415     _build_ext.build_ext.run(self)\\n#0 1.415   File “/usr/lib/python3.8/distutils/command/build_ext.py”, line 340, in run\\n#0 1.415     self.build_extensions()\\n#0 1.415   File “/usr/local/lib/python3.8/dist-packages/torch/utils/cpp_extension.py”, line 843, in build_extensions\\n#0 1.415     build_ext.build_extensions(self)\\n#0 1.415   File “/usr/local/lib/python3.8/dist-packages/Cython/Distutils/old_build_ext.py”, line 195, in build_extensions\\n#0 1.415     _build_ext.build_ext.build_extensions(self)\\n#0 1.415   File “/usr/lib/python3.8/distutils/command/build_ext.py”, line 449, in build_extensions\\n#0 1.415     self._build_extensions_serial()\\n#0 1.415   File “/usr/lib/python3.8/distutils/command/build_ext.py”, line 474, in _build_extensions_serial\\n#0 1.415     self.build_extension(ext)\\n#0 1.415   File “/usr/lib/python3/dist-packages/setuptools/command/build_ext.py”, line 208, in build_extension\\n#0 1.415     _build_ext.build_extension(self, ext)\\n#0 1.415   File “/usr/lib/python3.8/distutils/command/build_ext.py”, line 528, in build_extension\\n#0 1.415     objects = self.compiler.compile(sources,\\n#0 1.415   File “/usr/lib/python3.8/distutils/ccompiler.py”, line 574, in compile\\n#0 1.415     self._compile(obj, src, ext, cc_args, extra_postargs, pp_opts)\\n#0 1.415   File “/usr/local/lib/python3.8/dist-packages/torch/utils/cpp_extension.py”, line 581, in unix_wrap_single_compile\\n#0 1.415     cflags = unix_cuda_flags(cflags)\\n#0 1.415   File “/usr/local/lib/python3.8/dist-packages/torch/utils/cpp_extension.py”, line 548, in unix_cuda_flags\\n#0 1.415     cflags + _get_cuda_arch_flags(cflags))\\n#0 1.415   File “/usr/local/lib/python3.8/dist-packages/torch/utils/cpp_extension.py”, line 1780, in _get_cuda_arch_flags\\n#0 1.416     arch_list[-1] += ‘+PTX’\\n#0 1.416 IndexError: list index out of rangeHowever, Entering the same command inside a container instead of while building the image works. And I get no CUDA error.\\nBut now if the code proceeds, I face the error :Let’s use 2 GPUs!  # That means it is detecting 2 GPUs in the system\\nloading 3D models\\nlibEGL warning: DRI2: failed to create dri screen\\nlibEGL warning: DRI2: failed to create dri screen\\nUnable to initialize EGL\\nCommand ‘[’/deps/PoseCNN/tools/…/ycb_render/build/test_device’, ‘0’]’ returned non-zero exit status 1.\\nlibEGL warning: DRI2: failed to create dri screen\\nlibEGL warning: DRI2: failed to create dri screen\\nUnable to initialize EGL\\nCommand ‘[’/deps/PoseCNN/tools/…/ycb_render/build/test_device’, ‘1’]’ returned non-zero exit status 1.\\nTraceback (most recent call last):\\nFile “./tools/train_net.py”, line 141, in \\ncfg.renderer = YCBRenderer(width=cfg.TRAIN.SYN_WIDTH, height=cfg.TRAIN.SYN_HEIGHT, render_marker=False)\\nFile “/deps/PoseCNN/tools/…/ycb_render/ycb_renderer.py”, line 88, in _ init _\\nself.r = CppYCBRenderer.CppYCBRenderer(width, height, get_available_devices()[gpu_id])\\nIndexError: list index out of rangewhich means its not able to identify devices during the build stage of the project and this function(get_available_devices()) is not able to identify the gpusNVIDIA-SMI 525.85.05    Driver Version: 525.85.05    CUDA Version: 12.0  …nvcc: NVIDIA (R) Cuda compiler driver\\nCopyright (c) 2005-2022 NVIDIA Corporation\\nBuilt on Wed_Jun__8_16:49:14_PDT_2022\\nCuda compilation tools, release 11.7, V11.7.99\\nBuild cuda_11.7.r11.7/compiler.31442593_0Please share what could be the problem here. I have tried multiple images and installing multiple libraries but still there seems to be a problem in CUDA or OpenGL.Thank youHi there dheeraj.singh,A couple of notes that might help you along.First of all, the standard CUDA containers like the one you are using do not support EGL and will not support GLX. Both of which I think are necessary for the rendering part of the PoseCNN code from NVLabs. That means you should look for the cudagl docker images. Development is on hold at the moment, so the latest is CUDA 11.4 on Ubuntu 20.04.Regarding the CUDA not found in the first part, that is dependent on how you built your docker image. I am not a docker expert, so I am not able to help with that, but rebuilding the image might require additional configuration for the new image to correctly reference CUDA.I suppose you know all our resources on Container images?I hope this helps!Hello MarkusThank you very much for replying. I did not realise there are GL based image, now I have used cudagl image and the problem seems to be solved although with a reload:Let’s use 2 GPUs!\\nloading 3D models\\nlibEGL warning: DRI2: failed to create dri screen\\nlibEGL warning: DRI2: failed to create dri screen\\nUnable to initialize EGL\\nCommand ‘[’/deps/PoseCNN/tools/…/ycb_render/build/test_device’, ‘1’]’ returned non-zero exit status 1.\\nlibEGL warning: DRI2: failed to create dri screen\\nlibEGL warning: DRI2: failed to create dri screen\\nUnable to initialize EGL\\nCommand ‘[’/deps/PoseCNN/tools/…/ycb_render/build/test_device’, ‘2’]’ returned non-zero exit status 1.\\nnumber of devices found 4\\nLoaded EGL 1.5 after reload.And then the code runs forwardRegarding the first problem, I found out the issue that when running the setup.py file inside the Dockerfile, the system does not have any information about the CUDA architecture, so we need to specify the Compute Capability(CC) of the GPU in use, for me its RTX 4090 which was not present in the list given in the link provided in the solution of this Github issue: cuda does not install · Issue #71 · pytorch/extension-cpp · GitHub\\nHowever, I randomly considered the CC as 8.6(most recent) and it worked\\nSo i added this line in the DockerfileARG TORCH_CUDA_ARCH_LIST=“8.6+PTX”Thank you for your help!Great to hear that you addressed this and thank you for sharing your solution!This topic was automatically closed 14 days after the last reply. New replies are no longer allowed.Powered by Discourse, best viewed with JavaScript enabled',\n",
       " 'turing-t400-intermittently-recognising-second-monitor': 'I have a Win 11 dual monitor setup (Asus Pro Art + Samsung SyncMaster) Asus is primary display. Recently it has developed problems recognising the Asus in spite of doing a clean re-install of the Nvidia driver.It only intermittently recognises two monitors but will sometimes belatedly (after 3-4 mins) recognise the AsusWindows display  setup tells me I only have 1 monitor as does Nvidia control panel when the problem ocurrsSetup has previously worked perfectly for several monthsPowered by Discourse, best viewed with JavaScript enabled',\n",
       " 'can-the-optix6-5-support-geforce-rtx3070-card-in-windows': 'I want to know whether the Optix6.5 support Geforce RTX3070 card in windows10?Yes.\\nThe OptiX 6.5 and newer core functionality ships with the display driver. Any driver supporting the Ampere GPU architecture should be new enough.\\nPlease always refer to the OptiX Release Notes (link directly below the download button) for each individual OptiX SDK before setting up a development environment.I would not recommend to use OptiX versions before OptiX 7 for new projects though.\\nPlease search the various topics on this sub-forum which explain the benefits of the newer API.Powered by Discourse, best viewed with JavaScript enabled',\n",
       " 'pinhole-camera-model-from-the-tutorial-examples-has-a-bug': 'I would like to pick a fight with the pinhole_camera function shipped with the OptiX SDK examples ;)Round 1:Let’s have a look a the functionEspecially, these lines are my target:The variable “d” should go from -1 to +1 to cover the whole horizontal/vertical FoV. The issue is that the function won’t return a range from -1 to +1 but a -1 to +0.9XXX dependent on the screen width/height. The larger the screen, the smaller the error. The reason is that the screen variable goes from “1” to “width/height” and the launch_index range goes from “0” to “width-1 / height-1”. This means that the whole FoV orientation gets a little shift by one pixel since it cannot reach +1.The correct calculation should go like this in my opinion:With this change, we get a perfect distribution from “-1” to “+1”.\\nAre my calculations correct or did I miss something?That calculates the coordinate of the lower left corner of the pixel.You would normally add 0.5f to the pixel coordinate to hit the pixel center or add a two dimensional offset in the range [0.0f, 1.0f) to the pixel coordinate for progressive sampling over the pixel area which fills the whole screen coordinate range then.Examples for both cases here:\\n[url]optix_advanced_samples/raygeneration.cu at master · nvpro-samples/optix_advanced_samples · GitHub\\n[url]optix_advanced_samples/lens_shader.cu at master · nvpro-samples/optix_advanced_samples · GitHubWhen reading the documentation from your example it states a range from [-1, 1]. When I add a 0.5f offset, it won’t reach +1 since fragment won’t reach screen maximum value. At least it will be uniformly distributed since both positive and negative max will reach the same value.Example for 800x600 screen with launch_index=799/X and 0/X:So my suggestion is:Of course sampling a single ray inside the center of a pixel is not reaching the screen rectangle coordinates at 0, 0 and width, height, because the samples are at the fragment location half a pixel to the inside of the full rectangle, which is the right thing to do when only shooting a single ray per pixel.\\nThis is the same as if you would rasterize that screen area. The center of the pixel is where your attributes get interpolated for filled primitives.You will reach the outer most edges only when sampling the whole pixel area as shown in the progressive renderer of the second example link.\\nThat will automatically do antialiasing with the samples per pixel amount over the progressive accumulation of the rendered image.The OptiX SDK example samples at the bottom left corner of each pixel. It should have added 0.5f to sample in the pixel center. That’s all.Perfect thank you this is the answer I was expecting!\\nSince the example is sampling only the bottom left corner, you get an offset by the half of the pixel which makes the result “incorrect” by this small amount.Case closed!All the best,\\nJakubPowered by Discourse, best viewed with JavaScript enabled',\n",
       " 'differential-lock': 'Hello,I am working for an oil truck simulator, and I need the truck to have a differential lock (in case of driving in mud / sand). I use a PxVehicleNoDrive in order to command the torque on the wheels, but I don’t know if it is possible to add contraints on wheels, in order to force them to have the same rotation speed.\\nDid I miss something, or is it not possible ?\\nThank you.PxVehicleNoDrive doesn’t provide a differential.  The only types that provide an editable differential are PxVehicleDrive4W and PxVehicleDriveNW.  PxVehicleDriveTank has a very specific type of differential that is hard-coded and not editable.If you want to carry on using PxVehicleNoDrive then you will probably need to implement the differential yourself. You can do that by a combination of PxVehicleWheels::setWheelRotationSpeed and by applying less drive torque to the slipping wheel.You could try something more sophisticated than what I described above. You could look at the tank differential code in PxVehicleUpdate.cpp and attempt to replicate something like that outside the sdk to calculate the drive torque that leads to a differential lock.Thank you for the answer.\\nI will check that code to get inspired.Powered by Discourse, best viewed with JavaScript enabled',\n",
       " 'solved-stack-overflow-if-i-use-acceleration-structures-with-complex-trees-many-levels': 'Hi, I tried to use “Bvh” for Group Node for days but I failed.My Optix hierarchy it is created as follow: I have three layers of Group Objects: MainNode, TOptixGroupObject(GLItem) and TOptixGroupObject(GLMesh).Under the TOptixGroupObject(GLMesh) node, the last Group Objects, I have one Transform Object then a GeometryGroup, then a GeometryInstance finally a Geometry Object (classical structure).I’ve created Accelerators for Group Objects and GeometryGroup Objects.The “Bvh” works fine for GeometryGroup Objects (trinagle meshes) but not for Group Objects. The only way to obtain a valid image is to use “NoAccel” for Group Objects (still using “Bvh” for GeometryGroup Objects).If I use “Bvh” acceleration type for Group Objects, I get only black boxes (the exact size of the bounding boxes of the GeometryGroup Objects)See the images below.P.S. I’m using Optix 3.5.1 32 bit, Cuda 6.0 (or 5.5) with a GTX 460M cardThank youA. Delle[EDIT] More updated informations HERE\\nOptixRender_NoAccel.jpg1552×842 88.4 KB\\n\\n\\nOptixRender_Bvh.jpg1552×842 26.8 KB\\n\\n\\nOptixTreeStructure.png798×598 19.1 KB\\n\\n\\nTreeStructure.png1920×1080 98.7 KB\\n\\nOptixRender_NoAccel.jpg1552×842 88.4 KB\\n\\nOptixRender_Bvh.jpg1552×842 26.8 KB\\nAccording to the release notes, OptiX 3.5.1 supports CUDA up to version 5.5, maybe CUDA 6 is causing the problem?According to the release notes, OptiX 3.5.1 supports CUDA up to version 5.5, maybe CUDA 6 is causing the problem?Thank you.I’ve just tried to install the NVidia Drivers 320.49 WHQL (Cuda 5.5) without solving.I get the same exact images (same behavior)I’m probably missing something but I cant figure it out what…you sure you also switch the traversal algorithm?you sure you also switch the traversal algorithm?Yes ‘Bvh’ for both…I don’t set “refine” and “refit” values for BVH traversal algorithm (using default value) maybe in the afternoon (I’m in Italy) I’ll do some test on this…[UPDATE] No acceleration type different from “NoAccel” works, every acceleration different from “NoAccel” render only black bounding boxes…Thxyou sure you also switch the traversal algorithm?I’ve seen that in you code that you set the accelerators only to GeometryGroups, in my case I use them for GeometryGroups and for “simple” Groups (no geometry only tree structure).In my case GeometryGroups work (any accelerator) but Groups don’t (black BBox) maybe its an Optix bug?ADyou sure you also switch the traversal algorithm?I’ve seen, in your code,  that you set the accelerators only to GeometryGroups, in my case I use them for GeometryGroups and for “simple” Groups (no geometry only tree structure).In my case GeometryGroups work (any accelerator) but Groups don’t (black BBox) maybe its an Optix bug?ADI don’t use groups at all, so i can’t accelerate them :)did you check out sample7?\\nit’s possible to switch between acceleration enabled and not enabled…did you check out sample 7 ? It’s possible to switch between acceleration enabled and not enabled…Thank you, I’ll try to change the tree structureAdded exception handling to my code.The problem was due to “RT_EXCEPTION_STACK_OVERFLOW”.I’ve made some test to get the minimal configuration that cause the Problem (exception if I use an acceleration structure):If I use just one top Group object with GeometryGroups children it works (“bvh” accel on for Group Accelerator)If I use one top Group object with Tranform Objects children each with its GeometryGroup child, it doesn’t work (with “bvh” I get the exception, with “NoAccel” works fine)If I use one top Group object with Groups Objects children each with its GeometryGroup child, it doesn’t work (with “bvh” I get the exception, with “NoAccel” works fine)Somewhat it is connected with the depth of the Tree.ADIf you get a stack overflow exception, you should increase the stack size.See “3.1.3. Global State” in the OptiX Programming guide.If you get a stack overflow exception, you should increase the stack size.See “3.1.3. Global State” in the OptiX Programming guide.Works !Changing the Optix Stack Size from 1024 (default size) to 2048 solved…Thanks a lot!Works !Changing the Optix Stack Size from 1024 (default size) to 2048 solved…Thanks a lot!As a design note, the programming guide also recommends minimizing the stack size:Setting a large stack size will consume GPU device memory. Try to minimize the stack as much as possible. Start with a small stack and with the use of an exception program that will make it obvious you have exceeded your memory, increase the stack size until the stack is sufficiently largeAnd this is pretty easy to do:But if you’re not worried about using too much GPU memory, no biggy…OK, I’ll try to increment the Stack little by little.ADPowered by Discourse, best viewed with JavaScript enabled',\n",
       " 'how-to-configure-xserver-and-xvnc-for-the-server-with-rtx5000': 'How to configure xserver and xvnc for the server with rtx5000 (no display); after I configure, xvnc will log in to the black screPowered by Discourse, best viewed with JavaScript enabled',\n",
       " 'questions-about-physx': 'Hi every one,I have some questions about Physx, and which bother me very muchwould you please explain to me if you know? Thanks a lot!to combine Model to the Skeleton ), so could it support the Skeletal Model for Physique?seperated Skeleton, is there any seperated Animation required?Could the flesible body with size such as hair or sleeves represent better effects?What kind of models are suitable to be produced to flexible body?If we buy the authority of APEX, can we get the support about the editing tool andproducing process? Is there any training provided to the artists?Please contact me by my mail: zhzhrr@hotmail.com if you know. Thanks a lot!And Happy Valentin’s Day!If we buy the authority of APEX, can we get the support about the editing tool andproducing process?It is better to send licensing questions to PhysXlicensing@nvidia.comMaybe they’ll answer others as well.Powered by Discourse, best viewed with JavaScript enabled',\n",
       " 'video-codec-sdk-copytodeviceframe-data-rate-only-1-8gb-s': 'Hello,I compiled the samples from Video_Codec_SDK_11.1.5, and tried the example AppEncCuda on 3280x1460 nv12 image frames, generated using:ffmpeg -f lavfi -i testsrc2=s=3208x1460,format=nv12 -vframes 1000 sample.yuvI timed the operation: CopyToDeviceFrame in the function EncodeCuda, and calculated the data rate of copying frame from cpu the gpu, the data rate is only 1.8 GB/s (it took about 4000us to copy a frame of size 6.8MB) on a 3090 GPU, whereas encoding and copy data back EncodeFrame  only took about 50us.I am surprised at how slow copying data to gpu is. Is there a way to speed up this operation? We are trying to encode 4k camera frames at 210 frame/s, and right now the bottleneck seems to be the data transferring from cpu to gpu.It turns out I have been timing things wrong. It actually only takes 250us to transfer the data from cpu to gpu, which equals to a bandwidth about 21Gbytes/s. The bulk of time spent is actually in the encoding itself, which makes more sense. So I think this problem is resolved.Powered by Discourse, best viewed with JavaScript enabled',\n",
       " 'get-mass-matrix-and-collision-coordinates-in-local-object-coordinates': 'HiI have two questions about collisions in PhysX. I’m currently implementing an algorithm for haptics, and therefore need the mass matrix of a physX object. I use physx::PxRigidStatic and physx::PxRigidDynamic objects. Is there a function that gives me the mass matrix?And how do I get the collision points in the local coordinate system of the object, and not in world coordinates?Thanks,\\nRobertFor the mass matrix I have no clue, you could probably calculate one. But I’m to much of a rookie at mechanics to even know where to start.Transformation on the other hand. You just need to take the world matrix of the object, which local space you want the point in. Then transform the point with the inverted world matrix.Thanks for the reply. So you mean this?physx::PxShape *shape = …\\nshape->getLocalPose().transformInv(physx::PxVec3(x, y, z));Or is this the wrong transformation?Almost, should be:physx::PxShape *shape = …\\nshape->getGlobalPose().transformInv(physx::PxVec3(x, y, z));Then the Vector will be in the shapes local space.The mass matrix does not exist, you probably mean the inertia matrix or just the mass (or both depending on your intentions). As you mention to implement something for haptics, you probably want both to set controller parameters (e.g. Stiffness). Then you need the mass for tranlational control parameters and the inertia for rotational control parameters. For both you can find the functions in the physx documentation. Note that the inertia “matrix” provides you a PxVec3 with only the diagonal terms of the matrix (don’t bother on the of diagonal terms, you don’t need, want and have them)Powered by Discourse, best viewed with JavaScript enabled',\n",
       " 'deploy-jetson-nano-in-field': 'Hi,How to install Jetson nano in field practice by original hardware device with secured software ?  Once making sure the Python code and related software package ready, how to secured the code inside SDcard ?  Moreover, could build up the “Power-on run” environment even without Ubuntu ? I expect to run Nano like a simple microprocessor like 8051.thanksjasperPowered by Discourse, best viewed with JavaScript enabled',\n",
       " 'using-the-nvidia-encoder-sdk': \"I’m not sure if this is the correct forum to post, I’m new in NVidia Development. Windows Media Foundation can use the Video Codec SDK but not in 10-bit RGB, so I want to create my own Media Foundation Transform to be then used in my professional Windows Video Sequencer in order to create HEVC 10-bit HDR videos.I’m following advice from NVENC Video Encoder API Programming Guide :: NVIDIA Video Codec SDK DocumentationI’m trying at this time to use the H264 codec in the slightest configuration possible to see if it works. My problem is that, when locking the output bitstream buffer, I receive the error NV_ENC_ERR_INVALID_DEVICE.Code resume:This last call fails. What did I miss so far?Thanks a lot.Now something weird. The method returns failure, but it actually succeeds. The buffer is locked and I 'm able to copy bytes and unlock it.\\nWhat’s happening?Powered by Discourse, best viewed with JavaScript enabled\",\n",
       " 'cuda-opengl-and-net-interop': 'The following c++ code executes fine when invoked from a native C++ app.However, when I put that c++ code in a dll and invoke it from a C# application via interop it works fine up to line 28 where it fails with CUresult = CUDA_ERROR_INVALID_VALUE.In both cases, gl_pbo[0] = 1 prior to line 28.Any suggestions as to what might be wrong or how I might debug/diagnose the problem?Note, this is a abstracted version of my original application, simplified to illustrate the core problem\\n(Most is from Nvidia Video Decode OpenGL sample) .Using:Cheers, Wayne.Powered by Discourse, best viewed with JavaScript enabled',\n",
       " 'directx-12-and-nvidia-hardware-issue-in-case-of-busy-vieomemory-at-game-start': 'Dear NVidia,We are 1C Games Studios - developers of the IL-2 Sturmovik gaming aviation simulator. Now we’re working on the new iteration of our graphics engine for our next project. One of the major changes in it - is the migration to the DirectX 12 API. We have met several issues this way and one of them - we can’t pass without your assistance while it is hardware dependent. So we ask for your assistance or advice.\\nIssue is that if video memory is severely in-use (by internet browser for instance) in the moment before game start - we receive major graphical artefacts in the game (look for the video by the link below).\\nObservations and notes:So please assist us in solving this issue please. At the moment we’re out of ideas what we should change to avoid those artefacts on NVidia videocards.With Regards\\nDaniel Tuseyev\\nIL-2 ProducerPowered by Discourse, best viewed with JavaScript enabled',\n",
       " 'vkdevicelost-4-error-message-keeps-throwing-out': 'OS: Windows 10\\nVulkan Version: 1.0.65.0\\nNvidia driver version: 388.43\\nGPU: 750tiI’m working on a project which is to build a high-performance rendering server based on Vulkan. The rendering server is multi-threaded in order to process rendering requests sent by users. Initially, for each unique model, an exclusive logical device is created and taken care by one rendering thread. Basically, it works fine until we found that there seems to be a limitation on maximum number of devices that can be created. And this limitation varies from device to device (My tested results: 750ti [Windows10 driver] → Max = ~81, on Titan Xp [Linux driver] → Max = ~35). In order to overcome this pitfall, we tried to create only one single logical device which is shared by every 3D model rendering threads. Every model has their own command buffer which is built in multi-threaded fashion and submitted to one graphics queue whenever a render request is received. Locks are heavily used in drawCall, buildSecondBuffer and uploadModelData stages in order to prevent from race conditions. But it doesn’t work this time. Error msg “VkDeviceLost-4” keeps throwing out when multiple rendering requests received at the same time.According to Vulkan Specification:Due to patented issue, I cannot show you the source code(~5000 lines). But still, can any of you guys got any idea about this issue? What could potentially cause this problem? Does anyone else have the same issue? Really appreciate it if you can help me! Thanks in advance!Powered by Discourse, best viewed with JavaScript enabled',\n",
       " 'htc-desire-610-silver-66942': 'HTC Desire 610 on EE pay as you go.Great for music with BoomSound and quick to update with BlinkFeed.Go for it on the UK’s\\nbiggest and fastest 4G network.Battery Standby time (up to)650  hours,Talk Time (Up to)15.8  hours,Capacity 2040  mAh.\\nAudio supported,TV ouput,Video streaming,Dolby mobile supported,Java.Memory Internal memory 8.0 GB,External memory (Up to)\\n128.0 GB,External memory type Micro-SD,CPU Qualcomm MSM8226, CPU Cores Quad Core,CPU speed 1200.0  MHz.Size & Width Height\\n143  mm,Width 71  mm ,Depth 10  mm,Weight 146 g.Camera Video & Photo Main camera 8.0  mp,Screen resolution 540x960\",Flash\\nFocus,Video messaging,Connectivity,GPS receiver,Wi-Fi 802.11 b/g/n.Bluetooth.for more info Visit: http://www.phones4u.co.ukPowered by Discourse, best viewed with JavaScript enabled',\n",
       " 'does-optix-support-multi-thread-host-application': 'Hi,My project need to update mesh vertices each frame, so I have to ‘mark Dirty’ to update accelerate data structures every frame. When I use visual profiler to optimize the program, I realized there is a long GPU idle time, more than 20ms, after calling the Optix ‘launch’. I’ve tried different accelerator/travellers, but no big different.I’m wondering is it possible to use a separate host thread update scene by calling ‘launch(0, 0)’, then I can utilize the GPU idle time to do some other CUDA based computation? And how much benefit can I get from it?Straight from the OptiX Programming Guide, last paragraph before the appendix:\\n“Currently, OptiX is not guaranteed to be thread-safe. While it may be successful in some applications to use OptiX contexts in different host threads, it may fail in others. OptiX should therefore only be used from within a single host thread.”The separate launch(0,0) wouldn’t free the GPU anyways if the acceleration structure builder is running on the GPU.For maximum acceleration building performance try Lbvh.\\nIf you have rtPrintf() in your OptiX programs, benchmark without as well.Hi Detlef,Thanks a lot for the helpful reply. I’v tried Lbvh/Bvh accelerator today, that seems to be 20~30% faster than other accelerator. That’s really cool!And as you said, the GPU are not free when building the acceleration structure, there are a lot host to device memory copy and GPU computation during the structure building period. Then if I move some computation back to the host, and make it simultaneously with the launch(0, 0), maybe I can ‘hide’ the time cost. Am I right?The increased acceleration structure building speed of the Lbvh unfortunately comes with a slower rendering performance, but if the acceleration structure building performance is paramount, it’s the best choice.\\nThere is an additional method to increase the acceleration structure building speed of animated models even more by using refitting.Please have a look into this year’s GTC presentations.\\nGo here: GTC 2022: #1 AI Conference\\nand search for “ray tracing”\\nAs OptiX beginner watch the “GPU Ray Tracing Using OptiX” presentation and for optimizations watch the “Advanced OptiX Programming and Optimization”. Minutes 22:00-31:00 explain the refitting.Yes, you might try to do some tasks in parallel on the CPU, but note that OptiX can also do some tasks with multi-threading. Search the reference guide for RT_CONTEXT_ATTRIBUTE_CPU_NUM_THREADS.Powered by Discourse, best viewed with JavaScript enabled',\n",
       " 'apply-rendering-jitter-for-taa-without-projection-matrix': 'I’m trying to implement temporal anti-aliasing for an application. It’s an odd application where I can intercept and modify “fixed function” properties of a pipeline like the viewport, but I have no way to discern or modify the projection matrix. So I’m trying to figure out if there’s a way to apply rendering jitter for TAA without the projection matrix.\\nFor my first attempt I applied the offset to the viewport, which did cause the screen to jitter, but the offset is different at different depths, which doesn’t help.\\nNow I’m trying to apply the jitter to the sample location, which I think should work exactly the way that TAA expects, however I don’t actually see any jitter being applied to the rendering.\\nThis is what I’m doing at pipeline creation:The multisample and dynamic state infos are properly chained to the pipeline creation info.\\nThen before drawing I do:And I know from debugging that the value of jitter is updating from frame to frame the way I expect it to with this pattern:I also fixed any errors related to using custom sample sample locations with depth buffers.\\nSo is there something I’m missing that would cause the pipeline to ignore sample locations? Or am I completely misunderstanding the way “MSAA” sample locations work, and this won’t apply an offset to the rendered image?I’ve figured out that the custom sample locations are working; however, not in the way I need them to work. I’ve modified the SaschaWillems multisampling example and I can see that the edges of the object do shimmer, but the textures appear to be static. Is it normal that MSAA sample locations don’t affect the fragment inputs, like texture UVs?\\nAha, the fragment inputs need the sample decorator.Powered by Discourse, best viewed with JavaScript enabled',\n",
       " 'havok-and-physx-question': 'can the engine load animation data for models\\\\behaviors\\\\collision files like Havok? like .hkxPowered by Discourse, best viewed with JavaScript enabled',\n",
       " 'glvertexattribpointer-crash': 'I got a crash that puzzled me for some days. Crash frames as follows:#0  0x00007ffff491e337 in raise () from /lib64/libc.so.6\\n#1  0x00007ffff491fa28 in abort () from /lib64/libc.so.6\\n#2  0x00007ffff4960e87 in __libc_message () from /lib64/libc.so.6\\n#3  0x00007ffff4969679 in _int_free () from /lib64/libc.so.6\\n#4  0x00007fff1716cfdb in ?? () from /lib64/libnvidia-eglcore.so.384.66\\n#5  0x00007fff1716d35f in ?? () from /lib64/libnvidia-eglcore.so.384.66\\n#6  0x0000000000af3974 in mt::VertexArrayDetail::bindVertexBuffer (attrs=…, context=0x7fff642a3830, this=) at …/…/…/src/private/src/engine/geometry/VertexArray.cpp:65(‘bindVertexBuffer’ call simply ‘glVertexAttribPointer’)I have checked the details of vertex attributes, everything is OK. But WHY it crashed at ‘glVertexAttribPointer’?Does anyone have some ideas about this crash?My computer enviroment as follows:\\nOS : CentOS 7.0\\nGPU:\\n\\n截屏2020-05-11上午11.15.121696×442 172 KB\\nPowered by Discourse, best viewed with JavaScript enabled',\n",
       " 'programming-api-documentation-for-video-codec-sdk-11-1-5': 'Where do I find API documentation for NVEncode functions at the SDK 11.1.5 level? The programming guide is way too vague and the examples look a bit complicated. All I’m looking for is to convert a set of image buffers in RGB format into a video format supported by this SDK.Hi @drwootton1 ,did you check out the Online documentation for the SDK? The programming guide there is explicit and detailed, and admittedly rather technical by necessity.And looking at the examples in the SDK they are written as simple as possible. If you want an additional level of abstraction (or simplification), are you aware of the Python bindings for NVENC? Have a look at the Video Processing Framework on github.I hope this helps.Powered by Discourse, best viewed with JavaScript enabled',\n",
       " 'nvenclockbitstream-always-sets-picturestruct-to-0': 'When I’m encoding using nvenc the NV_ENC_LOCK_BITSTREAM structure returned by NvEncLockBitstream always has the pictureStruct set to 0. The comments in the code suggest it should return a NV_ENC_PIC_STRUCT. I can just keep track of it in my own code, since it will be constant across all frames, but this seems like a bug.Powered by Discourse, best viewed with JavaScript enabled',\n",
       " 'ubuntu-liquorix-kernel-updated-and-freezing-on-boot': 'nvidia-bug-report.log.gz (119.1 KB)Hi,MSI laptop, 12th intel cpu, gtx3050\\nLinux kernel is liquorix 5.18.0-12.2The booting is fine when just updated the kernel.\\nBut, after installing nvidia driver 515, 510 or 470, freezing on boot and does not work.Thank you.Hi, I noticed the same thing too.ShowBox Tutuapp MobdroPowered by Discourse, best viewed with JavaScript enabled',\n",
       " 'appdeclowlatency-decode-h264-not-one-in-and-one-out': 'When I use nvidia to decode a h264 stream,it’s not one in and one out(one input h264 frame, one output yuv frame). So cause high delay(about 200ms). When I using ffmpeg to decode the stream. It’s OK. There is no B-frame in the stream.\\nSo Can I set anything to force on in and one out?The output is:System:stream could be downloaded with: https://drive.google.com/file/d/10L_Qg3P7cTp9MuqqhB-dBLn2REzQbGSN/view?usp=sharingEncoder problem, SolvedHi qiushics, I meet the same problem now. My video stream is captured and encoded on Jetson Xavier by its Tegra API, and the video stream is decoded on PC host by its Nvidia video codec SDK. But it always cache 15 frames and then decode the first frame. So there is a huge latency. Can you give a tip how you solve your issue? Thanks in advance.Powered by Discourse, best viewed with JavaScript enabled',\n",
       " 'shield-tablet-android-kernel-configuration': 'Hello,\\nI launched $make menuconfig from “kernel” folder. Menuconfig shows, that modules I wish to add are already enabled. But they are not compiled actually. I noticed, that out/target/product/shieldtablet/obj/KERNEL/.config file is generated during compilation so it is impossible to change it manually. So I’m wondered, could somebody tell how to change kernel configuration for shield tablet?It is solved. Here is a file with config, that I changed manually.\\n~/shieldtablet-open-source/kernel/arch/arm/configs/tegra12_android_defconfig\\nThere are not all possible config entries, so I added those ones for modules I’d like to compile.\\nI took them from the resulting config\\n~/shieldtablet-open-source/out/target/product/shieldtablet/obj/KERNEL/.configPowered by Discourse, best viewed with JavaScript enabled',\n",
       " 'nvenc-recording-3-or-4-video-streams-on-multiple-gpus': 'hello,we need to record 3 or 4 video streams on Geforce cards. We know that the maximum number of video streams is 2. But we have multiple video cards plugged in.Do we still limited by 2 in this case? is there a way to select which card to use for nvenc?We are currently using gstreamer, its nvenc settings has cuda-device-id:cuda-device-id      : Set the GPU device to use for operations\\nflags: readable, writable\\nUnsigned Integer. Range: 0 - 4294967295 Default: 0we tried it, it doesn’t allow >2 video encoding streams.Please clarify the maximum video stream count on multiple gpus.Powered by Discourse, best viewed with JavaScript enabled',\n",
       " 'glsl-shader-compiler-bug-with-fxaa-regression-in-driver-334-67-isolated-to-kepler-gpus-only': 'There seems to be a GLSL shader compiler bug that was introduced with the 334.67 beta driver and still remains in 334.89. I have been able to reproduce the problem on both a GTX 770 and a GTX 780, but not on my laptop’s GTX 460M. The problem causes FXAA 3.11 to not work correctly.FXAA Off: [url]http://i.imgur.com/FkfrazB.jpg[/url]\\nFXAA On: [url]http://i.imgur.com/TMmvE8T.jpg[/url]\\nFXAA On with gather4 enabled: [url]http://i.imgur.com/aLQk8gD.jpg[/url]With FXAA enabled and gather4 disabled, FXAA is not working correctly and fails to handle lines that are almost vertical or horizontal. Enabling gather4 solves the problem, but gather4 is only supposed to be an optimization that does the exact same thing.Here’s the shader I am using: [url]http://pastebin.com/Preu7Y3c[/url]. This version has gather4 enabled if supported. Please change “#define FXAA_GATHER4_ALPHA 1” to “#define FXAA_GATHER4_ALPHA 0” to reproduce the problem.I have only modified the top part and added the main() method at the bottom. All input value are correct. No changes have been made in the internals of the FXAA 3.11 shader. All input values were correct during testing.Powered by Discourse, best viewed with JavaScript enabled',\n",
       " 'solved-gl-nv-command-list-shader-storage-buffers': 'Hi,I am trying to switch my rendering pipeline to be using the GL_NV_command_list extension.The current issue I am running into is, that GL_NV_command_list forbids using shaders with shader storage buffers.The specification sais, that an alternative exists:However, in the specification of gpu_shader5 I couldn’t find any informations how to do this http://developer.download.nvidia.com/opengl/specs/GL_NV_gpu_shader5.txtThe nearest thing I could find was the NV_shader_buffer_load extension which doesn’t work with uniform buffer blocks and so is useless for command_lists.Is there any way I can get the functionality of shader storage buffers (arrays with unknown length at compile time) to work with command-lists?Not sure if that is covered by our nvpro-samples, but there are some basic and advanced GL_NV_command_list examples here:\\n[url]https://github.com/nvpro-samples[/url]Check the READMEs of these examples:\\ngl_cadscene_rendertechniques\\ngl_commandlist_bk3d_models\\ngl_commandlist_basicHi,NV_shader_buffer_load/store are implicitly part of NV_gpu_shader5 and the extensions that give you this functionality, you would work directly with pointers to those buffers that you store within the UBO.Hey thanks for your answers.@ChristophKubish Works flawlessly :)For future readers: If the glsl throws strange error messages, recheck, whether you are using the commandBindableNV layout.hi, now I want to use the bindless techniqeu in my cluster fowrard pipeline, I do not use the token technique, so i do not use the layout(commandBindableNV) uniform;\\nin the shader i write like this:#extension GL_ARB_gpu_shader_int64 : enable\\n#extension GL_ARB_bindless_texture : enable\\n#extension GL_ARB_gpu_shader5 : enable\\n#extension GL_NV_shader_buffer_load : enable\\nlayout(std140) uniform RenderPipeline\\n{\\nmat4 SpotLightViewProjMatices[4];\\nsampler2D EnvTexture;\\nsampler2D ShadowMap;\\nuvec2* LightGrid_Cluster;\\nuint* LightIndexList_Cluster;\\n};and in the C++, i get the texture handle for EnvTexture and  ShadowMap like this:{    mHandle = glGetTextureSamplerHandleARB(mResource, mSamplerState->mResource); });\\nglMakeTextureHandleResidentARB(mHandle)\\n}and then i get the storage buffer handle for LightGrid_Cluster and LightIndexList_Cluster like this:{\\nglGetNamedBufferParameterui64vNV(mResource, GL_BUFFER_GPU_ADDRESS_NV, &mHandle)\\nglMakeNamedBufferResidentNV(mResource, GL_READ_ONLY)\\n}i write the texture handle and the strorage buffer handle to the uniform buffer like this:\\n{\\nglNamedBufferSubDataEXT(currentUniformBuffer->mResource, currentUniformBuffer->mOffset, handleCount *\\nsizeof(GLuint64), Handles)\\n}the result is that the EnvTexture and ShadowMap can worked, but the storage buffer can not worked, can you help me to figure out where the problem is it ?@ChristophKubishHi, don’t see a fundamental issue with this setup. Just double check the content of the UBO ensuring the pointer values are properly set.yes, it worked, thanks very much!!! there are some mistakes in my c++ code with subdata the uniform@ChristophKubish  I finished the bindless work, I place the texture and storage buffer in the uniform buffer, and use the glBufferAddressRangeNV() for the VBO, IBO, UBO,  when I run the program, at first it can render the scene, after a while, the program crashed in the nvoglv64.dll, there are not any used information for debug the program, the crashed information is as follow:T3d.dll!T3D::WGLContext::swapBuffer() line 43\\tC++or :glNamedBufferSubData()  line 2138can you give me some suggestion?Powered by Discourse, best viewed with JavaScript enabled',\n",
       " 'articulations-and-cuda': 'Hi,I am doing research on physics based character control which involves running lots of simulations with articulations and analysing the results, then tweaking the simulations and repeating.  Simulating on the CPU, I can run several simulations in parallel, which speeds things up a lot, but I still have so many iterations to run that it can take hours to complete.  I have noticed that recent versions of PhysX include CUDA support, and I am wondering whether I can speed things up further by using the GPU, but I have some questions:what kind of performance might I expect for simulating articulations (containing about 20 links) on GPU vs CPU, assuming that I can run a large number of independent simulations in parallel?if the GPU is likely to be significantly faster, what would be the best way to structure my problem?  For example, would one scene with lots of articulations, or many separate scenes be a better way to go?can anyone point me to a tutorial or some example code using the GPU in PhysX?  I’ve looked at the pages on the Nvidia developer website, but I don’t really feel confident yet that I could get it working easily.  Some example code would be really useful.would the output of the simulation using the GPU be identical to that from the CPU?I appreciate any help anyone can give me with this.Afaik, articulations do not support GPU acceleration.Thanks a lot for the reply.  Just out of interest, do you know if there is a list of what simulation tasks are or aren’t supported in CUDA, or any info on which simulation scenarios are likely to be suited to the GPU?Inside PhysX SDK, only particles/fluids and cloth are using CUDA acceleration - you can find more info in corresponding docs sections.APEX extensions on GPU are also featuring volumetric smoke sim (Turbulence) and rigid bodies (Destruction).\\nHowever, GPU Rigid bodies are samewhat limited in features (no joints, one-way interactio with CPU RB, etc) and are currently being reworked for PhysX 3.x SDKAlso, there is pure GPU accelerated FLEX solver, that can simulate variety of objects (rigids, cloth, softbody, fluids), but afaik it also does not support complex joints yet.Thanks for the detailed response, it’s very helpful.Powered by Discourse, best viewed with JavaScript enabled',\n",
       " '2xa6000s-nvlink-mosaic': 'Hi everyone!\\nFirst off I apologize, I’m kind of new to all this stuff and I’m trying to learn as I go.\\nI am running into an issue setting up 2 A6000 cards with NVlink while also configuring two mosaics.The way I have my system setup, it outputs a total of 8 displays, 4 from each card. I have 2 mosaics setup, 1 for each of the 4 connections so the computer sees 2 screens. All works great.The problem I run into is when I go to enable NVLink, the option just isn’t there in the control panel. It shows up when I disable the mosaics and I’m back to 8 displays.Is there a way to enable both Mosaics and NVLink? Is that a thing?\\nI keep seeing mention of “Premium Mosaic” using the NVLinks but I’m having trouble finding some relevant documentation to help me through that process.\\nI’ve also tried running only 4 of the 8 monitors using 1 card, while the other had 0 cables plugged in, but I still didn’t see the option to enable SLI after setting up mosaic.Any help is appreciated!Powered by Discourse, best viewed with JavaScript enabled',\n",
       " 'sparse-memory-requirement-mismatch': 'There is a mismatch in the returned values from vkGetPhysicalDeviceSparseImageFormatProperties2 and vkGetImageSparseMemoryRequirements2. If both functions are called for the same image parameters, there is a mismatch in the VkSparseImageFormatProperties. Specifically, for images of size smaller than the granularity, vkGetImageSparseMemoryRequirements2 will return VK_SPARSE_IMAGE_FORMAT_SINGLE_MIPTAIL_BIT in formatProperties.flags (and report a full imageMipTailSize and no imageMipTailStride), while vkGetPhysicalDeviceSparseImageFormatProperties2 reports no such flag.Specific image parameters in VkImageCreateInfo:\\nflags = VK_IMAGE_CREATE_SPARSE_BINDING_BIT | VK_IMAGE_CREATE_SPARSE_RESIDENCY_BIT\\nimageType = VK_IMAGE_TYPE_2D\\nformat = VK_FORMAT_BC7_SRGB_BLOCK\\nextent = { 128, 128, 1 }\\nmipLevels = 8\\narrayLayers = 2\\nsamples = VK_SAMPLE_COUNT_1_BIT\\ntiling = VK_IMAGE_TILING_OPTIMAL\\nusage = VK_IMAGE_USAGE_SAMPLED_BIT | VK_IMAGE_USAGE_TRANSFER_DST_BIT\\nsharingMode = VK_SHARING_MODE_EXCLUSIVE\\ninitialLayout = VK_IMAGE_LAYOUT_UNDEFINEDSpecific image parameters in VkPhysicalDeviceSparseImageFormatInfo2:\\nformat = VK_FORMAT_BC7_SRGB_BLOCK\\ntype = VK_IMAGE_TYPE_2D\\nsamples = VK_SAMPLE_COUNT_1_BIT\\nusage = VK_IMAGE_USAGE_SAMPLED_BIT | VK_IMAGE_USAGE_TRANSFER_DST_BIT\\ntiling = VK_IMAGE_TILING_OPTIMALThe presence of the flag in formatProperties.flags, as well as the resulting miptail size and stride are clearly a bug in the driver.I know everything sparse isn’t at the highest priority on the list of bugfixes, so until you fix this, can i just ignore the flag and make a simple educated guess about the miptail size per layer and stride or is there actually some fancy mapping going on underneath?My system:\\nWin7 SP1 x64\\nGTX 970\\nMSVC 2017 15.8.6 x64\\nDriver: 411.63\\nSDK + Validation: 1.1.85RegardsPowered by Discourse, best viewed with JavaScript enabled',\n",
       " 'optix-7-0-has-a-conflict-with-libcuda-so': 'I’m not a Linux user, but isn’t the libcuda.so the actual CUDA driver executable library and not the application link library?For applications linking against the CUDA Driver API you would normally use CMake’s FindCUDA.cmake and add the ${CUDA_CUDA_LIBRARY} to the target_link_libraries of your application.This is how I’m doing it in my examples which work under Windows and Linux.\\nhttps://github.com/NVIDIA/OptiX_Apps/blob/master/apps/intro_driver/CMakeLists.txt#L167\\nPrint that value with a message instruction and see what CMake picked.Compare that with the target_link_libraries used in the CUDA Runtime API case which additionally links against ${CUDA_LIBRARIES}.\\nhttps://github.com/NVIDIA/OptiX_Apps/blob/master/apps/intro_runtime/CMakeLists.txt#L168Start looking at the top-most CMakeLists.txt in that repository for Linux specific settings.Yeah, it’s the CUDA driver executable library. And I got that about cmake.\\nBut I have other module, for example nvcloth and libtorch, which need the libcuda.so\\nI need solve the crash anyway.\\nSo, any idea about the error info ?Thank you all the same!So you say your OptiX 7.0.0 program works fine when not linking against the CUDA driver library and fails if you do.\\nThat would require more information about what exactly you did.Please always specify at least the following system configuration information for any OptiX issue:\\nOS version, installed GPU(s), display driver version (this is mandatory!), OptiX version (major.minor.micro), CUDA version (major.minor) used to compile the input PTX code, host compiler version.The first link above contains an OptiX 7.0.0 application named intro_driver which only uses the CUDA Driver API and works under Windows and Linux.\\nYou might want to try if you can get that to work on your system and if yes, check what you did differently in your sources.Note that the CUDA Context initialization differs between the CUDA Runtime API and CUDA Driver API versions of this example.\\n(The intro_runtime example links against both the CUDA runtime and driver library because it uses driver API to get the current CUDA context.)Compare these two Application::initOptiX() functions:\\nhttps://github.com/NVIDIA/OptiX_Apps/blob/master/apps/intro_runtime/src/Application.cpp#L734\\nhttps://github.com/NVIDIA/OptiX_Apps/blob/master/apps/intro_driver/src/Application.cpp#L752Thank you for your patience and guidance. I will try it later.\\nThe configuration info:Powered by Discourse, best viewed with JavaScript enabled',\n",
       " 'graphic-card-fan-stops-and-starts-automatically': 'My nviidia GTX 1650 super windforcerce of fan starts and stops automaticallyHi, I suggest posting in the Geforce forums as this forum is mainly used by developers.Get the support you need.Powered by Discourse, best viewed with JavaScript enabled',\n",
       " 'how-to-move-collision-shapes-in-nvcloth': 'I am currently trying to run simple cloth on moving collision shapes(body) simulation.\\nFor this I use arbitrary triangle meshes for collisioni (ie Instead of setSphere\\nI use setTriangles ).I have so far been able to let the cloth fall on the collision shape (triangles).Now I want to let the cloth fall on moving collision shape(triangles).Can any one please let me know how is it done in NvCloth. I can see in physx they have\\nRigid Body Dynamics. Is there similar concept in NvCloth.What you need to do is just update the triangles vertices in the simulation loop.Powered by Discourse, best viewed with JavaScript enabled',\n",
       " 'xrandr-fractional-scaling-is-slow': 'Why is Xrandr scaling slow? Is is not GPU accelerated? or is it issue with Xrandr itself?May be could using an alternate solution like framebuffer scaling in the Nvidia driver, something like Macos does?Powered by Discourse, best viewed with JavaScript enabled',\n",
       " 'no-rendering-with-vk-khr-multiview-on-gtx-1080-for-1-view-multiple-drivers-versions': 'On the GTX 1080, with multiple views (>1 view) using the VK_KHR_Multiview extension, the render target images appear corrupted as I cannot view them in RenderDoc (both swapchain and offscreen VkImages) or through the standard GLFW window.Tested on different hardware (all Windows 10, i7-6850K, driver version in parantheses):Oddly enough, the GTX 1080 does work on Sascha Willem’s multiview example.Affected project here:Interactive medical simulation toolkitthanks for reporting the issue. We have filed an internal bug to track this issue.There are several applications/examples available in the iMSTK project. Can you point to the application, along with the steps to reproduce the issue you are seeing?Sorry for the late response. On compilation select the CMake options (iMSTK_USE_Vulkan and iMSTK_Enable_VR). Probably the best example to test is the Example-Rendering example. All examples exhibit this behavior, but there are two ways to reproduce it:Option 2 is probably easiest since you don’t need VR headset to test. If the rendering works (as is my experience on the RTX cards), it will display two renderings side-by-side in the render window.Also, I tested on GTX 1080 Ti, and it had the same problem as the GTX 1080. If you are testing, let me know if you have any problems.Root caused the issue to an application bug. The following patch fixes it:— a/Source/Rendering/VulkanRenderer/imstkVulkanRenderer.cpp\\n+++ b/Source/Rendering/VulkanRenderer/imstkVulkanRenderer.cpp\\n@@ -1440,7 +1440,7 @@ VulkanRenderer::initializePostProcesses()You are just getting lucky on RTX cards and not seeing the issue.Powered by Discourse, best viewed with JavaScript enabled',\n",
       " 'gtx-1050-video-memory-internal-bsod': 'Hi,we’re running into a reproducible VIDEO_MEMORY_MANAGEMENT_INTERNAL BSOD on Win 10 with one of our devices. I compiled an archive gtx-bsod.zip (465.6 KB) documenting the issue with the following contents:System specsDevice crashing:\\nNVIDIA GeForce GTX 1050Devices not crashing:\\nNVIDIA GeForce GTX 1070\\nAMD Radeon Pro 560\\nIntel UHD Graphics 630Driver versions tried:\\n472.12 (Studio)\\n496.76 (Game ready)Vulkan SDK versions tried:\\n1.2.148.1\\n1.2.189.2OS:\\nWin 10 x64\\nVersion 21H1Compiler:\\nMicrosoft Visual Studio Professional 2019\\nVersion 16.11.5Please forward this to the driver maintainers and let me know if I can contribute anything else.ThanksHello @bfuehrer and welcome to the NVIDIA developer forums!Thank you for bringing this to our attention, I will forward the information.Hi @MarkusHoHo ,is there any news/progress on this? I tried the game ready driver v512.15 but the problem still persists. Unfortunately this bug adds a lot of overhead for us because we currently need to disable affected devices or features for our customers from running at all. Can you maybe based on similarity of driver implementation give us a hint which series are more or less likely to be affected by this so we can ideally be less conservative in our heuristics of what we disable?Hi @bfuehrer , @MarkusHoHo@bfuehrer , Please Update  you simple project for multi GPU systems:Hi @AndreyOGL_D3D ,please try this version\\ngtx-bsod.zip (474.7 KB)Hi @bfuehrerPowered by Discourse, best viewed with JavaScript enabled',\n",
       " 'mocap-from-previously-shot-and-or-existing-2d-video-originally-posted-in-wrong-section': 'I am very interested in this tech as I have been waiting for it for many years and all I got to go on is a one-second tease from the Omniverse introduction video… So my question is this; does it exist and if so, how do I access it, and will it transfer over to BVH files?Any info would be appreciated; thanks in advance!Powered by Discourse, best viewed with JavaScript enabled',\n",
       " 'shader-reordering-execution-on-the-rtx-4090': 'Hello, I’ve heard that Shader Execution Reordering hasn’t been enabled on the majority of titles that support ray tracing. (In fact I believe that it is only enabled on a few at the moment…) What is the technical reason stopping shader execution reordering from working by default on all games that support RTX?Hello @stephen_martin and welcome to the NVIDIA developer forums!Did you see the excellent Technical Blog on the feature? It explains in detail, together with the linked whitepaper, how it works and what the benefits are.And after looking into the details I am certain it becomes clear that this feature is not only on the Hardware and driver side but involves some additional work by the individual game developers.I hope this answers your question!Hi @MarkusHoHo! I just had a brief look over the article. Quick question, is SER only DXR API specific? I mean, if someone using OpenGL or NVIDIA OptiX, will it still work?SER is not DirextX specific. It is rather an extension of NVAPI and as such has only Windows as a requirement.Markus, is there a chance this improvement will ever show up as a Vulkan extension? Even an NV-specific one?I’m using Vulkan Ray tracing and HLSL → DXC → SPV pipeline, if that matters.I honestly don’t know. Usually NVIDIA does like to include nice features like this to different APIs, but SER is still so fresh, hard to say if or when we get around putting it to other APIs.Hi Markus, do you know who one would reach out to do see about gaining access to DLSS Frame Generation and MicroMesh SDK?I applied six months ago and still can’t use these features I paid good money for on my RTX 4090, despite having private SDK access to virtually every other SDK NVidia has already. Can you help me out? It would really be appreciated.I am afraid you need to be a bit more patient, but general Frame Generation access will come soon.I recommend checking out the GTC Keynote in a bit over two weeks, and specifically attend “How to Build a Real-time Path Tracer”, to learn more about it. Compressed Micro-Meshes also have their own session by the way.Powered by Discourse, best viewed with JavaScript enabled',\n",
       " 'nview-virtual-monitors-gridlined-split-screen-browser-full-screen-mode-f11': 'First of all, apologies if this was mentioned somewhere else, I’ve done my best to search the forums and i couldn’t find any.simply put: not sure if it is improper nView Desktop Manager configuration, a missing feature or just the way it works:Suppose you have a 4K monitor, you spilt it into 4 x 1080 grids, you drag a chrome tab into one of the quarters, you maximize/snap it using nView Desktop manager, no worries.Trying to go full screen on youtube (or F11), it expands to the original monitor borders instead of the grid-lines, I tried all the options on nView on/off, no difference.To make sure this is not a driver limitation, a product called Virtual Display Manager (VDM) iShadow is able to achieve that.a link of what i am trying to achieve (working by Virtual Display Manager)[url]Dropbox - quad_VDM.png - Simplify your lifeHi Mesh,Currently, that feature is not in available in NView at the moment. However, we will consider the possibility of adding that feature you’ve requested in the future.Thanks,\\nRyan ParkHello,Is there any development on the requested feature?\\nIt would be very useful.Thank you.Powered by Discourse, best viewed with JavaScript enabled',\n",
       " 'flex-unity-callback-for-particle-lifetime-expiration': 'Hi, I was wondering if there is a way to access a callback for when a fluid particle times out (i.e. its lifetime expires)? If its not available thru Unity, then is there an avenue for me to modify the Flex source code to expose it? Thank you.Powered by Discourse, best viewed with JavaScript enabled',\n",
       " 'validation-message-improvement-suggestion': 'Lately I encountered (on a modified version of my pathtracer project, which architecture is based on the OptiX Apps 2018 samples) this validation error:\\n[2][ERROR]: Validation mode caught builtin exception OPTIX_EXCEPTION_CODE_TRAVERSAL_INVALID_HIT_SBTThe online doc says:\\n[…]The traversal hit SBT record index out of bounds.[…]\\nI did not find one post here in the forum about this validation error and nothing on the search engine.So it took me quite a while to figure out, how that error could happen.\\nAfter searching a lot in my SBT setup without finding any error, I checked the pipeline settings.\\nI use Hair Curves OPTIX_PRIMITIVE_TYPE_FLAGS_ROUND_CATMULLROM, and all the GAS / IAS settings seemed to be ok, but I did not set the   OptixPipelineCompileOptions.usesPrimitiveTypeFlags. It was zero. That caused it!\\nThe pipeline settings did not match the OptiX 7.1 Curve Primitive GAS\\nprimitive_input.type = OPTIX_BUILD_INPUT_TYPE_CURVES;  settings.\\nand builtinISOptions.builtinISModuleType was also not set in my code due to respecting the invalid pipleline setting.So the validation correctly told me, that something was wrong, but it was wrong in the pipeline and the inbuilt IS functions were missing but nothing wrong in the SBT-related code.\\nPlease improve the validation messages for such cases!Thank you.Project circumstances where I encountered it: see this postMy System:\\nOptiX 7.6.0 SDK\\nCUDA 11.8\\nGTX 1050 2GB\\nWin10PRO 64bit (version 21H1; build 19043.1237)\\ndevice driver: 526.47    VS2019 v16.11.17 (toolkit v140 of VS2015)\\nMDL SDK 2020.1.2\\nWindows SDK 10.0.19041.0Thanks that’s a good idea! I agree the SBT error is a bit unhelpful, I’ve filed a report to get it fixed.–\\nDavid.This topic was automatically closed 14 days after the last reply. New replies are no longer allowed.Powered by Discourse, best viewed with JavaScript enabled',\n",
       " 'is-it-possible-to-use-nvidia-a10-without-virtualization': 'Sorry if this is a trivial question, but I’m having a hard time trying to find a precise official answer.My goal is to install an A10 on a remote baremetal with Windows Server and access it via Remote Desktop to run rendering tasks such as 3DS Max Vray rendering and Unreal Engine 4 Pixel Streaming architectural visualization. I don’t intend to virtualize the server.Is it possible to use A10 for that purpose without needing to worry about vGPU, extra licenses, etc? All the documentation and promo material on A10 seems to imply I need vGPU solutions to use it.Hello @rabellogp and welcome back, it has been a while.This is definitely not a trivial question. These types of cards are meant as pure “work-horses” in (rack based) server setups with multi-GPU configurations. For those it only makes sense to use them in a virtualization environment for multiple users.On the other hand if you manage to set up your Windows Server in a headless fashion with one (or more) A10 as the main GPU to be used by locally (on that machine) installed apps, then no one will stop you from using that machine in a more classic workstation fashion by connecting through RDP.But what I do not know is if you can actually get that setup running. I am not aware of this being tested or reported by other developers. Which means that I would be very grateful if you could keep us updated on your success.Hope this helps!Thanks for the tips!Due to the lack of precise information on that regard, we ended up going for RTX A5000 instead of A10. They’re not as efficient and compact for a server rack situation… But at least it’s a safe bet for a non-virtualized environment. We couldn’t afford the risk of the A10 not working and needing to be swapped giving all the crazy situation with price and availability of GPUs.Anyway, the possibility of A10 working on non-virtualized environment remains a mystery.Having had some unfruitful exchange with the nvidia support on this topic, I can provide some of my experience and would like to ask if there’s anything new here.\\nWe (naively) bought an A10 for our server rack and couldn’t get it to run with the regular nvidia drivers. Only when using the vGPU drivers which we obtained through a test licence could we get the card to be used bare-metal by the system. Those vGPU drivers are subscription based so you have to pay annually to keep the card (which you also payed for) operational.Powered by Discourse, best viewed with JavaScript enabled',\n",
       " 'gldrawelements': 'Hi All,I’m developing a OpenGl Program on win7 x86 32bit VM using GRID K2 GPU.my program is like Pseudo code belowsometimes，A Write Memory Violation was throwed on the step 1 when i trying to modify the buffer of ptr pointer。when i debug this issue，i found that glDrawElement call VirtualProtect for ptr pointer and making the buffer readonly.how can i fix this issue or any tips for meAny suggestions greatly appreciated!\\nThanksthese days, i found that if ptr is static memory region, the Write Memory Violation  can not be throwed.It’s really werid.Any suggestions greatly appreciated!\\nThankswhen i use debug tools ,i found it show a message “__wglImpWWVirtualAddWatch”.so……what’s the meaning of “__wglImpWWVirtualAddWatch”?  how to triger this function?Any suggestions greatly appreciated!\\nThanksHi Xiaolang,I found out, that Nvidia makes a hidden “copy-on-write optimization” with the memory you provide to the driver.This optimization marks the memory-pages your memory intersect with as read-only (WinAPI-Function “VirtualProtect” parameter flNewProtect “PAGE_READONLY”).The page size is 4096 bytes, so it is not unusual you have some intersections with other memory in the same pages.When you try to write to this memory, NVidia catches the signal, copies the data and releases the write protection.But this optimization can lead to huge problems, because when other memory blocks in the process intersect with one of these pages, some WinAPI functions that check for write protection will fail (e.g. when the buffer you give to “ReadFile” is write protected by Nvidia)!Our customers had mysterios crushed binary-files over years, and we could not find a reason for this in our source code.And we have many other problems with “hanging” threads, when this COW-optimization is active.The nasty thing is, that this optimization will only be activated, when you start the program without an attached debugger!\\nSo the most of us will never find the reason for their problems…I found it after many month, by using a Windows API-Monitor attached to my process.\\n(it must be attached after the initialization of the graphics!).One solution for this is to select the “Workstation App - Dynamic Streaming” profile in the Nvidia Control Panel. In this profile, the “optimization” is deactivated.But most of our customers are using the “Base Profile”, which is the default.Perhaps Nvidia assumes that the users of their graphic adapters just playing online games, so this doesn’t matter.But our customers driving huge CNC-Machines by the programs we calculate with our CAD/CAM Software.\\nAnd it can be really dangerous to life, if the programs we produce for these machines contain rubbish because of failed file-reads…RegardsThanks for posting this.We’ve been tracking down a ReadFIle issue for almost a year now.  Microsoft support directed us to find what code (ours or 3rd party) was calling VirtualProtect (PAGE_READONLY) when one of our Devs found your comment.  After switching to the other nVidia profile, we haven’t been able to repro our issue.We also believe that another longstanding issue of ours is caused by this.  In the other issue, it’s WSARecv (in gRPC library) that is failing.If you don’t mind my asking … what “Windows API-Monitor” did you use to find this out?ThanksHi Christopher,I used the Rohitab API-Monitor (API Monitor: Spy on API Calls and COM Interfaces (Freeware 32-bit and 64-bit Versions!) | rohitab.com).It is very likely that the remote procedure call functions use the ReadFile-function to transmit the stream.But I think that other functions of the WinAPI also directly check the protection flags of the provided memory before writing to it…Best regards,\\nDennisPowered by Discourse, best viewed with JavaScript enabled',\n",
       " 'hey-i-am-new-to-compiling-and-running-samples-on-physx-on-windows-can-someone-help-me-in-that': 'Hey, so i am a civil engineering graduate , and i want to use physX using Discrete element method for  simulation , but i donteven  know  how to run the demo and compile , and i also couldnt find the Snippets/compiler/vc14win64/Snippets.sln  file , can someone help me with that please ? , im a beginner at this.Powered by Discourse, best viewed with JavaScript enabled',\n",
       " 'tonemapper-doing-weird-things': 'Hi,after getting the tonemapper finally running (see other thread) I struggle to grasp what it is doing. Even if exposure and gamma are set to 1.0f it modifies the image. You can test that in the denoiser example from the sdk. Line 546 and 547. Just set exposure and gamma to 1.0f. Or comment the lines.\\nI would expect it to not change the “original” image, but it does get darker here.I’m also unable to invert the tonemapping. One stage with gamma and a following stage with 1/gamma does not result in the orginal image.Is this a bug?Cheers,\\nNicoAccording to the documentation it simply does this:\\n[url]http://raytracing-docs.nvidia.com/optix/guide/index.html#post_processing_framework#6151[/url]A renderer normally works in linear color space, while displays and the human eye don’t.\\nWhen setting gamma and exposure to 1.0, you get the linear colors of the HDR image.The colors in the range [0.0, 1.0] get brighter when increasing the gamma value.\\nSee some examples here: [url]https://en.wikipedia.org/wiki/Gamma_correction[/url]But according to the docs setting both to 1.0f shouldn’t do anything. pow(value * 1.0f, 1.0f) is value again.\\nThat would mean inputbuffer == outputbuffer. But the output is darker, so something must happen.I’ve confirmed what you’re experiencing and the reason is that the TonemapperSimple is a three stage implementation of exposure, tonemapping, and gamma.\\nMeans even if the exposure and gamma values are set to default values, the tonemapper step is still applied and it’s inspired by what analog films or real-time games, for that matter, do to preserve a natural look as much as possible.The documentation touches on that by saying that the stage does tonemapping and gamma correction. That explanation is going to be clarified in the future. Sorry for the confusion.If you don’t want that tonemapping step to be applied you can simply roll your own small ray generation program which implements the necessary operations and add these custom stages to the post-processing command list with appendLaunch().Powered by Discourse, best viewed with JavaScript enabled',\n",
       " 'quadro-nvs-295-model-supports-directx-10-1-is-not-10': \"Hello.\\nI am a c++ programmer in Korea.\\nI have a question about a problem that the graphics card supports the Direct X.Do Quadro NVS 295 model supports DirectX 10.1 (is not 10) ?http://www.nvidia.com/object/IO_14605.html\\nAccording to the above it is that the support page.But\\nhttp://www.nvidia.com/object/product_quadro_nvs_295_us.html\\nThis list contains general information about graphics processing units (GPUs) and video cards from Nvidia, based on official specifications. In addition some Nvidia motherboards come with integrated onboard GPUs. Limited/Special/Collectors' Editions or AIB versions are not included.\\n The fields in the table listed below describe the following:\\n 180\\xa0nm\\n 150\\xa0nm\\n\\nThis page can be found that it does not support.\\nWhich is true? DirectX 10 or 10.1?And i use this directx function\\nHRESULT WINAPI D3D10CreateDevice1(In_opt IDXGIAdapter *pAdapter,\\nD3D10_DRIVER_TYPE DriverType,\\nHMODULE Software,\\nUINT Flags,\\nD3D10_FEATURE_LEVEL1 HardwareLevel,\\nUINT SDKVersion,\\nOut_opt ID3D10Device1 **ppDevice);in d3d10_1.h\\nD3D10_FEATURE_LEVEL1 is D3D10_FEATURE_LEVEL_10_1but function failed. Error code 0x8004002 (No such interface supported)\\nif NVS 295 support 10.1\\nWhy this function failed?“DirectX Support” entry is at 10.0 by GPU-Z programs.\\nAnd “DDI Version” is 10 by dxdiag.\\nDriver Version is 341.81 and BIOS Version 62.98.75.00.08.\\nI think that it does not support directX 10.1.Please let me know if you have additional BIOS and patch files.\\nI wait for the detailed answer.Powered by Discourse, best viewed with JavaScript enabled\",\n",
       " 'read-rst-gstreamer-with-nvidia-acceleration': 'i try to use gstreamer to read rstp without latency as below:it is work, but it use cpu too much.\\ni find out that, i can use gstreamer with nvcodec. (Install NVDEC and NVENC as GStreamer plugins · GitHub)\\nbut don’t know how to implement it. Could you guide me to do it?Powered by Discourse, best viewed with JavaScript enabled',\n",
       " 'how-to-correctly-replace-the-buffer-of-a-texturesample': 'Hi!I am trying to replace the buffer of a TextureSample with this code:The result is a grey sphere, without texture…External MediaBugs first:1.) You access a mapped buffer pointer after it has been unmapped!\\nThat is illegal and could just crash the app with an access violation.\\n=> Line 19 belongs to line 23.2.) Texture buffers cannot be RT_BUFFER_INPUT_OUTPUT. They must be RT_BUFFER_INPUT.Then I would not recommend to use those OptiX SDK example framework functions for your case.\\nThe loadTexture() creates a complete texture sampler and texture buffer from scratch.\\nYou neither use that, nor does your the code clean up any of the unused resource.How I would implement this is like this:If that works continue like this:Implement your own function which loads some image from disk.\\nThe loadTexture() code should contain that inside.\\nDon’t put the image data into OptiX buffers at that point, just keep the image data in an abstract host image format.For the texture you want to put into that single texture buffer, load the desired image, then resize the texture buffer to the matching size, map the buffer, copy the image data over from the host, unmap the texture buffer.Basically take PPMLoader::loadTexture() apart and split it into these two steps (texture sampler creation, texture buffer filling).Since the texture sampler is not changed and just the buffer data has been updated, that’s all!\\nThe next launch should grab the new data for that texture sampler.Generally I recommend to get used to the native OptiX API instead of trying to reuse code from the SDK examples which are often using convenience functions for exactly one specific purpose.\\nIf you have another purpose you’re better off to implement a more dedicated function for that which can be simpler and more efficient.Create a single buffer to hold the texture data. Maybe start off with a 1x1 white RGBA8 bufferThen create a single texture sampler and assign that one buffer to it. Rendering with that should result in a white textureBut I dont get a white texture.External MediaWhat am I doing wrong?Thanks by your help!!Are you sure, that you filled texture_buffer by “white pixel”?Use unsigned data!The code inside the PPMLoader::loadTexture() function shows that correctly:\\noptix::Buffer buffer = context->createBuffer( RT_BUFFER_INPUT, RT_FORMAT_UNSIGNED_BYTE4, 1u, 1u );Thanks!, that was my problem.But now the program render a black sphere\\nExternal MediaSeriously, how did you post that message without reading my post which explains exactly what is wrong?!Seriously, how did you post that message without reading my post which explains exactly what is wrong?!I’m sorry, in my haste I did not read this line:\\noptix::Buffer buffer = context->createBuffer( RT_BUFFER_INPUT, RT_FORMAT_UNSIGNED_BYTE4, 1u, 1u );Seriously, I’m sorry.The code generate correctly a white texture.Now I will try to load the texture…Perfect! I correctly loaded the texture with your method.Thank you very much for your help!Powered by Discourse, best viewed with JavaScript enabled',\n",
       " 'how-to-import-the-fbx-model-with-mateiral': 'Dear developers:I am new to Optix and have gone through the Documentation and quickstart guide already.\\nI use fbxsdk to import the model, but  the material display is incorrect. Could you tell how to import material？Please keep working through the OptiX Programming Guide and examples. What you need to learn is the following:The Material node in OptiX holds a closest hit and any hit program which you need to implement to handle a material system as needed. Means, first you need to know how an FBX material is working exactly and can then implement the necessary closest hit and any hit programs in an OptiX renderer.If you have experience with shaders in a rasterizer API, the OptiX closest hit program basically contains the calculations normally done in a fragment shader.How these programs look like also depends on the underlying renderer implementation itself and that is completely under your control and responsibility.\\nYou would need to implement code to calculate the radiance on a surface hit point (evaluation of the material and lighting) and some code which calculates the direction of the next ray(s) (sampling of the material).If you search the OptiX example source code for “closest_hit” you’ll find some simple example implementations. Either for Whitted rendering as used in the optixWhitted example or in the optixMeshViewer which handles the OBJ MTL material model a little (look for “phongShade”), or for a simple Lambert shading model for global illumination inside the optixPathTracer.Powered by Discourse, best viewed with JavaScript enabled',\n",
       " 'question-on-any-hit-and-closest-hit': 'I have a hard time understand the any hit and closest hit program. For example in the examples inside the SDK there are declared several anyhit and closehit programs. Does it mean that after the program starts running, after calling rTrace it will execute all the anyhit programs i declared? So for example if I declare a anyhit program that I want to be run when intersecting a sphere and a different anyhit program for intersection with a box, will both anyhit programs be triggered at each intersection?Hit programs are “shaders” for given material. Closest hit is called for object that is closest to the ray origin. Any hit is typically used for shadow rays when you don’t care about closest hit, just want to know is there is some occluder withing given distance (e.g. on the way to light).Check the quick start guide and go through tutorial programs. I saw that OptiX wizard generates programs with bunch of comments, might find some clarifications there.Hi, I understand the use of the program. I don’t really understand if there is a way to call a specific anyhit program or if they are all called at once every time there is a intersection. Let’s say I declare 10 different anyhit programs will they all be called every time rTrace is run?For example:Are both closehit programs going to be called everytime rTrace identifies a closest hit?You’re asking about any hit and presenting an example with closest hit programs. They’re not the same.\\nCheck the Programming Guide section 3.4.2 Material.Any hit is called for all potential closest intersections. Hit programs are defined per ray type (number 0 your example). If you have 10 any hit programs for 10 ray types only one of the will be called. If you have 10 any hits for 10 materials and same one type of ray, all of them (potential closest hits) will be called.Powered by Discourse, best viewed with JavaScript enabled',\n",
       " 'my-dx12-app-fails-to-load-nvngx-dlss-dll': \"NVSDK_NGX_D3D12_Init() and NVSDK_NGX_D3D12_GetCapabilityParameters(&ngxParameters) succeed but ngxParameters->Get(NVSDK_NGX_Parameter_SuperSampling_Available, &isDLSSAvailable) returns NVSDK_NGX_Result_FAIL_UnsupportedParameter.I think, this is because the SDK doesn’t find the nvngx_dlss.dll.I’m getting either\\n[NGXLoadAndValidateSnippet:1781] nvLoadSignedLibraryW() failed on snippet './nvngx_dlss.dll' missing or corrupted - last error One or more arguments are not correct.\\nor\\n[NGXSecureLoadFeature:519] warning: ModuleName - nvngx_dlss.dll doesn't exist in any of the search paths!\\nin the log, depending on whether I fill out NVSDK_NGX_FeatureCommonInfo.PathListInfo. I’ve copied the DLL to the working directory, to the binary directory, to the directory of the DLL that loads it and added it to PATH. I’ve also tested the sample that comes with the SDK, which works fine. If I delete the DLL from the sample, I’m getting the same behavior.I’m initializing NGX like this:but I’ve also tried it the way the sample does:I don’t understand what I’m doing wrong. Is it not possible to initialize NGX from within a DLL maybe? Does this have something to do with nvLoadSignedLibrary failing to recognize that the library is signed? I’ve tried both the debug and release DLLs.I’ve attached the entire log.dlss_log.txt (27.5 KB)Answering my own question, in case somebody else runs into this as well. There are multiple gotchas here:Of course, this is totally undocumented. I’ve tried the second tip but it didn’t work at the time because I also specified paths without defining NV_WINDOWS.Powered by Discourse, best viewed with JavaScript enabled\",\n",
       " 'code-understanding-help': 'Alright I’m trying to wrap my mind around this simple piece of code. But I’m really having a little hard time in understanding some specifics, while I totally understand what it does contextually.First off the computation inv_screen. I dont understand 1.0f/make_float2(screen) how it works dividing 1.0f by a vec2 kinda format. Similarly computation of pixel. I am also not sure how the variables x,y,jitter,d, ray_direction fall in place with respect to one another.“First off the computation inv_screen. I dont understand 1.0f/make_float2(screen) how it works dividing 1.0f by a vec2 kinda format.”Many standard operators are overloaded for the vector types.\\nscreen is a size_t2 and needs to be converted to float2 to get an operator/() overload working which implements division of a float by a float2.All these operators work component-wise.\\nMeans 1.0f / make_float2(screen) <==> make_float2(1.0f) / make_float2(screen) <==> make_float2(1.0f / float(screen.x), 1.0f / float(screen.y))Same for the operator*() in line 3 and operator-() in line 4.\\nSimilarly for float3 and float4 or other types.The jitter calculation computes a number of ray directions which are used for antialiasing. It’s basically a random sample point stratified to a sub-pixel grid in the cited implementation.Powered by Discourse, best viewed with JavaScript enabled',\n",
       " 'one-application-instance-and-two-context-any-restriction': 'HelloI have a app that show a main viewport with interop directx->optix, all works fine. But when I try open a new child window with new optix context and new contents (same as main contents but new instances) on the Launch invocation I receive the Unknow exception -1.The optix has any restriction about that?I use Optix 3.7.tanksI try open only the child window and works fine. If I create a main context the other context (child) does not work. What I have to do? Destroy main context before create a new (child)? and destroy child and recreate a main context when return?Anybody have the same problem?The problem occurs when the launch of main context are invoked, if I dont do that the child launch works great.Powered by Discourse, best viewed with JavaScript enabled',\n",
       " 'd3d12-driver-is-crashing-when-a-compute-shader-is-executed-with-a-power-of-2-numthreads': 'Ok so this is perhaps one of the craziest problems I have ever encountered. I have reduced this problem to a simple compute shader that does nothing but write a UAV:g_pathVisibility is a buffer of 128 int’s and g_count is set to the value of 128 as a root constant. This shader is executed on a compute list/queue. With any numthreads value that is a power of 2 nvlddmkm crashes and I get a TDR device reset with the following error:If I set numthread to [numthreads(31, 1, 1)] or [numthreads(33, 1, 1)] or any other non-power of 2 value the shader will run fine. If I set it to [numthreads(32, 1, 1)], [numthreads(64, 1, 1)], [numthreads(128, 1, 1)], etc then the driver will crash. What’s even crazier is that I can run the Microsoft n-Body Gravity sample, which uses a power of 2 numthreads, without any problems whatsoever! I’ve compared my code with theirs and I can’t see where there’s any potential problem. The D3D12 validation layer, which I have turned on max verbosity for all categories, says everything is all good. I get the following errors in the windows event viewer:I’m running the latest Windows 10 Creators Update with a GTX 1080 and Visual Studio 2017. I’ve reproduced this problem in both UWP and regular windows applications. The shader is compiled as cs_5_1. Sample code and system info is attached.\\nPathTransform.zip (61.2 KB)NVIDIA System Information 06-13-2017 01-51-23.txt (3.67 KB)Powered by Discourse, best viewed with JavaScript enabled',\n",
       " 'q-how-to-compile-simple-programs-with-nvidia-optix-prime': 'What is the command to compile simple Optix programs such as optixhello.cpp? It comes with a cmake example files, but when I try to compile, it complainsThe OPTIX_add_sample_executable() call is about the only thing in CMakeLists.txt.I’d prefer not to have to learn how to debug cmake because I can not use cmake in my project.  I just want to get simple Optix Prime examples working.  What is the command to compile optixhello.cpp?  Or, better yet, is there a document that explains how to compile Optix examples?Looks like you’re not creating a cmake build directory in the right place.  Please read the INSTALL-LINUX.txt file that comes with the SDK and follow the instructions there, then post back if that doesn’t work for you.Thank you.  Yes, I eventually figured that out.However, what is the correct way to compile Optix Prime programs from the Linux command-line?  I want to incorporate Optix Prime into my project and I need to know what libraries to include.  So far, I’ve come up with this:COMPILE:/usr/bin/c++    -fPIC -msse -msse2 -msse3 -mfpmath=sse -O3 -DNDEBUG -g3 -funroll-loops -I/opt/NVIDIA-OptiX-SDK-4.0.1-linux64/include -I/opt/NVIDIA-OptiX-SDK-4.0.1-linux64/SDK/sutil -I/opt/NVIDIA-OptiX-SDK-4.0.1-linux64/SDK -I/opt/NVIDIA-OptiX-SDK-4.0.1-linux64/include/optixu -I/usr/local/cuda/include -o mine.o -c mine.cppLINK:/usr/bin/c++    -fPIC -msse -msse2 -msse3 -mfpmath=sse -O3 -DNDEBUG -g3 -funroll-loops mine.o  -o mine -rdynamic /opt/NVIDIA-OptiX-SDK-4.0.1-linux64/lib64/liboptix_prime.so /opt/NVIDIA-OptiX-SDK-4.0.1-linux64/SDK/lib/libsutil_sdk.so /opt/NVIDIA-OptiX-SDK-4.0.1-linux64/lib64/liboptix.so -lm /opt/NVIDIA-OptiX-SDK-4.0.1-linux64/lib64/liboptixu.so /usr/local/cuda/lib64/libcudart_static.a -lpthread -lrt -ldl  -lglut -lXmu -lXi -lGLU -lGL -lSM -lICE -lX11 -lXext -Wl,-rpath,/opt/NVIDIA-OptiX-SDK-4.0.1-linux64/SDK/lib:/opt/NVIDIA-OptiX-SDK-4.0.1-linux64/lib64What should that be and where is this documented?If you already have a standalone app that builds with a Makefile and you’re adding OptiX Prime (not regular OptiX) from scratch, then the steps are simpler:The liboptix_prime library has no external dependencies, so you shouldn’t need to include or link anything else.  The SDK samples all link to libsutil, a utility library, and other things like GLUT.  You can omit all that in a standalone sample.For regular OptiX (not Prime) the steps are similar, using liboptix instead of liboptix_prime, but you will also need to generate PTX for your shaders, using nvcc.  See section 5 of the OptiX Programming Guide pdf for details.How to compile simple programs with Optix Prime in windows?Please read the OptiX SDK 5.0.1\\\\SDK\\\\INSTALL-WIN.txt document.\\nThat explains how to build the OptiX SDK examples yourself under Windows.Hello,Actually if you build the generated solution from cmake (at least on my machine: Win10, cmake: 3.9.12, msvc14) there are some linking errors in the prime* projects.\\nThat’s because the cu.obj files are never generated when building the prime projects. This happens due to the fact that the Build Event (that compiles the *cu.obj.cmake and generates the *cu.obj files is nested as a Custom Build Tool in the .cu file’s properties).\\nIf you copy the Command Line commands and paste them as a Pre-Build Event in each prime project’s properties, everything works fine. At least for me.I hope this helps anyone having trouble to build the prime* projects in the generated solution.Cheers,\\nAntonisPowered by Discourse, best viewed with JavaScript enabled',\n",
       " 'c-programmer-needs-guidance-on-how-something-happens': 'I developed a C# library (Nuget package DataJuggler.PixelDatabase) and I use it a number of different ways. One of them is for creating gradients and combining them together. This creates some interesting (shirt) textures such as this from random numbers:\\nimage1920×1920 157 KB\\nThis process is remarkably slow unless I open Character Creator by Reallusion. I don’t have any CUDA code in my apps because I do not know how, but when I open CC, what takes 3 minutes usually speeds up to 30 milliseconds (instant). Something in CC fires off GPU code and my app goes into super computer mode (I have an RTX 3090).What I would like to know is can I simulate this? Is there a C# way to just say ‘Activate GPU’ ?It’s the most unbelievable performance difference I have ever seen.I can’t tell people for a free app you have to open and close CC to get best performance.Thanks for any tips as to how this happens.I think I figured this out. It is not launching CC that makes it so fast, it is not updating the screen. If I minimize the window and come back the app speeds way up. It appears creating the image is not the slow part, but drawing it. I may change my code to not update until finished.Sorry to post. I thought it was some voodoo CC was unlocking.Powered by Discourse, best viewed with JavaScript enabled',\n",
       " 'access-to-all-vertices-in-the-miss-program': 'Is there a way to get access to all the vertices, which are available in the closest hit program within the miss program?There would be different ways to access the vertex attributes of geometric primitives of a scene inside the miss program but that would require some specific information.To state the obvious, if you reach the miss program there is no information available about geometric primitives or their indices per se since you missed all of them.So the first hurdle would be to know which geometry and which primitive you want to access.Now let’s assume you’re not using instancing, so no instance acceleration structures (IAS) inside your render graph. That would mean all your geometry is inside a single geometry acceleration structure (GAS) and you can simply store all vertex attributes (position, normal, texcoord, etc.) inside one or multiple buffers and the optional indices for the topology in case the primitives are indexed. That also means there is no transform hierarchy on top of the geometry, so object space equals world space.The pointer to these buffers can be stored inside the constant launch parameters which makes them accessible in all OptiX program domains when declaring that in each module accessing them.\\nWith that structure you would be able to access the object space vertex attributes in all program domains.\\n(That would be my recommended solution for single GAS use cases.)If you used instancing, the access to these would obviously get more difficult because there would need to be information about the instances including the effective transformation and the referenced geometry which could be used to access each of the geometry’s vertex attributes similar to the single GAS case, just with some array of the necessary information inside the launch parameters.OptiX added some getter functions for the transform hierarchy when you know the traversable handle, like optixGetInstanceTransformFromHandle() to get the transform matrix from one instance.\\nhttps://raytracing-docs.nvidia.com/optix7/guide/index.html#device_side_functions#device-side-functionsWhen using more than a two-level AS hierarchy (IAS->GAS) this would need more information about the transform list along each unique path through the render graph to the instanced GAS.You could also store a pointer to the necessary buffers of the whole scene inside the shader binding table miss record data and access that via the optixGetSbtDataPointer() function which is available inside all program domains.\\nMind that miss programs are per ray type.\\nI would rather store these pointers to buffers (global memory) inside the launch parameters. Mind that the launch parameter block is in constant memory which is limited to 64kB. Keep it small.For the vertex positions only, these can also be retrieved from the GAS when using the OPTIX_BUILD_FLAG_ALLOW_RANDOM_VERTEX_ACCESS, but that comes with some caveats described here:\\nhttps://raytracing-docs.nvidia.com/optix7/guide/index.html#device_side_functions#vertex-random-accessoptixGetTriangleVertexData is available in all program domains according to the device functions table chapter link above, but that requires information about the GAS traversable handle, number of primitives, and sbtGASIndex when using multiple SBT records per GAS.As you see, this can get a little complicated with instanced geometry.Now with all that said, what exactly would be the use case for vertex attribute data access inside a miss program?\\nMaybe there is a better way for what you would want to achieve.Powered by Discourse, best viewed with JavaScript enabled',\n",
       " 'layered-materials-also-bump-mapping-example-please': 'Hello all\\njust starting out with Optix and wondered if there was an example of layering one material on top of another via an alpha mask also a simple example of bump mapping via a texture lookup.Many thanksHi,I don’t think examples of these would/should be necessary. You should be able to implement them given a basic understanding of the techniques you desire.For bump-mapping simply google bump-mapping and there are many tutorials on how to implement this in OpenGL or Direct3D. You can do the same with OptiX and you will learn more on the way. From a brief view of a few results this looks like a good place to start: [url]http://www.opengl-tutorial.org/intermediate-tutorials/tutorial-13-normal-mapping/[/url]For layers materials you will probably want to use the OptiX v3.0 callable-programs and create a shade tree. There is a basic example for using callable programs for this already.EDIT: Also you will find there are new things to consider when it comes to bump-mapping in ray-tracing that are not as obvious from GL however. For example reflections of a bumped map surface can cause many self intersections, to resolve these you would need a costly displacement map implementation which is a whole can of worms in performance/implementation. The simplest method is to clamp the reflection vector to the planes surface and cannot face into the surface etc.Thanks Crog for the explaination and great link.Powered by Discourse, best viewed with JavaScript enabled',\n",
       " 'telsa-gpu-and-datacentres': 'Looking for some help in knowing exactly what I should be asking a Datacentre based on my requirements.where can i find out what cuda versions are compatible with telsa cards and operating systems?  it may sound an obvious question but checking the driver download page for example produces anything for download and we had already wasted alot of time trying to run a older card M2090 on windows server 2012 r2 using cuda 9.1 didnt work and found that out in a forum where they said support for cuda stopped at ver 8 for that card.look forward to any help trying to get the real information about telsa card/cuda version/OS version compatibilityPowered by Discourse, best viewed with JavaScript enabled',\n",
       " 'video-nvidia-driver-is-not-detecting-my-graphic-card': 'I’ve installed the proprietary driver(video-nvidia) for my gtx 960m graphic card on manjaro(arch base) linux using\\n$ sudo mhwd -auto pci nonfree 0300\\ncommand.In the booting screen under the /dev/nvme0n1p3: clean, .... blocks message, it says [FAILED] Failed to start Load Kernel Modules in a moment and the system boots up.I also had nvidia-settings could not find the registry key file or the X server is not accessible error and fixed it by following this thread: [HowTo] Fix the ERROR: nvidia-settings could not find the registry key file or the X server is not accessible - Tutorials - Manjaro Linux Forumhere is the result of sudo nvidia-bug-report.sh\\nnvidia-bug-report.log.gz (113.8 KB)and here are some other information that you might need based on this thread: Nvidia-settings incomplete on optimus laptop@generix can you help me please?The driver is installed but doesn’t load. First of all, please delete /etc/X11/xorg.conf.d/90-mhwd.conf since this would break your internal display once the driver loads. Then please post the output ofAlright I removed the link and grep didn’t find anything@generix what should I do?Powered by Discourse, best viewed with JavaScript enabled',\n",
       " 'tracenv-only-uses-the-4-lowest-bits-from-sbtrecordoffset': 'So i have run into a really annoying driver bug: traceNV only uses the 4 lowest bits from sbtRecordOffset. Any given offset is masked with 15 before it is used in the actual computation. This is a pretty bad restriction on the types of raytracing shaders that one can run.Repro:\\nTake any working raytracing shader and simply add any multiple of 16 to the sbtRecordOffset parameter. This should break the shader, but it does not affect it at all.\\nAlternatively, raytrace against a triangle that just outputs a color stored in the SBT, have 32 SBT entries and try to select them with sbtRecordOffset. Any color in an SBT at index 16 or above will never show.I can provide a repro app if you require one.My system:\\nWin7 x64\\nRTX 2070\\nDriver 417.63\\nMSVC 15.9.7Powered by Discourse, best viewed with JavaScript enabled',\n",
       " '344-75-driver-error': 'the following code compiles and runs on earlier version of the drivebut generates the error\\n0(100) : error C7531: global variable gl_ClipDistance requires “#extension GL_ARB_compatibility : enable” before useon the latest driver from the copy of clip distancesIf I try to enable this extension I get an errorPlease post the code and error message of the corrected shader.code compiles if I remove gl_ClipdistanceThat’s not what I meant. According to the former error message you must add #extension GL_ARB_compatibility : enable to your shader code to be able to use gl_ClipDistance. But you said “If I try to enable this extension I get an error”Now 1.) what is that new error and 2.) what is the complete(!) shader code you used when it happened?\\nReproducibility is the key here. A single function won’t help.\\nThat’s the only thing which could be investigated since the driver told you that it won’t compile the original code without the extension enabled.I can clarify a bit more for you. This code compiles and runs on Windows 8.1 but not on Windows 7.0.Here are the listings and errors from Windows 7.0other versionThanks, would you please also add the exact hardware configurations of the two systems?\\nThat information is required to be able to setup a matching reproducer.Under Windows the resulting text file after NVIDIA Control Panel → Help → System Information → Save contains the information for 1 to 3.If it’s the same machine just booting the different OSes, 2., 3., 4. are only needed once if the settings are identical.I am just dual booting with the same driver installed in Windows 7 and Windows 8.1. It is a 3 monitor system 2 Benq G2420HD and a Benq GL2440H setup as a continuous screen all at 1920 x 1080. I have the default driver setting from the install.The system is a dual Xeon X5650 with 24GBPowered by Discourse, best viewed with JavaScript enabled',\n",
       " 'how-to-obtain-satd-cost-from-nvenc-under-motion-estimation-only-mode': 'I want to obtain the satd( sum of absolute transformed differences) cost of each macro block  from nvenc under ME only mode, is there anyway to get it?  What does the mbcost field mean in the NV_ENC_H264_MV_DATA struct? Is it SAD? Is there any detail document about the motion estimation in nvenc?Hi,\\nNV_ENC_H264_MV_DATA::mbCost field is SATD cost.Thanks.Powered by Discourse, best viewed with JavaScript enabled',\n",
       " 'optix7-4-optix-samples-optixopticalflow': 'The results of optixOpticalFlow is different from the picture which under folder of “OptiX SDK 7.4.0\\\\SDK\\\\optixDenoiser\\\\motiondata”. For example:\\n\\nIt seems that optixOpticalFlow method cannot be used to calculate flow data for temporal denoising.Hi @yuezl_2012, welcome!Thank you for the bug report, I was able to reproduce this issue. This is not the intended behavior. It will be fixed ASAP and released in an upcoming driver.–\\nDavid.Hello again @yuezl_2012,The denoiser team further investigated the bug report I filed for this issue, and responded that they believe even though the flow image results look different than the example provided, the flow result is still valid and can be used for denoising. They are recommending to ignore what the flow image looks like, continue using the optical flow results, and check the denoised output results to determine whether optical flow is working. Will you please check your flow-based denoising results and let us know if that part is working for you?–\\nDavid.Hi! I processed the result and saved it in .gif format.\\nThe flow data that comes with the optixDenoiser example gives better denoise resultsPowered by Discourse, best viewed with JavaScript enabled',\n",
       " 'shield-tv-2015-rebooted-during-system-img-flashing': 'Hi guys,previous version 6.0.0, upgrading to 7.0.1. blob, boot and recovery images flashed fine. Board rebooted without a reason while flashing system.img. Now there is no video output and I am unable to enter fastboot. The question is what are the other methods of partitioning built-in NAND?Regards,\\nM.Powered by Discourse, best viewed with JavaScript enabled',\n",
       " 'bios-flash-2080ti-non-a-chip': 'Hello.Hi.  I hope I’m right here, I was referred here by support.  I have had a GeForce 2080 TI since the release.  this appeared in two types: with an A-Chip and a non-A-Chip.  The A-Chips were among others on the Founders Edition and some OC editions.  The BIOS differ with the respective cards / manufacturers in terms of the PowerLimit.  For the non-A cards, it is 280 watts (occasionally 310 watts).  The A cards are available with 380 watts and more.  It is possible to flash the BIOS between each other, but only the A-BIOS on an A card and the non-A BIOS on a non-A card.  In the 2070/2080, the distinction between the two types of chip was abolished over time.  Then there were only A cards.  For old “non-A cards” the board partners received a BIOS that canceled this.  With the 2080 TI, however, this limitation still exists.  Since I use water to cool my card and thus there are reserves, I would like to increase the power limit.  Does NVidia have the option of canceling the lock or the limitation or an adapted BIOS?Powered by Discourse, best viewed with JavaScript enabled',\n",
       " 'vkenumeratephysicaldevices-only-lists-1-of-3-gpus-from-x-env-all-3-from-tty': 'Ubuntu 16.04, Vulkan SDK 1.0.65.0, Beta Driver 387.42.01 (same Behavior with genereal release driver 387.34).\\n3 GPUs in a dual CPU system (i.e. 2 GPUs on one CPU, 1 on the other).vkEnumeratePhysicalDevices lists only 1 of the GPUs if the program is run from an X environment.\\nIf the same program is run from the TTY (i.e. Ctrl+Alt+1 into text mode), it lists all 3 GPUs. The X server does not have to be shut down.VkDeviceProerties for the GPU found from X environment:VkDeviceProerties for the GPUs found from TTY:nvidia-smi:Note that vulkaninfo cannot be used to query device data from TTY, since it terminates with an error if run outside an X env. The following program can be used instead:what is this ? I have the same with you, vulkuninfo cannot find all my video cards, I have two TX1080, but ubuntu vulkaninfo only find one 1080, and it can find all in tty, what wrong?jascha5bn4s,We already have filed an internal bug to track this issue. thanks for the report!jascha5bn4s，我们已经提交了一个内部错误来跟踪这个问题。谢谢你的报道！about this， when can you fix this bug?Powered by Discourse, best viewed with JavaScript enabled',\n",
       " 'interoperability-with-cuda': 'Optix programming guide described two context sharing ways. My program uses optix to update a device array and will take the first way: CUDA creates context before Optix initialization.According to programming guide, \" The application has created a CUDA context (by performing some CUDA runtime operations) prior to OptiX initialization. OptiX will latch on to the existing CUDA context owned by the runtime API instead of creating its own.\"In this case, am I allowed to call? If not, what should I do to launch the optix programs?Currently, I called these functions in optix. The device array created in CUDA can be passed to optix and read normally. But after optix hands the array(pointer) back to cuda, the pointer never works well.Powered by Discourse, best viewed with JavaScript enabled',\n",
       " 'steam-for-linux-client-not-showing-ui-with-535-43-02-245': 'When running steam-for-linux on with my 2070 SUPER steam loads up normal. I can even play games by using the menu from the tray icon. But no window is showing up anywhere. No store. No library. Any ideas?See this ticket.Hi there @enti1 and welcome to the NVIDIA developer forums.If the games itself are running ok I doubt this is directly related to NVIDIA drivers. But there have been changes to how Steam renders the GUI itself.I will check internally, but I can’t promise anything, especially since you seem to be using a Beta version of the Steam client.One more thing @enti1, can you share your system specs, like CPU, Desktop vs Laptop, which version of Artix Linux, and if possible run nvidia-bug-report.sh and attach the output here.Thanks!Computer Information:\\nManufacturer:  ASUSTeK COMPUTER INC.\\nModel:  PRIME X570-P\\nForm Factor: Desktop\\nNo Touch Input DetectedProcessor Information:\\nCPU Vendor:  AuthenticAMD\\nCPU Brand:  AMD Ryzen 7 3800X 8-Core Processor\\nCPU Family:  0x17\\nCPU Model:  0x71\\nCPU Stepping:  0x0\\nCPU Type:  0x0\\nSpeed:  4558 Mhz\\n16 logical processors\\n8 physical processors\\nHyperThreading:  Supported\\nFCMOV:  Supported\\nSSE2:  Supported\\nSSE3:  Supported\\nSSSE3:  Supported\\nSSE4a:  Supported\\nSSE41:  Supported\\nSSE42:  Supported\\nAES:  Supported\\nAVX:  Supported\\nAVX2:  Supported\\nAVX512F:  Unsupported\\nAVX512PF:  Unsupported\\nAVX512ER:  Unsupported\\nAVX512CD:  Unsupported\\nAVX512VNNI:  Unsupported\\nSHA:  Supported\\nCMPXCHG16B:  Supported\\nLAHF/SAHF:  Supported\\nPrefetchW:  UnsupportedOperating System Version:\\n“Artix Linux” (64 bit)\\nKernel Name:  Linux\\nKernel Version:  6.3.7-273-tkg-bmq\\nX Server Vendor:  The X.Org Foundation\\nX Server Release:  12101008\\nX Window Manager:  awesome\\nSteam Runtime Version:  steam-runtime_0.20230118.0Video Card:\\nDriver:  NVIDIA Corporation NVIDIA GeForce RTX 2070 SUPER/PCIe/SSE2\\nDriver Version:  4.6.0 NVIDIA 535.43.02\\nOpenGL Version: 4.6\\nDesktop Color Depth: 24 bits per pixel\\nMonitor Refresh Rate: 60 Hz\\nVendorID:  0x10de\\nDeviceID:  0x1e84\\nRevision Not Detected\\nNumber of Monitors:  1\\nNumber of Logical Video Cards:  1\\nPrimary Display Resolution:  1920 x 1080\\nDesktop Resolution: 1920 x 1080\\nPrimary Display Size: 23.54\" x 13.23\" (26.97\" diag)\\n59.8cm x 33.6cm (68.5cm diag)\\nPrimary Bus: PCI Express 16x\\nPrimary VRAM: 8192 MB\\nSupported MSAA Modes:  2x 4x 8x 16xSound card:\\nAudio device: Nvidia GPU 92 HDMI/DPMemory:\\nRAM:  32003 MBVR Hardware:\\nVR Headset: None detectedMiscellaneous:\\nUI Language:  English\\nLANG:  en_US.UTF-8\\nTotal Hard Disk Space Available:  937278 MB\\nLargest Free Hard Disk Block:  547942 MBStorage:\\nNumber of SSDs: 5\\nSSD sizes: 3000G,1000G,1000G,1000G,250G\\nNumber of HDDs: 0Since Artix is a rolling distro there isn’t really a version number attached to it.nvidia-bug-report.log.gz (5.6 MB)So Steam works “normally” when I’m not on the beta branch with the 534.43 driver. But as soon as I switch to the “hardware accelerated” beta branch it stops working. That’s a very strong indication that the driver is to blame. Especially since the beta of steam works “fine” with the 530.41.03 driver.Thank you!Right, I am not familiar with Artix, but makes sense that it has no version number. Commit hash might be something (just kidding). What might matter is the kernel version and that is already listed above.“Hardware accelerated” in this context means that Steam uses a built-in Chrome with GPU support to render the GUI as a Web-page. Which is known to have caused problems before.I will pass on your info, thanks again for bringing this up!A similar issue occurs on arch 535.43.03, No Steam GUI shows from their new web gui, the applet launches and games can still be launched.Steam is visible with steam -vgui (the older ui but the friends list does not work)The previous 530 drivers work with steam still however if you downgrade.Issue is present on stable and beta versions of steamHi All,\\nWe have filed a bug 4158137 locally for tracking purpose.\\nHowever, I am not able to repro on Arch Linux having kernel 6.3.8-arch1-1 with driver 535.43.02 and GPU GeForce RTX 2070.\\nCan someone please share repro video for better understanding of issue.\\nMeanwhile I will try to find similar system and GPU and will retry for local repro.Yesterday I upgraded my kernel toLinux eNTi 6.3.9-273-tkg-bmq #1 SMP PREEMPT_DYNAMIC TKG Wed, 21 Jun 2023 19:41:06 +0000 x86_64 GNU/Linuxand my video drivers tolocal/nvidia-dkms-tkg 530.41.03-247and at least the new release client of steam is working. Haven’t tried the latest beta yet.But there’s other issues, like random flickering (dual monitor issue?) and VEEERY long shader compiliation times in CSGO (with -vulkan). Is the shader cache not persistent between driver versions? I know… I should probably open another thread for that.Your dmesg log is full of:Which suggests that libcef (a Steam/Chromium component) is crashing on something.\\nThis seems like it could be similar to steamwebhelper invalid opcode with Arch NVIDIA 535.54.03-1 driver · Issue #9634 · ValveSoftware/steam-for-linux · GitHub. Can you please attempt to obtain a stack trace for comparison?Can you please try to run steam with -no-cef-sandbox argument to see if the problem goes away?\\nThank youJust in case, someone reported privately that setting __GL_THREADED_OPTIMIZATIONS=0 made the problem go away. Can anyone confirm?I’m on Arch Linux. Kernel is 6.3.9.arch1-1, Nvidia Driver version is 535.54.03-3.Can confirm that setting __GL_THREADED_OPTIMIZATIONS=0 makes Steam launch as expected.Maybe this issue is connected to some other problem I observe: Google Chrome(114.0.5735.198) has no hardware acceleration anymore with nvidia 535.54.03I hope this gives you an easier reproduction scenario.Chrome log on chrome://gpu shows:Console output of chrome shows:Setting __GL_THREADED_OPTIMIZATIONS=0 does not help.Rolling back to 530.41.03 and immediately hwaccel is back.\\nnvidia-bug-report.log.gz (377.6 KB)I’m on Arch Linux. Kernel is 6.3.9.arch1-1, Nvidia Driver version is 535.54.03-3.Can confirm that setting __GL_THREADED_OPTIMIZATIONS=0 makes Steam launch as expected.After several updates, I’m now on kernel 6.4.1-arch2-1, Nvidia Driver version is 535.54.03-6I sadly need to update here:\\nSetting __GL_THREADED_OPTIMIZATIONS=0 sometimes makes Steam launch, but it might take several tries until it finally works.I’m on the same version. Could not make steam open any window after several attempts even setting __GL_THREADED_OPTIMIZATIONS=0I’m also having the same issue on Arch based EndeavourOS.Powered by Discourse, best viewed with JavaScript enabled',\n",
       " 'vr-in-interior-design-field': 'Hello,I’m aiming to apply VR in Interior Design field,\\nand I need to know how the VR in Nvidia would help me.\\nAlso, I’m totally new to this so I would be glad if there’s\\nany guidance from you guys.Thanks in advance.Buthaina the VRWorks SDK has a number of APIs to improve image quality and performance for VR games and applications. You can also e-mail vrsupport@nvidia.com.Powered by Discourse, best viewed with JavaScript enabled',\n",
       " 'flex-direct3d11-get-set-buffer-issues': 'Hello, I started to use Flex 1.1 and had the following issues.I create a Flexlibrary instance and a Solver with Compute11 flag.Function works when using a buffer created with host, but when trying to pass a Direct3D11 Buffer using\\nNvFlexRegisterD3DBuffer some are working, some throw an access violation:I used either Default/Staging structured buffers for the tests (and made sure that size is big enough in every case.\\nIf needed I can provide reprothe following work passing a Direct3D Buffer\\nNvFlexGetActive\\nNvFlexGetParticles\\nNvFlexGetPhases\\nNvFlexGetSmoothParticlesthe following ones throw an access violation\\nNvFlexGetVelocitiesNvFlexSetParticles\\nNvFlexSetVelocities\\nNvFlexSetPhases\\nNvFlexSetActiveThanks\\nJulienThanks for the report Julien,We will take a look at this and try to repro this here. One issue may be that the structured buffers would need to have the expected layout, e.g.: float3 for velocities, int32 for active set, etc.Cheers,\\nMilesHello, thanks for the answerI made sure that for velocities buffers are set to float3 indeed (I actually tried both with float3 float4, but I generally made sure buffers are large enough too, since CopySubresourceRegion should normally be ok with buffer description .Please note I’m using 64 bits version (I did not try 32 bits), both debug and release give the same errorsI tried with all type of buffers (raw/vertex/staging) and the methods that works do with any type (as long as NvFlexRegisterD3DBuffer elemetncout/stride is set accordingly.Also NvFlexGetSprings in that case does not crash, but does not copy back any data.\\nI also have NvFlexGetNormals that gets me an Access violation as well (I made sure to use float4 in that case, copying back on an Host buffer was no problem).Also speaking about Direct3D11 interop, I was curious if it would be possible to use :\\nNvFlexUpdateDistanceField by passing a Volume texture at some point (Since I guess it gets copied into one anyway, but in my use cases a lot of my distance fields are already generated in compute shader, so that would save a round trip)In case of NvFlexUpdateTriangleMesh I guess the bounding volumes are done on cpu so it’s probably a non option.Hi Julien,I have sent you a private message.Cheers,\\nChenga have similar problem with NvFlexRegisterD3DBuffer for StructuredBufferThis is happening on NvFlexGetSmoothParticles\\ni feel like some of that difficulty was because of memory pool of structured buffer (default pool)As a side note, I did update to 1.2 beta, and with the new copy api the Get/Set specified above are working as intended.@mira :NvFlexGetSmoothParticles requires float4 (16 byte aligned).Powered by Discourse, best viewed with JavaScript enabled',\n",
       " 'can-i-link-rtx-2070-super-with-rtx-2080-super-with-nvlink': 'Hi,Can I link RTX 2070 Super with RTX 2080 Super with NVlink?Powered by Discourse, best viewed with JavaScript enabled',\n",
       " 'weaker-joint-drives-in-physx-3-3-3-when-compared-to-3-2-0': 'I’m simulating a hydraulic excavator in PhysX 3.2.0. The simulation models hydraulic rams as 2 bodies prismatically driven using a D6 joint (it’s loaded from repx so everything is D6Joints) with a damper, target velocity and limited force. I’ve configured the part masses to attempt to maintain a no more than 10:1 ratio between connected bodies. Arguably I should have modeled my system as an articulation but there are a lot of ‘loops’ and I believe at the time PxArticulation was still experimental.I’m now attempting to upgrade my simulation to 3.3.3. Having dealt with the various interface changes I’ve got my system running but I’m finding that a lot of my hydraulic rams are weaker than they previously were (by a factor of about 6). Why would this be happening?I’ve experimented with increasing the force limit but this doesn’t seem to make them any stronger.\\nThe target velocities are not particularly small.\\nI’ve experimented with eDEPRECATED_32_COMPATIBILITY (Which I gather only applies to hard limits) and eDRIVE_LIMITS_ARE_FORCES which should only change things by a factor of 120 for my 120Hz simulation.\\nI’ve tried upgrading to PhysX 3.2.5 which seems to work a lot more similarly to 3.2.0.At the moment my conclusion is that 3.2.0 can solve the system I have set up reasonably accurately and 3.3.3 cannot. This seems like a pretty significant regression, however, so I’m wondering if I’ve missed something.Powered by Discourse, best viewed with JavaScript enabled',\n",
       " 'traversing-order': 'Hi,I’m looking to report each unique SBT hit along a ray.\\nMy plan is to have a buffer per GAS, to use an anyhit program with optixGetInstanceId() to decide which buffer to write to and then optixGetSbtGASIndex() to decide the index in the buffer.Since I’m only interested in one hit per SBT I need an efficient way of skipping the subsequent primitives. Is there an easy way to do this? My first thought was to store the indexes on the ray and then compare to check if the next hit is on the same SBT and instance. But that would require the tracing to be linear to such an extent that all hits within an SBT is reported in sequence. Is this true?Would you maybe know of any other efficient concept for this?Thanks,\\nOscarI’m looking to report each unique SBT hit along a ray.\\nSince I’m only interested in one hit per SBT…Could you reformulate that question by describing exactly the desired data layout of the result vector?I have too many open questions why you would need this and what the resulting output from a launch should be.Do you need this per ray, or do you only need to know which SBT hit records are touched by all rays in a launch?\\nYou do not care about stopping rays anywhere? (Like X-Rays)This is purely for geometric analysis and nothing graphical.The final result would be how much a specific geometry obstructs one or more targets in relation to one or more viewpoints.So each ray is first traced on a traversable with my targets, the ones that miss don’t report anything, the ones that hit triggers a second pass. The second pass has the distance to the target as tmax, the target index in the prd and is traced on a IAS traverable with one or more “obstruction” GAS. I then have as many HitBuffers as “obstruction” GAS instances in the obstruction IAS, which are target count * obstruction count in size. For each unique SBT index/unique mesh along the ray I then want to atomicAdd toHitBuffer[obstruction GAS index][target count * SBT Index + target index]There might in the future be of interest to also incorporate the viewpoint index but leaving that for now.\\nAs a result I will then be able to query how much geometry number x in gas number y obstructs target number z.Hope that explains my intent.Hi Oscar,My first though was to store the indexes on the ray and then compare to check if the next hit is on the same SBT and Instance. But that would require the tracing to be linear to such an extent that all hits within an SBT is reported in sequence. Is this true?Any-hit shaders are not guaranteed to be called in depth order, and in practice you will see them called out of order. To guarantee you have a hit program called in depth order, you would need to use closest-hit, and potentially re-launch your rays.I think your any-hit plan sounds reasonable though, with slight modifications, so worth trying it out. My gut reaction is that you might see some contention using atomicAdd() that could slow things down, so it could be worth thinking about alternative approaches, depending on how efficient this part of your pipeline needs to be. With the any-hit approach, you would probably need to keep track of and sort the hits you want to remember via your payload, and wait until the ray traversal is complete before writing into your HitBuffer structure in raygen.Another way to structure the problem might be to cast a kernel of primary closest-hit rays, with any-hit shaders disabled for extra performance. And then collect a buffer of hits to relaunch in a 2nd kernel, and this time use an any-hit program that ignores intersections that match the SBT index of the first hit. Your closest-hit program would finish the job by recording the SBT & GAS index of the 2nd hit, which is guaranteed to be different from the first hit. You could do this without atomics, and after that, perform a reduction on the two buffers. This may or may not be faster than using atomics, it would need to be tested.Or you could mix the ideas and use a single kernel with any-hit, store the first and second hits into a buffer that matches your image pixel dimensions, and reduce on that single buffer after tracing is complete. You would need to perform your own depth sort on the two hits in your any-hit program as you go by comparing each incoming hit to the two stored hits and shuffling the stored hits or discarding the incoming hit as necessary - takes a little work but I would guess is doable.–\\nDavid.Hi David,Thanks for you suggestions! Will look into the double kernels, I guess that also gives me more coherent rays per launch compared to the single kernel doing both jobs.Any-hit shaders are not guaranteed to be called in depth order, and in practice you will see them called out of order. To guarantee you have a hit program called in depth order, you would need to use closest-hit, and potentially re-launch your rays.The dept order wouldn’t really help because I can’t ensure that multiple meshes won’t intersect. I just want to know if the mesh has at least one face that intersects with the ray.Tested it now with the “store only previous index approach” and it’s double counting just as you say, when multiple meshes are close to each other. So I guess I have to collect all the unique hits along the ray some way in the prd to make sure I only count each SBT once.Here’s just a test with 4 viewpoints looking down with the ground plane as a target. View cover from the ball meshes is graded from high-red to low-green.\\n\\nObstruction_Example.png1268×669 189 KB\\nThanks\\nOscarCool picture!The dept order wouldn’t really help because I can’t ensure that multiple meshes won’t intersect. I just want to know if the mesh has at least one face that intersects with the ray.Ah, so I think I misunderstood your question at the top of the thread about the linear ordering, I thought you meant depth order. I’m guessing the answer is still the same either way, that no linear ordering of anything is guaranteed when it comes to any-hit programs.So I guess I have to collect all the unique hits along the ray some way in the prd to make sure I only count each SBT once.This is another one I might not have grokked correctly. I had it in my head that you only wanted the first two unique SBT entries, where an SBT entry is a proxy for a unique combination of mesh & material. If you need all of them along a ray, and there can be an arbitrary number, it gets a little harder, of course.There’s a sample in the old Advanced Samples repo that does something a little bit like what you’re suggesting. Not the same, only vaguely similar. But might be worth at least glancing at. This sample is rendering transparent particles, and in order to get the compositing approximately correct, it collects a buffer of hits along the ray, and then sorts the buffer by depth order in raygen, after tracing the ray. It blindly appends to the buffer, and will silently discard hits once the buffer is full. One could be slightly smarter about it and evict any hits with a t value greater than the incoming hit. You could also imagine using a small hash table of SBT indices rather than unique hits.optix_advanced_samples/raygen.cu at master · nvpro-samples/optix_advanced_samples · GitHuboptix_advanced_samples/material.cu at master · nvpro-samples/optix_advanced_samples · GitHub–\\nDavid.Thought I’d post the Anyhit program I’ve ended up with for my second trace:The prd holds an array of previous hits which is checked against to make sure the current geometry hasn’t been seen before. Looping through the array in order of last seen assuming it’s most likely that we’re hitting a recent geometry again.\\n\\nimage1548×502 46.7 KB\\nBut I still haven’t implemented a double kernel launch so there is a big difference in computation between the threads that hit or miss in the first pass, which is bad for performance if I understand correctly. @dhart would you have any tips or examples on how to store the data from the 1st kernel and reduce the buffer so a 2nd kernel only traces the rays that hit something in the 1st kernel.Another way to structure the problem might be to cast a kernel of primary closest-hit rays, with any-hit shaders disabled for extra performance. And then collect a buffer of hits to relaunch in a 2nd kernel, and this time use an any-hit program that ignores intersections that match the SBT index of the first hit.Thanks\\nOscarPowered by Discourse, best viewed with JavaScript enabled',\n",
       " 'iray-for-cinema-4d-r18-and-the-mdl-library-missing': 'I was watching the Iray presentation on the C4D R18 event, I see when the presenter talks about the library, and I notice he mentioned there are like 200 presents, The way the library is display in his computer does not match the way is displays on mine computer. Wondering what is wrong?Also I want to now, as somebody from support told me that fully compatibility will be available at the end of September for Iray and Cinema 4D R18My Iray displays this info:when I go to my renderng settings I see this About Iray\\nPlugin version: 1.0.30\\nIray version: 2015.3So I am wondering if I’ve got the latest or not? and where (website) to check for the latest version.Help is appreciated. Thx ;-)The main hub for all Iray and MDL questions is the site of the NVIDIA Advanced Rendering Center: http://www.nvidia-arc.com.\\nThe related DesignWorks site on developer.nvidia.com which leads to the developer tools is this: [url]https://developer.nvidia.com/designworks[/url]\\nYou’ll find dedicated forums for related software products here: [url]https://forum.nvidia-arc.com/forum.php[/url]Powered by Discourse, best viewed with JavaScript enabled',\n",
       " 'simulating-the-sun-using-an-sdk-example': 'Hello, I am developing a project which finds the footprint of sun on a certain object. I started with optixBoundValues example in SDK examples of Optix 7.2.\\nIn my simple scenario, there is a light source and a flat plane. Light is up in y-axis away from the center of the plane and this generates a footprint on the plane with v1(75,0,0) and v2(0,0,75).\\nsunLighting11054×762 86.5 KB\\nWhen I move the light in just x-axis, I expect an elliptical-like footprint. However, the initial footprint moves along with the light move. As far as I understand, it happens because I didn’t change v1 and v2 vectors defined in light struct. Example output of my code is below.\\nsunLighting21031×808 69.8 KB\\nMy main goal is to simulate the sun. In real scenarios as you may guess, when the sun moves, its footprint changes elliptical to circular and then circular to elliptical again, not moves with the sun. As I understand, this is controlled by the v1 and v2 vector in SDK example. If so, first I would like to ask how I can calculate these v1 and v2 between the light source ( the sun) and the plane. Else, I am open to any suggestions.\\noptixBoundValues903×505 34 KB\\nAlso, is there a reference paper in the literature for this calculation in the SDK example?I hope I was able to explain the problem clearly.Thanks a lot in advance.Hello @gokkayae and welcome back.If you don’t mind I am moving your post into the Optix forums, they have more experience with these specific SDK examples.Thanks!Thanks. I actually understand how the light struct works. The code constructs the parallelogram light source using corner, v1 and v2.\\nNow I just wonder the reference of calculation in .cu file for further reading.To your screenshots:\\nFirst of all that small light shape looks like a triangle, not like a parallelogram.\\nThen the lighting effect on your surface is not at all where it should be if that parallelogram light is the only light in your scene.Looking into the optixBoundValues example, that is implementing a very simple path tracer with a Lambert surface material (seen by the cosine_sample_hemisphere() function in the device code) and a single parallelogram light source using direct lighting and four different material colors and emissions.If you look through the C++ sources, you’ll see that the light is represented in one part by the parallelogram data structure which is used for explicit light sampling only.\\nThis is the definition of the parallelogram light inside the host code:and this is the code doing the explicit light sampling for the direct lighting:Now that is not all. To be able to hit the light geometry implicitly (when the ray randomly goes into the direction of the light after sampling the continuation ray of the BRDF), it is also defined as two hardcoded triangles inside the geometry data here:You’ll see that these hardcoded coordinates match exactly.\\nBoth these places must be changed to have the explicit light sampling and the implicit light geometry hits work together correctly! I would guess you had only changed one of the two.Mind that when changing geometry positions inside the scene, the geometry acceleration structure must be updated or rebuilt! You cannot simply change the parametric representation of the light alone if it’s represented with geometry inside the scene.\\nhttps://raytracing-docs.nvidia.com/optix7/guide/index.html#acceleration_structures#dynamic-updatesI wouldn’t have hardcoded the geometry triangle but would have calculated the resulting triangles from the parallelogram parameters. Shown in this more advanced example: https://github.com/NVIDIA/OptiX_Apps/blob/master/apps/nvlink_shared/src/Application.cpp#L540Note that the implicit light hits will return the colors and emission values from the material index 3:and the emission of that is not white but RGB (15, 15, 5):and weirdly enough, the surface color of the light geometry (the last entry again) is not black. (I would change that as well.)Now with all that explained, if you want to simulate lighting by the sun there are better ways to implement that.I wouldn’t use a geometric primitive to represent the sun simply because you wouldn’t actually place that with the actual physical size and distance into the scene but would need to scale and place it nearer to not run out of floating precision bits of your scene units.Instead the sun is usually implemented as a directional light with a normalized direction vector from surface point to the light and a proper cone spread angle to simulate the solid angle of 0.53 for the sun “disk” when seen from earth.\\nMeans explicit light sampling would need to sample directions inside that cone only.Changing that direction vector would then directly represent the elevation and direction of the sun relative the earth surface point.There would not need to be geometry inside the scene for the sun, which means implicit light hits of the sun would be implemented inside the miss program by checking if the angle between ray direction and sun direction is smaller than the cone spread angle (that’s a dot product and a comparison).\\nThat effectively places the sun infinitely far away. Because there wouldn’t be any geometry representing the sun, there also wouldn’t need to be any rebuild of the acceleration structures required! You could simply change the sun direction inside your OptiX launch parameters and restart the rendering.The sun would always result in a perfectly circular shape when seen directly (except for projection distortions of the camera implementation).\\nThe effect on the surface, means the change from more circular lighting effects (when the sun and view directions are perpendicular to the surface, i.e. when surface normal and light direction and view direction are the same) to more spread out elliptical shapes is a matter of the bi-directional reflection distribution function (BRDF) and the angles between normal and view direction (to the observer) and normal and light direction (to the sun). This gets much more pronounced when using glossy reflections instead of purely diffuse materials.In addition to the simple sun disk light there would be much more elaborate sun and sky models which simulate the atmospheric scattering as well if you’d want to implement a more physically correct sun and sky lighting. Search for articles about Mie and Rayleigh scattering.Sorry for late response. There was something else i need to do.I implemented a code as far as i understand from what you suggest. The scenario is as below. where black line is surface normal, blue is incident ray, cyan is specular reflected ray (with 0.53 cone angle) and yellow is light direction.\\npanel.PNG1458×781 160 KB\\nCode is simple. If the light direction is inside the cone, that is acos of the dot product of the light direction and the reflected ray is less than 0.53, then add the weighted light emission. Else i set the diffuse color.I calculate the weight as below.\\nsphericalLightPosition948×342 9.57 KB\\nI select a random point on a sphere (Sphere Point Picking -- from Wolfram MathWorld) and then add to the light center. So, i find a random light position inside the sphere like the optix example does for parallelogram light position. If it is wrong or there is a better way, please tell me. Also, I am not sure about my assumption of LnDl is 1.0. I assumed so because i thought i did not need to rotate the sun towards the surface. In other words, i thought it was no matter how much a sphere was rotated. If wrong, what should the light normal be in order to calculate LnDl?The result of this implementation is below.\\nlightModel1369×489 65.5 KB\\nThe shape strictly depends on how the light position is selected. Also, even though there is an elliptical brightness in the center, whole shape still looks circular. As changing the observer (camera) location, i get different combinations of sun-surface-observer positions. But still, the shape on the surface is circular.Could you suggest any solution or refer me to a reference which mention how to handle this?What light transport are you implementing?Let’s name your vectors.\\nN == surface shading normal (black)\\nL == normalized direction vector to light, sun (yellow)\\nI == continuation ray of your BRDF sampling (cyan)\\nO == direction vector from surface to observer, negative ray direction (blue)Two cases:\\n1.) Explicit lighting.\\nThat’s when your continuation ray hits the sun. That condition can be checked very easily.The dot product of two normalized vectors is the cosine between these two vectors.\\nIf that value is 1.0, the angle between them is zero, if it’s 0.0 the angle is 90 degrees, if it’s -1.0 the angle is 180 degrees.\\nMeans if dot(L, I) == 1.0 then L == I and you hit the center of the sun with your continuation ray.(Note that mathematically, the probability to randomly hit an infinitely small point or a fixed 3D vector exactly, is zero (if the floating point precision wouldn’t be limited). These are singular lights (point and directional lights), which do not exist in the physical world and can only be handled with direct lighting.)If the sun has an angular diameter of 0.53 degrees when viewed from earth, you just need to check if the continuation ray is inside the cone with that angular diameter.Now to open that dot(L, I) condition to a cone you need to compare its result to a threshold slightly smaller than 1.0, means a bigger opening angle than 0.0.\\nThat threshold value is 1.0 minus the cosine of the half of the sun’s angular diameter, means\\nthreshold = 1.0f - cosf(degrees_to_radians(0.53f * 0.5f)); // == 1 - cos(radius).\\nThat’s a constant which can be calculated up front.The full check if the continuation ray hit the sun would be this:When not implementing direct lighting this will eventually converge against the correct result when shooting enough rays. This would be a brute force path tracer with no direct lighting. Because the sun’s solid angle is very small, this will need an enormous amount of rays though.\\nThis would be your reference against which you would need to compare any additional light calculations.\\nYou could make the sun radius bigger for tests to see that this works correctly.Else I set the diffuse color.That would imply the material is pure diffuse (Lambert) and there is another white environment light around your scene with no occluders. Don’t do that when debugging the sun’s contribution alone.Also, I am not sure about my assumption of LnDl is 1.0.Not sure what that is, but the normal comes into play during the continuation ray sampling and when contributing direct lighting.\\nThere is a cosine factor describing the falloff of light contributions depending on the incident angle of the incoming light direction inside the rendering equation.\\nhttps://www.pbr-book.org/3ed-2018/Light_Transport_I_Surface_Reflection/The_Light_Transport_Equation2.) Implicit lighting (“direct lighting”, “next event estimation”)\\nHere the light vector L needs to be sampled. To do that for the sun, you would need to generate vectors which are uniformly distributed inside the cone around the direction vector to the sun with the sun’s angular diameter.\\nThat’s effectively sampling directions on a sphere cap with the angular diameter of the sun.I would recommend reading the Physically Based Rendering book and its source code for light sampling:\\nhttps://www.pbr-book.org/3ed-2018/Light_Transport_I_Surface_Reflection/Sampling_Light_Sources\\nSampling cone directions is explained here:\\nhttps://www.pbr-book.org/3ed-2018/Monte_Carlo_Integration/2D_Sampling_with_Multidimensional_Transformations#SamplingaConeIf you have the sampled direction vector to the sun, you can then do direct lighting calculations for each surface hit point with it if the continuation ray isn’t already hitting the sun.Powered by Discourse, best viewed with JavaScript enabled',\n",
       " 'fusioncl-net-new-net-wrapper-around-opencl-visuals2017': 'FusionCL.Net is a new wrapper around OpenCL for C#(4.7.2) -The git is online, and it is working. It’s not 100% complete but you can create programs, run kernels, create membuffers and read/write them. It works fine for me on a Nvidia 1060.There are two projects on the git repo.FusionCLNative.dll - this is a C++ wrapper around opencl, you do not need to use this.FusionCL.Net - This is a C# set of classes(Namespace:FusionCL) that internally uses the native dll to create and use the OpenCL library.Like i said, not 100% complete, but it is working and usable. Only a small amount of code really, but it works well and I think is very easy to use.https://github.com/AntDevUnity/FusionCL.NETPowered by Discourse, best viewed with JavaScript enabled',\n",
       " 'h-265-and-h-264-what-would-you-suggest-for-the-following-scenario': 'We have about more than 22 “MonoChrome” GigE Vision Cameras streaming video with 1280x1024 resolution, (8bit/10bit). Data is captured by network card and we would like to use the most suitable technology available out there (Pascal/Maxwell/ Intel QuickSync etc) to encode those streams with 30fps rate.\\nI was wondering which pass would you pick ? also so far what I read is talking about YUC and I am not sure I am following it, does H.265 works fine with Monochrome images ?10-bit encoding is supported on Pascal, only for HEVC.  Therefore, if you need to perform 10-bit encoding, you’ll need to use one of the Pascal boards.Monochrome encoding is not supported in the GPU. If the decoder/receiver in your system is in your control, you can use standard YUV encoding capability but provide arbitrary chroma values to the encoder and discard them at the receiver/decoder side.Use one of the latest GP10x card having 2 NVENCs which will provide high throughput. For list of available GPUs, please see [url]https://developer.nvidia.com/video-encode-decode-gpu-support-matrix#Encoder[/url]Awesome! thanks for the info!\\nCan I use two GTX 1080 instead of Quadro p5000 ? (Limited budget)Yes you can, but note that systems with GeForce board are limited to 2 simultaneous encoding sessions (may not be suitable for your use-case).Thanks, can you elaborate Monochrome encoding a little more? I am not sure how I can discard at the decoder site?Powered by Discourse, best viewed with JavaScript enabled',\n",
       " 'is-typeduavload-attribute-configurable-on-my-k2100m-quadro-video-card': 'I got sent here when “Consumer Support” couldn’t answer my question.  I just bought a graphics software package that on startup tells me it can’t render 3D because my graphics card does not support it.  I just want to know if there’s a way to turn on the TypedUAVLoad attribute on my K2100M Quadro card or if I would need to get a new card. I tracked it down to this attribute with Google’s help, pretty sure I found it on the SW vendor’s support FAQ.  Seems to make sense since this is tied to DirectX and I’m running on Windows. If I need a new card, I would greatly appreciate a list of NVidia cards that will support it or some more general description of what to look for, like “Only GEForce cards support this” or whatever.  I’m using Home Designer Pro, a DIY version of Chief Architect, to design an addition.  I should be able to do 3D walkthroughs and want very much to have this ability.  Thank you very much for any information.  Peace.Powered by Discourse, best viewed with JavaScript enabled',\n",
       " 'are-continuation-callables-actually-necessary': 'Dear OptiX team,we realized the other day that code generation in our project places some recursive ray tracing calls into direct callables (this is done to find intersection information without recursive shading, so the stack growth is limited).We were surprised in retrospect that this works, and that OptiX even accepts such programs, since the documentation seems quite explicit that ray tracing is forbidden in direct callables.Turning all of our our project’s direct callables to continuation callables to anticipate such a potential case seems undesirable as they are apparently quite a bit slower. While looking into this, we also found it curious that other ray tracing APIs (e.g. DXR) don’t have a distinction between direct and continuation callables.Basically my question boils down to the following:Is the distinction between direct and continuation callables a relic needed to support some older hardware? (in the sense that we could ignore it when imposing some minimum hardware version). Is there any set of conditions in which we could safely trace rays from direct callables?Thanks,\\nWenzelHi @wenzel.jakob, sharp eye!You’ve discovered some recent internal changes we’ve made in support of the upcoming Shader Execution Reordering API. We’re not yet prepared to cover the new requirements and limitations publicly. We will be providing new guidance that should answer this questions and clarify with the next version of OptiX.The guidance in the Programming Guide still applies, and generally speaking it’s still best to stick to it for today’s callables. One reason your test happened to work is you didn’t use recursive shading, but (for the benefit of others reading this) continuation callables are still the only way to support recursive trace with recursive shading in a callable, the distinction is still relevant on current hardware. Note especially that even if it’s working, we don’t yet sanction tracing from direct callables. That might continue to work, or it might get broken for unforseen reasons. We have to be conservative and careful with which guarantees we make so you don’t get surprised.–\\nDavid.Excellent, thank you for the clarification. We will try to get our hands on one of the next-gen GPUs to be able to play with this once the new version of OptiX is released.Powered by Discourse, best viewed with JavaScript enabled',\n",
       " 'microfacet-based-normal-mapping': 'Hi,In the MDL SDK   “v-cavities masking and and importance sampling utility”  [url]https://github.com/NVIDIA/MDL-SDK/blob/3e6182bc3141ea79c10ddf092a081232ced8e5f3/src/mdl/jit/libbsdf/libbsdf.cpp#L594[/url] is implemented.I saw  “Microfacet-based Normal Mapping for Robust Monte Carlo Path Tracing” (also from Eric Heitz)\\n[url]Microfacet-based Normal Mapping for Robust Monte Carlo Path Tracing | Unity Blog including source code for mitsuba.Is that already implemented in the MDL SDK ? If not, are there any plans to add it ?\\nIt would be great!\\nAny answer/advice appreciated, how I could add it on my own (if its not present).thank you very much!hi,that is indeed a great paper. It is, however, not implemented in the MDL SDK and we currently also don’t have concrete plans to add it. The reason simply is that this would add quite some code, require some architectural changes, opens up some questions (e.g. how to handle normals in layering), and will come at the cost of some computational overhead. It is also not clear if every renderer will strongly benefit (some renderers already implement other tricks to handle strongly bent normals and don’t care for physical plausibility that much).You can of of course implement (at least parts of) it on you own: each elemental BSDF ends up with a normal and if the normal deviates from the geometric normal, you need the replace (or just “wrap up”) the elemental BSDF’s eval and sample functions (I would recommend implementing eq. 23).Thank you for your answer and the information.\\nI will try equation 23. Do you think any BSDF can be the nested one?  Or only diffuse reflection BSDF?Any BSDF can be used, as long as it only reflects (and does not transmit) light.While the paper only handles BRDFs explicitly, extending the technique to general BSDFs (including BTDF) should be possible (I think) but most likely requires some non-trivial extension.Great! Thanx for your answer.Powered by Discourse, best viewed with JavaScript enabled',\n",
       " 'nvidia-h100-trusted-execution-environment': 'There is a new GPU from NVIDIA called NVIDIA H100 Tensor Core GPU. One of the main features of this GPU is that it has hardware enclave (TEE). However, I was not able neither to find any manual/SDK on how to use it nor to purchase it in a cloud (AWS and Azure).Is there any manual/SDK to get acquainted with this GPU?Powered by Discourse, best viewed with JavaScript enabled',\n",
       " 'good-performance-in-bad-desktop-in-laptop': 'greetings to the forum and sorry for my English. render is in blender.\\nI have 2 computers one desktop with 16 gigabytes and an i7 processor 8700k and no dedicated graphics. the render of the amd processor logo goes fast in about 26 sg with 32.\\non the other hand the other equipment is a portable with 8 gigabytes and an i7 processor 8750H, and with graphics dedicated nvidea 1050 of 4 gigabytes, I have configured the render by cuda since it detects the gtx 1050 graph, but the render takes 1min 10seconds, that is to say practically double that of desktop and that the desktop does not have dedicated graphics, someone knows why?I do not think the difference in ram has anything to do since it is a scene that does not consume much.\\nThe 2 processors are 6 cores, and the 2 have win10 pro installed. and the desktop configured in maximum performance in battery options.Powered by Discourse, best viewed with JavaScript enabled',\n",
       " 'blitting-from-multisampled-fbo-to-a-multisampled-default-framebuffer-is-very-slow': 'While I was trying to solve a performance problem in my program, I found that the most time-consuming step is blitting color and depth buffer from a FBO to the default framebuffer. When both the FBO and the default framebuffer are in 1280x720 8xMSAA, it takes about 25 ms to finish blitting on a GT 640. If the blitting target is not the default framebuffer but another multisampled FBO, it takes no more than 6 ms.Using a fragment shader to copy two multisampled textures (color and depth) to the default framebuffer don’t have this problem. It’s almost as fast as FBO → FBO blitting.Can anyone explain why multisampled “FBO → default framebuffer” blitting is so slow?My driver is 347.09 on Windows 7 64 bit.Edit: Uploaded test code as attachment. By default it tests FBO → framebuffer blitting. If TEX_SRC macro is defined it tests texture → framebuffer copying with a fragment shader.\\nbuffercopy.cpp (7.38 KB)Powered by Discourse, best viewed with JavaScript enabled',\n",
       " 'anyone-see-a-pattern-51-59-63-83-87-95-99-101-111-117-123-125-135-143-147-glgeneratetexturemipmap-spikes': 'Reaching out for the strangest puzzle with glGenerateTextureMipmap() Spikes!When the texture height is h: 2, 4, 8, 16, 32, 64 …\\nglGenerateTextureMipmap() Spike shifts in y direction by exactly one pixel at the following widths:\\nw: 51, 59, 63, 83, 87, 95, 99, 101, 111, 117, 123, 125, 135, 143, 147 …It only happens at odd widths, but only those in the sequence.The Spiking pattern changes at non powers of two heights.When the height is odd, then the spiking happens in opposite x direction, in other width sequences.When the width and height are equal, the problem is gone, and also for powers of two, as conventionally expected.If I can get to the pattern, I would like to compensate for it in transforms.Are you sure your level of detail zero image is correct?\\nCheck your glPixelStore settings.\\nFor example RGB8 input data needs a glPixelStore unpack alignment of 1 but the default is 4 which will shift pixels in each line when using widths which are not multiple of four.Powered by Discourse, best viewed with JavaScript enabled',\n",
       " 'access-to-cloudxr-sdk': 'I’ve applied to the CloudXR SDK program, and didn’t received a response yet for a week or so.Also, I’ve sent an email to cloudxr-outreach@nvidia.com and didn’t hear from you just yet.You are probably busy, and don’t want to rush you, but I want to know if there’s anything I can do to make thing easier.My first use case is to do a POC of this technology, in my own free time, to show my current employer the possibilities of collaboration for VR in a 100% cloud based environment. (currently AWS), which allows more control and proper cost management.Hi David,Welcome to the NVIDIA Developer forums. I am trying to locate a CloudXR resource to help you. Please stand by.Thanks,\\nTomHi @david229  - I can help you out. We never received your email, can you please send me your email address that you applied with? You can send it to me via the email address that I just messaged you.VeronicaHi Veronica, I’ve just sent it 🙂@TomNVIDIA , @Veronica_NVIDIA  Good afternoon!\\nFor some reason, I was blocked from accessing cloudXR (( What is the reason for this? Could you open access again?\\nPowered by Discourse, best viewed with JavaScript enabled',\n",
       " 'i-have-problem-with-using-nvidia-smi': 'i want to use nvml with python , on wsl ubuntu and im trying to use nvidia-smi.\\nbut the problem is after i installed nvidia-driver , still im getting this error.\\nNVIDIA-SMI has failed because it couldn’t communicate with the NVIDIA driver. Make sure that the latest NVIDIA driver is installed and running.and i have problem with wsl.exe --update .\\nthe error is , Checking for updates…\\nThe service cannot be started, either because it is disabled or because it has no enabled devices associated with it.Hello @real_asgari and welcom to the NVIDIA developer forums.Please make sure you have read the instructions for CUDA and WSL. Make special note of sections 6 to 8 which might already help you find out if you can run NVML at all on your system.One typical pitfall is to install the NVIDIA driver a second time inside the virtual execution environment running on WSL. You MUST NOT do that. CUDA on WSL uses the standard Windows driver. This is also stated in the above documentation.If wsl.exe --update does not work I recommend uninstalling and reinstalling WSL and following the instructions from Microsoft in detail. Try the update command before doing any further software installation in your chosen Linux distribution.I hope this helps!Powered by Discourse, best viewed with JavaScript enabled',\n",
       " 'gas-rectangle-vs-triangle': 'I understand rectangle is the built-in shape for RT core, but what if it is for a GPU without RT core? Will there be a performance difference using triangle vs rectangle build with OptixAabb? To represent a rectangle with two triangles,  I will need 6 vect3, and for a rectangle, I only need 4. The memory benefits are straightforward.Also, I went through the optix example code, The only example of using OptixAabb is to build a sphere. I still do not quite know how to build a rectangle using optixAabb.\\nFollow are from the programming guide, but I still don’t know how to define rectangles for  d_aabbBuffer. Can you provide an example?OptixBuildInputCustomPrimitiveArray& buildInput = buildInputs[0].aabbArray;\\nbuildInput.type = OPTIX_BUILD_INPUT_TYPE_CUSTOM_PRIMITIVES;\\nbuildInput.aabbBuffers = d_aabbBuffer;\\nbuildInput.numPrimitives = numPrimitives;I understand rectangle is the built-in shape for RT core, but what if it is for a GPU without RT core?No! The only built-in geometric primitives in OptiX are triangles and curves (linear, quadratic and cubic B-splines).Please read the OptiX Programming Guide again. This is said in the second sentence of the first chapter OverviewWill there be a performance difference using triangle vs rectangle build with OptixAabb?For the acceleration structure build itself? No, not really.\\nBut for the runtime performance the performance difference between built-in triangle and custom primitives will be dramatic on RTX devices because these have hardware support for ray-triangle intersections,\\nFor devices without RT cores there are highly optimized intersection routines for triangles (and curves on all devices) inside OptiX which are hard to beat in performance at the implemented precision.To represent a rectangle with two triangles, I will need 6 vect3, and for a rectangle, I only need 4. The memory benefits are straightforward.That is actually the smaller part of possible memory savings.First, you don’t need to specify the triangles as independent triangle array with three vertices per triangle. You would normally define indexed triangles, which means you have the same four vertices and use six indices to build two triangles, sharing two of the vertices.\\nThis is especially efficient in fully connected triangle meshes where the internal vertices are reused for six triangles!\\nThis is the standard method for mesh definitions in rasterizer and raytracer APIs.The more important memory saving between these two geometric primitives is actually that a custom rectangle primitive needs only one axis aligned bounding box (AABB), and two triangles need two AABB. That can make quite a difference for the acceleration structure (AS) size and if the only concern is memory usage, it’s a valid method to use custom rectangle primitives with your own intersection program at the cost of a considerable runtime performance hit from not using hardware ray-triangle intersections.But also note that geometry AS can be compacted and esp. RTX devices can compress them very efficiently.Also, I went through the optix example code, The only example of using OptixAabb is to build a sphere. I still do not quite know how to build a rectangle using optixAabb.All custom geometric primitives are defined by an AABB per primitive you calculate and the intersection program you implement for them.Means you need to have a function which calculates the AABB over your four vertices defining your rectangle primitive and give that resulting array of AABBs per primitive as OptixBuildInputCustomPrimitiveArray to the optixAccelBuild() function as you already found inside the programming guide below.The primitive index you get inside your OptiX device program when hitting any of the AABBs is the same as the index of the AABB inside that build-input array.Follow are from the programming guide, but I still don’t know how to define rectangles for d_aabbBuffer. Can you provide an example?The calculation of the AABB for for point based primitives is super easy.  You only need to find the minimum and maximum x-, y-, z -components of all your positions per primitive. That’s all.The OptiX SDK example optixVolumeViewer builds AABBs for boxes. It’s always the same method.You can implement that on the host or with a native CUDA kernel on the device if it needs to be much faster.\\nIn the end, the d_aabbBuffer must be on the device, so if you calculated it on the host you need to copy it from host to device with a cudaMemCpy() or cuMemcpyHtoD() depending on which CUDA API you use (runtime or driver API).In the old OptiX API before version 7.0.0 you needed to specify a “bound box” program for that and before there where build-in triangles (since OptiX 6.0.0) you needed that for triangles as well. Here’s an example in one of my old OptiX 5.1. based examples:\\nhttps://github.com/nvpro-samples/optix_advanced_samples/blob/master/src/optixIntroduction/optixIntro_03/shaders/boundingbox_triangle_indexed.cu\\nFor indexed rectangles that would obviously be the same routine just with four vertices. Port that code to the host and adjust it to your rectangle primitive definition.The more complicated problem is how to implement an intersection program for rectangles.\\nIf they can be arbitrary and not even planar, that is going to become interesting. If they are special cases like a parallelogram, there are examples for that inside the older OptiX SDKs.With all that said, I seriously recommend using indexed triangles to represent your rectangles, simply for performance reasons and because you don’t need to implement a ray-rectangle intersection routine.(The OptiX 7 SDK’s optixPathTracer example defines the Cornell Box with rectangles but then uses two independent triangles (not indexed) to build the acceleration structures.\\nThe one in the older OptiX SDKs used a parallelogram primitive for the area light, but that is defined by an anchor point and two edge vectors.)Thanks for all the answers. It is very useful.\\n1.I knew built-in geometric primitives in OptiX are is triangles, that was a typo …sorry. That is why I asked the question performance vs memory trade-off.\\n2. So conclude with your answer, using triangle will be more optimized in terms of performance with or without RT cores. But the AS size of 2 triangles will be larger than one custom primitives for a rectangle? Can I say the memory usage is double ?\\n3. Is there an example for indexed triangles? Follows are a simple rectangle I learn from the example in optixPathTracer\\nHow can I turn it into an indexed triangle?\\nconst std::array<float3, 6> vertices =\\n{ {\\n{ -1.0f, 1.0f, 0.0f },\\n{ -1.0f, -1.0f, 0.0f },\\n{  1.0f,  1.0f, 0.0f },\\n{ -1.0f, -1.0f, 0.0f },\\n{  1.0f,  1.0f, 0.0f },\\n{  1.0f,  -1.0f, 0.0f }\\n} };1).I knew built-in geometric primitives in OptiX are is triangles, that was a typo …sorry.Focus! :-)I don’t think so, but I haven’t made that experiment. If you compact the GAS, there can be a lot of savings which depends on the size and spatial structure of the primitives. RTX devices compact better. YMMV.You would simply convert the hardcoded geometry data from independent triangles (the 6 vertices) to the four corners of the rectangle (use counter-clockwise winding) and then generate an index buffer which references into vertex pool and the indices should be ordered with (0, 1, 2) and  (2, 3, 0) to build two triangles from a rectangle.All my OptiX 7 examples are using indexed triangles https://github.com/NVIDIA/OptiX_AppsThe simplest is the runtime generation of a cube with 12 triangles which does exactly what I described above and generated the indices at the end:\\nhttps://github.com/NVIDIA/OptiX_Apps/blob/master/apps/intro_runtime/src/Box.cppJust that my VertexAttributes are holding vertex position, tangent normal and texcoord attributes. You can ignore the ones you don’t need. Concentrate on the lines assigning attrib.vertex.\\nAlso look at the Plane.cpp, Sphere.cpp, and Torus.cpp files for more runtime generated geometries.Code which builds a geometry acceleration structure from that host data:\\nhttps://github.com/NVIDIA/OptiX_Apps/blob/master/apps/intro_runtime/src/Application.cpp#L1331With compaction in one of the more advanced examples (also using a simple arena allocator):\\nhttps://github.com/NVIDIA/OptiX_Apps/blob/master/apps/nvlink_shared/src/Device.cpp#L1189Powered by Discourse, best viewed with JavaScript enabled',\n",
       " 'invalid-enum-given-by-rtcreatetexturesamplerfromglimage-after-driver-update': 'I had a program which created a texture sampler from a 3D texture but after I updated the driver I am getting an Invalid Enum error. My machine uses a GTX 1080. Unfortunately, I don’t manage this computer and all the “Help” desk told me about the driver update is that they pushed a “newer” version of “NVIDIA driver”. Here is the failing codeand here is the errorFull GPU specs as told to me by the sample code I stole from NVIDIA samplesAny help would be extremely appreciated as I have a presentation of this program coming up shortly.I forgot some likely helpful information. I am using Optix 6.0.0 and Cuda 10.1 and GLFW3Commenting out that section, just to see if the rest would run, reveals that it is not only a problem with 3D textures, it also fails on 2D textures with the same error code.Hi @alrowden,Does the optixTextureSampler program run for you, or produce the same GL error?–\\nDavid.Perhaps, somewhat related problem?Hello,  I recently updated Geforce driver for RTX 2070 to 430.86.  Now OptiX fails to create a buffer from an OpenGL buffer object.  I made a minimum sample to reproduce this problem. ...Sorry about the delay. I convinced the Help desk to roll back my drivers temporarily so I cannot debug right now. I will update this when I can take the necessary debug steps. Sorry about the delay.Powered by Discourse, best viewed with JavaScript enabled',\n",
       " 'why-rasterizer-is-faster': 'Hi,Using OptiX for raytracing, “Rasterizer is faster than ray tracer in recent GPUs on PC”, is that ALWAYS true? Considering the speed for just drawing constant colors for rasterized triangles and for primary hits (Triangle Only).When complexity and organization of scenes changes, will the observation change? Considering rasterizer is only a small part on hardware, whereas the rest of part can be for ray tracing, are there situations, ray tracer overwhelms rasterizer? If the rasterizer always win, what are the inherent issues of ray tracer make it fail?ThanksDepends simply on the use case and underlying hardware.Rasterization is similar to shooting primary rays only and since these are very convergent the performance is great and there is no overdraw inside the ray tracer, which comes at the expense of the BVH traversal.>>When complexity and organization of scenes changes, will the observation change?<<Simple thought experiment:Imagine a simple scene setup with 1 million instanced spheres, each consisting of 100,000 triangles, gives 100 billion triangles in the scene.\\nLet’s assume a rasterizer can handle 1 billion triangles per second, then this scene would need 100 seconds to raster just because of the vertex load, mostly independent of the viewport size or overdraw.\\nNow in a ray tracer (store the sphere geometry only once and instance it a million time otherwise the memory for the BVH would blow up) the same case would scale with the viewport size.\\nI’ve rendered a scene like that with about 1 fps at 1024x1024 some years ago, and that was with a specular material and multiple bounces.\\nSee the “Instancing with Transforms” image on slide 5 here: [url]http://on-demand.gputechconf.com/gtc/2016/presentation/s6244-roettger-optix-mdl.pdf[/url]Throw in some features like reflections, transparency, refractions, shadows etc. and the amount of work to emulate that in a rasterizer accurately(!) will become unwieldy where ray tracing handles this naturally.Powered by Discourse, best viewed with JavaScript enabled',\n",
       " 'desktop-vs-shield-discrepancy': 'Hi!I am trying to create a very minimal Vulkan example and have run into issues in that it works seemingly OK on desktop (Linux GeForce GT 720) but not so good on my Nvidia shield device.The example more or less creates an image, draws a grid into it (by memory mapping and using the CPU), acquires a swapchain image and copies into that swapchain image.Validation layers are enabled and they have no complaints (they did at first, of course, but then I fixed those remarks).The symptom is that on the shield device the grid looks skewed/corrupted and only every third frame seem to be shown.Would anyone with insight care to have a look and suggest what I am doing wrong?The code is at vulkan/minimal at master · markus-zzz/vulkan · GitHub and builds for android and/or xcb without any additional dependencies.Thanks!My issue has been resolved. It turned out that doing awas quite essential to make the swapchain work properly on Android. Not clear to me why/if this is common knowledge though…Powered by Discourse, best viewed with JavaScript enabled',\n",
       " 'p2200-application-clock-settings': 'Hello All,I  am using a P2200 GPU with the following software on Ubuntu 18.04:\\nNvidia-smi Driver Version                            : 495.29.05\\nCUDA Version                              : 11.5I am attempting to run a workload with specific core and memory clock frequencies, e.g. 139 MHz and 405 MHz respectively. I use ‘nvidia-smi -ac 405,139’ to set the application clocks and this completes successfully. I can see the settings under ‘Application Clocks’ in the output of ‘nvidia-smi -q -d CLOCK’.\\nHowever, when I run the workload, the memory clock goes up to 5005 MHz. I am expecting that the core and memory clocks stay at the specified application clock frequencies but that does not happen.\\nIf the application clocks do not stay at the set frequencies, it is not possible to correlate the workload’s performance to the desired (set) frequencies.Why does the memory clock reach 5005 MHz even if the application clocks have been set to 405 MHz for memory?While running the workload after setting the application clocks at 278 MHz (core)/405 MHz (memory): the core frequency shoots up to 1746 MHz. In other words, how do we ensure that the core clock stays always at the specific application clock frequency?\\na. The clock throttling reason shows ‘Active’ only for ‘Idle’. It is inactive for the reason ‘Application Clock Setting’. Is this related to the unwanted frequency boost issue? If so, how do we fix this?Also, is it possible to have the core and memory clocks stay at the specified application clock frequencies even when the applications are not running?Thanks in advance for your inputs.Powered by Discourse, best viewed with JavaScript enabled',\n",
       " 'windows-opengl-2-0-flaw': 'Hello,I’m developping an application I replace GDI by OpenGL 2.0 functionality.\\nI draw images by splitting them into two textured triangles. The problem\\nis that in NVidia hardware the pixels at the triangles edge are not drawn\\npropertly.External MediaDoes anybody have some idea on how to solve it?Is that by any chance with enabled polygon antialiasing?\\nDo you have glEnable(GL_POLYGON_SMOOTH) in your code?\\nIf yes, that doesn’t behave well with depth testing and requires specific blending behavior. It shouldn’t be required for blitting a texture.Also be very careful with the coordinate placement when rendering different primitives in 2D.\\nFor filled areas like rectangles use the pixel corner coordinates, for points and lines use pixel center coordinates.The problem was being originated by the GL_POLYGON_SMOOTH option.\\nMany thanks for your support and advise!Powered by Discourse, best viewed with JavaScript enabled',\n",
       " 'glcolor3f-isnt-working-in-opengl': 'Hello,I found an example in Qt in which a textured cube is rendered. The application uses shaders ( vertex and fragment shaders).I wanted to add a coordinate system to that cube ( I also changed the texture) but I didn’t use any shader for that, I only used glBegin(GL_LINES), glVertex3f, glEnd.\\nEach axis is supposed to have a specific color, for example x:red, y:green and z:blue.When I ran my application, the coordinate system’s axes seemed to have the same texture as the cube as if glColor3f was ignored by the program.I tried different solutions such as adding enabling color material, disabling lighting but that didn’t work.I’m still trying to get familiar with shaders because they’re kinda complicated, some suggested that  I should also use shaders to render the colored coordinate system.\\nIs there any other option ?\\nThank you all.Powered by Discourse, best viewed with JavaScript enabled',\n",
       " 'profiling-memory-coherency-of-optix-application-with-nsight-systems-and-nsight-compute': 'Hi,I was trying to profile my OptiX application with Nsight Systems (later planning to move to Nsight Compute once I have identified potential bottleneck regions). I am mainly interested in comparing the memory coherency of the implementation with a reference implementation, so metrics like cache hit ratios and more information about memory accesses would be great. I am very new to both Nsight Systems and Nsight Compute, so I was wondering if you had any guidelines of where I should best start looking. In Nsight Systems I spotted the DRAM bandwidth and local/non-local resident memory rows under the GPU metrics, but I would like more detailed information on cache hits vs. cache misses.Thanks in advance,– ChuppaIn Nsight Systems I spotted the DRAM bandwidth and local/non-local resident memory rows under the GPU metrics, but I would like more detailed information on cache hits vs. cache misses.You’ll get that information for your OptiX device code inside Nsight Compute.\\nMore information here: https://developer.nvidia.com/nsight-compute\\nThe Inspect Memory Workload section there shows how to read the graph.Thank you for the quick help, I’ve been able to start exploring the very broad but useful collection of statistics that Nsight Compute offers. However, I’ve been trying to use the source code correlation feature, but Nsight Compute will only show me the SASS code. I would like to inspect CUDA source code. I have launched the interactive profiler having set Import Source to Yes. My CUDA programs are also compiled with the --generate-line-info flag. Is there anything else that I am missing? I have seen that you can add CUDA source paths to the CLI profiler via source-folders, but I can’t seem to find where to add this path via the interactive profiler. Any thoughts on this?Did you follow this advice in the OptiX 7.6.0 Programming Guide about Nsight Compute?https://raytracing-docs.nvidia.com/optix7/guide/index.html#program_pipeline_creation#7017To profile your code with Nsight Compute, enable --generate-line-info and set debugLevel = OPTIX_COMPILE_DEBUG_LEVEL_MODERATE in both the OptixModuleCompileOptions and OptixPipelineLinkOptions in your application host code.Did you compile to PTX or OptiX IR input code?Which version of Nsight Compute are you using?Thank you! Setting debugLevel to OPTIX_COMPILE_DEBUG_LEVEL_MODERATE made it work. I read this has a slight impact on performance however, is it recommended to only set this property when debugging with tools such as Nsight Compute?Yes, for full performance release mode, use optLevel full optimizations and debugLevel none or minimal debug information.Enums here:\\nOptixCompileOptimizationLevel\\nOptixCompileDebugLevelThis topic was automatically closed 14 days after the last reply. New replies are no longer allowed.Powered by Discourse, best viewed with JavaScript enabled',\n",
       " 'visual-studio-2017-does-not-creating-kernel-h-and-kernel-cu': 'Hello all!\\nWhen I strarted learning CUDA C (using CUDA Toolkit 10.0) there was a problem - I create simple project which return “hello world” Compiler returned me,that no such file “kernel.cu”How to fix this?\\nAdvance thanksPowered by Discourse, best viewed with JavaScript enabled',\n",
       " 'optix-7-2-ray-triangle-intersection-failure': 'I am developing SBR algorithm utilizing Optix 7.2. I send rays from an observation plane towards meshes. For some meshes and some observation angles, ray-triangle intersection works unexpectedly.\\nFor example,  considering the sphere, each ray must hit the surface at most once or miss.\\nHowever in my case, when I send rays from theta 0 and phi 270 plane, some of the rays don’t intersect the triangles at the surface of the sphere, but intersect the triangles inside.\\nInterestingly, using same mesh, when I send rays from theta 0 and phi 0 plane, all rays behave expectedly and there is no misintersection. I added images of these scenarios respectively. What would be the reason of this problem? Any suggestions is appreciated. Thanks in advance.Environment:\\nCuda 11, Visual Studio 2019, Optix 7.2\\nSphereRays1346×1129 127 KB\\nThe hardware triangle intersection is watertight and should produce the closest hit when not doing optixIgnoreIntersection calls inside anyhit programs or when not using OPTIX_RAY_FLAG_TERMINATE_ON_FIRST_HIT.\\nSince this is view dependent, which means the BVH traversal happens in a different order, either of this could have that effect.If that is not it, I would guess the intersection wasn’t watertight.\\nCould you isolate some of the protruding rays and see if they all lie exactly on the border or corners of adjacent triangles?\\nIf that is the case, we would require a minimal reproducer in failing state to see what is going.’s your GPU and display driver versionI want to make a calculation for each ray-triangle intersection. When I did this with Any-Hit, the program was running slow. For this reason, I call the optixTrace function recursively by terminating the ray at the first hit and updating the ray origin and direction.\\nmisIntersection1342×558 98.6 KB\\nOPTIX_RAY_FLAG_TERMINATE_ON_FIRST_HIT is set in optixTrace function\\nFor this reason, I call the optixTrace function recursively by terminating the ray at the first hit and updating the ray origin and direction.That won’t work that way with OPTIX_RAY_FLAG_TERMINATE_ON_FIRST_HIT set.\\nInstead that calls into the closest hit program with the first hit, not necessarily the closest hit!\\nhttps://raytracing-docs.nvidia.com/optix7/guide/index.html#device_side_functions#12136Which one that is, depends on the BVH traversal which depends on the ray origin and direction. (Means it’s view dependent for camera rays.)That flag is normally used for a faster visibility tests than with anyhit programs and optixTerminateRay.\\nFor such a visibility test you would not need an anyhit or a closesthit program at all. That can be done with just a miss program which indicates nothing was in in the way of the visibility ray [t_min, t_max] range.If you always want the closest hit inside the closesthit program, please try removing the OPTIX_RAY_FLAG_TERMINATE_ON_FIRST_HIT from the optixTrace call.When possible it’s also recommended to use  iterative over recursive ray tracing. That will save device side stack space and perform better.\\nReaching the closesthit program is the end of the current ray anyway. If you want to continue a single path with another single ray from there, you can also return to the ray generation program and set up the next path segment ray there.Follow this link and the links in there for further threads about processing all intersections along a ray:\\nhttps://forums.developer.nvidia.com/t/processing-intersections-in-order/74859/2Thank you for the answer. It was my  misunderstood of “first hit”.Powered by Discourse, best viewed with JavaScript enabled',\n",
       " 'possible-bug-in-df-measured-factor-due-to-incorrect-texture-coordinate-wraparound': 'After some investigation, i suspect that the bsdf modifier “df::measured_factor” introduced in MDL 2019.1 has a bug regarding how the input image is handled at the 0°/90° border.I am using the MDL SDK 2020.0.1, with Optix 6.5.0This is my simple material:the gradient image is a 1px-wide and 91-px high bitmap with a gradient from “black” at the top to “grey” at the bottom, with two control pixel bands (see (https://filebin.ca/5RvrEdgXy3UJ/Gradient.png) )With a simple directional light source shining straight down, i now look straight down onto a flat surface using the above material, using a camera with a large viewing angle. I see a surface that nicely matches the gradient and shows the control rings\\n\\ncameraview1402×1053 217 KB\\n .BUT: Right at the center, directly below the viewpoint, as the viewing angle approaches the normal, there is a dark spot where the brightest spot should be. The dark spot vanishes if i make the top pixel in the image the same color as the bottom pixel, and becomes an almost white spot if i change the top pixel to white. From what i understand, the top pixel should not have any influence here, the reflectance values at this spot should be controlled only by the bottom pixels of the input image.Is this a bug in the MDL/Optix or am I maybe missing / misunderstanding the documentation?Regards, GregorThanks for the report, we will take a look!Hi Gregor!Is your texture runtime based on mdl_textures.cu from the OptiX 6.5.0 examples?\\nThis example runtime contains a bug in WRAP_AND_CROP_OR_RETURN_BLACK which causes the wrong clamping.Here is an updated version which should clamp correctly:Note, that this function takes an additional parameter “inv_dim”, which should be the reciproc value of the corresponding dimension. You could use it like this:Or precalculate those values and store them in another rtBuffer.Hope that helps!\\nMoritzSorry for the very late reply. For future reference: Yes, that was exactly the problem. Thank you so much :-)Great, good to know it solved the problem!Powered by Discourse, best viewed with JavaScript enabled',\n",
       " 'vulkan-beta-drivers-for-rtx-3080': 'I just got my RTX 3080 yesterday. My plan is to use it for research and education and I’m currently developing a Vulkan Path Tracer which uses the VK_KHR_ray_tracing extension. However this is just supported with Vulkan Beta drivers, which are not out for the RTX 3080, kinda bummed.Any way to work around this or at least get som info on when those drivers are coming?Same question here. Anyone have some info?Hi,Just FYI: new beta drivers available:\\nhttps://developer.nvidia.com/vulkan-driverI can confirm that VK_KHR_ray_tracing extension is working on 3080.Have fun!Powered by Discourse, best viewed with JavaScript enabled',\n",
       " 'nvdecode-api-reference-manual': 'Hi!is there no NVDecode API Reference Manual available? There is one for encoding (NVEncode API Reference Manual) so it seems reasonable that there should be one for decoding as well.If it is not available, where are NVDecode API functions documented?LinusPresently, there is no reference manual available for decode APIs. Please refer to the header file which is well-commented. If you have any specific questions, please post them here.Powered by Discourse, best viewed with JavaScript enabled',\n",
       " 'my-game-crash-when-vehicle-touch-the-ground': 'When my vehicle is in air there is no problem but when wheels touch the ground my game crash.\\nand I’m getting this error:First-chance exception at 0x00672cb7 in Game.exe: 0xC0000005: Access violation reading location 0x374a2c5c.\\nUnhandled exception at 0x00672cb7 in Game.exe: 0xC0000005: Access violation reading location 0x374a2c5c.this is my call stack:\\nGame.exe!physx::PxVehicleUpdate::updateDrive4W()  Line 2408 + 0x6 bytes\\tC++\\nGame.exe!physx::PxVehicleUpdate::update()  Line 3599 + 0x24 bytes\\tC++\\nGame.exe!physx::PxVehicleUpdates()  Line 3638 + 0x20 bytes\\tC++what is the problem?\\nI think there is some thing wrong in ray casts but I don’t know what is it.There was a similar thread\\n[url]PhysX 3.2.1 Vehicle and HeightMap - Physics Modeling (closed) - NVIDIA Developer ForumsMaybe it will give you some ideasHello,Without knowing version of the PhysX SDK you are using it is difficult to work out the line of code where the crash occurs.  A couple of things will really help here.  The first is to run in debug configuration to get a more detailed callstack.  The second is to post here the PhysX SDK version that you are running.It is possible that in checked or debug build the vehicle sdk will report the problem to the error stream.  Could you please try this and verify that no errors are being reported?The suggestions in the thread noted in the previous post are definitely valid here.  Definitely worth a look.This sounds like the vehicle code is trying to access memory that has already been deleted or hasn’t been configured properly.  The obvious things to think about are:  1) have you set up the friction table (PxVehicleDrivableSurfaceToTireFrictionPairs)? 2) was PxVehicleSuspensionRaycasts called prior to PxVehicleUpdates? 3) is the array of PxRaycastQueryResult passed to PxVehicleSuspensionRaycasts  still valid? 4) have any actors been deleted in-between the raycast and the vehicle update?I’m sure this is a straightforward fix.Thanks,GordonLooks like problem was solved\\n[url]http://www.gamedev.net/topic/643067-physx-vehicle-problem/[/url]Good news. Thanks.GordonPowered by Discourse, best viewed with JavaScript enabled',\n",
       " 'optixdevicecreatecontext-error-code-7051': 'Good Afternoon,I am playing around with the SDK/optixPathTracer code using a multiple GPU machine. Everything is going well using DEVICE 0, however if I try and set the context to DEVICE 1 I get error code 7051.The error output is shown below.Optix call (optixDeviceContextCreate(cu_ctx, &options, &context)) failed with code 7051The code where the problem appears to occur is shown below.I created an AppScene class that contains everything that optixPathTracer.cpp has. I did this just as a learning exercise to create the scene renderer as a class.I am sure this is probably something dumb on my behalf but I cannot seem to find any information regarding the 7051 error.Can anyone assist?Thanks,You can find code 7051 in optix_7_types.h  in enum OptixResult :\\nOPTIX_ERROR_INVALID_DEVICE_CONTEXTafter CUDA_CHECK(cudaSetDevice(device_id_));\\nyou did not use “CUDA_CHECK”, so did it successfully set the device ?after that you should also use:\\nCUresult cuRes = cuCtxGetCurrent(&cu_ctx);and pass that cu_ctx to optixDeviceContextCreate() instead of 0The OptiX 7 API doesn’t know about multiple GPUs. You need to make sure the proper CUDA context is selected per device before doing any OptiX operation as m001 explained.For OptiX 7 examples using multiple GPUs, have a look into the SDK examples optixMultiGPU and optixNVlink.For more advanced OptiX 7 examples explicitly showing multi-GPU workload distribution with different buffer allocation methods, CUDA peer-to-peer data sharing via NVLINK or PCI-E bus, and different OpenGL interop methods detecting which active GPU runs the OpenGL implementation, have a look here and read the repository’s README.md carefully:\\nhttps://forums.developer.nvidia.com/t/optix-advanced-samples-on-github/48410/4Hi @m001Thank you for the response. Sorry I took so long to reply, I got buried under another task.I had wrapped the cudaSetDevice(device_id_) in the CUDA_CHECK macro but neglected to include it in the code I posted - sorry that was a mistake. The good thing though is that it was successful call.I will give: CUresult cuRes = cuCtxGetCurrent(&cu_ctx); then passing cu_ctx to optixDeviceContextCreate a try. It is probably the problem.Thank you again for the assist.This topic was automatically closed 14 days after the last reply. New replies are no longer allowed.Powered by Discourse, best viewed with JavaScript enabled',\n",
       " 'cloudxr-3-2-2-has-worse-video-quality': 'CloudXR 3.2.2 with so great bandwidth control.\\nHowever, video likes “water” effect  when moving head, even using default 100Mb bandwidth.\\nThis does not happen in 3.1We also see this issue, vertical lines, creating a swimming motion, have confirmed it doesn’t happen in 3.1.1 but does with 3.2.\\nWe have reverted back for now.Hi folks! Thank you for trying out the CloudXR 3.2 SDK. I was wondering if you could provide some more details on which device you are using and what application you are running on the server? If you perform another run would it be possible to capture a bitstream from the client using -ccb in CloudXRLaunchOptions.txt? More details are here: Command-Line Options — NVIDIA CloudXR SDK documentationI am using Oculus clientI’m seeing the same. Using OculusVR client running on a Quest2 (tethered to a PC rather than running on WiFi). The server is on an ec2 g4dn.4xlarge instance. Both are running CloudXR SDK 3.2 (May 2022)It does seem intermittent. I get both horizontal and vertical lines but sometimes it quickly settles down and disappears, sometimes it stays until another session. I captured a bitstream but annoyingly, on that occasion the effect was minimal. It happens not just when running an app but also in the SteamVR environment. (I’ll PM you the log and hevc files)Do you have any advice on how to improve app quality? I’m also seeing what feels like sub par frame rate (e.g head tracking is slow and causes obvious jitter) even though the app on the server is running well (at 6ms)  and the captured bitstream seems fine too and of much higher quality than what is perceived in the HMD.From bitstream analysis, I also did not notice any strange.\\nThe “wave” effect is just similar to low quality VR headset distortion issue but just much more noticeable in 3.2.Maybe I should start another thread about this? I’m finding great variability in the way the stream is decoded by the client app. The bitstream I sent was actually not too bad, but it does get much worse. The client (Quest2) can sometimes play at 72fps without issues, but at other times shows high CPU usage (CPU Level goes up to 4) and FPS drops to 50-60fps which is when I see the jitter I mentioned. These two results are from the same server, playing the same VR app, using the same client app build on the same headset, connected to the same network. I’ve not changed anything at my end.Can you advise how to investigate this further? If it’s helpful I can send device logs and bitstream of each instance.UP. I just wonder if there are someone getting different result in Oculus Quest 2.Any news on this topic ?\\nWe are facing the same issue with CloudXR 3.2 and Vive Focus 3.\\nStrangely, the issue is only noticeable when looking inside the HMD, but we do not see it on the bitstream capture, either on the server or in the HMD.\\nThe issue was not visible in older versions, so it prevent us to upgrade.\\nBest regards,\\nAlexisHi, I’m getting the same “swimming issue” on Quest 2, v3.2. I’m streaming from local computer through steam - UE4 and UE5. I can try and send logs if needed.Hi Keith – can you attach with the client and server CloudXR and Streamer logs you have where you’re seeing this?  We’ve been unable to recreate this issue and curious what your logs may show. Thanks!Can you look at my log?\\nThey are just OculusVR sample with audio enabled.\\nOculusVR Sample Log 2022-07-12 11.59.40.zip (90.2 KB)This analysis may help you to debug:\\nI can “reduce” effect by changing these quality setting:#ifdef DISP_RES_OCULUS_SUGGESTED\\n// This is using the suggested texture size for optimal perf per Oculus sdk design\\ndesc.width = texW;\\ndesc.height = texH;\\n#else\\n// TODO: This is trying to use display-native per eye w/h instead.\\ndesc.width = 1.5 * dispW/2;\\ndesc.height = 1.5 * dispH;\\n#endifAlso, the issue is the most critical when enabling foveated rendering.We could really use -ccb/-csb to see the raw bitstreams.  the descriptions i’ve seen above don’t map to anything i have encountered before – but that could just be language semantics in the way.  seeing the bistream will let us at least see what’s going on on the network.   if you play that file and do NOT see the isssue in the raw stream, we may need verbose logs plus a video recording on-device… which hopefully will show things.  the two video files together gives us something to at least continue the discussion.I believe we have bug in foveation encode/decode\\nWith mFoveation = 100( disable) or low value like 30, you can see this issue.\\nAt recommended value(50) from document, I almost cannot see issue.I’d want to verify that 100 disables.  It might not, the foveation system might still be active.  0 is the official disable value.Again, if you can capture server or client bitstream that clearly shows the issues you perceive, and point out specific time markers and things of note, that would help us investigate further on our end.I’ve sent you via messageWe have found that if we force the use of the AIImageReaderDecoder at all refresh rates (including 72 hz on Quest 1, but also Quest 2 at 72 Hz), as well as hardcoding the resolution to 1832 x 1920, then foveation looks good again and the shimmering / vertical wavey lines problem goes away. This is 100% repro.We hope this can help solve the problem in the library, as we really need to be able to use any arbitrary resolution (or at least within some specifications such as modulo 16 or 32 or whatever CloudXR expects with foveation enabled). Although, 1832 width doesn’t divide into either 16 or 32, but it does go into 8 (8 x 229 = 1832). This may be a clue?The imagereader decoder will become the default for quest2 90hz.  As we get more testing in the field and ensure no corner cases or kinks, it MAY become the default and replace the older mediacodec decoder.But I’m a bit concerned that changing the decoder is affecting foveation at all.  Both decoders have the same stream and output the same frames, it’s what they do from that point that differs, what APIs they use to access the frame data and host it into a texture for GLES composition.  While it sounds like maybe there’s some kind of crop rect or sizing issue, again the two should generate same end result, IR should just do it much faster (shaves off ms/f).Powered by Discourse, best viewed with JavaScript enabled',\n",
       " '2-nvencs-dont-run-in-parallel': 'Hi,In my application I need to encode two video streams in parallel. I examine the performance with visual profiler and I notice a peculiar thing: I can see that there are two distinct CPU and two distinct GPU encoder queues. I can see that the GPU queues start nearly simultaneously (the frames become ready within 500 us from one another), they encode the same frame size, but the first frame is done and ready after 5.2ms (I look at the Dma Packet entry in the Video Encode GPU, which is the only entry in that queue in order to determine when the encoding started and ended. It coincides exactly with the Render entry in the Video Encoder CPU queue), while the second one takes 9.6ms. If I check the overlap between them (when both of them appear to be active in the profiler) I see that the overlap is 4ms. So now if I subtract 9.6ms-4ms=5.6ms ~ 5.2ms, i.e. it looks like the second encoder waited for the first one to complete before it started working. Is it supposed to be this way or am I missing something?I have an RTX A6000, encoding two 3200x3200 streams at 30fps.\\nimage1240×124 14.1 KB\\nHello @yarospo and welcome back, it has been a while.Check out Video Encode and Decode GPU Support Matrix | NVIDIA Developerand look in the Quadro tab for the RTX A6000. This GPU architecture has one NVENC chip, meaning that this is in fact expected behaviour.I hope this helps.Thank you very much MarkusHoHo! Indeed this helps, I was looking in vain for this information everywhere. Thanks!This topic was automatically closed 14 days after the last reply. New replies are no longer allowed.Powered by Discourse, best viewed with JavaScript enabled',\n",
       " 'getting-error-launching-work-to-rtx-when-calling-optixlaunch': 'After doing some refactoring of my OptiX based program, i have started getting the error “Error launching work to RTX” when calling optixLaunch.My program is way to large to post the code here, but i would appreciate any hints on where to start debugging as i am not sure what the error actually means.I initially thought there could be something wrong with my how i create the Raygen record or the raygen program group, but i cant see anything wrong with that when i step through the code.I was able to find the error.\\nTurns out i was reading the width and height of the launch from uninitialized memory. Causing the launch dimensions to be extremely large.Perhaps a more descriptive error message would be useful in this case.Powered by Discourse, best viewed with JavaScript enabled',\n",
       " 'ai-chatbot': 'Hello\\nI belong to a research group at the Polytechnic University of Valencia. We want to tackle a project in which we need to develop an interactive chatbot with the graphic representation of an avatar as realistic as possible and that responds immediately, giving rise to fluid communication.We are studying the tools that Nvidia Omniverse provides, but the set of elements is not clear to us today, they are public and can be used\\nWe’ve been reviewing the parts used for Misty’s tech demo and it’s clear to us that Nvidia Riva was involved for recognition and Audio2Face for creating the animations. But we are missing the rest of the elements.Could you provide us with more information?Thanks!Powered by Discourse, best viewed with JavaScript enabled',\n",
       " 'is-there-a-way-to-load-start-up-an-nview-profile-using-a-macro-vba-so-the-diff-nview-profiles-can-be-made-to-automatically-come-up': 'Hi,Is there a way to Load (start up) an nView profile using a macro?\\n(So the different NView profiles can be made to automatically come up?)(eg.[something like this in VBA…]\\nshell(\"C:   \\\\Program Files\\\\NVIDIA Corporation\\\\NView.exe ““C:\\\\Program Files\\\\NVIDIA Corporation\\\\NView\\\\Profile Test1\"””)-but with code that would work, of course.Thanks,\\nDavidPowered by Discourse, best viewed with JavaScript enabled',\n",
       " 'nvenc-dirty-rectangles': 'DXGI DDA supports Dirty Recs and dirty moves. I want to implement this within my NvEnc implementation for better encode times and less data transfer. Is this possible with NvEnc? Is there a way to directly inform NvEnc of the dirty recs? Or do i have to manually implement this?Also, is this even a reasonable feature enhancement? Would the additional overhead cause more latency?‘NV_ENC_PREPROCESS_FRAME’ is mentioned in nvEncodeAPI.h and seems related as well, but the only place its talked about is in NVENC_RECT structs defintion, which is never mentioned again as well.NV_ENC_PREPROCESS_FRAMEencoder only considers changes when encoding. you dont have to provide them and neither is there a way to do so.Closest feature to this emphasis map. Emphasis map hints the encoder as to what parts of the frame are more important vs less important. encoder considers this hint when allocating rate control budget. for perfect images like desktop windows, it almost does nothing.For example, if you are streaming on twitch and you have a camera on the bottom left corner. you can set emphasis map to give more priority to that space so that, in low bitrate conditions, the game might look low quality but your camera will always look best.Well, this is also that whole using B frames as reference after ffmpeg fixed it after 2 years. It makes picture much better.Powered by Discourse, best viewed with JavaScript enabled',\n",
       " 'open-source-code-for-shield-tablet-k1': 'Can the nVidia staff tell us when the open source for the K1 will be available? I see that the factory images have been made available at SHIELD Open Source Resources and Drivers | NVIDIA Developer.Powered by Discourse, best viewed with JavaScript enabled',\n",
       " 'setting-accel-structure-to-context': 'Hi.I am beginner using Optix. I need to build bvh fast, so I use sample codes “Traversal”.I don’t know which structure for acceleration. But, it is too slow.So I set the acceleration to use “LBvh” andin guide pdf , RTresult RTAPI rtuTraversalSetAccelData ( RTUtraversal traversal, const void ∗ data, RTsize data_size )i want to use this api. Which type or data does fit to “const void* data” and “RTsize data_size”From the API documentation :\\nSpecify acceleration data for current geometry. Input acceleration data should be result of rtuTraversalGetAccelData or rtAccelerationGetData call.From what I understood, what this could do to help you is to provide a way to keep your accelerating structure somewhere and to reload it whithout re-building it when needed.\\nSo tha data parameter should be set to a pointer to the memory space containing your accelerating structure.For example, after setting an accelerating structure accel, you could do that :Which would store your acceleration structure in memory space data.\\nAnd after, when you’d need to reload it :If your memory space is not altered, that should give you the same acceleration structure as before.\\nThe procedure should be the same pour traversal data.I hope it helps.Powered by Discourse, best viewed with JavaScript enabled',\n",
       " 'physx-3-2-x-cooking-convex-mesh-failing': 'I have been following the user guide to try to get physics meshes into my game. Unfortunately, no matter what kind of mesh I try (convex or triangle), or the model I give it (a small poly model, or even a simple cube), cookConvexMesh() keeps failing. Here is my implementation:Whenever the function is called it always enters the if-statement mentioned above. I check the contents of the buffer passed, and it is always NULL. Any idea what might be causing the problem, or any way to get more information about whatever error is being thrown within PhysX?try to change:Hope it helps.Powered by Discourse, best viewed with JavaScript enabled',\n",
       " 'quadro-p1000-4gb': 'Does the Quadro P1000 4GB support warp and blend?Thanksyes it does.This topic was automatically closed 60 days after the last reply. New replies are no longer allowed.Powered by Discourse, best viewed with JavaScript enabled',\n",
       " 'facing-issues-regularly-like-display-and-touchscreen-in-my-ipad-air': 'Hello,I am currently using my iPad Air which I just buy two months ago and really happy with its services but after using three or four weeks I am getting certain Issues regarding its screen and color adjustment.Come to its touchscreen issue, I am unable to click and write anything which creates lots of problems and irritates me.My iPad does not seem right to me as its color especially brightness and saturation does not match with each other and gives me more warmth or yellowish color.Please note that my night shield or any protective shield is not on so I think it is the problem which I facing right now is not due to my settings.But after moving my iPad, sometimes it shows a ray of hope that it will be fine but after moving again it comes backs to its original position.I am currently working as the Managing Director at the car wash Perth which best for its car services.Thanks for having me and give me a platform for asking my query.I checked all the queries on your platform and then I am sure that you will be able to answer this query as soon as possible.Powered by Discourse, best viewed with JavaScript enabled',\n",
       " 'optix-forum-migration': 'Hi everyone,The OptiX forum and all its posts will be moved to the new OptiX subboard, which is located under the Advanced Graphics board.The reason for this move is purely aesthetical. We wanted to keep the looks of the our forum subboards consistent across the board. If you have bookmarked the old OptiX forum link, don’t worry! We have included a redirect in the old OptiX forum link, so that you’ll still be directed to the new OptiX forum.Please let me know if you have any questions!Thanks,\\nRyan ParkPowered by Discourse, best viewed with JavaScript enabled',\n",
       " 'm4000-quadro-card-simultaneous-h264-encode-sessions': 'Hello Dev from Nvidia,i have spoken from livechat with Younus he advised me to open a ticket here to ask developers about my question.my question is thishow many h264 simultaneous sessions at 1080p 30fps can handle 1x M4000 quadro card\\nand if i want to use in paralel with sli (2x M4000 card) how many sessions are able to encode?I am not from Nvidia. But I give you some hints:1x M4000 = 430 FPS (Maxwell Gen 2, High Performance, Constant QP, 1920x1080, YUV4:2:0, 8 bit) * 2 encoders * 0.8 #underclocked M4000 = 688 FPS / 30 FPS per stream = less then 23 high performance streams.hi mcerveny thanks for reply.but i mean how many 720p/1080 streams i can encode on 2mbpsraw format streams are about 6000 kbps and 9000 kbps i want re encode this to at 2000 - 3000 kbps\\ngpu based instead of using cpu. i can now do 7x 720p on i7 4the gen cpu on cpu based.1x M4000 = 430 FPS (Maxwell Gen 2, High Performance, Constant QP, 1920x1080, YUV4:2:0, 8 bit) * 2 encoders * 0.8 #underclocked M4000 = 688 FPS / 30 FPS per stream = less then 23 high performance streams.do you mean i can encode 20 1080p streams in high performance with one m4000?but i mean how many 720p/1080 streams i can encode on 2mbps\\nraw format streams are about 6000 kbps and 9000 kbps i want re encode this to at 2000 - 3000 kbps\\ngpu based instead of using cpu. i can now do 7x 720p on i7 4the gen cpu on cpu based.You need decode and encode ? No problem, even decoder is accelerated (see the same referred materials/links - https://developer.nvidia.com/nvidia-video-codec-sdk). M4000 (Maxwell Gen 2, 1 decoder) decodes 392 FPS H264/1080p (https://developer.nvidia.com/nvdec-application-note)). You can study Video Codec SDK samples too. The encoder supports setup of CBR or VBR (bitrates) that influences quality (search https://developer.nvidia.com/nvenc-programming-guide).do you mean i can encode 20 1080p streams in high performance with one m4000?Yes and it depends (see referred materials/links). 1080p30 streams - encode ~20x / decode ~10x.hi mcerveny thanks for your response (y)Powered by Discourse, best viewed with JavaScript enabled',\n",
       " 'gstreamer-opencv-hangs-at-the-step-parsebin-to-decodebin-nvdec': 'I’m attaching INFO(debug=4) logs on my end, this is the pipeline string I’m using\\ngst = (“rtspsrc location=rtsps://:@ tls-validation-flags=insecure  protocols=GST_RTSP_LOWER_TRANS_TCP ! parsebin ! nvdec ! videoconvert ! appsink”)Even if I use this ^ or with rtph264depay+h264parse the result is the same the pipeline just hangs at the parse step and never moves forward\\nrtsp_app_logs.txt (110.4 KB)If anyone has faced similar issue do reach outgstreamer version: 1.14.5\\nOpenCV: 4.5\\nCUDA_VERSION=10.2\\nCUDNN_VERSION=7\\nUBUNTU_VERSION=18.04\\nPython3.8\\nNvidia driver: 460\\nRunning inside a docker with host machine having CUDA 11.2 and a Quadro RTX 8000 gpu same driver version.Powered by Discourse, best viewed with JavaScript enabled',\n",
       " 'is-there-any-way-to-reuse-cuvideodecoder': 'Hi, all:\\nI am trying to extract keyframes from various H264 shot videos. These videos are of different width and height. I want to decode keyframes and scale to 256 x 256.Before calling cuvidCreateDecoder, a CUVIDDECODECREATEINFO struct should be filled. Specifically, the ulWidth and ulHeight field of CUVIDDECODECREATEINFO structure should be filled with video’s coded width and height respectively.I found that this decoder creation and destroy cost much time compared to frame decoding. Is there any way to reuse CUvideodecoder so that video decoder be created only once?Sample code:Hi liuxionghust,Which video codec SDK are you using? If you are not using SDK 8.2, we would recommend to move to that SDK header version(Video_Codec_SDK_8.2.16). Before you want to use the 8.2 header, ensure that you have the right drivers installed on your machine, the Readme.txt mentions the minimum driver versions that are needed to be installed before you can use the SDK 8.2.In version 8.2, we have added support of a new API (cuvidReconfigureDecoder). Please refer to section 4.6 in …/doc/NVDEC_VideoDecoder_API_ProgGuide.pdf which explains how you can use this API. This API provides you the capability where you can re-use a single decoder object instead of destroying and creating a new one.The sample application inside the SDK also illustrates how to use the API, in case you want to get hold of a working code.Let us know if you have any more questions.Thanks,\\nRyan ParkHi rypark,\\nThanks for your reply. Previously I was using codec sdk 7.1. After I found the new cuvidReconfigureDecoder API added in codec sdk 8.2, I tried it. It works well.Hi rypark,I checked the work cuvidReconfigureDecoder onsame code1080 TI      work\\nTesla K80  not work (green corrupted scree)you can test work cuvidReconfigureDecoder no Tesla K80 card.Powered by Discourse, best viewed with JavaScript enabled',\n",
       " 'dxr-and-multiple-meshes': 'Hi everyone!I’m having a hard time understanding how to manage multiple meshes using DXR. I’ll explain better.\\nI have my test where I have some BLAS and one TLAS that reference them.\\nWhen firing rays, I see that I can get an InstanceID, which tells me what instance of the of a BLAS has been touched, but I cannot know what BLAS has been touched, so I cannot know what material I should use for it.\\nI’m also at a loss on how I should manage this: should I load all my meshes, textures and material data inside some structured buffer and access them according to the mesh that has been hit? (If I can find a way to know that, of course :D)Let me know if you can help :Dcheers,\\nAlexPowered by Discourse, best viewed with JavaScript enabled',\n",
       " 'how-to-transform-a-vector-to-be-in-triangle-orientation': 'Ok I can calculate the normal of a triangle, and I have another vector that want to perform an operation in with respect to the triangle normal. This other vector is in its own reference space, what should I use to transform?Thanks.Hi @user131777,Have a look at the optixTransform{Point,Vector,Normal}* functions in optix_7_device.h. You can also get fancier with your traversal transform stack if you need, as outlined in the “Transform List” section of the OptiX Programming Guide: https://raytracing-docs.nvidia.com/optix7/guide/index.html#device_side_functions#transform-list–\\nDavid.I only see things like optixTransform from World/Object space, and not general transforms. Am I right to say that I am not transforming from either object or world.I wouldn’t know what space your other vector is in. “Object space” is a term that commonly represents a space that is local to a given object/mesh/primitive, etc., so maybe your vector is in the object space of a different object than the triangle you’re working on? OptiX’ use of “object space” is referring to whatever coordinate system you used to define primitives when building a GAS. “World space” to OptiX matches the space of your root scene node, usually the IAS that you trace against in your raygen program.The general transforms are available in the transform list part of the API, the Programming Guide link I posted. These functions can be used to query the matrix transform at any level of the hierarchy, and used to construct a composite transform that will take you from any space you’ve told OptiX about to any other space.If you put an instance transform on top of the GAS as it’s connected to the IAS/scene, then that transform defines the conversion from the world space of the scene to the object space of the GAS. If you have a multi-level scene with nested instance transforms, then the conversion requires iterating through a list of transforms and collecting the combined transform, and then applying that to a point/normal/vector. This is what the convenience functions optixTransform* do for you, and they are best used when you only have one or two things to transform. If you have a lot of data to transform, or if your code to do transformations is branchy, then it may be more efficient to query the transforms and apply them yourself.So, given your description, I would guess that your goal is to either convert both your vector and your triangle into world space, and do the computation there, or figure out the local->world transform for your vector, and use OptiX to query the world->object transform for your triangle, multiply the two transforms together, and apply the combined transform to the vector, so that it will end up in your triangle’s object space.–\\nDavid.Ok I can calculate the normal of a triangle, and I have another vector that want to perform an operation in with respect to the triangle normal. This other vector is in its own reference space, what should I use to transform?If you’re using two different coordinate spaces and want to convert vectors from one coordinate space to the other, you need a basis transformation.\\nFor that you’re usually using an ortho-normal basis of the coordinate space you want to convert into or out from.\\nThen the transformations are simply three dot products to convert a vector into the ortho-normal basis space and a matrix multiplication on the vector to convert it back to the outer space.\\nThis is usually done when you’re doing shading calculations (== the BSDF implementation) in object (or texture) space while the ray directions are usually in world space outside the shading code.I’m using a small helper class named TBN (for Tangent, Bitangent, Normal) to generate ortho-normal bases and transform between them.\\nExample code here:\\nhttps://github.com/NVIDIA/OptiX_Apps/blob/master/apps/rtigo3/shaders/shader_common.h#L81\\nUsed inside the GGX shader to sample and evaluate directions in object space:\\nhttps://github.com/NVIDIA/OptiX_Apps/blob/master/apps/rtigo3/shaders/bxdf_ggx_smith.cu#L228Note that a single normal vector is not enough to generate a consistent ortho-normal basis orientation at runtime on round objects like a sphere with that. You must have a reference tangent vector as well to not get discontinuities in anisotropic materials or bump maps. That’s why, for example, the runtime generated meshes in my examples calculate the geometric tangent as well which matches the reference tangent in texture space so that I could implement bump mapping with that tangent and a bump normal for the texture space ortho-normal basis.\\nhttps://github.com/NVIDIA/OptiX_Apps/blob/master/apps/rtigo3/src/Sphere.cpp#L79Thanks, the TBN snippet helped me get on track. I am trying to use this to utilize a normal map, and it is mostly working now. I have a question about the tangent calculation. How can I get the tangent based on the orientation of the normal map texture?If you need a texture space ortho-normal basis, you would need to calculate the derivatives of the texture coordinates in the triangle plane to get reference vectors for the tangent (and bitangent).This is the first hit when searching for “texture tangent space” which seems to explain it nicely: https://learnopengl.com/Advanced-Lighting/Normal-Mapping\\nHere is another one: http://www.opengl-tutorial.org/intermediate-tutorials/tutorial-13-normal-mapping/Powered by Discourse, best viewed with JavaScript enabled',\n",
       " 'canfd': 'R32 (release),REVISION: 7.1; ubuntu18.04LTSsudo ip link set can1 type can bitrate 1000000 sample-point 0.8 dbitrate 5000000 dsample-point 0.8 fd on restart-ms 100Terminal display：\\nnvidia@nvidia-desktop:~$ ip -s -d link show can1\\n10: can1: <NOARP,UP,LOWER_UP,ECHO> mtu 72 qdisc pfifo_fast state UP mode DEFAULT group default qlen 10\\nlink/can  promiscuity 0\\ncan <BERR-REPORTING,FD> state ERROR-ACTIVE (berr-counter tx 0 rx 125) restart-ms 100\\nbitrate 1000000 sample-point 0.794\\ntq 29 prop-seg 13 phase-seg1 13 phase-seg2 7 sjw 1\\nmttcan: tseg1 2…255 tseg2 0…127 sjw 1…127 brp 1…511 brp-inc 1\\ndbitrate 4857142 dsample-point 0.714\\ndtq 29 dprop-seg 2 dphase-seg1 2 dphase-seg2 2 dsjw 1\\nmttcan: dtseg1 1…31 dtseg2 0…15 dsjw 1…15 dbrp 1…15 dbrp-inc 1\\nclock 34000000\\nre-started bus-errors arbit-lost error-warn error-pass bus-off\\n0          4127       0          1          799        0         numtxqueues 1 numrxqueues 1 gso_max_size 65536 gso_max_segs 65535\\nRX: bytes  packets  errors  dropped overrun mcast\\n101408     6479     4127    0       0       0\\nTX: bytes  packets  errors  dropped carrier collsns\\n0          0        0       0       0       0question：set the dbitrate 5000000，but show 4857142 ?There may be several reasons:\\n1.the Clock is wrong，maybe pllaon, not pll_c;\\n2.the tcan332G baud rate limit ,could not set 5000000;\\n3.other reasonsplease help me,thank you!Hello again,Thank you for translating the last part of your post.Unfortunately this does not make it much clearer.I assume based on the Tag that you are working with DriveOS. But which device are you trying to connect?\\nIf you can tell us that thatn we can point you to the correct DriveOS sub-category here on the forums.And for future reference please do not open new topics but rather reply on just one of them. I scheduled the other one to close, so please stay in this topic for further discussion.Thanks!Powered by Discourse, best viewed with JavaScript enabled',\n",
       " 'difference-in-flex-for-unity-and-ue4': 'Hi everyone,I am a game developer who is going to make a VR prototype using FleX. I noticed that there is a Unity plugin for it so want to ask if there is any difference between that plugin and others. And would anyone give me some advice on do it on UE4 or Unity for a prototype on VR? (Quality does not really matter).Thank you.Powered by Discourse, best viewed with JavaScript enabled',\n",
       " 'pxspatialindex-replacement': 'Since PxSpatialIndex is marked as deprecated, will there be replacement in future?There is a general plan to decouple the scene queries from simulation scene, in which case the functionality be very similar to PxSpatialIndex. So far PxSpatialIndex has not been removed in next version as the future plans are not yet fully decided.Powered by Discourse, best viewed with JavaScript enabled',\n",
       " 'qos-and-trace-local-events-tle-at-the-server': 'I am presently facing an issue of not being able to record the trace of local events (-tle) at the server end and also the trace QoS statsI have created the CloudXRServerOptions.txt and as the launch option have put “-tle” in the .txt file. After that I start the testserver.exe and then connect the client to the server.What I have observed: when I launch the SteamVR at the server it automatically launches another TestServer and disregards the CloudXRServerOptions.txt and the .json is not created.Any insight on how to overcome this issue?Are log files getting created at all?  if not, that might indicate a permission issue on the machine.Separately, when you say “TestServer”, that has a meaning internally as a tool we use for base verification, but it isn’t shipped in the sdk.  So just trying to understand the use of that name.  That app only takes commandline arguments, it doesn’t read the text file as far as I know.SteamVR should launch, and when the CloudXR server starts it should read the options file.  You will see in the “CloudXR Server - SteamVR” log file the line:\\nVisionServerDriver::Init()\\nAfter that will be one of the two lines either\\nRead server options file at \\nor\\nWARNING: Failed to parse options file at Which do you see?Powered by Discourse, best viewed with JavaScript enabled',\n",
       " 'which-nvidia-graphics-card-lineup-can-be-connected-efficiently-with-intel-nuc': 'E.g. Intel® NUC 10 Performance Mini PC - NUC10i7FNHAAWhich Nvidia gpu(CUDA) can be connected to NUC and through which connection?Powered by Discourse, best viewed with JavaScript enabled',\n",
       " 'what-should-i-take-into-account-about-the-cable': 'I’ve been working on a homebrew set of VR Goggles w/ head tracking, and am just beginning to plan the cable that will connect the parts on the user’s head to the electronics on the main unit (which will be mounted under & behind the seat the user will be seated in) I’m expecting about a 4ft cable run between the two units, and the connection will need somewhere between 6 and 10 wires (depending on how many ground wires I share):Manditory wires:+12v for the video display in the goggles\\nGND\\nI2C SDA (3.3v)\\nI2C SCL (3.3v)\\nComposite Video\\nMono Audio Optional wires:\\n+3.3V for the headtracker sensors (Could be obviated with an LDO regulator from the 12v\\nGND (paired with +3.3v)\\nGND (Video-)\\nGND (Audio-)\\nOther requirements:the cable needs to be flexible, light weight, and possibly coiled to handle head movement without discomfort or tangling.\\nthe cable needs to be commodity (this is a personal project)\\nthe cable needs to be reliably disconnectable/reconnectable\\nI think I understand the various factors in play (crosstalk, emf, capacitance, etc.) but I don’t have any perspective on their importance for a 4ft cable run.My back-of-napkin guess would be to use an ethernet cable (CAT 5), wired like this: Pair 1: Audio+/- Pair 2: Video+/- Pair 3: I2C SDA/SCL Pair 4: Power +12V/GND(Use an LDO for the 3.3v on the sensors:[url]http://www.kynix.com/Product/Cate/231.html[/url])Is this sensible? What factors should I be most concerned with? Which signals will be most impacted by connectors and wiring choices?Powered by Discourse, best viewed with JavaScript enabled',\n",
       " 'how-to-extract-custom-sei-unregistered-user-message-data-with-the-nvidia-decoder': 'It is straight forward to add the data using the video encoder by changing the NV_ENC_PIC_PARAMS structure, but I don’t see any structure on decode that provides metadata of decoded NALUs, only produced the actual decoded frames. Is there a way to obtain this metadata using the nvidia decoder?I don’t think NVDec will parse the SEI’s for you. You can get a pointer to the raw bitstream in pPicParams->pBitstreamData and parse it yourself. Or you can parse it yourself before passing it to the decoder.My application passes data to the decoder NALU-by-NALU and that enables me to detect the SEIs and do what I need to do before passing them to the decoder.How do you pass NALU-by-NALU? Did you write a parser yourself that reads and splits the video stream in NALUS?Yes, I wrote my own parser. Here is a CUDA 7.5-based VS2013 project that contains everything you need. It’s a little old but still works fine. Let me know if you need any help parsing SEI NALUs.http://rationalqm.us/misc/cuervonenod.rarHi @electrodynamics , I know this is an old thread but I would be interested in having the parser you wrote.\\nThanks!Powered by Discourse, best viewed with JavaScript enabled',\n",
       " 'top-level-array-stride-is-0-for-top-level-arrays': 'Hello,When using glGetProgramResourceiv to query this block member (GPUSkinningBlock.skinBlendMatrices[0]) for TOP_LEVEL_ARRAY_STRIDE, the Nvidia driver returns 0 where the AMD driver returns the array stride (same value as GL_ARRAY_STRIDE).For another block, both AMD and Nvidia drivers return the stride as I would expect when querying any of the two members:So it seems that with the Nvidia driver, GL_TOP_LEVEL_ARRAY_STRIDE returns 0 if the element queried is the top level array itself. The OpenGL specification doesn’t say anything about this special case, so I’d assume it should also return the stride in this case, it’s not completely clear, though.Am I missing something or could this be considered a bug in the driver?Regards,\\nYvesPowered by Discourse, best viewed with JavaScript enabled',\n",
       " 'pxparticlebase-createparticles-failing-only-on-gpu-mode-physx-3-4-sph-fluids': 'I have a scene that adds a small number of GPU accelerated fluid particles to the PhysX scene every tick. (16 or so per tick, increasing over time). The logic used is a modified version of the Samples project that came with the PhysX source files.100% of the time, at the same tick, (96 active particles, and attempting to create 32 more) the call into PxParticleBase::createParticles returns false and the GPU acceleration cannot be be used until the application is restarted. Particles can still be created if we turn off the eGPU flag (all during runtime).What could be causing an issue like this?Edit: It should be noted we’ve created dozens of other scenes of various scales and complexity, and we’ve only just now encountered this issue. It appears to be exceedingly rare.System Information\\nNVIDIA System Information report created on: 05/18/2017 15:26:10[Display]\\nOperating System:\\tWindows 8.1, 64-bit\\nDirectX version:\\t11.0\\nGPU processor:\\t\\tGeForce GTX 670MX\\nDriver version:\\t\\t382.05\\nDirect3D API version:\\t11.2\\nDirect3D feature level:\\t11_0\\nCUDA Cores:\\t\\t960\\nCore clock:\\t\\t601 MHz\\nMemory data rate:\\t2800 MHz\\nMemory interface:\\t192-bit\\nMemory bandwidth:\\t67.20 GB/s\\nTotal available graphics memory:\\t9191 MB\\nDedicated video memory:\\t3072 MB GDDR5\\nSystem video memory:\\t0 MB\\nShared system memory:\\t6119 MB\\nVideo BIOS version:\\t80.04.97.00.14\\nIRQ:\\t\\t\\tNot used\\nBus:\\t\\t\\tPCI Express x16 Gen2\\nDevice Id:\\t\\t10DE 11A1 10AD1043\\nPart Number:\\t\\t2051 0003[Components]NvGFTrayPluginr.dll\\t\\t3.6.0.74\\t\\tNVIDIA GeForce Experience\\nNvGFTrayPlugin.dll\\t\\t3.6.0.74\\t\\tNVIDIA GeForce Experience\\nnvui.dll\\t\\t8.17.13.8205\\t\\tNVIDIA User Experience Driver Component\\nnvxdplcy.dll\\t\\t8.17.13.8205\\t\\tNVIDIA User Experience Driver Component\\nnvxdbat.dll\\t\\t8.17.13.8205\\t\\tNVIDIA User Experience Driver Component\\nnvxdapix.dll\\t\\t8.17.13.8205\\t\\tNVIDIA User Experience Driver Component\\nNVCPL.DLL\\t\\t8.17.13.8205\\t\\tNVIDIA User Experience Driver Component\\nnvCplUIR.dll\\t\\t8.1.950.0\\t\\tNVIDIA Control Panel\\nnvCplUI.exe\\t\\t8.1.950.0\\t\\tNVIDIA Control Panel\\nnvWSSR.dll\\t\\t6.14.13.8205\\t\\tNVIDIA Workstation Server\\nnvWSS.dll\\t\\t6.14.13.8205\\t\\tNVIDIA Workstation Server\\nnvViTvSR.dll\\t\\t6.14.13.8205\\t\\tNVIDIA Video Server\\nnvViTvS.dll\\t\\t6.14.13.8205\\t\\tNVIDIA Video Server\\nNVSTVIEW.EXE\\t\\t7.17.13.8205\\t\\tNVIDIA 3D Vision Photo Viewer\\nNVSTTEST.EXE\\t\\t7.17.13.8205\\t\\tNVIDIA 3D Vision Test Application\\nNVSTRES.DLL\\t\\t7.17.13.8205\\t\\tNVIDIA 3D Vision Module\\nnvDispSR.dll\\t\\t6.14.13.8205\\t\\tNVIDIA Display Server\\nNVMCTRAY.DLL\\t\\t8.17.13.8205\\t\\tNVIDIA Media Center Library\\nnvDispS.dll\\t\\t6.14.13.8205\\t\\tNVIDIA Display Server\\nPhysX\\t\\t09.17.0329\\t\\tNVIDIA PhysX\\nNVCUDA.DLL\\t\\t6.14.13.8205\\t\\tNVIDIA CUDA 8.0.0 driver\\nnvGameSR.dll\\t\\t6.14.13.8205\\t\\tNVIDIA 3D Settings Server\\nnvGameS.dll\\t\\t6.14.13.8205\\t\\tNVIDIA 3D Settings Server[Licenses]3DTV PlayDigging further down, this is as far as I can get in the stack:Sample Projectcalls:Into the PhysX libraries\\nScbParticleSystem.cppto:\\nScParticleSystemCore.cppHowever, I can’t step through to the “addParticlesV” logic because the line apparently requires the symbols file for PhysX3GPU_64.dll to resolve getParticleState().Best I can figure, it goes to this method:ScParticleSystemSim.cppbut even if I set breakpoints in it, it never gets there, even on ticks where it successfully adds particles.Powered by Discourse, best viewed with JavaScript enabled',\n",
       " 'falcor-missing-parts': 'Hi, I’m trying to build https://github.com/league1991/RTCaustics project,In project instruction it said :\\nTo build the demo, one should take the following steps:So I did, I downloaded falcor 3.1.0 from github and built it, it was working fine, but when I add caustic project, it’s very confusing.\\nIt seems developer mixed two different versions of falcor for their craft, for example their source contains IRenderer which is presented in newer falcor and not in the 3.1, They used RtScene, StructuredBuffer, RtState which only exists in falcor 3.X, They used RasterScenePass which is only in falcor 4.X+I’m completely confused, anybody knows how to deal with the following issue?RegardsHello @thegpuguy and welcome to the NVIDIA developer forums!Falcor is already on version 5.1, did you try and contact the owner of the RTCaustics github project to update his dependencies and instructions to a newer version?Beyond this I am afraid i can’t help much, but maybe someone in the dev community has faced a similar issue?Thank you!Thank you for warm welcome!\\nI informed developer both in email and as github issues, He hasn’t been active almost for a year, Now he works at Nvidia I’m not sure if I can reach to him, I will be thankful if someone at Nvidia can inform him about the issue, it’s a very useful repo.Thank youOh I didn’t know that the person works at NVIDIA. I can make no promises, but I will see if I can find him.Thank you so much!\\nPerson name is Yiubun Auyeung and it seems he is the man behind the Unreal Engine 4 caustic system.\\nI hope he update the repo to latest Falcor soon.Hi, I am the author of the repo. Now I am migrating the code to the latest falcor. It’s not an easy task because lots of stuff has to be changed. I will teel you when I am done.You can also checkout yaobin/falcor5.1 to see the progress.Thanks,\\nYaobinHello!\\nThank you so much, I waited for this so long. Is there anything I can help you with let me know!Is there any chance that you add the dispersion like unreal engine to falcor version?Thanks again!Hi,I am sorry to tell you that I still cannot recover the project because the falcor code is so different from what it was.I suggest using the ue4 repo, in addition to my repo’s main funtionality, it also contains dispersion and supports various light sources. You can start from this file:\\nhttps://github.com/NvRTX/UnrealEngine/blob/NvRTX_Caustics-4.26/Engine/Source/Runtime/Renderer/Private/RayTracing/RayTracingMeshCaustics.cppOn the other hand, I did make some progress on the falcor demo. I can compile the project with falcor 5.1, though it still cannot initialize the scene and shaders and I have no confidence when I can fix all the errors. You can check out the “yaobin/Falcor5.1” branch if you like.Hopefully that can help. If you have any question, just leave the message here, or send an email to me. My email address is yaobino@nvidia.com.Thanks,\\nYaobinPowered by Discourse, best viewed with JavaScript enabled',\n",
       " 'nvidia-optical-flow-sdk-returns-unsupported-device-for-gtx-1650-ti': 'I am trying to use NVIDIA Optical Flow SDK 2.0.23 on my Dell XPS. It has the following output for `nvidia-smi.I build the NvOFBasicSamples and try to execute the AppOFCuda sample using the command - ./AppOFCuda  --input=<folder>/*.png --output=<folder> and get the following outputGPU in use: GeForce GTX 1650 Ti\\nNvOFCudaAPI : m_ofAPI->nvCreateOpticalFlowCuda(m_cuContext, &m_hOF)returned error 2 at Optical_Flow_SDK_2.0.23/Common/NvOFBase/NvOFCuda.cpp;35To my understanding, the error code 2 matches to NV_OF_ERR_UNSUPPORTED_DEVICE error code enum. However, the GTX 1650 Ti is a Turing architecture GPU supported by the SDK.Am I missing something here?@dusty_nv Any leads would be appreciatedGTX 1650 TiGTX 1650 Ti → TU117NVIDIA Optical Flow SDK Dependencies:\\nNVIDIA GeForce, Quadro and Tesla products with Turing (except TU117) and Ampere generation GPUsPowered by Discourse, best viewed with JavaScript enabled',\n",
       " 'bug-filterdata-for-character-controller': 'Hey. What i’m trying to do is prevent Character Controller interacting with certain type of objects. So i thought, i can use filter data, it works for normal shapes. In order to provide filterData for PxController, i pass it while calling to PxController::move(…) with PxControllerFilters. So for a quick test i do it like this:and that’s enough to cause mPlayerController to stop interacting with the world(it just falls through stuff).the weird thing is the line that causes such behavior is “characterFilterData.word0 = 1;”.so i found out that it happens if i set any of characterFilterData member variables to value other than zero. can someone explain why?and BTW, it happens even if i use simulation shader like this one:and Character Controller is initialized like that:can anybody point out anything wrong with it? i fail to come up with any valid logical reason for it to happen, so is suspect it may be a bug.\\ni’m using PhysX 3.2.4.Hi,I may be wrong but I think there is no such bug with CCT filtering. Perhaps collision only occures with default shader if (shader1 == 0 && shader2 == 0) || ((shader1.word0 & shader1.word1) | (shader1.word1 & shader1.word1) | (shader1.word2 & shader1.word2) | (shader1.word3 & shader1.word3)). You can also look inside the guide in Callbacks and Customization > Collision Filtering to understand how it works. I have implemented a physX 3.2.4 plugin for a 3D engine and was able to filter CCT collision against certain objects (What I was not been able to achieve though is CCT/CCT collision filtering. I wasn’t able to have a CCT pass through another). I it is possible to provide a simple project, I would be glad to help you.here’s the basic reproduction case, i hope you’re OK with just a single source file listing. project is just a default MSVC 2010 Console Application. PhysX SDK version is 3.2.4;\\nhttp://pastie.org/private/exnuwpr2qrez9tigslt3bqi’ve compiled it, watched through PVD and Character Controller fallen through the “floor”. line, that causes such behavior is surrounded with comments. if you comment it out, it works fine. tested with default and custom(most basic two-liner without any conditions) shader.i’ve also tried to do it that way, instead of providing filtering data on “move()” function call:it didn’t work either. character controller just collided with everything. but again, it works with regular actors.Since nobody had any comment and suggestions on that, i posted it as a bug report on developer center. Only to find out that there’s same attention amount to bug submissions(i have no idea about status of my submission. i can’t tell if someone read it in 10 days since i posted it. i didn’t get any kind of response\\\\confirmation). It kind of wastes my time, because i’m still uncertain about the nature of the problem and what to do with it. Maybe i’m doing something really stupid, misunderstanding something really badly? I’d like to find out.Hi,Sorry for the delay (was a little busy). I checked your code and also don’t understand how this work (I had the same problem and wasn’t able to solve it). Perhaps this is finally a bug. Anyway, another way to achieve what you want to do is by using preFilter callback. Also you can modify the source code of Character controllers if you want to.well… it has been almost a month since i’ve posted a bug submission. nor it was reviewed, nor i’ve got any answer from PhysX staff on this forum. either they’re too busy, or it is not a real bug.Is there any update on this problem? I suffer the exact same thing on 3.2.4*Edit:Nevermind i appear to have got it working now.Hi,Can you please give some tips? that will help N01(and me :)) to understand the problem. Thanks in advance.Yeah sure here ya go [url]http://pastie.org/private/xkib2jdenop09npezoaixa[/url]I just modified the above to make it work, i added a “health” box to help demonstrate it all. The player should fall through the health box but not the floor.Thanks,So it is the opposite of what I(we) thought in the first time. You need to specify the bitmask for the groups you want to be included in the collision.Yeah when you set it up that way with the collider against group. Take a look at the submarine sample as it shows another way of using it.Thanks very much. I was with similar problem.Here my post\\nhttps://devtalk.nvidia.com/default/topic/666014/?offset=15#4154315This line don’t exist more in PhysXI don’t was seting the queryfilterdata:I actually revisited this issue today. And stumbled upon the same problem.\\nWriting something like that for regular shapes seems to work:with a shader like that:but it seems that character controllers and raycast follow their own BS rules. they don’t use that shader. so changing any “wordX” value for them results in no collision. So are there other shaders you can specify besides “PxSceneDesc::filterShader” one? Or what logic do they actually use? Or where do you seek for an example? Submarine sample doesn’t even use character controller.Like it was mentioned above, the character controller uses scene queries, so all you need to do is set the appropriate scene query flags on your shapes with PxShape::setQueryFilterData (instead of PxShape::setSimulationFilterData).yes, i fixed it by now. it’s just when you try to implement it inside of already working system, it can get really confusing. it would be nice if documentation section on character controllers did elaborate on filtering, mentioning these little details, describing logics of interactions better and referencing documentation on query filtering.Powered by Discourse, best viewed with JavaScript enabled',\n",
       " 'prismatic-joint-position-depends-on-center-of-mass': 'I initialize two actors and set their center of mass like that:actor1 = thePhysics->createRigidDynamic(\\nphysx::PxTransform(0.0,3.0,0.0,physx::PxQuat(physx::PxIdentity)));actor2 = thePhysics->createRigidDynamic(\\nphysx::PxTransform(0.0,3.0,0.0,physx::PxQuat(physx::PxIdentity)));Than I create a prismatic joint between them:physx::PxPrismaticJointCreate(*thePhysics, actor1, physx::PxTransform(0.0, 0.0, 0.0, physx::PxQuat(physx::PxIdentity)), actor2, physx::PxTransform(0.0, 0.0, 0.0, physx::PxQuat(physx::PxIdentity)));If I start the simulation the joint position has in the first frame a value of 0.129 but the joint coordinate frames of both actors are exactly on the same position. If I don’t set the center of mass the joint position is 0.0 like I expect it.Is there something that I do wrong? How I get the right value for the joint position?Powered by Discourse, best viewed with JavaScript enabled',\n",
       " 'devtalk-survey-2018': 'Hi everyone!We’re conducting a brief survey to gain insight our developer community’s usage of DesignWorks SDKs, and also to gauge your overall experience on our Professional Graphics & Rendering DevTalk forums. Please fill out this survey:https://developer.nvidia.com/designworks/developer-surveyYour feedback will guide us to plan for future improvements and engagements on the forums and the developer community.Thanks,\\nRyan ParkPowered by Discourse, best viewed with JavaScript enabled',\n",
       " 'spawning-4-more-rays-in-miss-program': 'Basically, after an optixTrace call, if the ray misses I want to spawn 4 more rays at different locations, and if it intersects it will terminate. Is there a way to accomplish this using Optix?Yes, you are controlling when which ray is shot inside the OptiX device programs.Your algorithm description is a little sparse on the details, so I’m assuming the four additional rays are not spawning additional four rays recursively and the whole algorithm terminates on the very first intersection.Since the four additional rays should be started when the first one misses, this would best be done inside the ray generation program. (It’s also possible to call optixTrace from the miss program, but I wouldn’t do this.)So here’s some pseudo algorithm:If you don’t need the closest intersection but only a boolean result if there is any intersection on one of the probe rays, then the optixTrace ray flags can be set to terminate on the very first hit (not necessarily the closest hit).\\nThat looks like this: https://github.com/NVIDIA/OptiX_Apps/blob/master/apps/rtigo10/shaders/brdf_diffuse.cu#L180Thx for the response, I was wondering now, what if those 4 rays also should be able to spawn 4 more rays if either one of them misses, else that ray terminations.Your algorithm description is a little sparse on the details, so I’m assuming the four additional rays are not spawning additional four rays recursively and the whole algorithm terminates on the very first intersection.Basically thisIt’s possible with OptiX, and there are multiple ways you might handle this.One way is to generalize on what Detlef suggested, and loop over your spawned rays in your raygen program. You’ll need a way to collect spawned rays to work on in local or global memory, so this is feasible for a small number of spawned rays, but probably a bad idea if you have deep recursion and a branching factor of 4. The number of rays can increase exponentially, and you should avoid trying to keep a lot of intermediate state around for rays in-flight in a single thread.Another way is to use actual recursion. You can call optixTrace from your miss shader, which can trigger another miss shader, and recursively call optixTrace. Included in the reasons Detlef advised against this is because you will have to calculate your maximum stack depth, and allocate memory for it (see the optix*SetStackSize() functions), and it’s very easy to run out accidentally or miscalculate and experience hard-to-debug crashes. So recursion can be done, but will probably cause you some pain, and the amount of recursion you can get might end up being fairly limited in practice.A third way is to write a wavefront renderer and process one ‘wave’, or one segment of ray depth, at a time per OptiX launch / kernel. Save all the ray results to a buffer, and then generate a new wave (a new launch) to start where the previous one left off. Each wave can terminate rays and add new ones as you see fit. This is a very expensive option because the memory traffic required is significantly more than when you spawn a ray “inline” during a thread, but the advantage is that you will have the ability to control your memory usage and never risk running out like the above two suggestions. You might have to tile your renders and have a work queue management system. And you might have to wait a long time for the results, but compared to the other options it will be relatively easy to get the results without hitting any major walls.Do you have specific needs for a branching factor of 4, or any number higher than 1? Note that what people do most often is use a branching factor of 1. This means that when you want to cast secondary rays, each ray that hits or misses spawns only a maximum of 1 replacement ray. One way to think about this is it’s conceptually the same optimization as tail recursion. A branching factor of 1 is generally what people mean when they talk about “path tracing” (though the term does have overloaded meanings). One major benefit of a branching factor of 1 is that you can loop in your raygen program, and you don’t need to use recursion or wavefront, or worry about an exponential memory explosion. A higher branching factor leads to exponential work for a pixel, and each ray has exponentially diminishing results. I think it is considered best practice to expend the majority of work effort early in your ray ‘tree’, like in the first and second path segments, rather than deeper in the tree, and instead of having a high branching factor, spend the time you save casting more primary rays. Apologies for lecturing if you know all this and have definite reasons for needing a higher branching factor.–\\nDavid.Powered by Discourse, best viewed with JavaScript enabled',\n",
       " 'optix-prime-loading-meshes-building-sbvh-or-trbvh-using-watertight-option': 'Greetings,First of all I am completely new to OptiX and OptiX Prime, so if these are trivial questions please do forgive me. I am yet to go over the programming guide and api documentation. Even if you cannot give complete answers, I would greatly appreciate if you can point me in the right direction.I am involved in an application which maps aerial quadcopter images to digital elevation maps. So primeInstancing project in Optix SDK samples seemed as a great point to start. The elevation data is defined completely on a regular grid (every 1m along x and y axes on the ground). This regular mesh is completely opaque.(I am currently trying to create .obj files of my regular grid mesh, but this cannot be a long term solution)In order to make things faster I do wish to use a BVH structure (sBVH or TrBVH). Does OptiX Prime handle this by itself (for instance when I define a Model Object) or do I need to define this? If so, how?If I can construct these BVHs, is there a mechanism to serialize BVHs? Even though the portion of the elevation data at an instance I need to plan is small, total area to cover is not and loading precomputed BVHs from disk to ram to device memory in a predictive manner may be desirable.When I use RTP_BUFFER_FORMAT_HIT_T_TRIID_U_V, how can I determine which corner of the triangle correspond to u, which one to v and which one to w=1-u-v?I wanted to make sure that ray intersection operations to be watertight and I have come across RTP_QUERY_HINT_WATERTIGHT, but it says the following in its enumeration “Use watertight ray-triangle intersection, but only if the RTP_BUILDER_PARAM_USE_CALLER_TRIANGLES builder parameter is also set”. Could you please explain this a little bit more?I do wish to use u and v values of the intersection to compute actual physical coordinates and I do wish to do this with another CUDA kernel. Is there anything I need to consider while accessing, RTP_BUFFER_TYPE_CUDA_LINEAR type buffer.Thank you very much and sorry for the barrage of questionsHi there,The OptiX programming guide is covering quite a few of these topics, I recommend starting there, but I’ll make some notes here just to quick-start you.1- You give OptiX a buffer of vertices. Indices are optional. If your verts come in triplets that define triangles in order, then you don’t need indices. Otherwise, you can give a buffer of indices into the vertex array that define your triangles. This is pretty standard, similar to obj and gltf and other formats as well as most mesh APIs. https://raytracing-docs.nvidia.com/optix_6_0/guide_6_0/index.html#prime#triangle-models2- BVHs are created and traversed for you.3- It’s faster to build a BVH from scratch than serialize, so we do not provide a way to serialize.4- Your UVs will correspond to the vertex order of your triangle.5- The link above on triangle models describes what RTP_BUILDER_PARAM_USE_CALLER_TRIANGLES does.6- RTP_BUFFER_TYPE_CUDA_LINEAR means your buffer is a block of CUDA memory on your GPU. There’s nothing else to consider, you can run a separate kernel on it to compute hit points after your OptiX prime launch is complete.–\\nDavid.Powered by Discourse, best viewed with JavaScript enabled',\n",
       " 'connect-cloudxr-server-with-monado-runtime': 'Currently we are hosting CloudXR Server with SteamVR Runtime, Can we Host CloudXR Server with Monado.Powered by Discourse, best viewed with JavaScript enabled',\n",
       " 'optix-does-not-check-error-code-from-setdevices': 'Hi,the following code:outputs the following message.error  /home/ben/workspace/USSCudaLib/src/SceneWorld.cpp : 273 cannot set while device is active in this processIn my opinion setDevices (alias rtContextSetDevices) should’ve already catched the error and not my additional check afterwards.BennyCan you check the rtContextSetDevices return error code? Is it cudaSuccess?Yes the return code is cudaSuccess (0).Do you still get the error if you put a cudaThreadSynchronize before the optix::Context::setDevices call?Powered by Discourse, best viewed with JavaScript enabled',\n",
       " 'optix-6-0-0-failing-with-failed-to-load-optix-library-with-driver-425-25': 'Hi,I am running an Optix application on Windows Server 2016, using 4 x TeslaP100 GPU’s, running Nvidia driver 425.25, but get the error “Failed to load OptiX library.”On my dev-PC I got the same error before upgrading to a newer driver version, but upgrading on the server does not remove the error for me.Anything else that I might be doing wrong…? Thanks in advance for any help! (On my dev PC I have CUDA 9.2 installed - not sure that affects anything…?)nvidia-smi output:\\n±----------------------------------------------------------------------------+\\n| NVIDIA-SMI 425.25       Driver Version: 425.25       CUDA Version: 10.1     |\\n|-------------------------------±---------------------±---------------------+\\n| GPU  Name            TCC/WDDM | Bus-Id        Disp.A | Volatile Uncorr. ECC |\\n| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\\n|===============================+======================+======================|\\n|   0  Tesla P100-PCIE…  TCC  | 00000000:2D:00.0 Off |                    0 |\\n| N/A   31C    P0    25W / 250W |      0MiB / 16298MiB |      0%      Default |\\n±------------------------------±---------------------±---------------------+\\n|   1  Tesla P100-PCIE…  TCC  | 00000000:31:00.0 Off |                    0 |\\n| N/A   33C    P0    31W / 250W |   1169MiB / 16298MiB |      8%      Default |\\n±------------------------------±---------------------±---------------------+\\n|   2  Tesla P100-PCIE…  TCC  | 00000000:A9:00.0 Off |                    0 |\\n| N/A   33C    P0    24W / 250W |   1036MiB / 16298MiB |      0%      Default |\\n±------------------------------±---------------------±---------------------+\\n|   3  Tesla P100-PCIE…  TCC  | 00000000:B5:00.0 Off |                    0 |\\n| N/A   33C    P0    24W / 250W |   1036MiB / 16298MiB |      0%      Default |\\n±------------------------------±---------------------±---------------------+Powered by Discourse, best viewed with JavaScript enabled',\n",
       " 'video-codec-sdk-12-1-released': 'NVIDIA Video Codec SDK 12.1.14 is now available with following enhancements:Encode features:Powered by Discourse, best viewed with JavaScript enabled',\n",
       " 'sdk-for-c-and-directx-12-support': 'Hi programmers, reading the SDK Documentation and example apis folders, I see that they are C++ files with solutions in VS2013, with API’s from DirectX 9, 10 and 11 and OpenGL.I have an UI in Visual Studio 2017, with Visual C# 7 WPF prepared for DirectX12 Api for render desktop and games capture and codec. Will there be time to update this language C# 7.2 and the latest Microsoft API?My cordial greetings.PD Project Details\\n· Project: Application WPF Visual C# version 7\\n· SDK’s: Nvidia Capture, Nvidia Codec, NVAPI,\\nNVIDIA Management Library (NVML), Perfkit\\n· UI & Layers Design: Blend for Visual Studio 2017\\n· Project IDE: Microsoft Visual Studio Community 2017\\n· Installer: Visual Studio 2017 Installer Project\\n· Application only 64 bits in .NET Framework 4.7Compatibly:\\n· Active Template Library & Common Language Runtime 4\\n· Microsoft Foundation Classes\\n· Windows Presentation Foundation\\n· API’s: DirectX 12 and Vulkan\\n· DevExpress WPF ExtensionsHi Skylar,Just to clarify, are you asking if the samples will be written in C#?Ryan ParkHi Skylar,Just to clarify, are you asking if the samples will be written in C#?Ryan ParkYes, indeed.I decided to study the syntax of the new language, because I thought it was better.Thank and greetings.Updating the samples to C# is currently not on our roadmap, but I’ll provide feedback to the engineering team.Thanks,Ryan ParkPowered by Discourse, best viewed with JavaScript enabled',\n",
       " 'area-lights-with-triangle-mesh': 'My path tracer supports point light and area light. I store lights in the scene into a buffer. When I need to sample a point on light source, I randomly choose a light from the buffer, and sample a position according the type of the light and its corresponding parameters. This works for point light and area light with analytic shape, like disk or sphere, because a few parameters are need to specify the disk light or spherical light. I also saw many OptiX samples using ParallelogramLight, which can be handled similarly.But when I use triangle mesh for area light, the problem comes. To sample a position on the light, I need to access to the vertex buffer and index buffer of the mesh. The light sampling is called inside the Closest Hit Program, so where should I store the buffer objects?My idea is to, associate each triangle mesh area light with one imaginary geometry, then the buffer objects can be stored on these geometries(or GeometryInstance); when I need to sample a light position, I trace a ray (with special type) into this group of geometries to select an area light, and sample a position using the vertex buffer stored on the GeometryInstance. But this method seems a little overkill, I wonder if there is a better solution.If the light is supposed to be global you can put the buffers into the context. Many different solutions of placing light geometry are possible. It depends on what sampling method you are going to use. It can be placed independently from other scene geometry or together. If I understood it right, the method of sampling mesh light you described sounds like solid angle sampling with respect to self-intersection of the light mesh. Yes, it might be suboptimal. Good survey of sampling direct illumination you can find here http://www.cs.virginia.edu/~jdl/bib/globillum/mis/shirley96.pdfThank you. Sorry that my description is quite misleading. By sampling light I mean directly sampling a position on light source, not by a BSDF solid angle sampling. This is exactly the direct illumination method that you mentioned.The lights are global, and they are placed in a buffer on Context. Each light is a struct that specifies the light’s type and corresponding parameters. I illustrate the struct below.My problem is that, if the light is area light with mesh, it will need to access the mesh’s vertex buffer to sample a position on the mesh. This cannot be stored directly in the Light struct like other parameters, because it’s a buffer object, and the Light struct is already in a Buffer.Please forget the method I mentioned in the end of my first post. My second thought is to store the vertex buffer object on Context, and store a ‘tri_idx’ parameter in Light struct to indicate which vertex buffer will be used. But in this way, I can only support fixed number of mesh area lights (although this is not a problem for most scenes). For example, if I want to support at most 2 mesh area lights in my path tracer, then I store two buffer objects vertex_buffer_1 and vertex_buffer_2 on Context, although there may be more than(or less than) 2 mesh area lights in a scene.So the problem is that you want to use unknown vertex buffer in a scene at compile time. You can merge all mesh light buffers into one big buffer, that is obviously not very elegant. I think a new feature of coming very soon OptiX 3.5 bindless buffers will help to make it elegant.Thank you, the bindless buffer seems interesting.\\nBut I ended up with fitting disk arealights to all mesh arealights in the scene, currently it’s not a critical feature for me.Powered by Discourse, best viewed with JavaScript enabled',\n",
       " 'unreal-engine-5-2-dlss-g-frame-generation-set-dlss-g-mode-is-broken': 'Hello,I am posting this to inform NVIDIA and their Unreal Engine 5 Development Team that the ‘Set DLSS-G Mode’ function is broken.  I will submit a Pastebin for you to see the error codes it is throwing.  Also, when used in a widget with settings like a Video Settings Menu, it breaks all the Button States from being used.   This is caused by the broken function of DLSS-G (Frame Generation - Streamline).Everything else works great!  Thank you again for your help and support!Pastebin.com is the number one paste tool since 2002. Pastebin is a website where you can store text online for a set period of time.image1907×656 69.6 KBHello @JustinCason and welcome to the NVIDIA developer forums!At the top of your log I could find  the following:To utilize Frame Generation in UE 5.2 you will need to use a 40 series GPU.Thank you!Powered by Discourse, best viewed with JavaScript enabled',\n",
       " 'can-i-directly-program-with-rops-and-rt-unit': 'I just want to program directly with GPU and avoid invoking opengl , DirectX,  etc.\\nIs there any documentation about this?\\nThanks.Powered by Discourse, best viewed with JavaScript enabled',\n",
       " 'nvidia-p2p-get-pages-failing-with-error-code-22': 'I am implementing NVIDIA GDS with the following hardware config:GDS was installed and verified successfullyBut when I was running their test benchmarks it was failing as below:Checking dmesg logs found:Digging up some articles I found that GPU Direct RDMA is supported only for Tesla/Quadro class GPU’s. I am curious to know what’s preventing RTX 3090 to support this, is it something on the hardware that’s missing or some driver module?Hey @utkrishtp,Yes and Yes. There are significant differences in both the HW design of Server GPUs as well as the necessary SW. And as far as I am aware the underlying system architecture of the Server running one of the HPC GPUs needs to be compatible to use GPUDirect Storage as well.I am sorry, but right now your RTX 3090 in a normal desktop setup is not officially supported.Powered by Discourse, best viewed with JavaScript enabled',\n",
       " 'render-with-optix-inside-docker': 'Hi,I’m currently trying to render frames with Blender using OptiX inside of Docker. The host machine is a g4dn.xlarge EC2 instance, with a Nvidia T4 GPU attached and nvidia driver/nvidia-docker2 installed. When forcing the use of OptiX on the host machine, the rendering script works properly. However, if I try to run the same script inside of an Ubuntu Docker image, I get the following error:Please find here the Dockerfile I’m using to build the image.I found this issue that seems to be somehow related and states that some files need to be mounted into the container:When not mounting them, the OptiX device query does not work at all. When mounting them, the device query works but the rendering fails with the aforementioned error.\\nAlso please find below the output of the nvidia-smi command on the host machine:Do you folks have any hints ?Thanks a lot in advanceHi @paul61, welcome!I think a T4 is supposed to work with OptiX 7, but using OptiX from within docker has always been a little sketchy due to the need to mount driver components manually. Unfortunately I don’t yet have any experience using docker with OptiX, but my first guess is that there’s another .so file you need to mount.Here are a few things I can think to try:Inside your docker environment, you can run strace on your process and see if the failure is due to a missing .so file.Which version of the OptiX 7 SDK are you using? The latest version (7.2) requires a 455 driver or higher, and you’re on 450.Does nvidia-smi run from inside docker? What is the output?Does your code run on this host outside of the docker environment?Can you run a CUDA sample inside your docker environment? One easy thing you can test without installing CUDA samples is, if you haven’t already, put a call to cudaFree(0) before calling either optixDeviceContextCreate() or optixInit(), and make sure the cuda call succeeds.–\\nDavid.Powered by Discourse, best viewed with JavaScript enabled',\n",
       " 'help': 'Hello,Having MAJOR issues With my gpu driver or my card, but 90% looks like its the drivers, after the last updates the Graphics have been flickering on games like rays all over the screen, only in games not in desktop then it usses the Integrated card, and just now latly im getting driver crash errors. after this last update i tured my comp on use NVIDIA gtx 660m high preformanse on physic x and on 3d and it worked i was super happy, and i got to play BF4 agian for mabey 30 min and boom game frose, and the error driver 344.75 stoped working nvidia Windows kerel/kenerel or something crashed and rebooted happens alot ig not i get the flickering shitt!! my card is 2 years old. and had these problems for about 2 months is, taken card out and replugged it, deleted all drivers re installed em, should i buy a New card? has it started to get faulty? is the drivers falty? HELP!!! as i said everything worked Perfect for 30min and boom crashed again so im hope its driver issues not the card?!?!all help is wanted thx\\nPusHAlienwear M17XR4 Laptop case\\nCPU: Intel® Core™ i7-3720QM (6MB Cache, up to 3.6GHz w/ Turbo Boost 2.0\\nRAM: 8192 MB (2 x 4 GB) 1600 MHz two channel DDR3\\nGraphics : 2GB GDDR5 NVIDIA GeForce GTX 660M\\nHarddrive: 500GB 7,200 RPM + 32GB mSATA Caching SSD\\nsystem: Windows® 7 Home Premium, 64-bitersThis is a developer forum dedicated to GPU programming questions mainly.\\nYou might get more feedback for end user questions like yours on the GeForce community forums at [url]https://forums.geforce.com/[/url]Thats what i dident, and there is no Clear channel for irect contact With support this was the Closes i found =)[url]LMGTFY - Let Me Google That For YouIf you think this is a driver regression which happened in the last two months as you say, which driver version worked before?\\nDoes it work again if you revert to these drivers?\\nIf yes, follow the first link found by above search. The bottom most paragraph on that support page should enable you to report the issue directly.Powered by Discourse, best viewed with JavaScript enabled',\n",
       " 'solved-triaid-in-optix-and-other': 'Hi, I have two questions but they are not completely unrelated. I very much would appreciate to have an answer for the first one.I have a mesh made of triangles only.Looking at the SDK tutorial and samples, I found out that the any_hit and closest_hit are very convenient for the ray operation I want to do (monte carlo sampling).However, for doing that I need to access the coordinates of my mesh during the any_hit / closest_hit programs (I need uniform sampling on the surface of each triangle). I saw some examples where I can pass the coordinates of the vertex through buffer. However, I did not find any way to access the id of the triangle inside the any_hit / closest_hit programs.If I do not get that information, there is no way I can access the correct vertex information. (Or am I wrong ?) So my question are:Q1a) how can I access the id of the triangle that the ray hit in the any_hit routine ?Q1b) how can I access the id of the triangle that is the closest to the hay in the closest_hit routine  ?I know Q1a) and Q1b) are not relevant if I use optix prime (I had a look at the prime* examples, there as a structure with the triaId) but it looks far less flexible. Since I need to also store data related to each triangle during the any_hit operation, I definitely need the triangle id.Approach 2a): analytical samplingThis method uses quite recursive.\\nI estimate the number of time the number time a ray is going to generate other ray to be about 10 in average, but it will diverge very significantly.So my question is:Q2a)  If the multiple number of reflections for one thread is 100, will the other have to wait for that thread to finish so all the other ones starts the full sequence again ?Approach 2b): sampling using random numberThis method is far less recursive.\\nSo I assume I need a stack size which will be smaller so I will be able to launch much more thread (?), also Optix will be faster (?)The advantage of the method 2a) is that I can actually express in an analytical way all the reverse sample distributions I want so I do not really need random number sampling and I will converge  faster toward an accurate solution than with method 2b)The disadvantage is that I am afraid the speed will be limited by the big stack requirement and the fact I can launch less threads at the same time on my GPU (?)Am I correct with my affirmations ? please correct me if I am wrongEventually my question is: which one of the two method is going to be faster ? I assume the only way to answer my question is to implement both methods. But if there is an obvious answer I would like to know.Regarding your first question:The triangle id is used in the intersection program. If you want to pass data from the intersection program to the any/closest hit program, you need to declare a variable using an attribute:Write the triangleId within the intersection program:You can then access triangleId inside of your any/closest hit program.OK, thank you for the clear answer.Regarding the second part of your question, both of your solutions are equally recursive (they both have a recursion depth of 10 on average), so they both will need approximately the same stack size. Also, both will use the same number of threads, that being the number of threads indicated by your call to rtContextLaunch*D(). However, your second method will run much faster as a result of tracing far fewer rays.The downside of the second method is that the results from a single launch will be less accurate, but you could solve that problem by shooting many primary rays from your initial triangle and averaging the results.Regarding the second part of your question, both of your solutions are equally recursive (they both have a recursion depth of 10 on average), so they both will need approximately the same stack size. Also, both will use the same number of threads, that being the number of threads indicated by your call to rtContextLaunch*D(). However, your second method will run much faster as a result of tracing far fewer rays.I am not sure I fully agree, but that maybe because I was not clear enough. In the second case, when a ray is leaving the traingle, there is no need to keep track of the previous event (all the information is carried in the ray: just a single intensity variable).In the first case however, the program needs to keep track of the previous position in the for loops (if I am looping all over a half sphere with discrete time step. for example using this: http://corysimon.github.io/articles/uniformdistn-on-sphere/ ) so it can uniformly discretize the spaceIf you still disagree, please let me know, I must have miss something then.The downside of the second method is that the results from a single launch will be less accurate, but you could solve that problem by shooting many primary rays from your initial triangle and averaging the results.That definitely was my intention as the boundary condition at the wall are not deterministic but are defined in a probabilistic way.More important, I see how to use the 2nd approach with optiX prime (I notice that can also run on CPU which is important for my application).I mean that I can:That is the reason why I wanted to know how was behaving the regular OptiX, thus that question in my initial post:Q2a)  If the multiple number of reflections for one thread is 100, will the other have to wait for that thread to finish so all the other ones starts the full sequence again ?I am not sure I fully agree, but that maybe because I was not clear enough. In the second case, when a ray is leaving the traingle, there is no need to keep track of the previous event (all the information is carried in the ray: just a single intensity variable).It’s possible you are referring to iterative ray tracing, where the ray payload carries the new intersection and direction back to the camera program, and all rays are cast by the camera program. The key difference is where the call to rtTrace is located. If rtTrace is in the closest hit program, then the ray tracing is recursive and requires a larger stack size. If rtTrace is only in the ray generation program, then the ray tracing is iterative and can work with a smaller stack.Q2a)  If the multiple number of reflections for one thread is 100, will the other have to wait for that thread to finish so all the other ones starts the full sequence again ?Within a warp, yes. In the more general case, I’m not sure, but you can force this behavior with rtContextSetTimeoutCallback. Also, be aware that future OptiX versions might not behave like the current one in this regard.I am not sure I fully agree, but that maybe because I was not clear enough. In the second case, when a ray is leaving the traingle, there is no need to keep track of the previous event (all the information is carried in the ray: just a single intensity variable).It’s possible you are referring to iterative ray tracing, where the ray payload carries the new intersection and direction back to the camera program, and all rays are cast by the camera program. The key difference is where the call to rtTrace is located. If rtTrace is in the closest hit program, then the ray tracing is recursive and requires a larger stack size. If rtTrace is only in the ray generation program, then the ray tracing is iterative and can work with a smaller stack.I was indeed aiming at implementing rtTrace in the closest hit program. I actually have to do that for the 1st approach.Thanks for the clarificationPowered by Discourse, best viewed with JavaScript enabled',\n",
       " 'optix-6-0-sample-code-fails-to-run-with-gt-750m': \"I have succeeded to make my OptiX renderer working with RTX on/off with GT 750M/RTX 2080Ti.\\nHowever OptiX 6.0 sample code fails to run.\\nFor example, optixCompressedTexture sample reports exception at the first rtBufferCreate().OptiX Error: 'Unknown error (Details: Function “_rtBufferCreate” caught exception: Encountered a rtcore error: m_exports->rtcDeviceContextCreateForCUDA( context, devctx ) returned (2): Invalid device context)If I add rtGlobalSetAttribute(RT_GLOBAL_ATTRIBUTE_ENABLE_RTX) and disable RTX before creating the context then the program works fine.\\nWhen enabling RTX, the program reports the exception but this is reasonable because GPU used for this issue is GT 750M which doesn’t support RTX.However the programming guide says that RTX is disabled by default.\\nIs this correct?Thanks.Environment:\\nOS: Windows 10 1809\\nGPU: GT 750M 2GB\\nDriver version: 418.81\\nVS2017 15.9.6Sorry, you’re using an unsupported configuration. Please read the OptiX 6.0.0 release notes again.Also if you have these two boards installed in a single system, that also isn’t going to work automatically, because combining a GPU with RT cores and one without in a single OptiX multi-GPU context is not going to be supported either. You’d need to restrict usage to the RTX 2080Ti in that case.That could be done inside your application by explicitly using rtContextSetDevices with a compatible list of devices for OptiX 6.0.0.\\nOr without changing applications, you could use the CUDA_VISIBLE_DEVICES environment variable to limit the GPU devices visible to CUDA and therefore to OptiX.Thank you for your quick response.Sorry, you’re using an unsupported configuration. Please read the OptiX 6.0.0 release notes again.Oops, my mistake, I have supposed that I can use OptiX 6.0 for Kepler GPU while I can’t enable RTX.\\nTherefore should old GPU user continue to use OptiX 5.1?By the way, OptiX 6.0 brings the new sample about compressed texture (release note doesn’t mention it).\\nIs compressed texture supported natively? So GPU directly interpret the compressed texture like ordinary Graphics API?“The amount of missing documentation is just too high”  ;-)That example shows how to use one of the newly added block compressed (BC) buffer formats added to the RTformat enums in optix_declarations.h.\\nIn this example using RT_FORMAT_UNSIGNED_BC6H.CUDA added these sometime ago but OptiX didn’t have an API to make use of them so far.\\nE.g. at the end of this table: [url]https://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__TYPES.html#group__CUDART__TYPES_1g6b3a50368a0aa592f65e928adca9b929[/url]I don’t know, yet, how to generate these, but searching for “CUDA block compressed texture” finds some nice explanations.hi,the same error occurs to me while running optix 6.0 samples on my laptop with an nvidia 780 gtx.is there a way to have them working?No, that is a Kepler GPU. You’re limited to OptiX 5.1.x versions on that system.This topic was automatically closed 14 days after the last reply. New replies are no longer allowed.Powered by Discourse, best viewed with JavaScript enabled\",\n",
       " 'how-is-cuda-context-number-related-to-nvdec-engine-number': 'Talsa T4 has 2 decode engines.\\nAn experiment shows that only 1 cuda context for decoding cannot\\nmaximize the decoding throughtput, while 2 cuda ctx almost fully utilize decode engines.\\nWhy is it like this?It’s not constrained by having 1 or 2 cuda context (there really isn’t anything except the true primary context anyway), but rather by 1 or 2 video streams being decoded. There is a sequential dependency between frames being decoded, so you need at least as many decoder instances as there are decode engines in order to have enough independent work available.Powered by Discourse, best viewed with JavaScript enabled',\n",
       " 'appdecgl-sample-nv12tobgra32-kernel-function-of-colorspace-cu-file': 'In the ColorSpace.cu file how do you got the grid and block size of function Nv12ToBgra32.The Grid size: dim3((nWidth + 63) / 32 / 2, (nHeight + 3) / 2)\\nAnd the block size :dim3(32, 2)If I use less Grid size the picture will display only some part of my video not the full frame, what is the reason?Hi,Please go through the https://devtalk.nvidia.com/default/topic/404921/grid-size-block-size/ which explains the concepts of thread, Block and GIRD. In this specific Kernel you have mentioned one pixel is  run on thread. If you would reduce the GRID size then the kernel wont process the entire image and you are getting part of the pixels getting converted to RGB.Thanks,Ryan ParkPowered by Discourse, best viewed with JavaScript enabled',\n",
       " 'optixlaunch-configuration-revisited': 'Dear OptiX team,the OptiX optixLaunch() function provides launches with a configurable width, height, and depth, which can then be conveniently queried in the ray generation shader.In a previous question (Launch dimensions in LaunchContextnD and optixLaunch), @dhart mentioned that optix uses this information arrange computation for 2D launches in tiles, presumably to exploit coherence in primary rays.I am wondering: what are the best practices for optix kernel that computes multiple samples per pixel? How do I get the best mapping from launch indices to CUDA cores in this case? For instance, should I set the launch depth to the sample count? Or is it better to put the sample count in the ‘width’ position of the optix launch, since it is perhaps the most important dimension for capturing coherence?I am also curious to know if any of the best practices change when the ray generation program contains a full path tracer loop that does a sequence of ray tracing calls (these tend to become incoherent after the first 1-2 bounces).Currently, we just do 1D launches and this of course works fine, but I am wondering if we are leaving performance on the table. (The last time I tried to use the launch configuration in a better way, I actually saw some small performance regressions, thought I may have misused the feature)Thank you,\\nWenzelHi @wenzel.jakob, nice to see you!These are good questions. It depends a bit on what you need, but I’ll try to clarify several scenarios.For multiple samples, do you want/need each sample’s result to end up in a separate pixel? Or can you average the result of multiple samples in a single thread at the end of raygen before writing the result?Your question suggests you may want the former. Our SDK sample optixPathTracer does the latter. If you can avoid writing individual samples, and instead perform the reduction in a single thread then we would generally expect this to be faster than storing the results for each sample separately, regardless of the thread-pixel mapping. It’s perhaps assumed that each pixel in this case takes the same number of samples, to minimize thread divergence.We tile the 2d launch into warp-sized tiles in OptiX because there is usually some performance benefit from doing so for primary rays, and it almost never hurts. And yes, the benefit comes from ray coherence: usually memory loads and cache hit rates for BVH, geometry, shaders, and textures are all higher for rays that are very similar compared to rays that go different directions, just because the rays tend to traverse similar spaces and hit similar locations in the scene. For multi-bounce path tracing, especially for diffuse materials, this tiling scheme doesn’t provide much benefit, and I don’t have any clever tricks or good advice to offer in that case-  the speedup is mostly determined by the camera rays, and the amount of speedup you get may be limited by your average path depth.With a 1d launch, we don’t do any tiling, but you can do this tiling yourself, and verify whether you see some benefit. If you need to store each sample separately, then whatever your mapping is, try to arrange it so that consecutive thread ids correspond to very similar rays. This would mean putting (for example) jittered primary ray samples consecutively.With an OptiX 2d launch, if you use width*height*numSamples number of threads, then you would probably want to arrange your samples into blocks or tiles so all samples for a pixel are in a tile, which would happens automatically if you render a large image. For example, if you render a 1080p image with 16 samples per pixel, you could use a 2d launch of (4*1920, 4*1080) == (7680, 4320). This way you can get the tiling benefit without any change to your indexing at all. The only thing you’d need to do is run a separate reduction kernel on the 4x4 image tiles.We don’t tile the 3d launch. With a 3d launch, given a launch size specified by (width, height, depth), by default the launch is depth-major, followed by row-major, so the X (or width) coordinate is the inner-most index. This means you’d want to map your multiple camera ray samples to the X / width parameter in order to have coherent rays grouped into warps.So, advice is to not map samples to individual threads, if you can (understanding that this might not be possible in your case if your postprocessing reduction is more sophisticated than a simple averaging and/or if pixels need data from neighboring pixels). Not mapping samples to threads would mean use width*height number of threads for your launch, and for samples use a loop in raygen. The next easiest fallback is to render a larger image with a 2d launch that you can downsample/reduce with a post-kernel, but comes with the downside of needing a constant, rectangular number of sub-samples. Third would be a 1d or 3d launch where you pay attention to the indexing and put neighboring samples into neighboring threads.Does that help? Let me know if any of it is unclear or reveals more questions.–\\nDavid.We don’t tile the 3d launch. With a 3d launch, given a launch size specified by (width, height, depth), by default the launch is depth-major,Note that this detail actually allows 3D launches of width * height * samples per pixel dimensions without writing the individual results into separate 3D launch indices and a post-process for the accumulation.\\nInstead you can accumulate each sample per pixel on the fly into a 2D width * height output buffer using atomics, and these are not going to slow down much because the 3D launch indices are handled as 2D slices, means there will never be any congestion of the atomics running over the z-dimension at reasonably sizes.Note that the maximum launch dimension in OptiX is 2^30.I’ve written a tile-based renderer which launched all samples per pixel for each tile (with launch dimension of  around 1M launch indices) with separate output locations and a native CUDA accumulation kernel and the result was NOT faster than rendering full 2D images for each sample with accumulation inside the raygeneration program, which means the scheduler in OptiX worked well.\\nAlso trying to optimize with this information is essentially relying on implementation dependent behavior.For interactive workloads I would not recommend doing that. Long running kernels under Windows WDDM are usually bad. That’s something better suited for compute-only devices. Doing less work more often, which in this case for example could mean one 2D launch per sample or smaller tiles, would result in better interactivity.\\nOptiX launches are asynchronous and the launch overhead is a few microseconds. This can also fill the CUDA stream with enough work to make things non-interactive but would prevent Windows WDDM issues.Thanks so much and @dhart and @droettger, this is very useful information.Just for clarity, when we are referring to 1D or 2D launches, these are optixLaunch commands where the trailing dimensions are 1? (IIRC there used to be optixLaunch comments of different dimensions in pre-OptiX 7 times, but these don’t exist anymore)Now regarding what we do, I suspect it is probably crazy/different from typical OptiX usage:we render N monte carlo samples per pixel (where N might be relatively big, say, 1024). This is on linux where WDDM isn’t an issue, and the 2^30 launch limit is definitely in sight (it’s no problem to do multiple passes of course).each pixel sample is accumulated into multiple pixels based on a pixel reconstruction filter (gaussian, mitchell, etc.). This is the same image reconstruction approach also taken by PBRTv3 or Mitsuba on the CPU (in fact, the OptiX version of our renderer is generated by a JIT compiler based on the existing CPU rendering code).This does a lot of atomic operations: for a 4x4 reconstruction kernel, and RGB, alpha, weight output channels, we have 4x4x5 = 80 atomic scatter-adds per sample!Millions of threads hammering global memory using atomic memory operations–what could possibly go wrong? It’s incredibly impressive that this is actually quite performant on NVIDIA hardware (switching to a box filter only makes a small difference, usually < 5% of the total render time).The reconstruction filter is actually a quite critical aspect for us, because we are differentiating the rendering process to run gradient-based optimization algorithms. A box reconstruction filter would not be differentiable in its position argument and therefore produce incorrect results.A few more follow ups:I would be curious if you have feedback on the atomic sample splatting – is what we do reasonable given the requirement of using a non-box pixel reconstruction filter?What is the tile size used by OptiX for 2D launches? 4x4 for a 16 samples per pixel image was mentioned, but the SIMD width of CUDA ALUs is 32, correct?Right now, we use a 1D launch where samples within a pixel are next to each other (fastest), then rows, then columns of the image. However, if I interpret your suggestions above, it sounds like a 2D launch of shape (sqrt(samples_per_pixel) * width, sqrt(samples_per_pixel) * height, 1) might reap some additional benefits from ray coherence. Did I understand this correctly?Thanks again!when we are referring to 1D or 2D launches, these are optixLaunch commands where the trailing dimensions are 1Correct, 1D launches are (width, 1, 1) and 2D launches are (width, height, 1).I would be curious if you have feedback on the atomic sample splatting – is what we do reasonable given the requirement of using a non-box pixel reconstruction filter?There is not much to do about that when there is no 1-to-1 relationship between launch indices and result cells, that is, when not using gather algorithms.\\nScattering algorithms require atomics because there is no information about neighboring launch indices available in a single launch with OptiX’ single ray programming model. That information would only be available between launches and you could do whatever you want with the current results in native CUDA kernels or as input to other OptiX launches.What is the tile size used by OptiX for 2D launches?The warp-sized blocks are 32x1 in 1D and usually 8x4 in 2D and 3D.\\nYou can see them as blocky corruption inside the image when you forget to initialize some per ray payload ;-)\\nOr when visualizing the clocks taken for each launch index. Some examples have a “time view” feature which looks like this:\\nhttps://developer.nvidia.com/blog/profiling-dxr-shaders-with-timer-instrumentation/Right now, we use a 1D launch where samples within a pixel are next to each other (fastest), then rows, then columns of the image. However, if I interpret your suggestions above, it sounds like a 2D launch of shape (sqrt(samples_per_pixel) * width, sqrt(samples_per_pixel) * height, 1) might reap some additional benefits from ray coherence. Did I understand this correctly?1D launches are not tiled. They are simply run in 32x1 blocks for each warp and if the same pixel gets handled in a full warp that would spatially be optimal.The other ideas to make the launch dimension a super-resolution of the image, David described above, would result in a similar spatial ordering of the SPP launch indices to warps due to the 8x4 blocks as long as that covers pixel equally, but that wouldn’t be as perfect for all samples per pixel sizes.The 3D launch would revisit some of the 2D launch indices multiple times so that would also not be spatially perfect for the launch indices to warp assignments. Accumulating the SPP into a 2D image from a 3D launch is just a method to reduce the number of launches. It would actually be slower when iterating in z-dimension per launch index because that would be the worst memory access pattern.If you use a scattered write algorithm anyway, I think your 1D launch is just fine.\\nThese ideas were more along the lines of having gather algorithms and a native CUDA post-processing kernel in which you could use all available CUDA features like shared memory to speed up the accumulation.You could always experiment with a different spatial assignment of these 1D launch indices to 2D pixels in your image, simulating any block layout you want or space filling curves. It would be interesting to see if that would affect the congestion of the atomics. My gut feeling is that the 1D launch has less congestion than a 2D launch if you need to fill in square kernels around each pixel but that would need to be benchmarked.So a scatter with atomics really changes everything. ;) So far I’ve personally avoided atomics so much that my experience with them is limited.Avoiding atomic contention requires making sure adjacent threads executing at the same time don’t all try to lock the same resource at the same time, while the coherence benefit with tiling requires making sure adjacent threads in a warp are all trying to read from the same memory at the same time. There’s a built-in conflict here, and I’m guessing that having a write atomic per sample might outweigh the benefits of tiling coherence. It seems entirely possible you could get the best performance out of an incoherent ray workload, by avoiding any tiling. @droettger’s point about using a 3d launch with 2d indexing is good- less restrictive on number of samples than a large super-sampled image, though I would guess with an atomic write per sample you might want to do the opposite of what I suggested above and order your indices to ensure rays are far apart.If you were to separate your passes into a render of sample data followed by a gather, how much data would you need to store per sample? You could avoid atomics this way, though I would guess this would not be faster than atomics. Possibly much slower if the data is large, but maybe it’s not out of the question to consider if your sample data is very small.–\\nDavid.Thank you @dhart and @droettger for these helpful suggestions!Sharing in case it is interesting to you: limited experimentation thus far indicates that global memory atomics on the Turing architecture are fast! Simply removing all of the atomics and writing out the raw  per-sample information (image space position, RGB value) ends up being slightly slower (~1%) compared to merging those samples into a single image buffer using global atomics issued from within the raygen program.(I am speculating here, but perhaps this is due to the significantly larger number of writes that need to pass through L2 and go to global memory, whereas the cache can be more effective when writing to a comparably small output image?)Note that this is even missing the cost of an additional kernel that would be needed to perform a separate sample accumulation, so the slowdown would likely be greater in practice.Altogether, I am quite surprised by the reasonable performance of the naïve approach we are currently using, which seems to violate all common sense regarding atomic memory operations. I’ve read somewhere that newer NVIDIA GPUs have an ALU within the L2 cache that is used to merge contending updates, which might have have some role to play here…?I also tried to see if reordering the wavefront indices to avoid conflicts would help (for example, to render rows, then columns, then individual samples instead of samples, then columns, then rows). This produced a roughly 5% slowdown, so the coherence on the first bounce seems to be beneficial and outweigh the cost of large numbers (4x4x5 == 80) of contending global memory atomics at the end of the ray generation program.Details on setup: I am using a Titan RTX, rendering the ‘staircase’ scene from Benedikt Bitterli’s scene repository at 720p, 128 samples/pixel, 9 bounces.One unrelated observation is that the official API documentation of optixLaunch is a little terse. Documenting how the launch configuration affects ray coherence (& batching into tiles) might be useful to other users of this function.Yes this is super interesting, thanks for sharing @wenzel.jakob! These results indeed are better than what I assumed. Speaking personally, there does seem to be a pattern of having naïve and brute force solutions surprise me with better performance on the GPU than all the clever tricks I’ve learned for CPU renderers over the years. It’s great that you see a perf benefit with the coherent ray tiling in the presence of your atomic scatter!I will take the feedback on documentation to our team and maybe we can add something about the tiling, it’s a good suggestion to let people know the tiling is there, and might also help people who want to do their own tiling in the 1D and 3D launches.–\\nDavid.Powered by Discourse, best viewed with JavaScript enabled',\n",
       " ...}"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "forum_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01d0e9ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "keys = list(forum_data.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05cd25d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "values= list(forum_data.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "2c88d0ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataframe = pd.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b7f6c8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = {\n",
    "    'query': keys,\n",
    "    'data': values\n",
    "    \n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c90fc981",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(data)\n",
    "\n",
    "\n",
    "# Display the DataFrame\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2871ace1",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('infrastructure.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ced14f2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82f62de8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c1dc8bc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de16fab9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7a004df",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
